2006.amta-papers.7,E06-1032,0,0.0204439,"guistic Data Consortium (LDC). We use an Arabic-English parallel corpus of about 5 million words to train the translation model.5 For Arabic preprocessing, the Arabic Treebank scheme is used (Habash and Sadat, 2006). All systems use the same surface trigram language model, trained on approximately 340 million words of English newswire text from the English Gigaword corpus.6 English preprocessing simply included downcasing, separating punctuation from words and splitting off “’s”. Trigram language models are implemented using the SRILM toolkit (Stolcke, 2002). Both BLEU (Papineni et al., 2002; Callison-Burch et al., 2006) and NIST (Doddington, 2002) metric scores are reported. All scores are computed against four references with n-grams of maximum length four. As a post-processing step, the translations of all systems are true-cased, and all results reported below refer to the case-sensitive BLEU and NIST scores. We conducted three sets of evaluations that explore different aspects of the data sets and the system variants: a full system evaluation, a genre-specific evaluation, and a qualitative evaluation of specific linguistic phenomena. 6.1 Full Evaluation Six system variants are compared: • G IST is a simpl"
2006.amta-papers.7,P05-1066,0,0.16191,"rphological preprocessing helps, but only for smaller corpora. Habash and Sadat (2006) reached similar conclusions on a much larger set of experiments including various preprocessing schemes and techniques. They showed that genre variation interacts with preprocessing decisions. Within our approach, working with Arabic morphology is especially challenging. We discuss this issue in more detail in Section 3. 2.2 MT Hybridization More recently a number of statistical MT approaches included syntactic information as part of the preprocessing phase, the decoding phase or the n-best rescoring phase. Collins et al. (2005) incorporated syntactic information as part of preprocessing the parallel corpus. A series of transformations on the source parse trees were applied to make the order of the sourcelanguage words and phrases closer to that of the target language. The same reordering was done for a new source sentence before decoding. They showed a modest statistically significant improvement over basic phrase-based MT. Quirk et al. (2005) used sub-graphs of dependency trees to deal with word-order differences between the source and the target language. During training, dependency graphs on the source side were"
2006.amta-papers.7,P97-1003,0,0.0433478,"and training data size. Symbolic MT approaches tend to capture more abstract generalizations about the languages they translate between compared to statistical MT. This comes at a cost of being more complex than statistical MT, involving more human effort, and depending on already existing resources for morphological analysis and parsing. This dependence on existing resources highlights the problem of variation in morphological representations for Arabic. In a typical situation, the in58 put/output text of an MT system is in simple whitespace tokenization. But, a statistical parser (such as (Collins, 1997) or (Bikel, 2002)) trained out-of-thebox on the Penn Arabic Treebank (Maamouri et al., 2004) assumes the same kind of tokenization it uses (4-way normalized segments into conjunction, particle, word and pronominal clitic). This means a separate tokenizer is needed to convert input text to this representation (Habash and Rambow, 2005; Diab et al., 2004). An additional issue with a treebank-trained statistical parser is that its input/output is in normalized segmentation that does not contain morphological information such as features or lexemes that are important for translation. Arabic-English"
2006.amta-papers.7,N04-4038,0,0.0379168,"endence on existing resources highlights the problem of variation in morphological representations for Arabic. In a typical situation, the in58 put/output text of an MT system is in simple whitespace tokenization. But, a statistical parser (such as (Collins, 1997) or (Bikel, 2002)) trained out-of-thebox on the Penn Arabic Treebank (Maamouri et al., 2004) assumes the same kind of tokenization it uses (4-way normalized segments into conjunction, particle, word and pronominal clitic). This means a separate tokenizer is needed to convert input text to this representation (Habash and Rambow, 2005; Diab et al., 2004). An additional issue with a treebank-trained statistical parser is that its input/output is in normalized segmentation that does not contain morphological information such as features or lexemes that are important for translation. Arabic-English dictionaries use lexemes and proper translation of features, such as number and tense, requires access to these features in both source and target languages. As a result, additional conversion is needed to relate the normalized segmentation to the lexeme and feature level. Of course, in principle, the treebank and parser could be modified to be at the"
2006.amta-papers.7,H05-1085,0,0.0456068,"and to butter (Dorr, 1993). The overgeneration is constrained by multiple statistical targetlanguage models including surface n-grams and structural n-grams. The source-target asymmetry of systems developed in this approach makes them more easily retargetable to new source languages (provided a source-language parser and translation dictionary). In this paper, we describe these two specific extensions for Arabic in detail (Section 4). SMT quality focused on morphologically rich languages such as German (Nießen and Ney, 2004); Spanish, Catalan, and Serbian (Popovi´c and Ney, 2004); and Czech (Goldwater and McClosky, 2005). These studies examined the effects of various kinds of tokenization, lemmatization and part-of-speech (POS) tagging and showed a positive effect on SMT quality. Lee (2004) investigated the use of automatic alignment of POS-tagged English and affix-stem segmented Arabic to determine appropriate tokenizations of Arabic. Her results showed that morphological preprocessing helps, but only for smaller corpora. Habash and Sadat (2006) reached similar conclusions on a much larger set of experiments including various preprocessing schemes and techniques. They showed that genre variation interacts wi"
2006.amta-papers.7,habash-dorr-2002-handling,1,0.840173,"T implementations in Section 3. Section 4 describes the Arabic components of our basic GHMT system. Section 5 describes the extensions we made to integrate SMT components into the GHMT system. Section 6 presents three evaluations of multiple MT system variants. 2 Previous Work We discuss research related to our approach in the areas of generation-heavy MT and MT hybridization. 2.1 Generation-Heavy MT GHMT is an asymmetrical hybrid approach that addresses the issue of MT resource poverty in source-poor/target-rich language pairs by exploiting symbolic and statistical target-language resources (Habash and Dorr, 2002; Habash, 2003a; Habash, 56 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 56-65, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas 2003b). Expected source-language resources include a syntactic parser and a simple one-to-many translation dictionary. No transfer rules or complex interlingual representations are used. Rich targetlanguage symbolic resources such as word lexical semantics, categorial variations and subcategorization frames are used to overgenerate multiple structural variations from a target-"
2006.amta-papers.7,P05-1071,1,0.921181,"sis and parsing. This dependence on existing resources highlights the problem of variation in morphological representations for Arabic. In a typical situation, the in58 put/output text of an MT system is in simple whitespace tokenization. But, a statistical parser (such as (Collins, 1997) or (Bikel, 2002)) trained out-of-thebox on the Penn Arabic Treebank (Maamouri et al., 2004) assumes the same kind of tokenization it uses (4-way normalized segments into conjunction, particle, word and pronominal clitic). This means a separate tokenizer is needed to convert input text to this representation (Habash and Rambow, 2005; Diab et al., 2004). An additional issue with a treebank-trained statistical parser is that its input/output is in normalized segmentation that does not contain morphological information such as features or lexemes that are important for translation. Arabic-English dictionaries use lexemes and proper translation of features, such as number and tense, requires access to these features in both source and target languages. As a result, additional conversion is needed to relate the normalized segmentation to the lexeme and feature level. Of course, in principle, the treebank and parser could be m"
2006.amta-papers.7,N06-2013,1,0.900599,"SMT quality focused on morphologically rich languages such as German (Nießen and Ney, 2004); Spanish, Catalan, and Serbian (Popovi´c and Ney, 2004); and Czech (Goldwater and McClosky, 2005). These studies examined the effects of various kinds of tokenization, lemmatization and part-of-speech (POS) tagging and showed a positive effect on SMT quality. Lee (2004) investigated the use of automatic alignment of POS-tagged English and affix-stem segmented Arabic to determine appropriate tokenizations of Arabic. Her results showed that morphological preprocessing helps, but only for smaller corpora. Habash and Sadat (2006) reached similar conclusions on a much larger set of experiments including various preprocessing schemes and techniques. They showed that genre variation interacts with preprocessing decisions. Within our approach, working with Arabic morphology is especially challenging. We discuss this issue in more detail in Section 3. 2.2 MT Hybridization More recently a number of statistical MT approaches included syntactic information as part of the preprocessing phase, the decoding phase or the n-best rescoring phase. Collins et al. (2005) incorporated syntactic information as part of preprocessing the"
2006.amta-papers.7,koen-2004-pharaoh,0,0.392152,"ic N/ap N/ap N Nap writer;author clerk authors;writers authors;writers PV IV PV_Pass IV_Pass_yu write write be written;be fated;be destined be written;be fated;be destined kuwfiy˜_1 AJ Kufic/from_Kufa/of_Kufa kAtib_1 N katab-u_1 V author/clerk/writer be_destined/be_fated/ be_written/destine/fate/write 5 Integration of SMT Components into GHMT The main challenge for integrating SMT components into GHMT is that the conception of the phrase (anything beyond a single word) is radically different. Phrase-based SMT systems take a phrase to be a sequence of words with no hidden underlying structure (Koehn, 2004). On the other hand, for systems that use parsers, like GHMT, a phrase has a linguistic structure that defines it and its behavior in a bigger context. Both kinds come with problems. Statistical phrases are created from alignments, which may not be clean. This results in jagged edges to many phrases. For example, the phrase . on the other hand , the (containing seven words starting with a period and ending with “the”) overlaps multiple linguistic phrase boundaries. Another related phenomenon is that of statistical hallucination, e.g., the translation of AlswdAn w (literally, Sudan and) into en"
2006.amta-papers.7,N04-4015,0,0.0389125,"stems developed in this approach makes them more easily retargetable to new source languages (provided a source-language parser and translation dictionary). In this paper, we describe these two specific extensions for Arabic in detail (Section 4). SMT quality focused on morphologically rich languages such as German (Nießen and Ney, 2004); Spanish, Catalan, and Serbian (Popovi´c and Ney, 2004); and Czech (Goldwater and McClosky, 2005). These studies examined the effects of various kinds of tokenization, lemmatization and part-of-speech (POS) tagging and showed a positive effect on SMT quality. Lee (2004) investigated the use of automatic alignment of POS-tagged English and affix-stem segmented Arabic to determine appropriate tokenizations of Arabic. Her results showed that morphological preprocessing helps, but only for smaller corpora. Habash and Sadat (2006) reached similar conclusions on a much larger set of experiments including various preprocessing schemes and techniques. They showed that genre variation interacts with preprocessing decisions. Within our approach, working with Arabic morphology is especially challenging. We discuss this issue in more detail in Section 3. 2.2 MT Hybridiz"
2006.amta-papers.7,P03-1021,0,0.016325,"Missing"
2006.amta-papers.7,P02-1040,0,0.0743345,"available from the Linguistic Data Consortium (LDC). We use an Arabic-English parallel corpus of about 5 million words to train the translation model.5 For Arabic preprocessing, the Arabic Treebank scheme is used (Habash and Sadat, 2006). All systems use the same surface trigram language model, trained on approximately 340 million words of English newswire text from the English Gigaword corpus.6 English preprocessing simply included downcasing, separating punctuation from words and splitting off “’s”. Trigram language models are implemented using the SRILM toolkit (Stolcke, 2002). Both BLEU (Papineni et al., 2002; Callison-Burch et al., 2006) and NIST (Doddington, 2002) metric scores are reported. All scores are computed against four references with n-grams of maximum length four. As a post-processing step, the translations of all systems are true-cased, and all results reported below refer to the case-sensitive BLEU and NIST scores. We conducted three sets of evaluations that explore different aspects of the data sets and the system variants: a full system evaluation, a genre-specific evaluation, and a qualitative evaluation of specific linguistic phenomena. 6.1 Full Evaluation Six system variants ar"
2006.amta-papers.7,popovic-ney-2004-towards,0,0.0433443,"Missing"
2006.amta-papers.7,P05-1034,0,0.104069,"tion More recently a number of statistical MT approaches included syntactic information as part of the preprocessing phase, the decoding phase or the n-best rescoring phase. Collins et al. (2005) incorporated syntactic information as part of preprocessing the parallel corpus. A series of transformations on the source parse trees were applied to make the order of the sourcelanguage words and phrases closer to that of the target language. The same reordering was done for a new source sentence before decoding. They showed a modest statistically significant improvement over basic phrase-based MT. Quirk et al. (2005) used sub-graphs of dependency trees to deal with word-order differences between the source and the target language. During training, dependency graphs on the source side were projected onto the target side by using the alignment links between words in the two languages. The use of syntactic information is the main difference between their approach and phrase-based statistical MT approaches. During decoding, the different subgraphs were combined in order to generate the most likely dependency tree. This approach has been shown to provide significant improvements over a Research into MT hybrids"
2006.amta-papers.7,2006.amta-papers.25,1,0.654799,"more genre-independent than SMT approaches. We believe this is a result of the Arabic linguistic resources we use being biased towards news-genre. For example, the Arabic treebank used for training the parser is only in the news genre. The Buckwalter lexicon potentially also has some internal bias toward news genre because it was developed in tandem with the Arabic treebank. 6.3 Qualitative Evaluation Automatic evaluation systems are often criticized for not capturing linguistic subtleties. This is clearly apparent in the field’s moving back toward using human evaluation metrics such as HTER (Snover et al., 2006). We conducted a small human evaluation of verb and subject realization in eight random documents from MT04. The documents contained 47 sentences and reflect the distribution of genre in the MT04 test set. We compare three systems G HMT, G HMT P HARAOH and P HARAOH. The evaluation was conducted using one bilingual Arabic-English speaker (native Arabic, almost native English). The task is to determine for every verb that appears in the Arabic input whether it is Table 4: Verb and subject realization in eight documents from MT04 Genre News Speech Editorial All Verb Count 46 48 29 123 G HMT Verbs"
2006.amta-papers.7,J04-2003,0,\N,Missing
2006.amta-papers.7,N04-1021,0,\N,Missing
2008.iwslt-evaluation.15,P02-1038,0,0.024892,"ur decoder in Section 2.3. We then go on to look at the experimental set up and the data used in the in Section ??, before presenting the results of our system in Section 3. This approach is often referred to as the noisy channel approach [1]. p(f|e) is the translation model, or the likelihood of generating the source sentence given the target sentence, and e is the language model, which tells us the likelihood of a given English sentence. Together these form the core of all SMT systems, and thus equation 2 is described as the Fundamental Equation of SMT [1]. Phrase based SMT, as described by [2, 3], extends the noisy channel approach by using a weighted log linear combination of a set of H feature functions, hi , i = 1, ..., H, to score possible translations. ˆe = arg max 2. Translation Framework e,a The aim of Statistical Machine Translation(SMT) is to take a foreign sentence, f, and translate it into an English sentence, e using statistical models generated using machine learning techniques. Using a corpus of foreign and target sentences which we know to be translations of one another, SMT becomes a problem of constructing accurate probability distributions, or models, that can be use"
2008.iwslt-evaluation.15,J04-4002,0,0.0145139,"ur decoder in Section 2.3. We then go on to look at the experimental set up and the data used in the in Section ??, before presenting the results of our system in Section 3. This approach is often referred to as the noisy channel approach [1]. p(f|e) is the translation model, or the likelihood of generating the source sentence given the target sentence, and e is the language model, which tells us the likelihood of a given English sentence. Together these form the core of all SMT systems, and thus equation 2 is described as the Fundamental Equation of SMT [1]. Phrase based SMT, as described by [2, 3], extends the noisy channel approach by using a weighted log linear combination of a set of H feature functions, hi , i = 1, ..., H, to score possible translations. ˆe = arg max 2. Translation Framework e,a The aim of Statistical Machine Translation(SMT) is to take a foreign sentence, f, and translate it into an English sentence, e using statistical models generated using machine learning techniques. Using a corpus of foreign and target sentences which we know to be translations of one another, SMT becomes a problem of constructing accurate probability distributions, or models, that can be use"
2008.iwslt-evaluation.15,P03-1021,0,0.00496834,"e a collection of foreign sentences, unseen in the training data, into a target language. In this section we start by describing phrase based translation, before examining the models we use, and then describing the QMUL decoder. 2.1. Phrase Based SMT Intuitively, given a source sentence f, the problem of statistical machine translation can be formulated as picking the - 104 - H X λi hi (f, e) (3) i=1 p(f|e) and p(e) become a subset of the H different feature functions, each with their own weights λi , which can be trained according to an optimisation criterion based on the translation quality [4], as we have done for IWSLT 08. 2.2. Features In addition to the phrase translation probabilities that have already been discussed, there are a number of common features that we use during decoding. As these features operate over phrase segmentations, it is first usefull to define what a phrase is. If we represent a given sentence pair as (f1J , eI1 ), it’s phrase segmentation can be defined in terms of K units: k → sk := (ik ; bk , jk ), for k = 1...K (4) Proceedings of IWSLT 2008, Hawaii - U.S.A. (bk , jk ) denotes the start and end positions of the source phrase that is aligned to the k th"
2008.iwslt-evaluation.15,2007.iwslt-1.25,0,0.0200954,"full to define what a phrase is. If we represent a given sentence pair as (f1J , eI1 ), it’s phrase segmentation can be defined in terms of K units: k → sk := (ik ; bk , jk ), for k = 1...K (4) Proceedings of IWSLT 2008, Hawaii - U.S.A. (bk , jk ) denotes the start and end positions of the source phrase that is aligned to the k th target phrase, and ik denotes the last position of the k th target phrase. Because the models operate over the phrase segmentations k in s, we say the model features are over (f1J , eI1 , sK 1 ). A more detailed description of the phrase segmentation can be found in [5]. Our language model is a standard n-gram based feature function, where the probability of a given word is conditioned on its history: hLM (f1J , eI1 , sK 1 ) = log l+1 Y p(ei |ei−1 i−n+1 ) (5) used. If smaller phrase applications are desired, a negative penalty would be employed. In our system we also use a binary version of a phrase count feature, which favours phrase segmentations that appear in the bitext over a certain threshold. Generally rare phrase pairs have overestimated probabilities. By taking into account the phrase count, we can make up for data that may just be noise, representi"
2008.iwslt-evaluation.15,N03-1017,0,0.00295386,"penalty, described in Equation 8. A negative penalty favours longer translations, and positive penalties are used to produce shortened translations. hW P (f1J , eI1 , sK 1 )=I (8) Similar to the word penalty, we use a phrase penalty to control the number of phrase applications used in the translation of a source sentence: hP P (f1J , eI1 , sK 1 )=K The feature is binary, so that a phrase has a cost of 0 or 1 in log space, dependent on whether the threshold r is met or not. For our experiments an r value of 4 was manually chosen. For a more detailed description of these models please refer to [7]. 2.3. Decoding The QMUL system is a stack based decoder implemented in C++, similar to the publicly available Pharaoh system [8]. Hypotheses are ranked and stored in stacks, where each stack represents the number of source words translated so far. We start with a null hypothesis, and pick a segment of the source sentence to begin translating. We apply multiple different translations that exist for the same source phrase, and then store the hypothesis in a stack. The hypotheses in a stack are ranked according to their score, which takes into account the phrase translation costs, language model"
2008.iwslt-evaluation.15,koen-2004-pharaoh,0,0.176102,"rtened translations. hW P (f1J , eI1 , sK 1 )=I (8) Similar to the word penalty, we use a phrase penalty to control the number of phrase applications used in the translation of a source sentence: hP P (f1J , eI1 , sK 1 )=K The feature is binary, so that a phrase has a cost of 0 or 1 in log space, dependent on whether the threshold r is met or not. For our experiments an r value of 4 was manually chosen. For a more detailed description of these models please refer to [7]. 2.3. Decoding The QMUL system is a stack based decoder implemented in C++, similar to the publicly available Pharaoh system [8]. Hypotheses are ranked and stored in stacks, where each stack represents the number of source words translated so far. We start with a null hypothesis, and pick a segment of the source sentence to begin translating. We apply multiple different translations that exist for the same source phrase, and then store the hypothesis in a stack. The hypotheses in a stack are ranked according to their score, which takes into account the phrase translation costs, language model cost, and other feature functions in the system mentioned in section 2.2. The retrieved translation is the cheapest hypothesis i"
2008.iwslt-evaluation.15,J03-1002,0,0.00319148,"luenced by errors in our implementation. The experiments were conducted using the provided data sets from the Basic Travel Expression Corpus (BTEC). The BTEC corpus contains relatively short tourism related sentences. For the BTEC and PIVOT tasks, one set of training data comprising just 20k aligned sentences and six development sets from previous IWSLT conferences were provided. Corpus statistics can be seen in Table 2. For the Arabic and Chinese BTEC tasks we only used the bitext supplied to us by IWSLT for training purposes. Word alignment was conducted using the open source GIZA++ toolkit [9], and phrase extraction is done according to Philip Koehn’s refined alignment and extraction software [10]. We were provided with 6 different development sets. To be able to compare our systems during tuning to previous IWSLT submissions, we optimised our systems on devset5 with 7 references per translation, and evaluated on devset6 with 6 references per translation. Optimisation was carried out using a minimum error rate trainer (MERT). - 106 - BTEC AE BITEXT Sentences Tokens BTEC CE BITEXT Sentences Tokens PIVOT CE BITEXT Sentences Tokens PIVOT ES BITEXT + EUROPARL Sentences Tokens Arabic En"
2009.mtsummit-papers.19,P06-1067,0,0.114331,"er approaches, is eliminated. To take advantage of the language model feature, we prefer to make re-ordering decisions during decoding. In addition, since one of the strengths of phrase-based models is to learn many phrases which do not necessarily belong to any syntactic category (DeNeefe et al., 2007), we believe the syntactic chunks may diminish this feature. Therefore, we suggest to consider all possible chunks and identify the optimal chunk boundaries during decoding. There are also a number of re-ordering approaches that fully integrate re-ordering into the decoding process, see, e.g., (Al-Onaizan and Papineni, 2006; Tillmann, 2004). These models typically predict the jump orientation (and sometimes distance) based on the previously translated phrase and the phrase that is to be translated next. A few simple syntactic features have been used in some of these models (Crego and Marino, 2006), however the fully lexicalized parameters remain the main source of evidence. Our method differs from lexicalized re-ordering models as it allows permutations beyond the fixed distortion limit and also removes the need for considering many unnecessary local reorderings. 3 Integrating Chunking and Decoding In this secti"
2009.mtsummit-papers.19,D08-1078,0,0.121775,"d during decoding. Zens et al. (2004) examine the effect of different constraints on machine translation quality. A constraint commonly used in phrase-based machine translation is the so-called distortion limit, which restricts the distance between the next phrase and the previously translated phrase. Most approaches described in the literature report a distortion limit ranging between 4 and 12 words. This limitation of course prohibits any word-reordering going beyond the set limit. This might not be a problem for language pairs with similar word order such as English-French or Dutch-German (Birch et al., 2008). A good language model or a lexicalized re-ordering model (Koehn et al., 2005) will be enough to capture the word order differences in these cases. However, when translating between languages with rather different word order, for example an SOV (subject-object-verb) language into an SVO (subject-verb-object) language, the distortion limit restriction can severely affect the decoder’s ability to capture those word order differences correctly. When translating from German (an SOV language) into English (an SVO language), it is not unusual that more than 20 words on the source side need to be ju"
2009.mtsummit-papers.19,2006.iwslt-papers.4,0,0.381091,"5) present an approach similar to (Xia and McCord, 2004), but with handcrafted, syntax-based rules to re-write source sentences. They argue that baseline phrase-based models are unable to perform the re-orderings found in translating between German and English. They show that many of the re-orderings require long distance jumps which are heavily penalized by a decoder applying a distance-based re-ordering strategy. Another benefit of source re-ordering is its ability to bring together source words that cannot be extracted as a phrase as they are non-contiguous in the original source sentence. Chen et al. (2006) extract rules at the part-ofspeech (POS) level from the word alignments and apply these rules to reorder the source sentences. Crego and Marino (2006) extract rewrite patterns at POS level as well, however, instead of re-ordering the source sentence, the re-ordering operations are integrated into the decoding process. Zhang et al. (2007) developed a method similar to other source re-ordering methods, however their approach works on an intermediate level called ‘syntactic chunks’. A syntactic chunk is a series of words that consist of a grammatical unit such as noun and verb. They use a maximu"
2009.mtsummit-papers.19,P05-1066,0,0.188958,"Missing"
2009.mtsummit-papers.19,D07-1079,0,0.0232157,"source sentence makes hard decisions that cannot be undone. For example, Xia and McCord (2004) report a decrease in translation quality by allowing permutations after re-ordering the source sentence. Also, since all re-orderings are done beforehand, the impact of n-gram language models, which is quite crucial in other approaches, is eliminated. To take advantage of the language model feature, we prefer to make re-ordering decisions during decoding. In addition, since one of the strengths of phrase-based models is to learn many phrases which do not necessarily belong to any syntactic category (DeNeefe et al., 2007), we believe the syntactic chunks may diminish this feature. Therefore, we suggest to consider all possible chunks and identify the optimal chunk boundaries during decoding. There are also a number of re-ordering approaches that fully integrate re-ordering into the decoding process, see, e.g., (Al-Onaizan and Papineni, 2006; Tillmann, 2004). These models typically predict the jump orientation (and sometimes distance) based on the previously translated phrase and the phrase that is to be translated next. A few simple syntactic features have been used in some of these models (Crego and Marino, 2"
2009.mtsummit-papers.19,J99-4005,0,0.317146,"English translation are reported. 1 Introduction Despite the success of phrase-based statistical machine translation systems, fluency of the output, particularly for long sentences still remains one of the main challenges in current research on Machine Translation (MT). Most of the errors in the MT output are caused by word-order differences between the source and the target language. Compared to word-based Statistical Machine Translation (SMT) systems, phrase-based approaches perform very well in capturing local re-orderings. However, long distance re-orderings remain a serious challenge. As Knight (1999) showed, trying all the perChristof Monz ISLA, Informatics Institute University of Amsterdam, Science Park 107 1098 XG Amsterdam, The Netherlands c.monz@uva.nl mutations is computationally intractable, and most phrase-based MT systems restrict the search space by limiting the set of re-orderings that are explored during decoding. Zens et al. (2004) examine the effect of different constraints on machine translation quality. A constraint commonly used in phrase-based machine translation is the so-called distortion limit, which restricts the distance between the next phrase and the previously tra"
2009.mtsummit-papers.19,W06-3114,1,0.769981,"Sen. Length Sentences Words Vocabulary Avg Sen. Length Sentences Words Vocabulary Avg Sen. Length German English 1.4M 38M 40M 344K 113K 26.17 27.51 2,000 56K 60K 8844 6050 28.31 30.09 2,028 51K 49K 9849 7163 25.31 24.63 Table 1: German to English corpus statistics. Europarl (EP) and News Commentary (NC) test sets of ACL WMT 2008. 4 Experiments 4.1 Experimental Setup To examine the effects of dynamic chunking on translation quality, we have chosen German to English translation as it involves many long distance reorderings. The training and test data sets are taken from the ACL WMT evaluation (Koehn and Monz, 2006). The corpus statistics are shown in table 1. The preprocessing stage includes tokenization and lower casing. There is only one reference translation for each sentence. The evaluation metrics used here are BLEU (Papineni et al., 2001), NIST (Doddington, 2002) and TER (Snover et al., 2006). The baseline system is a common multi-beam, multi-stack phrase-based decoder, described in (Koehn et al., 2003) with following features: • phrase translation probabilities and lexical probabilities for both directions • a trigram language model • phrase and word penalty • distance-based re-ordering penalty T"
2009.mtsummit-papers.19,N03-1017,0,0.125975,"arsers or chunkers and uses the word alignment information to build the chunker. The rest of the paper is organized as follows: Section 2 provides an overview of the related work addressing the issue of word re-ordering in statistical machine translation and the use of chunking in particular. Section 3 explains our proposed method. Section 4 discusses our experimental settings and results comparing the chunking method to a baseline. In Section 5 we draw some conclusions and discuss open issues. 2 Related Work Several phrase-based SMT systems use a very simple distance-based re-ordering model (Koehn et al., 2003; Koehn et al., 2007). In such a distance-based model, monotone translation and short jumps are preferred over longer jumps. The cost in this model increases linearly by distance with a slight preference for jumps to the right: d(i) = starti − endi−1 − 1 (1) where d(i) is the distortion cost of translating the ith phrase after the (i − 1)th. More recently, there have been efforts to incorporate syntax into statistical machine translation, particularly in order to address the issue of word reordering. A method to incorporate syntactic information is to apply syntactically motivated rules to ren"
2009.mtsummit-papers.19,2005.iwslt-1.8,0,0.0827059,"ts on machine translation quality. A constraint commonly used in phrase-based machine translation is the so-called distortion limit, which restricts the distance between the next phrase and the previously translated phrase. Most approaches described in the literature report a distortion limit ranging between 4 and 12 words. This limitation of course prohibits any word-reordering going beyond the set limit. This might not be a problem for language pairs with similar word order such as English-French or Dutch-German (Birch et al., 2008). A good language model or a lexicalized re-ordering model (Koehn et al., 2005) will be enough to capture the word order differences in these cases. However, when translating between languages with rather different word order, for example an SOV (subject-object-verb) language into an SVO (subject-verb-object) language, the distortion limit restriction can severely affect the decoder’s ability to capture those word order differences correctly. When translating from German (an SOV language) into English (an SVO language), it is not unusual that more than 20 words on the source side need to be jumped over to translate the verb in the right position. While relaxing the disto"
2009.mtsummit-papers.19,P07-2045,0,0.0117884,"nd uses the word alignment information to build the chunker. The rest of the paper is organized as follows: Section 2 provides an overview of the related work addressing the issue of word re-ordering in statistical machine translation and the use of chunking in particular. Section 3 explains our proposed method. Section 4 discusses our experimental settings and results comparing the chunking method to a baseline. In Section 5 we draw some conclusions and discuss open issues. 2 Related Work Several phrase-based SMT systems use a very simple distance-based re-ordering model (Koehn et al., 2003; Koehn et al., 2007). In such a distance-based model, monotone translation and short jumps are preferred over longer jumps. The cost in this model increases linearly by distance with a slight preference for jumps to the right: d(i) = starti − endi−1 − 1 (1) where d(i) is the distortion cost of translating the ith phrase after the (i − 1)th. More recently, there have been efforts to incorporate syntax into statistical machine translation, particularly in order to address the issue of word reordering. A method to incorporate syntactic information is to apply syntactically motivated rules to render the word order of"
2009.mtsummit-papers.19,koen-2004-pharaoh,0,0.0416767,"we must recognise the difficulties in the provision of cause and effect Figure 2: An example of the decoding process by dynamic chunking. The C states are chunking states, which new chunking boundaries are detected and in P states, phrase translations are applied inside a chunk. The bold parts of the source sentence show the translated spans in that state. The rest of the decoding is chunking and translation the full stop. With extra information in every hypothesis, the recombination criteria are redefined to consider the chunking status of a hypothesis. For two hypotheses to be recombinable (Koehn, 2004), they should have identical chunk boundaries for the uncovered positions. This is in addition to commonly used recombination criteria such as identical cover vectors, language model history, and last foreign position covered. The chunking cost, estimated by the chunking scorer, is another feature along the baseline features. Also, the future cost computation component includes the future chunk distortion cost and future chunking cost together with the translation model and language model costs. The following feature functions are defined to incorporate chunking costs and chunk re-orderings co"
2009.mtsummit-papers.19,P03-1021,0,0.0287497,"Missing"
2009.mtsummit-papers.19,2001.mtsummit-papers.68,0,0.0379741,"man to English corpus statistics. Europarl (EP) and News Commentary (NC) test sets of ACL WMT 2008. 4 Experiments 4.1 Experimental Setup To examine the effects of dynamic chunking on translation quality, we have chosen German to English translation as it involves many long distance reorderings. The training and test data sets are taken from the ACL WMT evaluation (Koehn and Monz, 2006). The corpus statistics are shown in table 1. The preprocessing stage includes tokenization and lower casing. There is only one reference translation for each sentence. The evaluation metrics used here are BLEU (Papineni et al., 2001), NIST (Doddington, 2002) and TER (Snover et al., 2006). The baseline system is a common multi-beam, multi-stack phrase-based decoder, described in (Koehn et al., 2003) with following features: • phrase translation probabilities and lexical probabilities for both directions • a trigram language model • phrase and word penalty • distance-based re-ordering penalty The weights for the features are optimized by MER training (Och, 2003) to maximize the BLEU (Papineni et al., 2001) score. Run EP EP NC NC 1 2 3 4 System Baseline Chunk Baseline Chunk BLEU 0.2687 0.2716 0.2454 0.2487 NIST 7.0063 7.1084"
2009.mtsummit-papers.19,2006.amta-papers.25,0,0.0172261,"Commentary (NC) test sets of ACL WMT 2008. 4 Experiments 4.1 Experimental Setup To examine the effects of dynamic chunking on translation quality, we have chosen German to English translation as it involves many long distance reorderings. The training and test data sets are taken from the ACL WMT evaluation (Koehn and Monz, 2006). The corpus statistics are shown in table 1. The preprocessing stage includes tokenization and lower casing. There is only one reference translation for each sentence. The evaluation metrics used here are BLEU (Papineni et al., 2001), NIST (Doddington, 2002) and TER (Snover et al., 2006). The baseline system is a common multi-beam, multi-stack phrase-based decoder, described in (Koehn et al., 2003) with following features: • phrase translation probabilities and lexical probabilities for both directions • a trigram language model • phrase and word penalty • distance-based re-ordering penalty The weights for the features are optimized by MER training (Och, 2003) to maximize the BLEU (Papineni et al., 2001) score. Run EP EP NC NC 1 2 3 4 System Baseline Chunk Baseline Chunk BLEU 0.2687 0.2716 0.2454 0.2487 NIST 7.0063 7.1084 7.1591 7.1798 1−TER 0.3374 0.3261 0.3476 0.3599 Table"
2009.mtsummit-papers.19,N04-4026,0,0.0886554,"o take advantage of the language model feature, we prefer to make re-ordering decisions during decoding. In addition, since one of the strengths of phrase-based models is to learn many phrases which do not necessarily belong to any syntactic category (DeNeefe et al., 2007), we believe the syntactic chunks may diminish this feature. Therefore, we suggest to consider all possible chunks and identify the optimal chunk boundaries during decoding. There are also a number of re-ordering approaches that fully integrate re-ordering into the decoding process, see, e.g., (Al-Onaizan and Papineni, 2006; Tillmann, 2004). These models typically predict the jump orientation (and sometimes distance) based on the previously translated phrase and the phrase that is to be translated next. A few simple syntactic features have been used in some of these models (Crego and Marino, 2006), however the fully lexicalized parameters remain the main source of evidence. Our method differs from lexicalized re-ordering models as it allows permutations beyond the fixed distortion limit and also removes the need for considering many unnecessary local reorderings. 3 Integrating Chunking and Decoding In this section we describe ou"
2009.mtsummit-papers.19,D07-1077,0,0.154007,"Missing"
2009.mtsummit-papers.19,C04-1073,0,0.381168,"(i − 1)th. More recently, there have been efforts to incorporate syntax into statistical machine translation, particularly in order to address the issue of word reordering. A method to incorporate syntactic information is to apply syntactically motivated rules to render the word order of the source sentence similar to the target language. These transformation rules can be syntax-based or lexicalized rules. A syntax-based rule is a transformation rule that only contains syntactic tags (Collins et al., 2005; Wang et al., 2007), but a lexicalized rule contains at least one word as a constraint (Xia and McCord, 2004). Xia and McCord (2004) proposed a method to learn transformation rules, lexicalized and syntax-based (unlexicalized), from a parallel corpus. Their approach extracts re-write patterns, applies them to the source sentence after which the sentence is translated monotonically. To learn the rewrite patterns, the source side of the bitext is parsed, phrases are aligned and lexicalized, and unlexicalized patterns consisting of parent nodes with their children, plus their syntactic labels are extracted. Collins et al. (2005) present an approach similar to (Xia and McCord, 2004), but with handcrafted"
2009.mtsummit-papers.19,C04-1030,0,0.18697,"etween the source and the target language. Compared to word-based Statistical Machine Translation (SMT) systems, phrase-based approaches perform very well in capturing local re-orderings. However, long distance re-orderings remain a serious challenge. As Knight (1999) showed, trying all the perChristof Monz ISLA, Informatics Institute University of Amsterdam, Science Park 107 1098 XG Amsterdam, The Netherlands c.monz@uva.nl mutations is computationally intractable, and most phrase-based MT systems restrict the search space by limiting the set of re-orderings that are explored during decoding. Zens et al. (2004) examine the effect of different constraints on machine translation quality. A constraint commonly used in phrase-based machine translation is the so-called distortion limit, which restricts the distance between the next phrase and the previously translated phrase. Most approaches described in the literature report a distortion limit ranging between 4 and 12 words. This limitation of course prohibits any word-reordering going beyond the set limit. This might not be a problem for language pairs with similar word order such as English-French or Dutch-German (Birch et al., 2008). A good language"
2009.mtsummit-papers.19,W07-0401,0,0.0876263,"heavily penalized by a decoder applying a distance-based re-ordering strategy. Another benefit of source re-ordering is its ability to bring together source words that cannot be extracted as a phrase as they are non-contiguous in the original source sentence. Chen et al. (2006) extract rules at the part-ofspeech (POS) level from the word alignments and apply these rules to reorder the source sentences. Crego and Marino (2006) extract rewrite patterns at POS level as well, however, instead of re-ordering the source sentence, the re-ordering operations are integrated into the decoding process. Zhang et al. (2007) developed a method similar to other source re-ordering methods, however their approach works on an intermediate level called ‘syntactic chunks’. A syntactic chunk is a series of words that consist of a grammatical unit such as noun and verb. They use a maximum entropy tool to build the chunking model with training data provided by converting subtrees of Chinese treebank into chunks. A rule is composed of chunk and POS tags, where a chunk tag for each word determines the chunk type that the word belongs to and also whether the word is at the beginning of the chunk. Before extracting the rules"
2009.mtsummit-papers.19,P02-1040,0,\N,Missing
2010.amta-papers.1,2007.mtsummit-papers.3,0,0.0253967,"ce he reiterated ” full support of the islamic republic for islamic government interim ” in afghanistan. We present sentence-level BLEU scores for both translations. annotation phase considerably, and gives us significant improvements over the state-of-the-art. Shen et al. (Shen et al., 2004) are the first to use a perceptron like algorithm in a small scale application of reranking SMT n-best lists. They used the algorithm to optimise weights for a small number of features (tens instead of millions). The use of perceptron type algorithms with millions of features for SMT has been explored by (Arun and Koehn, 2007). They examine the use of online algorithms for the discriminative training of a phrase based SMT system. In this paper we focus on the use of perceptrons for reranking using only target side syntactic information. The first application of a large scale discriminative language model to SMT reranking is undertaken by Li and Khudanpur (2008). Using a standard ngram feature set they outperformed the 1-best output of their baseline SMT system. They focus on the application of n-gram only models to SMT and the use of data filtering thresholds. We concentrate on the efficient syntactic annotation of"
2010.amta-papers.1,D08-1024,0,0.0506262,"Missing"
2010.amta-papers.1,P02-1034,0,0.373003,"select the best one: NNP NNP mr. rubendall MD RB could n’t VP VB VP be VBN reached Figure 2: An example parse tree. (a) z ∗ = argmaxz∈GEN(xi ) S(z) 2.2 (3) Perceptron Variants A shortcoming of the perceptron is that it can be unstable if the training data is not linearly separable. A number of solutions have been proposed in the literature. One solution proposed is to use an averaged perceptron (Freund and Schapire, 1999), where the parameter vector w output by the algorithm is avwti eraged over each instance wavg = ΣTt=1 ΣN i=1 N ·T . Another solution is the pocket perceptron (Gallant, 1999; Collins and Duffy, 2002), where the weight vector returned is the one that correctly classifies the most training instances in a row, keeping an optimal model in its ‘pocket’. A third solution, called the committee or voting perceptron, keeps a cache of optimal models, sorted by their success counts (Roark et al., 2004a; Elsas et al., 2008). The cache sizes differentiate the voting and committee perceptron. 3 Syntactic Features The syntactic features used for reranking are extracted from either full parse trees (Section 3.1) or POS sequences (Section 3.2). Using parsers allows us to extract features that are global t"
2010.amta-papers.1,P05-1063,0,0.359636,"M is then returned as the selected translation. DLMs are advantageous as they can make use of negative examples and allow for a relatively easy inclusion of syntactic features that go beyond simple word ngrams. As our DLM, we use the perceptron algorithm for learning the weight vector. Building on the framework of Roark et al. (2007), Li and Khudanpur (2008) successfully applied the perceptron to SMT using lexical, n-gram features. In this paper, we investigate the addition of syntactic features to an ngram based perceptron when applied to SMT reranking, taking as a starting point the work of Collins et al. (2005). In particular, we show that: • the use of full parse tree features do not provide the improvements expected, however, • improvements can be gained by using features extracted from a novel, context-insensitive Part-of-Speech (POS) tagger, and, • these features lead to larger gains than using a state of the art conditional random field (CRF) POS tagger on two of the three test sets. The remainder of this paper is organised as follows: Section 2 describes different parameter estimation methods. Section 3 introduces the syntactic features used for reranking. Section 4 describes the experimental"
2010.amta-papers.1,P07-2045,0,0.006184,"Missing"
2010.amta-papers.1,W04-3250,0,0.0861683,"Missing"
2010.amta-papers.1,N04-1022,0,0.0192694,"decoding process. This leads to disfluent translations (Post and Gildea, 2008). Finally, standard generative LMs and full or partial syntax-based models are almost exclusively trained on well-formed English. Given the large feature space they operate in, the accurate assignment of probabilities to unseen events is a difficult problem, and has been a major field of research for the past sixty years (for a detailed overview on language modelling and smoothing techniques, see (Chen and Goodman, 1998)). Applying models in a reranking phase to achieve better results is a standard technique in SMT (Kumar et al., 2004). In a reranking approach, an SMT system outputs a list of the top n translations (an n-best list). A discriminative language model (DLM) (Roark et al., 2007) takes these n-best lists and reranks them according to a weight vector and a function φ(·) that maps a sentence into a feature Perceptron 1: w ← 0 2: for t = 1 to T do 3: for i = 1 to N do 4: y i ← ORACLE(xi ) 5: z i ← argmaxz∈GEN (xi ) φ(z) · w 6: if z i 6= y i then 7: w ← w + φ(y i ) − φ(z i ) 8: end if 9: end for 10: end for 11: return w Figure 1: The standard perceptron algorithm space. The best hypothesis according to the DLM is the"
2010.amta-papers.1,2008.amta-papers.12,0,0.538955,"for i = 1 to N do 4: y i ← ORACLE(xi ) 5: z i ← argmaxz∈GEN (xi ) φ(z) · w 6: if z i 6= y i then 7: w ← w + φ(y i ) − φ(z i ) 8: end if 9: end for 10: end for 11: return w Figure 1: The standard perceptron algorithm space. The best hypothesis according to the DLM is then returned as the selected translation. DLMs are advantageous as they can make use of negative examples and allow for a relatively easy inclusion of syntactic features that go beyond simple word ngrams. As our DLM, we use the perceptron algorithm for learning the weight vector. Building on the framework of Roark et al. (2007), Li and Khudanpur (2008) successfully applied the perceptron to SMT using lexical, n-gram features. In this paper, we investigate the addition of syntactic features to an ngram based perceptron when applied to SMT reranking, taking as a starting point the work of Collins et al. (2005). In particular, we show that: • the use of full parse tree features do not provide the improvements expected, however, • improvements can be gained by using features extracted from a novel, context-insensitive Part-of-Speech (POS) tagger, and, • these features lead to larger gains than using a state of the art conditional random field ("
2010.amta-papers.1,C04-1072,0,0.0230287,"t, assuming linearly separable data, can be shown to converge to a solution that perfectly classifies the data (Freund and Schapire, 1999). The standard perceptron algorithm is shown in Figure 1. The algorithm takes as input a set of n-best lists X, a function GEN (xi ) that enumerates over each sentence in a n-best list xi , and an oracle function ORACLE(xi ) that determines the best translation (oracle best) for each of the n-best lists xi according to the BLEU metric (Papineni et al., 2002). As DLMs make comparisons on the sentence level, we use sentence level BLEU with additive smoothing (Lin and Och, 2004). There are discrepancies between sentence and corpus-level BLEU, however we find sentence-level BLEU sufficient for reranking SMT. T defines the number of iterations and N defines the size of the test set, which in our case is the number of n-best lists. The algorithm iterates over the n-best lists in a sequential manner (lines 2 and 3). If the selected hypothesis and oracle best sentence match, then the algorithm continues to the next n-best list. Otherwise, the weight vector is updated (line 7). At the end, it returns a weight vector as its solution (line 11). S To use the weight vector ret"
2010.amta-papers.1,H94-1020,0,0.207022,"ds to optimise the parameters. The resulting setting was used to translate the remaining fold and to generate the n-best lists used for learning the parameter settings of the perceptron reranker. Note, that the Moses baseline was still trained on all development data at once. To generate the parses from which the syntactic features for our perceptron reranker are extracted, the n-best lists of the development and test sets are parsed by Dan Bikel’s implementation of Collins Model 2 (CM2) (2002). The parser was optimised on sections 02-21 of the Wall Street Journal of the Penn Tree Bank (PTB) (Marcus et al., 1994), approximately 40,000 sentences, with section 23 reserved for testing. As we parse SMT output, all sentences were tokenised and lowercased in accordance with the output of the SMT system prior to training the parser. Whenever the parser failed to generate a parse, the sentence was assigned the &lt;NOPARSE&gt; feature. The simple uni-gram tagger and Xuan-Hieu Phan’s implementation of a CRF tagger (available at http://crftagger.sourceforge.net) were trained analogously. Table 1 shows the POS accuracy of all three syntactic models on section 23 of the WSJ corpus. We refer to our simple POS tagger as S"
2010.amta-papers.1,P00-1056,0,0.102074,"4, MT05 and MT06).2 4.1 SMT System Moses is used as a state-of-the-art baseline SMT system for reporting experimental results (Koehn et al., 2007). It is a phrase-based MT system using stacks to organise partial translation candidates. The parameters used for the experiments discussed here are: stack size of 100, distortion limit of 6, and phrase table limit of 20. 4.2 Training Data To build the phrase table and language model, we used various corpora distributed by the Linguistic Data Consortium (LDC), totaling 300 thousand sentence pairs.3 Alignments were extracted using the GIZA++ toolkit (Och and Ney, 2000). The AFP and Xinhua portions of the English Gigaword corpus (LDC2003T05) and the English side of the bitext were used to build the target tri-gram language model using the SRILM toolkit with modified Kneser-Ney smoothing (Stolcke, 2002). 4.3 Parameter Optimisation The NIST Machine Translation MT02 and MT03 sets were used as a development set for optimising the parameters of the Moses baseline SMT system using Minimum Error Rate Training (MERT) 2 Statistics for each set (#source sentences/#refs): MT02 (1043/4), MT03 (663/4), MT04 (1353/5), MT05 (1056/5), MT06(1796/4). 3 The parallel text inclu"
2010.amta-papers.1,P03-1021,0,0.00859825,"sing Minimum Error Rate Training (MERT) 2 Statistics for each set (#source sentences/#refs): MT02 (1043/4), MT03 (663/4), MT04 (1353/5), MT05 (1056/5), MT06(1796/4). 3 The parallel text includes Arabic news LDC2004T18, automatically extracted parallel text LDC2007T08, eTIRR news LDC2004E72 and translated Arabic treebank data LDC2005E46. Moses + DLM n-gram Oracle MT04 48.97 49.57 61.09 MT05 53.92 54.42 66.34 MT06 38.40 39.08 50.11 Table 2: Baseline results on MT test sets. BLEU scores reported are uncased. ‘DLM n-gram’ refers to the competitive n-gram perceptron reranker we aim to outperform. (Och, 2003). Since the parameters of the perceptron reranker need to be optimised as well, the development set was split into K folds. MERT was run on the union of the K − 1 folds to optimise the parameters. The resulting setting was used to translate the remaining fold and to generate the n-best lists used for learning the parameter settings of the perceptron reranker. Note, that the Moses baseline was still trained on all development data at once. To generate the parses from which the syntactic features for our perceptron reranker are extracted, the n-best lists of the development and test sets are par"
2010.amta-papers.1,P02-1040,0,0.0789744,"the perceptron algorithm. 2.1 Perceptron The perceptron, proposed by (Rosenblatt, 1958) is an error minimisation learner that, assuming linearly separable data, can be shown to converge to a solution that perfectly classifies the data (Freund and Schapire, 1999). The standard perceptron algorithm is shown in Figure 1. The algorithm takes as input a set of n-best lists X, a function GEN (xi ) that enumerates over each sentence in a n-best list xi , and an oracle function ORACLE(xi ) that determines the best translation (oracle best) for each of the n-best lists xi according to the BLEU metric (Papineni et al., 2002). As DLMs make comparisons on the sentence level, we use sentence level BLEU with additive smoothing (Lin and Och, 2004). There are discrepancies between sentence and corpus-level BLEU, however we find sentence-level BLEU sufficient for reranking SMT. T defines the number of iterations and N defines the size of the test set, which in our case is the number of n-best lists. The algorithm iterates over the n-best lists in a sequential manner (lines 2 and 3). If the selected hypothesis and oracle best sentence match, then the algorithm continues to the next n-best list. Otherwise, the weight vect"
2010.amta-papers.1,2008.amta-papers.16,0,0.0762262,"ssumption that the probability of a word in a string can be approximated by considering only the limited history of the previous n − 1 words. Whilst LMs remain fundamental to SMT systems, they suffer from a number of flaws: first, given the local nature of n-gram models, there is a strong motivation for the applicability of syntactic LMs that can take into account long-range dependencies that are difficult to model by the limited context of ngram models. However, attempts to integrate syntactic information into language models used by SMT systems have met with mixed success (Och et al., 2004; Post and Gildea, 2008). Secondly, language model performance is sensitive to changes in genre and domain between training and test sets (Rosenfeld, 2000). Even when training and testing on very similar domains such as news, it has been shown that mundane differences between two sources of news sets can lead to a notable decrease of performance (Rosenfeld, 1996). Thirdly, the best choice according to the language model can often be outvoted by other models (i.e phrase table, re-ordering model) during the decoding process. This leads to disfluent translations (Post and Gildea, 2008). Finally, standard generative LMs"
2010.amta-papers.1,P04-1007,0,0.38941,"of-Speech (POS) tagger, and, • these features lead to larger gains than using a state of the art conditional random field (CRF) POS tagger on two of the three test sets. The remainder of this paper is organised as follows: Section 2 describes different parameter estimation methods. Section 3 introduces the syntactic features used for reranking. Section 4 describes the experimental set up and results. Related work is discussed in Section 5. 2 Parameter Estimation Different parameter estimation methods to estimate the weight vector w for a DLM have been previously examined in the speech domain (Roark et al., 2004b; Roark et al., 2007). These include optimising the log-likelihood under a log-linear model, a batch algorithm which requires processing all the data before outputting a weight vector as an answer, and approximating a 0/1 loss through the perceptron update rule, an online algorithm which examines and updates the parameter vector sequentially. The reader is referred to (Roark et al., 2004b; Emami et al., 2007) for a discussion on the benefits of the log-linear model and the perceptron. Given this paper examines the use of a syntactic feature space, which is larger than an already large n-gram"
2010.amta-papers.1,N04-1023,0,0.171623,"better’ taggers try to apply a more English like tag sequence to ill-formed sentences. In doing so, we potentially lose information that can lead to a better discriminative model. 5 Related Work Perceptrons have been successfully applied to parse reranking (Collins and Duffy, 2002), document reranking for IR (Crammer and Singer, 2001; Elsas et al., 2008; Chen et al., 2009), ASR reranking (Roark et al., 2004b; Collins et al., 2005; SinghMiller and Collins, 2007), and finally to SMT translation reranking, where significant improvements have been achieved for Chinese-English translation systems (Shen et al., 2004; Li and Khudanpur, 2008). The work most closely related to ours is the discriminative syntactic LM proposed in (Collins et al., 2005). The work presented in this paper differs in two important aspects: first, we focus on the use of syntactic features in SMT. Second, we propose the use of a simple POS tagger, which speeds up the data System CM2 CRF S-POS S-POS POS Sequence PRP/he VBD/reiterated ”/” JJ/full NN/support IN/of DT/the NNP/islamic NNP/republic IN/for JJ/islamic NN/government NN/interim IN/in ”/” NNP/afghanistan ./. PRP/he VBD/reiterated ”/” JJ/full NN/support IN/of DT/the JJ/islamic"
2010.amta-papers.1,J03-4003,0,\N,Missing
2010.amta-papers.1,N04-1021,0,\N,Missing
2015.mtsummit-papers.12,N15-1027,0,0.0151387,"eterministically mapped to a soft tag ri . where Wc , Wh , and Wm are weight matrices, cj and hi are shorthands for [sj−k ; . . . ; sj+k ] and [ri−1 ; . . . ; ri−n+1 ] respectively, [v; v0 ] denotes vector concatenation, and φ is a non-linear transfer prelu. As φ, we use in all experiments the channel-shared parametric rectified linear unit (PReLU) introduced by He et al. (2015). PReLU φ(x) is defined as: φ(x) ( x if x > 0 φ(x) = ax otherwise x where a is a parameter learned during training. To speed up decoding, we train the Inf-NN model with a self-normalized objective (Devlin et al., 2014; Andreas and Klein, 2015). More specifically, we adopt the objective function proposed by Andreas and Klein (2015):   2 `(θ) = − E ln p(ri |hi , cj ) + η kθk2  γ  + E ln2 Z(hi , cj )|(hi , cj ) ∈ H p where H is a set of random samples on which self-normalization is performed, θ =  {sj }, {ri }, Wc , Wh , Wm , bz , a are the parameters of the networks, and Z(hi , cj ) is the partition function of the input (hi , cj ). In practice, we obtain H by sampling from a Bernoulli distribution Bern(p). This is equivalent to applying dropout (Srivastava et al., 2014) on the loss gradient 1 ∈ Rm of self-normalization term, wh"
2015.mtsummit-papers.12,P08-1087,0,0.202399,"ural language or translation modeling. 3 A Distributed Inflection Model In MRLs, the surface form of a word is heavily determined by its grammatical features, such as number, case, tense etc. Choosing the right target word form during translation is a complex problem since some of these features depend on the source context while others depend on the target context (agreement phenomena). We model target language inflection by a Markov process generating a sequence of abstract word representations based on source and target context. This complements previous work focusing on either the former (Avramidis and Koehn, 2008; Chahuneau et al., 2013; Tran et al., 2014) or the latter (Green and DeNero, 2012; Fraser et al., 2012; Botha and Blunsom, 2014; Bisazza and Monz, 2014). 3.1 Soft Morphological Representations As previously stated, it is common for words in MRLs to admit multiple morphological analyses out of context. Rather than trying to disambiguate the analyses in context using for instance conditional random fields (Green and DeNero, 2012; Fraser et al., 2012), we modify the tagging scheme so that each word corresponds to only one tag. To also avoid the loss of useful information incurred when arbitraril"
2015.mtsummit-papers.12,C14-1181,1,0.916601,"rather than generating new inflections, motivated by previous observations that, when translating into MRLs, a large number of reference inflections are already available in the SMT models but are not selected for Viterbi translation (Green and DeNero, 2012; Tran et al., 2014). More in general, our work is related to class-based language modeling (Brown et al., 1992) with the major difference that we also condition on source-side context and that we use explicit morphological representations instead of data-driven word clusters (Uszkoreit and Brants, 2008), word suffixes (Müller et al., 2012; Bisazza and Monz, 2014) or coarse-grained part-ofspeech tags (Koehn et al., 2008). Modeling morphology using neural networks has recently shown promising results: in the context of monolingual neural language modeling, Luong et al. (2013); Botha and Blunsom (2014) obtain the vectorial representation of a word by composing the representations of its morphemes. Tran et al. (2014) model translation stem and suffix selection in SMT with a bilingual neural network. Soricut and Och (2015) discover morphological transformation rules from word embeddings learned by a shallow network. We are not aware of work that leveraged"
2015.mtsummit-papers.12,2011.iwslt-evaluation.18,1,0.882169,"Missing"
2015.mtsummit-papers.12,J92-4003,0,0.0650167,"e been integrated to SMT as additional feature functions: e.g. as an additional lexical translation score (Jeong et al., 2010; Tran et al., 2014) or as an additional target language model score (Green and DeNero, 2012). We follow this last strategy, rather than generating new inflections, motivated by previous observations that, when translating into MRLs, a large number of reference inflections are already available in the SMT models but are not selected for Viterbi translation (Green and DeNero, 2012; Tran et al., 2014). More in general, our work is related to class-based language modeling (Brown et al., 1992) with the major difference that we also condition on source-side context and that we use explicit morphological representations instead of data-driven word clusters (Uszkoreit and Brants, 2008), word suffixes (Müller et al., 2012; Bisazza and Monz, 2014) or coarse-grained part-ofspeech tags (Koehn et al., 2008). Modeling morphology using neural networks has recently shown promising results: in the context of monolingual neural language modeling, Luong et al. (2013); Botha and Blunsom (2014) obtain the vectorial representation of a word by composing the representations of its morphemes. Tran et"
2015.mtsummit-papers.12,W12-3102,1,0.788711,"edings), we construct phrase table and reordering models for this experiment using the fillup technique (Bisazza et al., 2011). Note that our baseline does not include previously proposed inflection models because the main goal of our experiment is to demonstrate the effectiveness of the proposed approach for languages where no sizable disambiguated data exists, which is indeed the case for Italian. Feature weights are tuned with pairwise ranking optimization (Hopkins and May, 2011) on the union of IWSLT’s dev10 and test10 in Italian, and on the first 2000 lines of wmt12 benchmark in Russian (Callison-Burch et al., 2012). During tuning, 14 PRO parameter estimation runs are performed in parallel on different samples of the n-best list after each decoder iteration. The weights of the individual PRO runs are then averaged and passed on to the next decoding iteration. Performing weight estimation independently for a number of samples corrects for some of the instability that can be caused by individual samples. 6.2 Results Translation quality is measured by case-insensitive BLEU (Papineni et al., 2002) on IWSLT’s test12 and test14 in Italian, and on wmt13 and wmt14 for Russian, all provided with one reference tra"
2015.mtsummit-papers.12,2014.iwslt-evaluation.1,0,0.0121013,"guated data exists. Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 149 4.1 Data As target languages, we choose two MRLs belonging to different language families and displaying different inflectional patterns: Russian has very rich nominal, adjectival and verbal inflection, while Italian has moderate nominal and adjectival inflection, but extremely rich verbal inflection. Experiments are performed on the following tasks: • English-Russian WMT (Bojar et al., 2013): translation of news commentaries with largescale training data. • English-Italian IWSLT (Cettolo et al., 2014): translation of speeches with either smallscale training data (TED talks only) or large-scale training data (TED talks and European proceedings). SMT training data statistics are reported in Table 2. The Russian Inf-NN model is trained on a 1M-sentence subset of the bilingual data, while the Italian one is trained on all the data available in each setting. For each data set, we create automatic word alignments using GIZA++ (Och and Ney, 2003). Bilingual Monoling. En-Ru large small large #sentences src/trg #tokens src/trg dict.size 2.4M 49.2/47.2M 774K/1100K 180K 3.6/3.4M 55K/80K 2.0M 57.4/57."
2015.mtsummit-papers.12,D13-1174,0,0.220091,"ype (Minkov et al., 2007; Toutanova et al., 2008; Jeong et al., 2010). As for how inflection models are integrated into the STM system, different strategies have been proposed. Minkov et al. (2007); Toutanova et al. (2008); Fraser et al. (2012) treat inflection as a post-processing task: the SMT model is trained to produce lemmatized target sentences (possibly enhanced with some form of morphological annotation) and afterwards the best surface form for each lemma is chosen by separate inflection models. Some work has focused on the generation of new inflected phrases given the input sentence (Chahuneau et al., 2013) or given the bilingual context during decoding (Koehn and Hoang, 2007; Subotin, 2011). Other inflection models have been integrated to SMT as additional feature functions: e.g. as an additional lexical translation score (Jeong et al., 2010; Tran et al., 2014) or as an additional target language model score (Green and DeNero, 2012). We follow this last strategy, rather than generating new inflections, motivated by previous observations that, when translating into MRLs, a large number of reference inflections are already available in the SMT models but are not selected for Viterbi translation ("
2015.mtsummit-papers.12,P14-1129,0,0.175437,"-trained embeddings, which has been shown to encode certain morphological regularities (Soricut and Och, 2015), whereas target tag representations are initialized randomly. Inf-NN is a feed-forward neural network whose output is a conditional probability distribution over a set of morphological tags given target history and source context. Formally, let hi = (ri−1 , . . . , ri−n+1 ) be the n−1 tag history of the target word wi , and cj = (sj−k , . . . , sj+k ) the source context centering at the word sj aligned to wi by an automatic aligner. We use simple heuristics similar to the approach by Devlin et al. (2014) to handle null and multiple alignments so that each target word wi can be mapped to exactly one source word sj . Let sj ∈ RD and ri ∈ RD denote the distributed representations of source sj and target tag ri respectively. Then, the conditional probability pInf-NN (ri |hi , cj ) is computed at the output layer y of the network as follows: zi = φ(Wc cj + Wh hi + bz ) y = softmax(Wm zi + by ) 3 The implementation is available at https://bitbucket.org/ketran/soft-tags Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 148 wi 2 wi 1 wi ri 2 ri 1 sj 2 sj 1 sj s"
2015.mtsummit-papers.12,E12-1068,0,0.0816176,"also been shown to affect syntactic parsing of SMT output (Post and Gildea, 2008; Carter and Monz, 2011). Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 146 Considerably less work has focused on MRLs where disambiguated data does not exist, with few exceptions where ambiguity is solved by randomly selecting one analysis per word type (Minkov et al., 2007; Toutanova et al., 2008; Jeong et al., 2010). As for how inflection models are integrated into the STM system, different strategies have been proposed. Minkov et al. (2007); Toutanova et al. (2008); Fraser et al. (2012) treat inflection as a post-processing task: the SMT model is trained to produce lemmatized target sentences (possibly enhanced with some form of morphological annotation) and afterwards the best surface form for each lemma is chosen by separate inflection models. Some work has focused on the generation of new inflected phrases given the input sentence (Chahuneau et al., 2013) or given the bilingual context during decoding (Koehn and Hoang, 2007; Subotin, 2011). Other inflection models have been integrated to SMT as additional feature functions: e.g. as an additional lexical translation score"
2015.mtsummit-papers.12,D08-1089,0,0.0186718,"introducing spurious ambiguity into what is already a huge search space.6 As a result, the integration of our Inf-NN does not affect decoding speed. 6 Green and DeNero (2012) also tag each target phrase in context as it is produced. However, they avoid the spurious ambiguity problem by only preserving the most probable tag sequence for each phrase (incremental greedy decoding). Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 152 6.1 Baseline Our SMT baseline is a competitive phrase-based SMT system including hierarchical lexicalized reordering models (Galley and Manning, 2008) and a 5-gram target LM trained with modified Kneser-Ney smoothing (Chen and Goodman, 1999). Since the large English-Italian data comes from very different sources (TED talks and European proceedings), we construct phrase table and reordering models for this experiment using the fillup technique (Bisazza et al., 2011). Note that our baseline does not include previously proposed inflection models because the main goal of our experiment is to demonstrate the effectiveness of the proposed approach for languages where no sizable disambiguated data exists, which is indeed the case for Italian. Feat"
2015.mtsummit-papers.12,P12-1016,0,0.220312,"lly disambiguated data. Integrated into an SMT decoder and evaluated for English-Italian and English-Russian translation, our model yields improvements of up to 1.0 BLEU over a competitive baseline. 1 Introduction In morphologically rich languages (MRLs), words can have many different surface forms depending on the grammatical context. When translating into MRLs, standard statistical machine translation (SMT) models such as phrase translation models and n-gram language models (LMs) often fail to select the right surface form due to the sparsity of observed word sequences (Minkov et al., 2007; Green and DeNero, 2012). While neural LMs (Bengio et al., 2003; Schwenk, 2007) address lexical sparsity to a certain degree by projecting word sequences to distributed vector representations, they still suffer from the problem of rare words which is particularly exacerbated in MRLs (Botha and Blunsom, 2014; Jean et al., 2015; Luong et al., 2015). A potential solution to overcome data sparsity in MRLs, is to use word representations that separate the grammatical aspects of a word, i. e. inflection, from the lexical ones. Such word representations already exist for many languages in the form of morphological analyzers"
2015.mtsummit-papers.12,D11-1125,0,0.0217634,"moothing (Chen and Goodman, 1999). Since the large English-Italian data comes from very different sources (TED talks and European proceedings), we construct phrase table and reordering models for this experiment using the fillup technique (Bisazza et al., 2011). Note that our baseline does not include previously proposed inflection models because the main goal of our experiment is to demonstrate the effectiveness of the proposed approach for languages where no sizable disambiguated data exists, which is indeed the case for Italian. Feature weights are tuned with pairwise ranking optimization (Hopkins and May, 2011) on the union of IWSLT’s dev10 and test10 in Italian, and on the first 2000 lines of wmt12 benchmark in Russian (Callison-Burch et al., 2012). During tuning, 14 PRO parameter estimation runs are performed in parallel on different samples of the n-best list after each decoder iteration. The weights of the individual PRO runs are then averaged and passed on to the next decoding iteration. Performing weight estimation independently for a number of samples corrects for some of the instability that can be caused by individual samples. 6.2 Results Translation quality is measured by case-insensitive"
2015.mtsummit-papers.12,P15-1001,0,0.0287832,"on the grammatical context. When translating into MRLs, standard statistical machine translation (SMT) models such as phrase translation models and n-gram language models (LMs) often fail to select the right surface form due to the sparsity of observed word sequences (Minkov et al., 2007; Green and DeNero, 2012). While neural LMs (Bengio et al., 2003; Schwenk, 2007) address lexical sparsity to a certain degree by projecting word sequences to distributed vector representations, they still suffer from the problem of rare words which is particularly exacerbated in MRLs (Botha and Blunsom, 2014; Jean et al., 2015; Luong et al., 2015). A potential solution to overcome data sparsity in MRLs, is to use word representations that separate the grammatical aspects of a word, i. e. inflection, from the lexical ones. Such word representations already exist for many languages in the form of morphological analyzers or lexicons. However, using these resources for statistical language modeling is far from trivial due to the issue of ambiguous word analyses. Table 1 illustrates this problem in Italian, for which a fine-grained morphological lexicon but no sizable disambiguated corpus exists. These morphological ana"
2015.mtsummit-papers.12,2010.amta-papers.33,0,0.161766,"model with disambiguating the word sequence under construction, which is difficult given the ill-formedness of SMT output and a cause of spurious ambiguity. 2 This issue has also been shown to affect syntactic parsing of SMT output (Post and Gildea, 2008; Carter and Monz, 2011). Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 146 Considerably less work has focused on MRLs where disambiguated data does not exist, with few exceptions where ambiguity is solved by randomly selecting one analysis per word type (Minkov et al., 2007; Toutanova et al., 2008; Jeong et al., 2010). As for how inflection models are integrated into the STM system, different strategies have been proposed. Minkov et al. (2007); Toutanova et al. (2008); Fraser et al. (2012) treat inflection as a post-processing task: the SMT model is trained to produce lemmatized target sentences (possibly enhanced with some form of morphological annotation) and afterwards the best surface form for each lemma is chosen by separate inflection models. Some work has focused on the generation of new inflected phrases given the input sentence (Chahuneau et al., 2013) or given the bilingual context during decodin"
2015.mtsummit-papers.12,W04-3250,0,0.0241613,"data would reduce the impact of our inflection model, but currently we do not have access to other data sets that would be relevant to our translation tasks. To put these results into perspective, our improvements are comparable to those achieved by previous work that generated new phrase inflections using a morphological disambiguator (Chahuneau et al., 2013) on the same large-scale English-Russian task. 7 Riezler and Maxwell (2005) have shown that approximate randomization is less sensitive to Type-I errors, i. e. less likely to falsely reject the null hypothesis, than bootstrap resampling (Koehn, 2004) in the context of SMT. Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 153 SRC REF (1) BASE INFNN Effect: SRC REF (2) BASE INFNN Effect: SRC REF (3) BASE INFNN Effect: SRC REF (4) BASE INFNN Effect: SRC REF (5) BASE INFNN Effect: and if you’re wondering about those other spikes, those are also fridays e se vi state chiedendo cosa sono questi altri picchi, sono anche loro dei venerdì e se vi state chiedendo di queste altre picchi, sono anche il venerdì e se vi state chiedendo di questi altri picchi, sono anche il venerdì Correct number agreement betwee"
2015.mtsummit-papers.12,W08-0318,0,0.0182843,"observations that, when translating into MRLs, a large number of reference inflections are already available in the SMT models but are not selected for Viterbi translation (Green and DeNero, 2012; Tran et al., 2014). More in general, our work is related to class-based language modeling (Brown et al., 1992) with the major difference that we also condition on source-side context and that we use explicit morphological representations instead of data-driven word clusters (Uszkoreit and Brants, 2008), word suffixes (Müller et al., 2012; Bisazza and Monz, 2014) or coarse-grained part-ofspeech tags (Koehn et al., 2008). Modeling morphology using neural networks has recently shown promising results: in the context of monolingual neural language modeling, Luong et al. (2013); Botha and Blunsom (2014) obtain the vectorial representation of a word by composing the representations of its morphemes. Tran et al. (2014) model translation stem and suffix selection in SMT with a bilingual neural network. Soricut and Och (2015) discover morphological transformation rules from word embeddings learned by a shallow network. We are not aware of work that leveraged fine-grained morphological tags for neural language or tra"
2015.mtsummit-papers.12,D07-1091,0,0.350654,"resentations (Section 3). In Section 4 we introduce the general experimental setup, followed by a detailed description of the re-inflection experiments (Section 5) and the end-to-end SMT experiments (Section 6). We conclude with a discussion of SMT output examples and an outlook of future work 2 Previous Work Previous work on inflection modeling for translation into MRLs has mostly relied on the availability of morphologically disambiguated data to choose the most probable analysis of each word in either a context-independent (Minkov et al., 2007) or context-dependent (Green and DeNero, 2012; Koehn and Hoang, 2007; Subotin, 2011) way. While the former irrevocably discards potentially useful attributes of the words, the latter tasks the inflection model with disambiguating the word sequence under construction, which is difficult given the ill-formedness of SMT output and a cause of spurious ambiguity. 2 This issue has also been shown to affect syntactic parsing of SMT output (Post and Gildea, 2008; Carter and Monz, 2011). Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 146 Considerably less work has focused on MRLs where disambiguated data does not exist, with f"
2015.mtsummit-papers.12,P07-2045,0,0.00811056,"Missing"
2015.mtsummit-papers.12,P09-1067,0,0.0567845,"Missing"
2015.mtsummit-papers.12,W13-3512,0,0.0472851,"bi translation (Green and DeNero, 2012; Tran et al., 2014). More in general, our work is related to class-based language modeling (Brown et al., 1992) with the major difference that we also condition on source-side context and that we use explicit morphological representations instead of data-driven word clusters (Uszkoreit and Brants, 2008), word suffixes (Müller et al., 2012; Bisazza and Monz, 2014) or coarse-grained part-ofspeech tags (Koehn et al., 2008). Modeling morphology using neural networks has recently shown promising results: in the context of monolingual neural language modeling, Luong et al. (2013); Botha and Blunsom (2014) obtain the vectorial representation of a word by composing the representations of its morphemes. Tran et al. (2014) model translation stem and suffix selection in SMT with a bilingual neural network. Soricut and Och (2015) discover morphological transformation rules from word embeddings learned by a shallow network. We are not aware of work that leveraged fine-grained morphological tags for neural language or translation modeling. 3 A Distributed Inflection Model In MRLs, the surface form of a word is heavily determined by its grammatical features, such as number, ca"
2015.mtsummit-papers.12,P15-1002,0,0.0260468,"context. When translating into MRLs, standard statistical machine translation (SMT) models such as phrase translation models and n-gram language models (LMs) often fail to select the right surface form due to the sparsity of observed word sequences (Minkov et al., 2007; Green and DeNero, 2012). While neural LMs (Bengio et al., 2003; Schwenk, 2007) address lexical sparsity to a certain degree by projecting word sequences to distributed vector representations, they still suffer from the problem of rare words which is particularly exacerbated in MRLs (Botha and Blunsom, 2014; Jean et al., 2015; Luong et al., 2015). A potential solution to overcome data sparsity in MRLs, is to use word representations that separate the grammatical aspects of a word, i. e. inflection, from the lexical ones. Such word representations already exist for many languages in the form of morphological analyzers or lexicons. However, using these resources for statistical language modeling is far from trivial due to the issue of ambiguous word analyses. Table 1 illustrates this problem in Italian, for which a fine-grained morphological lexicon but no sizable disambiguated corpus exists. These morphological analyses1 clearly contai"
2015.mtsummit-papers.12,P07-1017,0,0.477834,"ained on morphologically disambiguated data. Integrated into an SMT decoder and evaluated for English-Italian and English-Russian translation, our model yields improvements of up to 1.0 BLEU over a competitive baseline. 1 Introduction In morphologically rich languages (MRLs), words can have many different surface forms depending on the grammatical context. When translating into MRLs, standard statistical machine translation (SMT) models such as phrase translation models and n-gram language models (LMs) often fail to select the right surface form due to the sparsity of observed word sequences (Minkov et al., 2007; Green and DeNero, 2012). While neural LMs (Bengio et al., 2003; Schwenk, 2007) address lexical sparsity to a certain degree by projecting word sequences to distributed vector representations, they still suffer from the problem of rare words which is particularly exacerbated in MRLs (Botha and Blunsom, 2014; Jean et al., 2015; Luong et al., 2015). A potential solution to overcome data sparsity in MRLs, is to use word representations that separate the grammatical aspects of a word, i. e. inflection, from the lexical ones. Such word representations already exist for many languages in the form o"
2015.mtsummit-papers.12,J03-1002,0,0.00706548,"following tasks: • English-Russian WMT (Bojar et al., 2013): translation of news commentaries with largescale training data. • English-Italian IWSLT (Cettolo et al., 2014): translation of speeches with either smallscale training data (TED talks only) or large-scale training data (TED talks and European proceedings). SMT training data statistics are reported in Table 2. The Russian Inf-NN model is trained on a 1M-sentence subset of the bilingual data, while the Italian one is trained on all the data available in each setting. For each data set, we create automatic word alignments using GIZA++ (Och and Ney, 2003). Bilingual Monoling. En-Ru large small large #sentences src/trg #tokens src/trg dict.size 2.4M 49.2/47.2M 774K/1100K 180K 3.6/3.4M 55K/80K 2.0M 57.4/57.0M 139K/195K #sentences trg #tokens src/trg dict.size 21.0M 390M 2.7M En-It 2.1M 58.4M 199K Table 2: Training corpora statistics. The ambiguous morphological analyses are obtained from the Russian OpenCorpora lexicon4 (Bocharov et al., 2013) and from the Italian Morph-it!5 lexicon (Zanchetta and Baroni, 2005). Table 3 shows the number of tags and soft tags occurring in our training data, as well as the expected counts of analyses per word Ew ["
2015.mtsummit-papers.12,P02-1040,0,0.0921194,"Missing"
2015.mtsummit-papers.12,D14-1162,0,0.0793013,".7 versus 7.2 words per lemma). At a closer inspection, we find that most of this richness is due to verbal inflection which goes up to 50 forms for frequently observed verbs. 4.2 Neural network training The Inf-NN models are trained on a history of 4 target tags and source context of 7 words with the following configuration: Embedding size is set to 200 and the number of hidden units to 768. Target word and soft-tag embeddings are initialized randomly from a Gaussian distribution with mean zero and standard deviation 0.01. Source word embeddings are initialized from pretrained Glove vectors (Pennington et al., 2014) and rescaled by a factor of 0.1. Weight matrices of p linear layers are initialized from a zero-mean Gaussian distribution with standard deviation 2/ni where ni is the number of input units (He et al., 2015). We set self-normalization strength γ = 0.02, Bernoulli parameter p = 0.1, and regularization parameter η = 10−4 . All models are trained with a mini-batch size of 128 for 30 epochs. Our stochastic objective functions are optimized using the first-order gradient-based optimizer Adam (Kingma and Ba, 2015). We use the default settings suggested by the authors: α = 0.001, β1 = 0.9, β2 = 0.99"
2015.mtsummit-papers.12,2008.amta-papers.16,0,0.0255149,"d on the availability of morphologically disambiguated data to choose the most probable analysis of each word in either a context-independent (Minkov et al., 2007) or context-dependent (Green and DeNero, 2012; Koehn and Hoang, 2007; Subotin, 2011) way. While the former irrevocably discards potentially useful attributes of the words, the latter tasks the inflection model with disambiguating the word sequence under construction, which is difficult given the ill-formedness of SMT output and a cause of spurious ambiguity. 2 This issue has also been shown to affect syntactic parsing of SMT output (Post and Gildea, 2008; Carter and Monz, 2011). Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 146 Considerably less work has focused on MRLs where disambiguated data does not exist, with few exceptions where ambiguity is solved by randomly selecting one analysis per word type (Minkov et al., 2007; Toutanova et al., 2008; Jeong et al., 2010). As for how inflection models are integrated into the STM system, different strategies have been proposed. Minkov et al. (2007); Toutanova et al. (2008); Fraser et al. (2012) treat inflection as a post-processing task: the SMT model is"
2015.mtsummit-papers.12,W05-0908,0,0.015192,"ts that morphological phenomena are not sufficiently captured by phrases and stresses the importance of specifically modeling word inflection. It is possible that adding even more training data would reduce the impact of our inflection model, but currently we do not have access to other data sets that would be relevant to our translation tasks. To put these results into perspective, our improvements are comparable to those achieved by previous work that generated new phrase inflections using a morphological disambiguator (Chahuneau et al., 2013) on the same large-scale English-Russian task. 7 Riezler and Maxwell (2005) have shown that approximate randomization is less sensitive to Type-I errors, i. e. less likely to falsely reject the null hypothesis, than bootstrap resampling (Koehn, 2004) in the context of SMT. Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 153 SRC REF (1) BASE INFNN Effect: SRC REF (2) BASE INFNN Effect: SRC REF (3) BASE INFNN Effect: SRC REF (4) BASE INFNN Effect: SRC REF (5) BASE INFNN Effect: and if you’re wondering about those other spikes, those are also fridays e se vi state chiedendo cosa sono questi altri picchi, sono anche loro dei vene"
2015.mtsummit-papers.12,sharoff-etal-2008-designing,0,0.02683,"mma of w or, more formally, as: Iw = {wi |lem(wi ) ∩ lem(w) 6= ∅} where lem(w) denotes the set of lemmas returned by the lexicon for word w. For example, the Italian form baci has two possible lemmas: bacio (noun: kiss) and baciare (verb: to kiss). Its candidate set Iw will then include all the forms of the noun bacio and all the forms of the verb baciare: that is, bacio, baci, baciamo, baciate, baciano, etc. We compare the proposed soft-tag Inf-NN against an Inf-NN trained on randomly assigned tag per type and to another one trained on tag sequences disambiguated by TreeTagger (Schmid, 1994; Sharoff et al., 2008). The latter model must search through a much larger space of morphological tag sequences. Therefore, to allow for a fair comparison, we set a higher beam size when re-inflecting with this model. As another difference from the other models, the TreeTagger-based inflection model relies on the lemmatization performed by TreeTagger to define the candidate set Iw . To validate the effectiveness of the neural network approach, we also compare Inf-NN to a simpler MaxEnt model trained on a similar configuration. Finally, we evaluate the importance of source-side context features by experimenting with"
2015.mtsummit-papers.12,C96-2215,0,0.0422269,"Missing"
2015.mtsummit-papers.12,N15-1186,0,0.119135,"t morphological representations instead of data-driven word clusters (Uszkoreit and Brants, 2008), word suffixes (Müller et al., 2012; Bisazza and Monz, 2014) or coarse-grained part-ofspeech tags (Koehn et al., 2008). Modeling morphology using neural networks has recently shown promising results: in the context of monolingual neural language modeling, Luong et al. (2013); Botha and Blunsom (2014) obtain the vectorial representation of a word by composing the representations of its morphemes. Tran et al. (2014) model translation stem and suffix selection in SMT with a bilingual neural network. Soricut and Och (2015) discover morphological transformation rules from word embeddings learned by a shallow network. We are not aware of work that leveraged fine-grained morphological tags for neural language or translation modeling. 3 A Distributed Inflection Model In MRLs, the surface form of a word is heavily determined by its grammatical features, such as number, case, tense etc. Choosing the right target word form during translation is a complex problem since some of these features depend on the source context while others depend on the target context (agreement phenomena). We model target language inflection"
2015.mtsummit-papers.12,P11-1024,0,0.0883783,"). In Section 4 we introduce the general experimental setup, followed by a detailed description of the re-inflection experiments (Section 5) and the end-to-end SMT experiments (Section 6). We conclude with a discussion of SMT output examples and an outlook of future work 2 Previous Work Previous work on inflection modeling for translation into MRLs has mostly relied on the availability of morphologically disambiguated data to choose the most probable analysis of each word in either a context-independent (Minkov et al., 2007) or context-dependent (Green and DeNero, 2012; Koehn and Hoang, 2007; Subotin, 2011) way. While the former irrevocably discards potentially useful attributes of the words, the latter tasks the inflection model with disambiguating the word sequence under construction, which is difficult given the ill-formedness of SMT output and a cause of spurious ambiguity. 2 This issue has also been shown to affect syntactic parsing of SMT output (Post and Gildea, 2008; Carter and Monz, 2011). Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 146 Considerably less work has focused on MRLs where disambiguated data does not exist, with few exceptions wh"
2015.mtsummit-papers.12,P08-1059,0,0.0689098,"Missing"
2015.mtsummit-papers.12,D14-1175,1,0.904902,"a post-processing task: the SMT model is trained to produce lemmatized target sentences (possibly enhanced with some form of morphological annotation) and afterwards the best surface form for each lemma is chosen by separate inflection models. Some work has focused on the generation of new inflected phrases given the input sentence (Chahuneau et al., 2013) or given the bilingual context during decoding (Koehn and Hoang, 2007; Subotin, 2011). Other inflection models have been integrated to SMT as additional feature functions: e.g. as an additional lexical translation score (Jeong et al., 2010; Tran et al., 2014) or as an additional target language model score (Green and DeNero, 2012). We follow this last strategy, rather than generating new inflections, motivated by previous observations that, when translating into MRLs, a large number of reference inflections are already available in the SMT models but are not selected for Viterbi translation (Green and DeNero, 2012; Tran et al., 2014). More in general, our work is related to class-based language modeling (Brown et al., 1992) with the major difference that we also condition on source-side context and that we use explicit morphological representation"
2015.mtsummit-papers.12,P08-1086,0,0.0266727,"el score (Green and DeNero, 2012). We follow this last strategy, rather than generating new inflections, motivated by previous observations that, when translating into MRLs, a large number of reference inflections are already available in the SMT models but are not selected for Viterbi translation (Green and DeNero, 2012; Tran et al., 2014). More in general, our work is related to class-based language modeling (Brown et al., 1992) with the major difference that we also condition on source-side context and that we use explicit morphological representations instead of data-driven word clusters (Uszkoreit and Brants, 2008), word suffixes (Müller et al., 2012; Bisazza and Monz, 2014) or coarse-grained part-ofspeech tags (Koehn et al., 2008). Modeling morphology using neural networks has recently shown promising results: in the context of monolingual neural language modeling, Luong et al. (2013); Botha and Blunsom (2014) obtain the vectorial representation of a word by composing the representations of its morphemes. Tran et al. (2014) model translation stem and suffix selection in SMT with a bilingual neural network. Soricut and Och (2015) discover morphological transformation rules from word embeddings learned b"
2016.amta-researchers.12,C14-1181,1,0.812456,"king step since direct integration into the decoder would make hypothesis recombination more complex and substantially increase the size of the search space. In addition to reordering models, several approaches have used word classes to improve other models within a statistical machine translation system, including translation (Wuebker 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S et al., 2013) and language models, where the problem of data sparsity is particularly exacerbated for morphologically rich target languages (Chahuneau et al., 2013; Bisazza and Monz, 2014). 4 Model Deﬁnition In this section, we propose two different models which use different words as source of information to better estimate reordering distributions of sparse phrase pairs. Each model uses a different generalization scheme to obtain less sparse but still informative representations. 4.1 Interpolated Back-off Sub-phrases In n-gram language modeling shorter n-grams have been used to smooth the probability distributions of higher order sparse n-grams. Lower order n-grams form the basis of Jelinek-Mercer, Katz, Witten-Bell and absolute discount smoothing methods (Chen and Goodman, 1"
2016.amta-researchers.12,D13-1174,0,0.0262322,"itional hypergraph reranking step since direct integration into the decoder would make hypothesis recombination more complex and substantially increase the size of the search space. In addition to reordering models, several approaches have used word classes to improve other models within a statistical machine translation system, including translation (Wuebker 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S et al., 2013) and language models, where the problem of data sparsity is particularly exacerbated for morphologically rich target languages (Chahuneau et al., 2013; Bisazza and Monz, 2014). 4 Model Deﬁnition In this section, we propose two different models which use different words as source of information to better estimate reordering distributions of sparse phrase pairs. Each model uses a different generalization scheme to obtain less sparse but still informative representations. 4.1 Interpolated Back-off Sub-phrases In n-gram language modeling shorter n-grams have been used to smooth the probability distributions of higher order sparse n-grams. Lower order n-grams form the basis of Jelinek-Mercer, Katz, Witten-Bell and absolute discount smoothing met"
2016.amta-researchers.12,N13-1114,0,0.085697,"rder to smooth the original maximum likelihood estimation, LRMs originally back off to the general distribution over orientations: Cpo, f¯, e¯q ` σP poq P po |f¯, e¯q “ ř 1 ¯ ¯q ` σ o1 Cpo , f , e (1) which is also known as Dirichlet smoothing, where σP poq denotes the parameters of the Dirichlet prior that maximizes the likelihood of the observed data, Cpo, f¯, e¯q refers to the number of times a phrase pair cooccurs with orientation o, and σ is the equivalent sample size, i.e., the number of samples required from P poq to reﬂect the observed data (Smucker and Allan, 2005). Cherry (2013) and Chen et al. (2013) introduce recursive MAP smoothing, which makes use of more speciﬁc priors by recursively backing off to orientation priors, see Equation 2. While recursive MAP smoothing factorizes phrase pairs into source and target phrases, it still considers the phrases themselves as ﬁxed units. 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S Cpo, e¯, f¯q ` αs Ps po |f¯q ` αt Pt po |e¯q ř 1 ¯, f¯q ` α ` α s t o1 Cpo , e ř ¯ ¯q ` αg Pg poq e¯ Cpo, f , e Ps po |f¯q “ ř 1 , f¯, e Cpo ¯q ` αg 1 o ,¯ e ř ¯ ¯q ` αg Pg poq f¯ Cpo, f , e Pt po |e¯q “ ř 1 ¯ ¯q ` αg o"
2016.amta-researchers.12,D14-1082,0,0.0155774,"simply using the ﬁrst and last words. 5.2 Comparison Systems and Results We compare the baseline to the models described in Section 4. For all systems other than the baseline, the lexicalized and the hierarchical reordering models are replaced by the corresponding smoothed models. When computing the RecursiveBackOff model (Section 4.2), using Equation 5, we set the value of α to 10, following Cherry (2013) and Chen et al. (2013). For the dependency-based model, we use the dependency parses of the source and the target side of the training corpus. The Stanford Neural-network dependency parser (Chen and Manning, 2014) is used to generate parses for both sides of the training corpus. From a dependency parse, we extract the smallest subtree that includes all incoming and outgoing relations of the words of a phrase. This is done for both the source and the target side phrases. Considering these subtrees, all words with an incoming connection from outside are exposed heads. The experimental results for all models are shown in Table 3. As one can see, all our models achieve improvements in terms of BLEU on the test sets. The improvements for our back-off models are only signiﬁcant for RecursiveBackOff over MT06"
2016.amta-researchers.12,N13-1003,0,0.197815,"consists of: monotone (M), swap (S) and discontinuous (D). However, LRMs are limited to reorderings of neighboring phrases only. Galley and Manning (2008) proposed a hierarchical phrase reordering model (HRM) for more global reorderings. LRMs and HRMs both use relative frequencies observed in a parallel corpus to estimate the distribution of orientations conditioned on phrase pairs. As a result, they both suffer from the same problem of estimating reliable distributions for cases that occur rarely during training and therefore have to resort to smoothing methods to alleviate sparsity issues. Cherry (2013) builds on top of HRMs and proposes a sparse feature approach which uses word clusters instead of fully lexicalized forms for infrequent words to decrease the effect of sparsity on the estimated model. In this paper, we propose two types of approaches to use the most inﬂuential words from inside the original phrase pairs to estimate better orientation distributions for infrequent phrase pairs that takes phrase pair similarity more into account. In the ﬁrst approach, we deﬁne a backoff model to shorten towards important words inside the original phrase pairs following the idea 3URFHHGLQJVRI$0"
2016.amta-researchers.12,C14-1041,0,0.0143353,"still conditioned on entire phrase pairs, which means that their approach suffers from the same sparsity problems as LRMs. This problem has been more directly addressed by Cherry (2013) who uses unsupervised word classes for infrequent words of the phrase pairs in the form of sparse features. Like (Nagata et al., 2006), the ﬁrst and last words of phrase pair are used as features in his model. Unfortunately, this approach also introduces thousands of additional sparse features, many of which have to be extracted during decoding, requiring changes to the decoder as well as a sizable tuning set. Durrani et al. (2014) investigate the effect of generalized word forms on reordering in an n-gram-based operation sequence model, where they use different generalized representations including POS tags and unsupervised word classes to generalize reordering rules to similar cases with unobserved lexical operations. While the approaches above use discrete representations, (Li et al., 2014) propose a discriminative model using continuous space representations of phrase pairs to address data sparsity problems. They train a neural network classiﬁer based on recursive auto-encoders to generate vector space representatio"
2016.amta-researchers.12,D08-1089,0,0.45217,"arsity problem for long phrase pairs. 1 Introduction The introduction of lexicalized reordering models (LRMs) (Tillmann, 2004; Koehn et al., 2005) was a signiﬁcant step towards better reordering by modeling the orientation of the current phrase pair with respect to the previously translated phrase. LRMs score the order in which phrases are translated by using a distribution of distinguished orientations conditioned on phrase pairs. Typically, the set of orientations consists of: monotone (M), swap (S) and discontinuous (D). However, LRMs are limited to reorderings of neighboring phrases only. Galley and Manning (2008) proposed a hierarchical phrase reordering model (HRM) for more global reorderings. LRMs and HRMs both use relative frequencies observed in a parallel corpus to estimate the distribution of orientations conditioned on phrase pairs. As a result, they both suffer from the same problem of estimating reliable distributions for cases that occur rarely during training and therefore have to resort to smoothing methods to alleviate sparsity issues. Cherry (2013) builds on top of HRMs and proposes a sparse feature approach which uses word clusters instead of fully lexicalized forms for infrequent words"
2016.amta-researchers.12,D15-1287,1,0.835782,"ranslation (Li et al., 2012) suggests that using heads of phrases can be beneﬁcial for better reordering in general. In our work, we deﬁne the heads of a phrase to be its exposed heads. Given a dependency parse, the exposed heads are all words inside a subsequence that are modifying a word outside it. Figure 4 shows an example of exposed heads in a phrase pair. The highlighted words are the exposed heads of the phrase pair. Exposed heads have been used in multiple linguistically motivated approaches as strong predictors of the next word in structured language models (Chelba and Jelinek, 2000; Garmash and Monz, 2015) and the next rule in a hierarchical translation system (Li et al., 2012). In our model, besides training a regular lexicalized or hierarchical reordering model on surface forms of phrases, we train another reordering model which keeps the exposed heads lexicalized and replaces the remaining words in a phrase pair by a generalized representation. Assume that RE is the set of dependency relations in the dependency parse tree of sentence S. Figure 4: Examples of exposed heads in a Chinese-English phrase pair (between square brackets). The underlined words are the exposed heads since they have an"
2016.amta-researchers.12,D11-1125,0,0.0186333,"33.5K Table 2: Statistics for the Chinese-English bilingual corpora used in all experiments. Token counts for the English side of dev and test sets are averaged across all references. with 1.6B tokens using interpolated, modiﬁed Kneser-Ney smoothing. The lexicalized and the hierarchical reordering models are trained with relative and smoothed frequencies using Dirichlet smoothing (Equation 1), for both left-to-right and right-to-left directions distinguishing four orientations: monotone (M), swap (S), discontinuous left (DL), and discontinuous right (DR). Feature weights are tuned using PRO (Hopkins and May, 2011) and statistical differences are computed using approximate randomization (Riezler and Maxwell, 2005). In addition to the baseline, we reimplemented the 2POS model by Nagata et al. (2006), which uses the POS tag of the ﬁrst and last words of a phrase pair to smooth the reordering distributions. The 2POS model is used in combination with the baseline lexicalized and hierarchical reordering models. Comparing our models to the 2POS model allows us to see whether backed-off sub-phrases and exposed heads of phrase pairs yield better performance than simply using the ﬁrst and last words. 5.2 Compari"
2016.amta-researchers.12,D10-1092,0,0.0240128,"eft hand side Ĳ or Ÿ is with respect to Lex+Hrc and the right hand side ones with respect to Nagata’s 2POS. PMLH refers to the model using POS tags for modiﬁers and keeps exposed heads lexicalized. MMLH merges modiﬁers and keeps exposed heads lexicalized. LH removes modiﬁers. LHSmoothed uses the LH model with Dirichlet smoothing. In addition to BLEU, we also report results using TER. Results for TER are in line with BLEU. BLEU and TER are general translation quality metrics, which are known to be not very sensitive to reordering changes (Birch et al., 2010). To this end we also include RIBES (Isozaki et al., 2010), a reordering-speciﬁc metric that is designed to directly address word-order differences between hypothesis and reference translation in translation tasks with long distant reordering language pairs and is highly sensitive to word-order mistakes. 5.3 Analysis The improvements achieved by our BackOff and RecursiveBackOff methods show that these models capture some useful generalizations by shortening the phrase pairs towards the last aligned words in the target side as the most important words. The difference between the two models indicates that shortening is less beneﬁcial for frequent phras"
2016.amta-researchers.12,2005.iwslt-1.8,0,0.328075,"hese models can be easily applied to existing lexicalized and hierarchical reordering models. Our models achieve improvements of up to 0.40 BLEU points in Chinese-English translation compared to a baseline which uses a regular lexicalized reordering model and a hierarchical reordering model. The results show that not all the words inside a phrase pair are equally important in deﬁning phrase reordering behavior and shortening towards important words will decrease the sparsity problem for long phrase pairs. 1 Introduction The introduction of lexicalized reordering models (LRMs) (Tillmann, 2004; Koehn et al., 2005) was a signiﬁcant step towards better reordering by modeling the orientation of the current phrase pair with respect to the previously translated phrase. LRMs score the order in which phrases are translated by using a distribution of distinguished orientations conditioned on phrase pairs. Typically, the set of orientations consists of: monotone (M), swap (S) and discontinuous (D). However, LRMs are limited to reorderings of neighboring phrases only. Galley and Manning (2008) proposed a hierarchical phrase reordering model (HRM) for more global reorderings. LRMs and HRMs both use relative frequ"
2016.amta-researchers.12,P07-2045,0,0.00942895,"utions since it affects the distributions of frequent phrase pairs to a lesser extent. 5 Experiments We evaluate our models for Chinese-to-English translation. Our training data consists of the parallel and monolingual data released by NIST’s OpenMT campaign, with MT04 used for tuning and news data from MT05 and MT06 for testing, see Table 2. Case-insensitive BLEU (Papineni et al., 2002) and translation error rate (TER) (Snover et al., 2006) are used as evaluation metrics. 5.1 Baseline We use an in-house implementation of a phrase-based statistical machine translation system similar to Moses (Koehn et al., 2007), including the commonly used translation, lexical weighting, language, lexicalized reordering, and hierarchical reordering models. We use both lexicalized and hierarchical reordering models together, since this is the best model reported in (Galley and Manning, 2008) and our smoothing methods can be easily applied to the both models. Word alignments are produced using GIZA++ (Och and Ney, 2003), using grow-diag-ﬁnal-and (Koehn et al., 2003). A 5-gram language model is trained on the English Gigaword corpus 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_"
2016.amta-researchers.12,N03-1017,0,0.0731737,"are used as evaluation metrics. 5.1 Baseline We use an in-house implementation of a phrase-based statistical machine translation system similar to Moses (Koehn et al., 2007), including the commonly used translation, lexical weighting, language, lexicalized reordering, and hierarchical reordering models. We use both lexicalized and hierarchical reordering models together, since this is the best model reported in (Galley and Manning, 2008) and our smoothing methods can be easily applied to the both models. Word alignments are produced using GIZA++ (Och and Ney, 2003), using grow-diag-ﬁnal-and (Koehn et al., 2003). A 5-gram language model is trained on the English Gigaword corpus 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S Corpus train MT04 (dev) MT05 (test) MT06 (test) Lines 937K 1,788 1,082 1,181 Tokens(ch) 22.3M 49.6K 30.3K 29.7K Tokens(en) 25,9M 59.2K 35.8K 33.5K Table 2: Statistics for the Chinese-English bilingual corpora used in all experiments. Token counts for the English side of dev and test sets are averaged across all references. with 1.6B tokens using interpolated, modiﬁed Kneser-Ney smoothing. The lexicalized and the hierarchical reorde"
2016.amta-researchers.12,P12-2007,0,0.0576641,"Missing"
2016.amta-researchers.12,C14-1179,0,0.0171454,"eatures in his model. Unfortunately, this approach also introduces thousands of additional sparse features, many of which have to be extracted during decoding, requiring changes to the decoder as well as a sizable tuning set. Durrani et al. (2014) investigate the effect of generalized word forms on reordering in an n-gram-based operation sequence model, where they use different generalized representations including POS tags and unsupervised word classes to generalize reordering rules to similar cases with unobserved lexical operations. While the approaches above use discrete representations, (Li et al., 2014) propose a discriminative model using continuous space representations of phrase pairs to address data sparsity problems. They train a neural network classiﬁer based on recursive auto-encoders to generate vector space representations of phrase pairs and base reordering decision on those representations. They apply their model as an additional hypergraph reranking step since direct integration into the decoder would make hypothesis recombination more complex and substantially increase the size of the search space. In addition to reordering models, several approaches have used word classes to im"
2016.amta-researchers.12,P06-1090,0,0.306169,"eading to a very different distribution, due to the smoothing prior, while being semantically and syntactically close to (a) and (b). In Table 1, we can also observe that recursive MAP smoothing results in slightly more similar distributions compared to plain Dirichlet smoothing but the overall differences remain noticeable. In this paper, we argue that in order to obtain smoother reordering distributions for phrasepairs such as the ones in Table 1, one has to take phrase-internal information into account. 3 Related Work The problem of data sparsity of training LRMs has ﬁrst been addressed by Nagata et al. (2006) who propose to use POS tags and word clustering methods and distinguish the ﬁrst or last word of a phrase, based on the language, as the head of a phrase. Somewhat complementary to our work, Galley and Manning (2008) introduced hierarchical reordering models that group phrases occurring next to the current phrase into blocks, ignoring the internal derivation within a block, which biases orientations more towards monotone and swap. At the same time, orientations are still conditioned on entire phrase pairs, which means that their approach suffers from the same sparsity problems as LRMs. This p"
2016.amta-researchers.12,J03-1002,0,0.0584097,"ng at the kth word of f¯, e¯rl ,ns is a sub phrase of e¯ with the length of l1 which ends at the last word of e¯ and the function Ωpf¯rl,ks , f¯, oq returns the correct orientation considering the position of source sub-phrase f¯rl,ks with respect to either end of the source phrase f¯ and orientation o. In order to compute the linear interpolation over the conditional distributions of the subphrase pairs, we have to determine the weight of each term in the linear interpolation (Equation 4). Here, we use expectation-maximization (EM) over a held-out data set, which is wordaligned using GIZA++ (Och and Ney, 2003). We extract phrase pairs using a common phrase extraction algorithm (Koehn et al., 2005) and count the number of occurrences of orientations for each phrase pair. These counts are used with unsmoothed reordering probabilities learned over the training data to compute the likelihood over the held-out data. We designed the EM algorithm to learn a set of lambda parameters for each length combination of the original phrase pairs. To reduce the number of parameters, we assume that all sub-phrases with the same length on source and target side share the same weight. This model is referred to as the"
2016.amta-researchers.12,P02-1040,0,0.0952782,"e to use the generalized distributions as a prior distribution in Dirichlet smoothing, where the distribution of each phrase pair is smoothed with the distribution of its generalized form. This should result in more accurate distributions since it affects the distributions of frequent phrase pairs to a lesser extent. 5 Experiments We evaluate our models for Chinese-to-English translation. Our training data consists of the parallel and monolingual data released by NIST’s OpenMT campaign, with MT04 used for tuning and news data from MT05 and MT06 for testing, see Table 2. Case-insensitive BLEU (Papineni et al., 2002) and translation error rate (TER) (Snover et al., 2006) are used as evaluation metrics. 5.1 Baseline We use an in-house implementation of a phrase-based statistical machine translation system similar to Moses (Koehn et al., 2007), including the commonly used translation, lexical weighting, language, lexicalized reordering, and hierarchical reordering models. We use both lexicalized and hierarchical reordering models together, since this is the best model reported in (Galley and Manning, 2008) and our smoothing methods can be easily applied to the both models. Word alignments are produced using"
2016.amta-researchers.12,W05-0908,0,0.029763,"en counts for the English side of dev and test sets are averaged across all references. with 1.6B tokens using interpolated, modiﬁed Kneser-Ney smoothing. The lexicalized and the hierarchical reordering models are trained with relative and smoothed frequencies using Dirichlet smoothing (Equation 1), for both left-to-right and right-to-left directions distinguishing four orientations: monotone (M), swap (S), discontinuous left (DL), and discontinuous right (DR). Feature weights are tuned using PRO (Hopkins and May, 2011) and statistical differences are computed using approximate randomization (Riezler and Maxwell, 2005). In addition to the baseline, we reimplemented the 2POS model by Nagata et al. (2006), which uses the POS tag of the ﬁrst and last words of a phrase pair to smooth the reordering distributions. The 2POS model is used in combination with the baseline lexicalized and hierarchical reordering models. Comparing our models to the 2POS model allows us to see whether backed-off sub-phrases and exposed heads of phrase pairs yield better performance than simply using the ﬁrst and last words. 5.2 Comparison Systems and Results We compare the baseline to the models described in Section 4. For all systems"
2016.amta-researchers.12,2006.amta-papers.25,0,0.071128,"ution in Dirichlet smoothing, where the distribution of each phrase pair is smoothed with the distribution of its generalized form. This should result in more accurate distributions since it affects the distributions of frequent phrase pairs to a lesser extent. 5 Experiments We evaluate our models for Chinese-to-English translation. Our training data consists of the parallel and monolingual data released by NIST’s OpenMT campaign, with MT04 used for tuning and news data from MT05 and MT06 for testing, see Table 2. Case-insensitive BLEU (Papineni et al., 2002) and translation error rate (TER) (Snover et al., 2006) are used as evaluation metrics. 5.1 Baseline We use an in-house implementation of a phrase-based statistical machine translation system similar to Moses (Koehn et al., 2007), including the commonly used translation, lexical weighting, language, lexicalized reordering, and hierarchical reordering models. We use both lexicalized and hierarchical reordering models together, since this is the best model reported in (Galley and Manning, 2008) and our smoothing methods can be easily applied to the both models. Word alignments are produced using GIZA++ (Och and Ney, 2003), using grow-diag-ﬁnal-and ("
2016.amta-researchers.12,N04-4026,0,0.0695829,"o. We show how these models can be easily applied to existing lexicalized and hierarchical reordering models. Our models achieve improvements of up to 0.40 BLEU points in Chinese-English translation compared to a baseline which uses a regular lexicalized reordering model and a hierarchical reordering model. The results show that not all the words inside a phrase pair are equally important in deﬁning phrase reordering behavior and shortening towards important words will decrease the sparsity problem for long phrase pairs. 1 Introduction The introduction of lexicalized reordering models (LRMs) (Tillmann, 2004; Koehn et al., 2005) was a signiﬁcant step towards better reordering by modeling the orientation of the current phrase pair with respect to the previously translated phrase. LRMs score the order in which phrases are translated by using a distribution of distinguished orientations conditioned on phrase pairs. Typically, the set of orientations consists of: monotone (M), swap (S) and discontinuous (D). However, LRMs are limited to reorderings of neighboring phrases only. Galley and Manning (2008) proposed a hierarchical phrase reordering model (HRM) for more global reorderings. LRMs and HRMs bo"
2016.amta-researchers.12,D13-1138,0,0.0276247,"Missing"
2020.coling-main.304,N19-1388,0,0.0828262,"NMT system can perform relatively well under low-resource data conditions. Unfortunately, their results are confined to a recurrent NMT architecture (Sennrich et al., 2017), and it is not clear to what extent these findings also hold for the nowadays much more commonly used Transformer architecture (Vaswani et al., 2017). Like all NMT models, Transformer requires setting various hyper-parameters but researchers often stick to the default parameters, even when their data conditions differ substantially from the original data conditions used to determine those default values (Gu et al., 2018b; Aharoni et al., 2019). In this paper, we explore to what extent hyper-parameter optimization, which has been applied successfully to recurrent NMT models for low-resource translation, is also beneficial for the Transformer model. We show that with the appropriate settings, including the number of BPE merge operations, attention heads, and layers up to the degree of dropout and label smoothing, translation performance can be increased substantially, even for data sets with as little as 5k sentence pairs. Our experiments on different corpus sizes, ranging from 5k to 165k sentence pairs, show the importance of choosi"
2020.coling-main.304,2012.eamt-1.60,0,0.0107439,"165,667 sentence pairs for training and 1,938 sentence pairs for development. In order to create smaller training sets, we randomly sample 5k, 10k, 20k, 40k, and 80k sentence pairs from the training data. Similar to Sennrich and Zhang (2019), we use the concatenation of the IWLST 2014 dev sets (tst2010–2012, dev2010, dev2012) as our test set, which consists of 6,750 sentence pairs. For actual low-resource languages, we evaluate our optimized systems on the original test sets of Belarusian (Be), Galician (Gl), and Slovak (Sk) TED talks (Qi et al., 2018) and also Slovenian (Sl) from IWSLT2014 (Cettolo et al., 2012) with training sets ranging from 4.5k to 55k sentence pairs. We use Transformer-base and Transformer-big as our baselines, with the hyper-parameters and optimizer settings described in (Vaswani et al., 2017). We use the Fairseq library (Ott et al., 2019) for our experiments and sacreBLEU (Post, 2018) as evaluation metric. 3.2 Results and discussions BPE effect. To evaluate the effect of different degrees of BPE segmentation on performance, we consider merge operations ranging from 1k to 30k, training BPE on the full training corpus instead of subsets and also removing infrequent subword units"
2020.coling-main.304,2014.iwslt-evaluation.1,0,0.0449429,"Missing"
2020.coling-main.304,P18-1008,0,0.0250965,"er. However, we did not observe any improvements over the default Transformer learning rate scheduler. Optimized parameter settings. Table 3 shows the optimal settings for each dataset size, achieved by tuning the parameters on the development data. We observe that a shallower Transformer combined with a smaller feed-forward layer dimension and BPE vocabulary size is more effective under lower-resource conditions. However, as mentioned above, Transformer is not as sensitive to the BPE vocabulary size as RNNs and reducing the embedding dimension size is not effective. Vaswani et al. (2017) and Chen et al. (2018) show that reducing the number of attention heads decreases the BLEU score under high-resource conditions. Raganato et al. (2020) show that using one 3432 BLEU BLEU sentences words (En) T-base T-opt 100k 200k 410k 830k 1.6M 3.4M 6.4 9.3 13.5 20.2 24.2 27.4 11.3 15.6 20.8 24.5 27.2 29.8 En→De 5k 10k 20k 40k 80k 165k sentences En→Be (4.5k) En→Gl (10k) En→Sl (13k) En→Sk (55k) Table 5: Results for En→De based on the optimal settings for De→En for the corresponding corpus size (see Table 3). words (En) T-base T-opt 90k 196k 269k 1.2M 3.6 10.6 6.8 19.5 6.6 18.7 12.2 23.5 Table 6: Results for low-res"
2020.coling-main.304,D14-1179,0,0.0604276,"Missing"
2020.coling-main.304,P17-2090,1,0.663979,"the translation quality up to 7.3 BLEU points compared to using the Transformer default settings. 1 Introduction Despite the success of Neural Machine Translation (NMT) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015), for the vast majority of language pairs for which only limited amounts of training data exist (a.k.a. low-resource languages), the performance of NMT systems is relatively poor (Koehn and Knowles, 2017; Gu et al., 2018a). Most approaches focus on exploiting additional data to address this problem (G¨ulc¸ehre et al., 2015; Sennrich et al., 2016; He et al., 2016; Fadaee et al., 2017). However, Sennrich and Zhang (2019) show that a well-optimized NMT system can perform relatively well under low-resource data conditions. Unfortunately, their results are confined to a recurrent NMT architecture (Sennrich et al., 2017), and it is not clear to what extent these findings also hold for the nowadays much more commonly used Transformer architecture (Vaswani et al., 2017). Like all NMT models, Transformer requires setting various hyper-parameters but researchers often stick to the default parameters, even when their data conditions differ substantially from the original data condit"
2020.coling-main.304,N18-1032,0,0.0289363,"ce conditions is highly dependent on the hyper-parameter settings. Our experiments show that using an optimized Transformer for low-resource conditions improves the translation quality up to 7.3 BLEU points compared to using the Transformer default settings. 1 Introduction Despite the success of Neural Machine Translation (NMT) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015), for the vast majority of language pairs for which only limited amounts of training data exist (a.k.a. low-resource languages), the performance of NMT systems is relatively poor (Koehn and Knowles, 2017; Gu et al., 2018a). Most approaches focus on exploiting additional data to address this problem (G¨ulc¸ehre et al., 2015; Sennrich et al., 2016; He et al., 2016; Fadaee et al., 2017). However, Sennrich and Zhang (2019) show that a well-optimized NMT system can perform relatively well under low-resource data conditions. Unfortunately, their results are confined to a recurrent NMT architecture (Sennrich et al., 2017), and it is not clear to what extent these findings also hold for the nowadays much more commonly used Transformer architecture (Vaswani et al., 2017). Like all NMT models, Transformer requires sett"
2020.coling-main.304,D18-1398,0,0.0157589,"ce conditions is highly dependent on the hyper-parameter settings. Our experiments show that using an optimized Transformer for low-resource conditions improves the translation quality up to 7.3 BLEU points compared to using the Transformer default settings. 1 Introduction Despite the success of Neural Machine Translation (NMT) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015), for the vast majority of language pairs for which only limited amounts of training data exist (a.k.a. low-resource languages), the performance of NMT systems is relatively poor (Koehn and Knowles, 2017; Gu et al., 2018a). Most approaches focus on exploiting additional data to address this problem (G¨ulc¸ehre et al., 2015; Sennrich et al., 2016; He et al., 2016; Fadaee et al., 2017). However, Sennrich and Zhang (2019) show that a well-optimized NMT system can perform relatively well under low-resource data conditions. Unfortunately, their results are confined to a recurrent NMT architecture (Sennrich et al., 2017), and it is not clear to what extent these findings also hold for the nowadays much more commonly used Transformer architecture (Vaswani et al., 2017). Like all NMT models, Transformer requires sett"
2020.coling-main.304,W17-3204,0,0.0219848,"nsformer under low-resource conditions is highly dependent on the hyper-parameter settings. Our experiments show that using an optimized Transformer for low-resource conditions improves the translation quality up to 7.3 BLEU points compared to using the Transformer default settings. 1 Introduction Despite the success of Neural Machine Translation (NMT) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015), for the vast majority of language pairs for which only limited amounts of training data exist (a.k.a. low-resource languages), the performance of NMT systems is relatively poor (Koehn and Knowles, 2017; Gu et al., 2018a). Most approaches focus on exploiting additional data to address this problem (G¨ulc¸ehre et al., 2015; Sennrich et al., 2016; He et al., 2016; Fadaee et al., 2017). However, Sennrich and Zhang (2019) show that a well-optimized NMT system can perform relatively well under low-resource data conditions. Unfortunately, their results are confined to a recurrent NMT architecture (Sennrich et al., 2017), and it is not clear to what extent these findings also hold for the nowadays much more commonly used Transformer architecture (Vaswani et al., 2017). Like all NMT models, Transfor"
2020.coling-main.304,P07-2045,0,0.0124892,"r layerDrop (0 → 0.3) 8 + target word dropout (0 → 0.1) 9 + activation dropout (0 → 0.3) 8.8 9.2 10.6 10.9 11.3 12.9 13.7 14.3 12.0 12.7 17.0 16.9 16.5 17.3 18.1 18.3 16.7 19.0 21.9 21.9 22.0 22.5 23.1 23.6 22.3 23.6 26.7 26.0 26.9 26.9 27.0 27.4 27.7 28.7 31.0 30.2 30.4 30.3 30.7 30.4 31.7 32.3 33.4 33.0 33.3 33.1 33.0 32.6 Table 2: Results of Transformer optimized on the 5k dataset for different subsets and full corpus of IWSLT14 German → English. Averages over three runs from three different samples are reported. ization, tokenization, data cleaning, and truecasing using the Moses scripts (Koehn et al., 2007). We also limit the sentence length to a maximum of 175 tokens during training. Our pre-processing pipeline results in 165,667 sentence pairs for training and 1,938 sentence pairs for development. In order to create smaller training sets, we randomly sample 5k, 10k, 20k, 40k, and 80k sentence pairs from the training data. Similar to Sennrich and Zhang (2019), we use the concatenation of the IWLST 2014 dev sets (tst2010–2012, dev2010, dev2012) as our test set, which consists of 6,750 sentence pairs. For actual low-resource languages, we evaluate our optimized systems on the original test sets o"
2020.coling-main.304,N19-4009,0,0.131987,"catenation of the IWLST 2014 dev sets (tst2010–2012, dev2010, dev2012) as our test set, which consists of 6,750 sentence pairs. For actual low-resource languages, we evaluate our optimized systems on the original test sets of Belarusian (Be), Galician (Gl), and Slovak (Sk) TED talks (Qi et al., 2018) and also Slovenian (Sl) from IWSLT2014 (Cettolo et al., 2012) with training sets ranging from 4.5k to 55k sentence pairs. We use Transformer-base and Transformer-big as our baselines, with the hyper-parameters and optimizer settings described in (Vaswani et al., 2017). We use the Fairseq library (Ott et al., 2019) for our experiments and sacreBLEU (Post, 2018) as evaluation metric. 3.2 Results and discussions BPE effect. To evaluate the effect of different degrees of BPE segmentation on performance, we consider merge operations ranging from 1k to 30k, training BPE on the full training corpus instead of subsets and also removing infrequent subword units when applying the BPE model. In contrast to earlier results for an RNN model, we observe that discarding infrequent subword units under extreme low-resource conditions is detrimental to the performance of Transformer. Sennrich and Zhang (2019) report tha"
2020.coling-main.304,W18-6319,0,0.0166646,"dev2010, dev2012) as our test set, which consists of 6,750 sentence pairs. For actual low-resource languages, we evaluate our optimized systems on the original test sets of Belarusian (Be), Galician (Gl), and Slovak (Sk) TED talks (Qi et al., 2018) and also Slovenian (Sl) from IWSLT2014 (Cettolo et al., 2012) with training sets ranging from 4.5k to 55k sentence pairs. We use Transformer-base and Transformer-big as our baselines, with the hyper-parameters and optimizer settings described in (Vaswani et al., 2017). We use the Fairseq library (Ott et al., 2019) for our experiments and sacreBLEU (Post, 2018) as evaluation metric. 3.2 Results and discussions BPE effect. To evaluate the effect of different degrees of BPE segmentation on performance, we consider merge operations ranging from 1k to 30k, training BPE on the full training corpus instead of subsets and also removing infrequent subword units when applying the BPE model. In contrast to earlier results for an RNN model, we observe that discarding infrequent subword units under extreme low-resource conditions is detrimental to the performance of Transformer. Sennrich and Zhang (2019) report that reducing BPE merge operations from 30k to 5k"
2020.coling-main.304,N18-2084,0,0.0203818,"s during training. Our pre-processing pipeline results in 165,667 sentence pairs for training and 1,938 sentence pairs for development. In order to create smaller training sets, we randomly sample 5k, 10k, 20k, 40k, and 80k sentence pairs from the training data. Similar to Sennrich and Zhang (2019), we use the concatenation of the IWLST 2014 dev sets (tst2010–2012, dev2010, dev2012) as our test set, which consists of 6,750 sentence pairs. For actual low-resource languages, we evaluate our optimized systems on the original test sets of Belarusian (Be), Galician (Gl), and Slovak (Sk) TED talks (Qi et al., 2018) and also Slovenian (Sl) from IWSLT2014 (Cettolo et al., 2012) with training sets ranging from 4.5k to 55k sentence pairs. We use Transformer-base and Transformer-big as our baselines, with the hyper-parameters and optimizer settings described in (Vaswani et al., 2017). We use the Fairseq library (Ott et al., 2019) for our experiments and sacreBLEU (Post, 2018) as evaluation metric. 3.2 Results and discussions BPE effect. To evaluate the effect of different degrees of BPE segmentation on performance, we consider merge operations ranging from 1k to 30k, training BPE on the full training corpus"
2020.coling-main.304,J82-2005,0,0.555665,"Missing"
2020.coling-main.304,P16-1009,0,0.13995,"er for low-resource conditions improves the translation quality up to 7.3 BLEU points compared to using the Transformer default settings. 1 Introduction Despite the success of Neural Machine Translation (NMT) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015), for the vast majority of language pairs for which only limited amounts of training data exist (a.k.a. low-resource languages), the performance of NMT systems is relatively poor (Koehn and Knowles, 2017; Gu et al., 2018a). Most approaches focus on exploiting additional data to address this problem (G¨ulc¸ehre et al., 2015; Sennrich et al., 2016; He et al., 2016; Fadaee et al., 2017). However, Sennrich and Zhang (2019) show that a well-optimized NMT system can perform relatively well under low-resource data conditions. Unfortunately, their results are confined to a recurrent NMT architecture (Sennrich et al., 2017), and it is not clear to what extent these findings also hold for the nowadays much more commonly used Transformer architecture (Vaswani et al., 2017). Like all NMT models, Transformer requires setting various hyper-parameters but researchers often stick to the default parameters, even when their data conditions differ subs"
2020.coling-main.304,E17-3017,0,0.0245738,"Missing"
2020.coling-main.304,P19-1176,0,0.0216878,"10000 0.01, 0.001, 0.0001, 0.00001 Table 1: Order in which different hyper-parameters are explored and the corresponding values considered for each hyper-parameter. Underlined values indicate the default value. can result in substantial improvements of up to 5 BLEU points for a recurrent NMT model. It is natural to assume that reducing the BPE vocabulary is similarly effective for Transfomer. Architecture tuning. A current observation in neural networks, and in particular in Transformer architectures, is that increasing the number of model parameters improves performance (Raffel et al., 2019; Wang et al., 2019). However, those findings are mostly obtained for scenarios with ample training data and it is not clear if they are directly applicable to low-resource conditions. While Biljon et al. (2020) show that using fewer Transformer layers improves the quality of low-resource NMT, we expand our exploration towards the effects of using a narrow and shallow Transformer by reducing i) the number of layers in both the encoder and decoder, ii) the number of attention heads, iii) feed-forward layer dimension (dff ), and iv) embedding dimensions (dmodel ). Regularization. Following Sennrich and Zhang (2019)"
2020.ngt-1.10,W18-2709,0,0.0179669,"ome extent expected that the performance of NMT models that are trained on predominantly clean but tested on noisy data deteriorates, other changes are more unexpected. Introduction The performance of Neural Machine Translation (NMT) models has dramatically improved in recent years, and with sufficient and clean data these models outperform more traditional models. Challenges when sufficient data is not available include translations of rare words (Pham et al., 2018) and idiomatic phrases (Fadaee et al., 2018) as well as domain mismatches between training and testing (Koehn and Knowles, 2017; Khayrallah and Koehn, 2018). Recently, several approaches investigated NMT models when encountering noisy input and how worst-case examples of noisy input can ‘break’ state-of-the-art NMT models (Goodfellow et al., 2015; Michel and Neubig, 2018). Belinkov and Bisk (2018) show that character-level noise in the input leads to poor translation performance. Lee et al. (2018) randomly insert words in different positions in the source sentence and observe that in In this paper, we explore unexpected and erroneous changes in the output of NMT models. Consider the simple example in Table 1 where the Transformer model (Vaswani e"
2020.ngt-1.10,P17-4012,0,0.027536,"Tm φ], das Team motiviert zu halten. Es ist ein anstrengendes Pensum, aber die Dorfmusiker helfen [normalerweiseφ φ] help keep the team motivated. It’s a backbreaking pace, but village musicians [usuallyφ It’s a demanding child, but the village musicians usually help keep the team motivated. It is a hard-to-use, but the village musician helps to keep the team motivated. ∆T ranslation: [word form] [other] ∆Quality: Yes ter size is set to 2048. The multi-head attention has 8 attention heads. We use Adam (Kingma and Ba, 2015) for optimization. All parameters are set based on the suggestions in Klein et al. (2017) to replicate the results of the original paper. multi-bleu.perl. RNN As the first NMT system, we use a 2-layer bidirectional attention-based LSTM model implemented in OpenNMT (Klein et al., 2017) trained with an embedding size of 512, hidden dimension size of 1024, and batch size of 64 sentences. We use Adam (Kingma and Ba, 2015) for optimization. 3 Evaluation of unexpected and erroneous changes The modifications described above generate sentences that are extremely similar and hence are expected to have a very similar difficulty of translation. We evaluate the NMT models on how robust and co"
2020.ngt-1.10,W17-3204,0,0.0208128,"e input. While it is to some extent expected that the performance of NMT models that are trained on predominantly clean but tested on noisy data deteriorates, other changes are more unexpected. Introduction The performance of Neural Machine Translation (NMT) models has dramatically improved in recent years, and with sufficient and clean data these models outperform more traditional models. Challenges when sufficient data is not available include translations of rare words (Pham et al., 2018) and idiomatic phrases (Fadaee et al., 2018) as well as domain mismatches between training and testing (Koehn and Knowles, 2017; Khayrallah and Koehn, 2018). Recently, several approaches investigated NMT models when encountering noisy input and how worst-case examples of noisy input can ‘break’ state-of-the-art NMT models (Goodfellow et al., 2015; Michel and Neubig, 2018). Belinkov and Bisk (2018) show that character-level noise in the input leads to poor translation performance. Lee et al. (2018) randomly insert words in different positions in the source sentence and observe that in In this paper, we explore unexpected and erroneous changes in the output of NMT models. Consider the simple example in Table 1 where the"
2020.ngt-1.10,W18-6459,0,0.109592,"nal models. Challenges when sufficient data is not available include translations of rare words (Pham et al., 2018) and idiomatic phrases (Fadaee et al., 2018) as well as domain mismatches between training and testing (Koehn and Knowles, 2017; Khayrallah and Koehn, 2018). Recently, several approaches investigated NMT models when encountering noisy input and how worst-case examples of noisy input can ‘break’ state-of-the-art NMT models (Goodfellow et al., 2015; Michel and Neubig, 2018). Belinkov and Bisk (2018) show that character-level noise in the input leads to poor translation performance. Lee et al. (2018) randomly insert words in different positions in the source sentence and observe that in In this paper, we explore unexpected and erroneous changes in the output of NMT models. Consider the simple example in Table 1 where the Transformer model (Vaswani et al., 2017) is used to translate very similar sentences. Surprisingly, we observe that by simply altering one word in the source sentence—inserting the German word sehr (English: very)—an unrelated change occurs in the translation. In principle, an NMT model that generates the translation of the word erleichtert (English: relieved) in one cont"
2020.ngt-1.10,W11-2107,0,0.0353865,"Figure 1 shows the results for the RNN and Transformer model. While the majority of sentence variations have minor changes, a substan3.2 Oscillations of Variation in Translations In this section, we look into various sentence-level metrics to further analyze the observed behaviour. In particular, we focus on the SUBNUM modification because with this modification we can generate numerous variations of the same sentence. Having a high number of variations for each sentence gives us the opportunity of observing oscillations of various string matching metrics. We use sentence-level BLEU, METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006), and LengthRatio to quantify changes in the translations. LengthRatio represents the translation length over reference length as a percentage. For a given source sentence, we define the oscillation range as changes in the sentence-level metric for the translations of variations of a given sentence. While sentence-level metrics are not reliable indicators of translation quality, they do capture fluctuations in translations. With the variations we introduce, in theory there should be no fluctuations in the translations. Table 5 and Figure 3 provide the results. We obs"
2020.ngt-1.10,W19-5303,0,0.010561,"tences, we are interested in simple semantic and syntactic modifications. These trivial linguistic variations should have almost no effect on the translation of the rest of the sentence. We define a set of rules to slightly modify the source and target sentences in the test data and keep the sentences syntactically correct and semantically plausible. Sentence Variations Is this another noisy text translation problem? Noisy input text can cause mistranslations in most MT systems, and there has been growing research interest in studying the behaviour of MT systems when encountering noisy input (Li et al., 2019). Belinkov and Bisk (2018) propose to swap or randomize letters in a word in the input sentence. For instance, they change the word noise in the source sentence into iones. Lee et al. (2018) examine how the insertion of a random word in a random position in the source sentence leads to mistranslations. Michel and Neubig (2018) proposes a benchmark dataset for translation of noisy input sentences, consisting of noisy, user-generated comments on Reddit. The types of noisy input text they observe include spelling or typographical errors, word omission/insertion/repetition, and grammatical errors."
2020.ngt-1.10,W19-3821,0,0.0604321,"Missing"
2020.ngt-1.10,L18-1148,1,0.832189,"ntence from WMT 2017. some cases the translations are completely unrelated to the input. While it is to some extent expected that the performance of NMT models that are trained on predominantly clean but tested on noisy data deteriorates, other changes are more unexpected. Introduction The performance of Neural Machine Translation (NMT) models has dramatically improved in recent years, and with sufficient and clean data these models outperform more traditional models. Challenges when sufficient data is not available include translations of rare words (Pham et al., 2018) and idiomatic phrases (Fadaee et al., 2018) as well as domain mismatches between training and testing (Koehn and Knowles, 2017; Khayrallah and Koehn, 2018). Recently, several approaches investigated NMT models when encountering noisy input and how worst-case examples of noisy input can ‘break’ state-of-the-art NMT models (Goodfellow et al., 2015; Michel and Neubig, 2018). Belinkov and Bisk (2018) show that character-level noise in the input leads to poor translation performance. Lee et al. (2018) randomly insert words in different positions in the source sentence and observe that in In this paper, we explore unexpected and erroneous ch"
2020.ngt-1.10,D15-1166,0,0.0612103,"r variations. SUBGEN Finally, a local modification is changing the gender of the person in the sentences. The goal of this modification is to investigate the existence and severity of gender bias in our models. This is inspired by recent approaches that have shown that NMT models learn social stereotypes such as gender bias from training data (Escud´e Font and Costa-juss`a, 2019; Stanovsky et al., 2019). 2.3 Experimental setup In the translation experiments, we use the standard ENØDE WMT-2017 training data (Bojar et al., 2018). We perform NMT experiments with two different architectures: RNN (Luong et al., 2015) and Transformer (Vaswani et al., 2017). We preprocess the training data with Byte-Pair Encoding (BPE) using 32K merge operations (Sennrich et al., 2016). During inference, we use beam search with a beam size of 5. Table 3 shows the case-sensitive BLEU scores as calculated by Note that in a minority of cases these procedures can lead to semantically incorrect sentences, for instance, by substituting numbers we can potentially generate sentences such as “She was 90 Table 4: A random sample of sentences from the WMT test sets and our proposed variations shown with ‘unexpected change’ annotations"
2020.ngt-1.10,D18-1050,0,0.145382,"lation (NMT) models has dramatically improved in recent years, and with sufficient and clean data these models outperform more traditional models. Challenges when sufficient data is not available include translations of rare words (Pham et al., 2018) and idiomatic phrases (Fadaee et al., 2018) as well as domain mismatches between training and testing (Koehn and Knowles, 2017; Khayrallah and Koehn, 2018). Recently, several approaches investigated NMT models when encountering noisy input and how worst-case examples of noisy input can ‘break’ state-of-the-art NMT models (Goodfellow et al., 2015; Michel and Neubig, 2018). Belinkov and Bisk (2018) show that character-level noise in the input leads to poor translation performance. Lee et al. (2018) randomly insert words in different positions in the source sentence and observe that in In this paper, we explore unexpected and erroneous changes in the output of NMT models. Consider the simple example in Table 1 where the Transformer model (Vaswani et al., 2017) is used to translate very similar sentences. Surprisingly, we observe that by simply altering one word in the source sentence—inserting the German word sehr (English: very)—an unrelated change occurs in th"
2020.ngt-1.10,W18-2712,0,0.013447,"translations. ; indicates the original sentence from WMT 2017. some cases the translations are completely unrelated to the input. While it is to some extent expected that the performance of NMT models that are trained on predominantly clean but tested on noisy data deteriorates, other changes are more unexpected. Introduction The performance of Neural Machine Translation (NMT) models has dramatically improved in recent years, and with sufficient and clean data these models outperform more traditional models. Challenges when sufficient data is not available include translations of rare words (Pham et al., 2018) and idiomatic phrases (Fadaee et al., 2018) as well as domain mismatches between training and testing (Koehn and Knowles, 2017; Khayrallah and Koehn, 2018). Recently, several approaches investigated NMT models when encountering noisy input and how worst-case examples of noisy input can ‘break’ state-of-the-art NMT models (Goodfellow et al., 2015; Michel and Neubig, 2018). Belinkov and Bisk (2018) show that character-level noise in the input leads to poor translation performance. Lee et al. (2018) randomly insert words in different positions in the source sentence and observe that in In this p"
2020.ngt-1.10,P16-1162,0,0.0575571,"ate the existence and severity of gender bias in our models. This is inspired by recent approaches that have shown that NMT models learn social stereotypes such as gender bias from training data (Escud´e Font and Costa-juss`a, 2019; Stanovsky et al., 2019). 2.3 Experimental setup In the translation experiments, we use the standard ENØDE WMT-2017 training data (Bojar et al., 2018). We perform NMT experiments with two different architectures: RNN (Luong et al., 2015) and Transformer (Vaswani et al., 2017). We preprocess the training data with Byte-Pair Encoding (BPE) using 32K merge operations (Sennrich et al., 2016). During inference, we use beam search with a beam size of 5. Table 3 shows the case-sensitive BLEU scores as calculated by Note that in a minority of cases these procedures can lead to semantically incorrect sentences, for instance, by substituting numbers we can potentially generate sentences such as “She was 90 Table 4: A random sample of sentences from the WMT test sets and our proposed variations shown with ‘unexpected change’ annotations (∆T ranslation). The cases where the unexpected change leads to a change in translation quality are marked in column ∆Quality. [wi wj ] indicates that"
2020.ngt-1.10,2006.amta-papers.25,0,0.0787713,"he RNN and Transformer model. While the majority of sentence variations have minor changes, a substan3.2 Oscillations of Variation in Translations In this section, we look into various sentence-level metrics to further analyze the observed behaviour. In particular, we focus on the SUBNUM modification because with this modification we can generate numerous variations of the same sentence. Having a high number of variations for each sentence gives us the opportunity of observing oscillations of various string matching metrics. We use sentence-level BLEU, METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006), and LengthRatio to quantify changes in the translations. LengthRatio represents the translation length over reference length as a percentage. For a given source sentence, we define the oscillation range as changes in the sentence-level metric for the translations of variations of a given sentence. While sentence-level metrics are not reliable indicators of translation quality, they do capture fluctuations in translations. With the variations we introduce, in theory there should be no fluctuations in the translations. Table 5 and Figure 3 provide the results. We observe that even though these"
2020.ngt-1.10,P19-1164,0,0.0162479,"ation DEØEN. We generate 10k sentence variations by applying these modifications to all sentence pairs in WMT test sets 2013–2018 (Bojar et al., 2018). We use RNN and Transformer models to translate sentences and their variations. SUBGEN Finally, a local modification is changing the gender of the person in the sentences. The goal of this modification is to investigate the existence and severity of gender bias in our models. This is inspired by recent approaches that have shown that NMT models learn social stereotypes such as gender bias from training data (Escud´e Font and Costa-juss`a, 2019; Stanovsky et al., 2019). 2.3 Experimental setup In the translation experiments, we use the standard ENØDE WMT-2017 training data (Bojar et al., 2018). We perform NMT experiments with two different architectures: RNN (Luong et al., 2015) and Transformer (Vaswani et al., 2017). We preprocess the training data with Byte-Pair Encoding (BPE) using 32K merge operations (Sennrich et al., 2016). During inference, we use beam search with a beam size of 5. Table 3 shows the case-sensitive BLEU scores as calculated by Note that in a minority of cases these procedures can lead to semantically incorrect sentences, for instance,"
2020.wmt-1.1,2020.nlpcovid19-2.5,1,0.796341,"tails of the evaluation. 4.1.1 Covid Test Suite TICO-19 The TICO-19 test suite was developed to evaluate how well can MT systems handle the newlyemerged topic of COVID-19. Accurate automatic translation can play an important role in facilitating communication in order to protect at-risk populations and combat the infodemic of misinformation, as described by the World Health Organization. The test suite has no corresponding paper so its authors provided an analysis of the outcomes directly here. The submitted systems were evaluated using the test set from the recently-released TICO-19 dataset (Anastasopoulos et al., 2020). The dataset provides manually created translations of COVID19 related data. The test set consists of PubMed articles (678 sentences from 5 scientific articles), patient-medical professional conversations (104 sentences), as well as related Wikipedia articles (411 sentences), announcements (98 sentences from Wikisource), and news items (67 sentences from Wikinews), for a total of 2100 sentences. Table 15 outlines the BLEU scores by each submitted system in the English-to-X directions, also breaking down the results per domain. The analysis shows that some systems are significantly more prepar"
2020.wmt-1.1,2020.wmt-1.6,0,0.0647231,"AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua Universit"
2020.wmt-1.1,2020.wmt-1.38,0,0.0746945,"Missing"
2020.wmt-1.1,2020.wmt-1.54,1,0.802974,"Missing"
2020.wmt-1.1,W07-0718,1,0.671054,"Missing"
2020.wmt-1.1,W08-0309,1,0.762341,"Missing"
2020.wmt-1.1,W12-3102,1,0.500805,"Missing"
2020.wmt-1.1,2020.lrec-1.461,0,0.0795779,"Missing"
2020.wmt-1.1,2012.eamt-1.60,0,0.124643,"tted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, English and German, with both the document and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained the document boundaries and text ordering of the originals. Training, development, and test data for Pashto↔English and Khmer↔English are shared with the Parallel Corpus Filtering Shared Task (Koehn et al., 2020). The training data mostly comes from OPUS"
2020.wmt-1.1,2020.wmt-1.3,0,0.0731913,"on Machine Translation (WMT20)1 was held online with EMNLP 2020 and hosted a number of shared tasks on various aspects of machine translation. This conference built on 14 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; CallisonBurch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018; Barrault et al., 2019). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • automatic post-editing (Chatterjee et al., 2020) • biomedical translation (Bawden et al., 2020b) • chat translation (Farajian et al., 2020) • lifelong learning (Barrault et al., 2020) 1 Makoto Morishita NTT Santanu Pal WIPRO AI Abstract 1 Philipp Koehn JHU http://www.statmt.org/wmt20/ 1 Proceedings of the 5th Conference on Machine Translation (WMT), pages 1–55 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics as “direct assessment”) that we explored in the previous years with convincing results in terms of the trade-off between annotation effort and reliable distinctions between systems. The primary objectives o"
2020.wmt-1.1,2020.wmt-1.8,0,0.0898111,"D D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Know"
2020.wmt-1.1,2009.freeopmt-1.3,0,0.088081,"ve (CONTRASTIVE) or primary (PRIMARY), and the BLEU, RIBES and TER results. The scores are sorted by BLEU. In general, primary systems tend to be better than contrastive systems, as expected, but there are some exceptions. This year we recived major number of participants for the case of Indo-Aryan language group NUST-FJWU NUST-FJWU system is an extension of state-of-the-art Transformer model with hierarchical attention networks to incorporate contextual information. During training the model used back-translation. Prompsit This team is participating with a rulebased system based on Apertium (Forcada et al., 2009-11). Apertium is a free/open-source platform for developing rule-based machine translation systems and language technology that was first released in 2005. Apertium is hosted in Github where both language data and code are licensed under the GNU GPL. It is a research and business platform with a very active community that loves small languages. Language pairs are at a very different level of development and output quality in the platform, depending on two main variables: how much funded or in-kind effort has 32 5.4 i.e. Hindi–Marathi (in both directions). We received 22 submissions from 14 te"
2020.wmt-1.1,2020.wmt-1.80,0,0.0933589,"airs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inuktitut, Japanese, Polish and Tamil being new for this year. Furthermore, English to and from Khmer and Pashto were included, using the same test sets as in the corpus filtering task. Th"
2020.wmt-1.1,W19-5204,0,0.0543621,"Missing"
2020.wmt-1.1,2020.emnlp-main.5,0,0.0410594,"luation of out-ofEnglish translations, HITs were generated using the same method as described for the SR+DC evaluation of into-English translations in Section 3.2.1 with minor modifications. Source-based DA allows to include human references in the evaluation as another system to provide an estimate of human performance. Human references were added to the pull of system outputs prior to sampling documents for tasks generation. If multiple references are available, which is the case for English→German (3 alternative reference translations, including 1 generated using the paraphrasing method of Freitag et al. (2020)) and English→Chinese (2 translations), each reference is assessed individually. Since the annotations are made by researchers and professional translators who ensure a betTable 11: Amount of data collected in the WMT20 manual document- and segment-level evaluation campaigns for bilingual/source-based evaluation out of English and nonEnglish pairs. et al., 2020; Laubli et al., 2020). It differs from SR+DC DA introduced in WMT19 (Bojar et al., 2019), and still used in into-English human evaluation this year, where a single segment from a document is provided on a screen at a time, followed by s"
2020.wmt-1.1,W18-3931,1,0.874637,"ese improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate the performance of state-of-the-art translation systems on trans"
2020.wmt-1.1,2020.wmt-1.18,0,0.0913945,"2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2020) Université du Québec à Montréal (no associated paper) ByteDance AI Lab (Wu et al., 2020a) WeChat (Meng et al., 2020) Baseline System from Biomedical Task (Bawden et al., 2020b) American University of Beirut (no associated paper) Zoho Corporation (no associated paper) Table 6: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion"
2020.wmt-1.1,2020.wmt-1.43,0,0.0835067,"Missing"
2020.wmt-1.1,2020.wmt-1.9,0,0.0939415,"Missing"
2020.wmt-1.1,2020.wmt-1.19,0,0.0674131,"Missing"
2020.wmt-1.1,2009.mtsummit-btm.6,0,0.103443,"Missing"
2020.wmt-1.1,W13-2305,1,0.929934,"work which can be well applied to different translation. directions. Techniques used in the submitted systems include optional multilingual pre-training (mRASP) for low resource languages, very deep Transformer or dynamic convolution models up to 50 encoder layers, iterative backtranslation, knowledge distillation, model ensemble and development set fine-tuning. The key ingredient of the process seems the strong focus on diversification of the (synthetic) training data, using multiple scalings of the Transformer model 3.1 Direct Assessment Since running a comparison of direct assessments (DA, Graham et al., 2013, 2014, 2016) and relative ranking in 2016 (Bojar et al., 2016) and verifying a high correlation of system rankings for the two methods, as well as the advantages of DA, such as quality controlled crowd-sourcing and linear growth relative to numbers of submissions, we have employed DA as the primary mechanism for evaluating systems. With DA human evaluation, 15 human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation or source language input on an analogue scale, which corresponds to an underlying absolute 0–100"
2020.wmt-1.1,E14-1047,1,0.888167,"Missing"
2020.wmt-1.1,2020.lrec-1.312,1,0.804196,"A screenshot of OCELoT is shown in Figure 5. For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human evaluations. In the rest of this section, we provide brief details of the submitted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, Engli"
2020.wmt-1.1,2020.emnlp-main.6,1,0.839606,"shown in Table 3, where the first and second are simple merges or splits, whereas the third is a rare case of more complex reordering. We leave a detailed analysis of the translators’ treatment of paragraph-split data for future work. development set is provided, it is a mixture of both “source-original” and “target-original” texts, in order to maximise its size, although the original language is always marked in the sgm file, except for Inuktitut↔English. The consequences of directionality in test sets has been discussed recently in the literature (Freitag et al., 2019; Laubli et al., 2020; Graham et al., 2020), and the conclusion is that it can have an effect on detrimental effect on the accuracy of system evaluation. We use “source-original” parallel sentences wherever possible, on the basis that it is the more realistic scenario for practical MT usage. Exception: the test sets for the two Inuktitut↔English translation directions contain the same data, without regard to original direction. For most news text in the test and development sets, English was the original language and Inuktitut the translation, while the parliamentary data mixes the two directions. The origins of the news test documents"
2020.wmt-1.1,2020.wmt-1.11,0,0.0940191,"N GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) S"
2020.wmt-1.1,D19-1632,1,0.881933,"ent and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained the document boundaries and text ordering of the originals. Training, development, and test data for Pashto↔English and Khmer↔English are shared with the Parallel Corpus Filtering Shared Task (Koehn et al., 2020). The training data mostly comes from OPUS (software localization, Tatoeba, Global Voices), the Bible, and specialprepared corpora from TED Talks and the Jehova Witness web site (JW300). The development and test sets were created as part of the Flores initiative (Guzmán et al., 2019) by professional translation of Wikipedia content with careful vetting of the translations. Please refer the to the Parallel Corpus Filtering Shared Task overview paper for details on these corpora. 2.3.1 AFRL (Gwinnup and Anderson, 2020) AFRL - SYSCOMB 20 is a system combination consisting of two Marian transformer ensembles, one OpenNMT transformer system and a Moses phrase-based system. AFRL - FINETUNE is an OpenNMT transformer system fine-tuned on newstest2014-2017. 2.3.2 (Xv, 2020) ARIEL XV is a Transformer base model trained with the Sockeye sequence modeling toolkit usSome statistics ab"
2020.wmt-1.1,2020.wmt-1.12,1,0.754946,"Missing"
2020.wmt-1.1,2020.wmt-1.13,0,0.0737827,"2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2020) Université du Québec à Montréal (no associated paper) ByteDance AI Lab (Wu et al., 2020a) WeChat (Meng et al"
2020.wmt-1.1,2020.wmt-1.20,0,0.057602,"Missing"
2020.wmt-1.1,2020.wmt-1.14,1,0.820019,"set, and the origlang tag indicates the original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Mari"
2020.wmt-1.1,2020.wmt-1.39,1,0.812433,"kables was collected in the first phase of the annotation, which amounted to 4k assessments across the systems. The second annotation phase with 6.5k assessments compared markable translations, always checking outputs of all the 13 competing MT systems but still considering the document-level context of each of them. Among other things, the observations indicate that the better the system, the lower the variance in manual scores. Markables annotation then confirms that frequent errors like bad translation of a term need not be the most severe and conversely, 4.1.3 Gender Coreference and Bias (Kocmi et al., 2020) The test suite by Kocmi et al. (2020) focuses on the gender bias in professions (e.g. physician, teacher, secretary) for the translation from English into Czech, German, Polish and Russian. These nouns are ambiguous with respect to gender in English but exhibit gender in the examined target languages. The test suite is based on the fact that a pronoun referring to the ambiguous noun can reveal the gender of the noun in the English source sentence. Once disambiguated, the gender needs to be preserved in translation. To correctly translate the given noun, the translation system thus has to corr"
2020.wmt-1.1,2020.wmt-1.53,0,0.089538,"Missing"
2020.wmt-1.1,2020.wmt-1.78,1,0.815194,"rence on Machine Translation (WMT) 2020. In the news task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inuktitut, Japanese, Polish and Tamil being new fo"
2020.wmt-1.1,W17-1208,0,0.0524248,"ttribute all these improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate the performance of state-of-the-art tr"
2020.wmt-1.1,2020.wmt-1.21,0,0.0791885,"Missing"
2020.wmt-1.1,2020.wmt-1.23,0,0.0607352,"020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2"
2020.wmt-1.1,2020.wmt-1.77,1,0.84299,"slation task, both organised alongside the Conference on Machine Translation (WMT) 2020. In the news task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inu"
2020.wmt-1.1,2020.wmt-1.24,0,0.0435945,"Missing"
2020.wmt-1.1,D18-1512,0,0.05415,"Missing"
2020.wmt-1.1,2020.wmt-1.47,1,0.740067,"Missing"
2020.wmt-1.1,W18-3601,1,0.891679,", we have employed DA as the primary mechanism for evaluating systems. With DA human evaluation, 15 human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation or source language input on an analogue scale, which corresponds to an underlying absolute 0–100 rating scale.5 No sentence or document length restriction is applied during manual evaluation. Direct Assessment is also employed for evaluation of video captioning systems at TRECvid (Graham et al., 2018; Awad et al., 2019) and multilingual surface realisation (Mille et al., 2018, 2019). 3.1.1 tion 2, most of our test sets do not include reversecreated sentence pairs, except when there were resource constraints on the creation of the test sets. 3.1.3 Prior to WMT19, the issue of including document context was raised within the community (Läubli et al., 2018; Toral et al., 2018) and at WMT19 a range of DA styles were subsequently tested that included document context. In WMT19, two options were run, firstly, an evaluation that included the document context “+DC” (with document context), and secondly, a variation that omitted document context “−DC” (without document con"
2020.wmt-1.1,2020.wmt-1.27,0,0.247738,"he original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans"
2020.wmt-1.1,D19-6301,1,0.888512,"Missing"
2020.wmt-1.1,W18-6424,0,0.0431841,"(Kocmi, 2020) combines transfer learning from a high-resource language pair Czech–English into the low-resource Inuktitut-English with an additional backtranslation step. Surprising behaviour is noticed when using synthetic data, which can be possibly attributed to a narrow domain of training and test data. The system is the Transformer model in a constrained submission. 2.3.3 Charles University (CUNI) CUNI-D OC T RANSFORMER (Popel, 2020) is similar to the sentence-level version (CUNI-T2T2018, CUBBITT), but trained on sequences with multiple sentences of up to 3000 characters. CUNI-T2T-2018 (Popel, 2018), also called CUBBITT, is exactly the same system as in WMT2018. It is the Transformer model trained according to Popel and Bojar (2018) plus a novel concat-regime backtranslation with checkpoint averaging (Popel et al., 2020), tuned separately for CZ-domain and non CZ-domain articles, possibly handling also translation-direction (“translationese”) issues. For cs→en also a coreference preprocessing was used adding the female-gender CUNI-T RANSFORMER (Popel, 2020) is similar to the WMT2018 version of CUBBITT, but with 12 encoder layers instead of 6 and trained on CzEng 2.0 instead of CzEng 1.7."
2020.wmt-1.1,2020.wmt-1.25,0,0.094349,"Missing"
2020.wmt-1.1,2020.wmt-1.28,0,0.0792624,"cument in the test set, and the origlang tag indicates the original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 20"
2020.wmt-1.1,2020.lrec-1.443,1,0.79707,"er their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human evaluations. In the rest of this section, we provide brief details of the submitted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, English and German, with both the document and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained t"
2020.wmt-1.1,2020.wmt-1.48,0,0.090424,"Missing"
2020.wmt-1.1,2020.wmt-1.49,0,0.0519886,"Missing"
2020.wmt-1.1,W19-6712,0,0.0573476,"tly crawled multilingual parallel corpora from Indian government websites (Haddow and Kirefu, 2020; Siripragada et al., 2020), the Tanzil corpus (Tiedemann, 2009), the Pavlick dicParagraph-split Test Sets For the language pairs English↔Czech, English↔German and English→Chinese, we provided the translators with paragraph-split texts, instead of sentence-split texts. We did this in order to provide the translators with greater freedom and, hopefully, to improve the quality of the translation. Allowing translators to merge and split sentences removes one of the “translation shifts” identified by Popovic (2019), which can make translations create solely for MT evaluation different from translations produced for other purposes. We first show some descriptive statistics of the source texts, for Czech, English and German, in 3 Europarl Parallel Corpus Czech ↔ English German ↔ English Polish↔ English German ↔ French Sentences 645,241 1,825,745 632,435 1,801,076 Words 14,948,900 17,380,340 48,125,573 50,506,059 14,691,199 16,995,232 47,517,102 55,366,136 Distinct words 172,452 63,289 371,748 113,960 170,271 62,694 368,585 134,762 News Commentary Parallel Corpus Czech ↔ English 248,927 5,570,734 6,156,063"
2020.wmt-1.1,2020.wmt-1.26,0,0.0845151,"Missing"
2020.wmt-1.1,2020.vardial-1.10,0,0.0933804,"Missing"
2020.wmt-1.1,W18-6301,0,0.038239,"Missing"
2020.wmt-1.1,2020.wmt-1.51,0,0.0917045,"Missing"
2020.wmt-1.1,2020.wmt-1.50,1,0.78567,"Missing"
2020.wmt-1.1,2020.wmt-1.52,0,0.0485419,"Missing"
2020.wmt-1.1,P19-1164,0,0.0581517,"26 26.37 25.51 24.82 28.33 23.33 21.13 21.96 20.43 22.90 22.58 21.90 22.17 22.17 20.53 19.40 20.01 40.44 32.39 30.39 37.04 32.27 27.54 25.97 26.09 46.38 37.30 36.05 35.96 33.76 33.07 27.20 27.07 Table 15: TICO-19 test suite results on the English-to-X WMT20 translation directions. 26 4.1.5 antecedent (a less common direction of information flow), and then correctly express the noun in the target language. The success of the MT system in this test can be established automatically, whenever the gender of the target word can be automatically identified. Kocmi et al. (2020) build upon the WinoMT (Stanovsky et al., 2019) test set, which provides exactly the necessary type of sentences containing an ambiguous profession noun and a personal pronoun which unambiguously (for the human eye) refers to it based the situation described. When extending WinMT with Czech and Polish, Stanovsky et al. have to disregard some test patterns but the principle remains. The results indicate that all MT systems fail in this test, following gender bias (stereotypical patterns attributing the masculine gender to some professions and feminine gender to others) rather than the coreference link. Word Sense Disambiguation (Scherrer et"
2020.wmt-1.1,2020.wmt-1.31,0,0.0881792,"morphological segmentation of the polysynthetic Inuktitut, testing rule-based, supervised, semi-supervised as well as unsupervised word segmentation methods, (2) whether or not adding data from a related language (Greenlandic) helps, and (3) whether contextual word embeddings (XLM) improve translation. G RONINGEN - ENIU use Transformer implemented in Marian with the default setting, improving the performance also with tagged backtranslation, domain-specific data, ensembling and finetuning. 2.3.7 DONG - NMT (no associated paper) No description provided. 2.3.8 ENMT (Kim et al., 2020) Kim et al. (2020) base their approach on transferring knowledge of domain and linguistic characteristics by pre-training the encoder-decoder model with large amount of in-domain monolingual data through unsupervised and supervised prediction task. The model is then fine-tuned with parallel data and in-domain synthetic data, generated with iterative back-translation. For additional gain, final results are generated with an ensemble model and re-ranked with averaged models and language models. G RONINGEN - ENTAM (Dhar et al., 2020) study the effects of various techniques such as linguistically motivated segmenta"
2020.wmt-1.1,2020.wmt-1.32,0,0.0839317,"- NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ub"
2020.wmt-1.1,2020.wmt-1.33,0,0.080166,"Missing"
2020.wmt-1.1,2020.wmt-1.34,0,0.0803867,"Missing"
2020.wmt-1.1,2020.wmt-1.35,0,0.0951745,"ss web site (JW300). The development and test sets were created as part of the Flores initiative (Guzmán et al., 2019) by professional translation of Wikipedia content with careful vetting of the translations. Please refer the to the Parallel Corpus Filtering Shared Task overview paper for details on these corpora. 2.3.1 AFRL (Gwinnup and Anderson, 2020) AFRL - SYSCOMB 20 is a system combination consisting of two Marian transformer ensembles, one OpenNMT transformer system and a Moses phrase-based system. AFRL - FINETUNE is an OpenNMT transformer system fine-tuned on newstest2014-2017. 2.3.2 (Xv, 2020) ARIEL XV is a Transformer base model trained with the Sockeye sequence modeling toolkit usSome statistics about the training and test materials are given in Figures 1, 2, 3 and 4. 4 8 ARIEL XV https://github.com/AppraiseDev/OCELoT English I English II English III Chinese Czech German Inuktitut Japanese Polish Russian Tamil ABC News (2), All Africa (5), Brisbane Times (1), CBS LA (1), CBS News (1), CNBC (3), CNN (2), Daily Express (1), Daily Mail (2), Fox News (1), Gateway (1), Guardian (3), Huffington Post (2), London Evening Standard (2), Metro (2), NDTV (7), RTE (7), Reuters (4), STV (2), S"
2020.wmt-1.1,2020.wmt-1.55,0,0.0877526,"Missing"
2020.wmt-1.1,P17-4012,0,0.0273892,"o SJTU-NICT using large XLM model to improve NMT but the exact relation is unclear. 2.3.14 H UAWEI TSC (Wei et al., 2020a) H UAWEI TSC use Transformer-big with a further increased model size, focussing on standard techniques of careful pre-processing and filtering, back-translation and forward translation, including self-training, i.e. translating one of the sides of the original parallel data. Ensembling of individual training runs is used in the forward as well as backward translation, and single models are created from the ensembles using knowledge distillation. The submission uses THUNMT (Zhang et al., 2017) open-source engine. 2.3.19 N IU T RANS (Zhang et al., 2020) N IU T RANS gain their performance from focussed attention to six areas: (1) careful data preprocessing and filtering, (2) iterative back-translation to generate additional training data, (3) using different model architectures, such as wider and/or deeper models, relative position representation and relative length, to enhance the diversity of translations, (4) iterative knowledge distillation by in-domain monolingual data, (5) iterative finetuning for domain adaptation using small training batches, (6) rule-based post-processing of"
2020.wmt-1.1,P98-2238,0,0.590812,"and punctuation, and we tend to attribute all these improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate th"
2020.wmt-1.1,2020.wmt-1.41,1,0.814291,"Missing"
2021.eacl-main.106,N19-1246,0,0.0396725,"Missing"
2021.eacl-main.106,P19-1346,0,0.0279988,"ction 3.2 discusses our question types in detail. In most existing QA data sets such as SQuAD, crowd-workers generate questions based on provided short passages and extract answers from the passages (Rajpurkar et al., 2016). This method of question generation can make QA samples trivial because models can simply detect the most related span to the question by guessing based on shallow pattern matching (Koˇcisk´y et al., 2018). In contrast, all annotations in NLQuAD are done automatically and directly based on the news articles themselves. NLQuAD, unlike MS MARCO (Bajaj et al., 2016) and ELI5 (Fan et al., 2019), does not use information retrieval (IR) methods to collect supporting documents. Retrieved documents in these data sets are not guaranteed to contain all facts required to answer the question or they occasionally just contain information related to the question but no answers. NLQuAD requires document-level language understanding. With an average document length and answer length of 877 and 175 words, respectively, it exceeds the maximum input length of the state of the art QA models such as BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) due to their memory and computational requi"
2021.eacl-main.106,W18-2605,0,0.0364058,"Missing"
2021.eacl-main.106,D19-1243,0,0.0353733,"Missing"
2021.eacl-main.106,Q18-1023,0,0.0476647,"Missing"
2021.eacl-main.106,Q19-1026,0,0.080284,"Missing"
2021.eacl-main.106,D17-1082,0,0.0234733,"screte reasoning over the text. SQuAD and DROP use Wikipedia pages as context passages whereas SearchQA (Dunn et al., 2017) uses IR approaches to collect context passages. Answer generation based on a set of passages is another approach to address this task. MS MARCO (Bajaj et al., 2016) consists of real-world search queries and retrieved documents corresponding to the queries. There are also different types of QA data sets such as Antique (Hashemi et al., 2020), which is a data set for answer retrieval for non-factoid questions. There is also a range of multiple-choice QA tasks such as RACE (Lai et al., 2017), ARC (Clark et al., 2018), SWAQ (Zellers et al., 2018), and COSMOS QA (Huang et al., 2019) that are clustered together with the short-context QA data sets. 2.2 Long-Context Question Answering Factoid QA has been applied to longer documents, however, the nature of factoid questions limits answers to short texts. NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al., 2017), NarrativeQA (Koˇcisk´y et al., 2018), and DuoRC (Saha et al., 2018) fall into this category and their documents are extracted from news articles, stories, and movie plots, respectively. On the other hand, DQA (ter Hoeve et"
2021.eacl-main.106,P04-1077,0,0.561134,"rs (yes/no, entities) as well as long answers (bounding boxes with the information to infer the answer). However, due to the nature of factoid questions, the majority of long answers are sections containing exactly the short answer or simple facts. ELI5 (Fan et al., 2019) consists of real-world questions with answers provided by the Reddit community. The task is to generate answers given a set of documents retrieved from the Web. However, the documents are not guaranteed to completely address the questions. Furthermore, evaluation metrics for sequence generation tasks such as the ROUGE score (Lin and Och, 2004) are far from perfect to assess the quality of generated answers. Table 1 compares existing long-context question answering data sets along with SQuAD and MS MARCO. We report the average length for data sets with different types of answers. 3 Data Set Design NLQuAD consists of news articles as context documents, interrogative sub-headings in the articles as questions, and body paragraphs corresponding to the sub-headings as contiguous answers to the questions. We automatically extract target answers because annotating for non-factoid long QA is rather challenging and costly. To ensure the qual"
2021.eacl-main.106,2021.ccl-1.108,0,0.0567004,"Missing"
2021.eacl-main.106,D18-1206,0,0.02702,"nt of high-quality question-like sub-headings which are suitable for the QA task. NLQuAD’s characteristics make it an appealing and challenging data set for the non-factoid long QA task: Its context documents are long, and its questions are non-factoid in a way that cannot be answered by single or multiple entities. The questions are addressed by more than seven sentences on average. Meanwhile, it covers a wide range of topics, making it an open-domain QA data set. The BBC news articles typically follow a specific template. They begin with an introductory section consisting of news summaries (Narayan et al., 2018) and one or more sections accompanied by sub-headings. Each section contains multiple short to medium-length paragraphs. We remove the template and section break-lines to prevent revealing possible answer boundaries. 3.1 Data Curation We exploit Wayback Machine,2 a digital archive of the Web, and Wayback Machine Scraper3 to scrape the article archives. Links in the scraped pages are used to collect additional pages from the original website. We scraped the English BBC news website from 2016 to 2020 as a limited number of questions can be found in articles before 2016. Only textual information"
2021.eacl-main.106,D16-1264,0,0.530114,"d not many people are willing to go out.” (141 words). Vietnam and Singapore were on Thursday added to the nations recording confirmed cases, joining Thailand, the US, Taiwan and South Korea. [...] Taiwan has banned people arriving from Wuhan and the US state department warned American travellers to exercise increased caution in China. (document length: 921 words) Introduction Over the last few years, there have been remarkable improvements in the area of Machine Reading Comprehension (MRC) and open-domain Question Answering (QA) due to the availability of large scale data sets such as SQuAD (Rajpurkar et al., 2016) and pre-trained language models such as BERT (Devlin et al., 2018). Although non-factoid questions represent a large number of real-life questions, current QA data sets barely cover this area. The reason is that context passages in existing QA data sets are mostly very short and questions mostly factoid, i.e., can be answered by simple facts or entities such as a person name and location (Jurafsky and Martin, 2019). Little attention has been 1 Dataset and Models: github.com/asoleimanib/NLQuAD Figure 1: A question-answer pair in NLQuAD. QA models must predict the answer span within the context"
2021.eacl-main.106,P17-1147,0,0.0658127,"Missing"
2021.eacl-main.106,P18-1156,0,0.0356943,"Missing"
2021.eacl-main.106,W17-2623,0,0.0283002,"Missing"
2021.eacl-main.106,D18-1259,0,0.0209847,"rated the goodness of answers on a 3-point scale: (1: Irrelevant answer; 2: Good answer after adding or removing some sentences; 3: Perfect answer). The average score is 2.56 indicating the high quality of NLQuAD’s QA samples. In order to benchmark human performance, we asked the four volunteers to answer 50 questions, a randomly sampled subset of evaluation set. They were given unlimited time to detect the answers, but on average, it took them about 270 seconds to answer a question. Table 7 compares human performance with Longformer and RoBERTa-large on the same subset. Similar to HotpotQA (Yang et al., 2018), we estimate the human upper bound by tak5 Method RoBERTa-large Human-AVG Longformer Human-UB github.com/allenai/longformer ing the best human answer in terms of our primary evaluation metric (IoU) for each sample. While NLQuAD is a challenging task both for humans and the state of the art QA models, the human upper bound performance significantly outperforms the models. We suspect that the mediocre average of human performance, considering the high score of the target answers, might be because volunteers are not familiar with the articles’ writing style or they might have become exhausted by"
2021.eacl-main.106,D18-1009,0,0.0244212,"Wikipedia pages as context passages whereas SearchQA (Dunn et al., 2017) uses IR approaches to collect context passages. Answer generation based on a set of passages is another approach to address this task. MS MARCO (Bajaj et al., 2016) consists of real-world search queries and retrieved documents corresponding to the queries. There are also different types of QA data sets such as Antique (Hashemi et al., 2020), which is a data set for answer retrieval for non-factoid questions. There is also a range of multiple-choice QA tasks such as RACE (Lai et al., 2017), ARC (Clark et al., 2018), SWAQ (Zellers et al., 2018), and COSMOS QA (Huang et al., 2019) that are clustered together with the short-context QA data sets. 2.2 Long-Context Question Answering Factoid QA has been applied to longer documents, however, the nature of factoid questions limits answers to short texts. NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al., 2017), NarrativeQA (Koˇcisk´y et al., 2018), and DuoRC (Saha et al., 2018) fall into this category and their documents are extracted from news articles, stories, and movie plots, respectively. On the other hand, DQA (ter Hoeve et al., 2020) is a document-centred QA data set aimed at"
C14-1181,W13-2205,0,0.219241,". These problems have been long studied in the field of speech recognition but much less in SMT, although the target LM is a core component of all state-of-the-art SMT frameworks. Partly inspired by successful research in the field of speech recognition, various forms of class-based LMs have been shown to improve the quality of SMT when used in combination with standard wordlevel LMs. These approaches, however, have mostly focused on English (Uszkoreit and Brants, 2008; Dyer et al., 2011; Monz, 2011; Hassan et al., 2007; Birch et al., 2007) with only recent exceptions (Green and DeNero, 2012; Ammar et al., 2013; Wuebker et al., 2013; Durrani et al., 2014). Moreover, there is no published work that systematically evaluates different kinds of classes, model forms and LM combination methods in a unified SMT setting. On the contrary, most of the existing literature on LM combination uses mixtures of multiple word-level LMs for domain adaptation purposes. This paper aims to fill these gaps by applying various class-based LM techniques to the challenging problem of translating into a morphologically rich language. In particular we focus on English-Russian, a language pair for which a fair amount of both p"
C14-1181,W07-0702,0,0.113831,"order n-gram estimates, even when large amounts of training data are used. These problems have been long studied in the field of speech recognition but much less in SMT, although the target LM is a core component of all state-of-the-art SMT frameworks. Partly inspired by successful research in the field of speech recognition, various forms of class-based LMs have been shown to improve the quality of SMT when used in combination with standard wordlevel LMs. These approaches, however, have mostly focused on English (Uszkoreit and Brants, 2008; Dyer et al., 2011; Monz, 2011; Hassan et al., 2007; Birch et al., 2007) with only recent exceptions (Green and DeNero, 2012; Ammar et al., 2013; Wuebker et al., 2013; Durrani et al., 2014). Moreover, there is no published work that systematically evaluates different kinds of classes, model forms and LM combination methods in a unified SMT setting. On the contrary, most of the existing literature on LM combination uses mixtures of multiple word-level LMs for domain adaptation purposes. This paper aims to fill these gaps by applying various class-based LM techniques to the challenging problem of translating into a morphologically rich language. In particular we foc"
C14-1181,E12-1045,1,0.845755,"ents or grammatical agreement. Hassan et al. (2007) and Birch et al. (2007) went as far as scoring n-grams of Combinatorial Categorial Grammar supertags. When using linguistic classes, one has to deal with the fact that the same word can belong to different classes when used in different contexts. Solutions to this problem include tagging the target word sequence as it is generated (Koehn et al., 2007; Birch et al., 2007; Green and DeNero, 2012), choosing the most probable class sequence for each phrase pair (Monz, 2011) or—even more lightweight—choosing the most probable class for each word (Bisazza and Federico, 2012). Alternatively, simpler deterministic class mappings can be derived by using shallow linguistic knowledge, such as suffixes or orthographic features. The former can be obtained with a rule-based stemmer (as in this work), or, even more simply, by selecting the φ most common word suffixes in a training corpus and then mapping each word to its longest matching suffix (M¨uller et al., 2012). Orthographic features may include capitalization information or the presence of digits, punctuation or other special characters (M¨uller et al., 2012). 2.3 Hybrid surface/class models M¨uller et al. (2012) o"
C14-1181,J92-4003,0,0.664911,"ranslation quality when used in combination with standard word-level LMs but no published work has systematically compared different kinds of classes, model forms and LM combination methods in a unified SMT setting. This paper aims to fill these gaps by focusing on the challenging problem of translating into Russian, a language with rich inflectional morphology and complex agreement phenomena. We conduct our evaluation in a large-data scenario and report statistically significant BLEU improvements of up to 0.6 points when using a refined variant of the class-based model originally proposed by Brown et al. (1992). 1 Introduction Class-based n-gram modeling is an effective approach to overcome data sparsity in language model (LM) training. By grouping words with similar distributional behavior into equivalence classes, class-based LMs have less parameters to train and can make predictions based on longer histories. This makes them particularly attractive in situations where n-gram coverage is low due to shortage of training data or to specific properties of the language at hand. While translation into English has drawn most of the research effort in statistical machine translation (SMT) so far, there i"
C14-1181,D13-1174,0,0.104091,")) · p1 (wi |C(wi )) (1) This results in models that are more compact and more robust to data sparsity. Often, in the context of SMT, the word emission probability is dropped and only the class sequence is modeled. In this work, we refer to this model form as stream-based n-gram LM:1 i−1 i−1 Pstream (wi |wi−n+1 ) = p0 (C(wi )|C(wi−n+1 )) (2) Stream-based LMs are used, for instance, in factored SMT (Koehn et al., 2007), and in general many of the ‘class-based LMs’ mentioned in the SMT literature are actually of the latter form (2) (Dyer et al., 2011; Green and DeNero, 2012; Ammar et al., 2013; Chahuneau et al., 2013; Wuebker et al., 2013; Durrani et al., 2014). One exception is the work of Uszkoreit and Brants (2008), who incorporate word emission probabilities in their class-based LM used as an additional feature function in the log-linear combination (cf. Section 3.1). Interestingly, we are not aware of work that compares actual class-based LMs and stream-based LMs with respect to SMT quality. While class-based LMs are known to be effective at counteracting data sparsity issues due to rich vocabularies, it is worth noting that they adhere to the fundamental constraints of n-gram modeling. Thus, grammat"
C14-1181,W12-3125,0,0.165523,"Missing"
C14-1181,C14-1041,0,0.11568,"the field of speech recognition but much less in SMT, although the target LM is a core component of all state-of-the-art SMT frameworks. Partly inspired by successful research in the field of speech recognition, various forms of class-based LMs have been shown to improve the quality of SMT when used in combination with standard wordlevel LMs. These approaches, however, have mostly focused on English (Uszkoreit and Brants, 2008; Dyer et al., 2011; Monz, 2011; Hassan et al., 2007; Birch et al., 2007) with only recent exceptions (Green and DeNero, 2012; Ammar et al., 2013; Wuebker et al., 2013; Durrani et al., 2014). Moreover, there is no published work that systematically evaluates different kinds of classes, model forms and LM combination methods in a unified SMT setting. On the contrary, most of the existing literature on LM combination uses mixtures of multiple word-level LMs for domain adaptation purposes. This paper aims to fill these gaps by applying various class-based LM techniques to the challenging problem of translating into a morphologically rich language. In particular we focus on English-Russian, a language pair for which a fair amount of both parallel data and monolingual data has been pr"
C14-1181,W11-2139,0,0.105356,"cabulary word rates and frequent backing-off to low order n-gram estimates, even when large amounts of training data are used. These problems have been long studied in the field of speech recognition but much less in SMT, although the target LM is a core component of all state-of-the-art SMT frameworks. Partly inspired by successful research in the field of speech recognition, various forms of class-based LMs have been shown to improve the quality of SMT when used in combination with standard wordlevel LMs. These approaches, however, have mostly focused on English (Uszkoreit and Brants, 2008; Dyer et al., 2011; Monz, 2011; Hassan et al., 2007; Birch et al., 2007) with only recent exceptions (Green and DeNero, 2012; Ammar et al., 2013; Wuebker et al., 2013; Durrani et al., 2014). Moreover, there is no published work that systematically evaluates different kinds of classes, model forms and LM combination methods in a unified SMT setting. On the contrary, most of the existing literature on LM combination uses mixtures of multiple word-level LMs for domain adaptation purposes. This paper aims to fill these gaps by applying various class-based LM techniques to the challenging problem of translating into"
C14-1181,W07-0717,0,0.0597554,"(global λ) 18.9 +0.1 24.8 +0.1 5g class, linear (class λ’s) 18.6 −0.1 24.6 −0.1 Table 4: SMT translation quality on newstest13 when using different LM combining frameworks: additional feature in the log-linear combination or linear interpolation with perplexity-tuned weights (one global lambda or class-specific lambdas). 1924 guage models may be linearly interpolated with weights determined by maximizing the likelihood of a held-out monolingual data set (see Section 3.2). While linear interpolation often outperforms log-linear interpolation for combining language models for domain adaptation (Foster and Kuhn, 2007), this does not seem to be the case for language models for morphologically rich target languages. The results presented in Table 4 consistently show that linear interpolation under-performs log-linear combination under all conditions. Even using class-specific interpolation weights as suggested by M¨uller et al. (2012) did not lead to any further improvements. 5 Conclusion We have presented the first systematic comparison of different forms of class-based LMs and different class LM combination methods in the context of SMT into a morphologically rich language. First of all, our results have s"
C14-1181,D08-1089,0,0.573326,"Missing"
C14-1181,P09-1087,0,0.0133636,"ctual class-based LMs and stream-based LMs with respect to SMT quality. While class-based LMs are known to be effective at counteracting data sparsity issues due to rich vocabularies, it is worth noting that they adhere to the fundamental constraints of n-gram modeling. Thus, grammatical agreement may be improved by a class-based LM approach only within a limited context window. Previous work that attempted to overcome this limitation includes (i) syntactic LMs for n-best reranking (Hasan et al., 2006; Carter and Monz, 2011) or integrated into decoding with significant engineering challenges (Galley and Manning, 2009; Schwartz et al., 2011) and (ii) unification-based constraints applied to a syntax-based SMT framework (Williams and Koehn, 2011). We will now describe different kinds of word-to-class mapping functions used by class-based LMs. These can be completely data-driven or based on different sorts of linguistic or orthographic features. 2.1 Data-driven classes The most popular form of class-based LMs was introduced by (Brown et al., 1992). In this approach, the corpus vocabulary is partitioned into a preset number of clusters by directly maximizing the likelihood of a training corpus. No linguistic"
C14-1181,P12-1016,0,0.724899,"f training data are used. These problems have been long studied in the field of speech recognition but much less in SMT, although the target LM is a core component of all state-of-the-art SMT frameworks. Partly inspired by successful research in the field of speech recognition, various forms of class-based LMs have been shown to improve the quality of SMT when used in combination with standard wordlevel LMs. These approaches, however, have mostly focused on English (Uszkoreit and Brants, 2008; Dyer et al., 2011; Monz, 2011; Hassan et al., 2007; Birch et al., 2007) with only recent exceptions (Green and DeNero, 2012; Ammar et al., 2013; Wuebker et al., 2013; Durrani et al., 2014). Moreover, there is no published work that systematically evaluates different kinds of classes, model forms and LM combination methods in a unified SMT setting. On the contrary, most of the existing literature on LM combination uses mixtures of multiple word-level LMs for domain adaptation purposes. This paper aims to fill these gaps by applying various class-based LM techniques to the challenging problem of translating into a morphologically rich language. In particular we focus on English-Russian, a language pair for which a f"
C14-1181,W06-2606,0,0.0298512,"unction in the log-linear combination (cf. Section 3.1). Interestingly, we are not aware of work that compares actual class-based LMs and stream-based LMs with respect to SMT quality. While class-based LMs are known to be effective at counteracting data sparsity issues due to rich vocabularies, it is worth noting that they adhere to the fundamental constraints of n-gram modeling. Thus, grammatical agreement may be improved by a class-based LM approach only within a limited context window. Previous work that attempted to overcome this limitation includes (i) syntactic LMs for n-best reranking (Hasan et al., 2006; Carter and Monz, 2011) or integrated into decoding with significant engineering challenges (Galley and Manning, 2009; Schwartz et al., 2011) and (ii) unification-based constraints applied to a syntax-based SMT framework (Williams and Koehn, 2011). We will now describe different kinds of word-to-class mapping functions used by class-based LMs. These can be completely data-driven or based on different sorts of linguistic or orthographic features. 2.1 Data-driven classes The most popular form of class-based LMs was introduced by (Brown et al., 1992). In this approach, the corpus vocabulary is p"
C14-1181,P07-1037,0,0.0550328,"Missing"
C14-1181,D11-1125,0,0.60521,"in the global combination. 3.1 Log-linear combination The standard log-linear approach to SMT allows for the combination of m arbitrary model components (or feature functions), each weighted by a corresponding weight αm : p(x|h) = Y pm (x|h)αm (4) m In typical SMT settings, pm (x|h) are phrase- or word-level translation probabilities, reordering probabilities, and so on. Treating the new LM as an additional feature function has the advantage that its weight can be directly optimized for SMT quality together with all other feature weights, using standard parameter tuning techniques (Och, 2003; Hopkins and May, 2011). 3.2 Linear interpolation The other widely used combining framework is linear interpolation or mixture model: p(x|h) = X λq pq (x|h) (5) q More specifically, word LMs are usually interpolated as a word-level weighted average of the n-gram probabilities: ! n Y X pmixLM (e) = λq pq (ei |hi ) (6) i=1 q The drawback of this approach is that the linear interpolation weights, or lambdas, cannot be set with standard SMT tuning techniques. Instead, interpolation weights are typically determined by maximizing the likelihood of a held-out monolingual data set, but this does not always outperform simple"
C14-1181,N03-1017,0,0.06745,"Missing"
C14-1181,P07-2045,0,0.0240289,"into equivalence classes. The word transition probability is then decomposed into a class transition probability and a word emission probability: i−1 i−1 Pclass (wi |wi−n+1 ) = p0 (C(wi )|C(wi−n+1 )) · p1 (wi |C(wi )) (1) This results in models that are more compact and more robust to data sparsity. Often, in the context of SMT, the word emission probability is dropped and only the class sequence is modeled. In this work, we refer to this model form as stream-based n-gram LM:1 i−1 i−1 Pstream (wi |wi−n+1 ) = p0 (C(wi )|C(wi−n+1 )) (2) Stream-based LMs are used, for instance, in factored SMT (Koehn et al., 2007), and in general many of the ‘class-based LMs’ mentioned in the SMT literature are actually of the latter form (2) (Dyer et al., 2011; Green and DeNero, 2012; Ammar et al., 2013; Chahuneau et al., 2013; Wuebker et al., 2013; Durrani et al., 2014). One exception is the work of Uszkoreit and Brants (2008), who incorporate word emission probabilities in their class-based LM used as an additional feature function in the log-linear combination (cf. Section 3.1). Interestingly, we are not aware of work that compares actual class-based LMs and stream-based LMs with respect to SMT quality. While class"
C14-1181,D09-1079,0,0.0274821,"number of clusters by directly maximizing the likelihood of a training corpus. No linguistic or orthographic features are taken into account while training the classes.2 Later work has focused on decreasing the large computational cost of the exchange algorithm proposed by Brown et al. (1992), either with a distributed algorithm (Uszkoreit and Brants, 2008) or by using a whole-context distributional vector space model (Sch¨utze and Walsh, 2011). In this paper we use the standard SRILM implementation of Brown clustering. 1 Not to be confused with the incrementally trainable stream-based LMs of Levenberg and Osborne (2009). Och (1999) extends a similar approach to bilingual clustering with the aim of generalizing the applicability of translation rules in an alignment template SMT framework. 2 1919 2.2 Linguistic classes Linguistic knowledge is another way to establish word equivalence classes. Common examples include lemma, part of speech and morphology-based classes, each of which can capture different aspects of the word sequence, such as the relative order of syntactic constituents or grammatical agreement. Hassan et al. (2007) and Birch et al. (2007) went as far as scoring n-grams of Combinatorial Categoria"
C14-1181,D11-1080,1,0.935922,"and frequent backing-off to low order n-gram estimates, even when large amounts of training data are used. These problems have been long studied in the field of speech recognition but much less in SMT, although the target LM is a core component of all state-of-the-art SMT frameworks. Partly inspired by successful research in the field of speech recognition, various forms of class-based LMs have been shown to improve the quality of SMT when used in combination with standard wordlevel LMs. These approaches, however, have mostly focused on English (Uszkoreit and Brants, 2008; Dyer et al., 2011; Monz, 2011; Hassan et al., 2007; Birch et al., 2007) with only recent exceptions (Green and DeNero, 2012; Ammar et al., 2013; Wuebker et al., 2013; Durrani et al., 2014). Moreover, there is no published work that systematically evaluates different kinds of classes, model forms and LM combination methods in a unified SMT setting. On the contrary, most of the existing literature on LM combination uses mixtures of multiple word-level LMs for domain adaptation purposes. This paper aims to fill these gaps by applying various class-based LM techniques to the challenging problem of translating into a morpholog"
C14-1181,N12-1043,0,0.206176,"Missing"
C14-1181,E99-1010,0,0.369416,"maximizing the likelihood of a training corpus. No linguistic or orthographic features are taken into account while training the classes.2 Later work has focused on decreasing the large computational cost of the exchange algorithm proposed by Brown et al. (1992), either with a distributed algorithm (Uszkoreit and Brants, 2008) or by using a whole-context distributional vector space model (Sch¨utze and Walsh, 2011). In this paper we use the standard SRILM implementation of Brown clustering. 1 Not to be confused with the incrementally trainable stream-based LMs of Levenberg and Osborne (2009). Och (1999) extends a similar approach to bilingual clustering with the aim of generalizing the applicability of translation rules in an alignment template SMT framework. 2 1919 2.2 Linguistic classes Linguistic knowledge is another way to establish word equivalence classes. Common examples include lemma, part of speech and morphology-based classes, each of which can capture different aspects of the word sequence, such as the relative order of syntactic constituents or grammatical agreement. Hassan et al. (2007) and Birch et al. (2007) went as far as scoring n-grams of Combinatorial Categorial Grammar su"
C14-1181,P03-1021,0,0.0161206,"e function in the global combination. 3.1 Log-linear combination The standard log-linear approach to SMT allows for the combination of m arbitrary model components (or feature functions), each weighted by a corresponding weight αm : p(x|h) = Y pm (x|h)αm (4) m In typical SMT settings, pm (x|h) are phrase- or word-level translation probabilities, reordering probabilities, and so on. Treating the new LM as an additional feature function has the advantage that its weight can be directly optimized for SMT quality together with all other feature weights, using standard parameter tuning techniques (Och, 2003; Hopkins and May, 2011). 3.2 Linear interpolation The other widely used combining framework is linear interpolation or mixture model: p(x|h) = X λq pq (x|h) (5) q More specifically, word LMs are usually interpolated as a word-level weighted average of the n-gram probabilities: ! n Y X pmixLM (e) = λq pq (ei |hi ) (6) i=1 q The drawback of this approach is that the linear interpolation weights, or lambdas, cannot be set with standard SMT tuning techniques. Instead, interpolation weights are typically determined by maximizing the likelihood of a held-out monolingual data set, but this does not"
C14-1181,P02-1040,0,0.0900176,"s that using class-specific interpolation weights is not significantly better, and sometimes is even worse than using only one generic λ, at least from the point of view of perplexity. Since weight estimation for linear interpolation is still an open problem for SMT, we decide nevertheless to compare these two interpolation methods in our translation experiments (see Table 4). 4.3 SMT results Table 3 shows the results for English to Russian translation using log-linear combination with Brown clusters and the hybrid suffix/word classes. Translation quality is measured by case-insensitive BLEU (Papineni et al., 2002) on newstest13 using one reference translation. The relative improvements of the different class-based LM runs are with respect to the baseline which uses a word-based LM only and achieves comparable results to the state-of-the-art. We use approximate randomization (Noreen, 1989) to test for statistically significant differences between runs (Riezler and Maxwell, 2005). We can see from Table 2(a) that using a stream-based LM as an additional feature, which is log-linearly interpolated with the other decoder features during parameter estimation, leads to small but statistically significant impr"
C14-1181,W05-0908,0,0.474134,"ee Table 4). 4.3 SMT results Table 3 shows the results for English to Russian translation using log-linear combination with Brown clusters and the hybrid suffix/word classes. Translation quality is measured by case-insensitive BLEU (Papineni et al., 2002) on newstest13 using one reference translation. The relative improvements of the different class-based LM runs are with respect to the baseline which uses a word-based LM only and achieves comparable results to the state-of-the-art. We use approximate randomization (Noreen, 1989) to test for statistically significant differences between runs (Riezler and Maxwell, 2005). We can see from Table 2(a) that using a stream-based LM as an additional feature, which is log-linearly interpolated with the other decoder features during parameter estimation, leads to small but statistically significant improvements. The results also indicate that using a higher n-gram class model (7-gram) does not yield additional improvements over a 5-gram class model, which is in contrast with the results reported by Wuebker et al. (2013) on a French-German task. Since the stream-based models ignore word emission probabilities, one would expect further improvements from the theoretical"
C14-1181,J11-4008,0,0.0432536,"Missing"
C14-1181,P11-1063,0,0.0158854,"stream-based LMs with respect to SMT quality. While class-based LMs are known to be effective at counteracting data sparsity issues due to rich vocabularies, it is worth noting that they adhere to the fundamental constraints of n-gram modeling. Thus, grammatical agreement may be improved by a class-based LM approach only within a limited context window. Previous work that attempted to overcome this limitation includes (i) syntactic LMs for n-best reranking (Hasan et al., 2006; Carter and Monz, 2011) or integrated into decoding with significant engineering challenges (Galley and Manning, 2009; Schwartz et al., 2011) and (ii) unification-based constraints applied to a syntax-based SMT framework (Williams and Koehn, 2011). We will now describe different kinds of word-to-class mapping functions used by class-based LMs. These can be completely data-driven or based on different sorts of linguistic or orthographic features. 2.1 Data-driven classes The most popular form of class-based LMs was introduced by (Brown et al., 1992). In this approach, the corpus vocabulary is partitioned into a preset number of clusters by directly maximizing the likelihood of a training corpus. No linguistic or orthographic features"
C14-1181,P08-1086,0,0.54089,"at reflect in high out-of-vocabulary word rates and frequent backing-off to low order n-gram estimates, even when large amounts of training data are used. These problems have been long studied in the field of speech recognition but much less in SMT, although the target LM is a core component of all state-of-the-art SMT frameworks. Partly inspired by successful research in the field of speech recognition, various forms of class-based LMs have been shown to improve the quality of SMT when used in combination with standard wordlevel LMs. These approaches, however, have mostly focused on English (Uszkoreit and Brants, 2008; Dyer et al., 2011; Monz, 2011; Hassan et al., 2007; Birch et al., 2007) with only recent exceptions (Green and DeNero, 2012; Ammar et al., 2013; Wuebker et al., 2013; Durrani et al., 2014). Moreover, there is no published work that systematically evaluates different kinds of classes, model forms and LM combination methods in a unified SMT setting. On the contrary, most of the existing literature on LM combination uses mixtures of multiple word-level LMs for domain adaptation purposes. This paper aims to fill these gaps by applying various class-based LM techniques to the challenging problem"
C14-1181,W11-2126,0,0.0178667,"racting data sparsity issues due to rich vocabularies, it is worth noting that they adhere to the fundamental constraints of n-gram modeling. Thus, grammatical agreement may be improved by a class-based LM approach only within a limited context window. Previous work that attempted to overcome this limitation includes (i) syntactic LMs for n-best reranking (Hasan et al., 2006; Carter and Monz, 2011) or integrated into decoding with significant engineering challenges (Galley and Manning, 2009; Schwartz et al., 2011) and (ii) unification-based constraints applied to a syntax-based SMT framework (Williams and Koehn, 2011). We will now describe different kinds of word-to-class mapping functions used by class-based LMs. These can be completely data-driven or based on different sorts of linguistic or orthographic features. 2.1 Data-driven classes The most popular form of class-based LMs was introduced by (Brown et al., 1992). In this approach, the corpus vocabulary is partitioned into a preset number of clusters by directly maximizing the likelihood of a training corpus. No linguistic or orthographic features are taken into account while training the classes.2 Later work has focused on decreasing the large comput"
C14-1181,D13-1138,0,0.498402,"e been long studied in the field of speech recognition but much less in SMT, although the target LM is a core component of all state-of-the-art SMT frameworks. Partly inspired by successful research in the field of speech recognition, various forms of class-based LMs have been shown to improve the quality of SMT when used in combination with standard wordlevel LMs. These approaches, however, have mostly focused on English (Uszkoreit and Brants, 2008; Dyer et al., 2011; Monz, 2011; Hassan et al., 2007; Birch et al., 2007) with only recent exceptions (Green and DeNero, 2012; Ammar et al., 2013; Wuebker et al., 2013; Durrani et al., 2014). Moreover, there is no published work that systematically evaluates different kinds of classes, model forms and LM combination methods in a unified SMT setting. On the contrary, most of the existing literature on LM combination uses mixtures of multiple word-level LMs for domain adaptation purposes. This paper aims to fill these gaps by applying various class-based LM techniques to the challenging problem of translating into a morphologically rich language. In particular we focus on English-Russian, a language pair for which a fair amount of both parallel data and monol"
C14-1181,W13-2201,1,\N,Missing
C16-1133,W14-3302,1,0.783225,"system combination can be cast as an ensemble prediction task and a variety of existing general prediction combination methods can be applied. While this paper focuses on word-based models, the ensemble methods discussed in this paper can be applied to character-based sequential NMT models (Ling et al., 2015) in a very similar fashion. Ensemble prediction is frequently used in NMT. A commonly reported method is uniform weighting of the output layers, i.e., distributions over the target vocabulary, produced by different trained instances of the same NMT architecture for the same language pair (Bojar et al., 2014). We use this method as 1409 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1409–1418, Osaka, Japan, December 11-17 2016. a baseline where the variations of the same system are produced by different parameter initializations. Alternatively, it is also possible to take parameter snapshots from different training epochs (Sennrich and Haddow, 2016). Recently, a new type of ensemble has been introduced in NMT: a multi-source ensemble (Firat et al., 2016), which is a set of NMT systems that translate from different source language"
C16-1133,2012.eamt-1.60,0,0.0205077,"We expect a French-English system to perform better than German-English. But also, given the linguistic intuition about the structural differences between these translation pairs, we hope that the two systems compensate for each other’s weaknesses when used in an ensemble. To ensure that the only distinguishing factor between different language pairs is the source language, we chose training data which is to a large extent parallel across all three languages, i.e., it is a trilingual parallel text with small bilingual parts. To this end, we train all of our systems on the TED talks data set (Cettolo et al., 2012). All available data is split into a training and a validation set to train individual NMT systems, a training and a validation set to train a combination function for ensembles, and a test set for the final evaluation. The training data for learning the ensemble combination function and the test set should necessarily be fully parallel (tri-parallel). Therefore, we extracted our test set from the available trilingual data and did not use the test sets provided by (Cettolo et al., 2012) since they are not parallel across all three languages. Of course, the test set does not overlap with the tr"
C16-1133,J07-2003,0,0.0521919,"o 2.2 BLEU points over the strongest singlesource ensemble baseline, and a 2 BLEU improvement over a multi-source ensemble baseline. 1 Introduction It has been shown for various machine learning applications that combining multiple systems can substantially improve performance (Rokach, 2010). System combination has also been successfully applied to statistical machine translation system (SMT) (Och and Ney, 2001; Matusov et al., 2006; Schwartz, 2008; Schroeder et al., 2009). However, system combination methods in the phrase-based (PBSMT) (Koehn et al., 2003) and hierarchical (HSMT) frameworks (Chiang, 2007) tend to be rather complex, requiring potentially non-trivial mappings between the partial hypotheses across the search spaces of the individual systems. For this reason SMT system combination is often limited to combining hypotheses from the n-best list. Alternatively, SMT systems can also be combined by processing different inputs as is the case for multilingual system combination. Unfortunately, input sentences in different languages may have very different structure, requiring elaborate methods to align sentences, which means that multilingual system combination is in practice restricted t"
C16-1133,D16-1026,0,0.526952,"and Christof Monz Informatics Institute, University of Amsterdam Science Park 904, 1098 XH Amsterdam, The Netherlands {e.garmash,c.monz}@uva.nl Abstract In this paper we describe and evaluate methods to perform ensemble prediction in neural machine translation (NMT). We compare two methods of ensemble set induction: sampling parameter initializations for an NMT system, which is a relatively established method in NMT (Sutskever et al., 2014), and NMT systems translating from different source languages into the same target language, i.e., multi-source ensembles, a method recently introduced by Firat et al. (2016). We are motivated by the observation that for different language pairs systems make different types of mistakes. We propose several methods with different degrees of parameterization to combine individual predictions of NMT systems so that they mutually compensate for each other’s mistakes and improve overall performance. We find that the biggest improvements can be obtained from a context-dependent weighting scheme for multi-source ensembles. This result offers stronger support for the linguistic motivation of using multi-source ensembles than previous approaches. Evaluation is carried out f"
C16-1133,D13-1176,0,0.0279128,"ining hypotheses from the n-best list. Alternatively, SMT systems can also be combined by processing different inputs as is the case for multilingual system combination. Unfortunately, input sentences in different languages may have very different structure, requiring elaborate methods to align sentences, which means that multilingual system combination is in practice restricted to languages with similar structures. On the other hand, the recently emerged neural machine translation (NMT) framework offers a straightforward way to combine multiple systems. Most of the current NMT architectures (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) formalize the target sentence generation as a word sequence prediction task. At each step during sequence prediction, a translation system outputs a full probability distribution over the target vocabulary. Therefore, the task of NMT system combination can be cast as an ensemble prediction task and a variety of existing general prediction combination methods can be applied. While this paper focuses on word-based models, the ensemble methods discussed in this paper can be applied to character-based sequential NMT models (Ling et al., 2015) in a v"
C16-1133,N03-1017,0,0.0401321,"i-source ensemble method achieves an improvement of up to 2.2 BLEU points over the strongest singlesource ensemble baseline, and a 2 BLEU improvement over a multi-source ensemble baseline. 1 Introduction It has been shown for various machine learning applications that combining multiple systems can substantially improve performance (Rokach, 2010). System combination has also been successfully applied to statistical machine translation system (SMT) (Och and Ney, 2001; Matusov et al., 2006; Schwartz, 2008; Schroeder et al., 2009). However, system combination methods in the phrase-based (PBSMT) (Koehn et al., 2003) and hierarchical (HSMT) frameworks (Chiang, 2007) tend to be rather complex, requiring potentially non-trivial mappings between the partial hypotheses across the search spaces of the individual systems. For this reason SMT system combination is often limited to combining hypotheses from the n-best list. Alternatively, SMT systems can also be combined by processing different inputs as is the case for multilingual system combination. Unfortunately, input sentences in different languages may have very different structure, requiring elaborate methods to align sentences, which means that multiling"
C16-1133,D15-1166,0,0.133588,"e with different random initializations and multi-source). The experiment confirms our intuition that linguistic diversity can be beneficial for building NMT ensembles. In Section 4 we describe two models to train an ensemble combination function: a global weighting function and a context-dependent gating network. Section 5 contains results of translation experiments with different types of ensembles and their discussion. Section 6 provides some conclusions and outlook on future work. 2 Attention-based sequence-to-sequence NMT We use an encoder-decoder neural translation model as described in Luong et al. (2015) to train individual predictors in an ensemble. The source encoder is a four-layer unidirectional LSTM. The final hidden 1410 (a) (b) (c) Set train DE-EN train FR-EN valid DE-EN valid FR-EN combin.train DE-EN-FR combin.valid DE-EN-FR test DE-EN-FR N. of lines 123,955 127,755 2,052 887 19,000 1,000 3,000 N. of word tokens DE: 2,3M; EN: 2,5M FR: 2,9M; EN: 2,6M DE: 40.3K; EN: 41.5K FR: 21.5K; EN: 20K DE: 372K; FR: 447K; EN: 396K DE: 19K; FR: 23K; EN: 20.5K DE: 62.9K; FR: 78K; EN: 69.5K N. of word types DE: 89K; EN: 41K FR: 58K; EN: 41,9K DE: 6.3K; EN: 4.7K FR: 3.7K; EN: 3.1K DE: 29K; FR: 22.8K; E"
C16-1133,E06-1005,0,0.0124357,"urce ensembles than previous approaches. Evaluation is carried out for German and French into English translation. The best multi-source ensemble method achieves an improvement of up to 2.2 BLEU points over the strongest singlesource ensemble baseline, and a 2 BLEU improvement over a multi-source ensemble baseline. 1 Introduction It has been shown for various machine learning applications that combining multiple systems can substantially improve performance (Rokach, 2010). System combination has also been successfully applied to statistical machine translation system (SMT) (Och and Ney, 2001; Matusov et al., 2006; Schwartz, 2008; Schroeder et al., 2009). However, system combination methods in the phrase-based (PBSMT) (Koehn et al., 2003) and hierarchical (HSMT) frameworks (Chiang, 2007) tend to be rather complex, requiring potentially non-trivial mappings between the partial hypotheses across the search spaces of the individual systems. For this reason SMT system combination is often limited to combining hypotheses from the n-best list. Alternatively, SMT systems can also be combined by processing different inputs as is the case for multilingual system combination. Unfortunately, input sentences in di"
C16-1133,2001.mtsummit-papers.46,0,0.442636,"n of using multi-source ensembles than previous approaches. Evaluation is carried out for German and French into English translation. The best multi-source ensemble method achieves an improvement of up to 2.2 BLEU points over the strongest singlesource ensemble baseline, and a 2 BLEU improvement over a multi-source ensemble baseline. 1 Introduction It has been shown for various machine learning applications that combining multiple systems can substantially improve performance (Rokach, 2010). System combination has also been successfully applied to statistical machine translation system (SMT) (Och and Ney, 2001; Matusov et al., 2006; Schwartz, 2008; Schroeder et al., 2009). However, system combination methods in the phrase-based (PBSMT) (Koehn et al., 2003) and hierarchical (HSMT) frameworks (Chiang, 2007) tend to be rather complex, requiring potentially non-trivial mappings between the partial hypotheses across the search spaces of the individual systems. For this reason SMT system combination is often limited to combining hypotheses from the n-best list. Alternatively, SMT systems can also be combined by processing different inputs as is the case for multilingual system combination. Unfortunately,"
C16-1133,P02-1040,0,0.113186,"ampling from a Gaussian with unit variance. Each translation system is trained for 20 epochs. We use SGD with mini-batches of size 20 with a learning rate of 1 and a decay rate of 0.8 after the fifth epoch. During training we limit the lengths of predicted sequences to 50 tokens. 2.2 Translation experiments with individual systems For each language pair we train four systems by sampling different initial parameter values. Table 2 summarizes the performances of the individual systems. In all of the translation experiments the beam decoding size was set to 20. We evaluate performance with BLEU (Papineni et al., 2002) and METEOR (Lavie and Denkowski, 2009). The first thing to notice is that distributions for both metrics are consistent. Since we focus on ensemble translation, we are interested in how diverse the translation systems are in their performance. First, we can see that the two language pairs have different degrees of internal variation. The performance variance of German-English is smaller than that of French-English. Second, we see differences between the two source languages when translating into the same target language. These differences are much higher than those within one language pair. T"
C16-1133,E09-1082,0,0.0207011,"s. Evaluation is carried out for German and French into English translation. The best multi-source ensemble method achieves an improvement of up to 2.2 BLEU points over the strongest singlesource ensemble baseline, and a 2 BLEU improvement over a multi-source ensemble baseline. 1 Introduction It has been shown for various machine learning applications that combining multiple systems can substantially improve performance (Rokach, 2010). System combination has also been successfully applied to statistical machine translation system (SMT) (Och and Ney, 2001; Matusov et al., 2006; Schwartz, 2008; Schroeder et al., 2009). However, system combination methods in the phrase-based (PBSMT) (Koehn et al., 2003) and hierarchical (HSMT) frameworks (Chiang, 2007) tend to be rather complex, requiring potentially non-trivial mappings between the partial hypotheses across the search spaces of the individual systems. For this reason SMT system combination is often limited to combining hypotheses from the n-best list. Alternatively, SMT systems can also be combined by processing different inputs as is the case for multilingual system combination. Unfortunately, input sentences in different languages may have very different"
C16-1133,2008.amta-srw.6,0,0.0261338,"evious approaches. Evaluation is carried out for German and French into English translation. The best multi-source ensemble method achieves an improvement of up to 2.2 BLEU points over the strongest singlesource ensemble baseline, and a 2 BLEU improvement over a multi-source ensemble baseline. 1 Introduction It has been shown for various machine learning applications that combining multiple systems can substantially improve performance (Rokach, 2010). System combination has also been successfully applied to statistical machine translation system (SMT) (Och and Ney, 2001; Matusov et al., 2006; Schwartz, 2008; Schroeder et al., 2009). However, system combination methods in the phrase-based (PBSMT) (Koehn et al., 2003) and hierarchical (HSMT) frameworks (Chiang, 2007) tend to be rather complex, requiring potentially non-trivial mappings between the partial hypotheses across the search spaces of the individual systems. For this reason SMT system combination is often limited to combining hypotheses from the n-best list. Alternatively, SMT systems can also be combined by processing different inputs as is the case for multilingual system combination. Unfortunately, input sentences in different language"
C16-1133,W16-2209,0,0.0242682,"od is uniform weighting of the output layers, i.e., distributions over the target vocabulary, produced by different trained instances of the same NMT architecture for the same language pair (Bojar et al., 2014). We use this method as 1409 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1409–1418, Osaka, Japan, December 11-17 2016. a baseline where the variations of the same system are produced by different parameter initializations. Alternatively, it is also possible to take parameter snapshots from different training epochs (Sennrich and Haddow, 2016). Recently, a new type of ensemble has been introduced in NMT: a multi-source ensemble (Firat et al., 2016), which is a set of NMT systems that translate from different source languages into the same target language. The general idea behind ensembles is that different predictors are likely to produce slightly different errors for different input instances, and if their predictions are combined, the overall error is reduced. Therefore, it is essential to introduce a minimum of diversity into an ensemble. Different random initializations force the same training algorithm to converge to different"
C16-1242,P12-2040,0,0.0619647,"Missing"
C16-1242,2011.mtsummit-papers.32,0,0.068911,"Missing"
C16-1242,2012.eamt-1.41,0,0.0367107,"Missing"
C16-1242,N10-1064,0,0.0183104,"ers are harder to translate and use more vulgar language than female speakers, and that vulgarity is often not preserved during translation. 1 Introduction Research in statistical machine translation (SMT) has mostly been driven by formal translation tasks. These are, however, not representative for the abundance of informal data emerging on the Internet, for which state-of-the-art SMT systems perform markedly worse (van der Wees et al., 2015a). Recent years have therefore shown an increasing effort in improving SMT for informal text, for example by normalizing noisy text to more formal text (Bertoldi et al., 2010; Banerjee et al., 2012; Ling et al., 2013a), or by enhancing formal training data with user-generated data (Banerjee et al., 2011; Jehl et al., 2012; Ling et al., 2013b). In this paper we focus on SMT for dialogues, an informal genre that involves, by definition, multiple speakers, and is thus noticeably different from formal text (Fern´andez, 2014). Formal text is typically written by a single writer with a clear intention (e.g., informing or persuading), and moreover has been editorially controlled according to standards of language use. In dialogues, on the other hand, different speakers h"
C16-1242,W11-0609,0,0.0328163,"main This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2571 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2571–2581, Osaka, Japan, December 11-17 2016. messages (Forchini, 2009; Forchini, 2012; Dose, 2013). In fact, movie dialogues approximate real faceto-face conversation more than for example SMS and chat, in which media constraints influence the flow of a conversation (Whittaker, 2003; Brennan and Lockridge, 2006). Finally, Danescu-Niculescu-Mizil and Lee (2011) have shown that certain psycholinguistic and gender-specific aspects of language are also observed in fictional dialogues, indicating that conclusions drawn from experiments on fictional dialogues generalize at least partially to real spoken conversations. The structure and contributions of this paper are as follows: First, in Section 2 we annotate multilingual movie dialogues with four dialogue-specific variables; dialogue acts, speakers, gender, and register level, and we release these annotated corpora. In Section 3 we describe our approach to measure how SMT quality is affected by each of"
C16-1242,D11-1125,0,0.0594698,"Missing"
C16-1242,W12-3153,0,0.0203806,"on Research in statistical machine translation (SMT) has mostly been driven by formal translation tasks. These are, however, not representative for the abundance of informal data emerging on the Internet, for which state-of-the-art SMT systems perform markedly worse (van der Wees et al., 2015a). Recent years have therefore shown an increasing effort in improving SMT for informal text, for example by normalizing noisy text to more formal text (Bertoldi et al., 2010; Banerjee et al., 2012; Ling et al., 2013a), or by enhancing formal training data with user-generated data (Banerjee et al., 2011; Jehl et al., 2012; Ling et al., 2013b). In this paper we focus on SMT for dialogues, an informal genre that involves, by definition, multiple speakers, and is thus noticeably different from formal text (Fern´andez, 2014). Formal text is typically written by a single writer with a clear intention (e.g., informing or persuading), and moreover has been editorially controlled according to standards of language use. In dialogues, on the other hand, different speakers have different intentions and language use, affected, for example, by their gender. Such variations are reflected by register, a term referring to soc"
C16-1242,P07-2045,0,0.00740994,"Missing"
C16-1242,D13-1008,0,0.0215073,"r language than female speakers, and that vulgarity is often not preserved during translation. 1 Introduction Research in statistical machine translation (SMT) has mostly been driven by formal translation tasks. These are, however, not representative for the abundance of informal data emerging on the Internet, for which state-of-the-art SMT systems perform markedly worse (van der Wees et al., 2015a). Recent years have therefore shown an increasing effort in improving SMT for informal text, for example by normalizing noisy text to more formal text (Bertoldi et al., 2010; Banerjee et al., 2012; Ling et al., 2013a), or by enhancing formal training data with user-generated data (Banerjee et al., 2011; Jehl et al., 2012; Ling et al., 2013b). In this paper we focus on SMT for dialogues, an informal genre that involves, by definition, multiple speakers, and is thus noticeably different from formal text (Fern´andez, 2014). Formal text is typically written by a single writer with a clear intention (e.g., informing or persuading), and moreover has been editorially controlled according to standards of language use. In dialogues, on the other hand, different speakers have different intentions and language use,"
C16-1242,L16-1147,0,0.148557,"Missing"
C16-1242,ma-2006-champollion,0,0.0308838,"Missing"
C16-1242,D15-1238,0,0.0259923,"Missing"
C16-1242,D15-1130,0,0.0785304,"Missing"
C16-1242,P02-1040,0,0.098913,"Missing"
C16-1242,W05-0908,0,0.123442,"Missing"
C16-1242,tiedemann-2008-synchronizing,0,0.352631,"Missing"
C16-1242,2009.eamt-1.16,0,0.0663045,"Missing"
C16-1242,L16-1559,0,0.251654,"Missing"
C16-1242,W15-4304,1,0.866525,"Missing"
C16-1242,P15-2092,1,0.839705,"Missing"
C16-1242,walker-etal-2012-annotated,0,0.0530557,"Missing"
C16-1242,L16-1436,0,0.281665,"Missing"
C16-1242,P13-1018,0,\N,Missing
D11-1080,P06-1067,0,0.0606363,"Missing"
D11-1080,N03-2002,0,0.0896531,"dual documents and the relative, absolute improvements achieved by +posLM (left) and +locLM (right). BLEU scores (and improvements) are computed at the document level. ble 3). For the local language model, the regression line intersects with the neutral line at about 40 BLEU, suggesting that until translation quality improves substantially, local language models could still have a positive impact. 6 Related Work The main goal of this paper is to show that by tying POS language models to lexical items, we get more accurate distributions for specific words. The work on factored language models (Bilmes and Kirchhoff, 2003) is related to our work to the extent that it also mixes POS tags with lexical information, albeit in a very different manner. Factored language models use more general representations, such as POS tags or stems, only during back-off. Kirchhoff and Yang (2005) applied factored language models to machine translation but the improvements were negligible. Collins et al. (2005) proposed a discriminative language modeling approach that uses mixtures of POS and surface information and showed that it leads to a reduction in speech recognition word er877 ror rates. On the other hand, their approach se"
D11-1080,W07-0702,0,0.199186,"am local language models. Translation quality is evaluated for two language pairs: Arabic-to-English and Chinese-to-English. NIST’s MT-Eval test sets are used for both pairs. Only resources allowed under NIST’s constrained data conditions are used to train the language, translation, and lexicalized distortion models. To see whether our local language models result in improvements over a competitive baseline, we designed the baseline to use a large 5-gram word language model and lexicalized distortion modeling, both of which are known to cancel-out improvements gained from POS language models (Birch et al., 2007; Kirchhoff and Yang, 2005). The 5-gram word language model is trained on the Xinhua and AFP sections of the Gigaword corpus (3rd edition, LDC2007T40) and the target side of the bitext. We removed from the training data all documents released during the periods that overlap with the publication dates of the documents included in our development or test data sets. In total, 630 million tokens were used to build the word language model. The language model was trained using SRILM with modified Kneser-Ney smoothing and interpolation (Chen and Goodman, 1999). It is common practice not to include hi"
D11-1080,W07-0409,0,0.0293566,"Missing"
D11-1080,D07-1090,0,0.0327147,"lly significant improvements for multiple test sets and also across different genres, when compared against a competitive baseline and a system using a part-of-speech model. 1 Introduction Language models are an important component of current statistical machine translation systems. They affect the selection of phrase translation candidates and reordering choices by estimating the probability that an application of a phrase translation is a fluent continuation of the current translation hypothesis. The size and domain of the language model can have a significant impact on translation quality. Brants et al. (2007) have shown that each doubling of the training data from the news domain (used to build the language model), leads to improvements of approximately 0.5 BLEU points. On the other hand, 869 each doubling using general web data leads to improvements of approximately 0.15 BLEU points. While large n-gram language models do lead to improved translation quality, they still lack any generalization beyond the surface forms (Schwenk, 2007). Consider example (1), which is a short sentence fragment from the MT09 Arabic-English test set, with the corresponding machine translation output (1.b), from a phras"
D11-1080,A00-2018,0,0.062828,"g Local Language Models To build the local language models, we use the SRILM toolkit (Stolcke, 2002), which is commonly applied in speech recognition and statistical machine translation. While SRILM collects n-gram statistics from all n-grams occurring in a corpus to build a single global language model, we build a language model for each word-POS pair only using the ngrams within the proximity of occurrences for that word-POS pair in a POS-tagged corpus. This results in separate n-gram count files, which are then processed by SRILM to build the individual language models.4 Charniak’s parser (Charniak, 2000) is used to POS tag the corpus. 4 The pre-processing scripts are available at http://www. science.uva.nl/˜christof/locLM/. 873 3.3 Decoder Integration Several approaches that integrate POS language models have focused on n-best list re-ranking only (Hasan et al., 2006; Wang et al., 2007). Often this is due to the computational (and implementational) complexities of integrating more complex language models with the decoder, although it is expected that a tighter integration with the decoder itself leads to better improvements than n-best list re-ranking. Integrating our local language modeling"
D11-1080,N09-1025,0,0.0201839,"s straightforward. Our baseline decoder already uses SRILM’s API for computing word language model probabilities. Since SRILM supports arbitrarily many language models, local language models can be added using the same functionalities of SRILM’s API. For the experiments discussed in Section 4, we add about 150,000 local language models to the word model. All local language model probabilities are coupled with the same feature weight. Potentially, improvements could be gained from using separate weights for individual local models, but this would require an optimization procedure such as MIRA (Chiang et al., 2009), which can handle a larger number of features. During decoding no POS tagging ambiguities are resolved. Each target phrase is associated with its most likely POS tag sequence, given the source and target side of the phrase pair; see Section 2.1. 4 Experimental Setup Three approaches are compared in our experiments: the baseline system is a phrase-based statistical machine translation system (Koehn et al., 2003), very similar to Moses (Koehn et al., 2007), using a wordbased 5-gram language model. The second approach extends the baseline by including a 7-gram POSbased language model. The third"
D11-1080,P05-1063,0,0.0237825,"a positive impact. 6 Related Work The main goal of this paper is to show that by tying POS language models to lexical items, we get more accurate distributions for specific words. The work on factored language models (Bilmes and Kirchhoff, 2003) is related to our work to the extent that it also mixes POS tags with lexical information, albeit in a very different manner. Factored language models use more general representations, such as POS tags or stems, only during back-off. Kirchhoff and Yang (2005) applied factored language models to machine translation but the improvements were negligible. Collins et al. (2005) proposed a discriminative language modeling approach that uses mixtures of POS and surface information and showed that it leads to a reduction in speech recognition word er877 ror rates. On the other hand, their approach seems more suited for n-best list re-ranking and it is not clear whether those improvements carry over to machine translation. Li and Khudanpur (2008) adapted this discriminative approach to machine translation re-ranking but used surface forms only. Wang et al. (2007) and Zheng et al. (2008) use elaborately enriched representations, called super abstract role values (Wang an"
D11-1080,W06-2606,0,0.073555,"to build a single global language model, we build a language model for each word-POS pair only using the ngrams within the proximity of occurrences for that word-POS pair in a POS-tagged corpus. This results in separate n-gram count files, which are then processed by SRILM to build the individual language models.4 Charniak’s parser (Charniak, 2000) is used to POS tag the corpus. 4 The pre-processing scripts are available at http://www. science.uva.nl/˜christof/locLM/. 873 3.3 Decoder Integration Several approaches that integrate POS language models have focused on n-best list re-ranking only (Hasan et al., 2006; Wang et al., 2007). Often this is due to the computational (and implementational) complexities of integrating more complex language models with the decoder, although it is expected that a tighter integration with the decoder itself leads to better improvements than n-best list re-ranking. Integrating our local language modeling approach with a decoder is straightforward. Our baseline decoder already uses SRILM’s API for computing word language model probabilities. Since SRILM supports arbitrarily many language models, local language models can be added using the same functionalities of SRILM"
D11-1080,W98-1121,0,0.0600108,"ere V is the vocabulary, and n is the order of the language model. The vocabulary of POS models, (typically ranging between 40 and 100 tags), is much smaller than the vocabulary of a word model, which can easily approach a million words. Nevertheless, most POS language modeling approaches apply some form of smoothing to account for unseen events (Bonneau-Maynard et al., 2007). To deploy POS language models in machine translation, translation candidates need to be annotated with POS tags. Each target phrase e¯ in a phrase pair (f¯, e¯) can be associated with a number of POS tag sequences t¯e¯. Heeman (1998) shows that using the joint probability leads to improved perplexity for POS models. For machine translation one can sum over all possible tag sequences, as in Equation 4. p(e|f ) = arg maxe Background Typically, POS language models are used like wordbased language models. N-grams are extracted from a POS-tagged corpus and an n-gram language model is built from that. While word-based models estimate the probability of a string of m words by Equation 2, POS-based models estimate the probability of string of m POS tags by Equation 3. p(w1m ) ∝ p(tm 1 )∝ m Y i=1 m Y i=1 i−1 p(wi |wi−n+1 ) (2) p(t"
D11-1080,W05-0821,0,0.502992,"ress statements and accused him ... Clearly, the adjective “controversial” should precede the nouns “press statement”, but since the AFP and Xinhua portions of the Gigaword corpus, used to build the language model for the translation system, do not contain this surface n-gram, translations with obviously ungrammatical constructions such as (1.b) can result. For unseen n-grams, one would like to model adjectives as being likely to precede nouns in English, for example. A straightforward approach to address this is to exploit the part-of-speech (POS) tags of the target words during translation (Kirchhoff and Yang, 2005). Though models exploiting POS information are not expressive enough to model long-distance dependencies, they can account for locally ungrammatical constructions such as (1.b). Several attempts have been made to interpolate POS language models Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 869–879, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics with surface models. Under constrained data conditions, this can lead to improvements. But once larger amounts of training data are used, the gains obtained fro"
D11-1080,D07-1091,0,0.177215,"Missing"
D11-1080,N03-1017,0,0.0105355,"same feature weight. Potentially, improvements could be gained from using separate weights for individual local models, but this would require an optimization procedure such as MIRA (Chiang et al., 2009), which can handle a larger number of features. During decoding no POS tagging ambiguities are resolved. Each target phrase is associated with its most likely POS tag sequence, given the source and target side of the phrase pair; see Section 2.1. 4 Experimental Setup Three approaches are compared in our experiments: the baseline system is a phrase-based statistical machine translation system (Koehn et al., 2003), very similar to Moses (Koehn et al., 2007), using a wordbased 5-gram language model. The second approach extends the baseline by including a 7-gram POSbased language model. The third approach represents the work described in this paper, extending the baseline by including 4-gram local language models. Translation quality is evaluated for two language pairs: Arabic-to-English and Chinese-to-English. NIST’s MT-Eval test sets are used for both pairs. Only resources allowed under NIST’s constrained data conditions are used to train the language, translation, and lexicalized distortion models. To"
D11-1080,2005.iwslt-1.8,0,0.0750383,"ey significantly improve quality either. Koehn and Hoang (2007) have reported an increase of 0.86 BLEU points for German-to-English translation for small training data. After relaxing phrase-matching to include lemma and morphological information on the source side, POS language models lead to a decrease of -0.42 BLEU points. Supertagging encapsulates more contextual information than POS tags and Birch et al. (2007) report improvements when comparing a supertag language model to a baseline using a word language model only. Once the baseline incorporates lexicalized distortion (Tillmann, 2004; Koehn et al., 2005), these improvements disappear. Factored language models have not resulted in significant improvements either. Kirchhoff and Yang (2005) report slight improvements when re-ranking the n-best lists of their decoder, which word tri-grams. But these improvements are less than those gained by re-ranking the n-best lists with a 4-gram word language model. The impact of POS language models depends among other things on the size of the parallel corpus, the size and order of the word language model, and whether lexicalized distortion models are used. To gauge the potential effectiveness of POS languag"
D11-1080,W08-0318,0,0.0182289,"∝ p(tm 1 )∝ m Y i=1 m Y i=1 i−1 p(wi |wi−n+1 ) (2) p(ti |ti−1 i−n+1 ) (3) where, n is the order of the language model, and wij refers to the sub-sequence of words (or tags) from positions i to j. Word language models can be built directly from large text corpora, such as LDC’s Gigaword corpus, but POS models require texts that are annotated with POS tags. Ideally, one would use manually annotated corpora such as the Penn Treebank (Marcus et al., 1993), but since those tend to be small, most approaches rely on larger corpora which have been automatically annotated by a POS tagger or a parser (Koehn et al., 2008). Though automated annotation 870 X p(e, t|f ) (4) t Summing over all possible tag sequences has the disadvantage that it requires one to keep this information during decoding. Below, we opt for an approximate solution, where each target phrase is annotated with the most likely POS tag sequence given the source and target phrase: t¯e¯ = arg maxt¯ p(t¯|¯ e, f¯). 2.2 Effectiveness of POS Language Models Reported results on the effectiveness of POS language models for machine translation are mixed, in particular when translating into languages that are not morphologically rich, such as English. W"
D11-1080,W04-3250,0,0.0220795,"13a, where the brevity penalty is based on the reference translation with the closest length, and translation error rate (TER) version 0.7.25 (Snover et al., 2006). All results reported here are case-insensitive. TER scores are shown as 1-TER. To see whether the differences between the approaches we compared in our experiments are statistically significant, we apply approximate randomization (Noreen, 1989); Riezler and Maxwell (2005) have shown that approximate randomization is less sensitive to Type-I errors, i.e., less likely to falsely reject the null hypothesis, than bootstrap resampling (Koehn, 2004) in the context of machine translation. 5 Results and Analysis The Arabic-to-English results are shown in Table 2, and the Chinese-to-English results in Table 3. All results are subdivided by genre following NIST’s genre classification. Note that MT06 conLDC2004T17, LDC2004T18, LDC2005E46, LDC2005E83, LDC2006E25, LDC2006E34, LDC2006E85, LDC2006E92, and LDC2007T08. For Chinese-English: LDC2002E18, LDC2003E07, LDC2003E14, LDC2005E83, LDC2005T06, LDC2006E34, LDC2006E85, and LDC2006E92. systems and improvements MT04 MT05 tune NW 1a 2a 3a 4a 5a 6a wordLM +posLM > wordLM +locLM > wordLM > +posLM 51."
D11-1080,C88-1071,0,0.298712,"andard POS language models. Instead of using one global POS language model that is built by using all of a mono-lingual corpus in the target language, we build individual models, or local models, for each word-POS pair using the POS tags surrounding each occurrence of that pair. This adds an aspect of lexicalization that is entirely absent in previous POS language models. The effect is that the resulting ngram probability distributions of each local model are more biased towards the contextual constraints of each individual word-POS pair. This is similar to the idea of cached language models (Kuhn, 1988), but more fine-grained and with a tighter integration of POS and lexical information. 3.1 Definition of Local Language Models Each conditional probability of order n in a local model for the word-POS pair w : t is of the form: pw:t (tn , pn |t1 : p1 , . . . , tn−1 : pn−1 ) where ti refers to POS tags and pi to positions relative to an occurrence of the pair (w : t). For example, consider the sentence fragment in Figure 1. The conditional local n-gram probabilities (a–d) are generated from the occurrence of the word told with POS tag VBD. Probability (c) in Figure 1 estimates that a word with"
D11-1080,2008.amta-papers.12,0,0.036974,". Factored language models use more general representations, such as POS tags or stems, only during back-off. Kirchhoff and Yang (2005) applied factored language models to machine translation but the improvements were negligible. Collins et al. (2005) proposed a discriminative language modeling approach that uses mixtures of POS and surface information and showed that it leads to a reduction in speech recognition word er877 ror rates. On the other hand, their approach seems more suited for n-best list re-ranking and it is not clear whether those improvements carry over to machine translation. Li and Khudanpur (2008) adapted this discriminative approach to machine translation re-ranking but used surface forms only. Wang et al. (2007) and Zheng et al. (2008) use elaborately enriched representations, called super abstract role values (Wang and Harper, 2002), which capture contextual dependencies using lexical categories, role labels, and dependency grammar structures. So far their approach has been limited to re-ranking n-best lists only. 7 Conclusion Though POS language models do not lead to significant improvements over a competitive baseline, we have shown that a competitive phrase-based baseline system"
D11-1080,J93-2004,0,0.0357375,"based models estimate the probability of a string of m words by Equation 2, POS-based models estimate the probability of string of m POS tags by Equation 3. p(w1m ) ∝ p(tm 1 )∝ m Y i=1 m Y i=1 i−1 p(wi |wi−n+1 ) (2) p(ti |ti−1 i−n+1 ) (3) where, n is the order of the language model, and wij refers to the sub-sequence of words (or tags) from positions i to j. Word language models can be built directly from large text corpora, such as LDC’s Gigaword corpus, but POS models require texts that are annotated with POS tags. Ideally, one would use manually annotated corpora such as the Penn Treebank (Marcus et al., 1993), but since those tend to be small, most approaches rely on larger corpora which have been automatically annotated by a POS tagger or a parser (Koehn et al., 2008). Though automated annotation 870 X p(e, t|f ) (4) t Summing over all possible tag sequences has the disadvantage that it requires one to keep this information during decoding. Below, we opt for an approximate solution, where each target phrase is annotated with the most likely POS tag sequence given the source and target phrase: t¯e¯ = arg maxt¯ p(t¯|¯ e, f¯). 2.2 Effectiveness of POS Language Models Reported results on the effectiv"
D11-1080,P03-1021,0,0.042962,"ring, all with respect to the previous and next phrase (Koehn et al., 2005). The distortion limit is set to 5 for Arabicto-English, and 6 for Chinese-to-English. For each source phrase the top 30 translations are considered. For tuning and testing we use NIST’s official MTEval test sets. MT04 was used as the development set for both language pairs. Testing was carried out on MT05 to MT09 for Arabic-English and MT05 to MT08 for Chinese-English. NIST did not release a new Chinese-English test set for MT-Eval 2009. Parameter tuning of the decoder was done with minimum error rate training (MERT) (Och, 2003), adapted to BLEU maximization. As evaluation metrics we used NIST’s adaptation of BLEU-4 (Papineni et al., 2001), version 13a, where the brevity penalty is based on the reference translation with the closest length, and translation error rate (TER) version 0.7.25 (Snover et al., 2006). All results reported here are case-insensitive. TER scores are shown as 1-TER. To see whether the differences between the approaches we compared in our experiments are statistically significant, we apply approximate randomization (Noreen, 1989); Riezler and Maxwell (2005) have shown that approximate randomizati"
D11-1080,2001.mtsummit-papers.68,0,0.0158312,"set to 5 for Arabicto-English, and 6 for Chinese-to-English. For each source phrase the top 30 translations are considered. For tuning and testing we use NIST’s official MTEval test sets. MT04 was used as the development set for both language pairs. Testing was carried out on MT05 to MT09 for Arabic-English and MT05 to MT08 for Chinese-English. NIST did not release a new Chinese-English test set for MT-Eval 2009. Parameter tuning of the decoder was done with minimum error rate training (MERT) (Och, 2003), adapted to BLEU maximization. As evaluation metrics we used NIST’s adaptation of BLEU-4 (Papineni et al., 2001), version 13a, where the brevity penalty is based on the reference translation with the closest length, and translation error rate (TER) version 0.7.25 (Snover et al., 2006). All results reported here are case-insensitive. TER scores are shown as 1-TER. To see whether the differences between the approaches we compared in our experiments are statistically significant, we apply approximate randomization (Noreen, 1989); Riezler and Maxwell (2005) have shown that approximate randomization is less sensitive to Type-I errors, i.e., less likely to falsely reject the null hypothesis, than bootstrap re"
D11-1080,2008.amta-papers.16,0,0.0381294,"Missing"
D11-1080,W05-0908,0,0.169373,"coder was done with minimum error rate training (MERT) (Och, 2003), adapted to BLEU maximization. As evaluation metrics we used NIST’s adaptation of BLEU-4 (Papineni et al., 2001), version 13a, where the brevity penalty is based on the reference translation with the closest length, and translation error rate (TER) version 0.7.25 (Snover et al., 2006). All results reported here are case-insensitive. TER scores are shown as 1-TER. To see whether the differences between the approaches we compared in our experiments are statistically significant, we apply approximate randomization (Noreen, 1989); Riezler and Maxwell (2005) have shown that approximate randomization is less sensitive to Type-I errors, i.e., less likely to falsely reject the null hypothesis, than bootstrap resampling (Koehn, 2004) in the context of machine translation. 5 Results and Analysis The Arabic-to-English results are shown in Table 2, and the Chinese-to-English results in Table 3. All results are subdivided by genre following NIST’s genre classification. Note that MT06 conLDC2004T17, LDC2004T18, LDC2005E46, LDC2005E83, LDC2006E25, LDC2006E34, LDC2006E85, LDC2006E92, and LDC2007T08. For Chinese-English: LDC2002E18, LDC2003E07, LDC2003E14, L"
D11-1080,D09-1008,0,0.029175,"Missing"
D11-1080,2006.amta-papers.25,0,0.0135713,"l test sets. MT04 was used as the development set for both language pairs. Testing was carried out on MT05 to MT09 for Arabic-English and MT05 to MT08 for Chinese-English. NIST did not release a new Chinese-English test set for MT-Eval 2009. Parameter tuning of the decoder was done with minimum error rate training (MERT) (Och, 2003), adapted to BLEU maximization. As evaluation metrics we used NIST’s adaptation of BLEU-4 (Papineni et al., 2001), version 13a, where the brevity penalty is based on the reference translation with the closest length, and translation error rate (TER) version 0.7.25 (Snover et al., 2006). All results reported here are case-insensitive. TER scores are shown as 1-TER. To see whether the differences between the approaches we compared in our experiments are statistically significant, we apply approximate randomization (Noreen, 1989); Riezler and Maxwell (2005) have shown that approximate randomization is less sensitive to Type-I errors, i.e., less likely to falsely reject the null hypothesis, than bootstrap resampling (Koehn, 2004) in the context of machine translation. 5 Results and Analysis The Arabic-to-English results are shown in Table 2, and the Chinese-to-English results i"
D11-1080,N04-4026,0,0.187686,"Missing"
D11-1080,W02-1031,0,0.0287632,"(2005) proposed a discriminative language modeling approach that uses mixtures of POS and surface information and showed that it leads to a reduction in speech recognition word er877 ror rates. On the other hand, their approach seems more suited for n-best list re-ranking and it is not clear whether those improvements carry over to machine translation. Li and Khudanpur (2008) adapted this discriminative approach to machine translation re-ranking but used surface forms only. Wang et al. (2007) and Zheng et al. (2008) use elaborately enriched representations, called super abstract role values (Wang and Harper, 2002), which capture contextual dependencies using lexical categories, role labels, and dependency grammar structures. So far their approach has been limited to re-ranking n-best lists only. 7 Conclusion Though POS language models do not lead to significant improvements over a competitive baseline, we have shown that a competitive phrase-based baseline system can benefit from using POS information by building lexically anchored local models. Our local model approach does not only lead to more contextspecific probability distributions, but also takes advantage of the language model probability of ea"
D11-1080,P02-1040,0,\N,Missing
D11-1080,P93-1032,0,\N,Missing
D11-1080,P07-2045,0,\N,Missing
D11-1080,N04-1021,0,\N,Missing
D14-1175,D13-1106,0,0.0245771,"the forms observed in the target monolingual data, but not to those aligned to a source word s, which opens the possibility of generating inflected forms that are missing from the translation models. We leave this possibility to future work. 1678 have been used for monolingual target language modeling (Schwenk et al., 2006; Le et al., 2011; Duh et al., 2013; Vaswani et al., 2013), n-gram translation modeling (Son et al., 2012), phrase translation modeling (Schwenk, 2012; Zou et al., 2013; Gao et al., 2014) and minimal translation modeling (Hu et al., 2014). The recurrent neural network LMs of Auli et al. (2013) are primarily trained to predict target word sequences. However, they also experiment with an additional input layer representing source side context. Our models differ from most previous work in neural language modeling in that we predict a target translation given a source context while previous models predict the next word given a target word history. Unlike previous work in phrase translation modeling with NNs, our models have the advantage of accessing source context that can fall outside the phrase boundaries. We now describe our models in a general setting, predicting target translatio"
D14-1175,C14-1181,1,0.896409,"Missing"
D14-1175,W12-3102,1,0.882805,"Missing"
D14-1175,D13-1174,0,0.43889,"14 Association for Computational Linguistics pairs where the source language is morphologically poor, such as English, and the target language is morphologically rich, such as Russian, i.e., language pairs with a high degree of surface realization ambiguity (Minkov et al., 2007). To address this problem we propose a general approach based on bilingual neural networks (BNN) exploiting source-side contextual information. This paper makes a number of contributions: Unlike previous approaches our models do not require any form of linguistic annotation (Minkov et al., 2007; Kholy and Habash, 2012; Chahuneau et al., 2013), nor do they require any feature engineering (Gimpel and Smith, 2008). Moreover, besides directly predicting fully inflected forms as Jeong et al. (2010), our approach can also model stem and suffix prediction explicitly. Prediction accuracy is evaluated with respect to three morphologically rich target languages (Bulgarian, Czech, and Russian) showing that our approach consistently yields substantial improvements over a competitive baseline. We also show that these improvements in prediction accuracy can be beneficial in an end-to-end machine translation scenario by integrating into a large-"
D14-1175,W12-3125,0,0.0176506,"otherwise where a is the word-level alignment of the phrase pair (˜ s, t˜) and {ai } is the set of target positions aligned to si . If a source-target link cannot be scored by the BNN model, we give it a PBNN probability of 1 and increment a separate count feature ε. Note that the same phrase pair can get different BNN scores if used in different source side contexts. Our baseline is an in-house phrase-based (Koehn et al., 2003) statistical machine translation system very similar to Moses (Koehn et al., 2007). All system runs use hierarchical lexicalized reordering (Galley and Manning, 2008; Cherry et al., 2012), distinguishing between monotone, swap, and discontinuous reordering, all with respect to left-to-right and right-to-left decoding. Other features include linear distortion, bidirectional lexical weighting (Koehn et al., 2003), word and phrase penalties, and finally a word-level 5gram target LM trained on all available monolingual data with modified Kneser-Ney smoothing (Chen and Goodman, 1999). The distortion 1683 Corpus paral.train Wiki dict. mono.train WMT2012 WMT2013 Lang. EN RU EN/RU RU EN #Sent. 1.9M 508K 21.0M 3K 3K SMT system Baseline + stem/suff. BNN Base+suffLM + word BNN + stem/suf"
D14-1175,P13-2119,0,0.0251887,"en applied to several natural language processing tasks (Bengio et al., 2003; Collobert and Weston, 2008; Socher et al., 2011; Socher et al., 2012). Within statistical machine translation, they 4 Note that our suffix generation function Mσ is restricted to the forms observed in the target monolingual data, but not to those aligned to a source word s, which opens the possibility of generating inflected forms that are missing from the translation models. We leave this possibility to future work. 1678 have been used for monolingual target language modeling (Schwenk et al., 2006; Le et al., 2011; Duh et al., 2013; Vaswani et al., 2013), n-gram translation modeling (Son et al., 2012), phrase translation modeling (Schwenk, 2012; Zou et al., 2013; Gao et al., 2014) and minimal translation modeling (Hu et al., 2014). The recurrent neural network LMs of Auli et al. (2013) are primarily trained to predict target word sequences. However, they also experiment with an additional input layer representing source side context. Our models differ from most previous work in neural language modeling in that we predict a target translation given a source context while previous models predict the next word given a targ"
D14-1175,N13-1001,0,0.0150919,"significant improvements in word translation prediction accuracy for three morphologically rich target languages. In addition, preliminary results for integrating our approach into a largescale English-Russian statistical machine translation system show small but statistically significant improvements in translation quality. 1 Introduction The ability to make context-sensitive translation decisions is one of the major strengths of phrasebased SMT (PSMT). However, the way PSMT exploits source-language context has several limitations as pointed out, for instance, by Quirk and Menezes (2006) and Durrani et al. (2013). First, the amount of context used to translate a given input word depends on the phrase segmentation, with hypotheses resulting from different segmentations competing with one another. Another issue is that, given a phrase segmentation, each source phrase is translated independently from the others, which can be problematic especially for short phrases. As a result, the predictive translation of a source phrase does not access useful linguistic clues in the source sentence that are outside of the scope of the phrase. Lexical weighting tackles the problem of unreliable phrase probabilities, t"
D14-1175,D08-1089,0,0.0321145,"(NULL|si ) if |{ai } |> 0 otherwise where a is the word-level alignment of the phrase pair (˜ s, t˜) and {ai } is the set of target positions aligned to si . If a source-target link cannot be scored by the BNN model, we give it a PBNN probability of 1 and increment a separate count feature ε. Note that the same phrase pair can get different BNN scores if used in different source side contexts. Our baseline is an in-house phrase-based (Koehn et al., 2003) statistical machine translation system very similar to Moses (Koehn et al., 2007). All system runs use hierarchical lexicalized reordering (Galley and Manning, 2008; Cherry et al., 2012), distinguishing between monotone, swap, and discontinuous reordering, all with respect to left-to-right and right-to-left decoding. Other features include linear distortion, bidirectional lexical weighting (Koehn et al., 2003), word and phrase penalties, and finally a word-level 5gram target LM trained on all available monolingual data with modified Kneser-Ney smoothing (Chen and Goodman, 1999). The distortion 1683 Corpus paral.train Wiki dict. mono.train WMT2012 WMT2013 Lang. EN RU EN/RU RU EN #Sent. 1.9M 508K 21.0M 3K 3K SMT system Baseline + stem/suff. BNN Base+suffLM"
D14-1175,P14-1066,0,0.0291014,"thin statistical machine translation, they 4 Note that our suffix generation function Mσ is restricted to the forms observed in the target monolingual data, but not to those aligned to a source word s, which opens the possibility of generating inflected forms that are missing from the translation models. We leave this possibility to future work. 1678 have been used for monolingual target language modeling (Schwenk et al., 2006; Le et al., 2011; Duh et al., 2013; Vaswani et al., 2013), n-gram translation modeling (Son et al., 2012), phrase translation modeling (Schwenk, 2012; Zou et al., 2013; Gao et al., 2014) and minimal translation modeling (Hu et al., 2014). The recurrent neural network LMs of Auli et al. (2013) are primarily trained to predict target word sequences. However, they also experiment with an additional input layer representing source side context. Our models differ from most previous work in neural language modeling in that we predict a target translation given a source context while previous models predict the next word given a target word history. Unlike previous work in phrase translation modeling with NNs, our models have the advantage of accessing source context that can fall o"
D14-1175,W08-0302,0,0.248829,"oblem of unreliable phrase probabilities, typically associated with long phrases, but does not alleviate the problem of context segmentation. An important share of the translation selection task is then left to the language model (LM), which is certainly very effective but can only leverage target language context. Moreover, decisions that are taken at early decoding stages—such as the common practice of retaining only top n translation options for each source span—depend only on the translation models and on the target context available in the phrase. Source context based translation models (Gimpel and Smith, 2008; Mauser et al., 2009; Jeong et al., 2010; Haque et al., 2011) naturally address these limitations. These models can exploit a boundless context of the input text, but they assume that target words can be predicted independently from each other, which makes them easy to integrate into state-of-the-art PSMT systems. Even though the independence assumption is made on the target side, these models have shown the benefits of utilizing source context, especially in translating into morphologically rich languages. One drawback of previous research on this topic, though, is that it relied on rich set"
D14-1175,P12-1016,0,0.0442407,"coverage of SMT models The first question we ask is whether translation can be improved by a more accurate selection of the translation options already existing in the SMT models, as opposed to generating new options. To answer this question we measure the lexical coverage of a baseline PSMT system trained on English-Russian.1 We choose this language pair because of the morphological richness on the target side: Russian is characterized by a highly inflectional morphology with a particularly complex nominal declension (six core cases, three genders and two number categories). As suggested by Green and DeNero (2012), we compute the recall of reference tokens in the set of target tokens that the decoder could produce in a translation of the source, that is the target tokens of all phrase pairs that matched the input sentence 1 Training data and SMT setup are described in Section 6. and that were actually used for decoding.2 We call this the decoder’s lexical search space. Then, we compare the reference/space recall against the reference/MT-output recall: that is, the percentage of reference tokens that also appeared in the 1-best translation output by the SMT system. Results for the WMT12 benchmark are pr"
D14-1175,D11-1125,0,0.0373637,"Missing"
D14-1175,E14-1003,0,0.0151954,"at our suffix generation function Mσ is restricted to the forms observed in the target monolingual data, but not to those aligned to a source word s, which opens the possibility of generating inflected forms that are missing from the translation models. We leave this possibility to future work. 1678 have been used for monolingual target language modeling (Schwenk et al., 2006; Le et al., 2011; Duh et al., 2013; Vaswani et al., 2013), n-gram translation modeling (Son et al., 2012), phrase translation modeling (Schwenk, 2012; Zou et al., 2013; Gao et al., 2014) and minimal translation modeling (Hu et al., 2014). The recurrent neural network LMs of Auli et al. (2013) are primarily trained to predict target word sequences. However, they also experiment with an additional input layer representing source side context. Our models differ from most previous work in neural language modeling in that we predict a target translation given a source context while previous models predict the next word given a target word history. Unlike previous work in phrase translation modeling with NNs, our models have the advantage of accessing source context that can fall outside the phrase boundaries. We now describe our m"
D14-1175,2010.amta-papers.33,0,0.269594,"ically associated with long phrases, but does not alleviate the problem of context segmentation. An important share of the translation selection task is then left to the language model (LM), which is certainly very effective but can only leverage target language context. Moreover, decisions that are taken at early decoding stages—such as the common practice of retaining only top n translation options for each source span—depend only on the translation models and on the target context available in the phrase. Source context based translation models (Gimpel and Smith, 2008; Mauser et al., 2009; Jeong et al., 2010; Haque et al., 2011) naturally address these limitations. These models can exploit a boundless context of the input text, but they assume that target words can be predicted independently from each other, which makes them easy to integrate into state-of-the-art PSMT systems. Even though the independence assumption is made on the target side, these models have shown the benefits of utilizing source context, especially in translating into morphologically rich languages. One drawback of previous research on this topic, though, is that it relied on rich sets of manually designed features, which in"
D14-1175,D13-1176,0,0.0565001,"g row in the previous layer resulting in a weight matrix whose number of columns decreases by one. Thus after 2k convolutional layers, the network transforms the source context matrix mcsi to a feature vector q ˆ ∈ Rd . A fully connected layer with weight matrix W followed by a softmax layer are placed after the last convolutional layer L2k to perform classification. The parameters of the convolutional neural network model are θ = {rsi , mj , W}. Here, we focus on a fixed length input, however convolutional neural networks may be used to model variable length input (Kalchbrenner et al., 2014; Kalchbrenner and Blunsom, 2013). 4.3 Training In training, for each example (t, cs ), we maximize the conditional probability Pθ (t|cs ) of a correct target label t. The contribution of the training example (t, cs ) to the gradient of the log conditional probability is given by: ∂ ∂ log Pθ (t|cs ) = sθ (t|cs ) ∂θ ∂θ X ∂ − Pθ (t0 |cs ) sθ (t0 , cs ) ∂θ 0 t ∈Ts Note that in the gradient, we do not sum over all target translations T but a set of possible candidates Ts of a source word s. In practice |Ts |≤ 200 with our pruning settings (see Section 5.1), thus training time for one example does not depend on the vocabulary size"
D14-1175,P14-1062,0,0.203091,"a natural mechanism to compute word surface conditional probability p(t|cs ) given individual stem probability p(σ|cs ) and suffix probability p(µ|cs , σ), and (ii) FFNNs do not provide the flexibility to capture long dependencies among words if they lie outside the source context window. Hence, we consider two BNN variants: a log-bilinear model (LBL) and a convolutional neural network model (ConvNet). LBL could potentially address (i) by factorizing target representations into target stem and suffix representations whereas ConvNets offer the advantage of modeling variable input length (ii) (Kalchbrenner et al., 2014). Log-bilinear model. The FFNN models stem and suffix probabilities separately. A log-bilinear model instead could directly model word prediction through a factored representation of target words, similarly to Botha and Blunsom (2014). Thus, no probability mass would be wasted over stem-suffix combinations that are not in the candidate generation function. The LBL model specifies the conditional distribution for the word translation tj ∈ Tsi given a source context csi : exp(sθ (tj , csi )) Pθ (tj |csi ) = P exp(sθ (t0j , csi )) (3) t0j ∈Tsi We use an additional set of word representations qtj"
D14-1175,2012.eamt-1.6,0,0.0373511,"Missing"
D14-1175,N03-1017,0,0.00693631,"air matching the input, the phrase BNN score PBNN-p is computed as follows: PBNN-p (˜ s, t˜, a) =  X 1 |˜ s | PBNN (tj |csi )  Y |{ai }| j∈{ai }  i=1  Pmle (NULL|si ) if |{ai } |> 0 otherwise where a is the word-level alignment of the phrase pair (˜ s, t˜) and {ai } is the set of target positions aligned to si . If a source-target link cannot be scored by the BNN model, we give it a PBNN probability of 1 and increment a separate count feature ε. Note that the same phrase pair can get different BNN scores if used in different source side contexts. Our baseline is an in-house phrase-based (Koehn et al., 2003) statistical machine translation system very similar to Moses (Koehn et al., 2007). All system runs use hierarchical lexicalized reordering (Galley and Manning, 2008; Cherry et al., 2012), distinguishing between monotone, swap, and discontinuous reordering, all with respect to left-to-right and right-to-left decoding. Other features include linear distortion, bidirectional lexical weighting (Koehn et al., 2003), word and phrase penalties, and finally a word-level 5gram target LM trained on all available monolingual data with modified Kneser-Ney smoothing (Chen and Goodman, 1999). The distortio"
D14-1175,P07-2045,0,0.00446829,"(˜ s, t˜, a) =  X 1 |˜ s | PBNN (tj |csi )  Y |{ai }| j∈{ai }  i=1  Pmle (NULL|si ) if |{ai } |> 0 otherwise where a is the word-level alignment of the phrase pair (˜ s, t˜) and {ai } is the set of target positions aligned to si . If a source-target link cannot be scored by the BNN model, we give it a PBNN probability of 1 and increment a separate count feature ε. Note that the same phrase pair can get different BNN scores if used in different source side contexts. Our baseline is an in-house phrase-based (Koehn et al., 2003) statistical machine translation system very similar to Moses (Koehn et al., 2007). All system runs use hierarchical lexicalized reordering (Galley and Manning, 2008; Cherry et al., 2012), distinguishing between monotone, swap, and discontinuous reordering, all with respect to left-to-right and right-to-left decoding. Other features include linear distortion, bidirectional lexical weighting (Koehn et al., 2003), word and phrase penalties, and finally a word-level 5gram target LM trained on all available monolingual data with modified Kneser-Ney smoothing (Chen and Goodman, 1999). The distortion 1683 Corpus paral.train Wiki dict. mono.train WMT2012 WMT2013 Lang. EN RU EN/RU"
D14-1175,koen-2004-pharaoh,0,0.0681058,"logy, generation seems to be of secondary importance compared to better selection in our experimental setup. 3 Predicting word translations in context It is standard practice in PSMT to use wordto-word translation probabilities as an additional phrase score. More specifically, state-of-the-art PSMT systems employ the maximum-likelihood estimate of the context-independent probability of a target word given its aligned source word P (tj |si ) for each word alignment link aij . 2 This corresponds to the top 30 phrases sorted by weighted phrase, lexical and LM probabilities, for each source span. Koehn (2004) and our own experience suggest that using more phrases has little or no impact on MT quality. 3 Word segmentation for this analysis is performed by the Russian Snowball stemmer, see also Section 5.3. 1677 1. predict target stem σ given source context cs ; [конституционность] [индиана закон] [.] 2. predict target suffix µ given source context cs and target stem σ. [the constitutionality of the] [indiana law] [.] Figure 1: Fragment of English sentence and its incorrect Russian translation produced by the baseline SMT system. Square brackets indicate phrase boundaries. The main goal of our work"
D14-1175,W11-0301,0,0.0699695,"Missing"
D14-1175,D09-1022,0,0.120957,"Missing"
D14-1175,P07-1017,0,0.0742988,"his paper, we specifically focus on improving the prediction accuracy for word translations. Achieving high levels of word translation accuracy is particularly challenging for language 1676 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1676–1688, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics pairs where the source language is morphologically poor, such as English, and the target language is morphologically rich, such as Russian, i.e., language pairs with a high degree of surface realization ambiguity (Minkov et al., 2007). To address this problem we propose a general approach based on bilingual neural networks (BNN) exploiting source-side contextual information. This paper makes a number of contributions: Unlike previous approaches our models do not require any form of linguistic annotation (Minkov et al., 2007; Kholy and Habash, 2012; Chahuneau et al., 2013), nor do they require any feature engineering (Gimpel and Smith, 2008). Moreover, besides directly predicting fully inflected forms as Jeong et al. (2010), our approach can also model stem and suffix prediction explicitly. Prediction accuracy is evaluated"
D14-1175,N12-1043,0,0.064525,"Missing"
D14-1175,N06-1002,0,0.0284164,"ature engineering. We report significant improvements in word translation prediction accuracy for three morphologically rich target languages. In addition, preliminary results for integrating our approach into a largescale English-Russian statistical machine translation system show small but statistically significant improvements in translation quality. 1 Introduction The ability to make context-sensitive translation decisions is one of the major strengths of phrasebased SMT (PSMT). However, the way PSMT exploits source-language context has several limitations as pointed out, for instance, by Quirk and Menezes (2006) and Durrani et al. (2013). First, the amount of context used to translate a given input word depends on the phrase segmentation, with hypotheses resulting from different segmentations competing with one another. Another issue is that, given a phrase segmentation, each source phrase is translated independently from the others, which can be problematic especially for short phrases. As a result, the predictive translation of a source phrase does not access useful linguistic clues in the source sentence that are outside of the scope of the phrase. Lexical weighting tackles the problem of unreliab"
D14-1175,W05-0908,0,0.0753458,"Missing"
D14-1175,P06-2093,0,0.031168,"ion over the last few years and have been applied to several natural language processing tasks (Bengio et al., 2003; Collobert and Weston, 2008; Socher et al., 2011; Socher et al., 2012). Within statistical machine translation, they 4 Note that our suffix generation function Mσ is restricted to the forms observed in the target monolingual data, but not to those aligned to a source word s, which opens the possibility of generating inflected forms that are missing from the translation models. We leave this possibility to future work. 1678 have been used for monolingual target language modeling (Schwenk et al., 2006; Le et al., 2011; Duh et al., 2013; Vaswani et al., 2013), n-gram translation modeling (Son et al., 2012), phrase translation modeling (Schwenk, 2012; Zou et al., 2013; Gao et al., 2014) and minimal translation modeling (Hu et al., 2014). The recurrent neural network LMs of Auli et al. (2013) are primarily trained to predict target word sequences. However, they also experiment with an additional input layer representing source side context. Our models differ from most previous work in neural language modeling in that we predict a target translation given a source context while previous models"
D14-1175,C12-2104,0,0.0229426,"., 2011; Socher et al., 2012). Within statistical machine translation, they 4 Note that our suffix generation function Mσ is restricted to the forms observed in the target monolingual data, but not to those aligned to a source word s, which opens the possibility of generating inflected forms that are missing from the translation models. We leave this possibility to future work. 1678 have been used for monolingual target language modeling (Schwenk et al., 2006; Le et al., 2011; Duh et al., 2013; Vaswani et al., 2013), n-gram translation modeling (Son et al., 2012), phrase translation modeling (Schwenk, 2012; Zou et al., 2013; Gao et al., 2014) and minimal translation modeling (Hu et al., 2014). The recurrent neural network LMs of Auli et al. (2013) are primarily trained to predict target word sequences. However, they also experiment with an additional input layer representing source side context. Our models differ from most previous work in neural language modeling in that we predict a target translation given a source context while previous models predict the next word given a target word history. Unlike previous work in phrase translation modeling with NNs, our models have the advantage of acc"
D14-1175,sharoff-etal-2008-designing,0,0.0917098,"Missing"
D14-1175,P05-1044,0,0.018985,"ditional probability Pθ (t|cs ) of a correct target label t. The contribution of the training example (t, cs ) to the gradient of the log conditional probability is given by: ∂ ∂ log Pθ (t|cs ) = sθ (t|cs ) ∂θ ∂θ X ∂ − Pθ (t0 |cs ) sθ (t0 , cs ) ∂θ 0 t ∈Ts Note that in the gradient, we do not sum over all target translations T but a set of possible candidates Ts of a source word s. In practice |Ts |≤ 200 with our pruning settings (see Section 5.1), thus training time for one example does not depend on the vocabulary size. Our training criterion can be seen as a form of contrastive estimation (Smith and Eisner, 2005), however we explicitly move the probability mass from competing candidates to the correct translation candidate, thus obtaining more reliable estimates of the conditional probabilities. The BNN parameters are initialized randomly according to a zero-mean Gaussian. We regularize the models with L2 . As an alternative to the L2 regularizer, we also experiment with dropout (Hinton et al., 2012), where the neurons are randomly zeroed out with dropout rate p. This technique is known to be useful in computer vision tasks but has been rarely used in NLP tasks. In FFNN, we use dropout after the hidde"
D14-1175,D11-1014,0,0.0704887,"nt for each of the modeled tasks, directly from word-aligned data. To make the approach completely language independent, the word segmentation function g can be trained with an unsupervised segmentation tool. The effects of using different word segmentation techniques are discussed in Section 5. 4 Bilingual neural networks for translation prediction Probabilistic neural network (NN), or continuous space, language models have received increasing attention over the last few years and have been applied to several natural language processing tasks (Bengio et al., 2003; Collobert and Weston, 2008; Socher et al., 2011; Socher et al., 2012). Within statistical machine translation, they 4 Note that our suffix generation function Mσ is restricted to the forms observed in the target monolingual data, but not to those aligned to a source word s, which opens the possibility of generating inflected forms that are missing from the translation models. We leave this possibility to future work. 1678 have been used for monolingual target language modeling (Schwenk et al., 2006; Le et al., 2011; Duh et al., 2013; Vaswani et al., 2013), n-gram translation modeling (Son et al., 2012), phrase translation modeling (Schwenk"
D14-1175,D12-1110,0,0.0428411,"deled tasks, directly from word-aligned data. To make the approach completely language independent, the word segmentation function g can be trained with an unsupervised segmentation tool. The effects of using different word segmentation techniques are discussed in Section 5. 4 Bilingual neural networks for translation prediction Probabilistic neural network (NN), or continuous space, language models have received increasing attention over the last few years and have been applied to several natural language processing tasks (Bengio et al., 2003; Collobert and Weston, 2008; Socher et al., 2011; Socher et al., 2012). Within statistical machine translation, they 4 Note that our suffix generation function Mσ is restricted to the forms observed in the target monolingual data, but not to those aligned to a source word s, which opens the possibility of generating inflected forms that are missing from the translation models. We leave this possibility to future work. 1678 have been used for monolingual target language modeling (Schwenk et al., 2006; Le et al., 2011; Duh et al., 2013; Vaswani et al., 2013), n-gram translation modeling (Son et al., 2012), phrase translation modeling (Schwenk, 2012; Zou et al., 20"
D14-1175,N12-1005,0,0.0302915,", 2003; Collobert and Weston, 2008; Socher et al., 2011; Socher et al., 2012). Within statistical machine translation, they 4 Note that our suffix generation function Mσ is restricted to the forms observed in the target monolingual data, but not to those aligned to a source word s, which opens the possibility of generating inflected forms that are missing from the translation models. We leave this possibility to future work. 1678 have been used for monolingual target language modeling (Schwenk et al., 2006; Le et al., 2011; Duh et al., 2013; Vaswani et al., 2013), n-gram translation modeling (Son et al., 2012), phrase translation modeling (Schwenk, 2012; Zou et al., 2013; Gao et al., 2014) and minimal translation modeling (Hu et al., 2014). The recurrent neural network LMs of Auli et al. (2013) are primarily trained to predict target word sequences. However, they also experiment with an additional input layer representing source side context. Our models differ from most previous work in neural language modeling in that we predict a target translation given a source context while previous models predict the next word given a target word history. Unlike previous work in phrase translation modeling wi"
D14-1175,W07-1709,0,0.0695006,"Missing"
D14-1175,P08-1059,0,0.187605,"rget words Ts = {t1 , . . . , tm } that were aligned to si in the parallel training corpus, which in turn corresponds to the set of target words that the SMT system can produce for a given source. In practice, we use a pruned version of Ts to speed up training and reduce noise (see details in Section 5). As for the morphological models, given Ts and g, we can obtain Ls = {σ1 , . . . , σk }, the set of possible target stem translations of s, and Mσ = {µ1 , . . . , µl }, the set of possible suffixes for a target stem σ. The use of Ls , and Mσ is similar to stemming and inflection operations in (Toutanova et al., 2008) while the set Ts is similar to the GEN function in (Jeong et al., 2010).4 Our approach differs crucially from previous work (Minkov et al., 2007; Chahuneau et al., 2013) in that it does not require linguistic features such as part-of-speech and syntactic tree on the source side. The proposed models automatically learn features that are relevant for each of the modeled tasks, directly from word-aligned data. To make the approach completely language independent, the word segmentation function g can be trained with an unsupervised segmentation tool. The effects of using different word segmentati"
D14-1175,D13-1140,0,0.0351327,"ral natural language processing tasks (Bengio et al., 2003; Collobert and Weston, 2008; Socher et al., 2011; Socher et al., 2012). Within statistical machine translation, they 4 Note that our suffix generation function Mσ is restricted to the forms observed in the target monolingual data, but not to those aligned to a source word s, which opens the possibility of generating inflected forms that are missing from the translation models. We leave this possibility to future work. 1678 have been used for monolingual target language modeling (Schwenk et al., 2006; Le et al., 2011; Duh et al., 2013; Vaswani et al., 2013), n-gram translation modeling (Son et al., 2012), phrase translation modeling (Schwenk, 2012; Zou et al., 2013; Gao et al., 2014) and minimal translation modeling (Hu et al., 2014). The recurrent neural network LMs of Auli et al. (2013) are primarily trained to predict target word sequences. However, they also experiment with an additional input layer representing source side context. Our models differ from most previous work in neural language modeling in that we predict a target translation given a source context while previous models predict the next word given a target word history. Unlike"
D14-1175,D13-1141,0,0.0350035,"et al., 2012). Within statistical machine translation, they 4 Note that our suffix generation function Mσ is restricted to the forms observed in the target monolingual data, but not to those aligned to a source word s, which opens the possibility of generating inflected forms that are missing from the translation models. We leave this possibility to future work. 1678 have been used for monolingual target language modeling (Schwenk et al., 2006; Le et al., 2011; Duh et al., 2013; Vaswani et al., 2013), n-gram translation modeling (Son et al., 2012), phrase translation modeling (Schwenk, 2012; Zou et al., 2013; Gao et al., 2014) and minimal translation modeling (Hu et al., 2014). The recurrent neural network LMs of Auli et al. (2013) are primarily trained to predict target word sequences. However, they also experiment with an additional input layer representing source side context. Our models differ from most previous work in neural language modeling in that we predict a target translation given a source context while previous models predict the next word given a target word history. Unlike previous work in phrase translation modeling with NNs, our models have the advantage of accessing source cont"
D14-1175,bojar-etal-2012-joy,0,\N,Missing
D14-1175,W13-2201,1,\N,Missing
D14-1176,C14-1181,1,0.827072,"ns (given the internal alignment information) and substitutes the words in the phrase pair with syntactic labels (given the source parse and the target POS labeling associated with the phrase). The new syntactified bilingual tokens are added to the stack of preceding n−1 tokens, and the feature function computes the weighted updated model probability. During decoding, the probabilities of the BiLMs are computed in a stream-based fashion, with bilingual tokens as string tokens, and not in a class-based fashion, with syntactic source-side representations emitting the corresponding target words (Bisazza and Monz, 2014). 4 4.1 Experiments Setup We conduct translation experiments with a baseline PBSMT system with additionally one of the dependency-based BiLM feature functions specified in Section 3. We compare the translation performance to a baseline PBSMT system and to a baseline augmented with the original BiLMs from (Niehues et al., 2011). Word-alignment is produced with GIZA++ (Och and Ney, 2003). We use an in-house implementation of a PBSMT system similar to Moses (Koehn et al., 2007). Our baseline contains all standard PBSMT features including language model, lexical weighting, and lexicalized reorderi"
D14-1176,W09-2307,0,0.173384,"ng set Source side of Ar-En set Target side of Ar-En set Source side of Ch-En set Target side of Ch-En set NNS exports VBD NNS NNP IN to Figure 4: Sequence of bilingual tokens produced by a Pos→Pos→Pos+sibl•Lex after translating three words of the source sentence: VBD→NNS→+NNS+IN•Egyptian, ROOT→VBD→ +NNS+•exports, VBD→NNS→NNP+IN+•to (if there is no sibling on either of the sides,  is returned). sentences are segmented according to Equation 3. We produce a dependency parse of a source sentence and a POS-tag labeling of a target sentence. For Chinese, we use the Stanford dependency parser (Chang et al., 2009). For Arabic a dependency parser is not available for public use, so we produce a constituency parse with the Stanford parser (Green and Manning, 2010) and extract dependencies based on the rules in Collins (1999). For English POS-tagging, we use the Stanford POS-tagger (Toutanova et al., 2003). After having produced a labeled sequence of tokens, we learn a 5-gram model using SRILM (Stolcke et al., 2011). Kneyser-Ney smoothing is used for all model variations except for Pos•Pos where Witten-Bell smoothing is used due to zero countof-counts. 3.4 N. of tokens 148M 146M 20M 28M Table 1: Training"
D14-1176,P08-1009,0,0.0815529,"ly used lexical information, although Crego and Yvon (2010a) and Crego and Yvon (2010b) label bilingual to1 Note that the standard PBSMT translation model assumes that events of translating separate phrases in a sentence are independent. 1689 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1689–1700, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics kens with a rich set of POS tags. But in general, reordering is considered to be a syntactic phenomenon and thus the relevant features are syntactic (Fox, 2002; Cherry, 2008). Syntactic information is incorporated in tree-based approaches in SMT, allowing one to provide a more detailed definition of translation events and to redefine decoding as parsing of a source string (Liu et al., 2006; Huang et al., 2006; Marton and Resnik, 2008), of a target string (Shen et al., 2008), or both (Chiang, 2007; Chiang, 2010). Reordering is a result of a given derivation, and CYK-based decoding used in tree-based approaches is more syntax-aware than the simple PBSMT decoding algorithm. Although tree-based approaches potentially offer a more accurate model of translation, they ar"
D14-1176,J07-2003,0,0.496294,"g (EMNLP), pages 1689–1700, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics kens with a rich set of POS tags. But in general, reordering is considered to be a syntactic phenomenon and thus the relevant features are syntactic (Fox, 2002; Cherry, 2008). Syntactic information is incorporated in tree-based approaches in SMT, allowing one to provide a more detailed definition of translation events and to redefine decoding as parsing of a source string (Liu et al., 2006; Huang et al., 2006; Marton and Resnik, 2008), of a target string (Shen et al., 2008), or both (Chiang, 2007; Chiang, 2010). Reordering is a result of a given derivation, and CYK-based decoding used in tree-based approaches is more syntax-aware than the simple PBSMT decoding algorithm. Although tree-based approaches potentially offer a more accurate model of translation, they are also a lot more complex and requiring more intricate optimization and estimation techniques (Huang and Mi, 2010). Our idea is to keep the simplicity of PBSMT but move towards the expressiveness typical of treebased models. We incrementally build up the syntactic representation of a translation during decoding by adding prec"
D14-1176,P10-1146,0,0.0202681,"es 1689–1700, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics kens with a rich set of POS tags. But in general, reordering is considered to be a syntactic phenomenon and thus the relevant features are syntactic (Fox, 2002; Cherry, 2008). Syntactic information is incorporated in tree-based approaches in SMT, allowing one to provide a more detailed definition of translation events and to redefine decoding as parsing of a source string (Liu et al., 2006; Huang et al., 2006; Marton and Resnik, 2008), of a target string (Shen et al., 2008), or both (Chiang, 2007; Chiang, 2010). Reordering is a result of a given derivation, and CYK-based decoding used in tree-based approaches is more syntax-aware than the simple PBSMT decoding algorithm. Although tree-based approaches potentially offer a more accurate model of translation, they are also a lot more complex and requiring more intricate optimization and estimation techniques (Huang and Mi, 2010). Our idea is to keep the simplicity of PBSMT but move towards the expressiveness typical of treebased models. We incrementally build up the syntactic representation of a translation during decoding by adding precomputed fragmen"
D14-1176,C10-2023,0,0.104201,"generalize the approach of Niehues et al. (2011) to investigate several variations of bilingual language models. Our method consists of labeling elementary translation events (tokens of bilingual LMs) with their different contextual properties. What kind of contextual information should be incorporated in a reordering model? Lexical information has been used by Tillmann (2004) but is known to suffer from data sparsity (Galley and Manning, 2008). Also previous contributions to bilingual language modeling (Marino et al., 2006; Niehues et al., 2011) have mostly used lexical information, although Crego and Yvon (2010a) and Crego and Yvon (2010b) label bilingual to1 Note that the standard PBSMT translation model assumes that events of translating separate phrases in a sentence are independent. 1689 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1689–1700, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics kens with a rich set of POS tags. But in general, reordering is considered to be a syntactic phenomenon and thus the relevant features are syntactic (Fox, 2002; Cherry, 2008). Syntactic information is incorporated in tr"
D14-1176,P11-1105,0,0.0168128,"increases the token vocabulary, and thus the model sparsity. Another disadvantage comes from the fact that we want to compare permutations of the same set of elements. For example, the two different segmentations of ba into [ba] and [b][a] still represent the same permutation of the sequence ab. In Figure 1 one can produce a segmentation of (AsEAr Albtrwl, oil prices) into (Albtrwl, oil) and (AsEAr, prices) or leave it as is. If we allow for both segmentations, the learnt probability parameters may be different for the sum of (Albtrwl, oil) and (AsEAr, prices) and for the unsegmented phrase. Durrani et al. (2011) introduce an alternative method for unambiguous bilingual segmentation where tokens are defined as minimal phrases, called minimal translation units (MTUs). Figure 1 compares the BiLM and MTU tokenization for a specific example. Since Niehues et al. (2011) have shown their model to work successfully as an additional feature in combination with commonly used standard phrase-based features, we use their approach as the main point of reference and base our approach on their segmentation method. In the rest of the text we refer to Niehues et al. (2011) as the original BiLM.4 At the same time, we"
D14-1176,P06-1067,0,0.189144,"machine translation (SMT) reordering (also called distortion) refers to the order in which source words are translated to generate the translation in the target language. Word orders can differ significantly across languages. For instance, Arabic declarative sentences can be verbinitial, while the corresponding English translation should realize the verb after the subject, hence requiring a reordering. Determining the correct reordering during decoding is a major challenge for SMT. This problem has received a lot of attention in the literature (see, e.g., Tillmann (2004), Zens and Ney (2003), Al-Onaizan and Papineni (2006)), as choosing the correct reordering improves readability of the translation and can have a substantial impact on translation quality (Birch, 2011). In this paper, we only consider those approaches that include a reordering feature function into the loglinear interpolation used during decoding. The simplest reordering model is linear distortion (Koehn et al., 2003) which scores the distance between phrases translated at steps t and t + 1 of the derivation. This model ignores any contextual information, as the distance between translated phrases is its only parameter. Lexical distortion modeli"
D14-1176,D08-1089,0,0.037528,"t al. (2006) used this kind of model as the translation model, while more recently it has been used as an additional model in PBSMT systems (Niehues et al., 2011). We adopt and generalize the approach of Niehues et al. (2011) to investigate several variations of bilingual language models. Our method consists of labeling elementary translation events (tokens of bilingual LMs) with their different contextual properties. What kind of contextual information should be incorporated in a reordering model? Lexical information has been used by Tillmann (2004) but is known to suffer from data sparsity (Galley and Manning, 2008). Also previous contributions to bilingual language modeling (Marino et al., 2006; Niehues et al., 2011) have mostly used lexical information, although Crego and Yvon (2010a) and Crego and Yvon (2010b) label bilingual to1 Note that the standard PBSMT translation model assumes that events of translating separate phrases in a sentence are independent. 1689 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1689–1700, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics kens with a rich set of POS tags. But in genera"
D14-1176,C10-1045,0,0.0247854,"Sequence of bilingual tokens produced by a Pos→Pos→Pos+sibl•Lex after translating three words of the source sentence: VBD→NNS→+NNS+IN•Egyptian, ROOT→VBD→ +NNS+•exports, VBD→NNS→NNP+IN+•to (if there is no sibling on either of the sides,  is returned). sentences are segmented according to Equation 3. We produce a dependency parse of a source sentence and a POS-tag labeling of a target sentence. For Chinese, we use the Stanford dependency parser (Chang et al., 2009). For Arabic a dependency parser is not available for public use, so we produce a constituency parse with the Stanford parser (Green and Manning, 2010) and extract dependencies based on the rules in Collins (1999). For English POS-tagging, we use the Stanford POS-tagger (Toutanova et al., 2003). After having produced a labeled sequence of tokens, we learn a 5-gram model using SRILM (Stolcke et al., 2011). Kneyser-Ney smoothing is used for all model variations except for Pos•Pos where Witten-Bell smoothing is used due to zero countof-counts. 3.4 N. of tokens 148M 146M 20M 28M Table 1: Training data for Arabic-English and Chinese-English experiments. … VBD N. of lines 4,376,320 4,376,320 2,104,652 2,104,652 Decoder integration Dependency-based"
D14-1176,N10-1129,0,0.0150092,"calized BiLM also helps to differentiate between word orders. For Chinese, Pos→Pos→Pos•Pos and the system combining the latter and Lex•Lex also obtain the best results. However, other dependency-based configurations do not outperform Lex•Lex. All the experiments so far were run with a distortion limit of 5. But both of the languages, especially Chinese, often require reorderings over a longer distance. We performed additional experiments with a distortion limit of 10 for the Lex•Lex and Pos→Pos→Pos•Pos systems (Tables 7 and 8). It is more difficult to translate with a higher distortion limit (Green et al., 2010) as the set of permutations grows larger thereby making it more difficult to differentiate between correct and incorrect continuations of the current hypothesis. It has also been noted that higher distortion limits are more likely to result in improvements for Chinese rather than Arabic to English translation (Chiang, 2007; Green et al., 2010). We compared performance of fixed BiLM models at distortion lengths of 5 and 10. ArabicEnglish results did not reveal statistically significant differences between the two distortion limits for Pos→Pos→Pos•Pos. On the other hand, for Lex•Lex BLEU decreas"
D14-1176,D11-1125,0,0.0672608,"x (b). Since TER is an error rate, lower scores are better. Configuration Pos→Pos•  Pos→Pos•Pos Pos→Pos•Lex MT08 BLEU TER 45.66N,M 47.44N,M 45.66N,M 47.17N,N 45.48M,− 47.34N,N MT09 BLEU TER 48.78N,− 43.94N,− 49.00N,− 43.45N,N 48.90N,− 43.87N,M MT08+MT09 BLEU TER 47.15N,− 45.77N,M 47.25N,M 45.40N,N 47.12N,− 45.69N,N Table 3: Different combinations of a target contextual function with the Pos→Pos source contextual function for Arabic-English. See Table 2 for the notation regarding statistical significance. shown in Table 1. The feature weights were tuned by using pairwise ranking optimization (Hopkins and May, 2011) on the MT04 benchmark (for both language pairs). During tuning, 14 PRO parameter estimation runs are performed in parallel on different samples of the n-best list after each decoder iteration. The weights of the individual PRO runs are then averaged and passed on to the next decoding iteration. Performing weight estimation independently for a number of samples corrects for some of the instability that can be caused by individual samples. For testing, we used MT08 and MT09 for Arabic, and MT06 and MT08 for Chinese. We use approximate randomization (Noreen, 1989; Riezler and Maxwell, 2005) to t"
D14-1176,D10-1027,0,0.0149905,"e a more detailed definition of translation events and to redefine decoding as parsing of a source string (Liu et al., 2006; Huang et al., 2006; Marton and Resnik, 2008), of a target string (Shen et al., 2008), or both (Chiang, 2007; Chiang, 2010). Reordering is a result of a given derivation, and CYK-based decoding used in tree-based approaches is more syntax-aware than the simple PBSMT decoding algorithm. Although tree-based approaches potentially offer a more accurate model of translation, they are also a lot more complex and requiring more intricate optimization and estimation techniques (Huang and Mi, 2010). Our idea is to keep the simplicity of PBSMT but move towards the expressiveness typical of treebased models. We incrementally build up the syntactic representation of a translation during decoding by adding precomputed fragments from the source parse tree. The idea to combine the merits of the two SMT paradigms has been proposed before, where Huang and Mi (2010) introduce incremental decoding for a tree-based model. On a very general level, our approach is similar to theirs in that it keeps track of a sequence of source syntactic subtrees that are being translated at consecutive decoding ste"
D14-1176,2006.amta-papers.8,0,0.0244806,"ent. 1689 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1689–1700, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics kens with a rich set of POS tags. But in general, reordering is considered to be a syntactic phenomenon and thus the relevant features are syntactic (Fox, 2002; Cherry, 2008). Syntactic information is incorporated in tree-based approaches in SMT, allowing one to provide a more detailed definition of translation events and to redefine decoding as parsing of a source string (Liu et al., 2006; Huang et al., 2006; Marton and Resnik, 2008), of a target string (Shen et al., 2008), or both (Chiang, 2007; Chiang, 2010). Reordering is a result of a given derivation, and CYK-based decoding used in tree-based approaches is more syntax-aware than the simple PBSMT decoding algorithm. Although tree-based approaches potentially offer a more accurate model of translation, they are also a lot more complex and requiring more intricate optimization and estimation techniques (Huang and Mi, 2010). Our idea is to keep the simplicity of PBSMT but move towards the expressiveness typical of treebased models. We incrementa"
D14-1176,N03-1017,0,0.00873095,"ce requiring a reordering. Determining the correct reordering during decoding is a major challenge for SMT. This problem has received a lot of attention in the literature (see, e.g., Tillmann (2004), Zens and Ney (2003), Al-Onaizan and Papineni (2006)), as choosing the correct reordering improves readability of the translation and can have a substantial impact on translation quality (Birch, 2011). In this paper, we only consider those approaches that include a reordering feature function into the loglinear interpolation used during decoding. The simplest reordering model is linear distortion (Koehn et al., 2003) which scores the distance between phrases translated at steps t and t + 1 of the derivation. This model ignores any contextual information, as the distance between translated phrases is its only parameter. Lexical distortion modeling (Tillmann, 2004) conditions reordering probabilities on the phrase pairs translated at the current and previous steps. Unlike linear distortion, it characterizes reordering not in terms of distance but type: monotone, swap, or discontinuous. In this paper, we base our approach to reordering on bilingual language models (Marino et al., 2006; Niehues et al., 2011)."
D14-1176,P06-1077,0,0.0221951,"tence are independent. 1689 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1689–1700, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics kens with a rich set of POS tags. But in general, reordering is considered to be a syntactic phenomenon and thus the relevant features are syntactic (Fox, 2002; Cherry, 2008). Syntactic information is incorporated in tree-based approaches in SMT, allowing one to provide a more detailed definition of translation events and to redefine decoding as parsing of a source string (Liu et al., 2006; Huang et al., 2006; Marton and Resnik, 2008), of a target string (Shen et al., 2008), or both (Chiang, 2007; Chiang, 2010). Reordering is a result of a given derivation, and CYK-based decoding used in tree-based approaches is more syntax-aware than the simple PBSMT decoding algorithm. Although tree-based approaches potentially offer a more accurate model of translation, they are also a lot more complex and requiring more intricate optimization and estimation techniques (Huang and Mi, 2010). Our idea is to keep the simplicity of PBSMT but move towards the expressiveness typical of treebased m"
D14-1176,J06-4004,0,0.0949952,"Missing"
D14-1176,P08-1114,0,0.0223808,"s of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1689–1700, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics kens with a rich set of POS tags. But in general, reordering is considered to be a syntactic phenomenon and thus the relevant features are syntactic (Fox, 2002; Cherry, 2008). Syntactic information is incorporated in tree-based approaches in SMT, allowing one to provide a more detailed definition of translation events and to redefine decoding as parsing of a source string (Liu et al., 2006; Huang et al., 2006; Marton and Resnik, 2008), of a target string (Shen et al., 2008), or both (Chiang, 2007; Chiang, 2010). Reordering is a result of a given derivation, and CYK-based decoding used in tree-based approaches is more syntax-aware than the simple PBSMT decoding algorithm. Although tree-based approaches potentially offer a more accurate model of translation, they are also a lot more complex and requiring more intricate optimization and estimation techniques (Huang and Mi, 2010). Our idea is to keep the simplicity of PBSMT but move towards the expressiveness typical of treebased models. We incrementally build up the syntactic"
D14-1176,W11-2124,0,0.0660911,"etherlands {e.garmash,c.monz}@uva.nl Abstract This paper presents a novel approach to improve reordering in phrase-based machine translation by using richer, syntactic representations of units of bilingual language models (BiLMs). Our method to include syntactic information is simple in implementation and requires minimal changes in the decoding algorithm. The approach is evaluated in a series of ArabicEnglish and Chinese-English translation experiments. The best models demonstrate significant improvements in BLEU and TER over the phrase-based baseline, as well as over the lexicalized BiLM by Niehues et al. (2011). Further improvements of up to 0.45 BLEU for ArabicEnglish and up to 0.59 BLEU for ChineseEnglish are obtained by combining our dependency BiLM with a lexicalized BiLM. An improvement of 0.98 BLEU is obtained for Chinese-English in the setting of an increased distortion limit. 1 Introduction In statistical machine translation (SMT) reordering (also called distortion) refers to the order in which source words are translated to generate the translation in the target language. Word orders can differ significantly across languages. For instance, Arabic declarative sentences can be verbinitial, wh"
D14-1176,J03-1002,0,0.00865412,"he BiLMs are computed in a stream-based fashion, with bilingual tokens as string tokens, and not in a class-based fashion, with syntactic source-side representations emitting the corresponding target words (Bisazza and Monz, 2014). 4 4.1 Experiments Setup We conduct translation experiments with a baseline PBSMT system with additionally one of the dependency-based BiLM feature functions specified in Section 3. We compare the translation performance to a baseline PBSMT system and to a baseline augmented with the original BiLMs from (Niehues et al., 2011). Word-alignment is produced with GIZA++ (Och and Ney, 2003). We use an in-house implementation of a PBSMT system similar to Moses (Koehn et al., 2007). Our baseline contains all standard PBSMT features including language model, lexical weighting, and lexicalized reordering. The distortion limit is set to 5. A 5-gram LM is trained on the English Gigaword corpus (1.6B tokens) using SRILM with modified Kneyser-Ney smoothing and interpolation. The BiLMs were trained as described in Section 3.3. Information about the parallel data used for training the Arabic-English7 and Chinese-English systems8 is 7 The following Arabic-English parallel corpora were used"
D14-1176,P02-1040,0,0.0911184,"ter each decoder iteration. The weights of the individual PRO runs are then averaged and passed on to the next decoding iteration. Performing weight estimation independently for a number of samples corrects for some of the instability that can be caused by individual samples. For testing, we used MT08 and MT09 for Arabic, and MT06 and MT08 for Chinese. We use approximate randomization (Noreen, 1989; Riezler and Maxwell, 2005) to test for statistically significant differences. In the next two subsections we discuss the general results for Arabic and Chinese, where we use case-insensitive BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as evaluation metrics. This is followed by a preliminary analysis of observed reorderings where we compare 4-gram precision results and conduct experiments with an increased distortion limit. 4.2 Arabic-English translation experiments We are interested in how a translation system with an integrated dependency-based BiLM feaand several gale corpora. ture performs as compared to the standard PBSMT baseline and, more importantly, to the original BiLM model. We consider two variants of BiLM discussed by Niehues et al. (2011): the standard one, Lex•Lex, and the simple"
D14-1176,W05-0908,0,0.161133,"ization (Hopkins and May, 2011) on the MT04 benchmark (for both language pairs). During tuning, 14 PRO parameter estimation runs are performed in parallel on different samples of the n-best list after each decoder iteration. The weights of the individual PRO runs are then averaged and passed on to the next decoding iteration. Performing weight estimation independently for a number of samples corrects for some of the instability that can be caused by individual samples. For testing, we used MT08 and MT09 for Arabic, and MT06 and MT08 for Chinese. We use approximate randomization (Noreen, 1989; Riezler and Maxwell, 2005) to test for statistically significant differences. In the next two subsections we discuss the general results for Arabic and Chinese, where we use case-insensitive BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as evaluation metrics. This is followed by a preliminary analysis of observed reorderings where we compare 4-gram precision results and conduct experiments with an increased distortion limit. 4.2 Arabic-English translation experiments We are interested in how a translation system with an integrated dependency-based BiLM feaand several gale corpora. ture performs as compared"
D14-1176,P07-2045,0,0.00716033,"nd not in a class-based fashion, with syntactic source-side representations emitting the corresponding target words (Bisazza and Monz, 2014). 4 4.1 Experiments Setup We conduct translation experiments with a baseline PBSMT system with additionally one of the dependency-based BiLM feature functions specified in Section 3. We compare the translation performance to a baseline PBSMT system and to a baseline augmented with the original BiLMs from (Niehues et al., 2011). Word-alignment is produced with GIZA++ (Och and Ney, 2003). We use an in-house implementation of a PBSMT system similar to Moses (Koehn et al., 2007). Our baseline contains all standard PBSMT features including language model, lexical weighting, and lexicalized reordering. The distortion limit is set to 5. A 5-gram LM is trained on the English Gigaword corpus (1.6B tokens) using SRILM with modified Kneyser-Ney smoothing and interpolation. The BiLMs were trained as described in Section 3.3. Information about the parallel data used for training the Arabic-English7 and Chinese-English systems8 is 7 The following Arabic-English parallel corpora were used: LDC2006E25, LDC2004T18, several gale corpora, LDC2004T17, LDC2005E46, LDC2007T08, LDC2004"
D14-1176,P08-1066,0,0.112313,"in Natural Language Processing (EMNLP), pages 1689–1700, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics kens with a rich set of POS tags. But in general, reordering is considered to be a syntactic phenomenon and thus the relevant features are syntactic (Fox, 2002; Cherry, 2008). Syntactic information is incorporated in tree-based approaches in SMT, allowing one to provide a more detailed definition of translation events and to redefine decoding as parsing of a source string (Liu et al., 2006; Huang et al., 2006; Marton and Resnik, 2008), of a target string (Shen et al., 2008), or both (Chiang, 2007; Chiang, 2010). Reordering is a result of a given derivation, and CYK-based decoding used in tree-based approaches is more syntax-aware than the simple PBSMT decoding algorithm. Although tree-based approaches potentially offer a more accurate model of translation, they are also a lot more complex and requiring more intricate optimization and estimation techniques (Huang and Mi, 2010). Our idea is to keep the simplicity of PBSMT but move towards the expressiveness typical of treebased models. We incrementally build up the syntactic representation of a translation during"
D14-1176,D13-1049,0,0.0482623,"We also exploit some very simple target contextual functions, but do not go into an in-depth exploration. 3.2 Dependency-based contextual functions In NLP approaches exploiting dependency structure, two kinds of relations are of special importance: the parent-child relation and the sibling relation. Shen et al. (2008) work with two wellformed dependency structures, both of which are defined in such a way that there is one common parent and a set of siblings. Li et al. (2012) characterize rules in hierarchical SMT by labeling them with the POS tags of the parents of the words inside the rule. Lerner and Petrov (2013) model reordering as a sequence of classification steps based on a dependency parse of a sentence. Their model first decides how a word is reordered with respect to its parent and then how it is reordered with respect to its siblings. Based on these previous approaches, we propose to characterize contextual syntactic roles of a word in terms of POS tags of the words themselves and their relatives in a dependency tree. It is straightforward to incorporate parent information since each node has a unique parent. As for siblings information, we incorporate POS tags of the closest sibling to the le"
D14-1176,2006.amta-papers.25,0,0.0147449,"weights of the individual PRO runs are then averaged and passed on to the next decoding iteration. Performing weight estimation independently for a number of samples corrects for some of the instability that can be caused by individual samples. For testing, we used MT08 and MT09 for Arabic, and MT06 and MT08 for Chinese. We use approximate randomization (Noreen, 1989; Riezler and Maxwell, 2005) to test for statistically significant differences. In the next two subsections we discuss the general results for Arabic and Chinese, where we use case-insensitive BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as evaluation metrics. This is followed by a preliminary analysis of observed reorderings where we compare 4-gram precision results and conduct experiments with an increased distortion limit. 4.2 Arabic-English translation experiments We are interested in how a translation system with an integrated dependency-based BiLM feaand several gale corpora. ture performs as compared to the standard PBSMT baseline and, more importantly, to the original BiLM model. We consider two variants of BiLM discussed by Niehues et al. (2011): the standard one, Lex•Lex, and the simplest syntactic one, Pos•Pos. Res"
D14-1176,P12-2007,0,0.034218,"Missing"
D14-1176,N04-4026,0,0.340299,"limit. 1 Introduction In statistical machine translation (SMT) reordering (also called distortion) refers to the order in which source words are translated to generate the translation in the target language. Word orders can differ significantly across languages. For instance, Arabic declarative sentences can be verbinitial, while the corresponding English translation should realize the verb after the subject, hence requiring a reordering. Determining the correct reordering during decoding is a major challenge for SMT. This problem has received a lot of attention in the literature (see, e.g., Tillmann (2004), Zens and Ney (2003), Al-Onaizan and Papineni (2006)), as choosing the correct reordering improves readability of the translation and can have a substantial impact on translation quality (Birch, 2011). In this paper, we only consider those approaches that include a reordering feature function into the loglinear interpolation used during decoding. The simplest reordering model is linear distortion (Koehn et al., 2003) which scores the distance between phrases translated at steps t and t + 1 of the derivation. This model ignores any contextual information, as the distance between translated phr"
D14-1176,N03-1033,0,0.103204,"ROOT→VBD→ +NNS+•exports, VBD→NNS→NNP+IN+•to (if there is no sibling on either of the sides,  is returned). sentences are segmented according to Equation 3. We produce a dependency parse of a source sentence and a POS-tag labeling of a target sentence. For Chinese, we use the Stanford dependency parser (Chang et al., 2009). For Arabic a dependency parser is not available for public use, so we produce a constituency parse with the Stanford parser (Green and Manning, 2010) and extract dependencies based on the rules in Collins (1999). For English POS-tagging, we use the Stanford POS-tagger (Toutanova et al., 2003). After having produced a labeled sequence of tokens, we learn a 5-gram model using SRILM (Stolcke et al., 2011). Kneyser-Ney smoothing is used for all model variations except for Pos•Pos where Witten-Bell smoothing is used due to zero countof-counts. 3.4 N. of tokens 148M 146M 20M 28M Table 1: Training data for Arabic-English and Chinese-English experiments. … VBD N. of lines 4,376,320 4,376,320 2,104,652 2,104,652 Decoder integration Dependency-based BiLMs are integrated into our phrase-based SMT decoder as follows: Before translating a sentence, we produce its dependency parse. Phrase-inter"
D14-1176,P03-1019,0,0.0530996,"ction In statistical machine translation (SMT) reordering (also called distortion) refers to the order in which source words are translated to generate the translation in the target language. Word orders can differ significantly across languages. For instance, Arabic declarative sentences can be verbinitial, while the corresponding English translation should realize the verb after the subject, hence requiring a reordering. Determining the correct reordering during decoding is a major challenge for SMT. This problem has received a lot of attention in the literature (see, e.g., Tillmann (2004), Zens and Ney (2003), Al-Onaizan and Papineni (2006)), as choosing the correct reordering improves readability of the translation and can have a substantial impact on translation quality (Birch, 2011). In this paper, we only consider those approaches that include a reordering feature function into the loglinear interpolation used during decoding. The simplest reordering model is linear distortion (Koehn et al., 2003) which scores the distance between phrases translated at steps t and t + 1 of the derivation. This model ignores any contextual information, as the distance between translated phrases is its only para"
D14-1176,J03-4003,0,\N,Missing
D14-1176,W02-1039,0,\N,Missing
D14-1176,W04-3250,0,\N,Missing
D15-1287,N09-2001,0,0.250467,"y defining a translation model in terms of treelet pairs where target-side treelets are produced by projecting source dependencies via word alignments. Closely related to DCA is the notion of syntactic cohesion of translation (Fox, 2002; Cherry, 2008). This is a constraint that does not allow for non-projective reordering: Given a source parse DS , a translation W is cohesive if all translated target words wi , wj do not have any word wk between them such that there is a source subtree sub in DS such that some parts of it are translated by wi and wj but not by wk (Figure 2). Cherry (2008) and Bach et al. (2009) define a set of soft constraints based on the syntactic cohesion assumption which are applicable to PBSMT decoding. They only require phrase applications, and not necessarily individual target words, to conform to the cohesion principle. For example, if we imagine a situation where a subtree as in Figure 2(b) is translated as a whole with one phrase application (and not word by word), then it does not violate the cohesion principle, although it is internally uncohesive. Both our approach and Cherry (2008) implement the idea of conforming the target translation to the source syntactic structur"
D15-1287,P10-2033,0,0.0482806,"anguage models. However, earlier work has also shown that statistical MT can benefit from additional linguistically motivated models. Most prominent among the linguistically motivated approaches are syntax-based MT systems which take into account the syntactic structure of sentences through CKY decoding and categorial labels (Zollmann and Venugopal, 2006; Shen et al., 2008). On the other hand, the commonly used phrase-based SMT approaches can also reap some of the benefits of using syntactic information by integrating linguistic components addressing specific phenomena, such as Cherry (2008), Carpuat et al. (2010), Crego and Yvon (2010), Ge (2010), Xiang et al. (2011), Lerner and Petrov (2013), Garmash and Monz (2014). This paper is a contribution to the existing body of work on how syntactically motivated models help translation performance. We work with the phrase-based SMT (PBSMT) (Koehn et al., 2003) framework as the baseline system. Our choice is motivated by the fact that PBSMT is a conceptually simple and therefore flexible framework. It is typically quite straightforward to integrate an additional model into the system. Also, PBSMT is the most widely used framework in the SMT research community"
D15-1287,W09-2307,0,0.0170654,"me as in simple reduction labeling, but add the POS tag of the root of the reduced subtree to the label. 4.4 Implementation and training To use BiSLM during decoding, one needs access to phrase-internal alignments and target POS tags. We store phrase-internal alignments and targetside POS annotations of each phrase in the phrase table, based on the most frequent internal alignment during training and the most likely targetside POS labeling tˆ given the phrase pair: tˆ = arg maxt¯ p(t¯|¯ e, f¯). We train BiSLMs on the parallel training data (Section 5.1) and use the Stanford dependency parser (Chang et al., 2009) for Chinese and and the Stanford constituency parser (Green and Manning, 2010) for Arabic7 . POStagging of the training data is produced with the Stanford POS-tagger (Toutanova et al., 2003). We learn a 5-gram model using SRILM (Stolcke et al., 2011) with modified Kneser-Ney smoothing. 5 Experiments To evaluate the effectiveness of BiSLMs for PBSMT, we performed rescoring experiments for 7 We extract dependency parses from its output based on Collins (1999) Arabic-English and Chinese-English. We compare the resulting 1-best translation lists with an output of the baseline system and the basel"
D15-1287,P08-1009,0,0.127231,"g, and n-gram language models. However, earlier work has also shown that statistical MT can benefit from additional linguistically motivated models. Most prominent among the linguistically motivated approaches are syntax-based MT systems which take into account the syntactic structure of sentences through CKY decoding and categorial labels (Zollmann and Venugopal, 2006; Shen et al., 2008). On the other hand, the commonly used phrase-based SMT approaches can also reap some of the benefits of using syntactic information by integrating linguistic components addressing specific phenomena, such as Cherry (2008), Carpuat et al. (2010), Crego and Yvon (2010), Ge (2010), Xiang et al. (2011), Lerner and Petrov (2013), Garmash and Monz (2014). This paper is a contribution to the existing body of work on how syntactically motivated models help translation performance. We work with the phrase-based SMT (PBSMT) (Koehn et al., 2003) framework as the baseline system. Our choice is motivated by the fact that PBSMT is a conceptually simple and therefore flexible framework. It is typically quite straightforward to integrate an additional model into the system. Also, PBSMT is the most widely used framework in the"
D15-1287,C10-2023,0,0.105237,", earlier work has also shown that statistical MT can benefit from additional linguistically motivated models. Most prominent among the linguistically motivated approaches are syntax-based MT systems which take into account the syntactic structure of sentences through CKY decoding and categorial labels (Zollmann and Venugopal, 2006; Shen et al., 2008). On the other hand, the commonly used phrase-based SMT approaches can also reap some of the benefits of using syntactic information by integrating linguistic components addressing specific phenomena, such as Cherry (2008), Carpuat et al. (2010), Crego and Yvon (2010), Ge (2010), Xiang et al. (2011), Lerner and Petrov (2013), Garmash and Monz (2014). This paper is a contribution to the existing body of work on how syntactically motivated models help translation performance. We work with the phrase-based SMT (PBSMT) (Koehn et al., 2003) framework as the baseline system. Our choice is motivated by the fact that PBSMT is a conceptually simple and therefore flexible framework. It is typically quite straightforward to integrate an additional model into the system. Also, PBSMT is the most widely used framework in the SMT research community, which ensures compara"
D15-1287,N10-1127,0,0.0213149,"shown that statistical MT can benefit from additional linguistically motivated models. Most prominent among the linguistically motivated approaches are syntax-based MT systems which take into account the syntactic structure of sentences through CKY decoding and categorial labels (Zollmann and Venugopal, 2006; Shen et al., 2008). On the other hand, the commonly used phrase-based SMT approaches can also reap some of the benefits of using syntactic information by integrating linguistic components addressing specific phenomena, such as Cherry (2008), Carpuat et al. (2010), Crego and Yvon (2010), Ge (2010), Xiang et al. (2011), Lerner and Petrov (2013), Garmash and Monz (2014). This paper is a contribution to the existing body of work on how syntactically motivated models help translation performance. We work with the phrase-based SMT (PBSMT) (Koehn et al., 2003) framework as the baseline system. Our choice is motivated by the fact that PBSMT is a conceptually simple and therefore flexible framework. It is typically quite straightforward to integrate an additional model into the system. Also, PBSMT is the most widely used framework in the SMT research community, which ensures comparability of o"
D15-1287,C10-1045,0,0.0127505,"reduced subtree to the label. 4.4 Implementation and training To use BiSLM during decoding, one needs access to phrase-internal alignments and target POS tags. We store phrase-internal alignments and targetside POS annotations of each phrase in the phrase table, based on the most frequent internal alignment during training and the most likely targetside POS labeling tˆ given the phrase pair: tˆ = arg maxt¯ p(t¯|¯ e, f¯). We train BiSLMs on the parallel training data (Section 5.1) and use the Stanford dependency parser (Chang et al., 2009) for Chinese and and the Stanford constituency parser (Green and Manning, 2010) for Arabic7 . POStagging of the training data is produced with the Stanford POS-tagger (Toutanova et al., 2003). We learn a 5-gram model using SRILM (Stolcke et al., 2011) with modified Kneser-Ney smoothing. 5 Experiments To evaluate the effectiveness of BiSLMs for PBSMT, we performed rescoring experiments for 7 We extract dependency parses from its output based on Collins (1999) Arabic-English and Chinese-English. We compare the resulting 1-best translation lists with an output of the baseline system and the baseline augmented with soft cohesion constraints from Bach et al. (2009). System ba"
D15-1287,P07-1077,0,0.0244721,"of which is its ancestor. is more primitive.1 A dependency parse D is a dependency tree analysis of a sentence W , and we will think of it as a relation between words of W , such that D(w, v) if w is a parent (head) of v (v being a child/modifier). D can be generalized to D∗ which is an relation between words that are connected by a continuous path in a dependency tree (i.e. D∗ (w, v) if D(w, v) or if ∃u s.t. D(w, u) ∧ D∗ (u, v)). We assume unlabeled dependency trees. Finally, we make a projectivity assumption, which is supported by empirical data in many languages (Kuhlmann and Nivre, 2006; Havelka, 2007), and makes a model computationally less expensive. A dependency parse D of a sentence W = w1 , . . . , wn is projective, if for every word pair wi , wj ∈ W s.t. D(wi , wj ) it holds that every wk ∈ W s.t. i &lt; k &lt; j or j &lt; k &lt; i is a descendant of wi , i.e., D∗ (wi , wk ); see Figure 1. Most NLP models that address the interaction of two or more languages are based (explicitly or implicitly) on the direct correspondence assumption (DCA) (Hwa et al., 2002). It states that close translation equivalents in different languages have the same dependency structure. This is grounded linguistically, as"
D15-1287,D11-1125,0,0.0432671,"nment is produced with GIZA++ (Och and Ney, 2003). We use an inhouse implementation of a PBSMT system similar to Moses (Koehn et al., 2007). Our baseline has all standard PBSMT features including language model, lexical weighting, and lexicalized reordering. The distortion limit is set to 5. A 5-gram LM is trained on the English Gigaword corpus (1.6B tokens) using SRILM with modified Kneser-Ney smoothing and linear interpolation. Information about the training data for the Arabic-English and Chinese-English systems is in Table 3.8 Feature weights are tuned using pairwise ranking optimization (Hopkins and May, 2011) on the MT04 benchmark (for both language pairs). For testing, we use MT08 and MT09 for Arabic, and MT06 and MT08 for Chinese. We use case-insensitive BLEU (Papineni et al., 2002) as evaluation metric. Approximate randomization (Noreen, 1989; Riezler and Maxwell, 2005) is used to detect statistically significant differences. 5.2 Baseline and comparison systems As a comparison model, we implemented six features from Cherry (2008) and Bach et al. (2009)9 and added them to the log-linear interpolation used 8 The standard LDC corpora were used for training. Exhaustive and non-exhaustive interrupti"
D15-1287,P02-1050,0,0.038801,"dependency trees. Finally, we make a projectivity assumption, which is supported by empirical data in many languages (Kuhlmann and Nivre, 2006; Havelka, 2007), and makes a model computationally less expensive. A dependency parse D of a sentence W = w1 , . . . , wn is projective, if for every word pair wi , wj ∈ W s.t. D(wi , wj ) it holds that every wk ∈ W s.t. i &lt; k &lt; j or j &lt; k &lt; i is a descendant of wi , i.e., D∗ (wi , wk ); see Figure 1. Most NLP models that address the interaction of two or more languages are based (explicitly or implicitly) on the direct correspondence assumption (DCA) (Hwa et al., 2002). It states that close translation equivalents in different languages have the same dependency structure. This is grounded linguistically, as translation equivalence implies semantic equivalence and therefore thematic relations are preserved (Hwa et al., 2002). Thus dependency relations are preserved, as they are defined based on thematic relations between words. On the other hand, there is plenty empirical evidence supporting the violation of DCA under certain conditions (Hwa et al., 2002). For instance, even semantically very close sentences in different languages may have a different number"
D15-1287,N03-1017,0,0.0224887,"coding and categorial labels (Zollmann and Venugopal, 2006; Shen et al., 2008). On the other hand, the commonly used phrase-based SMT approaches can also reap some of the benefits of using syntactic information by integrating linguistic components addressing specific phenomena, such as Cherry (2008), Carpuat et al. (2010), Crego and Yvon (2010), Ge (2010), Xiang et al. (2011), Lerner and Petrov (2013), Garmash and Monz (2014). This paper is a contribution to the existing body of work on how syntactically motivated models help translation performance. We work with the phrase-based SMT (PBSMT) (Koehn et al., 2003) framework as the baseline system. Our choice is motivated by the fact that PBSMT is a conceptually simple and therefore flexible framework. It is typically quite straightforward to integrate an additional model into the system. Also, PBSMT is the most widely used framework in the SMT research community, which ensures comparability of our results to other people’s work on the topic. There is a variety of ways syntax can be used in a PBSMT model. Typically a syntactic representation of a source sentence is used to define constraints on the order in which the decoder translates it. For example,"
D15-1287,2010.amta-papers.22,0,0.0497904,"N. of tokens 148M 146M 20M 28M labeling complete plain strong weak Table 3: Training data for Arabic-English and Chinese-English experiments. reduce by the baseline system. Since these features are binary or count-based, we cannot use them directly in rescoring. For that reason we integrated the features into the decoder and tuned the corresponding weights. The results for Chinese-English and Arabic-English translation experiments are presented in Table 1 and 2, respectively. We see that adding the cohesion constraints does not improve performance. This finding is different from, for example, Feng et al. (2010), where they get improvement for Chinese-English: however, we note that their training set is smaller than ours, and their baseline is weaker as it does not contain lexicalized distortion models. reduce-POS 5.3 Rescoring experiments Rescoring with BiSLMs is performed as follows: For the test runs of the baseline system we compute the n = 1000 best translation hypotheses for each source sentence and extract their derivations (sequence of phrase pair applications). Each phrase pair in our implementation is associated with a unique phrase-internal alignment and target POS-sequence. We fully recon"
D15-1287,P07-2045,0,0.00618093,"m Bach et al. (2009). System baseline cohesion MT06 32.60 32.52 MT08 25.94 25.98 MT06+MT08 29.56 29.54 Table 1: Chinese-English baseline and comparison model (Cherry, 2008; Bach et al., 2009) results. System baseline cohesion constr. MT08 45.84 45.61 MT09 48.61 48.49 MT08+MT09 47.18 47.02 Table 2: Arabic-English baseline and comparison model (Cherry, 2008; Bach et al., 2009) results. 5.1 Experimental setup This section provides information about our baseline system. Word-alignment is produced with GIZA++ (Och and Ney, 2003). We use an inhouse implementation of a PBSMT system similar to Moses (Koehn et al., 2007). Our baseline has all standard PBSMT features including language model, lexical weighting, and lexicalized reordering. The distortion limit is set to 5. A 5-gram LM is trained on the English Gigaword corpus (1.6B tokens) using SRILM with modified Kneser-Ney smoothing and linear interpolation. Information about the training data for the Arabic-English and Chinese-English systems is in Table 3.8 Feature weights are tuned using pairwise ranking optimization (Hopkins and May, 2011) on the MT04 benchmark (for both language pairs). For testing, we use MT08 and MT09 for Arabic, and MT06 and MT08 for"
D15-1287,W02-1039,0,0.076114,"ansfer that define syntactic structure of one language by conditioning it on the structure of semantically equivalent sentences in another language (Naseem et al., 2012). DCA has also been used in SMT. In particular, syntax-based SMT is built implicitly around this assumption (Wu, 1997; Yamada and Knight, 2001). In Quirk and Menezes (2006) DCA is explicitly implemented by defining a translation model in terms of treelet pairs where target-side treelets are produced by projecting source dependencies via word alignments. Closely related to DCA is the notion of syntactic cohesion of translation (Fox, 2002; Cherry, 2008). This is a constraint that does not allow for non-projective reordering: Given a source parse DS , a translation W is cohesive if all translated target words wi , wj do not have any word wk between them such that there is a source subtree sub in DS such that some parts of it are translated by wi and wj but not by wk (Figure 2). Cherry (2008) and Bach et al. (2009) define a set of soft constraints based on the syntactic cohesion assumption which are applicable to PBSMT decoding. They only require phrase applications, and not necessarily individual target words, to conform to the"
D15-1287,P06-2066,0,0.0116282,"sibling (node 3), neither of which is its ancestor. is more primitive.1 A dependency parse D is a dependency tree analysis of a sentence W , and we will think of it as a relation between words of W , such that D(w, v) if w is a parent (head) of v (v being a child/modifier). D can be generalized to D∗ which is an relation between words that are connected by a continuous path in a dependency tree (i.e. D∗ (w, v) if D(w, v) or if ∃u s.t. D(w, u) ∧ D∗ (u, v)). We assume unlabeled dependency trees. Finally, we make a projectivity assumption, which is supported by empirical data in many languages (Kuhlmann and Nivre, 2006; Havelka, 2007), and makes a model computationally less expensive. A dependency parse D of a sentence W = w1 , . . . , wn is projective, if for every word pair wi , wj ∈ W s.t. D(wi , wj ) it holds that every wk ∈ W s.t. i &lt; k &lt; j or j &lt; k &lt; i is a descendant of wi , i.e., D∗ (wi , wk ); see Figure 1. Most NLP models that address the interaction of two or more languages are based (explicitly or implicitly) on the direct correspondence assumption (DCA) (Hwa et al., 2002). It states that close translation equivalents in different languages have the same dependency structure. This is grounded li"
D15-1287,D14-1176,1,0.807458,"Missing"
D15-1287,D13-1049,0,0.0216819,"enefit from additional linguistically motivated models. Most prominent among the linguistically motivated approaches are syntax-based MT systems which take into account the syntactic structure of sentences through CKY decoding and categorial labels (Zollmann and Venugopal, 2006; Shen et al., 2008). On the other hand, the commonly used phrase-based SMT approaches can also reap some of the benefits of using syntactic information by integrating linguistic components addressing specific phenomena, such as Cherry (2008), Carpuat et al. (2010), Crego and Yvon (2010), Ge (2010), Xiang et al. (2011), Lerner and Petrov (2013), Garmash and Monz (2014). This paper is a contribution to the existing body of work on how syntactically motivated models help translation performance. We work with the phrase-based SMT (PBSMT) (Koehn et al., 2003) framework as the baseline system. Our choice is motivated by the fact that PBSMT is a conceptually simple and therefore flexible framework. It is typically quite straightforward to integrate an additional model into the system. Also, PBSMT is the most widely used framework in the SMT research community, which ensures comparability of our results to other people’s work on the topic."
D15-1287,P12-1066,0,0.0285833,"he source subtree {(1, 2)}, but the target word b does not translate this subtree. (c-d): cohesive (c) and uncohesive (d) translations. (d) is uncohesive because a and c translate the source subtree {(0, 1)}, but b does not translate it. words. Syntactic divergence increases if the two languages are typologically different. Even though DCA only holds up to a certain level of precision, it is widely used in NLP. There are models of cross-lingual transfer that define syntactic structure of one language by conditioning it on the structure of semantically equivalent sentences in another language (Naseem et al., 2012). DCA has also been used in SMT. In particular, syntax-based SMT is built implicitly around this assumption (Wu, 1997; Yamada and Knight, 2001). In Quirk and Menezes (2006) DCA is explicitly implemented by defining a translation model in terms of treelet pairs where target-side treelets are produced by projecting source dependencies via word alignments. Closely related to DCA is the notion of syntactic cohesion of translation (Fox, 2002; Cherry, 2008). This is a constraint that does not allow for non-projective reordering: Given a source parse DS , a translation W is cohesive if all translated"
D15-1287,W11-2124,0,0.0160445,"to be grammatical and chooses the most likely parse for it. What we are interested in during translation is how gram2398 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2398–2408, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. matical the target sentence actually is. In addition to reordering constraints, source syntax can be used for target-side language modeling. A target side string can be encoded with source-syntactic building blocks and then scored as to how well-formed it is. Crego and Yvon (2010), Niehues et al. (2011), Garmash and Monz (2014) model target sequences as strings of tokens built from the target POS tag and the POS tags of the source words related to it through alignment and the source parse. In this paper, we define a target-side syntactic language model that takes structural constraints from the source sentence, but uses the words from the target side (as ‘building blocks’). We do it by adapting an existing monolingual model of Chelba and Jelinek (2000), structured language models, to the bilingual setting. Our contributions can be summarized as follows: • we propose a novel method to adapt m"
D15-1287,J03-1002,0,0.0103754,"tput of the baseline system and the baseline augmented with soft cohesion constraints from Bach et al. (2009). System baseline cohesion MT06 32.60 32.52 MT08 25.94 25.98 MT06+MT08 29.56 29.54 Table 1: Chinese-English baseline and comparison model (Cherry, 2008; Bach et al., 2009) results. System baseline cohesion constr. MT08 45.84 45.61 MT09 48.61 48.49 MT08+MT09 47.18 47.02 Table 2: Arabic-English baseline and comparison model (Cherry, 2008; Bach et al., 2009) results. 5.1 Experimental setup This section provides information about our baseline system. Word-alignment is produced with GIZA++ (Och and Ney, 2003). We use an inhouse implementation of a PBSMT system similar to Moses (Koehn et al., 2007). Our baseline has all standard PBSMT features including language model, lexical weighting, and lexicalized reordering. The distortion limit is set to 5. A 5-gram LM is trained on the English Gigaword corpus (1.6B tokens) using SRILM with modified Kneser-Ney smoothing and linear interpolation. Information about the training data for the Arabic-English and Chinese-English systems is in Table 3.8 Feature weights are tuned using pairwise ranking optimization (Hopkins and May, 2011) on the MT04 benchmark (for"
D15-1287,P02-1040,0,0.0931811,"atures including language model, lexical weighting, and lexicalized reordering. The distortion limit is set to 5. A 5-gram LM is trained on the English Gigaword corpus (1.6B tokens) using SRILM with modified Kneser-Ney smoothing and linear interpolation. Information about the training data for the Arabic-English and Chinese-English systems is in Table 3.8 Feature weights are tuned using pairwise ranking optimization (Hopkins and May, 2011) on the MT04 benchmark (for both language pairs). For testing, we use MT08 and MT09 for Arabic, and MT06 and MT08 for Chinese. We use case-insensitive BLEU (Papineni et al., 2002) as evaluation metric. Approximate randomization (Noreen, 1989; Riezler and Maxwell, 2005) is used to detect statistically significant differences. 5.2 Baseline and comparison systems As a comparison model, we implemented six features from Cherry (2008) and Bach et al. (2009)9 and added them to the log-linear interpolation used 8 The standard LDC corpora were used for training. Exhaustive and non-exhaustive interruption check, exhaustive and non-exhaustive interruption count, verb- and noun-dominated subtree interruption count. 2404 9 Training set Source side of Ar-En set Target side of Ar-En"
D15-1287,2008.amta-papers.16,0,0.0183374,"n be used in a PBSMT model. Typically a syntactic representation of a source sentence is used to define constraints on the order in which the decoder translates it. For example, Cherry (2008) defines soft constraints based on the notion of syntactic cohesion (Section 2). Ge (2010) captures reordering patterns by defining soft constraints based on the currently translated word’s POS tag and the words structurally related to it. On the other hand, target syntax is more challenging to use in PBSMT, since a target-side syntactic model does not have access to the whole target sentence at decoding. Post and Gildea (2008) is one of the few targetside syntactic approaches applicable to PBSMT, but it has been shown not to improve translation. Their approach uses a target side parser as a language model: one of the reasons why it fails is that a parser assumes its input to be grammatical and chooses the most likely parse for it. What we are interested in during translation is how gram2398 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2398–2408, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. matical the target sentence actua"
D15-1287,N03-1033,0,0.0449152,"phrase-internal alignments and target POS tags. We store phrase-internal alignments and targetside POS annotations of each phrase in the phrase table, based on the most frequent internal alignment during training and the most likely targetside POS labeling tˆ given the phrase pair: tˆ = arg maxt¯ p(t¯|¯ e, f¯). We train BiSLMs on the parallel training data (Section 5.1) and use the Stanford dependency parser (Chang et al., 2009) for Chinese and and the Stanford constituency parser (Green and Manning, 2010) for Arabic7 . POStagging of the training data is produced with the Stanford POS-tagger (Toutanova et al., 2003). We learn a 5-gram model using SRILM (Stolcke et al., 2011) with modified Kneser-Ney smoothing. 5 Experiments To evaluate the effectiveness of BiSLMs for PBSMT, we performed rescoring experiments for 7 We extract dependency parses from its output based on Collins (1999) Arabic-English and Chinese-English. We compare the resulting 1-best translation lists with an output of the baseline system and the baseline augmented with soft cohesion constraints from Bach et al. (2009). System baseline cohesion MT06 32.60 32.52 MT08 25.94 25.98 MT06+MT08 29.56 29.54 Table 1: Chinese-English baseline and co"
D15-1287,J97-3002,0,0.468207,"tions. (d) is uncohesive because a and c translate the source subtree {(0, 1)}, but b does not translate it. words. Syntactic divergence increases if the two languages are typologically different. Even though DCA only holds up to a certain level of precision, it is widely used in NLP. There are models of cross-lingual transfer that define syntactic structure of one language by conditioning it on the structure of semantically equivalent sentences in another language (Naseem et al., 2012). DCA has also been used in SMT. In particular, syntax-based SMT is built implicitly around this assumption (Wu, 1997; Yamada and Knight, 2001). In Quirk and Menezes (2006) DCA is explicitly implemented by defining a translation model in terms of treelet pairs where target-side treelets are produced by projecting source dependencies via word alignments. Closely related to DCA is the notion of syntactic cohesion of translation (Fox, 2002; Cherry, 2008). This is a constraint that does not allow for non-projective reordering: Given a source parse DS , a translation W is cohesive if all translated target words wi , wj do not have any word wk between them such that there is a source subtree sub in DS such that so"
D15-1287,W11-1007,0,0.0182781,"statistical MT can benefit from additional linguistically motivated models. Most prominent among the linguistically motivated approaches are syntax-based MT systems which take into account the syntactic structure of sentences through CKY decoding and categorial labels (Zollmann and Venugopal, 2006; Shen et al., 2008). On the other hand, the commonly used phrase-based SMT approaches can also reap some of the benefits of using syntactic information by integrating linguistic components addressing specific phenomena, such as Cherry (2008), Carpuat et al. (2010), Crego and Yvon (2010), Ge (2010), Xiang et al. (2011), Lerner and Petrov (2013), Garmash and Monz (2014). This paper is a contribution to the existing body of work on how syntactically motivated models help translation performance. We work with the phrase-based SMT (PBSMT) (Koehn et al., 2003) framework as the baseline system. Our choice is motivated by the fact that PBSMT is a conceptually simple and therefore flexible framework. It is typically quite straightforward to integrate an additional model into the system. Also, PBSMT is the most widely used framework in the SMT research community, which ensures comparability of our results to other p"
D15-1287,P01-1067,0,0.709798,"is uncohesive because a and c translate the source subtree {(0, 1)}, but b does not translate it. words. Syntactic divergence increases if the two languages are typologically different. Even though DCA only holds up to a certain level of precision, it is widely used in NLP. There are models of cross-lingual transfer that define syntactic structure of one language by conditioning it on the structure of semantically equivalent sentences in another language (Naseem et al., 2012). DCA has also been used in SMT. In particular, syntax-based SMT is built implicitly around this assumption (Wu, 1997; Yamada and Knight, 2001). In Quirk and Menezes (2006) DCA is explicitly implemented by defining a translation model in terms of treelet pairs where target-side treelets are produced by projecting source dependencies via word alignments. Closely related to DCA is the notion of syntactic cohesion of translation (Fox, 2002; Cherry, 2008). This is a constraint that does not allow for non-projective reordering: Given a source parse DS , a translation W is cohesive if all translated target words wi , wj do not have any word wk between them such that there is a source subtree sub in DS such that some parts of it are transla"
D15-1287,C14-1107,0,0.0458702,"Missing"
D15-1287,W06-3119,0,0.0508745,"abicEnglish. 1 Introduction Many model components of competitive statistical machine translation (SMT) systems are based on rather simplistic definitions with little linguistic grounding, which includes the definitions of phrase pairs, lexicalized reordering, and n-gram language models. However, earlier work has also shown that statistical MT can benefit from additional linguistically motivated models. Most prominent among the linguistically motivated approaches are syntax-based MT systems which take into account the syntactic structure of sentences through CKY decoding and categorial labels (Zollmann and Venugopal, 2006; Shen et al., 2008). On the other hand, the commonly used phrase-based SMT approaches can also reap some of the benefits of using syntactic information by integrating linguistic components addressing specific phenomena, such as Cherry (2008), Carpuat et al. (2010), Crego and Yvon (2010), Ge (2010), Xiang et al. (2011), Lerner and Petrov (2013), Garmash and Monz (2014). This paper is a contribution to the existing body of work on how syntactically motivated models help translation performance. We work with the phrase-based SMT (PBSMT) (Koehn et al., 2003) framework as the baseline system. Our"
D15-1287,2006.amta-panels.3,0,0.215937,"c translate the source subtree {(0, 1)}, but b does not translate it. words. Syntactic divergence increases if the two languages are typologically different. Even though DCA only holds up to a certain level of precision, it is widely used in NLP. There are models of cross-lingual transfer that define syntactic structure of one language by conditioning it on the structure of semantically equivalent sentences in another language (Naseem et al., 2012). DCA has also been used in SMT. In particular, syntax-based SMT is built implicitly around this assumption (Wu, 1997; Yamada and Knight, 2001). In Quirk and Menezes (2006) DCA is explicitly implemented by defining a translation model in terms of treelet pairs where target-side treelets are produced by projecting source dependencies via word alignments. Closely related to DCA is the notion of syntactic cohesion of translation (Fox, 2002; Cherry, 2008). This is a constraint that does not allow for non-projective reordering: Given a source parse DS , a translation W is cohesive if all translated target words wi , wj do not have any word wk between them such that there is a source subtree sub in DS such that some parts of it are translated by wi and wj but not by w"
D15-1287,W05-0908,0,0.0400122,"stortion limit is set to 5. A 5-gram LM is trained on the English Gigaword corpus (1.6B tokens) using SRILM with modified Kneser-Ney smoothing and linear interpolation. Information about the training data for the Arabic-English and Chinese-English systems is in Table 3.8 Feature weights are tuned using pairwise ranking optimization (Hopkins and May, 2011) on the MT04 benchmark (for both language pairs). For testing, we use MT08 and MT09 for Arabic, and MT06 and MT08 for Chinese. We use case-insensitive BLEU (Papineni et al., 2002) as evaluation metric. Approximate randomization (Noreen, 1989; Riezler and Maxwell, 2005) is used to detect statistically significant differences. 5.2 Baseline and comparison systems As a comparison model, we implemented six features from Cherry (2008) and Bach et al. (2009)9 and added them to the log-linear interpolation used 8 The standard LDC corpora were used for training. Exhaustive and non-exhaustive interruption check, exhaustive and non-exhaustive interruption count, verb- and noun-dominated subtree interruption count. 2404 9 Training set Source side of Ar-En set Target side of Ar-En set Source side of Ch-En set Target side of Ch-En set N. of lines 4,376,320 4,376,320 2,10"
D15-1287,Q15-1013,0,0.0218279,"d to a bilingual setting and apply to SMT in Section 4. SLMs have been applied in SMT before (Yamada and Knight, 2001; Yu et al., 2014), but as we show in Section 4, we provide a much simpler method to integrate it into the system. While a SLM is not the only syntactically defined LM, it is one of the few that models sentence generation sequentially. And due to the way the decoding procedure of PBSMT is defined, it is natural and straightforward to use models whose score can be computed sequentially. Other syntactic language models define sentence generation hierarchically (Shen et al., 2008; Sennrich, 2015), which complicates their integration into a PBSMT system. The linguistic intuition behind SLMs is that the structural children of a word do not essentially change its distributional properties but just provide additional specification. In Figure 3(a) the word president has two modifiers: the and former and it follows yesterday (an adjunct) and precedes met (a predicate). This ordering is correct in English. If instead its modifier was a or an entire relative clause, it would not make it incorrect. To capture this observation, (Chelba and Jelinek, 2000) propose a language model where each word"
D15-1287,P08-1066,0,0.243263,"ny model components of competitive statistical machine translation (SMT) systems are based on rather simplistic definitions with little linguistic grounding, which includes the definitions of phrase pairs, lexicalized reordering, and n-gram language models. However, earlier work has also shown that statistical MT can benefit from additional linguistically motivated models. Most prominent among the linguistically motivated approaches are syntax-based MT systems which take into account the syntactic structure of sentences through CKY decoding and categorial labels (Zollmann and Venugopal, 2006; Shen et al., 2008). On the other hand, the commonly used phrase-based SMT approaches can also reap some of the benefits of using syntactic information by integrating linguistic components addressing specific phenomena, such as Cherry (2008), Carpuat et al. (2010), Crego and Yvon (2010), Ge (2010), Xiang et al. (2011), Lerner and Petrov (2013), Garmash and Monz (2014). This paper is a contribution to the existing body of work on how syntactically motivated models help translation performance. We work with the phrase-based SMT (PBSMT) (Koehn et al., 2003) framework as the baseline system. Our choice is motivated"
D15-1287,J03-4003,0,\N,Missing
D17-1147,D11-1033,0,0.772776,"time-consuming task, with training times of several weeks not being unusual. Despite its training inefficiency, most work in NMT greedily uses all available training data for a given language pair. However, it is unlikely ∗ Work done while at University of Amsterdam Christof Monz Informatics Institute University of Amsterdam that all data is equally helpful to create the bestperforming system. In PBMT, this issue has been addressed by applying intelligent data selection, and it has consistently been shown that using more data does not always improve translation quality (Moore and Lewis, 2010; Axelrod et al., 2011; Gasc´o et al., 2012). Instead, for a given translation task, the training bitext likely contains sentences that are irrelevant or even harmful, making it beneficial to keep only the most relevant subset of the data while discarding the rest, with the additional benefit of smaller models and faster training. Motivated by the success of data selection in PBMT, we investigate in this paper to what extent and how NMT can benefit from data selection as well. While data selection has been applied to NMT to reduce the size of the data (Cho et al., 2014; Luong et al., 2015b), the effects on translat"
D17-1147,W15-3003,0,0.0331958,"Missing"
D17-1147,D16-1025,1,0.852981,"parallelizing models or data (Wu et al., 2016), modifying the NMT network structure (Kalchbrenner et al., 2016), decreasing the number of parameters through knowledge distillation (Crego et al., 2016; Kim and Rush, 2016), or by boosting parts of the data that are ‘challenging’ to the NMT system (Zhang et al., 2016). The latter is most related to our work since training data is also adjusted during training, however we reduce the training data size much more aggressively and study different techniques of data selection. Finally, recent work comparing various aspects for PBMT and NMT includes (Bentivogli et al., 2016; Farajian et al., 2017; Toral and S´anchezCartagena, 2017; Koehn and Knowles, 2017). 8 Conclusions With the recent increase in popularity of neural machine translation (NMT), we explored in this paper to what extent and how NMT can benefit from data selection. We first showed that a stateof-the-art data selection method yields unreliable results for NMT while consistently performing well for PBMT. Next, we have introduced dynamic data selection for NMT, which entails varying the selected subset of training data between different training epochs. We explored two techniques of dynamic data sele"
D17-1147,2012.eamt-1.60,0,0.0605642,"okens Lines Tokens EMEA 206K 3.3M Movies 101K 1.2M TED 189K 3.3M WMT 3.8M 84M 3.9K 4.5K 2.5K 3.0K 59K 54K 50K 64K 5.8K 7.1K 5.4K 3.0K 93K 87K 99K 65K Mix 3.5K 61K – – 4.3M 92M Table 1: Data specifications with tokens counted on the German side. The WMT training corpus contains Commoncrawl, Europarl, and News Commentary but no in-domain news data. 4.2 Training and evaluation data We evaluate all experiments on four domains: (i) EMEA medical guidelines (Tiedemann, 2009), (ii) movie dialogues (van der Wees et al., 2016) constructed from OpenSubtitles (Lison and Tiedemann, 2016), (iii) TED talks (Cettolo et al., 2012), and (iv) WMT news. For TED, we use IWSLT2010 as development set and IWSLT20112014 as test set, and for WMT we use newstest2013 as development set and newstest2016 as test set. We train our systems on a mixture of domains, comprising Commoncrawl, Europarl, News Commentary, EMEA, Movies, and TED. Corpus specifications are listed in Table 1. The in-domain LMs used to rank training sentences for data selection are trained on small portions of in-domain parallel data whenever available (3.3M, 1.2M and 3.3M German tokens for EMEA, Movies and TED, respectively). Since no sizeable in-domain parallel"
D17-1147,K16-1031,0,0.107622,"domains. Data selection methods for domain adaptation mostly employ information theory metrics to rank training sentences by their relevance to the domain at hand. This has been applied monolingually (Gao et al., 2002) as well as bilingually (Yasuda et al., 2008). In more recent work, training sentences are typically ranked according to their cross-entropy difference between in-domain and general-domain data (Moore and Lewis, 2010; Axelrod et al., 2011, 2015), favoring sentences that are similar to the test domain and at the same time dissimilar from the general domain. Duh et al. (2013) and Chen and Huang (2016) present similar methods in which n-gram LMs are replaced by neural LMs or neural classifiers, respectively. Data selection with the aim of model size and training time reduction has the objective to use the minimum amount of data while still maintaining high vocabulary coverage (Eck et al., 2005; Gasc´o et al., 2012; Lewis and Eetemadi, 2013). In a comparative study, Mirkin and Besacier (2014) find that similarity-objected methods perform best if the test domain and general corpus are very different, while a coverage-objected method is superior if test and general corpus are relatively simila"
D17-1147,P17-2061,0,0.0838773,"15). While in this work we have used a similarity objective to rank our bitext, one could also apply dynamic data selection using a coverage objective. In NMT, data selection can serve similar goals as in PBMT; increasing training efficiency or domain adaptation. Domain adaptation in NMT typically involves training a model on the complete bitext, followed by fine-tuning the parameters on a smaller in-domain corpus (Luong and Manning, 2015; Zoph et al., 2016). Other work combines fine-tuning with model ensembles (Freitag and AlOnaizan, 2016) or with domain-specific tags in the training corpus (Chu et al., 2017). Finally, Sennrich et al. (2016a) adapt their systems by backtranslating in-domain data, which is then added to the training data and used for fine-tuning. Some other previous work has addressed training efficiency for NMT, for example by parallelizing models or data (Wu et al., 2016), modifying the NMT network structure (Kalchbrenner et al., 2016), decreasing the number of parameters through knowledge distillation (Crego et al., 2016; Kim and Rush, 2016), or by boosting parts of the data that are ‘challenging’ to the NMT system (Zhang et al., 2016). The latter is most related to our work sin"
D17-1147,1981.tc-1.3,0,0.59213,"Missing"
D17-1147,P13-2119,0,0.116555,"es, or to adapt to new domains. Data selection methods for domain adaptation mostly employ information theory metrics to rank training sentences by their relevance to the domain at hand. This has been applied monolingually (Gao et al., 2002) as well as bilingually (Yasuda et al., 2008). In more recent work, training sentences are typically ranked according to their cross-entropy difference between in-domain and general-domain data (Moore and Lewis, 2010; Axelrod et al., 2011, 2015), favoring sentences that are similar to the test domain and at the same time dissimilar from the general domain. Duh et al. (2013) and Chen and Huang (2016) present similar methods in which n-gram LMs are replaced by neural LMs or neural classifiers, respectively. Data selection with the aim of model size and training time reduction has the objective to use the minimum amount of data while still maintaining high vocabulary coverage (Eck et al., 2005; Gasc´o et al., 2012; Lewis and Eetemadi, 2013). In a comparative study, Mirkin and Besacier (2014) find that similarity-objected methods perform best if the test domain and general corpus are very different, while a coverage-objected method is superior if test and general co"
D17-1147,2005.iwslt-1.7,0,0.110622,"Missing"
D17-1147,P17-2090,1,0.867279,"est, with the additional benefit of smaller models and faster training. Motivated by the success of data selection in PBMT, we investigate in this paper to what extent and how NMT can benefit from data selection as well. While data selection has been applied to NMT to reduce the size of the data (Cho et al., 2014; Luong et al., 2015b), the effects on translation quality have not been investigated. Intuitively, and confirmed by our exploratory experiments in Section 5.1, this is a challenging task; NMT systems are known to under-perform when trained on limited parallel data (Zoph et al., 2016; Fadaee et al., 2017), and do not have a separate large-scale target-side language model to compensate for smaller parallel training data. To alleviate the negative effect of small training data on NMT, we introduce dynamic data selection. Following conventional data selection, we still dramatically reduce the training data size, favoring parts of the data which are most relevant to the translation task at hand. However, we exploit the fact that the NMT training process iterates over the training corpus in multiple epochs, and we alter the quantity or the composition of the training data between epochs. The propos"
D17-1147,E17-2045,0,0.083002,"data (Wu et al., 2016), modifying the NMT network structure (Kalchbrenner et al., 2016), decreasing the number of parameters through knowledge distillation (Crego et al., 2016; Kim and Rush, 2016), or by boosting parts of the data that are ‘challenging’ to the NMT system (Zhang et al., 2016). The latter is most related to our work since training data is also adjusted during training, however we reduce the training data size much more aggressively and study different techniques of data selection. Finally, recent work comparing various aspects for PBMT and NMT includes (Bentivogli et al., 2016; Farajian et al., 2017; Toral and S´anchezCartagena, 2017; Koehn and Knowles, 2017). 8 Conclusions With the recent increase in popularity of neural machine translation (NMT), we explored in this paper to what extent and how NMT can benefit from data selection. We first showed that a stateof-the-art data selection method yields unreliable results for NMT while consistently performing well for PBMT. Next, we have introduced dynamic data selection for NMT, which entails varying the selected subset of training data between different training epochs. We explored two techniques of dynamic data selection and found that ou"
D17-1147,E12-1016,0,0.0954133,"Missing"
D17-1147,D11-1125,0,0.0823035,"actual selection contains the top n sentences pairs of G. 4 Experimental settings We evaluate static and dynamic data selection on a German→English translation task comprising four test sets. Below we describe the MT systems and data specifications. 4.1 Machine translation systems While the main aim of this paper is to improve data selection for NMT, we also perform comparative experiments using PBMT. Our PBMT system is an in-house system similar to Moses (Koehn et al., 2007). To create optimal PBMT systems given the available resources, we apply test-set-specific parameter tuning using PRO (Hopkins and May, 2011). In addition, we use a linearly interpolated target-side language model trained with KneserNey smoothing on 480M tokens of data in various domains. LM interpolation weights are also optimized per test set. Consistent with Axelrod et al. (2011), we do not vary the target-side LM between different experiments on the same test set. All ngram models in our work are 5-gram. For our NMT experiments we use an in-house encoder-decoder3 model with global attention as described in Luong et al. (2015a). This choice comes at the cost of optimal translation quality but allows for a relatively fast realiza"
D17-1147,D16-1139,0,0.0569009,"016). Other work combines fine-tuning with model ensembles (Freitag and AlOnaizan, 2016) or with domain-specific tags in the training corpus (Chu et al., 2017). Finally, Sennrich et al. (2016a) adapt their systems by backtranslating in-domain data, which is then added to the training data and used for fine-tuning. Some other previous work has addressed training efficiency for NMT, for example by parallelizing models or data (Wu et al., 2016), modifying the NMT network structure (Kalchbrenner et al., 2016), decreasing the number of parameters through knowledge distillation (Crego et al., 2016; Kim and Rush, 2016), or by boosting parts of the data that are ‘challenging’ to the NMT system (Zhang et al., 2016). The latter is most related to our work since training data is also adjusted during training, however we reduce the training data size much more aggressively and study different techniques of data selection. Finally, recent work comparing various aspects for PBMT and NMT includes (Bentivogli et al., 2016; Farajian et al., 2017; Toral and S´anchezCartagena, 2017; Koehn and Knowles, 2017). 8 Conclusions With the recent increase in popularity of neural machine translation (NMT), we explored in this pa"
D17-1147,W17-3204,0,0.0675471,"e (Kalchbrenner et al., 2016), decreasing the number of parameters through knowledge distillation (Crego et al., 2016; Kim and Rush, 2016), or by boosting parts of the data that are ‘challenging’ to the NMT system (Zhang et al., 2016). The latter is most related to our work since training data is also adjusted during training, however we reduce the training data size much more aggressively and study different techniques of data selection. Finally, recent work comparing various aspects for PBMT and NMT includes (Bentivogli et al., 2016; Farajian et al., 2017; Toral and S´anchezCartagena, 2017; Koehn and Knowles, 2017). 8 Conclusions With the recent increase in popularity of neural machine translation (NMT), we explored in this paper to what extent and how NMT can benefit from data selection. We first showed that a stateof-the-art data selection method yields unreliable results for NMT while consistently performing well for PBMT. Next, we have introduced dynamic data selection for NMT, which entails varying the selected subset of training data between different training epochs. We explored two techniques of dynamic data selection and found that our gradual fine-tuning technique, in which we gradually reduce"
D17-1147,W13-2235,0,0.0477883,"uation (2). Given this ranking, we investigate two dynamic data selection techniques2 that vary per epoch the composition or the size of the selected training data. Both techniques aim to favor highly relevant sentences over less relevant sentences while not completely discarding the latter. In all experiments, we use a fixed vocabulary created from the complete bitext. While we use in this work a domain-relevance ranking of the bitext following Axelrod et al. (2011), dynamic data selection can also be applied using other ranking criteria, for example limiting redundancy in the training data (Lewis and Eetemadi, 2013) or complementing similarity with diversity (Ruder and Plank, 2017). Sampling sentence pairs In the first technique, illustrated in Figure 1a, we sample for every epoch n sentence pairs from G, using a distribution computed from the domain-specific CEDs scores. Concretely, this is done as follows: First, since higher ranked sentence pairs have lower CEDs scores, and they can be either negative or positive, we scale and invert CEDs scores such that 0 ≤ CED0s ≤ 1 for each sentence pair s ∈ G: CED0s = 1 − 2 CEDs − min(CEDG ) , max(CEDG ) − min(CEDG ) (3) Code for bitext ranking and both selection"
D17-1147,L16-1147,0,0.0454519,"in Dev/valid Test Corpus Lines Tokens Lines Tokens Lines Tokens EMEA 206K 3.3M Movies 101K 1.2M TED 189K 3.3M WMT 3.8M 84M 3.9K 4.5K 2.5K 3.0K 59K 54K 50K 64K 5.8K 7.1K 5.4K 3.0K 93K 87K 99K 65K Mix 3.5K 61K – – 4.3M 92M Table 1: Data specifications with tokens counted on the German side. The WMT training corpus contains Commoncrawl, Europarl, and News Commentary but no in-domain news data. 4.2 Training and evaluation data We evaluate all experiments on four domains: (i) EMEA medical guidelines (Tiedemann, 2009), (ii) movie dialogues (van der Wees et al., 2016) constructed from OpenSubtitles (Lison and Tiedemann, 2016), (iii) TED talks (Cettolo et al., 2012), and (iv) WMT news. For TED, we use IWSLT2010 as development set and IWSLT20112014 as test set, and for WMT we use newstest2013 as development set and newstest2016 as test set. We train our systems on a mixture of domains, comprising Commoncrawl, Europarl, News Commentary, EMEA, Movies, and TED. Corpus specifications are listed in Table 1. The in-domain LMs used to rank training sentences for data selection are trained on small portions of in-domain parallel data whenever available (3.3M, 1.2M and 3.3M German tokens for EMEA, Movies and TED, respectivel"
D17-1147,2015.iwslt-evaluation.11,0,0.264764,"drawing for each epoch n sentence pairs without replacement. While all selection weights are very close to zero, higher ranked sentences have a noticeably higher probability of being selected than lower-ranked sentences; in practice we find that top-ranked sentences get selected in nearly each epoch, while bottom-ranked sentence pairs get selected at most once. Note that the sampled selection for any epoch is independent of selections for all other epochs. Gradual fine-tuning The second dynamic data selection technique, see Figure 1b, is inspired by the success of domain-specific fine-tuning (Luong and Manning, 2015; Zoph et al., 2016; Sennrich et al., 2016a; Freitag and Al-Onaizan, 2016), in which a model trained on a large general-domain bitext is trained for a few additional epochs only on small in-domain data. However, rather than training a full model on the complete bitext G, we gradually decrease the training data size, starting from G and keeping only the top n sentence pairs for the duration of η epochs, where the top n pairs are defined by their CEDs scores. Given its resemblance to fine-tuning, we refer to this variant as gradual fine-tuning. 1402 During gradual fine-tuning, the selection size"
D17-1147,D15-1166,0,0.181845,"lassifiers, respectively. Data selection with the aim of model size and training time reduction has the objective to use the minimum amount of data while still maintaining high vocabulary coverage (Eck et al., 2005; Gasc´o et al., 2012; Lewis and Eetemadi, 2013). In a comparative study, Mirkin and Besacier (2014) find that similarity-objected methods perform best if the test domain and general corpus are very different, while a coverage-objected method is superior if test and general corpus are relatively similar. A comprehensive survey on data selection for SMT is provided by Eetemadi et al. (2015). While in this work we have used a similarity objective to rank our bitext, one could also apply dynamic data selection using a coverage objective. In NMT, data selection can serve similar goals as in PBMT; increasing training efficiency or domain adaptation. Domain adaptation in NMT typically involves training a model on the complete bitext, followed by fine-tuning the parameters on a smaller in-domain corpus (Luong and Manning, 2015; Zoph et al., 2016). Other work combines fine-tuning with model ensembles (Freitag and AlOnaizan, 2016) or with domain-specific tags in the training corpus (Chu"
D17-1147,P15-1002,0,0.0826066,"(Moore and Lewis, 2010; Axelrod et al., 2011; Gasc´o et al., 2012). Instead, for a given translation task, the training bitext likely contains sentences that are irrelevant or even harmful, making it beneficial to keep only the most relevant subset of the data while discarding the rest, with the additional benefit of smaller models and faster training. Motivated by the success of data selection in PBMT, we investigate in this paper to what extent and how NMT can benefit from data selection as well. While data selection has been applied to NMT to reduce the size of the data (Cho et al., 2014; Luong et al., 2015b), the effects on translation quality have not been investigated. Intuitively, and confirmed by our exploratory experiments in Section 5.1, this is a challenging task; NMT systems are known to under-perform when trained on limited parallel data (Zoph et al., 2016; Fadaee et al., 2017), and do not have a separate large-scale target-side language model to compensate for smaller parallel training data. To alleviate the negative effect of small training data on NMT, we introduce dynamic data selection. Following conventional data selection, we still dramatically reduce the training data size, fav"
D17-1147,2014.amta-researchers.23,0,0.0646374,"general-domain data (Moore and Lewis, 2010; Axelrod et al., 2011, 2015), favoring sentences that are similar to the test domain and at the same time dissimilar from the general domain. Duh et al. (2013) and Chen and Huang (2016) present similar methods in which n-gram LMs are replaced by neural LMs or neural classifiers, respectively. Data selection with the aim of model size and training time reduction has the objective to use the minimum amount of data while still maintaining high vocabulary coverage (Eck et al., 2005; Gasc´o et al., 2012; Lewis and Eetemadi, 2013). In a comparative study, Mirkin and Besacier (2014) find that similarity-objected methods perform best if the test domain and general corpus are very different, while a coverage-objected method is superior if test and general corpus are relatively similar. A comprehensive survey on data selection for SMT is provided by Eetemadi et al. (2015). While in this work we have used a similarity objective to rank our bitext, one could also apply dynamic data selection using a coverage objective. In NMT, data selection can serve similar goals as in PBMT; increasing training efficiency or domain adaptation. Domain adaptation in NMT typically involves tra"
D17-1147,P10-2041,0,0.53045,"n NMT model is often a time-consuming task, with training times of several weeks not being unusual. Despite its training inefficiency, most work in NMT greedily uses all available training data for a given language pair. However, it is unlikely ∗ Work done while at University of Amsterdam Christof Monz Informatics Institute University of Amsterdam that all data is equally helpful to create the bestperforming system. In PBMT, this issue has been addressed by applying intelligent data selection, and it has consistently been shown that using more data does not always improve translation quality (Moore and Lewis, 2010; Axelrod et al., 2011; Gasc´o et al., 2012). Instead, for a given translation task, the training bitext likely contains sentences that are irrelevant or even harmful, making it beneficial to keep only the most relevant subset of the data while discarding the rest, with the additional benefit of smaller models and faster training. Motivated by the success of data selection in PBMT, we investigate in this paper to what extent and how NMT can benefit from data selection as well. While data selection has been applied to NMT to reduce the size of the data (Cho et al., 2014; Luong et al., 2015b), t"
D17-1147,P02-1040,0,0.0985775,"Missing"
D17-1147,P16-1009,0,0.160861,"encountering out-of-vocabulary words. In this neural variant to rank sentences, the score for each sentence pair in G is still computed as the bilingual cross-entropy difference in Equation (2). In addition, we use the same in-domain and general corpora as with the ngram method, and we again restrict the vocabulary to the most frequent words. 3 Dynamic data selection While data selection aims to discard irrelevant data, it can also exacerbate the problem of low vocabulary coverage and unreliable statistics for rarer words in the ‘long tail’, which are major issues in NMT (Luong et al., 2015b; Sennrich et al., 2016b). In addition, it has been shown that NMT performance drops tremendously in low-resource scenarios (Zoph et al., 2016; Fadaee et al., 2017; Koehn and Knowles, 2017). To overcome this problem, we introduce dynamic data selection, in which we vary the selected data subsets during training. Unlike other MT paradigms, which require training data to be fixed during the entire training process, NMT iterates over the training corpus in several epochs, 1 We use four-layer LSTMs with embedding and hidden sizes of 1,024, which we train for 30 epochs. 1401 a) Dynamic data selection: sampling b) Dynamic"
D17-1147,P16-1162,0,0.342819,"encountering out-of-vocabulary words. In this neural variant to rank sentences, the score for each sentence pair in G is still computed as the bilingual cross-entropy difference in Equation (2). In addition, we use the same in-domain and general corpora as with the ngram method, and we again restrict the vocabulary to the most frequent words. 3 Dynamic data selection While data selection aims to discard irrelevant data, it can also exacerbate the problem of low vocabulary coverage and unreliable statistics for rarer words in the ‘long tail’, which are major issues in NMT (Luong et al., 2015b; Sennrich et al., 2016b). In addition, it has been shown that NMT performance drops tremendously in low-resource scenarios (Zoph et al., 2016; Fadaee et al., 2017; Koehn and Knowles, 2017). To overcome this problem, we introduce dynamic data selection, in which we vary the selected data subsets during training. Unlike other MT paradigms, which require training data to be fixed during the entire training process, NMT iterates over the training corpus in several epochs, 1 We use four-layer LSTMs with embedding and hidden sizes of 1,024, which we train for 30 epochs. 1401 a) Dynamic data selection: sampling b) Dynamic"
D17-1147,E17-1100,0,0.0420941,"Missing"
D17-1147,C16-1242,1,0.901265,"Missing"
D17-1147,1983.tc-1.13,0,0.683309,"Missing"
D17-1147,I08-2088,0,0.11659,"ing rate, BLEU scores hardly change or even improve, indicating that the implicit change in search behavior may contribute to the success of gradual fine-tuning. 7 Related work A few research topics are related to our work. Regarding data selection for SMT, previous work has targeted two goals; to reduce model sizes and training times, or to adapt to new domains. Data selection methods for domain adaptation mostly employ information theory metrics to rank training sentences by their relevance to the domain at hand. This has been applied monolingually (Gao et al., 2002) as well as bilingually (Yasuda et al., 2008). In more recent work, training sentences are typically ranked according to their cross-entropy difference between in-domain and general-domain data (Moore and Lewis, 2010; Axelrod et al., 2011, 2015), favoring sentences that are similar to the test domain and at the same time dissimilar from the general domain. Duh et al. (2013) and Chen and Huang (2016) present similar methods in which n-gram LMs are replaced by neural LMs or neural classifiers, respectively. Data selection with the aim of model size and training time reduction has the objective to use the minimum amount of data while still"
D17-1147,P16-5005,0,0.0422657,"domain-specific tags in the training corpus (Chu et al., 2017). Finally, Sennrich et al. (2016a) adapt their systems by backtranslating in-domain data, which is then added to the training data and used for fine-tuning. Some other previous work has addressed training efficiency for NMT, for example by parallelizing models or data (Wu et al., 2016), modifying the NMT network structure (Kalchbrenner et al., 2016), decreasing the number of parameters through knowledge distillation (Crego et al., 2016; Kim and Rush, 2016), or by boosting parts of the data that are ‘challenging’ to the NMT system (Zhang et al., 2016). The latter is most related to our work since training data is also adjusted during training, however we reduce the training data size much more aggressively and study different techniques of data selection. Finally, recent work comparing various aspects for PBMT and NMT includes (Bentivogli et al., 2016; Farajian et al., 2017; Toral and S´anchezCartagena, 2017; Koehn and Knowles, 2017). 8 Conclusions With the recent increase in popularity of neural machine translation (NMT), we explored in this paper to what extent and how NMT can benefit from data selection. We first showed that a stateof-t"
D17-1147,D16-1163,0,0.0455804,"le discarding the rest, with the additional benefit of smaller models and faster training. Motivated by the success of data selection in PBMT, we investigate in this paper to what extent and how NMT can benefit from data selection as well. While data selection has been applied to NMT to reduce the size of the data (Cho et al., 2014; Luong et al., 2015b), the effects on translation quality have not been investigated. Intuitively, and confirmed by our exploratory experiments in Section 5.1, this is a challenging task; NMT systems are known to under-perform when trained on limited parallel data (Zoph et al., 2016; Fadaee et al., 2017), and do not have a separate large-scale target-side language model to compensate for smaller parallel training data. To alleviate the negative effect of small training data on NMT, we introduce dynamic data selection. Following conventional data selection, we still dramatically reduce the training data size, favoring parts of the data which are most relevant to the translation task at hand. However, we exploit the fact that the NMT training process iterates over the training corpus in multiple epochs, and we alter the quantity or the composition of the training data betw"
D17-1147,D17-1038,0,0.145857,"Missing"
D17-1147,P07-2045,0,\N,Missing
D17-1147,W16-2301,1,\N,Missing
D18-1040,D11-1033,0,0.0572713,"rtant question in this context is how to select the monolingual data in the target language that is to be back-translated into the source language to optimally benefit translation quality. Pham et al. (2017) experimented with using domain adaptation methods to select monolingual data based on the cross-entropy between the monolingual data and in-domain corpus (Axelrod et al., 2015) but did not find any improvements over random sampling as originally proposed by Sennrich et al. (2016a). Earlier work has explored to what extent data selection of parallel corpora can benefit translation quality (Axelrod et al., 2011; van der Wees et al., 2017), but such selection techniques have not been investigated in the context of back-translation. In this work, we explore different aspects of the back-translation method to gain a better understanding of its performance. Our analyses show that the quality of the synthetic data acquired with Neural Machine Translation has achieved state-of-the-art performance for several language pairs using a combination of parallel and synthetic data. Synthetic data is often generated by back-translating sentences randomly sampled from monolingual data using a reverse translation mo"
D18-1040,W18-2709,0,0.0229941,"ailable, but on the advantage of having more fluent source or target sentences. Lambert et al. (2011) show that adding synthetic source and real target data achieves improvements in traditional phrase-based machine translation (PBMT). Similarly in previous works 102 101 0 200K 400K 600K 800K 1M 1.2M 1.4M Training time (minibatches) Figure 1: Training plots for systems with different ratios of (real : syn) training data, showing perplexity on development set. 2.5 Quality of the Synthetic Data in Back-Translation One selection criterion for back-translation is the quality of the synthetic data. Khayrallah and Koehn (2018) studied the effects of noise in the training data on a translation model and discov438 Fadaee et al. (2017) showed that targeting specific words during data augmentation improves the generation of these words in the right context. Specifically, adding synthetic data to the training data has an impact on the prediction probabilities of individual words. In this section, we further examine the effects of the back-translated synthetic data on the prediction of target tokens. ered that NMT models are less robust to many types of noise than PBMT models. In order for the NMT model to learn from the"
D18-1040,2015.iwslt-papers.9,0,0.0400994,"s been shown to be very effective to improve translation quality, it is not exactly clear why it helps. Generally speaking, it mitigates the problem of overfitting and fluency by exploiting additional data in the target language. An important question in this context is how to select the monolingual data in the target language that is to be back-translated into the source language to optimally benefit translation quality. Pham et al. (2017) experimented with using domain adaptation methods to select monolingual data based on the cross-entropy between the monolingual data and in-domain corpus (Axelrod et al., 2015) but did not find any improvements over random sampling as originally proposed by Sennrich et al. (2016a). Earlier work has explored to what extent data selection of parallel corpora can benefit translation quality (Axelrod et al., 2011; van der Wees et al., 2017), but such selection techniques have not been investigated in the context of back-translation. In this work, we explore different aspects of the back-translation method to gain a better understanding of its performance. Our analyses show that the quality of the synthetic data acquired with Neural Machine Translation has achieved state"
D18-1040,P17-4012,0,0.0343213,"ver back-translation using random sampling. 2 L“ m ÿ ´ log ppyi |yăi , sn q pX,Y qPD i“1 The objective of this function is to improve the model’s estimation of predicting target words given the source sentence and the target context. The model is trained end-to-end by minimizing the negative log likelihood of the target words. 2.2 Experimental Setup For the translation experiments, we use EnglishØGerman WMT17 training data and report results on newstest 2014, 2015, 2016, and 2017 (Bojar et al., 2017). As NMT system, we use a 2-layer attentionbased encoder-decoder model implemented in OpenNMT (Klein et al., 2017) trained with embedding size 512, hidden dimension size 1024, and batch size 64. We pre-process the training data with Byte-Pair Encoding (BPE) using 32K merge operations (Sennrich et al., 2016b). We compare the results to Sennrich et al. (2016a) by back-translating random sentences from the monolingual data and combine them with the parallel training data. We perform the random selection and re-training 3 times and report the averaged outcomes for the 3 models. In all experiments the sentence pairs are shuffled before each epoch. We measure translation quality by singlereference case-sensitiv"
D18-1040,W11-2132,0,0.444599,"d 1.2 B LEU points over back-translation using random sampling for GermanÑEnglish and EnglishÑGerman, respectively. 1 Introduction Neural machine translation (NMT) using a sequence-to-sequence model has achieved stateof-the-art performance for several language pairs (Bahdanau et al., 2015; Sutskever et al., 2014; Cho et al., 2014). The availability of large-scale training data for these sequence-to-sequence models is essential for achieving good translation quality. Previous approaches have focused on leveraging monolingual data which is available in much larger quantities than parallel data (Lambert et al., 2011). Gulcehre et al. (2017) proposed two methods, shallow and deep fusion, for integrating a neural language model into the NMT system. They observe improvements by combining 436 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 436–446 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics For the token yt , the conditional probability ppyt |yăt , sn q during training quantifies the difficulty of predicting that token in the context yăt . The prediction loss of token yt is the negative loglikelihood of this p"
D18-1040,D15-1166,0,0.0615467,"riments the sentence pairs are shuffled before each epoch. We measure translation quality by singlereference case-sensitive B LEU (Papineni et al., 2002) computed with the multi-bleu.perl script from Moses. Back-Translation for NMT In this section, we briefly review a sequence-tosequence NMT system and describe our experimental settings. We then investigate different aspects and modeling challenges of integrating the back-translation method into the NMT pipeline. 2.1 ÿ Neural Machine Translation The NMT system used for our experiments is an encoder-decoder network with recurrent architecture (Luong et al., 2015). For training the of tokens, X “ “NMT system, ‰ two sequences “ ‰ x1 , . . . , xn and Y “ y1 , . . . , ym , are given in the source and target language, respectively. The source sequence is the input to the encoder which is a bidirectional long short-term memory network generating a representation sn . Using an attention mechanism (Bahdanau et al., 2015), the attentional hidden state is: 2.3 r t “ tanhpWc rct ; ht sq h Size of the Synthetic Data in Back-Translation One selection criterion for using back-translation is the ratio of real to synthetic data. Sennrich et al. (2016a) showed that hi"
D18-1040,P96-1041,0,0.498433,"are verging on being stable. The values are sorted by mean token prediction loss of the system trained on real parallel data. Back-Translation and Token Prediction In the previous section, we observed that the reverse model used to back-translate achieves results comparable to manually translated sentences. Also, there is a limit in learning from synthetic data, and with more synthetic data the model unlearns its parameters completely. In this section, we investigate the influence of the sampled sentences on the learning model. 439 4.1 We observe an effect similar to distributional smoothing (Chen and Goodman, 1996): While prediction loss increases slightly for most tokens, the largest decrease in loss occurs for tokens with high prediction loss values. This indicates that by randomly sampling sentences for back-translation, the model improves its estimation of tokens that were originally more difficult to predict, i.e., tokens that had a high prediction loss. Note that we compute the token prediction loss in just one pass over the training corpus with the final model and as a result it is not biased towards the order of the data. Figure 2 shows that the majority of tokens with high mean prediction losse"
D18-1040,W14-4012,0,0.146558,"Missing"
D18-1040,P02-1040,0,0.100957,"with embedding size 512, hidden dimension size 1024, and batch size 64. We pre-process the training data with Byte-Pair Encoding (BPE) using 32K merge operations (Sennrich et al., 2016b). We compare the results to Sennrich et al. (2016a) by back-translating random sentences from the monolingual data and combine them with the parallel training data. We perform the random selection and re-training 3 times and report the averaged outcomes for the 3 models. In all experiments the sentence pairs are shuffled before each epoch. We measure translation quality by singlereference case-sensitive B LEU (Papineni et al., 2002) computed with the multi-bleu.perl script from Moses. Back-Translation for NMT In this section, we briefly review a sequence-tosequence NMT system and describe our experimental settings. We then investigate different aspects and modeling challenges of integrating the back-translation method into the NMT pipeline. 2.1 ÿ Neural Machine Translation The NMT system used for our experiments is an encoder-decoder network with recurrent architecture (Luong et al., 2015). For training the of tokens, X “ “NMT system, ‰ two sequences “ ‰ x1 , . . . , xn and Y “ y1 , . . . , ym , are given in the source a"
D18-1040,P17-2090,1,0.870636,"ing synthetic source and real target data achieves improvements in traditional phrase-based machine translation (PBMT). Similarly in previous works 102 101 0 200K 400K 600K 800K 1M 1.2M 1.4M Training time (minibatches) Figure 1: Training plots for systems with different ratios of (real : syn) training data, showing perplexity on development set. 2.5 Quality of the Synthetic Data in Back-Translation One selection criterion for back-translation is the quality of the synthetic data. Khayrallah and Koehn (2018) studied the effects of noise in the training data on a translation model and discov438 Fadaee et al. (2017) showed that targeting specific words during data augmentation improves the generation of these words in the right context. Specifically, adding synthetic data to the training data has an impact on the prediction probabilities of individual words. In this section, we further examine the effects of the back-translated synthetic data on the prediction of target tokens. ered that NMT models are less robust to many types of noise than PBMT models. In order for the NMT model to learn from the parallel data, the data should be fluent and close to the manually generated translations. However, automat"
D18-1040,W17-4736,0,0.176769,"allel data are available and has become common practice in NMT (Sennrich et al., 2017; Garc´ıa-Mart´ınez et al., 2017; Ha et al., 2017). While back-translation has been shown to be very effective to improve translation quality, it is not exactly clear why it helps. Generally speaking, it mitigates the problem of overfitting and fluency by exploiting additional data in the target language. An important question in this context is how to select the monolingual data in the target language that is to be back-translated into the source language to optimally benefit translation quality. Pham et al. (2017) experimented with using domain adaptation methods to select monolingual data based on the cross-entropy between the monolingual data and in-domain corpus (Axelrod et al., 2015) but did not find any improvements over random sampling as originally proposed by Sennrich et al. (2016a). Earlier work has explored to what extent data selection of parallel corpora can benefit translation quality (Axelrod et al., 2011; van der Wees et al., 2017), but such selection techniques have not been investigated in the context of back-translation. In this work, we explore different aspects of the back-translati"
D18-1040,W17-4726,0,0.0325794,"Missing"
D18-1040,W17-4739,0,0.0196698,"back-translation using random sampling for GermanÑEnglish and EnglishÑGerman, respectively. 1 Introduction Neural machine translation (NMT) using a sequence-to-sequence model has achieved stateof-the-art performance for several language pairs (Bahdanau et al., 2015; Sutskever et al., 2014; Cho et al., 2014). The availability of large-scale training data for these sequence-to-sequence models is essential for achieving good translation quality. Previous approaches have focused on leveraging monolingual data which is available in much larger quantities than parallel data (Lambert et al., 2011). Gulcehre et al. (2017) proposed two methods, shallow and deep fusion, for integrating a neural language model into the NMT system. They observe improvements by combining 436 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 436–446 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics For the token yt , the conditional probability ppyt |yăt , sn q during training quantifies the difficulty of predicting that token in the context yăt . The prediction loss of token yt is the negative loglikelihood of this probability. During train"
D18-1040,P16-1009,0,0.442833,"Generally speaking, it mitigates the problem of overfitting and fluency by exploiting additional data in the target language. An important question in this context is how to select the monolingual data in the target language that is to be back-translated into the source language to optimally benefit translation quality. Pham et al. (2017) experimented with using domain adaptation methods to select monolingual data based on the cross-entropy between the monolingual data and in-domain corpus (Axelrod et al., 2015) but did not find any improvements over random sampling as originally proposed by Sennrich et al. (2016a). Earlier work has explored to what extent data selection of parallel corpora can benefit translation quality (Axelrod et al., 2011; van der Wees et al., 2017), but such selection techniques have not been investigated in the context of back-translation. In this work, we explore different aspects of the back-translation method to gain a better understanding of its performance. Our analyses show that the quality of the synthetic data acquired with Neural Machine Translation has achieved state-of-the-art performance for several language pairs using a combination of parallel and synthetic data."
D18-1040,P16-1162,0,0.542055,"Generally speaking, it mitigates the problem of overfitting and fluency by exploiting additional data in the target language. An important question in this context is how to select the monolingual data in the target language that is to be back-translated into the source language to optimally benefit translation quality. Pham et al. (2017) experimented with using domain adaptation methods to select monolingual data based on the cross-entropy between the monolingual data and in-domain corpus (Axelrod et al., 2015) but did not find any improvements over random sampling as originally proposed by Sennrich et al. (2016a). Earlier work has explored to what extent data selection of parallel corpora can benefit translation quality (Axelrod et al., 2011; van der Wees et al., 2017), but such selection techniques have not been investigated in the context of back-translation. In this work, we explore different aspects of the back-translation method to gain a better understanding of its performance. Our analyses show that the quality of the synthetic data acquired with Neural Machine Translation has achieved state-of-the-art performance for several language pairs using a combination of parallel and synthetic data."
D18-1040,D17-1147,1,0.894055,"Missing"
D18-1503,P18-2003,0,0.0343089,", making them more suited for modeling long distance dependencies than CNNs. Additionally, FANs promise to be more interpretable than LSTMs by visualizing attention weights. The rest of the paper is organized as follows: We first highlight the differences between the two architectures (§2) and introduce the two tasks (§3). Then we provide setup and results for each task (§4 and §5) and discuss our findings (§6). Recent work has shown that recurrent neural networks (RNNs) can implicitly capture and exploit hierarchical information when trained to solve common natural language processing tasks (Blevins et al., 2018) such as language modeling (Linzen et al., 2016; Gulordava et al., 2018) and neural machine translation (Shi et al., 2016). In contrast, the ability to model structured data with non-recurrent neural networks has received little attention despite their success in many NLP tasks (Gehring et al., 2017; Vaswani et al., 2017). In this work, we compare the two architectures—recurrent versus non-recurrent—with respect to their ability to model hierarchical structure and find that recurrency is indeed important for this purpose. The code and data used in our experiments is available at https://github"
D18-1503,D15-1075,0,0.449039,"showing the main difference between a LSTM and a FAN. Purple boxes indicate the summarized vector at current time step t which is used to make prediction. Orange arrows indicate the information flow from a previous input to that vector. learn hierarchical structure with a set of controlled experiments. 3 Tasks We choose two tasks to study in this work: (1) subject-verb agreement, and (2) logical inference. The first task was proposed by Linzen et al. (2016) to test the ability of recurrent neural networks to capture syntactic dependencies in natural language. The second task was introduced by Bowman et al. (2015b) to compare tree-based recursive neural networks against sequence-based recurrent networks with respect to their ability to exploit hierarchical structures to make accurate inferences. The choice of tasks here is important to ensure that both models have to exploit hierarchical structural features (Jia and Liang, 2017). 4 Subject-Verb Agreement Linzen et al. (2016) propose the task of predicting number agreement between subject and verb in naturally occurring English sentences as a proxy for the ability of LSTMs to capture hierarchical structure in natural language. We use the dataset provid"
D18-1503,P18-2103,0,0.0457007,"Missing"
D18-1503,N18-1108,0,0.0600681,"CNNs. Additionally, FANs promise to be more interpretable than LSTMs by visualizing attention weights. The rest of the paper is organized as follows: We first highlight the differences between the two architectures (§2) and introduce the two tasks (§3). Then we provide setup and results for each task (§4 and §5) and discuss our findings (§6). Recent work has shown that recurrent neural networks (RNNs) can implicitly capture and exploit hierarchical information when trained to solve common natural language processing tasks (Blevins et al., 2018) such as language modeling (Linzen et al., 2016; Gulordava et al., 2018) and neural machine translation (Shi et al., 2016). In contrast, the ability to model structured data with non-recurrent neural networks has received little attention despite their success in many NLP tasks (Gehring et al., 2017; Vaswani et al., 2017). In this work, we compare the two architectures—recurrent versus non-recurrent—with respect to their ability to model hierarchical structure and find that recurrency is indeed important for this purpose. The code and data used in our experiments is available at https://github.com/ ketranm/fan_vs_rnn 1 Introduction Recurrent neural networks (RNNs)"
D18-1503,N18-2017,0,0.0285549,"Missing"
D18-1503,D17-1215,0,0.0182162,"two tasks to study in this work: (1) subject-verb agreement, and (2) logical inference. The first task was proposed by Linzen et al. (2016) to test the ability of recurrent neural networks to capture syntactic dependencies in natural language. The second task was introduced by Bowman et al. (2015b) to compare tree-based recursive neural networks against sequence-based recurrent networks with respect to their ability to exploit hierarchical structures to make accurate inferences. The choice of tasks here is important to ensure that both models have to exploit hierarchical structural features (Jia and Liang, 2017). 4 Subject-Verb Agreement Linzen et al. (2016) propose the task of predicting number agreement between subject and verb in naturally occurring English sentences as a proxy for the ability of LSTMs to capture hierarchical structure in natural language. We use the dataset provided by Linzen et al. (2016) and follow their experimental protocol of training each model using either (a) a general language model, i.e., next word prediction objective, and (b) an explicit supervision objective, i.e., predicting the number of the verb given its sentence history. Table 1 illustrates the training and test"
D18-1503,Q16-1037,0,0.619087,"nce dependencies than CNNs. Additionally, FANs promise to be more interpretable than LSTMs by visualizing attention weights. The rest of the paper is organized as follows: We first highlight the differences between the two architectures (§2) and introduce the two tasks (§3). Then we provide setup and results for each task (§4 and §5) and discuss our findings (§6). Recent work has shown that recurrent neural networks (RNNs) can implicitly capture and exploit hierarchical information when trained to solve common natural language processing tasks (Blevins et al., 2018) such as language modeling (Linzen et al., 2016; Gulordava et al., 2018) and neural machine translation (Shi et al., 2016). In contrast, the ability to model structured data with non-recurrent neural networks has received little attention despite their success in many NLP tasks (Gehring et al., 2017; Vaswani et al., 2017). In this work, we compare the two architectures—recurrent versus non-recurrent—with respect to their ability to model hierarchical structure and find that recurrency is indeed important for this purpose. The code and data used in our experiments is available at https://github.com/ ketranm/fan_vs_rnn 1 Introduction Recurre"
D18-1503,E17-2025,0,0.0249278,"replaced by their part-of-speech. (a) (b) Input Train Test the keys to the cabinet the keys to the cabinet are plural p(are) > p(is)? plural/singular? Hyperparameters: To allow for a fair comparison, we find the best configuration for each model by running a grid search over the following hyperparameters: number of layers in {2, 3, 4}, dropout rate in {0.2, 0.3, 0.5}, embedding size and number of hidden units in {128, 256, 512}, number of heads (for FAN) in {2, 4}, and learning rate in {0.00001, 0.0001, 0.001}. The weights of the word embeddings and output layer are shared (Inan et al., 2017; Press and Wolf, 2017). Models are optimized by Adam (Kingma and Ba, 2015). We first assess whether the LSTM and FAN models trained with respect to the language model objective assign higher probabilities to the correctly inflected verbs. As shown in Figures 2a and 2b, both models achieve high accuracies for this task, but LSTMs consistently outperform FANs. Moreover, LSTMs are clearly more robust than FANs with respect to task difficulty, measured both in terms of word distance and number of agreement attractors1 between subject and verb. Christiansen and Chater (2016); Cornish et al. (2017) have argued that human"
D18-1503,D16-1159,0,0.0750391,"le than LSTMs by visualizing attention weights. The rest of the paper is organized as follows: We first highlight the differences between the two architectures (§2) and introduce the two tasks (§3). Then we provide setup and results for each task (§4 and §5) and discuss our findings (§6). Recent work has shown that recurrent neural networks (RNNs) can implicitly capture and exploit hierarchical information when trained to solve common natural language processing tasks (Blevins et al., 2018) such as language modeling (Linzen et al., 2016; Gulordava et al., 2018) and neural machine translation (Shi et al., 2016). In contrast, the ability to model structured data with non-recurrent neural networks has received little attention despite their success in many NLP tasks (Gehring et al., 2017; Vaswani et al., 2017). In this work, we compare the two architectures—recurrent versus non-recurrent—with respect to their ability to model hierarchical structure and find that recurrency is indeed important for this purpose. The code and data used in our experiments is available at https://github.com/ ketranm/fan_vs_rnn 1 Introduction Recurrent neural networks (RNNs), in particular Long Short-Term Memory networks (L"
D18-1503,N16-1036,1,0.849596,"Missing"
E09-1048,W03-0501,0,0.0336599,"ection 5 concludes with a short discussion. 2 Related Work As mentioned above, the problem of identifying story highlight lies somewhere between keyword extraction and single-document summarization. The K EA keyphrase extraction system (Witten et al., 1999) mainly relies on purely statistical features such as term frequencies, using the tf.idf 1 tf (t, d) = frequency of term t in document d. idf (t, N ) = inverse frequency of documents d containing |N | term t in corpus N , log( |d ) t| 416 Regarding syntax, it seems to be used mainly in sentence compression or trimming. The algorithm used by Dorr et al. (2003) removes subordinate clauses, to name one example. While our approach does not use syntactical features as such, it is worth noting these possible enhancements. 3 according} would receive an identical score, since {not, worth, poll} are in the highlights as well. Spawned phrases. Conversely, spawned phrases occur frequently in the highlights and in close proximity to trigger phrases. Continuing the example in Table 1, {invading, Iraq, poll, not, worth} are all considered to be spawned phrases. Of course, simply using the identities of words neglects the issue of lexical paraphrasing, e.g., inv"
E09-1048,W04-1009,0,0.0615158,"Missing"
E09-1048,W04-1013,0,0.00342102,"ores are similar but there is a difference in the number of unique sentences extracted, this means a system has gone beyond the first 4 sentences and extracted others from deeper down inside the text. To get a better understanding of the importance of the individual features we examined the weights as determined by YASMET. Table 8 contains example output from the development sets, with feature selection determined implicitly by the weights the MaxEnt model assigns, where non-discriminative features receive a low weight. Figure 4: Position of correctly extracted sources by AURUM-thresh. ROUGE (Lin, 2004), a recall-oriented evaluation package for automatic summarization. ROUGE operates essentially by comparing n-gram cooccurrences between a candidate summary and a number of reference summaries, and comparing that number in turn to the total number of n-grams in the reference summaries: ROUGE-n = X X M atch(ngramn ) S∈Ref erences ngramn ∈S X X Count(ngramn ) S∈Ref erences ngramn ∈S Where n is the length of the n-gram, with lengths of 1 and 2 words most commonly used in current evaluations. ROUGE has become the standard tool for evaluating automatic summaries, though it is not the optimal system"
E09-1048,W02-2018,0,0.022465,"s the position of sentence s. Each word j of sentence s is tested against all applicable word features gjk , see Section 3.2.2. A weight (wpos and wk ) is associated with each feature. How to estimate the weights is discussed next. 3.3 Experiments and Results Parameter Estimation Corpus subset Documents Avg. sentences per article Avg. sentence length Avg. number of highlights Avg. number of highlight sources Avg. highlight length in words There are various optimization methods that allow one to estimate the weights of features, including generalized iterative scaling and quasi-Newton methods (Malouf, 2002). We opted for generalized iterative scaling as it is commonly used for other NLP tasks and off-the-shelf implementations exist. Here we used YASMET.3 Dev/Test 300 33.26 20.62 3.71 4.32 10.26 Train 1220 31.02 20.50 3.67 10.28 Table 4: Characteristics of the evaluation corpus. 3 A maximum entropy toolkit by Franz Josef Och, http: //www.fjoch.com/YASMET.html 4 The annotated data set is available at: http://www. science.uva.nl/˜christof/data/hl/. 419 Most summarization evaluation campaigns, such as NIST’s Document Understanding Conferences (DUC), impose a maximum length on summaries (e.g., 75 cha"
E09-1048,D07-1047,0,0.0501957,"Missing"
E09-1048,W07-0203,0,\N,Missing
E09-1048,A00-2018,0,\N,Missing
E12-1002,D08-1021,0,0.0894098,"Missing"
E12-1002,W07-0718,1,0.875792,"Missing"
E12-1002,N06-1003,0,0.0276432,"Missing"
E12-1002,C04-1051,0,0.184142,"asure, the hitting time is known to perform well in harvesting paraphrases on a graph constructed from multiple phrase-tables (KB). Generally in NLP, power-law distributions are typically encountered in the collection of counts during the training stage. The distances of Section 3.1 are converted into artificial co-occurrence counts with a novel technique (Section 3.2). Although they need not be integers, the main challenge is the type of the underlying distributions; it should ideally emulate the resulting count distributions from the phrase extraction stage of a monolingual parallel corpus (Dolan et al., 2004). These counts give rise to the desired probability distributions by means of relative frequencies. 2 2.1 done by identifying all vertices such that, upon removal, the component becomes disconnected. Such vertices are called articulation points or cutvertices. Cut-vertices of high connectivity degree are removed from the giant component (see Section 4.1). For the remaining vertices of the giant component, new components are identified and we proceed iteratively, while keeping track of the cut-vertices that are removed at each iteration, until the size of the largest component is less than a ce"
E12-1002,D11-1108,0,0.196933,"Missing"
E12-1002,P09-5002,0,0.0259275,"i had). Phrase, POS tag feature and stem feature vertices are drawn in circles, dotted rectangles and solid rectangles respectively. All edges are bidirectional. ture vertices. The purpose of the feature vertices, unlike KB, is primarily for smoothing and secondarily for identifying paraphrases with the same syntactic information and this will become clear in the description of the computation of weights. The set of all phrase vertices that are adjacent to s is written as Γ(s), and referred to as the neighborhood of s. Let n(s, t) denote the co-occurrence count of a phrase-table entry (s, t) (Koehn, 2009). We define the strength of s in the subgraph generated by cluster T as X n(s; T ) = n(s, t), (2) w(s → fX ) =< w(s → s0 ) > +net(s), (5) where the first summand is the average weight from s to its neighboring phrase vertices and X = POS, STEM. If s has multiple POS tag sequences, we distribute the weight of eqn. (5) relatively to the co-occurrences of s with the respective POS tag feature vertices. The quantity < w(s → s0 ) > accounts for the basic smoothing and is augmented by a value net(s) that measures the reliability of s’s neighborhood; the more unreliable the neighborhood, the larger t"
E12-1002,N10-1017,0,0.0835063,"hrases from monolingual, comparable and bilingual corpora (Madnani and Dorr, 2010; Androutsopoulos and Malakasiotis, 2010); we focus on the latter and specifically on the phrase-table that is extracted from a bitext during the training stage of Statistical Machine Translation (SMT). Bannard and Callison-Burch (2005) introduced the pivoting approach, which relies on a 2-step transition from a phrase, via its translations, to a paraphrase candidate. By incorporating the syntactic structure of phrases (Callison-Burch, 2005), the quality of the paraphrases extracted with pivoting can be improved. Kok and Brockett (2010) (henceforth KB) used a random walk framework to determine the similarity between phrases, which The initial problem in harvesting paraphrases from a phrase-table is the identification of the search space. Previous work has relied on breadth first search from the query phrase with a depth of 2 (pivoting) and 6 (KB). The former can be too restrictive and the latter can lead to excessive noise contamination when taking shallow syntactic information features into account. Instead, we choose to cluster the phrase-table into separate source and target clusters and in order to make this task computa"
E12-1002,C10-1069,0,0.0376409,"Missing"
E12-1002,J10-3003,0,0.0223022,"placed on the quality of the phrase-paraphrase probabilities as well as on providing a stepping stone for extracting syntactic paraphrases with equally reliable probabilities. In line with previous work, our method depends on the connectivity of the phrase-table, but the resulting construction treats each side separately, which can potentially be benefited from additional monolingual data. Introduction Paraphrase extraction has emerged as an important problem in NLP. Currently, there exists an abundance of methods for extracting paraphrases from monolingual, comparable and bilingual corpora (Madnani and Dorr, 2010; Androutsopoulos and Malakasiotis, 2010); we focus on the latter and specifically on the phrase-table that is extracted from a bitext during the training stage of Statistical Machine Translation (SMT). Bannard and Callison-Burch (2005) introduced the pivoting approach, which relies on a 2-step transition from a phrase, via its translations, to a paraphrase candidate. By incorporating the syntactic structure of phrases (Callison-Burch, 2005), the quality of the paraphrases extracted with pivoting can be improved. Kok and Brockett (2010) (henceforth KB) used a random walk framework to determine"
E12-1002,P11-2096,0,0.0125993,"s0 )|s), (21) where f1 (s0 ) and f2 (s0 ) denote the POS tag sequence and stem sequence of s0 respectively. All three summands of eqn. (21) are computed from eqn. (19). The baseline is given by pivoting (Bannard and Callison-Burch, 2005), X P (s0 |s) = p(t|s)p(s0 |t), (22) t p(s0 |t) where p(t|s) and are the phrase-based relative frequencies of the translation model. We select 150 phrases (an equal number for unigrams, bigrams and trigrams), for which we expect to see paraphrases, and keep the top-10 paraphrases for each phrase, ranked by the above measures. We follow (Kok and Brockett, 2010; Metzler et al., 2011) in the evaluation of the extracted paraphrases: Each phrase-paraphrase pair is manually annotated with the following options: 0) Different meaning; 1) (i) Same meaning, but potential replacement of the phrase with the paraphrase in a sentence ruins the grammatical structure of the sentence. (ii) Tokens of the paraphrase are morphological inflections of the phrase’s tokens. 2) Same meaning. Although useful for SMT purposes, ‘super/substrings of’ are annotated with 0 to achieve an objective evaluation. Both methods are evaluated in terms of the Mean Expected Precision (MEP) at k; the Expected P"
E12-1002,P10-2001,0,0.0415093,"Missing"
E12-1002,W08-2006,0,0.0306149,"y of the source and target clusters gives rise to a natural graph representation for each cluster (Section 3.1). The vertices of the graphs consist of phrases and features with a dual smoothing/syntacticinformation-carrier role. The latter allow (a) redistribution of the mass for phrases with no appropriate paraphrases and (b) the extraction of syntactic paraphrases. The proximity among vertices of a graph is measured by means of a random walk distance measure, the commute time (Aldous and Fill, 2001). This measure is known to perform well in identifying similar words on the graph of WordNet (Rao et al., 2008) and a related measure, the hitting time is known to perform well in harvesting paraphrases on a graph constructed from multiple phrase-tables (KB). Generally in NLP, power-law distributions are typically encountered in the collection of counts during the training stage. The distances of Section 3.1 are converted into artificial co-occurrence counts with a novel technique (Section 3.2). Although they need not be integers, the main challenge is the type of the underlying distributions; it should ideally emulate the resulting count distributions from the phrase extraction stage of a monolingual"
E12-1002,P08-1089,0,0.193228,"zoukos Christof Monz Informatics Institute, University of Amsterdam Science Park 904, 1098 XH Amsterdam, The Netherlands {s.martzoukos, c.monz}@uva.nl Abstract was shown to outperform pivoting with syntactic information, when multiple phrase-tables are used. In SMT, extracted paraphrases with associated pivot-based (Callison-Burch et al., 2006; Onishi et al., 2010) and cluster-based (Kuhn et al., 2010) probabilities have been found to improve the quality of translation. Pivoting has also been employed in the extraction of syntactic paraphrases, which are a mixture of phrases and nonterminals (Zhao et al., 2008; Ganitkevitch et al., 2011). We describe a novel method that extracts paraphrases from a bitext, for both the source and target languages. In order to reduce the search space, we decompose the phrase-table into sub-phrase-tables and construct separate clusters for source and target phrases. We convert the clusters into graphs, add smoothing/syntacticinformation-carrier vertices, and compute the similarity between phrases with a random walk-based measure, the commute time. The resulting phrase-paraphrase probabilities are built upon the conversion of the commute times into artificial cooccurre"
E12-1002,P05-1074,0,\N,Missing
E12-1012,W09-0432,0,0.0376985,"Missing"
E12-1012,W12-1009,1,0.816385,"+ moses features), posmaplex (lexical PoS mapping + moses features ), lab-lex (label-specific coupling + lexical coupling + moses features), lablex-posmap (label-specific coupling + lexical coupling features + generic PoS mapping). To reduce the size of feature-functions vectors we take only the 20 most frequent features in the training data for Label-specific coupling and PoS mapping features. The computation of the syntax features is based on the rule-based XIP parser, where some heuristics specific to query processing have been integrated into English and French (but not German) grammars (Brun et al., 2012). The results of these experiments are illustrated 115 Lng pair Fr-En De-En En-Fr En-De Tune set Europarl CLEF Europarl CLEF Europarl CLEF Europarl CLEF DW 0.0801 0.0015 0.0588 0.3568 0.0789 0.0322 0.0584 0.3451 LM 0.1397 0.0795 0.1341 0.1151 0.1373 0.1251 0.1396 0.1001 φ(f |e) 0.0431 -0.0046 0.0380 0.1168 0.0002 0.0350 0.0092 0.0248 lex(f |e) 0.0625 0.0348 0.0181 0.0549 0.0766 0.1023 0.0821 0.0872 φ(e|f ) 0.1463 0.1977 0.1382 0.0932 0.1798 0.0534 0.1823 0.2629 lex(e|f ) 0.0638 0.0208 0.0398 0.0805 0.0293 0.0365 0.0437 0.0153 PP -0.0670 -0.2904 -0.0904 0.0391 -0.0978 -0.3182 -0.1613 -0.0431 WP"
E12-1012,D08-1024,0,0.0269313,"query qi , i = 1..K. • Each translation t ∈ GEN (qi ) is used to perform a retrieval from a target document collection, and an Average Precision score (AP (t)) is computed for each t ∈ GEN (qi ) by comparing its retrieval to the relevance annotations done during the CLEF campaign. The weights λ are learned with the objective of maximizing MAP for all the queries of the training set, and, therefore, are optimized for retrieval quality. The weights optimization is done with the Margin Infused Relaxed Algorithm (MIRA)(Crammer and Singer, 2003), which was applied to SMT by (Watanabe et al., 2007; Chiang et al., 2008). MIRA is an online learning algorithm where each weights update is done to keep the new weights as close as possible to the old weights (first term), and score oracle translation (the translation giving the best retrieval score : t∗i = arg maxt AP (t)) higher than each non-oracle translation (tij ) by a margin at least as wide as the loss lij (second term): 0 λ = minλ0 21 kλ − λk2 +   P 0 ∗ ) − F (t ) C K max l − λ · (F (t ij ij j=1..N i i=1 1. The baseline (generic-purpose) MT system generates a list of candidate translations GEN (q) for each query q; 2. A vector of features F (t) is assig"
E12-1012,P04-1015,0,0.0331885,"verage Retrieval Precision is well defined for a single query translation, while BLEU is defined on the corpus level and correlates poorly with human quality judgements for the individual translations (Specia et al., 2009; Callison-Burch et al., 2009). Finally, the reranking framework allows a lot of flexibility. Thus, it allows enriching the baseline translation model with new complex features which might be difficult to introduce into the translation model directly. Other works applied the reranking framework to different NLP tasks such as Named Entities Extraction (Collins, 2001), parsing (Collins and Roark, 2004), and language modelling (Roark et al., 2004). Most of these works used the reranking framework to combine generative and discriminative methods when both approaches aim at solving the same problem: the generative model produces a set of hypotheses, and the best hypothesis is chosen afterwards via the discriminative reranking model, which allows to enrich the baseline model with the new complex and heterogeneous features. We suggest using the reranking framework to combine two different tasks: Machine Translation and Cross-lingual Information Retrieval. In this context the reranking framework"
E12-1012,W07-0733,0,0.0405138,"Missing"
E12-1012,N03-1017,0,0.00618548,"9) between standard MT evaluation metrics (METEOR(Banerjee and Lavie, 2005), BLEU, NIST(Doddington, 2002)) and retrieval precision for long queries. However, the same work shows that the correlation decreases ( 0.6 − 0.7) for short queries. In this paper we propose two approaches to SMT adaptation for queries. The first one optimizes BLEU, while the second one optimizes Mean Average Precision (MAP), a standard metric in information retrieval. We’ll address the issue of the correlation between BLEU and MAP in Section 4. Both of the proposed approaches rely on the phrase-based SMT (PBMT) model (Koehn et al., 2003) implemented in the Open Source SMT toolkit MOSES (Koehn et al., 2007). 3.1 Tuning for genre adaptation First, we propose to adapt the PBMT model by tuning the model’s weights on a parallel set of queries. This approach addresses the first aspect of the problem, which is producing a “good” translation. The PBMT model combines different types of features via a log-linear model. The standard features include (Koehn, 2010, Chapter 5): language model, word penalty, distortion, different translation models, etc. The weights of these features are learned during the tuning step with the MERT (Och, 20"
E12-1012,P07-2045,0,0.00706764,"2005), BLEU, NIST(Doddington, 2002)) and retrieval precision for long queries. However, the same work shows that the correlation decreases ( 0.6 − 0.7) for short queries. In this paper we propose two approaches to SMT adaptation for queries. The first one optimizes BLEU, while the second one optimizes Mean Average Precision (MAP), a standard metric in information retrieval. We’ll address the issue of the correlation between BLEU and MAP in Section 4. Both of the proposed approaches rely on the phrase-based SMT (PBMT) model (Koehn et al., 2003) implemented in the Open Source SMT toolkit MOSES (Koehn et al., 2007). 3.1 Tuning for genre adaptation First, we propose to adapt the PBMT model by tuning the model’s weights on a parallel set of queries. This approach addresses the first aspect of the problem, which is producing a “good” translation. The PBMT model combines different types of features via a log-linear model. The standard features include (Koehn, 2010, Chapter 5): language model, word penalty, distortion, different translation models, etc. The weights of these features are learned during the tuning step with the MERT (Och, 2003) algorithm. Roughly the MERT algorithm tunes feature weights one by"
E12-1012,J10-4005,0,0.0056841,"ation retrieval. We’ll address the issue of the correlation between BLEU and MAP in Section 4. Both of the proposed approaches rely on the phrase-based SMT (PBMT) model (Koehn et al., 2003) implemented in the Open Source SMT toolkit MOSES (Koehn et al., 2007). 3.1 Tuning for genre adaptation First, we propose to adapt the PBMT model by tuning the model’s weights on a parallel set of queries. This approach addresses the first aspect of the problem, which is producing a “good” translation. The PBMT model combines different types of features via a log-linear model. The standard features include (Koehn, 2010, Chapter 5): language model, word penalty, distortion, different translation models, etc. The weights of these features are learned during the tuning step with the MERT (Och, 2003) algorithm. Roughly the MERT algorithm tunes feature weights one by one and optimizes them according to the BLEU score obtained. Our hypothesis is that the impact of different features should be different depending on whether we translate a full sentence, or a query-genre entry. Thus, one would expect that in the case of query-genre the language model or the distortion features should get less importance than in the"
E12-1012,J03-3003,0,0.0142545,"in groups of approaches to CLIR: document translation and query translation. We concentrate on the second group which is more relevant to our settings. The standard query translation methods use different translation resources such as bilingual dictionaries, parallel corpora and/or machine translation. The aspect of disambiguation is important for the first two techniques. Different methods were proposed to deal with disambiguation issues, often relying on the document collection or embedding the translation step directly into the retrieval model (Hiemstra and Jong, 1999; Berger et al., 1999; Kraaij et al., 2003). Other methods rely on external resources like query logs (Gao et al., 2010), Wikipedia (Jadidinejad and Mahmoudi, 2009) or the web (Nie and Chen, 2002; Hu et al., 2008). (Gao et al., 2006) proposes syntax-based translation models to deal with the disambiguation issues (NP-based, dependency-based). The candidate translations proposed by these models are then reranked with the model learned to minimize the translation er2 Standard MT evaluation metric ror on the training data. To our knowledge, existing work that use MTbased techniques for query translation use an outof-the-box MT system, with"
E12-1012,W08-0407,1,0.877103,"er which is crucial for translation quality (and is taken into account by most MT evaluation metrics) is often ignored by IR models. Our second approach follows (Nie, 2010, pp.106) argument that “the translation problem is an integral part of the whole CLIR problem, and unified CLIR models integrating translation should be defined”. We propose integrating the IR metric (MAP) into the translation model optimisation step via the reranking framework. Previous attempts to apply the reranking approach to SMT did not show significant improvements in terms of MT evaluation metrics (Och et al., 2003; Nikoulina and Dymetman, 2008). One of the reasons being the poor diversity of the Nbest list of the translations. However, we be111 lieve that this approach has more potential in the context of query translation. First of all the average query length is ˜5 words, which means that the Nbest list of the translations is more diverse than in the case of general phrase translation (average length 25-30 words). Moreover, the retrieval precision is more naturally integrated into the reranking framework than standard MT evaluation metrics such as BLEU. The main reason is that the notion of Average Retrieval Precision is well defi"
E12-1012,J03-1002,0,0.00189185,"access to the relevance judgements. We didn’t have access to all relevance judgements of the previously desribed tracks. Thus we used only a subset of the previously extracted parallel set, which includes CLEF 2000-2008 topics from the AdHoc-main, AdHocTEL and GeoCLEF tracks. The number of queries obtained altogether is shown in (Table 1). 4.1.2 Baseline We tested our approaches on the CLEF AdHocTEL 2009 task (50 topics). This task dealt with monolingual and cross-lingual search in a library catalog. The monolingual retrieval is 3 This alignment can be either produced by a toolkit like GIZA++(Och and Ney, 2003) or obtained directly by a system that produced the Nbest list of the translations (Moses). 113 Language pair Number of queries Total queries En - Fr, Fr - En 470 En - De, De - En 714 Annotated queries En - Fr, Fr - En 400 En - De, De - En 350 Table 1: Top: total number of parallel queries gathered from all the CLEF tasks (size of the tuning set). Bottom: number of queries extracted from the tasks for which the human relevance judgements were availble (size of the reranking training set). performed with the lemur4 toolkit (Ogilvie and Callan, 2001). The preprocessing includes lemmatisation (wi"
E12-1012,P03-1021,0,0.00602668,", 2003) implemented in the Open Source SMT toolkit MOSES (Koehn et al., 2007). 3.1 Tuning for genre adaptation First, we propose to adapt the PBMT model by tuning the model’s weights on a parallel set of queries. This approach addresses the first aspect of the problem, which is producing a “good” translation. The PBMT model combines different types of features via a log-linear model. The standard features include (Koehn, 2010, Chapter 5): language model, word penalty, distortion, different translation models, etc. The weights of these features are learned during the tuning step with the MERT (Och, 2003) algorithm. Roughly the MERT algorithm tunes feature weights one by one and optimizes them according to the BLEU score obtained. Our hypothesis is that the impact of different features should be different depending on whether we translate a full sentence, or a query-genre entry. Thus, one would expect that in the case of query-genre the language model or the distortion features should get less importance than in the case of the full-sentence translation. MERT tuning on a genre-adapted parallel corpus should leverage this information from the data, adapting the SMT model to the query-genre. We"
E12-1012,2001.mtsummit-papers.68,0,0.0200987,"corpora in order to adapt the SMT model parameters for query translation. In our approach the parameters of the SMT models are optimized on the basis of the parallel queries set. This is achieved either directly in the SMT system using the MERT (Minimum Error Rate Training) algorithm and optimiz1 Insufficient for a full SMT system training (˜500 entries) 109 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 109–119, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics ing according to the BLEU2 (Papineni et al., 2001) score, or via reranking the Nbest translation candidates generated by a baseline system based on new parameters (and possibly new features) that aim to optimize a retrieval metric. It is important to note that both of the proposed approaches allow keeping the MT system independent of the document collection and indexing, and thus suitable for a query translation service. These two approaches can also be combined by using the model produced with the first approach as a baseline that produces the Nbest list of translations that is then given to the reranking approach. The remainder of this pape"
E12-1012,2011.eamt-1.40,0,0.0198972,"Missing"
E12-1012,P04-1007,0,0.0123348,"ingle query translation, while BLEU is defined on the corpus level and correlates poorly with human quality judgements for the individual translations (Specia et al., 2009; Callison-Burch et al., 2009). Finally, the reranking framework allows a lot of flexibility. Thus, it allows enriching the baseline translation model with new complex features which might be difficult to introduce into the translation model directly. Other works applied the reranking framework to different NLP tasks such as Named Entities Extraction (Collins, 2001), parsing (Collins and Roark, 2004), and language modelling (Roark et al., 2004). Most of these works used the reranking framework to combine generative and discriminative methods when both approaches aim at solving the same problem: the generative model produces a set of hypotheses, and the best hypothesis is chosen afterwards via the discriminative reranking model, which allows to enrich the baseline model with the new complex and heterogeneous features. We suggest using the reranking framework to combine two different tasks: Machine Translation and Cross-lingual Information Retrieval. In this context the reranking framework doesn’t only allow enriching the baseline tra"
E12-1012,2009.eamt-1.5,0,0.00586229,"f query translation. First of all the average query length is ˜5 words, which means that the Nbest list of the translations is more diverse than in the case of general phrase translation (average length 25-30 words). Moreover, the retrieval precision is more naturally integrated into the reranking framework than standard MT evaluation metrics such as BLEU. The main reason is that the notion of Average Retrieval Precision is well defined for a single query translation, while BLEU is defined on the corpus level and correlates poorly with human quality judgements for the individual translations (Specia et al., 2009; Callison-Burch et al., 2009). Finally, the reranking framework allows a lot of flexibility. Thus, it allows enriching the baseline translation model with new complex features which might be difficult to introduce into the translation model directly. Other works applied the reranking framework to different NLP tasks such as Named Entities Extraction (Collins, 2001), parsing (Collins and Roark, 2004), and language modelling (Roark et al., 2004). Most of these works used the reranking framework to combine generative and discriminative methods when both approaches aim at solving the same problem"
E12-1012,D07-1080,0,0.0172392,"line MT model for each query qi , i = 1..K. • Each translation t ∈ GEN (qi ) is used to perform a retrieval from a target document collection, and an Average Precision score (AP (t)) is computed for each t ∈ GEN (qi ) by comparing its retrieval to the relevance annotations done during the CLEF campaign. The weights λ are learned with the objective of maximizing MAP for all the queries of the training set, and, therefore, are optimized for retrieval quality. The weights optimization is done with the Margin Infused Relaxed Algorithm (MIRA)(Crammer and Singer, 2003), which was applied to SMT by (Watanabe et al., 2007; Chiang et al., 2008). MIRA is an online learning algorithm where each weights update is done to keep the new weights as close as possible to the old weights (first term), and score oracle translation (the translation giving the best retrieval score : t∗i = arg maxt AP (t)) higher than each non-oracle translation (tij ) by a margin at least as wide as the loss lij (second term): 0 λ = minλ0 21 kλ − λk2 +   P 0 ∗ ) − F (t ) C K max l − λ · (F (t ij ij j=1..N i i=1 1. The baseline (generic-purpose) MT system generates a list of candidate translations GEN (q) for each query q; 2. A vector of f"
E12-1012,C08-1125,0,0.0128819,"010), Wikipedia (Jadidinejad and Mahmoudi, 2009) or the web (Nie and Chen, 2002; Hu et al., 2008). (Gao et al., 2006) proposes syntax-based translation models to deal with the disambiguation issues (NP-based, dependency-based). The candidate translations proposed by these models are then reranked with the model learned to minimize the translation er2 Standard MT evaluation metric ror on the training data. To our knowledge, existing work that use MTbased techniques for query translation use an outof-the-box MT system, without adapting it for query translation in particular (Jones et al., 1999; Wu et al., 2008) (although some query expansion techniques might be applied to the produced translation afterwards (Wu and He, 2010)). There is a number of works done for domain adaptation in Statistical Machine Translation. However, we want to distinguish between genre and domain adaptation in this work. Generally, genre can be seen as a sub-problem of domain. Thus, we consider genre to be the general style of the text e.g. conversation, news, blog, query (responsible mostly for the text structure) while the domain reflects more what the text is about – eg. social science, healthcare, history, so domain adap"
E12-1012,C04-1059,0,0.0137281,"Missing"
E12-1012,P02-1040,0,\N,Missing
E12-1012,W09-0401,1,\N,Missing
E12-1012,W05-0909,0,\N,Missing
E12-1012,P02-1062,0,\N,Missing
E12-1036,E09-1017,0,0.0292109,"usage, time and place, which are generally useful for the detection of online malicious activities (West et al., 2010; West and Lee, 2011). We deliberately refrain from using such features. A wide range of methods and approaches has been applied to the similar tasks of textual entailment and paraphrase recognition, see Androutsopoulos and Malakasiotis (2010) for a comprehensive review. These are all related because paraphrases and bidirectional entailments represent types of fluency edits. A different line of research uses classifiers to predict sentence-level fluency (Zwarts and Dras, 2008; Chae and Nenkova, 2009). These could be useful for fluency edits detection. Alternatively, user edits could be a potential source of humanproduced training data for fluency models. 3 Definition of User Edits Scope Within our approach we distinguish between edit segments, which represent the comparison (diff) between two document revisions, and user edits, which are the input for classification. An edit segment is a contiguous sequence of deleted, inserted or equal words. The difference between two document revisions (vi , vj ) is represented by a sequence of edit segments E. Each edit segment (δ, w1m ) ∈ E is a pair"
E12-1036,P09-3004,0,0.0272324,"e same classifier), and by ∧ w.r.t to another classifier marked by ∨ (using the same features). Highest accuracy per classifier is marked in bold. 5.2 Feature Analysis We experiment with three classifiers: Support Vector Machines (SVM), Random Forests (RF) and Logistic Regression (Logit).7 SVMs (Cortes and Vapnik, 1995) and Logistic Regression (or Maximum Entropy classifiers) are two widely used machine learning techniques. SVMs have been applied to many text classification problems (Joachims, 1998). Maximum Entropy classifiers have been applied to the similar tasks of paraphrase recognition (Malakasiotis, 2009) and textual entailment (Hickl et al., 2006). Random Forests (Breiman, 2001) as well as other decision tree algorithms are successfully used for classifying Wikipedia edits for the purpose of vandalism detection (Potthast et al., 2010; Potthast and Holfeld, 2011). Experiments begin with the edit-distance basescience.uva.nl/˜abronner/uec/data. 7 Using Weka classifiers: SMO (SVM), RandomForest & Logistic (Hall et al., 2009). Classifier’s parameters are tuned using the held-out development set. Feature set SVM flu. / fac. RF flu. / fac. Logit flu. / fac. Baseline + Char-level + Word-level + PoS +"
E12-1036,max-wisniewski-2010-mining,0,0.370748,"n edits per month is about 4 million in English and almost 11 million in total for all languages.2 Christof Monz Informatics Institute University of Amsterdam c.monz@uva.nl Exploiting document revision histories has proven useful for a variety of natural language processing (NLP) tasks, including sentence compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008) and simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011), information retrieval (Aji et al., 2010; Nunes et al., 2011), textual entailment recognition (Zanzotto and Pennacchiotti, 2010), and paraphrase extraction (Max and Wisniewski, 2010; Dutrey et al., 2011). The ability to distinguish between factual changes or edits, which alter the meaning, and fluency edits, which improve the style or readability, is a crucial requirement for approaches exploiting revision histories. The need for an automated classification method has been identified (Nelken and Yamangil, 2008; Max and Wisniewski, 2010), but to the best of our knowledge has not been directly addressed. Previous approaches have either applied simple heuristics (Yatskar et al., 2010; Woodsend and Lapata, 2011) or manual annotations (Dutrey et al., 2011) to restrict the dat"
E12-1036,2006.amta-papers.25,0,0.0261469,"NN NNS VBD VBN IN NNP DT NN NN VBD VBN IN NNP 2 NE tags pre post LOCATION LOCATION 0 Table 2: Word- and tag-level edit distance measured over different representations (example from Wikipedia revisions 2678278 & 2682972). Fluency edits may shift words, which sometimes may be slightly modified. Fluency edits may also add or remove words that already appear in context. Optimal calculation of edit distance with shifts is computationally expensive (Shapira and Storer, 2002). Translation error rate (TER) provides an approximation but it is designed for the needs of machine translation evaluation (Snover et al., 2006). To have a more sensitive estimation of the degree of edit, we compute the minimal character-level edit distance between every pair of words that belong to different edit segments. For each pair of edit segments (δ, w1m ), (δ 0 , w0 k1 ) overlapping with a user edit, if δ 6= δ 0 we compute: ∀w ∈ w1m : min EditDist(w, w0 ) w0 ∈w0 k1 the deleted NE tag and the inserted PoS tag. This is an inherent weakness of these features when compared to parsing-based alternatives. An additional set of counts, NE values, describes the number of deleted, inserted and equal normalized values of numeric entitie"
E12-1036,D11-1038,0,0.211225,"ing. According to Wikipedia statistics, in August 2011 the English Wikipedia contained 3.8 million articles with an average of 78.3 revisions per article. The average number of revision edits per month is about 4 million in English and almost 11 million in total for all languages.2 Christof Monz Informatics Institute University of Amsterdam c.monz@uva.nl Exploiting document revision histories has proven useful for a variety of natural language processing (NLP) tasks, including sentence compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008) and simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011), information retrieval (Aji et al., 2010; Nunes et al., 2011), textual entailment recognition (Zanzotto and Pennacchiotti, 2010), and paraphrase extraction (Max and Wisniewski, 2010; Dutrey et al., 2011). The ability to distinguish between factual changes or edits, which alter the meaning, and fluency edits, which improve the style or readability, is a crucial requirement for approaches exploiting revision histories. The need for an automated classification method has been identified (Nelken and Yamangil, 2008; Max and Wisniewski, 2010), but to the best of our knowledge has not been directly"
E12-1036,P08-2035,0,0.523664,"Such data is publicly available in large volumes and constantly growing. According to Wikipedia statistics, in August 2011 the English Wikipedia contained 3.8 million articles with an average of 78.3 revisions per article. The average number of revision edits per month is about 4 million in English and almost 11 million in total for all languages.2 Christof Monz Informatics Institute University of Amsterdam c.monz@uva.nl Exploiting document revision histories has proven useful for a variety of natural language processing (NLP) tasks, including sentence compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008) and simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011), information retrieval (Aji et al., 2010; Nunes et al., 2011), textual entailment recognition (Zanzotto and Pennacchiotti, 2010), and paraphrase extraction (Max and Wisniewski, 2010; Dutrey et al., 2011). The ability to distinguish between factual changes or edits, which alter the meaning, and fluency edits, which improve the style or readability, is a crucial requirement for approaches exploiting revision histories. The need for an automated classification method has been identified (Nelken and Yamangil, 2008; Max and Wisni"
E12-1036,N10-1056,0,0.381779,"es and constantly growing. According to Wikipedia statistics, in August 2011 the English Wikipedia contained 3.8 million articles with an average of 78.3 revisions per article. The average number of revision edits per month is about 4 million in English and almost 11 million in total for all languages.2 Christof Monz Informatics Institute University of Amsterdam c.monz@uva.nl Exploiting document revision histories has proven useful for a variety of natural language processing (NLP) tasks, including sentence compression (Nelken and Yamangil, 2008; Yamangil and Nelken, 2008) and simplification (Yatskar et al., 2010; Woodsend and Lapata, 2011), information retrieval (Aji et al., 2010; Nunes et al., 2011), textual entailment recognition (Zanzotto and Pennacchiotti, 2010), and paraphrase extraction (Max and Wisniewski, 2010; Dutrey et al., 2011). The ability to distinguish between factual changes or edits, which alter the meaning, and fluency edits, which improve the style or readability, is a crucial requirement for approaches exploiting revision histories. The need for an automated classification method has been identified (Nelken and Yamangil, 2008; Max and Wisniewski, 2010), but to the best of our know"
E12-1036,C08-1145,0,0.020744,"etection by patterns of usage, time and place, which are generally useful for the detection of online malicious activities (West et al., 2010; West and Lee, 2011). We deliberately refrain from using such features. A wide range of methods and approaches has been applied to the similar tasks of textual entailment and paraphrase recognition, see Androutsopoulos and Malakasiotis (2010) for a comprehensive review. These are all related because paraphrases and bidirectional entailments represent types of fluency edits. A different line of research uses classifiers to predict sentence-level fluency (Zwarts and Dras, 2008; Chae and Nenkova, 2009). These could be useful for fluency edits detection. Alternatively, user edits could be a potential source of humanproduced training data for fluency models. 3 Definition of User Edits Scope Within our approach we distinguish between edit segments, which represent the comparison (diff) between two document revisions, and user edits, which are the input for classification. An edit segment is a contiguous sequence of deleted, inserted or equal words. The difference between two document revisions (vi , vj ) is represented by a sequence of edit segments E. Each edit segmen"
E12-1036,W10-3504,0,\N,Missing
E14-1004,2011.eamt-1.35,0,0.0346499,"Missing"
E14-1004,D12-1089,0,0.0270402,"Missing"
E14-1004,C08-2005,0,0.0512054,"Missing"
E14-1004,N10-1140,0,0.0264721,"Missing"
E14-1004,W11-2165,0,0.0141625,"r die können can between der switzerland entwicklung on spent zwischen you mir tell sagen me mich für as far as development the sie i eines am and namen the of , an wie you die concerned , will qualität the of quality the european der of union energiesektors able sustainable resulted to energy reform is , ensure wichtiger in ausgegeben werden reform of commission union sicherstellen be deal kommission der environmentally sie good europäischen umweltgerechten how ist im a sentence pairs can improve translation quality.1 As the number of such components is much bigger than the continuous ones, (Gimpel and Smith, 2011) propose a Bayesian nonparametric model for finding the most probable discontinuous phrase pairs. This can also be done from the N -best lists that are generated in Section 4, and it would be interesting to see the effect of such phrase pairs in our existing models. In a longer version of this work we intend to study the effect in translation quality when varying some of the parameters (size of N -best lists, sample sizes for training gain in Section 2 and for the CE method), as well as when extracting source-driven bilingual segmentations as in (Sanchis-Trilles et al., 2011). menge gab das in"
E14-1004,J97-4004,0,0.0471411,"is fairly general it is thus suitable for most NLP tasks. In particular, it is tailored to the type of segments that are suitable for the purposes of SMT and is in line with previous work (Blackwood et al., 2008; Paul et al., 2010). With this definition in mind, we introduce a monolingual segment quality measure that is based on assessing the cost of converting one segmentation into another by means of an elementary operation. This operation, namely the ‘splitting’ of a segment into two segments, together with all possible segmentations of a sentence are known to form a partially ordered set (Guo, 1997). Such a construction is known as partition refinement and gives rise to the desired monolingual surface quality measure. The presence of word alignments between the sentence pair provides additional structure which should not be ignored. In the language of graph theory, a segment can also be viewed as a chain, i.e., a graph in which vertices are the segment’s words and Given a pair of source and target language sentences which are translations of each other with known word alignments between them, we extract bilingual phrase-level segmentations of such a pair. This is done by identifying two"
E14-1004,D07-1103,0,0.0695845,"Missing"
E14-1004,P07-2045,0,0.027844,"Missing"
E14-1004,N03-1017,0,0.13608,"Conference of the European Chapter of the Association for Computational Linguistics, pages 30–38, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics • σ 000 : (s1 s2 )(s3 s4 )(s5 ), by splitting at the second position of the second segment of σ, an edge between two words exists if and only if these words are consecutive. Then, a bilingual segmentation is represented by the graph that is formed by all its source and target language chains together with edges induced by word alignments. Motivated by the phrase pair extraction methods of SMT (Och et al., 1999; Koehn et al., 2003), we focus on the connected components, or simply components of such a representation. We explain that the extent to which we can delete word alignments from a component without violating its component status, gives rise to a bilingual, purely structural quality measure. The surface and structural measures are incorporated in one algorithm that extracts an N -best list of bilingual word-aligned segmentations. This algorithm, which is an adaptation of the Cross-Entropy method (Rubinstein, 1997), performs joint maximization of surface (in both languages) and structural quality measures. Componen"
E14-1004,W13-3010,1,0.875814,"Missing"
E14-1004,P03-1021,0,0.0187749,"the baseline and our system are standard phrase-based MT systems. Bidirectional word alignments are generated with GIZA++ (Och and Ney, 2003) and ‘grow-diag-final-and’. These are used to construct a phrase-table with bidirectional phrase probabilities, lexical weights and a reordering model with monotone, swap and discontinuous orientations, conditioned on both the previous and the next phrase. 4-gram interpolated language models with Kneser-Ney smoothing are built with SRILM (Stolcke, 2002). A distortion limit of 6 and a phrasepenalty are also used. All model parameters are tuned with MERT (Och, 2003). Decoding during tuning and testing is done with Moses (Koehn et. al, 2007). Since our system only affects which phrases are extracted, lexical weights and reordering orientations are the same for both systems. Datasets are from the WMT’13 translation task (Bojar et al., 2013): Translation and reordering models are trained on Czech–English and German– English corpora (Table 1). Language models and segment measures gain, as defined in (5), are trained on 35.3M Czech, 50.0M German and 94.5M English sentences from the provided monolingual data. Tuning is done on newstest2010 and performance is e"
E14-1004,J03-1002,0,0.00733383,"ion process. 5 Europarl (v7) News Commentary (v8) Total Cz–En 642,505 139,679 782,184 De–En 1,889,791 177,079 2,066,870 Table 1: Number of filtered parallel sentences for Czech–English and German–English. that correspond to components only. A reduction in phrase-table size is guaranteed because we are essentially extracting only a subset of all possible continuous phrase pairs. The challenge is to verify whether this subset can provide a sufficient translation model. Both the baseline and our system are standard phrase-based MT systems. Bidirectional word alignments are generated with GIZA++ (Och and Ney, 2003) and ‘grow-diag-final-and’. These are used to construct a phrase-table with bidirectional phrase probabilities, lexical weights and a reordering model with monotone, swap and discontinuous orientations, conditioned on both the previous and the next phrase. 4-gram interpolated language models with Kneser-Ney smoothing are built with SRILM (Stolcke, 2002). A distortion limit of 6 and a phrasepenalty are also used. All model parameters are tuned with MERT (Och, 2003). Decoding during tuning and testing is done with Moses (Koehn et. al, 2007). Since our system only affects which phrases are extrac"
E14-1004,W99-0604,0,0.581282,"dings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 30–38, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics • σ 000 : (s1 s2 )(s3 s4 )(s5 ), by splitting at the second position of the second segment of σ, an edge between two words exists if and only if these words are consecutive. Then, a bilingual segmentation is represented by the graph that is formed by all its source and target language chains together with edges induced by word alignments. Motivated by the phrase pair extraction methods of SMT (Och et al., 1999; Koehn et al., 2003), we focus on the connected components, or simply components of such a representation. We explain that the extent to which we can delete word alignments from a component without violating its component status, gives rise to a bilingual, purely structural quality measure. The surface and structural measures are incorporated in one algorithm that extracts an N -best list of bilingual word-aligned segmentations. This algorithm, which is an adaptation of the Cross-Entropy method (Rubinstein, 1997), performs joint maximization of surface (in both languages) and structural quali"
E14-1004,W10-1760,0,\N,Missing
E14-1004,P02-1040,0,\N,Missing
E14-1004,W13-2201,1,\N,Missing
H05-1009,W99-0606,0,0.024551,"alignment have combined alignment models, e.g., using a log-linear combination (Och and Ney, 2003) or mutually independent association clues (Tiedemann, 2003). A simpler approach was developed by Ayan et al. (2004), where word alignment outputs are combined using a linear combination of feature weights assigned to the individual aligners. Our method is more general in that it uses a neural network model that is capable of learning nonlinear functions. Classifier ensembles are used in several NLP applications. Some NLP applications for classifier ensembles are POS tagging (Brill and Wu, 1998; Abney et al., 1999), PP attachment (Abney et al., 1999), word sense disambiguation (Florian and Yarowsky, 2002), and parsing (Henderson and Brill, 2000). The work reported in this paper is the first application of classifier ensembles to the word-alignment problem. We use a different methodology to combine classifiers that is based on stacked generalization (Wolpert, 1992), i.e., learning an additional model on the outputs of individual classifiers. 3 Neural Networks A multi-layer perceptron (MLP) is a feed-forward neural network that consists of several units (neurons) that are connected to each other by weight"
H05-1009,ayan-etal-2004-multi,1,0.829301,"n Overview 2 Related Work Previous algorithms for improving word alignments have attempted to incorporate additional knowledge into their modeling. For example, Liu (2005) uses a log-linear combination of linguistic features. Additional linguistic knowledge can be in the form of part-of-speech tags. (Toutanova et al., 2002) or dependency relations (Cherry and Lin, 2003). Other approaches to improving alignment have combined alignment models, e.g., using a log-linear combination (Och and Ney, 2003) or mutually independent association clues (Tiedemann, 2003). A simpler approach was developed by Ayan et al. (2004), where word alignment outputs are combined using a linear combination of feature weights assigned to the individual aligners. Our method is more general in that it uses a neural network model that is capable of learning nonlinear functions. Classifier ensembles are used in several NLP applications. Some NLP applications for classifier ensembles are POS tagging (Brill and Wu, 1998; Abney et al., 1999), PP attachment (Abney et al., 1999), word sense disambiguation (Florian and Yarowsky, 2002), and parsing (Henderson and Brill, 2000). The work reported in this paper is the first application of c"
H05-1009,P98-1029,0,0.0298457,"roaches to improving alignment have combined alignment models, e.g., using a log-linear combination (Och and Ney, 2003) or mutually independent association clues (Tiedemann, 2003). A simpler approach was developed by Ayan et al. (2004), where word alignment outputs are combined using a linear combination of feature weights assigned to the individual aligners. Our method is more general in that it uses a neural network model that is capable of learning nonlinear functions. Classifier ensembles are used in several NLP applications. Some NLP applications for classifier ensembles are POS tagging (Brill and Wu, 1998; Abney et al., 1999), PP attachment (Abney et al., 1999), word sense disambiguation (Florian and Yarowsky, 2002), and parsing (Henderson and Brill, 2000). The work reported in this paper is the first application of classifier ensembles to the word-alignment problem. We use a different methodology to combine classifiers that is based on stacked generalization (Wolpert, 1992), i.e., learning an additional model on the outputs of individual classifiers. 3 Neural Networks A multi-layer perceptron (MLP) is a feed-forward neural network that consists of several units (neurons) that are connected to"
H05-1009,P03-1012,0,0.0712536,"ce and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 65–72, Vancouver, October 2005. 2005 Association for Computational Linguistics ai i Output layer wij j Hidden layer Input layer Figure 1: Multilayer Perceptron Overview 2 Related Work Previous algorithms for improving word alignments have attempted to incorporate additional knowledge into their modeling. For example, Liu (2005) uses a log-linear combination of linguistic features. Additional linguistic knowledge can be in the form of part-of-speech tags. (Toutanova et al., 2002) or dependency relations (Cherry and Lin, 2003). Other approaches to improving alignment have combined alignment models, e.g., using a log-linear combination (Och and Ney, 2003) or mutually independent association clues (Tiedemann, 2003). A simpler approach was developed by Ayan et al. (2004), where word alignment outputs are combined using a linear combination of feature weights assigned to the individual aligners. Our method is more general in that it uses a neural network model that is capable of learning nonlinear functions. Classifier ensembles are used in several NLP applications. Some NLP applications for classifier ensembles are PO"
H05-1009,P97-1003,0,0.0189016,"istinguish between instances. One of the strategies in the classification literature is to supply the input data to the set of features as well. While combining word alignments, we use two types of features to describe each instance (i, j): (1) linguistic features and (2) alignment features. Linguistic features include POS tags of both words (ei and fj ) and a dependency relation for one of the words (ei ). We generate POS tags using the MXPOST tagger (Ratnaparkhi, 1996) for English and Chinese, and Connexor for Spanish. Dependency relations are produced using a version of the Collins parser (Collins, 1997) that has been adapted for building dependencies. Alignment features consist of features that are extracted from the outputs of individual alignment systems. For each alignment Ak ∈ A, the following are some of the alignment features that can be used to describe an instance (i, j): 1. 2. 3. 4. 5. 6. It is also possible to use variants, or combinations, of these features to reduce feature space. Figure 2 shows an example of how we transform the outputs of 2 alignment systems, A1 and A2 , for an alignment link (i, j) into data with some of the features above. We use -1 and 1 to represent the abs"
H05-1009,P02-1033,0,0.0183863,"t alignment combination as a classifier ensemble (Hansen and Salamon, 1990; Wolpert, 1992). The ensemble-based approach was developed to select the best features of different learning algorithms, including those that may not produce a globally optimal solution (Minsky, 1991). Introduction Parallel texts are a valuable resource in natural language processing and essential for projecting knowledge from one language onto another. Word-level alignment is a critical component of a wide range of NLP applications, such as construction of bilingual lexicons (Melamed, 2000), word sense disambiguation (Diab and Resnik, 2002), projection of language resources (Yarowsky et al., 2001), and statistical machine translation. Although word-level aligners tend to perform well when there is sufficient training data, the quality decreases as the size of training data decreases. Even with large amounts of training data, statistical aligners have been shown to be susceptible to mis-aligning phrasal constructions (Dorr et al., 2002) due to many-to-many correspondences, morphological language distinctions, paraphrased and We use neural networks to implement the classifier-ensemble approach, as these have previously been shown"
H05-1009,dorr-etal-2002-duster,1,0.844825,"language onto another. Word-level alignment is a critical component of a wide range of NLP applications, such as construction of bilingual lexicons (Melamed, 2000), word sense disambiguation (Diab and Resnik, 2002), projection of language resources (Yarowsky et al., 2001), and statistical machine translation. Although word-level aligners tend to perform well when there is sufficient training data, the quality decreases as the size of training data decreases. Even with large amounts of training data, statistical aligners have been shown to be susceptible to mis-aligning phrasal constructions (Dorr et al., 2002) due to many-to-many correspondences, morphological language distinctions, paraphrased and We use neural networks to implement the classifier-ensemble approach, as these have previously been shown to be effective for combining classifiers (Hansen and Salamon, 1990). Neural nets with 2 or more layers and non-linear activation functions are capable of learning any function of the feature space with arbitrarily small error. Neural nets have been shown to be effective with (1) highdimensional input vectors, (2) relatively sparse data, and (3) noisy data with high within-class variability, all of w"
H05-1009,W02-1004,0,0.0211778,"and Ney, 2003) or mutually independent association clues (Tiedemann, 2003). A simpler approach was developed by Ayan et al. (2004), where word alignment outputs are combined using a linear combination of feature weights assigned to the individual aligners. Our method is more general in that it uses a neural network model that is capable of learning nonlinear functions. Classifier ensembles are used in several NLP applications. Some NLP applications for classifier ensembles are POS tagging (Brill and Wu, 1998; Abney et al., 1999), PP attachment (Abney et al., 1999), word sense disambiguation (Florian and Yarowsky, 2002), and parsing (Henderson and Brill, 2000). The work reported in this paper is the first application of classifier ensembles to the word-alignment problem. We use a different methodology to combine classifiers that is based on stacked generalization (Wolpert, 1992), i.e., learning an additional model on the outputs of individual classifiers. 3 Neural Networks A multi-layer perceptron (MLP) is a feed-forward neural network that consists of several units (neurons) that are connected to each other by weighted links. As illustrated in Figure 1, an MLP consists 66 of one input layer, one or more hid"
H05-1009,A00-2005,0,0.0117646,"sociation clues (Tiedemann, 2003). A simpler approach was developed by Ayan et al. (2004), where word alignment outputs are combined using a linear combination of feature weights assigned to the individual aligners. Our method is more general in that it uses a neural network model that is capable of learning nonlinear functions. Classifier ensembles are used in several NLP applications. Some NLP applications for classifier ensembles are POS tagging (Brill and Wu, 1998; Abney et al., 1999), PP attachment (Abney et al., 1999), word sense disambiguation (Florian and Yarowsky, 2002), and parsing (Henderson and Brill, 2000). The work reported in this paper is the first application of classifier ensembles to the word-alignment problem. We use a different methodology to combine classifiers that is based on stacked generalization (Wolpert, 1992), i.e., learning an additional model on the outputs of individual classifiers. 3 Neural Networks A multi-layer perceptron (MLP) is a feed-forward neural network that consists of several units (neurons) that are connected to each other by weighted links. As illustrated in Figure 1, an MLP consists 66 of one input layer, one or more hidden layers, and one output layer. The ext"
H05-1009,N03-1017,0,0.0342375,"Missing"
H05-1009,P05-1057,0,0.153569,"Missing"
H05-1009,J00-2004,0,0.0720442,"s a pattern classification problem and treat alignment combination as a classifier ensemble (Hansen and Salamon, 1990; Wolpert, 1992). The ensemble-based approach was developed to select the best features of different learning algorithms, including those that may not produce a globally optimal solution (Minsky, 1991). Introduction Parallel texts are a valuable resource in natural language processing and essential for projecting knowledge from one language onto another. Word-level alignment is a critical component of a wide range of NLP applications, such as construction of bilingual lexicons (Melamed, 2000), word sense disambiguation (Diab and Resnik, 2002), projection of language resources (Yarowsky et al., 2001), and statistical machine translation. Although word-level aligners tend to perform well when there is sufficient training data, the quality decreases as the size of training data decreases. Even with large amounts of training data, statistical aligners have been shown to be susceptible to mis-aligning phrasal constructions (Dorr et al., 2002) due to many-to-many correspondences, morphological language distinctions, paraphrased and We use neural networks to implement the classifier-ense"
H05-1009,P00-1056,0,0.0698415,"links (in the gold standard) for the same set of sentences. Precision (P r), recall (Rc) and alignment error rate (AER) are defined as follows: |A ∩ P | |A ∩ S| Rc = |A| |S| |A ∩ S |+ |A ∩ P | AER = 1 − |A |+ |S| 2. A set of 491 English-Chinese sentence pairs (nearly 13K words on each side) from 2002 NIST MT evaluation test set. We computed precision, recall and error rate on the entire set of sentence pairs for each data set.5 To evaluate NeurAlign, we used GIZA++ in both directions (E-to-F and F -to-E, where F is either Chinese (C) or Spanish (S)) as input and a refined alignment approach (Och and Ney, 2000) that uses a heuristic combination method called grow-diagfinal (Koehn et al., 2003) for comparison. (We henceforth refer to the refined-alignment approach as “RA.”) For the English-Spanish experiments, GIZA++ was trained on 48K sentence pairs from a mixed corpus (UN + Bible + FBIS), with nearly 1.2M of words on each side, using 10 iterations of Model 1, 5 iterations of HMM, and 5 iterations of Model 4. For the English-Chinese experiments, we used 107K sentence pairs from FBIS corpus (nearly 4.1M English and 3.3M Chinese words) to train GIZA++, using 5 iterations of Model 1, 5 iterations of HM"
H05-1009,J03-1002,0,0.0194675,"iation for Computational Linguistics ai i Output layer wij j Hidden layer Input layer Figure 1: Multilayer Perceptron Overview 2 Related Work Previous algorithms for improving word alignments have attempted to incorporate additional knowledge into their modeling. For example, Liu (2005) uses a log-linear combination of linguistic features. Additional linguistic knowledge can be in the form of part-of-speech tags. (Toutanova et al., 2002) or dependency relations (Cherry and Lin, 2003). Other approaches to improving alignment have combined alignment models, e.g., using a log-linear combination (Och and Ney, 2003) or mutually independent association clues (Tiedemann, 2003). A simpler approach was developed by Ayan et al. (2004), where word alignment outputs are combined using a linear combination of feature weights assigned to the individual aligners. Our method is more general in that it uses a neural network model that is capable of learning nonlinear functions. Classifier ensembles are used in several NLP applications. Some NLP applications for classifier ensembles are POS tagging (Brill and Wu, 1998; Abney et al., 1999), PP attachment (Abney et al., 1999), word sense disambiguation (Florian and Yar"
H05-1009,E03-1026,0,0.0202233,"idden layer Input layer Figure 1: Multilayer Perceptron Overview 2 Related Work Previous algorithms for improving word alignments have attempted to incorporate additional knowledge into their modeling. For example, Liu (2005) uses a log-linear combination of linguistic features. Additional linguistic knowledge can be in the form of part-of-speech tags. (Toutanova et al., 2002) or dependency relations (Cherry and Lin, 2003). Other approaches to improving alignment have combined alignment models, e.g., using a log-linear combination (Och and Ney, 2003) or mutually independent association clues (Tiedemann, 2003). A simpler approach was developed by Ayan et al. (2004), where word alignment outputs are combined using a linear combination of feature weights assigned to the individual aligners. Our method is more general in that it uses a neural network model that is capable of learning nonlinear functions. Classifier ensembles are used in several NLP applications. Some NLP applications for classifier ensembles are POS tagging (Brill and Wu, 1998; Abney et al., 1999), PP attachment (Abney et al., 1999), word sense disambiguation (Florian and Yarowsky, 2002), and parsing (Henderson and Brill, 2000). The w"
H05-1009,W02-1012,0,0.043247,"Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 65–72, Vancouver, October 2005. 2005 Association for Computational Linguistics ai i Output layer wij j Hidden layer Input layer Figure 1: Multilayer Perceptron Overview 2 Related Work Previous algorithms for improving word alignments have attempted to incorporate additional knowledge into their modeling. For example, Liu (2005) uses a log-linear combination of linguistic features. Additional linguistic knowledge can be in the form of part-of-speech tags. (Toutanova et al., 2002) or dependency relations (Cherry and Lin, 2003). Other approaches to improving alignment have combined alignment models, e.g., using a log-linear combination (Och and Ney, 2003) or mutually independent association clues (Tiedemann, 2003). A simpler approach was developed by Ayan et al. (2004), where word alignment outputs are combined using a linear combination of feature weights assigned to the individual aligners. Our method is more general in that it uses a neural network model that is capable of learning nonlinear functions. Classifier ensembles are used in several NLP applications. Some N"
H05-1009,H01-1035,0,0.0980243,"Missing"
H05-1009,W96-0213,0,\N,Missing
H05-1009,P02-1038,0,\N,Missing
H05-1009,C98-1029,0,\N,Missing
H05-1024,ayan-etal-2004-multi,1,0.815206,"Ney, 2003), techniques that rely on dependency relations (Cherry and Lin, 2003), and a log-linear combination of IBM Model 3 alignment probabilities, POS tags, and bilingual dictionary coverage (Liu et al., 2005). A common theme for these methods is the use of additional features for enriching the alignment process. These methods perform better than the IBM models and their variants but still tend to make similar errors because of the bias in their alignment modeling. We adopt an approach that post-processes a given alignment using linguistically-oriented rules. The idea is similar to that of Ayan et al. (2004), where manually-crafted rules are used to correct alignment links related to language divergences. Our approach differs, however, in that the rules are extracted automatically—not manually—by examining an initial alignment and categorizing the errors according to features of the words. 186 Annotated Corpus Initial Annotation Rule Instantiation Rule Application Templates Ground Truth Best Rule Selection Rules Figure 1: TBL Architecture 3 Related Work Corpus Transformation-based Learning As shown in Figure 1, the input to TBL is an unannotated corpus that is first passed to an initial annotator"
H05-1024,J95-4004,0,0.248379,"Missing"
H05-1024,J93-2003,0,0.00596484,"185–192, Vancouver, October 2005. 2005 Association for Computational Linguistics The rest of the paper is organized as follows: In the next section we describe previous work on improving word alignments. Section 3 presents a brief overview of TBL. Section 4 describes the adaptation of TBL to the word alignment problem. Section 5 compares ALP to various alignments and presents results on English-Spanish and EnglishChinese. We show that ALP yields a significant reductions in alignment error rate over that of the best performing alignment system. 2 One of the major problems with the IBM models (Brown et al., 1993) and the HMM models (Vogel et al., 1996) is that they are restricted to the alignment of each source-language word to at most one targetlanguage word. The standard method to overcome this problem to use the model in both directions (interchanging the source and target languages) and applying heuristic-based combination techniques to produce a refined alignment (Och and Ney, 2000; Koehn et al., 2003)—henceforth referred to as “RA.” Several researchers have proposed algorithms for improving word alignment systems by injecting additional knowledge or combining different alignment models. These ap"
H05-1024,P03-1012,0,0.0427099,"rchanging the source and target languages) and applying heuristic-based combination techniques to produce a refined alignment (Och and Ney, 2000; Koehn et al., 2003)—henceforth referred to as “RA.” Several researchers have proposed algorithms for improving word alignment systems by injecting additional knowledge or combining different alignment models. These approaches include an enhanced HMM alignment model that uses part-ofspeech tags (Toutanova et al., 2002), a log-linear combination of IBM translation models and HMM models (Och and Ney, 2003), techniques that rely on dependency relations (Cherry and Lin, 2003), and a log-linear combination of IBM Model 3 alignment probabilities, POS tags, and bilingual dictionary coverage (Liu et al., 2005). A common theme for these methods is the use of additional features for enriching the alignment process. These methods perform better than the IBM models and their variants but still tend to make similar errors because of the bias in their alignment modeling. We adopt an approach that post-processes a given alignment using linguistically-oriented rules. The idea is similar to that of Ayan et al. (2004), where manually-crafted rules are used to correct alignment"
H05-1024,P97-1003,0,0.0429404,"links based on linguistic features of words, rather than the words themselves. Using these features is what sets ALP apart from systems like the RA approach. Specifically, three features are used to instantiate the templates: • • • POS tags on both sides: We assign POS tags using the MXPOST tagger (Ratnaparkhi, 1996) for English and Chinese, and Connexor for Spanish. Dependency relations: ALP utilizes dependencies for a better generalization—if a dependency parser is available in either language. In our experiments, we used a dependency parser only in English (a version of the Collins parser (Collins, 1997) that has been adapted for building dependencies) but not in the other language. A set of closed-class words: We use 16 different classes, 9 of which are different semantic verb classes while the other 7 are function words, prepositions, and complementizers.5 If both POS tags and dependency relations are available, they can be used together to instantiate the templates. That is, a word can be instantiated in a TBL template with: (1) a POS tag (e.g., Noun, Adj); (2) a relation (e.g., Subj, Obj); (3) a parameter class (e.g., Change of State); or (4) different subsets of (1)–(3). We also employ a"
H05-1024,P02-1033,0,0.0509733,"Missing"
H05-1024,dorr-etal-2002-duster,1,0.855239,"(e.g., Noun, Adj); (2) a relation (e.g., Subj, Obj); (3) a parameter class (e.g., Change of State); or (4) different subsets of (1)–(3). We also employ a more generalized form of instantiation, where words in the templates may match the keyword anything. 4.4 Best Rule Selection The rules are selected using two different metrics: The accuracy of the rule or the overall impact of the application of the rule on the entire data. Two different mechanisms may be used for selecting the best rule after generating all possible instantiations of templates: 5 These are based on the parameter classes of (Dorr et al., 2002). 189 1. 2. 5 Rule Accuracy: The goal is to minimize the errors introduced by the application of a transformation rule. To measure accuracy of a rule r, we use good(r)−2×bad(r), where good(r) is the number of alignment links that are corrected by the rule, and bad(r) is the number of incorrect alignment links produced. Overall impact on the training data: The accuracy mechanism (above) is useful for biasing the system toward higher precision. However, if the overall system is evaluated using a metric other than precision (e.g., recall), the accuracy mechanism may not guarantee that the best ru"
H05-1024,N03-1017,0,0.0337481,"Missing"
H05-1024,P05-1057,0,0.108584,"y, 2000; Koehn et al., 2003)—henceforth referred to as “RA.” Several researchers have proposed algorithms for improving word alignment systems by injecting additional knowledge or combining different alignment models. These approaches include an enhanced HMM alignment model that uses part-ofspeech tags (Toutanova et al., 2002), a log-linear combination of IBM translation models and HMM models (Och and Ney, 2003), techniques that rely on dependency relations (Cherry and Lin, 2003), and a log-linear combination of IBM Model 3 alignment probabilities, POS tags, and bilingual dictionary coverage (Liu et al., 2005). A common theme for these methods is the use of additional features for enriching the alignment process. These methods perform better than the IBM models and their variants but still tend to make similar errors because of the bias in their alignment modeling. We adopt an approach that post-processes a given alignment using linguistically-oriented rules. The idea is similar to that of Ayan et al. (2004), where manually-crafted rules are used to correct alignment links related to language divergences. Our approach differs, however, in that the rules are extracted automatically—not manually—by e"
H05-1024,J00-2004,0,0.0628999,"Missing"
H05-1024,P00-1056,0,0.364521,"ults on English-Spanish and EnglishChinese. We show that ALP yields a significant reductions in alignment error rate over that of the best performing alignment system. 2 One of the major problems with the IBM models (Brown et al., 1993) and the HMM models (Vogel et al., 1996) is that they are restricted to the alignment of each source-language word to at most one targetlanguage word. The standard method to overcome this problem to use the model in both directions (interchanging the source and target languages) and applying heuristic-based combination techniques to produce a refined alignment (Och and Ney, 2000; Koehn et al., 2003)—henceforth referred to as “RA.” Several researchers have proposed algorithms for improving word alignment systems by injecting additional knowledge or combining different alignment models. These approaches include an enhanced HMM alignment model that uses part-ofspeech tags (Toutanova et al., 2002), a log-linear combination of IBM translation models and HMM models (Och and Ney, 2003), techniques that rely on dependency relations (Cherry and Lin, 2003), and a log-linear combination of IBM Model 3 alignment probabilities, POS tags, and bilingual dictionary coverage (Liu et"
H05-1024,J03-1002,0,0.0131704,"to overcome this problem to use the model in both directions (interchanging the source and target languages) and applying heuristic-based combination techniques to produce a refined alignment (Och and Ney, 2000; Koehn et al., 2003)—henceforth referred to as “RA.” Several researchers have proposed algorithms for improving word alignment systems by injecting additional knowledge or combining different alignment models. These approaches include an enhanced HMM alignment model that uses part-ofspeech tags (Toutanova et al., 2002), a log-linear combination of IBM translation models and HMM models (Och and Ney, 2003), techniques that rely on dependency relations (Cherry and Lin, 2003), and a log-linear combination of IBM Model 3 alignment probabilities, POS tags, and bilingual dictionary coverage (Liu et al., 2005). A common theme for these methods is the use of additional features for enriching the alignment process. These methods perform better than the IBM models and their variants but still tend to make similar errors because of the bias in their alignment modeling. We adopt an approach that post-processes a given alignment using linguistically-oriented rules. The idea is similar to that of Ayan et al"
H05-1024,W02-1012,0,0.0364367,"gnment of each source-language word to at most one targetlanguage word. The standard method to overcome this problem to use the model in both directions (interchanging the source and target languages) and applying heuristic-based combination techniques to produce a refined alignment (Och and Ney, 2000; Koehn et al., 2003)—henceforth referred to as “RA.” Several researchers have proposed algorithms for improving word alignment systems by injecting additional knowledge or combining different alignment models. These approaches include an enhanced HMM alignment model that uses part-ofspeech tags (Toutanova et al., 2002), a log-linear combination of IBM translation models and HMM models (Och and Ney, 2003), techniques that rely on dependency relations (Cherry and Lin, 2003), and a log-linear combination of IBM Model 3 alignment probabilities, POS tags, and bilingual dictionary coverage (Liu et al., 2005). A common theme for these methods is the use of additional features for enriching the alignment process. These methods perform better than the IBM models and their variants but still tend to make similar errors because of the bias in their alignment modeling. We adopt an approach that post-processes a given a"
H05-1024,C96-2141,0,0.546546,"Missing"
H05-1024,H01-1035,0,0.127958,"Missing"
H05-1024,W96-0213,0,\N,Missing
H05-1098,P05-1033,1,0.428241,", and Analysis David Chiang, Adam Lopez, Nitin Madnani, Christof Monz, Philip Resnik, Michael Subotin Institute for Advanced Computer Studies (UMIACS) University of Maryland, College Park, MD 20742, USA {dchiang,alopez,nmadnani,christof,resnik,msubotin}@umiacs.umd.edu Abstract Hierarchical organization is a well known property of language, and yet the notion of hierarchical structure has been largely absent from the best performing machine translation systems in recent community-wide evaluations. In this paper, we discuss a new hierarchical phrase-based statistical machine translation system (Chiang, 2005), presenting recent extensions to the original proposal, new evaluation results in a community-wide evaluation, and a novel technique for fine-grained comparative analysis of MT systems. 1 Introduction Hierarchical organization is a well known property of language, and yet the notion of hierarchical structure has, for the last several years, been absent from the best performing machine translation systems in community-wide evaluations. Statistical phrase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of non"
H05-1098,W02-1039,0,0.35507,"evaluations. Statistical phrase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of nonoverlapping subsequences, or “phrases”, f¯1 · · · f¯J , and the process of translation involves selecting target phrases e¯i corresponding to the f¯j and modifying their sequential order. The need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as a unit, is widely recognized—the role of syntactic units is well attested in recent systematic studies of translation (Fox, 2002; Hwa et al., 2002; Koehn and Knight, 2003), and their absence in phrase-based models is quite evident when looking at MT system output. Nonetheless, attempts to incorporate richer linguistic features have generally met with little success (Och et al., 2004a). Chiang (2005) introduces Hiero, a hierarchical phrase-based model for statistical machine translation. Hiero extends the standard, non-hierarchical notion of “phrases” to include nonterminal symbols, which permits it to capture both word-level and phrase-level reorderings within the same framework. The model has the formal structure of a"
H05-1098,P02-1050,1,0.909542,"s. Statistical phrase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of nonoverlapping subsequences, or “phrases”, f¯1 · · · f¯J , and the process of translation involves selecting target phrases e¯i corresponding to the f¯j and modifying their sequential order. The need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as a unit, is widely recognized—the role of syntactic units is well attested in recent systematic studies of translation (Fox, 2002; Hwa et al., 2002; Koehn and Knight, 2003), and their absence in phrase-based models is quite evident when looking at MT system output. Nonetheless, attempts to incorporate richer linguistic features have generally met with little success (Och et al., 2004a). Chiang (2005) introduces Hiero, a hierarchical phrase-based model for statistical machine translation. Hiero extends the standard, non-hierarchical notion of “phrases” to include nonterminal symbols, which permits it to capture both word-level and phrase-level reorderings within the same framework. The model has the formal structure of a synchronous CFG,"
H05-1098,P03-1040,0,0.00569351,"ase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of nonoverlapping subsequences, or “phrases”, f¯1 · · · f¯J , and the process of translation involves selecting target phrases e¯i corresponding to the f¯j and modifying their sequential order. The need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as a unit, is widely recognized—the role of syntactic units is well attested in recent systematic studies of translation (Fox, 2002; Hwa et al., 2002; Koehn and Knight, 2003), and their absence in phrase-based models is quite evident when looking at MT system output. Nonetheless, attempts to incorporate richer linguistic features have generally met with little success (Och et al., 2004a). Chiang (2005) introduces Hiero, a hierarchical phrase-based model for statistical machine translation. Hiero extends the standard, non-hierarchical notion of “phrases” to include nonterminal symbols, which permits it to capture both word-level and phrase-level reorderings within the same framework. The model has the formal structure of a synchronous CFG, but it does not make any"
H05-1098,N03-1017,0,0.134136,"aper, we discuss a new hierarchical phrase-based statistical machine translation system (Chiang, 2005), presenting recent extensions to the original proposal, new evaluation results in a community-wide evaluation, and a novel technique for fine-grained comparative analysis of MT systems. 1 Introduction Hierarchical organization is a well known property of language, and yet the notion of hierarchical structure has, for the last several years, been absent from the best performing machine translation systems in community-wide evaluations. Statistical phrase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of nonoverlapping subsequences, or “phrases”, f¯1 · · · f¯J , and the process of translation involves selecting target phrases e¯i corresponding to the f¯j and modifying their sequential order. The need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as a unit, is widely recognized—the role of syntactic units is well attested in recent systematic studies of translation (Fox, 2002; Hwa et al., 2002; Koehn and Knight, 2003), and their absence in phrase-based mo"
H05-1098,koen-2004-pharaoh,0,0.0221738,"e-based model for statistical machine translation. Hiero extends the standard, non-hierarchical notion of “phrases” to include nonterminal symbols, which permits it to capture both word-level and phrase-level reorderings within the same framework. The model has the formal structure of a synchronous CFG, but it does not make any commitment to a linguistically relevant analysis, and it does not require syntactically annotated training data. Chiang (2005) reported significant performance improvements in Chinese-English translation as compared with Pharaoh, a state-of-the-art phrase-based system (Koehn, 2004). In Section 2, we review the essential elements of Hiero. In Section 3 we describe extensions to this system, including new features involving named entities and numbers and support for a fourfold scale-up in training set size. Section 4 presents new evaluation results for Chinese-English as well as Arabic-English translation, obtained in the context of the 2005 NIST MT Eval exercise. In Section 5, we introduce a novel technique for fine-grained comparative analysis of MT systems, which we employ in analyzing differences between Hiero’s and Pharaoh’s translations. 2 Hiero Hiero is a stochasti"
H05-1098,P04-1077,0,0.0151475,"on the FBIS data; Hiero, with various combinations of the new features and the larger training data.4 This table also shows Hiero’s performance on the NIST 2005 MT evaluation task.5 The metric here is case-sensitive BLEU.6 Figure 2 shows the performance of two systems on Arabic in the NIST 2005 MT Evaluation task: DC, a phrase-based decoder for a model trained by Pharaoh, and Hiero. 5 Analysis Over the last few years, several automatic metrics for machine translation evaluation have been introduced, largely to reduce the human cost of iterative system evaluation during the development cycle (Lin and Och, 2004; Melamed et al., 2003; Papineni et al., 2002). All are predicated on the concept 4 The third line, corresponding to the model without new features trained on the larger data, may be slightly depressed because the feature weights from the fourth line were used instead of doing minimum-error-rate training specially for this model. 5 Full results are available at http://www.nist.gov/ speech/tests/summaries/2005/mt05.htm. For this test, a phrase length limit of 15 was used during decoding. 6 For this task, the translation output was uppercased using the SRI-LM toolkit: essentially, it was decoded"
H05-1098,H05-2007,1,0.821428,"Missing"
H05-1098,W02-1018,0,0.0259841,"ew hierarchical phrase-based statistical machine translation system (Chiang, 2005), presenting recent extensions to the original proposal, new evaluation results in a community-wide evaluation, and a novel technique for fine-grained comparative analysis of MT systems. 1 Introduction Hierarchical organization is a well known property of language, and yet the notion of hierarchical structure has, for the last several years, been absent from the best performing machine translation systems in community-wide evaluations. Statistical phrase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of nonoverlapping subsequences, or “phrases”, f¯1 · · · f¯J , and the process of translation involves selecting target phrases e¯i corresponding to the f¯j and modifying their sequential order. The need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as a unit, is widely recognized—the role of syntactic units is well attested in recent systematic studies of translation (Fox, 2002; Hwa et al., 2002; Koehn and Knight, 2003), and their absence in phrase-based models is quite evident w"
H05-1098,N03-2021,0,0.177666,"Hiero, with various combinations of the new features and the larger training data.4 This table also shows Hiero’s performance on the NIST 2005 MT evaluation task.5 The metric here is case-sensitive BLEU.6 Figure 2 shows the performance of two systems on Arabic in the NIST 2005 MT Evaluation task: DC, a phrase-based decoder for a model trained by Pharaoh, and Hiero. 5 Analysis Over the last few years, several automatic metrics for machine translation evaluation have been introduced, largely to reduce the human cost of iterative system evaluation during the development cycle (Lin and Och, 2004; Melamed et al., 2003; Papineni et al., 2002). All are predicated on the concept 4 The third line, corresponding to the model without new features trained on the larger data, may be slightly depressed because the feature weights from the fourth line were used instead of doing minimum-error-rate training specially for this model. 5 Full results are available at http://www.nist.gov/ speech/tests/summaries/2005/mt05.htm. For this test, a phrase length limit of 15 was used during decoding. 6 For this task, the translation output was uppercased using the SRI-LM toolkit: essentially, it was decoded again using an HMM wh"
H05-1098,P00-1056,0,0.0589312,"igure 1: Example synchronous CFG • the lexical weights Pw (γ |α) and Pw (α |γ) (Koehn et al., 2003);1 2.1 Grammar • a phrase penalty exp(1); A synchronous CFG or syntax-directed transduction grammar (Lewis and Stearns, 1968) consists of pairs of CFG rules with aligned nonterminal symbols. We denote this alignment by coindexation with boxed numbers (Figure 1). A derivation starts with a pair of aligned start symbols, and proceeds by rewriting pairs of aligned nonterminal symbols using the paired rules (Figure 2). Training begins with phrase pairs, obtained as by Och, Koehn, and others: GIZA++ (Och and Ney, 2000) is used to obtain one-to-many word alignments in both directions, which are combined into a single set of refined alignments using the “final-and” method of Koehn et al. (2003); then those pairs of substrings that are exclusively aligned to each other are extracted as phrase pairs. Then, synchronous CFG rules are constructed out of the initial phrase pairs by subtraction: every phrase pair h f¯, e¯ i becomes a rule X → h f¯, e¯ i, and a phrase pair h f¯, e¯ i can be subtracted from a rule X → hγ1 f¯γ2 , α1 e¯ α2 i to form a new rule X → hγ1 X i γ2 , α1 X i α2 i, where i is an index not alread"
H05-1098,P02-1038,0,0.00566047,"onous CFG, whose productions are extracted automatically from unannotated parallel texts, and whose rule probabilities form a log-linear model learned by minimum-errorrate training; together with a modified CKY beamsearch decoder (similar to that of Wu (1996)). We describe these components in brief below. 779 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 779–786, Vancouver, October 2005. 2005 Association for Computational Linguistics 2.2 S → hS 1 X 2 , S 1 X 2 i Model The model is a log-linear model (Och and Ney, 2002) over synchronous CFG derivations. The weight of a derivation is PLM (e)λLM , the weighted language model probability, multiplied by the product of the weights of the rules used in the derivation. The weight of each rule is, in turn: Y (1) w(X → hγ, αi) = φi (X → hγ, αi)λi S → hX 1 , X 1 i X → hyu X 1 you X 2 , have X 2 with X 1 i X → hX 1 de X 2 , the X 2 that X 1 i X → hX 1 zhiyi, one of X 1 i X → hAozhou, Australiai X → hshi, isi i X → hshaoshu guojia, few countriesi where the φi are features defined on rules. The basic model uses the following features, analogous to Pharaoh’s default featu"
H05-1098,J04-4002,0,0.0105751,"luations. In this paper, we discuss a new hierarchical phrase-based statistical machine translation system (Chiang, 2005), presenting recent extensions to the original proposal, new evaluation results in a community-wide evaluation, and a novel technique for fine-grained comparative analysis of MT systems. 1 Introduction Hierarchical organization is a well known property of language, and yet the notion of hierarchical structure has, for the last several years, been absent from the best performing machine translation systems in community-wide evaluations. Statistical phrase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of nonoverlapping subsequences, or “phrases”, f¯1 · · · f¯J , and the process of translation involves selecting target phrases e¯i corresponding to the f¯j and modifying their sequential order. The need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as a unit, is widely recognized—the role of syntactic units is well attested in recent systematic studies of translation (Fox, 2002; Hwa et al., 2002; Koehn and Knight, 2003), and their absenc"
H05-1098,P03-1021,0,0.00789852,"verage. hS 1 , S 1 i ⇒ hS 2 X 3 , S 2 X 3 i ⇒ hS 4 X 5 X 3 , S 4 X 5 X 3 i ⇒ hX 6 X 5 X 3 , X 6 X 5 X 3 i ⇒ hAozhou X 5 X 3 , Australia X 5 X 3 i ⇒ hAozhou shi X 3 , Australia is X 3 i ⇒ hAozhou shi X 7 zhiyi, Australia is one of X 7 i ⇒ hAozhou shi X 8 de X 9 zhiyi, Australia is one of the X 9 that X 8 i ⇒ hAozhou shi yu X 1 you X 2 de X 9 zhiyi, Australia is one of the X 9 that have X 2 with X 1 i Figure 2: Example partial derivation of a synchronous CFG. The feature weights are learned by maximizing the BLEU score (Papineni et al., 2002) on held-out data, using minimum-error-rate training (Och, 2003) as implemented by Koehn. The implementation was slightly modified to ensure that the BLEU scoring matches NIST’s definition and that hypotheses in the n-best lists are merged when they have the same translation and the same feature vector. 3 Extensions In this section we describe our extensions to the base Hiero system that improve its performance significantly. First, we describe the addition of two new features to the Chinese model, in a manner similar to that of Och et al. (2004b); then we describe how we scaled the system up to a much larger training set. 3.1 New features supplementary ph"
H05-1098,P02-1040,0,0.109612,"nment, Koehn et al. take the maximum lexical weight; Hiero uses a weighted average. hS 1 , S 1 i ⇒ hS 2 X 3 , S 2 X 3 i ⇒ hS 4 X 5 X 3 , S 4 X 5 X 3 i ⇒ hX 6 X 5 X 3 , X 6 X 5 X 3 i ⇒ hAozhou X 5 X 3 , Australia X 5 X 3 i ⇒ hAozhou shi X 3 , Australia is X 3 i ⇒ hAozhou shi X 7 zhiyi, Australia is one of X 7 i ⇒ hAozhou shi X 8 de X 9 zhiyi, Australia is one of the X 9 that X 8 i ⇒ hAozhou shi yu X 1 you X 2 de X 9 zhiyi, Australia is one of the X 9 that have X 2 with X 1 i Figure 2: Example partial derivation of a synchronous CFG. The feature weights are learned by maximizing the BLEU score (Papineni et al., 2002) on held-out data, using minimum-error-rate training (Och, 2003) as implemented by Koehn. The implementation was slightly modified to ensure that the BLEU scoring matches NIST’s definition and that hypotheses in the n-best lists are merged when they have the same translation and the same feature vector. 3 Extensions In this section we describe our extensions to the base Hiero system that improve its performance significantly. First, we describe the addition of two new features to the Chinese model, in a manner similar to that of Och et al. (2004b); then we describe how we scaled the system up"
H05-1098,W96-0213,0,0.00943678,"t work has shown that applying statistical parsers to ungrammatical MT output is unreliable at best, with the parser often assigning unreasonable probabilities and incongruent structure (Yamada and Knight, 2002; Och et al., 2004a). Anticipating that this would be equally problematic for part-of-speech tagging, we make the conservative choice to apply annotation only to the reference corpus. Word ngram correspondences with a reference translation are used to infer the part-of-speech tags for words in the system output. First, we tagged the reference corpus with parts of speech. We used MXPOST (Ratnaparkhi, 1996), and in order to discover more general patterns, we map the tag set down after tagging, e.g. NN, NNP, NNPS and NNS all map to NN. Second, we computed the frequency freq(ti . . . t j ) of every possible tag sequence ti . . . t j in the reference corpus. Third, we computed the correspondence between each hypothesis sentence and each of its corresponding reference sentences using an approximation to maximum matching (Melamed et al., 2003). This algorithm provides a list of runs or contiguous sequences of words ei . . . e j in the reference that are also present in the hypothesis. (Note that runs"
H05-1098,2003.mtsummit-papers.53,0,0.0144904,"ed Chinese number/date translator, and created a new model feature for it. Again, the weight given to this module was optimized during minimum-error-rate training. In some cases we wrote the rules to provide multiple uniformlyweighted English translations for a Chinese phrase (for example, kå (bari) could become “the 8th” or “on the 8th”), allowing the language model to decide between the options. 3.2 The LDC Chinese-English named entity lists (900k entries) are a potentially valuable resource, but previous experiments have suggested that simply adding them to the training data does not help (Vogel et al., 2003). Instead, we placed them in a supplementary phrase-translation table, giving greater weight to phrases that occurred less frequently in the primary training data. For each entry h f, {e1 , . . . , en }i, we counted the number of times c( f ) that f appeared in the primary training data, and assigned the entry the weight c( f1)+1 , which was then distributed evenly among the supplementary phrase pairs {h f, ei i}. We then created a new model feature for named entities. When one of these 781 Scaling up training Chiang (2005) reports on experiments in ChineseEnglish translation using a model tra"
H05-1098,P96-1021,0,0.157884,"tion results for Chinese-English as well as Arabic-English translation, obtained in the context of the 2005 NIST MT Eval exercise. In Section 5, we introduce a novel technique for fine-grained comparative analysis of MT systems, which we employ in analyzing differences between Hiero’s and Pharaoh’s translations. 2 Hiero Hiero is a stochastic synchronous CFG, whose productions are extracted automatically from unannotated parallel texts, and whose rule probabilities form a log-linear model learned by minimum-errorrate training; together with a modified CKY beamsearch decoder (similar to that of Wu (1996)). We describe these components in brief below. 779 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 779–786, Vancouver, October 2005. 2005 Association for Computational Linguistics 2.2 S → hS 1 X 2 , S 1 X 2 i Model The model is a log-linear model (Och and Ney, 2002) over synchronous CFG derivations. The weight of a derivation is PLM (e)λLM , the weighted language model probability, multiplied by the product of the weights of the rules used in the derivation. The weight of each rule is, in turn: Y (1) w"
H05-1098,J97-3002,0,0.094693,"ned to each other are extracted as phrase pairs. Then, synchronous CFG rules are constructed out of the initial phrase pairs by subtraction: every phrase pair h f¯, e¯ i becomes a rule X → h f¯, e¯ i, and a phrase pair h f¯, e¯ i can be subtracted from a rule X → hγ1 f¯γ2 , α1 e¯ α2 i to form a new rule X → hγ1 X i γ2 , α1 X i α2 i, where i is an index not already used. Various filters are also applied to reduce the number of extracted rules. Since one of these filters restricts the number of nonterminal symbols to two, our extracted grammar is equivalent to an inversion transduction grammar (Wu, 1997). 780 • a word penalty exp(l), where l is the number of terminals in α. The exceptions to the above are the two “glue” rules, which are the rules with left-hand side S in Figure 1. The second has weight one, and the first has weight w(S → hS 1 X 2 , S 1 X 2 i) = exp(−λg ), the idea being that parameter λg controls the model’s preference for hierarchical phrases over serial combination of phrases. Phrase translation probabilities are estimated by relative-frequency estimation. Since the extraction process does not generate a unique derivation for each training sentence pair, a distribution over"
H05-1098,P02-1039,0,0.0166923,"pect to syntax, although there remains a high degree of flexibility (Fox, 2002; Hwa et al., 2002). This suggests that in a comparative analysis of two MT systems, it may be useful to look for syntactic patterns that one system captures well in the target language and the other does not, using a syntax based metric. We propose to summarize reordering patterns using part-of-speech sequences. Unfortunately, recent work has shown that applying statistical parsers to ungrammatical MT output is unreliable at best, with the parser often assigning unreasonable probabilities and incongruent structure (Yamada and Knight, 2002; Och et al., 2004a). Anticipating that this would be equally problematic for part-of-speech tagging, we make the conservative choice to apply annotation only to the reference corpus. Word ngram correspondences with a reference translation are used to infer the part-of-speech tags for words in the system output. First, we tagged the reference corpus with parts of speech. We used MXPOST (Ratnaparkhi, 1996), and in order to discover more general patterns, we map the tag set down after tagging, e.g. NN, NNP, NNPS and NNS all map to NN. Second, we computed the frequency freq(ti . . . t j ) of ever"
H05-1098,N04-1021,0,\N,Missing
I17-1004,W17-3204,0,0.0735784,"or their experimental analysis is not an attentional model and they argue that attention does not have any impact for learning syntactic information. However, performing the same analysis for morphological information, Belinkov et al. (2017) show that attention has also some effect on the information that the encoder of neural machine translation system encodes in its output hidden states. As part of their analysis they show that a neural machine translation system that has an attention model can learn the POS tags of the source side more efficiently than a system without attention. Recently, Koehn and Knowles (2017) carried out a brief analysis of how much attention and alignment match in different languages by measuring the probability mass that attention gives to alignments obtained from an automatic alignment tool. They also report differences based on the most attended words. Related Work Liu et al. (2016) investigate how training the attention model in a supervised manner can benefit machine translation quality. To this end they use traditional alignments obtained by running automatic alignment tools (GIZA++ (Och and Ney, 2003) and fast align (Dyer et al., 2013)) on the training data and feed it as"
I17-1004,C16-1291,0,0.398371,"urce words to generate a target word at each step of the translation. In this paper we focus on studying the relevance of the attended parts, especially cases where attention is ‘smeared out’ over multiple source words where their relevance is not entirely obvious, see, e.g., “would” and “like” in Figure 1. Here, we ask whether these are due to errors of the attention mechanism or are a desired behavior of the model. Since the introduction of attention models in neural machine translation (Bahdanau et al., 2015) various modifications have been proposed (Luong et al., 2015a; Cohn et al., 2016; Liu et al., 2016). However, to the best of our knowledge 30 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 30–39, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP there is no study that provides an analysis of what kind of phenomena is being captured by attention. There are some works that have looked to attention as being similar to traditional word alignment (Alkhouli et al., 2016; Cohn et al., 2016; Liu et al., 2016; Chen et al., 2016). Some of these approaches also experimented with training the attention model using traditional alignments (Alkhou"
I17-1004,D15-1166,0,0.697004,"more information. We show that attention is different from alignment in some cases and is capturing useful information other than alignments. 1 Figure 1: Visualization of the attention paid to the relevant parts of the source sentence for each generated word of a translation example. See how the attention is ‘smeared out’ over multiple source words in the case of “would” and “like”. Introduction Neural machine translation (NMT) has gained a lot of attention recently due to its substantial improvements in machine translation quality achieving state-of-the-art performance for several languages (Luong et al., 2015b; Jean et al., 2015; Wu et al., 2016). The core architecture of neural machine translation models is based on the general encoder-decoder approach (Sutskever et al., 2014). Neural machine translation is an end-toend approach that learns to encode source sentences into distributed representations and decode these representations into sentences in the target language. Among the different neural MT models, attentional NMT (Bahdanau et al., 2015; Luong et al., 2015a) has become popular due to its capability to use the most relevant parts of the source sentence at each translation step. This capab"
I17-1004,W16-2206,0,0.451431,"ce the introduction of attention models in neural machine translation (Bahdanau et al., 2015) various modifications have been proposed (Luong et al., 2015a; Cohn et al., 2016; Liu et al., 2016). However, to the best of our knowledge 30 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 30–39, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP there is no study that provides an analysis of what kind of phenomena is being captured by attention. There are some works that have looked to attention as being similar to traditional word alignment (Alkhouli et al., 2016; Cohn et al., 2016; Liu et al., 2016; Chen et al., 2016). Some of these approaches also experimented with training the attention model using traditional alignments (Alkhouli et al., 2016; Liu et al., 2016; Chen et al., 2016). Liu et al. (2016) have shown that attention could be seen as a reordering model as well as an alignment model. In this paper, we focus on investigating the differences between attention and alignment and what is being captured by the attention mechanism in general. The questions that we are aiming to answer include: Is the attention model only capable of modelling alignm"
I17-1004,P15-1002,0,0.416775,"more information. We show that attention is different from alignment in some cases and is capturing useful information other than alignments. 1 Figure 1: Visualization of the attention paid to the relevant parts of the source sentence for each generated word of a translation example. See how the attention is ‘smeared out’ over multiple source words in the case of “would” and “like”. Introduction Neural machine translation (NMT) has gained a lot of attention recently due to its substantial improvements in machine translation quality achieving state-of-the-art performance for several languages (Luong et al., 2015b; Jean et al., 2015; Wu et al., 2016). The core architecture of neural machine translation models is based on the general encoder-decoder approach (Sutskever et al., 2014). Neural machine translation is an end-toend approach that learns to encode source sentences into distributed representations and decode these representations into sentences in the target language. Among the different neural MT models, attentional NMT (Bahdanau et al., 2015; Luong et al., 2015a) has become popular due to its capability to use the most relevant parts of the source sentence at each translation step. This capab"
I17-1004,P00-1056,0,0.456541,"ct on Translation Based on the results in Section 5.1, one might be inclined to conclude that the closer the attention is to the word alignments the better the translation. However, Chen et al. (2016); Liu et al. (2016); Alkhouli et al. (2016) report mixed results by optimizing their NMT system with respect to word prediction and alignment quality. These findings warrant a more fine-grained analysis of attention. To this end, we include POS tags in our analysis and study the patterns of attention based on POS tags of the target words. We choose POS tags beWe report alignment error rate (AER) (Och and Ney, 2000), which is commonly used to measure alignment quality, in Table 4 to show the difference between attentions and human alignments provided by RWTH German-English dataset. To compute AER over attentions, we follow Luong 34 (a) Average attention loss based on the POS tags of the target (b) Average word prediction loss based on the POS tags of the side. target side. Figure 3: Average attention losses and word prediction losses from the input-feeding system. Tag ADJ ADP ADV CONJ DET NOUN NUM PRT PRON PUNC VERB Meaning Adjective Adposition Adverb Conjunction Determiner Noun Numeral Particle Pronoun"
I17-1004,P17-1080,0,0.0606315,"flexibility in the model types and training of the models. They use a feed-forward neural network as their alignment model that learns to model jumps in the source side using HMM/IBM alignments obtained by using GIZA++. Shi et al. (2016) show that various kinds of syntactic information are being learned and encoded in the output hidden states of the encoder. The neural system for their experimental analysis is not an attentional model and they argue that attention does not have any impact for learning syntactic information. However, performing the same analysis for morphological information, Belinkov et al. (2017) show that attention has also some effect on the information that the encoder of neural machine translation system encodes in its output hidden states. As part of their analysis they show that a neural machine translation system that has an attention model can learn the POS tags of the source side more efficiently than a system without attention. Recently, Koehn and Knowles (2017) carried out a brief analysis of how much attention and alignment match in different languages by measuring the probability mass that attention gives to alignments obtained from an automatic alignment tool. They also"
I17-1004,J03-1002,0,0.0571823,"coder system as described in (Luong et al., 2015a), using 4 recurrent layers. We trained the systems with dimension size of 1,000 and batch size of 80 for 20 epochs. The vocabulary for both source and target side is set to be the 30K most common words. The learning rate is set to be 1 and a maximum gradient norm of 5 has been used. We also use a dropout rate of 0.3 to avoid overfitting. Data WMT15 # of Sent 4,240,727 Min Len 1 Max Len 100 Attention loss et al. (2015a) to produce hard alignments from attentions by choosing the most attended source word for each target word. We also use GIZA++ (Och and Ney, 2003) to produce automatic alignments over the data set to allow for a comparison between automatically generated alignments and the attentions generated by our systems. GIZA++ is run in both directions and alignments are symmetrized using the grow-diag-final-and refined alignment heuristic. As shown in Table 4, the input-feeding system not only achieves a higher BLEU score, but also uses attentions that are closer to the human alignments. Table 5 compares input-feeding and nonrecurrent attention in terms of attention loss computed using Equation 8. Here the losses between the attention produced by"
I17-1004,2016.amta-researchers.10,0,0.403298,"anslation (Bahdanau et al., 2015) various modifications have been proposed (Luong et al., 2015a; Cohn et al., 2016; Liu et al., 2016). However, to the best of our knowledge 30 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 30–39, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP there is no study that provides an analysis of what kind of phenomena is being captured by attention. There are some works that have looked to attention as being similar to traditional word alignment (Alkhouli et al., 2016; Cohn et al., 2016; Liu et al., 2016; Chen et al., 2016). Some of these approaches also experimented with training the attention model using traditional alignments (Alkhouli et al., 2016; Liu et al., 2016; Chen et al., 2016). Liu et al. (2016) have shown that attention could be seen as a reordering model as well as an alignment model. In this paper, we focus on investigating the differences between attention and alignment and what is being captured by the attention mechanism in general. The questions that we are aiming to answer include: Is the attention model only capable of modelling alignment? And how similar is attention to alignment in differe"
I17-1004,P02-1040,0,0.0979209,"nonrecurrent attention in terms of attention loss computed using Equation 8. Here the losses between the attention produced by each system and the human alignments is reported. As expected, the difference in attention losses are in line with AER. The difference between these comparisons is that AER only takes the most attended word into account while attention loss considers the entire attention distribution. Average Len 24.7 Impact of Attention Mechanism We train both of the systems on the WMT15 German-to-English training data, see Table 3 for some statistics. Table 2 shows the BLEU scores (Papineni et al., 2002) for both systems on different test sets. Since we use POS tags and dependency roles in our analysis, both of which are based on words, we chose not to use BPE (Sennrich et al., 2016) which operates at the sub-word level. AER non-recurrent 0.60 input-feeding 0.37 input-feeding 0.25 Table 5: Average loss between attention generated by input-feeding and non-recurrent systems and the manual alignment over RWTH GermanEnglish data. Table 3: Statistics for the parallel corpus used to train our models. The length statistics are based on the source side. 5.1 non-recurrent 0.46 GIZA++ 0.31 Table 4: Ali"
I17-1004,petrov-etal-2012-universal,0,0.014395,"of only, whenever and, or the, a market, system 2, two ’s, off, up she, they ;, . come, including to alignments. However, Figure 3b shows that the average word prediction loss for verbs is actually smaller compared to the loss for nouns. In other words, although the attention for verbs is substantially more inconsistent with the word alignments than for nouns, the NMT system translates verbs more accurately than nouns on average. Table 6: List of the universal POS tags used in our analysis. cause they exhibit some simple syntactic characteristics. We use the coarse grained universal POS tags (Petrov et al., 2012) given in Table 6. To better understand how attention accuracy affects translation quality, we analyse the relationship between attention loss and word prediction loss for individual part-of-speech classes. Figure 3a shows how attention loss differs when generating different POS tags. One can see that attention loss varies substantially across different POS tags. In particular, we focus on the cases of NOUN and VERB which are the most frequent POS tags in the dataset. As shown, the attention of NOUN is the closest to alignments on average. But the average attention loss for VERB is almost two"
I17-1004,P16-1162,0,0.189627,"pected, the difference in attention losses are in line with AER. The difference between these comparisons is that AER only takes the most attended word into account while attention loss considers the entire attention distribution. Average Len 24.7 Impact of Attention Mechanism We train both of the systems on the WMT15 German-to-English training data, see Table 3 for some statistics. Table 2 shows the BLEU scores (Papineni et al., 2002) for both systems on different test sets. Since we use POS tags and dependency roles in our analysis, both of which are based on words, we chose not to use BPE (Sennrich et al., 2016) which operates at the sub-word level. AER non-recurrent 0.60 input-feeding 0.37 input-feeding 0.25 Table 5: Average loss between attention generated by input-feeding and non-recurrent systems and the manual alignment over RWTH GermanEnglish data. Table 3: Statistics for the parallel corpus used to train our models. The length statistics are based on the source side. 5.1 non-recurrent 0.46 GIZA++ 0.31 Table 4: Alignment error rate (AER) of the hard alignments produced from the output attentions of the systems with input-feeding and non-recurrent attention models. We use the most attended sourc"
I17-1004,N13-1073,0,0.198962,"Missing"
I17-1004,R13-1079,0,0.0132737,"Subjects Punctuations Adverbial functions including negation All members in a coordination3 Table 7: The most attended dependency roles with their received attention percentage from the attention probability mass paid to the words other than the alignment points. Here, we focus on the POS tags discussed earlier. 5.4 Attention Distribution the POS tags. To examine how the rest of attention in each case has been distributed over the source sentence we measure the attention distribution over dependency roles in the source side. We first parse the source side of RWTH data using the ParZu parser (Sennrich et al., 2013). Then we compute how the attention probability mass given to the words other than the alignment points, is distributed over dependency roles. Table 7 gives the most attended roles for each POS tag. Here, we focus on POS tags discussed earlier. One can see that the most attended roles when translating to nouns include adjectives and determiners and in the case of translating to verbs, it includes auxiliary verbs, adverbs (including negation), subjects, and objects. To further understand under which conditions attention is paid to words other than the aligned words, we study the distribution of"
I17-1004,P15-1001,0,0.0262276,"show that attention is different from alignment in some cases and is capturing useful information other than alignments. 1 Figure 1: Visualization of the attention paid to the relevant parts of the source sentence for each generated word of a translation example. See how the attention is ‘smeared out’ over multiple source words in the case of “would” and “like”. Introduction Neural machine translation (NMT) has gained a lot of attention recently due to its substantial improvements in machine translation quality achieving state-of-the-art performance for several languages (Luong et al., 2015b; Jean et al., 2015; Wu et al., 2016). The core architecture of neural machine translation models is based on the general encoder-decoder approach (Sutskever et al., 2014). Neural machine translation is an end-toend approach that learns to encode source sentences into distributed representations and decode these representations into sentences in the target language. Among the different neural MT models, attentional NMT (Bahdanau et al., 2015; Luong et al., 2015a) has become popular due to its capability to use the most relevant parts of the source sentence at each translation step. This capability Figure 1 shows"
I17-1004,D16-1159,0,0.0247529,"uided attention with traditional alignment helps in the domain of e-commerce data which includes lots of out of vocabulary (OOV) product names and placeholders, but not much in the other domains. Alkhouli et al. (2016) have separated the alignment model and translation model, reasoning that this avoids propagation of errors from one model to the other as well as providing more flexibility in the model types and training of the models. They use a feed-forward neural network as their alignment model that learns to model jumps in the source side using HMM/IBM alignments obtained by using GIZA++. Shi et al. (2016) show that various kinds of syntactic information are being learned and encoded in the output hidden states of the encoder. The neural system for their experimental analysis is not an attentional model and they argue that attention does not have any impact for learning syntactic information. However, performing the same analysis for morphological information, Belinkov et al. (2017) show that attention has also some effect on the information that the encoder of neural machine translation system encodes in its output hidden states. As part of their analysis they show that a neural machine transl"
I17-1004,1983.tc-1.13,0,0.609623,"Missing"
L18-1148,D16-1025,1,0.858108,"nary NMT experiments as the first step towards better idiom translation. Keywords: multiword expression, idioms, bilingual corpora, machine translation 1. Introduction Neural Machine Translation (NMT) (Bahdanau et al., 2015; Sutskever et al., 2014; Cho et al., 2014) has achieved substantial improvements in translation quality over traditional Rule-based and Phrase-based Translation (PBMT) in recent years. For instance, subject-verb agreement, doubleobject verbs, and overlapping subcategorization are various areas where NMT successfully overcomes the limitations of PBMT (Isabelle et al., 2017; Bentivogli et al., 2016). However, one of the remaining challenges of NMT is translating infrequent words and phrases (Koehn and Knowles, 2017; Fadaee et al., 2017) and idioms are a particular instance of this problem (Isabelle et al., 2017). Idioms are semantic lexical units whose meaning is often not simply a function of the meaning of its constituent parts (Nunberg et al., 1994; K¨ovecses and Szabo, 1996). The non-compositionality characteristic of idiom expressions exists in different degrees in a language (Nunberg et al., 1994). In English for example, for the idiom “spill the beans”, the word ‘spill’ symbolizes"
L18-1148,W14-4012,0,0.0258764,"Missing"
L18-1148,N13-1073,0,0.0112828,"e and by itself does not focus on the translation quality of the idiomatic expressions. Modified Unigram Precision To specifically concentrate on the quality of the translation of idiom expressions, we also look at the localized precision. In this approach we translate the idiomatic expression in the context of a sentence, and only evaluate the translation quality of the idiom phrase. To isolate the idiom translation in the sentence, we look at the word-level alignments between the idiom expression in the source sentence and the generated translation in the target sentence. We use fast-align (Dyer et al., 2013) to extract word alignments. Since idiomatic phrases and the respective translations are not contiguous in many cases we only compare the unigrams of the two phrases. Note that for this metric we have two references: The idiom translation as an independent expression, and the human generated idiom translation in the target sentence. Word-level Idiom Accuracy We also use another metric to evaluate the word-level translation accuracy of the idiom phrase. We use word alignments between source and target sentences to determine the number of correctly translated words. We use the following equation"
L18-1148,P17-2090,1,0.792484,"slation 1. Introduction Neural Machine Translation (NMT) (Bahdanau et al., 2015; Sutskever et al., 2014; Cho et al., 2014) has achieved substantial improvements in translation quality over traditional Rule-based and Phrase-based Translation (PBMT) in recent years. For instance, subject-verb agreement, doubleobject verbs, and overlapping subcategorization are various areas where NMT successfully overcomes the limitations of PBMT (Isabelle et al., 2017; Bentivogli et al., 2016). However, one of the remaining challenges of NMT is translating infrequent words and phrases (Koehn and Knowles, 2017; Fadaee et al., 2017) and idioms are a particular instance of this problem (Isabelle et al., 2017). Idioms are semantic lexical units whose meaning is often not simply a function of the meaning of its constituent parts (Nunberg et al., 1994; K¨ovecses and Szabo, 1996). The non-compositionality characteristic of idiom expressions exists in different degrees in a language (Nunberg et al., 1994). In English for example, for the idiom “spill the beans”, the word ‘spill’ symbolizes ‘reveal’ and ‘beans’ symbolizes the ‘secrets’. With the idiomatic expression “kick the bucket”, on the other hand, no such analysis is poss"
L18-1148,D17-1263,0,0.0814123,"e it to perform preliminary NMT experiments as the first step towards better idiom translation. Keywords: multiword expression, idioms, bilingual corpora, machine translation 1. Introduction Neural Machine Translation (NMT) (Bahdanau et al., 2015; Sutskever et al., 2014; Cho et al., 2014) has achieved substantial improvements in translation quality over traditional Rule-based and Phrase-based Translation (PBMT) in recent years. For instance, subject-verb agreement, doubleobject verbs, and overlapping subcategorization are various areas where NMT successfully overcomes the limitations of PBMT (Isabelle et al., 2017; Bentivogli et al., 2016). However, one of the remaining challenges of NMT is translating infrequent words and phrases (Koehn and Knowles, 2017; Fadaee et al., 2017) and idioms are a particular instance of this problem (Isabelle et al., 2017). Idioms are semantic lexical units whose meaning is often not simply a function of the meaning of its constituent parts (Nunberg et al., 1994; K¨ovecses and Szabo, 1996). The non-compositionality characteristic of idiom expressions exists in different degrees in a language (Nunberg et al., 1994). In English for example, for the idiom “spill the beans”, t"
L18-1148,P17-4012,0,0.0214191,"o not have just a white vest. Coca-Cola and Nestl´e are among the signatories. Both don’t have a white essence. Table 1: Example of an idiom phrase in German and its translation. We compare the output of DeepL, GoogleNMT, and OpenNMT translating a sentence with this idiom phrase and notice that none capture the idiom translation correctly. not the correct translation, neither does it capture part of the meaning. To illustrate the problem of idiom translation we also provide the output of three NMT systems for this sentence: GoogleNMT (Wu et al., 2016), DeepL1 , and the OpenNMT implementation (Klein et al., 2017) based on Bahdanau et al. (2015) and Luong et al. (2015). All systems fail to generate the proper translation of the idiom expression. This problem is particularly pronounced when the source idiom is very different from its equivalent in the target language, as the case here. 1 925 www.deepl.com/translator Idiom Identification no idiom phrase Bilingual Training Data New Training Data idiom 1 idiom 2 New Test Data … idiom k Bilingual Idiom Resource Sampling Figure 1: The process of data collection and construction of the test set containing only sentence pairs with idiom phrases. Although there"
L18-1148,W17-3204,0,0.0215271,"ual corpora, machine translation 1. Introduction Neural Machine Translation (NMT) (Bahdanau et al., 2015; Sutskever et al., 2014; Cho et al., 2014) has achieved substantial improvements in translation quality over traditional Rule-based and Phrase-based Translation (PBMT) in recent years. For instance, subject-verb agreement, doubleobject verbs, and overlapping subcategorization are various areas where NMT successfully overcomes the limitations of PBMT (Isabelle et al., 2017; Bentivogli et al., 2016). However, one of the remaining challenges of NMT is translating infrequent words and phrases (Koehn and Knowles, 2017; Fadaee et al., 2017) and idioms are a particular instance of this problem (Isabelle et al., 2017). Idioms are semantic lexical units whose meaning is often not simply a function of the meaning of its constituent parts (Nunberg et al., 1994; K¨ovecses and Szabo, 1996). The non-compositionality characteristic of idiom expressions exists in different degrees in a language (Nunberg et al., 1994). In English for example, for the idiom “spill the beans”, the word ‘spill’ symbolizes ‘reveal’ and ‘beans’ symbolizes the ‘secrets’. With the idiomatic expression “kick the bucket”, on the other hand, no"
L18-1148,P07-2045,0,0.00531733,"U Unigram Precision Word-level Accuracy 20.2 26.9 25.2 19.7 24.8 22.5 57.7 53.2 64.1 71.6 67.8 73.2 Table 5: Translation performance on German idiom translation test set. Word-level Idiom Accuracy and Unigram Precision are computed only on the idiom phrase and its corresponding translation in the sentence. In all experiments the NMT vocabulary is limited to the most common 30K words in both languages and we preprocess source and target language data with Byte-pair encoding (BPE) (Sennrich et al., 2016) using 30K merge operations. We also use a Phrase-based translation system similar to Moses (Koehn et al., 2007) as baseline to explore PBMT performance for idiom translation. 4. Idiom Translation Evaluation Ideally idiom translation should be evaluated manually, but this is a very costly process. Automatic metrics, on the other hand, can be used on large data sets at no cost and have the advantage of replicability. We use the following metrics to evaluate the translation quality with a specific focus on idiom translation accuracy: BLEU The traditional BLEU score (Papineni et al., 2002) is a good measure to determine the overall quality of the translations. However this measure considers the precision o"
L18-1148,D15-1166,0,0.273681,"among the signatories. Both don’t have a white essence. Table 1: Example of an idiom phrase in German and its translation. We compare the output of DeepL, GoogleNMT, and OpenNMT translating a sentence with this idiom phrase and notice that none capture the idiom translation correctly. not the correct translation, neither does it capture part of the meaning. To illustrate the problem of idiom translation we also provide the output of three NMT systems for this sentence: GoogleNMT (Wu et al., 2016), DeepL1 , and the OpenNMT implementation (Klein et al., 2017) based on Bahdanau et al. (2015) and Luong et al. (2015). All systems fail to generate the proper translation of the idiom expression. This problem is particularly pronounced when the source idiom is very different from its equivalent in the target language, as the case here. 1 925 www.deepl.com/translator Idiom Identification no idiom phrase Bilingual Training Data New Training Data idiom 1 idiom 2 New Test Data … idiom k Bilingual Idiom Resource Sampling Figure 1: The process of data collection and construction of the test set containing only sentence pairs with idiom phrases. Although there are a number of monolingual data sets available for ide"
L18-1148,D13-1145,0,0.0260704,"te the proper translation of the idiom expression. This problem is particularly pronounced when the source idiom is very different from its equivalent in the target language, as the case here. 1 925 www.deepl.com/translator Idiom Identification no idiom phrase Bilingual Training Data New Training Data idiom 1 idiom 2 New Test Data … idiom k Bilingual Idiom Resource Sampling Figure 1: The process of data collection and construction of the test set containing only sentence pairs with idiom phrases. Although there are a number of monolingual data sets available for identifying idiom expressions (Muzny and Zettlemoyer, 2013; Markantonatou et al., 2017), there is limited work on building a parallel corpus annotated with idioms, which is necessary to investigate this problem more systematically. Salton et al. (2014) selected a small subset of 17 English idioms, collected 10 sentence examples for each idiom from the internet, and manually translated them into Brazilian-Portuguese to use for the translation task. Building a hand-crafted data set for idiom translation is costly and time-consuming. In this paper we automatically build a new bilingual data set for idiom translation extracted from an existing general-pu"
L18-1148,P02-1040,0,0.10232,"(BPE) (Sennrich et al., 2016) using 30K merge operations. We also use a Phrase-based translation system similar to Moses (Koehn et al., 2007) as baseline to explore PBMT performance for idiom translation. 4. Idiom Translation Evaluation Ideally idiom translation should be evaluated manually, but this is a very costly process. Automatic metrics, on the other hand, can be used on large data sets at no cost and have the advantage of replicability. We use the following metrics to evaluate the translation quality with a specific focus on idiom translation accuracy: BLEU The traditional BLEU score (Papineni et al., 2002) is a good measure to determine the overall quality of the translations. However this measure considers the precision of all n-grams in a sentence and by itself does not focus on the translation quality of the idiomatic expressions. Modified Unigram Precision To specifically concentrate on the quality of the translation of idiom expressions, we also look at the localized precision. In this approach we translate the idiomatic expression in the context of a sentence, and only evaluate the translation quality of the idiom phrase. To isolate the idiom translation in the sentence, we look at the wo"
L18-1148,W14-1007,0,0.0155232,"w.deepl.com/translator Idiom Identification no idiom phrase Bilingual Training Data New Training Data idiom 1 idiom 2 New Test Data … idiom k Bilingual Idiom Resource Sampling Figure 1: The process of data collection and construction of the test set containing only sentence pairs with idiom phrases. Although there are a number of monolingual data sets available for identifying idiom expressions (Muzny and Zettlemoyer, 2013; Markantonatou et al., 2017), there is limited work on building a parallel corpus annotated with idioms, which is necessary to investigate this problem more systematically. Salton et al. (2014) selected a small subset of 17 English idioms, collected 10 sentence examples for each idiom from the internet, and manually translated them into Brazilian-Portuguese to use for the translation task. Building a hand-crafted data set for idiom translation is costly and time-consuming. In this paper we automatically build a new bilingual data set for idiom translation extracted from an existing general-purpose German↔English parallel corpus. The first part of our data set consists of 1,500 parallel sentences whose German side contains an idiom, while the second consists of 1,500 parallel sentenc"
L18-1148,P16-1162,0,0.0259326,"chs. 927 WMT test sets 2008-2016 Model PBMT Baseline NMT Baseline NMT &lt;idm&gt; token on source Idiom test set BLEU BLEU Unigram Precision Word-level Accuracy 20.2 26.9 25.2 19.7 24.8 22.5 57.7 53.2 64.1 71.6 67.8 73.2 Table 5: Translation performance on German idiom translation test set. Word-level Idiom Accuracy and Unigram Precision are computed only on the idiom phrase and its corresponding translation in the sentence. In all experiments the NMT vocabulary is limited to the most common 30K words in both languages and we preprocess source and target language data with Byte-pair encoding (BPE) (Sennrich et al., 2016) using 30K merge operations. We also use a Phrase-based translation system similar to Moses (Koehn et al., 2007) as baseline to explore PBMT performance for idiom translation. 4. Idiom Translation Evaluation Ideally idiom translation should be evaluated manually, but this is a very costly process. Automatic metrics, on the other hand, can be used on large data sets at no cost and have the advantage of replicability. We use the following metrics to evaluate the translation quality with a specific focus on idiom translation accuracy: BLEU The traditional BLEU score (Papineni et al., 2002) is a g"
L18-1148,W17-4717,1,\N,Missing
L18-1148,W17-1700,0,\N,Missing
L18-1604,2010.amta-papers.16,0,0.0390509,"Missing"
L18-1604,E12-1045,1,0.832814,"rpora available. To alleviate this problem, we present in this paper novel parallel training and evaluation corpora covering four genres for four language pairs that we automatically harvested from the web. Next, we evaluate the usefulness of the newly collected bilingual resources by exploiting them for genre adaptation of SMT systems. Most existing adaptation approaches depend on the availability of provenance information and make the strong assumption that a translation task has known domain, genre or topic that is exploited to adapt the system (Matsoukas et al., 2009; Foster et al., 2010; Bisazza and Federico, 2012; Chen et al., 2013; Chen et al., 2014; Kobus et al., 2016; Sennrich et al., 2016; Freitag and AlOnaizan, 2016; Chu et al., 2017, among others). While this is a fair assumption in a controlled research setting, it is less realistic in real world applications, such as general-purpose online MT services. In this paper, we provide the SMT system with a test document of unknown origin, and we show that we can use automatic genre classification to guide each test document to the most appropriate pre-trained system. While similar setups have been used in previous work (Xu et al., 2007; Banerjee et a"
L18-1604,2011.iwslt-evaluation.18,1,0.72949,"lates various Gigaword subcorpora with the English sides of the bilingual training corpora. To evaluate the effect of our new bilingual resources, we do not vary the language model between experiments. In order to create genre-specific SMT systems, we have to adequately use the available data. Simply concatenating the different corpora yields a general SMT system that performs reasonably well across a variety of genres, i.e., those covered in the training data, but is not optimal for each individual genre. Since we aim to create genre-specific systems, we use the fill-up technique proposed by Bisazza et al. (2011), in which we combine models trained on a particular genre with models trained on the remaining training corpora. Using this model combination technique, an additional feature is learned that favors genre-specific models, and ‘backs off’ to additional (out-of-genre) models for phrases that are unseen in the genre of interest. For instance, to train our news translation system, we train two phrase tables: one using all news data and one using all non-news data. We use the latter to complement the first with phrase pairs that are not covered in the first. Following the above strategy, we can tra"
L18-1604,P13-1126,0,0.0175776,"e this problem, we present in this paper novel parallel training and evaluation corpora covering four genres for four language pairs that we automatically harvested from the web. Next, we evaluate the usefulness of the newly collected bilingual resources by exploiting them for genre adaptation of SMT systems. Most existing adaptation approaches depend on the availability of provenance information and make the strong assumption that a translation task has known domain, genre or topic that is exploited to adapt the system (Matsoukas et al., 2009; Foster et al., 2010; Bisazza and Federico, 2012; Chen et al., 2013; Chen et al., 2014; Kobus et al., 2016; Sennrich et al., 2016; Freitag and AlOnaizan, 2016; Chu et al., 2017, among others). While this is a fair assumption in a controlled research setting, it is less realistic in real world applications, such as general-purpose online MT services. In this paper, we provide the SMT system with a test document of unknown origin, and we show that we can use automatic genre classification to guide each test document to the most appropriate pre-trained system. While similar setups have been used in previous work (Xu et al., 2007; Banerjee et al., 2010; Pecina et"
L18-1604,2014.amta-researchers.10,0,0.0179565,"present in this paper novel parallel training and evaluation corpora covering four genres for four language pairs that we automatically harvested from the web. Next, we evaluate the usefulness of the newly collected bilingual resources by exploiting them for genre adaptation of SMT systems. Most existing adaptation approaches depend on the availability of provenance information and make the strong assumption that a translation task has known domain, genre or topic that is exploited to adapt the system (Matsoukas et al., 2009; Foster et al., 2010; Bisazza and Federico, 2012; Chen et al., 2013; Chen et al., 2014; Kobus et al., 2016; Sennrich et al., 2016; Freitag and AlOnaizan, 2016; Chu et al., 2017, among others). While this is a fair assumption in a controlled research setting, it is less realistic in real world applications, such as general-purpose online MT services. In this paper, we provide the SMT system with a test document of unknown origin, and we show that we can use automatic genre classification to guide each test document to the most appropriate pre-trained system. While similar setups have been used in previous work (Xu et al., 2007; Banerjee et al., 2010; Pecina et al., 2011; Wang et"
L18-1604,P17-2061,0,0.0167851,"or four language pairs that we automatically harvested from the web. Next, we evaluate the usefulness of the newly collected bilingual resources by exploiting them for genre adaptation of SMT systems. Most existing adaptation approaches depend on the availability of provenance information and make the strong assumption that a translation task has known domain, genre or topic that is exploited to adapt the system (Matsoukas et al., 2009; Foster et al., 2010; Bisazza and Federico, 2012; Chen et al., 2013; Chen et al., 2014; Kobus et al., 2016; Sennrich et al., 2016; Freitag and AlOnaizan, 2016; Chu et al., 2017, among others). While this is a fair assumption in a controlled research setting, it is less realistic in real world applications, such as general-purpose online MT services. In this paper, we provide the SMT system with a test document of unknown origin, and we show that we can use automatic genre classification to guide each test document to the most appropriate pre-trained system. While similar setups have been used in previous work (Xu et al., 2007; Banerjee et al., 2010; Pecina et al., 2011; Wang et al., 2012; Pecina et al., 2015), we are the first to extend this setup to four genres and"
L18-1604,W01-1007,0,0.1249,"Missing"
L18-1604,W07-0717,0,0.0414157,"rather than a single genre-agnostic system. Finally, we train and use genre classifiers to route test documents to the most appropriate genre systems. The results of these experiments show that our multi-genre benchmarks can serve to advance research on text genre adaptation for MT. Keywords: Machine translation, parallel benchmarks, text genres, genre adaptation 1. Introduction Text genre differences have shown to affect the output quality of statistical machine translation (SMT) systems: SMT systems trained on one genre often achieve poor performance when used for translating another genre (Foster and Kuhn, 2007; Matsoukas et al., 2009; Wang et al., 2012, among others). In addition, even if different genres in a test set are both present in equal amounts in the bilingual training data, performance differences between the test genres can be large, mostly due to poor model coverage for certain genres (van der Wees et al., 2015a; van der Wees et al., 2015b). In this paper, we evaluate the impact of genre differences on phrase-based SMT for a diverse set of language pairs, covering both commonly and rarely studied language pairs. For common language pairs, parallel training data is abundant but limited t"
L18-1604,D10-1044,0,0.027298,"ew to no bilingual corpora available. To alleviate this problem, we present in this paper novel parallel training and evaluation corpora covering four genres for four language pairs that we automatically harvested from the web. Next, we evaluate the usefulness of the newly collected bilingual resources by exploiting them for genre adaptation of SMT systems. Most existing adaptation approaches depend on the availability of provenance information and make the strong assumption that a translation task has known domain, genre or topic that is exploited to adapt the system (Matsoukas et al., 2009; Foster et al., 2010; Bisazza and Federico, 2012; Chen et al., 2013; Chen et al., 2014; Kobus et al., 2016; Sennrich et al., 2016; Freitag and AlOnaizan, 2016; Chu et al., 2017, among others). While this is a fair assumption in a controlled research setting, it is less realistic in real world applications, such as general-purpose online MT services. In this paper, we provide the SMT system with a test document of unknown origin, and we show that we can use automatic genre classification to guide each test document to the most appropriate pre-trained system. While similar setups have been used in previous work (Xu"
L18-1604,P05-1071,0,0.102597,"n the sky, but we have already tasted it in Africa, where Sierra Leone’s agenda for prosperity 2013–2017 and the Liberia Vision 2030 exemplify the potential of such programs. News She is not only the first Saudi woman to ever attempt the climb but also the youngest Arab to make it to the top of the world’s highest peak. Speech These are just a few of the milestones of recent progress. I have another reason to be optimistic. I know global health is guided by the right values. Table 2: English example sentences for four genres in the web-harvested evaluation corpora. all Arabic data using MADA (Habash and Rambow, 2005), segment the Chinese data following (Tseng et al., 2005), and use a simple in-house tokenizer for the other languages. The total numbers of foreign→English sentence pairs for the four genres and four language pairs are listed in Tables 1a–1d. In addition, Table 2 shows English example sentences for each of the four genres. 3. Evaluating genre differences in SMT In this section, we use our newly assembled resources to evaluate SMT performance across different genres and language pairs. 3823 Coll. Edit. News Speech 11.7 22.6 22.6 11.5 SMT system optimized for Combined Coll. Edit. News Speech be"
L18-1604,C94-2174,0,0.627046,"Missing"
L18-1604,P97-1005,0,0.047111,"Missing"
L18-1604,P07-2045,0,0.00720101,"7 22.6 All 21.9 20.8 Test genre Baseline (c) Bulgarian→English results. – 21.3 21.5 (d) Persian→English results. Table 3: Translation quality in BLEU of four test genres using genre-optimized systems and a genre-agnostic baseline. Best results for each test set genre are boldfaced. ‘Combined best BLEU’ indicates the overall BLEU score when combining the bold-faced results of all test genres in a single test set, followed by the difference with the genre-agnostic system. 3.1. Experimental setup All SMT systems in this paper are trained using an inhouse phrase-based SMT system similar to Moses (Koehn et al., 2007). To train our systems, we use our webcrawled corpora, supplemented with commonly used training data, if available: LDC corpora for Arabic→English and Chinese→English, and Europarl data (Koehn, 2005) for Bulgarian→English. In addition, we use a 5-gram language model that linearly interpolates various Gigaword subcorpora with the English sides of the bilingual training corpora. To evaluate the effect of our new bilingual resources, we do not vary the language model between experiments. In order to create genre-specific SMT systems, we have to adequately use the available data. Simply concatenat"
L18-1604,2005.mtsummit-papers.11,0,0.0603303,"genre-agnostic baseline. Best results for each test set genre are boldfaced. ‘Combined best BLEU’ indicates the overall BLEU score when combining the bold-faced results of all test genres in a single test set, followed by the difference with the genre-agnostic system. 3.1. Experimental setup All SMT systems in this paper are trained using an inhouse phrase-based SMT system similar to Moses (Koehn et al., 2007). To train our systems, we use our webcrawled corpora, supplemented with commonly used training data, if available: LDC corpora for Arabic→English and Chinese→English, and Europarl data (Koehn, 2005) for Bulgarian→English. In addition, we use a 5-gram language model that linearly interpolates various Gigaword subcorpora with the English sides of the bilingual training corpora. To evaluate the effect of our new bilingual resources, we do not vary the language model between experiments. In order to create genre-specific SMT systems, we have to adequately use the available data. Simply concatenating the different corpora yields a general SMT system that performs reasonably well across a variety of genres, i.e., those covered in the training data, but is not optimal for each individual genre."
L18-1604,ma-2006-champollion,0,0.0368668,"‘en-US’ with ‘ar’. To determine the genre of each text we use categories indicated on the respective website. For example, websites that support news articles and user comments (i.e., the genres news and colloquial), have clear website sections for these different genres. While not all language-genre combinations are equally common, we can construct at least a translation test set for each of the four genres in each of the four language pairs. To do so, we first create sentence-parallel corpora using a combination of Moore’s sentence alignment (Moore, 2002) and Champollion sentence alignment (Ma, 2006). We then organize the collected bilingual data into training, development, and test sets, such that each portion contains documents from non-overlapping time periods.2 We tokenize 2 The benchmarks are available for download http://ilps.science.uva.nl/resources/ genre-benchmarks. at Genre Example sentence(s) Colloquial Ministers should be sitting and attending the oath, like in Italy. Editorial This may sound like pie in the sky, but we have already tasted it in Africa, where Sierra Leone’s agenda for prosperity 2013–2017 and the Liberia Vision 2030 exemplify the potential of such programs. Ne"
L18-1604,D09-1074,0,0.158285,"nre-agnostic system. Finally, we train and use genre classifiers to route test documents to the most appropriate genre systems. The results of these experiments show that our multi-genre benchmarks can serve to advance research on text genre adaptation for MT. Keywords: Machine translation, parallel benchmarks, text genres, genre adaptation 1. Introduction Text genre differences have shown to affect the output quality of statistical machine translation (SMT) systems: SMT systems trained on one genre often achieve poor performance when used for translating another genre (Foster and Kuhn, 2007; Matsoukas et al., 2009; Wang et al., 2012, among others). In addition, even if different genres in a test set are both present in equal amounts in the bilingual training data, performance differences between the test genres can be large, mostly due to poor model coverage for certain genres (van der Wees et al., 2015a; van der Wees et al., 2015b). In this paper, we evaluate the impact of genre differences on phrase-based SMT for a diverse set of language pairs, covering both commonly and rarely studied language pairs. For common language pairs, parallel training data is abundant but limited to a few genres such as p"
L18-1604,moore-2002-fast,0,0.166016,"anguage abbreviation in the url, e.g., replacing ‘en-US’ with ‘ar’. To determine the genre of each text we use categories indicated on the respective website. For example, websites that support news articles and user comments (i.e., the genres news and colloquial), have clear website sections for these different genres. While not all language-genre combinations are equally common, we can construct at least a translation test set for each of the four genres in each of the four language pairs. To do so, we first create sentence-parallel corpora using a combination of Moore’s sentence alignment (Moore, 2002) and Champollion sentence alignment (Ma, 2006). We then organize the collected bilingual data into training, development, and test sets, such that each portion contains documents from non-overlapping time periods.2 We tokenize 2 The benchmarks are available for download http://ilps.science.uva.nl/resources/ genre-benchmarks. at Genre Example sentence(s) Colloquial Ministers should be sitting and attending the oath, like in Italy. Editorial This may sound like pie in the sky, but we have already tasted it in Africa, where Sierra Leone’s agenda for prosperity 2013–2017 and the Liberia Vision 203"
L18-1604,P02-1040,0,0.114321,"data. Genres not covered in the training data have to be translated using a system trained on a mixture of genres or on one of the other genre-specific systems. For example, editorial Persian→English data is scarce, so for Persian editorial documents we have to resort to our colloquial, news, speech or mixed system. In addition to using the fill-up approach, we tune each genre-specific system on a development set covering only the genre of interest. 3.2. Results Tables 3a–3d show the translation quality results for all language pairs. For each language pair, we measure case-insensitive BLEU (Papineni et al., 2002) for our four test genres with the available genre-specific systems as well as the genre-agnostic system. Note that some Arabic→English and Chinese→English BLEU scores might be lower than those reported in literature since our test data contains only a single reference translation. The results confirm our expectation that the various test set genres benefit from being translated using a genreoptimized system rather than using a general system: generally, the highest BLEU scores are located on the diagonal of each table. In cases where no genre-specific system is available, we see that the best"
L18-1604,2011.eamt-1.40,0,0.0221275,"al., 2013; Chen et al., 2014; Kobus et al., 2016; Sennrich et al., 2016; Freitag and AlOnaizan, 2016; Chu et al., 2017, among others). While this is a fair assumption in a controlled research setting, it is less realistic in real world applications, such as general-purpose online MT services. In this paper, we provide the SMT system with a test document of unknown origin, and we show that we can use automatic genre classification to guide each test document to the most appropriate pre-trained system. While similar setups have been used in previous work (Xu et al., 2007; Banerjee et al., 2010; Pecina et al., 2011; Wang et al., 2012; Pecina et al., 2015), we are the first to extend this setup to four genres and four language pairs. Finally, we show that an adaptation method based on automatic classifiers also improves translation quality for genres with no parallel training data available. 2. Multi-genre benchmarks In this section, we describe the construction of multi-genre corpora for four language pairs and four genres, which we obtained using an automated web-harvesting process. 2.1. Language pairs and genres While most research in MT is evaluated on a small number of well resourced language pairs"
L18-1604,W05-0908,0,0.150376,"Missing"
L18-1604,P16-1009,0,0.0205039,"aining and evaluation corpora covering four genres for four language pairs that we automatically harvested from the web. Next, we evaluate the usefulness of the newly collected bilingual resources by exploiting them for genre adaptation of SMT systems. Most existing adaptation approaches depend on the availability of provenance information and make the strong assumption that a translation task has known domain, genre or topic that is exploited to adapt the system (Matsoukas et al., 2009; Foster et al., 2010; Bisazza and Federico, 2012; Chen et al., 2013; Chen et al., 2014; Kobus et al., 2016; Sennrich et al., 2016; Freitag and AlOnaizan, 2016; Chu et al., 2017, among others). While this is a fair assumption in a controlled research setting, it is less realistic in real world applications, such as general-purpose online MT services. In this paper, we provide the SMT system with a test document of unknown origin, and we show that we can use automatic genre classification to guide each test document to the most appropriate pre-trained system. While similar setups have been used in previous work (Xu et al., 2007; Banerjee et al., 2010; Pecina et al., 2011; Wang et al., 2012; Pecina et al., 2015), we are th"
L18-1604,C00-2117,0,0.0297715,"Missing"
L18-1604,W15-4304,1,0.893602,"Missing"
L18-1604,P15-2092,1,0.87531,"Missing"
L18-1604,2012.amta-papers.18,0,0.298726,"ally, we train and use genre classifiers to route test documents to the most appropriate genre systems. The results of these experiments show that our multi-genre benchmarks can serve to advance research on text genre adaptation for MT. Keywords: Machine translation, parallel benchmarks, text genres, genre adaptation 1. Introduction Text genre differences have shown to affect the output quality of statistical machine translation (SMT) systems: SMT systems trained on one genre often achieve poor performance when used for translating another genre (Foster and Kuhn, 2007; Matsoukas et al., 2009; Wang et al., 2012, among others). In addition, even if different genres in a test set are both present in equal amounts in the bilingual training data, performance differences between the test genres can be large, mostly due to poor model coverage for certain genres (van der Wees et al., 2015a; van der Wees et al., 2015b). In this paper, we evaluate the impact of genre differences on phrase-based SMT for a diverse set of language pairs, covering both commonly and rarely studied language pairs. For common language pairs, parallel training data is abundant but limited to a few genres such as parliamentary and le"
L18-1604,2007.mtsummit-papers.68,0,0.215911,"10; Bisazza and Federico, 2012; Chen et al., 2013; Chen et al., 2014; Kobus et al., 2016; Sennrich et al., 2016; Freitag and AlOnaizan, 2016; Chu et al., 2017, among others). While this is a fair assumption in a controlled research setting, it is less realistic in real world applications, such as general-purpose online MT services. In this paper, we provide the SMT system with a test document of unknown origin, and we show that we can use automatic genre classification to guide each test document to the most appropriate pre-trained system. While similar setups have been used in previous work (Xu et al., 2007; Banerjee et al., 2010; Pecina et al., 2011; Wang et al., 2012; Pecina et al., 2015), we are the first to extend this setup to four genres and four language pairs. Finally, we show that an adaptation method based on automatic classifiers also improves translation quality for genres with no parallel training data available. 2. Multi-genre benchmarks In this section, we describe the construction of multi-genre corpora for four language pairs and four genres, which we obtained using an automated web-harvesting process. 2.1. Language pairs and genres While most research in MT is evaluated on a sm"
N16-1036,W14-4012,0,0.0588047,"Missing"
N16-1036,P15-1033,0,0.0192988,"features within LSTMs in their sentence compression model hurts the performance of overall system. They then hypothesize that a basic LSTM is powerful enough to capture syntactic aspects which are useful for compression. Introduction Recurrent Neural Networks (RNNs) (Elman, 1990; Mikolov et al., 2010) are remarkably powerful models for sequential data. Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), a specific architecture of RNN, has a track record of success in many natural language processing tasks such as language modeling (J´ozefowicz et al., 2015), dependency parsing (Dyer et al., 2015), sentence com1 Our code and data are available at https://github. com/ketranm/RMN To understand and explain which linguistic dimensions are captured by an LSTM is non-trivial. This is due to the fact that the sequences of input histories are compressed into several dense vectors by the LSTM’s components whose purposes with respect to representing linguistic information is not evident. To our knowledge, the only attempt to better understand the reasons of an LSTM’s performance and limitations is the work of Karpathy et al. (2015) by means of visualization experiments and cell activation statis"
N16-1036,D15-1042,0,0.0726834,"Missing"
N16-1036,W14-0406,0,0.0784721,"Missing"
N16-1036,J93-2004,0,0.0539502,"122.9 92.4 94.0 127.2 130.4 +tM-g –tM-g 118.6 129.7 88.9 96.6 128.8 135.7 RMR RM Table 2: Perplexity comparison including RMN variants with and without temporal matrix (tM) and linear (l) versus gating (g) composition function. overall performance is not as good as RM. This suggests that we need a better mechanism to address the interaction between MBs, which we leave to future work. Finally, the proposed gating composition function outperforms the linear one in most cases. For historical reasons, we also run a stacked threelayer LSTM and a RM(+tM-g) on the much smaller Penn Treebank dataset (Marcus et al., 1993) with the same setting described above. The respective perplexities are 126.1 and 123.5. 5 Results De Attention Analysis The goal of our RMN design is twofold: (i) to obtain better predictive power and (ii) to facilitate understanding of the model and discover patterns in data. In Section 4, we have validated the predictive power of the RMN and below we investigate the source of this performance based on linguistic assumptions of word co-occurrences and dependency structures. 5.1 Positional and lexical analysis As a first step towards understanding RMN, we look at the average attention weights"
N16-1036,P15-2084,0,0.0789589,"Missing"
N16-1036,R13-1079,0,0.0561822,"Missing"
N16-1036,simi-etal-2014-less,0,0.0136173,"occurrences (bottom of Figure 6), the most attended are argument heads (→arg), complement heads (→comp), object heads (→obj) and subjects (subj←). This suggests that RMN is mainly capturing predicate argument structure in Italian. Notice that syntactic annotation is never used to train the model, but only to analyze its predictions. We can also use RMN to discover which complex dependency paths are important for word prediction. To mention just a few examples, high attention on 2 The full plots are available at https://github.com/ ketranm/RMN. The German and Italian tag sets are explained in (Simi et al., 2014) and (Foth, 2006) respectively. 327 3 Some dependency directions, like obj← in Italian, are almost never observed due to order constraints of the language. adv← →aux →avz →kon konj← obja← →objc →obji →rel subj← [ALL] 0.5 0.4 0.3 0.2 0.1 [-15, -12] [-11, -8] [-7, -4] -3 -2 -1 →arg →comp comp← →con →mod mod← →obj →pred →sub subj← [ALL] 0.0 0.5 0.4 0.3 0.2 0.1 [-15, -12] [-11, -8] [-7, -4] -3 -2 -1 Figure 6: Average attention weights per position, broken down by dependency relation type+direction between the attended word and the word to predict. Top: German. Bottom: Italian. More distant positio"
N16-1036,D11-1014,0,0.0158104,"ir 5,400 experimental runs suggest that forget gates and output gates are the most critical components of LSTMs. J´ozefowicz et al. (2015) conduct and evaluate over ten thousand RNN architectures and find that the initialization of the forget gate bias is crucial to the LSTM’s performance. While these findings are important to help choosing appropriate LSTM architectures, they do not shed light on what information is captured by the hidden states of an LSTM. Bowman et al. (2015) show that a vanilla LSTM, such as described above, performs reasonably well compared to a recursive neural network (Socher et al., 2011) that explicitly exploits tree structures on two artificial datasets. They find that LSTMs can effectively exploit recursive structure in the artificial datasets. In contrast to these simple datasets containing a few logical operations in their experiments, natural languages exhibit highly complex patterns. The extent to which linguistic assumptions about syntactic structures and compositional semantics are reflected in LSTMs is rather poorly understood. Thus it is desirable to have a more principled mechanism allowing us to inspect recurrent architectures from a linguistic perspective. In the"
N16-1036,W12-2704,0,0.0658696,"Missing"
N16-1036,W15-3001,1,\N,Missing
P15-2092,D10-1044,0,0.190732,"Missing"
P15-2092,D11-1033,0,0.189801,"Missing"
P15-2092,W12-3154,0,0.061537,"ry to topic, covering the non-topical text properties function, style, and text type. Like topics, genres can also exhibit different levels of granularity (Lee, 2001). Examples of genres include formal or informal text (high-level), and newswire, editorials, and user-generated text (low-level). Genre Topic Topic and genre are both intrinsic properties of texts, but most work on domain adaptation uses provenance or subcorpus information to adapt SMT systems to a specific translation task (Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Sennrich, 2012; Bisazza and Federico, 2012; Haddow and Koehn, 2012, among others). In recent years, some work has explicitly addressed topic adaptation for SMT (Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a; Hasler et al., 2014c) using latent Dirichlet allocation (Blei et al., 2003). While Hasler et al. (2014b) showed that provenance and topic can serve as complements to each other, the effects of genre and topic on SMT have not been systematically studied. 3 NW UG Total Culture segments tokens 654 15.5K 507 14.9K 1161 30.4K Economy segments tokens 500 16.0K 578 15.5K 1078 31.5K Health segments tokens 384 9.7K 319 9.3K 703 19.1K Pol"
P15-2092,N10-1064,0,0.345414,"ic and genre distributions, we have shown that earlier findings explaining the differences across topics only explain to a limited degree translation performance differences across genres. Our analysis shows that genre-specific errors are more attributable to model-coverage errors than to suboptimal scoring of existing translation candidates. This suggests that future work on improving SMT across genres needs to investigate approaches that increase model coverage. Our fine-grained manual error analysis at the word level also suggests that source coverage could benefit from text normalization (Bertoldi et al., 2010). Finally, we make both our benchmark and the manual OOV annotations publicly available. is a subclass of spelling errors). In total, we consider 17 subclasses which we group into five main classes, see Table 5 for examples. Table 6 shows the type level percentages3 for each main OOV class per genre or topic. When comparing the two genres, a number of observations emerge. Firstly, rare but correct words (e.g., proper nouns and technical terms, both regular issues for adaptation in SMT) make up the vast majority of the OOVs in NW, but are relatively infrequent in UG. By contrast, OOVs containin"
P15-2092,E14-1035,0,0.074205,"Examples of genres include formal or informal text (high-level), and newswire, editorials, and user-generated text (low-level). Genre Topic Topic and genre are both intrinsic properties of texts, but most work on domain adaptation uses provenance or subcorpus information to adapt SMT systems to a specific translation task (Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Sennrich, 2012; Bisazza and Federico, 2012; Haddow and Koehn, 2012, among others). In recent years, some work has explicitly addressed topic adaptation for SMT (Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a; Hasler et al., 2014c) using latent Dirichlet allocation (Blei et al., 2003). While Hasler et al. (2014b) showed that provenance and topic can serve as complements to each other, the effects of genre and topic on SMT have not been systematically studied. 3 NW UG Total Culture segments tokens 654 15.5K 507 14.9K 1161 30.4K Economy segments tokens 500 16.0K 578 15.5K 1078 31.5K Health segments tokens 384 9.7K 319 9.3K 703 19.1K Politics segments tokens 494 15.8K 646 15.8K 1140 31.6K Security segments tokens 532 16.1K 826 15.9K 1358 32.0K Total segments tokens 2564 73.2K 2876 71.3K 5440 144.5K"
P15-2092,E12-1045,1,0.885574,"sed as a concept complementary to topic, covering the non-topical text properties function, style, and text type. Like topics, genres can also exhibit different levels of granularity (Lee, 2001). Examples of genres include formal or informal text (high-level), and newswire, editorials, and user-generated text (low-level). Genre Topic Topic and genre are both intrinsic properties of texts, but most work on domain adaptation uses provenance or subcorpus information to adapt SMT systems to a specific translation task (Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Sennrich, 2012; Bisazza and Federico, 2012; Haddow and Koehn, 2012, among others). In recent years, some work has explicitly addressed topic adaptation for SMT (Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a; Hasler et al., 2014c) using latent Dirichlet allocation (Blei et al., 2003). While Hasler et al. (2014b) showed that provenance and topic can serve as complements to each other, the effects of genre and topic on SMT have not been systematically studied. 3 NW UG Total Culture segments tokens 654 15.5K 507 14.9K 1161 30.4K Economy segments tokens 500 16.0K 578 15.5K 1078 31.5K Health segments tokens 384 9.7"
P15-2092,2014.amta-researchers.11,0,0.0316693,"Examples of genres include formal or informal text (high-level), and newswire, editorials, and user-generated text (low-level). Genre Topic Topic and genre are both intrinsic properties of texts, but most work on domain adaptation uses provenance or subcorpus information to adapt SMT systems to a specific translation task (Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Sennrich, 2012; Bisazza and Federico, 2012; Haddow and Koehn, 2012, among others). In recent years, some work has explicitly addressed topic adaptation for SMT (Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a; Hasler et al., 2014c) using latent Dirichlet allocation (Blei et al., 2003). While Hasler et al. (2014b) showed that provenance and topic can serve as complements to each other, the effects of genre and topic on SMT have not been systematically studied. 3 NW UG Total Culture segments tokens 654 15.5K 507 14.9K 1161 30.4K Economy segments tokens 500 16.0K 578 15.5K 1078 31.5K Health segments tokens 384 9.7K 319 9.3K 703 19.1K Politics segments tokens 494 15.8K 646 15.8K 1140 31.6K Security segments tokens 532 16.1K 826 15.9K 1358 32.0K Total segments tokens 2564 73.2K 2876 71.3K 5440 144.5K"
P15-2092,2011.iwslt-evaluation.18,1,0.880842,"04) concludes that the term genre is used as a concept complementary to topic, covering the non-topical text properties function, style, and text type. Like topics, genres can also exhibit different levels of granularity (Lee, 2001). Examples of genres include formal or informal text (high-level), and newswire, editorials, and user-generated text (low-level). Genre Topic Topic and genre are both intrinsic properties of texts, but most work on domain adaptation uses provenance or subcorpus information to adapt SMT systems to a specific translation task (Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Sennrich, 2012; Bisazza and Federico, 2012; Haddow and Koehn, 2012, among others). In recent years, some work has explicitly addressed topic adaptation for SMT (Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a; Hasler et al., 2014c) using latent Dirichlet allocation (Blei et al., 2003). While Hasler et al. (2014b) showed that provenance and topic can serve as complements to each other, the effects of genre and topic on SMT have not been systematically studied. 3 NW UG Total Culture segments tokens 654 15.5K 507 14.9K 1161 30.4K Economy segments tokens 500 16.0K 578 15."
P15-2092,W14-3358,0,0.0351605,"Examples of genres include formal or informal text (high-level), and newswire, editorials, and user-generated text (low-level). Genre Topic Topic and genre are both intrinsic properties of texts, but most work on domain adaptation uses provenance or subcorpus information to adapt SMT systems to a specific translation task (Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Sennrich, 2012; Bisazza and Federico, 2012; Haddow and Koehn, 2012, among others). In recent years, some work has explicitly addressed topic adaptation for SMT (Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a; Hasler et al., 2014c) using latent Dirichlet allocation (Blei et al., 2003). While Hasler et al. (2014b) showed that provenance and topic can serve as complements to each other, the effects of genre and topic on SMT have not been systematically studied. 3 NW UG Total Culture segments tokens 654 15.5K 507 14.9K 1161 30.4K Economy segments tokens 500 16.0K 578 15.5K 1078 31.5K Health segments tokens 384 9.7K 319 9.3K 703 19.1K Politics segments tokens 494 15.8K 646 15.8K 1140 31.6K Security segments tokens 532 16.1K 826 15.9K 1358 32.0K Total segments tokens 2564 73.2K 2876 71.3K 5440 144.5K"
P15-2092,P13-2122,0,0.0363058,"of granularity (Lee, 2001). Examples of genres include formal or informal text (high-level), and newswire, editorials, and user-generated text (low-level). Genre Topic Topic and genre are both intrinsic properties of texts, but most work on domain adaptation uses provenance or subcorpus information to adapt SMT systems to a specific translation task (Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Sennrich, 2012; Bisazza and Federico, 2012; Haddow and Koehn, 2012, among others). In recent years, some work has explicitly addressed topic adaptation for SMT (Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a; Hasler et al., 2014c) using latent Dirichlet allocation (Blei et al., 2003). While Hasler et al. (2014b) showed that provenance and topic can serve as complements to each other, the effects of genre and topic on SMT have not been systematically studied. 3 NW UG Total Culture segments tokens 654 15.5K 507 14.9K 1161 30.4K Economy segments tokens 500 16.0K 578 15.5K 1078 31.5K Health segments tokens 384 9.7K 319 9.3K 703 19.1K Politics segments tokens 494 15.8K 646 15.8K 1140 31.6K Security segments tokens 532 16.1K 826 15.9K 1358 32.0K Total segments tokens 2564 73.2K 28"
P15-2092,N06-1003,0,0.175312,"Missing"
P15-2092,D11-1125,0,0.0593822,"what extent this gap can be attributed to genre differences. We first run a translation experiment on the Gen&Topic test set using our in-house phrasebased SMT system similar to Moses (Koehn et al., 2007). Features include lexicalized reordering, linear distortion with limit 5, and lexical weighting. In addition, we use a 5-gram linearly interpolated language model, trained on 1.6B words with Kneser-Ney smoothing (Chen and Goodman, 1999), that covers all topics and genres contained in the benchmark. We tune our system on the Gen&Topic development set using pairwise ranking optimization (PRO) (Hopkins and May, 2011). Naturally, performance differences across topics and genres depend on the degree to which both are represented in the parallel training data. To allow for fair comparison, we down-sample our available training data to be as balanced as possible in terms of topics and genres. The resulting system is trained on approximately 200K sentence pairs with 6M source tokens per genre, as much as is available for UG. All data originates from the same web sources as the documents in the benchmark. Our more competitive system (van der Wees et al., 2015) that uses also LDC-distributed data yields slightly"
P15-2092,P13-1126,0,0.175386,"Missing"
P15-2092,W14-1617,0,0.0921081,"Missing"
P15-2092,Q13-1035,0,0.0966154,"Missing"
P15-2092,2010.iwslt-papers.5,0,0.0602458,"tions, Santini (2004) concludes that the term genre is used as a concept complementary to topic, covering the non-topical text properties function, style, and text type. Like topics, genres can also exhibit different levels of granularity (Lee, 2001). Examples of genres include formal or informal text (high-level), and newswire, editorials, and user-generated text (low-level). Genre Topic Topic and genre are both intrinsic properties of texts, but most work on domain adaptation uses provenance or subcorpus information to adapt SMT systems to a specific translation task (Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Sennrich, 2012; Bisazza and Federico, 2012; Haddow and Koehn, 2012, among others). In recent years, some work has explicitly addressed topic adaptation for SMT (Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a; Hasler et al., 2014c) using latent Dirichlet allocation (Blei et al., 2003). While Hasler et al. (2014b) showed that provenance and topic can serve as complements to each other, the effects of genre and topic on SMT have not been systematically studied. 3 NW UG Total Culture segments tokens 654 15.5K 507 14.9K 1161 30.4K Economy segments to"
P15-2092,P12-2023,0,0.0276135,"hibit different levels of granularity (Lee, 2001). Examples of genres include formal or informal text (high-level), and newswire, editorials, and user-generated text (low-level). Genre Topic Topic and genre are both intrinsic properties of texts, but most work on domain adaptation uses provenance or subcorpus information to adapt SMT systems to a specific translation task (Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Sennrich, 2012; Bisazza and Federico, 2012; Haddow and Koehn, 2012, among others). In recent years, some work has explicitly addressed topic adaptation for SMT (Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a; Hasler et al., 2014c) using latent Dirichlet allocation (Blei et al., 2003). While Hasler et al. (2014b) showed that provenance and topic can serve as complements to each other, the effects of genre and topic on SMT have not been systematically studied. 3 NW UG Total Culture segments tokens 654 15.5K 507 14.9K 1161 30.4K Economy segments tokens 500 16.0K 578 15.5K 1078 31.5K Health segments tokens 384 9.7K 319 9.3K 703 19.1K Politics segments tokens 494 15.8K 646 15.8K 1140 31.6K Security segments tokens 532 16.1K 826 15.9K 1358 32.0K Total s"
P15-2092,P07-2045,0,0.0057445,"truction of the Gen&Topic data set and the use of down-sampled training data. On the other hand, the gap between different topics is only 0.6 BLEU points on average, and at most 1.1 (between culture and politics). A translation quality gap between genres has also been observed in past OpenMT evaluation campaigns. However, as the NIST benchmarks have not been controlled for topics across genres, it is unclear to what extent this gap can be attributed to genre differences. We first run a translation experiment on the Gen&Topic test set using our in-house phrasebased SMT system similar to Moses (Koehn et al., 2007). Features include lexicalized reordering, linear distortion with limit 5, and lexical weighting. In addition, we use a 5-gram linearly interpolated language model, trained on 1.6B words with Kneser-Ney smoothing (Chen and Goodman, 1999), that covers all topics and genres contained in the benchmark. We tune our system on the Gen&Topic development set using pairwise ranking optimization (PRO) (Hopkins and May, 2011). Naturally, performance differences across topics and genres depend on the degree to which both are represented in the parallel training data. To allow for fair comparison, we down-"
P15-2092,D09-1074,0,0.264401,"Missing"
P15-2092,P02-1040,0,0.0921924,"approximately 200K sentence pairs with 6M source tokens per genre, as much as is available for UG. All data originates from the same web sources as the documents in the benchmark. Our more competitive system (van der Wees et al., 2015) that uses also LDC-distributed data yields slightly higher BLEU scores, but is more favorable for NW than for UG translation tasks. Due to the strict data requirements in terms of topic and genre distributions, as well as the availability of sizable parallel training data, our current experimental set-up covers Arabic-English only. Table 3 compares BLEU scores (Papineni et al., 2002, 1 reference) of the Gen&Topic data, split down by topics and genres. We observe that trans4.2 Model coverage analysis Next, to explain the large performance gap between genres, we analyze the phrase lengths within Viterbi translations, source phrase and phrase pair recall, and phrase pair OOV of the Gen&Topic test set (Table 4). Average source-side phrase length We first compute the average number of source words contained in the phrases that our SMT system uses to produce the 1-best translations for the Gen&Topic test set. One can see that UG is translated with shorter phrases than NW, and"
P15-2092,E12-1055,0,0.0286028,"term genre is used as a concept complementary to topic, covering the non-topical text properties function, style, and text type. Like topics, genres can also exhibit different levels of granularity (Lee, 2001). Examples of genres include formal or informal text (high-level), and newswire, editorials, and user-generated text (low-level). Genre Topic Topic and genre are both intrinsic properties of texts, but most work on domain adaptation uses provenance or subcorpus information to adapt SMT systems to a specific translation task (Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Sennrich, 2012; Bisazza and Federico, 2012; Haddow and Koehn, 2012, among others). In recent years, some work has explicitly addressed topic adaptation for SMT (Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a; Hasler et al., 2014c) using latent Dirichlet allocation (Blei et al., 2003). While Hasler et al. (2014b) showed that provenance and topic can serve as complements to each other, the effects of genre and topic on SMT have not been systematically studied. 3 NW UG Total Culture segments tokens 654 15.5K 507 14.9K 1161 30.4K Economy segments tokens 500 16.0K 578 15.5K 1078 31.5K He"
P15-2092,W15-4304,1,0.787798,"Missing"
P15-2092,W07-0717,0,\N,Missing
P16-2007,W02-1018,0,0.0720679,"strategy to utilize additional monoIntroduction In phrase-based SMT, the phrase pairs in the translation model are traditionally trained by applying a heuristic extraction method (Och and Ney, 2000) which extracts phrase pairs based on consistency of word alignments from a word-aligned bilingual training data. The probabilities of the translation model are then calculated based on the relative frequencies of the extracted phrase pairs. A notable shortcoming of this approach is that the translation model probabilities thus calculated from the training bitext can be unintuitive and unreliable (Marcu and Wong, 2002; Foster et al., 2006) as they reflect only the distribution over the phrase pairs observed in the training data. However, from an SMT perspective it is important that the models reflect probability distributions which are preferred by the decoding process, i.e., phrase translations which are likely to be used frequently to achieve better translations should get higher scores and phrases which are 38 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 38–44, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics “ mose Mo"
P16-2007,D08-1024,0,0.503551,"f aligning them to high quality human translations as in forced decoding. Another relevant line of research relates tuning (weight optimisation), where our work lies between forced decoding (Wuebker et al., 2010) and the bold updating approach of (Liang et al., 2006). However, our approach specifically proposes a novel method for training models using oracle BLEU translations. 3 3.1 Oracle BLEU Ideally, one would like to re-estimate translation models directly from the n-best BLEU translations. However there are two problems in calculating BLEU for individual sentence: First, as discussed in (Chiang et al., 2008), BLEU is not designed to be used for sentences in isolation where it can exhibit rather volatile behavior. Hence, following their work and (Watanabe et al., 2007), we calculate BLEU for a sentence in the context of a exponentially-weighted moving average of previous translations. We briefly discuss the computation from (Chiang et al., 2008) as follows: Given a source sentence f, and its reference translation r, for an n-best translation e∗ , let c(e) be defined as the vector of target length |e|, source length |f|, reference length |r|, and the number of n-gram matches between e and r, then t"
P16-2007,W07-0414,0,0.0293512,"r|, and the number of n-gram matches between e and r, then two pseudo document parameters O and Of are defined as: Model Re-estimation O ← 0.9 · (O + c(e∗ )), Of ← 0.9 · (Of + |f |) (1) The idea of our approach is to re-estimate the models with n-best oracle-BLEU translations and sentence alignments resulting from decoding the source sentence. Given a source and its reference translation, the oracle-BLEU translation is defined as the translation output with highest BLEU score. Oracle BLEU translations have been previously used for different analytical purposes in SMT (Srivastava et al., 2011; Dreyer et al., 2007; Wisniewski et al., 2010). Figure 1 shows example of word alignment obtained from EM training, segmentations and alignment obtained from forced decoding and oracleO is an exponentially-weighted moving average of the vectors from previous sentences and Of is the correction of source length with respect to the previous sentences. Then the BLEU score for a sentence pairs (f,r) and translation e∗ is defined as: B(e; f, r) = (Of + |f |) · BLEU (O + c(e∗ ; r)) (2) The second problem as discussed in Chiang et al. (2008) is that due to noise in the training data, a high-BLEU translation may contain c"
P16-2007,W11-2124,0,0.0245589,"compared to the baseline verified that using only the best BLEU translation indeed degrades the performance of the re-estimated models. This finding for the optimal value of µ has also been established in (Chiang et al., 2008) through a series of experiments. 3.2 Avoiding over-fitting Training φinit = φbaseline − φC(e,f )&lt;2 For obtaining the oracle-BLEU translations, we first train the translation models from the bitext using the standard pipeline of word alignment and heuristic extraction. Along with the phrase translation and language models, we also train a bilingual language model (BiLM) (Niehues et al., 2011; Garmash and Monz, 2014), as well as lexicalized (Tillman, 2004) and hierarchical reordering models (Galley and Manning, 2008). We use a BiLM specifically as an instance of a reordering model in order to determine the effect of re-estimating re-ordering decisions from oracleBLEU translations. We use the decoder trained on these models to translate the training bitext. Along with the 1best translation (based on model scores), we also store search graphs or lattices generated during the translations process. Using the target sentences, we convert the translation lattice to an isomorphic oracle-"
P16-2007,W06-1607,0,0.0315803,"dditional monoIntroduction In phrase-based SMT, the phrase pairs in the translation model are traditionally trained by applying a heuristic extraction method (Och and Ney, 2000) which extracts phrase pairs based on consistency of word alignments from a word-aligned bilingual training data. The probabilities of the translation model are then calculated based on the relative frequencies of the extracted phrase pairs. A notable shortcoming of this approach is that the translation model probabilities thus calculated from the training bitext can be unintuitive and unreliable (Marcu and Wong, 2002; Foster et al., 2006) as they reflect only the distribution over the phrase pairs observed in the training data. However, from an SMT perspective it is important that the models reflect probability distributions which are preferred by the decoding process, i.e., phrase translations which are likely to be used frequently to achieve better translations should get higher scores and phrases which are 38 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 38–44, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics “ mose Mousa “ yese L Tauﬃr 40"
P16-2007,P00-1056,0,0.337871,"instead of the frequencies of all possible phrase pairs in the bitext. However, one limitation of forced alignment is that only the phrase translation model can be re-estimated since it is restricted to align the source sentence to the given target reference, thus fixing the choice of reordering decisions. A similar line of work is proposed by Lambert et al. (2011) and Schwenk et al. (2011) who use a self-enhancing strategy to utilize additional monoIntroduction In phrase-based SMT, the phrase pairs in the translation model are traditionally trained by applying a heuristic extraction method (Och and Ney, 2000) which extracts phrase pairs based on consistency of word alignments from a word-aligned bilingual training data. The probabilities of the translation model are then calculated based on the relative frequencies of the extracted phrase pairs. A notable shortcoming of this approach is that the translation model probabilities thus calculated from the training bitext can be unintuitive and unreliable (Marcu and Wong, 2002; Foster et al., 2006) as they reflect only the distribution over the phrase pairs observed in the training data. However, from an SMT perspective it is important that the models"
P16-2007,D08-1089,0,0.0372364,"mated models. This finding for the optimal value of µ has also been established in (Chiang et al., 2008) through a series of experiments. 3.2 Avoiding over-fitting Training φinit = φbaseline − φC(e,f )&lt;2 For obtaining the oracle-BLEU translations, we first train the translation models from the bitext using the standard pipeline of word alignment and heuristic extraction. Along with the phrase translation and language models, we also train a bilingual language model (BiLM) (Niehues et al., 2011; Garmash and Monz, 2014), as well as lexicalized (Tillman, 2004) and hierarchical reordering models (Galley and Manning, 2008). We use a BiLM specifically as an instance of a reordering model in order to determine the effect of re-estimating re-ordering decisions from oracleBLEU translations. We use the decoder trained on these models to translate the training bitext. Along with the 1best translation (based on model scores), we also store search graphs or lattices generated during the translations process. Using the target sentences, we convert the translation lattice to an isomorphic oracle-BLEU lattice which has the same set of nodes but the edges represent BLEU score differences corresponding to each transition. F"
P16-2007,J03-1002,0,0.0209429,"t low frequency. 4 Experimental set up Our experiments are carried out for an ArabicEnglish parallel corpus of approximately 1 million sentence pairs. We establish a baseline system by training models on this bitext and then compare this to a forced decoding implementation and to oracle-BLEU re-estimation using the same bitext. 4.1 Baseline and forced decoding The initial training corpus we use is a collection of parallel sentences taken from OpenMT data sources released by the LDC. Phrase table, distortion models and the lexical BiLM are trained with initial alignments obtained using GIZA++ (Och and Ney, 2003). The English 5-gram target language model is trained with Kneser-Ney smoothing on news data of nearly 1.6B tokens. We use an in-house phrase-based SMT system similar to Moses. For all settings in this paper, weights were optimized on NIST’s MT04 data set using pairwise ranked optimization (Hopkins and May, 2011). For forced alignment we use the existing implementation within the Moses SMT toolkit (Koehn 40 Baseline PTre PTin BiLMre + PTin 50.1 n=1 n=10 n=100 50.1(0.0) 50.1(0.0) 50.0(-0.1) 50.7N (+0.6) 50.5N (+0.4) 50.0(-0.1) 50.9N (+0.8) 50.5N (+0.4) 49.6(-0.5) els and settings and provide a"
P16-2007,P02-1040,0,0.095219,"tracted and the models are reestimated by re-calculating the translation probabilities. Hierarchical and lexicalized re-ordering models as well as the BiLM are re-trained using the source sentences, oracle-BLEU translations and word alignments. For testing the performance of the re-estimated models, we tune different systems while replacing the baseline models with the corresponding re-estimated models. We also experiment with the interpolation of re-estimated models with the respective baseline models. We evaluate against 4 test sets: MT05, MT06, MT08, and MT09. Case-insensitive 4-gram BLEU (Papineni et al., 2002) is used as evaluation metric. Approximate randomization (Noreen., 1989; Riezler and Maxwell, 2005) is used to detect statistically significant differences. 5 An important novelty of oracle-BLEU reestimation is that it also allows for re-training of other models alongside the phrase table. Here we provide the results for the re-estimation of a BiLM. For all test sets, BiLM re-estimation provides additional improvements over simple phrase table interpolation, demonstrating that reestimation of re-ordering models can further improve translation performance. The last row of Table 2 shows that the"
P16-2007,D14-1176,1,0.883391,"Missing"
P16-2007,W05-0908,0,0.0494726,"hical and lexicalized re-ordering models as well as the BiLM are re-trained using the source sentences, oracle-BLEU translations and word alignments. For testing the performance of the re-estimated models, we tune different systems while replacing the baseline models with the corresponding re-estimated models. We also experiment with the interpolation of re-estimated models with the respective baseline models. We evaluate against 4 test sets: MT05, MT06, MT08, and MT09. Case-insensitive 4-gram BLEU (Papineni et al., 2002) is used as evaluation metric. Approximate randomization (Noreen., 1989; Riezler and Maxwell, 2005) is used to detect statistically significant differences. 5 An important novelty of oracle-BLEU reestimation is that it also allows for re-training of other models alongside the phrase table. Here we provide the results for the re-estimation of a BiLM. For all test sets, BiLM re-estimation provides additional improvements over simple phrase table interpolation, demonstrating that reestimation of re-ordering models can further improve translation performance. The last row of Table 2 shows that the re-estimated BiLM on its own adds BLEU improvement of up to +0.5 (for MT09). The highest BLEU impr"
P16-2007,D11-1125,0,0.0206713,"ng the same bitext. 4.1 Baseline and forced decoding The initial training corpus we use is a collection of parallel sentences taken from OpenMT data sources released by the LDC. Phrase table, distortion models and the lexical BiLM are trained with initial alignments obtained using GIZA++ (Och and Ney, 2003). The English 5-gram target language model is trained with Kneser-Ney smoothing on news data of nearly 1.6B tokens. We use an in-house phrase-based SMT system similar to Moses. For all settings in this paper, weights were optimized on NIST’s MT04 data set using pairwise ranked optimization (Hopkins and May, 2011). For forced alignment we use the existing implementation within the Moses SMT toolkit (Koehn 40 Baseline PTre PTin BiLMre + PTin 50.1 n=1 n=10 n=100 50.1(0.0) 50.1(0.0) 50.0(-0.1) 50.7N (+0.6) 50.5N (+0.4) 50.0(-0.1) 50.9N (+0.8) 50.5N (+0.4) 49.6(-0.5) els and settings and provide a comparison with the baseline (heuristic training) and forced alignment. Re-estimated models with three different values of n ∈ {1, 10, 100} were evaluated under three settings: phrase table re-estimation, interpolation, and BiLM re-estimation. The best improvements over the baseline are obtained by using only 1-b"
P16-2007,W11-2158,0,0.0192278,"e-estimated using the phrase pair segmentations obtained from forced decoding. Thus forced alignment is a reestimation technique where translation probabilities are calculated based on their frequency in best-scoring hypotheses instead of the frequencies of all possible phrase pairs in the bitext. However, one limitation of forced alignment is that only the phrase translation model can be re-estimated since it is restricted to align the source sentence to the given target reference, thus fixing the choice of reordering decisions. A similar line of work is proposed by Lambert et al. (2011) and Schwenk et al. (2011) who use a self-enhancing strategy to utilize additional monoIntroduction In phrase-based SMT, the phrase pairs in the translation model are traditionally trained by applying a heuristic extraction method (Och and Ney, 2000) which extracts phrase pairs based on consistency of word alignments from a word-aligned bilingual training data. The probabilities of the translation model are then calculated based on the relative frequencies of the extracted phrase pairs. A notable shortcoming of this approach is that the translation model probabilities thus calculated from the training bitext can be uni"
P16-2007,2011.eamt-1.24,0,0.0176347,"h |f|, reference length |r|, and the number of n-gram matches between e and r, then two pseudo document parameters O and Of are defined as: Model Re-estimation O ← 0.9 · (O + c(e∗ )), Of ← 0.9 · (Of + |f |) (1) The idea of our approach is to re-estimate the models with n-best oracle-BLEU translations and sentence alignments resulting from decoding the source sentence. Given a source and its reference translation, the oracle-BLEU translation is defined as the translation output with highest BLEU score. Oracle BLEU translations have been previously used for different analytical purposes in SMT (Srivastava et al., 2011; Dreyer et al., 2007; Wisniewski et al., 2010). Figure 1 shows example of word alignment obtained from EM training, segmentations and alignment obtained from forced decoding and oracleO is an exponentially-weighted moving average of the vectors from previous sentences and Of is the correction of source length with respect to the previous sentences. Then the BLEU score for a sentence pairs (f,r) and translation e∗ is defined as: B(e; f, r) = (Of + |f |) · BLEU (O + c(e∗ ; r)) (2) The second problem as discussed in Chiang et al. (2008) is that due to noise in the training data, a high-BLEU tran"
P16-2007,W11-2132,0,0.0217168,"end, the phrase table is re-estimated using the phrase pair segmentations obtained from forced decoding. Thus forced alignment is a reestimation technique where translation probabilities are calculated based on their frequency in best-scoring hypotheses instead of the frequencies of all possible phrase pairs in the bitext. However, one limitation of forced alignment is that only the phrase translation model can be re-estimated since it is restricted to align the source sentence to the given target reference, thus fixing the choice of reordering decisions. A similar line of work is proposed by Lambert et al. (2011) and Schwenk et al. (2011) who use a self-enhancing strategy to utilize additional monoIntroduction In phrase-based SMT, the phrase pairs in the translation model are traditionally trained by applying a heuristic extraction method (Och and Ney, 2000) which extracts phrase pairs based on consistency of word alignments from a word-aligned bilingual training data. The probabilities of the translation model are then calculated based on the relative frequencies of the extracted phrase pairs. A notable shortcoming of this approach is that the translation model probabilities thus calculated from the"
P16-2007,N04-4026,0,0.0640616,"tion indeed degrades the performance of the re-estimated models. This finding for the optimal value of µ has also been established in (Chiang et al., 2008) through a series of experiments. 3.2 Avoiding over-fitting Training φinit = φbaseline − φC(e,f )&lt;2 For obtaining the oracle-BLEU translations, we first train the translation models from the bitext using the standard pipeline of word alignment and heuristic extraction. Along with the phrase translation and language models, we also train a bilingual language model (BiLM) (Niehues et al., 2011; Garmash and Monz, 2014), as well as lexicalized (Tillman, 2004) and hierarchical reordering models (Galley and Manning, 2008). We use a BiLM specifically as an instance of a reordering model in order to determine the effect of re-estimating re-ordering decisions from oracleBLEU translations. We use the decoder trained on these models to translate the training bitext. Along with the 1best translation (based on model scores), we also store search graphs or lattices generated during the translations process. Using the target sentences, we convert the translation lattice to an isomorphic oracle-BLEU lattice which has the same set of nodes but the edges repres"
P16-2007,P06-1096,0,0.114459,"Missing"
P16-2007,D07-1080,0,0.0370972,"lies between forced decoding (Wuebker et al., 2010) and the bold updating approach of (Liang et al., 2006). However, our approach specifically proposes a novel method for training models using oracle BLEU translations. 3 3.1 Oracle BLEU Ideally, one would like to re-estimate translation models directly from the n-best BLEU translations. However there are two problems in calculating BLEU for individual sentence: First, as discussed in (Chiang et al., 2008), BLEU is not designed to be used for sentences in isolation where it can exhibit rather volatile behavior. Hence, following their work and (Watanabe et al., 2007), we calculate BLEU for a sentence in the context of a exponentially-weighted moving average of previous translations. We briefly discuss the computation from (Chiang et al., 2008) as follows: Given a source sentence f, and its reference translation r, for an n-best translation e∗ , let c(e) be defined as the vector of target length |e|, source length |f|, reference length |r|, and the number of n-gram matches between e and r, then two pseudo document parameters O and Of are defined as: Model Re-estimation O ← 0.9 · (O + c(e∗ )), Of ← 0.9 · (Of + |f |) (1) The idea of our approach is to re-est"
P16-2007,D10-1091,0,0.0193816,"n-gram matches between e and r, then two pseudo document parameters O and Of are defined as: Model Re-estimation O ← 0.9 · (O + c(e∗ )), Of ← 0.9 · (Of + |f |) (1) The idea of our approach is to re-estimate the models with n-best oracle-BLEU translations and sentence alignments resulting from decoding the source sentence. Given a source and its reference translation, the oracle-BLEU translation is defined as the translation output with highest BLEU score. Oracle BLEU translations have been previously used for different analytical purposes in SMT (Srivastava et al., 2011; Dreyer et al., 2007; Wisniewski et al., 2010). Figure 1 shows example of word alignment obtained from EM training, segmentations and alignment obtained from forced decoding and oracleO is an exponentially-weighted moving average of the vectors from previous sentences and Of is the correction of source length with respect to the previous sentences. Then the BLEU score for a sentence pairs (f,r) and translation e∗ is defined as: B(e; f, r) = (Of + |f |) · BLEU (O + c(e∗ ; r)) (2) The second problem as discussed in Chiang et al. (2008) is that due to noise in the training data, a high-BLEU translation may contain certain rules which are unl"
P16-2007,P10-1049,0,0.390573,"ely to be used should get low scores. In addition, the heuristic extraction algorithm generates all possible, consistent phrases including overlapping phrases. This means that translation probabilities are distributed over a very large number of phrase translation candidates most of which never lead to the best possible translation of a sentence. In this paper, we propose a novel solution which is to re-estimate the models from the best BLEU translation of each source sentence in the bitext. An important contribution of our approach is that unlike previous approaches such as forced alignment (Wuebker et al., 2010), reordering and language models can also be re-estimated. We present a novel technique for training translation models for statistical machine translation by aligning source sentences to their oracle-BLEU translations. In contrast to previous approaches which are constrained to phrase training, our method also allows the re-estimation of reordering models along with the translation model. Experiments show an improvement of up to 0.8 BLEU for our approach over a competitive Arabic-English baseline trained directly on the word-aligned bitext using heuristic extraction. As an additional benefit,"
P16-2007,P07-2045,0,\N,Missing
P17-2070,Q15-1016,0,0.0383683,"ze). 3.3 41.1 30.4 41.6 32.8 42.9 33.9 n/a 36.2 36.6 37.8 LS-CIC Dimension 100 300 600 HTLE 40.3N 42.8N 43.4N 36.6N 40.9N 41.3N HTLEadd Exp 39.9N 41.8N 42.2 35.5M 37.9M 38.6 STLE 38.7M 41.0 41.0 36.8N 36.8 37.1 ant with window size c “ 10 and different embedding sizes (100, 300, 600) initialized randomly. We compare our models to several baselines: Skipgram (SGE) and the best-performing multisense embeddings model per word type (MSSG) (Neelakantan et al., 2014). All model variants are trained on the same training data with the same settings, following suggestions by Mikolov et al. (2013a) and Levy et al. (2015). For MSSG we use the best performing similarity measure (avgSimC) as proposed by Neelakantan et al. (2014). 3.2 Dimension Infer. 100 300 600 Sampled (Smp): SimTSE pws , wt q “ ř 1 cosphpwsτ q, opwc qq cosphpwsτ q, hpwtτ qq ` c C ÿ Expected (Exp): SimTSE pws , wt q “ ppτ q ppτ 1 q 1 cosphpwsτ q, hpwtτ qq ` τ,τ 1 ř τ τ,c cosphpws q, opwc qq C 1 ppτ q where hpwsτ q and hpwtτ q are the representations for substitution word s with topic τ and target word t with topic τ 1 respectively (cf. Section 2), wc are context words of wt taken from a sliding window of the same size as the embeddings, opwc q"
P17-2070,D15-1200,0,0.0718063,"Missing"
P17-2070,W16-2506,0,0.0154425,"mer et al., 2014). Unlike previous work (Szarvas et al., 2013; Kremer et al., 2014; Melamud et al., 2015) we do not use any syntactic information, motivated by the fact that high-quality parsers are not available for most languages. The evaluation is performed by computing the Generalized Average Precision (GAP) score (Kishida, 2005). We run HDP on the evaluation set and compute the similarity between target word wt and each substitution ws using two different inference methods in line with how we incorporate topics during training: Context-Aware Word Similarity Task Despite its shortcomings (Faruqui et al., 2016), word similarity remains the most frequently used method of evaluation in the literature. There are multiple test sets available but in almost all of them word pairs are considered out of context. To the best of our knowledge, the only word similarity data set providing word context is SCWS (Huang et al., 2012). To evaluate our models on SCWS, we run HDP on the data treating each word’s context as a separate document. We compute the similarity of each word pair as follows: Simpw1 , w2 q “ cosphpw1 q, hpw2 qq where hpwi q refers to any of the topic-sensitive representations defined in Section"
P17-2070,S07-1009,0,0.0739372,"2.1 HTLEadd Smp 39.4M 41.3N 41.8 30.4 STLE 35.2 36.7 39.0 32.9 32.7 31.5 32.3 33.0 31.7 33.9 Table 3: GAP scores on LS-SE07 and LS-CIC sets. For SGE + C we use the context embeddings to disambiguate the substitutions. Improvements over the best baseline (MSSG) are marked N at p ă .01 and M at p ă .05. ence of many polysemous target words makes this task more suitable for evaluating sense embedding. Following Melamud et al. (2015) we pool substitutions from different instances and rank them by the number of annotators that selected them for a given context. We use two evaluation sets: LS-SE07 (McCarthy and Navigli, 2007), and LS-CIC (Kremer et al., 2014). Unlike previous work (Szarvas et al., 2013; Kremer et al., 2014; Melamud et al., 2015) we do not use any syntactic information, motivated by the fact that high-quality parsers are not available for most languages. The evaluation is performed by computing the Generalized Average Precision (GAP) score (Kishida, 2005). We run HDP on the evaluation set and compute the similarity between target word wt and each substitution ws using two different inference methods in line with how we incorporate topics during training: Context-Aware Word Similarity Task Despite i"
P17-2070,W16-1817,0,0.0223469,"in representations that are able to distinguish between different meanings of a given word. Our models yield statistically significant improvements for the lexical substitution task indicating that commonly used single word representations, even when combined with contextual information, are insufficient for this task. 1 Introduction Word representations in the form of dense vectors, or word embeddings, capture semantic and syntactic information (Mikolov et al., 2013a; Pennington et al., 2014) and are widely used in many NLP tasks (Zou et al., 2013; Levy and Goldberg, 2014; Tang et al., 2014; Gharbieh et al., 2016). Most of the existing models generate one representation per word and do not distinguish between different meanings of a word. However, many tasks can benefit from using multiple representations per word to capture polysemy (Reisinger and Mooney, 2010). There have been several attempts to build repositories for word senses (Miller, 1995; Navigli and Ponzetto, 2010), but this is laborious and limited to few languages. Moreover, defining a universal set of word senses is challenging as polysemous words can exist at many levels of 2 Topic-Sensitive Representations In this section we describe the"
P17-2070,W15-1501,0,0.0683766,".58 0.55 Model SGE SGE + C MSSG Table 2: Spearman’s rank correlation performance for the Word Similarity task on SCWS. 40.5 40.9 41.1 32.1 36.1 37.8 32.3 36.8 39.1 HTLE 39.8N 42.5N 43.0N 32.1 HTLEadd Smp 39.4M 41.3N 41.8 30.4 STLE 35.2 36.7 39.0 32.9 32.7 31.5 32.3 33.0 31.7 33.9 Table 3: GAP scores on LS-SE07 and LS-CIC sets. For SGE + C we use the context embeddings to disambiguate the substitutions. Improvements over the best baseline (MSSG) are marked N at p ă .01 and M at p ă .05. ence of many polysemous target words makes this task more suitable for evaluating sense embedding. Following Melamud et al. (2015) we pool substitutions from different instances and rank them by the number of annotators that selected them for a given context. We use two evaluation sets: LS-SE07 (McCarthy and Navigli, 2007), and LS-CIC (Kremer et al., 2014). Unlike previous work (Szarvas et al., 2013; Kremer et al., 2014; Melamud et al., 2015) we do not use any syntactic information, motivated by the fact that high-quality parsers are not available for most languages. The evaluation is performed by computing the Generalized Average Precision (GAP) score (Kishida, 2005). We run HDP on the evaluation set and compute the sim"
P17-2070,P12-1092,0,0.185994,"core (Kishida, 2005). We run HDP on the evaluation set and compute the similarity between target word wt and each substitution ws using two different inference methods in line with how we incorporate topics during training: Context-Aware Word Similarity Task Despite its shortcomings (Faruqui et al., 2016), word similarity remains the most frequently used method of evaluation in the literature. There are multiple test sets available but in almost all of them word pairs are considered out of context. To the best of our knowledge, the only word similarity data set providing word context is SCWS (Huang et al., 2012). To evaluate our models on SCWS, we run HDP on the data treating each word’s context as a separate document. We compute the similarity of each word pair as follows: Simpw1 , w2 q “ cosphpw1 q, hpw2 qq where hpwi q refers to any of the topic-sensitive representations defined in Section 2. Note that w1 and w2 can refer to the same word. Table 2 provides the Spearman’s correlation scores for different models against the human ranking. We see that with dimensions 100 and 300, two of our models obtain improvements over the baseline. The MSSG model of Neelakantan et al. (2014) performs only slightl"
P17-2070,E14-1057,0,0.282865,"Missing"
P17-2070,P14-1025,0,0.0273283,"distinguish between different meanings of a word. However, many tasks can benefit from using multiple representations per word to capture polysemy (Reisinger and Mooney, 2010). There have been several attempts to build repositories for word senses (Miller, 1995; Navigli and Ponzetto, 2010), but this is laborious and limited to few languages. Moreover, defining a universal set of word senses is challenging as polysemous words can exist at many levels of 2 Topic-Sensitive Representations In this section we describe the proposed models. To learn topics from a corpus we use HDP (Teh et al., 2006; Lau et al., 2014). The main advantage of this model compared to non-hierarchical methods like the Chinese Restaurant Process (CRP) is that each document in the corpus is modeled using a mixture model with topics shared between all documents (Teh et al., 2005; Brody and Lapata, 2009). HDP yields two sets of distributions that we use in our methods: distributions over topics for words in the vocabulary, and distributions over topics for documents in the corpus. Similarly to Neelakantan et al. (2014), we use neighboring words to detect the meaning of the context, however, we also use the two HDP dis441 Proceeding"
P17-2070,P14-2050,0,0.0565111,"opic distributions for each document we obtain representations that are able to distinguish between different meanings of a given word. Our models yield statistically significant improvements for the lexical substitution task indicating that commonly used single word representations, even when combined with contextual information, are insufficient for this task. 1 Introduction Word representations in the form of dense vectors, or word embeddings, capture semantic and syntactic information (Mikolov et al., 2013a; Pennington et al., 2014) and are widely used in many NLP tasks (Zou et al., 2013; Levy and Goldberg, 2014; Tang et al., 2014; Gharbieh et al., 2016). Most of the existing models generate one representation per word and do not distinguish between different meanings of a word. However, many tasks can benefit from using multiple representations per word to capture polysemy (Reisinger and Mooney, 2010). There have been several attempts to build repositories for word senses (Miller, 1995; Navigli and Ponzetto, 2010), but this is laborious and limited to few languages. Moreover, defining a universal set of word senses is challenging as polysemous words can exist at many levels of 2 Topic-Sensitive Repr"
P17-2070,P10-1023,0,0.0142999,"form of dense vectors, or word embeddings, capture semantic and syntactic information (Mikolov et al., 2013a; Pennington et al., 2014) and are widely used in many NLP tasks (Zou et al., 2013; Levy and Goldberg, 2014; Tang et al., 2014; Gharbieh et al., 2016). Most of the existing models generate one representation per word and do not distinguish between different meanings of a word. However, many tasks can benefit from using multiple representations per word to capture polysemy (Reisinger and Mooney, 2010). There have been several attempts to build repositories for word senses (Miller, 1995; Navigli and Ponzetto, 2010), but this is laborious and limited to few languages. Moreover, defining a universal set of word senses is challenging as polysemous words can exist at many levels of 2 Topic-Sensitive Representations In this section we describe the proposed models. To learn topics from a corpus we use HDP (Teh et al., 2006; Lau et al., 2014). The main advantage of this model compared to non-hierarchical methods like the Chinese Restaurant Process (CRP) is that each document in the corpus is modeled using a mixture model with topics shared between all documents (Teh et al., 2005; Brody and Lapata, 2009). HDP y"
P17-2070,D14-1113,0,0.553306,"presentations In this section we describe the proposed models. To learn topics from a corpus we use HDP (Teh et al., 2006; Lau et al., 2014). The main advantage of this model compared to non-hierarchical methods like the Chinese Restaurant Process (CRP) is that each document in the corpus is modeled using a mixture model with topics shared between all documents (Teh et al., 2005; Brody and Lapata, 2009). HDP yields two sets of distributions that we use in our methods: distributions over topics for words in the vocabulary, and distributions over topics for documents in the corpus. Similarly to Neelakantan et al. (2014), we use neighboring words to detect the meaning of the context, however, we also use the two HDP dis441 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 441–447 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2070 hHTLE owi+j owi+j … … … hHTLEadd owi+j … … hSTLE … … … … r(wi⌧ ) r0 (wi⌧ ) r0 (wi ) r00 (wi⌧ ) (a) p(⌧ |di ) … … 1 (b) … … 2 r00 (wi⌧ ) k r00 (wi⌧ ) (c) Figure 1: Illustration of our topic-sensitive representation models: (a) hard-topic labeled r"
P17-2070,D14-1162,0,0.111106,"erarchical Dirichlet Process. We observe that by modeling topics and integrating topic distributions for each document we obtain representations that are able to distinguish between different meanings of a given word. Our models yield statistically significant improvements for the lexical substitution task indicating that commonly used single word representations, even when combined with contextual information, are insufficient for this task. 1 Introduction Word representations in the form of dense vectors, or word embeddings, capture semantic and syntactic information (Mikolov et al., 2013a; Pennington et al., 2014) and are widely used in many NLP tasks (Zou et al., 2013; Levy and Goldberg, 2014; Tang et al., 2014; Gharbieh et al., 2016). Most of the existing models generate one representation per word and do not distinguish between different meanings of a word. However, many tasks can benefit from using multiple representations per word to capture polysemy (Reisinger and Mooney, 2010). There have been several attempts to build repositories for word senses (Miller, 1995; Navigli and Ponzetto, 2010), but this is laborious and limited to few languages. Moreover, defining a universal set of word senses is c"
P17-2070,D13-1141,0,0.033758,"and integrating topic distributions for each document we obtain representations that are able to distinguish between different meanings of a given word. Our models yield statistically significant improvements for the lexical substitution task indicating that commonly used single word representations, even when combined with contextual information, are insufficient for this task. 1 Introduction Word representations in the form of dense vectors, or word embeddings, capture semantic and syntactic information (Mikolov et al., 2013a; Pennington et al., 2014) and are widely used in many NLP tasks (Zou et al., 2013; Levy and Goldberg, 2014; Tang et al., 2014; Gharbieh et al., 2016). Most of the existing models generate one representation per word and do not distinguish between different meanings of a word. However, many tasks can benefit from using multiple representations per word to capture polysemy (Reisinger and Mooney, 2010). There have been several attempts to build repositories for word senses (Miller, 1995; Navigli and Ponzetto, 2010), but this is laborious and limited to few languages. Moreover, defining a universal set of word senses is challenging as polysemous words can exist at many levels"
P17-2070,D16-1018,0,0.0147706,"are consistent with how we train HTLE and STLE. The sampled method, similar to HTLE, uses the HDP model to assign topics to word occurrences during testing. The expected method, similar to STLE, uses the HDP model to learn the probability distribution of topics of the context sentence and uses the entire distribution to compute the similarity. For the Skipgram baseline we compute the similarity SimSGE+C pws , wt q as follows: cosphpws q, hpwt qq ` ř c 2013a; Pennington et al., 2014), recent studies have focused on learning multiple embeddings per word due to the ambiguous nature of language (Qiu et al., 2016). Huang et al. (2012) cluster word contexts and use the average embedding of each cluster as word sense embeddings, which yields improvements on a word similarity task. Neelakantan et al. (2014) propose two approaches, both based on clustering word contexts: In the first, they fix the number of senses manually, and in the second, they use an ad-hoc greedy procedure that allocates a new representation to a word if existing representations explain the context below a certain threshold. Li and Jurafsky (2015) used a CRP model to distinguish between senses of words and train vectors for senses, wh"
P17-2070,N10-1013,0,0.0181907,"en combined with contextual information, are insufficient for this task. 1 Introduction Word representations in the form of dense vectors, or word embeddings, capture semantic and syntactic information (Mikolov et al., 2013a; Pennington et al., 2014) and are widely used in many NLP tasks (Zou et al., 2013; Levy and Goldberg, 2014; Tang et al., 2014; Gharbieh et al., 2016). Most of the existing models generate one representation per word and do not distinguish between different meanings of a word. However, many tasks can benefit from using multiple representations per word to capture polysemy (Reisinger and Mooney, 2010). There have been several attempts to build repositories for word senses (Miller, 1995; Navigli and Ponzetto, 2010), but this is laborious and limited to few languages. Moreover, defining a universal set of word senses is challenging as polysemous words can exist at many levels of 2 Topic-Sensitive Representations In this section we describe the proposed models. To learn topics from a corpus we use HDP (Teh et al., 2006; Lau et al., 2014). The main advantage of this model compared to non-hierarchical methods like the Chinese Restaurant Process (CRP) is that each document in the corpus is model"
P17-2070,D13-1198,0,0.0502976,"Missing"
P17-2070,P14-1146,0,0.0370005,"ch document we obtain representations that are able to distinguish between different meanings of a given word. Our models yield statistically significant improvements for the lexical substitution task indicating that commonly used single word representations, even when combined with contextual information, are insufficient for this task. 1 Introduction Word representations in the form of dense vectors, or word embeddings, capture semantic and syntactic information (Mikolov et al., 2013a; Pennington et al., 2014) and are widely used in many NLP tasks (Zou et al., 2013; Levy and Goldberg, 2014; Tang et al., 2014; Gharbieh et al., 2016). Most of the existing models generate one representation per word and do not distinguish between different meanings of a word. However, many tasks can benefit from using multiple representations per word to capture polysemy (Reisinger and Mooney, 2010). There have been several attempts to build repositories for word senses (Miller, 1995; Navigli and Ponzetto, 2010), but this is laborious and limited to few languages. Moreover, defining a universal set of word senses is challenging as polysemous words can exist at many levels of 2 Topic-Sensitive Representations In this"
P17-2070,W11-1102,0,\N,Missing
P17-2090,W16-2301,1,0.695868,"Missing"
P17-2090,D15-1166,0,0.045947,"Missing"
P17-2090,D09-1040,0,0.0212379,"e meaning of the sentence, requiring sentential paraphrasing systems which are not available for many language pairs. Instead, we propose a weaker notion of label preservation that allows to alter both source and target sentences at the same time as long as they remain translations of each other. While our approach allows us to augment data in numerous ways, we focus on augmenting instances involving low-frequency words, because the parameter estimation of rare words is challenging, and further exacerbated in a lowresource setting. We simulate a low-resource setting as done in the literature (Marton et al., 2009; Duong et al., 2015) and obtain substantial improvements for translating EnglishÑGerman and GermanÑEnglish. 2 Implausible substitutions need to be ruled out during data augmentation. To this end, rather than relying on linguistic resources which are not available for many languages, we rely on LSTM language models (LM) (Hochreiter and Schmidhuber, 1997; Jozefowicz et al., 2015) trained on large amounts of monolingual data in both forward and backward directions. Our data augmentation method involves the following steps: Targeted words selection: Following common practice, our NMT system limit"
P17-2090,P02-1040,0,0.115466,"Missing"
P17-2090,W14-4012,0,0.197783,"Missing"
P17-2090,D15-1040,0,0.012259,"ence, requiring sentential paraphrasing systems which are not available for many language pairs. Instead, we propose a weaker notion of label preservation that allows to alter both source and target sentences at the same time as long as they remain translations of each other. While our approach allows us to augment data in numerous ways, we focus on augmenting instances involving low-frequency words, because the parameter estimation of rare words is challenging, and further exacerbated in a lowresource setting. We simulate a low-resource setting as done in the literature (Marton et al., 2009; Duong et al., 2015) and obtain substantial improvements for translating EnglishÑGerman and GermanÑEnglish. 2 Implausible substitutions need to be ruled out during data augmentation. To this end, rather than relying on linguistic resources which are not available for many languages, we rely on LSTM language models (LM) (Hochreiter and Schmidhuber, 1997; Jozefowicz et al., 2015) trained on large amounts of monolingual data in both forward and backward directions. Our data augmentation method involves the following steps: Targeted words selection: Following common practice, our NMT system limits its vocabulary V to"
P17-2090,P16-1009,0,0.404694,"rget translation. To train a model with reliable parameter estimations, these networks require numerous instances of sentence translation pairs with words occurring in diverse contexts, which is typically not available in low-resource language pairs. As a result NMT falls short of reaching state-of-the-art performances for these language pairs (Zoph et al., 2016). The solution is to either manually annotate more data or perform unsupervised data augmentation. Since manual annotation of data is timeconsuming, data augmentation for low-resource language pairs is a more viable approach. Recently Sennrich et al. (2016a) proposed a method to back-translate sentences from monolingual data and augment the bitext with the resulting pseudo parallel corpora. In this paper, we propose a simple yet effective approach, translation data augmentation (TDA), that augments the training data by altering existing sentences in the parallel corpus, similar in spirit to the data augmentation approaches in computer vision (see Figure 1). In order for the augmentation process in this scenario to be label-preserving, any change to a sentence in one language must preIntroduction In computer vision, data augmentation techniques"
P17-2090,N13-1073,0,0.0802615,"Missing"
P17-2090,P82-1020,0,0.855271,"Missing"
P17-2090,P16-1162,0,0.124968,"rget translation. To train a model with reliable parameter estimations, these networks require numerous instances of sentence translation pairs with words occurring in diverse contexts, which is typically not available in low-resource language pairs. As a result NMT falls short of reaching state-of-the-art performances for these language pairs (Zoph et al., 2016). The solution is to either manually annotate more data or perform unsupervised data augmentation. Since manual annotation of data is timeconsuming, data augmentation for low-resource language pairs is a more viable approach. Recently Sennrich et al. (2016a) proposed a method to back-translate sentences from monolingual data and augment the bitext with the resulting pseudo parallel corpora. In this paper, we propose a simple yet effective approach, translation data augmentation (TDA), that augments the training data by altering existing sentences in the parallel corpus, similar in spirit to the data augmentation approaches in computer vision (see Figure 1). In order for the augmentation process in this scenario to be label-preserving, any change to a sentence in one language must preIntroduction In computer vision, data augmentation techniques"
P17-2090,D16-1163,0,0.0216969,"ck. Figure 1: Top: flip and crop, two label-preserving data augmentation techniques in computer vision. Bottom: Altering one sentence in a parallel corpus requires changing its translation. LSTM hidden states and an attention mechanism, generates the target translation. To train a model with reliable parameter estimations, these networks require numerous instances of sentence translation pairs with words occurring in diverse contexts, which is typically not available in low-resource language pairs. As a result NMT falls short of reaching state-of-the-art performances for these language pairs (Zoph et al., 2016). The solution is to either manually annotate more data or perform unsupervised data augmentation. Since manual annotation of data is timeconsuming, data augmentation for low-resource language pairs is a more viable approach. Recently Sennrich et al. (2016a) proposed a method to back-translate sentences from monolingual data and augment the bitext with the resulting pseudo parallel corpora. In this paper, we propose a simple yet effective approach, translation data augmentation (TDA), that augments the training data by altering existing sentences in the parallel corpus, similar in spirit to th"
W01-1008,C96-1021,0,0.024932,"In general, this problem does not only hold for pronouns but for all kind of anaphoric expressions such as pronouns, definite noun phrases (e.g., the negotiations) and anaphoric adverbials (e.g., later). To cope with this problem simple segmentation is applied as a pre-processing step where paragraphs that contain pronouns or simple definite noun phrases are attached to the preceding paragraph. A more sophisticated approach to text segmentation is described in (Hearst, 1997). Obviously, it would be better to use an automatic anaphora resolution component to cope with this problem, see, e.g., (Kennedy and Boguraev, 1996; Kameyama, 1997), where anaphoric expressions are replaced by their antecedents, but at the moment, the integration of such a component remains future work. 2.2 Informativity (Radev, 2000) describes 24 cross-document relations that can hold between their segments, one of which is the subsumption (or entailment) relation. In the context of document fusion, we focus on the entailment relation and how it can be formally defined; unfortunately, (Radev, 2000) provides no formal definition for any of the relations. Computing the informativity of a segment compared to another segment is an essential"
W01-1008,P99-1071,0,0.0223079,"differently. For instance, they provide different background information, helping the reader to situate the story, they interview different people to comment on an event, and they provide additional, The aim of this paper is to describe an approach automatizing this process by fusing information stemming from different documents to generate a single comprehensive document, containing the information of all original documents without repeating information which is conveyed by two or more documents. The work described in this paper is closely related to the area of multi-document summarization (Barzilay et al., 1999; Mani and Bloedorn, 1999; McKeown and Radev, 1995; Radev, 2000), where related documents are analyzed to use frequently occurring segments for identifying relevant information that has to be included in the summary. Our work differs from the work on multi-document summarization as we focus on document fusion disregarding summarization. On the contrary, we are not aiming for the shortest description containing the most relevant information, but for the shortest description containing all information. For instance, even historic background information is included, as long as it allows the reade"
W01-1008,C00-1043,0,0.020607,"Missing"
W01-1008,J97-1003,0,0.0236391,"ext and placed in a new context (the fused document), this can lead to dangling pronouns, which cannot be correctly resolved anymore. In general, this problem does not only hold for pronouns but for all kind of anaphoric expressions such as pronouns, definite noun phrases (e.g., the negotiations) and anaphoric adverbials (e.g., later). To cope with this problem simple segmentation is applied as a pre-processing step where paragraphs that contain pronouns or simple definite noun phrases are attached to the preceding paragraph. A more sophisticated approach to text segmentation is described in (Hearst, 1997). Obviously, it would be better to use an automatic anaphora resolution component to cope with this problem, see, e.g., (Kennedy and Boguraev, 1996; Kameyama, 1997), where anaphoric expressions are replaced by their antecedents, but at the moment, the integration of such a component remains future work. 2.2 Informativity (Radev, 2000) describes 24 cross-document relations that can hold between their segments, one of which is the subsumption (or entailment) relation. In the context of document fusion, we focus on the entailment relation and how it can be formally defined; unfortunately, (Radev,"
W01-1008,W97-1307,0,0.048776,"s not only hold for pronouns but for all kind of anaphoric expressions such as pronouns, definite noun phrases (e.g., the negotiations) and anaphoric adverbials (e.g., later). To cope with this problem simple segmentation is applied as a pre-processing step where paragraphs that contain pronouns or simple definite noun phrases are attached to the preceding paragraph. A more sophisticated approach to text segmentation is described in (Hearst, 1997). Obviously, it would be better to use an automatic anaphora resolution component to cope with this problem, see, e.g., (Kennedy and Boguraev, 1996; Kameyama, 1997), where anaphoric expressions are replaced by their antecedents, but at the moment, the integration of such a component remains future work. 2.2 Informativity (Radev, 2000) describes 24 cross-document relations that can hold between their segments, one of which is the subsumption (or entailment) relation. In the context of document fusion, we focus on the entailment relation and how it can be formally defined; unfortunately, (Radev, 2000) provides no formal definition for any of the relations. Computing the informativity of a segment compared to another segment is an essential task during docu"
W01-1008,W00-1009,0,0.118685,"helping the reader to situate the story, they interview different people to comment on an event, and they provide additional, The aim of this paper is to describe an approach automatizing this process by fusing information stemming from different documents to generate a single comprehensive document, containing the information of all original documents without repeating information which is conveyed by two or more documents. The work described in this paper is closely related to the area of multi-document summarization (Barzilay et al., 1999; Mani and Bloedorn, 1999; McKeown and Radev, 1995; Radev, 2000), where related documents are analyzed to use frequently occurring segments for identifying relevant information that has to be included in the summary. Our work differs from the work on multi-document summarization as we focus on document fusion disregarding summarization. On the contrary, we are not aiming for the shortest description containing the most relevant information, but for the shortest description containing all information. For instance, even historic background information is included, as long as it allows the reader to get a more comprehensive description of an event. Although"
W01-1008,stein-etal-2000-evaluating,0,0.0604669,"Missing"
W01-1008,W00-0403,0,\N,Missing
W05-0820,2005.mtsummit-papers.11,1,0.226362,"Missing"
W05-0820,E03-1076,1,0.415417,"02). Finally, we suggested the use of Pharaoh (Koehn, 2004b), a phrase-based machine translation decoder. How well does this setup match the state of the art? The MIT system using the Pharaoh decoder (Koehn, 2004a) proved to be very competitive in last year’s NIST evaluation. However, the field is moving fast, and a number of steps help to improve upon the provided baseline setup, e.g., larger language models trained on general text (up to a billion words have been used), better reodering models (e.g., suggested by Tillman (2004) and Och et al. (2004)), better language-specific preprocessing (Koehn and Knight, 2003) and restructuring (Collins et al., 2005), additional feature functions such as word class language models, and minimum error rate training (Och, 2003) to optimize parameters. Some of these steps (e.g., improved reordering models) go beyond the current capabilities of Pharaoh. However, we are hopeful that freely available software continues to match or at least follow closely the state of the art. We announced the shared task on March 3, and provided all the resources mentioned above (also a development test corpus to track the quality of systems being developed). The test schedule called for"
W05-0820,N03-1017,1,0.0943469,"e provided not only training data from the Europarl corpus (Koehn, 2005), but also additional resources: sentence and word alignments, the decoder Pharaoh1 (Koehn, 2004b), and a language model, so that participation was feasible even as a graduate level class project. Using about 15 million words of translated text, participants were asked to build a phrase-based statistical machine translation system. The focus of the task was to build a probabilistic phrase translation table, since most of the other resources were provided — for more on phrase-based statistical machine translation, refer to Koehn et al. (2003). The participants’ systems were compared by how well they translated 2000 previously unseen test sentences from the same domain. The shared task operated within an extremely short timeframe. The workshop and hence the shared task was accepted on February 22, 2005 and announced on March 3. The official test data was made available on April 3, results were due one week later. Despite this tight schedule, eleven research groups participated and built a total of 32 machine translation systems for the four language pairs. 1 Goals When setting up this competition, we were motivated by a number of g"
W05-0820,P03-1021,0,0.0562852,"IT system using the Pharaoh decoder (Koehn, 2004a) proved to be very competitive in last year’s NIST evaluation. However, the field is moving fast, and a number of steps help to improve upon the provided baseline setup, e.g., larger language models trained on general text (up to a billion words have been used), better reodering models (e.g., suggested by Tillman (2004) and Och et al. (2004)), better language-specific preprocessing (Koehn and Knight, 2003) and restructuring (Collins et al., 2005), additional feature functions such as word class language models, and minimum error rate training (Och, 2003) to optimize parameters. Some of these steps (e.g., improved reordering models) go beyond the current capabilities of Pharaoh. However, we are hopeful that freely available software continues to match or at least follow closely the state of the art. We announced the shared task on March 3, and provided all the resources mentioned above (also a development test corpus to track the quality of systems being developed). The test schedule called for the translation of 2000 sentence for each of the four language pairs in the week between April 3–10. We allowed late submissions up to April 17. 121 3"
W05-0820,N04-1021,0,0.0329865,"l corpus using the SRI language modeling toolkit (Stolke, 2002). Finally, we suggested the use of Pharaoh (Koehn, 2004b), a phrase-based machine translation decoder. How well does this setup match the state of the art? The MIT system using the Pharaoh decoder (Koehn, 2004a) proved to be very competitive in last year’s NIST evaluation. However, the field is moving fast, and a number of steps help to improve upon the provided baseline setup, e.g., larger language models trained on general text (up to a billion words have been used), better reodering models (e.g., suggested by Tillman (2004) and Och et al. (2004)), better language-specific preprocessing (Koehn and Knight, 2003) and restructuring (Collins et al., 2005), additional feature functions such as word class language models, and minimum error rate training (Och, 2003) to optimize parameters. Some of these steps (e.g., improved reordering models) go beyond the current capabilities of Pharaoh. However, we are hopeful that freely available software continues to match or at least follow closely the state of the art. We announced the shared task on March 3, and provided all the resources mentioned above (also a development test corpus to track the"
W05-0820,J03-1002,0,0.0939299,"research: One of our main goals with providing as many resources as possible was to keep the barrier of entry low. Participants could use the word alignment and other resources and focus on phrase extraction. We hoped to attract researchers that are relatively new to the field. We were satisfied to learn that many entries are by graduate students working on their own. Promote and create free resources: Academic research thrives on freely available resources. The field of statistical machine translation has been blessed with a long tradition of freely available software tools — such as GIZA++ (Och and Ney, 2003) — and parallel corpora — such as the Canadian Hansards2 . Following this lead, we made word alignments and a language model available for this competition in addition to our previously published resources (Europarl and Pharaoh). The competition created resources as well. Most teams agreed to share system output and their model files. You can download them from the competition web site3 . Promote work on European language pairs: Finally, we wanted to promote work on European languages. The increasing economic and political ties within the European Union create a huge need for translation servi"
W05-0820,P02-1040,0,0.111328,"developed). The test schedule called for the translation of 2000 sentence for each of the four language pairs in the week between April 3–10. We allowed late submissions up to April 17. 121 3 Results Eleven teams from eight institutions in Europe and North America participated, see Figure 2 for a complete list. The figure also indicates, if a team used the Pharaoh decoder (eight teams), the provided language model (seven teams) and the provided word alignment (four did, three of those with additional preprocessing or additional data). Translation performance was measured using the BLEU score (Papineni et al., 2002), which measures n-gram overlap with a reference translation. In our case, we only used a single reference translation, since the test set was taken from a held-out portion of the Europarl corpus. On the other hand we used a relatively large number of test sentences to guarantee that the BLEU results are stable despite the fact that we used only one reference translation for each sentence. Shared tasks like this one, of course, bring out the competitive spirit of participants and can draw criticisms about being a horse race. From an outside perspective, however, it is far more interesting to l"
W05-0820,N04-4026,0,0.0204291,"task of the Europarl corpus using the SRI language modeling toolkit (Stolke, 2002). Finally, we suggested the use of Pharaoh (Koehn, 2004b), a phrase-based machine translation decoder. How well does this setup match the state of the art? The MIT system using the Pharaoh decoder (Koehn, 2004a) proved to be very competitive in last year’s NIST evaluation. However, the field is moving fast, and a number of steps help to improve upon the provided baseline setup, e.g., larger language models trained on general text (up to a billion words have been used), better reodering models (e.g., suggested by Tillman (2004) and Och et al. (2004)), better language-specific preprocessing (Koehn and Knight, 2003) and restructuring (Collins et al., 2005), additional feature functions such as word class language models, and minimum error rate training (Och, 2003) to optimize parameters. Some of these steps (e.g., improved reordering models) go beyond the current capabilities of Pharaoh. However, we are hopeful that freely available software continues to match or at least follow closely the state of the art. We announced the shared task on March 3, and provided all the resources mentioned above (also a development tes"
W05-0820,koen-2004-pharaoh,0,\N,Missing
W05-0820,P05-1066,1,\N,Missing
W05-0901,J96-2004,0,0.0669426,"rd. Unfortunately, a consistent gold standard has not yet been reported. For example, in two previous studies (Mani, 2001; Tombros and Sanderson, 1998), users’ judgments were compared to “gold standard judgments” produced by members of the University of Pennsylvania’s Linguistic Data Consortium. Although these judgments were supposed to represent the correct relevance judgments for each of the documents associated with an event, both studies reported that annotators’ judgments varied greatly and that this was a significant issue for the evaluations. In the SUMMAC experiments, the Kappa score (Carletta, 1996; Eugenio and Glass, 2004) for interannotator agreement was reported to be 0.38 (Mani et al., 2002). In fact, large variations have been found in the initial summary scoring of an individual participant and a subsequent scoring that occurs a few weeks later (Mani, 2001; van Halteren and Teufel, 2003). This paper attempts to overcome the problem of interannotator inconsistency by measuring summary effectiveness in an extrinsic task using a much more consistent form of user judgment instead of a gold standard. Using Relevance-Prediction increases the confidence in our results and strengthens the"
W05-0901,J04-1005,0,0.0262553,"Missing"
W05-0901,N03-1020,0,0.0884371,"-based usefulness. Section 3 presents a novel extrinsic measure called Relevance-Prediction. Section 4 demonstrates that this is a more reliable measure than that of previous gold standard methods, e.g., the LDC-Agreement method used for SUMMAC-style evaluations, and that this reliability allows us to make stronger statistical statements about the benefits of summarization. We expect these findings to be important for future summarization evaluations. Section 5 presents the results of correlation between task usefulness and the Recall Oriented Understudy for Gisting Evaluation (ROUGE) metric (Lin and Hovy, 2003).1 While we show that ROUGE correlates with task usefulness (using our Relevance-Prediction measure), we detect a slight difference between informative, extractive headlines (containing words from the full document) and less informative, non-extractive “eye-catchers” (containing words that might not appear in the full document, and intended to entice a reader to read the entire document). Section 6 further highlights the importance of this point and discusses the implications for automatic evaluation of non-extractive summaries. To evaluate nonextractive summaries reliably, an automatic measur"
W05-0901,C04-1072,0,0.0210096,"t whether ROUGE correlates more highly with Relevance-Prediction than with LDC-Agreement, we calculated the correlation for the results of both techniques using Pearson’s r (Siegel and Castellan, 1988): Pn (ri − r¯)(si − s¯) pPn i=1 pPn (r − r¯)2 ¯)2 i=1 i i=1 (si − s where ri is the ROUGE score of surrogate i, r¯ is the average ROUGE score of all data points, si is the agreement score of summary i (using Relevance-Prediction or LDC-Agreement), and s¯ is the average agreement score. Pearson’s statistics is commonly used in summarization and machine translation evaluation, see e.g. (Lin, 2004; Lin and Och, 2004). As one might expect, there is some variability in the correlation between ROUGE and human judgments for 9 We also computed ROUGE 2-gram, ROUGE L and ROUGE W, but the trend for these did not differ from ROUGE1. 6 Figure 1: Distribution of the Correlation Variation for Relevance-Prediction on HEAD and HUM the different partitions. However, the boxplots for both HEAD and HUM indicate that the first and third quartile were relatively close to the median (see Figure 1). Table 5 shows the Pearson Correlations with ROUGE1 using Relevance-Prediction and LDC-Agreement. For Relevance-Prediction, we ob"
W05-0901,W04-1013,0,0.0100221,"tion To test whether ROUGE correlates more highly with Relevance-Prediction than with LDC-Agreement, we calculated the correlation for the results of both techniques using Pearson’s r (Siegel and Castellan, 1988): Pn (ri − r¯)(si − s¯) pPn i=1 pPn (r − r¯)2 ¯)2 i=1 i i=1 (si − s where ri is the ROUGE score of surrogate i, r¯ is the average ROUGE score of all data points, si is the agreement score of summary i (using Relevance-Prediction or LDC-Agreement), and s¯ is the average agreement score. Pearson’s statistics is commonly used in summarization and machine translation evaluation, see e.g. (Lin, 2004; Lin and Och, 2004). As one might expect, there is some variability in the correlation between ROUGE and human judgments for 9 We also computed ROUGE 2-gram, ROUGE L and ROUGE W, but the trend for these did not differ from ROUGE1. 6 Figure 1: Distribution of the Correlation Variation for Relevance-Prediction on HEAD and HUM the different partitions. However, the boxplots for both HEAD and HUM indicate that the first and third quartile were relatively close to the median (see Figure 1). Table 5 shows the Pearson Correlations with ROUGE1 using Relevance-Prediction and LDC-Agreement. For Relevan"
W05-0901,N04-1019,0,0.0504141,"her highlights the importance of this point and discusses the implications for automatic evaluation of non-extractive summaries. To evaluate nonextractive summaries reliably, an automatic measure may require knowledge of sophisticated meaning units.2 It is our hope that the conclusions drawn herein will prompt investigation into more sophisticated automatic metrics as researchers shift their focus to non-extractive summaries. 1 ROUGE has been previously used as the primary automatic evaluation metric by NIST in the 2003 and 2004 DUC Evaluations. 2 The content units proposed in recent methods (Nenkova and Passonneau, 2004) are a first step in this direction. 1 Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation c and/or Summarization, pages 1–8, Ann Arbor, June 2005. 2005 Association for Computational Linguistics 2 Background In the past, assessments of usefulness involved a wide range of both intrinsic and extrinsic (task-based) measures (Sparck-Jones and Gallier, 1996). Intrinsic evaluations focus on coherence and informativeness (Jing et al., 1998) and often involve quality comparisons between automatic summaries and reference summaries that are pre-determin"
W05-0901,W00-0407,0,0.0284813,". Using Relevance-Prediction increases the confidence in our results and strengthens the statistical statements we can make about the benefits of summarization. The next section describes an alternative approach to measuring task-based usefulness, where the usage of external judgments as a gold standard is replaced by the 3 A topic is an event or activity, along with all directly related events and activities. An event is something that happens at some specific time and place, and the unavoidable consequences. 2 user’s own decisions on the full text. Following the lead of earlier evaluations (Oka and Ueda, 2000; Mani et al., 2002; Sakai and Sparck-Jones, 2001), we focus on relevance assessment as our extrinsic task. 3 Evaluation of Usefulness of Summaries We define a new extrinsic measure of task-based usefulness called Relevance-Prediction, where we compare a summary-based decision to the subject’s own full-text decision rather than to a different subject’s decision. Our findings differ from that of the SUMMAC results (Mani et al., 2002) in that using Relevance-Prediction as an alternative to comparision to a gold standard is a more realistic agreement measure for assessing usefulness in a relevanc"
W05-0901,J98-3005,0,0.0406616,"used. Definitive conclusions about the usefulness of summaries would provide justification for continued research and development of new summarization methods. To investigate the question of whether text summarization is useful in an extrinsic task, we examined human performance in a relevance assessment task using a human text surrogate (i.e. text intended to stand in the place of a document). We use single-document English summaries as these are sufficient for investigating task-based usefulness, although more elaborate surrogates are possible, e.g., those that span more than one document (Radev and McKeown, 1998; Mani and Bloedorn, 1998). The next section motivates the need for developing a new framework for measuring task-based usefulness. Section 3 presents a novel extrinsic measure called Relevance-Prediction. Section 4 demonstrates that this is a more reliable measure than that of previous gold standard methods, e.g., the LDC-Agreement method used for SUMMAC-style evaluations, and that this reliability allows us to make stronger statistical statements about the benefits of summarization. We expect these findings to be important for future summarization evaluations. Section 5 presents the results"
W05-0901,W03-0508,0,0.0592052,"Missing"
W06-3114,W06-3123,1,0.282531,"Missing"
W06-3114,E06-1032,1,0.793931,"uation For the automatic evaluation, we used B LEU, since it is the most established metric in the field. The B LEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use. It rewards matches of n-gram sequences, but measures only at most indirectly overall grammatical coherence. The B LEU score has been shown to correlate well with human judgement, when statistical machine translation systems are compared (Doddington, 2002; Przybocki, 2004; Li, 2005). However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong. They demonstrated this with the comparison of statistical systems against (a) manually post-edited MT output, and (b) a rule-based commercial system. The development of automatic scoring methods is an open field of research. It was our hope that this competition, which included the manual and automatic evaluation of statistical systems and one rulebased commercial system, will give further insight into the relation between automatic and manual evaluation. At the very least, we are creating a data resource (the manual annotations) th"
W06-3114,P05-1066,1,0.520404,"Missing"
W06-3114,W06-3120,0,0.0236158,"Missing"
W06-3114,W06-3125,0,0.0522684,"Missing"
W06-3114,2005.iwslt-1.1,0,0.0444919,", prepared training and test sets, resulting in actual machine translation output from several state-of-the-art systems and manual evaluations. All this is available at the workshop website1 . The shared task is a follow-up to the one we organized in the previous year, at a similar venue (Koehn and Monz, 2005). As then, we concentrated on the translation of European languages and the use of the Europarl corpus for training. Again, most systems that participated could be categorized as statistical phrase-based systems. While there is now a number of competitions — DARPA/NIST (Li, 2005), IWSLT (Eck and Hori, 2005), TC-Star — this one focuses on text translation between various European languages. This year’s shared task changed in some aspects from last year’s: • We carried out a manual evaluation in addition to the automatic scoring. Manual evaluation 1 http://www.statmt.org/wmt06/ • We evaluated translation from English, in addition to into English. English was again paired with German, French, and Spanish. We dropped, however, one of the languages, Finnish, partly to keep the number of tracks manageable, partly because we assumed that it would be hard to find enough Finnish speakers for the manual e"
W06-3114,W06-3126,0,0.0173835,"Missing"
W06-3114,W06-3118,0,0.0167429,"Missing"
W06-3114,W04-3250,1,0.375882,"two systems of equal performance. Let say, if we find one system doing better on 20 of the blocks, and worse on 80 of the blocks, is it significantly worse? We check, how likely only up to k = 20 better scores out of n = 100 would have been generated by two equal systems, using the binomial distribution: p(0..k; n, p) = 2.2 Statistical Significance k   X i i=0 Confidence Interval: Since B LEU scores are not computed on the sentence level, traditional methods to compute statistical significance and confidence intervals do not apply. Hence, we use the bootstrap resampling method described by Koehn (2004). Following this method, we repeatedly — say, 1000 times — sample sets of sentences from the out105 = 0.5n n pi pn−i k   X i i=0 (3) n If p(0..k; n, p) < 0.05, or p(0..k; n, p) > 0.95 then we have a statistically significant difference between the systems. Figure 3: Annotation tool for manual judgement of adequacy and fluency of the system output. Translations from 5 randomly selected systems for a randomly selected sentence is presented. No additional information beyond the instructions on this page are given to the judges. The tool tracks and reports annotation speed. 3 Manual Evaluation 3"
W06-3114,W05-0820,1,0.725787,"ranging from commercial companies, industrial research labs to individual graduate students. The motivation for such a competition is to establish baseline performance numbers for defined training scenarios and test sets. We assembled various forms of data and resources: a baseline MT system, language models, prepared training and test sets, resulting in actual machine translation output from several state-of-the-art systems and manual evaluations. All this is available at the workshop website1 . The shared task is a follow-up to the one we organized in the previous year, at a similar venue (Koehn and Monz, 2005). As then, we concentrated on the translation of European languages and the use of the Europarl corpus for training. Again, most systems that participated could be categorized as statistical phrase-based systems. While there is now a number of competitions — DARPA/NIST (Li, 2005), IWSLT (Eck and Hori, 2005), TC-Star — this one focuses on text translation between various European languages. This year’s shared task changed in some aspects from last year’s: • We carried out a manual evaluation in addition to the automatic scoring. Manual evaluation 1 http://www.statmt.org/wmt06/ • We evaluated tr"
W06-3114,W06-3121,0,0.0396613,"Missing"
W06-3114,W06-3122,0,0.0280607,"Missing"
W06-3114,W06-3116,0,0.0257747,"Missing"
W06-3114,W05-0908,0,0.0290753,"Missing"
W06-3114,W06-3117,0,0.0296838,"Missing"
W06-3114,W06-3115,0,0.0225709,"Missing"
W06-3114,W06-3124,0,\N,Missing
W06-3114,2006.amta-papers.2,0,\N,Missing
W06-3114,W06-3119,0,\N,Missing
W07-0718,W07-0735,0,0.066797,"Missing"
W07-0718,E06-1032,1,0.501959,"ve the time. 3 The judgment data along with all system translations are available at http://www.statmt.org/wmt07/ 142 4 Automatic evaluation The past two ACL workshops on machine translation used Bleu as the sole automatic measure of translation quality. Bleu was used exclusively since it is the most widely used metric in the field and has been shown to correlate with human judgments of translation quality in many instances (Doddington, 2002; Coughlin, 2003; Przybocki, 2004). However, recent work suggests that Bleu’s correlation with human judgments may not be as strong as previously thought (Callison-Burch et al., 2006). The results of last year’s workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006). We used the manual evaluation data as a means of testing the correlation of a range of automatic metrics in addition to Bleu. In total we used eleven different automatic evaluation measures to rank the shared task submissions. They are: • Meteor (Banerjee and Lavie, 2005)—Meteor measures precision and recall of unigrams when comparing a hypothesis translation Language Pair English-German German-English English-Spanish Span"
W07-0718,J96-2004,0,0.0978688,"—the inter-annotator agreement was barely considered fair, and the intra-annotator agreement was only moderate. Even when we reassessed fluency and adequacy as relative ranks the agreements increased only minimally. 6 Meta-evaluation In addition to evaluating the translation quality of the shared task entries, we also performed a “metaevaluation” of our evaluation methodologies. 6.1 Inter- and Intra-annotator agreement We measured pairwise agreement among annotators using the kappa coefficient (K) which is widely used in computational linguistics for measuring agreement in category judgments (Carletta, 1996). It is defined as P (A) − P (E) K= 1 − P (E) where P (A) is the proportion of times that the annotators agree, and P (E) is the proportion of time that they would agree by chance. We define chance agreement for fluency and adequacy as 51 , since they are based on five point scales, and for ranking as 13 145 0.02 These timing figures are promising because they indicate that the tasks which the annotators were the most reliable on (constituent ranking and sentence ranking) were also much quicker to complete than the ones that they were unreliable on (assigning fluency and adequacy scores). This"
W07-0718,koen-2004-pharaoh,0,\N,Missing
W07-0718,H05-1100,0,\N,Missing
W07-0718,W07-0721,0,\N,Missing
W07-0718,W07-0731,0,\N,Missing
W07-0718,W07-0728,0,\N,Missing
W07-0718,W07-0738,0,\N,Missing
W07-0718,W07-0727,0,\N,Missing
W07-0718,N03-2021,0,\N,Missing
W07-0718,P05-1038,0,\N,Missing
W07-0718,P02-1040,0,\N,Missing
W07-0718,W05-0820,1,\N,Missing
W07-0718,W06-1610,0,\N,Missing
W07-0718,D07-1091,1,\N,Missing
W07-0718,W07-0730,0,\N,Missing
W07-0718,J04-4002,0,\N,Missing
W07-0718,W05-0909,0,\N,Missing
W07-0718,W07-0734,0,\N,Missing
W07-0718,2005.iwslt-1.1,0,\N,Missing
W07-0718,W07-0733,1,\N,Missing
W07-0718,W07-0707,0,\N,Missing
W07-0718,W07-0725,0,\N,Missing
W07-0718,W07-0723,0,\N,Missing
W07-0718,P05-1074,1,\N,Missing
W07-0718,W07-0732,1,\N,Missing
W07-0718,C04-1046,0,\N,Missing
W07-0718,W07-0724,0,\N,Missing
W07-0718,P05-1039,0,\N,Missing
W07-0718,N07-1006,0,\N,Missing
W07-0718,W06-3114,1,\N,Missing
W07-0718,N03-1017,1,\N,Missing
W07-0718,J03-1002,0,\N,Missing
W07-0718,P06-2003,0,\N,Missing
W07-0718,W07-0722,0,\N,Missing
W07-0718,2005.mtsummit-papers.11,1,\N,Missing
W07-0718,2006.iwslt-evaluation.1,0,\N,Missing
W07-0718,2003.mtsummit-papers.9,0,\N,Missing
W07-0718,W07-0726,0,\N,Missing
W07-0718,W07-0729,0,\N,Missing
W08-0309,W08-0312,0,0.0979584,"MT 3 f r .717 .708 .706 .704 .702 .699 .699 .695 .678 .674 .661 .654 .652 .638 .637 .633 .628 .627 .624 .616 .615 .615 .612 SAAR f r SAAR - C de RBMT 4 de CUED es RBMT 3 de CMU - SMTes UCB es LIMSI es RBMT 6 de RBMT 5 de LIMSI de LIU de SAAR de CMU - STATXFR f r UMD cz BBN - COMBO de UEDIN de MORPHOLOGIC hu DCU cz UEDIN - COMBO de UEDIN cz CMU - STATXFER de UEDIN hu .584 .574 .573 .572 .552 .548 .547 .537 .509 .493 .469 .447 .445 .444 .429 .407 .402 .387 .380 .327 .293 .280 .188 some of the allowable variation in translation. We use a single reference translation in our experiments. • Meteor (Agarwal and Lavie, 2008)—Meteor measures precision and recall for unigrams and applies a fragmentation penalty. It uses flexible word matching based on stemming and WordNet-synonymy. A number of variants are investigated here: meteor-baseline and meteorranking are optimized for correlation with adequacy and ranking judgments respectively. mbleu and mter are Bleu and TER computed using the flexible matching used in Meteor. Table 7: The average number of times that each system was judged to be better than or equal to all other systems in the sentence ranking task for the All-English condition. The subscript indicates t"
W08-0309,P05-1038,0,0.00937556,"le judge the translations of those syntactic phrases. In order to draw judges’ attention to these regions, we highlighted the selected source phrases and the corresponding phrases in the translations. The corresponding phrases in the translations were located via automatic word alignments. Figure 2 illustrates how the source and reference phrases are highlighted via automatic word alignments. The same is done for sentence and each of the system translations. The English, French, German and Spanish test sets were automatically parsed using high quality parsers for those languages (Bikel, 2002; Arun and Keller, 2005; Dubey, 2005; Bick, 2006). The word alignments were created with Giza++ 76 (Och and Ney, 2003) applied to a parallel corpus containing the complete Europarl training data, plus sets of 4,051 sentence pairs created by pairing the test sentences with the reference translations, and the test sentences paired with each of the system translations. The phrases in the translations were located using standard phrase extraction techniques (Koehn et al., 2003). Because the word-alignments were created automatically, and because the phrase extraction is heuristic, the phrases that were selected may not"
W08-0309,W08-0314,0,0.0198808,"Missing"
W08-0309,W08-0321,0,0.0147788,"Missing"
W08-0309,W08-0316,0,0.0218948,"Missing"
W08-0309,W08-0319,0,0.0287247,"Missing"
W08-0309,E06-1032,1,0.371813,"ranslations are acceptable for each sentence in our test corpus. When we change our system and want to evaluate it, we do not need to manually evaluate those segments that match against the database, and could instead have people evaluate only those phrasal translations which are new. Accumulating these judgments over time would give a very reliable idea of what alternative translations were allowable. This would be useful because it could alleviate the problems associated with Bleu failing to recognize allowable variation in translation when multiple reference translations are not available (Callison-Burch et al., 2006). A large database of human judgments might also be useful as an objective function for minimum error rate training (Och, 2003) or in other system development tasks. 8 Conclusions Similar to previous editions of this workshop we carried out an extensive manual and automatic evaluation of machine translation performance for translating from European languages into English, and vice versa. One important aspect in which this year’s shared task differed from previous years was the introduction of an additional newswire test set that was different in nature to the training data. We 85 also added ne"
W08-0309,W07-0718,1,0.713541,"air that did not include English, but was not manually evaluated since it attracted minimal participation. • System combination – Saarland University entered a system combination over a number of rule-based MT systems, and provided their output, which were also treated as fully fledged entries in the manual evaluation. Three additional groups were invited to apply their system combination algorithms to all systems. Introduction This paper presents the results the shared tasks of the 2008 ACL Workshop on Statistical Machine Translation, which builds on two past workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007). There were two shared tasks this year: a translation task which evaluated translation between 10 pairs of European languages, and an evaluation task which examines automatic evaluation metrics. There were a number of differences between this year’s workshop and last year’s workshop: • Test set selection – Instead of creating our test set by reserving a portion of the training data, we instead hired translators to translate a set of • Refined manual evaluation – Because last year’s study indicated that fluency and adequacy judgments were slow and unreliable, we dropped them from manual evalua"
W08-0309,J96-2004,0,0.0556395,"roving the process of manual evaluation. 7.1 P (E) .333 .333 .333 .5 .5 K .367 .506 .517 .642 .649 Table 12: Kappa coefficient values representing the inter-annotator agreement for the different types of manual evaluation Table 11: The percent of time that each automatic metric was consistent with human judgments for translations into other languages 7 P (A) .578 .671 .678 .821 .825 Inter- and Intra-annotator agreement We measured pairwise agreement among annotators using the kappa coefficient (K) which is widely used in computational linguistics for measuring agreement in category judgments (Carletta, 1996). It is defined as P (A) − P (E) K= 1 − P (E) 83 Evaluation type Sentence ranking Constituent ranking Constituent (w/identicals) Yes/No judgments Yes/No (w/identicals) P (A) .691 .825 .832 .928 .930 P (E) .333 .333 .333 .5 .5 K .537 .737 .748 .855 .861 Table 13: Kappa coefficient values for intraannotator agreement for the different types of manual evaluation where P (A) is the proportion of times that the annotators agree, and P (E) is the proportion of time that they would agree by chance. We define chance agreement for ranking tasks as 13 since there are three possible outcomes when ranking"
W08-0309,W08-0310,0,0.0252447,"Missing"
W08-0309,P05-1039,0,0.00794095,"ns of those syntactic phrases. In order to draw judges’ attention to these regions, we highlighted the selected source phrases and the corresponding phrases in the translations. The corresponding phrases in the translations were located via automatic word alignments. Figure 2 illustrates how the source and reference phrases are highlighted via automatic word alignments. The same is done for sentence and each of the system translations. The English, French, German and Spanish test sets were automatically parsed using high quality parsers for those languages (Bikel, 2002; Arun and Keller, 2005; Dubey, 2005; Bick, 2006). The word alignments were created with Giza++ 76 (Och and Ney, 2003) applied to a parallel corpus containing the complete Europarl training data, plus sets of 4,051 sentence pairs created by pairing the test sentences with the reference translations, and the test sentences paired with each of the system translations. The phrases in the translations were located using standard phrase extraction techniques (Koehn et al., 2003). Because the word-alignments were created automatically, and because the phrase extraction is heuristic, the phrases that were selected may not exactly corre"
W08-0309,W08-0327,1,0.775819,"Missing"
W08-0309,W08-0331,0,0.117642,"asure) by matching part of speech 4grams in a hypothesis translation against the reference translation. In addition to the above metrics, which scored the translations on both the system-level5 and the sentence-level, there were a number of metrics which focused on the sentence-level: • Albrecht and Hwa (2008) use support vector regression to score translations using past WMT manual assessment data as training examples. The metric uses features derived from targetside language models and machine-generated translations (svm-pseudo-ref) as well as reference human translations (svm-human-ref). • Duh (2008) similarly used support vector machines to predict an ordering over a set of 5 We provide the scores assigned to each system by these metrics in Appendix A. Measuring system-level correlation To measure the correlation of the automatic metrics with the human judgments of translation quality at the system-level we used Spearman’s rank correlation coefficient ρ. We converted the raw scores assigned each system into ranks. We assigned a ranking to the systems for each of the three types of manual evaluation based on: • The percent of time that the sentences it produced were judged to be better th"
W08-0309,W07-0729,0,0.0315049,"Missing"
W08-0309,W08-0328,0,0.0942343,"s submitted to shared translation task. We designated the translations of the Europarl set as the development data for combination techniques which weight each system.3 CMU combined the French-English systems, BBN combined the French-English and German-English systems, and Edinburgh submitted combinations for the French-English and GermanEnglish systems as well as a multi-source system combination which combined all systems which translated from any language pair into English for the News test set. The University of Saarland also produced a system combination over six commercial RBMT systems (Eisele et al., 2008). Saarland graciously provided the output of these systems, which we manually evaluated alongside all other entries. For more on the participating systems, please refer to the respective system descriptions in the proceedings of the workshop. 3 Human evaluation As with last year’s workshop, we placed greater emphasis on the human evaluation than on the automatic evaluation metric scores. It is our contention that automatic measures are an imperfect substitute for human assessment of translation quality. Therefore, rather than select an official automatic evaluation metric like the NIST Machine"
W08-0309,W08-0332,0,0.47939,"ning how well various automatic metrics correlate with human judgments. In addition to examining how well the automatic evaluation metrics predict human judgments at the system-level, this year we have also started to measure their ability to predict sentence-level judgments. The automatic metrics that were evaluated in this year’s shared task were the following: • Bleu (Papineni et al., 2002)—Bleu remains the de facto standard in machine translation evaluation. It calculates n-gram precision and a brevity penalty, and can make use of multiple reference translations as a way of capturing 80 • Gimenez and Marquez (2008) measure overlapping grammatical dependency relationships (DP), semantic roles (SR), and discourse representations (DR). The authors further investigate combining these with other metrics including TER, Bleu, GTM, Rouge, and Meteor (ULC and ULCh). • Popovic and Ney (2007) automatically evaluate translation quality by examining sequences of parts of speech, rather than words. They calculate Bleu (posbleu) and F-measure (pos4gramFmeasure) by matching part of speech 4grams in a hypothesis translation against the reference translation. In addition to the above metrics, which scored the translation"
W08-0309,W08-0302,0,0.0296793,"Missing"
W08-0309,P05-3026,0,0.0948555,"Missing"
W08-0309,W08-0315,0,0.0199873,"Missing"
W08-0309,W06-3114,1,0.562417,"s our first language pair that did not include English, but was not manually evaluated since it attracted minimal participation. • System combination – Saarland University entered a system combination over a number of rule-based MT systems, and provided their output, which were also treated as fully fledged entries in the manual evaluation. Three additional groups were invited to apply their system combination algorithms to all systems. Introduction This paper presents the results the shared tasks of the 2008 ACL Workshop on Statistical Machine Translation, which builds on two past workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007). There were two shared tasks this year: a translation task which evaluated translation between 10 pairs of European languages, and an evaluation task which examines automatic evaluation metrics. There were a number of differences between this year’s workshop and last year’s workshop: • Test set selection – Instead of creating our test set by reserving a portion of the training data, we instead hired translators to translate a set of • Refined manual evaluation – Because last year’s study indicated that fluency and adequacy judgments were slow and unreliable, we d"
W08-0309,W07-0733,1,0.572842,"a Europarl test set. The Europarl test data was again drawn from the transcripts of EU parliamentary proceedings from the fourth quarter of 2000, which is excluded from the Europarl training data. Our rationale behind investing a considerable sum to create the News test set was that we believe that it more accurately represents the quality of systems’ translations than when we simply hold out a portion of the training data as the test set, as with the Europarl set. For instance, statistical systems are heavily optimized to their training data, and do not perform as well on out-of-domain data (Koehn and Schroeder, 2007). Having both the News test set and the Europarl test set allows us to contrast the performance of systems on in-domain and out-of-domain data, and provides a fairer comparison between systems trained on the Europarl corpus and systems that were developed without it. 2.2 Provided materials To lower the barrier of entry for newcomers to the field, we provided a complete baseline MT system, along with data resources. We provided: • • • • sentence-aligned training corpora language model data development and dev-test sets Moses open source toolkit for phrase-based statistical translation (Koehn et"
W08-0309,N03-1017,1,0.00762077,"ions. The English, French, German and Spanish test sets were automatically parsed using high quality parsers for those languages (Bikel, 2002; Arun and Keller, 2005; Dubey, 2005; Bick, 2006). The word alignments were created with Giza++ 76 (Och and Ney, 2003) applied to a parallel corpus containing the complete Europarl training data, plus sets of 4,051 sentence pairs created by pairing the test sentences with the reference translations, and the test sentences paired with each of the system translations. The phrases in the translations were located using standard phrase extraction techniques (Koehn et al., 2003). Because the word-alignments were created automatically, and because the phrase extraction is heuristic, the phrases that were selected may not exactly correspond to the translations of the selected source phrase. We noted this in the instructions to judges: Rank each constituent translation from Best to Worst relative to the other choices (ties are allowed). Grade only the highlighted part of each translation. Please note that segments are selected automatically, and they should be taken as an approximate guide. They might include extra words that are not in the actual alignment, or miss wor"
W08-0309,W08-0329,0,0.0334792,"Missing"
W08-0309,W08-0320,0,0.0114639,"Missing"
W08-0309,W08-0313,0,0.036043,"Missing"
W08-0309,W08-0323,0,0.0290179,"Missing"
W08-0309,W08-0317,0,0.0278407,"Missing"
W08-0309,W08-0311,0,0.034369,"Missing"
W08-0309,W08-0326,0,0.025716,"Missing"
W08-0309,J03-1002,0,0.00405346,"regions, we highlighted the selected source phrases and the corresponding phrases in the translations. The corresponding phrases in the translations were located via automatic word alignments. Figure 2 illustrates how the source and reference phrases are highlighted via automatic word alignments. The same is done for sentence and each of the system translations. The English, French, German and Spanish test sets were automatically parsed using high quality parsers for those languages (Bikel, 2002; Arun and Keller, 2005; Dubey, 2005; Bick, 2006). The word alignments were created with Giza++ 76 (Och and Ney, 2003) applied to a parallel corpus containing the complete Europarl training data, plus sets of 4,051 sentence pairs created by pairing the test sentences with the reference translations, and the test sentences paired with each of the system translations. The phrases in the translations were located using standard phrase extraction techniques (Koehn et al., 2003). Because the word-alignments were created automatically, and because the phrase extraction is heuristic, the phrases that were selected may not exactly correspond to the translations of the selected source phrase. We noted this in the inst"
W08-0309,P03-1021,0,0.0375741,"aluate those segments that match against the database, and could instead have people evaluate only those phrasal translations which are new. Accumulating these judgments over time would give a very reliable idea of what alternative translations were allowable. This would be useful because it could alleviate the problems associated with Bleu failing to recognize allowable variation in translation when multiple reference translations are not available (Callison-Burch et al., 2006). A large database of human judgments might also be useful as an objective function for minimum error rate training (Och, 2003) or in other system development tasks. 8 Conclusions Similar to previous editions of this workshop we carried out an extensive manual and automatic evaluation of machine translation performance for translating from European languages into English, and vice versa. One important aspect in which this year’s shared task differed from previous years was the introduction of an additional newswire test set that was different in nature to the training data. We 85 also added new language pairs to our evaluation: Hungarian-English and German-Spanish. As in previous years we were pleased to notice an inc"
W08-0309,P02-1040,0,0.12021,"nt systems. In particular, it is especially useful for validating the automatic metrics which are frequently used by the machine translation research community. We continued the shared task which we debuted last year, by examining how well various automatic metrics correlate with human judgments. In addition to examining how well the automatic evaluation metrics predict human judgments at the system-level, this year we have also started to measure their ability to predict sentence-level judgments. The automatic metrics that were evaluated in this year’s shared task were the following: • Bleu (Papineni et al., 2002)—Bleu remains the de facto standard in machine translation evaluation. It calculates n-gram precision and a brevity penalty, and can make use of multiple reference translations as a way of capturing 80 • Gimenez and Marquez (2008) measure overlapping grammatical dependency relationships (DP), semantic roles (SR), and discourse representations (DR). The authors further investigate combining these with other metrics including TER, Bleu, GTM, Rouge, and Meteor (ULC and ULCh). • Popovic and Ney (2007) automatically evaluate translation quality by examining sequences of parts of speech, rather than"
W08-0309,W07-0707,0,0.0613323,"The automatic metrics that were evaluated in this year’s shared task were the following: • Bleu (Papineni et al., 2002)—Bleu remains the de facto standard in machine translation evaluation. It calculates n-gram precision and a brevity penalty, and can make use of multiple reference translations as a way of capturing 80 • Gimenez and Marquez (2008) measure overlapping grammatical dependency relationships (DP), semantic roles (SR), and discourse representations (DR). The authors further investigate combining these with other metrics including TER, Bleu, GTM, Rouge, and Meteor (ULC and ULCh). • Popovic and Ney (2007) automatically evaluate translation quality by examining sequences of parts of speech, rather than words. They calculate Bleu (posbleu) and F-measure (pos4gramFmeasure) by matching part of speech 4grams in a hypothesis translation against the reference translation. In addition to the above metrics, which scored the translations on both the system-level5 and the sentence-level, there were a number of metrics which focused on the sentence-level: • Albrecht and Hwa (2008) use support vector regression to score translations using past WMT manual assessment data as training examples. The metric use"
W08-0309,P07-1040,0,0.0268048,"at indicated which language the system was originally translating from. This entry was part of ongoing research in multi-lingual, multisource translation. Since there was no official multilingual system combination track, this entry should be viewed only as a contrastive data point. Table 4: Summary results for the sentence ranking judgments. The numbers report the percent of time that each system was judged to be greater than or equal to any other system. Bold indicates the highest score for that task. was the University of Edinburgh’s system combination entry. It uses a technique similar to Rosti et al. (2007) to perform system combination. Like the other system combination entrants, it was tuned on the Europarl test set and tested on the News test set, using systems that submitted entries to both tasks. BBN - COMBO CMU - COMBO CMU - GIMPEL CMU - SMT CMU - STATXFER CU - BOJAR CU - TECTOMT CUED CUED - CONTR DCU LIMSI LIU LIUM - SYSTRAN LIUM - SYS - CONTR MORPHOLOGIC PC - TRANSLATOR RBMT 2 RBMT 3 RBMT 4 RBMT 5 RBMT 6 SAAR SAAR - CONTR SYSTRAN UCB UCL UEDIN UEDIN - COMBO UMD UPC UW XEROX Czech-English Commentary Czech-English News English-Czech Commentary English-Czech News English-French Europarl Eng"
W08-0309,2004.tmi-1.8,0,\N,Missing
W08-0309,W08-0322,0,\N,Missing
W08-0309,W08-0318,1,\N,Missing
W08-0309,W08-0324,0,\N,Missing
W08-0309,W08-0325,0,\N,Missing
W08-0309,P07-1111,0,\N,Missing
W08-0309,P07-1038,0,\N,Missing
W08-0309,W08-0330,0,\N,Missing
W08-0309,2005.eamt-1.20,0,\N,Missing
W08-0309,D08-1076,0,\N,Missing
W09-0401,W08-0312,0,0.0796143,"calculates n-gram precision and a brevity penalty, and can make use of multiple reference translations as a way of capturing some of the allowable variation in translation. We use a single reference translation in our experiments. • ULC (Gim´enez and M`arquez, 2008)—ULC is an arithmetic mean over other automatic metrics. The set of metrics used include Rouge, Meteor, measures of overlap between constituent parses, dependency parses, semantic roles, and discourse representations. The ULC metric had the strongest correlation with human judgments in WMT08 (CallisonBurch et al., 2008). • Meteor (Agarwal and Lavie, 2008)—Meteor measures precision and recall for unigrams and applies a fragmentation penalty. It uses flexible word matching based on stemming and WordNet-synonymy. meteor-ranking is optimized for correlation with ranking judgments. • Translation Error Rate (Snover et al., 2006)—TER calculates the number of edits required to change a hypothesis translation into a reference translation. The possible edits in TER include insertion, deletion, and substitution of single words, and an edit which moves sequences of contiguous words. Two variants of TER are also included: TERp (Snover et al., 2009), a new"
W09-0401,1998.amta-tutorials.1,0,0.0723526,"Missing"
W09-0401,W09-0426,0,0.0294171,"Missing"
W09-0401,P07-1038,0,0.0331701,"Missing"
W09-0401,W09-0417,0,0.033793,"Missing"
W09-0401,W09-0411,0,0.034886,"Missing"
W09-0401,W07-0718,1,0.791479,"Missing"
W09-0401,W09-0420,0,0.0243178,"Missing"
W09-0401,W08-0309,1,0.578575,"est sets and public training data with published performance numbers, and to refine evaluation methodologies for machine translation. All of the data, translations, and human judgments produced for our workshop are publicly available.1 We hope they form a valuable resource for research into statistical machine translation, system combination, and automatic evaluation of translation quality. This paper presents the results of the shared tasks of the 2009 EACL Workshop on Statistical Machine Translation, which builds on three previous workshops (Koehn and Monz, 2006; CallisonBurch et al., 2007; Callison-Burch et al., 2008). There were three shared tasks this year: a translation task between English and five other European languages, a task to combine the output of multiple machine translation systems, and a task to predict human judgments of translation quality using automatic evaluation metrics. The performance on each of these shared task was determined after a comprehensive human evaluation. There were a number of differences between this year’s workshop and last year’s workshop: 2 Overview of the shared translation and system combination tasks The workshop examined translation between English and five other"
W09-0401,W09-0408,0,0.0416758,"Missing"
W09-0401,W09-0416,0,0.030911,"Missing"
W09-0401,W09-0406,0,0.0789703,"Missing"
W09-0401,W09-0419,1,0.738361,"Missing"
W09-0401,W09-0413,0,0.0338062,"Missing"
W09-0401,W09-0428,0,0.0340245,"Missing"
W09-0401,2004.tmi-1.8,0,0.00591015,"score to the higher ranked system). We divided this by the total number of pairwise comparisons to get a percentage. Because the systems generally assign real numbers as scores, we excluded pairs that the human annotators ranked as ties. Average terp ter bleusp4114 bleusp bleu bleu (cased) bleu-ter/2 wcd6p4er nist (cased) nist wpF wpbleu en-cz (5 systems) Because the sentence-level judgments collected in the manual evaluation are relative judgments rather than absolute judgments, it is not possible for us to measure correlation at the sentencelevel in the same way that previous work has done (Kulesza and Shieber, 2004; Albrecht and Hwa, 2007a; Albrecht and Hwa, 2007b). en-es (11 systems) Measuring sentence-level consistency en-fr (16 systems) 5.2 Average where di is the difference between the rank for systemi and n is the number of systems. The possible values of ρ range between 1 (where all systems are ranked in the same order) and −1 (where the systems are ranked in the reverse order). Thus an automatic evaluation metric with a higher absolute value for ρ is making predictions that are more similar to the human judgments than an automatic evaluation metric with a lower absolute ρ. hu-en (6 systems) 6 d2i"
W09-0401,W09-0404,0,0.0128365,"-S (Lin and Och, 2004), with tuned n-gram weights, and bleusp, with constant weights. wcd6p4er is an error measure and bleusp is a quality score. In addition to allowing us to analyze the translation quality of different systems, the data gathered during the manual evaluation is useful for validating the automatic evaluation metrics. Last year, NIST began running a similar “Metrics for MAchine TRanslation” challenge (MetricsMATR), and presented their findings at a workshop at AMTA (Przybocki et al., 2008). In this year’s shared task we evaluated a number of different automatic metrics: • RTE (Pado et al., 2009)—The RTE metric follows a semantic approach which applies recent work in rich textual entailment to the problem of MT evaluation. Its predictions are based on a regression model over a feature set adapted from an entailment systems. The features primarily model alignment quality and (mis-)matches of syntactic and semantic structures. • Bleu (Papineni et al., 2002)—Bleu remains the de facto standard in machine translation evaluation. It calculates n-gram precision and a brevity penalty, and can make use of multiple reference translations as a way of capturing some of the allowable variation in"
W09-0401,P02-1040,0,0.123715,"ing a similar “Metrics for MAchine TRanslation” challenge (MetricsMATR), and presented their findings at a workshop at AMTA (Przybocki et al., 2008). In this year’s shared task we evaluated a number of different automatic metrics: • RTE (Pado et al., 2009)—The RTE metric follows a semantic approach which applies recent work in rich textual entailment to the problem of MT evaluation. Its predictions are based on a regression model over a feature set adapted from an entailment systems. The features primarily model alignment quality and (mis-)matches of syntactic and semantic structures. • Bleu (Papineni et al., 2002)—Bleu remains the de facto standard in machine translation evaluation. It calculates n-gram precision and a brevity penalty, and can make use of multiple reference translations as a way of capturing some of the allowable variation in translation. We use a single reference translation in our experiments. • ULC (Gim´enez and M`arquez, 2008)—ULC is an arithmetic mean over other automatic metrics. The set of metrics used include Rouge, Meteor, measures of overlap between constituent parses, dependency parses, semantic roles, and discourse representations. The ULC metric had the strongest correlati"
W09-0401,W09-0407,0,0.0551118,"Missing"
W09-0401,W09-0418,0,0.024243,"Missing"
W09-0401,W09-0424,1,0.0806642,"Missing"
W09-0401,W09-0402,0,0.0228572,"dNet-synonymy. meteor-ranking is optimized for correlation with ranking judgments. • Translation Error Rate (Snover et al., 2006)—TER calculates the number of edits required to change a hypothesis translation into a reference translation. The possible edits in TER include insertion, deletion, and substitution of single words, and an edit which moves sequences of contiguous words. Two variants of TER are also included: TERp (Snover et al., 2009), a new version which introduces a number of different features, and (Bleu − TER)/2, a combination of Bleu and Translation Edit Rate. • wpF and wpBleu (Popovic and Ney, 2009) These metrics are based on words and part of speech sequences. wpF is an n-gram based Fmeasure which takes into account both word n-grams and part of speech n-grams. wpBLEU is a combnination of the normal Blue score and a part of speech-based Bleu score. • SemPOS (Kos and Bojar, 2009) – the SemPOS metric computes overlapping words, as defined in (Gim´enez and M`arquez, 2007), with respect to their semantic part of speech. Moreover, it does not use the surface representation of words but their underlying forms obtained from the TectoMT framework. • MaxSim (Chan and Ng, 2008)—MaxSim calculates"
W09-0401,E09-1082,1,0.692929,"ing the secondary system submissions. Baseline system To lower the barrier of entry for newcomers to the field, we provided Moses, an open source toolkit for phrase-based statistical translation (Koehn et al., 2007). The performance of this baseline system is similar to the best submissions in last year’s shared task. Twelve participating groups used the Moses toolkit for the development of their system. 2.4 System combination In addition to soliciting system combination entries for each of the language pairs, we treated system combination as a way of doing multi-source translation, following Schroeder et al. (2009). For the multi-source system combination task, we provided all 46 primary system submissions from any language into English, along with an additional 32 secondary systems. Table 2 lists the six participants in the system combination task. Submitted systems We received submissions from 22 groups from 20 institutions, as listed in Table 1, a similar turnout to last year’s shared task. Of the 20 groups that participated with regular system submissions in last year’s shared task, 12 groups returned this year. A major hurdle for many was a DARPA/GALE evaluation that occurred at the same time as th"
W09-0401,W09-0423,0,0.0383252,"Missing"
W09-0401,2006.amta-papers.25,0,0.223011,"Missing"
W09-0401,W09-0441,0,0.0797267,"or (Agarwal and Lavie, 2008)—Meteor measures precision and recall for unigrams and applies a fragmentation penalty. It uses flexible word matching based on stemming and WordNet-synonymy. meteor-ranking is optimized for correlation with ranking judgments. • Translation Error Rate (Snover et al., 2006)—TER calculates the number of edits required to change a hypothesis translation into a reference translation. The possible edits in TER include insertion, deletion, and substitution of single words, and an edit which moves sequences of contiguous words. Two variants of TER are also included: TERp (Snover et al., 2009), a new version which introduces a number of different features, and (Bleu − TER)/2, a combination of Bleu and Translation Edit Rate. • wpF and wpBleu (Popovic and Ney, 2009) These metrics are based on words and part of speech sequences. wpF is an n-gram based Fmeasure which takes into account both word n-grams and part of speech n-grams. wpBLEU is a combnination of the normal Blue score and a part of speech-based Bleu score. • SemPOS (Kos and Bojar, 2009) – the SemPOS metric computes overlapping words, as defined in (Gim´enez and M`arquez, 2007), with respect to their semantic part of speech."
W09-0401,D07-1049,0,0.0154831,"systems evaluated in this workshop, since they were trained using only the provided materials. In addition to cleaning the sentence-aligned parallel corpus we also de-duplicated the corpus, removing all sentence pairs that occured more than once in the parallel corpus. Many of the documents gathered in our web crawl were duplicates or near duplicates, and a lot of the text is repeated, as with web site navigation. We further eliminated sentence pairs that varied from previous sentences by only numbers, which helped eliminate template web pages such as expense reports. We used a Bloom Filter (Talbot and Osborne, 2007) to do de-duplication, so it may have discarded more sentence pairs than strictly necessary. After deduplication, the parallel corpus contained 28 million sentence pairs with 0.8 billion French words and 0.7 billion English words. 2.5 In total, we received 87 primary system submissions along with 42 secondary submissions. These were made available to participants in the system combination shared task. Based on feedback that we received on last year’s system combination task, we provided two additional resources to participants: Monolingual news corpora We have crawled the news sources that wer"
W09-0401,moore-2002-fast,0,\N,Missing
W09-0401,W09-0422,0,\N,Missing
W09-0401,W09-0421,0,\N,Missing
W09-0401,W07-0738,0,\N,Missing
W09-0401,W09-0427,0,\N,Missing
W09-0401,W08-0332,0,\N,Missing
W09-0401,W09-0425,0,\N,Missing
W09-0401,W09-0429,1,\N,Missing
W09-0401,P04-1077,0,\N,Missing
W09-0401,W09-0410,0,\N,Missing
W09-0401,W09-0415,0,\N,Missing
W09-0401,P07-1111,0,\N,Missing
W09-0401,W09-0412,0,\N,Missing
W09-0401,W06-3114,1,\N,Missing
W09-0401,W09-0414,0,\N,Missing
W09-0401,W10-1720,0,\N,Missing
W09-0401,W09-0405,0,\N,Missing
W09-0401,W09-0409,0,\N,Missing
W10-1703,W10-0701,1,0.75236,"Missing"
W10-1703,W10-1706,0,0.0242402,"Missing"
W10-1703,W10-1714,0,0.0249619,"Missing"
W10-1703,W10-1709,0,0.0199718,"Missing"
W10-1703,W06-3114,1,0.791425,"ion – This year we excluded Google translations from the systems used in system combination. In last year’s evaluation, the large margin between Google and many of the other systems meant that it was hard to improve on when combining systems. This year, the system combinations perform better than their component systems more often than last year. Introduction This paper presents the results of the shared tasks of the joint Workshop on statistical Machine Translation (WMT) and Metrics for MAchine TRanslation (MetricsMATR), which was held at ACL 2010. This builds on four previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007; Callison-Burch et al., 2008; Callison-Burch et al., 2009), and one previous MetricsMATR meeting (Przybocki et al., 2008). There were three shared tasks this year: a translation task between English and four other European languages, a task to combine the output of multiple machine translation systems, and a task to predict human judgments of translation quality using automatic evaluation metrics. The • Fewer rule-based systems – This year there were fewer rule-based systems submitted. In past years, University of Saarland compiled a large set of outputs from rule"
W10-1703,W10-1710,0,0.0338164,"Missing"
W10-1703,P07-2045,1,0.00952672,"chine translation. As with past years, all of the data, translations, and human judgments produced for our workshop are publicly available.2 We hope they form a valuable resource for research into statistical machine translation, system combination, and automatic evaluation of translation quality. 2 train language models, and development sets to tune parameters. Some statistics about the training materials are given in Figure 1. 2.3 To lower the barrier of entry for newcomers to the field, we provided two open source toolkits for phrase-based and parsing-based statistical machine translation (Koehn et al., 2007; Li et al., 2009). Overview of the shared translation and system combination tasks 2.4 Test data The test data for this year’s task was created by hiring people to translate news articles that were drawn from a variety of sources from midDecember 2009. A total of 119 articles were selected, in roughly equal amounts from a variety of Czech, English, French, German and Spanish news sites:3 2.5 System combination In total, we received 153 primary system submissions along with 28 secondary submissions. These were made available to participants in the system combination shared task. Based on feedb"
W10-1703,W10-1722,0,0.0405706,"Missing"
W10-1703,W09-0424,1,0.519668,"s with past years, all of the data, translations, and human judgments produced for our workshop are publicly available.2 We hope they form a valuable resource for research into statistical machine translation, system combination, and automatic evaluation of translation quality. 2 train language models, and development sets to tune parameters. Some statistics about the training materials are given in Figure 1. 2.3 To lower the barrier of entry for newcomers to the field, we provided two open source toolkits for phrase-based and parsing-based statistical machine translation (Koehn et al., 2007; Li et al., 2009). Overview of the shared translation and system combination tasks 2.4 Test data The test data for this year’s task was created by hiring people to translate news articles that were drawn from a variety of sources from midDecember 2009. A total of 119 articles were selected, in roughly equal amounts from a variety of Czech, English, French, German and Spanish news sites:3 2.5 System combination In total, we received 153 primary system submissions along with 28 secondary submissions. These were made available to participants in the system combination shared task. Based on feedback that we receiv"
W10-1703,W10-1723,0,0.0273443,"Missing"
W10-1703,W10-1718,1,0.767992,"Missing"
W10-1703,W10-1748,0,0.0371589,"Missing"
W10-1703,W10-1754,0,0.0648908,"Missing"
W10-1703,W10-1731,0,0.0201809,"Missing"
W10-1703,W10-1725,0,0.0253357,"Missing"
W10-1703,W10-1732,0,0.0225271,"Missing"
W10-1703,W10-1733,0,0.0311783,"Missing"
W10-1703,W10-1726,0,0.0139847,"Missing"
W10-1703,W10-1727,0,0.0169981,"Missing"
W10-1703,W10-1728,0,0.0374247,"Missing"
W10-1703,W10-1729,0,0.0349316,"Missing"
W10-1703,W10-1716,0,\N,Missing
W10-1703,W10-1715,1,\N,Missing
W10-1703,W10-1751,0,\N,Missing
W10-1703,W10-1755,0,\N,Missing
W10-1703,W10-1721,0,\N,Missing
W10-1703,W10-1719,0,\N,Missing
W10-1703,W10-1746,0,\N,Missing
W10-1703,W10-1750,0,\N,Missing
W10-1703,W10-1708,0,\N,Missing
W10-1703,W10-1717,0,\N,Missing
W10-1703,W09-0401,1,\N,Missing
W10-1703,W10-1724,0,\N,Missing
W10-1703,W10-1745,0,\N,Missing
W10-1703,W10-1711,0,\N,Missing
W10-1703,2010.iwslt-evaluation.22,0,\N,Missing
W10-1703,D09-1030,1,\N,Missing
W10-1703,W07-0718,1,\N,Missing
W10-1703,W10-1743,0,\N,Missing
W10-1703,W10-1749,0,\N,Missing
W10-1703,W10-1744,0,\N,Missing
W10-1703,W08-0309,1,\N,Missing
W10-1703,W10-1713,0,\N,Missing
W10-1703,W09-0426,0,\N,Missing
W10-1703,W10-1704,0,\N,Missing
W10-1703,W10-1720,0,\N,Missing
W10-1703,W10-1747,0,\N,Missing
W10-1703,W10-1712,0,\N,Missing
W10-1703,W10-1741,0,\N,Missing
W10-1703,W10-1753,0,\N,Missing
W10-1703,W10-1740,0,\N,Missing
W10-1703,W10-1730,0,\N,Missing
W10-1703,W10-1705,0,\N,Missing
W10-1703,W10-1742,0,\N,Missing
W10-1703,W10-1752,0,\N,Missing
W11-2103,W11-2134,0,0.0605238,"Missing"
W11-2103,W11-2137,0,0.0520272,"Missing"
W11-2103,W11-2138,0,0.0321994,"Missing"
W11-2103,W07-0718,1,0.805048,"el to publish a paper about their experience developing translation technology in response to the crisis (Lewis et al., 2011). They provided the data used in the Haitian Creole featured translation task. We hope that the introduction of this new dataset will provide a testbed for dealing with low resource languages and the informal language usage found in SMS messages. Introduction This paper presents the results of the shared tasks of the Workshop on statistical Machine Translation (WMT), which was held at EMNLP 2011. This workshop builds on five previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007; Callison-Burch et al., 2008; Callison-Burch et al., 2009; Callison-Burch et al., 2010). The workshops feature three shared tasks: a translation task between English and other languages, a task to combine the output of multiple machine translation systems, and a task to predict human judgments of translation quality using automatic evaluation metrics. The performance for each of these shared tasks is determined through a comprehensive human eval• Tunable metric shared task – We conducted a pilot of a new shared task to use evaluation metrics to tune the parameters of a machine translation sys"
W11-2103,W08-0309,1,0.779572,"heir experience developing translation technology in response to the crisis (Lewis et al., 2011). They provided the data used in the Haitian Creole featured translation task. We hope that the introduction of this new dataset will provide a testbed for dealing with low resource languages and the informal language usage found in SMS messages. Introduction This paper presents the results of the shared tasks of the Workshop on statistical Machine Translation (WMT), which was held at EMNLP 2011. This workshop builds on five previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007; Callison-Burch et al., 2008; Callison-Burch et al., 2009; Callison-Burch et al., 2010). The workshops feature three shared tasks: a translation task between English and other languages, a task to combine the output of multiple machine translation systems, and a task to predict human judgments of translation quality using automatic evaluation metrics. The performance for each of these shared tasks is determined through a comprehensive human eval• Tunable metric shared task – We conducted a pilot of a new shared task to use evaluation metrics to tune the parameters of a machine translation system. Although previous worksh"
W11-2103,W09-0401,1,0.532823,"anslation technology in response to the crisis (Lewis et al., 2011). They provided the data used in the Haitian Creole featured translation task. We hope that the introduction of this new dataset will provide a testbed for dealing with low resource languages and the informal language usage found in SMS messages. Introduction This paper presents the results of the shared tasks of the Workshop on statistical Machine Translation (WMT), which was held at EMNLP 2011. This workshop builds on five previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007; Callison-Burch et al., 2008; Callison-Burch et al., 2009; Callison-Burch et al., 2010). The workshops feature three shared tasks: a translation task between English and other languages, a task to combine the output of multiple machine translation systems, and a task to predict human judgments of translation quality using automatic evaluation metrics. The performance for each of these shared tasks is determined through a comprehensive human eval• Tunable metric shared task – We conducted a pilot of a new shared task to use evaluation metrics to tune the parameters of a machine translation system. Although previous workshops have shown evaluation met"
W11-2103,W10-1703,1,0.633993,"nse to the crisis (Lewis et al., 2011). They provided the data used in the Haitian Creole featured translation task. We hope that the introduction of this new dataset will provide a testbed for dealing with low resource languages and the informal language usage found in SMS messages. Introduction This paper presents the results of the shared tasks of the Workshop on statistical Machine Translation (WMT), which was held at EMNLP 2011. This workshop builds on five previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007; Callison-Burch et al., 2008; Callison-Burch et al., 2009; Callison-Burch et al., 2010). The workshops feature three shared tasks: a translation task between English and other languages, a task to combine the output of multiple machine translation systems, and a task to predict human judgments of translation quality using automatic evaluation metrics. The performance for each of these shared tasks is determined through a comprehensive human eval• Tunable metric shared task – We conducted a pilot of a new shared task to use evaluation metrics to tune the parameters of a machine translation system. Although previous workshops have shown evaluation metrics other than BLEU are more"
W11-2103,W11-2105,0,0.0708412,"Missing"
W11-2103,W11-2140,0,0.0631771,"Missing"
W11-2103,W11-2141,0,0.049057,"Missing"
W11-2103,W97-0409,0,0.103469,"o’s crowdsourcing translation efforts, the Microsoft Translator team developed a Haitian Creole statistical machine translation engine from scratch in a compressed timeframe (Lewis, 2010). Despite the impressive number of translations completed by volunteers, machine translation was viewed as a potentially useful tool for higher volume applications or to provide translations of English medical documents into Haitian Creole. The Microsoft Translator team quickly assembled parallel data from a number of sources, including Mission 4636 and from the archives of Carnegie Mellon’s DIPLOMAT project (Frederking et al., 1997). Through a series of rapid prototyping efforts, the team improved their system to deal with non-standard orthography, reduced pronouns, and SMS shorthand. They deployed a functional translation system to relief workers in the field in less than 5 days – impressive even when measured against previous rapid MT development efforts like DARPA’s surprise language exercise (Oard, 2003; Oard and Och, 2003). We were inspired by the efforts of Rob Munro and Will Lewis on translating Haitian Creole in the aftermath of the disaster, so we worked with them to create a featured task at WMT11. We thank the"
W11-2103,2003.mtsummit-papers.37,0,0.0310815,"nto Haitian Creole. The Microsoft Translator team quickly assembled parallel data from a number of sources, including Mission 4636 and from the archives of Carnegie Mellon’s DIPLOMAT project (Frederking et al., 1997). Through a series of rapid prototyping efforts, the team improved their system to deal with non-standard orthography, reduced pronouns, and SMS shorthand. They deployed a functional translation system to relief workers in the field in less than 5 days – impressive even when measured against previous rapid MT development efforts like DARPA’s surprise language exercise (Oard, 2003; Oard and Och, 2003). We were inspired by the efforts of Rob Munro and Will Lewis on translating Haitian Creole in the aftermath of the disaster, so we worked with them to create a featured task at WMT11. We thank them for generously sharing the data they assembled in their own efforts. We invited Rob Munro, Will Lewis, and Stephan Vogel to speak at the workshop on the topic of developing translation technology for future 27 crises, and they recorded their thoughts in an invited publication (Lewis et al., 2011). 3.1 Haitian Creole Data For the WMT11 featured translation task, we anonymized the SMS Haitian Creole"
W11-2103,P03-1021,0,0.527543,"an Creole. The Microsoft Translator team quickly assembled parallel data from a number of sources, including Mission 4636 and from the archives of Carnegie Mellon’s DIPLOMAT project (Frederking et al., 1997). Through a series of rapid prototyping efforts, the team improved their system to deal with non-standard orthography, reduced pronouns, and SMS shorthand. They deployed a functional translation system to relief workers in the field in less than 5 days – impressive even when measured against previous rapid MT development efforts like DARPA’s surprise language exercise (Oard, 2003; Oard and Och, 2003). We were inspired by the efforts of Rob Munro and Will Lewis on translating Haitian Creole in the aftermath of the disaster, so we worked with them to create a featured task at WMT11. We thank them for generously sharing the data they assembled in their own efforts. We invited Rob Munro, Will Lewis, and Stephan Vogel to speak at the workshop on the topic of developing translation technology for future 27 crises, and they recorded their thoughts in an invited publication (Lewis et al., 2011). 3.1 Haitian Creole Data For the WMT11 featured translation task, we anonymized the SMS Haitian Creole"
W11-2103,P02-1040,0,0.09575,"ETEOR -1.3- RANK MT E R ATER , MT E R ATER -P LUS MP 4 IBM 1, MP F, WMP F PARSECONF ROSE , ROSE - POS TESLA - B , TESLA - F, TESLA - M TINE BLEU TER Participant National Research Council Canada (Chen and Kuhn, 2011) Koc¸ University (Bicici and Yuret, 2011) Carnegie Mellon University (Denkowski and Lavie, 2011a) Columbia / ETS (Parton et al., 2011) DFKI (Popovi´c, 2011; Popovi´c et al., 2011) DFKI (Avramidis et al., 2011) The University of Sheffield (Song and Cohn, 2011) National University of Singapore (Dahlmeier et al., 2011) University of Wolverhampton (Rios et al., 2011) provided baseline (Papineni et al., 2002) provided baseline (Snover et al., 2006) Table 11: Participants in the evaluation shared task. For comparison purposes, we include the BLEU and TER metrics as baselines. - 15 SYSTEMS - 17 SYSTEMS EN - ES EN - FR AVERAGE W / O CZ - 22 SYSTEMS EN - DE AVERAGE - 10 SYSTEMS EN - CZ 6.1 System-level correlation for translation out of English TESLA - M .90 .95 .96 .94 TESLA - B .81 .90 .91 .87 MP F .72 .63 .87 .89 .78 .80 WMP F .72 .61 .87 .89 .77 .79 MP 4 IBM 1 -.76 -.91 -.71 -.61 .75 .74 ROSE .65 .41 .90 .86 .71 .73 .70 .72 BLEU .65 .44 .87 .86 AMBER- TI .56 .54 .88 .84 .70 .75 AMBER .56 .53 .87 ."
W11-2103,W11-2111,0,0.0968425,"th other MT evaluation metrics and heuristics that take the reference translations into account. Please refer to the proceedings for papers providing detailed descriptions of all of the metrics. Metric IDs AMBER , AMBER - NL , AMBER - IT F15, F15 G 3 METEOR -1.3- ADQ , METEOR -1.3- RANK MT E R ATER , MT E R ATER -P LUS MP 4 IBM 1, MP F, WMP F PARSECONF ROSE , ROSE - POS TESLA - B , TESLA - F, TESLA - M TINE BLEU TER Participant National Research Council Canada (Chen and Kuhn, 2011) Koc¸ University (Bicici and Yuret, 2011) Carnegie Mellon University (Denkowski and Lavie, 2011a) Columbia / ETS (Parton et al., 2011) DFKI (Popovi´c, 2011; Popovi´c et al., 2011) DFKI (Avramidis et al., 2011) The University of Sheffield (Song and Cohn, 2011) National University of Singapore (Dahlmeier et al., 2011) University of Wolverhampton (Rios et al., 2011) provided baseline (Papineni et al., 2002) provided baseline (Snover et al., 2006) Table 11: Participants in the evaluation shared task. For comparison purposes, we include the BLEU and TER metrics as baselines. - 15 SYSTEMS - 17 SYSTEMS EN - ES EN - FR AVERAGE W / O CZ - 22 SYSTEMS EN - DE AVERAGE - 10 SYSTEMS EN - CZ 6.1 System-level correlation for translation out"
W11-2103,W11-2153,0,0.0551069,"Missing"
W11-2103,W11-2110,0,0.0542786,"Missing"
W11-2103,W11-2154,0,0.0525812,"Missing"
W11-2103,W11-2112,0,0.042178,"Missing"
W11-2103,W11-2158,0,0.072375,"Missing"
W11-2103,W11-2120,0,0.0448017,"Missing"
W11-2103,2006.amta-papers.25,0,0.129527,"-P LUS MP 4 IBM 1, MP F, WMP F PARSECONF ROSE , ROSE - POS TESLA - B , TESLA - F, TESLA - M TINE BLEU TER Participant National Research Council Canada (Chen and Kuhn, 2011) Koc¸ University (Bicici and Yuret, 2011) Carnegie Mellon University (Denkowski and Lavie, 2011a) Columbia / ETS (Parton et al., 2011) DFKI (Popovi´c, 2011; Popovi´c et al., 2011) DFKI (Avramidis et al., 2011) The University of Sheffield (Song and Cohn, 2011) National University of Singapore (Dahlmeier et al., 2011) University of Wolverhampton (Rios et al., 2011) provided baseline (Papineni et al., 2002) provided baseline (Snover et al., 2006) Table 11: Participants in the evaluation shared task. For comparison purposes, we include the BLEU and TER metrics as baselines. - 15 SYSTEMS - 17 SYSTEMS EN - ES EN - FR AVERAGE W / O CZ - 22 SYSTEMS EN - DE AVERAGE - 10 SYSTEMS EN - CZ 6.1 System-level correlation for translation out of English TESLA - M .90 .95 .96 .94 TESLA - B .81 .90 .91 .87 MP F .72 .63 .87 .89 .78 .80 WMP F .72 .61 .87 .89 .77 .79 MP 4 IBM 1 -.76 -.91 -.71 -.61 .75 .74 ROSE .65 .41 .90 .86 .71 .73 .70 .72 BLEU .65 .44 .87 .86 AMBER- TI .56 .54 .88 .84 .70 .75 AMBER .56 .53 .87 .84 .70 .74 AMBER- NL .56 .45 .88 .83 .68"
W11-2103,W11-2113,0,0.0828194,"Missing"
W11-2103,W11-2159,0,0.0580674,"Missing"
W11-2103,W11-2155,0,\N,Missing
W11-2103,2010.amta-papers.7,1,\N,Missing
W11-2103,W11-2152,0,\N,Missing
W11-2103,W11-2150,0,\N,Missing
W11-2103,W10-1719,0,\N,Missing
W11-2103,W10-1718,1,\N,Missing
W11-2103,C08-1109,0,\N,Missing
W11-2103,W11-2144,0,\N,Missing
W11-2103,W11-2148,0,\N,Missing
W11-2103,D11-1035,0,\N,Missing
W11-2103,W11-2147,0,\N,Missing
W11-2103,W11-2161,0,\N,Missing
W11-2103,W11-2108,0,\N,Missing
W11-2103,2010.eamt-1.37,0,\N,Missing
W11-2103,W11-2143,0,\N,Missing
W11-2103,W11-2139,0,\N,Missing
W11-2103,P07-2045,1,\N,Missing
W11-2103,W09-0415,0,\N,Missing
W11-2103,W11-2142,0,\N,Missing
W11-2103,W10-1711,0,\N,Missing
W11-2103,2010.iwslt-evaluation.22,0,\N,Missing
W11-2103,2005.eamt-1.12,0,\N,Missing
W11-2103,W11-2146,0,\N,Missing
W11-2103,W11-2104,0,\N,Missing
W11-2103,W11-2109,0,\N,Missing
W11-2103,W06-3114,1,\N,Missing
W11-2103,W05-0904,0,\N,Missing
W11-2103,W11-2157,0,\N,Missing
W11-2103,W11-2156,0,\N,Missing
W11-2103,W11-2151,0,\N,Missing
W11-2103,W11-2136,0,\N,Missing
W11-2103,W11-2135,0,\N,Missing
W11-2103,W11-2118,0,\N,Missing
W11-2103,W11-2145,0,\N,Missing
W11-2103,W11-2163,0,\N,Missing
W11-2103,W11-2160,1,\N,Missing
W11-2103,W09-0407,0,\N,Missing
W11-2103,D08-1076,0,\N,Missing
W11-2103,W11-2149,0,\N,Missing
W11-2103,W11-2117,0,\N,Missing
W11-2103,W11-2107,0,\N,Missing
W11-2103,W11-2164,0,\N,Missing
W11-2103,W11-2121,0,\N,Missing
W11-2103,W11-2119,0,\N,Missing
W11-2103,W11-2106,0,\N,Missing
W11-2103,W11-2116,0,\N,Missing
W12-3102,W10-1703,1,0.557844,"Missing"
W12-3102,W11-2103,1,0.709065,"- ANNOTATOR AGREEMENT P (A) 0.567 0.576 0.595 0.598 0.540 0.504 0.568 0.519 0.568 0.601 P (A) 0.660 0.566 0.733 0.732 0.792 0.566 0.719 0.634 0.671 0.722 P (E) 0.405 0.383 0.401 0.394 0.408 0.398 0.406 0.388 0.396 0.362 κ 0.272 0.312 0.323 0.336 0.222 0.176 0.272 0.214 0.284 0.375 P (E) 0.405 0.383 0.401 0.394 0.408 0.398 0.406 0.388 0.396 0.362 κ 0.428 0.296 0.554 0.557 0.648 0.279 0.526 0.401 0.455 0.564 Table 3: Inter- and intra-annotator agreement rates for the WMT12 manual evaluation. For comparison, the WMT11 rows contain the results from the European languages individual systems task (Callison-Burch et al. (2011), Table 7). Agreement rates vary widely across languages. For inter-annotator agreements, the range is 0.176 to 0.336, while intra-annotator agreement ranges from 0.279 to 0.648. We note in particular the low agreement rates among judgments in the English-Spanish task, which is reflected in the relative lack of statistical significance Table 4. The agreement rates for this year were somewhat lower than last year. 3.3 Results of the Translation Task We used the results of the manual evaluation to analyze the translation quality of the different systems that were submitted to the workshop. In ou"
W12-3102,D09-1030,1,0.149941,"Missing"
W12-3102,W12-3103,0,0.0434403,"Missing"
W12-3102,W11-2107,0,0.311039,"Missing"
W12-3102,W12-3131,0,0.0259292,"Missing"
W12-3102,W12-3133,0,0.0166865,"Missing"
W12-3102,W12-3134,1,0.0973948,"Missing"
W12-3102,W12-3135,0,0.0202653,"Missing"
W12-3102,W12-3111,0,0.0311487,"Missing"
W12-3102,W12-3136,0,0.0403021,"Missing"
W12-3102,W12-3112,0,0.176358,"Missing"
W12-3102,2011.eamt-1.32,0,0.0209858,"with only two linear equations. System “SDLLW SVM” uses a 20-feature set and an SVM epsilon regression model with radial basis function kernel with parameters C, gamma, and epsilon tuned on a development set (305 training instances). The model was trained with 10-fold cross validation and the tuning process was restarted several times using different starting points and step sizes to avoid overfitting. The final model was selected based on its performance on the development set and the number of support vectors. UU (R, S): System “UU best” uses the 17 baseline features, plus 82 features from Hardmeier (2011) (with some redundancy and some overlap with baseline features), and constituency trees over input sentences generated by the Stanford parser and dependency trees over both input and output sentences generated by the MaltParser. System “UU bltk” uses only the 17 baseline features plus constituency and dependency trees as above. The machine learning component in both cases is SVM regression (SVMlight software). For the ranking task, 29 the ranking induced by the regression output is used. The system uses polynomial kernels of degree 2 (UU best) and 3 (UU bltk) as well as two different types of"
W12-3102,W12-3137,0,0.0533752,"Missing"
W12-3102,W12-3106,0,0.0413119,"Missing"
W12-3102,W12-3145,0,0.0522389,"Missing"
W12-3102,W12-3146,0,0.0998404,"Missing"
W12-3102,W09-0415,0,0.0172168,"Missing"
W12-3102,W12-3150,1,0.188192,"Missing"
W12-3102,W12-3119,0,0.030516,"Missing"
W12-3102,W12-3151,0,0.046244,"Missing"
W12-3102,W12-3117,0,\N,Missing
W12-3102,W12-3138,0,\N,Missing
W12-3102,W12-3130,0,\N,Missing
W12-3102,W12-3148,0,\N,Missing
W12-3102,W12-3114,0,\N,Missing
W12-3102,W12-3107,0,\N,Missing
W12-3102,W12-3139,1,\N,Missing
W12-3102,W11-2108,0,\N,Missing
W12-3102,W09-0401,1,\N,Missing
W12-3102,W12-3140,0,\N,Missing
W12-3102,W12-3132,0,\N,Missing
W12-3102,W12-3109,0,\N,Missing
W12-3102,W10-1711,0,\N,Missing
W12-3102,2010.iwslt-evaluation.22,0,\N,Missing
W12-3102,W07-0718,1,\N,Missing
W12-3102,W06-3114,1,\N,Missing
W12-3102,2009.eamt-1.5,1,\N,Missing
W12-3102,W12-3144,0,\N,Missing
W12-3102,W11-2158,0,\N,Missing
W12-3102,W12-3110,1,\N,Missing
W12-3102,W08-0309,1,\N,Missing
W12-3102,W12-3105,0,\N,Missing
W12-3102,W12-3149,0,\N,Missing
W12-3102,2011.eamt-1.12,1,\N,Missing
W12-3102,W04-3250,1,\N,Missing
W12-3102,W12-3142,0,\N,Missing
W12-3102,W11-2113,0,\N,Missing
W12-3102,W12-3143,0,\N,Missing
W12-3102,W11-2145,0,\N,Missing
W12-3102,W12-3147,0,\N,Missing
W12-3102,W12-3101,0,\N,Missing
W12-3102,W12-3115,0,\N,Missing
W12-3102,W11-2101,0,\N,Missing
W12-3102,W12-3113,0,\N,Missing
W12-3102,W12-3104,0,\N,Missing
W12-3102,W12-3108,0,\N,Missing
W12-3102,W12-3118,1,\N,Missing
W13-2201,W13-2205,0,0.0583032,"Missing"
W13-2201,S13-1034,0,0.0277746,"Missing"
W13-2201,C12-1008,0,0.0091484,"ignments is used to extract quantitative (amount and distribution of the alignments) and qualitative (importance of the aligned terms) features under the assumption that alignment information can help tasks where sentencelevel semantic relations need to be identified (Souza et al., 2013). Three similar EnglishSpanish systems are built and used to provide pseudo-references (Soricut et al., 2012) and back-translations, from which automatic MT evaluation metrics could be computed and used as features. DFKI (T1.2, T1.3): DFKI’s submission for Task 1.2 was based on decomposing rankings into pairs (Avramidis, 2012), where the best system for each pair was predicted with Logistic Regression (LogReg). For GermanEnglish, LogReg was trained with Stepwise Feature Selection (Hosmer, 1989) on two feature sets: Feature Set 24 includes basic counts augmented with PCFG parsing features (number of VPs, alternative parses, parse probability) on both source and target sentences (Avramidis et al., 2011), and pseudo-reference METEOR score; the most successful set, Feature Set 33 combines those 24 features with the 17 baseline features. For English-Spanish, LogReg was used with L2 Regularisation (Lin et al., 2007) and"
W13-2201,W13-2206,0,0.0913052,"ool of Data Analysis (Borisov et al., 2013) Carnegie Mellon University (Ammar et al., 2013) CMU - TREE - TO - TREE CU - BOJAR , Charles University in Prague (Bojar et al., 2013) CU - DEPFIX , CU - TAMCHYNA CU - KAREL , CU - ZEMAN CU - PHRASEFIX , Charles University in Prague (B´ılek and Zeman, 2013) Charles University in Prague (Galuˇscˇ a´ kov´a et al., 2013) CU - TECTOMT DCU DCU - FDA DCU - OKITA DESRT ITS - LATL JHU KIT LIA LIMSI MES -* OMNIFLUENT PROMT QCRI - MES QUAERO RWTH SHEF STANFORD TALP - UPC TUBITAK UCAM UEDIN , Dublin City University (Rubino et al., 2013a) Dublin City University (Bicici, 2013a) Dublin City University (Okita et al., 2013) Universit`a di Pisa (Miceli Barone and Attardi, 2013) University of Geneva Johns Hopkins University (Post et al., 2013) Karlsruhe Institute of Technology (Cho et al., 2013) Universit´e d’Avignon (Huet et al., 2013) LIMSI (Allauzen et al., 2013) Munich / Edinburgh / Stuttgart (Durrani et al., 2013a; Weller et al., 2013) SAIC (Matusov and Leusch, 2013) PROMT Automated Translations Solutions Qatar / Munich / Edinburgh / Stuttgart (Sajjad et al., 2013) QUAERO (Peitz et al., 2013a) RWTH Aachen (Peitz et al., 2013b) University of Sheffield Stanford Univ"
W13-2201,W13-2240,0,0.0186069,"Missing"
W13-2201,W13-2242,0,0.220429,"ool of Data Analysis (Borisov et al., 2013) Carnegie Mellon University (Ammar et al., 2013) CMU - TREE - TO - TREE CU - BOJAR , Charles University in Prague (Bojar et al., 2013) CU - DEPFIX , CU - TAMCHYNA CU - KAREL , CU - ZEMAN CU - PHRASEFIX , Charles University in Prague (B´ılek and Zeman, 2013) Charles University in Prague (Galuˇscˇ a´ kov´a et al., 2013) CU - TECTOMT DCU DCU - FDA DCU - OKITA DESRT ITS - LATL JHU KIT LIA LIMSI MES -* OMNIFLUENT PROMT QCRI - MES QUAERO RWTH SHEF STANFORD TALP - UPC TUBITAK UCAM UEDIN , Dublin City University (Rubino et al., 2013a) Dublin City University (Bicici, 2013a) Dublin City University (Okita et al., 2013) Universit`a di Pisa (Miceli Barone and Attardi, 2013) University of Geneva Johns Hopkins University (Post et al., 2013) Karlsruhe Institute of Technology (Cho et al., 2013) Universit´e d’Avignon (Huet et al., 2013) LIMSI (Allauzen et al., 2013) Munich / Edinburgh / Stuttgart (Durrani et al., 2013a; Weller et al., 2013) SAIC (Matusov and Leusch, 2013) PROMT Automated Translations Solutions Qatar / Munich / Edinburgh / Stuttgart (Sajjad et al., 2013) QUAERO (Peitz et al., 2013a) RWTH Aachen (Peitz et al., 2013b) University of Sheffield Stanford Univ"
W13-2201,W13-2207,0,0.0357036,"Missing"
W13-2201,W11-2104,0,0.00810596,"Missing"
W13-2201,P10-2016,1,0.826191,"Missing"
W13-2201,W13-2208,1,0.743011,"Missing"
W13-2201,P11-1022,0,0.112625,"the intended application. In Task 2 we tested binary word-level classification in a post-editing setting. If such annotation is presented through a user interface we imagine that words marked as incorrect would be hidden from the editor, highlighted as possibly wrong or that a list of alternatives would we generated. With respect to the poor improvements over trivial baselines, we consider that the results for word-level prediction could be mostly connected to limitations of the datasets provided, which are very small for word-level prediction, as compared to successful previous work such as (Bach et al., 2011). Despite the limited amount of training data, several systems were able to predict dubious words (binary variant of the task), showing that this can be a promising task. Extending the granularity even further by predicting the actual editing action necessary for a word yielded less positive results than the binary setting. We cannot directly compare sentence- and word-level results. However, since sentence-level predictions can benefit from more information available and therefore more signal on which the prediction is based, the natural conclusion is that, if there is a choice in the predict"
W13-2201,W13-2209,0,0.034542,"Missing"
W13-2201,W13-2241,1,0.0862471,"Missing"
W13-2201,W07-0718,1,0.697635,"to five alternative translations for a given source sentence, prediction of post-editing time for a sentence, and prediction of word-level scores for a given translation (correct/incorrect and types of edits). The datasets included English-Spanish and GermanEnglish news translations produced by a number of machine translation systems. This marks the second year we have conducted this task. Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at ACL 2013. This workshop builds on seven previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012). This year we conducted three official tasks: a translation task, a human evaluation of translation results, and a quality estimation task.1 In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Spanish, and Russian. The Russian translation tasks were new this year, and were also the most popular. The system outputs for each task were evaluated both automatically and manually."
W13-2201,W11-2103,1,0.5768,"Missing"
W13-2201,W13-2243,1,0.754887,"Missing"
W13-2201,W13-2213,0,0.0301468,"Missing"
W13-2201,P05-1022,0,0.0401846,"parses of the source and target sentence, the positions of the phrases with the lowest and highest probability and future cost estimate in the translation, the counts of phrases in the decoding graph whose probability or whether the future cost estimate is higher/lower than their standard deviation, counts of verbs and determiners, etc. The second submission (pls8) was trained with Partial Least Squares regression (Stone and Brooks, 1990) including more glass-box features. The following NLP tools were used in feature extraction: the Brown English Wall-StreetJournal-trained statistical parser (Charniak and Johnson, 2005), a Lexical Functional Grammar parser (XLE), together with a hand-crafted Lexical Functional Grammar, the English ParGram grammar (Kaplan et al., 2004), and the TreeTagger part-of-speech tagger (Schmidt, 1994) with off-the-shelf publicly available pre-trained tagging models for English and Spanish. For pseudoreference features, the Bing, Moses and Systran translation systems were used. The Mallet toolkit (McCallum, 2002) was used to build the topic models and features based on a grammar checker were extracted with LanguageTool.16 FBK-Uedin (T1.1, T1.3): The submissions explored features built"
W13-2201,W13-2210,0,0.0366902,"Missing"
W13-2201,W13-2212,1,0.1814,"Missing"
W13-2201,W13-2214,0,0.0302242,"Missing"
W13-2201,W13-2217,0,0.0313645,"Missing"
W13-2201,W13-2215,0,0.0174795,"Missing"
W13-2201,W13-2253,0,0.055006,"Missing"
W13-2201,W13-2246,0,0.0627442,"Missing"
W13-2201,N06-1058,0,0.0556187,"of the official test set size, and provide 4311 unique references. On average, one sentence in our set has 2.94±2.17 unique reference translations. Table 10 provides a histogram. It is well known that automatic MT evaluation methods perform better with more references, because a single one may not confirm a correct part of MT output. This issue is more severe for morphologically rich languages like Czech where about 1/3 of MT output was correct but not confirmed by the reference (Bojar et al., 2010). Advanced evaluation methods apply paraphrasing to smooth out some of the lexical divergence (Kauchak and Barzilay, 2006; Snover et al., 2009; Denkowski and Lavie, 2010). Simpler techniques such as lemmatizing are effective for morphologically rich languages (Tantug et al., 2008; Kos and Bojar, 2009) but they will lose resolution once the systems start performing generally well. WMTs have taken the stance that a big enough test set with just a single reference should compensate for the lack of other references. We use our post-edited reference translations to check this assumption for BLEU and NIST as implemented in mteval-13a (international tokenization switched on, which is not the default setting). We run ma"
W13-2201,W13-2248,0,0.0488662,"Missing"
W13-2201,2012.iwslt-papers.5,1,0.674636,"ts are somewhat artificially more diverse; in narrow domains, source sentences can repeat and even appear verbatim in the training data, and in natural test sets with multiple references, short sentences can receive several identical translations. For each probe, we measure the Spearman’s rank correlation coefficient ρ of the ranks proposed by BLEU or NIST and the manual ranks. We use the same implementation as applied in the WMT13 Shared Metrics Task (Mach´acˇ ek and Bojar, 2013). Note that the WMT13 metrics task still uses the WMT12 evaluation method ignoring ties, not the expected wins. As Koehn (2012) shows, the two methods do not differ much. Overall, the correlation is strongly impacted by Figure 5: Correlation of BLEU and WMT13 manual ranks for English→Czech translation Figure 6: Correlation of NIST and WMT13 manual ranks for English→Czech translation the particular choice of test sentences and reference translations. By picking sentences randomly, similarly or equally sized test sets can reach different correlations. Indeed, e.g. for a test set of about 1500 distinct sentences selected from the 3000-sentence official test set (1 reference translation), we obtain correlations for BLEU b"
W13-2201,W13-2202,1,0.761984,"Missing"
W13-2201,W06-3114,1,0.635019,"ntence, ranking of up to five alternative translations for a given source sentence, prediction of post-editing time for a sentence, and prediction of word-level scores for a given translation (correct/incorrect and types of edits). The datasets included English-Spanish and GermanEnglish news translations produced by a number of machine translation systems. This marks the second year we have conducted this task. Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at ACL 2013. This workshop builds on seven previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012). This year we conducted three official tasks: a translation task, a human evaluation of translation results, and a quality estimation task.1 In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Spanish, and Russian. The Russian translation tasks were new this year, and were also the most popular. The system outputs for each task were evaluated bot"
W13-2201,W13-2219,0,0.0360343,"Missing"
W13-2201,W13-2220,0,0.017834,"Missing"
W13-2201,W13-2255,0,0.0136612,"Missing"
W13-2201,W12-3113,0,0.012942,"Missing"
W13-2201,W13-2247,0,0.0208658,"Missing"
W13-2201,W13-2221,1,0.762444,"Missing"
W13-2201,W13-2228,0,0.0270572,"Missing"
W13-2201,W13-2224,0,0.0611782,"Missing"
W13-2201,2013.mtsummit-papers.21,1,0.577491,"ve learning to reduce the training set size (and therefore the annotation effort). The initial set features contains all black box and glass box features available within the Q U E ST framework (Specia et al., 2013) for the dataset at hand (160 in total for Task 1.1, and 80 for Task 1.3). The query selection strategy for active learning is based on the informativeness of the instances using Information Density, a measure that leverages between the variance among instances and how dense the region (in the feature space) where the instance is located is. To perform feature selection, following (Shah et al., 2013) features are ranked by the Gaussian Process 23 algorithm according to their learned length scales, which can be interpreted as the relevance of such feature for the model. This information was used for feature selection by discarding the lowest ranked (least useful) ones. based on empirical results found in (Shah et al., 2013), the top 25 features for both models were selected and used to retrain the same regression algorithm. Semantic Roles could bring marginally better accuracy. TCD-CNGL (T1.1) and TCD-DCU-CNGL (T1.3): The system is based on features which are commonly used for style classi"
W13-2201,2012.amta-papers.13,0,0.0208135,"Missing"
W13-2201,W13-2250,0,0.0320711,"Missing"
W13-2201,W13-2225,0,0.0376682,"Missing"
W13-2201,P13-1135,1,0.207781,"Missing"
W13-2201,W13-2226,1,0.759686,"Missing"
W13-2201,2006.amta-papers.25,0,0.295541,"ation Based on the data of Task 1.3, we define Task 2, a word-level annotation task for which participants are asked to produce a label for each token that indicates whether the word should be changed by a post-editor or kept in the final translation. We consider the following two sets of labels for prediction: Sentence-level Quality Estimation Task 1.1 Predicting Post-editing Distance This task is similar to the quality estimation task in WMT12, but with one important difference in the scoring variant: instead of using the post-editing effort scores in the [1, 2, 3, 4, 5] range, we use HTER (Snover et al., 2006) as quality score. This score is to be interpreted as the minimum edit distance between the machine translation and its manually post-edited version, and its range is [0, 1] (0 when no edit needs to be made, and 1 when all words need to be edited). Two variants of the results could be submitted in the shared task: • Binary classification: a keep/change label, the latter meaning that the token should be corrected in the post-editing process. • Multi-class classification: a label specifying the edit action that should be performed on the token (keep as is, delete, or substitute). 6.3 Datasets Ta"
W13-2201,W13-2227,0,0.0389834,"Missing"
W13-2201,W09-0441,0,0.0271208,"ze, and provide 4311 unique references. On average, one sentence in our set has 2.94±2.17 unique reference translations. Table 10 provides a histogram. It is well known that automatic MT evaluation methods perform better with more references, because a single one may not confirm a correct part of MT output. This issue is more severe for morphologically rich languages like Czech where about 1/3 of MT output was correct but not confirmed by the reference (Bojar et al., 2010). Advanced evaluation methods apply paraphrasing to smooth out some of the lexical divergence (Kauchak and Barzilay, 2006; Snover et al., 2009; Denkowski and Lavie, 2010). Simpler techniques such as lemmatizing are effective for morphologically rich languages (Tantug et al., 2008; Kos and Bojar, 2009) but they will lose resolution once the systems start performing generally well. WMTs have taken the stance that a big enough test set with just a single reference should compensate for the lack of other references. We use our post-edited reference translations to check this assumption for BLEU and NIST as implemented in mteval-13a (international tokenization switched on, which is not the default setting). We run many probes, randomly p"
W13-2201,W13-2230,0,0.0166145,"Missing"
W13-2201,W12-3118,1,0.609118,"1.1, T1.3): The submissions explored features built on MT engine resources including automatic word alignment, n-best candidate translation lists, back-translations and word posterior probabilities. Information about word alignments is used to extract quantitative (amount and distribution of the alignments) and qualitative (importance of the aligned terms) features under the assumption that alignment information can help tasks where sentencelevel semantic relations need to be identified (Souza et al., 2013). Three similar EnglishSpanish systems are built and used to provide pseudo-references (Soricut et al., 2012) and back-translations, from which automatic MT evaluation metrics could be computed and used as features. DFKI (T1.2, T1.3): DFKI’s submission for Task 1.2 was based on decomposing rankings into pairs (Avramidis, 2012), where the best system for each pair was predicted with Logistic Regression (LogReg). For GermanEnglish, LogReg was trained with Stepwise Feature Selection (Hosmer, 1989) on two feature sets: Feature Set 24 includes basic counts augmented with PCFG parsing features (number of VPs, alternative parses, parse probability) on both source and target sentences (Avramidis et al., 2011"
W13-2201,P13-2135,0,0.148019,"the difference between selecting the best or the worse translation. 19 ID CMU CNGL DCU DCU-SYMC DFKI FBK-UEdin LIG LIMSI LORIA SHEF TCD-CNGL TCD-DCU-CNGL UMAC UPC Participating team Carnegie Mellon University, USA (Hildebrand and Vogel, 2013) Centre for Next Generation Localization, Ireland (Bicici, 2013b) Dublin City University, Ireland (Almaghout and Specia, 2013) Dublin City University & Symantec, Ireland (Rubino et al., 2013b) German Research Centre for Artificial Intelligence, Germany (Avramidis and Popovic, 2013) Fondazione Bruno Kessler, Italy & University of Edinburgh, UK (Camargo de Souza et al., 2013) Laboratoire d’Informatique Grenoble, France (Luong et al., 2013) Laboratoire d’Informatique pour la M´ecanique et les Sciences de l’Ing´enieur, France (Singh et al., 2013) Lorraine Laboratory of Research in Computer Science and its Applications, France (Langlois and Smaili, 2013) University of Sheffield, UK (Beck et al., 2013) Trinity College Dublin & CNGL, Ireland (Moreau and Rubino, 2013) Trinity College Dublin, Dublin City University & CNGL, Ireland (Moreau and Rubino, 2013) University of Macau, China (Han et al., 2013) Universitat Politecnica de Catalunya, Spain (Formiga et al., 2013b) Ta"
W13-2201,2011.eamt-1.12,1,0.741789,"ar (CCG) features: CCG supertag language model perplexity and log probability, the number of maximal CCG constituents in the translation output which are the highestprobability minimum number of CCG constituents that span the translation output, the percentage of CCG argument mismatches between each subsequent CCG supertags, the percentage of CCG argument mismatches between each subsequent CCG maximal categories and the minimum number of phrases detected in the translation output. A second submission uses the aforementioned CCG features combined with 80 features from Q U E ST as described in (Specia, 2011). For the CCG features, the C&C parser was used to parse the translation output. Moses was used to build the phrase table from the SMT training corpus with maximum phrase length set to 7. The language model of supertags was built using the SRILM toolkit. As learning algorithm, Logistic Regression as provided by the SCIKIT- LEARN toolkit was used. The training data was prepared by converting each ranking of translation outputs to a set of pairwise comparisons according to the approach proposed by Avramidis et al. (2011). The rankings were generated back from pairwise comparisons predicted by th"
W13-2201,P13-4014,1,0.118757,"Missing"
W13-2201,W13-2229,0,0.0371208,"Missing"
W13-2201,tantug-etal-2008-bleu,0,0.0131442,"a histogram. It is well known that automatic MT evaluation methods perform better with more references, because a single one may not confirm a correct part of MT output. This issue is more severe for morphologically rich languages like Czech where about 1/3 of MT output was correct but not confirmed by the reference (Bojar et al., 2010). Advanced evaluation methods apply paraphrasing to smooth out some of the lexical divergence (Kauchak and Barzilay, 2006; Snover et al., 2009; Denkowski and Lavie, 2010). Simpler techniques such as lemmatizing are effective for morphologically rich languages (Tantug et al., 2008; Kos and Bojar, 2009) but they will lose resolution once the systems start performing generally well. WMTs have taken the stance that a big enough test set with just a single reference should compensate for the lack of other references. We use our post-edited reference translations to check this assumption for BLEU and NIST as implemented in mteval-13a (international tokenization switched on, which is not the default setting). We run many probes, randomly picking the test set size (number of distinct sentences) and the number of distinct references per sentence. Note that such test sets are s"
W13-2201,W10-1751,0,\N,Missing
W13-2201,de-marneffe-etal-2006-generating,0,\N,Missing
W13-2201,W13-2249,0,\N,Missing
W13-2201,N04-1013,0,\N,Missing
W13-2201,W02-1001,0,\N,Missing
W13-2201,W12-3102,1,\N,Missing
W13-2201,P12-3024,0,\N,Missing
W13-2201,W09-0401,1,\N,Missing
W13-2201,W13-2222,0,\N,Missing
W13-2201,W10-1711,0,\N,Missing
W13-2201,2010.iwslt-evaluation.22,0,\N,Missing
W13-2201,W10-1703,1,\N,Missing
W13-2201,W08-0309,1,\N,Missing
W13-2201,W13-2218,0,\N,Missing
W13-2201,P13-1004,1,\N,Missing
W13-2201,2013.mtsummit-papers.9,0,\N,Missing
W13-2201,W13-2216,1,\N,Missing
W13-2201,W13-2244,0,\N,Missing
W13-3010,W06-3123,0,0.0223293,"han the segments themselves. 3. If {Ei } is a countable collection of sets in B, then so is their union ∪i Ei . Condition 1 guarantees that B is non-empty and Conditions 2 and 3 say that B is closed under complementation and countable unions respectively. The pair (X, B) is called a measurable space. A function f : B → [0, ∞) is called a measure if the following conditions hold: Several phrase-level generative models have been proposed, almost all relying on multinomial distributions for the phrase alignments (Marcu and Wong, 2002; Zhang et al., 2003; Deng and Byrne 2005; DeNero et al., 2006; Birch et al., 2006). This is a consequence of treating alignments as functions rather than partitions. Word alignment and phrase extraction via Inversion Transduction Grammars (Wu, 1997), is a linguistically motivated method that relies on simultaneous parsing of source and target sentences (DeNero and Klein, 2010; Cherry and Lin 2007; Neubig et al., 2012). The partition probabilities we introduced in Section 5.2 share the same tree structure discussed in (Dennis III, 1991), which has found applications in Information Retrieval (Haffari and Teh, 2009). 7 1. f (∅) = 0. 2. If {Ei } is a countable collection of pai"
W13-3010,W07-0403,0,0.0386399,"Missing"
W13-3010,W99-0604,0,0.832267,"y-product of this model is a derivation of probability mass functions for random partitions. These are realized as cases of constrained, biased sampling without replacement and we provide an exact formula for the probability of a segmentation of a sentence. 1 These models, known as IBM models, are based on standard machine learning techniques. Their output is a matrix of word alignments for each sentence pair in the training data. These word alignments provide the input for later approaches that construct phrase-level translation rules which may (Wu, 1997; Yamada and Knight, 2001) or may not (Och et al., 1999; Marcu and Wong, 2002) rely on linguistic information. The method developed in (Och et al., 1999), known as the consistency method, is a simple yet effective method that has become the standard way of extracting (source, target)-pairs of phrases as translation rules. The development of consistency has been done entirely on empirical evidence and it has thus been termed a heuristic. In this work we show that the method of (Och et al., 1999) actually encodes a particular type of structural information induced by the word alignment matrices. Moreover, we show that the way in which statistics are"
W13-3010,W06-3105,0,0.0232812,"n each pair, rather than the segments themselves. 3. If {Ei } is a countable collection of sets in B, then so is their union ∪i Ei . Condition 1 guarantees that B is non-empty and Conditions 2 and 3 say that B is closed under complementation and countable unions respectively. The pair (X, B) is called a measurable space. A function f : B → [0, ∞) is called a measure if the following conditions hold: Several phrase-level generative models have been proposed, almost all relying on multinomial distributions for the phrase alignments (Marcu and Wong, 2002; Zhang et al., 2003; Deng and Byrne 2005; DeNero et al., 2006; Birch et al., 2006). This is a consequence of treating alignments as functions rather than partitions. Word alignment and phrase extraction via Inversion Transduction Grammars (Wu, 1997), is a linguistically motivated method that relies on simultaneous parsing of source and target sentences (DeNero and Klein, 2010; Cherry and Lin 2007; Neubig et al., 2012). The partition probabilities we introduced in Section 5.2 share the same tree structure discussed in (Dennis III, 1991), which has found applications in Information Retrieval (Haffari and Teh, 2009). 7 1. f (∅) = 0. 2. If {Ei } is a counta"
W13-3010,J97-3002,0,0.280433,"es rise to a phrase-based generative model. A by-product of this model is a derivation of probability mass functions for random partitions. These are realized as cases of constrained, biased sampling without replacement and we provide an exact formula for the probability of a segmentation of a sentence. 1 These models, known as IBM models, are based on standard machine learning techniques. Their output is a matrix of word alignments for each sentence pair in the training data. These word alignments provide the input for later approaches that construct phrase-level translation rules which may (Wu, 1997; Yamada and Knight, 2001) or may not (Och et al., 1999; Marcu and Wong, 2002) rely on linguistic information. The method developed in (Och et al., 1999), known as the consistency method, is a simple yet effective method that has become the standard way of extracting (source, target)-pairs of phrases as translation rules. The development of consistency has been done entirely on empirical evidence and it has thus been termed a heuristic. In this work we show that the method of (Och et al., 1999) actually encodes a particular type of structural information induced by the word alignment matrices."
W13-3010,P10-1147,0,0.0296665,"Missing"
W13-3010,P01-1067,0,0.109624,"a phrase-based generative model. A by-product of this model is a derivation of probability mass functions for random partitions. These are realized as cases of constrained, biased sampling without replacement and we provide an exact formula for the probability of a segmentation of a sentence. 1 These models, known as IBM models, are based on standard machine learning techniques. Their output is a matrix of word alignments for each sentence pair in the training data. These word alignments provide the input for later approaches that construct phrase-level translation rules which may (Wu, 1997; Yamada and Knight, 2001) or may not (Och et al., 1999; Marcu and Wong, 2002) rely on linguistic information. The method developed in (Och et al., 1999), known as the consistency method, is a simple yet effective method that has become the standard way of extracting (source, target)-pairs of phrases as translation rules. The development of consistency has been done entirely on empirical evidence and it has thus been termed a heuristic. In this work we show that the method of (Och et al., 1999) actually encodes a particular type of structural information induced by the word alignment matrices. Moreover, we show that th"
W13-3010,H05-1022,0,0.0261137,"contiguous segments in each pair, rather than the segments themselves. 3. If {Ei } is a countable collection of sets in B, then so is their union ∪i Ei . Condition 1 guarantees that B is non-empty and Conditions 2 and 3 say that B is closed under complementation and countable unions respectively. The pair (X, B) is called a measurable space. A function f : B → [0, ∞) is called a measure if the following conditions hold: Several phrase-level generative models have been proposed, almost all relying on multinomial distributions for the phrase alignments (Marcu and Wong, 2002; Zhang et al., 2003; Deng and Byrne 2005; DeNero et al., 2006; Birch et al., 2006). This is a consequence of treating alignments as functions rather than partitions. Word alignment and phrase extraction via Inversion Transduction Grammars (Wu, 1997), is a linguistically motivated method that relies on simultaneous parsing of source and target sentences (DeNero and Klein, 2010; Cherry and Lin 2007; Neubig et al., 2012). The partition probabilities we introduced in Section 5.2 share the same tree structure discussed in (Dennis III, 1991), which has found applications in Information Retrieval (Haffari and Teh, 2009). 7 1. f (∅) = 0. 2."
W13-3010,N09-1020,0,0.0434125,"Missing"
W13-3010,N03-1017,0,0.108118,"cy by P (S, T ). Figure 1(a) shows an example of a sentence pair with an alignment matrix together with all its consistent pairs. In SMT the extraction of each consistent pair (s, t) from (S, T ) is followed by a statistic f (s, t; S, T ). Typically f (s, t; S, T ) counts the occurrences of (s, t) in (S, T ). By considering all sentence pairs in the training data, the translation probability is constructed as P (S,T ) f (s, t; S, T ) P , (2) p(t|s) = P 0 (S,T ) t0 f (s, t ; S, T ) In this section we provide the definition of consistency, which was introduced in (Och et al., 1999), refined in (Koehn et al., 2003), and we follow (Koehn, 2009) in our description. We start with some preliminary definitions. Let S = s1 ...s|S |be a source sentence, i.e., a string that consists of consecutive source words; each word si is drawn from a source language vocabulary and i indicates the position of the word in S. The operation of string extraction from the words of S is defined as the construction of the string s = si1 ...sin from the words of S, with 1 ≤ i1 &lt; ... &lt; in ≤ |S|. If i1 , ..., in are consecutive, which implies that s is a substring of S, then s is called a source phrase and we write s ⊆ S. As a short"
W13-3010,P09-5002,0,0.0153921,"n example of a sentence pair with an alignment matrix together with all its consistent pairs. In SMT the extraction of each consistent pair (s, t) from (S, T ) is followed by a statistic f (s, t; S, T ). Typically f (s, t; S, T ) counts the occurrences of (s, t) in (S, T ). By considering all sentence pairs in the training data, the translation probability is constructed as P (S,T ) f (s, t; S, T ) P , (2) p(t|s) = P 0 (S,T ) t0 f (s, t ; S, T ) In this section we provide the definition of consistency, which was introduced in (Och et al., 1999), refined in (Koehn et al., 2003), and we follow (Koehn, 2009) in our description. We start with some preliminary definitions. Let S = s1 ...s|S |be a source sentence, i.e., a string that consists of consecutive source words; each word si is drawn from a source language vocabulary and i indicates the position of the word in S. The operation of string extraction from the words of S is defined as the construction of the string s = si1 ...sin from the words of S, with 1 ≤ i1 &lt; ... &lt; in ≤ |S|. If i1 , ..., in are consecutive, which implies that s is a substring of S, then s is called a source phrase and we write s ⊆ S. As a shorthand we also write siin1 for"
W13-3010,W02-1018,0,0.155997,"model is a derivation of probability mass functions for random partitions. These are realized as cases of constrained, biased sampling without replacement and we provide an exact formula for the probability of a segmentation of a sentence. 1 These models, known as IBM models, are based on standard machine learning techniques. Their output is a matrix of word alignments for each sentence pair in the training data. These word alignments provide the input for later approaches that construct phrase-level translation rules which may (Wu, 1997; Yamada and Knight, 2001) or may not (Och et al., 1999; Marcu and Wong, 2002) rely on linguistic information. The method developed in (Och et al., 1999), known as the consistency method, is a simple yet effective method that has become the standard way of extracting (source, target)-pairs of phrases as translation rules. The development of consistency has been done entirely on empirical evidence and it has thus been termed a heuristic. In this work we show that the method of (Och et al., 1999) actually encodes a particular type of structural information induced by the word alignment matrices. Moreover, we show that the way in which statistics are extracted from the ass"
W13-3010,2006.amta-papers.2,0,\N,Missing
W14-3302,bojar-etal-2014-hindencorp,1,0.801715,"Missing"
W14-3302,W13-2242,0,0.189283,"Missing"
W14-3302,W14-3305,0,0.0227897,"Missing"
W14-3302,W14-3326,1,0.729814,"Missing"
W14-3302,W14-3313,0,0.0328356,"Missing"
W14-3302,W14-3342,0,0.0770851,"Missing"
W14-3302,2012.iwslt-papers.5,1,0.897779,"Each system SJ in the pool {Sj } is represented by an associated relative ability µj and a variance σa2 (fixed across all systems) which serve as the parameters of a Gaussian distribution. Samples from this distribution represent the quality of sentence translations, with higher quality samples having higher values. Pairwise annotations (S1 , S2 , π) are generated according to the following process: Method 1: Expected Wins (EW) Introduced for WMT13, the E XPECTED W INS has an intuitive score demonstrated to be accurate in ranking systems according to an underlying model of “relative ability” (Koehn, 2012a). The idea is to gauge the probability that a system Si will be ranked better than another system randomly chosen from a pool of opponents {Sj : j 6= i}. If we define the function win(A, B) as the number of times system A is ranked better than system B, 19 1. Select two systems S1 and S2 from the pool of systems {Sj } This score is then used to sort the systems and produce the ranking. 2. Draw two “translations”, adding random 2 to simulate Gaussian noise with variance σobs the subjectivity of the task and the differences among annotators: 3.4 Method Selection We have three methods which, pr"
W14-3302,W06-3114,1,0.176114,"estimation of machine translation quality, and a metrics task. This year, 143 machine translation systems from 23 institutions were submitted to the ten translation directions in the standard translation task. An additional 6 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had four subtasks, with a total of 10 teams, submitting 57 entries. 1 Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at ACL 2014. This workshop builds on eight previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013). This year we conducted four official tasks: a translation task, a quality estimation task, a metrics task1 and a medical translation task. In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Hindi, and Russian. The Hindi translation tasks were new this year, providing a lesser resourced data condition on a challenging languag"
W14-3302,W12-3101,0,0.0505246,"Missing"
W14-3302,P02-1040,0,0.10401,"inuous-space language model is also used in a post-processing step for each system. POSTECH submitted a phrase-based SMT system and query translation system for the DE–EN language pair in both subtasks. They analysed three types of query formation, generated query translation candidates using term-to-term dictionaries and a phrase-based system, and then scored them using a co-occurrence word frequency measure to select the best candidate. UEDIN applied the Moses phrase-based system to 5.5 Results MT quality in the Medical Translation Task is evaluated using automatic evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), PER (Tillmann et al., 1997), and CDER (Leusch et al., 2006). BLEU scores are reported as percentage and all error rates are reported as one minus the original value, also as percentage, so that all metrics are in the 0-100 range, and higher scores indicate better translations. The main reason for not conducting human evaluation, as it happens in the standard Trans45 original ID BLEU normalized truecased normalized lowercased 1-TER 1-PER 1-CDER BLEU 1-TER 1-PER 1-CDER Czech→English CUNI 29.64 29.79±1.07 47.45±1.15 61.64±1.06 52.18±0.98 31.68±1.14 49.84±1.10 64.38±1."
W14-3302,W14-3328,0,0.0383178,"Missing"
W14-3302,W14-3301,1,0.485672,"guage pair by translating newspaper articles and provided training data. 2.1 2.2 As in past years we provided parallel corpora to train translation models, monolingual corpora to train language models, and development sets to tune system parameters. Some training corpora were identical from last year (Europarl4 , United Nations, French-English 109 corpus, CzEng, Common Crawl, Russian-English Wikipedia Headlines provided by CMU), some were updated (Russian-English parallel data provided by Yandex, News Commentary, monolingual data), and a new corpus was added (HindiEnglish corpus, Bojar et al. (2014)), Hindi-English Wikipedia Headline corpus). Some statistics about the training materials are given in Figure 1. Test data The test data for this year’s task was selected from news stories from online sources, as before. However, we changed our method to create the test sets. In previous years, we took equal amounts of source sentences from all six languages involved (around 500 sentences each), and translated them into all other languages. While this produced a multi-parallel test corpus that could be also used for language pairs (such as Czech-Russian) that we did not include in the evaluati"
W14-3302,W14-3330,0,0.0464303,"Missing"
W14-3302,W14-3320,0,0.0387654,"Missing"
W14-3302,W14-3317,0,0.0337314,"Missing"
W14-3302,W14-3343,1,0.726267,"Finally, with respect to out-of-domain (different 41 It is interesting to mention the indirect use of human translations by USHEFF for Tasks 1.1-1.3: given a translation for a source segment, all other translations for the same segment were used as pseudo-references. Apart from when this translation was actually the human translation, the human translation was effectively used as a reference. While this reference was mixed with 23 other pseudo-references (other machine translations) for the feature computations, these features led to significant gains in performance over the baseline features Scarton and Specia (2014). We believe that more investigation is needed for human translation quality prediction. Tasks dedicated to this type of data at both sentence- and word-level in the next editions of this shared task would be a possible starting point. The acquisition of such data is however much more costly, as it is arguably hard to find examples of low quality human translation, unless specific settings, such as translation learner corpora, are considered. text domain and MT system) test data, for Task 1.1, none of the papers submitted included experiments. (Shah and Specia, 2014) applied the models trained"
W14-3302,W02-1001,0,\N,Missing
W14-3302,E06-1031,0,\N,Missing
W14-3302,W12-3102,1,\N,Missing
W14-3302,P13-2135,0,\N,Missing
W14-3302,W14-3311,0,\N,Missing
W14-3302,W14-3314,0,\N,Missing
W14-3302,W09-0401,1,\N,Missing
W14-3302,W14-3319,0,\N,Missing
W14-3302,W14-3344,0,\N,Missing
W14-3302,W14-3327,0,\N,Missing
W14-3302,W07-0718,1,\N,Missing
W14-3302,P11-1132,0,\N,Missing
W14-3302,W13-2248,0,\N,Missing
W14-3302,W14-3307,0,\N,Missing
W14-3302,W10-1703,1,\N,Missing
W14-3302,W14-3323,0,\N,Missing
W14-3302,P13-4014,1,\N,Missing
W14-3302,W08-0309,1,\N,Missing
W14-3302,W14-3340,1,\N,Missing
W14-3302,W14-3312,0,\N,Missing
W14-3302,P13-1139,0,\N,Missing
W14-3302,W14-3310,1,\N,Missing
W14-3302,P13-1004,1,\N,Missing
W14-3302,W14-3304,0,\N,Missing
W14-3302,W14-3321,0,\N,Missing
W14-3302,W14-3308,0,\N,Missing
W14-3302,uresova-etal-2014-multilingual,1,\N,Missing
W14-3302,W14-3336,1,\N,Missing
W14-3302,W14-3331,0,\N,Missing
W14-3302,W14-3332,0,\N,Missing
W14-3302,W14-3325,0,\N,Missing
W14-3302,W14-3329,0,\N,Missing
W14-3302,W14-3315,0,\N,Missing
W14-3302,W13-2201,1,\N,Missing
W14-3302,W14-3339,0,\N,Missing
W14-3302,W14-3322,1,\N,Missing
W14-3302,W14-3303,0,\N,Missing
W14-3302,W14-3318,0,\N,Missing
W14-3302,W14-3341,0,\N,Missing
W14-3302,W11-2101,1,\N,Missing
W14-3302,W14-3338,1,\N,Missing
W15-2518,D11-1033,0,0.0160741,"Kuhn, 2007) or log-linearly (Koehn and Schroeder, 2007). Sennrich (2012) enhances the approach by interpolating up to ten models, and Bertoldi and Federico (2009) use in-domain monolingual data to automatically generate in-domain bilingual data. Second, instance weighting methods prioritize training instances that are most relevant to the test data, by assigning weights to sentence pairs (Matsoukas et al., 2009) or phrase pairs (Foster et al., 2010; Chen et al., 2013). In the most extreme case, weights are binary and training instances are either selected or discarded (Moore and Lewis, 2010; Axelrod et al., 2011). In most previous work, domains are typically hard-labeled concepts that correspond to provenance or particular topic-genre combinations. In recent years, some work has explicitly addressed topic adaptation for SMT (Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) using latent Dirichlet allocation (Blei et al., 2003). Surprisingly, genre (or style) adaptation has only been addressed to a limited extent (Bisazza and Federico, 2012; Wang et al., 2012), with methods requiring the availability of clearly separable in-domain and out-of-domain training"
W15-2518,W09-0432,0,0.0244016,"e fine-grained subclasses, such as dialog-oriented content (e.g., SMS or chat messages), weblogs, or commentaries to news articles, all of which pose different challenges to SMT (van der Wees et al., 2015a). Most existing domain adaptation approaches can be grouped into two categories, depending on where in the SMT pipeline they adapt the system. First, mixture modeling approaches learn models from different subcorpora and interpolate these linearly (Foster and Kuhn, 2007) or log-linearly (Koehn and Schroeder, 2007). Sennrich (2012) enhances the approach by interpolating up to ten models, and Bertoldi and Federico (2009) use in-domain monolingual data to automatically generate in-domain bilingual data. Second, instance weighting methods prioritize training instances that are most relevant to the test data, by assigning weights to sentence pairs (Matsoukas et al., 2009) or phrase pairs (Foster et al., 2010; Chen et al., 2013). In the most extreme case, weights are binary and training instances are either selected or discarded (Moore and Lewis, 2010; Axelrod et al., 2011). In most previous work, domains are typically hard-labeled concepts that correspond to provenance or particular topic-genre combinations. In"
W15-2518,P05-1071,0,0.0316744,"ing data with a variety of web-crawled manually translated documents, containing user comments that are of a similar nature as the UG documents in the Gen&Topic, set as well as a number of other genres. Table 3 lists the corpus statistics of the training data, split by manual subcorpus labels as used for the subcorpus VSM variant (see Section 3.2). While our manually grouped subcorpora approximate those used by Chen et al. (2013), exact agreement was impossible to obtain, illustrating that it is not trivial to manually generate optimal subcorpus labels. We tokenize all Arabic data using MADA (Habash and Rambow, 2005), ATB scheme. Word alignment was performed by running GIZA++ in both directions and generating the symmetric alignments using the ‘grow-diag-final-and’ heuristics. We use an adapted language model which Experimental setup We evaluate the methods described in Section 3 on two Arabic-to-English translation tasks, both comprising the NW and UG. The first evaluation set is the Gen&Topic benchmark (van der Wees et al., 2015b), which consists of manually translated web-crawled news articles and their respective manually translated user comments, both covering five different topics. Since this evalua"
W15-2518,E14-1035,0,0.0131251,"ize training instances that are most relevant to the test data, by assigning weights to sentence pairs (Matsoukas et al., 2009) or phrase pairs (Foster et al., 2010; Chen et al., 2013). In the most extreme case, weights are binary and training instances are either selected or discarded (Moore and Lewis, 2010; Axelrod et al., 2011). In most previous work, domains are typically hard-labeled concepts that correspond to provenance or particular topic-genre combinations. In recent years, some work has explicitly addressed topic adaptation for SMT (Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) using latent Dirichlet allocation (Blei et al., 2003). Surprisingly, genre (or style) adaptation has only been addressed to a limited extent (Bisazza and Federico, 2012; Wang et al., 2012), with methods requiring the availability of clearly separable in-domain and out-of-domain training corpora. Recently, we studied the impact of topic and genre differences on SMT quality using the Gen&Topic benchmark set, an Arabic-English evaluation set with controlled topic distributions over two genres; newswire and UG comments (van der Wees et al., 2015b). Motivated by the observa"
W15-2518,E12-1045,1,0.862424,"In the most extreme case, weights are binary and training instances are either selected or discarded (Moore and Lewis, 2010; Axelrod et al., 2011). In most previous work, domains are typically hard-labeled concepts that correspond to provenance or particular topic-genre combinations. In recent years, some work has explicitly addressed topic adaptation for SMT (Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) using latent Dirichlet allocation (Blei et al., 2003). Surprisingly, genre (or style) adaptation has only been addressed to a limited extent (Bisazza and Federico, 2012; Wang et al., 2012), with methods requiring the availability of clearly separable in-domain and out-of-domain training corpora. Recently, we studied the impact of topic and genre differences on SMT quality using the Gen&Topic benchmark set, an Arabic-English evaluation set with controlled topic distributions over two genres; newswire and UG comments (van der Wees et al., 2015b). Motivated by the observation that translation quality varies more between the two genres than across topics, we explore in this paper the task of genre adaptation. Concretely, we incorporate genre-revealing features,"
W15-2518,W14-3358,0,0.0163973,"ize training instances that are most relevant to the test data, by assigning weights to sentence pairs (Matsoukas et al., 2009) or phrase pairs (Foster et al., 2010; Chen et al., 2013). In the most extreme case, weights are binary and training instances are either selected or discarded (Moore and Lewis, 2010; Axelrod et al., 2011). In most previous work, domains are typically hard-labeled concepts that correspond to provenance or particular topic-genre combinations. In recent years, some work has explicitly addressed topic adaptation for SMT (Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) using latent Dirichlet allocation (Blei et al., 2003). Surprisingly, genre (or style) adaptation has only been addressed to a limited extent (Bisazza and Federico, 2012; Wang et al., 2012), with methods requiring the availability of clearly separable in-domain and out-of-domain training corpora. Recently, we studied the impact of topic and genre differences on SMT quality using the Gen&Topic benchmark set, an Arabic-English evaluation set with controlled topic distributions over two genres; newswire and UG comments (van der Wees et al., 2015b). Motivated by the observa"
W15-2518,W12-3156,0,0.0318312,"nd Simard show that translation consistency does not imply higher quality, they also conclude that consistently translated phrases are more often translated correctly than inconsistently translated phrases. Table 7 shows some examples of phrases that Translation consistency analysis In the proposed translation model adaptation approach lexical choice is more tailored towards the different genres than in the baseline. We therefore hypothesize that the adapted system increases consistency of output translations within genres. To test this hypothesis, we measure translation consistency following Carpuat and Simard (2012). Their approach studies repeated phrases, defined 138 Genre Source phrase Baseline translation(s) VSM automatic genre translation(s) Inconsistent in baseline, consistent in adapted system: UG UG NW NW ð ÈYK @ Yë XAêk. B@ ð  ú jË@ ¨A¢®Ë@ éKAÖÏ@ áÓ and this indicates / and this shows that fatigue and stress / and the stress the health sector / workers in the health sector and this shows and the stress the health sector percent of egyptians / percent of them percent of Consistent in baseline, inconsistent in adapted system: UG UG NW NW PBðX PAJ ÊÓ AK ñJ Õæ ª¢JË@    AJ ®K Q¯@ Qå AJ ÖÏA«"
W15-2518,P13-2122,0,0.0121754,"ce weighting methods prioritize training instances that are most relevant to the test data, by assigning weights to sentence pairs (Matsoukas et al., 2009) or phrase pairs (Foster et al., 2010; Chen et al., 2013). In the most extreme case, weights are binary and training instances are either selected or discarded (Moore and Lewis, 2010; Axelrod et al., 2011). In most previous work, domains are typically hard-labeled concepts that correspond to provenance or particular topic-genre combinations. In recent years, some work has explicitly addressed topic adaptation for SMT (Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) using latent Dirichlet allocation (Blei et al., 2003). Surprisingly, genre (or style) adaptation has only been addressed to a limited extent (Bisazza and Federico, 2012; Wang et al., 2012), with methods requiring the availability of clearly separable in-domain and out-of-domain training corpora. Recently, we studied the impact of topic and genre differences on SMT quality using the Gen&Topic benchmark set, an Arabic-English evaluation set with controlled topic distributions over two genres; newswire and UG comments (van der Wees et al., 2015b). Mot"
W15-2518,D11-1125,0,0.0270128,"with varying numbers of latent dimensions (5, 10, 20, and 50). Of these, LDA with 10 dimensions yields the best translation performance, which is consistent with findings in a related topic adaptation approach by Eidelman et al. (2012). The LDA features in this VSM variant are inferred from the source side of the training data. 4 Genre Other features include linear distortion with limit 5, lexical weighting (Koehn et al., 2003), and a 5gram target language model trained with KneserNey smoothing (Chen and Goodman, 1999). The feature weights are tuned using pairwise ranking optimization (PRO) (Hopkins and May, 2011). For all experiments, tuning is done separately for the two genre-specific development sets. All runs use parallel corpora made available for NIST OpenMT 2012, excluding the UN data. While LDC-distributed data sets contain substantial portions of documents within the NW genre, they only contain small portions of UG documents. To alleviate this imbalance we augment our LDC-distributed training data with a variety of web-crawled manually translated documents, containing user comments that are of a similar nature as the UG documents in the Gen&Topic, set as well as a number of other genres. Tabl"
W15-2518,P13-1126,0,0.0291009,"Missing"
W15-2518,Q13-1035,0,0.0332699,"Missing"
W15-2518,W01-1007,0,0.315253,"tionally other textual characteristics such as dialect and register. This definition, however, has two major shortcomings. First, subcorpusbased domains depend on provenance information, which might not be available, or on manual grouping of documents into subcorpora, which is labor intensive and often carried out according to arbitrary criteria. Second, the commonly used notion of a domain neglects the fact that topic and genre are two distinct properties of text (Stein and Meyer Zu Eissen, 2006). While this distinction has long been acknowledged in text classification literature (Lee, 2001; Dewdney et al., 2001; Lee and Myaeng, 2002), most work on domain adaptation in SMT uses in-domain and out-of-domain data that differs on both the topic and the genre level (e.g., Europarl political proceedings (Koehn, 2005) versus EMEA medical text (Tiedemann, 2009)), making it unclear whether the proposed solutions address topic or genre differences. In this work, we follow text classification literature for definitions of the concepts topic and genre. While topic refers to the general subject (e.g., sports, politics or science) of a document, genre is harder to define since existing definitions vary. Swales (19"
W15-2518,C94-2174,0,0.467237,"s when using the automatic genre indicators on top of manual subcorpus labels. We also find that our genre-revealing feature values can be computed on either side of the training bitext, indicating that the proposed features are to a large extent language independent. Finally, we notice that our genre-adapted translation models encourage document-level translation consistency with respect to the unadapted baseline. 2 Domain adaptation for SMT 2.2 Text genre classification Work on text genre classification has resulted in various methods that use different sets of genre-specific text features. Karlgren and Cutting (1994) were among the first to use simple document statistics, such as common word frequencies, first-person pronoun count, and average sentence length. Kessler et al. (1997) categorize four types of genre-revealing cues: structural cues (e.g., part-of-speech (POS) tag counts), lexical cues (specific words), character-level cues (e.g., punctuation marks), and derivative cues (ratios and variation measures based on other types of cues). Dewdney et al. (2001) compare a large number of document features and show that these outperform bag-of-words approaches, which are traditionally used in topic-based"
W15-2518,P12-2023,0,0.0956579,"al data. Second, instance weighting methods prioritize training instances that are most relevant to the test data, by assigning weights to sentence pairs (Matsoukas et al., 2009) or phrase pairs (Foster et al., 2010; Chen et al., 2013). In the most extreme case, weights are binary and training instances are either selected or discarded (Moore and Lewis, 2010; Axelrod et al., 2011). In most previous work, domains are typically hard-labeled concepts that correspond to provenance or particular topic-genre combinations. In recent years, some work has explicitly addressed topic adaptation for SMT (Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) using latent Dirichlet allocation (Blei et al., 2003). Surprisingly, genre (or style) adaptation has only been addressed to a limited extent (Bisazza and Federico, 2012; Wang et al., 2012), with methods requiring the availability of clearly separable in-domain and out-of-domain training corpora. Recently, we studied the impact of topic and genre differences on SMT quality using the Gen&Topic benchmark set, an Arabic-English evaluation set with controlled topic distributions over two genres; newswire and UG comments (van"
W15-2518,P97-1005,0,0.345578,"Missing"
W15-2518,W07-0717,0,0.0205513,"-generated (UG) text, i.e., content written by lay-persons that has not undergone any editorial control. Within the latter we can distinguish more fine-grained subclasses, such as dialog-oriented content (e.g., SMS or chat messages), weblogs, or commentaries to news articles, all of which pose different challenges to SMT (van der Wees et al., 2015a). Most existing domain adaptation approaches can be grouped into two categories, depending on where in the SMT pipeline they adapt the system. First, mixture modeling approaches learn models from different subcorpora and interpolate these linearly (Foster and Kuhn, 2007) or log-linearly (Koehn and Schroeder, 2007). Sennrich (2012) enhances the approach by interpolating up to ten models, and Bertoldi and Federico (2009) use in-domain monolingual data to automatically generate in-domain bilingual data. Second, instance weighting methods prioritize training instances that are most relevant to the test data, by assigning weights to sentence pairs (Matsoukas et al., 2009) or phrase pairs (Foster et al., 2010; Chen et al., 2013). In the most extreme case, weights are binary and training instances are either selected or discarded (Moore and Lewis, 2010; Axelrod et a"
W15-2518,W07-0733,0,0.0242192,"ten by lay-persons that has not undergone any editorial control. Within the latter we can distinguish more fine-grained subclasses, such as dialog-oriented content (e.g., SMS or chat messages), weblogs, or commentaries to news articles, all of which pose different challenges to SMT (van der Wees et al., 2015a). Most existing domain adaptation approaches can be grouped into two categories, depending on where in the SMT pipeline they adapt the system. First, mixture modeling approaches learn models from different subcorpora and interpolate these linearly (Foster and Kuhn, 2007) or log-linearly (Koehn and Schroeder, 2007). Sennrich (2012) enhances the approach by interpolating up to ten models, and Bertoldi and Federico (2009) use in-domain monolingual data to automatically generate in-domain bilingual data. Second, instance weighting methods prioritize training instances that are most relevant to the test data, by assigning weights to sentence pairs (Matsoukas et al., 2009) or phrase pairs (Foster et al., 2010; Chen et al., 2013). In the most extreme case, weights are binary and training instances are either selected or discarded (Moore and Lewis, 2010; Axelrod et al., 2011). In most previous work, domains ar"
W15-2518,D10-1044,0,0.0558209,"Missing"
W15-2518,N03-1017,0,0.0151878,"ted on the Arabic side. Genre mapping: BC=broadcast conversation, BN=broadcast news, NG=newsgroup, NW=newswire, WL=UG weblogs, CM=UG comments, ED=editorials, SP=speech transcripts. 2010), with varying numbers of latent dimensions (5, 10, 20, and 50). Of these, LDA with 10 dimensions yields the best translation performance, which is consistent with findings in a related topic adaptation approach by Eidelman et al. (2012). The LDA features in this VSM variant are inferred from the source side of the training data. 4 Genre Other features include linear distortion with limit 5, lexical weighting (Koehn et al., 2003), and a 5gram target language model trained with KneserNey smoothing (Chen and Goodman, 1999). The feature weights are tuned using pairwise ranking optimization (PRO) (Hopkins and May, 2011). For all experiments, tuning is done separately for the two genre-specific development sets. All runs use parallel corpora made available for NIST OpenMT 2012, excluding the UN data. While LDC-distributed data sets contain substantial portions of documents within the NW genre, they only contain small portions of UG documents. To alleviate this imbalance we augment our LDC-distributed training data with a v"
W15-2518,P06-1070,0,0.0350482,"n any previous work. 133 In addition to the phrase pair vectors, a single vector is created for the development set which is assumed to be similar to the test data: tion. Finn and Kushmerick (2006) also compare the bag-of-words approach with simple text statistics and conclude that both methods achieve high classification accuracy on fixed topic-genre combinations but perform worse when predicting topic-independent genre labels. While mostly focused on the English language, some work has addressed language-independent (Sharoff, 2007; Sharoff et al., 2010) or crosslingual genre classification (Gliozzo and Strapparava, 2006; Petrenz, 2012; Petrenz and Webber, 2012), indicating that a single set of genrerevealing features can generalize across multiple languages. In this paper, we examine whether genre-revealing features are also language independent when applied to translation model genre adaptation for SMT. 3 V (dev ) = < w1 (dev ), . . . , wN (dev ) >, where weights wi (dev ) are computed for the entire development set, summing over the vectors of all phrase pairs that occur in the development set: X cdev (f¯, e¯) wi (f¯, e¯). (3) wi (dev ) = (f¯,¯ e)∈Pdev Here Pdev refers to the set of phrase pairs that can b"
W15-2518,2005.iwslt-1.8,0,0.0248349,"tuning, and NIST 2008 and NIST 2009 combined for testing. These data sets cover the genres NW and UG weblogs but are not controlled for topic distributions. Specifications for both evaluation sets are shown in Table 2. Note that Gen&Topic contains one reference translation per sentence, while NIST has four sets of reference translations. We perform our experiments using an inhouse phrase-based SMT system similar to Moses (Koehn et al., 2007). All runs use lexicalized reordering, distinguishing between monotone, swap, and discontinuous reordering, with respect to the previous and next phrase (Koehn et al., 2005). 136 Gen&Topic (1 reference) NIST (4 references) Method NW UG All NW UG All Baseline 21.5 17.2 19.3 55.3 40.4 48.5 VSM variants using automatic indicators of genre: LDA 10 topics 21.7 (+0.2) 17.3 (+0.1) 19.4M (+0.1) 55.9N (+0.6) 40.7M (+0.3) 49.0N (+0.5) Genre features Source Target 21.9N (+0.4) 21.7 (+0.2) 17.4M (+0.2) 17.5N (+0.3) 19.6N (+0.3) 19.6N (+0.3) 55.7N (+0.4) 55.9N (+0.6) 41.0N (+0.6) 41.2N (+0.8) 49.0N (+0.5) 49.1N (+0.6) Genre+LDA Source Target 21.9N (+0.4) 21.8N (+0.3) 17.5N (+0.3) 17.5N (+0.3) 19.7N (+0.4) 19.6N (+0.3) 56.1N (+0.8) 56.2N (+0.9) 41.2N (+0.8) 41.2N (+0.8) 49.2N"
W15-2518,E12-1055,0,0.0195346,"not undergone any editorial control. Within the latter we can distinguish more fine-grained subclasses, such as dialog-oriented content (e.g., SMS or chat messages), weblogs, or commentaries to news articles, all of which pose different challenges to SMT (van der Wees et al., 2015a). Most existing domain adaptation approaches can be grouped into two categories, depending on where in the SMT pipeline they adapt the system. First, mixture modeling approaches learn models from different subcorpora and interpolate these linearly (Foster and Kuhn, 2007) or log-linearly (Koehn and Schroeder, 2007). Sennrich (2012) enhances the approach by interpolating up to ten models, and Bertoldi and Federico (2009) use in-domain monolingual data to automatically generate in-domain bilingual data. Second, instance weighting methods prioritize training instances that are most relevant to the test data, by assigning weights to sentence pairs (Matsoukas et al., 2009) or phrase pairs (Foster et al., 2010; Chen et al., 2013). In the most extreme case, weights are binary and training instances are either selected or discarded (Moore and Lewis, 2010; Axelrod et al., 2011). In most previous work, domains are typically hard-"
W15-2518,2005.mtsummit-papers.11,0,0.00719864,"able, or on manual grouping of documents into subcorpora, which is labor intensive and often carried out according to arbitrary criteria. Second, the commonly used notion of a domain neglects the fact that topic and genre are two distinct properties of text (Stein and Meyer Zu Eissen, 2006). While this distinction has long been acknowledged in text classification literature (Lee, 2001; Dewdney et al., 2001; Lee and Myaeng, 2002), most work on domain adaptation in SMT uses in-domain and out-of-domain data that differs on both the topic and the genre level (e.g., Europarl political proceedings (Koehn, 2005) versus EMEA medical text (Tiedemann, 2009)), making it unclear whether the proposed solutions address topic or genre differences. In this work, we follow text classification literature for definitions of the concepts topic and genre. While topic refers to the general subject (e.g., sports, politics or science) of a document, genre is harder to define since existing definitions vary. Swales (1990), for example, refers to genre as a class of communicative events with a shared set of communicative purposes, and Karlgren (2004) calls it a grouping of documents that are stylistically consistent. B"
W15-2518,sharoff-etal-2010-web,0,0.0278368,"tures. To our knowledge, the fields have not been combined in any previous work. 133 In addition to the phrase pair vectors, a single vector is created for the development set which is assumed to be similar to the test data: tion. Finn and Kushmerick (2006) also compare the bag-of-words approach with simple text statistics and conclude that both methods achieve high classification accuracy on fixed topic-genre combinations but perform worse when predicting topic-independent genre labels. While mostly focused on the English language, some work has addressed language-independent (Sharoff, 2007; Sharoff et al., 2010) or crosslingual genre classification (Gliozzo and Strapparava, 2006; Petrenz, 2012; Petrenz and Webber, 2012), indicating that a single set of genrerevealing features can generalize across multiple languages. In this paper, we examine whether genre-revealing features are also language independent when applied to translation model genre adaptation for SMT. 3 V (dev ) = < w1 (dev ), . . . , wN (dev ) >, where weights wi (dev ) are computed for the entire development set, summing over the vectors of all phrase pairs that occur in the development set: X cdev (f¯, e¯) wi (f¯, e¯). (3) wi (dev ) ="
W15-2518,D09-1074,0,0.0291166,"grouped into two categories, depending on where in the SMT pipeline they adapt the system. First, mixture modeling approaches learn models from different subcorpora and interpolate these linearly (Foster and Kuhn, 2007) or log-linearly (Koehn and Schroeder, 2007). Sennrich (2012) enhances the approach by interpolating up to ten models, and Bertoldi and Federico (2009) use in-domain monolingual data to automatically generate in-domain bilingual data. Second, instance weighting methods prioritize training instances that are most relevant to the test data, by assigning weights to sentence pairs (Matsoukas et al., 2009) or phrase pairs (Foster et al., 2010; Chen et al., 2013). In the most extreme case, weights are binary and training instances are either selected or discarded (Moore and Lewis, 2010; Axelrod et al., 2011). In most previous work, domains are typically hard-labeled concepts that correspond to provenance or particular topic-genre combinations. In recent years, some work has explicitly addressed topic adaptation for SMT (Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) using latent Dirichlet allocation (Blei et al., 2003). Surprisingly, genre (or styl"
W15-2518,P10-2041,0,0.020232,"se linearly (Foster and Kuhn, 2007) or log-linearly (Koehn and Schroeder, 2007). Sennrich (2012) enhances the approach by interpolating up to ten models, and Bertoldi and Federico (2009) use in-domain monolingual data to automatically generate in-domain bilingual data. Second, instance weighting methods prioritize training instances that are most relevant to the test data, by assigning weights to sentence pairs (Matsoukas et al., 2009) or phrase pairs (Foster et al., 2010; Chen et al., 2013). In the most extreme case, weights are binary and training instances are either selected or discarded (Moore and Lewis, 2010; Axelrod et al., 2011). In most previous work, domains are typically hard-labeled concepts that correspond to provenance or particular topic-genre combinations. In recent years, some work has explicitly addressed topic adaptation for SMT (Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) using latent Dirichlet allocation (Blei et al., 2003). Surprisingly, genre (or style) adaptation has only been addressed to a limited extent (Bisazza and Federico, 2012; Wang et al., 2012), with methods requiring the availability of clearly separable in-domain and"
W15-2518,W15-4304,1,0.851321,"Missing"
W15-2518,P15-2092,1,0.745217,"Missing"
W15-2518,P02-1040,0,0.093436,"er overall results for NIST. is trained on 1.6B tokens and linearly interpolates different English Gigaword subcorpora with the English side of our bitext. The resulting model covers both genres in the benchmark sets, but is not varied between experiments since we want to investigate the effects of different features on translation model adaptation. 5 Results In this section we compare a number of variants of the general VSM framework, differing in the way vectors are defined and constructed (see Sections 3.2–3.4). Translation quality of all experiments is measured with case-insensitive BLEU (Papineni et al., 2002) using the closest-reference brevity penalty. We use approximate randomization (Noreen, 1989) for significance testing (Riezler and Maxwell, 2005). Statistically significant differences are marked by M and N for the p ≤ 0.05 and the p ≤ 0.01 level, respectively. VSM using intrinsic text features. We first test various VSM variants that use automatic indicators of genre and do not depend on the availability of provenance information or manual subcorpus labels (Table 4). Of these, genre adaptation with LDA-based features (Section 3.4) achieves strongly significant improvements over the unadapted"
W15-2518,2012.amta-papers.18,0,0.0169071,"ights are binary and training instances are either selected or discarded (Moore and Lewis, 2010; Axelrod et al., 2011). In most previous work, domains are typically hard-labeled concepts that correspond to provenance or particular topic-genre combinations. In recent years, some work has explicitly addressed topic adaptation for SMT (Eidelman et al., 2012; Hewavitharana et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) using latent Dirichlet allocation (Blei et al., 2003). Surprisingly, genre (or style) adaptation has only been addressed to a limited extent (Bisazza and Federico, 2012; Wang et al., 2012), with methods requiring the availability of clearly separable in-domain and out-of-domain training corpora. Recently, we studied the impact of topic and genre differences on SMT quality using the Gen&Topic benchmark set, an Arabic-English evaluation set with controlled topic distributions over two genres; newswire and UG comments (van der Wees et al., 2015b). Motivated by the observation that translation quality varies more between the two genres than across topics, we explore in this paper the task of genre adaptation. Concretely, we incorporate genre-revealing features, inspired by previous"
W15-2518,E12-3002,0,0.0193562,"ition to the phrase pair vectors, a single vector is created for the development set which is assumed to be similar to the test data: tion. Finn and Kushmerick (2006) also compare the bag-of-words approach with simple text statistics and conclude that both methods achieve high classification accuracy on fixed topic-genre combinations but perform worse when predicting topic-independent genre labels. While mostly focused on the English language, some work has addressed language-independent (Sharoff, 2007; Sharoff et al., 2010) or crosslingual genre classification (Gliozzo and Strapparava, 2006; Petrenz, 2012; Petrenz and Webber, 2012), indicating that a single set of genrerevealing features can generalize across multiple languages. In this paper, we examine whether genre-revealing features are also language independent when applied to translation model genre adaptation for SMT. 3 V (dev ) = < w1 (dev ), . . . , wN (dev ) >, where weights wi (dev ) are computed for the entire development set, summing over the vectors of all phrase pairs that occur in the development set: X cdev (f¯, e¯) wi (f¯, e¯). (3) wi (dev ) = (f¯,¯ e)∈Pdev Here Pdev refers to the set of phrase pairs that can be extracted fro"
W15-2518,W05-0908,0,0.101247,"f our bitext. The resulting model covers both genres in the benchmark sets, but is not varied between experiments since we want to investigate the effects of different features on translation model adaptation. 5 Results In this section we compare a number of variants of the general VSM framework, differing in the way vectors are defined and constructed (see Sections 3.2–3.4). Translation quality of all experiments is measured with case-insensitive BLEU (Papineni et al., 2002) using the closest-reference brevity penalty. We use approximate randomization (Noreen, 1989) for significance testing (Riezler and Maxwell, 2005). Statistically significant differences are marked by M and N for the p ≤ 0.05 and the p ≤ 0.01 level, respectively. VSM using intrinsic text features. We first test various VSM variants that use automatic indicators of genre and do not depend on the availability of provenance information or manual subcorpus labels (Table 4). Of these, genre adaptation with LDA-based features (Section 3.4) achieves strongly significant improvements over the unadapted baseline for the NIST-NW and the complete NIST test sets, however improvements on the other test portions are very small. When manually inspectin"
W15-2518,P07-2045,0,\N,Missing
W15-3001,W05-0909,0,0.0473932,"asks 1 and 2 provide the same dataset with English-Spanish translations generated by the statistical machine translation (SMT) system, while Task 3 provides two different datasets, for two language pairs: English-German (EN-DE) and German-English (DE-EN) translations taken from all participating systems in WMT13 (Bojar et al., 2013). These datasets were annotated with different labels for quality: for Tasks 1 and 2, the labels were automatically derived from the post-editing of the machine translation output, while for Task 3, scores were computed based on reference translations using Meteor (Banerjee and Lavie, 2005). Any external resource, including additional quality estimation training data, could be used by participants (no distinction between open and close tracks was made). As presented in Section 4.1, participants were also provided with a baseline set of features for each task, and a software package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 4.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scor"
W15-3001,2011.mtsummit-papers.35,0,0.348896,"Missing"
W15-3001,W13-2241,1,0.851171,"Missing"
W15-3001,P13-2097,1,0.801912,"Missing"
W15-3001,W13-2242,0,0.0386206,"Missing"
W15-3001,W14-3339,0,0.0452066,"Missing"
W15-3001,W15-3035,0,0.054515,"Missing"
W15-3001,W11-2103,1,0.524514,"Missing"
W15-3001,W13-2201,1,0.499374,"Missing"
W15-3001,W14-3302,1,0.498006,"Missing"
W15-3001,W14-3340,1,0.54686,"Missing"
W15-3001,W15-3007,0,0.0166876,"Missing"
W15-3001,W15-3006,1,0.827804,"Missing"
W15-3001,W07-0718,1,0.664541,"om 24 institutions were submitted to the ten translation directions in the standard translation task. An additional 7 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had three subtasks, with a total of 10 teams, submitting 34 entries. The pilot automatic postediting task had a total of 4 teams, submitting 7 entries. 1 Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at EMNLP 2015. This workshop builds on eight previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014). This year we conducted five official tasks: a translation task, a quality estimation task, a metrics task, a tuning task1 , and a automatic postediting task. In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Finnish, and Russian. The Finnish translation 1 The metrics and tuning tasks are reported in separate papers (Stanojevi´c et al., 2015a,b)."
W15-3001,2014.amta-researchers.13,0,0.0200349,"Missing"
W15-3001,W08-0309,1,0.406319,"2. machine translation and automatic evaluation or prediction of translation quality. 2 Overview of the Translation Task The recurring task of the workshop examines translation between English and other languages. As in the previous years, the other languages include German, French, Czech and Russian. Finnish replaced Hindi as the special language this year. Finnish is a lesser resourced language compared to the other languages and has challenging morphological properties. Finnish represents also a different language family that we had not tackled since we included Hungarian in 2008 and 2009 (Callison-Burch et al., 2008, 2009). We created a test set for each language pair by translating newspaper articles and provided training data, except for French, where the test set was drawn from user-generated comments on the news articles. 2.1 2.3 We received 68 submissions from 24 institutions. The participating institutions and their entry names are listed in Table 2; each system did not necessarily appear in all translation tasks. We also included 1 commercial off-the-shelf MT system and 6 online statistical MT systems, which we anonymized. For presentation of the results, systems are treated as either constrained"
W15-3001,W15-3025,1,0.914094,"teps aimed at removing duplicates and those triplets in which any of the elements (source, target, post-edition) was either too long or too short compared to the others, or included tags or special problematic symbols. The main reason for random sampling was to induce some homogeneity across the three datasets and, in turn, Automatic Post-editing Task This year WMT hosted for the first time a shared task on automatic post-editing (APE) for machine translation. The task requires to automatically correct the errors present in a machine translated text. As pointed out in Parton et al. (2012) and Chatterjee et al. (2015b), from the application point of view, APE components would make it possible to: • Improve MT output by exploiting information unavailable to the decoder, or by per22 The original triplets were provided by Unbabel (https: //unbabel.com/). 28 in terms of Translation Error Rate (TER) (Snover et al., 2006a), an evaluation metric commonly used in MT-related tasks (e.g. in quality estimation) to measure the minimum edit distance between an automatic translation and a reference translation.23 Systems are ranked based on the average TER calculated on the test set by using the TERcom24 software: lowe"
W15-3001,W10-1703,1,0.163282,"Missing"
W15-3001,P15-2026,1,0.797338,"teps aimed at removing duplicates and those triplets in which any of the elements (source, target, post-edition) was either too long or too short compared to the others, or included tags or special problematic symbols. The main reason for random sampling was to induce some homogeneity across the three datasets and, in turn, Automatic Post-editing Task This year WMT hosted for the first time a shared task on automatic post-editing (APE) for machine translation. The task requires to automatically correct the errors present in a machine translated text. As pointed out in Parton et al. (2012) and Chatterjee et al. (2015b), from the application point of view, APE components would make it possible to: • Improve MT output by exploiting information unavailable to the decoder, or by per22 The original triplets were provided by Unbabel (https: //unbabel.com/). 28 in terms of Translation Error Rate (TER) (Snover et al., 2006a), an evaluation metric commonly used in MT-related tasks (e.g. in quality estimation) to measure the minimum edit distance between an automatic translation and a reference translation.23 Systems are ranked based on the average TER calculated on the test set by using the TERcom24 software: lowe"
W15-3001,W12-3102,1,0.571688,"ator agreement, both for inter- and intra-annotator agreement scores. not included to make the graphs viewable). The plots cleary suggest that a fair comparison of systems of different kinds cannot rely on automatic scores. Rule-based systems receive a much lower BLEU score than statistical systems (see for instance English–German, e.g., PROMT- RULE). The same is true to a lesser degree for statistical syntax-based systems (see English–German, UEDIN - SYNTAX ) and online systems that were not tuned to the shared task (see Czech–English, CU TECTO vs. the cluster of tuning task systems TT*). 4 (Callison-Burch et al., 2012; Bojar et al., 2013, 2014), with tasks including both sentence and word-level estimation, using new training and test datasets, and an additional task: document-level prediction. The goals of this year’s shared task were: • Advance work on sentence- and wordlevel quality estimation by providing larger datasets. • Investigate the effectiveness of quality labels, features and learning methods for documentlevel prediction. Quality Estimation Task • Explore differences between sentence-level and document-level prediction. The fourth edition of the WMT shared task on quality estimation (QE) of mac"
W15-3001,W15-3008,0,0.0135172,"secondary key. The results for the scoring variant are presented in Table 9, sorted from best to worst by using the MAE metric scores as primary key and the RMSE metric scores as secondary key. Pearson’s r coefficients for all systems against HTER is given in Table 10. As discussed in (Graham, 2015), the results according to this metric can rank participating systems differently. In particular, we note the SHEF/GP submission, are which is deemed significantly worse than the baseline system according to MAE, but substantially better than the baseline according to Pearson’s correlation. Graham (2015) argues that the use of MAE as evaluation score for quality estimation tasks is inadequate, as MAE is very sensitive to variance. This means that a system that outputs predictions with high variance is more likely to have high MAE score, even if the distribution follows that of the true labels. Interestingly, according to Pearson’s correlation, the systems are • Source sentence (English). • Automatic translation (Spanish). • Manual post-edition of the automatic translation. • Word-level binary (“OK”/“BAD”) labelling of the automatic translation. The binary labels for the datasets were acquired"
W15-3001,W09-0401,1,0.251315,"Missing"
W15-3001,W15-3009,0,0.0460438,"Missing"
W15-3001,W15-3010,0,0.035772,"Missing"
W15-3001,P10-4002,0,0.00861255,"t sentences and number of punctuation marks in source and target sentences. All other features were averaged. The implementation for document-level feature extraction is available in Q U E ST ++ (Specia et al., 2015).12 These features were then used to train a SVR algorithm with RBF kernel using the SCIKIT- LEARN toolkit. The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. • Source token aligned to the target token, its left and right contexts of one word. The alignments were produced with the force align.py script, which is part of cdec (Dyer et al., 2010). It allows to align new parallel data with a pre-trained alignment model built with the cdec word aligner (fast align). The alignment model was trained on the Europarl corpus (Koehn, 2005). • Boolean dictionary features: whether target token is a stopword, a punctuation mark, a proper noun, a number. 4.2 Participants Table 7 lists all participating teams submitting systems to any of the tasks. Each team was allowed up to two submissions for each task and language pair. In the descriptions below, participation in specific tasks is denoted by a task identifier. • Target language model features:"
W15-3001,W15-3011,0,0.0373266,"Missing"
W15-3001,W15-3036,0,0.0738999,"Missing"
W15-3001,W15-3012,0,0.0168435,"secondary key. The results for the scoring variant are presented in Table 9, sorted from best to worst by using the MAE metric scores as primary key and the RMSE metric scores as secondary key. Pearson’s r coefficients for all systems against HTER is given in Table 10. As discussed in (Graham, 2015), the results according to this metric can rank participating systems differently. In particular, we note the SHEF/GP submission, are which is deemed significantly worse than the baseline system according to MAE, but substantially better than the baseline according to Pearson’s correlation. Graham (2015) argues that the use of MAE as evaluation score for quality estimation tasks is inadequate, as MAE is very sensitive to variance. This means that a system that outputs predictions with high variance is more likely to have high MAE score, even if the distribution follows that of the true labels. Interestingly, according to Pearson’s correlation, the systems are • Source sentence (English). • Automatic translation (Spanish). • Manual post-edition of the automatic translation. • Word-level binary (“OK”/“BAD”) labelling of the automatic translation. The binary labels for the datasets were acquired"
W15-3001,W15-4903,0,0.0623526,"Missing"
W15-3001,W15-3013,1,0.843414,"Missing"
W15-3001,W11-2123,0,0.0240468,"27,101 5,966 8,816 SRC 13,701 3,765 5,307 Lemmas TGT PE 7,624 7,689 2,810 2,819 3,778 3,814 Table 18: Data statistics. and classifying each word of a sentence as good or bad. An automatic translation to be post-edited is first decoded by our SPE system, then fed into one of the classifiers identified as SVM180feat and RNN. The HTER estimator selects the translation with the lower score while the binary word-level classifier selects the translation with the fewer amount of bad tags. The official evaluation of the shared task show an advantage of the RNN approach compared to SVM. KenLM toolkit (Heafield, 2011) for standard ngram modeling with an n-gram length of 5. Finally, the APE system was tuned on the development set, optimizing TER with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison, computed for both evaluation modalities (case sensitive/insensitive), are also reported in Tables 20 and 21. For each submitted run, the statistical significance of performance differences with respect to the baseline and the re-implementation of Simard et al. (2007) is calculated with the bootstrap test (Koehn, 2004). 5.2 FBK. The two runs submitted by FBK (Chatterjee e"
W15-3001,W08-0509,0,0.0268564,"ere collected by means of a crowdsourcing platform developed by the data provider. Monolingual translation as another term of comparison. To get further insights about the progress with respect to previous APE methods, participants’ results are also analysed with respect to another term of comparison: a reimplementation of the state-of-the-art approach firstly proposed by Simard et al. (2007).27 For this purpose, a phrase-based SMT system based on Moses (Koehn et al., 2007) is used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the Test data (1, 817 instances) consists of (source, target) pairs having similar characteristics of those in the training set. Human post-editions of the test target instances were left apart to measure system performance. The data creation procedure adopted, as well as the origin and the domain of the texts pose specific challenges to the participating systems. As discussed in Section 5.4, the results of this pilot task can be partially explained in light of such challenges. This dataset, however, has three major advantages that made it sui"
W15-3001,W15-3014,0,0.0344469,"Missing"
W15-3001,P15-1174,0,0.0105686,"415 for test. Since no human annotation exists for the quality of entire paragraphs (or documents), Meteor against reference translations was used as quality label for this task. Meteor was calculated using its implementation within the Asyia toolkit, with the following settings: exact match, tokenised and case insensitive (Gim´enez and M`arquez, 2010). guage pairs. All systems were significantly better than the baseline. However, the difference between the baseline system and all submissions was much lower in the scoring evaluation than in the ranking evaluation. Following the suggestion in (Graham, 2015), Table 16 shows an alternative ranking of systems considering Pearson’s r correlation results. The alternative ranking differs from the official ranking in terms of MAE: for EN-DE, RTMDCU/RTM-FS-SVR is no longer in the winning group, while for DE-EN, USHEF/QUEST-DISCBO and USAAR-USHEF/BFF did not show statistically significant difference against the baseline. However, as with Task 1 these results are the same as the official ones in terms of DeltaAvg. 4.6 Discussion In what follows, we discuss the main findings of this year’s shared task based on the goals we had previously identified for it."
W15-3001,W04-3250,1,0.485885,"f the RNN approach compared to SVM. KenLM toolkit (Heafield, 2011) for standard ngram modeling with an n-gram length of 5. Finally, the APE system was tuned on the development set, optimizing TER with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison, computed for both evaluation modalities (case sensitive/insensitive), are also reported in Tables 20 and 21. For each submitted run, the statistical significance of performance differences with respect to the baseline and the re-implementation of Simard et al. (2007) is calculated with the bootstrap test (Koehn, 2004). 5.2 FBK. The two runs submitted by FBK (Chatterjee et al., 2015a) are based on combining the statistical phrase-based post-editing approach proposed by Simard et al. (2007) and its most significant variant proposed by B´echara et al. (2011). The APE systems are built-in an incremental manner. At each stage of the APE pipeline, the best configuration of a component is decided and then used in the next stage. The APE pipeline begins with the selection of the best language model from several language models trained on different types and quantities of data. The next stage addresses the possible"
W15-3001,2005.mtsummit-papers.11,1,0.0759255,"(Specia et al., 2015).12 These features were then used to train a SVR algorithm with RBF kernel using the SCIKIT- LEARN toolkit. The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. • Source token aligned to the target token, its left and right contexts of one word. The alignments were produced with the force align.py script, which is part of cdec (Dyer et al., 2010). It allows to align new parallel data with a pre-trained alignment model built with the cdec word aligner (fast align). The alignment model was trained on the Europarl corpus (Koehn, 2005). • Boolean dictionary features: whether target token is a stopword, a punctuation mark, a proper noun, a number. 4.2 Participants Table 7 lists all participating teams submitting systems to any of the tasks. Each team was allowed up to two submissions for each task and language pair. In the descriptions below, participation in specific tasks is denoted by a task identifier. • Target language model features: – The order of the highest order n-gram which starts or ends with the target token. – Backoff behaviour of the n-grams (ti−2 , ti−1 , ti ), (ti−1 , ti , ti+1 ), (ti , ti+1 , ti+2 ), where"
W15-3001,W15-3039,1,0.80659,"Missing"
W15-3001,J10-4005,0,0.0619684,"Missing"
W15-3001,J82-2005,0,0.818658,"Missing"
W15-3001,W14-3342,0,0.0353204,"t although the system is referred to as “baseline”, it is in fact a strong system. It has proved robust across a range of language pairs, MT systems, and text domains for predicting various forms of post-editing effort (Callison-Burch et al., 2012; Bojar et al., 2013, 2014). Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool9 . For the baseline system we used a number of features that have been found the most informative in previous research on word-level quality estimation. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 25 features: Baseline systems Sentence-level baseline system: For Task 1, Q U E ST7 (Specia et al., 2013) was used to extract 17 MT system-independent features from the source and translation (target) files and parallel corpora: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), but the length of a sentence might influence the probability of a word being incorrect. • Number of tokens in the source and target sentences. • Ave"
W15-3001,W13-2248,0,0.0824189,"Missing"
W15-3001,W06-3114,1,0.427665,"translation systems from 24 institutions were submitted to the ten translation directions in the standard translation task. An additional 7 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had three subtasks, with a total of 10 teams, submitting 34 entries. The pilot automatic postediting task had a total of 4 teams, submitting 7 entries. 1 Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at EMNLP 2015. This workshop builds on eight previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014). This year we conducted five official tasks: a translation task, a quality estimation task, a metrics task, a tuning task1 , and a automatic postediting task. In the translation task (§2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Finnish, and Russian. The Finnish translation 1 The metrics and tuning tasks are reported in separate papers (S"
W15-3001,W15-3015,0,0.0443946,"Missing"
W15-3001,W15-3016,0,0.0442638,"Missing"
W15-3001,W15-3037,0,0.0800739,"Missing"
W15-3001,P03-1021,0,0.0587969,"utomatic translation to be post-edited is first decoded by our SPE system, then fed into one of the classifiers identified as SVM180feat and RNN. The HTER estimator selects the translation with the lower score while the binary word-level classifier selects the translation with the fewer amount of bad tags. The official evaluation of the shared task show an advantage of the RNN approach compared to SVM. KenLM toolkit (Heafield, 2011) for standard ngram modeling with an n-gram length of 5. Finally, the APE system was tuned on the development set, optimizing TER with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison, computed for both evaluation modalities (case sensitive/insensitive), are also reported in Tables 20 and 21. For each submitted run, the statistical significance of performance differences with respect to the baseline and the re-implementation of Simard et al. (2007) is calculated with the bootstrap test (Koehn, 2004). 5.2 FBK. The two runs submitted by FBK (Chatterjee et al., 2015a) are based on combining the statistical phrase-based post-editing approach proposed by Simard et al. (2007) and its most significant variant proposed by B´echara"
W15-3001,padro-stanilovsky-2012-freeling,0,0.011024,"o rankings are not identical, none of the systems was particularly penalized by the case sensitive evaluation. Indeed, individual differences in the two modes are always close to the same value (∼ 0.7 TER difference) measured for the two baselines. USAAR-SAPE. The USAAR-SAPE system (Pal et al., 2015b) is designed with three basic components: corpus preprocessing, hybrid word alignment and a state-of-the-art phrase-based SMT system integrated with the hybrid word alignment. The preprocessing of the training corpus is carried out by stemming the Spanish MT output and the PE data using Freeling (Padr and Stanilovsky, 2012). The hybrid word alignment method combines different kinds of word alignment: GIZA++ word alignment with the 31 ID Baseline FBK Primary LIMSI Primary USAAR-SAPE LIMSI Contrastive Abu-MaTran Primary FBK Contrastive (Simard et al., 2007) Abu-MaTran Contrastive Avg. TER 22.913 23.228 23.331 23.426 23.573 23.639 23.649 23.839 24.715 ID Baseline LIMSI Primary FBK Primary USAAR-SAPE Abu-MaTran Primary LIMSI Contrastive FBK Contrastive (Simard et al., 2007) Abu-MaTran Contrastive Table 20: Official results for the WMT15 Automatic Post-editing task – average TER (↓) case sensitive. Table 21: Official"
W15-3001,W15-3038,0,0.0668224,"Missing"
W15-3001,W13-2814,0,0.0155219,"ject (Prompsit) Fondazione Bruno Kessler, Italy (Chatterjee et al., 2015a) Laboratoire d’Informatique pour la M´ecanique et les Sciences de l’Ing´enieur, France (Wisniewski et al., 2015) Saarland University, Germany & Jadavpur University, India (Pal et al., 2015b) Table 19: Participants in the WMT15 Automatic Post-editing pilot task. grow-diag-final-and (GDFA) heuristic (Koehn, 2010), SymGiza++ (Junczys-Dowmunt and Szal, 2011), the Berkeley aligner (Liang et al., 2006), and the edit distance-based aligners (Snover et al., 2006a; Lavie and Agarwal, 2007). These different word alignment tables (Pal et al., 2013) are combined by a mathematical union method. For the phrase-based SMT system various maximum phrase lengths for the translation model and n–gram settings for the language model are used. The best results in terms of BLEU (Papineni et al., 2002) score are achieved by a maximum phrase length of 7 for the translation model and a 5-gram language model. the model. These features measure the similarity and the reliability of the translation options and help to improve the precision of the resulting APE system. LIMSI. For the first edition of the APE shared task LIMSI submitted two systems (Wisniews"
W15-3001,W07-0734,0,0.0277649,"Abu-MaTran FBK LIMSI USAAR-SAPE Participating team Abu-MaTran Project (Prompsit) Fondazione Bruno Kessler, Italy (Chatterjee et al., 2015a) Laboratoire d’Informatique pour la M´ecanique et les Sciences de l’Ing´enieur, France (Wisniewski et al., 2015) Saarland University, Germany & Jadavpur University, India (Pal et al., 2015b) Table 19: Participants in the WMT15 Automatic Post-editing pilot task. grow-diag-final-and (GDFA) heuristic (Koehn, 2010), SymGiza++ (Junczys-Dowmunt and Szal, 2011), the Berkeley aligner (Liang et al., 2006), and the edit distance-based aligners (Snover et al., 2006a; Lavie and Agarwal, 2007). These different word alignment tables (Pal et al., 2013) are combined by a mathematical union method. For the phrase-based SMT system various maximum phrase lengths for the translation model and n–gram settings for the language model are used. The best results in terms of BLEU (Papineni et al., 2002) score are achieved by a maximum phrase length of 7 for the translation model and a 5-gram language model. the model. These features measure the similarity and the reliability of the translation options and help to improve the precision of the resulting APE system. LIMSI. For the first edition of"
W15-3001,W15-3017,0,0.0328586,"Missing"
W15-3001,W15-3026,0,0.0488223,"Missing"
W15-3001,W15-3040,1,0.889327,"HIDDEN Participating team Dublin City University, Ireland and University of Sheffield, UK (Logacheva et al., 2015) Heidelberg University, Germany (Kreutzer et al., 2015) Lorraine Laboratory of Research in Computer Science and its Applications, France (Langlois, 2015) Dublin City University, Ireland (Bicici et al., 2015) Shenyang Aerospace University, China (Shang et al., 2015) University of Sheffield Team 1, UK (Shah et al., 2015) Alicant University, Spain (Espl`a-Gomis et al., 2015a) Ghent University, Belgium (Tezcan et al., 2015) University of Sheffield, UK and Saarland University, Germany (Scarton et al., 2015a) University of Sheffield, UK (Scarton et al., 2015a) Undisclosed Table 7: Participants in the WMT15 quality estimation shared task. one from the official training data. Pseudoreferences were produced by three online systems. These features measure the intersection between n-gram sets of the target sentence and of the pseudo-references. Three sets of features were extracted from each online system, and a fourth feature was extracted measuring the inter-agreement among the three online systems and the target system. and fine-tuned for the quality estimation classification task by back-propagat"
W15-3001,W15-4916,1,0.80858,"Missing"
W15-3001,P02-1040,0,0.108957,"ndia (Pal et al., 2015b) Table 19: Participants in the WMT15 Automatic Post-editing pilot task. grow-diag-final-and (GDFA) heuristic (Koehn, 2010), SymGiza++ (Junczys-Dowmunt and Szal, 2011), the Berkeley aligner (Liang et al., 2006), and the edit distance-based aligners (Snover et al., 2006a; Lavie and Agarwal, 2007). These different word alignment tables (Pal et al., 2013) are combined by a mathematical union method. For the phrase-based SMT system various maximum phrase lengths for the translation model and n–gram settings for the language model are used. The best results in terms of BLEU (Papineni et al., 2002) score are achieved by a maximum phrase length of 7 for the translation model and a 5-gram language model. the model. These features measure the similarity and the reliability of the translation options and help to improve the precision of the resulting APE system. LIMSI. For the first edition of the APE shared task LIMSI submitted two systems (Wisniewski et al., 2015). The first one is based on the approach of Simard et al. (2007) and considers the APE task as a monolingual translation between a translation hypothesis and its post-edition. This straightforward approach does not succeed in imp"
W15-3001,2012.eamt-1.34,0,0.0799883,"Missing"
W15-3001,W15-3023,0,0.0267499,"Missing"
W15-3001,W15-3018,0,0.0412931,"Missing"
W15-3001,W15-3041,1,0.847113,"Missing"
W15-3001,potet-etal-2012-collection,0,0.011237,"Missing"
W15-3001,W15-3042,0,0.0858502,"Missing"
W15-3001,W15-3019,0,0.0362545,"Missing"
W15-3001,N07-1064,0,0.644928,"nsitive) are reported in Tables 20 and 21. • The target (TGT) is a tokenized Spanish translation of the source, produced by an unknown MT system; • The human post-edition (PE) is a manuallyrevised version of the target. PEs were collected by means of a crowdsourcing platform developed by the data provider. Monolingual translation as another term of comparison. To get further insights about the progress with respect to previous APE methods, participants’ results are also analysed with respect to another term of comparison: a reimplementation of the state-of-the-art approach firstly proposed by Simard et al. (2007).27 For this purpose, a phrase-based SMT system based on Moses (Koehn et al., 2007) is used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the Test data (1, 817 instances) consists of (source, target) pairs having similar characteristics of those in the training set. Human post-editions of the test target instances were left apart to measure system performance. The data creation procedure adopted, as well as the origin and the domain of the texts pose specifi"
W15-3001,W15-3022,0,0.0629217,"Missing"
W15-3001,P15-4020,1,0.0689693,"://github.com/lspecia/quest 14 http://scikit-learn.org/ https://github.com/qe-team/marmot • Target token, its left and right contexts of one word. Document-level baseline system: For Task 3, the baseline features for sentence-level prediction were used. These are aggregated by summing or averaging their values for the entire document. Features that were summed: number of tokens in the source and target sentences and number of punctuation marks in source and target sentences. All other features were averaged. The implementation for document-level feature extraction is available in Q U E ST ++ (Specia et al., 2015).12 These features were then used to train a SVR algorithm with RBF kernel using the SCIKIT- LEARN toolkit. The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. • Source token aligned to the target token, its left and right contexts of one word. The alignments were produced with the force align.py script, which is part of cdec (Dyer et al., 2010). It allows to align new parallel data with a pre-trained alignment model built with the cdec word aligner (fast align). The alignment model was trained on the Europarl corpus (Koehn, 2005). • Boole"
W15-3001,W15-3027,0,0.0625354,"Missing"
W15-3001,P13-4014,1,0.762876,"Missing"
W15-3001,2013.mtsummit-papers.15,0,0.055239,"these rules is based on an analysis of the most frequent error corrections and aims at: i) predicting word case; ii) predicting exclamation and interrogation marks; and iii) predicting verbal endings. Experiments with this approach show that this system also hurts translation quality. An in-depth analysis revealed that this negative result is mainly explained by two reasons: i) most of the post-edition operations are nearly unique, which makes very difficult to generalize from a small amount of data; and ii) even when they are not, the high variability of post-editing, already pointed out by Wisniewski et al. (2013), results in predicting legitimate corrections that have not been made by the annotators, therefore preventing from improving over the baseline. 5.3 Results The official results achieved by the participating systems are reported in Tables 20 and 21. The seven runs submitted are sorted based on the average TER they achieve on test data. Table 20 shows the results computed in case sensitive mode, while Table 21 provides scores computed in the case insensitive mode. Both rankings reveal an unexpected outcome: none of the submitted runs was able to beat the baselines (i.e. average TER scores of 22"
W15-3001,W15-3032,1,0.810291,"Missing"
W15-3001,W15-3031,1,0.376914,"Missing"
W15-3001,W15-3020,1,0.808362,"Missing"
W15-3001,W15-3043,0,0.0443554,"Missing"
W15-3001,W15-3021,0,0.038175,"Missing"
W15-3001,P07-2045,1,\N,Missing
W15-3001,W15-3004,0,\N,Missing
W15-3001,2015.eamt-1.4,0,\N,Missing
W15-3001,N06-1014,0,\N,Missing
W15-3001,2015.eamt-1.17,1,\N,Missing
W15-3001,2012.iwslt-papers.12,0,\N,Missing
W15-4304,D10-1044,0,0.216563,"Missing"
W15-4304,D11-1033,0,0.0616033,"Missing"
W15-4304,P05-1071,0,0.0736573,"periments on a wellknown and established online SMT system. 4 Error analysis and results We perform four series of experiments, each with the goal of answering different questions about SMT for UG text: SMT systems All experiments presented in this paper are performed with our in-house state-of-the-art system based on phrase-based SMT and similar to Moses (Koehn et al., 2007). Our Arabic-English system is built from 1.75M lines (52.9M source tokens) of parallel text, and our Chinese-English system from 3.13M lines (55.4M source tokens) of parallel text. We tokenize all Arabic data using MADA (Habash and Rambow, 2005), ATB scheme, and we segment the Chinese data following Tseng et al. (2005). Both systems use an adapted 5-gram English language model that linearly interpolates different English Gigaword subcorpora with the 1. How large is the gap in translation quality between news and different types of UG data? (§4.1). To answer this question, we measure the BLEU score of two state-of-the-art SMT system outputs on all our data sets. 2. What kind of translation choices does the SMT system make for UG data? To answer this question, we measure phrase lengths used during the translation (or decoding) process"
W15-4304,W05-0909,0,0.0136668,"es on five different UG benchmark sets for two language pairs, Arabic-English and Chinese-English, with the goals of (i) explaining the typically poor SMT performance observed for UG texts, and (ii) identifying translation modeling Introduction User-generated (UG) text such as found on social media and web forums poses different challenges to statistical machine translation (SMT) than formal text. This is reflected by poor translation quality for informal genres (see for example Figure 1), which is typically measured with automatic quality metrics such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), or TER (Snover et al., 2006). These scores alone, however, only reflect the overall translation quality, and do not provide any insight in what exactly makes translating UG text hard. While such knowledge is crucial for improving SMT of UG text, surprisingly little work on error analysis for SMT of usergenerated text has been reported. Moreover, the notion of user-generated content 1 One of the very few exceptions is NIST OpenMT 2015, which focusses entirely on translating informal genres. 28 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 28–37, c Beijing, China, Ju"
W15-4304,W12-3154,0,0.0451109,"slation of UG data. We not only contrast our observations with two news data sets, but we also show that SMT quality can vary significantly across different types of UG content, and that different UG types exhibit dissimilar error distributions. Specifically, we summarize our main findings as follows: on SMT error analysis studies the effect of domain adaptation on SMT, for example by examining in which stage of the SMT pipeline the available indomain data can best be used (Duh et al., 2010), or whether it is more promising to improve either phrase extraction or scoring (Bisazza et al., 2011; Haddow and Koehn, 2012). The vast majority of SMT research, including the above described work on error analysis, is evaluated on data containing formal language. Work on SMT of informal text mostly targets reduction of OOV words in the source text, for example by correcting spelling errors (Bertoldi et al., 2010), normalizing noisy text to more formal text (Banerjee et al., 2012; Ling et al., 2013a), or enhancing the training data with bilingual segments extracted from Twitter (Jehl et al., 2012; Ling et al., 2013b). Other work improves SMT of UG text by combining statistical and rule-based MT (Carrera et al., 2009"
W15-4304,2011.mtsummit-papers.32,0,0.206512,"Missing"
W15-4304,D11-1125,0,0.185121,"Missing"
W15-4304,2012.eamt-1.41,0,0.200338,"Missing"
W15-4304,W14-1617,0,0.0726374,". For example, SMS and chat data might benefit from text normalization (Bertoldi et al., 2010; Yvon, 2010; Ling et al., 2013a) or otherwise resolving source OOVs, which also has been the main focus of previous work on SMT for UG. On the other hand, while research in domain adaptation for SMT often aims at better scoring of existing translation candidates, we have shown that for many UG tasks the most promising direction involves increasing phrase pair recall of the SMT models (i.e., reducing phrase pair OOVs), for example by paraphrasing (Callison-Burch et al., 2006) or translation synthesis (Irvine and Callison-Burch, 2014). Conclusions and future directions Translating user-generated (UG) text is a difficult task for SMT. To explain the poor translation quality observed for UG data, we have performed a detailed error analysis on two language pairs (Arabic-English and Chinese-English) and five different types of UG data (SMS, chat, CTS, weblogs, and comments). Our quantitative results show among others that (i) UG data is translated with shorter source phrases than news, (ii) UG translation model coverage deteriorates substantially for longer phrases, and (iii) phrase-pair Acknowledgments This research was funde"
W15-4304,Q13-1035,0,0.14721,"Missing"
W15-4304,N10-1064,0,0.504951,"follows: on SMT error analysis studies the effect of domain adaptation on SMT, for example by examining in which stage of the SMT pipeline the available indomain data can best be used (Duh et al., 2010), or whether it is more promising to improve either phrase extraction or scoring (Bisazza et al., 2011; Haddow and Koehn, 2012). The vast majority of SMT research, including the above described work on error analysis, is evaluated on data containing formal language. Work on SMT of informal text mostly targets reduction of OOV words in the source text, for example by correcting spelling errors (Bertoldi et al., 2010), normalizing noisy text to more formal text (Banerjee et al., 2012; Ling et al., 2013a), or enhancing the training data with bilingual segments extracted from Twitter (Jehl et al., 2012; Ling et al., 2013b). Other work improves SMT of UG text by combining statistical and rule-based MT (Carrera et al., 2009), or models trained on formal and informal data (Banerjee et al., 2011). Finally, Roturier and Bensadoun (2011) conduct a comparative study to determine the ability of several SMT systems to translate UG text, but they do not examine what errors the systems make. To our knowledge, our work"
W15-4304,W12-3153,0,0.0782671,"t al., 2010), or whether it is more promising to improve either phrase extraction or scoring (Bisazza et al., 2011; Haddow and Koehn, 2012). The vast majority of SMT research, including the above described work on error analysis, is evaluated on data containing formal language. Work on SMT of informal text mostly targets reduction of OOV words in the source text, for example by correcting spelling errors (Bertoldi et al., 2010), normalizing noisy text to more formal text (Banerjee et al., 2012; Ling et al., 2013a), or enhancing the training data with bilingual segments extracted from Twitter (Jehl et al., 2012; Ling et al., 2013b). Other work improves SMT of UG text by combining statistical and rule-based MT (Carrera et al., 2009), or models trained on formal and informal data (Banerjee et al., 2011). Finally, Roturier and Bensadoun (2011) conduct a comparative study to determine the ability of several SMT systems to translate UG text, but they do not examine what errors the systems make. To our knowledge, our work is the first that looks inside an SMT system to systematically inspect its behavior across a diverse spectrum of UG text types. • The SMS and chat benchmarks are the most distant from fo"
W15-4304,2011.iwslt-evaluation.18,1,0.877498,"ressed to improve translation of UG data. We not only contrast our observations with two news data sets, but we also show that SMT quality can vary significantly across different types of UG content, and that different UG types exhibit dissimilar error distributions. Specifically, we summarize our main findings as follows: on SMT error analysis studies the effect of domain adaptation on SMT, for example by examining in which stage of the SMT pipeline the available indomain data can best be used (Duh et al., 2010), or whether it is more promising to improve either phrase extraction or scoring (Bisazza et al., 2011; Haddow and Koehn, 2012). The vast majority of SMT research, including the above described work on error analysis, is evaluated on data containing formal language. Work on SMT of informal text mostly targets reduction of OOV words in the source text, for example by correcting spelling errors (Bertoldi et al., 2010), normalizing noisy text to more formal text (Banerjee et al., 2012; Ling et al., 2013a), or enhancing the training data with bilingual segments extracted from Twitter (Jehl et al., 2012; Ling et al., 2013b). Other work improves SMT of UG text by combining statistical and rule-based"
W15-4304,N03-1017,0,0.0350837,"Chat SMS Figure 3: Average source-side and target-side phrase lengths used during decoding. 4.2 4. Why did the SMT system make the translation choices that it made? What errors are observed for each benchmark, and how often? To answer these questions, we reimplement the word-alignment driven error analysis approach by Irvine et al. (2013) and perform a qualitative analysis on the results (§4.4). 4.1 Translation phrase length analysis Most state-of-the-art SMT systems, including our in-house system, are phrase-based, with translations being generated phrase by phrase rather than word by word (Koehn et al., 2003). An abundant use of small phrases during decoding indicates that the system is not taking advantage of the model’s ability to memorize large contextual and possibly non-compositional translation blocks. It is therefore interesting to measure the average phrase length (i.e., number of tokens) used by the system, for the source as well as the target language (Figure 3). For Arabic-English we see that source-side phrases are noticeably longer for both news benchmarks than for the UG data sets. The average target-side phrase length, on the other hand, shows less correlation with the genres of the"
W15-4304,N06-1003,0,0.0673689,"ns, demanding diverse strategies to improve SMT quality. For example, SMS and chat data might benefit from text normalization (Bertoldi et al., 2010; Yvon, 2010; Ling et al., 2013a) or otherwise resolving source OOVs, which also has been the main focus of previous work on SMT for UG. On the other hand, while research in domain adaptation for SMT often aims at better scoring of existing translation candidates, we have shown that for many UG tasks the most promising direction involves increasing phrase pair recall of the SMT models (i.e., reducing phrase pair OOVs), for example by paraphrasing (Callison-Burch et al., 2006) or translation synthesis (Irvine and Callison-Burch, 2014). Conclusions and future directions Translating user-generated (UG) text is a difficult task for SMT. To explain the poor translation quality observed for UG data, we have performed a detailed error analysis on two language pairs (Arabic-English and Chinese-English) and five different types of UG data (SMS, chat, CTS, weblogs, and comments). Our quantitative results show among others that (i) UG data is translated with shorter source phrases than news, (ii) UG translation model coverage deteriorates substantially for longer phrases, an"
W15-4304,P13-1126,0,0.400947,"Missing"
W15-4304,P07-2045,0,0.0360285,"eriments use the same SMT models, but we tune parameters separately for each benchmark set using pairwise ranking optimization (PRO) (Hopkins and May, 2011). To put the results of our system into perspective, we also run a first series of experiments on a wellknown and established online SMT system. 4 Error analysis and results We perform four series of experiments, each with the goal of answering different questions about SMT for UG text: SMT systems All experiments presented in this paper are performed with our in-house state-of-the-art system based on phrase-based SMT and similar to Moses (Koehn et al., 2007). Our Arabic-English system is built from 1.75M lines (52.9M source tokens) of parallel text, and our Chinese-English system from 3.13M lines (55.4M source tokens) of parallel text. We tokenize all Arabic data using MADA (Habash and Rambow, 2005), ATB scheme, and we segment the Chinese data following Tseng et al. (2005). Both systems use an adapted 5-gram English language model that linearly interpolates different English Gigaword subcorpora with the 1. How large is the gap in translation quality between news and different types of UG data? (§4.1). To answer this question, we measure the BLEU"
W15-4304,2010.iwslt-papers.5,0,0.0753594,"ng, China, July 31, 2015. 2015 Association for Computational Linguistics aspects that should be addressed to improve translation of UG data. We not only contrast our observations with two news data sets, but we also show that SMT quality can vary significantly across different types of UG content, and that different UG types exhibit dissimilar error distributions. Specifically, we summarize our main findings as follows: on SMT error analysis studies the effect of domain adaptation on SMT, for example by examining in which stage of the SMT pipeline the available indomain data can best be used (Duh et al., 2010), or whether it is more promising to improve either phrase extraction or scoring (Bisazza et al., 2011; Haddow and Koehn, 2012). The vast majority of SMT research, including the above described work on error analysis, is evaluated on data containing formal language. Work on SMT of informal text mostly targets reduction of OOV words in the source text, for example by correcting spelling errors (Bertoldi et al., 2010), normalizing noisy text to more formal text (Banerjee et al., 2012; Ling et al., 2013a), or enhancing the training data with bilingual segments extracted from Twitter (Jehl et al.,"
W15-4304,D13-1008,0,0.235712,"by examining in which stage of the SMT pipeline the available indomain data can best be used (Duh et al., 2010), or whether it is more promising to improve either phrase extraction or scoring (Bisazza et al., 2011; Haddow and Koehn, 2012). The vast majority of SMT research, including the above described work on error analysis, is evaluated on data containing formal language. Work on SMT of informal text mostly targets reduction of OOV words in the source text, for example by correcting spelling errors (Bertoldi et al., 2010), normalizing noisy text to more formal text (Banerjee et al., 2012; Ling et al., 2013a), or enhancing the training data with bilingual segments extracted from Twitter (Jehl et al., 2012; Ling et al., 2013b). Other work improves SMT of UG text by combining statistical and rule-based MT (Carrera et al., 2009), or models trained on formal and informal data (Banerjee et al., 2011). Finally, Roturier and Bensadoun (2011) conduct a comparative study to determine the ability of several SMT systems to translate UG text, but they do not examine what errors the systems make. To our knowledge, our work is the first that looks inside an SMT system to systematically inspect its behavior ac"
W15-4304,P13-1018,0,0.221962,"by examining in which stage of the SMT pipeline the available indomain data can best be used (Duh et al., 2010), or whether it is more promising to improve either phrase extraction or scoring (Bisazza et al., 2011; Haddow and Koehn, 2012). The vast majority of SMT research, including the above described work on error analysis, is evaluated on data containing formal language. Work on SMT of informal text mostly targets reduction of OOV words in the source text, for example by correcting spelling errors (Bertoldi et al., 2010), normalizing noisy text to more formal text (Banerjee et al., 2012; Ling et al., 2013a), or enhancing the training data with bilingual segments extracted from Twitter (Jehl et al., 2012; Ling et al., 2013b). Other work improves SMT of UG text by combining statistical and rule-based MT (Carrera et al., 2009), or models trained on formal and informal data (Banerjee et al., 2011). Finally, Roturier and Bensadoun (2011) conduct a comparative study to determine the ability of several SMT systems to translate UG text, but they do not examine what errors the systems make. To our knowledge, our work is the first that looks inside an SMT system to systematically inspect its behavior ac"
W15-4304,D09-1074,0,0.461159,"Missing"
W15-4304,J03-1002,0,0.00461181,"the different data sets and between the two SMT systems. Still, translation quality is worse for the UG data sets than for news, indicating that also for this language pair translating UG text is more challenging than translating news. As all subsequent analyses require systeminternal information, we carry out the experiments with our in-house system only. 4.3 Model coverage analysis Next, we examine the translation model coverage for each data set, which tells us what phrases the system could have used for decoding. For each of our test sets, we create automatic word alignments using GIZA++ (Och and Ney, 2003), and extract from these the set of all reference phrase pairs using Moses’ phrase extraction algorithm (Koehn et al., 2007). By comparing this set of phrase pairs to the available phrases in the SMT models, which 31 Source phrase recall Genre BLEU LM PP Target phrase recall Phrase pair recall 1 2 3 4 1 2 3 4 1 2 3 4 News 1 News 2 33.8 21.5 65 86 99.7 99.6 88.9 88.1 56.3 53.7 26.1 21.8 99.7 99.5 91.1 88.1 61.5 53.4 29.6 23.6 84.9 77.4 54.4 46.9 23.6 18.8 8.1 5.9 Weblogs Comments CTS Chat SMS 22.3 17.2 16.0 10.0 8.8 152 117 103 179 196 99.2 97.7 97.4 94.1 93.7 80.5 80.2 66.3 56.0 57.8 40.6 43.0"
W15-4304,P02-1040,0,0.0927576,"k, we conduct a series of analyses on five different UG benchmark sets for two language pairs, Arabic-English and Chinese-English, with the goals of (i) explaining the typically poor SMT performance observed for UG texts, and (ii) identifying translation modeling Introduction User-generated (UG) text such as found on social media and web forums poses different challenges to statistical machine translation (SMT) than formal text. This is reflected by poor translation quality for informal genres (see for example Figure 1), which is typically measured with automatic quality metrics such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), or TER (Snover et al., 2006). These scores alone, however, only reflect the overall translation quality, and do not provide any insight in what exactly makes translating UG text hard. While such knowledge is crucial for improving SMT of UG text, surprisingly little work on error analysis for SMT of usergenerated text has been reported. Moreover, the notion of user-generated content 1 One of the very few exceptions is NIST OpenMT 2015, which focusses entirely on translating informal genres. 28 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text"
W15-4304,J11-4002,0,0.0497536,"Missing"
W15-4304,2011.mtsummit-papers.27,0,0.027633,"alysis, is evaluated on data containing formal language. Work on SMT of informal text mostly targets reduction of OOV words in the source text, for example by correcting spelling errors (Bertoldi et al., 2010), normalizing noisy text to more formal text (Banerjee et al., 2012; Ling et al., 2013a), or enhancing the training data with bilingual segments extracted from Twitter (Jehl et al., 2012; Ling et al., 2013b). Other work improves SMT of UG text by combining statistical and rule-based MT (Carrera et al., 2009), or models trained on formal and informal data (Banerjee et al., 2011). Finally, Roturier and Bensadoun (2011) conduct a comparative study to determine the ability of several SMT systems to translate UG text, but they do not examine what errors the systems make. To our knowledge, our work is the first that looks inside an SMT system to systematically inspect its behavior across a diverse spectrum of UG text types. • The SMS and chat benchmarks are the most distant from formal text at all the analyzed levels. Errors in other types of UG are often more similar to news errors than to those in SMS and chat messages. • SMT model coverage dramatically deteriorates for phrases of length 3 or longer in most o"
W15-4304,2006.amta-papers.25,0,0.0591767,"ets for two language pairs, Arabic-English and Chinese-English, with the goals of (i) explaining the typically poor SMT performance observed for UG texts, and (ii) identifying translation modeling Introduction User-generated (UG) text such as found on social media and web forums poses different challenges to statistical machine translation (SMT) than formal text. This is reflected by poor translation quality for informal genres (see for example Figure 1), which is typically measured with automatic quality metrics such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), or TER (Snover et al., 2006). These scores alone, however, only reflect the overall translation quality, and do not provide any insight in what exactly makes translating UG text hard. While such knowledge is crucial for improving SMT of UG text, surprisingly little work on error analysis for SMT of usergenerated text has been reported. Moreover, the notion of user-generated content 1 One of the very few exceptions is NIST OpenMT 2015, which focusses entirely on translating informal genres. 28 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 28–37, c Beijing, China, July 31, 2015. 2015 Association"
W15-4304,P15-2092,1,0.787076,"Missing"
W15-4304,I05-3027,0,\N,Missing
W16-2301,W13-3520,0,0.0148252,"Missing"
W16-2301,2011.mtsummit-papers.35,0,0.440323,"Missing"
W16-2301,W15-3001,1,0.419439,"Missing"
W16-2301,W16-2305,0,0.0273073,"Missing"
W16-2301,L16-1662,0,0.00889007,"introduce a new set of features for the sentence-Level QE task. The features extracted include three alignment-based features, three bilingual embedding-based features, two embeddingbased features constrained on alignment links, as well as a set of 74 bigrams used as boolean features. The set of bigrams represents the most frequent bigrams in translations that have changed after the post-edition, and they are compiled by aligning translations to their post-editions provided in the WMT QE datasets. To produce these features, GIZA++ (Och and Ney, 2003) was used for word alignment and Multivec (Berard et al., 2016) was used for the bilingual model, which jointly learns distributed representations for source and target languages using a parallel corpus. To build the bilingual model, domain-specific data compiled from the resources made available for the WMT 16 IT-Domain shared task was used. As prediction model, a Linear Regression model using scikit-learn was built using a combination of QuEst++ baseline features and the new features proposed. tional features for both tasks. UNBABEL (Task 2): Two systems were submitted for the word-level task. UNBABEL 2 linear is a feature-based linear sequential model."
W16-2301,W11-2101,1,0.851508,"Missing"
W16-2301,W16-2306,0,0.0263259,"Missing"
W16-2301,W16-2382,0,0.0451767,"Missing"
W16-2301,W16-2302,1,0.346609,"Missing"
W16-2301,W16-2307,1,0.756182,"Missing"
W16-2301,W16-2308,0,0.0373021,"Missing"
W16-2301,buck-etal-2014-n,0,0.0150538,"Missing"
W16-2301,W13-2201,1,0.287085,"Missing"
W16-2301,W14-3302,1,0.399147,"Missing"
W16-2301,W14-3340,1,0.419045,"Missing"
W16-2301,W07-0718,1,0.759293,"oth automatically and manually. The human evaluation (§3) involves asking human judges to rank sentences output by anonymized systems. We obtained large numbers of rankings from researchers who contributed The quality estimation task had three subtasks, with a total of 14 teams, submitting 39 entries. The automatic post-editing task had a total of 6 teams, submitting 11 entries. 1 Introduction We present the results of the shared tasks of the First Conference on Statistical Machine Translation (WMT) held at ACL 2016. This conference builds on nine previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015). 131 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 131–198, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics to refine evaluation and estimation methodologies for machine translation. As before, all of the data, translations, and collected human judgments are publicly available.1 We hope these datasets serve as a valuable resource for research into statistical machine translation and automatic evaluation or prediction of translation quality. New"
W16-2301,W15-3025,1,0.888663,"used for the Task 2p. The submissions to the word-level task are modified in order to comply with the phrase-level task. sequences. Nevertheless, it is worth noticing the phraselevel QE systems introduced a number of interesting strategies that allowed them to outperform a strong baseline phrase-level model. Finally, we recall that the evaluation metric – word-level F1 mult – has difficulties to distinguish phrase-level systems. This suggests that we may need to find a different metric for evaluation of the phrase-level task, with phrase-level F1 -mult one of the candidates. 7 pointed out by Chatterjee et al. (2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; Automatic Post-editing Task This year WMT hosted the second round of the shared task on MT automatic post-editing (APE), which consists in automatically correcting"
W16-2301,P15-2026,1,0.895785,"used for the Task 2p. The submissions to the word-level task are modified in order to comply with the phrase-level task. sequences. Nevertheless, it is worth noticing the phraselevel QE systems introduced a number of interesting strategies that allowed them to outperform a strong baseline phrase-level model. Finally, we recall that the evaluation metric – word-level F1 mult – has difficulties to distinguish phrase-level systems. This suggests that we may need to find a different metric for evaluation of the phrase-level task, with phrase-level F1 -mult one of the candidates. 7 pointed out by Chatterjee et al. (2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; Automatic Post-editing Task This year WMT hosted the second round of the shared task on MT automatic post-editing (APE), which consists in automatically correcting"
W16-2301,W08-0309,1,0.809107,"Missing"
W16-2301,W16-2309,0,0.0305938,"Missing"
W16-2301,W10-1703,1,0.312318,"Missing"
W16-2301,W16-2336,0,0.0356983,"Missing"
W16-2301,W12-3102,1,0.235434,"ir native language. Consequently, health professionals may use the translated information to make clinical decisions impacting patients care. It is vital that translation systems do not contribute to the dissemination of incorrect clinical information. Therefore, the evaluation of biomedical translation systems should include an assessment at the document level indicating whether a translation conveyed erroneous clinical information. 6 Quality Estimation The fifth edition of the WMT shared task on quality estimation (QE) of machine translation (MT) builds on the previous editions of the task (Callison-Burch et al., 2012; Bojar et al., 2013, 2014, 2015), with “traditional” tasks at sentence and word levels, a new task for entire documents quality prediction, and a variant of the word-level task: phrase-level estimation. The goals of this year’s shared task were: • To advance work on sentence and wordlevel quality estimation by providing domainspecific, larger and professionally annotated datasets. • To analyse the effectiveness of different types of quality labels provided by humans for longer texts in document-level prediction. • To investigate quality estimation at a new level of granularity: phrases. Plan"
W16-2301,W16-2330,0,0.0328047,"Missing"
W16-2301,W16-2310,1,0.835279,"Missing"
W16-2301,W11-2103,1,0.65163,"Missing"
W16-2301,W16-2331,0,0.0203904,"cance test results for pairs of systems competing in the news domain translation task (en-ru), where a green cell denotes a significantly higher DA adequacy score for the system in a given row over the system in a given column. cs-en fi-en tr-en de-en ru-en ro-en 0.997 0.996 0.988 0.964 0.961 0.920 en-ru 0.975 DA Correlation with RR Table 11: Correlation between overall DA standardized mean adequacy scores and RR Trueskill scores. 150 4 IT Translation Task from the QTLeap project: HF&FCUL for Portuguese, UPV/EHU for Spanish and Basque, IICT-BAS for Bulgarian, CUNI for Czech and UG for Dutch). Duma and Menzel (2016) describe UHDS- DOC 2 VEC and UHBS- LMI (University of Hamburg). Pahari et al. (2016) describe JU-USAAR (Jadavpur University & Saarland University). Cuong et al. (2016) describe ILLC-U VA-S CORPIO (University of Amsterdam). IILC-U VA-DS is based on Hoang and Sima’an (2014). PROMT-RULE - BASED and PROMT-H YBRID systems were submitted by the PROMT LLC company and they are not described in any paper. QTL -M OSES is the standard Moses setup (MERT-tuned on the in-domain training data, but otherwise without any domain-adaptation) and serves as a baseline. The IT-domain translation task introduced th"
W16-2301,W15-3009,1,0.788321,"Missing"
W16-2301,P15-1174,1,0.28154,"endently inserted in another part of this sentence, i.e. to correct an unrelated error. The statistics of the datasets are outlined in Table 20. Evaluation Evaluation was performed against the true HTER label and/or ranking, using the following metrics: • Scoring: Pearson’s r correlation score (primary metric, official score for ranking submissions), Mean Average Error (MAE) and Root Mean Squared Error (RMSE). • Ranking: Spearman’s ρ rank correlation and DeltaAvg. Statistical significance on Pearson r and Spearman rho was computed using the William’s test, following the approach suggested in (Graham, 2015). Results Table 19 summarises the results for Task 1, ranking participating systems best to worst using Pearson’s r correlation as primary key. Spearman’s ρ correlation scores should be used to rank systems according to the ranking variant. We note that three systems have not submitted results ranking evaluation variant. 6.4 Task 2: Predicting word-level quality The goal of this task is to evaluate the extent to which we can detect word-level errors in MT output. Various classes of errors can be found in translations, but for this task we consider all error types together, aiming at making a b"
W16-2301,W16-2311,0,0.0122367,"016) PROMT Automated Translation Solutions (Molchanov and Bykov, 2016) QT21 System Combination (Peter et al., 2016b) RWTH Aachen (Peter et al., 2016a) ¨ TUBITAK (Bektas¸ et al., 2016) University of Edinburgh (Sennrich et al., 2016) University of Edinburgh (Williams et al., 2016) UEDIN - SYNTAX UEDIN - LMU UH -* USFD - RESCORING UUT YSDA ONLINE -[ A , B , F, G ] University of Edinburgh / University of Munich (Huck et al., 2016) University of Helsinki (Tiedemann et al., 2016) University of Sheffield (Blain et al., 2016) Uppsala University (Tiedemann et al., 2016) Yandex School of Data Analysis (Dvorkovich et al., 2016) Four online statistical machine translation systems Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. 136 Figure 2: Screenshot of the Appraise interface used in the human evaluation campaign. The annotator is presented with a source segment, a reference translation, and up to five outputs from competing systems (anonymized"
W16-2301,W13-2305,1,0.509145,"Missing"
W16-2301,W15-3036,0,0.0603245,"Missing"
W16-2301,E14-1047,1,0.831614,"Missing"
W16-2301,W16-2378,0,0.168663,"ts for pairs of systems competing in the news domain translation task (en-ru), where a green cell denotes a significantly higher DA adequacy score for the system in a given row over the system in a given column. cs-en fi-en tr-en de-en ru-en ro-en 0.997 0.996 0.988 0.964 0.961 0.920 en-ru 0.975 DA Correlation with RR Table 11: Correlation between overall DA standardized mean adequacy scores and RR Trueskill scores. 150 4 IT Translation Task from the QTLeap project: HF&FCUL for Portuguese, UPV/EHU for Spanish and Basque, IICT-BAS for Bulgarian, CUNI for Czech and UG for Dutch). Duma and Menzel (2016) describe UHDS- DOC 2 VEC and UHBS- LMI (University of Hamburg). Pahari et al. (2016) describe JU-USAAR (Jadavpur University & Saarland University). Cuong et al. (2016) describe ILLC-U VA-S CORPIO (University of Amsterdam). IILC-U VA-DS is based on Hoang and Sima’an (2014). PROMT-RULE - BASED and PROMT-H YBRID systems were submitted by the PROMT LLC company and they are not described in any paper. QTL -M OSES is the standard Moses setup (MERT-tuned on the in-domain training data, but otherwise without any domain-adaptation) and serves as a baseline. The IT-domain translation task introduced th"
W16-2301,W11-2123,0,0.0116622,"s were also evaluated against a reimplementation of the approach firstly proposed by Simard et al. (2007).36 Last year, in fact, this statistical post-editing approach represented the common backbone of all submissions (this is also reflected by the close results achieved by participants in the pilot task). For this purpose, a phrasebased SMT system based on Moses (Koehn et al., 2007) was used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the APE system was tuned on Adam Mickiewicz University. This system is among the very first ones exploring the application of neural translation models to the APE task. In particular, it investigates the following aspects: i) the use of artificially-created postedited data to train the neural models, ii) the loglinear combination of monolingual and bilingual models in an ensemble-like manner, iii) the addition of task-specific features in the log-linear model to control the final output quality. Concerning the data, in addition"
W16-2301,W16-2384,0,0.0416841,"Missing"
W16-2301,W13-0805,0,0.0110515,"of words in the reference. Lower TER values indicate lower distance from the reference as a proxy for higher MT quality. 33 http://www.cs.umd.edu/˜snover/tercom/ 34 https://github.com/moses-smt/mosesdecoder/ blob/master/scripts/generic/multi-bleu.perl 27 The source sentences (together with their reference translations which were not used for the task) were provided by TAUS (https://www.taus.net/) and originally come from a unique IT vendor. 28 It consists of a phrase-based machine translation system leveraging generic and in-domain parallel training data and using a pre-reordering technique (Herrmann et al., 2013). It takes also advantages of POS and word class-based language models. 29 German native speakers working at Text&Form https: //www.textform.com/. 176 Train (12,000) Dev (1,000) Test (2,000) SRC 201,505 17,827 31,477 Tokens TGT 210,573 19,355 34,332 PE 214,720 19,763 35,276 Table 31: Types SRC TGT 9,328 14,185 2,931 3,333 3,908 4,695 Data statistics. APE@WMT15 APE@WMT16 (EN-ES, news, crowd) (EN-DE, IT, prof.) SRC 2.905 6.616 TGT 3.312 8.845 PE 3.085 8.245 Table 32: Repetition Rate (RR) of the WMT15 (EnglishSpanish, news domain, crowdsourced post-edits) and WMT16 (English-German, IT domain, pro"
W16-2301,W16-2315,1,0.824148,"Missing"
W16-2301,P07-2045,1,0.017773,"gets unmodified.35 Baseline results are reported in Table 34. 7.2 Participants Monolingual translation as another term of comparison. To get some insights about the progress with respect to the first pilot task, participating systems were also evaluated against a reimplementation of the approach firstly proposed by Simard et al. (2007).36 Last year, in fact, this statistical post-editing approach represented the common backbone of all submissions (this is also reflected by the close results achieved by participants in the pilot task). For this purpose, a phrasebased SMT system based on Moses (Koehn et al., 2007) was used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the APE system was tuned on Adam Mickiewicz University. This system is among the very first ones exploring the application of neural translation models to the APE task. In particular, it investigates the following aspects: i) the use of artificially-created postedited data to train the neural models, ii)"
W16-2301,W16-2337,0,0.0376798,"Missing"
W16-2301,W16-2303,1,0.821661,"Missing"
W16-2301,L16-1582,1,0.790825,"Missing"
W16-2301,W16-2385,0,0.0304494,"Missing"
W16-2301,P16-2095,1,0.885979,"e package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 6.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Tas"
W16-2301,W15-3037,0,0.00849978,"stems were submitted for the word-level task. UNBABEL 2 linear is a feature-based linear sequential model. It uses the baseline features provided by the shared task organisers (with slight changes) conjoined with individual labels and pairs of consecutive labels. It also uses various syntactic dependency-based features (dependency relations, heads, and second-order structures like siblings and grandparents). The syntactic dependencies are predicted with TurboParser trained on the TIGER German treebank. UNBABEL 2 ensemble uses a stacked architecture, inspired by the last year’s QUETCH+ system (Kreutzer et al., 2015), which combines three neural systems: one feedforward and two recurrent ones. The predictions of these systems are added as additional features in the linear system above. The following external resources were used: part-of-speech tags and extra syntactic dependency information obtained with TurboTagger and TurboParser (Martins et al., 2013), trained on the Penn Treebank (for English) and on the version of the German TIGER corpus used in the SPMRL shared task (Seddah et al., 2014) for German. For the neural models, pretrained word embeddings from Polyglot (AlRfou et al., 2013) and those produ"
W16-2301,W14-3342,0,0.0237077,"Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Task 1, QuEst++13 (Specia et al., 2015) was used to extract 17 features from the SMT source/target language training corpus: • Number of tokens in source & target sentences. • Target token, its left and right contexts of one word."
W16-2301,W16-2318,0,0.0145349,"Missing"
W16-2301,N06-1014,0,0.0215966,"d on n previous translation and reordering decisions. This technique is able to model both local and long-range reorderings that are quite useful when dealing with the German language. To improve the capability of choosing the correct edit to process, eight new features are added to the loglinear model. These features capture the cost of deleting a phrase and different information on possible gaps in reordering operations. The monolingual alignments between the MT outputs and their post-edits are computed using different methods based on TER, METEOR (Snover et al., 2006) and Berkeley Aligner (Liang et al., 2006). Only the task data is used for these submissions. 7.3 179 TER/BLEU results ID AMU Primary AMU Contrastive FBK Contrastive FBK Primary USAAR Primary USAAR Constrastive CUNI Primary (Simard et al., 2007) Baseline DCU Contrastive JUSAAR Primary JUSAAR Contrastive DCU Primary Avg. TER 21.52 23.06 23.92 23.94 24.14 24.14 24.31 24.64 24.76 26.79 26.92 26.97 28.97 BLEU 67.65 66.09 64.75 64.75 64.10 64.00 63.32 63.47 62.11 58.60 59.44 59.18 55.19 tained this year by the top runs can only be reached by moving from the basic statistical MT backbone shared by all last year’s participants to new and mor"
W16-2301,P13-2109,0,0.0179517,"eads, and second-order structures like siblings and grandparents). The syntactic dependencies are predicted with TurboParser trained on the TIGER German treebank. UNBABEL 2 ensemble uses a stacked architecture, inspired by the last year’s QUETCH+ system (Kreutzer et al., 2015), which combines three neural systems: one feedforward and two recurrent ones. The predictions of these systems are added as additional features in the linear system above. The following external resources were used: part-of-speech tags and extra syntactic dependency information obtained with TurboTagger and TurboParser (Martins et al., 2013), trained on the Penn Treebank (for English) and on the version of the German TIGER corpus used in the SPMRL shared task (Seddah et al., 2014) for German. For the neural models, pretrained word embeddings from Polyglot (AlRfou et al., 2013) and those produced with a neural MT system (Bahdanau et al., 2014) were used. UGENT-LT3 (Task 1, Task 2): The submissions for the word-level task use 41 features in combination with the baseline feature set to train binary classifiers. The 41 additional features attempt to capture accuracy errors (concerned with the meaning transfer from the source to targe"
W16-2301,W16-2387,0,0.0253511,"Missing"
W16-2301,W16-2317,0,0.0414065,"Missing"
W16-2301,N13-1090,0,0.00833788,"Missing"
W16-2301,2015.iwslt-papers.4,1,0.737239,"Missing"
W16-2301,W16-2319,0,0.0339318,"Missing"
W16-2301,W16-2386,1,0.814188,"e package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 6.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Tas"
W16-2301,L16-1470,1,0.826657,"Missing"
W16-2301,P03-1021,0,0.0340784,"7 Tokens TGT 210,573 19,355 34,332 PE 214,720 19,763 35,276 Table 31: Types SRC TGT 9,328 14,185 2,931 3,333 3,908 4,695 Data statistics. APE@WMT15 APE@WMT16 (EN-ES, news, crowd) (EN-DE, IT, prof.) SRC 2.905 6.616 TGT 3.312 8.845 PE 3.085 8.245 Table 32: Repetition Rate (RR) of the WMT15 (EnglishSpanish, news domain, crowdsourced post-edits) and WMT16 (English-German, IT domain, professional posteditors) APE Task data. 7.1.3 PE 16,388 3,506 5,047 SRC 5,628 1,922 2,479 Lemmas TGT PE 11,418 13,244 2,686 2,806 3,753 4,050 the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Table 34. For each submitted run, the statistical significance of performance differences with respect to the baselines and the re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). Baseline The official baseline results are the TER and BLEU scores calculated by comparing the raw MT output with the human post-edits. In practice, the baseline APE system is a system that leaves all the test targets unmodified.35 Baseline results are reported in Table 34. 7.2 Participants Monolingual"
W16-2301,W16-2338,0,0.0374669,"Missing"
W16-2301,J03-1002,0,0.0268639,"word alignments and bilingual distributed representations to introduce a new set of features for the sentence-Level QE task. The features extracted include three alignment-based features, three bilingual embedding-based features, two embeddingbased features constrained on alignment links, as well as a set of 74 bigrams used as boolean features. The set of bigrams represents the most frequent bigrams in translations that have changed after the post-edition, and they are compiled by aligning translations to their post-editions provided in the WMT QE datasets. To produce these features, GIZA++ (Och and Ney, 2003) was used for word alignment and Multivec (Berard et al., 2016) was used for the bilingual model, which jointly learns distributed representations for source and target languages using a parallel corpus. To build the bilingual model, domain-specific data compiled from the resources made available for the WMT 16 IT-Domain shared task was used. As prediction model, a Linear Regression model using scikit-learn was built using a combination of QuEst++ baseline features and the new features proposed. tional features for both tasks. UNBABEL (Task 2): Two systems were submitted for the word-level tas"
W16-2301,W16-2321,0,0.0372138,"Missing"
W16-2301,W16-2388,1,0.748184,"Missing"
W16-2301,W16-2333,0,0.0259256,"Missing"
W16-2301,W15-3026,0,0.0339859,"Missing"
W16-2301,W16-2379,1,0.819672,"Missing"
W16-2301,W16-2334,1,0.811656,"Missing"
W16-2301,P02-1040,0,0.105557,", eventually, make the APE task more feasible by automatic systems. Other changes concern the language combination and the evaluation mode. As regards the languages, we moved from English-Spanish to English-German, which is one of the language pairs covered by the QT21 Project26 that supported data collection and post-editing. Concerning the evaluation, we changed from TER scores computed both in case-sensitive and caseinsensitive mode to a single ranking based on case sensitive measurements. Besides these changes the new round of the APE task included some extensions in the evaluation. BLEU (Papineni et al., 2002) has been introduced as a secondary evaluation metric to measure the improvements over the rough MT output. In addition, to gain further insights on final output quality, a subset of the outputs of the submitted systems has also been manually evaluated. Based on these changes and extensions, the goals of this year’s shared task were to: i) improve and stabilize the evaluation framework in view of future rounds, ii) analyze the effect on task 26 feasibility of data coming from a narrow domain, iii) analyze the effect of post-edits collected from professional translators, iv) analyze how humans"
W16-2301,W16-2389,0,0.0210658,"Missing"
W16-2301,W16-2390,0,0.0283246,"Missing"
W16-2301,W16-2322,0,0.0139204,"ng and evaluating the manual translations has settled into the following pattern. We ask human annotators to rank the outputs of five systems. From these rankings, we produce pairwise translation comparisons, and then evaluate them with a version of the TrueSkill algorithm adapted to our task. We refer to this approach (described in Section 3.4) as the relative ranking approach (RR), so named because the pairwise comparisons denote only relative ability between a pair of systems, and cannot be used to infer their absolute quality. These results are used to produce the official ranking for the WMT 2016 tasks. However, work in evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality. In this setting, annotators are asked to provide an assessment of the direct quality of the output of a system relative to a reference translation. In order to evaluate the potential of this approach for future WMT evaluations, we conducted a direct assessment evaluation in parallel. This evaluation, together with a comparison of the official results, is described in Section 3.5. 2.3 3.1 2.2 Training data As in past years we provide"
W16-2301,W16-2392,1,0.772695,"Missing"
W16-2301,L16-1649,1,0.704564,"): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1): The YSDA submission is based on a simple idea that the more complex the sentence is the more difficult it is to translate. For this purpose, it uses information provided by syntactic parsing (information from parsing trees,"
W16-2301,W16-2391,1,0.796337,"Missing"
W16-2301,2014.eamt-1.21,1,0.643048,"e extracted by averaging word embeddings in the document. The GP model was trained with two Rational Quadratic kernels (Rasmussen and Williams, 2006): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1): The YSDA submission is based on a simple idea that the more complex the sentence"
W16-2301,W15-3040,1,0.909092,"on-word corpus,18 with a vocabulary size of 527K words. Document embeddings are extracted by averaging word embeddings in the document. The GP model was trained with two Rational Quadratic kernels (Rasmussen and Williams, 2006): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1)"
W16-2301,2006.amta-papers.25,0,0.850121,"ombinations of several features. A regression model was training to predict BLEU as target metric instead HTER. The machine learning pipeline uses an SVR with RBF kernel to predict BLEU scores, followed by a linear SVR to predict HTER scores from BLEU scores. As external resources, the system uses a syntactic parser, pseudo-references and back-translation from web-scale MT system, and a web-scale language model. 6.3 Task 1: Predicting sentence-level quality This task consists in scoring (and ranking) translation sentences according to the percentage of their words that need to be fixed. HTER (Snover et al., 2006) is used as quality score, i.e. the minimum edit distance between the machine translation and its manually post-edited version in [0,1]. As in previous years, two variants of the results could be submitted: • Scoring: An absolute HTER score for each sentence translation, to be interpreted as an error metric: lower scores mean better translations. • Ranking: A ranking of sentence translations for all source sentences from best to worst. For this variant, it does not matter how the ranking is produced (from HTER predictions or by other means). The reference ranking is defined based on the true H"
W16-2301,W15-4916,1,0.824025,"Missing"
W16-2301,W16-2346,1,0.0602876,"Missing"
W16-2301,W16-2325,1,0.816227,"Missing"
W16-2301,W16-2393,0,0.033041,"Missing"
W16-2301,W16-2326,0,0.0221449,"Missing"
W16-2301,W16-2327,1,0.737603,"Missing"
W16-2301,W16-2328,0,0.0403946,"Missing"
W16-2301,C00-2137,0,0.0384283,"Missing"
W16-2301,D07-1091,1,\N,Missing
W16-2301,W09-0401,1,\N,Missing
W16-2301,P11-1105,0,\N,Missing
W16-2301,N07-1064,0,\N,Missing
W16-2301,W04-3250,1,\N,Missing
W16-2301,P15-4020,1,\N,Missing
W16-2301,W16-2377,1,\N,Missing
W16-2301,aziz-etal-2012-pet,1,\N,Missing
W16-2301,2012.eamt-1.31,0,\N,Missing
W16-2301,C14-1182,0,\N,Missing
W16-2301,W16-2316,0,\N,Missing
W16-2301,W16-2332,1,\N,Missing
W16-2301,W16-2320,1,\N,Missing
W16-2301,W16-2335,0,\N,Missing
W16-2301,W16-2329,0,\N,Missing
W16-2301,W16-2383,0,\N,Missing
W16-2301,W16-2381,1,\N,Missing
W16-2301,W16-2312,0,\N,Missing
W16-2301,P16-1162,1,\N,Missing
W16-2301,W13-2243,1,\N,Missing
W16-2301,W08-0509,0,\N,Missing
W16-2301,2015.eamt-1.17,1,\N,Missing
W16-2301,W14-6111,0,\N,Missing
W16-2301,2012.tc-1.5,1,\N,Missing
W16-2301,W16-2347,1,\N,Missing
W16-2301,W16-2314,0,\N,Missing
W16-3908,W14-1604,0,0.217424,"when Arabizi text is alternated with words from other languages, such as French or English, which it regularly is. While our approach leaves unconvertible words unchanged and often yields the right SMT output for English source words, addressing the problem of language detection before transliteration will likely benefit our approach. Bies et al. (2014) present their work on manually transliterating Arabizi SMS and chat messages to Arabic. Their work is focused on releasing a new resource rather than presenting a transliteration methodology, and naturally yields high-quality transliteration. Al-Badrashiny et al. (2014) use a weighted finite-state transducer (wFST) approach to converting Arabizi to Arabic in their system “3arrib”. They incorporate linguistic information by using CODA, a conventional orthography for Dialectal Arabic (Habash et al., 2012) and morphological analysis, and thus heavily rely on expert knowledge. All of the above focus on Arabizi-to-Arabic conversion outside the context of SMT from Arabizi. The work by May et al. (2014) is the only that presents an Arabizi-to-English SMT system, in which the authors not only focus on transliterating Arabizi to Arabic, but also evaluate performance"
W16-3908,W14-3612,0,0.0732706,"t a transliteration engine that, like our approach, follows the SMT paradigm. However, they complement their method with handcrafted rules. A similar approach by Darwish (2014) focuses on Arabizi detection as well as conversion to Arabic. The former is important when Arabizi text is alternated with words from other languages, such as French or English, which it regularly is. While our approach leaves unconvertible words unchanged and often yields the right SMT output for English source words, addressing the problem of language detection before transliteration will likely benefit our approach. Bies et al. (2014) present their work on manually transliterating Arabizi SMS and chat messages to Arabic. Their work is focused on releasing a new resource rather than presenting a transliteration methodology, and naturally yields high-quality transliteration. Al-Badrashiny et al. (2014) use a weighted finite-state transducer (wFST) approach to converting Arabizi to Arabic in their system “3arrib”. They incorporate linguistic information by using CODA, a conventional orthography for Dialectal Arabic (Habash et al., 2012) and morphological analysis, and thus heavily rely on expert knowledge. All of the above fo"
W16-3908,2011.iwslt-evaluation.18,1,0.842234,"word subcorpora with the English side of our bitext. When no valid Arabic transliteration is found for an Arabizi word, our software component leaves it unchanged. To increase the chances of handling such cases, we exploit our in-house Arabizi-English corpus of web-crawled user comments (see Section 2), on which we train a separate Arabizi-English system. Instead of using this system for the actual translation task, which would suffer from very low coverage, we merge the Arabizi-English phrase translation and phrase reordering models to the main Arabic-English models using a fillup technique (Bisazza et al., 2011). In this way, a non-transliterated word that is not matched by the main Arabic-English models has still a chance of being translated directly by the Arabizi-English models. We tokenize all Arabic data—training data as well as transliterated Arabizi—using the MADA toolkit (Habash and Rambow, 2005). 4.2 Results Table 4 shows SMT quality measured with case-insensitive BLEU (Papineni et al., 2002) for a number of transliteration scenarios. First, we see that Arabizi-to-English translation without any preprocessing (top row) results in very poor translation quality. There is, however, a large diff"
W16-3908,W12-4808,0,0.0236874,"able 5: SMT input-output example pairs for a sentence containing English words in the original Arabizi text. English reference translation: fine, okay, did you have fun?. 5 Related work In the past years, a few approaches to transliterate Arabizi to Arabic (or other tasks of deromanizing text in non-native Romanized script (Irvine et al., 2012)) have been presented, most of which rely to at least some extent on knowledge of experts or native speakers. In contrast, the approach we have presented does not rely on expert knowledge and is constructed using only publicly available data sources. 48 Chalabi and Gerges (2012) present a transliteration engine that, like our approach, follows the SMT paradigm. However, they complement their method with handcrafted rules. A similar approach by Darwish (2014) focuses on Arabizi detection as well as conversion to Arabic. The former is important when Arabizi text is alternated with words from other languages, such as French or English, which it regularly is. While our approach leaves unconvertible words unchanged and often yields the right SMT output for English source words, addressing the problem of language detection before transliteration will likely benefit our app"
W16-3908,W14-3629,0,0.0133144,"ast years, a few approaches to transliterate Arabizi to Arabic (or other tasks of deromanizing text in non-native Romanized script (Irvine et al., 2012)) have been presented, most of which rely to at least some extent on knowledge of experts or native speakers. In contrast, the approach we have presented does not rely on expert knowledge and is constructed using only publicly available data sources. 48 Chalabi and Gerges (2012) present a transliteration engine that, like our approach, follows the SMT paradigm. However, they complement their method with handcrafted rules. A similar approach by Darwish (2014) focuses on Arabizi detection as well as conversion to Arabic. The former is important when Arabizi text is alternated with words from other languages, such as French or English, which it regularly is. While our approach leaves unconvertible words unchanged and often yields the right SMT output for English source words, addressing the problem of language detection before transliteration will likely benefit our approach. Bies et al. (2014) present their work on manually transliterating Arabizi SMS and chat messages to Arabic. Their work is focused on releasing a new resource rather than present"
W16-3908,P05-1071,0,0.171505,"Missing"
W16-3908,habash-etal-2012-conventional,0,0.0317555,"g the problem of language detection before transliteration will likely benefit our approach. Bies et al. (2014) present their work on manually transliterating Arabizi SMS and chat messages to Arabic. Their work is focused on releasing a new resource rather than presenting a transliteration methodology, and naturally yields high-quality transliteration. Al-Badrashiny et al. (2014) use a weighted finite-state transducer (wFST) approach to converting Arabizi to Arabic in their system “3arrib”. They incorporate linguistic information by using CODA, a conventional orthography for Dialectal Arabic (Habash et al., 2012) and morphological analysis, and thus heavily rely on expert knowledge. All of the above focus on Arabizi-to-Arabic conversion outside the context of SMT from Arabizi. The work by May et al. (2014) is the only that presents an Arabizi-to-English SMT system, in which the authors not only focus on transliterating Arabizi to Arabic, but also evaluate performance in end-to-end Arabizi-to-English SMT experiments. Their transliteration approach uses wFSTs which are constructed by (i) experts, (ii) machine translation, or (iii) semi-automatically. In downstream SMT experiments, the semi-automatic con"
W16-3908,W12-2109,0,0.161558,"you have fun okay mashyyyy did you have fun Char.map.+disambig Char.map.+disambig+word pairs Human transliteration  did you have fun ú» úæAÓ  did you have fun ú»ð@ úæ AÓ AK Y ú»ð@ úæ AÓ à@ ¬ ¬Aë in order to did you have fun ok, ok, did you have fun ok, ok, against,, Table 5: SMT input-output example pairs for a sentence containing English words in the original Arabizi text. English reference translation: fine, okay, did you have fun?. 5 Related work In the past years, a few approaches to transliterate Arabizi to Arabic (or other tasks of deromanizing text in non-native Romanized script (Irvine et al., 2012)) have been presented, most of which rely to at least some extent on knowledge of experts or native speakers. In contrast, the approach we have presented does not rely on expert knowledge and is constructed using only publicly available data sources. 48 Chalabi and Gerges (2012) present a transliteration engine that, like our approach, follows the SMT paradigm. However, they complement their method with handcrafted rules. A similar approach by Darwish (2014) focuses on Arabizi detection as well as conversion to Arabic. The former is important when Arabizi text is alternated with words from oth"
W16-3908,P07-2045,0,0.00444937,"due to different possible spellings of English words in Arabic (e.g. baby / úG AK. or úæJ K.) or different spellings of Arabic dialectal forms (e.g. when / 4 . .  ), which reveals the difficulty éJÓ@ or úæÓ@ of evaluating Arabizi transliteration. Arabizi-to-English machine translation We examine the success of the proposed transliteration approaches by running Arabic-to-English SMT experiments, which we describe in Section 4.1 and discuss in Section 4.2. 4.1 Experimental setup We perform our translation experiments using an in-house state-of-the-art phrase-based SMT system similar to Moses (Koehn et al., 2007). The system is trained on the collection of Arabic-English parallel corpora discussed in Section 2, comprising 1.75M lines (52.9M Arabic tokens) of parallel text. In addition, we use a 5-gram English language model that linearly interpolates different English Gigaword subcorpora with the English side of our bitext. When no valid Arabic transliteration is found for an Arabizi word, our software component leaves it unchanged. To increase the chances of handling such cases, we exploit our in-house Arabizi-English corpus of web-crawled user comments (see Section 2), on which we train a separate A"
W16-3908,2014.amta-researchers.25,0,0.546466,"tical machine translation systems use a word-based language model over target language sequences to estimate the fluency of translation hypotheses. Here, we use an Arabiccharacter based language model instead. Decoding is carried out in the same manner as the normal translation setting, except that the distortion limit is set to 0, enforcing monotone decoding. Note that while we opt for using a publicly available character table, it would also be possible to learn a character mapping and its corresponding probabilities from an Arabizi-Arabic bitext. This is done for example in related work by May et al. (2014). An important problem at this point is that vowel mappings result in Arabic words having too many vowels orthographically present. This is particularly problematic for Arabizi words with repetitive vowels, a common phenomenon in user-generated text, such as observed in the word henaaa in Table 1. In order to address this problem, we allow for more flexible character mappings of vowels in which Arabizi vowels can be dropped. As a result, transliteration candidates for henaaa also include candidates for . hena, among which the correct Arabic word AJë 3.2 Filtering non-Arabic words Using the des"
W16-3908,P02-1040,0,0.0985415,"Missing"
W17-4717,W17-4758,0,0.0835228,"ese submissions investigate alternative machine learning models for the prediction of the HTER score on the sentencelevel task. Instead of directly predicting the HTER score, the systems use a single-layer perceptron with four outputs that jointly predict the number of each of the four distinct post-editing operations that are then used to calculate the HTER score. This also gives the possibility to correct invalid (e.g. negative) predicted values prior to the calculation of the HTER score. The two submissions use the baseline features and the EnglishGerman submission also uses features from (Avramidis, 2017a). JXNU (T1): The JXNU submissions use features extracted from a neural network, including embedding features and cross-entropy features of the source sentences and their machine translations. The sentence embedding features are extracted through global average pooling from word embedding, which are trained using the WORD 2 VEC toolkit. The sentence cross-entropy features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus."
W17-4717,W17-4772,0,0.0310859,"Missing"
W17-4717,W17-4759,0,0.0327025,"Missing"
W17-4717,L16-1356,1,0.770826,"or BAD. The ﬁrst two matrices are built from 300 dimension word vectors computed with pretrained in-domain word embeddings. They train their model over 100 iterations with the l2 norm as regulariser and using the forward-backward splitting algorithm (FOBOS) (Duchi and Singer, 2009) as optimisation method. They report results considering the word and its context versus the word in isolation, as well as variants with and without the gold labels at training time. Finally, for the phrase-level task, SHEF made use of predictions generated by BMAPS for task 2 and the phrase labelling approaches in (Blain et al., 2016). These approaches use the number of BAD word-level predictions in a phrase: an optimistic version labels the phrase as OK if at least half of the words in it are predicted to be OK, and a superpessimistic version labels the phrase as BAD if any word is in is predicted to be BAD. UHH (T1): The UHH-STK submission is based on sequence and tree kernels applied on the source and target input data for predicting the HTER score. The kernels use a backtranslation of the MT output into the source language as an additional input data representation. Further hand-crafted features were deﬁned in the form"
W17-4717,W17-4760,1,0.841464,"Missing"
W17-4717,W17-4755,1,0.0393895,"nd Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (constraint condition). We held 14 translation tasks this year, between English and each of Chinese, Czech, German, Finnish, Latvian, Russian, and Turkish. The Latvian and Chinese translation tasks were new this year. Latvian is a lesser resourced data condition on challenging language pair"
W17-4717,W07-0718,1,0.696979,") • bandit learning (Sokolov et al., 2017) This paper presents the results of the WMT17 shared tasks, which included three machine translation (MT) tasks (news, biomedical, and multimodal), two evaluation tasks (metrics and run-time estimation of MT quality), an automatic post-editing task, a neural MT training task, and a bandit learning task. 1 Introduction We present the results of the shared tasks of the Second Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), part"
W17-4717,W08-0309,1,0.537902,"Missing"
W17-4717,W10-1703,1,0.603181,"Missing"
W17-4717,W12-3102,1,0.508067,"Missing"
W17-4717,W17-4761,0,0.0349831,"Missing"
W17-4717,W17-4723,0,0.0362034,"Missing"
W17-4717,W17-4724,1,0.839009,"Missing"
W17-4717,W11-2103,1,0.744276,"Missing"
W17-4717,W17-4773,1,0.839713,"Missing"
W17-4717,W15-3025,1,0.905105,"Instead of predicting the HTER score, the systems attempted to predict the number of each of the four postediting operations (add, replace, shift, delete) at the sentence level. However, this did not lead to positive results. In future editions of the task, we plan to make this detailed post-editing information available again and suggest clear ways of using it. 5 Automatic Post-editing Task The WMT shared task on MT automatic postediting (APE), this year at its third round at WMT, aims to evaluate systems for the automatic correction of errors in a machine translated text. As pointed out by (Chatterjee et al., 2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; • Adapt the output of a general-purpose MT system to the lexicon/style requested in a speciﬁc application domain. The third round of the APE task proposed to parti"
W17-4717,W17-4718,1,0.0662673,"builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (constraint condition). We held 14 translation tasks this year, between English and each of Chinese, Czech, German, Finnish, Latvian, Russian, and Turkish. The Latvian and Chinese translation tasks were new this year. Latvian is a lesser resourced data condition on challenging language pair. Chinese allowed us to co-operate with an ongoing evaluation campaign on Asian languages org"
W17-4717,W17-4725,0,0.0391077,"Missing"
W17-4717,W08-0509,0,0.00859402,"l post-editors) and WMT17 DE-EN (German-English, pharmacological domain, professional post-editors) APE task data. TER BLEU APE15 23.84 n/a APE16 24.76 62.11 APE17 EN-DE 24.48 62.49 APE17 DE-EN 15.55 79.54 Table 26: Translation quality (TER/BLEU of TGT and proportion of TGTs with TER=0) of the WMT15, WMT16, WMT17 EN-DE and WMT17 DE-EN data. Figure 7: TER distribution over the EN-DE test set Figure 8: TER distribution over the DE-EN test set TERcom27 software: lower average TER scores correspond to higher ranks. BLEU was computed using the multi-bleu.perl package28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁci"
W17-4717,W17-4726,0,0.0387381,"Missing"
W17-4717,W13-2305,1,0.899702,"(RR) approach, so named because the pairwise comparisons denote only relative ability between a pair of systems, and cannot be used to infer absolute quality. For example, RR can be used to discover which systems perform better than others, but RR does not provide any information about the absolute quality of system translations, i.e. it provides no information about how far a given system is from producing perfect output according to a human user. Work on evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality (Graham et al., 2013, 2014, 2016), and last year’s evaluation campaign included parallel assessment of a subset of News task language pairs evaluated with RR and DA. DA has some clear advantages over RR, namely the evaluation of absolute translation quality and the ability to carry out evaluations through quality controlled crowd-sourcing. As established last year (Bojar et al., 2016a), DA results (via crowd-sourcing) and RR results (produced by researchers) correlate strongly, with Pearson correlation ranging from 0.920 to 0.997 across several source languages into English and at 0.975 for English-to-Russian (th"
W17-4717,E14-1047,1,0.508986,"Missing"
W17-4717,N15-1124,1,0.658439,"Missing"
W17-4717,W13-0805,0,0.0140095,"ely contain 25,000 and 1,000 triplets, while the test set consists of 2,000 instances. Table 24 provides some basic statistics about the data (the same used for the sentence-level quality estimation task), which has been released by the European Project QT21 (Specia et al., 2017b).25 In addition, Tables 25 and 26 provide a view of the data from a task difﬁculty standpoint. Table 25 shows the repetition rate (RR) values of the data sets released in the three rounds 24 We used phrase-based MT systems trained with generic and in-domain parallel training data, leveraging prereordering techniques (Herrmann et al., 2013), and taking advantage of POS and word class-based language models. 25 For both language directions, the source sentences and reference translations were provided by TAUS (https://www.taus.net/). 197 EN-DE Train (23,000) Dev (1,000) Test (2,000) DE-EN Train (25,000) Dev (1,000) Test (2,000) SRC Tokens TGT PE SRC Types TGT PE SRC Lemmas TGT PE 384448 17827 65120 403306 19355 69812 411246 19763 71483 18220 2931 8061 27382 3333 9765 31652 3506 10502 10946 1922 2626 21959 2686 3976 25550 2806 4282 437833 17578 35087 453096 18130 36082 456163 18313 36480 29745 4426 6987 19866 3583 5391 19172 3642 5"
W17-4717,W17-4775,0,0.0513454,"Missing"
W17-4717,W17-4730,1,0.829861,"Missing"
W17-4717,W17-4731,0,0.029579,"Missing"
W17-4717,W17-4727,0,0.046917,"Missing"
W17-4717,W16-2378,0,0.0869978,"a manuallyrevised version of the target, done by professional translators. Test data consists of (source, target) pairs having similar characteristics of those in the training set. Human post-edits of the test target instances were left apart to measure system performance. English-German data were drawn from the Information Technology (IT) domain. Training and test sets respectively contain 11,000 and 2,000 triplets. The data released for the 2016 round of the task (15,000 instances) and the artiﬁcially generated post-editing triplets (4 million instances) used by last year’s winning system (Junczys-Dowmunt and Grundkiewicz, 2016) were also provided as additional training material. German-English data were drawn from the Pharmacological domain. Training and development sets respectively contain 25,000 and 1,000 triplets, while the test set consists of 2,000 instances. Table 24 provides some basic statistics about the data (the same used for the sentence-level quality estimation task), which has been released by the European Project QT21 (Specia et al., 2017b).25 In addition, Tables 25 and 26 provide a view of the data from a task difﬁculty standpoint. Table 25 shows the repetition rate (RR) values of the data sets rele"
W17-4717,W11-2123,0,0.00943973,"-editors) APE task data. TER BLEU APE15 23.84 n/a APE16 24.76 62.11 APE17 EN-DE 24.48 62.49 APE17 DE-EN 15.55 79.54 Table 26: Translation quality (TER/BLEU of TGT and proportion of TGTs with TER=0) of the WMT15, WMT16, WMT17 EN-DE and WMT17 DE-EN data. Figure 7: TER distribution over the EN-DE test set Figure 8: TER distribution over the DE-EN test set TERcom27 software: lower average TER scores correspond to higher ranks. BLEU was computed using the multi-bleu.perl package28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁcial baseline results are the TER and BLEU scores calculated by comparing the raw MT o"
W17-4717,I17-1013,0,0.0334345,"Missing"
W17-4717,W16-2384,0,0.0137431,"features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus. The experimental results show that the neural network features lead to signiﬁcant improvements over the baseline, and that combining the neural network features with baseline features leads to further improvement. POSTECH (T1, T2, T3): POSTECH’s submissions to the sentence/word/phrase-level QE tasks are based on predictor-estimator architecture (Kim et al., 2017; Kim and Lee, 2016), which is the two-stage end-to-end ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma a"
W17-4717,W17-4763,0,0.0300648,"ence cross-entropy features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus. The experimental results show that the neural network features lead to signiﬁcant improvements over the baseline, and that combining the neural network features with baseline features leads to further improvement. POSTECH (T1, T2, T3): POSTECH’s submissions to the sentence/word/phrase-level QE tasks are based on predictor-estimator architecture (Kim et al., 2017; Kim and Lee, 2016), which is the two-stage end-to-end ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamb"
W17-4717,W04-3250,1,0.380171,"age28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁcial baseline results are the TER and BLEU scores calculated by comparing the raw MT output with the human post-edits. In practice, the baseline APE system is a “donothing” system that leaves all the test targets unmodiﬁed. Baseline results, the same shown in Table 26, are also reported in Tables 28-29 for comparison with participants’ submissions. In continuity with the previous rounds, we used as additional term of comparison a reimplementation of the method ﬁrstly proposed by Simard et al. (2007). It relies on a phrasebased post-editing approach to t"
W17-4717,P07-2045,1,0.0149362,"t with the human post-edits. In practice, the baseline APE system is a “donothing” system that leaves all the test targets unmodiﬁed. Baseline results, the same shown in Table 26, are also reported in Tables 28-29 for comparison with participants’ submissions. In continuity with the previous rounds, we used as additional term of comparison a reimplementation of the method ﬁrstly proposed by Simard et al. (2007). It relies on a phrasebased post-editing approach to the task, which represented the common backbone of APE systems before the spread of neural solutions. The system is based on Moses (Koehn et al., 2007); translation and reordering models were estimated following the Moses protocol with default setup using 27 http://www.cs.umd.edu/˜snover/tercom/ https://github.com/moses-smt/mosesdecoder/ blob/master/scripts/generic/multi-bleu.perl 28 5.2 Participants Seven teams participated in the English-German task by submitting a total of ﬁfteen runs. Two of them also participated in the German-English task with ﬁve submitted runs. Participants are listed in Table 27, and a short description of their systems is provided in the following. Adam Mickiewicz University. AMU’s (ENDE) participation explores and"
W17-4717,C14-1017,0,0.0141812,"ord embeddings approach used by (Scarton et al., 2016) for document-level QE. Here in-domain word embeddings are used instead of embeddings obtained general purpose data (same as in task 2, below). Word embeddings were averaged to generate a single vector for each sentence. Source and target word embeddings were then concatenated with the baseline features and given to an SVM regressor for model building. For the word-level task SHEF investigated a new approach based on predicting the strength of the lexical relationships between the source and target sentences (BMAPS). Following the work by (Madhyastha et al., 2014), a bilinear model is trained from three matrices corresponding to the training data, the development set and a “truth” matrix between them, which is built from the word alignments and the gold labels to indicate which lexical items form a pair, and whether or not their lexical relation is OK or BAD. The ﬁrst two matrices are built from 300 dimension word vectors computed with pretrained in-domain word embeddings. They train their model over 100 iterations with the l2 norm as regulariser and using the forward-backward splitting algorithm (FOBOS) (Duchi and Singer, 2009) as optimisation method."
W17-4717,Q17-1015,0,0.0112193,"ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma and Menzel, 2017) Unbabel, Portugal (Martins et al., 2017b) Table 10: Participants in the WMT17 quality estimation shared task. neural QE model. The predictor-estimator architecture consists of two types of stacked neural network models: 1) a word prediction model based on bidirectional and bilingual recurrent neural network language model trained on additional large-scale parallel corpora and 2) a neural quality estimation model trained on quality-annotated noisy parallel corpora. To jointly learn the two-stage model, a stack propagation method was applied (Zhang and Weiss, 2016). In addition, a “multilevel model” was developed where a task-speciﬁc"
W17-4717,W16-2387,0,0.0126259,"the SMT parallel corps. These features were used to train a Support Vector Regression (SVR) algorithm using a Radial Basis Function (RBF) kernel within the SCIKITLEARN toolkit.13 The γ, � and C parameters were optimised via grid search with 5-fold cross validation on the training set, resulting in γ=0.01, � = 0.0825, C = 20. This baseline system has proved robust across a range of language pairs, MT systems, and text domains for predicting various In addition to that, 6 new features were included which contain combinations of other features, and which proved useful in (Kreutzer et al., 2015; Martins et al., 2016): 12 https://github.com/ghpaetzold/ questplusplus 13 http://scikit-learn.org/ 184 • • • • Target word + left context. Target word + right context. Target word + aligned source word. POS of target word + POS of aligned source word. • Target word + left context + source word. • Target word + right context + source word. • Punctuation features: The baseline system models the task as a sequence prediction problem using the LinearChain Conditional Random Fields (CRF) algorithm within the CRFSuite tool (Okazaki, 2007). The model was trained using passive-aggressive optimisation algorithm. We note th"
W17-4717,W17-4764,0,0.0178514,"ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma and Menzel, 2017) Unbabel, Portugal (Martins et al., 2017b) Table 10: Participants in the WMT17 quality estimation shared task. neural QE model. The predictor-estimator architecture consists of two types of stacked neural network models: 1) a word prediction model based on bidirectional and bilingual recurrent neural network language model trained on additional large-scale parallel corpora and 2) a neural quality estimation model trained on quality-annotated noisy parallel corpora. To jointly learn the two-stage model, a stack propagation method was applied (Zhang and Weiss, 2016). In addition, a “multilevel model” was developed where a task-speciﬁc"
W17-4717,W06-3114,1,0.104699,"g (Bojar et al., 2017b) • bandit learning (Sokolov et al., 2017) This paper presents the results of the WMT17 shared tasks, which included three machine translation (MT) tasks (news, biomedical, and multimodal), two evaluation tasks (metrics and run-time estimation of MT quality), an automatic post-editing task, a neural MT training task, and a bandit learning task. 1 Introduction We present the results of the shared tasks of the Second Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news trans"
W17-4717,P03-1021,0,0.144477,"ed better than monolingual models. The code for these models is freely available.15 DCU (T2): DCU’s submission is an ensemble of neural MT systems with different input factors, designed to jointly tackle both the automatic post-editing and word-level QE. 15 https://github.com/patelrajnath/rnn4nlp 186 Word-level features which have proven effective for QE, such as part-of-speech tags and dependency labels are included as input factors to NMT systems. NMT systems using different input representations are ensembled together in a log-linear model which is tuned for the F1 -mult metric using MERT (Och, 2003). The output of the ensemble is a pseudo-reference that is then TER aligned with the original MT to obtain OK/BAD tags for each word in the MT hypothesis. DFKI (T1): These submissions investigate alternative machine learning models for the prediction of the HTER score on the sentencelevel task. Instead of directly predicting the HTER score, the systems use a single-layer perceptron with four outputs that jointly predict the number of each of the four distinct post-editing operations that are then used to calculate the HTER score. This also gives the possibility to correct invalid (e.g. negativ"
W17-4717,W15-3037,0,0.0138062,"n in the source side of the SMT parallel corps. These features were used to train a Support Vector Regression (SVR) algorithm using a Radial Basis Function (RBF) kernel within the SCIKITLEARN toolkit.13 The γ, � and C parameters were optimised via grid search with 5-fold cross validation on the training set, resulting in γ=0.01, � = 0.0825, C = 20. This baseline system has proved robust across a range of language pairs, MT systems, and text domains for predicting various In addition to that, 6 new features were included which contain combinations of other features, and which proved useful in (Kreutzer et al., 2015; Martins et al., 2016): 12 https://github.com/ghpaetzold/ questplusplus 13 http://scikit-learn.org/ 184 • • • • Target word + left context. Target word + right context. Target word + aligned source word. POS of target word + POS of aligned source word. • Target word + left context + source word. • Target word + right context + source word. • Punctuation features: The baseline system models the task as a sequence prediction problem using the LinearChain Conditional Random Fields (CRF) algorithm within the CRFSuite tool (Okazaki, 2007). The model was trained using passive-aggressive optimisatio"
W17-4717,P16-1160,0,0.0330991,"t attention (looking at information anywhere in the source sequence during decoding) and hard monotonic attention (looking at one encoder state at a time from left to right, thus being more conservative and faithful to the original input), which are combined in different ways in the case of multi-source models. The artiﬁcial data provided by JunczysDowmunt and Grundkiewicz (2016) are used to boost performance by increasing the size of the corpus used for training. Univerzita Karlova v Praze. CUNI’s (EN-DE) system is based on the character-to-character neural network architecture described in (Lee et al., 2016). This architecture was compared with the standard neural network architecture proposed by Bahdanau et al. (2014) which uses byte-pair encoding (Sennrich et al., 2015) for generating translation tokens. During the experiments, two setups have been compared for each architecture: i) a single encoder with SRC and MT sentences concatenated, and ii) a two-encoder system, where each SRC and MT sentence is fed to a separate encoder. The submitted system uses the two-encoder architecture with a character-level encoder and decoder. The initial state of the decoder is a weighted combination of the ﬁnal"
W17-4717,W17-4733,0,0.0368107,"Missing"
W17-4717,W17-4765,1,0.786172,"Missing"
W17-4717,L16-1582,1,0.768758,"T 183 translations labelled with task-speciﬁc labels. Participants were also provided with a baseline set of features for each task, and a software package to extract these and other quality estimation features and perform model learning (Section 4.1). Participants (Section 4.2) could submit up to two systems for each task. A discussion on the main goals and ﬁndings from this year’s task is given in Section 4.7. 4.1 Baseline systems forms of post-editing effort (2012; 2013; 2014; 2015; 2016a). Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool (Logacheva et al., 2016). These are 28 features that have been deemed the most informative in previous research on word-level QE. 22 of them were taken from the feature set described in (Luong et al., 2014), and had also been used as a baseline feature set at WMT16: Sentence-level baseline system: For Task 1, Q U E ST ++12 (2015) was used to extract 17 MT system-independent features from the source and translation (target) ﬁles and parallel corpora: • Word count in the source and target sentences, and source and target token count ratio. Although these features are sentencelevel (i.e. their values will be the same fo"
W17-4717,C16-1241,0,0.0353857,"Missing"
W17-4717,W14-3342,0,0.0181229,"lity estimation features and perform model learning (Section 4.1). Participants (Section 4.2) could submit up to two systems for each task. A discussion on the main goals and ﬁndings from this year’s task is given in Section 4.7. 4.1 Baseline systems forms of post-editing effort (2012; 2013; 2014; 2015; 2016a). Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool (Logacheva et al., 2016). These are 28 features that have been deemed the most informative in previous research on word-level QE. 22 of them were taken from the feature set described in (Luong et al., 2014), and had also been used as a baseline feature set at WMT16: Sentence-level baseline system: For Task 1, Q U E ST ++12 (2015) was used to extract 17 MT system-independent features from the source and translation (target) ﬁles and parallel corpora: • Word count in the source and target sentences, and source and target token count ratio. Although these features are sentencelevel (i.e. their values will be the same for all words in a sentence), the length of a sentence might inﬂuence the probability of a word being wrong. • Target token, its left and right contexts of 1 word. • Source word aligne"
W17-4717,P16-2046,0,0.0171489,"Missing"
W17-4717,W16-2379,0,0.0228966,"Missing"
W17-4717,P02-1040,0,0.119222,"Missing"
W17-4717,W16-2389,0,0.038207,"Missing"
W17-4717,W17-4736,0,0.0268357,"Missing"
W17-4717,W17-4737,0,0.0383776,"Missing"
W17-4717,W17-4738,0,0.0427074,"Missing"
W17-4717,W14-3301,1,0.655758,"Missing"
W17-4717,W16-2391,1,0.761669,"target sentences into sequences of character embeddings, and then passes them through a series of deep parallel stacked convolution/max pooling layers. The baseline features are provided through a multi-layer perceptron, 187 and then concatenated with the characterlevel information. Finally, the concatenation is passed onto another multi-layer perceptron and the very last layer outputs HTER values. The two submissions differ in the the use of standard (CNN+BASE-Single) and multi-task learning (CNN+BASE-Multi) for training. The QUEST-EMB submission follows the word embeddings approach used by (Scarton et al., 2016) for document-level QE. Here in-domain word embeddings are used instead of embeddings obtained general purpose data (same as in task 2, below). Word embeddings were averaged to generate a single vector for each sentence. Source and target word embeddings were then concatenated with the baseline features and given to an SVM regressor for model building. For the word-level task SHEF investigated a new approach based on predicting the strength of the lexical relationships between the source and target sentences (BMAPS). Following the work by (Madhyastha et al., 2014), a bilinear model is trained"
W17-4717,W16-2323,1,0.349233,"theses. Concatenated source + MT hypothesis are also used as an input representation for some models. The system makes extensive use of the synthetic training data provided by Junczys-Dowmunt and Grundkiewicz (2016), as well as min-risk training for ﬁne-tuning (Shen et al., 2016). The neural systems, which use different input representations but share the same output vocabulary, are then ensembled together in a loglinear model which is tuned for the TER metric using MERT. Fondazione Bruno Kessler. FBK’s (EN-DE & DE-EN) submission extends the existing NMT implementation in the Nematus toolkit (Sennrich et al., 2016) to train an ensemble of multi-source neural APE systems. Building on previous participations based on the phrase-based paradigm (Chatterjee et al., 2015a, 2016), and similar to (Libovick´y et al., 2016), such systems jointly learn from source and target information in order to increase robustness and precision of the automatic corrections. The n-best hypotheses produced by 200 this ensemble are further re-ranked using features based on the edit distance between the original MT output and each APE hypothesis, as well as other statistical models (n-gram language model and operation sequence mod"
W17-4717,P16-1159,0,0.0156896,"different input factors, designed to jointly tackle both the APE task and the Word-Level QE task. Word-Level features which have proven effective for QE, such as word-alignments, partof-speech tags, and dependency labels, are included as input factors to neural machine translation systems, which are trained to output PostEdited MT hypotheses. Concatenated source + MT hypothesis are also used as an input representation for some models. The system makes extensive use of the synthetic training data provided by Junczys-Dowmunt and Grundkiewicz (2016), as well as min-risk training for ﬁne-tuning (Shen et al., 2016). The neural systems, which use different input representations but share the same output vocabulary, are then ensembled together in a loglinear model which is tuned for the TER metric using MERT. Fondazione Bruno Kessler. FBK’s (EN-DE & DE-EN) submission extends the existing NMT implementation in the Nematus toolkit (Sennrich et al., 2016) to train an ensemble of multi-source neural APE systems. Building on previous participations based on the phrase-based paradigm (Chatterjee et al., 2015a, 2016), and similar to (Libovick´y et al., 2016), such systems jointly learn from source and target inf"
W17-4717,2006.amta-papers.25,0,0.817721,"trast to last year, we also provide datasets for two language pairs. The structure used for the data have been the same since WMT15. Each data instance consists of (i) a source sentence, (ii) its automatic translation into the target language, (iii) the manually post-edited version of the automatic translation, (iv) a free reference translation of the source sentence. Post-edits are used to extract labels for the 4.4 Task 1: Predicting sentence-level quality This task consists in scoring (and ranking) translation sentences according to the proportion of their words that need to be ﬁxed. HTER (Snover et al., 2006b) is used as quality score, i.e. the minimum edit distance between the machine translation and its manually post-edited version. Labels HTER labels were computed using the TERCOM tool16 with default settings (tokenised, case insensitive, exact matching only), with scores capped to 1. 188 16 http://www.cs.umd.edu/˜snover/tercom/ Evaluation Evaluation was performed against the true HTER label and/or ranking, using the following metrics: with an edit operation: insertion, deletion, substitution or no edit (correct word). We mark each edited word as BAD, and the remainingn as OK. • Scoring: Pears"
W17-4717,W17-4756,0,0.0529328,"Missing"
W17-4717,P15-4020,1,0.738308,"Missing"
W17-4717,P13-4014,1,0.634274,"Missing"
W17-4717,W17-4720,1,0.830567,"Missing"
W17-4717,W17-4776,0,0.0596806,"Missing"
W17-4717,W17-4777,1,0.832756,"Missing"
W17-4717,W17-4742,0,0.0342495,"Missing"
W17-4717,W17-4744,0,0.0607458,"Missing"
W17-4717,C00-2137,0,0.0420443,"ch edited word as BAD, and the remainingn as OK. • Scoring: Pearson’s r correlation score (primary metric, ofﬁcial score for ranking submissions), Mean Average Error (MAE) and Root Mean Squared Error (RMSE). Evaluation Analogously to the last year’s task, the primary evaluation metric is the multiplication of F1 -scores for the OK and BAD classes, denoted as F1 -mult. Unlike previously used F1 BAD score this metric is not biased towards “pessimistic” labellings. We also report F1 -scores for individual classes for completeness. We test the signiﬁcance of the results using randomisation tests (Yeh, 2000) with Bonferroni correction (Abdi, 2007). • Ranking: Spearman’s ρ rank correlation and DeltaAvg. Statistical signiﬁcance on Pearson r was computed using the William’s test.17 Results Tables 13 and 14 summarise the results for Task 1 on German–English and English– German datasets, respectively, ranking participating systems best to worst using Pearson’s r correlation as primary key. Spearman’s ρ correlation scores should be used to rank systems for the ranking variant. The top three systems are the same for both datasets, and the ranking of systems according to their performance is similar for"
W17-4717,W17-4745,1,0.825998,"Missing"
W17-4717,P16-1147,0,0.0102821,"Missing"
W18-6401,W18-6432,1,0.779979,"Missing"
W18-6401,W07-0718,1,0.492999,"Participants were asked to build machine translation systems for any of 7 language pairs in both directions, to be evaluated on a test set of news stories. The main metric for this task is human judgment of translation quality. This year, we also opened up the task to additional test sets to probe specific aspects of translation. 1 Introduction The Third Conference on Machine Translation (WMT) held at EMNLP 20181 host a number of shared tasks on various aspects of machine translation. This conference builds on twelve previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a, 2017). This year we conducted several official tasks. We report in this paper on the news translation task. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Neves et al., 2018), • multimodal machine translation (Barrault et al., 2018), • metrics (Ma et al., 2018), • quality estimation (Specia et al., 2018), • automatic post-editing (Chatterjee et al., 2018), and • parallel corpus filtering (Koehn et al., 2018b). In the news translation task (Section 2), participants we"
W18-6401,W11-2101,1,0.731598,"Missing"
W18-6401,W08-0309,1,0.64499,"Missing"
W18-6401,W10-1703,1,0.498736,"Missing"
W18-6401,W12-3102,1,0.647494,"Missing"
W18-6401,E14-2008,0,0.0242419,"n the WMT18 bitexts provided, including ParaCrawl. Some models employed pretrained word embeddings built on BPE’d corpora (Sennrich et al., 2016). A Marian transformer model performed right-to-left rescoring for this system. The third system is trained with Moses (Koehn et al., 2007), using the same data as the Marian system. Hierarchical reordering and Operation Sequence Model were employed. The 5-gram English language model was trained with KenLM (Heafield, 2011) on the same corpus as the AFRL WMT15 system with the same BPE used in the Marian systems. Lastly, RWTH Jane’s system combination (Freitag et al., 2014) was applied yielding approximately a +0.5 gain in BLEU. 2.3.3 CUNI-KOCMI (Kocmi et al., 2018) 2.3.6 FACEBOOK -FAIR ? (Edunov et al., 2018) FACEBOOK -FAIR is an ensemble of six selfattentional models with back-translation data according to Edunov et al. (2018). Synthetic sources are sampled instead of beam search, oversampling the real bitext at a rate of 16, i.e., each bitext is sampled 16 times more often per epoch than the back-translated data. At inference time, translations which are copies of the source are filtered out, replacing them with the output of a very small news-commentary only"
W18-6401,E17-2058,0,0.0576994,"Missing"
W18-6401,W18-6406,0,0.107817,"t (Junczys-Dowmunt, 2018) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2018) ModernMT, MMT s.r.l. (no associated paper) University of Tartu (Tars and Fishel, 2018) National Institute of Information and Communications Technology (Marie et al., 2018) Northeastern University / NiuTrans Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Catalonia (Casas et al., 2018) Tencent (Wang et al., 2018a) Tilde (Pinnis et al., 2018) Ubiqus (no associated paper) University of Cambridge (Stahlberg et al., 2018) University of Edinburgh (Haddow et al., 2018) University of Maryland (Xu and Carpuat, 2018) Unisound (no associated paper) University of Tartu (Del et al., 2018) Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous y"
W18-6401,W13-2305,1,0.876146,"017 (English) and news 2011 (Chinese). Subwords (BPE) are used for both English and Chinese sentences. 3 Human Evaluation A human evaluation campaign is run each year to assess translation quality and to determine the final ranking of systems taking part in the competition. This section describes how preparation of evaluation data, collection of human assessments, and computation of the official results of the shared task was carried out this year. Work on evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality (Graham et al., 2013, 2014, 2016), and two years ago the evaluation campaign included parallel assessment of a subset of News task language pairs evaluated with relative ranking (RR) and DA. DA has some clear advantages over RR, namely the evaluation of absolute translation quality and the ability to carry out evaluations through quality controlled crowd-sourcing. As established in 2016 (Bojar et al., 2016a), DA results (via crowd-sourcing) and RR results (produced by researchers) correlate strongly, with Pearson correlation ranging from 0.920 to 0.997 across several source languages into English and at 0.975 for"
W18-6401,E14-1047,1,0.89975,"Missing"
W18-6401,W18-6407,1,0.888041,"ns Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Catalonia (Casas et al., 2018) Tencent (Wang et al., 2018a) Tilde (Pinnis et al., 2018) Ubiqus (no associated paper) University of Cambridge (Stahlberg et al., 2018) University of Edinburgh (Haddow et al., 2018) University of Maryland (Xu and Carpuat, 2018) Unisound (no associated paper) University of Tartu (Del et al., 2018) Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. “?” indicates invited participation with a late submission, where the team is not considered a regular participant. 276 and Estonian. The system is based on the Transformer (Vaswani et al., 2017) implementation in OpenNMT-py (Klein et al., 2017). It is trained on filtered pa"
W18-6401,W18-6410,0,0.0609282,"ions, organized into 35 teams are listed in Table 2 and detailed in the rest of this section. Each system did not necessarily appear in all translation tasks. We also included 39 online MT systems (originating from 5 services), which we anonymized as ONLINE -A,B,F,G. For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, these online and commercial systems are treated as unconstrained during the automatic and human evaluations. 2.3.1 A ALTO (Grönroos et al., 2018) Aalto participated in the constrained condition of the multi-lingual subtrack, with a single system trained to translate from English to both Finnish 3 http://www.yandex.com/ Estonian Research Council institutional research grant IUT20-56: “Computational models of the Estonian Language” 4 5 As of Fall 2011, the proceedings of the European Parliament are no longer translated into all official languages. 273 Europarl Parallel Corpus German ↔ English Czech ↔ English Finnish ↔ English Estonian ↔ English Sentences 1,920,209 646,605 1,926,114 652,944 Words 50,486,398 53,008,851 14,946,399 17,376,43"
W18-6401,D18-1045,0,0.0609466,"Missing"
W18-6401,W11-2123,0,0.0087119,"t sets. The second is a Marian (Junczys-Dowmunt et al., 2018) system ensembling 5 Univ. Edinburgh “bi-deep” and 6 transformer models all trained on the WMT18 bitexts provided, including ParaCrawl. Some models employed pretrained word embeddings built on BPE’d corpora (Sennrich et al., 2016). A Marian transformer model performed right-to-left rescoring for this system. The third system is trained with Moses (Koehn et al., 2007), using the same data as the Marian system. Hierarchical reordering and Operation Sequence Model were employed. The 5-gram English language model was trained with KenLM (Heafield, 2011) on the same corpus as the AFRL WMT15 system with the same BPE used in the Marian systems. Lastly, RWTH Jane’s system combination (Freitag et al., 2014) was applied yielding approximately a +0.5 gain in BLEU. 2.3.3 CUNI-KOCMI (Kocmi et al., 2018) 2.3.6 FACEBOOK -FAIR ? (Edunov et al., 2018) FACEBOOK -FAIR is an ensemble of six selfattentional models with back-translation data according to Edunov et al. (2018). Synthetic sources are sampled instead of beam search, oversampling the real bitext at a rate of 16, i.e., each bitext is sampled 16 times more often per epoch than the back-translated da"
W18-6401,E17-3017,1,0.773822,"eriment, right-toleft reranking does not help. Another focus is 277 (SMT) submission for the Finnish morphology test suite (Burlot et al., 2018). given to data filtering through rules, translation model and language model including parallel data and monolingual data. The language model is based the Transformer architecture as well. The final system is trained with four different seeds and mixed data. 2.3.8 2.3.9 JHU (Koehn et al., 2018a) The JHU systems are the result of two relatively independent efforts on German–English language directions and Russian–English, using the Marian and Sockeye (Hieber et al., 2017) neural machine translation toolkits, respectively. The novel contributions are iterative back-translation (for German) and fine-tuning on test sets from prior years (for both languages). HY (Raganato et al., 2018; Hurskainen and Tiedemann, 2017) The University of Helsinki (HY) submitted four systems: HY-AH, HY-NMT, HY-NMT-2 STEP and HY-SMT. 2.3.10 JUCBNMT (Mahata et al., 2018) JUCBNMT is an encoder-decoder sequence-tosequence NMT model with character level encoding. The submission uses preprocessing like tokenization, truecasing and corpus cleaning. Both encoder and decoder use a single LSTM"
W18-6401,P17-4012,0,0.0266435,"ted paper) University of Tartu (Del et al., 2018) Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. “?” indicates invited participation with a late submission, where the team is not considered a regular participant. 276 and Estonian. The system is based on the Transformer (Vaswani et al., 2017) implementation in OpenNMT-py (Klein et al., 2017). It is trained on filtered parallel and filtered back-translated monolingual data. The main contribution is a novel cross-lingual Morfessor (Virpioja et al., 2013) segmentation using cognates extracted from the parallel data. The aim is to improve the consistency of the morphological segmentation. Aalto decode using an ensemble of 3 (et) or 8 (fi) models. 2.3.4 2.3.2 2.3.5 AFRL The CUNI-KOCMI submission focuses on the low-resource language neural machine translation (NMT). The final submission uses a method of transfer learning: the model is pretrained on a related high-resource language (her"
W18-6401,W18-6413,0,0.01703,"he constrained systems, however, the data, taking into account its relatively large size, was not factored. T ENCENT (Wang et al., 2018a) T ENCENT- ENSEMBLE (called TenTrans) is an improved NMT system on Transformer based on self-attention mechanism. In addition to the basic settings of Transformer training, T ENCENTENSEMBLE uses multi-model fusion techniques, multiple features reranking, different segmentation models and joint learning. Additionally, data selection strategies were adopted to fine-tune the trained system, achieving a stable performance improvement. An additional system paper (Hu et al., 2018) describes a non-primary submission. 2.3.29 TILDE 2.3.30 U BIQUS The U BIQUS -NMT system is probably developed by the Ubiqus company (www.ubiqus.com). No further information is available. 2.3.31 UCAM (Stahlberg et al., 2018) UCAM is a generalization of previous work (de Gispert et al., 2017) to multiple architectures. It is a system combination of two Transformer-like models, a recurrent model, a convolutional model, and a phrase-based SMT system. The output is probably dominated by the Transformer, and to some extend by the SMT system. (Pinnis et al., 2018) submitted four systems: TILDE - C -"
W18-6401,W18-6416,1,0.791378,"Missing"
W18-6401,W17-4730,0,0.0118952,". HY-SMT (Tiedemann et al., 2016) is the Helsinki SMT system submitted at WMT 2016 (the constrained-basic+back-translated version). The system was not retrained and it may thus suffer from poor lexical coverage on recent test data. The main motivation for including this baseline was to have a statistical machine translation 2.3.13 LMU- NMT (Huck et al., 2018) For the WMT18 news translation shared task, LMU Munich (Huck et al., 2018) has trained ba278 2.3.15 sic shallow attentional encoder-decoder systems (Bahdanau et al., 2014) with the Nematus toolkit (Sennrich et al., 2017), like last year (Huck et al., 2017a). LMU has participated with these NMT systems for the English–German language pair in both translation directions. The training data is a concatenation of Europarl, News Commentary, Common Crawl, and some synthetic data in the form of backtranslated monolingual news texts. The 2017 monolingual News Crawl is not employed, nor are the parallel Rapid and ParaCrawl corpora. The German data is preprocessed with a linguistically informed word segmentation technique (Huck et al., 2017b). By using a linguistically more sound word segmentation, advantages over plain BPE segmentation are expected in t"
W18-6401,W18-6417,1,0.821949,"as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a, 2017). This year we conducted several official tasks. We report in this paper on the news translation task. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Neves et al., 2018), • multimodal machine translation (Barrault et al., 2018), • metrics (Ma et al., 2018), • quality estimation (Specia et al., 2018), • automatic post-editing (Chatterjee et al., 2018), and • parallel corpus filtering (Koehn et al., 2018b). In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We held 1 2 http://www.statmt.org/wmt18/ 272 http://statmt.org/wmt18/results.html Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 272–303 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64028 tions are also available for interactive visualization and comparison of diff"
W18-6401,W17-4706,0,0.0132191,". HY-SMT (Tiedemann et al., 2016) is the Helsinki SMT system submitted at WMT 2016 (the constrained-basic+back-translated version). The system was not retrained and it may thus suffer from poor lexical coverage on recent test data. The main motivation for including this baseline was to have a statistical machine translation 2.3.13 LMU- NMT (Huck et al., 2018) For the WMT18 news translation shared task, LMU Munich (Huck et al., 2018) has trained ba278 2.3.15 sic shallow attentional encoder-decoder systems (Bahdanau et al., 2014) with the Nematus toolkit (Sennrich et al., 2017), like last year (Huck et al., 2017a). LMU has participated with these NMT systems for the English–German language pair in both translation directions. The training data is a concatenation of Europarl, News Commentary, Common Crawl, and some synthetic data in the form of backtranslated monolingual news texts. The 2017 monolingual News Crawl is not employed, nor are the parallel Rapid and ParaCrawl corpora. The German data is preprocessed with a linguistically informed word segmentation technique (Huck et al., 2017b). By using a linguistically more sound word segmentation, advantages over plain BPE segmentation are expected in t"
W18-6401,W18-6430,0,0.299738,"of Helsinki (Raganato et al., 2018) Johns Hopkins University (Koehn et al., 2018a) Jadavpur University (Mahata et al., 2018) Karlsruhe Institute of Technology (Pham et al., 2018) Li Muze (no associated paper) LMU Munich (Huck et al., 2018) LMU Munich (Stojanovski et al., 2018) Microsoft (Junczys-Dowmunt, 2018) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2018) ModernMT, MMT s.r.l. (no associated paper) University of Tartu (Tars and Fishel, 2018) National Institute of Information and Communications Technology (Marie et al., 2018) Northeastern University / NiuTrans Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Catalonia (Casas et al., 2018) Tencent (Wang et al., 2018a) Tilde (Pinnis et al., 2018) Ubiqus (no associated paper) University of Cambridge (Stahlberg et al., 2018) University of Edinburgh (Haddow et al., 2018) University of Maryland (Xu and Carpuat, 2018) Unisound (no associated paper) University of Tartu (Del et al., 2018) Table 2: Part"
W18-6401,W18-6427,0,0.0533796,"Missing"
W18-6401,W18-6428,0,0.0966755,"D U NISOUND U NSUP TARTU Institution Aalto University (Grönroos et al., 2018) Air Force Research Laboratory (Gwinnup et al., 2018) Alibaba Group (Deng et al., 2018) Charles University (Kocmi et al., 2018) Charles University (Popel, 2018) Facebook AI Research (Edunov et al., 2018) Global Tone Communication Technology (Bei et al., 2018) University of Helsinki (Raganato et al., 2018) Johns Hopkins University (Koehn et al., 2018a) Jadavpur University (Mahata et al., 2018) Karlsruhe Institute of Technology (Pham et al., 2018) Li Muze (no associated paper) LMU Munich (Huck et al., 2018) LMU Munich (Stojanovski et al., 2018) Microsoft (Junczys-Dowmunt, 2018) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2018) ModernMT, MMT s.r.l. (no associated paper) University of Tartu (Tars and Fishel, 2018) National Institute of Information and Communications Technology (Marie et al., 2018) Northeastern University / NiuTrans Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Ca"
W18-6401,W18-6431,0,0.0938702,"nications Technology (Marie et al., 2018) Northeastern University / NiuTrans Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Catalonia (Casas et al., 2018) Tencent (Wang et al., 2018a) Tilde (Pinnis et al., 2018) Ubiqus (no associated paper) University of Cambridge (Stahlberg et al., 2018) University of Edinburgh (Haddow et al., 2018) University of Maryland (Xu and Carpuat, 2018) Unisound (no associated paper) University of Tartu (Del et al., 2018) Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. “?” indicates invited participation with a late submission, where the team is not considered a regular participant. 276 and Estonian. The system is based on the Transformer (Vaswani et al., 2017) implement"
W18-6401,W16-2326,0,0.0278975,"Missing"
W19-5301,W19-5424,1,0.858444,"Missing"
W19-5301,W19-5306,0,0.248769,"al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated"
W19-5301,D18-1549,0,0.116951,"s are available for this system. 2.5.7 BASELINE - RE - RERANK (no associated CUNI-T RANSFORMER -T2T2018 (Popel, 2018) is the exact same system as used last year. paper) BASELINE - RE - RERANK is a standard Transformer, with corpus filtering, pre-processing, postprocessing, averaging and ensembling as well as n-best list reranking. 2.5.8 CUNI-T RANSFORMER -M ARIAN (Popel et al., 2019) is a “reimplementation” of the last year’s system (Popel, 2018) in Marian (JunczysDowmunt et al., 2018). CA I RE (Liu et al., 2019) CUNI-U NSUPERVISED -NER- POST (Kvapilíková et al., 2019) follows the strategy of Artetxe et al. (2018), creating a seed phrase-based system where the phrase table is initialized from cross-lingual embedding mappings trained on monolingual data, followed by a neural machine translation system trained on synthetic parallel corpus. The synthetic corpus is produced by the seed phrase-based MT system or by a such a model refined through iterative back-translation. CUNI-U NSUPERVISED -NER- POST further focuses on the handling of named entities, i.e. the part of vocabulary where the cross-lingual embedding mapping suffer most. CA I RE is a hybrid system that took part only in the unsupervised track."
W19-5301,D18-1332,0,0.0215805,"et al., 2018). For English↔Gujarati, synthetic parallel data from two sources, backtranslation and pivoting through Hindi, is produced using unsupervised and semi-supervised NMT models, pre-trained using a cross-lingual language objective (Lample and Conneau, 2019) For German→English, the impact of vast amounts of back-translated training data on translation quality is studied, and some additional insights are gained over (Edunov et al., 2018). Towards the end of training, for German→English and Chinese↔English, the mini-batch size was increased up to fifty-fold by delaying gradient updates (Bogoychev et al., 2018) as an alternative to learning rate cooldown (Smith, 2018). For Chinese↔English, a comparison of different segmentation strategies showed that character-based decoding was superior to the translation of subwords when translating into Chinese. Pre-processing strategies were also investigated for English→Czech, showing that preprocessing can be simplified without loss to MT quality. UEDIN’s main findings on the Chinese↔English translation task are that character-level model on the Chinese side can be used when translating into Chinese to improve the BLEU score. The same does not hold when transl"
W19-5301,W19-5351,0,0.0505622,"Missing"
W19-5301,W19-5423,0,0.0419015,"Missing"
W19-5301,W18-6412,1,0.856623,"Missing"
W19-5301,W19-5305,0,0.0496912,"Missing"
W19-5301,W12-3102,1,0.474924,"Missing"
W19-5301,W19-5310,0,0.0432396,"Missing"
W19-5301,W19-5425,0,0.0236032,"ys-Dowmunt et al., 2018) and Phrase-based machine translation system (implemented with Moses) and for the Spanish-Portuguese task. The system combination included features formerly presented in (Marie and Fujita, 2018), including scores left-to-right and right-to-left, sentence level translation probabilities and language model scores. Also authors provide contrastive results with an unsupervised phrase-based MT system which achieves quite close results to their primary system. Authors associate high performance of the unsupervised system to the language similarity. Incomslav: Team INCOMSLAV (Chen and Avgustinova, 2019) by Saarlad University participated in the Czech to Polish translation task only. The team’s primary submission builds on a transformer-based NMT baseline with back translation which has been submitted one of their contrastive submission. Incomslav’s primary system is a phoneme-based system re-scored using their NMT baseline. A second contrastive submission builds our phrase-based SMT system combined with a joint BPE model. NITS-CNLP: The NITS-CNLP team (Laskar et al., 2019) by the National Institute of Technology Silchar in India submitted results to the HI-NE translation task in both directi"
W19-5301,W07-0718,1,0.530103,"ojar Charles University Yvette Graham Barry Haddow Dublin City University University of Edinburgh Philipp Koehn JHU / University of Edinburgh Mathias Müller University of Zurich Marta R. Costa-jussà Christian Federmann UPC Microsoft Cloud + AI Shervin Malmasi Harvard Medical School Santanu Pal Saarland University Matt Post JHU Abstract Introduction The Fourth Conference on Machine Translation (WMT) held at ACL 20191 hosts a number of shared tasks on various aspects of machine translation. This conference builds on 13 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Bawden et al., 2019b) • automatic post-editing (Chatterjee et al., 2019) • metrics (Ma et al., 2019) • quality estimation (Fonseca et al., 2019) • parallel corpus filtering (Koehn et al., 2019) • robustness (Li et al., 2019b) In the news translation task (Section 2), participants were asked to tra"
W19-5301,P16-2058,1,0.819865,"ipated with the Transformer (Vaswani et al., 2017) implemented in the OpenNMT toolkit. They focused on word segmentation methods and compared a cognate-aware segmentation method, Cognate Morfessor (Grönroos et al., 2018), with character segmentation and unsupervised segmentation methods. As primary submission they submitted this Cognate Morfessor that optimizes subword segmentations consistently for cognates. They participated for all translation directions in Spanish-Portuguese and Czech-Polish, and this Cognate Morfessor performed better for Czech-Polish, while characterbased segmentations (Costa-jussà and Fonollosa, 2016), while much more inefficient, were superior for Spanish-Portuguese. UPC-TALP: The UPC-TALP team (Biesialska et al., 2019) by the Universitat Politècnica de Catalunya submitted a Transformer (implemented with Fairseq (Ott et al., 2019)) for the Czechto-Polish task and a Phrase-based system (implemented with Moses (Koehn et al., 2007)) for Spanish-to-Portuguese. They tested adding monolingual data to the NMT system by copying the same data on the source and target sides, with negative results. Also, their system combination based on sentence-level BLEU in back-translation 5.4 Conclusion of Simi"
W19-5301,W08-0309,1,0.659809,"Missing"
W19-5301,W18-3931,1,0.820211,"or they use English as a pivot language to translate between resource-poorer languages. The interest in English is reflected, for example, in the WMT translation tasks (e.g. News, Biomedical) which have always included language pairs in which texts are translated to and/or from English. With the widespread use of MT technology, there is more and more interest in training systems to translate between languages other than English. One evidence of this is the need of directly translating between pairs of similar languages, varieties, and dialects (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018). The main challenge is to take advantage of the similarity between languages to overcome the limitation given the low amount of available parallel data to produce an accurate output. Given the interest of the community in this topic we organize, for the first time at WMT, a shared task on ""Similar Language Translation"" to evaluate the performance of state-of-the-art translation systems on translating between pairs of languages from the same language family. We provide participants with training and testing data from three language pairs: Spanish - Portuguese (Romance languages), Czech - Polis"
W19-5301,W19-5312,0,0.0773587,"Missing"
W19-5301,W19-5313,0,0.0931699,"University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of"
W19-5301,W19-5314,0,0.0200465,"Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated paper) 8 participated in all language pairs. The translations from the Table 5: Participants in the shared translation task. Not all teams online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. 2.5.6 BTRANS only the middle sentence was considered for the final translation hypothesis, otherwise shorter context of two sentences or just a single sentence was used. Unfortunately, no details are available for this system. 2.5.7 BASELINE - RE - RERANK (no associ"
W19-5301,D18-1045,0,0.0285693,"xt was morphologically segmented with Apertium. The UEDIN systems are supervised NMT systems based on the transformer architecture and trained using Marian (Junczys-Dowmunt et al., 2018). For English↔Gujarati, synthetic parallel data from two sources, backtranslation and pivoting through Hindi, is produced using unsupervised and semi-supervised NMT models, pre-trained using a cross-lingual language objective (Lample and Conneau, 2019) For German→English, the impact of vast amounts of back-translated training data on translation quality is studied, and some additional insights are gained over (Edunov et al., 2018). Towards the end of training, for German→English and Chinese↔English, the mini-batch size was increased up to fifty-fold by delaying gradient updates (Bogoychev et al., 2018) as an alternative to learning rate cooldown (Smith, 2018). For Chinese↔English, a comparison of different segmentation strategies showed that character-based decoding was superior to the translation of subwords when translating into Chinese. Pre-processing strategies were also investigated for English→Czech, showing that preprocessing can be simplified without loss to MT quality. UEDIN’s main findings on the Chinese↔Engl"
W19-5301,W18-6410,0,0.0193718,"ormance can be found in Hindi-Nepali (both directions), where the best performing system is around 50 BLEU (53 for Hindi-to-Nepali and 49.1 for Nepali-toHindi), and the lowest entry is 1.4 for Hindi-toNepali and 0 for Nepali-to-Hindi. The lowest variance is for Polish-to-Czech and it may be because only two teams participated. UHelsinki: The University of Helsinki team (Scherrer et al., 2019) participated with the Transformer (Vaswani et al., 2017) implemented in the OpenNMT toolkit. They focused on word segmentation methods and compared a cognate-aware segmentation method, Cognate Morfessor (Grönroos et al., 2018), with character segmentation and unsupervised segmentation methods. As primary submission they submitted this Cognate Morfessor that optimizes subword segmentations consistently for cognates. They participated for all translation directions in Spanish-Portuguese and Czech-Polish, and this Cognate Morfessor performed better for Czech-Polish, while characterbased segmentations (Costa-jussà and Fonollosa, 2016), while much more inefficient, were superior for Spanish-Portuguese. UPC-TALP: The UPC-TALP team (Biesialska et al., 2019) by the Universitat Politècnica de Catalunya submitted a Transform"
W19-5301,W19-5317,0,0.114089,"ed paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 201"
W19-5301,W19-5315,0,0.0266353,"Missing"
W19-5301,W19-5318,0,0.198338,"ance of the systems when translating from French to German seems to heavily depend on the 7 http://data.statmt.org/wmt19/ translation-task/dev.tgz 6 Systems MSRA.MADL eTranslation LIUM MLLP-UPV onlineA TartuNLP onlineB onlineY onlineG onlineX FULL 47.3 45.4 43.7 41.5 40.8 39.2 39.1 39.0 38.5 38.1 source FR 38.3 37.4 37.5 36.4 35.4 34.8 35.3 34.7 34.6 35.6 source DE 50.0 47.8 45.5 43.0 42.3 40.5 40.2 40.2 39.7 38.8 evaluations. In the rest of this sub-section, we provide brief details of the submitted systems, for those in cases where the authors provided such details. 2.5.1 AFRL - SYSCOMB 19 (Gwinnup et al., 2019) is a system combination of a Marian ensemble system, two distinct OpenNMT systems, a Sockeyebased Elastic Weight Consolidation system, and one Moses phrase-based system. Table 3: French→German Meteor scores. Systems MSRA.MADL LinguaCustodia MLLP_UPV Kyoto_University_T2T LIUM onlineY onlineB TartuNLP onlineA onlineX onlineG FULL 52.0 51.3 49.5 48.8 48.3 47.5 46.4 46.3 45.3 42.7 41.7 source FR 51.9 52.5 49.9 49.7 46.5 43.7 43.7 45.0 43.7 41.6 40.9 source DE 52.0 51.0 49.4 48.6 48.7 48.4 47.0 46.7 45.8 42.9 41.9 AFRL- EWC (Gwinnup et al., 2019) is a Sockeye Transformer system trained with the de"
W19-5301,W19-5316,0,0.109691,"boratory (Gwinnup et al., 2019) Apertium (Pirinen, 2019) Apprentice (Li and Specia, 2019) Aylien Ltd. (Hokamp et al., 2019) Baidu (Sun et al., 2019) (no associated paper) (no associated paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of"
W19-5301,E14-1047,1,0.904335,"Missing"
W19-5301,W19-5427,0,0.0467733,"Missing"
W19-5301,W19-5322,1,0.807321,"Missing"
W19-5301,W19-5302,1,0.715203,"20191 hosts a number of shared tasks on various aspects of machine translation. This conference builds on 13 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Bawden et al., 2019b) • automatic post-editing (Chatterjee et al., 2019) • metrics (Ma et al., 2019) • quality estimation (Fonseca et al., 2019) • parallel corpus filtering (Koehn et al., 2019) • robustness (Li et al., 2019b) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We 1 Christof Monz University of Amsterdam Marcos Zampieri University of Wolverhampton held 18 translation tasks this year, between English and each of Chinese, Czech (into Czech only), German, Finnish, Lithuanian, and Russian. New this year were Gujarati↔English and Kazakh↔English. B"
W19-5301,W19-5333,0,0.0923169,"Missing"
W19-5301,W19-5353,0,0.0658382,"Missing"
W19-5301,W19-5430,1,0.873847,"Missing"
W19-5301,W19-5431,0,0.0199622,"Universitat Politècnica de València (UPV) participated with a Transformer (implemented with FairSeq (Ott et al., 2019)) and a finetuning strategy for domain adaptaion in the task of Spanish-Portuguese. Fine-tunning on the development data provide improvements of almost 12 BLEU points, which may explain their clear best performance in the task for this language pair. As a contrastive system authors provided only for the Portuguese-to-Spanish a novel 2D alternating RNN model which did not respond so well when fine-tunning. UBC-NLP: Team UBC-NLP from the University of British Columbia in Canada (Przystupa and Abdul-Mageed, 2019) compared the performance of the LSTM plus attention (Bahdanau et al., 2015) and Transformer (Vaswani et al., 2017) (implemented in OpenNMT toolkit22 ) perform for the three tasks at hand. Authors use backtranslation to introduce monolingual data in their systems. LSTM plus attention outperformed Transformer for Hindi-Nepali, and viceversa for the other two tasks. As reported by the authors, Hindi-Nepali task provides much more shorter sentences than KYOTOUNIVERSITY: Kyoto University’s submission, listed simply as KYOTO in Table 25 for PT → ES task is based on transformer NMT system. They used"
W19-5301,P02-1040,0,0.11337,"ation of the source (CS), and a second encoder to encode sub-word (byte-pair-encoding) information of the source (CS). The results obtained by their system in translating from Czech→Polish and comment on the impact of out-of-domain test data in the performance of their system. UDSDFKI ranked second among ten teams in Czech– Polish translation. 5.3 Results We present results for the three language pairs, each of them in the two possible directions. For this first edition of the Similar Translation Task and differently from News task, evaluation was only performed on automatic basis using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) measures. Each language direction is reported in one different table which contain information of the team; type of system, either contrastive (C) or primary (P), and the BLEU and TER results. In general, primary systems tend to be better than contrastive systems, as expected, but there are some exceptions. Even if we are presenting 3 pairs of languages each pair belonging to the same family, translation quality in terms of BLEU varies signficantly. While the best systems for Spanish-Portuguese are above 64 BLEU and below 21 TER (see Tables 26 and 27), best syste"
W19-5301,W19-5354,0,0.0611791,"Missing"
W19-5301,W18-6486,0,0.0189853,"the agglutinative nature of Kazakh, (ii) data from an additional language (Russian), given the scarcity of English–Kazakh data and (iii) synthetic data for the source language filtered using language-independent sentence similarity. RUG _ KKEN _ MORFESSOR Tilde developed both constrained and unconstrained NMT systems for English-Lithuanian and Lithuanian-English using the Marian toolkit. All systems feature ensembles of four to five transformer models that were trained using the quasi-hyperbolic Adam optimiser (Ma and Yarats, 2018). Data for the systems were prepared using TildeMT filtering (Pinnis, 2018) and preprocessing (Pinnis et al., 2018) methods. For unconstrained systems, data were additionally filtered using dual conditional cross-entropy filtering (Junczys-Dowmunt, 2018a). All systems were trained using iterative back-translation (Rikters, 2018) and feature synthetic data that allows training NMT systems to support handling of unknown phenomena (Pinnis et al., 2017). During translation, automatic named entity and nontranslatable phrase post-editing were performed. For constrained systems, named entities and nontranslatable phrase lists were extracted from the parallel training data."
W19-5301,W19-5335,0,0.0408704,"Missing"
W19-5301,W19-5344,1,0.904781,"n Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas e"
W19-5301,W19-5346,0,0.197908,"rtium (Pirinen, 2019) Apprentice (Li and Specia, 2019) Aylien Ltd. (Hokamp et al., 2019) Baidu (Sun et al., 2019) (no associated paper) (no associated paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communicatio"
W19-5301,W19-5341,0,0.0172601,"A,B,G,X,Y. For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human AYLIEN _ MULTILINGUAL (Hokamp et al., 2019) The Aylien research team built a Multilingual NMT system which is trained on all WMT2019 language pairs in all directions, then fine-tuned for a small number of iterations on Gujarati-English data only, including some self-backtranslated data. 2.5.5 BAIDU (Sun et al., 2019) Baidu systems are based on the Transformer architecture with several improvements. Data selection, back translation, data augmentation, knowledge distillation, domain adaptation, model ensemble and re-ranking are employed and proven effective in our experiments. 7 Team AFRL A PERTIUM - FIN - ENG A PPRENTICE - C AYLIEN _ MULTILINGUAL BAIDU BTRANS BASELINE - RE - RERANK CA I RE CUNI DBMS-KU DFKI - NMT E T RANSLATION FACEBOOK FAIR GTCOM H ELSINKI NLP IIITH-MT IITP JHU JUMT JU_S AARLAND KSAI K YOTO U NIVERSITY L INGUA C USTODIA LIUM LMU-NMT MLLP-UPV MS T RANSLATOR MSRA N IU T RANS NICT NRC PARFDA"
W19-5301,P16-1162,1,0.310296,"ssible, 2.5.13 E T RANSLATION (Oravecz et al., 2019) E T RANSLATION En-De E T RANSLATION ’s EnDe system is an ensemble of 3 base Transformers and a Transformer-type language model, trained from all available parallel data (cleaned up and filtered with dual conditional cross-entropy filtering) and with additional back-translated data generated 9 2.5.17 from monolingual news. Each Transformer model is fine tuned on previous years’ test sets. H ELSINKI NLP is a Transformer (Vaswani et al., 2017) style model implemented in OpenNMTpy using a variety of corpus filtering techniques, truecasing, BPE (Sennrich et al., 2016), backtranslation, ensembling and fine-tuning for domain adaptation. E T RANSLATION Fr-De The Fr-De system is an ensemble of 2 big Transformers (with size 8192 FFN layers). Back-translation data was selected using topic modelling techniques to tune the model towards the domain defined in the task. 2.5.18 En-Lt The En-Lt system is an ensemble of 2 big Transformers (as for Fr-De) and a Transformer type language model. The training data contains the Rapid corpus and the news domain back-translated data sets 2 times oversampled. E T RANSLATION 2.5.19 FACEBOOK FAIR (Ng et al., 2019) 2.5.20 JHU (Mar"
W19-5301,W19-5339,0,0.0767002,"Missing"
W19-5301,W19-5347,0,0.0328257,"Missing"
W19-5301,W19-5342,1,0.887858,"encia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated paper) 8 participated in all language pairs. The translations from the Table 5: Participants in the shared translation task. Not all teams online systems were not submitted by their respective companies but were obtained by us, and"
W19-5301,W19-5355,1,0.869964,"Missing"
W19-5301,W19-5350,0,0.0441915,"Missing"
W19-5301,P98-2238,0,0.38957,"n trained to translate texts from and to English or they use English as a pivot language to translate between resource-poorer languages. The interest in English is reflected, for example, in the WMT translation tasks (e.g. News, Biomedical) which have always included language pairs in which texts are translated to and/or from English. With the widespread use of MT technology, there is more and more interest in training systems to translate between languages other than English. One evidence of this is the need of directly translating between pairs of similar languages, varieties, and dialects (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018). The main challenge is to take advantage of the similarity between languages to overcome the limitation given the low amount of available parallel data to produce an accurate output. Given the interest of the community in this topic we organize, for the first time at WMT, a shared task on ""Similar Language Translation"" to evaluate the performance of state-of-the-art translation systems on translating between pairs of languages from the same language family. We provide participants with training and testing data from three language"
W19-6611,P17-1080,0,0.0471713,"r architecture that encodes the source sentence into a distributed representation and then decodes this representation into a sentence in the target language. While earlier work has investigated what information is captured by the attention mechanism of an NMT system (Ghader and Monz, 2017), it is not exactly clear what linguistic information from the source sentence is encoded in the hidden distributed representation themselves. Recently, some attempts have been made to shed some light on the information that is being encoded in the intermediate distributed representations (Shi et al., 2016; Belinkov et al., 2017). Feeding the hidden states of the encoder of different seq2seq systems, including multiple NMT systems, as the input to different classifiers, Shi et al. (2016) aim to show what syntactic information is encoded in the hidden states. They provide evidence that syntactic information such as the voice and tense of a sentence and the part-ofspeech (POS) tags of words are being learned with reasonable accuracy. They also provide evidence that more complex syntactic information such as the parse tree of a sentence is also learned, but with lower accuracy. Belinkov et al. (2017) follow the same appr"
W19-6611,D17-1151,0,0.0193446,"t al., 2015a). The encoder consists of a two-layer unidirectional forward and a two-layer unidirectional backward pass. The corresponding output representations from each direction are concatenated to form the encoder hidden state representation for each word. A concatenation and down-projection of the last states of the encoder is used to initialize the first state of the decoder. The decoder uses a two-layer unidirectional (forward) LSTM. We use no residual connection in our recurrent model as they have been shown to result in performance drop if used on the encoder side of recurrent model (Britz et al., 2017). Our transformer model is a 6-layer transformer with multi-headed attention of 8 heads (Vaswani et al., 2017). We choose these settings to obtain competitive models with the relevant core components from each architecture. We train our models for two directions, namely English-German and German-English, both of which use the WMT15 parallel training data. We exclude 100k randomly chosen sentence pairs Dublin, Aug. 19-23, 2019 |p. 108 Model Recurrent Transformer English-German test2014 test2015 test2016 24.65 26.75 30.53 26.93 29.01 32.44 test2017 25.51 27.36 Model Recurrent Transformer German-"
W19-6611,W97-0802,0,0.704621,"Missing"
W19-6611,henrich-hinrichs-2010-gernedit,0,0.0169219,"and the word embeddings. We collect statistics showing how much information from embeddings of the input words is preserved by the corresponding hidden states. We also try to shed some light on the information encoded in the hidden states that goes beyond what is transferred from the word embeddings. To this end, we analyze how much the nearest neighbors of words based on their hidden state representations are covered by direct relations in WordNet (Fellbaum, 1998; Miller, 1995). For our German side experiments, we use GermaNet (Hamp and FeldProceedings of MT Summit XVII, volume 1 weg, 1997; Henrich and Hinrichs, 2010). From now on, we use WordNet to refer to either WordNet or GermaNet. This paper does not directly seek improvements to neural translation models, but to further our understanding of the inside behaviour of these models. It explains what information is learned in addition to what is already captured by embeddings. This paper makes the following contributions: 1. We provide interpretable representations of hidden states in NMT systems highlighting the differences between hidden state representations and word embeddings. 2. We compare transformer and recurrent models in a more intrinsic way in t"
W19-6611,P15-1001,0,0.0368103,"ntax. Additionally, we show that the backward recurrent layer in a recurrent model learns more about the semantics of words, whereas the forward recurrent layer encodes more context. 1 Introduction Neural machine translation (NMT) has achieved state-of-the-art performance for many language c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 Christof Monz Informatics Institute, University of Amsterdam, The Netherlands c.monz@uva.nl pairs (Bahdanau et al., 2015; Luong et al., 2015b; Jean et al., 2015; Wu et al., 2016; Vaswani et al., 2017). Additionally, it is straightforward to train an NMT system in an end-to-end fashion. This has been made possible with an encoderdecoder architecture that encodes the source sentence into a distributed representation and then decodes this representation into a sentence in the target language. While earlier work has investigated what information is captured by the attention mechanism of an NMT system (Ghader and Monz, 2017), it is not exactly clear what linguistic information from the source sentence is encoded in the hidden distributed representation th"
W19-6611,C18-1054,0,0.0134705,"t compare different state-of-the-art encoder-decoder architectures in terms of their capabilities to capture syntactic structures (Tran et al., 2018) and lexical semantics (Tang et al., 2018). These works also use some extrinsic tasks to do the comparison. Tran et al. (2018) use subject-verb agreement and logical inference tasks to compare recurrent models with transformers. On the other hand, Tang et al. (2018) use subject-verb agreement and word sense disambiguation for comparing those architectures in terms of capturing syntax and lexical semantics respectively. In addition to these tasks, Lakew et al. (2018) compare recurrent models with transformers on a multilingual machine translation task. Despite the approaches discussed above, attempts to study the hidden states more intrinsically are still missing. For example, to the best of our knowledge, there is no work that studies the encoder hidden states from a nearest neighbor perspective to compare these distributed word representations with the underlying word embeddings. It seems intuitive to assume that the hidden state of the encoder corresponding to an input word conveys more contextual information compared to the embedding of the input word"
W19-6611,D15-1166,0,0.530365,"ing the underlying syntax. Additionally, we show that the backward recurrent layer in a recurrent model learns more about the semantics of words, whereas the forward recurrent layer encodes more context. 1 Introduction Neural machine translation (NMT) has achieved state-of-the-art performance for many language c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 Christof Monz Informatics Institute, University of Amsterdam, The Netherlands c.monz@uva.nl pairs (Bahdanau et al., 2015; Luong et al., 2015b; Jean et al., 2015; Wu et al., 2016; Vaswani et al., 2017). Additionally, it is straightforward to train an NMT system in an end-to-end fashion. This has been made possible with an encoderdecoder architecture that encodes the source sentence into a distributed representation and then decodes this representation into a sentence in the target language. While earlier work has investigated what information is captured by the attention mechanism of an NMT system (Ghader and Monz, 2017), it is not exactly clear what linguistic information from the source sentence is encoded in the hidden distribut"
W19-6611,P15-1002,0,0.0980118,"ing the underlying syntax. Additionally, we show that the backward recurrent layer in a recurrent model learns more about the semantics of words, whereas the forward recurrent layer encodes more context. 1 Introduction Neural machine translation (NMT) has achieved state-of-the-art performance for many language c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 Christof Monz Informatics Institute, University of Amsterdam, The Netherlands c.monz@uva.nl pairs (Bahdanau et al., 2015; Luong et al., 2015b; Jean et al., 2015; Wu et al., 2016; Vaswani et al., 2017). Additionally, it is straightforward to train an NMT system in an end-to-end fashion. This has been made possible with an encoderdecoder architecture that encodes the source sentence into a distributed representation and then decodes this representation into a sentence in the target language. While earlier work has investigated what information is captured by the attention mechanism of an NMT system (Ghader and Monz, 2017), it is not exactly clear what linguistic information from the source sentence is encoded in the hidden distribut"
W19-6611,P02-1040,0,0.104125,"out rate is set to 0.1, and no gradient clipping is used. The word embedding size of both models is 512. We apply Byte-Pair Encoding (BPE) (Sennrich et al., 2016) with 32K merge operation. We train our models until convergence and then use the trained models to translate 100K sentences from a held-out dataset and log the hidden states for later use in our analyses. The 100K held-out data is randomly chosen from the WMT15 parallel training data. The remaining of the WMT15 parallel training data is used as our training data. Table 1 summarizes the performance of our experimental models in BLEU (Papineni et al., 2002) on different standard test sets. This is to make sure that the models are trustable. 3 Nearest Neighbors Analysis Following earlier work on word embeddings (Mikolov et al., 2013; Pelevina et al., 2016), we choose to look into the nearest neighbors of the hidden state representations to learn more about the information encoded in them. We treat each hidden state as the representation of the corresponding input token. This way, each occurrence of a word has its own representation. Based on this representation, we compute the list of n nearest neighbors of each word occurrence. We set n Proceedi"
W19-6611,W16-1620,0,0.169853,"models until convergence and then use the trained models to translate 100K sentences from a held-out dataset and log the hidden states for later use in our analyses. The 100K held-out data is randomly chosen from the WMT15 parallel training data. The remaining of the WMT15 parallel training data is used as our training data. Table 1 summarizes the performance of our experimental models in BLEU (Papineni et al., 2002) on different standard test sets. This is to make sure that the models are trustable. 3 Nearest Neighbors Analysis Following earlier work on word embeddings (Mikolov et al., 2013; Pelevina et al., 2016), we choose to look into the nearest neighbors of the hidden state representations to learn more about the information encoded in them. We treat each hidden state as the representation of the corresponding input token. This way, each occurrence of a word has its own representation. Based on this representation, we compute the list of n nearest neighbors of each word occurrence. We set n Proceedings of MT Summit XVII, volume 1 Figure 1: An example of 5 nearest neighbors of two different occurrences of the word “deregulation”. Triangles are the nearest neighbors of “deregulation” shown with the"
W19-6611,W16-2209,0,0.0574731,"Missing"
W19-6611,P16-1162,0,0.0467744,"0.001 for the Adam optimizer (Kingma and Ba, 2015) with a maximum gradient norm of 5. A dropout rate of 0.3 has been used to avoid overfitting. Our transformer model has hidden state dimensions of 512 and a batch size of 4096 tokens and uses layer normalization (Vaswani et al., 2017). A learning rate of 2 changed under warmup strategy with 8000 warm-up steps is used for Adam optimizer with β1 = 0.9, β2 = 0.998 and  = 10−9 (Vaswani et al., 2017). The dropout rate is set to 0.1, and no gradient clipping is used. The word embedding size of both models is 512. We apply Byte-Pair Encoding (BPE) (Sennrich et al., 2016) with 32K merge operation. We train our models until convergence and then use the trained models to translate 100K sentences from a held-out dataset and log the hidden states for later use in our analyses. The 100K held-out data is randomly chosen from the WMT15 parallel training data. The remaining of the WMT15 parallel training data is used as our training data. Table 1 summarizes the performance of our experimental models in BLEU (Papineni et al., 2002) on different standard test sets. This is to make sure that the models are trustable. 3 Nearest Neighbors Analysis Following earlier work on"
W19-6611,D16-1159,0,0.0604814,"Missing"
W19-6611,D18-1458,0,0.033315,"Missing"
W19-6611,N03-1033,0,0.0523626,"Missing"
W19-6611,D18-1503,1,0.86199,"Missing"
W19-6611,P13-1043,0,0.0707015,"Missing"
W19-6611,W17-4717,1,\N,Missing
W19-6612,W18-6474,0,0.0326628,"Missing"
W19-6612,W17-4717,1,0.816393,"t + st-1 stht 1 Encoder LNLL Decoder x 1 Encoder Decoder yt-1 x yt-1 st-1 D_combined D_clean Teacher network trained only on clean data Student netowrk Figure 1: Distillation for noisy data. Both the teacher and student network have same architecture. Teacher network is trained only on the clean data, student network is trained for two losses : LN LL wrt target labels and LKL wrt to output distribution of teacher network 4 Knowledge distillation for noisy data We discuss the main intuition and idea behind using knowledge distillation for noisy labels. A detailed analysis is given in Li et al. (2017). As shown in Figure 1, the idea is to first train the teacher model f on the clean data Dclean and then transfer the knowledge from the teacher to a student network which is trained on the entire dataset D by optimising the following loss: LD (yi , f (xi )) = λl(yi , f (xi ))+(1−λ)l(si , f (xi )) (9) where si = fDclean (xi )/τ and τ is the temperature of the softmax. In equation 9, the student model is trained on the combination of two loss functions, the first term is the cross-entropy loss l between the prediction of the student model and the ground truth yi , while the second term is the c"
W19-6612,P17-1176,0,0.045981,"Missing"
W19-6612,W18-6477,0,0.024593,"f recurrent NMT models (Khayrallah and Koehn, 2018). Therefore, there is a growing interest in filtering and sampling techniques to extract high-quality sentence pairs from such large noisy parallel texts. Recently, the “Parallel corpus filtering” (Koehn et al., 2018) shared task was held at WMT-2018. This task aims at extracting high-quality sentence pairs from Paracrawl1 , which is a large noisy parallel corpus. Most of the participants in this task, used rule-based pre-filtering followed by a classifier-based scoring of sentence pairs (Barbu and Barbu Mititelu, 2018; Junczys-Dowmunt, 2018; Hangya and Fraser, 2018). A subset sampled with a fixed number of target tokens is then used to train recurrent NMT systems in order to evaluate the relative quality of the filtered bitexts. Some of the submissions show good translation performance for the German-English translation task by training on the filtered bitext only. In this paper, we propose a strategy to leverage additional low-quality bitexts without any filtering when used in conjunction with a high-quality parallel corpus. Motivated by the “knowledge distillation” frame1 https://paracrawl.eu/ Dublin, Aug. 19-23, 2019 |p. 118 Src: Trg: Human: Src: Trg:"
W19-6612,P84-1044,0,0.14236,"Missing"
W19-6612,W18-6478,0,0.277704,"ct on the performance of recurrent NMT models (Khayrallah and Koehn, 2018). Therefore, there is a growing interest in filtering and sampling techniques to extract high-quality sentence pairs from such large noisy parallel texts. Recently, the “Parallel corpus filtering” (Koehn et al., 2018) shared task was held at WMT-2018. This task aims at extracting high-quality sentence pairs from Paracrawl1 , which is a large noisy parallel corpus. Most of the participants in this task, used rule-based pre-filtering followed by a classifier-based scoring of sentence pairs (Barbu and Barbu Mititelu, 2018; Junczys-Dowmunt, 2018; Hangya and Fraser, 2018). A subset sampled with a fixed number of target tokens is then used to train recurrent NMT systems in order to evaluate the relative quality of the filtered bitexts. Some of the submissions show good translation performance for the German-English translation task by training on the filtered bitext only. In this paper, we propose a strategy to leverage additional low-quality bitexts without any filtering when used in conjunction with a high-quality parallel corpus. Motivated by the “knowledge distillation” frame1 https://paracrawl.eu/ Dublin, Aug. 19-23, 2019 |p. 118"
W19-6612,W18-2709,0,0.209159,"arable corpora can be created by crawling large monolingual data in the source and target languages from multilingual news portals such as Agence FrancePresse (AFP), BBC news, Euronews etc. Source and target sentences in these monolingual corpora are then aligned by automatic document and sentence alignment techniques (Munteanu and Marcu, 2005). Such a bitext extracted from comparable data is usually not of the same quality as annotated parallel corpora. Recent research has shown that building models from low-quality data can have a degrading effect on the performance of recurrent NMT models (Khayrallah and Koehn, 2018). Therefore, there is a growing interest in filtering and sampling techniques to extract high-quality sentence pairs from such large noisy parallel texts. Recently, the “Parallel corpus filtering” (Koehn et al., 2018) shared task was held at WMT-2018. This task aims at extracting high-quality sentence pairs from Paracrawl1 , which is a large noisy parallel corpus. Most of the participants in this task, used rule-based pre-filtering followed by a classifier-based scoring of sentence pairs (Barbu and Barbu Mititelu, 2018; Junczys-Dowmunt, 2018; Hangya and Fraser, 2018). A subset sampled with a f"
W19-6612,D16-1139,0,0.0542979,"en pointed out in Koehn et al. (2018), the aim of “Parallel corpus filtering” task proposed at WMT18 was not to select data relevant for a targeted domain, but to focus on the selection of high quality data that is relevant to all domains. Similarly, in this paper, we do not aim to propose a technique for domain adaptation for NMT but to propose a technique to leverage low quality or noisy training data for training high performing NMT models. Although knowledge distillation has been used Proceedings of MT Summit XVII, volume 1 as a solution to other problems of NMT such as model compression (Kim and Rush, 2016), domain adaptation (Dakwale and Monz, 2017) or transfer learning for low-resource languages (Chen et al., 2017) and for leveraging noisy data for image recognition (Li et al., 2017), our approach is the first attempt to exploit distillation for training NMT systems with noisy data. 3 3.1 Background Noise in the training corpora Khayrallah and Koehn (2018) analyzed the Paracrawal corpus, identifying various types of noise in this corpus. They found that although there are some instances of incorrect language, untranslated sentences, and non-linguistic characters, the majority of noisy samples"
W19-6612,P17-4012,0,0.0807488,"Missing"
W19-6612,P16-1009,0,0.0836662,"achine learning literature, various methods have been proposed for efficient learning with label noise. One of the recent methods is the bootstrapping (Reed et al., 2014) approach where improved labels for noisy or unlabeled data can be obtained by predictions of another classifier. For NMT, forward translations of the noisy bitext can be used as a variant of bootstrapping where the target side of the noisy bitext can be replaced by translations of the source sentence obtained by a model trained on the high-quality data. However, a better alternative for NMT would be to use back-translations (Sennrich et al., 2016), i.e., to replace the source side of the noisy bitext by translations of the target side obtained by a model trained in the reverse direction. Our experiments show that although backward translations of noisy data cause lower degradations than the original noisy data, they provide only moderate improvements. Moreover, cleansing the comparable data by backtranslation is expensive as it requires the generation of pseudo source sentences using beam search decoding. Fine-tuning (Miceli Barone et al., 2017) is a well-known technique for domain adaptation for NMT but can also be used as a possible"
W19-6612,W18-6453,0,0.238052,"e monolingual corpora are then aligned by automatic document and sentence alignment techniques (Munteanu and Marcu, 2005). Such a bitext extracted from comparable data is usually not of the same quality as annotated parallel corpora. Recent research has shown that building models from low-quality data can have a degrading effect on the performance of recurrent NMT models (Khayrallah and Koehn, 2018). Therefore, there is a growing interest in filtering and sampling techniques to extract high-quality sentence pairs from such large noisy parallel texts. Recently, the “Parallel corpus filtering” (Koehn et al., 2018) shared task was held at WMT-2018. This task aims at extracting high-quality sentence pairs from Paracrawl1 , which is a large noisy parallel corpus. Most of the participants in this task, used rule-based pre-filtering followed by a classifier-based scoring of sentence pairs (Barbu and Barbu Mititelu, 2018; Junczys-Dowmunt, 2018; Hangya and Fraser, 2018). A subset sampled with a fixed number of target tokens is then used to train recurrent NMT systems in order to evaluate the relative quality of the filtered bitexts. Some of the submissions show good translation performance for the German-Engl"
W19-6612,D15-1166,0,0.0512035,"have a teacher network trained plied on the output of the feedforward network g, on the same data and with a learned distribution which generates the probability of each word in the q(y|x; θT ), the student network (model parameters target vocabulary. Here, sj is the hidden state rep- represented by θ) can be trained by minimizing the resentation corresponding to each token in the tar- following loss: get sequence generated by the decoder RNN. |V |   X sj = fdec (sj−1 , yj−1 , cj ) (3) LKD (θ, θT ) = − KL q(y|x; θT ) p(y|x; θ) The context vector cj is computed using an attention mechanism (Luong et al., 2015) as the weighted sum of the hidden states hi of the encoder. n X cj = αji hi (4) i=1 where αji are attention weights corresponding to each encoder hidden state output hi calculated as follows : exp(a(sj−1 , hi )) αji = Pn k=1 exp(a(sj−1 , hk )) (5) Activations a(s, h) are calculated by using a scoring function such as dot product between the current decoder state sj−1 and each of the hidden Proceedings of MT Summit XVII, volume 1 k=1 (7) where θT is the parameter distribution of the teacher network. Commonly, this loss is interpolated with the log-likelihood loss which is calculated with regar"
W19-6612,D17-1156,0,0.0205068,"gh-quality data. However, a better alternative for NMT would be to use back-translations (Sennrich et al., 2016), i.e., to replace the source side of the noisy bitext by translations of the target side obtained by a model trained in the reverse direction. Our experiments show that although backward translations of noisy data cause lower degradations than the original noisy data, they provide only moderate improvements. Moreover, cleansing the comparable data by backtranslation is expensive as it requires the generation of pseudo source sentences using beam search decoding. Fine-tuning (Miceli Barone et al., 2017) is a well-known technique for domain adaptation for NMT but can also be used as a possible solution for training with noisy data where the idea is to first pre-train on noisy data and then continue training on high-quality data. Our experiments show that when using noisy data for training NMT models, fine-tuning fails to provide any additional improvements. Moreover, bootstrapping based on filtering and backtranslation, as explained above, show only small improvements over a model trained on highquality data only. In order to overcome the dependence on filtering-based data selection or other"
W19-6612,J05-4003,0,0.455106,"019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 Christof Monz Informatics Institute University of Amsterdam c.monz@uva.nl in substantially larger amounts. Such comparable corpora can be created by crawling large monolingual data in the source and target languages from multilingual news portals such as Agence FrancePresse (AFP), BBC news, Euronews etc. Source and target sentences in these monolingual corpora are then aligned by automatic document and sentence alignment techniques (Munteanu and Marcu, 2005). Such a bitext extracted from comparable data is usually not of the same quality as annotated parallel corpora. Recent research has shown that building models from low-quality data can have a degrading effect on the performance of recurrent NMT models (Khayrallah and Koehn, 2018). Therefore, there is a growing interest in filtering and sampling techniques to extract high-quality sentence pairs from such large noisy parallel texts. Recently, the “Parallel corpus filtering” (Koehn et al., 2018) shared task was held at WMT-2018. This task aims at extracting high-quality sentence pairs from Parac"
W19-6612,P02-1040,0,0.106749,"Missing"
W19-6612,W05-0908,0,0.0747546,"Missing"
