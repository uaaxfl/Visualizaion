2014.lilt-9.3,W08-2222,1,0.771435,"thumb, the loss in coverage should outweigh the gain in performance using deep linguistic analysis. Due to the development of tree-banks in the past decades, many high-performing statistical parsers are available that o↵er broad coverage syntactic analysis for open-domain texts. The parser employed in our RTE system, the C&C parser (Clark and Curran 2004), combines speed and robustness with detailed syntactic analyses in the form of derivations of categorial grammar (Steedman 2001). Categorial grammar o↵ers a neat way to construct formal meaning representations with the help of the -calculus (Bos 2008). Each basic syntactic category is associated with a basic semantic type, and using the recursive definition of categories and types, this also fixes the semantic types of complex syntactic categories. This results in a strongly lexically-driven 30 / Johan Bos approach, where only the semantic representations have to be provided for the lexical categories. Function application will take care of the rest and produce meaning representations for phrases beyond the token level, and eventually a complete meaning representation for the entire sentence will be produced. Next we arrive at the choice o"
2014.lilt-9.3,H05-1079,1,0.698673,"simple and rooted in the formal approaches to natural language semantics mentioned before: we translate the texts into logical formulas, and then use (classical) logical inference to find out whether one text entails the other or the other way around, whether they are consistent or contradictory, and so on. Even though this idea itself sounds simple, its execution is not. In this article we describe a framework for textual inference based on first-order logic and formal theory. It comprises a system for RTE, Nutcracker, developed by myself over the years since the start of the RTE challenge (Bos and Markert 2005).2 The input of this system is a text, and an hypothesis (another text). The output of the system is an entailment prediction for the hypothesis given the text. The system makes use of external theorem provers to calculate its predictions. Performance on RTE data sets is measured in terms of recall (the number of correctly predicted entailments divided by the total number of text–hypothesis pairs given to a system) and precision (the number of correctly predicted entailments divided by the number of predictions made by the system). RTE systems based on logical inference tend to be low in recal"
2014.lilt-9.3,P04-1014,0,0.114957,"alysis needs to be sophisticated because a shallow analysis would not support the required logical inferences and hence sacrifice precision in performance. It needs to be robust and o↵er wide coverage to achieve a high recall in performance. As a practical rule of thumb, the loss in coverage should outweigh the gain in performance using deep linguistic analysis. Due to the development of tree-banks in the past decades, many high-performing statistical parsers are available that o↵er broad coverage syntactic analysis for open-domain texts. The parser employed in our RTE system, the C&C parser (Clark and Curran 2004), combines speed and robustness with detailed syntactic analyses in the form of derivations of categorial grammar (Steedman 2001). Categorial grammar o↵ers a neat way to construct formal meaning representations with the help of the -calculus (Bos 2008). Each basic syntactic category is associated with a basic semantic type, and using the recursive definition of categories and types, this also fixes the semantic types of complex syntactic categories. This results in a strongly lexically-driven 30 / Johan Bos approach, where only the semantic representations have to be provided for the lexical c"
2014.lilt-9.3,P07-2009,1,0.781139,"ith higher recall and low precision. The logical inference approach for RTE has been criticized by other RTE practitioners with respect to its low recall. However, in doing so, not always the correct explanation is given. MacCartney et al. (2006), for instance, write “few problem sentences can be accurately translated to logical form” when discussing Bos and Markert (2005), and al2 The Nutcracker system has been briefly described by others (Balduccini et al. 2008), but never been the focus of publication itself. The source code of the system can be downloaded via the website of the C&C tools (Curran et al. 2007). Is there a place for logic in recognizing textual entailment? / 29 though one could debate the notion of accurate translation, it is doubtful whether this is the main reason for the lack of recall in RTE systems using deductive inference. In fact, one of the aims of this article is to show that logical inference is a promising approach to RTE, despite its limitations. The rest of this article is organized as follows. First we explain what we mean by semantic interpretation in the context of RTE, and what formalism is useful for doing so, both from a theoretical and practical perspective. The"
2014.lilt-9.3,N06-1006,0,0.0804785,"Missing"
2014.lilt-9.3,W98-0604,0,0.0803208,"s are considered to correspond to equivalent concepts. A case in point is Ex. 9, where we can observe that honor and honour are members of the same synset in WordNet. Members of the same synset are translated into axioms with a bi-implication. Returning to Ex. 9, we trigger the following axiom: 8w(possible-world(w)! 8x(v1honor(w,x)$v1honour(w,x))) There is more information in WordNet that could form the basis for background knowledge axioms. The antonymy relation found between adjectives is a good candidate. But other lexical resources could supply useful information too. The NomLex database (Meyers et al. 1998) provides information about normalizations, thereby making it possible to compute background knowledge axioms that relate concepts and events. Axioms for embedded contexts The axioms for embedded contexts all follow the same pattern. They are manually picked for sentential complement verbs like know, regret, say, report, tell, reveal, as well as for sentential adverbs such as because, although and when, that presuppose their subordinated sentential argument. They are manually selected because existing lexical resources such as WordNet do not contain this information. Ex. 10 illustrates the ide"
2014.lilt-9.3,P07-1058,0,0.0321703,"writes Y, by applying the distributional hypothesis to syntactic dependency analysis. The method of Ihsani (2012) could be viewed as a variation of this, but di↵ers in the level of supervision during learning (DIRT is unsupervised). The level of linguistic analysis is also di↵erent, as DIRT produces (non-directional) surface string paraphrases, and Ihsani’s method yields (directional) first-order axioms. In general, Ihsani’s method produces Is there a place for logic in recognizing textual entailment? / 39 axioms with high precision and low recall, while DIRT tends to yield opposite results (Szpektor et al. 2007). 6 Implementation and Evaluation The framework presented before has been implemented in a complete RTE system known as Nutcracker. The system (including source code) is distributed as part of the C&C tools (Clark and Curran 2004). A description of the most important components of this complex system follows below. The Nutcracker system has a traditional pipeline architecture of components, starting with a tokenizer, POS tagger, lemmatizer (Minnen et al. 2001) and named entity recognizer. This is followed by syntactic and semantic parsing. The meaning representations are produced by the semant"
2020.conll-shared.1,W13-2322,0,0.190653,"owards verbal senses, such that AMR graphs often appear to ‘abstract’ furthest from the surface signal. Since the first general release of an AMR graph bank in 2014, the framework has provided a popular target for data-driven meaning representation parsing and has been the subject of two consecutive tasks at SemEval 2016 and 2017 (May, 2016; May and Priyadarshi, 2017). The AMR example graph in Figure 4 has a topopossible-01 polarity ARG1 apply-02 ARG1 technique almost ARG2 crop (ARG1)-of resemble-01 Abstract Meaning Representation The shared task includes Abstract Meaning Representation (AMR; Banarescu et al., 2013), which in the MRP hierarchy of different formal types of semantic graphs (see §2 above) is simply unanchored, i.e. represents Flavor (2). The AMR framework is independent of particular approaches to derivation and compositionality and, accordingly, does not make explicit how elements of the graph correspond to the surface utterance. Although most AMR parsing research presupposes a pre-processing step that mod (domain) mod (domain) other (ARG1)-of exemplify-01 ARG0 and op1 cotton soybean op2 op3 rice op4 et-cetera Figure 4: Abstract Meaning Representation (AMR) for the running example A simila"
2020.conll-shared.1,W15-0128,1,0.796384,"des). Conversely, the two nodes associated with similar indicate lexical decomposition as a comparative predicate, where the second argument of the comp relation (the ‘point of reference’) remains unexpressed in Example (1). Elementary Dependency Structures The EDS graphs (Oepen and Lønning, 2006) originally derive from the underspecified logical forms computed by the English Resource Grammar (Flickinger et al., 2017; Copestake et al., 2005). These logical forms are not in and of themselves semantic graphs (in the sense of §2 above) and are often refered to as English Resource Semantics (ERS; Bender et al., 2015).3 Elementary Dependency Structures (EDS; Oepen and Lønning, 2006) encode English Resource Semantics in a variablefree semantic dependency graph—not limited to bi-lexical dependencies—where graph nodes correspond to logical predications and edges to labeled argument positions. The EDS conversion from underspecified logical forms to directed graphs discards partial information on semantic scope from the full ERS, which makes these graphs abstractly— if not linguistically—similar to Abstract Meaning Representation (see below). Nodes in EDS are in principle independent of surface lexical units, b"
2020.conll-shared.1,D16-1134,1,0.827452,"(UCCA; Abend and Rappoport, 2013) is based on cognitive linguistic and typological theories, primarily Basic Linguistic Theory (Dixon, 2010/2012). The shared task targets the UCCA foundational layer, which focuses on argument structure phenomena (where predicates may be verbal, nominal, adjectival, or otherwise). This coarse-grained level of semantics has been shown to be preserved well across translations (Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018c), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). the deep object of apply can be argued to not have a semantic contribution of their own. The ADDR argument relation to the apply predicate has been recursively propagated to both elements of the apposition and to all members of the coordinate structure. Accordingly, edge labels in PTG are not always functional, in the sense of allowing multiple outgoing edges from one node with the same label. In FGD, role labels (called functors) ACT(or), PAT(ient), ADDR(essee), ORIG(in), and EFF(ect) indicate ‘participant’ positions in an underlying valency fr"
2020.conll-shared.1,W13-0101,1,0.816811,"imilar 〈2:9〉 sempos adj.denot ACT #Benef sempos x EXT almost 〈23:29〉 sempos adv.denot.grad.neg coref.gram #Gen sempos x RSTR other 〈53:58〉 sempos adj.denot Figure 2: Semantic dependency graphs for the running example A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice: Prague Tectogrammatical Graphs (PTG). In addition to node properties, visualized similarly to the EDS in Figure 1, boolean edge attributes are abbreviated below edge labels, for true values. Universal Conceptual Cognitive Annotation Universal Cognitive Conceptual Annotation (UCCA; Abend and Rappoport, 2013) is based on cognitive linguistic and typological theories, primarily Basic Linguistic Theory (Dixon, 2010/2012). The shared task targets the UCCA foundational layer, which focuses on argument structure phenomena (where predicates may be verbal, nominal, adjectival, or otherwise). This coarse-grained level of semantics has been shown to be preserved well across translations (Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018c), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al.,"
2020.conll-shared.1,E17-2039,1,0.896998,"Missing"
2020.conll-shared.1,2020.emnlp-main.195,0,0.0866269,"with graph encoding (Cai and Lam, 2019, 2020). Xu et al. (2020a) improved sequence-to-sequence parsing for AMR by using pre-trained encoders, reaching similar performance to Cai and Lam (2020). Astudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR parser similar to the approach of Zhang et al. (2019b), using cross-lingual transfer learning, outperforming the transition-based cross-lingual AMR parser of Damonte and Cohen (2018) on German, Spanish, Italian, and Chinese. Reflections and Outlook The MRP series of shared tasks has contributed to general availability of accurate data-driven parsers for a broad range of different frameworks, with performance levels ranging between 0.76 MRP F1 (English UCCA) and 0.94 F1 (English EDS). Parsing accuracies in the cross-lingual track present comparable levels of performance, despite"
2020.conll-shared.1,2020.conll-shared.2,1,0.940067,"Structure (DRS), the meaning representations at the core of Discourse Representation Theory (DRT; Kamp and Reyle, 1993; Van der Sandt, 1992; Asher, 1993). DRSs can model many challenging semantic phenomena including quantifiers, negation, scope, pronoun resolution, presupposition accommodation, and discourse structure. Moreover, they are directly translatable into first-order logic formulas to account for logical inference. DRG used in the shared task represents a type of graph encoding of DRS that makes the graphs structurally as close as possible to the structures found in other frameworks; Abzianidze et al. (2020) provide more details on the design choices in the DRG encoding. The source DRS annotations are taken from data release 3.0.0 of the Parallel Meaning Bank (PMB; Abzianidze et al., 2017; Bos et al., 2017).6 Although the annotations in the PMB are compositionally derived from lexical semantics, anchoring information is not explicit in its DRSs; thus, (like AMR) the DRG framework formally instantiates Flavor (2) of meaning representations. The DRG of the running example is given in Figure 5. The concepts (vissualized as oval shapes) are represented by WordNet 3.0 senses and semantic roles (in dia"
2020.conll-shared.1,W19-1201,1,0.906261,"Missing"
2020.conll-shared.1,2020.lrec-1.234,1,0.763502,"s (Abzianidze et al., 2020). However, several semantic parsers exist for DRS, which employ different encodings. Liu et al. (2018) used a DRG format that dominantly labels edges compared to nodes. van Noord et al. (2018) process DRSs in a clausal form, sets of triples and quadruples. The latter format is more common among DRS parsers, as it was officially used by the shared task on DRS parsing (Abzianidze et al., 2019). The shared task gave rise to several DRS parsers: Evang (2019); Liu et al. (2019); van Noord (2019); 16 both quantitative contrastive studies (e.g. the ‘postmortem’ analysis by Buljan et al. (2020), which observes that top-performing MRP 2019 parsers have complementary strengths and weaknesses) but also more linguistic, qualitative comparison. General availability of parallel gold-standard annotations over the same text samples—drawing from the WSJ and LPPS corpora—enables side-by-side comparison of linguistic design choices in the different frameworks. This is an area of investigation that we hope will see increased interest in the aftermath of the MRP task series, to go well beyond the impressionistic observations from §3 and ideally lead to contrastive refinement across linguistic sc"
2020.conll-shared.1,2020.conll-shared.7,1,0.75652,"Missing"
2020.conll-shared.1,D19-1393,0,0.0309981,"nt alignment parser using stochastic softmax. Lindemann et al. (2019) trained a composition-based parser on five frameworks including AMR and EDS, using the Apply–Modify algebra, on which the third-ranked Saarland submission to MRP 2019 was based (Donatelli et al., 2019). They employed multi-task training with all tackled semantic frameworks and UD, establishing the state of the art on all graph banks but AMR 2017. Since then, a new state-of-the-art has been established for English AMR, using sequenceto-sequence transduction (Zhang et al., 2019a,b) and iterative inference with graph encoding (Cai and Lam, 2019, 2020). Xu et al. (2020a) improved sequence-to-sequence parsing for AMR by using pre-trained encoders, reaching similar performance to Cai and Lam (2020). Astudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR"
2020.conll-shared.1,2020.findings-emnlp.89,0,0.016475,"ebra, on which the third-ranked Saarland submission to MRP 2019 was based (Donatelli et al., 2019). They employed multi-task training with all tackled semantic frameworks and UD, establishing the state of the art on all graph banks but AMR 2017. Since then, a new state-of-the-art has been established for English AMR, using sequenceto-sequence transduction (Zhang et al., 2019a,b) and iterative inference with graph encoding (Cai and Lam, 2019, 2020). Xu et al. (2020a) improved sequence-to-sequence parsing for AMR by using pre-trained encoders, reaching similar performance to Cai and Lam (2020). Astudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR parser similar to the approach of Zhang et al. (2019b), using cross-lingual transfer learning, outperforming the transition-based cross-lingual AMR parser of Damonte and Cohen (20"
2020.conll-shared.1,2020.acl-main.119,0,0.179058,"s similar to the use of Factored Concept Labels in Wang and Xue (2017). Another innovation of the PERIN system is that it is trained with a permutation-invariant loss function that returns the same value independently of how the nodes in the graph are ordered. This captures the unordered nature of nodes in (most of the MRP 2020) meaning representation graphs and prevents situations in which the model is penalized for generating the correct nodes in an order that is different from that in the training data. The HIT-SCIR and JBNU systems adopt the iterative inference framework first proposed by Cai and Lam (2020) for Flavor (2) meaning representation graphs that do not enforce strict correspondences between tokens in the input sentence and the concepts in meaning representation graphs. The iterative inference framework is also based on an encoder–decoder architecture. The encoder takes the sentence as input and computes contextualized token embeddings that are used as text memory by a decoder that iteratively predicts the next node given the text memory and a predicted parent node in the partially constructed graph memory at the previous time step, and then identifies the parent node for the newly pre"
2020.conll-shared.1,2020.conll-shared.6,0,0.0921787,"guage per framework. The task received submissions from eight teams, of which two do not participate in the official ranking because they arrived after the closing deadline or made use of additional training data. All technical information regarding the task, including system submissions, official results, and links to supporting resources and software are available from the task web site at: http://mrp.nlpl.eu 1 Background and Motivation The 2020 Conference on Computational Language Learning (CoNLL) hosts a shared task (or ‘system bake-off’) on Cross-Framework Meaning Representation Parsing (MRP 2020), which is a revised and extended re-run of a similar CoNLL shared task in the preceding year. The goal of these tasks is to advance data-driven parsing into graph-structured representations of sentence meaning. For the first time, the MRP task series combines formally and linguistically different approaches to meaning rep1 To reduce the threshold to participation, two of the target frameworks represented in MRP 2019 are not in focus this year, viz. the purely bi-lexical DELPH-IN MRS Bi-Lexical Dependencies and Prague Semantic Dependencies (PSD). These graphs largely overlap with the correspon"
2020.conll-shared.1,P13-2131,0,0.187673,"Missing"
2020.conll-shared.1,W11-2927,1,0.757531,"was not formally enforced. 5 Evaluation Following the previous edition of the shared task, the official MRP metric for the task is the microaverage F1 score across frameworks over all tuple types that encode ‘atoms’ of information in MRP graphs. The cross-framework metric uniformly evaluates graphs of different flavors, regardless of a specific framework exhibiting (a) labeled or unlabeled nodes or edges, (b) nodes with or without anchors, and (c) nodes and edges with optional properties and attributes, respectively (see Table 4). The MRP metric generalizes earlier frameworkspecific metrics (Dridan and Oepen, 2011; Cai and Knight, 2013; Hershcovich et al., 2019a) in terms of decomposing each graph into sets of typed tuples, as indicated in Figure 6. To quantify graph similarity in terms of tuple overlap, a correspondence relation between the nodes of the goldstandard and system graphs must be determined. Adapting a search procedure for the NP-hard maximum common edge subgraph (MCES) isomorphism problem, the MRP scorer will search for the node-to-node correspondence that maximizes the intersection of tuples between two graphs, where node identifiers (m and n in Figure 6) act like variables that can be e"
2020.conll-shared.1,K19-2007,0,0.0722366,"ground and Motivation The 2020 Conference on Computational Language Learning (CoNLL) hosts a shared task (or ‘system bake-off’) on Cross-Framework Meaning Representation Parsing (MRP 2020), which is a revised and extended re-run of a similar CoNLL shared task in the preceding year. The goal of these tasks is to advance data-driven parsing into graph-structured representations of sentence meaning. For the first time, the MRP task series combines formally and linguistically different approaches to meaning rep1 To reduce the threshold to participation, two of the target frameworks represented in MRP 2019 are not in focus this year, viz. the purely bi-lexical DELPH-IN MRS Bi-Lexical Dependencies and Prague Semantic Dependencies (PSD). These graphs largely overlap with the corresponding (but richer) frameworks in 2020, EDS and PTG, respectively, and the original bi-lexical semantic dependency graphs remain independently available (Oepen et al., 2015). 1 Proceedings of the CoNLL 2020 Shared Task: Cross-Framework Meaning Representation Parsing, pages 1–22 c Online, Nov. 19-20, 2020. 2020 Association for Computational Linguistics (a) a unifying formal model over different semantic graph banks (§2)"
2020.conll-shared.1,1997.iwpt-1.10,0,0.772333,"Missing"
2020.conll-shared.1,W19-1202,0,0.0200921,"ntation format for DRS that was specially designed for MRP 2020 to make it structurally as close as possible to other frameworks (Abzianidze et al., 2020). However, several semantic parsers exist for DRS, which employ different encodings. Liu et al. (2018) used a DRG format that dominantly labels edges compared to nodes. van Noord et al. (2018) process DRSs in a clausal form, sets of triples and quadruples. The latter format is more common among DRS parsers, as it was officially used by the shared task on DRS parsing (Abzianidze et al., 2019). The shared task gave rise to several DRS parsers: Evang (2019); Liu et al. (2019); van Noord (2019); 16 both quantitative contrastive studies (e.g. the ‘postmortem’ analysis by Buljan et al. (2020), which observes that top-performing MRP 2019 parsers have complementary strengths and weaknesses) but also more linguistic, qualitative comparison. General availability of parallel gold-standard annotations over the same text samples—drawing from the WSJ and LPPS corpora—enables side-by-side comparison of linguistic design choices in the different frameworks. This is an area of investigation that we hope will see increased interest in the aftermath of the MRP"
2020.conll-shared.1,K19-2016,0,0.0872997,"UCCA, and AMR. This allows a comparison on nearly equal grounds: as Table 9 shows, in terms of LPPS F1 , the state-of-the-art has substantially improved for EDS and AMR parsing, but stayed the same for UCCA. However, as mentioned in §6, remote edge detection for UCCA improved substantially, though it carries only a small weight in terms of overall scores due to the scarcity of remote edges. For EDS, the strongest results were obtained in the MRP 2019 official competition by SUDA– Alibaba (Zhang et al., 2019c). However, in the post-evaluation stage, they were outperformed by the Peking system (Chen et al., 2019). Both used factorization-based parsing with pre-trained contextualized language model embeddings (which has consistently proved to be very effective for other frameworks too). These parsers even approached the performance of the carefully designed grammarbased ERG parser (Oepen and Flickinger, 2019). English PTG has not been comprehensively addressed by parsers prior to MRP 2020, but a bilexical framework called PSD is a subset of PTG. It was included in the SDP shared tasks (Oepen et al., 2014, 2015) as well as in MRP 2019, and has been addressed by numerous parsers since (Kurita and Søgaard"
2020.conll-shared.1,D19-1278,0,0.0153051,"Donatelli et al., 2019), respectively; in MRP 2020 the Hitachi system (Ozaki et al., 2020) was at the top for all three frameworks, sharing the UCCA first ´ rank with UFAL (Samuel and Straka, 2020). UCCA parsing has been dominated by transitionbased methods (Hershcovich et al., 2017, 2018; Che et al., 2019). However, both English and German UCCA parsing featured in a SemEval shared task (Hershcovich et al., 2019b), where the best system, a composition-based parser (Jiang et al., 2019), treated the task as constituency tree parsing with the recovery of remote edges as a postprocess15 ing task. Fancellu et al. (2019), among which the best results (F1 = 0.85) were achieved by the word-level sequence-to-sequence model with Tranformer (Liu et al., 2019). Note that the DRS shared task used F1 calculated based on the DRS clausal forms, which is not comparable to MRP F1 over DRGs. Similarly to English DRG, German DRG has not been used for semantic parsing prior to the shared task due to the new DRG format. Moreover, semantic parsing with German DRG is novel in the sense that its DRS counterpart is also new. In German DRG, concepts are grounded in English WordNet 3.0 (Fellbaum, 2012) senses assuming that synsets"
2020.conll-shared.1,N18-2020,1,0.822488,"ed on cognitive linguistic and typological theories, primarily Basic Linguistic Theory (Dixon, 2010/2012). The shared task targets the UCCA foundational layer, which focuses on argument structure phenomena (where predicates may be verbal, nominal, adjectival, or otherwise). This coarse-grained level of semantics has been shown to be preserved well across translations (Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018c), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). the deep object of apply can be argued to not have a semantic contribution of their own. The ADDR argument relation to the apply predicate has been recursively propagated to both elements of the apposition and to all members of the coordinate structure. Accordingly, edge labels in PTG are not always functional, in the sense of allowing multiple outgoing edges from one node with the same label. In FGD, role labels (called functors) ACT(or), PAT(ient), ADDR(essee), ORIG(in), and EFF(ect) indicate ‘participant’ positions in an underlying valency frame and, thus, correspond more closely to the n"
2020.conll-shared.1,P19-4007,0,0.0514516,"Missing"
2020.conll-shared.1,2020.acl-main.629,0,0.125349,"Missing"
2020.conll-shared.1,N18-1104,0,0.0512423,"tudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR parser similar to the approach of Zhang et al. (2019b), using cross-lingual transfer learning, outperforming the transition-based cross-lingual AMR parser of Damonte and Cohen (2018) on German, Spanish, Italian, and Chinese. Reflections and Outlook The MRP series of shared tasks has contributed to general availability of accurate data-driven parsers for a broad range of different frameworks, with performance levels ranging between 0.76 MRP F1 (English UCCA) and 0.94 F1 (English EDS). Parsing accuracies in the cross-lingual track present comparable levels of performance, despite limited training data in the case of UCCA and DRG. Furthermore, the evaluation sets for most of the frameworks comprise different text types and subject matters—offering some hope of robustness to"
2020.conll-shared.1,N18-1000,0,0.197513,"Missing"
2020.conll-shared.1,K19-2006,0,0.10434,"been included in the CoNLL 2009 Shared Task on Semantic Role Labeling (Hajiˇc et al., 2009), but the differences in task design are and conversion make empirical comparison impossible. AMR P R F P R F P R F .92 .97 .93 .97 .93 .97 .84 .86 .82 .80 .83 .83 .74 .78 .72 .79 .73 .79 Table 9: Per-framework cross-task comparison of top MRP metric scores on LPPS between the 2019 and 2020 editions of the MRP task, on the three frameworks represented in both year, for English. The top systems in MRP 2019 for EDS, UCCA, and AMR were Peking (Chen et al., 2019), HIT-SCIR (Che et al., 2019), and Saarland (Donatelli et al., 2019), respectively; in MRP 2020 the Hitachi system (Ozaki et al., 2020) was at the top for all three frameworks, sharing the UCCA first ´ rank with UFAL (Samuel and Straka, 2020). UCCA parsing has been dominated by transitionbased methods (Hershcovich et al., 2017, 2018; Che et al., 2019). However, both English and German UCCA parsing featured in a SemEval shared task (Hershcovich et al., 2019b), where the best system, a composition-based parser (Jiang et al., 2019), treated the task as constituency tree parsing with the recovery of remote edges as a postprocess15 ing task. Fancellu et al. (2019),"
2020.conll-shared.1,S19-2002,0,0.0495413,"Missing"
2020.conll-shared.1,hajic-etal-2012-announcing,0,0.357504,"Missing"
2020.conll-shared.1,kingsbury-palmer-2002-treebank,0,0.452155,"o distinguish different types of the underlying DRS elements. logy broadly comparable to EDS, with some notable differences. Similar to the UCCA example graph (and unlike EDS), the AMR representation of the coordinate structure is flat. Although most lemmas are linked to derivationally related forms in the sense lexicon, this is not universal, as seen by the nodes corresponding to similar and such as, which are labeled as resemble-01 and exemplify-01, respectively. These sense distinctions (primarily for verbal predicates) are grounded in the inventory of predicates from the PropBank lexicon (Kingsbury and Palmer, 2002; Hovy et al., 2006). Role labels in AMR encode semantic argument positions, with the particular roles defined according to each PropBank sense, though the counting in AMR is zero-based such that the ARG1 and ARG2 roles in Figure 4 often correspond to ARG2 and ARG3, respectively, in the EDS of Figure 1. PropBank distinguishes such numbered arguments from non-core roles labeled from a general semantic inventory, such as frequency, duration, or domain. Figure 4 also shows the use of inverted edges in AMR, for example ARG1-of and mod. These serve to allow annotators (and in principle also parsing"
2020.conll-shared.1,J16-4009,1,0.877647,"d (d) increased crossfertilization of parsing approaches (§7). 2 tute ordered graphs. A natural way to visualize a bi-lexical dependency graph is to draw its edges as semicircles in the halfplane above the sentence. An ordered graph is called noncrossing if in such a drawing, the semicircles intersect only at their endpoints (this property is a natural generalization of projectivity as it is known from dependency trees). A natural generalization of the noncrossing property, where one is allowed to also use the halfplane below the sentence for drawing edges is a property called pagenumber two. Kuhlmann and Oepen (2016) provide additional definitions and a quantitative summary of various formal graph properties across frameworks. Definitions: Graphs and Flavors Reflecting different traditions and communities, there is wide variation in how individual meaning representation frameworks think (and talk) about semantic graphs, down to the level of visual conventions used in rendering graph structures. Increased terminological uniformity and guidance in how to navigate this rich and diverse landscape are among the desirable side-effects of the MRP task series. The following paragraphs provide semi-formal definiti"
2020.conll-shared.1,P17-1104,1,0.878575,".78 .72 .79 .73 .79 Table 9: Per-framework cross-task comparison of top MRP metric scores on LPPS between the 2019 and 2020 editions of the MRP task, on the three frameworks represented in both year, for English. The top systems in MRP 2019 for EDS, UCCA, and AMR were Peking (Chen et al., 2019), HIT-SCIR (Che et al., 2019), and Saarland (Donatelli et al., 2019), respectively; in MRP 2020 the Hitachi system (Ozaki et al., 2020) was at the top for all three frameworks, sharing the UCCA first ´ rank with UFAL (Samuel and Straka, 2020). UCCA parsing has been dominated by transitionbased methods (Hershcovich et al., 2017, 2018; Che et al., 2019). However, both English and German UCCA parsing featured in a SemEval shared task (Hershcovich et al., 2019b), where the best system, a composition-based parser (Jiang et al., 2019), treated the task as constituency tree parsing with the recovery of remote edges as a postprocess15 ing task. Fancellu et al. (2019), among which the best results (F1 = 0.85) were achieved by the word-level sequence-to-sequence model with Tranformer (Liu et al., 2019). Note that the DRS shared task used F1 calculated based on the DRS clausal forms, which is not comparable to MRP F1 over DRG"
2020.conll-shared.1,P19-1232,0,0.0175512,"Chen et al., 2019). Both used factorization-based parsing with pre-trained contextualized language model embeddings (which has consistently proved to be very effective for other frameworks too). These parsers even approached the performance of the carefully designed grammarbased ERG parser (Oepen and Flickinger, 2019). English PTG has not been comprehensively addressed by parsers prior to MRP 2020, but a bilexical framework called PSD is a subset of PTG. It was included in the SDP shared tasks (Oepen et al., 2014, 2015) as well as in MRP 2019, and has been addressed by numerous parsers since (Kurita and Søgaard, 2019; Kurtz et al., 2019; Jia et al., 2020, among others). Wang et al. (2019) established the state of the art in supervised PSD using a second-order factorization-based parser, and Fern´andez-Gonz´alez and G´omez-Rodr´ıguez (2020) matched it using a stack-pointer parser. On the State of the Art MRP 2019 (Oepen et al., 2019) yielded parsers for five frameworks in a uniform format, of which EDS, UCCA, and AMR are represented in MRP 2020 again. Submissions included transition-, factorization-, and composition-based systems, and gold-standard target structures in 2019 were solely for English. Compara"
2020.conll-shared.1,P18-1035,1,0.856893,"e invited to develop parsing systems that support five distinct semantic graph frameworks in four languages (see §3 below)— all encoding core predicate–argument structure, among other things—in the same implementation. Ideally, these parsers predict sentence-level meaning representations in all frameworks in parallel. Architectures utilizing complementary knowledge sources (e.g. via parameter sharing) were encouraged, though not required. Learning from multiple flavors of meaning representation in tandem has hardly been explored (with notable exceptions, e.g. the parsers of Peng et al., 2017; Hershcovich et al., 2018; Stanovsky and Dagan, 2018; or Lindemann et al., 2019). The task design aims to reduce frameworkspecific ‘balkanization’ in the field of meaning representation parsing. Its contributions include The 2020 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks and languages. Extending a similar setup from the previous year, five distinct approaches to the representation of sentence meaning in the form of directed graphs were represented in the English training and evaluation data for the task, packaged in a"
2020.conll-shared.1,W19-6202,0,0.0118155,"sed factorization-based parsing with pre-trained contextualized language model embeddings (which has consistently proved to be very effective for other frameworks too). These parsers even approached the performance of the carefully designed grammarbased ERG parser (Oepen and Flickinger, 2019). English PTG has not been comprehensively addressed by parsers prior to MRP 2020, but a bilexical framework called PSD is a subset of PTG. It was included in the SDP shared tasks (Oepen et al., 2014, 2015) as well as in MRP 2019, and has been addressed by numerous parsers since (Kurita and Søgaard, 2019; Kurtz et al., 2019; Jia et al., 2020, among others). Wang et al. (2019) established the state of the art in supervised PSD using a second-order factorization-based parser, and Fern´andez-Gonz´alez and G´omez-Rodr´ıguez (2020) matched it using a stack-pointer parser. On the State of the Art MRP 2019 (Oepen et al., 2019) yielded parsers for five frameworks in a uniform format, of which EDS, UCCA, and AMR are represented in MRP 2020 again. Submissions included transition-, factorization-, and composition-based systems, and gold-standard target structures in 2019 were solely for English. Comparability is limited by"
2020.conll-shared.1,S19-2001,1,0.83448,"Missing"
2020.conll-shared.1,2020.findings-emnlp.288,0,0.0119332,"with all tackled semantic frameworks and UD, establishing the state of the art on all graph banks but AMR 2017. Since then, a new state-of-the-art has been established for English AMR, using sequenceto-sequence transduction (Zhang et al., 2019a,b) and iterative inference with graph encoding (Cai and Lam, 2019, 2020). Xu et al. (2020a) improved sequence-to-sequence parsing for AMR by using pre-trained encoders, reaching similar performance to Cai and Lam (2020). Astudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR parser similar to the approach of Zhang et al. (2019b), using cross-lingual transfer learning, outperforming the transition-based cross-lingual AMR parser of Damonte and Cohen (2018) on German, Spanish, Italian, and Chinese. Reflections and Outlook The MRP series of shared tasks has contributed to general"
2020.conll-shared.1,P19-1450,0,0.182109,"istinct semantic graph frameworks in four languages (see §3 below)— all encoding core predicate–argument structure, among other things—in the same implementation. Ideally, these parsers predict sentence-level meaning representations in all frameworks in parallel. Architectures utilizing complementary knowledge sources (e.g. via parameter sharing) were encouraged, though not required. Learning from multiple flavors of meaning representation in tandem has hardly been explored (with notable exceptions, e.g. the parsers of Peng et al., 2017; Hershcovich et al., 2018; Stanovsky and Dagan, 2018; or Lindemann et al., 2019). The task design aims to reduce frameworkspecific ‘balkanization’ in the field of meaning representation parsing. Its contributions include The 2020 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks and languages. Extending a similar setup from the previous year, five distinct approaches to the representation of sentence meaning in the form of directed graphs were represented in the English training and evaluation data for the task, packaged in a uniform graph abstraction and serialization; for four"
2020.conll-shared.1,P18-1040,0,0.0323045,", summarization, or text generation. Maybe equally importantly, the MRP task design capitalizes on uniformity of representations and evaluation, enabling resource creators and parser developers to more closely (inter)relate representations and parsing approaches across a diverse range of semantic graph frameworks. This facilitates DRG is a novel graph representation format for DRS that was specially designed for MRP 2020 to make it structurally as close as possible to other frameworks (Abzianidze et al., 2020). However, several semantic parsers exist for DRS, which employ different encodings. Liu et al. (2018) used a DRG format that dominantly labels edges compared to nodes. van Noord et al. (2018) process DRSs in a clausal form, sets of triples and quadruples. The latter format is more common among DRS parsers, as it was officially used by the shared task on DRS parsing (Abzianidze et al., 2019). The shared task gave rise to several DRS parsers: Evang (2019); Liu et al. (2019); van Noord (2019); 16 both quantitative contrastive studies (e.g. the ‘postmortem’ analysis by Buljan et al. (2020), which observes that top-performing MRP 2019 parsers have complementary strengths and weaknesses) but also m"
2020.conll-shared.1,W19-1203,0,0.0982514,"he UCCA first ´ rank with UFAL (Samuel and Straka, 2020). UCCA parsing has been dominated by transitionbased methods (Hershcovich et al., 2017, 2018; Che et al., 2019). However, both English and German UCCA parsing featured in a SemEval shared task (Hershcovich et al., 2019b), where the best system, a composition-based parser (Jiang et al., 2019), treated the task as constituency tree parsing with the recovery of remote edges as a postprocess15 ing task. Fancellu et al. (2019), among which the best results (F1 = 0.85) were achieved by the word-level sequence-to-sequence model with Tranformer (Liu et al., 2019). Note that the DRS shared task used F1 calculated based on the DRS clausal forms, which is not comparable to MRP F1 over DRGs. Similarly to English DRG, German DRG has not been used for semantic parsing prior to the shared task due to the new DRG format. Moreover, semantic parsing with German DRG is novel in the sense that its DRS counterpart is also new. In German DRG, concepts are grounded in English WordNet 3.0 (Fellbaum, 2012) senses assuming that synsets are language-neutral. The mismatch between German tokens and English lemmas of senses must be expected to add additional complexity to"
2020.conll-shared.1,2020.acl-main.607,0,0.0482936,"Missing"
2020.conll-shared.1,K19-2001,1,0.563644,"Missing"
2020.conll-shared.1,P18-1037,0,0.0412388,"tokens and English lemmas of senses must be expected to add additional complexity to German DRG parsing. Direct comparison to non-MRP results is impossible: we are using a new version of AMRbank. Gold-standard tokenization is not provided for any of the frameworks. We use the MRP scorer. However, general trends appear consistent with recent developments. Pretrained embeddings and crosslingual transfer help; but multi-task learning less so. There is yet progress to be made in sharing information between parsers for different frameworks and making better use of their overlap. Prior to MRP 2019, Lyu and Titov (2018) parsed AMR using a joint probabilistic model with latent alignments, avoiding cascading errors due to alignment inaccuracies and outperforming previous approaches. Lyu et al. (2020) recently improved the latent alignment parser using stochastic softmax. Lindemann et al. (2019) trained a composition-based parser on five frameworks including AMR and EDS, using the Apply–Modify algebra, on which the third-ranked Saarland submission to MRP 2019 was based (Donatelli et al., 2019). They employed multi-task training with all tackled semantic frameworks and UD, establishing the state of the art on al"
2020.conll-shared.1,K19-2003,1,0.845577,"on marks in the left or right periphery of a normalized anchor. Assuming the string Oh no! as a hypothetical parser input, the following anchorings will all be considered equivalent: {h0 : 6i}, {h0 : 2i, h3 : 6i}, {h0 : 1i, h1 : 6i}, and {h0 : 5i}. 6 Six teams submitted parser outputs to the shared task within the official evaluation period. In addition, we received two submissions after the submission deadline, which we mark as ‘unofficial’. We further include results from an additional ‘reference’ system by one of the task co-organizers, namely EDS outputs from the grammar-based ERG parser (Oepen and Flickinger, 2019). Table 5 presents an overview of the participating systems and the tracks and frameworks they submitted results for. All official systems submitted results for the cross-framework track (across all frameworks), and additionally five of them submitted results to the cross-lingual track as well (where TJU-BLCU did not submit UCCA parser outputs in the cross-lingual track). We note that the shared task explicitly allowed partial submissions, in order to lower the bar for participation (which is no doubt substantial). Two of the teams—ISCAS and TJUBLCU—declined the invitation to submit a system d"
2020.conll-shared.1,P14-5010,0,0.0027367,"ces of ‘raw’ sentence strings and (b) in pre-tokenized, partof-speech–tagged, lemmatized, and syntactically parsed form. For the latter, premium-quality morpho-syntactic dependency analyses were provided to participants, called the MRP 2020 companion parses. These parses were obtained using a prerelease of the ‘future’ UDPipe architecture (Straka, 2018; Straka and Strakov´a, 2020), trained on available gold-standard UD 2.x treebanks, for English augmented with conversions from PTB-style annotations in the WSJ and OntoNotes corpora (Hovy et al., 2006), using the UD-style CoreNLP 4.0 tokenizer (Manning et al., 2014) and jack-knifing where appropriate (to avoid overlap with the texts underlying the MRP semantic graphs). Table 4: Different tuple types per framework. on-line CodaLab infrastructure. Teams were allowed to make repeated submissions, but only the most recent successful upload to CodaLab within the evaluation period was considered for the official, primary ranking of submissions. Task participants were encouraged to process all inputs using the same general parsing system, but—owing to inevitable fuzziness about what constitutes ‘one’ parser—this constraint was not formally enforced. 5 Evaluatio"
2020.conll-shared.1,S15-2153,1,0.842966,"Missing"
2020.conll-shared.1,J93-2004,0,0.0699436,", as fully lexically anchored and wholly unanchored, respectively, leading to the categorization of mixed forms of anchoring as Flavor (1), and allow for the presence of ordered graphs, in principle at least, at all levels of the hierarchy.2 Meaning Representation Frameworks The shared task combines five distinct frameworks for graph-based meaning representation, each with its specific formal and linguistic assumptions. This section reviews the frameworks and presents English example graphs for sentence #20209013 from the venerable Wall Street Journal (WSJ) Corpus from the Penn Treebank (PTB; Marcus et al., 1993): (1) A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice. The example exhibits some interesting linguistic complexity, including what is called a tough adjective (impossible), a scopal adverb (almost), a tripartite coordinate structure, and apposition. The example graphs in Figures 1 through 4 are prewhere unanchored nodes for unexpressed material beyond the surface string can be postulated (Schuster and Manning, 2016). Whether or not these nodes occupy a well-defined position in the otherwise total order of basic UD nodes remains an open questi"
2020.conll-shared.1,S16-1166,0,0.0329712,"context of the shared task. ‘aligns’ graph nodes with (possibly discontinuous) sets of tokens in the underlying input, this anchoring is not part of the meaning representation proper. At the same time, AMR frequently invokes lexical decomposition and normalization towards verbal senses, such that AMR graphs often appear to ‘abstract’ furthest from the surface signal. Since the first general release of an AMR graph bank in 2014, the framework has provided a popular target for data-driven meaning representation parsing and has been the subject of two consecutive tasks at SemEval 2016 and 2017 (May, 2016; May and Priyadarshi, 2017). The AMR example graph in Figure 4 has a topopossible-01 polarity ARG1 apply-02 ARG1 technique almost ARG2 crop (ARG1)-of resemble-01 Abstract Meaning Representation The shared task includes Abstract Meaning Representation (AMR; Banarescu et al., 2013), which in the MRP hierarchy of different formal types of semantic graphs (see §2 above) is simply unanchored, i.e. represents Flavor (2). The AMR framework is independent of particular approaches to derivation and compositionality and, accordingly, does not make explicit how elements of the graph correspond to the su"
2020.conll-shared.1,S14-2008,1,0.896688,"Missing"
2020.conll-shared.1,S17-2090,0,0.0310415,"the shared task. ‘aligns’ graph nodes with (possibly discontinuous) sets of tokens in the underlying input, this anchoring is not part of the meaning representation proper. At the same time, AMR frequently invokes lexical decomposition and normalization towards verbal senses, such that AMR graphs often appear to ‘abstract’ furthest from the surface signal. Since the first general release of an AMR graph bank in 2014, the framework has provided a popular target for data-driven meaning representation parsing and has been the subject of two consecutive tasks at SemEval 2016 and 2017 (May, 2016; May and Priyadarshi, 2017). The AMR example graph in Figure 4 has a topopossible-01 polarity ARG1 apply-02 ARG1 technique almost ARG2 crop (ARG1)-of resemble-01 Abstract Meaning Representation The shared task includes Abstract Meaning Representation (AMR; Banarescu et al., 2013), which in the MRP hierarchy of different formal types of semantic graphs (see §2 above) is simply unanchored, i.e. represents Flavor (2). The AMR framework is independent of particular approaches to derivation and compositionality and, accordingly, does not make explicit how elements of the graph correspond to the surface utterance. Although mo"
2020.conll-shared.1,2020.conll-shared.4,0,0.136232,"(Hajiˇc et al., 2009), but the differences in task design are and conversion make empirical comparison impossible. AMR P R F P R F P R F .92 .97 .93 .97 .93 .97 .84 .86 .82 .80 .83 .83 .74 .78 .72 .79 .73 .79 Table 9: Per-framework cross-task comparison of top MRP metric scores on LPPS between the 2019 and 2020 editions of the MRP task, on the three frameworks represented in both year, for English. The top systems in MRP 2019 for EDS, UCCA, and AMR were Peking (Chen et al., 2019), HIT-SCIR (Che et al., 2019), and Saarland (Donatelli et al., 2019), respectively; in MRP 2020 the Hitachi system (Ozaki et al., 2020) was at the top for all three frameworks, sharing the UCCA first ´ rank with UFAL (Samuel and Straka, 2020). UCCA parsing has been dominated by transitionbased methods (Hershcovich et al., 2017, 2018; Che et al., 2019). However, both English and German UCCA parsing featured in a SemEval shared task (Hershcovich et al., 2019b), where the best system, a composition-based parser (Jiang et al., 2019), treated the task as constituency tree parsing with the recovery of remote edges as a postprocess15 ing task. Fancellu et al. (2019), among which the best results (F1 = 0.85) were achieved by the word"
2020.conll-shared.1,2020.conll-shared.8,0,0.415434,"Missing"
2020.conll-shared.1,P17-1186,0,0.154444,"Missing"
2020.conll-shared.1,2020.lrec-1.497,1,0.867008,"Missing"
2020.conll-shared.1,W19-1204,0,0.0450391,"Missing"
2020.conll-shared.1,W16-6401,0,0.0666192,"Missing"
2020.conll-shared.1,Q18-1043,1,0.868093,"Missing"
2020.conll-shared.1,2020.conll-shared.5,0,0.310999,"impossible. AMR P R F P R F P R F .92 .97 .93 .97 .93 .97 .84 .86 .82 .80 .83 .83 .74 .78 .72 .79 .73 .79 Table 9: Per-framework cross-task comparison of top MRP metric scores on LPPS between the 2019 and 2020 editions of the MRP task, on the three frameworks represented in both year, for English. The top systems in MRP 2019 for EDS, UCCA, and AMR were Peking (Chen et al., 2019), HIT-SCIR (Che et al., 2019), and Saarland (Donatelli et al., 2019), respectively; in MRP 2020 the Hitachi system (Ozaki et al., 2020) was at the top for all three frameworks, sharing the UCCA first ´ rank with UFAL (Samuel and Straka, 2020). UCCA parsing has been dominated by transitionbased methods (Hershcovich et al., 2017, 2018; Che et al., 2019). However, both English and German UCCA parsing featured in a SemEval shared task (Hershcovich et al., 2019b), where the best system, a composition-based parser (Jiang et al., 2019), treated the task as constituency tree parsing with the recovery of remote edges as a postprocess15 ing task. Fancellu et al. (2019), among which the best results (F1 = 0.85) were achieved by the word-level sequence-to-sequence model with Tranformer (Liu et al., 2019). Note that the DRS shared task used F1"
2020.conll-shared.1,N18-2040,1,0.887249,"Missing"
2020.conll-shared.1,L16-1376,0,0.0817331,"Missing"
2020.conll-shared.1,D17-1129,1,0.843784,"contextualized token embeddings with XLM-R (Conneau et al., 2019) on the encoder side, and then on the decoder side, uses separate attention heads to predict the node labels, identify anchors for nodes, and predict edges between nodes, as well as edge labels. Because the label set for nodes is typically very large, rather than predicting the node labels directly, the PERIN system reduces the search space by predicting ‘relative rules’ that can be used to map surface token strings to node labels in meaning representation graphs, an idea that is similar to the use of Factored Concept Labels in Wang and Xue (2017). Another innovation of the PERIN system is that it is trained with a permutation-invariant loss function that returns the same value independently of how the nodes in the graph are ordered. This captures the unordered nature of nodes in (most of the MRP 2020) meaning representation graphs and prevents situations in which the model is penalized for generating the correct nodes in an order that is different from that in the training data. The HIT-SCIR and JBNU systems adopt the iterative inference framework first proposed by Cai and Lam (2020) for Flavor (2) meaning representation graphs that d"
2020.conll-shared.1,P19-1454,0,0.0201296,"extualized language model embeddings (which has consistently proved to be very effective for other frameworks too). These parsers even approached the performance of the carefully designed grammarbased ERG parser (Oepen and Flickinger, 2019). English PTG has not been comprehensively addressed by parsers prior to MRP 2020, but a bilexical framework called PSD is a subset of PTG. It was included in the SDP shared tasks (Oepen et al., 2014, 2015) as well as in MRP 2019, and has been addressed by numerous parsers since (Kurita and Søgaard, 2019; Kurtz et al., 2019; Jia et al., 2020, among others). Wang et al. (2019) established the state of the art in supervised PSD using a second-order factorization-based parser, and Fern´andez-Gonz´alez and G´omez-Rodr´ıguez (2020) matched it using a stack-pointer parser. On the State of the Art MRP 2019 (Oepen et al., 2019) yielded parsers for five frameworks in a uniform format, of which EDS, UCCA, and AMR are represented in MRP 2020 again. Submissions included transition-, factorization-, and composition-based systems, and gold-standard target structures in 2019 were solely for English. Comparability is limited by the fact that two of the 2020 frameworks (PTG and DR"
2020.conll-shared.1,D18-1263,0,0.0141923,"ng systems that support five distinct semantic graph frameworks in four languages (see §3 below)— all encoding core predicate–argument structure, among other things—in the same implementation. Ideally, these parsers predict sentence-level meaning representations in all frameworks in parallel. Architectures utilizing complementary knowledge sources (e.g. via parameter sharing) were encouraged, though not required. Learning from multiple flavors of meaning representation in tandem has hardly been explored (with notable exceptions, e.g. the parsers of Peng et al., 2017; Hershcovich et al., 2018; Stanovsky and Dagan, 2018; or Lindemann et al., 2019). The task design aims to reduce frameworkspecific ‘balkanization’ in the field of meaning representation parsing. Its contributions include The 2020 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks and languages. Extending a similar setup from the previous year, five distinct approaches to the representation of sentence meaning in the form of directed graphs were represented in the English training and evaluation data for the task, packaged in a uniform graph abstraction"
2020.conll-shared.1,2020.emnlp-main.196,0,0.450632,"stochastic softmax. Lindemann et al. (2019) trained a composition-based parser on five frameworks including AMR and EDS, using the Apply–Modify algebra, on which the third-ranked Saarland submission to MRP 2019 was based (Donatelli et al., 2019). They employed multi-task training with all tackled semantic frameworks and UD, establishing the state of the art on all graph banks but AMR 2017. Since then, a new state-of-the-art has been established for English AMR, using sequenceto-sequence transduction (Zhang et al., 2019a,b) and iterative inference with graph encoding (Cai and Lam, 2019, 2020). Xu et al. (2020a) improved sequence-to-sequence parsing for AMR by using pre-trained encoders, reaching similar performance to Cai and Lam (2020). Astudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR parser similar to the ap"
2020.conll-shared.1,2020.wmt-1.104,0,0.540185,"stochastic softmax. Lindemann et al. (2019) trained a composition-based parser on five frameworks including AMR and EDS, using the Apply–Modify algebra, on which the third-ranked Saarland submission to MRP 2019 was based (Donatelli et al., 2019). They employed multi-task training with all tackled semantic frameworks and UD, establishing the state of the art on all graph banks but AMR 2017. Since then, a new state-of-the-art has been established for English AMR, using sequenceto-sequence transduction (Zhang et al., 2019a,b) and iterative inference with graph encoding (Cai and Lam, 2019, 2020). Xu et al. (2020a) improved sequence-to-sequence parsing for AMR by using pre-trained encoders, reaching similar performance to Cai and Lam (2020). Astudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR parser similar to the ap"
2020.conll-shared.1,2020.lt4hala-1.20,0,0.0828616,"Missing"
2020.conll-shared.1,2020.conll-shared.3,1,0.64593,"Missing"
2020.conll-shared.1,W15-3502,1,0.865834,"arly to the EDS in Figure 1, boolean edge attributes are abbreviated below edge labels, for true values. Universal Conceptual Cognitive Annotation Universal Cognitive Conceptual Annotation (UCCA; Abend and Rappoport, 2013) is based on cognitive linguistic and typological theories, primarily Basic Linguistic Theory (Dixon, 2010/2012). The shared task targets the UCCA foundational layer, which focuses on argument structure phenomena (where predicates may be verbal, nominal, adjectival, or otherwise). This coarse-grained level of semantics has been shown to be preserved well across translations (Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018c), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). the deep object of apply can be argued to not have a semantic contribution of their own. The ADDR argument relation to the apply predicate has been recursively propagated to both elements of the apposition and to all members of the coordinate structure. Accordingly, edge labels in PTG are not always functional, in the sense of allowing multiple outgoing edges fr"
2020.conll-shared.1,P19-1009,0,0.173641,"Missing"
2020.conll-shared.1,D19-1392,0,0.119567,"Missing"
2020.conll-shared.1,N18-1063,1,0.899522,"Missing"
2020.conll-shared.1,K19-2014,0,0.104365,"Missing"
2020.conll-shared.1,P18-1016,1,0.879706,"true values. Universal Conceptual Cognitive Annotation Universal Cognitive Conceptual Annotation (UCCA; Abend and Rappoport, 2013) is based on cognitive linguistic and typological theories, primarily Basic Linguistic Theory (Dixon, 2010/2012). The shared task targets the UCCA foundational layer, which focuses on argument structure phenomena (where predicates may be verbal, nominal, adjectival, or otherwise). This coarse-grained level of semantics has been shown to be preserved well across translations (Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018c), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). the deep object of apply can be argued to not have a semantic contribution of their own. The ADDR argument relation to the apply predicate has been recursively propagated to both elements of the apposition and to all members of the coordinate structure. Accordingly, edge labels in PTG are not always functional, in the sense of allowing multiple outgoing edges from one node with the same label. In FGD, role labels (called functors) ACT(or), PAT(ient),"
2020.conll-shared.2,W13-0101,0,0.514464,"Missing"
2020.conll-shared.2,hajic-etal-2012-announcing,0,0.215418,"Missing"
2020.conll-shared.2,E17-2039,1,0.896243,"Missing"
2020.conll-shared.2,S19-2001,0,0.0898366,"ve roots in formal semantics, and they are geared to account for negation, quantification, and semantic scope rather than for syntactic Introduction Graphs are a common data structure for representing meaning of natural language sentences or texts. Several shared tasks on semantic parsing have been organized, and the target meaning representations of the shared tasks were predominantly encoded as directed labeled graphs:1 Semantic Dependency Graphs (Oepen et al., 2014, 2015), Abstract Meaning Representation (May, 2016; May and Priyadarshi, 2017), and Universal Conceptual Cognitive Annotation (Hershcovich et al., 2019). Some of these graphs are presented in Figure 1. Recently, Oepen et al. (2019) packaged several meaning representation graphs in a uniform graph ∗ Part of the work was done while the author was at the University of Groningen. 1 Throughout the paper, we mean a directed labeled graph when simply talking about graphs, unless stated otherwise. 23 Proceedings of the CoNLL 2020 Shared Task: Cross-Framework Meaning Representation Parsing, pages 23–32 c Online, Nov. 19-20, 2020. 2020 Association for Computational Linguistics contrast-01 ARG2 _but_c TENSE pres PERF + R-HNDL act-02 polarity - L-HNDL ne"
2020.conll-shared.2,W19-1201,1,0.899402,"Missing"
2020.conll-shared.2,W13-2322,0,0.316416,"Missing"
2020.conll-shared.2,P18-1040,0,0.365872,"4 CTR CTR CTR CTR a2 a2senate senate bin bin Name Name senatesenate a2 a2 NameName c5 c5 a1 a1 b3 b3 b3 b3 a1 ref ref a1 ref ref con PRE PRE b4 b4 vote.v.01 vote.v.01 con senate.n.01 senate.n.01 a1 rol b5 NEG NEG b5 con AgentAgent a1 rol ref con ??2 a1 a2 ref a1 act.v.01 act.v.01 ??2 a2 a1 ??2 a1 ??2 senate.n.01 senate.n.01 c6 c6 a1 a1 ??2 PRE PRE a2 Agent Agent a2 c7 c7 a1 a1 ?? b4 ref ref b4 b5 b5 a1 NEG NEGact.v.01 act.v.01 c8 c8 2 a1 ??2 ??2 vote.v.01 vote.v.01 senatesen Name Name CTR CTR senate.n.01 senate. PRE NEG NEG act.v. contentcontentact.v.01 (c) L18 is the edge-centric encoding by Liu et al. (2018). B and C are represented as unlabeled nodes with B- and C-labeled incoming edges. R, O and argument positions (A) are encoded as labeled edges. Unlabeled nodes are introduced not only by B and r but also by B and C. (b) The BB∗ encoding largely follows Basile and Bos (2013) and incorporates several additional simplifications. The encoding is node-centric. B and C are encoded as labeled nodes while R, O and argument positions (A) as labeled edges. Only B and r are unlabeled nodes. Figure 3: Contrasting the existing graph representations of DRSs. The graphs encode the sample DRS from Figure 2."
2020.conll-shared.2,W13-2101,1,0.830951,"act.v.01 ??2 a2 a1 ??2 a1 ??2 senate.n.01 senate.n.01 c6 c6 a1 a1 ??2 PRE PRE a2 Agent Agent a2 c7 c7 a1 a1 ?? b4 ref ref b4 b5 b5 a1 NEG NEGact.v.01 act.v.01 c8 c8 2 a1 ??2 ??2 vote.v.01 vote.v.01 senatesen Name Name CTR CTR senate.n.01 senate. PRE NEG NEG act.v. contentcontentact.v.01 (c) L18 is the edge-centric encoding by Liu et al. (2018). B and C are represented as unlabeled nodes with B- and C-labeled incoming edges. R, O and argument positions (A) are encoded as labeled edges. Unlabeled nodes are introduced not only by B and r but also by B and C. (b) The BB∗ encoding largely follows Basile and Bos (2013) and incorporates several additional simplifications. The encoding is node-centric. B and C are encoded as labeled nodes while R, O and argument positions (A) as labeled edges. Only B and r are unlabeled nodes. Figure 3: Contrasting the existing graph representations of DRSs. The graphs encode the sample DRS from Figure 2. For brevity, the tense information is omitted from the DRS. Unlabeled nodes have a gray background. The shapes of nodes are not part of the graphs but simply help with reading to distinguish the types of symbols. The work by Power (1999) doesn’t aim to convert DRSs into grap"
2020.conll-shared.2,S16-1166,0,0.100358,"ing representations is that DRSs are very different from syntactic structures. DRSs have roots in formal semantics, and they are geared to account for negation, quantification, and semantic scope rather than for syntactic Introduction Graphs are a common data structure for representing meaning of natural language sentences or texts. Several shared tasks on semantic parsing have been organized, and the target meaning representations of the shared tasks were predominantly encoded as directed labeled graphs:1 Semantic Dependency Graphs (Oepen et al., 2014, 2015), Abstract Meaning Representation (May, 2016; May and Priyadarshi, 2017), and Universal Conceptual Cognitive Annotation (Hershcovich et al., 2019). Some of these graphs are presented in Figure 1. Recently, Oepen et al. (2019) packaged several meaning representation graphs in a uniform graph ∗ Part of the work was done while the author was at the University of Groningen. 1 Throughout the paper, we mean a directed labeled graph when simply talking about graphs, unless stated otherwise. 23 Proceedings of the CoNLL 2020 Shared Task: Cross-Framework Meaning Representation Parsing, pages 23–32 c Online, Nov. 19-20, 2020. 2020 Association for"
2020.conll-shared.2,S17-2090,0,0.0817584,"ntations is that DRSs are very different from syntactic structures. DRSs have roots in formal semantics, and they are geared to account for negation, quantification, and semantic scope rather than for syntactic Introduction Graphs are a common data structure for representing meaning of natural language sentences or texts. Several shared tasks on semantic parsing have been organized, and the target meaning representations of the shared tasks were predominantly encoded as directed labeled graphs:1 Semantic Dependency Graphs (Oepen et al., 2014, 2015), Abstract Meaning Representation (May, 2016; May and Priyadarshi, 2017), and Universal Conceptual Cognitive Annotation (Hershcovich et al., 2019). Some of these graphs are presented in Figure 1. Recently, Oepen et al. (2019) packaged several meaning representation graphs in a uniform graph ∗ Part of the work was done while the author was at the University of Groningen. 1 Throughout the paper, we mean a directed labeled graph when simply talking about graphs, unless stated otherwise. 23 Proceedings of the CoNLL 2020 Shared Task: Cross-Framework Meaning Representation Parsing, pages 23–32 c Online, Nov. 19-20, 2020. 2020 Association for Computational Linguistics co"
2020.conll-shared.2,W08-2222,1,0.646327,"o-Abn-Ia1 10 4 Cor-Bo-Ab-Ia1 20 Cor-Bo-Abn 0.1 Cor-Bo-Ab 1.0 Cor-Bo-An 2.2 Cor-B!-An 3.5 Cl-Bo-Abn 2.2 Cl-Bo-Ab 2.0 Cl-Bo-An 4.8 Cl-B!-An 6.7 C!-B!-An 4.8 chLSTM↑ "" Table 4: The percentage of approximate (i.e., non-exact) matches w.r.t. the total non-null DRGs. Lower numbers are better as more graph matches corresponding to MCES are found. The total number of DRSs is 885. While converting DRSs into DRGs, 8-number of DRGs become null due to ill-formed DRSs and are excluded during calculating the percentages. Encoding with C• additionally renders C• -number of DRSs untranslatable. parser Boxer (Bos, 2008), which is used in the PMB to pack all annotations layers into DRS boxes. Boxer↓ is Boxer based on the NLP tools of the PMB pipeline6 , on the other hand, Boxer↑ is Boxer employing annotation layers output by MaChAmp (van der Goot et al., 2020). As the names suggest, Boxer↑ is a better model than Boxer↓ . The output DRSs are obtained by parsing the development set (885 documents) of the PMB v3.0.0.7 Evaluation of the models based on the DRSs of the dev set is given in Table 3. DRSs are scored with Counter (van Noord et al., 2018a), the clause matching tool for DRSs in clausal form.8 For MCES-b"
2020.conll-shared.2,L18-1267,1,0.864181,"Missing"
2020.conll-shared.2,Q18-1043,1,0.910083,"Missing"
2020.conll-shared.2,W13-0122,1,0.730526,"d from the Parallel Meaning Bank (PMB, Abzianidze et al., 2017). One such DRS is presented in Figure 2. The DRS signature is given in Table 1. The PMB incorporates several extensions to DRSs. On a micro level, the extensions aim to make DRSs language-neutral by disambiguating non-logical symbols with WordNet (Miller, 1995) synsets and VerbNet (Bonial et al., 2011) roles, where the VerbNet roles are used in combination with neo-Davidsonian event semantics (Parsons, 1990). On a macro level, presuppositions are modeled and explicitly represented following Van der Sandt (1992) and Projective DRT (Venhuizen et al., 2013) while discourse is analyzed following Segmented DRT (Asher and Lascarides, 2003) and flattened by treating discourse relations and DRS operators in a unified way. Due to these extensions, 3 Related Work There have been several approaches to represent DRSs as graphs. These representations are put side-by-side in Figure 3. 3 Note that t2 is in b4 because it has to be out of the scope of negation: there is a time t2 , and it is not the case that at t2 the Senate acts. 25 a2 1 1 a1 a1 c2 a1 c4 ??1 c6 a1 a2 ref v.01 c8 Name Agent PRE house ??2 a1 a1 ??2 bin b1 b1 bin CTR Name NEG content senate.n."
2020.conll-shared.2,2020.conll-shared.1,1,0.908406,"Missing"
2020.conll-shared.2,2020.conll-shared.3,0,0.229774,"Missing"
2020.conll-shared.2,K19-2001,1,0.857613,"Missing"
2020.conll-shared.2,2020.cl-3.3,0,0.171003,"based meaning representations in two aspects. First, DRSs are not inherently graphs. A DRS is more like a formula of predicate logic which is further organized in sub-formulas and governed with additional operations that account for co-reference and presupposition. That’s why DRSs are usually not considered as graphbased meaning representations. For example, DRT was not among the frameworks of the shared task on cross-framework meaning representation parsing (MRP 2019, Oepen et al., 2019) since the meaning representations at the shared task were ˇ all uniformly formatted as graphs. Zabokrtsk´ y et al. (2020) excluded DRSs when surveying sentence meaning representations as they “limit [themselves] to meaning representations whose backbone structure can be described as a graph over words (possibly with added non-lexical nodes) [. . . ]”. The second main contrast between DRSs and several of the graph-based meaning representations is that DRSs are very different from syntactic structures. DRSs have roots in formal semantics, and they are geared to account for negation, quantification, and semantic scope rather than for syntactic Introduction Graphs are a common data structure for representing meaning"
2020.conll-shared.2,S15-2153,1,0.928445,"Missing"
2020.conll-shared.2,S14-2008,1,0.925336,"Missing"
2020.conll-shared.2,oepen-lonning-2006-discriminant,1,0.79677,"Missing"
2020.dmr-1.2,E17-2039,1,0.871815,"Missing"
2020.dmr-1.2,D15-1198,0,0.646419,"d ”B” is true), and assuming that an AMR is interpreted as a conjunction of clauses, AMR will make the right predictions as long as no negation is involved (e.g., it will yield the correct inference “Mary left” from “Mary left yesterday”). But since, in AMR, negation is represented as a predicate rather than an operator that takes scope, it will make wrong predications for negated sentences (e.g., it will allow the inference “Mary left” from “Mary did not leave”. This is why AMRs need some kind of reformulation before interpretation, and that is exactly what has been proposed in earlier work (Artzi et al., 2015; Bos, 2016; Stabler, 2017; Lai et al., 2020). In this paper I seek the solution at the representational level. I think the contribution of this paper is that this extension is simpler of nature than those proposed earlier (Bos, 2016; Stabler, 2017). It bears similarities with named graphs for semantic representations (Crouch and Kalouli, 2018). I argue that, if we want to fix the bound variable problem and the scope representation problem, AMR requires explicit scope in their representations (Bos and Abzianidze, 2019). I propose a method to do this by keeping the underlying predicate-argument"
2020.dmr-1.2,W13-2322,0,0.582379,"put forward a proposal to extend AMRs with a logical dimension, in order to—from a formal semantics point of view—correctly capture negation, quantification, and presuppositional phenomena. It is desirable to investigate such an extension, because (i) it would make a comparison of AMR with other semantics formalisms possible (in particular Discourse Representation Theory); (ii) it would make AMR suitable for performing logical inferences; and (iii) it would be an important step in sharing resources for semantic parsing. The aim is to do this in such a way that existing AMR-annotated corpora (Banarescu et al., 2013) can be relatively easily extended with the desired extensions. This is not the first proposal of extending the PENMAN notation to handle scope phenomena. The need to do so was recognized by other researchers (Bos, 2016; Stabler, 2017; Lai et al., 2020). Pustejovsky et al. (2019) extend AMR with a possibility of adding explicit scope relations. This extension, however, doesn’t solve a fundamental problem that AMR faces, namely viewing AMRs as directed acyclic graphs and basing their interpretation on this. Consider examples such as “every snake bit itself” or “all dogs want to swim”. In the or"
2020.dmr-1.2,W19-3302,1,0.757563,"before interpretation, and that is exactly what has been proposed in earlier work (Artzi et al., 2015; Bos, 2016; Stabler, 2017; Lai et al., 2020). In this paper I seek the solution at the representational level. I think the contribution of this paper is that this extension is simpler of nature than those proposed earlier (Bos, 2016; Stabler, 2017). It bears similarities with named graphs for semantic representations (Crouch and Kalouli, 2018). I argue that, if we want to fix the bound variable problem and the scope representation problem, AMR requires explicit scope in their representations (Bos and Abzianidze, 2019). I propose a method to do this by keeping the underlying predicate-argument structure, and adding a second, logical layer (Section 2). In Section 3, a list of examples of extended AMRs demonstrate the approach. Some loose ends are discussed in Section 4. 2 Method The idea is to extend the AMR with logical structure, obtaining a scoped representation AMR+ with two dimensions: one level comprising predicate-argument structure (the original AMR, minus polarity attributes), and one level consisting of the logical structure (information about logical operators such as negation and the scope they t"
2020.dmr-1.2,J16-3006,1,0.917317,"ension, because (i) it would make a comparison of AMR with other semantics formalisms possible (in particular Discourse Representation Theory); (ii) it would make AMR suitable for performing logical inferences; and (iii) it would be an important step in sharing resources for semantic parsing. The aim is to do this in such a way that existing AMR-annotated corpora (Banarescu et al., 2013) can be relatively easily extended with the desired extensions. This is not the first proposal of extending the PENMAN notation to handle scope phenomena. The need to do so was recognized by other researchers (Bos, 2016; Stabler, 2017; Lai et al., 2020). Pustejovsky et al. (2019) extend AMR with a possibility of adding explicit scope relations. This extension, however, doesn’t solve a fundamental problem that AMR faces, namely viewing AMRs as directed acyclic graphs and basing their interpretation on this. Consider examples such as “every snake bit itself” or “all dogs want to swim”. In the original AMR graph notation, where quantifiers are expressed as a predicate rather than taking scope, the resulting diagrams are: This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licen"
2020.dmr-1.2,P13-2131,0,0.0708253,"Missing"
2020.dmr-1.2,1995.tmi-1.2,0,0.764793,"ated by numbers enclosed in square brackets). Every AMR is augmented by a set of scoping constraints on the labels. This way, a sub-AMR can be viewed as describing a “context”. The constraints state how the contexts relate to each other. They can be declared as the same contexts (=), a negated context (¬), a conditional context (⇒), or a presuppositional context (&lt;). Colons are used to denote inclusion, i.e., l : C states that context l contains condition C. Note that these labels are similar in spirit to those used in underspecification formalisms as proposed in the early 1990s (Reyle, 1993; Copestake et al., 1995; Bos, 1996). The treatment of presuppositions is inspired by semantic formalism extending Discourse Representation Theory (Van der Sandt, 1992; Geurts, 1999; Venhuizen et al., 2013; Venhuizen et al., 2018). 3 Results Below I illustrate the idea with several canonical examples involving existential and universal quantification, definite descriptions, proper names, and, of course, negation. 14 3.1 Existential Quantification Consider the AMR for “a dog scared a cat” with a transitive verb and two indefinite noun phrases in PENMAN notation: (e / scare-01 :ARG0 (x / dog) :ARG1 (y / cat)) Within th"
2020.dmr-1.2,E17-1051,0,0.106023,"otated AMR corpus (Banarescu et al., 2013), this can be done semi-automatically: first add indices automatically (by replacing ”/” by ”/1/”). Then manually correct cases of negation (search for ”:polarity -”), universal quantification, and definite descriptions, for a sample of the corpus. Finally, use machine learning to annotate the rest (or hire or persuade human annotators to do the job). The polarity and mod relations can be optionally removed from the AMRs. An interesting question concerns the use of existing parsing models developed for AMR (Artzi et al., 2015; Van Noord and Bos, 2017; Damonte et al., 2017). How can these be extended to deal with the extended AMRs as proposed in this paper? As the original AMR is not affected, a sensible approach would be one in a modular fashion, where datasets consisting of AMRs paired with AMR+ s could be used as training material. 4.3 Triple Format of Logical Structure AMRs are converted to sets of triples for evaluation purposes. Therefore, a sensible question to ask is how the logical constraints in this proposal are converted to triples. There are two new types of triple instances. The indexed sub-AMRs all introduce a membership triple (with the edge name"
2020.dmr-1.2,H89-1022,0,0.711957,"nt structure and was not designed to deal with scope and quantifiers. By extending AMR with indices for contexts and formulating constraints on these contexts, a formalism is derived that makes correct predictions for inferences involving negation and bound variables. The attractive core predicate-argument structure of AMR is preserved. The resulting framework is similar to the meaning representations of Discourse Representation Theory employed in the Parallel Meaning Bank. 1 Introduction Abstract Meaning Representation, AMR (Langkilde and Knight, 1998), or the PENMAN notation it is based on (Kasper, 1989), puts emphasis on argument structure. In this paper I put forward a proposal to extend AMRs with a logical dimension, in order to—from a formal semantics point of view—correctly capture negation, quantification, and presuppositional phenomena. It is desirable to investigate such an extension, because (i) it would make a comparison of AMR with other semantics formalisms possible (in particular Discourse Representation Theory); (ii) it would make AMR suitable for performing logical inferences; and (iii) it would be an important step in sharing resources for semantic parsing. The aim is to do th"
2020.dmr-1.2,2020.dmr-1.1,0,0.456467,"ld make a comparison of AMR with other semantics formalisms possible (in particular Discourse Representation Theory); (ii) it would make AMR suitable for performing logical inferences; and (iii) it would be an important step in sharing resources for semantic parsing. The aim is to do this in such a way that existing AMR-annotated corpora (Banarescu et al., 2013) can be relatively easily extended with the desired extensions. This is not the first proposal of extending the PENMAN notation to handle scope phenomena. The need to do so was recognized by other researchers (Bos, 2016; Stabler, 2017; Lai et al., 2020). Pustejovsky et al. (2019) extend AMR with a possibility of adding explicit scope relations. This extension, however, doesn’t solve a fundamental problem that AMR faces, namely viewing AMRs as directed acyclic graphs and basing their interpretation on this. Consider examples such as “every snake bit itself” or “all dogs want to swim”. In the original AMR graph notation, where quantifiers are expressed as a predicate rather than taking scope, the resulting diagrams are: This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons"
2020.dmr-1.2,P98-1116,0,0.0629998,"ning of natural language sentences puts emphasis on predicate-argument structure and was not designed to deal with scope and quantifiers. By extending AMR with indices for contexts and formulating constraints on these contexts, a formalism is derived that makes correct predictions for inferences involving negation and bound variables. The attractive core predicate-argument structure of AMR is preserved. The resulting framework is similar to the meaning representations of Discourse Representation Theory employed in the Parallel Meaning Bank. 1 Introduction Abstract Meaning Representation, AMR (Langkilde and Knight, 1998), or the PENMAN notation it is based on (Kasper, 1989), puts emphasis on argument structure. In this paper I put forward a proposal to extend AMRs with a logical dimension, in order to—from a formal semantics point of view—correctly capture negation, quantification, and presuppositional phenomena. It is desirable to investigate such an extension, because (i) it would make a comparison of AMR with other semantics formalisms possible (in particular Discourse Representation Theory); (ii) it would make AMR suitable for performing logical inferences; and (iii) it would be an important step in shari"
2020.dmr-1.2,W19-3303,0,0.254347,"n of AMR with other semantics formalisms possible (in particular Discourse Representation Theory); (ii) it would make AMR suitable for performing logical inferences; and (iii) it would be an important step in sharing resources for semantic parsing. The aim is to do this in such a way that existing AMR-annotated corpora (Banarescu et al., 2013) can be relatively easily extended with the desired extensions. This is not the first proposal of extending the PENMAN notation to handle scope phenomena. The need to do so was recognized by other researchers (Bos, 2016; Stabler, 2017; Lai et al., 2020). Pustejovsky et al. (2019) extend AMR with a possibility of adding explicit scope relations. This extension, however, doesn’t solve a fundamental problem that AMR faces, namely viewing AMRs as directed acyclic graphs and basing their interpretation on this. Consider examples such as “every snake bit itself” or “all dogs want to swim”. In the original AMR graph notation, where quantifiers are expressed as a predicate rather than taking scope, the resulting diagrams are: This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/. 13 P"
2020.dmr-1.2,W13-0122,1,0.673622,"constraints state how the contexts relate to each other. They can be declared as the same contexts (=), a negated context (¬), a conditional context (⇒), or a presuppositional context (&lt;). Colons are used to denote inclusion, i.e., l : C states that context l contains condition C. Note that these labels are similar in spirit to those used in underspecification formalisms as proposed in the early 1990s (Reyle, 1993; Copestake et al., 1995; Bos, 1996). The treatment of presuppositions is inspired by semantic formalism extending Discourse Representation Theory (Van der Sandt, 1992; Geurts, 1999; Venhuizen et al., 2013; Venhuizen et al., 2018). 3 Results Below I illustrate the idea with several canonical examples involving existential and universal quantification, definite descriptions, proper names, and, of course, negation. 14 3.1 Existential Quantification Consider the AMR for “a dog scared a cat” with a transitive verb and two indefinite noun phrases in PENMAN notation: (e / scare-01 :ARG0 (x / dog) :ARG1 (y / cat)) Within this AMR we can identify three sub-AMRs. We index and constrain them and arrive at the following AMR+ : (e /1/ scare-01 :ARG0 (x /2/ dog) :ARG1 (y /3/ cat)) {1 = 2, 1 = 3} Here, there"
2020.emnlp-main.371,D17-1130,0,0.0540793,"Missing"
2020.emnlp-main.371,D15-1041,0,0.0506521,"uage identification (Dunning, 1994), topical similarity prediction (Cavnar, 1994), named entity recognition (Klein et al., 2003), authorship attribution (Peng et al., 2003) and statistical machine translation (Vilar et al., 2007). More recently, they also proved useful as input representations for neural networks, starting with success in general language modelling (Sutskever et al., 2011; Kim et al., 2016; Bojanowski et al., 2017), but also for a range of other tasks, including tokenization (Evang et al., 2013), POS-tagging (Santos and Zadrozny, 2014; Plank et al., 2016), dependency parsing (Ballesteros et al., 2015; Vania et al., 2018) and neural machine translation (Chung et al., 2016; Costa-juss`a and Fonollosa, 2016; Luong and Manning, 2016; Cherry et al., 2018). In semantic parsing, if character-level representations are employed, they are commonly used in combination with non-contextual word-level representations (Lewis et al., 2016; Ballesteros and Al-Onaizan, 2017; Groschwitz et al., 2018; Cai and Lam, 2019). There are a few recent studies that did use character-level representations in combination with BERT (Zhang et al., 2019a,b; Cai and Lam, 2020), though only Zhang et al. (2019a) provided an"
2020.emnlp-main.371,W13-2322,0,0.0528518,"hat adding character-level representations generally improved performance, though we did not find a clear preference for either the oneencoder or two-encoder model. We believe that, given the better performance of the two-encoder model on the fairly short documents of the nonEnglish languages (see Figure 3), this model is likely the most useful in semantic parsing tasks with single sentences, such as SQL parsing (Zelle and Mooney, 1996; Iyer et al., 2017; FineganDollak et al., 2018), while the one encoder charCNN model has more potential for tasks with longer sentences/documents, such as AMR (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013) and GMB-based DRS parsing (Bos et al., 2017; Liu et al., 2018, 2019a). The latter model also has more potential to be applicable for other (semantic parsing) systems as it can be applied to all systems that form token-level representations from a document. In this sense, we hope that our findings here are also applicable for other, more structured, encoder-decoder models developed for semantic parsing (e.g., Yin and Neubig, 2017; Krishnamurthy et al., 2017; Dong and Lapata, 2018; Liu et al., 2019a). An unexpected finding is that the BERT models outperformed t"
2020.emnlp-main.371,Q17-1010,0,0.0950073,"Missing"
2020.emnlp-main.371,W08-2222,1,0.610515,"h, with more fine-grained and language-neutral DRSs. Semantic tags are used during annotation (Bjerva et al., 2016; Abzianidze and Bos, 2017), and all non-logical DRS symbols 4588 are grounded in either WordNet (Fellbaum, 1998) or VerbNet (Bonial et al., 2011). Moreover, its releases contain gold standard DRSs. For these reasons, we take the PMB as our corpus of choice to evaluate our DRS parsers. DRS parsing Early approaches to DRS parsing employed rule-based systems for small English texts (Johnson and Klein, 1986; Wada and Asher, 1986; Bos, 2001). The first open domain DRS parser is Boxer (Bos, 2008, 2015), which is a combination of rule-based and statistical models. Le and Zuidema (2012) used a probabilistic parsing model that used dependency structures to parse GMB data as graphs. More recently, Liu et al. (2018) proposed a neural model that produces (treestructured) DRSs in three steps by first learning the general (box) structure of a DRS, after which specific conditions and referents are filled in. In followup work (Liu et al., 2019a) they extend this work by adding an improved attention mechanism and constraining the decoder to ensure well-formed output. This model achieved impress"
2020.emnlp-main.371,W15-1841,1,0.896704,"Missing"
2020.emnlp-main.371,A00-1031,0,0.0390252,"h 0 529 483 1,301 21,550 3.3 Sent I Linguistic features We want to contrast our method of character-level information to adding sources of linguistic information. Based on van Noord et al. (2019), we employ these five sources: part-of-speech tags (POS), dependency parses (DEP), lemmas (LEM), CCG supertags (CCG) and semantic tags (SEM). For the first three sources, we use Stanford CoreNLP (Manning et al., 2014) to parse the documents in our dataset. The CCG supertags are obtained by using easyCCG (Lewis and Steedman, 2014). For semantic tagging, we train our own trigram-based tagger using TnT (Brants, 2000).7 Table 2 shows a tagged example sentence for all five sources of information. Moreover, we also include non-contextual GLOVE and FASTTEXT embeddings as an extra source of information. We add these sources of linguistic information in the same way as we add the character-level information, in either one or two encoders (see Section 3.1). In two encoders, we can use the exact same architecture. For one encoder, we (obviously) do not use the char-CNN, but learn a separate embedding for the tags (of size 200), that is then concatenated to the token-level representation, i.e., ei = [etoki ; eling"
2020.emnlp-main.371,D19-1393,0,0.020546,"anowski et al., 2017), but also for a range of other tasks, including tokenization (Evang et al., 2013), POS-tagging (Santos and Zadrozny, 2014; Plank et al., 2016), dependency parsing (Ballesteros et al., 2015; Vania et al., 2018) and neural machine translation (Chung et al., 2016; Costa-juss`a and Fonollosa, 2016; Luong and Manning, 2016; Cherry et al., 2018). In semantic parsing, if character-level representations are employed, they are commonly used in combination with non-contextual word-level representations (Lewis et al., 2016; Ballesteros and Al-Onaizan, 2017; Groschwitz et al., 2018; Cai and Lam, 2019). There are a few recent studies that did use character-level representations in combination with BERT (Zhang et al., 2019a,b; Cai and Lam, 2020), though only Zhang et al. (2019a) provided an ablation score without the characters. Moreover, it is not clear if this small improvement was significant. van Noord and Bos (2017) and van Noord et al. (2018b), on the other hand, used solely character-level representations in an end-to-end fashion, using a bi-LSTM sequence-to-sequence model, which outperformed word-based models that employed non-contextual embeddings. 2.2 Discourse Representation Struc"
2020.emnlp-main.371,2020.acl-main.119,0,0.0169374,"Plank et al., 2016), dependency parsing (Ballesteros et al., 2015; Vania et al., 2018) and neural machine translation (Chung et al., 2016; Costa-juss`a and Fonollosa, 2016; Luong and Manning, 2016; Cherry et al., 2018). In semantic parsing, if character-level representations are employed, they are commonly used in combination with non-contextual word-level representations (Lewis et al., 2016; Ballesteros and Al-Onaizan, 2017; Groschwitz et al., 2018; Cai and Lam, 2019). There are a few recent studies that did use character-level representations in combination with BERT (Zhang et al., 2019a,b; Cai and Lam, 2020), though only Zhang et al. (2019a) provided an ablation score without the characters. Moreover, it is not clear if this small improvement was significant. van Noord and Bos (2017) and van Noord et al. (2018b), on the other hand, used solely character-level representations in an end-to-end fashion, using a bi-LSTM sequence-to-sequence model, which outperformed word-based models that employed non-contextual embeddings. 2.2 Discourse Representation Structures DRSs are formal meaning representations introduced by Discourse Representation Theory (Kamp and Reyle, 1993) with the aim to capture the me"
2020.emnlp-main.371,D18-1327,0,0.0127154,"ion function (dot-product) and dj the final decoder hidden state at step j. This model can easily be extended to more than two encoders, which we will experiment with in Section 4. This type of multi-source model is commonly used to represent different languages, e.g., in machine translation (Zoph and Knight, 2016; Firat et al., 2016) and semantic parsing (Susanto and Lu, 2017; Duong et al., 2017), though it has also been successfully applied in multi-modal translation (Libovick´y and Helcl, 2017), multi-framework semantic parsing (Stanovsky and Dagan, 2018) and adding linguistic information (Currey and Heafield, 2018; van Noord et al., 2019). To the best of our knowledge, we are the first to represent the characters as a source of extra information in a multisource sequence-to-sequence model. Transformer We also experiment with the Transformer model (Vaswani et al., 2017), using the stacked self attention model as implemented in AllenNLP. A possible advantage of this model is that it might handle longer sentences and documents better. However, it might be harder to tune (Popel and Bojar, 2018)2 and its improved performance has mainly been shown for large data sets, as 2 Also see: https://twitter.com/srush"
2020.emnlp-main.371,P17-1175,0,0.0232468,"Missing"
2020.emnlp-main.371,D18-1461,0,0.0134369,"g et al., 2003) and statistical machine translation (Vilar et al., 2007). More recently, they also proved useful as input representations for neural networks, starting with success in general language modelling (Sutskever et al., 2011; Kim et al., 2016; Bojanowski et al., 2017), but also for a range of other tasks, including tokenization (Evang et al., 2013), POS-tagging (Santos and Zadrozny, 2014; Plank et al., 2016), dependency parsing (Ballesteros et al., 2015; Vania et al., 2018) and neural machine translation (Chung et al., 2016; Costa-juss`a and Fonollosa, 2016; Luong and Manning, 2016; Cherry et al., 2018). In semantic parsing, if character-level representations are employed, they are commonly used in combination with non-contextual word-level representations (Lewis et al., 2016; Ballesteros and Al-Onaizan, 2017; Groschwitz et al., 2018; Cai and Lam, 2019). There are a few recent studies that did use character-level representations in combination with BERT (Zhang et al., 2019a,b; Cai and Lam, 2020), though only Zhang et al. (2019a) provided an ablation score without the characters. Moreover, it is not clear if this small improvement was significant. van Noord and Bos (2017) and van Noord et al."
2020.emnlp-main.371,P16-1160,0,0.0503453,"94), named entity recognition (Klein et al., 2003), authorship attribution (Peng et al., 2003) and statistical machine translation (Vilar et al., 2007). More recently, they also proved useful as input representations for neural networks, starting with success in general language modelling (Sutskever et al., 2011; Kim et al., 2016; Bojanowski et al., 2017), but also for a range of other tasks, including tokenization (Evang et al., 2013), POS-tagging (Santos and Zadrozny, 2014; Plank et al., 2016), dependency parsing (Ballesteros et al., 2015; Vania et al., 2018) and neural machine translation (Chung et al., 2016; Costa-juss`a and Fonollosa, 2016; Luong and Manning, 2016; Cherry et al., 2018). In semantic parsing, if character-level representations are employed, they are commonly used in combination with non-contextual word-level representations (Lewis et al., 2016; Ballesteros and Al-Onaizan, 2017; Groschwitz et al., 2018; Cai and Lam, 2019). There are a few recent studies that did use character-level representations in combination with BERT (Zhang et al., 2019a,b; Cai and Lam, 2020), though only Zhang et al. (2019a) provided an ablation score without the characters. Moreover, it is not clear if this"
2020.emnlp-main.371,P93-1001,0,0.0463629,"d quantification, as has been advocated in formal semantics. Moreover, DRSs can be translated to formal logic, which allows for automatic forms of inference by third parties. Lastly, annotated DRSs are available in four languages (Abzianidze et al., 2017, see Section 3.3), allowing us to evaluate our models on multiple languages. 2 2.1 Background Character-level models The power of character-level representations has long been known in the field. In earlier work, they were successfully used in a range of tasks, including text-to-speech (Sejnowski and Rosenberg, 1987), parallel text alignment (Church, 1993), grapheme to phoneme conversion (Kaplan and Kay, 1994), language identification (Dunning, 1994), topical similarity prediction (Cavnar, 1994), named entity recognition (Klein et al., 2003), authorship attribution (Peng et al., 2003) and statistical machine translation (Vilar et al., 2007). More recently, they also proved useful as input representations for neural networks, starting with success in general language modelling (Sutskever et al., 2011; Kim et al., 2016; Bojanowski et al., 2017), but also for a range of other tasks, including tokenization (Evang et al., 2013), POS-tagging (Santos"
2020.emnlp-main.371,2020.acl-main.747,0,0.0671246,"Missing"
2020.emnlp-main.371,P16-2058,0,0.0461228,"Missing"
2020.emnlp-main.371,P18-1033,0,0.0284404,"Missing"
2020.emnlp-main.371,N16-1101,0,0.0192089,"s in the decoder (also see Figure 2):  d0j = LSTM1 dj−1 , etj−1    aj = ATT Ctok , d0j ; ATT Cchar , d0j  dj = LSTM2 d0j , aj Here, etj−1 is the embedding of the previously decoded symbol t, C the set of encoder hidden states for either the tokens or characters, ATT the attention function (dot-product) and dj the final decoder hidden state at step j. This model can easily be extended to more than two encoders, which we will experiment with in Section 4. This type of multi-source model is commonly used to represent different languages, e.g., in machine translation (Zoph and Knight, 2016; Firat et al., 2016) and semantic parsing (Susanto and Lu, 2017; Duong et al., 2017), though it has also been successfully applied in multi-modal translation (Libovick´y and Helcl, 2017), multi-framework semantic parsing (Stanovsky and Dagan, 2018) and adding linguistic information (Currey and Heafield, 2018; van Noord et al., 2019). To the best of our knowledge, we are the first to represent the characters as a source of extra information in a multisource sequence-to-sequence model. Transformer We also experiment with the Transformer model (Vaswani et al., 2017), using the stacked self attention model as impleme"
2020.emnlp-main.371,2020.acl-main.609,0,0.17243,"a (2012) used a probabilistic parsing model that used dependency structures to parse GMB data as graphs. More recently, Liu et al. (2018) proposed a neural model that produces (treestructured) DRSs in three steps by first learning the general (box) structure of a DRS, after which specific conditions and referents are filled in. In followup work (Liu et al., 2019a) they extend this work by adding an improved attention mechanism and constraining the decoder to ensure well-formed output. This model achieved impressive performance on both sentence-level and document-level DRS parsing on GMB data. Fu et al. (2020) in turn improve on this work by employing a Graph Attention Network during both encoding and decoding. The introduction of gold standard DRSs in the PMB enabled a principled comparison of approaches. In our previous work (van Noord et al., 2018b), we showed that sequence-to-sequence models can successfully learn to produce DRSs, with characters as the preferred representation. In follow-up work, we improved on these scores by adding linguistic features (van Noord et al., 2019). The first shared task on DRS parsing (Abzianidze et al., 2019) sparked more interested in the topic, with a system b"
2020.emnlp-main.371,C12-1094,0,0.0213732,"otation (Bjerva et al., 2016; Abzianidze and Bos, 2017), and all non-logical DRS symbols 4588 are grounded in either WordNet (Fellbaum, 1998) or VerbNet (Bonial et al., 2011). Moreover, its releases contain gold standard DRSs. For these reasons, we take the PMB as our corpus of choice to evaluate our DRS parsers. DRS parsing Early approaches to DRS parsing employed rule-based systems for small English texts (Johnson and Klein, 1986; Wada and Asher, 1986; Bos, 2001). The first open domain DRS parser is Boxer (Bos, 2008, 2015), which is a combination of rule-based and statistical models. Le and Zuidema (2012) used a probabilistic parsing model that used dependency structures to parse GMB data as graphs. More recently, Liu et al. (2018) proposed a neural model that produces (treestructured) DRSs in three steps by first learning the general (box) structure of a DRS, after which specific conditions and referents are filled in. In followup work (Liu et al., 2019a) they extend this work by adding an improved attention mechanism and constraining the decoder to ensure well-formed output. This model achieved impressive performance on both sentence-level and document-level DRS parsing on GMB data. Fu et al"
2020.emnlp-main.371,D14-1107,0,0.0180428,",620 885 898 97,598 146,371 German 1,159 417 403 5,250 121,111 Italian 0 515 547 2,772 64,305 Dutch 0 529 483 1,301 21,550 3.3 Sent I Linguistic features We want to contrast our method of character-level information to adding sources of linguistic information. Based on van Noord et al. (2019), we employ these five sources: part-of-speech tags (POS), dependency parses (DEP), lemmas (LEM), CCG supertags (CCG) and semantic tags (SEM). For the first three sources, we use Stanford CoreNLP (Manning et al., 2014) to parse the documents in our dataset. The CCG supertags are obtained by using easyCCG (Lewis and Steedman, 2014). For semantic tagging, we train our own trigram-based tagger using TnT (Brants, 2000).7 Table 2 shows a tagged example sentence for all five sources of information. Moreover, we also include non-contextual GLOVE and FASTTEXT embeddings as an extra source of information. We add these sources of linguistic information in the same way as we add the character-level information, in either one or two encoders (see Section 3.1). In two encoders, we can use the exact same architecture. For one encoder, we (obviously) do not use the char-CNN, but learn a separate embedding for the tags (of size 200),"
2020.emnlp-main.371,P17-2031,0,0.0196596,"Missing"
2020.emnlp-main.371,P18-1040,0,0.359196,"Fellbaum, 1998) or VerbNet (Bonial et al., 2011). Moreover, its releases contain gold standard DRSs. For these reasons, we take the PMB as our corpus of choice to evaluate our DRS parsers. DRS parsing Early approaches to DRS parsing employed rule-based systems for small English texts (Johnson and Klein, 1986; Wada and Asher, 1986; Bos, 2001). The first open domain DRS parser is Boxer (Bos, 2008, 2015), which is a combination of rule-based and statistical models. Le and Zuidema (2012) used a probabilistic parsing model that used dependency structures to parse GMB data as graphs. More recently, Liu et al. (2018) proposed a neural model that produces (treestructured) DRSs in three steps by first learning the general (box) structure of a DRS, after which specific conditions and referents are filled in. In followup work (Liu et al., 2019a) they extend this work by adding an improved attention mechanism and constraining the decoder to ensure well-formed output. This model achieved impressive performance on both sentence-level and document-level DRS parsing on GMB data. Fu et al. (2020) in turn improve on this work by employing a Graph Attention Network during both encoding and decoding. The introduction"
2020.emnlp-main.371,P19-1629,0,0.183763,"sing employed rule-based systems for small English texts (Johnson and Klein, 1986; Wada and Asher, 1986; Bos, 2001). The first open domain DRS parser is Boxer (Bos, 2008, 2015), which is a combination of rule-based and statistical models. Le and Zuidema (2012) used a probabilistic parsing model that used dependency structures to parse GMB data as graphs. More recently, Liu et al. (2018) proposed a neural model that produces (treestructured) DRSs in three steps by first learning the general (box) structure of a DRS, after which specific conditions and referents are filled in. In followup work (Liu et al., 2019a) they extend this work by adding an improved attention mechanism and constraining the decoder to ensure well-formed output. This model achieved impressive performance on both sentence-level and document-level DRS parsing on GMB data. Fu et al. (2020) in turn improve on this work by employing a Graph Attention Network during both encoding and decoding. The introduction of gold standard DRSs in the PMB enabled a principled comparison of approaches. In our previous work (van Noord et al., 2018b), we showed that sequence-to-sequence models can successfully learn to produce DRSs, with characters"
2020.emnlp-main.371,W19-1203,0,0.185792,"sing employed rule-based systems for small English texts (Johnson and Klein, 1986; Wada and Asher, 1986; Bos, 2001). The first open domain DRS parser is Boxer (Bos, 2008, 2015), which is a combination of rule-based and statistical models. Le and Zuidema (2012) used a probabilistic parsing model that used dependency structures to parse GMB data as graphs. More recently, Liu et al. (2018) proposed a neural model that produces (treestructured) DRSs in three steps by first learning the general (box) structure of a DRS, after which specific conditions and referents are filled in. In followup work (Liu et al., 2019a) they extend this work by adding an improved attention mechanism and constraining the decoder to ensure well-formed output. This model achieved impressive performance on both sentence-level and document-level DRS parsing on GMB data. Fu et al. (2020) in turn improve on this work by employing a Graph Attention Network during both encoding and decoding. The introduction of gold standard DRSs in the PMB enabled a principled comparison of approaches. In our previous work (van Noord et al., 2018b), we showed that sequence-to-sequence models can successfully learn to produce DRSs, with characters"
2020.emnlp-main.371,E03-1053,0,0.165795,"s (Abzianidze et al., 2017, see Section 3.3), allowing us to evaluate our models on multiple languages. 2 2.1 Background Character-level models The power of character-level representations has long been known in the field. In earlier work, they were successfully used in a range of tasks, including text-to-speech (Sejnowski and Rosenberg, 1987), parallel text alignment (Church, 1993), grapheme to phoneme conversion (Kaplan and Kay, 1994), language identification (Dunning, 1994), topical similarity prediction (Cavnar, 1994), named entity recognition (Klein et al., 2003), authorship attribution (Peng et al., 2003) and statistical machine translation (Vilar et al., 2007). More recently, they also proved useful as input representations for neural networks, starting with success in general language modelling (Sutskever et al., 2011; Kim et al., 2016; Bojanowski et al., 2017), but also for a range of other tasks, including tokenization (Evang et al., 2013), POS-tagging (Santos and Zadrozny, 2014; Plank et al., 2016), dependency parsing (Ballesteros et al., 2015; Vania et al., 2018) and neural machine translation (Chung et al., 2016; Costa-juss`a and Fonollosa, 2016; Luong and Manning, 2016; Cherry et al.,"
2020.emnlp-main.371,D14-1162,0,0.0853187,"that (possible) improved performance is not simply due to larger model capacity, since during tuning of the baseline models a larger RNN hidden size did not result in better performance. 3.2 Representations We will experiment with five well-known pretrained language models: ELMO (Peters et al., 2018), BERT base/large (Devlin et al., 2019) and ROBERTA base/large (Liu et al., 2019c).4 The performance of these five large LMs is contrasted with results of a character-level model and three wordbased models. The word-based models either learn the embeddings from scratch or use non-contextual GLOVE (Pennington et al., 2014) or FASTTEXT (Grave et al., 2018) embeddings. Pre- and postprocessing of the DRSs is done using the method described in van Noord et al. (2018b).5 The DRSs are linearized, after which the variables are rewritten to a relative representation. The character-level model 3 See Appendix B for specific hyperparameter settings. We are aware that there exist several other large pretrained language models (e.g., Yang et al., 2019; Raffel et al., 2020; Clark et al., 2020), but we believe that the models we used have had the largest impact on the field. 5 https://github.com/RikVN/Neural_DRS/ 4590 4 Gold"
2020.emnlp-main.371,N18-1202,0,0.326977,"arger than adding individual sources of linguistic information or adding non-contextual embeddings. A new method of analysis based on semantic tags demonstrates that the character-level representations improve performance across a subset of selected semantic phenomena. 1 Introduction Character-level models have obtained impressive performance on a number of NLP tasks, ranging from the classic POS-tagging (Santos and Zadrozny, 2014) to complex tasks such as Discourse Representation Structure (DRS) parsing (van Noord et al., 2018b). However, this was before the large pretrained language models (Peters et al., 2018; Devlin et al., 2019) took over the field, with the consequence that for most NLP tasks, state-ofthe-art performance is now obtained by fine-tuning on one of these models (e.g., Conneau et al., 2020). Does this mean that, despite a long tradition of being used in language-related tasks (see Section 2.1), character-level representations are no longer useful? We try to answer this question by looking at semantic parsing, specifically DRS parsing (Abzianidze et al., 2017; van Noord et al., 2018a). We aim to answer the following research questions: 1. Do pretrained language models (LMs) outperfor"
2020.emnlp-main.371,P16-2067,0,0.0320971,"e conversion (Kaplan and Kay, 1994), language identification (Dunning, 1994), topical similarity prediction (Cavnar, 1994), named entity recognition (Klein et al., 2003), authorship attribution (Peng et al., 2003) and statistical machine translation (Vilar et al., 2007). More recently, they also proved useful as input representations for neural networks, starting with success in general language modelling (Sutskever et al., 2011; Kim et al., 2016; Bojanowski et al., 2017), but also for a range of other tasks, including tokenization (Evang et al., 2013), POS-tagging (Santos and Zadrozny, 2014; Plank et al., 2016), dependency parsing (Ballesteros et al., 2015; Vania et al., 2018) and neural machine translation (Chung et al., 2016; Costa-juss`a and Fonollosa, 2016; Luong and Manning, 2016; Cherry et al., 2018). In semantic parsing, if character-level representations are employed, they are commonly used in combination with non-contextual word-level representations (Lewis et al., 2016; Ballesteros and Al-Onaizan, 2017; Groschwitz et al., 2018; Cai and Lam, 2019). There are a few recent studies that did use character-level representations in combination with BERT (Zhang et al., 2019a,b; Cai and Lam, 2020),"
2020.emnlp-main.371,E17-3017,0,0.0262593,"Missing"
2020.emnlp-main.371,Q19-1002,0,0.0167484,"vel representations are no longer useful? We try to answer this question by looking at semantic parsing, specifically DRS parsing (Abzianidze et al., 2017; van Noord et al., 2018a). We aim to answer the following research questions: 1. Do pretrained language models (LMs) outperform character-level models for DRS parsing? Why semantic parsing? Semantic parsing is the task of automatically mapping natural language utterances to interpretable meaning representations. The produced meaning representations can then potentially be used to improve downstream NLP applications (e.g., Issa et al., 2018; Song et al., 2019; Mihaylov and Frank, 2019), though the introduction of large pretrained language models has shown that explicit formal meaning representations might not be a necessary component to achieve high accuracy. However, it is now known that these models lack reasoning capabilities, often simply exploiting statistical artifacts in the data sets, instead of actually understanding language (Niven and Kao, 2019; McCoy et al., 2019). Moreover, Ettinger (2020) found that the popular BERT model (Devlin et al., 2019) completely failed to acquire a general understanding of negation. Related, Bender and Kolle"
2020.emnlp-main.371,D18-1263,0,0.0227515,"den states for either the tokens or characters, ATT the attention function (dot-product) and dj the final decoder hidden state at step j. This model can easily be extended to more than two encoders, which we will experiment with in Section 4. This type of multi-source model is commonly used to represent different languages, e.g., in machine translation (Zoph and Knight, 2016; Firat et al., 2016) and semantic parsing (Susanto and Lu, 2017; Duong et al., 2017), though it has also been successfully applied in multi-modal translation (Libovick´y and Helcl, 2017), multi-framework semantic parsing (Stanovsky and Dagan, 2018) and adding linguistic information (Currey and Heafield, 2018; van Noord et al., 2019). To the best of our knowledge, we are the first to represent the characters as a source of extra information in a multisource sequence-to-sequence model. Transformer We also experiment with the Transformer model (Vaswani et al., 2017), using the stacked self attention model as implemented in AllenNLP. A possible advantage of this model is that it might handle longer sentences and documents better. However, it might be harder to tune (Popel and Bojar, 2018)2 and its improved performance has mainly been shown"
2020.emnlp-main.371,P17-2007,0,0.0282557,"j = LSTM1 dj−1 , etj−1    aj = ATT Ctok , d0j ; ATT Cchar , d0j  dj = LSTM2 d0j , aj Here, etj−1 is the embedding of the previously decoded symbol t, C the set of encoder hidden states for either the tokens or characters, ATT the attention function (dot-product) and dj the final decoder hidden state at step j. This model can easily be extended to more than two encoders, which we will experiment with in Section 4. This type of multi-source model is commonly used to represent different languages, e.g., in machine translation (Zoph and Knight, 2016; Firat et al., 2016) and semantic parsing (Susanto and Lu, 2017; Duong et al., 2017), though it has also been successfully applied in multi-modal translation (Libovick´y and Helcl, 2017), multi-framework semantic parsing (Stanovsky and Dagan, 2018) and adding linguistic information (Currey and Heafield, 2018; van Noord et al., 2019). To the best of our knowledge, we are the first to represent the characters as a source of extra information in a multisource sequence-to-sequence model. Transformer We also experiment with the Transformer model (Vaswani et al., 2017), using the stacked self attention model as implemented in AllenNLP. A possible advantage of t"
2020.emnlp-main.371,D18-1278,0,0.0295909,"Missing"
2020.emnlp-main.371,W07-0705,0,0.0526411,"s to evaluate our models on multiple languages. 2 2.1 Background Character-level models The power of character-level representations has long been known in the field. In earlier work, they were successfully used in a range of tasks, including text-to-speech (Sejnowski and Rosenberg, 1987), parallel text alignment (Church, 1993), grapheme to phoneme conversion (Kaplan and Kay, 1994), language identification (Dunning, 1994), topical similarity prediction (Cavnar, 1994), named entity recognition (Klein et al., 2003), authorship attribution (Peng et al., 2003) and statistical machine translation (Vilar et al., 2007). More recently, they also proved useful as input representations for neural networks, starting with success in general language modelling (Sutskever et al., 2011; Kim et al., 2016; Bojanowski et al., 2017), but also for a range of other tasks, including tokenization (Evang et al., 2013), POS-tagging (Santos and Zadrozny, 2014; Plank et al., 2016), dependency parsing (Ballesteros et al., 2015; Vania et al., 2018) and neural machine translation (Chung et al., 2016; Costa-juss`a and Fonollosa, 2016; Luong and Manning, 2016; Cherry et al., 2018). In semantic parsing, if character-level representa"
2020.lrec-1.35,E17-2039,1,0.891138,"Missing"
2020.lrec-1.35,J09-1005,0,0.0228919,"tically used PIEs per million tokens. the highest idiom frequencies aligns nicely with previous work on idiom frequencies. For example, Moon (1998) notes that the frequency of idioms in spoken language has been overestimated relative to written language. She suggest this may be caused by the high frequency of idioms in scripted speech, such as in fiction, film, and television, a category which also covers W news script. As for W newsp other (and W newsp brdsht, which has the third-highest fIdiom), it has been noted that journalistic writing is a particularly rich source of idioms (Moon, 1998; Fazly et al., 2009; Gr´egoire, 2009). More generally, we see that academic texts (W ac) have the lowest frequency of PIEs, followed by non-academic non-fiction (W nonAc), i.e. texts whose main purpose is instruction, information, and education. PIEs are most frequent in news, prose fiction, conversations, and popular magazines (pop lore), i.e. texts whose main purpose is entertainment. However, spoken conversations (S conv) do not fit this category neatly, even if they have a similar PIE frequency. We have no clear explanation for its high PIE frequency, but we do note that it stands out of the group of news, p"
2020.lrec-1.35,N06-2015,0,0.166345,"form, use a large amount of almost 2,000 PIE types with no restriction on syntactic pattern, automatic pre-extraction, and five sense labels. As far as we know, crowdsourcing has not been utilised for creating an idiom corpus before. However, there is closely related work by Kato et al. (2018), who create a corpus of verbal multiword expressions, a group which includes idioms as well, but is a lot broader, incorporating particle verbs, collocations, and other types of set phrases. Kato et al. extract all instances of a set of MWE types taken from Wiktionary from part of the OntoNotes corpus (Hovy et al., 2006). Since simple extraction based on words can yield a lot of noise, i.e. non-instances, they refine those extractions based on the gold-standard part-of-speech tags and parse trees that are present in the OntoNotes corpus. Most novel, however, is their use of crowdsourcing for distinguishing between literal equivalents of MWE phrases like get up in ‘He gets up early’ and actual MWE instances like in ‘He gets up a hill’. They frame the task as a sense annotation task, asking crowdworkers to label instances as either literal, non-literal, unclear, or ‘none of the above’. Using this procedure, the"
2020.lrec-1.35,L18-1396,0,0.0281549,"s done on the PIE corpora, there is significant variation, with Cook et al. (2008) using only three tags (idiomatic, literal, unclear), whereas Sporleder et al. (2010) use six (idiomatic, literal, both, unclear, embedded, meta-linguistic). As for our approach, we allow a large amount of deviation from the PIE’s dictionary form, use a large amount of almost 2,000 PIE types with no restriction on syntactic pattern, automatic pre-extraction, and five sense labels. As far as we know, crowdsourcing has not been utilised for creating an idiom corpus before. However, there is closely related work by Kato et al. (2018), who create a corpus of verbal multiword expressions, a group which includes idioms as well, but is a lot broader, incorporating particle verbs, collocations, and other types of set phrases. Kato et al. extract all instances of a set of MWE types taken from Wiktionary from part of the OntoNotes corpus (Hovy et al., 2006). Since simple extraction based on words can yield a lot of noise, i.e. non-instances, they refine those extractions based on the gold-standard part-of-speech tags and parse trees that are present in the OntoNotes corpus. Most novel, however, is their use of crowdsourcing for"
2020.lrec-1.35,E09-1086,0,0.0250721,"e.g. idiomatic, literal, and unclear), and the ‘syntactic type’ of the expressions covered. Syntactic type means that, in some cases, only idiom types following a certain syntactic pattern were included, e.g. only verb-(determiner)-noun combinations such as hold your fire and see stars. In general, there is large variation in corpus creation methods, regarding PIE definition, extraction method, annotation schemes, base corpus, and PIE type inventory. Depending on the goal of the corpus, the amount of deviation that is allowed from the PIE’s dictionary form to the instances can be very little (Sporleder and Li, 2009), to quite a lot (Sporleder et al., 2010). The number of PIE types covered by each corpus is limited, ranging from 17 to 65 types, often limited to one or more syntactic patterns. The extraction of PIE instances is usually done in a semiautomatic manner, by manually defining patterns in a text or parse tree, and doing some manual filtering afterwards. This works well, but an extension to a large number of PIE types (e.g. several hundreds) would also require a large increase in the amount of manual effort involved. Considering the sense annotations done on the PIE corpora, there is significant"
2020.lrec-1.35,sporleder-etal-2010-idioms,0,0.0301652,"d the ‘syntactic type’ of the expressions covered. Syntactic type means that, in some cases, only idiom types following a certain syntactic pattern were included, e.g. only verb-(determiner)-noun combinations such as hold your fire and see stars. In general, there is large variation in corpus creation methods, regarding PIE definition, extraction method, annotation schemes, base corpus, and PIE type inventory. Depending on the goal of the corpus, the amount of deviation that is allowed from the PIE’s dictionary form to the instances can be very little (Sporleder and Li, 2009), to quite a lot (Sporleder et al., 2010). The number of PIE types covered by each corpus is limited, ranging from 17 to 65 types, often limited to one or more syntactic patterns. The extraction of PIE instances is usually done in a semiautomatic manner, by manually defining patterns in a text or parse tree, and doing some manual filtering afterwards. This works well, but an extension to a large number of PIE types (e.g. several hundreds) would also require a large increase in the amount of manual effort involved. Considering the sense annotations done on the PIE corpora, there is significant variation, with Cook et al. (2008) using"
2021.acl-short.97,E17-2039,1,0.890997,"Missing"
2021.acl-short.97,W19-1201,1,0.901733,"Missing"
2021.acl-short.97,D15-1041,0,0.0275795,"y et al., 2001; Xue, 2003; Zheng et al., 2013; Cai and Zhao, 2016; Min et al., 2019), is a fundamental step for many Chinese NLP applications, which directly affects downstream performance (Foo and Li, 2004; Xu et al., 2004). Despite the large body of existing research, the quality of Chinese word segmentation remains far from perfect, because many characters are highly ambiguous. Input Formats for Neural Methods Characterlevel representations have proved useful for neural network models in many NLP tasks such as POS-tagging (Santos and Zadrozny, 2014; Plank et al., 2016), dependency parsing (Ballesteros et al., 2015) and neural machine translation (Chung et al., 2016). However, only a few studies have used character-level representations as input representations for Chinese NLP tasks (Yu et al., 2017; Li et al., 2018, 2019; Min et al., 2019). For Chinese semantic parsing, previous studies mostly used wordbased representations as well (Che et al., 2016; Wang et al., 2018). For English DRS parsing, how768 Type English input representation Chinese input representation Char (raw) Char (continuous) Char (tokenized) Word BPE (5k) ˆbrad|ˆpitt|is|an|actor. ˆbradˆpittisanactor. ˆbrad|ˆpitt|is|an|actor|. brad pitt"
2021.acl-short.97,P13-1133,0,0.0210816,"Q3), though it will be interesting to experiment with higher quality word segmentation systems (Higashiyama et al., 2019; Tian et al., 2020). There are many research directions one could take next. One is to include pre-trained models. For instance, we could use recently proposed pretrained models such as BART (Lewis et al., 2020) or mBART (Liu et al., 2020) to improve parsing performance. Another interesting idea is, rather than assuming the English WordNet as a background ontology for concepts in the DRS, using concepts based on Chinese WordNet or multilingual wordnets (Wang and Bond, 2013; Bond and Foster, 2013). Both possibilities will likely further improve performance of semantic parsing for Chinese and inspire research for developing semantic parsing models for languages other than English. Chinese English F-score 80 60 40 20 0 overall oper roles conc nouns verbs adj adv events Figure 3: F-scores per clause type (DRS operators, VerbNet roles and WordNet concepts) and concept type (nouns, verbs, adjectives, adverbs and events) as introduced by van Noord et al. (2018b). Reported scores are on the Chinese and English dev set for the raw character-level models, averaged over 5 runs. Figure 3 shows de"
2021.acl-short.97,P16-1039,0,0.025293,"(Kipper et al., 2008). DRSs can be represented in box format or clause format (see Figure 1), where x, e, s, and t are discourse referents denoting individuals, events, Chinese Word Segmentation Differently from English, Chinese words are not separated by white spaces, as shown in Table 1. The first step of a typical Chinese NLP task is usually to use separators to mark boundaries at appropriate positions to identify words in a sentence. These words define the basic semantic units of Chinese. This process, i.e., Chinese word segmentation (Lafferty et al., 2001; Xue, 2003; Zheng et al., 2013; Cai and Zhao, 2016; Min et al., 2019), is a fundamental step for many Chinese NLP applications, which directly affects downstream performance (Foo and Li, 2004; Xu et al., 2004). Despite the large body of existing research, the quality of Chinese word segmentation remains far from perfect, because many characters are highly ambiguous. Input Formats for Neural Methods Characterlevel representations have proved useful for neural network models in many NLP tasks such as POS-tagging (Santos and Zadrozny, 2014; Plank et al., 2016), dependency parsing (Ballesteros et al., 2015) and neural machine translation (Chung e"
2021.acl-short.97,S16-1167,0,0.0197545,"characters are highly ambiguous. Input Formats for Neural Methods Characterlevel representations have proved useful for neural network models in many NLP tasks such as POS-tagging (Santos and Zadrozny, 2014; Plank et al., 2016), dependency parsing (Ballesteros et al., 2015) and neural machine translation (Chung et al., 2016). However, only a few studies have used character-level representations as input representations for Chinese NLP tasks (Yu et al., 2017; Li et al., 2018, 2019; Min et al., 2019). For Chinese semantic parsing, previous studies mostly used wordbased representations as well (Che et al., 2016; Wang et al., 2018). For English DRS parsing, how768 Type English input representation Chinese input representation Char (raw) Char (continuous) Char (tokenized) Word BPE (5k) ˆbrad|ˆpitt|is|an|actor. ˆbradˆpittisanactor. ˆbrad|ˆpitt|is|an|actor|. brad pitt is an actor . ˆ b@ ra@ d ˆ p@ it@ t is an ac@ tor@ . 布 拉 德 · 皮 特 是 个 演 员 。 布 拉 德 |· |皮 特 |是 |个 |演 员 |。 布拉德 · 皮特 是 个 演员 。 布 拉 德 · 皮 特 是个 演 员。 Table 1: Input representations for the English sentence Brad Pitt is an actor and its Chinese translation (布拉德.皮 特是个演员). Note that raw and continuous character representations are identical in Chinese"
2021.acl-short.97,P16-1160,0,0.0286902,"o, 2016; Min et al., 2019), is a fundamental step for many Chinese NLP applications, which directly affects downstream performance (Foo and Li, 2004; Xu et al., 2004). Despite the large body of existing research, the quality of Chinese word segmentation remains far from perfect, because many characters are highly ambiguous. Input Formats for Neural Methods Characterlevel representations have proved useful for neural network models in many NLP tasks such as POS-tagging (Santos and Zadrozny, 2014; Plank et al., 2016), dependency parsing (Ballesteros et al., 2015) and neural machine translation (Chung et al., 2016). However, only a few studies have used character-level representations as input representations for Chinese NLP tasks (Yu et al., 2017; Li et al., 2018, 2019; Min et al., 2019). For Chinese semantic parsing, previous studies mostly used wordbased representations as well (Che et al., 2016; Wang et al., 2018). For English DRS parsing, how768 Type English input representation Chinese input representation Char (raw) Char (continuous) Char (tokenized) Word BPE (5k) ˆbrad|ˆpitt|is|an|actor. ˆbradˆpittisanactor. ˆbrad|ˆpitt|is|an|actor|. brad pitt is an actor . ˆ b@ ra@ d ˆ p@ it@ t is an ac@ tor@ ."
2021.acl-short.97,E17-1051,0,0.0251358,"ynsets present in the DRS are reminiscent of English, they really represent concepts, not words. Similarly, the VerbNet roles have English names, but are universal thematic roles. An exception is formed by named entities, that are grounded by the orthography used in the source language. In sum, we assume that the translations are, by and large, meaning preserving, and project English to Chinese DRSs by changing all English named entities to Chinese ones as they appeared in the Chinese input (see Figure 1). This semantic annotation projection method bears strong similarities and is inspired by Damonte et al. (2017) and Min et al. (2019). Input Representation Types We consider five types of input representations, outlined in Table 1: (i) raw characters, (ii) continuous characters (i.e., without spaces), (iii) tokenised characters, (iv) tokenised words, and (v) byte-pair encoded text (BPE, Sennrich et al., 2016). Note that for Chinese, the first two options amount to the same kind of input. For BPE, we experiment with the number of merges (1k, 5k and 10k) and found in preliminary experiments that it was preferable to not add the indicator “@” for Chinese. For English character input we use an explicit “sh"
2021.acl-short.97,2020.acl-main.609,0,0.0172315,"task of mapping natural language to formal meaning representations (Figure 1). In this short paper we focus on parsing Discourse Representation Structures (DRSs): the meaning representations proposed in Discourse Representation Theory (DRT, Kamp and Reyle, 1993), covering a large variety of linguistic phenomena including coreference, thematic roles, presuppositions, scope, quantification, tense, and discourse relations. Several data-driven methods based on neural networks have been proposed for DRS parsing (van Noord et al., 2018b, 2019; Liu et al., 2019a; Evang, 2019; Fancellu et al., 2019; Fu et al., 2020; van Noord et al., 2020). These approaches frame semantic parsing as a sequence transformation problem and map the target meaning representation to string format. These models learn the meaning of a series of semantic phenomena by taking sentences Figure 1: Example DRS for Chinese in both clause and box representation. as input and directly outputting the corresponding DRSs, without the aid of any extra linguistic information (such as part-of-speech or syntactic structure). These previous studies have achieved good results, but have mostly focused on English or other languages that use the La"
2021.acl-short.97,N19-1276,0,0.0189248,"adverbs remain challenging. English results outperform those of Chinese, but it is likely that this is due to the general bias of the meaning representations towards English. Similar as for English, we find that characters are the preferred input representation for Chinese (RQ2). Surprisingly, for English, good results are even obtained by using characters without spaces as input. Tokenisation (segmenting the text into words) of the input offers a small advantage for English, but not for Chinese (RQ3), though it will be interesting to experiment with higher quality word segmentation systems (Higashiyama et al., 2019; Tian et al., 2020). There are many research directions one could take next. One is to include pre-trained models. For instance, we could use recently proposed pretrained models such as BART (Lewis et al., 2020) or mBART (Liu et al., 2020) to improve parsing performance. Another interesting idea is, rather than assuming the English WordNet as a background ontology for concepts in the DRS, using concepts based on Chinese WordNet or multilingual wordnets (Wang and Bond, 2013; Bond and Foster, 2013). Both possibilities will likely further improve performance of semantic parsing for Chinese and i"
2021.acl-short.97,P16-1002,0,0.0123575,"ke Chinese, a language with a large vocabulary of characters? Does rule-based tokenisation of the input help, and which granularity is preferred: characters, or words? The results are promising. Even with DRSs based on English, good results for Chinese are obtained. Tokenisation offers a small advantage for English, but not for Chinese. Overall, characters are preferred as input, both for English and Chinese. 1 Introduction Recently, sequence-to-sequence models have achieved remarkable performance in various natural language processing tasks, including semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016; Konstas et al., 2017; Dong and Lapata, 2018), the task of mapping natural language to formal meaning representations (Figure 1). In this short paper we focus on parsing Discourse Representation Structures (DRSs): the meaning representations proposed in Discourse Representation Theory (DRT, Kamp and Reyle, 1993), covering a large variety of linguistic phenomena including coreference, thematic roles, presuppositions, scope, quantification, tense, and discourse relations. Several data-driven methods based on neural networks have been proposed for DRS parsing (van Noord et al., 2018b, 2019; Liu"
2021.acl-short.97,W18-2716,0,0.0177919,"8,403 English–Chinese documents with gold data, of which 885 are used as development set and 898 as test set. The silver data (97,597 documents) is only used to augment the training data, following van Noord et al. (2018b). We use a fine-tuning approach to effectively use high-quality data in our experiments: first training the system with silver and gold data, then restarting the training to finetune on only the gold data. Neural Architecture We use a recurrent sequence-to-sequence neural network with two bi-directional LSTM layers (Hochreiter and Schmidhuber, 1997) as implemented by Marian (Junczys-Dowmunt et al., 2018), similar to van Noord et al. (2019).3 Specific hyper-parameters are shown in Appendix A. We also experimented with the Transformer model (Vaswani et al., 2017), as implemented in the same framework. However, similar to van Noord et al. (2020), none of our experiments reached the performance of the bi-LSTM model. We will therefore only show results of the bi-LSTM model in this paper. Evaluation DRS output is evaluated by using Counter (van Noord et al., 2018a), a tool that calculates the micro precision and recall of matching DRS clauses. Counter has been widely used in the evaluation of DRS p"
2021.acl-short.97,P07-2045,0,0.00670119,"), (iii) tokenised characters, (iv) tokenised words, and (v) byte-pair encoded text (BPE, Sennrich et al., 2016). Note that for Chinese, the first two options amount to the same kind of input. For BPE, we experiment with the number of merges (1k, 5k and 10k) and found in preliminary experiments that it was preferable to not add the indicator “@” for Chinese. For English character input we use an explicit “shift” symbol (ˆ) to indicate uppercased characters, to keep the vocabulary size low. Moreover, the |symbol represents an explicit word boundary. For tokenisation we use the Moses tokenizer (Koehn et al., 2007) for English, while we use the default mode of the Jieba tokenizer2 to segment the Chinese sentences. To fairly compare these different input representations, we do not employ pretrained embeddings. 1 https://github.com/wangchunliu/ Chinese-DRS-data 2 769 https://github.com/fxsjy/jieba Output Representation Appendix B shows how DRSs are represented for the purpose of training neural models, following van Noord et al. (2018b). Variables are replaced by indices, and the DRSs are coded in either a linearised character-level or wordlevel clause format. For Chinese, we experimented with both repres"
2021.acl-short.97,P17-1014,0,0.0240438,"e with a large vocabulary of characters? Does rule-based tokenisation of the input help, and which granularity is preferred: characters, or words? The results are promising. Even with DRSs based on English, good results for Chinese are obtained. Tokenisation offers a small advantage for English, but not for Chinese. Overall, characters are preferred as input, both for English and Chinese. 1 Introduction Recently, sequence-to-sequence models have achieved remarkable performance in various natural language processing tasks, including semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016; Konstas et al., 2017; Dong and Lapata, 2018), the task of mapping natural language to formal meaning representations (Figure 1). In this short paper we focus on parsing Discourse Representation Structures (DRSs): the meaning representations proposed in Discourse Representation Theory (DRT, Kamp and Reyle, 1993), covering a large variety of linguistic phenomena including coreference, thematic roles, presuppositions, scope, quantification, tense, and discourse relations. Several data-driven methods based on neural networks have been proposed for DRS parsing (van Noord et al., 2018b, 2019; Liu et al., 2019a; Evang,"
2021.acl-short.97,P03-1056,0,0.135574,"ls learn the meaning of a series of semantic phenomena by taking sentences Figure 1: Example DRS for Chinese in both clause and box representation. as input and directly outputting the corresponding DRSs, without the aid of any extra linguistic information (such as part-of-speech or syntactic structure). These previous studies have achieved good results, but have mostly focused on English or other languages that use the Latin alphabet. Our objective is to investigate whether the same method is applicable to Mandarin Chinese, an extremely analytic language which makes deep parsing challenging (Levy and Manning, 2003; Yu et al., 2011; Tse and Curran, 2012; Min et al., 2019). But Chinese is not only different on the level of syntax; its writing system also shows large differences with English, as there are no explicit word separators in written Chinese, and there is no distinction between 767 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 767–775 August 1–6, 2021. ©2021 Association for Computational Linguistics lower- and upper case characters. Unlike English, Chinese wo"
2021.acl-short.97,2020.acl-main.703,0,0.011455,"acters are the preferred input representation for Chinese (RQ2). Surprisingly, for English, good results are even obtained by using characters without spaces as input. Tokenisation (segmenting the text into words) of the input offers a small advantage for English, but not for Chinese (RQ3), though it will be interesting to experiment with higher quality word segmentation systems (Higashiyama et al., 2019; Tian et al., 2020). There are many research directions one could take next. One is to include pre-trained models. For instance, we could use recently proposed pretrained models such as BART (Lewis et al., 2020) or mBART (Liu et al., 2020) to improve parsing performance. Another interesting idea is, rather than assuming the English WordNet as a background ontology for concepts in the DRS, using concepts based on Chinese WordNet or multilingual wordnets (Wang and Bond, 2013; Bond and Foster, 2013). Both possibilities will likely further improve performance of semantic parsing for Chinese and inspire research for developing semantic parsing models for languages other than English. Chinese English F-score 80 60 40 20 0 overall oper roles conc nouns verbs adj adv events Figure 3: F-scores per clause type"
2021.acl-short.97,P19-1314,0,0.01798,"given the differences between the languages in how they convey meaning (Levy and Manning, 2003). In general, F-scores start to decrease when sentences get longer (Figure 2), though there is no clear difference between the character and wordlevel models. This is in line with the findings of van Noord et al. (2018b). For English, the input types based on characters outperform those based on words. BPE approaches character-level performance for small amounts of merges (1k), but never surpasses it. This too is in line with van Noord et al. (2018b), but also with previous work on NMT for Chinese (Li et al., 2019). There is a small benefit (0.5) for tokenizing the input text before converting the input to character-level format, though the continuous character representation also works surprisingly well. For Chinese, characterbased input shows the best performance too, though for a very small amount of merges BPE obtains a similar score. As opposed to English, tokenizing the Chinese input is not beneficial when using a character-level representation, though it also does not hurt performance. In general, character-level models seem the most promising for Chinese DRS parsing. Similar results were obtaine"
2021.acl-short.97,P19-1629,0,0.0212653,"2016; Konstas et al., 2017; Dong and Lapata, 2018), the task of mapping natural language to formal meaning representations (Figure 1). In this short paper we focus on parsing Discourse Representation Structures (DRSs): the meaning representations proposed in Discourse Representation Theory (DRT, Kamp and Reyle, 1993), covering a large variety of linguistic phenomena including coreference, thematic roles, presuppositions, scope, quantification, tense, and discourse relations. Several data-driven methods based on neural networks have been proposed for DRS parsing (van Noord et al., 2018b, 2019; Liu et al., 2019a; Evang, 2019; Fancellu et al., 2019; Fu et al., 2020; van Noord et al., 2020). These approaches frame semantic parsing as a sequence transformation problem and map the target meaning representation to string format. These models learn the meaning of a series of semantic phenomena by taking sentences Figure 1: Example DRS for Chinese in both clause and box representation. as input and directly outputting the corresponding DRSs, without the aid of any extra linguistic information (such as part-of-speech or syntactic structure). These previous studies have achieved good results, but have mostly"
2021.acl-short.97,W19-1203,0,0.0172344,"2016; Konstas et al., 2017; Dong and Lapata, 2018), the task of mapping natural language to formal meaning representations (Figure 1). In this short paper we focus on parsing Discourse Representation Structures (DRSs): the meaning representations proposed in Discourse Representation Theory (DRT, Kamp and Reyle, 1993), covering a large variety of linguistic phenomena including coreference, thematic roles, presuppositions, scope, quantification, tense, and discourse relations. Several data-driven methods based on neural networks have been proposed for DRS parsing (van Noord et al., 2018b, 2019; Liu et al., 2019a; Evang, 2019; Fancellu et al., 2019; Fu et al., 2020; van Noord et al., 2020). These approaches frame semantic parsing as a sequence transformation problem and map the target meaning representation to string format. These models learn the meaning of a series of semantic phenomena by taking sentences Figure 1: Example DRS for Chinese in both clause and box representation. as input and directly outputting the corresponding DRSs, without the aid of any extra linguistic information (such as part-of-speech or syntactic structure). These previous studies have achieved good results, but have mostly"
2021.acl-short.97,2020.tacl-1.47,0,0.0141999,"representation for Chinese (RQ2). Surprisingly, for English, good results are even obtained by using characters without spaces as input. Tokenisation (segmenting the text into words) of the input offers a small advantage for English, but not for Chinese (RQ3), though it will be interesting to experiment with higher quality word segmentation systems (Higashiyama et al., 2019; Tian et al., 2020). There are many research directions one could take next. One is to include pre-trained models. For instance, we could use recently proposed pretrained models such as BART (Lewis et al., 2020) or mBART (Liu et al., 2020) to improve parsing performance. Another interesting idea is, rather than assuming the English WordNet as a background ontology for concepts in the DRS, using concepts based on Chinese WordNet or multilingual wordnets (Wang and Bond, 2013; Bond and Foster, 2013). Both possibilities will likely further improve performance of semantic parsing for Chinese and inspire research for developing semantic parsing models for languages other than English. Chinese English F-score 80 60 40 20 0 overall oper roles conc nouns verbs adj adv events Figure 3: F-scores per clause type (DRS operators, VerbNet rol"
2021.acl-short.97,D19-1377,0,0.354055,"g sentences Figure 1: Example DRS for Chinese in both clause and box representation. as input and directly outputting the corresponding DRSs, without the aid of any extra linguistic information (such as part-of-speech or syntactic structure). These previous studies have achieved good results, but have mostly focused on English or other languages that use the Latin alphabet. Our objective is to investigate whether the same method is applicable to Mandarin Chinese, an extremely analytic language which makes deep parsing challenging (Levy and Manning, 2003; Yu et al., 2011; Tse and Curran, 2012; Min et al., 2019). But Chinese is not only different on the level of syntax; its writing system also shows large differences with English, as there are no explicit word separators in written Chinese, and there is no distinction between 767 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 767–775 August 1–6, 2021. ©2021 Association for Computational Linguistics lower- and upper case characters. Unlike English, Chinese words comprise few characters, but the number of different c"
2021.acl-short.97,L18-1267,1,0.897284,"Missing"
2021.acl-short.97,Q18-1043,1,0.855655,"Missing"
2021.acl-short.97,W19-0504,1,0.899694,"Missing"
2021.acl-short.97,2020.emnlp-main.371,1,0.855066,"Missing"
2021.acl-short.97,P16-2067,0,0.0167105,"i.e., Chinese word segmentation (Lafferty et al., 2001; Xue, 2003; Zheng et al., 2013; Cai and Zhao, 2016; Min et al., 2019), is a fundamental step for many Chinese NLP applications, which directly affects downstream performance (Foo and Li, 2004; Xu et al., 2004). Despite the large body of existing research, the quality of Chinese word segmentation remains far from perfect, because many characters are highly ambiguous. Input Formats for Neural Methods Characterlevel representations have proved useful for neural network models in many NLP tasks such as POS-tagging (Santos and Zadrozny, 2014; Plank et al., 2016), dependency parsing (Ballesteros et al., 2015) and neural machine translation (Chung et al., 2016). However, only a few studies have used character-level representations as input representations for Chinese NLP tasks (Yu et al., 2017; Li et al., 2018, 2019; Min et al., 2019). For Chinese semantic parsing, previous studies mostly used wordbased representations as well (Che et al., 2016; Wang et al., 2018). For English DRS parsing, how768 Type English input representation Chinese input representation Char (raw) Char (continuous) Char (tokenized) Word BPE (5k) ˆbrad|ˆpitt|is|an|actor. ˆbradˆpitt"
2021.acl-short.97,P16-1162,0,0.0170398,"ssume that the translations are, by and large, meaning preserving, and project English to Chinese DRSs by changing all English named entities to Chinese ones as they appeared in the Chinese input (see Figure 1). This semantic annotation projection method bears strong similarities and is inspired by Damonte et al. (2017) and Min et al. (2019). Input Representation Types We consider five types of input representations, outlined in Table 1: (i) raw characters, (ii) continuous characters (i.e., without spaces), (iii) tokenised characters, (iv) tokenised words, and (v) byte-pair encoded text (BPE, Sennrich et al., 2016). Note that for Chinese, the first two options amount to the same kind of input. For BPE, we experiment with the number of merges (1k, 5k and 10k) and found in preliminary experiments that it was preferable to not add the indicator “@” for Chinese. For English character input we use an explicit “shift” symbol (ˆ) to indicate uppercased characters, to keep the vocabulary size low. Moreover, the |symbol represents an explicit word boundary. For tokenisation we use the Moses tokenizer (Koehn et al., 2007) for English, while we use the default mode of the Jieba tokenizer2 to segment the Chinese se"
2021.acl-short.97,2020.acl-main.734,0,0.0373935,"g. English results outperform those of Chinese, but it is likely that this is due to the general bias of the meaning representations towards English. Similar as for English, we find that characters are the preferred input representation for Chinese (RQ2). Surprisingly, for English, good results are even obtained by using characters without spaces as input. Tokenisation (segmenting the text into words) of the input offers a small advantage for English, but not for Chinese (RQ3), though it will be interesting to experiment with higher quality word segmentation systems (Higashiyama et al., 2019; Tian et al., 2020). There are many research directions one could take next. One is to include pre-trained models. For instance, we could use recently proposed pretrained models such as BART (Lewis et al., 2020) or mBART (Liu et al., 2020) to improve parsing performance. Another interesting idea is, rather than assuming the English WordNet as a background ontology for concepts in the DRS, using concepts based on Chinese WordNet or multilingual wordnets (Wang and Bond, 2013; Bond and Foster, 2013). Both possibilities will likely further improve performance of semantic parsing for Chinese and inspire research for"
2021.acl-short.97,N12-1030,0,0.028497,"tic phenomena by taking sentences Figure 1: Example DRS for Chinese in both clause and box representation. as input and directly outputting the corresponding DRSs, without the aid of any extra linguistic information (such as part-of-speech or syntactic structure). These previous studies have achieved good results, but have mostly focused on English or other languages that use the Latin alphabet. Our objective is to investigate whether the same method is applicable to Mandarin Chinese, an extremely analytic language which makes deep parsing challenging (Levy and Manning, 2003; Yu et al., 2011; Tse and Curran, 2012; Min et al., 2019). But Chinese is not only different on the level of syntax; its writing system also shows large differences with English, as there are no explicit word separators in written Chinese, and there is no distinction between 767 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 767–775 August 1–6, 2021. ©2021 Association for Computational Linguistics lower- and upper case characters. Unlike English, Chinese words comprise few characters, but the nu"
2021.acl-short.97,N18-2040,0,0.0256041,"Missing"
2021.acl-short.97,W13-4302,0,0.015457,"ut not for Chinese (RQ3), though it will be interesting to experiment with higher quality word segmentation systems (Higashiyama et al., 2019; Tian et al., 2020). There are many research directions one could take next. One is to include pre-trained models. For instance, we could use recently proposed pretrained models such as BART (Lewis et al., 2020) or mBART (Liu et al., 2020) to improve parsing performance. Another interesting idea is, rather than assuming the English WordNet as a background ontology for concepts in the DRS, using concepts based on Chinese WordNet or multilingual wordnets (Wang and Bond, 2013; Bond and Foster, 2013). Both possibilities will likely further improve performance of semantic parsing for Chinese and inspire research for developing semantic parsing models for languages other than English. Chinese English F-score 80 60 40 20 0 overall oper roles conc nouns verbs adj adv events Figure 3: F-scores per clause type (DRS operators, VerbNet roles and WordNet concepts) and concept type (nouns, verbs, adjectives, adverbs and events) as introduced by van Noord et al. (2018b). Reported scores are on the Chinese and English dev set for the raw character-level models, averaged over 5"
2021.acl-short.97,W04-1118,0,0.0301864,"events, Chinese Word Segmentation Differently from English, Chinese words are not separated by white spaces, as shown in Table 1. The first step of a typical Chinese NLP task is usually to use separators to mark boundaries at appropriate positions to identify words in a sentence. These words define the basic semantic units of Chinese. This process, i.e., Chinese word segmentation (Lafferty et al., 2001; Xue, 2003; Zheng et al., 2013; Cai and Zhao, 2016; Min et al., 2019), is a fundamental step for many Chinese NLP applications, which directly affects downstream performance (Foo and Li, 2004; Xu et al., 2004). Despite the large body of existing research, the quality of Chinese word segmentation remains far from perfect, because many characters are highly ambiguous. Input Formats for Neural Methods Characterlevel representations have proved useful for neural network models in many NLP tasks such as POS-tagging (Santos and Zadrozny, 2014; Plank et al., 2016), dependency parsing (Ballesteros et al., 2015) and neural machine translation (Chung et al., 2016). However, only a few studies have used character-level representations as input representations for Chinese NLP tasks (Yu et al., 2017; Li et al.,"
2021.acl-short.97,O03-4002,0,0.253152,"ntic relations by Verbnet roles (Kipper et al., 2008). DRSs can be represented in box format or clause format (see Figure 1), where x, e, s, and t are discourse referents denoting individuals, events, Chinese Word Segmentation Differently from English, Chinese words are not separated by white spaces, as shown in Table 1. The first step of a typical Chinese NLP task is usually to use separators to mark boundaries at appropriate positions to identify words in a sentence. These words define the basic semantic units of Chinese. This process, i.e., Chinese word segmentation (Lafferty et al., 2001; Xue, 2003; Zheng et al., 2013; Cai and Zhao, 2016; Min et al., 2019), is a fundamental step for many Chinese NLP applications, which directly affects downstream performance (Foo and Li, 2004; Xu et al., 2004). Despite the large body of existing research, the quality of Chinese word segmentation remains far from perfect, because many characters are highly ambiguous. Input Formats for Neural Methods Characterlevel representations have proved useful for neural network models in many NLP tasks such as POS-tagging (Santos and Zadrozny, 2014; Plank et al., 2016), dependency parsing (Ballesteros et al., 2015)"
2021.acl-short.97,D17-1027,0,0.0283507,"Li, 2004; Xu et al., 2004). Despite the large body of existing research, the quality of Chinese word segmentation remains far from perfect, because many characters are highly ambiguous. Input Formats for Neural Methods Characterlevel representations have proved useful for neural network models in many NLP tasks such as POS-tagging (Santos and Zadrozny, 2014; Plank et al., 2016), dependency parsing (Ballesteros et al., 2015) and neural machine translation (Chung et al., 2016). However, only a few studies have used character-level representations as input representations for Chinese NLP tasks (Yu et al., 2017; Li et al., 2018, 2019; Min et al., 2019). For Chinese semantic parsing, previous studies mostly used wordbased representations as well (Che et al., 2016; Wang et al., 2018). For English DRS parsing, how768 Type English input representation Chinese input representation Char (raw) Char (continuous) Char (tokenized) Word BPE (5k) ˆbrad|ˆpitt|is|an|actor. ˆbradˆpittisanactor. ˆbrad|ˆpitt|is|an|actor|. brad pitt is an actor . ˆ b@ ra@ d ˆ p@ it@ t is an ac@ tor@ . 布 拉 德 · 皮 特 是 个 演 员 。 布 拉 德 |· |皮 特 |是 |个 |演 员 |。 布拉德 · 皮特 是 个 演员 。 布 拉 德 · 皮 特 是个 演 员。 Table 1: Input representations for the English"
2021.acl-short.97,W11-2907,0,0.0330785,"a series of semantic phenomena by taking sentences Figure 1: Example DRS for Chinese in both clause and box representation. as input and directly outputting the corresponding DRSs, without the aid of any extra linguistic information (such as part-of-speech or syntactic structure). These previous studies have achieved good results, but have mostly focused on English or other languages that use the Latin alphabet. Our objective is to investigate whether the same method is applicable to Mandarin Chinese, an extremely analytic language which makes deep parsing challenging (Levy and Manning, 2003; Yu et al., 2011; Tse and Curran, 2012; Min et al., 2019). But Chinese is not only different on the level of syntax; its writing system also shows large differences with English, as there are no explicit word separators in written Chinese, and there is no distinction between 767 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 767–775 August 1–6, 2021. ©2021 Association for Computational Linguistics lower- and upper case characters. Unlike English, Chinese words comprise few"
2021.acl-short.97,D13-1061,0,0.0930746,"Missing"
2021.gem-1.8,W17-5519,0,0.0415756,"Missing"
2021.gem-1.8,2020.tacl-1.3,0,0.0141098,"nse sentences, but past tense and present tense can be generated well. The original test set contained not so many DRSs in future tense, but in the challenge set we added relatively many of them, which likely caused the lower performance on the challenge set. With regards to the polarity challenge set, inspection of the output shows that a common error is to confuse “never” with “not”. This difference in meaning is reflected in a DRS by the relative order of the reference time and the DRS negation operator. Interestingly, recent work in machine translation (Tang, 2020) and language modelling (Ettinger, 2020) has also shown that state-of-the-art neural models still struggle with handling negation. Although the results of the automatic evaluation metrics in the last three challenge sets have no obvious changes compared with the original data sets, our manual evaluation results show that the performance of the model in all challenge sets is lower than the original data sets. This further shows that there is not always a positive correlation between automatic evaluation and manual evaluation, and it is still necessary to rely on manual evaluation. BLEU METEOR ROUGE Char-level (raw) Word-level (tok) 6"
2021.gem-1.8,W13-2322,0,0.0493702,"es. The aim of these challenge sets is to assess the neural generator’s systematicity and generalization to unseen inputs. 1 Arianna Bisazza CLCG Univ. of Groningen Introduction Faithfully generating text from structured representations is an important task in NLP. Common tasks include generations from tables (Parikh et al., 2020), knowledge graphs (Gardent et al., 2017) and meaning representations (Horvat et al., 2015; Flanigan et al., 2016; Duˇsek and Jurˇc´ıcˇ ek, 2019). Recently, many research efforts have focused on the graph-based semantic formalism Abstract Meaning Representation (AMR, Banarescu et al., 2013), with approaches based on machine translation (Pourdamghani et al., 2016; Konstas et al., 2017), specialized graph encoders (Song et al., 2018; Zhu et al., 2019; Cai and Lam, 2020; Zhao et al., 2020; Jin and Gildea, 2020) and pre-trained language models (Mager et al., 2020; Ribeiro et al., 2020). However, far less attention has been given to generating text from formal meaning representation, such as Discourse Representation Structures (DRSs). DRSs are proposed in Discourse Representation Theory (Kamp and Reyle, 1993; Kadmon, 1 Concurrently to this work, Liu et al. (2021) published a DRS-to-t"
2021.gem-1.8,W19-1202,0,0.011861,"MR is that DRSs are in principle language neutral (at least the version of DRS that we use in this paper), with gold standard annotations publicly available in four languages (Abzianidze et al., 2017). For these reasons, developing portable and high-quality generation systems for DRSs is a promising research direction. While there has been some initial work on DRSto-text generation (Basile and Bos, 2011; Narayan and Gardent, 2014; Basile, 2015), most DRS-based work has focused on semantic parsing, that is mapping text to DRS (Liu et al., 2018; van Noord et al., 2018b, 2019; Liu et al., 2019b; Evang, 2019; van Noord et al., 2020; Fancellu et al., 2020). Our work has two main contributions. The first is on the modelling side, as we take the first step in DRS-to-text generation with neural networks.1 Specifically, we use a bi-LSTM sequence-to-sequence model that processes linearized DRSs representations and produces English texts using a character-level decoder (see pipeline in Figure 1). Our second contribution regards the evaluation of the produced text. Given the known limitations of reference-based automatic metrics for natural language generation (Reiter and Belz, 2009; Novikova et al., 201"
2021.gem-1.8,2020.findings-emnlp.320,0,0.0597967,"Missing"
2021.gem-1.8,W11-2819,1,0.45373,", but their inclusion presents a challenge for the generation methods as well, as they, for example, have to deal with a lot more variables in the representation (van Noord et al., 2018a). Another difference with AMR is that DRSs are in principle language neutral (at least the version of DRS that we use in this paper), with gold standard annotations publicly available in four languages (Abzianidze et al., 2017). For these reasons, developing portable and high-quality generation systems for DRSs is a promising research direction. While there has been some initial work on DRSto-text generation (Basile and Bos, 2011; Narayan and Gardent, 2014; Basile, 2015), most DRS-based work has focused on semantic parsing, that is mapping text to DRS (Liu et al., 2018; van Noord et al., 2018b, 2019; Liu et al., 2019b; Evang, 2019; van Noord et al., 2020; Fancellu et al., 2020). Our work has two main contributions. The first is on the modelling side, as we take the first step in DRS-to-text generation with neural networks.1 Specifically, we use a bi-LSTM sequence-to-sequence model that processes linearized DRSs representations and produces English texts using a character-level decoder (see pipeline in Figure 1). Our s"
2021.gem-1.8,N16-1087,0,0.0238144,"ROSE, that targets specific semantic phenomena. We do this with five DRS generation challenge sets focusing on tense, grammatical number, polarity, named entities and quantities. The aim of these challenge sets is to assess the neural generator’s systematicity and generalization to unseen inputs. 1 Arianna Bisazza CLCG Univ. of Groningen Introduction Faithfully generating text from structured representations is an important task in NLP. Common tasks include generations from tables (Parikh et al., 2020), knowledge graphs (Gardent et al., 2017) and meaning representations (Horvat et al., 2015; Flanigan et al., 2016; Duˇsek and Jurˇc´ıcˇ ek, 2019). Recently, many research efforts have focused on the graph-based semantic formalism Abstract Meaning Representation (AMR, Banarescu et al., 2013), with approaches based on machine translation (Pourdamghani et al., 2016; Konstas et al., 2017), specialized graph encoders (Song et al., 2018; Zhu et al., 2019; Cai and Lam, 2020; Zhao et al., 2020; Jin and Gildea, 2020) and pre-trained language models (Mager et al., 2020; Ribeiro et al., 2020). However, far less attention has been given to generating text from formal meaning representation, such as Discourse Represe"
2021.gem-1.8,S12-1040,1,0.775175,"these test suites it shows that it can deal with specific semantic phenomena adequately in unforeseen circumstances. We carry out these modifications on subsets of the PMB test data, and we group them into those that assess systematic predictions (tense, polarity, and grammatical number) and those that assess generalisation to unseen input (names and quantities). The specific challenge sets are described in detail below. 3.2 As negation plays a crucial role to determine the truth conditions of a sentence, there has been ample interest in recognizing negation in text (Morante and Blanco, 2012; Basile et al., 2012) and translating accurately (Sennrich, 2017; Tang, 2020). Here we focus on generation, that is expressing negation appropriately in a sentence given a meaning representation. Negation is expressed in a DRS with a unary operator, introducing an embedded DRS. For the first 100 instances of the test set we removed negation if it was already present, or, more frequently, added it if it was not. Again, the corresponding reference text was changed to reflect this change in meaning. Example: I cooked dinner. → I didn’t cook dinner. 3.3 Tom had three thousand books. Tom does not have three thousand bo"
2021.gem-1.8,W17-3518,0,0.0244069,"evaluate the surface level of generated output, we develop a new metric, ROSE, that targets specific semantic phenomena. We do this with five DRS generation challenge sets focusing on tense, grammatical number, polarity, named entities and quantities. The aim of these challenge sets is to assess the neural generator’s systematicity and generalization to unseen inputs. 1 Arianna Bisazza CLCG Univ. of Groningen Introduction Faithfully generating text from structured representations is an important task in NLP. Common tasks include generations from tables (Parikh et al., 2020), knowledge graphs (Gardent et al., 2017) and meaning representations (Horvat et al., 2015; Flanigan et al., 2016; Duˇsek and Jurˇc´ıcˇ ek, 2019). Recently, many research efforts have focused on the graph-based semantic formalism Abstract Meaning Representation (AMR, Banarescu et al., 2013), with approaches based on machine translation (Pourdamghani et al., 2016; Konstas et al., 2017), specialized graph encoders (Song et al., 2018; Zhu et al., 2019; Cai and Lam, 2020; Zhao et al., 2020; Jin and Gildea, 2020) and pre-trained language models (Mager et al., 2020; Ribeiro et al., 2020). However, far less attention has been given to gener"
2021.gem-1.8,P07-2045,0,0.00835635,"of text representation possibilities, we considered just two ways to represent text: character-based, where raw characters are separate entities and spaces are indicated by a special symbol (three vertical bars); or (tokenised) word-based, where tokenised words form the basic entities. The character-based approach has the advantage that post-processing is straightforward. The use of word-level representations is the classical approach in natural language processing, but requires a de-tokenisation step after generating. Tokenisation and de-tokenisation is carried out with the Moses tokenizer (Koehn et al., 2007). 2.3 Value Parameter decoder, a mini-batch size of 48 and the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 0.002. All hyper-parameters are shown in Table1. We use the English gold standard training, dev and test data of PMB 3.0.03 , containing 6,620, 885 and 898 instances, respectively. During training, we merge the gold standard with the only partially manually annotated silver standard of 97,598 instances. Differently from van Noord et al. (2018b), we do not fine-tune on the gold standard data in a second step, as this did not lead to improved performance. Vocabulary For a w"
2021.gem-1.8,C16-1103,0,0.0546334,"Missing"
2021.gem-1.8,P17-1014,0,0.0206008,"zation to unseen inputs. 1 Arianna Bisazza CLCG Univ. of Groningen Introduction Faithfully generating text from structured representations is an important task in NLP. Common tasks include generations from tables (Parikh et al., 2020), knowledge graphs (Gardent et al., 2017) and meaning representations (Horvat et al., 2015; Flanigan et al., 2016; Duˇsek and Jurˇc´ıcˇ ek, 2019). Recently, many research efforts have focused on the graph-based semantic formalism Abstract Meaning Representation (AMR, Banarescu et al., 2013), with approaches based on machine translation (Pourdamghani et al., 2016; Konstas et al., 2017), specialized graph encoders (Song et al., 2018; Zhu et al., 2019; Cai and Lam, 2020; Zhao et al., 2020; Jin and Gildea, 2020) and pre-trained language models (Mager et al., 2020; Ribeiro et al., 2020). However, far less attention has been given to generating text from formal meaning representation, such as Discourse Representation Structures (DRSs). DRSs are proposed in Discourse Representation Theory (Kamp and Reyle, 1993; Kadmon, 1 Concurrently to this work, Liu et al. (2021) published a DRS-to-text model that is based on tree-LSTMs. 73 Proceedings of the 1st Workshop on Natural Language Ge"
2021.gem-1.8,W15-0116,0,0.01771,"develop a new metric, ROSE, that targets specific semantic phenomena. We do this with five DRS generation challenge sets focusing on tense, grammatical number, polarity, named entities and quantities. The aim of these challenge sets is to assess the neural generator’s systematicity and generalization to unseen inputs. 1 Arianna Bisazza CLCG Univ. of Groningen Introduction Faithfully generating text from structured representations is an important task in NLP. Common tasks include generations from tables (Parikh et al., 2020), knowledge graphs (Gardent et al., 2017) and meaning representations (Horvat et al., 2015; Flanigan et al., 2016; Duˇsek and Jurˇc´ıcˇ ek, 2019). Recently, many research efforts have focused on the graph-based semantic formalism Abstract Meaning Representation (AMR, Banarescu et al., 2013), with approaches based on machine translation (Pourdamghani et al., 2016; Konstas et al., 2017), specialized graph encoders (Song et al., 2018; Zhu et al., 2019; Cai and Lam, 2020; Zhao et al., 2020; Jin and Gildea, 2020) and pre-trained language models (Mager et al., 2020; Ribeiro et al., 2020). However, far less attention has been given to generating text from formal meaning representation, su"
2021.gem-1.8,W07-0734,0,0.131628,"st set with numbers and then changed the numbers in the DRS representation to unknown quantity expressions, represented as a sequence of characters. For example, we changed Quantity(x, 150) to Quantity(x, 152). This way, we test if the model can systematically generalize to generate the right numeral expression, even though it has not seen this particular sequence of characters before. We use three standard metrics measuring wordoverlap between system output and references. They are BLEU (Papineni et al., 2002) used as standard in machine translation evaluation and very common in NLG, METEOR (Lavie and Agarwal, 2007), and ROUGE-L (Lin, 2004), which were applied in the COCO caption generation challenge as well as other NLG experiments (Novikova et al., 2017b; Duˇsek et al., 2020). As is well known, these standard metrics give a first, rough impression about the quality of the generated output, but often reveal only part of the story. This is why we also consider a further form of assessment. 4 4.2 Assessment Methods We consider two types of assessment for the generated English sentences. Our point of departure are the well-known automatic metrics based on Expert Assessment Inspired by work of Jagfeld et al"
2021.gem-1.8,W18-6529,0,0.0856308,"garwal, 2007), and ROUGE-L (Lin, 2004), which were applied in the COCO caption generation challenge as well as other NLG experiments (Novikova et al., 2017b; Duˇsek et al., 2020). As is well known, these standard metrics give a first, rough impression about the quality of the generated output, but often reveal only part of the story. This is why we also consider a further form of assessment. 4 4.2 Assessment Methods We consider two types of assessment for the generated English sentences. Our point of departure are the well-known automatic metrics based on Expert Assessment Inspired by work of Jagfeld et al. (2018) and Belz et al. (2020), we believe that the manual evaluation method for our task should be simple in definition, 77 deed learn the shallow information contained in the input data and copy it to generate, even if these subsets (numbers, quantities and name entities) in the DRSs do not appear in the training set. However, for tense and polarity, all three automatic metrics are significantly lower in the challenge sentences than in the original sentences. Through the observation of the generated texts of the tense challenge set, we find that it is difficult for the model to generate future tens"
2021.gem-1.8,W04-1013,0,0.0440214,"e numbers in the DRS representation to unknown quantity expressions, represented as a sequence of characters. For example, we changed Quantity(x, 150) to Quantity(x, 152). This way, we test if the model can systematically generalize to generate the right numeral expression, even though it has not seen this particular sequence of characters before. We use three standard metrics measuring wordoverlap between system output and references. They are BLEU (Papineni et al., 2002) used as standard in machine translation evaluation and very common in NLG, METEOR (Lavie and Agarwal, 2007), and ROUGE-L (Lin, 2004), which were applied in the COCO caption generation challenge as well as other NLG experiments (Novikova et al., 2017b; Duˇsek et al., 2020). As is well known, these standard metrics give a first, rough impression about the quality of the generated output, but often reveal only part of the story. This is why we also consider a further form of assessment. 4 4.2 Assessment Methods We consider two types of assessment for the generated English sentences. Our point of departure are the well-known automatic metrics based on Expert Assessment Inspired by work of Jagfeld et al. (2018) and Belz et al."
2021.gem-1.8,2020.coling-main.181,0,0.0243648,"resentations is an important task in NLP. Common tasks include generations from tables (Parikh et al., 2020), knowledge graphs (Gardent et al., 2017) and meaning representations (Horvat et al., 2015; Flanigan et al., 2016; Duˇsek and Jurˇc´ıcˇ ek, 2019). Recently, many research efforts have focused on the graph-based semantic formalism Abstract Meaning Representation (AMR, Banarescu et al., 2013), with approaches based on machine translation (Pourdamghani et al., 2016; Konstas et al., 2017), specialized graph encoders (Song et al., 2018; Zhu et al., 2019; Cai and Lam, 2020; Zhao et al., 2020; Jin and Gildea, 2020) and pre-trained language models (Mager et al., 2020; Ribeiro et al., 2020). However, far less attention has been given to generating text from formal meaning representation, such as Discourse Representation Structures (DRSs). DRSs are proposed in Discourse Representation Theory (Kamp and Reyle, 1993; Kadmon, 1 Concurrently to this work, Liu et al. (2021) published a DRS-to-text model that is based on tree-LSTMs. 73 Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 73–83 August 5–6, 2021. ©2021 Association for Computational Linguistics Fi"
2021.gem-1.8,P18-1040,0,0.0259786,"representation (van Noord et al., 2018a). Another difference with AMR is that DRSs are in principle language neutral (at least the version of DRS that we use in this paper), with gold standard annotations publicly available in four languages (Abzianidze et al., 2017). For these reasons, developing portable and high-quality generation systems for DRSs is a promising research direction. While there has been some initial work on DRSto-text generation (Basile and Bos, 2011; Narayan and Gardent, 2014; Basile, 2015), most DRS-based work has focused on semantic parsing, that is mapping text to DRS (Liu et al., 2018; van Noord et al., 2018b, 2019; Liu et al., 2019b; Evang, 2019; van Noord et al., 2020; Fancellu et al., 2020). Our work has two main contributions. The first is on the modelling side, as we take the first step in DRS-to-text generation with neural networks.1 Specifically, we use a bi-LSTM sequence-to-sequence model that processes linearized DRSs representations and produces English texts using a character-level decoder (see pipeline in Figure 1). Our second contribution regards the evaluation of the produced text. Given the known limitations of reference-based automatic metrics for natural l"
2021.gem-1.8,W18-2716,0,0.0233605,"icial to not include the full vocabulary. For example, it might learn to handle unknown words better if it was exposed to unknown word tokens during training. We experimented with the vocabulary size of the target representation on the development set, as is shown in Figure 2. We find that the we get best performance when including the full vocabulary, with decreasing performance as we decrease the vocabulary. We use this setting for our word-level experiments. Neural Generation Model We use a standard recurrent encoder-decoder architecture with attention as implemented in the Marian toolkit (Junczys-Dowmunt et al., 2018), using two bi-directional LSTM layers (Hochreiter and Schmidhuber, 1997). In particular, we use an embedding size of 300 for both the encoder and 3 75 https://pmb.let.rug.nl/data.php 3 Semantic Challenge Sets tense. Example: She bought a vacuum cleaner at the supermarket. → She will buy a vacuum cleaner at the supermarket. Challenge sets are often used in Machine Translation to assess a model’s ability to systematically deal with specific linguistic phenomena that may be infrequent in standard test sets (Popovi´c and Castilho, 2019). Following this practice, we created five challenge sets for"
2021.gem-1.8,P19-1629,0,0.095897,"r difference with AMR is that DRSs are in principle language neutral (at least the version of DRS that we use in this paper), with gold standard annotations publicly available in four languages (Abzianidze et al., 2017). For these reasons, developing portable and high-quality generation systems for DRSs is a promising research direction. While there has been some initial work on DRSto-text generation (Basile and Bos, 2011; Narayan and Gardent, 2014; Basile, 2015), most DRS-based work has focused on semantic parsing, that is mapping text to DRS (Liu et al., 2018; van Noord et al., 2018b, 2019; Liu et al., 2019b; Evang, 2019; van Noord et al., 2020; Fancellu et al., 2020). Our work has two main contributions. The first is on the modelling side, as we take the first step in DRS-to-text generation with neural networks.1 Specifically, we use a bi-LSTM sequence-to-sequence model that processes linearized DRSs representations and produces English texts using a character-level decoder (see pipeline in Figure 1). Our second contribution regards the evaluation of the produced text. Given the known limitations of reference-based automatic metrics for natural language generation (Reiter and Belz, 2009; Noviko"
2021.gem-1.8,W19-1203,0,0.103335,"r difference with AMR is that DRSs are in principle language neutral (at least the version of DRS that we use in this paper), with gold standard annotations publicly available in four languages (Abzianidze et al., 2017). For these reasons, developing portable and high-quality generation systems for DRSs is a promising research direction. While there has been some initial work on DRSto-text generation (Basile and Bos, 2011; Narayan and Gardent, 2014; Basile, 2015), most DRS-based work has focused on semantic parsing, that is mapping text to DRS (Liu et al., 2018; van Noord et al., 2018b, 2019; Liu et al., 2019b; Evang, 2019; van Noord et al., 2020; Fancellu et al., 2020). Our work has two main contributions. The first is on the modelling side, as we take the first step in DRS-to-text generation with neural networks.1 Specifically, we use a bi-LSTM sequence-to-sequence model that processes linearized DRSs representations and produces English texts using a character-level decoder (see pipeline in Figure 1). Our second contribution regards the evaluation of the produced text. Given the known limitations of reference-based automatic metrics for natural language generation (Reiter and Belz, 2009; Noviko"
2021.gem-1.8,2021.naacl-main.35,0,0.0500456,"entation (AMR, Banarescu et al., 2013), with approaches based on machine translation (Pourdamghani et al., 2016; Konstas et al., 2017), specialized graph encoders (Song et al., 2018; Zhu et al., 2019; Cai and Lam, 2020; Zhao et al., 2020; Jin and Gildea, 2020) and pre-trained language models (Mager et al., 2020; Ribeiro et al., 2020). However, far less attention has been given to generating text from formal meaning representation, such as Discourse Representation Structures (DRSs). DRSs are proposed in Discourse Representation Theory (Kamp and Reyle, 1993; Kadmon, 1 Concurrently to this work, Liu et al. (2021) published a DRS-to-text model that is based on tree-LSTMs. 73 Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 73–83 August 5–6, 2021. ©2021 Association for Computational Linguistics Figure 1: An example of the DRS data and a corresponding reference text with their processing procedures. form a fine-grained manual evaluation. The general goal of these challenge sets is to assess the robustness of a DRS generator with respect to a number of linguistic phenomena. More specifically, we assess (i) generation systematicity with respect to th"
2021.gem-1.8,2020.emnlp-main.371,1,0.817108,"Missing"
2021.gem-1.8,2020.acl-main.167,0,0.011238,"include generations from tables (Parikh et al., 2020), knowledge graphs (Gardent et al., 2017) and meaning representations (Horvat et al., 2015; Flanigan et al., 2016; Duˇsek and Jurˇc´ıcˇ ek, 2019). Recently, many research efforts have focused on the graph-based semantic formalism Abstract Meaning Representation (AMR, Banarescu et al., 2013), with approaches based on machine translation (Pourdamghani et al., 2016; Konstas et al., 2017), specialized graph encoders (Song et al., 2018; Zhu et al., 2019; Cai and Lam, 2020; Zhao et al., 2020; Jin and Gildea, 2020) and pre-trained language models (Mager et al., 2020; Ribeiro et al., 2020). However, far less attention has been given to generating text from formal meaning representation, such as Discourse Representation Structures (DRSs). DRSs are proposed in Discourse Representation Theory (Kamp and Reyle, 1993; Kadmon, 1 Concurrently to this work, Liu et al. (2021) published a DRS-to-text model that is based on tree-LSTMs. 73 Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 73–83 August 5–6, 2021. ©2021 Association for Computational Linguistics Figure 1: An example of the DRS data and a correspondi"
2021.gem-1.8,D17-1238,0,0.026236,"Missing"
2021.gem-1.8,2020.coling-main.420,0,0.0118019,"o main contributions. The first is on the modelling side, as we take the first step in DRS-to-text generation with neural networks.1 Specifically, we use a bi-LSTM sequence-to-sequence model that processes linearized DRSs representations and produces English texts using a character-level decoder (see pipeline in Figure 1). Our second contribution regards the evaluation of the produced text. Given the known limitations of reference-based automatic metrics for natural language generation (Reiter and Belz, 2009; Novikova et al., 2017a) and in particular for AMRto-text (May and Priyadarshi, 2017; Manning et al., 2020), we design five DRS-specific challenge sets (Popovi´c and Castilho, 2019) and use them to perWe present an end-to-end neural approach to generate English sentences from formal meaning representations, Discourse Representation Structures (DRSs). We use a rather standard bi-LSTM sequence-to-sequence model, work with a linearized DRS input representation, and evaluate character-level and word-level decoders. We obtain very encouraging results in terms of reference-based automatic metrics such as BLEU. But because such metrics only evaluate the surface level of generated output, we develop a new"
2021.gem-1.8,W17-5525,0,0.0587451,"Missing"
2021.gem-1.8,S17-2090,0,0.0177406,"al., 2020). Our work has two main contributions. The first is on the modelling side, as we take the first step in DRS-to-text generation with neural networks.1 Specifically, we use a bi-LSTM sequence-to-sequence model that processes linearized DRSs representations and produces English texts using a character-level decoder (see pipeline in Figure 1). Our second contribution regards the evaluation of the produced text. Given the known limitations of reference-based automatic metrics for natural language generation (Reiter and Belz, 2009; Novikova et al., 2017a) and in particular for AMRto-text (May and Priyadarshi, 2017; Manning et al., 2020), we design five DRS-specific challenge sets (Popovi´c and Castilho, 2019) and use them to perWe present an end-to-end neural approach to generate English sentences from formal meaning representations, Discourse Representation Structures (DRSs). We use a rather standard bi-LSTM sequence-to-sequence model, work with a linearized DRS input representation, and evaluate character-level and word-level decoders. We obtain very encouraging results in terms of reference-based automatic metrics such as BLEU. But because such metrics only evaluate the surface level of generated ou"
2021.gem-1.8,P02-1040,0,0.111245,"changed to expressions that were never seen in the training data. We took the first 50 instances of the test set with numbers and then changed the numbers in the DRS representation to unknown quantity expressions, represented as a sequence of characters. For example, we changed Quantity(x, 150) to Quantity(x, 152). This way, we test if the model can systematically generalize to generate the right numeral expression, even though it has not seen this particular sequence of characters before. We use three standard metrics measuring wordoverlap between system output and references. They are BLEU (Papineni et al., 2002) used as standard in machine translation evaluation and very common in NLG, METEOR (Lavie and Agarwal, 2007), and ROUGE-L (Lin, 2004), which were applied in the COCO caption generation challenge as well as other NLG experiments (Novikova et al., 2017b; Duˇsek et al., 2020). As is well known, these standard metrics give a first, rough impression about the quality of the generated output, but often reveal only part of the story. This is why we also consider a further form of assessment. 4 4.2 Assessment Methods We consider two types of assessment for the generated English sentences. Our point of"
2021.gem-1.8,S12-1035,0,0.0341847,"enerator performs well on these test suites it shows that it can deal with specific semantic phenomena adequately in unforeseen circumstances. We carry out these modifications on subsets of the PMB test data, and we group them into those that assess systematic predictions (tense, polarity, and grammatical number) and those that assess generalisation to unseen input (names and quantities). The specific challenge sets are described in detail below. 3.2 As negation plays a crucial role to determine the truth conditions of a sentence, there has been ample interest in recognizing negation in text (Morante and Blanco, 2012; Basile et al., 2012) and translating accurately (Sennrich, 2017; Tang, 2020). Here we focus on generation, that is expressing negation appropriately in a sentence given a meaning representation. Negation is expressed in a DRS with a unary operator, introducing an embedded DRS. For the first 100 instances of the test set we removed negation if it was already present, or, more frequently, added it if it was not. Again, the corresponding reference text was changed to reflect this change in meaning. Example: I cooked dinner. → I didn’t cook dinner. 3.3 Tom had three thousand books. Tom does not"
2021.gem-1.8,2020.emnlp-main.89,0,0.0212567,"as BLEU. But because such metrics only evaluate the surface level of generated output, we develop a new metric, ROSE, that targets specific semantic phenomena. We do this with five DRS generation challenge sets focusing on tense, grammatical number, polarity, named entities and quantities. The aim of these challenge sets is to assess the neural generator’s systematicity and generalization to unseen inputs. 1 Arianna Bisazza CLCG Univ. of Groningen Introduction Faithfully generating text from structured representations is an important task in NLP. Common tasks include generations from tables (Parikh et al., 2020), knowledge graphs (Gardent et al., 2017) and meaning representations (Horvat et al., 2015; Flanigan et al., 2016; Duˇsek and Jurˇc´ıcˇ ek, 2019). Recently, many research efforts have focused on the graph-based semantic formalism Abstract Meaning Representation (AMR, Banarescu et al., 2013), with approaches based on machine translation (Pourdamghani et al., 2016; Konstas et al., 2017), specialized graph encoders (Song et al., 2018; Zhu et al., 2019; Cai and Lam, 2020; Zhao et al., 2020; Jin and Gildea, 2020) and pre-trained language models (Mager et al., 2020; Ribeiro et al., 2020). However, f"
2021.gem-1.8,P14-1041,0,0.0280533,"presents a challenge for the generation methods as well, as they, for example, have to deal with a lot more variables in the representation (van Noord et al., 2018a). Another difference with AMR is that DRSs are in principle language neutral (at least the version of DRS that we use in this paper), with gold standard annotations publicly available in four languages (Abzianidze et al., 2017). For these reasons, developing portable and high-quality generation systems for DRSs is a promising research direction. While there has been some initial work on DRSto-text generation (Basile and Bos, 2011; Narayan and Gardent, 2014; Basile, 2015), most DRS-based work has focused on semantic parsing, that is mapping text to DRS (Liu et al., 2018; van Noord et al., 2018b, 2019; Liu et al., 2019b; Evang, 2019; van Noord et al., 2020; Fancellu et al., 2020). Our work has two main contributions. The first is on the modelling side, as we take the first step in DRS-to-text generation with neural networks.1 Specifically, we use a bi-LSTM sequence-to-sequence model that processes linearized DRSs representations and produces English texts using a character-level decoder (see pipeline in Figure 1). Our second contribution regards"
2021.gem-1.8,W19-7602,0,0.0614782,"Missing"
2021.gem-1.8,L18-1267,1,0.882016,"Missing"
2021.gem-1.8,W16-6603,0,0.0208096,"systematicity and generalization to unseen inputs. 1 Arianna Bisazza CLCG Univ. of Groningen Introduction Faithfully generating text from structured representations is an important task in NLP. Common tasks include generations from tables (Parikh et al., 2020), knowledge graphs (Gardent et al., 2017) and meaning representations (Horvat et al., 2015; Flanigan et al., 2016; Duˇsek and Jurˇc´ıcˇ ek, 2019). Recently, many research efforts have focused on the graph-based semantic formalism Abstract Meaning Representation (AMR, Banarescu et al., 2013), with approaches based on machine translation (Pourdamghani et al., 2016; Konstas et al., 2017), specialized graph encoders (Song et al., 2018; Zhu et al., 2019; Cai and Lam, 2020; Zhao et al., 2020; Jin and Gildea, 2020) and pre-trained language models (Mager et al., 2020; Ribeiro et al., 2020). However, far less attention has been given to generating text from formal meaning representation, such as Discourse Representation Structures (DRSs). DRSs are proposed in Discourse Representation Theory (Kamp and Reyle, 1993; Kadmon, 1 Concurrently to this work, Liu et al. (2021) published a DRS-to-text model that is based on tree-LSTMs. 73 Proceedings of the 1st Workshop"
2021.gem-1.8,Q18-1043,1,0.891388,"Missing"
2021.gem-1.8,J09-4008,0,0.0335459,"2018b, 2019; Liu et al., 2019b; Evang, 2019; van Noord et al., 2020; Fancellu et al., 2020). Our work has two main contributions. The first is on the modelling side, as we take the first step in DRS-to-text generation with neural networks.1 Specifically, we use a bi-LSTM sequence-to-sequence model that processes linearized DRSs representations and produces English texts using a character-level decoder (see pipeline in Figure 1). Our second contribution regards the evaluation of the produced text. Given the known limitations of reference-based automatic metrics for natural language generation (Reiter and Belz, 2009; Novikova et al., 2017a) and in particular for AMRto-text (May and Priyadarshi, 2017; Manning et al., 2020), we design five DRS-specific challenge sets (Popovi´c and Castilho, 2019) and use them to perWe present an end-to-end neural approach to generate English sentences from formal meaning representations, Discourse Representation Structures (DRSs). We use a rather standard bi-LSTM sequence-to-sequence model, work with a linearized DRS input representation, and evaluate character-level and word-level decoders. We obtain very encouraging results in terms of reference-based automatic metrics s"
2021.gem-1.8,P18-1150,0,0.0183422,"v. of Groningen Introduction Faithfully generating text from structured representations is an important task in NLP. Common tasks include generations from tables (Parikh et al., 2020), knowledge graphs (Gardent et al., 2017) and meaning representations (Horvat et al., 2015; Flanigan et al., 2016; Duˇsek and Jurˇc´ıcˇ ek, 2019). Recently, many research efforts have focused on the graph-based semantic formalism Abstract Meaning Representation (AMR, Banarescu et al., 2013), with approaches based on machine translation (Pourdamghani et al., 2016; Konstas et al., 2017), specialized graph encoders (Song et al., 2018; Zhu et al., 2019; Cai and Lam, 2020; Zhao et al., 2020; Jin and Gildea, 2020) and pre-trained language models (Mager et al., 2020; Ribeiro et al., 2020). However, far less attention has been given to generating text from formal meaning representation, such as Discourse Representation Structures (DRSs). DRSs are proposed in Discourse Representation Theory (Kamp and Reyle, 1993; Kadmon, 1 Concurrently to this work, Liu et al. (2021) published a DRS-to-text model that is based on tree-LSTMs. 73 Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), p"
2021.gem-1.8,2020.tacl-1.2,0,0.0341543,"Missing"
2021.gem-1.8,2020.acl-main.224,0,0.024264,"from structured representations is an important task in NLP. Common tasks include generations from tables (Parikh et al., 2020), knowledge graphs (Gardent et al., 2017) and meaning representations (Horvat et al., 2015; Flanigan et al., 2016; Duˇsek and Jurˇc´ıcˇ ek, 2019). Recently, many research efforts have focused on the graph-based semantic formalism Abstract Meaning Representation (AMR, Banarescu et al., 2013), with approaches based on machine translation (Pourdamghani et al., 2016; Konstas et al., 2017), specialized graph encoders (Song et al., 2018; Zhu et al., 2019; Cai and Lam, 2020; Zhao et al., 2020; Jin and Gildea, 2020) and pre-trained language models (Mager et al., 2020; Ribeiro et al., 2020). However, far less attention has been given to generating text from formal meaning representation, such as Discourse Representation Structures (DRSs). DRSs are proposed in Discourse Representation Theory (Kamp and Reyle, 1993; Kadmon, 1 Concurrently to this work, Liu et al. (2021) published a DRS-to-text model that is based on tree-LSTMs. 73 Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 73–83 August 5–6, 2021. ©2021 Association for Compu"
2021.gem-1.8,D19-1548,0,0.0151835,"roduction Faithfully generating text from structured representations is an important task in NLP. Common tasks include generations from tables (Parikh et al., 2020), knowledge graphs (Gardent et al., 2017) and meaning representations (Horvat et al., 2015; Flanigan et al., 2016; Duˇsek and Jurˇc´ıcˇ ek, 2019). Recently, many research efforts have focused on the graph-based semantic formalism Abstract Meaning Representation (AMR, Banarescu et al., 2013), with approaches based on machine translation (Pourdamghani et al., 2016; Konstas et al., 2017), specialized graph encoders (Song et al., 2018; Zhu et al., 2019; Cai and Lam, 2020; Zhao et al., 2020; Jin and Gildea, 2020) and pre-trained language models (Mager et al., 2020; Ribeiro et al., 2020). However, far less attention has been given to generating text from formal meaning representation, such as Discourse Representation Structures (DRSs). DRSs are proposed in Discourse Representation Theory (Kamp and Reyle, 1993; Kadmon, 1 Concurrently to this work, Liu et al. (2021) published a DRS-to-text model that is based on tree-LSTMs. 73 Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 73–83 August"
basile-etal-2012-developing,miltsakaki-etal-2004-penn,0,\N,Missing
basile-etal-2012-developing,E12-2019,1,\N,Missing
basile-etal-2012-developing,sekine-etal-2002-extended,0,\N,Missing
basile-etal-2012-developing,W08-2230,0,\N,Missing
basile-etal-2012-developing,N06-2015,0,\N,Missing
basile-etal-2012-developing,P98-1013,0,\N,Missing
basile-etal-2012-developing,C98-1013,0,\N,Missing
basile-etal-2012-developing,J05-1004,0,\N,Missing
basile-etal-2012-developing,P07-2009,1,\N,Missing
basile-etal-2012-developing,P10-2013,0,\N,Missing
bos-2008-lets,kingsbury-palmer-2002-treebank,0,\N,Missing
bos-2008-lets,Y07-1002,0,\N,Missing
bos-2008-lets,J93-2004,0,\N,Missing
bos-2008-lets,C04-1180,1,\N,Missing
bos-2008-lets,W05-1206,0,\N,Missing
bos-2008-lets,P98-1013,0,\N,Missing
bos-2008-lets,C98-1013,0,\N,Missing
bos-2008-lets,P06-2091,0,\N,Missing
bos-2008-lets,W07-1401,0,\N,Missing
bos-2008-lets,W00-1317,0,\N,Missing
bos-2008-lets,W07-1400,0,\N,Missing
C02-1067,C02-1095,1,\N,Missing
C02-1095,N01-1030,0,\N,Missing
C02-1095,P01-1022,0,\N,Missing
C02-1095,C00-2097,0,\N,Missing
C04-1043,C88-1018,0,\N,Missing
C04-1180,briscoe-carroll-2002-robust,0,0.0181404,"Missing"
C04-1180,E99-1042,0,0.00750892,"Missing"
C04-1180,2003.mtsummit-papers.6,0,0.0315977,"Missing"
C04-1180,A00-2018,0,0.0299319,"Missing"
C04-1180,W03-1013,1,0.567339,"mma and returns a sentential modifier of the same type. Type-raising is applied to the categories NP, PP and S adj NP (adjectival phrase), and is implemented by adding the relevant set of type-raised categories to the chart whenever an NP, PP or S adj NP is present. The sets of type-raised categories are based on the most commonly used typeraising rule instantiations in sections 2-21 of CCGbank, and currently contain 8 type-raised categories for NP and 1 each for PP and S adj NP. For a given sentence, the automatically extracted grammar can produce a very large number of derivations. Clark and Curran (2003) and Clark and Curran (2004b) describe how a packed chart can be used to efficiently represent the derivation space, and also efficient algorithms for finding the most probable derivation. The parser uses a log-linear model over normal-form derivations.3 Features are defined in terms of the local trees in the derivation, including lexical head information and word-word dependencies. The normal-form derivations in CCGbank provide the gold standard training data. For a given sentence, the output of the parser is a set of syntactic dependencies corresponding to the 3 most probable derivation. How"
C04-1180,C04-1041,1,0.441407,"es are suppressed, since these are developed beWhile the proliferation of surface constituents allowed by CCG adds to derivational ambiguity (since the constituent taxpayers £15 million to install is also allowed in the non-coordinate sentence It could cost taxpayers £15 million to install), previous work has shown that standard techniques from the statistical parsing literature can be used for practical widecoverage parsing with state-of-the-art performance. 3 The Parser A number of statistical parsers have recently been developed for CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b; Clark and Curran, 2004b). All of these parsers use a grammar derived from CCGbank (Hockenmaier and Steedman, 2002a; Hockenmaier, 2003), a treebank of normalform CCG derivations derived semi-automatically from the Penn Treebank. In this paper we use the Clark and Curran (2004b) parser, which uses a loglinear model of normal-form derivations to select an analysis. The parser takes a POS tagged sentence as input with a set of lexical categories assigned to each word. A CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. The supertagger uses a log-linear model of the target word’s context to dec"
C04-1180,P04-1014,1,0.669575,"es are suppressed, since these are developed beWhile the proliferation of surface constituents allowed by CCG adds to derivational ambiguity (since the constituent taxpayers £15 million to install is also allowed in the non-coordinate sentence It could cost taxpayers £15 million to install), previous work has shown that standard techniques from the statistical parsing literature can be used for practical widecoverage parsing with state-of-the-art performance. 3 The Parser A number of statistical parsers have recently been developed for CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b; Clark and Curran, 2004b). All of these parsers use a grammar derived from CCGbank (Hockenmaier and Steedman, 2002a; Hockenmaier, 2003), a treebank of normalform CCG derivations derived semi-automatically from the Penn Treebank. In this paper we use the Clark and Curran (2004b) parser, which uses a loglinear model of normal-form derivations to select an analysis. The parser takes a POS tagged sentence as input with a set of lexical categories assigned to each word. A CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. The supertagger uses a log-linear model of the target word’s context to dec"
C04-1180,P02-1042,1,0.779112,"of the derivation and of the semantics of noun phrases are suppressed, since these are developed beWhile the proliferation of surface constituents allowed by CCG adds to derivational ambiguity (since the constituent taxpayers £15 million to install is also allowed in the non-coordinate sentence It could cost taxpayers £15 million to install), previous work has shown that standard techniques from the statistical parsing literature can be used for practical widecoverage parsing with state-of-the-art performance. 3 The Parser A number of statistical parsers have recently been developed for CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b; Clark and Curran, 2004b). All of these parsers use a grammar derived from CCGbank (Hockenmaier and Steedman, 2002a; Hockenmaier, 2003), a treebank of normalform CCG derivations derived semi-automatically from the Penn Treebank. In this paper we use the Clark and Curran (2004b) parser, which uses a loglinear model of normal-form derivations to select an analysis. The parser takes a POS tagged sentence as input with a set of lexical categories assigned to each word. A CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. The supertagger us"
C04-1180,hockenmaier-steedman-2002-acquiring,1,0.878485,"nd of the semantics of noun phrases are suppressed, since these are developed beWhile the proliferation of surface constituents allowed by CCG adds to derivational ambiguity (since the constituent taxpayers £15 million to install is also allowed in the non-coordinate sentence It could cost taxpayers £15 million to install), previous work has shown that standard techniques from the statistical parsing literature can be used for practical widecoverage parsing with state-of-the-art performance. 3 The Parser A number of statistical parsers have recently been developed for CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b; Clark and Curran, 2004b). All of these parsers use a grammar derived from CCGbank (Hockenmaier and Steedman, 2002a; Hockenmaier, 2003), a treebank of normalform CCG derivations derived semi-automatically from the Penn Treebank. In this paper we use the Clark and Curran (2004b) parser, which uses a loglinear model of normal-form derivations to select an analysis. The parser takes a POS tagged sentence as input with a set of lexical categories assigned to each word. A CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. The supertagger uses a log-linear model of the tar"
C04-1180,P02-1043,1,0.826238,"nd of the semantics of noun phrases are suppressed, since these are developed beWhile the proliferation of surface constituents allowed by CCG adds to derivational ambiguity (since the constituent taxpayers £15 million to install is also allowed in the non-coordinate sentence It could cost taxpayers £15 million to install), previous work has shown that standard techniques from the statistical parsing literature can be used for practical widecoverage parsing with state-of-the-art performance. 3 The Parser A number of statistical parsers have recently been developed for CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b; Clark and Curran, 2004b). All of these parsers use a grammar derived from CCGbank (Hockenmaier and Steedman, 2002a; Hockenmaier, 2003), a treebank of normalform CCG derivations derived semi-automatically from the Penn Treebank. In this paper we use the Clark and Curran (2004b) parser, which uses a loglinear model of normal-form derivations to select an analysis. The parser takes a POS tagged sentence as input with a set of lexical categories assigned to each word. A CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. The supertagger uses a log-linear model of the tar"
C04-1180,N04-1013,0,0.0413167,"Missing"
C04-1180,C02-1105,0,0.0414764,"Missing"
C04-1180,J03-4003,0,\N,Missing
C16-1056,Q16-1022,0,0.0497847,"Missing"
C16-1056,W08-2227,0,0.251231,"Missing"
C16-1056,D11-1039,0,0.0217385,"et al., 2007; Le and Zuidema, 2012; Lewis and Steedman, 2013) have wide coverage but require much syntactically annotated training data. Other approaches are restricted to relatively narrow linguistic domains but manage to do without strong syntactic supervision. Forms of supervision used include sentence/meaning representation pairs (Wong and Mooney, 2007; Zettlemoyer and Collins, 2007) and even weaker forms of supervision (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013; Goldwasser and Roth, 2011; Chen and Mooney, 2011; Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Artzi and Zettlemoyer, 2011; Poon, 2013). Only recently have approaches not relying on explicit syntactic supervision successfully been applied to This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 579 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 579–588, Osaka, Japan, December 11-17 2016. She likes to read books NP : S[dcl]/(S[to]NP ) : (S[to]NP )/(S[b]NP ) : (S[b]NP )/NP : NP : JsheK JlikesK JtoK JreadK JbooksK S[b]NP : JreadK@JbooksK S[to]NP : Jt"
C16-1056,D15-1198,0,0.0215566,"saka, Japan, December 11-17 2016. She likes to read books NP : S[dcl]/(S[to]NP ) : (S[to]NP )/(S[b]NP ) : (S[b]NP )/NP : NP : JsheK JlikesK JtoK JreadK JbooksK S[b]NP : JreadK@JbooksK S[to]NP : JtoK@(JreadK@JbooksK) S[dcl]NP : JlikesK@(JtoK@(JreadK@JbooksK)) S[dcl] : (JlikesK@(JtoK@(JreadK@JbooksK)))@JsheK &gt; &gt; &gt; &gt; Figure 1: Example CCG derivation. x1 p1 e1 x2 e2 female(x1) like.v.02(e1) Experiencer(e1, x1) Stimulus(e1, p1) p1: book.n.01(x2) read.v.01(e2) Agent(e2, x1) Theme(e2, x2) Figure 2: Example DRS for the sentence in Figure 1. more open-domain sentences (Vanderwende et al., 2015; Artzi et al., 2015). Ours is, to the best of our knowledge, the first such work using cross-lingual learning. Cross-lingual learning has previously been applied to different NLP tasks, notably part-of-speech tagging and dependency parsing. For dependency parsing, the task most similar to ours, three families of approaches can be distinguished. In annotation projection, existing annotations of source-language text are automatically projected to target-language translations in a parallel corpus; the result is used to train a target-language system (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015; Joha"
C16-1056,W13-2322,0,0.084852,"romising numbers compared to state-of-the-art semantic parsing in open domains. 1 Introduction Scarceness of manually annotated corpora for training dependency parsers has led researchers to explore more indirect forms of supervision, such as cross-lingual learning, where annotations in one language are used in training a system for another language. Semantic parsers, which map sentences directly to logically interpretable meaning representations, equally suffer from a lack of annotated training corpora, despite recent efforts like the Groningen Meaning Bank (Basile et al., 2012) or AMR Bank (Banarescu et al., 2013). The lack is especially pronounced for languages other than English. This paper aims to show that cross-lingual learning can help create semantic parsers for new languages with little knowledge about those languages and minimal human intervention. We present a method that takes an existing (source-language) semantic parser and parallel data and learns a semantic parser for the target language. Our method is in principle applicable to all parsers producing interpreted derivations (i.e., parse trees) of Combinatory Categorial Grammar (CCG; Steedman 2001) . It is independent of the concrete mean"
C16-1056,basile-etal-2012-developing,1,0.842288,"ource-language system. These are promising numbers compared to state-of-the-art semantic parsing in open domains. 1 Introduction Scarceness of manually annotated corpora for training dependency parsers has led researchers to explore more indirect forms of supervision, such as cross-lingual learning, where annotations in one language are used in training a system for another language. Semantic parsers, which map sentences directly to logically interpretable meaning representations, equally suffer from a lack of annotated training corpora, despite recent efforts like the Groningen Meaning Bank (Basile et al., 2012) or AMR Bank (Banarescu et al., 2013). The lack is especially pronounced for languages other than English. This paper aims to show that cross-lingual learning can help create semantic parsers for new languages with little knowledge about those languages and minimal human intervention. We present a method that takes an existing (source-language) semantic parser and parallel data and learns a semantic parser for the target language. Our method is in principle applicable to all parsers producing interpreted derivations (i.e., parse trees) of Combinatory Categorial Grammar (CCG; Steedman 2001) . I"
C16-1056,C10-1011,0,0.0192172,"s the function of cleaning the training set. Despite the pruning, for some sentences the search space is prohibitively large, so we restrict the size of the parser agenda to 256, a number that still allows us to run this step in reasonable time. If this limit is exceeded or if we do not find a complete derivation with the target interpretation, we discard the sentence. If we do find one or more—such as the one in Figure 5—the sentence becomes part of the training data for the following step. 4.3 Step 3: Parser Learning For statistical parsing, we use an averaged perceptron with a hash kernel (Bohnet, 2010) and the same feature templates as Zhang and Clark (2011), plus, for shift actions, a feature uniquely identifying a lexical item including the (multi)word, its part(s) of speech and the chosen category and interpretation. The parser uses the full global lexicon L. The same grammar as in derivation projection is used. The parser uses beam search. If at some point during training on one example there is no item on the beam anymore that could lead to one of the “correct” derivations found in derivation projection, the parser aborts training on this example and performs an early perceptron update"
C16-1056,P13-1133,0,0.102684,"Missing"
C16-1056,P13-2131,0,0.126899,"Missing"
C16-1056,J07-4004,0,0.0345097,"The translation approach pioneered by Tiedemann et al. (2014) is similar to annotation projection, but instead of relying on existing translations, it automatically translates the data and synchronously projects annotations to the translation result. Our approach falls within the annotation projection family, with the new challenge that entire CCG derivations with logical interpretations need to be transferred. 3 Combinatory Categorial Grammar Combinatory Categorial Grammar (Steedman, 2001) is a grammar formalism widely used for semantic parsing due to its suitability to statistical parsing (Clark and Curran, 2007) and its transparent syntaxsemantics interface. Every constituent has a category—either a basic one (S for sentence, N for noun, NP for noun phrase, PP for prepositional argument) or a functional one such as SNP for verb phrase, indicating that a constituent can combine with a noun phrase to its left to yield a sentence. Smaller constituents are combined into larger ones according to a handful of combinatorial rules such as application and composition. Every constituent also has a semantics, its interpretation, which is a term of the lambda calculus. Crucially, the combinatorial rules specify"
C16-1056,W10-2903,0,0.0312902,"k Semantic parsing has been tackled from a wide variety of angles. Systems that add a semantic interpretation component to an existing supervised syntactic parser (Curran et al., 2007; Le and Zuidema, 2012; Lewis and Steedman, 2013) have wide coverage but require much syntactically annotated training data. Other approaches are restricted to relatively narrow linguistic domains but manage to do without strong syntactic supervision. Forms of supervision used include sentence/meaning representation pairs (Wong and Mooney, 2007; Zettlemoyer and Collins, 2007) and even weaker forms of supervision (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013; Goldwasser and Roth, 2011; Chen and Mooney, 2011; Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Artzi and Zettlemoyer, 2011; Poon, 2013). Only recently have approaches not relying on explicit syntactic supervision successfully been applied to This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 579 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 579–588, Osaka, Japan, December 1"
C16-1056,P04-1015,0,0.0217857,"nd the same feature templates as Zhang and Clark (2011), plus, for shift actions, a feature uniquely identifying a lexical item including the (multi)word, its part(s) of speech and the chosen category and interpretation. The parser uses the full global lexicon L. The same grammar as in derivation projection is used. The parser uses beam search. If at some point during training on one example there is no item on the beam anymore that could lead to one of the “correct” derivations found in derivation projection, the parser aborts training on this example and performs an early perceptron update (Collins and Roark, 2004). 5 Experimental Setup To ensure that derivation projection can find a large number of high-quality derivations, we need training data with a large proportion of “literally” translated sentences. By this we do not mean that the translation has to be syntactically isomorphic—our projection approach can actually deal with a wide range of such syntactic divergences (cf. Dorr, 1993), such as the likes to/graag example. But translations should not be informative or loose, as this changes their meaning. More literal translations than in freely occurring text can be found in resources aimed at human"
C16-1056,P07-2009,1,0.769262,"dent of the concrete meaning representation formalism used, as long as meaning representations are assembled in the standard CCG way using the lambda calculus. We evaluate our method by applying it to English as source language, Dutch as target language and Discourse Representation Theory (DRT; Kamp and Reyle 1993) as meaning representation formalism, and measuring the performance of the obtained Dutch semantic parser. 2 Related Work Semantic parsing has been tackled from a wide variety of angles. Systems that add a semantic interpretation component to an existing supervised syntactic parser (Curran et al., 2007; Le and Zuidema, 2012; Lewis and Steedman, 2013) have wide coverage but require much syntactically annotated training data. Other approaches are restricted to relatively narrow linguistic domains but manage to do without strong syntactic supervision. Forms of supervision used include sentence/meaning representation pairs (Wong and Mooney, 2007; Zettlemoyer and Collins, 2007) and even weaker forms of supervision (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013; Goldwasser and Roth, 2011; Chen and Mooney, 2011; Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Artzi and Ze"
C16-1056,P09-1042,0,0.104637,"rsing, the task most similar to ours, three families of approaches can be distinguished. In annotation projection, existing annotations of source-language text are automatically projected to target-language translations in a parallel corpus; the result is used to train a target-language system (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015; Johannsen et al., 2016; Agi´c et al., 2016). In model transfer, parsers for different languages share some of their model parameters, thereby using information from annotations in multiple languages at the same time. (Zeman and Resnik, 2008; Ganchev et al., 2009; McDonald et al., 2011; Naseem et al., 2012; T¨ackstr¨om et al., 2013). The translation approach pioneered by Tiedemann et al. (2014) is similar to annotation projection, but instead of relying on existing translations, it automatically translates the data and synchronously projects annotations to the translation result. Our approach falls within the annotation projection family, with the new challenge that entire CCG derivations with logical interpretations need to be transferred. 3 Combinatory Categorial Grammar Combinatory Categorial Grammar (Steedman, 2001) is a grammar formalism widely u"
C16-1056,P16-2091,0,0.0664781,"Missing"
C16-1056,D12-1069,0,0.0204757,"nt to an existing supervised syntactic parser (Curran et al., 2007; Le and Zuidema, 2012; Lewis and Steedman, 2013) have wide coverage but require much syntactically annotated training data. Other approaches are restricted to relatively narrow linguistic domains but manage to do without strong syntactic supervision. Forms of supervision used include sentence/meaning representation pairs (Wong and Mooney, 2007; Zettlemoyer and Collins, 2007) and even weaker forms of supervision (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013; Goldwasser and Roth, 2011; Chen and Mooney, 2011; Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Artzi and Zettlemoyer, 2011; Poon, 2013). Only recently have approaches not relying on explicit syntactic supervision successfully been applied to This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 579 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 579–588, Osaka, Japan, December 11-17 2016. She likes to read books NP : S[dcl]/(S[to]NP ) : (S[to]NP )/(S[b]NP ) : (S[b]NP )/NP : NP : JsheK JlikesK JtoK Jrea"
C16-1056,D10-1119,0,0.042752,"l incomplete (Zhu et al., 2013). We now describe the three steps in more detail. 4.1 Step 1: Category Projection Category projection assigns candidate categories and interpretations to target-language (multi)words in the training data. It thereby also induces the target-language lexicon that we use in subsequent steps. It serves as a cross-lingual alternative to the two traditional main strategies of inducing CCG lexicons for semantic parsing, namely hand-written, language-specific lexical templates (Zettlemoyer and Collins, 2005) and higher-order unification constrained by search heuristics (Kwiatkowksi et al., 2010). Category projection first word-aligns the training corpus—we use the n best alignments found by GIZA++ (Och and Ney, 2003) with default settings. The result is a large, noisy set of translation units. From each contiguous translation unit, we try to induce a candidate lexical item. Figure 4 shows an 581 NP : S[dcl]/(S[to]NP ) : (S[to]NP )/(S[b]NP ) : (S[b]NP )/NP : NP : JsheK JlikesK JtoK JreadK JbooksK She likes to read books Zij leest graag boeken NP : (S|NP )|NP : (S|NP )|(S|NP ) : NP : JsheK JreadK λx.JlikesK@(JtoK@x) JbooksK Figure 4: Category projection: word alignments induce cand"
C16-1056,D13-1161,0,0.0416758,"a wide variety of angles. Systems that add a semantic interpretation component to an existing supervised syntactic parser (Curran et al., 2007; Le and Zuidema, 2012; Lewis and Steedman, 2013) have wide coverage but require much syntactically annotated training data. Other approaches are restricted to relatively narrow linguistic domains but manage to do without strong syntactic supervision. Forms of supervision used include sentence/meaning representation pairs (Wong and Mooney, 2007; Zettlemoyer and Collins, 2007) and even weaker forms of supervision (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013; Goldwasser and Roth, 2011; Chen and Mooney, 2011; Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Artzi and Zettlemoyer, 2011; Poon, 2013). Only recently have approaches not relying on explicit syntactic supervision successfully been applied to This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 579 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 579–588, Osaka, Japan, December 11-17 2016. She likes to read books NP : S[dcl]"
C16-1056,C12-1094,0,0.0186593,"representation formalism used, as long as meaning representations are assembled in the standard CCG way using the lambda calculus. We evaluate our method by applying it to English as source language, Dutch as target language and Discourse Representation Theory (DRT; Kamp and Reyle 1993) as meaning representation formalism, and measuring the performance of the obtained Dutch semantic parser. 2 Related Work Semantic parsing has been tackled from a wide variety of angles. Systems that add a semantic interpretation component to an existing supervised syntactic parser (Curran et al., 2007; Le and Zuidema, 2012; Lewis and Steedman, 2013) have wide coverage but require much syntactically annotated training data. Other approaches are restricted to relatively narrow linguistic domains but manage to do without strong syntactic supervision. Forms of supervision used include sentence/meaning representation pairs (Wong and Mooney, 2007; Zettlemoyer and Collins, 2007) and even weaker forms of supervision (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013; Goldwasser and Roth, 2011; Chen and Mooney, 2011; Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Artzi and Zettlemoyer, 2011; Poon,"
C16-1056,Q13-1015,0,0.0252575,"formalism used, as long as meaning representations are assembled in the standard CCG way using the lambda calculus. We evaluate our method by applying it to English as source language, Dutch as target language and Discourse Representation Theory (DRT; Kamp and Reyle 1993) as meaning representation formalism, and measuring the performance of the obtained Dutch semantic parser. 2 Related Work Semantic parsing has been tackled from a wide variety of angles. Systems that add a semantic interpretation component to an existing supervised syntactic parser (Curran et al., 2007; Le and Zuidema, 2012; Lewis and Steedman, 2013) have wide coverage but require much syntactically annotated training data. Other approaches are restricted to relatively narrow linguistic domains but manage to do without strong syntactic supervision. Forms of supervision used include sentence/meaning representation pairs (Wong and Mooney, 2007; Zettlemoyer and Collins, 2007) and even weaker forms of supervision (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013; Goldwasser and Roth, 2011; Chen and Mooney, 2011; Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Artzi and Zettlemoyer, 2011; Poon, 2013). Only recently have"
C16-1056,P11-1060,0,0.0296257,"s been tackled from a wide variety of angles. Systems that add a semantic interpretation component to an existing supervised syntactic parser (Curran et al., 2007; Le and Zuidema, 2012; Lewis and Steedman, 2013) have wide coverage but require much syntactically annotated training data. Other approaches are restricted to relatively narrow linguistic domains but manage to do without strong syntactic supervision. Forms of supervision used include sentence/meaning representation pairs (Wong and Mooney, 2007; Zettlemoyer and Collins, 2007) and even weaker forms of supervision (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013; Goldwasser and Roth, 2011; Chen and Mooney, 2011; Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Artzi and Zettlemoyer, 2011; Poon, 2013). Only recently have approaches not relying on explicit syntactic supervision successfully been applied to This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 579 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 579–588, Osaka, Japan, December 11-17 2016. She likes"
C16-1056,S16-1166,0,0.042968,"Missing"
C16-1056,D11-1006,0,0.0649475,"imilar to ours, three families of approaches can be distinguished. In annotation projection, existing annotations of source-language text are automatically projected to target-language translations in a parallel corpus; the result is used to train a target-language system (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015; Johannsen et al., 2016; Agi´c et al., 2016). In model transfer, parsers for different languages share some of their model parameters, thereby using information from annotations in multiple languages at the same time. (Zeman and Resnik, 2008; Ganchev et al., 2009; McDonald et al., 2011; Naseem et al., 2012; T¨ackstr¨om et al., 2013). The translation approach pioneered by Tiedemann et al. (2014) is similar to annotation projection, but instead of relying on existing translations, it automatically translates the data and synchronously projects annotations to the translation result. Our approach falls within the annotation projection family, with the new challenge that entire CCG derivations with logical interpretations need to be transferred. 3 Combinatory Categorial Grammar Combinatory Categorial Grammar (Steedman, 2001) is a grammar formalism widely used for semantic parsin"
C16-1056,P12-1066,0,0.0348972,"amilies of approaches can be distinguished. In annotation projection, existing annotations of source-language text are automatically projected to target-language translations in a parallel corpus; the result is used to train a target-language system (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015; Johannsen et al., 2016; Agi´c et al., 2016). In model transfer, parsers for different languages share some of their model parameters, thereby using information from annotations in multiple languages at the same time. (Zeman and Resnik, 2008; Ganchev et al., 2009; McDonald et al., 2011; Naseem et al., 2012; T¨ackstr¨om et al., 2013). The translation approach pioneered by Tiedemann et al. (2014) is similar to annotation projection, but instead of relying on existing translations, it automatically translates the data and synchronously projects annotations to the translation result. Our approach falls within the annotation projection family, with the new challenge that entire CCG derivations with logical interpretations need to be transferred. 3 Combinatory Categorial Grammar Combinatory Categorial Grammar (Steedman, 2001) is a grammar formalism widely used for semantic parsing due to its suitabil"
C16-1056,L16-1262,0,0.0539905,"Missing"
C16-1056,J03-1002,0,0.00602672,"n assigns candidate categories and interpretations to target-language (multi)words in the training data. It thereby also induces the target-language lexicon that we use in subsequent steps. It serves as a cross-lingual alternative to the two traditional main strategies of inducing CCG lexicons for semantic parsing, namely hand-written, language-specific lexical templates (Zettlemoyer and Collins, 2005) and higher-order unification constrained by search heuristics (Kwiatkowksi et al., 2010). Category projection first word-aligns the training corpus—we use the n best alignments found by GIZA++ (Och and Ney, 2003) with default settings. The result is a large, noisy set of translation units. From each contiguous translation unit, we try to induce a candidate lexical item. Figure 4 shows an 581 NP : S[dcl]/(S[to]NP ) : (S[to]NP )/(S[b]NP ) : (S[b]NP )/NP : NP : JsheK JlikesK JtoK JreadK JbooksK She likes to read books Zij leest graag boeken NP : (S|NP )|NP : (S|NP )|(S|NP ) : NP : JsheK JreadK λx.JlikesK@(JtoK@x) JbooksK Figure 4: Category projection: word alignments induce candidate categories and interpretations for target-language words. example sentence pair: at the top there are the lexical item"
C16-1056,P13-1092,0,0.0174935,"2012; Lewis and Steedman, 2013) have wide coverage but require much syntactically annotated training data. Other approaches are restricted to relatively narrow linguistic domains but manage to do without strong syntactic supervision. Forms of supervision used include sentence/meaning representation pairs (Wong and Mooney, 2007; Zettlemoyer and Collins, 2007) and even weaker forms of supervision (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013; Goldwasser and Roth, 2011; Chen and Mooney, 2011; Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Artzi and Zettlemoyer, 2011; Poon, 2013). Only recently have approaches not relying on explicit syntactic supervision successfully been applied to This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 579 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 579–588, Osaka, Japan, December 11-17 2016. She likes to read books NP : S[dcl]/(S[to]NP ) : (S[to]NP )/(S[b]NP ) : (S[b]NP )/NP : NP : JsheK JlikesK JtoK JreadK JbooksK S[b]NP : JreadK@JbooksK S[to]NP : JtoK@(JreadK@Jb"
C16-1056,D15-1039,0,0.0251143,"al., 2015; Artzi et al., 2015). Ours is, to the best of our knowledge, the first such work using cross-lingual learning. Cross-lingual learning has previously been applied to different NLP tasks, notably part-of-speech tagging and dependency parsing. For dependency parsing, the task most similar to ours, three families of approaches can be distinguished. In annotation projection, existing annotations of source-language text are automatically projected to target-language translations in a parallel corpus; the result is used to train a target-language system (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015; Johannsen et al., 2016; Agi´c et al., 2016). In model transfer, parsers for different languages share some of their model parameters, thereby using information from annotations in multiple languages at the same time. (Zeman and Resnik, 2008; Ganchev et al., 2009; McDonald et al., 2011; Naseem et al., 2012; T¨ackstr¨om et al., 2013). The translation approach pioneered by Tiedemann et al. (2014) is similar to annotation projection, but instead of relying on existing translations, it automatically translates the data and synchronously projects annotations to the translation result. Our approach"
C16-1056,Q14-1030,0,0.0430621,"ctic parser (Curran et al., 2007; Le and Zuidema, 2012; Lewis and Steedman, 2013) have wide coverage but require much syntactically annotated training data. Other approaches are restricted to relatively narrow linguistic domains but manage to do without strong syntactic supervision. Forms of supervision used include sentence/meaning representation pairs (Wong and Mooney, 2007; Zettlemoyer and Collins, 2007) and even weaker forms of supervision (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013; Goldwasser and Roth, 2011; Chen and Mooney, 2011; Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Artzi and Zettlemoyer, 2011; Poon, 2013). Only recently have approaches not relying on explicit syntactic supervision successfully been applied to This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 579 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 579–588, Osaka, Japan, December 11-17 2016. She likes to read books NP : S[dcl]/(S[to]NP ) : (S[to]NP )/(S[b]NP ) : (S[b]NP )/NP : NP : JsheK JlikesK JtoK JreadK JbooksK S[b]NP :"
C16-1056,N13-1126,0,0.0785464,"Missing"
C16-1056,W14-1614,0,0.0733132,"Missing"
C16-1056,C14-1175,0,0.0277323,"s (Vanderwende et al., 2015; Artzi et al., 2015). Ours is, to the best of our knowledge, the first such work using cross-lingual learning. Cross-lingual learning has previously been applied to different NLP tasks, notably part-of-speech tagging and dependency parsing. For dependency parsing, the task most similar to ours, three families of approaches can be distinguished. In annotation projection, existing annotations of source-language text are automatically projected to target-language translations in a parallel corpus; the result is used to train a target-language system (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015; Johannsen et al., 2016; Agi´c et al., 2016). In model transfer, parsers for different languages share some of their model parameters, thereby using information from annotations in multiple languages at the same time. (Zeman and Resnik, 2008; Ganchev et al., 2009; McDonald et al., 2011; Naseem et al., 2012; T¨ackstr¨om et al., 2013). The translation approach pioneered by Tiedemann et al. (2014) is similar to annotation projection, but instead of relying on existing translations, it automatically translates the data and synchronously projects annotations to the trans"
C16-1056,P07-1121,0,0.0185599,"ormalism, and measuring the performance of the obtained Dutch semantic parser. 2 Related Work Semantic parsing has been tackled from a wide variety of angles. Systems that add a semantic interpretation component to an existing supervised syntactic parser (Curran et al., 2007; Le and Zuidema, 2012; Lewis and Steedman, 2013) have wide coverage but require much syntactically annotated training data. Other approaches are restricted to relatively narrow linguistic domains but manage to do without strong syntactic supervision. Forms of supervision used include sentence/meaning representation pairs (Wong and Mooney, 2007; Zettlemoyer and Collins, 2007) and even weaker forms of supervision (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013; Goldwasser and Roth, 2011; Chen and Mooney, 2011; Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Artzi and Zettlemoyer, 2011; Poon, 2013). Only recently have approaches not relying on explicit syntactic supervision successfully been applied to This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 579 Proceedings of COLING 2016, the 26th International Conferen"
C16-1056,I08-3008,0,0.137851,"rsing. For dependency parsing, the task most similar to ours, three families of approaches can be distinguished. In annotation projection, existing annotations of source-language text are automatically projected to target-language translations in a parallel corpus; the result is used to train a target-language system (Hwa et al., 2005; Tiedemann, 2014; Rasooli and Collins, 2015; Johannsen et al., 2016; Agi´c et al., 2016). In model transfer, parsers for different languages share some of their model parameters, thereby using information from annotations in multiple languages at the same time. (Zeman and Resnik, 2008; Ganchev et al., 2009; McDonald et al., 2011; Naseem et al., 2012; T¨ackstr¨om et al., 2013). The translation approach pioneered by Tiedemann et al. (2014) is similar to annotation projection, but instead of relying on existing translations, it automatically translates the data and synchronously projects annotations to the translation result. Our approach falls within the annotation projection family, with the new challenge that entire CCG derivations with logical interpretations need to be transferred. 3 Combinatory Categorial Grammar Combinatory Categorial Grammar (Steedman, 2001) is a gram"
C16-1056,D07-1071,0,0.061001,"the performance of the obtained Dutch semantic parser. 2 Related Work Semantic parsing has been tackled from a wide variety of angles. Systems that add a semantic interpretation component to an existing supervised syntactic parser (Curran et al., 2007; Le and Zuidema, 2012; Lewis and Steedman, 2013) have wide coverage but require much syntactically annotated training data. Other approaches are restricted to relatively narrow linguistic domains but manage to do without strong syntactic supervision. Forms of supervision used include sentence/meaning representation pairs (Wong and Mooney, 2007; Zettlemoyer and Collins, 2007) and even weaker forms of supervision (Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013; Goldwasser and Roth, 2011; Chen and Mooney, 2011; Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Artzi and Zettlemoyer, 2011; Poon, 2013). Only recently have approaches not relying on explicit syntactic supervision successfully been applied to This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 579 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics:"
C16-1056,P11-1069,0,0.161557,"same interpretation— thus we can train the target-language parser to produce the same interpretations as the source-language parser. To this end, we try to find target-language derivations resulting in the same interpretations as the source-language ones, based on the target-language candidate lexical items found in category projection. We call this second step derivation projection. The derivations thus found are used to train a statistical parsing model for the target language. We call this third step parser learning. All three steps make use of a shift-reduce CCG parser similar to that of Zhang and Clark (2011). Parse actions are SHIFT-C-I, COMBINE-C, UNARY-C (where C is the category placed on top of the stack by shifting or applying a binary/unary rule and I is the interpretation of the (multi)word placed on top of the stack by shifting), SKIP for skipping words as semantically empty, FINISH for marking a parse as complete and IDLE for keeping complete parses on the agenda while others are still incomplete (Zhu et al., 2013). We now describe the three steps in more detail. 4.1 Step 1: Category Projection Category projection assigns candidate categories and interpretations to target-language (multi)"
C16-1056,N15-1162,0,0.0129865,"tems assigned to target-language words in category projection give rise to a space of possible CCG derivations. The space is large and noisy, partly because of the pervasive syntactic ambiguity of natural language, partly because we use more than one word alignment in category projection. In derivation projection, the task is to filter out only the “correct” derivations so we can then train on these. We regard as “correct” any derivation that results in the same interpretation for the whole sentence as the source-language derivation. For finding the “correct” derivations, we use the method of Zhao and Huang (2015) of running the parser in forced decoding mode: we use a beam of unlimited width but prune away parse items where, based on their interpretations, we can rule out that they could lead to a “correct” derivation. For instance, in our example, an item with interpretation JreadK@JsheK would be pruned because it cannot be part of (JlikesK@(JtoK@(JreadK@JbooksK)))@JsheK. To make this check tractable, we treat English lexical interpretations such as JreadK as atomic. The forced decoding uses a local lexicon, using only lexical items induced from the same sentence pair. 582 Zij leest graag boeken NP :"
C16-1056,P13-1043,0,0.0265983,"rain a statistical parsing model for the target language. We call this third step parser learning. All three steps make use of a shift-reduce CCG parser similar to that of Zhang and Clark (2011). Parse actions are SHIFT-C-I, COMBINE-C, UNARY-C (where C is the category placed on top of the stack by shifting or applying a binary/unary rule and I is the interpretation of the (multi)word placed on top of the stack by shifting), SKIP for skipping words as semantically empty, FINISH for marking a parse as complete and IDLE for keeping complete parses on the agenda while others are still incomplete (Zhu et al., 2013). We now describe the three steps in more detail. 4.1 Step 1: Category Projection Category projection assigns candidate categories and interpretations to target-language (multi)words in the training data. It thereby also induces the target-language lexicon that we use in subsequent steps. It serves as a cross-lingual alternative to the two traditional main strategies of inducing CCG lexicons for semantic parsing, namely hand-written, language-specific lexical templates (Zettlemoyer and Collins, 2005) and higher-order unification constrained by search heuristics (Kwiatkowksi et al., 2010). Cate"
C16-1056,N15-3006,0,\N,Missing
C16-1333,W13-3520,0,0.0122335,"e code is available at https://github.com/bjerva/semantic-tagging. We represent each sentence using both a character-based representation (Sc ) and a word-based representation (Sw ). The character-based representation is a 3-dimensional matrix Sc ∈ Rs×w×dc , where s is the zero-padded sentence length, w is the zero-padded word length, and dc is the dimensionality of the character embeddings. The word-based representation is a 2-dimensional matrix Sw ∈ Rs×dw , where s is the zero-padded sentence length and dw is the dimensionality of the word embeddings. We use the English Polyglot embeddings (Al-Rfou et al., 2013) in order to initialise the word embedding layer, but also experiment with randomly initialised word embeddings. Word embeddings are passed directly into a two-layer bi-GRU (Chung et al., 2014). We also experimented using a bi-LSTM. However, we found GRUs to yield comparatively better validation data performance on semtags. We also observe better validation data performance when running two consecutive forward and backward passes before concatenating the GRU layers, rather than concatenating after each forward/backward pass as is commonplace in NLP literature. 3535 We use CNNs for character-le"
C16-1333,P14-1133,0,0.0284775,"Missing"
C16-1333,W16-4816,1,0.900908,"n’ information path in the network facilitates optimisation (He et al., 2016). ResNets have recently shown state-of-the-art performance for image classification tasks (He et al., 2015; He et al., 2016), and have This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 3531 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3531–3541, Osaka, Japan, December 11-17 2016. ¨ also seen some recent use in NLP (Ostling, 2016; Conneau et al., 2016; Bjerva, 2016; Wu et al., 2016). However, no previous work has attempted to apply ResNets to NLP tagging tasks. To answer our second question, we carry out an extrinsic evaluation exercise. We investigate the effect of using semantic tags as an auxiliary loss for POS tagging. Since POS tags are useful for many NLP tasks, it follows that semantic tags must be useful if they can improve POS tagging. 2 2.1 Semantic Tagging Background We refer to semantic tagging, or sem-tagging, as the task of assigning semantic class categories to the smallest meaningful units in a sentence. In the context of this paper thes"
C16-1333,W08-2222,1,0.211215,"bank (PTB) Part-of-Speech tagset (Marcus et al., 1993) does not make the necessary semantic distinctions, in addition to containing redundant information for semantic processing. Let us consider a couple of examples. There are significant differences in meaning between the determiners every (universal quantification), no (negation), and some (existential quantification), but they all receive the DT (determiner) POS label in PTB. Since determiners form a closed class, one could enumerate all word forms for each class. Indeed some recent implementations of semantic parsing follow this strategy (Bos, 2008; Butler, 2010). This might work for a single language, but it falls short when considering a multilingual setting. Furthermore, determiners like any can have several interpretations and need to be disambiguated in context. Semantic tagging does not only apply to determiners, but reaches all parts of speech. Other examples where semantic classes disambiguate are reflexive versus emphasising pronouns (both POS-tagged as PRP, personal pronoun); the comma, that could be a conjunction, disjunction, or apposition; intersective vs. subsective and privative adjectives (all POS-tagged as JJ, adjective"
C16-1333,A00-1031,0,0.0508761,"ing independent variables in our experiments: 1. character and word representations (w, ~ ~c); 2. residual bypass for character representations (~cbp ); 3. convolutional representations (Basic CNN and ResNets); 4. auxiliary loss (using coarse semtags on ST and fine semtags on UD). 3536 We compare our results to four baselines: 1. the most frequent baseline per word (MFC), where we assign the most frequent tag for a word in the training data to that word in the test data, and unseen words get the global majority tag; 2. the trigram statistic based TNT tagger offers a slightly tougher baseline (Brants, 2000); 3. the B I - LSTM baseline, running the off-the-shelf state-of-the-art POS tagger for the UD dataset (Plank et al., 2016) (using default parameters with pre-trained Polyglot embeddings); 4. we use a baseline consisting of running our own system with only a B I - GRU using word representations (w), ~ with pre-trained Polyglot embeddings. 4.1 Experiments on semantic tagging We evaluate our system on two semantic tagging (ST) datasets: our silver semtag dataset and our gold semtag dataset. For the +AUX condition we use coarse semtags as an auxiliary loss. Results from these experiments are show"
C16-1333,D15-1085,0,0.0325507,"in a space with dimensionality dc as an image of dimensionality n × dc . This view gives us additional freedom in terms of sizes of convolutional patches used, which offers more computational flexibility than using only, e.g., 4 × dc convolutions. This view is applied to all CNN variations explored in this work. A neural network is trained with respect to some loss function, such as the cross-entropy between the predicted tag probability distribution and the gold probability distribution. Recent work has shown that the addition of an auxiliary loss function can be beneficial to several tasks. Cheng et al. (2015) use a language modelling task as an auxiliary loss, as they attempt to predict the next token while performing named entity recognition. Plank et al. (2016) use the log frequency of the current token as an auxiliary loss function, and find this to improve POS tagging accuracy. Since our semantic tagging task is based on predicting fine semtags, which can be mapped to coarse semtags, we add the prediction of these coarse semtags as an auxiliary loss for the sem-tagging experiments. Similarly, we also experiment with POS tagging, where we use the fine semtags as an auxiliary information. 3.4.1"
C16-1333,P16-1160,0,0.0190157,"ork is the first to apply ResNets to NLP sequence tagging tasks. We further contribute to the literature on ResNets by introducing a residual bypass function. The intuition is to combine both deep and shallow processing, which opens a path of easy signal propagation between lower and higher layers in the network. 3.3 Modelling character information and residual bypass Using sub-token representations instead of, or in combination with, word-level representations has recently obtained a lot of attention due to their effectiveness (Sutskever et al., 2011; Chrupała, 2013; 3534 Zhang et al., 2015; Chung et al., 2016; Gillick et al., 2015). The use of sub-token representations can be approached in several ways. Plank et al. (2016) and Yang et al. (2016) use a hierarchical bidirectional RNN, first passing over characters in order to create word-level representations. Gillick et al. (2015) similarly apply an LSTM-based model using byte-level information directly. Dos Santos and Zadrozny (2014) construct character-based word-level representations by running a convolutional network over the character representations of each word. All of these approaches have in common that the character-based representation i"
C16-1333,D13-1146,1,0.363033,"matically obtained with the C&C tools (Curran et al., 2007) and then manually corrected), as well as a set of manually crafted rules to output semantic tags. Some tags related to specific phenomena were hand-corrected in a second stage. Our second dataset is smaller but equipped with gold standard semantic tags and used for testing (PMB, the Parallel Meaning Bank). It comprises a selection of 400 sentences of the English part of a parallel corpus. It has no overlap with the GMB corpus. For this dataset, we used the Elephant tokeniser, which performs word, multi-word and sentence segmentation (Evang et al., 2013). We then used the 3532 ANA PRO DEF HAS REF EMP ACT GRE ITJ HES QUE ATT QUA UOM IST REL RLI SST PRI INT SCO LOG ALT EXC NIL DIS IMP AND BUT pronoun definite possessive reflexive emphasizing greeting interjection hesitation interrogative quantity measurement intersective relation rel. inv. scope subsective privative intensifier score alternative exclusive empty disjunct./exist. implication conjunct./univ. contrast COM EQA MOR LES TOP BOT ORD DEM PRX MED DST DIS SUB COO APP MOD NOT NEC POS ENT CON ROL NAM GPE PER LOC ORG ART NAT HAP URL equative comparative pos. comparative neg. pos. superlative"
C16-1333,J93-2004,0,0.0565979,". Since POS tags are useful for many NLP tasks, it follows that semantic tags must be useful if they can improve POS tagging. 2 2.1 Semantic Tagging Background We refer to semantic tagging, or sem-tagging, as the task of assigning semantic class categories to the smallest meaningful units in a sentence. In the context of this paper these units can be morphemes, words, punctuation, or multi-word expressions. The linguistic information traditionally obtained for deep processing is insufficient for fine-grained lexical semantic analysis. The widely used Penn Treebank (PTB) Part-of-Speech tagset (Marcus et al., 1993) does not make the necessary semantic distinctions, in addition to containing redundant information for semantic processing. Let us consider a couple of examples. There are significant differences in meaning between the determiners every (universal quantification), no (negation), and some (existential quantification), but they all receive the DT (determiner) POS label in PTB. Since determiners form a closed class, one could enumerate all word forms for each class. Indeed some recent implementations of semantic parsing follow this strategy (Bos, 2008; Butler, 2010). This might work for a single"
C16-1333,W16-2003,0,0.135016,"eeper networks, since keeping a ‘clean’ information path in the network facilitates optimisation (He et al., 2016). ResNets have recently shown state-of-the-art performance for image classification tasks (He et al., 2015; He et al., 2016), and have This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 3531 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3531–3541, Osaka, Japan, December 11-17 2016. ¨ also seen some recent use in NLP (Ostling, 2016; Conneau et al., 2016; Bjerva, 2016; Wu et al., 2016). However, no previous work has attempted to apply ResNets to NLP tagging tasks. To answer our second question, we carry out an extrinsic evaluation exercise. We investigate the effect of using semantic tags as an auxiliary loss for POS tagging. Since POS tags are useful for many NLP tasks, it follows that semantic tags must be useful if they can improve POS tagging. 2 2.1 Semantic Tagging Background We refer to semantic tagging, or sem-tagging, as the task of assigning semantic class categories to the smallest meaningful units in a sentenc"
C16-1333,P16-2067,1,0.925084,"cope with longer input sequences than vanilla RNNs. GRUs are similar to the more commonly-used Long Short-Term Memory networks (LSTMs), both in purpose and implementation (Chung et al., 2014). A bi-directional GRU is a GRU which makes both forward and backward passes over sequences, and can therefore use both preceding and succeeding contexts to predict a tag (Graves and Schmidhuber, 2005; Goldberg, 2015). Bi-directional GRUs and LSTMs have been shown to yield high performance on several NLP tasks, such as POS tagging, named entity tagging, and chunking (Wang et al., 2015; Yang et al., 2016; Plank et al., 2016). We build on previous approaches by combining bi-GRUs with character representations from a basic CNN and ResNets. 3533 Figure 1: Model architecture. Left: Architecture with basic CNN char representations (~c), Middle: basic CNN with char and word representations and bypass (~cbp ∧ w), ~ Right: ResNet with auxiliary loss and residual bypass (+AUXbp ). 3.2 Deep Residual Networks Deep Residual Networks (ResNets) are built up by stacking residual units. A residual unit can be expressed as: yl = h(xl ) + F(xl , Wl ), (3) xl+1 = f (yl ), where xl and xl+1 are the input and output of the l-th layer"
C16-1333,N04-1030,0,0.0174103,"Missing"
C16-1333,P07-2009,1,\N,Missing
C16-1333,L16-1262,0,\N,Missing
C94-2193,E93-1025,0,0.0642654,"Missing"
C94-2193,E93-1018,0,0.023317,"Missing"
C94-2193,J86-3001,0,\N,Missing
C96-1024,P91-1021,1,0.751083,"Missing"
C96-1024,C92-1017,0,0.0762618,"onotonic representation language for compositional semantics as discussed in (Alshawi and Crouch, 1992). The QLF formalism incorporates a Davidsonian approach to semantics, containing underspecified quantifiers and operators, as well as 'anaphoric terms' which stand for entities and relations to be determined by reference resolution. In these respects, the basic ideas of the QLF formalism are quite similar to LUD. 5 5.1 Syntax-Semantics Implementation Interface and Grammar The LUD semantic construction component has been implemented in the grammar formalism TUG, Trace and Unification Grammar (Block and Schachtl, 1992), in a system called TrUG (in cooperation with Siemens AG, Munich, who provided the German syntax and the TrUG system). TUG is a formalism that combines ideas from Government and Binding theory, namely the use of traces, with unification in order to account for, for example, the free word order phenomena found in German. 5.1.1 S y n t a x and S e m a n t i c s A TUG grammar basically consists of PATR-II style context free rules with feature annotations. Each syntactic rule gets annotated with a semantic counterpart. In this way, syntactic derivation and semantic construction are fully interlea"
C96-1024,1995.tmi-1.2,0,0.334937,"Missing"
C96-1024,1993.mtsummit-1.11,0,0.0813897,"he interpretation of a category on the right side of a rule subsumes the interpretation of the left side of the nile. lar tasks. The actual implementation is described in Section 5, which also discusses coverage and points to some areas of further research. Finally, Section 6 sums up the previous discussion. 2 The Verbmobil Project The project Verbmobil funded by the German Federal Ministry of Research and Technology (BMBF) combines speech technology with machine translation techniques in order to develop a system for translation in face-to-face dialogues. The overall project is described in (Wahlster, 1993); in this section we will give a short overview of the key aspects. The ambitious overall objective of the Verbmobil project is to produce a device which will provide English translations of dialogues between German and Japanese businessmen who only have a restricted active, but larger passive knowledge of English. The domain is the scheduling of business appointments. The major requirement is to provide translations as and when users need them, and do so robustly and in (near) real-time. In order to achieve this, the system is composed of time-limited processing components which on the source"
C96-1024,P92-1005,0,\N,Missing
C98-1024,P91-1021,0,0.207041,"ndividual data items. This property has been presented as an independent motivation for minilnally recursive representations from the Machine 'Danslation point of view (Copestake et al., 1995), and has been most thoroughly explored in the context of the substitution operations required for transfer. We believe we have taken this argument to its logical conclusion, in implementing a non-recursive selnantic metalanguage in an appropriate data structure. This, in itself, provides sufficient motiw~tion for opting for such representations rather than, say, feature structures or the recursive QLFs (Alshawi et al., 1991) of CLE (Alshawi, 1992). 3.2 ADT Package In general, linguistic analysis components are very sensitive to changes in input data caused by modifi4in typical AI languages, such as Lisp and Prolog, lists are built-in, and they can be ported easily to other programming languages. 165 cations of analyses or by increasing coverage. Obviously, there is a need for some kind of robusthess at the interface level, especially in large distributed software projects like VerbmoDil with parallel development of different components. Therefore, components that communicate with each other should abstract over d"
C98-1024,C96-1008,0,0.0283127,"and cyclicity. As far as we are aware, tiffs is the first time that the results of linguistic colnponents dealing with semantics can be systematically checked at module interfaces. It has been shown that this form of testing is well-suited lbr error detection in components with rapidly growing linguistic coverage. It is worth noting that the source language lexical coverage in the Verbmobil Research Prototype is around 2500 words, rising to 1OK at the end of the second phase a. Furthermore, the complex information produced by 5The kind of data structure used by the communication architecture (Amtrup and Benra, 1996) is, simihuly, transparent to the Inodules. aln the year 2000. linguistic components even makes automatic output control necessary. The same checking can be used to define a quality rating, e.g. for correctness, interpretability, etc. of the content of a VIT. Such results are much better and more productive in improving a system than common, purely quantitative, measures based on failure or success rates. 4 Conclusion We have described the interface terms used to carry linguistic information in the Verbmobil system. The amount and type of information that they carry are adapted to the various"
C98-1024,C96-1024,1,0.857467,"uage. In UDRSs quantifier dependencies and other scope information are underspecified because the constraints provide incomplete information about the assignment of object language structures to labels. However, a constraint set nmy be monotonically extended to provide a complete resolution. VYF semantics follows a similar strategy but somewhat extends the cxpressivity of the metalanguage. There are two constructs in the VIT semantic metalanguage which provkte for an cxtension in expressivity relative to UDRSs. These have both been adopted from immediate precursors within the project, such as Bos et al. (1996), and further refined. The first of these is the mechanism of holes and plugging wlaich originates in the hole semantics of Bos (1996). This requires a distinction between two types of metawlriable employed: labels and holes. Labels denote the instantiated structures, primarily the individual predicates. Holes, on the other hand, mark the underspecified argument positions of propositional arguments and scope domains. Resolution consists of an assignment of la3WE o w e the term minimal rectowion to Copestakc et al. (1995), hut tile mechanism they describe was already in use in UDRSs. 163 vit( S"
C98-1024,1995.tmi-1.2,0,0.0453822,"different slots according to its linguistic nature (see Section 3.2 below) or the producer of the specific kind of data (see Section 2.1). The kind of structuring adopted and the relative shortness of the lists make for rapid access to and operations on VITs. It is significant that efficient information access is dependent not only on an appropriate data structure, but also on the representational formalism implemented in the individual data items. This property has been presented as an independent motivation for minilnally recursive representations from the Machine 'Danslation point of view (Copestake et al., 1995), and has been most thoroughly explored in the context of the substitution operations required for transfer. We believe we have taken this argument to its logical conclusion, in implementing a non-recursive selnantic metalanguage in an appropriate data structure. This, in itself, provides sufficient motiw~tion for opting for such representations rather than, say, feature structures or the recursive QLFs (Alshawi et al., 1991) of CLE (Alshawi, 1992). 3.2 ADT Package In general, linguistic analysis components are very sensitive to changes in input data caused by modifi4in typical AI languages, s"
C98-1024,C96-1054,1,0.803321,"theory independent lingua fi'anca for discussions which may involve both linguists and computer scientists. The Verbmobil commuuity is actually large enough to require such off-line constructs, too. 1.2 Encoding of Linguistic Information A key question in the design of an interface language is what information must be carried and to what purpose. The primary definition criterion within the linguistic modules has been the translation task. The actual translation operation is performed in the transfer module as a mapping between semantic representations of the source and target languages, see (Dorna and Emele, 1996). However, the information requirements of this process are flexible, since information from various levels of analysis are used in disambiguation within the transfer module, including prosody and dialogue structure. To a large extent the information requirements divide into two parts: • the expressive adequacy of the semantic representation; • representing other types of linguistic information so as to meet the disambiguation requirements with the minimum of redundancy. The design of the semantic representations encoded within VITs has been guided by an ongoing movement in representational se"
C98-1069,P91-1021,1,0.835158,"Missing"
C98-1069,P98-1024,1,0.788101,"of the sentential complements get plugged with their own sb-labels. This complicates the implementation of rules (9) and (10) a bit; they must also account for the fact that a daughter node may carry an i s l a n d type hole. 5 I m p l e m e n t a t i o n and E v a l u a t i o n The resolution algorithm described in Section 4 has been implemented in Verbmobil, a system which translates spoken German and Japanese into English (Bub et al., 1997). The underspecified semantic representation technique we have used in this paper reflects the core semantic part of the Verbmobil Interface Term, VIT (Bos et al., 1998). The aim of VIT is to describe a consistent interface structure between the different language analysis modules within Verbmobil. Thus, in contrast to our USR, VIT is a representation that encodes all the linguistic information of an utterance; in addition to the USR semantic structure of Sectiom 2, the Verbmobil Interface Term contains prosodic, syntactic, and discourse related information. In order to evaluate the algorithm, the results of the pluggings obtained for four dialogues in the Verbmobil test set were checked (Table 1). We only consider utterances for which the VITs contain more t"
C98-1069,Y96-1006,1,0.906882,"Missing"
C98-1069,C96-1024,1,\N,Missing
C98-1069,C98-1024,1,\N,Missing
D13-1146,basile-etal-2012-developing,1,0.726779,", as e.g. in did|n’t. An example is given in Figure 1. Table 1: Datasets characteristics. Name Language Domain Sentences Tokens GMB English Newswire 2,886 64,443 TNC Dutch Newswire 49,537 860,637 PAI Italian Web/various 42,674 869,095 The data was converted into IOB format by inferring an alignment between the raw text and the segmented text. 3.3 Figure 1: Example of IOB-labeled characters Datasets In our experiments we use three datasets to compare our method for different languages and for different domains: manually checked English newswire texts taken from the Groningen Meaning Bank, GMB (Basile et al., 2012), Dutch newswire texts, comprising two days from January 2000 extracted from the Twente News Corpus, TwNC (Ordelman et al., 1423 Sequence labeling We apply the Wapiti implementation (Lavergne et al., 2010) of Conditional Random Fields (Lafferty et al., 2001), using as features the output label of each character, combined with 1) the character itself, 2) the output label on the previous character, 3) characters and/or their Unicode categories from context windows of varying sizes. For example, with a context size of 3, in Figure 1, features for the E in Eighty-three with the output label S woul"
D13-1146,P12-2074,0,0.0190705,"kenization, there is still room for improvement, in particular on the methodological side of the task. We are particularly interested in the following questions: Can we use supervised learning to avoid hand-crafting rules? Can we use unsupervised feature learning to reduce feature engineering effort and boost performance? Can we use the same method across languages? Can we combine word and sentence boundary detection into one task? An Elephant in the Room Tokenization, the task of segmenting a text into words and sentences, is often regarded as a solved problem in natural language processing (Dridan and Oepen, 2012), probably because many corpora are already in tokenized format. But like an elephant in the living room, it is a problem that is impossible to overlook whenever new raw datasets need to be processed or when tokenization conventions are reconsidered. It is moreover an important problem, because any errors occurring early in the NLP pipeline affect further analysis negatively. And even though current tokenizers reach high performance, there are three issues that we feel haven’t been addressed satisfactorily so far: • Most tokenizers are rule-based and therefore hard to maintain and hard to adap"
D13-1146,J06-4003,0,0.0968001,"ner, 2004); • Word and sentence segmentation are often seen as separate tasks, but they obviously inform each other and it could be advantageous to view them as a combined task; 2 Related Work Usually the text segmentation task is split into word tokenization and sentence boundary detection. Rulebased systems for finding word and sentence boundaries often are variations on matching hand-coded regular expressions (Grefenstette, 1999; Silla Jr. and Kaestner, 2004; Jurafsky and Martin, 2008; Dridan and Oepen, 2012). Several unsupervised systems have been proposed for sentence boundary detection. Kiss and Strunk (2006) present a language-independent, unsupervised approach and note that abbreviations form a major source of ambiguity in sentence boundary detection and use collocation detection to build a high-accuracy abbreviation detector. The resulting system reaches high accuracy, rivalling handcrafted rule-based and supervised systems. A similar system was proposed earlier by Mikheev (2002). Existing supervised learning approaches for sentence boundary detection use as features tokens preceding and following potential sentence boundary, part of speech, capitalization information and lists of abbreviations"
D13-1146,P10-1052,0,0.0419322,"Missing"
D13-1146,J02-3002,0,0.0248183,"-coded regular expressions (Grefenstette, 1999; Silla Jr. and Kaestner, 2004; Jurafsky and Martin, 2008; Dridan and Oepen, 2012). Several unsupervised systems have been proposed for sentence boundary detection. Kiss and Strunk (2006) present a language-independent, unsupervised approach and note that abbreviations form a major source of ambiguity in sentence boundary detection and use collocation detection to build a high-accuracy abbreviation detector. The resulting system reaches high accuracy, rivalling handcrafted rule-based and supervised systems. A similar system was proposed earlier by Mikheev (2002). Existing supervised learning approaches for sentence boundary detection use as features tokens preceding and following potential sentence boundary, part of speech, capitalization information and lists of abbreviations. Learning methods employed in 1422 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1422–1426, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics these approaches include maximum entropy models (Reynar and Ratnaparkhi, 1997) decision trees (Riley, 1989), and neural networks (Palmer and Hear"
D13-1146,J97-2002,0,0.0325392,"Mikheev (2002). Existing supervised learning approaches for sentence boundary detection use as features tokens preceding and following potential sentence boundary, part of speech, capitalization information and lists of abbreviations. Learning methods employed in 1422 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1422–1426, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics these approaches include maximum entropy models (Reynar and Ratnaparkhi, 1997) decision trees (Riley, 1989), and neural networks (Palmer and Hearst, 1997). Closest to our work are approaches that present token and sentence splitters using conditional random fields (Tomanek et al., 2007; Fares et al., 2013). However, these previous approaches consider tokens (i.e. character sequences) as basic units for labeling, whereas we consider single characters. As a consequence, labeling is more resource-intensive, but it also gives us more expressive power. In fact, our approach kills two birds with one stone, as it allows us to integrate token and sentence boundaries detection into one task. 3 3.1 Method IOB Tokenization IOB tagging is widely used in ta"
D13-1146,A97-1004,0,0.112885,"afted rule-based and supervised systems. A similar system was proposed earlier by Mikheev (2002). Existing supervised learning approaches for sentence boundary detection use as features tokens preceding and following potential sentence boundary, part of speech, capitalization information and lists of abbreviations. Learning methods employed in 1422 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1422–1426, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics these approaches include maximum entropy models (Reynar and Ratnaparkhi, 1997) decision trees (Riley, 1989), and neural networks (Palmer and Hearst, 1997). Closest to our work are approaches that present token and sentence splitters using conditional random fields (Tomanek et al., 2007; Fares et al., 2013). However, these previous approaches consider tokens (i.e. character sequences) as basic units for labeling, whereas we consider single characters. As a consequence, labeling is more resource-intensive, but it also gives us more expressive power. In fact, our approach kills two birds with one stone, as it allows us to integrate token and sentence boundaries detection i"
D13-1146,H89-2048,0,0.0341909,"ilar system was proposed earlier by Mikheev (2002). Existing supervised learning approaches for sentence boundary detection use as features tokens preceding and following potential sentence boundary, part of speech, capitalization information and lists of abbreviations. Learning methods employed in 1422 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1422–1426, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics these approaches include maximum entropy models (Reynar and Ratnaparkhi, 1997) decision trees (Riley, 1989), and neural networks (Palmer and Hearst, 1997). Closest to our work are approaches that present token and sentence splitters using conditional random fields (Tomanek et al., 2007; Fares et al., 2013). However, these previous approaches consider tokens (i.e. character sequences) as basic units for labeling, whereas we consider single characters. As a consequence, labeling is more resource-intensive, but it also gives us more expressive power. In fact, our approach kills two birds with one stone, as it allows us to integrate token and sentence boundaries detection into one task. 3 3.1 Method IO"
D13-1146,J00-4006,0,\N,Missing
D18-1526,E17-2039,1,0.673609,"Missing"
D18-1526,W17-6901,1,0.82116,"paper is to investigate whether learning to predict lexical semantic categories can be beneficial to other NLP tasks. To achieve this we augment single-task models (ST)1 1 We replicate models which perform at or close to the We test our hypothesis on three disparate NLP tasks: (i) Universal Dependency part-of-speech tagging (UPOS), (ii) Universal Dependency parsing (UD DEP), a complex syntactic task; and (iii) Natural Language Inference (NLI), a complex task requiring deep natural language understanding. 2 2.1 Background and Related work Semantic Tagging Semantic tagging (Bjerva et al., 2016; Abzianidze and Bos, 2017) is the task of assigning languageneutral semantic categories to words. It is designed to overcome a lack of semantic information syntax-oriented part-of-speech tagsets, such as the state-of-the-art. Our choice of models is based on replicability. 2 The ability to fit random noise. 4881 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4881–4889 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Figure 1: Our three multi-task learning settings: (A) fully shared networks, (B) partially shared networks, a"
D18-1526,J93-2004,0,0.0608965,"Missing"
D18-1526,marelli-etal-2014-sick,0,0.0125384,"tasets of main and auxiliary tasks, i.e., each instance is labeled with both the main task’s labels and semantic tags. We use the Stanford POS Tagger (Toutanova et al., 2003) trained on sem-PMB to tag the UD corpus and NLI datasets with semantic tags, and then use those assigned tags for the MTL settings of our dependency parsing and NLI models. We find that this approach leads to better results when the main task is only loosely related to the auxiliary task. The UD DEP experiments use the English UD 2.0 corpus, and the NLI experiments use the SNLI (Bowman et al., 2015) and SICK-E4 datasets (Marelli et al., 2014). The provided train, development, and test splits are used for all datasets. For sem-PMB, the silver and gold parts are used for training and testing respectively. 5 Experiments We run four experiments for each of the four tasks (UPOS, UD DEP, SNLI, SICK-E), one using the ST model and one for each of the three MTL settings. Each experiment is run five times, and the average of the five runs is reported. We briefly describe the ST models and refer the reader to the 3 4 http://pmb.let.rug.nl/data.php SICK-E refers to the entailment part of the SICK dataset. original work for further details due"
D18-1526,P13-2017,0,0.0238238,"Missing"
D18-1526,D14-1162,0,0.0803816,"Missing"
D18-1526,P16-2067,0,0.0283937,"earning in which multiple tasks are simultaneously learned. By optimising the multiple loss functions of related tasks at once, multi-task learning models can achieve superior results compared to models trained on a single task. The key principle is summarized by Caruana (1998) as “MTL improves generalization by leveraging the domain-specific information contained in the training signals of related tasks”. Neural MTL has become an increasingly successful approach by exploiting similarities between Natural Language Processing (NLP) tasks (Collobert and Weston, 2008; Søgaard and Goldberg, 2016; Plank et al., 2016). Our work builds upon Bjerva et al. (2016), who demonstrate that employing semantic tagging as an auxiliary task for Universal Dependency (McDonald et al., 2013) part-of-speech tagging can lead to improved performance. The objective of this paper is to investigate whether learning to predict lexical semantic categories can be beneficial to other NLP tasks. To achieve this we augment single-task models (ST)1 1 We replicate models which perform at or close to the We test our hypothesis on three disparate NLP tasks: (i) Universal Dependency part-of-speech tagging (UPOS), (ii) Universal Dependenc"
D18-1526,P16-2038,0,0.0207463,"urgent approach to machine learning in which multiple tasks are simultaneously learned. By optimising the multiple loss functions of related tasks at once, multi-task learning models can achieve superior results compared to models trained on a single task. The key principle is summarized by Caruana (1998) as “MTL improves generalization by leveraging the domain-specific information contained in the training signals of related tasks”. Neural MTL has become an increasingly successful approach by exploiting similarities between Natural Language Processing (NLP) tasks (Collobert and Weston, 2008; Søgaard and Goldberg, 2016; Plank et al., 2016). Our work builds upon Bjerva et al. (2016), who demonstrate that employing semantic tagging as an auxiliary task for Universal Dependency (McDonald et al., 2013) part-of-speech tagging can lead to improved performance. The objective of this paper is to investigate whether learning to predict lexical semantic categories can be beneficial to other NLP tasks. To achieve this we augment single-task models (ST)1 1 We replicate models which perform at or close to the We test our hypothesis on three disparate NLP tasks: (i) Universal Dependency part-of-speech tagging (UPOS), (ii"
D18-1526,N03-1033,0,0.0927235,"unit modulates the transfer of information between the shared subspaces as shown in Equations (1) and (2). 4 Data In the UPOS tagging experiments, we utilize the UD 2.0 English corpus (Nivre et al., 2017) for the POS tagging and the semantically tagged PMB release 0.1.0 (sem-PMB)3 for the MTL settings. Note that there is no overlap between the two datasets. Conversely, for the UD DEP and NLI experiments there is a complete overlap between the datasets of main and auxiliary tasks, i.e., each instance is labeled with both the main task’s labels and semantic tags. We use the Stanford POS Tagger (Toutanova et al., 2003) trained on sem-PMB to tag the UD corpus and NLI datasets with semantic tags, and then use those assigned tags for the MTL settings of our dependency parsing and NLI models. We find that this approach leads to better results when the main task is only loosely related to the auxiliary task. The UD DEP experiments use the English UD 2.0 corpus, and the NLI experiments use the SNLI (Bowman et al., 2015) and SICK-E4 datasets (Marelli et al., 2014). The provided train, development, and test splits are used for all datasets. For sem-PMB, the silver and gold parts are used for training and testing re"
D18-1526,C16-1333,1,\N,Missing
D18-1526,P17-1152,0,\N,Missing
D18-1526,K17-3002,0,\N,Missing
E12-2019,W08-2222,1,0.870717,"with links to the original, raw documents. We employ a chain of NLP components that carry out, respectively, tokenization and sentence boundary detection, POS tagging, lemmatization, named entity recognition, supertagging, parsing using the formalism of Combinatory Categorial Grammar (Steedman, 2001), and semantic and discourse analysis using the framework of Discourse Representation Theory (DRT) (Kamp and Reyle, 1993) with rhetorical relations (Asher, 1993). The lemmatizer used is morpha (Minnen et al., 2001), the other steps are carried out by the C&C tools (Curran et al., 2007) and Boxer (Bos, 2008). 3.1 Bits of Wisdom After each step in the toolchain, the intermediate result may be automatically adjusted by auxiliary components that apply annotations provided by expert users or other sources. These annotations are represented as “Bits of Wisdom” (BOWs): tuples of information regarding, for example, token and sentence boundaries, tags, word senses or discourse relations. They are stored in a MySQL database and can originate from three different sources: (i) explicit annotation changes made by experts using the Explorer Web interface (see Section 4); (ii) an annotation game played by none"
E12-2019,W08-2230,0,0.149743,"diate result may be automatically adjusted by auxiliary components that apply annotations provided by expert users or other sources. These annotations are represented as “Bits of Wisdom” (BOWs): tuples of information regarding, for example, token and sentence boundaries, tags, word senses or discourse relations. They are stored in a MySQL database and can originate from three different sources: (i) explicit annotation changes made by experts using the Explorer Web interface (see Section 4); (ii) an annotation game played by nonexperts, similar to ‘games with a purpose’ like Phrase Detectives (Chamberlain et al., 2008) and Jeux de Mots (Artignan et al., 2009); and (iii) external NLP tools (e.g. for word sense disambiguation or co-reference resolution). Since BOWs come from various sources, they may contradict each other. In such cases, a judge component resolves the conflict, currently by preferring the most recent expert BOW. Future work will involve the application of different judging techniques. 3.2 Processing Cycle The widely known open-source tool GNU make is used to orchestrate the toolchain while avoiding unnecessary reprocessing. The need to rerun the toolchain for a document arises in three situat"
E12-2019,P07-2009,1,0.839389,"and-off annotations, i.e., files with links to the original, raw documents. We employ a chain of NLP components that carry out, respectively, tokenization and sentence boundary detection, POS tagging, lemmatization, named entity recognition, supertagging, parsing using the formalism of Combinatory Categorial Grammar (Steedman, 2001), and semantic and discourse analysis using the framework of Discourse Representation Theory (DRT) (Kamp and Reyle, 1993) with rhetorical relations (Asher, 1993). The lemmatizer used is morpha (Minnen et al., 2001), the other steps are carried out by the C&C tools (Curran et al., 2007) and Boxer (Bos, 2008). 3.1 Bits of Wisdom After each step in the toolchain, the intermediate result may be automatically adjusted by auxiliary components that apply annotations provided by expert users or other sources. These annotations are represented as “Bits of Wisdom” (BOWs): tuples of information regarding, for example, token and sentence boundaries, tags, word senses or discourse relations. They are stored in a MySQL database and can originate from three different sources: (i) explicit annotation changes made by experts using the Explorer Web interface (see Section 4); (ii) an annotati"
E12-2019,W07-1505,0,0.0240347,"k (GMB). We aim to reach this goal by: 1. Providing a wiki-like platform supporting collaborative annotation efforts; 2. Employing state-of-the-art NLP software for bootstrapping semantic analysis; 3. Giving real-time feedback of annotation adjustments in their resulting syntactic and semantic analysis; 4. Ensuring kerfuffle-free dissemination of our semantic resource by considering only public-domain texts for annotation. We have developed the wiki-like platform from scratch simply because existing annotation systems, such as GATE (Dowman et al., 2005), NITE (Carletta et al., 2003), or UIMA (Hahn et al., 2007), do not offer the functionality required for deep semantic annotation combined with crowdsourcing. In this description of our platform, we motivate our choice of data and explain how we manage it (Section 2), we describe the complete toolchain of NLP components employed in the annotationfeedback process (Section 3), and the Web-based interface itself is introduced, describing how linguists can adjust boundaries of tokens and sentences, and revise tags of named entities, parts of speech and lexical categories (Section 4). 2 Data The goal of the Groningen Meaning Bank is to provide a widely ava"
E12-2019,P10-2013,0,0.0462546,"and sources, resulting in a rich, comprehensive 92 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 92–96, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics corpus appropriate for use in various disciplines within NLP. The documents in the current version of the GMB are all in English and originate from four main sources: (i) Voice of America (VOA), an online newspaper published by the US Federal Government; (ii) the Manually Annotated Sub-Corpus (MASC) from the Open American National Corpus (Ide et al., 2010); (iii) country descriptions from the CIA World Factbook (CIA) (Central Intelligence Agency, 2006), in particular the Background and Economy sections, and (iv) a collection of Aesop’s fables (AF). All these documents are in the public domain and are thus redistributable, unlike for example the WSJ data used in the Penn Treebank (Miltsakaki et al., 2004). Each document is stored with a separate file containing metadata. This may include the language the text is written in, the genre, date of publication, source, title, and terms of use of the document. This metadata is stored as a simple featur"
E12-2019,miltsakaki-etal-2004-penn,0,0.0585418,"t version of the GMB are all in English and originate from four main sources: (i) Voice of America (VOA), an online newspaper published by the US Federal Government; (ii) the Manually Annotated Sub-Corpus (MASC) from the Open American National Corpus (Ide et al., 2010); (iii) country descriptions from the CIA World Factbook (CIA) (Central Intelligence Agency, 2006), in particular the Background and Economy sections, and (iv) a collection of Aesop’s fables (AF). All these documents are in the public domain and are thus redistributable, unlike for example the WSJ data used in the Penn Treebank (Miltsakaki et al., 2004). Each document is stored with a separate file containing metadata. This may include the language the text is written in, the genre, date of publication, source, title, and terms of use of the document. This metadata is stored as a simple feature-value list. The documents in the GMB are categorized with different statuses. Initially, newly added documents are labeled as uncategorized. As we manually review them, they are relabeled as either accepted (document will be part of the next stable version, which will be released in regular intervals), postponed (there is some difficulty with the docu"
E17-2039,W13-2322,0,0.0486133,"d sentences in other languages. The aim of this paper is to present a method that implements this idea in practice, by building a parallel corpus with shared formal meaning representations, that is, the Parallel Meaning Bank (PMB). Recently, several semantic resources—corpora of texts annotated with meanings—have been developed to stimulate and evaluate semantic parsing. Usually, such resources are manually or semiautomatically created, and this process is expensive since it requires training of and annotation by human annotators. The AMR banks of Abstract Meaning Representations for English (Banarescu et al., 2013) or Chinese and Czech (Xue et al., 2014) sentences, for instance, are the result of manual annotation efforts. Another example is the development of the Groningen Meaning Bank (Bos et al., 2017), a corpus of English texts annotated with formal, compositional meaning representations, which took advantage of existing semantic parsing tools, combining them with human corrections. In this paper we propose a method for producing meaning banks for several languages (English, Dutch, German and Italian), by taking advantage of translations. On the conceptual level we follow the approach of the Groning"
E17-2039,W07-1401,0,0.00916599,"or selection is that freely distributable texts are preferable over texts which are under copyright and require (paid) licensing. Besides English we chose two other Germanic languages, Dutch and German, because they are similar to English. We also include one Romance language, Italian, in order to test whether our method works for languages which are typologically more different from English. The texts in the PMB are sourced from twelve different corpora from a wide range of genres, including, among others: Tatoeba1 , NewsCommentary (via OPUS, Tiedemann, 2012), Recognizing Textual Entailment (Giampiccolo et al., 2007), Sherlock Holmes stories2 , and the Bible (Christodouloupoulos and Steedman, 2015). These corpora are divided over 100 parts in a balanced way. Initially, two of these parts, 00 and 3.1 Segmentation Text segmentation involves word and sentence boundary detection. Multiword expressions that represent constituents are treated as single tokens. Closed compound words that have a semantically transparent structure are decomposed. For example, impossible is decomposed into im and possible while Las Vegas and 2 pm are analysed as a single token. In this way we aim to assign ‘atomic’ meanings to toke"
E17-2039,E12-2019,1,0.866687,"e et al., 2014) sentences, for instance, are the result of manual annotation efforts. Another example is the development of the Groningen Meaning Bank (Bos et al., 2017), a corpus of English texts annotated with formal, compositional meaning representations, which took advantage of existing semantic parsing tools, combining them with human corrections. In this paper we propose a method for producing meaning banks for several languages (English, Dutch, German and Italian), by taking advantage of translations. On the conceptual level we follow the approach of the Groningen Meaning Bank project (Basile et al., 2012), and use some of the tools developed in it. The main reason for this choice is that we are not only interested in the final meaning of a sentence, but also in how it is derived—the compositional semantics. These derivations, based on Combinatory Categorial Grammar (CCG, Steedman, 2001), give us the means to project semantic information from one sentence to its translated counterpart. The goal of the PMB is threefold. First, it will The Parallel Meaning Bank is a corpus of translations annotated with shared, formal meaning representations comprising over 11 million words divided over four lang"
E17-2039,C16-1333,1,0.685288,"Missing"
E17-2039,D14-1107,0,0.0183364,"G makes the derivations suitable for widecoverage compositional semantics (Bos et al., 2004). CCG is also a lexicalised theory of grammar, which makes cross-lingual projection of grammatical information from source to target sentence more convenient (see Section 4). The version of CCG that we employ differs from standard CCG: in order to facilitate the crosslingual projection process and retain compositionality, type-changing rules of a CCG parser are explicated by inserting (unprojected) empty elements which have their own semantics (see the token ∅ in Figure 2). For parsing, we use EasyCCG (Lewis and Steedman, 2014), which was chosen because it is accurate, does not require part-of-speech annotation (which would require different annotation schemes for each language) and is easily adaptable to our modified grammar formalism. 3.3 3.4 Universal Semantic Tagging Symbolization The meaning representations that we use contain logical symbols and non-logical symbols. The latter are based on the words mentioned in the input text. We refer to this process as symbolization. It combines lemmatization with normalization, and To facilitate the organization of a wide-coverage semantic lexicon for cross-lingual semanti"
E17-2039,C04-1180,1,0.672748,"Missing"
E17-2039,J03-1002,0,0.00928522,"antic representation in the PMB. It is a well-studied theory from a linguistic semantic viewpoint and suitable for compositional semantics.5 Expressions in DRT, called Discourse 4 http://unece.org/cefact/codesfortrade In particular, we employ Projective DRT (Venhuizen, 2015)—an extension of DRT that accounts for presupposi5 tions, anaphora and conventional implicatures in a generalized way. 245 one-to-one heuristic, with each English sentence aligned to a non-English sentence in order, to be corrected manually. Subsequently, we automatically align words in the aligned sentences using GIZA ++ (Och and Ney, 2003). Although we use existing tools for the initial annotation of English and projection as the initial annotation of non-English documents, our aim is to train new language-neutral models. Training new models on just the automatic annotation will not yield better performance than the combination of existing tools and projection. However, we improve these models constantly by adding manual corrections to the initial automatic annotation, and retraining them. In addition, this approach lets us adapt to revisions of the annotation guidelines. 5 annotation conflicts are then slated for resolution by"
E17-2039,W15-1841,1,0.476422,"ing knowledge sources, such as WordNet (Fellbaum, 1998), Wikipedia, and UNECE codes for trade4 , to do symbolization. We are currently investigating how the performance of machine learning-based symbolizer compares to a rule-based one incorporating the lemmatizer Morpha (Minnen et al., 2001). 3.5 Representation Structures (DRSs), have a recursive structure and are usually depicted as boxes. An upper part of a DRS contains a set of referents while the lower part lists a conjunction of atomic or compound conditions over these referents (see an example of a DRS in the bottom of Figure 2). Boxer (Bos, 2015), a system that employs λ-calculus to construct DRSs in a compositional way, is used to derive meaning representations of the documents. However, the original version of Boxer is tailored to the English language. We have adapted Boxer to work with the universal semtags rather than English-specific part-ofspeech tags. Boxer also assigns VerbNet/LIRICS thematic roles (Bonial et al., 2011) to verbs so that the lexical semantics of verbs include the corresponding thematic predicates (see came in Figure 2). Hence an input to Boxer is a CCG derivation where all tokens are decorated with semtags and"
E17-2039,tiedemann-2012-parallel,0,0.00956183,"s sufficient for our purposes. Another criterion for selection is that freely distributable texts are preferable over texts which are under copyright and require (paid) licensing. Besides English we chose two other Germanic languages, Dutch and German, because they are similar to English. We also include one Romance language, Italian, in order to test whether our method works for languages which are typologically more different from English. The texts in the PMB are sourced from twelve different corpora from a wide range of genres, including, among others: Tatoeba1 , NewsCommentary (via OPUS, Tiedemann, 2012), Recognizing Textual Entailment (Giampiccolo et al., 2007), Sherlock Holmes stories2 , and the Bible (Christodouloupoulos and Steedman, 2015). These corpora are divided over 100 parts in a balanced way. Initially, two of these parts, 00 and 3.1 Segmentation Text segmentation involves word and sentence boundary detection. Multiword expressions that represent constituents are treated as single tokens. Closed compound words that have a semantically transparent structure are decomposed. For example, impossible is decomposed into im and possible while Las Vegas and 2 pm are analysed as a single to"
E17-2039,C16-1056,1,0.83036,"igure 2). 4 Cross-lingual Projection The initial annotation for Dutch, German and Italian is bootstrapped via word alignments. Each non-English text is automatically word-aligned with its English counterpart, and non-English words initially receive semtags, CCG categories and symbols based on those of their English counterparts (see Figure 2). CCG slashes are flipped as needed, and 2:1 alignments are handled through functional composition. Then, the CCG derivations and DRSs can be obtained by applying CCG’s combinatory rules in such a way that the same DRS as for the English sentence results (Evang and Bos, 2016; Evang, 2016). If the alignment is incorrect, it can be corrected manually (see Section 5). The idea behind this way of bootstrapping is to exploit the advanced state-of-the-art of NLP for English, and to encourage parallelism between the syntactic and semantic analyses of different languages. To facilitate cross-lingual projection, alignment has to be done at two levels: sentences and words. Sentence alignment is initially done with a simple Semantic Interpretation Discourse Representation Theory (DRT, Kamp and Reyle, 1993), is the semantic formalism that is used as a semantic representation"
E17-2039,xue-etal-2014-interlingua,0,0.0257014,"s paper is to present a method that implements this idea in practice, by building a parallel corpus with shared formal meaning representations, that is, the Parallel Meaning Bank (PMB). Recently, several semantic resources—corpora of texts annotated with meanings—have been developed to stimulate and evaluate semantic parsing. Usually, such resources are manually or semiautomatically created, and this process is expensive since it requires training of and annotation by human annotators. The AMR banks of Abstract Meaning Representations for English (Banarescu et al., 2013) or Chinese and Czech (Xue et al., 2014) sentences, for instance, are the result of manual annotation efforts. Another example is the development of the Groningen Meaning Bank (Bos et al., 2017), a corpus of English texts annotated with formal, compositional meaning representations, which took advantage of existing semantic parsing tools, combining them with human corrections. In this paper we propose a method for producing meaning banks for several languages (English, Dutch, German and Italian), by taking advantage of translations. On the conceptual level we follow the approach of the Groningen Meaning Bank project (Basile et al.,"
E17-2039,D13-1146,1,0.743699,"ion. Multiword expressions that represent constituents are treated as single tokens. Closed compound words that have a semantically transparent structure are decomposed. For example, impossible is decomposed into im and possible while Las Vegas and 2 pm are analysed as a single token. In this way we aim to assign ‘atomic’ meanings to tokens and avoid redundant lexical semantics. Segmentation follows an IOB-annotation scheme on the level of characters, with four labels: beginning of sentence, beginning of word, inside a word, and outside a word. We use the same statistical tokenizer, Elephant (Evang et al., 2013), for all four languages, but with language-specific models. 1 https://tatoeba.org http://gutenberg.org, http://etc.usf. edu/lit2go, http://gutenberg.spiegel.de 2 243 NP SNP He came back male come back ((SNP )(SNP ))/NP EPS at ∅ 5 o’clock ∅ 17 : 00 IST REL DIS (SNP )(SNP ) ((SNP )(SNP ))/NP NP/N at CLO N λV Gp.V G(λx.D ; px) λGVHp.V H(λx.G(λy.D ; px)) λpq.D ; (px; qx) s M anner(x, s) back(s) SNP x y e s t1 t2 come(e) T ime(e, t2 ) now(t1 ) t2 &lt; t 1 λx.D = e t1 t2 come(e) T heme(e, x) T ime(e, t2 ) now(t1 ) t2 &lt; t1 zur¨uck f¨unf Uhr = x male(x) = SNP λGp.G(λx.D ; pe) = NP λp.D ∗ px"
H05-1079,C04-1180,1,0.6132,"lap is always a real number between 0 and 1 and also ensures independence of the length of the hypothesis. Apart from wnoverlap we take into account length (as measured by number of lemmas) of text and hypothesis, because in most of the observed cases for true entailments the hypothesis is shorter than the text as it contains less information. This is covered by three numerical features measuring the length of the text, of the hypothesis and the relative length of hypothesis with regard to the text. 3 Deep Semantic Analysis 3.1 Semantic Interpretation We use a robust wide-coverage CCG-parser (Bos et al., 2004) to generate fine-grained semantic representations for each T/H-pair. The semantic representation language is a first-order fragment of the DRSlanguage used in Discourse Representation Theory (Kamp and Reyle, 1993), conveying argument structure with a neo-Davidsonian analysis and including the recursive DRS structure to cover negation, disjunction, and implication. Consider for example: Example: 78 (FALSE) T: Clinton’s new book is not big seller here. H: Clinton’s book is a big seller. x1 x2 x3 book(x1) book(x2) ¬ x1=x2 clinton(x3) of(x1,x3) e4 x5 drs(T): ¬ big(x5) seller(x5) be(e4) agent(e4,x"
J03-2002,C02-1067,1,0.792358,"anding system. OAA is a collection of software agents that communicate with each other via a facilitator, a piece of middleware that distributes requests to appropriate agents and returns the responses to the requester. OAA makes it convenient to combine different components that are required in natural language process204 Bos Implementing Binding and Accommodation Theory ing, such as speech recognition or parsing, the presupposition resolution component, and theorem provers, because OAA agents can be implemented in different programming languages and run simultaneously on different machines (Bos and Oka 2002). The resolution component is realized as an OAA agent implemented in PROLOG. 5.2 Acceptability Constraints To implement inference, a theorem prover as well as a model builder is used, both encapsulated as OAA agents. The theorem-proving agent is used to find a counterproof for the DRS translated into first-order logic. The model-building agent is used to check whether the same DRS is satisfiable. So, although we are faced with the limitations for reasoning with first-order logic (validity is undecidable in first-order logic, and model generation is restricted to finite models), these limitati"
J03-2002,J95-2003,0,0.160757,"Missing"
J16-3006,D15-1198,0,0.0425586,"Missing"
J16-3006,W15-0128,0,0.029235,"connects so that it needs a universal quantifier. Definition 7 can then be extended with clauses like (x/P :quant ∀),φ↓ = ∀x(P(x)→ φ(x)). In this way, quantifier scope ambiguities are underspecified and different readings can be obtained by changing the order of applications in Clauses 4 and 5 of Definition 8. 6. Conclusion Are AMRs any different from traditional meaning representations? Yes. What AMRs have in common with DRSs and MRSs is that logical conjunction is implicit (apart from those cases where it is triggered by coordination phenomena). The expressive power of AMRs is also lower (Bender et al. 2015). Scope is not explicitly present in AMRs, and therefore the way negation is represented in AMRs is substantially different from the ways this is done in DRSs or MRSs. Unlike DRSs or MRSs, AMRs have the capacity to display some aspects of information structure (the process of role inversion can change the scope of concepts), although, as soon as negation is present, this can lead to changes in meaning. A = A↑ (A,λu.↓) = [6] λp.∃x(person(x)∧named(x,”Mr Krupp”)∧p)(∃e(dry(e)∧ARG0(e,x)∧ARG1(e,x))) = [β-conv] ∃x(person(x)∧named(x,”Mr Krupp”)∧∃e(dry(e)∧ARG0(e,x)∧ARG1(e,x))) Figure 8 Combining"
J16-3006,P13-2131,0,0.38875,"pularity in computational linguistics (Artzi, Lee, and Zettlemoyer 2015; Chen 2015; Peng, Song, and Gildea 2015; Pust et al. 2015; Sawai, Shindo, and Matsumoto 2015; Wang, Xue, and Pradhan 2015; Werling, Angeli, and Manning 2015). There are several reasons for this trend. First of all, the simple tree structure of AMRs, showing the connections between concepts and events, make them easy to read. Second, because AMRs can simply be expressed as directed acyclic graphs, machine-generated output can be evaluated in a standard way by computing precision and recall on triples of gold-standard AMRs (Cai and Knight 2013). And, third, AMRs are arguably easier to produce manually than traditional formal meaning representations, and, as a result, there are now corpora with gold-standard AMRs available (Banarescu et al. 2015). AMRs do not resemble classical meaning representations and seem substantially different from well-understood formalisms such as discourse representation structures (DRSs) (Kamp and Reyle 1993) or minimal recursion structures (MRSs) (Copestake et al. 2005). AMR has an unorthodox way of expressing negation, no explicit means to deal with universal quantification, and no model theory. This rai"
J16-3006,P15-3007,0,0.0151484,"current variables are in the decidable two-variable fragment of FOL. The current definition of AMRs has limited expressive power for universal quantification (up to one universal quantifier per sentence). A simple extension of the AMR syntax and translation to FOL provides the means to represent projection and scope phenomena. 1. Introduction There is a new kid on the semantics block: AMR. Abstract meaning representation, based on the PENMAN notation and introduced in Langkilde and Knight (1998), has suddenly gained in popularity in computational linguistics (Artzi, Lee, and Zettlemoyer 2015; Chen 2015; Peng, Song, and Gildea 2015; Pust et al. 2015; Sawai, Shindo, and Matsumoto 2015; Wang, Xue, and Pradhan 2015; Werling, Angeli, and Manning 2015). There are several reasons for this trend. First of all, the simple tree structure of AMRs, showing the connections between concepts and events, make them easy to read. Second, because AMRs can simply be expressed as directed acyclic graphs, machine-generated output can be evaluated in a standard way by computing precision and recall on triples of gold-standard AMRs (Cai and Knight 2013). And, third, AMRs are arguably easier to produce manually tha"
J16-3006,P98-1116,0,0.114722,"a systematic translation to first-order logic (FOL) can be specified, including a proper treatment of negation. AMRs without recurrent variables are in the decidable two-variable fragment of FOL. The current definition of AMRs has limited expressive power for universal quantification (up to one universal quantifier per sentence). A simple extension of the AMR syntax and translation to FOL provides the means to represent projection and scope phenomena. 1. Introduction There is a new kid on the semantics block: AMR. Abstract meaning representation, based on the PENMAN notation and introduced in Langkilde and Knight (1998), has suddenly gained in popularity in computational linguistics (Artzi, Lee, and Zettlemoyer 2015; Chen 2015; Peng, Song, and Gildea 2015; Pust et al. 2015; Sawai, Shindo, and Matsumoto 2015; Wang, Xue, and Pradhan 2015; Werling, Angeli, and Manning 2015). There are several reasons for this trend. First of all, the simple tree structure of AMRs, showing the connections between concepts and events, make them easy to read. Second, because AMRs can simply be expressed as directed acyclic graphs, machine-generated output can be evaluated in a standard way by computing precision and recall on trip"
J16-3006,K15-1004,0,0.036235,"Missing"
J16-3006,D15-1136,0,0.023928,"Missing"
J16-3006,P15-2140,0,0.020743,"Missing"
J16-3006,P15-2141,0,0.03618,"Missing"
J16-3006,P15-1095,0,0.0217717,"Missing"
J16-3006,C98-1112,0,\N,Missing
L18-1267,W17-6901,1,0.916945,"a comparison operator. In order to give an intuition about the diversity of clauses in DRSs, Table 1 shows a distribution of various types of clauses in a corpus of DRSs (see Section 3). Since every logical operator carries a scope, their number represents a lower bound of the number of scopes in the meaning representations. In addition to logical operators, scopes are introduced by presupposition triggers like proper names or pronouns. To make a meaningful comparison between AMRs and DRSs in terms of size, we compare the DRSs of 250,000 English sentences from the Parallel Meaning Bank (PMB; Abzianidze et al., 2017) to AMRs of the same sentences, produced by the state-of-the-art AMR parser from van Noord and Bos (2017). Statistics of the comparison are shown in Figure 2. On average, DRSs are about twice as large as AMRs, in terms of the number of clauses as well as the number of unique variables. This is obviously due to the explicit presence of scope in the meaning representation. However, for both meaning representations the number of clauses and variables increase linearly with sentence length. 120 Avg clauses DRS Avg variables DRS Avg triples AMR Avg variables AMR 100 Number 80 60 40 20 00 5 10 15 20"
L18-1267,E17-2039,1,0.89376,"Missing"
L18-1267,W08-2227,0,0.295228,"Missing"
L18-1267,W13-2322,0,0.259186,"Missing"
L18-1267,E12-2019,1,0.826004,"2016; Evang and Bos, 2016). All the information provided by these layers is combined into a single meaning representation using the semantic parser Boxer (Bos, 2015), in the form of Discourse Representation Structures. Note that the goal is to produce annotations that capture the most probable interpretation of a sentence; no ambiguities or under-specification techniques are employed. At each step in this pipeline, a single component produces the automatic annotation for all four languages, using language-specific models. Human annotators can correct machine output by adding ‘Bits of Wisdom’ (Basile et al., 2012). These corrections serve as data for training better models, and create a gold standard annotated subset of the data. Annotation quality is defined per layer and language, at three levels: bronze (fully automatic), silver (automatic with some manual corrections), and gold (fully manually checked and corrected). If all layers are marked as gold, it follows that the resulting DRS can be considered gold standard, too. The first public release1 of the PMB contains gold standard scoped meaning representations for over 3,000 sentences in total (see Table 2). The release includes mainly relatively s"
L18-1267,C16-1333,1,0.825183,"ns, integrating word senses, thematic roles, and the list of operators, form the final product of our semantically annotated corpus: the Parallel Meaning Bank. The PMB is a semantically annotated corpus of English texts aligned with translations in Dutch, German and Italian (Abzianidze et al., 2017). It uses the same framework as the Groningen Meaning Bank (Bos et al., 2017), but aims to abstract away from language-specific annotation models. There are five annotation layers present in the PMB: segmentation of words, multi-word expressions and sentences (Evang et al., 2013), semantic tagging (Bjerva et al., 2016; Abzianidze and Bos, 2017), syntactic analysis based on CCG (Lewis and Steedman, 2014), word senses based on WordNet (Fellbaum, 1998), and thematic role labelling (Bos et al., 2012). The semantic analysis for English is projected on the other languages, to save manual annotation efforts (Evang, 2016; Evang and Bos, 2016). All the information provided by these layers is combined into a single meaning representation using the semantic parser Boxer (Bos, 2015), in the form of Discourse Representation Structures. Note that the goal is to produce annotations that capture the most probable interpre"
L18-1267,W15-1841,1,0.768105,"n layers present in the PMB: segmentation of words, multi-word expressions and sentences (Evang et al., 2013), semantic tagging (Bjerva et al., 2016; Abzianidze and Bos, 2017), syntactic analysis based on CCG (Lewis and Steedman, 2014), word senses based on WordNet (Fellbaum, 1998), and thematic role labelling (Bos et al., 2012). The semantic analysis for English is projected on the other languages, to save manual annotation efforts (Evang, 2016; Evang and Bos, 2016). All the information provided by these layers is combined into a single meaning representation using the semantic parser Boxer (Bos, 2015), in the form of Discourse Representation Structures. Note that the goal is to produce annotations that capture the most probable interpretation of a sentence; no ambiguities or under-specification techniques are employed. At each step in this pipeline, a single component produces the automatic annotation for all four languages, using language-specific models. Human annotators can correct machine output by adding ‘Bits of Wisdom’ (Basile et al., 2012). These corrections serve as data for training better models, and create a gold standard annotated subset of the data. Annotation quality is defi"
L18-1267,J16-3006,1,0.90705,"the Groningen Meaning Bank (Bos et al., 2017), but aims to abstract away from language-specific annotation models. There are five annotation layers present in the PMB: segmentation of words, multi-word expressions and sentences (Evang et al., 2013), semantic tagging (Bjerva et al., 2016; Abzianidze and Bos, 2017), syntactic analysis based on CCG (Lewis and Steedman, 2014), word senses based on WordNet (Fellbaum, 1998), and thematic role labelling (Bos et al., 2012). The semantic analysis for English is projected on the other languages, to save manual annotation efforts (Evang, 2016; Evang and Bos, 2016). All the information provided by these layers is combined into a single meaning representation using the semantic parser Boxer (Bos, 2015), in the form of Discourse Representation Structures. Note that the goal is to produce annotations that capture the most probable interpretation of a sentence; no ambiguities or under-specification techniques are employed. At each step in this pipeline, a single component produces the automatic annotation for all four languages, using language-specific models. Human annotators can correct machine output by adding ‘Bits of Wisdom’ (Basile et al., 2012). Thes"
L18-1267,W17-6905,1,0.774681,"s in our annotated corpus is formed by the Discourse Representation Structures (DRS) of Discourse Representation Theory (Kamp and Reyle, 1993). Our version of DRS integrates WordNet senses (Fellbaum, 1998), adopts a neoDavidsonian analysis of events employing VerbNet roles (Bonial et al., 2011), and includes an extensive set of comparison operators. More formally, a DRS is an ordered pair of a set of variables (discourse referents) and a set of conditions. There are basic and complex conditions. Terms are either variables or constants, where the latter ones are used to account for indexicals (Bos, 2017). Basic conditions are defined as follows: • If W is a symbol denoting a WordNet concept and x is a term, then W(x) is a basic condition; • If V is a symbol denoting a thematic role and x and y are terms, then V(x,y) is a basic condition; • If x and y are terms, then x=y, x6=y, x∼y, x&lt;y, x≤y, x≺y, and x./y are basic conditions formed with comparison operators. WordNet concepts are represented as word.POS.SenseNum, denoting a unique synset within WordNet. Thematic roles, including the VerbNet roles, always have two arguments and start with an uppercase character. Complex conditions introduce sc"
L18-1267,P13-2131,0,0.633786,"y annotated corpora as training data. However, there are not many annotated corpora available. We present a parallel corpus annotated with formal meaning representations for English, Dutch, German, and Italian, and a way to evaluate the quality of machinegenerated meaning representations by comparing them to gold standard annotations. Our work shows many similarities with recent annotation and parsing efforts around Abstract Meaning Representations, (AMR; Banarescu et al., 2013) in that we abstract away from syntax, use firstorder meaning representations, and use an adapted version of SMATCH (Cai and Knight, 2013) for evaluation. However, we deviate from AMR on several points: meanings are represented by scoped meaning representations (arriving at a more linguistically motivated treatment of modals, negation, presupposition, and quantification), and the nonlogical symbols that we use are grounded in WordNet (concepts) and VerbNet (thematic roles), rather than PropBank (Palmer et al., 2005). We also provide a syntactic analysis in the annotated corpus, in order to derive the semantic analyses in a compositional way. We make the following contributions: • A meaning representation with explicit scopes tha"
L18-1267,D07-1074,0,0.0586278,"Missing"
L18-1267,C16-1056,1,0.874808,"mework as the Groningen Meaning Bank (Bos et al., 2017), but aims to abstract away from language-specific annotation models. There are five annotation layers present in the PMB: segmentation of words, multi-word expressions and sentences (Evang et al., 2013), semantic tagging (Bjerva et al., 2016; Abzianidze and Bos, 2017), syntactic analysis based on CCG (Lewis and Steedman, 2014), word senses based on WordNet (Fellbaum, 1998), and thematic role labelling (Bos et al., 2012). The semantic analysis for English is projected on the other languages, to save manual annotation efforts (Evang, 2016; Evang and Bos, 2016). All the information provided by these layers is combined into a single meaning representation using the semantic parser Boxer (Bos, 2015), in the form of Discourse Representation Structures. Note that the goal is to produce annotations that capture the most probable interpretation of a sentence; no ambiguities or under-specification techniques are employed. At each step in this pipeline, a single component produces the automatic annotation for all four languages, using language-specific models. Human annotators can correct machine output by adding ‘Bits of Wisdom’ (Basile et al., 2012). Thes"
L18-1267,D13-1146,1,0.643542,"Missing"
L18-1267,W97-0802,0,0.241111,"extensions). This makes evaluation using matching computationally challenging, in particular for long sentences, but our matching system COUNTER achieves a reasonable trade-off between speed and accuracy. Several extensions to the annotation scheme are possible. Currently, the DRSs for the non-English languages contain references to synsets of the English WordNet. Conceptually, there is nothing wrong with this (as synsets can be viewed as identifiers for concepts that are languageindependent), but for practical reasons it makes more sense to provide links to synsets of the original language (Hamp and Feldweg, 1997; Postma et al., 2016; Roventini et al., 2000; Pianta et al., 2002). In addition, we consider implementing semantic grounding such as wikification in the Parallel Meaning Bank. As for other future work, we plan to include a more finegrained matching regarding WordNet synsets, since the current evaluation of concepts is purely string-based, with only identical strings resulting in a matching clause. For many synsets, however, it is possible to refer to them with more than one word.POS.SenseNum triple, and this should be accounted for (e.g. fox.n.02 and dodger.n.01 both refer to the same synset)"
L18-1267,D14-1107,0,0.043189,"final product of our semantically annotated corpus: the Parallel Meaning Bank. The PMB is a semantically annotated corpus of English texts aligned with translations in Dutch, German and Italian (Abzianidze et al., 2017). It uses the same framework as the Groningen Meaning Bank (Bos et al., 2017), but aims to abstract away from language-specific annotation models. There are five annotation layers present in the PMB: segmentation of words, multi-word expressions and sentences (Evang et al., 2013), semantic tagging (Bjerva et al., 2016; Abzianidze and Bos, 2017), syntactic analysis based on CCG (Lewis and Steedman, 2014), word senses based on WordNet (Fellbaum, 1998), and thematic role labelling (Bos et al., 2012). The semantic analysis for English is projected on the other languages, to save manual annotation efforts (Evang, 2016; Evang and Bos, 2016). All the information provided by these layers is combined into a single meaning representation using the semantic parser Boxer (Bos, 2015), in the form of Discourse Representation Structures. Note that the goal is to produce annotations that capture the most probable interpretation of a sentence; no ambiguities or under-specification techniques are employed. At"
L18-1267,J05-1004,0,0.033567,"notation and parsing efforts around Abstract Meaning Representations, (AMR; Banarescu et al., 2013) in that we abstract away from syntax, use firstorder meaning representations, and use an adapted version of SMATCH (Cai and Knight, 2013) for evaluation. However, we deviate from AMR on several points: meanings are represented by scoped meaning representations (arriving at a more linguistically motivated treatment of modals, negation, presupposition, and quantification), and the nonlogical symbols that we use are grounded in WordNet (concepts) and VerbNet (thematic roles), rather than PropBank (Palmer et al., 2005). We also provide a syntactic analysis in the annotated corpus, in order to derive the semantic analyses in a compositional way. We make the following contributions: • A meaning representation with explicit scopes that combines WordNet and VerbNet with elements of formal logic (Section 2). • A gold standard annotated parallel corpus of formal meaning representations for four languages (Section 3). • A tool that compares two scoped meaning representations for the purpose of evaluation (Section 4 and Section 5). 2. 2.1. Scoped Meaning Representations Discourse Representation Structures The backb"
L18-1267,2016.gwc-1.43,0,0.0255946,"Missing"
L18-1267,roventini-etal-2000-italwordnet,0,0.0194539,"ching computationally challenging, in particular for long sentences, but our matching system COUNTER achieves a reasonable trade-off between speed and accuracy. Several extensions to the annotation scheme are possible. Currently, the DRSs for the non-English languages contain references to synsets of the English WordNet. Conceptually, there is nothing wrong with this (as synsets can be viewed as identifiers for concepts that are languageindependent), but for practical reasons it makes more sense to provide links to synsets of the original language (Hamp and Feldweg, 1997; Postma et al., 2016; Roventini et al., 2000; Pianta et al., 2002). In addition, we consider implementing semantic grounding such as wikification in the Parallel Meaning Bank. As for other future work, we plan to include a more finegrained matching regarding WordNet synsets, since the current evaluation of concepts is purely string-based, with only identical strings resulting in a matching clause. For many synsets, however, it is possible to refer to them with more than one word.POS.SenseNum triple, and this should be accounted for (e.g. fox.n.02 and dodger.n.01 both refer to the same synset). In a similar vein, we plan to experiment wi"
P07-2009,P04-1014,1,0.880545,"vels of robustness and efficiency in NLP systems and made linguistically motivated large-scale language processing a possibility (Matsuzaki et al., 2007; Kaplan et al., 2004). This paper describes an NLP system which is based on syntactic and semantic formalisms from theoretical linguistics, and which we have used to analyse the entire Gigaword corpus (1 billion words) in less than 5 days using only 18 processors. This combination of detail and speed of analysis represents a breakthrough in NLP technology. The system is built around a wide-coverage Combinatory Categorial Grammar (CCG) parser (Clark and Curran, 2004b). The parser not only recovers the local dependencies output by treebank parsers such as Collins (2003), but also the long-range depdendencies inherent in constructions such as extraction and coordination. CCG is a lexicalized grammar formalism, so that each word in a sentence is assigned an elementary syntactic structure, in CCG’s case a lexical category expressing subcategorisation information. Statistical tagging techniques can assign lexical categories with high accuracy and low ambiguity (Curran et al., 2006). The combination of finite-state supertagging and highly engineered C++ leads"
P07-2009,P07-1032,1,0.634558,"teedman, 2000). A Maximum Entropy supertagger first assigns lexical categories to the words in a sentence (Curran et al., 2006), which are then combined by the parser using the combinatory rules and the CKY algorithm. Clark and Curran (2004b) describes log-linear parsing models for CCG. The features in the models are defined over local parts of CCG derivations and include word-word dependencies. A disadvantage of the log-linear models is that they require cluster computing resources for practical training (Clark and Curran, 2004b). We have also investigated perceptron training for the parser (Clark and Curran, 2007b), obtaining comparable accuracy scores and similar training times (a few hours) compared with the log-linear models. The significant advantage of Proceedings of the ACL 2007 Demo and Poster Sessions, pages 33–36, c Prague, June 2007. 2007 Association for Computational Linguistics the perceptron training is that it only requires a single processor. The training is online, updating the model parameters one sentence at a time, and it converges in a few passes over the CCGbank data. A packed chart representation allows efficient decoding, with the same algorithm — the Viterbi algorithm — finding"
P07-2009,W07-1202,1,0.65614,"teedman, 2000). A Maximum Entropy supertagger first assigns lexical categories to the words in a sentence (Curran et al., 2006), which are then combined by the parser using the combinatory rules and the CKY algorithm. Clark and Curran (2004b) describes log-linear parsing models for CCG. The features in the models are defined over local parts of CCG derivations and include word-word dependencies. A disadvantage of the log-linear models is that they require cluster computing resources for practical training (Clark and Curran, 2004b). We have also investigated perceptron training for the parser (Clark and Curran, 2007b), obtaining comparable accuracy scores and similar training times (a few hours) compared with the log-linear models. The significant advantage of Proceedings of the ACL 2007 Demo and Poster Sessions, pages 33–36, c Prague, June 2007. 2007 Association for Computational Linguistics the perceptron training is that it only requires a single processor. The training is online, updating the model parameters one sentence at a time, and it converges in a few passes over the CCGbank data. A packed chart representation allows efficient decoding, with the same algorithm — the Viterbi algorithm — finding"
P07-2009,W04-3215,1,0.743379,"Missing"
P07-2009,J03-4003,0,0.0233363,"g a possibility (Matsuzaki et al., 2007; Kaplan et al., 2004). This paper describes an NLP system which is based on syntactic and semantic formalisms from theoretical linguistics, and which we have used to analyse the entire Gigaword corpus (1 billion words) in less than 5 days using only 18 processors. This combination of detail and speed of analysis represents a breakthrough in NLP technology. The system is built around a wide-coverage Combinatory Categorial Grammar (CCG) parser (Clark and Curran, 2004b). The parser not only recovers the local dependencies output by treebank parsers such as Collins (2003), but also the long-range depdendencies inherent in constructions such as extraction and coordination. CCG is a lexicalized grammar formalism, so that each word in a sentence is assigned an elementary syntactic structure, in CCG’s case a lexical category expressing subcategorisation information. Statistical tagging techniques can assign lexical categories with high accuracy and low ambiguity (Curran et al., 2006). The combination of finite-state supertagging and highly engineered C++ leads to a parser which can analyse up to 30 sentences per second on standard hardware (Clark and Curran, 2004a"
P07-2009,E03-1071,1,0.503429,"xicalized grammar formalism, so that each word in a sentence is assigned an elementary syntactic structure, in CCG’s case a lexical category expressing subcategorisation information. Statistical tagging techniques can assign lexical categories with high accuracy and low ambiguity (Curran et al., 2006). The combination of finite-state supertagging and highly engineered C++ leads to a parser which can analyse up to 30 sentences per second on standard hardware (Clark and Curran, 2004a). The C & C tools also contain a number of Maximum Entropy taggers, including the CCG supertagger, a POS tagger (Curran and Clark, 2003a), chun33 bos@di.uniroma1.it ker, and named entity recogniser (Curran and Clark, 2003b). The taggers are highly efficient, with processing speeds of over 100,000 words per second. Finally, the various components, including the morphological analyser morpha (Minnen et al., 2001), are combined into a single program. The output from this program — a CCG derivation, POS tags, lemmas, and named entity tags — is used by the module Boxer (Bos, 2005) to produce interpretable structure in the form of Discourse Representation Structures (DRSs). 2 The CCG Parser The grammar used by the parser is extract"
P07-2009,W03-0424,1,0.650833,"xicalized grammar formalism, so that each word in a sentence is assigned an elementary syntactic structure, in CCG’s case a lexical category expressing subcategorisation information. Statistical tagging techniques can assign lexical categories with high accuracy and low ambiguity (Curran et al., 2006). The combination of finite-state supertagging and highly engineered C++ leads to a parser which can analyse up to 30 sentences per second on standard hardware (Clark and Curran, 2004a). The C & C tools also contain a number of Maximum Entropy taggers, including the CCG supertagger, a POS tagger (Curran and Clark, 2003a), chun33 bos@di.uniroma1.it ker, and named entity recogniser (Curran and Clark, 2003b). The taggers are highly efficient, with processing speeds of over 100,000 words per second. Finally, the various components, including the morphological analyser morpha (Minnen et al., 2001), are combined into a single program. The output from this program — a CCG derivation, POS tags, lemmas, and named entity tags — is used by the module Boxer (Bos, 2005) to produce interpretable structure in the form of Discourse Representation Structures (DRSs). 2 The CCG Parser The grammar used by the parser is extract"
P07-2009,P06-1088,1,0.722633,"is built around a wide-coverage Combinatory Categorial Grammar (CCG) parser (Clark and Curran, 2004b). The parser not only recovers the local dependencies output by treebank parsers such as Collins (2003), but also the long-range depdendencies inherent in constructions such as extraction and coordination. CCG is a lexicalized grammar formalism, so that each word in a sentence is assigned an elementary syntactic structure, in CCG’s case a lexical category expressing subcategorisation information. Statistical tagging techniques can assign lexical categories with high accuracy and low ambiguity (Curran et al., 2006). The combination of finite-state supertagging and highly engineered C++ leads to a parser which can analyse up to 30 sentences per second on standard hardware (Clark and Curran, 2004a). The C & C tools also contain a number of Maximum Entropy taggers, including the CCG supertagger, a POS tagger (Curran and Clark, 2003a), chun33 bos@di.uniroma1.it ker, and named entity recogniser (Curran and Clark, 2003b). The taggers are highly efficient, with processing speeds of over 100,000 words per second. Finally, the various components, including the morphological analyser morpha (Minnen et al., 2001),"
P07-2009,N04-1013,0,0.00471736,"School of Information Technologies Computing Laboratory Dipartimento di Informatica University of Sydney Oxford University Universit`a di Roma “La Sapienza” NSW 2006, Australia Wolfson Building, Parks Road via Salaria 113 james@it.usyd.edu.au Oxford, OX1 3QD, UK 00198 Roma, Italy stephen.clark@comlab.ox.ac.uk 1 Introduction The statistical modelling of language, together with advances in wide-coverage grammar development, have led to high levels of robustness and efficiency in NLP systems and made linguistically motivated large-scale language processing a possibility (Matsuzaki et al., 2007; Kaplan et al., 2004). This paper describes an NLP system which is based on syntactic and semantic formalisms from theoretical linguistics, and which we have used to analyse the entire Gigaword corpus (1 billion words) in less than 5 days using only 18 processors. This combination of detail and speed of analysis represents a breakthrough in NLP technology. The system is built around a wide-coverage Combinatory Categorial Grammar (CCG) parser (Clark and Curran, 2004b). The parser not only recovers the local dependencies output by treebank parsers such as Collins (2003), but also the long-range depdendencies inheren"
P07-2009,P06-4020,0,0.0246004,"Missing"
P07-2009,C04-1041,1,\N,Missing
P10-1022,J07-3004,0,0.679984,"ditional information. That is, sometimes the existing trees allow transformation rules to be written that improve the quality of the grammar. Linguistic theories are constantly changing, which means that there is a substantial lag between what we (think we) understand of grammar and the annotations in our corpora. The grammar engineering process we describe, which we dub rebanking, is intended to reduce this gap, tightening the feedback loop between formal and computational linguistics. X /Y Y ⇒ X Y X Y ⇒ X X /Y Y /Z ⇒ X /Z Y  X Y ⇒ X  Y /Z X Y ⇒ X /Z (&gt;) (<) (&gt;B) (< B) (< B×) CCGbank (Hockenmaier and Steedman, 2007) extends this compact set of combinatory rules with a set of type-changing rules, designed to strike a better balance between sparsity in the category set and ambiguity in the grammar. We mark typechanging rules TC in our derivations. In wide-coverage descriptions, categories are generally modelled as typed-feature structures (Shieber, 1986), rather than atomic symbols. This allows the grammar to include a notion of headedness, and to unify under-specified features. We occasionally must refer to these additional details, for which we employ the following notation. Features are annotated in squ"
P10-1022,J93-2004,0,0.0434038,"epresentations from surface strings, which is why they are sometimes referred to as deep grammars. Analyses produced by these formalisms can be more detailed than those produced by skeletal phrasestructure parsers, because they produce fully specified predicate-argument structures. Unfortunately, statistical parsers do not take advantage of this potential detail. Statistical parsers induce their grammars from corpora, and the corpora for linguistically motivated formalisms currently do not contain high quality predicateargument annotation, because they were derived from the Penn Treebank (PTB Marcus et al., 1993). Manually written grammars for these formalisms, such as the ERG HPSG grammar (Flickinger, 2000) and the XLE LFG grammar (Butt et al., 2006) produce far more detailed and linguistically correct analyses than any English statistical parser, due to the comparatively coarse-grained annotation schemes of the corpora statistical parsers are trained on. While rule-based parsers use grammars that are carefully engineered (e.g. Oepen et al., 2004), and can be updated to reflect the best linguistic analyses, statistical parsers have so far had to take what they are given. What we suggest in this paper"
P10-1022,boxwell-white-2008-projecting,0,0.0297208,"vation is shown so that instantiated variables can be seen. Carthage 0s NP (NPy /(Ny /PPz )y )NPz (NPy /(Ny /PPCarthage )y )0 s destruction N /PPy < &gt; NPdestruction In this analysis, we regard the genitive clitic as a case-marker that performs a movement operation roughly analogous to WH-extraction. Its category is therefore similar to the one used in object extraction, (N N )/(S /NP ). Figure 1 shows an example with multiple core arguments. This analysis allows recovery of verbal arguments of nominalised raising and control verbs, a construction which both Gildea and Hockenmaier (2003) and Boxwell and White (2008) identify as a problem case when aligning Propbank and CCGbank. Our analysis accommodates this construction effortlessly, as shown in Figure 2. The category assigned to decision can coindex the missing NP argument of buy with its own PP argument. When that argument is supplied by the genitive, it is also supplied to the verb, buy, filling its dependency with its agent, Google. This argument would be quite difficult to recover using a shallow syntactic analysis, as the path would be quite long. There are 494 such verb arguments mediated by nominal predicates in Sections 02-21. These analyses al"
P10-1022,W04-2705,0,0.31047,"s University of Sydney NSW 2006, Australia {mhonn,james}@it.usyd.edu.au Abstract mation from existing resources. We chose to work on CCGbank (Hockenmaier and Steedman, 2007), a Combinatory Categorial Grammar (Steedman, 2000) treebank acquired from the Penn Treebank (Marcus et al., 1993). This work is equally applicable to the corpora described by Miyao et al. (2004), Shen et al. (2008) or Cahill et al. (2008). Our first changes integrate four previously suggested improvements to CCGbank. We then describe a novel CCG analysis of NP predicateargument structure, which we implement using NomBank (Meyers et al., 2004). Our analysis allows the distinction between core and peripheral arguments to be represented for predicate nouns. With this distinction, an entailment recognition system could recognise that Google’s acquisition of YouTube entailed Google acquired YouTube, because equivalent predicate-argument structures are built for both. Our analysis also recovers nonlocal dependencies mediated by nominal predicates; for instance, Google is the agent of acquire in Google’s decision to acquire YouTube. The rebanked corpus extends CCGbank with: Once released, treebanks tend to remain unchanged despite any sh"
P10-1022,J08-1003,0,0.0439007,"Missing"
P10-1022,J07-4004,1,0.928685,"Missing"
P10-1022,J05-1004,0,0.0628466,"parser’s maximum entropy features or hyperparameters, which are tuned for CCGbank. C AT 94.4 93.9 94.0 94.0 93.8 92.2 Table 3: Parser evaluation on the rebanked corpora. Corpus +NP brackets +Quotes +Propbank +Particles All Rebanking Rebanked CCGbank LF UF LF UF 86.45 86.57 87.76 87.50 87.23 92.36 92.40 92.96 92.77 92.71 86.52 86.52 87.74 87.67 88.02 92.35 92.35 92.99 92.93 93.51 10 Conclusion Research in natural language understanding is driven by the datasets that we have available. The most cited computational linguistics work to date is the Penn Treebank (Marcus et al., 1993)1 . Propbank (Palmer et al., 2005) has also been very influential since its release, and NomBank has been used for semantic dependency parsing in the CoNLL 2008 and 2009 shared tasks. This paper has described how these resources can be jointly exploited using a linguistically motivated theory of syntax and semantics. The semantic annotations provided by Propbank and NomBank allowed us to build a corpus that takes much greater advantage of the semantic transparency of a deep grammar, using careful analyses and phenomenon-specific conversion rules. The major areas of CCGbank’s grammar left to be improved are the analysis of comp"
P10-1022,U09-1017,1,0.908881,"Missing"
P10-1022,C94-2149,0,0.085401,"nd 2009 shared tasks. This paper has described how these resources can be jointly exploited using a linguistically motivated theory of syntax and semantics. The semantic annotations provided by Propbank and NomBank allowed us to build a corpus that takes much greater advantage of the semantic transparency of a deep grammar, using careful analyses and phenomenon-specific conversion rules. The major areas of CCGbank’s grammar left to be improved are the analysis of comparatives, and the analysis of named entities. English comparatives are diverse and difficult to analyse. Even the XTAG grammar (Doran et al., 1994), which deals with the major constructions of English in enviable detail, does not offer a full analysis of these phenomena. Named entities are also difficult to analyse, as many entity types obey their own specific grammars. This is another example of a phenomenon that could be analysed much better in CCGbank using an existing resource, the BBN named entity corpus. Our rebanking has substantially improved CCGbank, by increasing the granularity and linguistic fidelity of its analyses. We achieved this by exploiting existing resources and crafting novel analyses. The process we have demonstrate"
P10-1022,W03-1008,0,0.0379571,"recovered. A non-normal form derivation is shown so that instantiated variables can be seen. Carthage 0s NP (NPy /(Ny /PPz )y )NPz (NPy /(Ny /PPCarthage )y )0 s destruction N /PPy < &gt; NPdestruction In this analysis, we regard the genitive clitic as a case-marker that performs a movement operation roughly analogous to WH-extraction. Its category is therefore similar to the one used in object extraction, (N N )/(S /NP ). Figure 1 shows an example with multiple core arguments. This analysis allows recovery of verbal arguments of nominalised raising and control verbs, a construction which both Gildea and Hockenmaier (2003) and Boxwell and White (2008) identify as a problem case when aligning Propbank and CCGbank. Our analysis accommodates this construction effortlessly, as shown in Figure 2. The category assigned to decision can coindex the missing NP argument of buy with its own PP argument. When that argument is supplied by the genitive, it is also supplied to the verb, buy, filling its dependency with its agent, Google. This argument would be quite difficult to recover using a shallow syntactic analysis, as the path would be quite long. There are 494 such verb arguments mediated by nominal predicates in Sect"
P10-1022,U08-1019,1,0.879565,"Missing"
P10-1022,P07-1031,1,0.90971,"Missing"
P10-1022,P08-1039,1,0.907886,"Missing"
P10-1022,hockenmaier-steedman-2002-acquiring,0,0.0891872,"Missing"
P10-1022,P02-1043,0,\N,Missing
P15-1146,P05-1074,1,0.5637,"DB, annotated on MTurk as described in Section 5 1514 Lexical Distributional Paraphrase Translation Path WordNet We use the lemmas, POS tags, and phrase lengths of p1 and p2 , the substrings shared by p1 and p2 , and the Levenstein, Jaccard, and Hamming distances between p1 and p2 . Given a dependency context vectors for p1 and p2 , we compute the number of shared contexts, and the Jaccard, Cosine, Lin1998, Weeds2004, Clarke2009, and Szpektor2008 similarities between the vectors. We include 33 paraphrase features distributed with PPDB, which include the paraphrase probabilities as computed in Bannard and Callison-Burch (2005). We refer the reader to Ganitkevitch and CallisonBurch (2014) for a complete description of all of the features included with PPDB. We include the number of foreign language “pivots” (translations) shared by p1 and p2 for each of 24 languages used in the construction of PPDB, as a fraction of the total number of translations observed for each of p1 and p2 . We include a sparse vector of all lexico-syntactic patterns (paths through a dependency parse) which are observed between p1 and p2 in the Annotated Gigaword corpus (Napoles et al., 2012). We include binary features indicating whether Word"
P15-1146,D07-1017,0,0.0891599,"country/patriotic drive/vehicle family/home basketball/court playing/toy islamic/jihad delay/time Unrelated girl/play found/party profit/year man/talk car/family holiday/series green/tennis sunday/tour city/south back/view Table 1: Examples of different types of entailment relations appearing in PPDB. Burch, 2007), and unrelated pairs, e.g. due to misalignments or polysemy in the foreign language. The unclear semantics severely limits the applicability of paraphrase resources to natural language understanding (NLU) tasks. Some efforts have been made to identify directionality of paraphrases (Bhagat et al., 2007; Kotlerman et al., 2010), but tasks like RTE require even richer semantic information. For example, in the T/H pair shown in Figure 1, a system needs information not only about equivalent words (12/twelve) and asymmetric entailments (riots/unrest), but also semantic exclusion (Denmark/Jordan). Such lexical entailment relations are captured by natural logic, a formalism which views natural language itself as a meaning representation, eschewing external representations such as First Order Logic (FOL). This is a great fit for automatically extracted paraphrases, since the phrase pairs themselves"
P15-1146,S14-2114,1,0.90105,"Missing"
P15-1146,W08-2222,1,0.588578,"Missing"
P15-1146,S14-2141,0,0.0747714,"dependent), compared to only 700K in PPDB-S (where the majority type is equivalent). using data motivated by the applications to information retrieval, information extraction, summarization, machine translation evaluation, and more recently, question answering (Giampiccolo et al., 2007) and essay grading (Clark et al., 2013). RTE systems vary considerably in their choice of representation and inference procedure. In the most recent shared task on RTE, some systems used deep logical representations of text, allowing them to invoke theorem provers (Bjerva et al., 2014) or Markov Logic Networks (Beltagy et al., 2014) to perform the inference, while others used shallower representations, relying on machine learning to perform inference (Lai and Hockenmaier, 2014; Zhao et al., 2014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a representation, but still perform inference using a structured algebra rather than a statistical model. Regardless of the inference procedure, improvements to external lexical resources can improve RTE systems across the board (Clark et al., 2007). 3 The Paraphrase Database (PPDB) PPDB is currently the largest available collection of paraphr"
P15-1146,W04-3205,0,0.0861134,"tempted to build or augment lexical ontologies automatically, to discover other types of lexical relations like hypernyms. Most of these approaches rely on lexico-syntactic patterns. Hearst (1992) searched for hand-written patterns (e.g. “an X is a Y”) in a large corpus in order to learn taxonomic relations between nouns. Snow et al. (2006) used dependency parses to automatically learn such patterns, which they used to augment WordNet with new hypernym relations. Similar monolingual signals have been used to learn fine-grained relationships between verbs, such as enablement and happensbefore (Chklovski and Pantel, 2004; Hashimoto et al., 2009). Recognizing Textual Entailment The shared RTE tasks (Dagan et al., 2006) have been a springboard for research in natural language inference, 1513 Figure 2: Distribution of entailment relations in different sizes of PPDB. Distributions are estimated from our manual annotations of randomly sampled pairs. PPDB-XXXL contains over 77MM paraphrase pairs (where the majority type is independent), compared to only 700K in PPDB-S (where the majority type is equivalent). using data motivated by the applications to information retrieval, information extraction, summarization, ma"
P15-1146,P11-1062,0,0.0709412,"Missing"
P15-1146,S13-2045,0,0.00626621,"ringboard for research in natural language inference, 1513 Figure 2: Distribution of entailment relations in different sizes of PPDB. Distributions are estimated from our manual annotations of randomly sampled pairs. PPDB-XXXL contains over 77MM paraphrase pairs (where the majority type is independent), compared to only 700K in PPDB-S (where the majority type is equivalent). using data motivated by the applications to information retrieval, information extraction, summarization, machine translation evaluation, and more recently, question answering (Giampiccolo et al., 2007) and essay grading (Clark et al., 2013). RTE systems vary considerably in their choice of representation and inference procedure. In the most recent shared task on RTE, some systems used deep logical representations of text, allowing them to invoke theorem provers (Bjerva et al., 2014) or Markov Logic Networks (Beltagy et al., 2014) to perform the inference, while others used shallower representations, relying on machine learning to perform inference (Lai and Hockenmaier, 2014; Zhao et al., 2014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a representation, but still perform inference usin"
P15-1146,W09-0215,0,0.0129149,"ve of the ¬ relation. Distributional features Lin and Pantel (2001) attempted to mine inference rules from text by finding paths in a dependency tree which connect the same nouns. The intuition is that good paraphrases should tend to modify and be modified by the same words. Given context vectors, Lin and Pantel (2001) used a symmetric similarity metric (Lin, 1998) to find candidate paraphrases. We build dependency context vectors for each word in our data and compute both symmetric as well as more recently proposed asymmetric similarity measures (Weeds et al., 2004; Szpektor and Dagan, 2008; Clarke, 2009), which are potentially better suited for identifying A paraphrases. Table 3 gives a comparison of the pairs which are considered “most similar” according to several of these metrics. 6.2 Bilingual features We explore a variety of bilingual features, which we expect to provide complimentary signal to the monolingual features. Each pair in PPDB is associated with several paraphrase probabilities, which are based on the probabilities of aligning each word to the foreign “pivot” phrase (a foreign translation shared by the two phrases), computed as described in Bannard and Callison-Burch (2005). W"
P15-1146,C04-1051,0,0.466255,"sk of recognizing textual entailment (RTE). In RTE, a system is given two pieces of text, often called the text (T) and the hypothesis (H), and asked to determine whether T entails H, T contradicts H, or T and H are unrelatable (Figure 1). In contrast, data-driving paraphrasing typically sidesteps developing a clear definition of “meaning the same thing” and instead “assume[s] paraphrasing is a coherent notion and concentrate[s] on devices that can produce paraphrases” (Barzilay, 2003). Recent work on paraphrase extraction has resulted in enormous paraphrase collections (Lin and Pantel, 2001; Dolan et al., 2004; Ganitkevitch et al., 2013), but the usefulness of these collections is limited by the fast-and-loose treatment of the meaning of paraphrases. One concrete definition that is sometimes used for paraphrases requires that they be bidirectionally entailing (Androutsopoulos and Malakasiotis, 2010). That is, in terms of RTE, it is assumed that if P is a paraphrase of Q, then P entails Q and Q entails P. In reality, paraphrases are often more nuanced (Bhagat and Hovy, 2013), and the entries in most paraphrase resources certainly do not match this definition. For instance, Lin and Pantel (2001) extr"
P15-1146,ganitkevitch-callison-burch-2014-multilingual,1,0.922495,"Missing"
P15-1146,S14-2055,0,0.0262925,"trieval, information extraction, summarization, machine translation evaluation, and more recently, question answering (Giampiccolo et al., 2007) and essay grading (Clark et al., 2013). RTE systems vary considerably in their choice of representation and inference procedure. In the most recent shared task on RTE, some systems used deep logical representations of text, allowing them to invoke theorem provers (Bjerva et al., 2014) or Markov Logic Networks (Beltagy et al., 2014) to perform the inference, while others used shallower representations, relying on machine learning to perform inference (Lai and Hockenmaier, 2014; Zhao et al., 2014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a representation, but still perform inference using a structured algebra rather than a statistical model. Regardless of the inference procedure, improvements to external lexical resources can improve RTE systems across the board (Clark et al., 2007). 3 The Paraphrase Database (PPDB) PPDB is currently the largest available collection of paraphrases. Compared to other paraphrase resources such as the DIRT database (12 million rules) (Lin and Pantel, 2001) and the MSR paraphrase phrase tabl"
P15-1146,P98-2127,0,0.382648,"⇠ Cosine Similarity shades/the shade yard/backyard each other/man picture/drawing practice/target Monolingual (symmetric) ¬ large/small ⌘ few/several ¬ different/same ¬ other/same ¬ put/take Monolingual (asymmetric) A boy/little boy A man/two men A child/three children ⌘ is playing/play A side/both sides ⌘ A ⌘ ⌘ ⌘ Bilingual dad/father some kid/child a lot of/many female/woman male/man Table 3: Top scoring pairs (x/y) according to various similarity measures, along with their manually classified entailment labels. Column 1 is cosine similarity based on dependency contexts. Column 2 is based on Lin (1998), column 3 on Weeds (2004), and column 4 is a novel feature. Precise definitions of each metric are given in the supplementary material. in X and in Y separate X from Y to X and/or to Y from X to Y more/less X than Y ate levels of agreement (Fleiss’s  = 0.56) (Landis and Koch, 1977). For a fuller discussion of the annotation, refer to the supplementary material. 6 Automatic Classification Table 4: Top paths associated with the ¬ class. We aim to build a classifier to automatically assign entailment types to entries in the PPDB, and to demonstrate that it performs well both intrinsically and e"
P15-1146,W07-1431,0,0.153459,"wing external representations such as First Order Logic (FOL). This is a great fit for automatically extracted paraphrases, since the phrase pairs themselves can be used as the semantic representation with minimal additional annotation. But as is, paraphrase resources lack such annotation. As a result, NLU systems rely on manually built resources like WordNet, which are limited in coverage and often lead to incorrect inferences (Kaplan and Schubert, 2001). In fact, in the most recent RTE challenge, over half of the submitted systems used WordNet (Pontiki et al., 2014). Even the NatLog system (MacCartney and Manning, 2007), which popularized natural logic for RTE, relied on WordNet and did not solve the problem of assigning natural logic relations at scale. The main contributions of this paper are: • We add a concrete, interpretable semantics to the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013), the largest paraphrase resource currently available. We give each entry in the database a label describing the entailment relationship between the phrases. • We develop a statistical model to predict these relations. The enormous size of PPDB– over 77 million phrase pairs!– makes it impossible to perform this t"
P15-1146,J10-3003,0,0.048805,"Missing"
P15-1146,N13-1092,1,0.191264,"Missing"
P15-1146,S14-2001,0,0.0556365,"Missing"
P15-1146,W07-1401,0,0.0157195,"RTE tasks (Dagan et al., 2006) have been a springboard for research in natural language inference, 1513 Figure 2: Distribution of entailment relations in different sizes of PPDB. Distributions are estimated from our manual annotations of randomly sampled pairs. PPDB-XXXL contains over 77MM paraphrase pairs (where the majority type is independent), compared to only 700K in PPDB-S (where the majority type is equivalent). using data motivated by the applications to information retrieval, information extraction, summarization, machine translation evaluation, and more recently, question answering (Giampiccolo et al., 2007) and essay grading (Clark et al., 2013). RTE systems vary considerably in their choice of representation and inference procedure. In the most recent shared task on RTE, some systems used deep logical representations of text, allowing them to invoke theorem provers (Bjerva et al., 2014) or Markov Logic Networks (Beltagy et al., 2014) to perform the inference, while others used shallower representations, relying on machine learning to perform inference (Lai and Hockenmaier, 2014; Zhao et al., 2014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a represent"
P15-1146,W12-3018,1,0.892473,"Missing"
P15-1146,D09-1122,0,0.082205,"Missing"
P15-1146,C92-2082,0,0.478737,"ources Approaches to paraphrase identification have exploited signal from distributional contexts (Lin and Pantel, 2001; Szpektor et al., 2004), comparable corpora (Dolan et al., 2004; Xu et al., 2014), and graph structures (Berant et al., 2011; Brockett et al., 2013). These approaches are scalable, but they often assume that all relations are equivalence relations (Madnani and Dorr, 2010). Several efforts have attempted to build or augment lexical ontologies automatically, to discover other types of lexical relations like hypernyms. Most of these approaches rely on lexico-syntactic patterns. Hearst (1992) searched for hand-written patterns (e.g. “an X is a Y”) in a large corpus in order to learn taxonomic relations between nouns. Snow et al. (2006) used dependency parses to automatically learn such patterns, which they used to augment WordNet with new hypernym relations. Similar monolingual signals have been used to learn fine-grained relationships between verbs, such as enablement and happensbefore (Chklovski and Pantel, 2004; Hashimoto et al., 2009). Recognizing Textual Entailment The shared RTE tasks (Dagan et al., 2006) have been a springboard for research in natural language inference, 15"
P15-1146,P06-1101,0,0.0656433,"4), comparable corpora (Dolan et al., 2004; Xu et al., 2014), and graph structures (Berant et al., 2011; Brockett et al., 2013). These approaches are scalable, but they often assume that all relations are equivalence relations (Madnani and Dorr, 2010). Several efforts have attempted to build or augment lexical ontologies automatically, to discover other types of lexical relations like hypernyms. Most of these approaches rely on lexico-syntactic patterns. Hearst (1992) searched for hand-written patterns (e.g. “an X is a Y”) in a large corpus in order to learn taxonomic relations between nouns. Snow et al. (2006) used dependency parses to automatically learn such patterns, which they used to augment WordNet with new hypernym relations. Similar monolingual signals have been used to learn fine-grained relationships between verbs, such as enablement and happensbefore (Chklovski and Pantel, 2004; Hashimoto et al., 2009). Recognizing Textual Entailment The shared RTE tasks (Dagan et al., 2006) have been a springboard for research in natural language inference, 1513 Figure 2: Distribution of entailment relations in different sizes of PPDB. Distributions are estimated from our manual annotations of randomly"
P15-1146,C08-1107,0,0.0134612,"of the paths most indicative of the ¬ relation. Distributional features Lin and Pantel (2001) attempted to mine inference rules from text by finding paths in a dependency tree which connect the same nouns. The intuition is that good paraphrases should tend to modify and be modified by the same words. Given context vectors, Lin and Pantel (2001) used a symmetric similarity metric (Lin, 1998) to find candidate paraphrases. We build dependency context vectors for each word in our data and compute both symmetric as well as more recently proposed asymmetric similarity measures (Weeds et al., 2004; Szpektor and Dagan, 2008; Clarke, 2009), which are potentially better suited for identifying A paraphrases. Table 3 gives a comparison of the pairs which are considered “most similar” according to several of these metrics. 6.2 Bilingual features We explore a variety of bilingual features, which we expect to provide complimentary signal to the monolingual features. Each pair in PPDB is associated with several paraphrase probabilities, which are based on the probabilities of aligning each word to the foreign “pivot” phrase (a foreign translation shared by the two phrases), computed as described in Bannard and Callison-"
P15-1146,W04-3206,0,0.0162431,"Missing"
P15-1146,C04-1146,0,0.0100585,"es examples of some of the paths most indicative of the ¬ relation. Distributional features Lin and Pantel (2001) attempted to mine inference rules from text by finding paths in a dependency tree which connect the same nouns. The intuition is that good paraphrases should tend to modify and be modified by the same words. Given context vectors, Lin and Pantel (2001) used a symmetric similarity metric (Lin, 1998) to find candidate paraphrases. We build dependency context vectors for each word in our data and compute both symmetric as well as more recently proposed asymmetric similarity measures (Weeds et al., 2004; Szpektor and Dagan, 2008; Clarke, 2009), which are potentially better suited for identifying A paraphrases. Table 3 gives a comparison of the pairs which are considered “most similar” according to several of these metrics. 6.2 Bilingual features We explore a variety of bilingual features, which we expect to provide complimentary signal to the monolingual features. Each pair in PPDB is associated with several paraphrase probabilities, which are based on the probabilities of aligning each word to the foreign “pivot” phrase (a foreign translation shared by the two phrases), computed as describe"
P15-1146,Q14-1034,1,0.68632,"Missing"
P15-1146,S14-2044,0,0.0221701,"tion, summarization, machine translation evaluation, and more recently, question answering (Giampiccolo et al., 2007) and essay grading (Clark et al., 2013). RTE systems vary considerably in their choice of representation and inference procedure. In the most recent shared task on RTE, some systems used deep logical representations of text, allowing them to invoke theorem provers (Bjerva et al., 2014) or Markov Logic Networks (Beltagy et al., 2014) to perform the inference, while others used shallower representations, relying on machine learning to perform inference (Lai and Hockenmaier, 2014; Zhao et al., 2014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a representation, but still perform inference using a structured algebra rather than a statistical model. Regardless of the inference procedure, improvements to external lexical resources can improve RTE systems across the board (Clark et al., 2007). 3 The Paraphrase Database (PPDB) PPDB is currently the largest available collection of paraphrases. Compared to other paraphrase resources such as the DIRT database (12 million rules) (Lin and Pantel, 2001) and the MSR paraphrase phrase table (13 million) (Dola"
P15-1146,W07-1409,0,\N,Missing
P15-1146,C98-2122,0,\N,Missing
P15-1146,S14-2004,0,\N,Missing
P15-1146,J13-3001,0,\N,Missing
P98-1024,P91-1021,0,0.174985,"individual data items. This property has been presented as an independent motivation for minimally recursive representations from the Machine Translation point of view (Copestake et al., 1995), and has been most thoroughly explored in the context of the substitution operations required for transfer. We believe we have taken this argument to its logical conclusion, in implementing a non-recursive semantic metalanguage in an appropriate data structure. This, in itself, provides sufficient motivation for opting for such representations rather than, say, feature structures or the recursive QLFs (Alshawi et al., 1991) of CLE (Alshawi, 1992). 3.2 ADT Package In general, linguistic analysis components are very sensitive to changes in input data caused by modifi4In typical AI languages, such as Lisp and Prolog, lists are built-in, and they can be ported easily to other programming languages. 165 cations of analyses or by increasing coverage. Obviously, there is a need for some kind of robustness at the interface level, especially in large distributed software projects like Verbmobil with parallel development of different components. Therefore, components that communicate with each other should abstract over d"
P98-1024,C96-1008,0,0.0267535,", and cyclicity. As far as we are aware, this is the first time that the results of linguistic components dealing with semantics can be systematically checked at module interfaces. It has been shown that this form of testing is well-suited for error detection in components with rapidly growing linguistic coverage. It is worth noting that the source language lexical coverage in the Verbmobil Research Prototype is around 2500 words, rising to 10K at the end of the second phase 6. Furthermore, the complex information produced by 5The kind of data structure used by the communication architecture (Amtrup and Benra, 1996) is, similarly, transparent to the modules. 6In the year 2000. linguistic components even makes automatic output control necessary. The same checking can be used to define a quality rating, e.g. for correctness, interpretability, etc. of the content of a VIT. Such results are much better and more productive in improving a system than common, purely quantitative, measures based on failure or success rates. 4 Conclusion We have described the interface terms used to carry linguistic information in the Verbmobil system. The amount and type of information that they carry are adapted to the various"
P98-1024,C96-1024,1,0.860683,"uage. In UDRSs quantifier dependencies and other scope information are underspecified because the constraints provide incomplete information about the assignment of object language structures to labels. However, a constraint set may be monotonically extended to provide a complete resolution. VIT semantics follows a similar strategy but somewhat extends the expressivity of the metalanguage. There are two constructs in the VIT semantic metalanguage which provide for an extension in expressivity relative to UDRSs. These have both been adopted from immediate precursors within the project, such as Bos et al. (1996), and further refined. The first of these is the mechanism of holes and plugging which originates in the hole semantics of Bos (1996). This requires a distinction between two types of metavariable employed: labels and holes. Labels denote the instantiated structures, primarily the individual predicates. Holes, on the other hand, mark the underspecified argument positions of propositional arguments and scope domains. Resolution consists of an assignment of la3We owe the term minimal recursion to Copestake et al. (1995), but the mechanism they describe was already in use in UDRSs. 163 vit( X Seg"
P98-1024,1995.tmi-1.2,0,0.245389,"These have both been adopted from immediate precursors within the project, such as Bos et al. (1996), and further refined. The first of these is the mechanism of holes and plugging which originates in the hole semantics of Bos (1996). This requires a distinction between two types of metavariable employed: labels and holes. Labels denote the instantiated structures, primarily the individual predicates. Holes, on the other hand, mark the underspecified argument positions of propositional arguments and scope domains. Resolution consists of an assignment of la3We owe the term minimal recursion to Copestake et al. (1995), but the mechanism they describe was already in use in UDRSs. 163 vit( X Segment ID vitID(sid(ll6.a.ge.2.181.2.ge.y.syntaxger). X WHG StrinE [word(jedes.24.[i5]). word(treffen.25.[19]). word(mit.26.[112]). word(ihnen.27.[114]). word(hat.28.[ll]). ,ord(ein.29.[117]). word(interessantes.30.[121]). word(thema°31.[122])]). X Index index(12.11.i3). Conditions [decl(12.h23). jed(1S,i6,18,h7), treffen(19,i6), mit(112,i6,i13), pron(l14.i13). haben(11.i3). arg3(ll,i3,i6), arg2(ll,i3,i16), ein(llT.il6.120.hl9). interessant(121.i16). thema(122.i16)]. Constraints [in_g(122.120). in_g(121.120). in_g(114.1"
P98-1024,C96-1054,1,0.811819,"a theory independent linguafranca for discussions which may involve both linguists and computer scientists. The Verbraobil community is actually large enough to require such off-line constructs, too. 1.2 Encoding of Linguistic Information A key question in the design of an interface language is what information must be carried and to what purpose. The primary definition criterion within the linguistic modules has been the translation task. The actual translation operation is performed in the transfer module as a mapping between semantic representations of the source and target languages, see (Dorna and Emele, 1996). However, the information requirements of this process are flexible, since information from various levels of analysis are used in disambiguation within the transfer module, including prosody and dialogue structure. To a large extent the information requirements divide into two parts: • the expressive adequacy of the semantic representation; • representing other types of linguistic information so as to meet the disambiguation requirements with the minimum of redundancy. The design of the semantic representations encoded within VITs has been guided by an ongoing movement in representational se"
P98-1072,P91-1021,1,0.831298,"Missing"
P98-1072,P98-1024,1,0.781595,"e lexical setting (12) holeinfo isa-hole hole island Hole So, isa-hole indicates which type of hole a structure contains. The values are no, yes, and i s l a n d , i s l a n d is used to override the argument structure to produce a plugging where 436 Implementation and Evaluation The resolution algorithm described in Section 4 has been implemented in Verbmobil, a system which translates spoken German and Japanese into English (Bub et al., 1997). The underspecified semantic representation technique we have used in this paper reflects the core semantic part of the Verbmobil Interface Term, VIT (Bos et al., 1998). The aim of VIT is to describe a consistent interface structure between the different language analysis modules within Verbmobil. Thus, in contrast to our USR, VIT is a representation that encodes all the linguistic information of an utterance; in addition to the USR semantic structure of Sectiom 2, the Verbmobil Interface Term contains prosodic, syntactic, and discourse related information. In order to evaluate the algorithm, the results of the pluggings obtained for four dialogues in the Verbmobil test set were checked (Table 1). We only consider utterances for which the VITs contain more t"
P98-1072,Y96-1006,1,0.906315,"Missing"
P98-1072,C96-1024,1,\N,Missing
P98-1072,C98-1024,1,\N,Missing
Q18-1043,W13-2322,0,0.0777827,"Reyle, 1993; Muskens, 1996; Van Eijck and Kamp 1997; Kadmon, 2001; Asher and Las-carides, 2003), dealing with many semantic phenomena: quantifiers, negation, scope ambiguities, pronouns, presuppositions, and discourse structure (see Figure 1). DRSs are recursive structures and thus form a challenge for sequence-tosequence models because they need to generate a well-formed structure and not something that looks like one but is not interpretable. The problem that we try to tackle bears similarities to the recently introduced task of mapping sentences to an Abstract Meaning Representation (AMR; Banarescu et al. 2013). But there are notable differences between DRS and AMR. Firstly, DRSs contain scope, which results in a more linguistically motivated treatment of modals, quantification, and negation. Secondly, DRSs contain a substantially higher number of variable bindings (reentrant nodes in AMR terminology), which are challenging for learning (Damonte et al., 2017). DRS parsing was attempted in the 1980s for small fragments of English (Johnson and Klein, 1986; Wada and Asher, 1986). Wide-coverage 619 Transactions of the Association for Computational Linguistics, vol. 6, pp. 619–634, 2018. Action Editor: A"
Q18-1043,basile-etal-2012-developing,1,0.830725,"acters or sequences of words; does tokenization help; and what kind of casing is best used? 3. What is the best way of dealing with variables that occur in DRSs? 4. Does adding silver data increase the performance of the neural parser? 5. What parts of semantics are learned and what parts of semantics are still challenging? 2.2 Annotated Corpora Despite a long tradition of formal interest in DRT, it is only recently that textual corpora annotated with DRSs have been made available. The Groningen Meaning Bank (GMB) is a large corpus with DRS annotation for mostly short English newspaper texts (Basile et al., 2012; Bos et al., 2017). The DRSs in this corpus are produced by an existing semantic parser and then partially corrected. The DRSs in the GMB are therefore not gold standard. A similar corpus is the Parallel Meaning Bank (PMB), which provides DRSs for English, German, Dutch, and Italian sentences based on a parallel corpus (Abzianidze et al., 2017). The PMB, too, is constructed using an existing semantic parser, but a part of it is completely manually checked and corrected (i.e., gold standard). In contrast to the GMB, the PMB involves two major We make the following contributions to semantic par"
Q18-1043,P14-1133,0,0.0977959,"Missing"
Q18-1043,E17-2039,1,0.801656,"Missing"
Q18-1043,C16-1333,1,0.844198,"ish, German, Dutch, and Italian sentences based on a parallel corpus (Abzianidze et al., 2017). The PMB, too, is constructed using an existing semantic parser, but a part of it is completely manually checked and corrected (i.e., gold standard). In contrast to the GMB, the PMB involves two major We make the following contributions to semantic parsing:1 (a) The output of our parser consists of interpretable scoped meaning representations, 1 The code is available here: https://github.com/ RikVN/Neural_DRS. 620 additions: (a) its semantics are refined by modeling tense and using semantic tagging (Bjerva et al., 2016; Abzianidze and Bos, 2017), and (b) the non-logical symbols of the DRSs corresponding to concepts and semantic roles are grounded in WordNet (Fellbaum, 1998) and VerbNet (Bonial et al., 2011), respectively. These additions make the DRSs of the PMB more fine-grained meaning representations. For this reason we choose the PMB (over the GMB) as our corpus for evaluating our semantic parser. Even though the sentences in the current release of the PMB are relatively short, they contain many difficult semantic phenomena that a semantic parser has to deal with: pronoun resolution, quantifiers, scope"
Q18-1043,P17-1112,0,0.0250347,"ently, however, neural methods, and in particular sequenceto-sequence models, have been successfully applied to a wide range of semantic parsing tasks. These include code generation (Ling et al., 2016), question answering (Dong and Lapata, 2016; He and Golub, 2016) and Abstract Meaning Representation parsing (Konstas et al., 2017). Because these models have no intrinsic knowledge of the structure (tree, graph, set) they have to produce, recent work also focused on structured decoding methods, creating neural architectures that always output a graph or a tree (Alvarez-Melis and Jaakkola, 2017; Buys and Blunsom, 2017). These methods often outperform the more general sequence-to-sequence models but are tailored to specific meaning representations. This paper will focus on parsing Discourse Representation Structures (DRSs) proposed in Discourse Representation Theory (DRT), a wellstudied formalism developed in formal semantics (Kamp, 1984; Van der Sandt, 1992; Asher, 1993; Kamp and Reyle, 1993; Muskens, 1996; Van Eijck and Kamp 1997; Kadmon, 2001; Asher and Las-carides, 2003), dealing with many semantic phenomena: quantifiers, negation, scope ambiguities, pronouns, presuppositions, and discourse structure (se"
Q18-1043,P13-2131,0,0.0992049,"onze data to further push the score of our best systems. 3.2 3.3 Evaluation A DRS parser is evaluated by comparing its output DRS to a gold standard DRS using the Counter tool (van Noord et al., 2018). Counter calculates an F-score over matching clauses. Because variable names are meaningless, obtaining the matching clauses essentially is a search for the best variable mapping between two DRSs. Counter tries to find this mapping by performing a hill-climbing search with a predefined number of restarts to avoid getting stuck in a local optimum, which is similar to the evaluation system SMATCH (Cai and Knight, 2013) for AMR parsing.4 Counter generalizes over WordNet synsets (i.e., a system is not penalized for predicting a word sense that is in the same synset as the gold standard word sense). To calculate whether there is a significant difference between two systems, we perform approximate randomization (Noreen, 1989) with α = 0.05, R = 1,000, and F (model1 ) &gt; F (model2 ) as test statistics for each individual DRS pair. Clausal Form Checker The clausal form of a DRS needs to satisfy a set of constraints in order to correspond to a semantically interpretable DRS, that is, translatable into a first-order"
Q18-1043,P07-2009,1,0.704408,"RSs. 6 6.1 Discussion Comparison In this section, we compare our best neural models (with and without silver data, see Table 6) with two baseline systems and with two DRS parsers: AMR 2 DRS and Boxer. AMR 2 DRS is a parser that obtains DRSs from AMRs by applying a set of rules (van Noord et al., 2018), in our case using AMRs produced by the AMR parser of van Noord and Bos (2017b). Boxer is an existing DRS parser using a statistical combinatory categorical grammar parser for syntactic analysis and a compositional semantics based on λ-calculus, followed by pronoun and presupposition resolution (Curran et al., 2007; Bos, 2008b). SPAR is a baseline parser that outputs the same (fixed) default DRS for each input sentence. We implemented a second baseline model, SIM - SPAR, which outputs, for each sentence in the test set, the DRS of the most similar sentence in the training set. This similarity is calculated by taking the cosine similarity of the average word embedding vector (with removed stopwords) based on the GloVe embeddings (Pennington et al., 2014). Table 8 shows the result of the comparison. The neural models comfortably outperform the baselines. We see that both our neural models 8 Note that we c"
Q18-1043,bos-2008-lets,1,0.924281,"re discourse structure by connecting two units of discourse by a discourse relation (Asher and Lascarides, 2003). Figure 1: DRS parsing in a nutshell. Given a raw text, a system has to generate a DRS in the clause format, a flat version of the standard box notation. The semantic representation formats are made more readable by using various letters for variables: the letters x, e, s, and t are used for discourse referents denoting individuals, events, states, and time, respectively, and b is used for variables denoting DRS boxes. DRS parsers based on supervised machine learning emerged later (Bos, 2008b; Le and Zuidema, 2012; Bos, 2015; Liu et al., 2018). The objectives of this paper are to apply neural methods to DRS parsing. In particular, we are interested in answers to the following research questions (RQs): 1. Are sequence-to-sequence models able to produce formal meaning representations (DRSs)? 2. What is better for input: sequences of characters or sequences of words; does tokenization help; and what kind of casing is best used? 3. What is the best way of dealing with variables that occur in DRSs? 4. Does adding silver data increase the performance of the neural parser? 5. What parts"
Q18-1043,E17-1051,0,0.0337778,"erate a well-formed structure and not something that looks like one but is not interpretable. The problem that we try to tackle bears similarities to the recently introduced task of mapping sentences to an Abstract Meaning Representation (AMR; Banarescu et al. 2013). But there are notable differences between DRS and AMR. Firstly, DRSs contain scope, which results in a more linguistically motivated treatment of modals, quantification, and negation. Secondly, DRSs contain a substantially higher number of variable bindings (reentrant nodes in AMR terminology), which are challenging for learning (Damonte et al., 2017). DRS parsing was attempted in the 1980s for small fragments of English (Johnson and Klein, 1986; Wada and Asher, 1986). Wide-coverage 619 Transactions of the Association for Computational Linguistics, vol. 6, pp. 619–634, 2018. Action Editor: Asli Celikyilmaz. Submission batch: 7/2018; Revision batch: 9/2018; Published 12/2018. c 2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. guaranteed by a specially designed checking tool (§3). (b) We compare different methods of representing input and output in §4. (c) We show in §5 that using additional, non-gold st"
Q18-1043,W08-2222,1,0.945128,"re discourse structure by connecting two units of discourse by a discourse relation (Asher and Lascarides, 2003). Figure 1: DRS parsing in a nutshell. Given a raw text, a system has to generate a DRS in the clause format, a flat version of the standard box notation. The semantic representation formats are made more readable by using various letters for variables: the letters x, e, s, and t are used for discourse referents denoting individuals, events, states, and time, respectively, and b is used for variables denoting DRS boxes. DRS parsers based on supervised machine learning emerged later (Bos, 2008b; Le and Zuidema, 2012; Bos, 2015; Liu et al., 2018). The objectives of this paper are to apply neural methods to DRS parsing. In particular, we are interested in answers to the following research questions (RQs): 1. Are sequence-to-sequence models able to produce formal meaning representations (DRSs)? 2. What is better for input: sequences of characters or sequences of words; does tokenization help; and what kind of casing is best used? 3. What is the best way of dealing with variables that occur in DRSs? 4. Does adding silver data increase the performance of the neural parser? 5. What parts"
Q18-1043,W15-1841,1,0.843442,"ng two units of discourse by a discourse relation (Asher and Lascarides, 2003). Figure 1: DRS parsing in a nutshell. Given a raw text, a system has to generate a DRS in the clause format, a flat version of the standard box notation. The semantic representation formats are made more readable by using various letters for variables: the letters x, e, s, and t are used for discourse referents denoting individuals, events, states, and time, respectively, and b is used for variables denoting DRS boxes. DRS parsers based on supervised machine learning emerged later (Bos, 2008b; Le and Zuidema, 2012; Bos, 2015; Liu et al., 2018). The objectives of this paper are to apply neural methods to DRS parsing. In particular, we are interested in answers to the following research questions (RQs): 1. Are sequence-to-sequence models able to produce formal meaning representations (DRSs)? 2. What is better for input: sequences of characters or sequences of words; does tokenization help; and what kind of casing is best used? 3. What is the best way of dealing with variables that occur in DRSs? 4. Does adding silver data increase the performance of the neural parser? 5. What parts of semantics are learned and what"
Q18-1043,W17-3203,0,0.0153535,"additional data to improve the score. For DRSs, the PMB-2.1.0 release already contains a large set of silver standard data (71,308 instances), containing DRSs that are only partially manually corrected. We then train a model on both the gold and silver standard data, making no distinction between them during training. After training we take the last model and restart the training on only the gold data, in a similar process as described in Konstas et al. (2017) and van Noord and Bos (2017b). In general, restarting the training to fine-tune the weights of the model is a common technique in NMT (Denkowski and Neubig, 2017). We are aware that there are many methods to obtain and utilize additional data. However, our main aim is not to find the optimal method for DRS parsing, but to demonstrate that using additional data is indeed beneficial for neural DRS parsing. Because we are not further fine-tuning our model, we will show results on the test set in this section. Table 6 shows the results of adding the silver data. This results in a large increase in performance, for both the character- and word-level models. We are still reliant on manually annotated data, however, because without the gold data (so training"
Q18-1043,P16-1004,0,0.022972,"creases parser performance. Adding silver training data boosts performance even further. 1 Introduction Semantic parsing is the task of mapping a natural language expression to an interpretable meaning representation. Semantic parsing used to be the domain of symbolic and statistical approaches (Pereira and Shieber, 1987; Zelle and Mooney, 1996; Blackburn and Bos, 2005). Recently, however, neural methods, and in particular sequenceto-sequence models, have been successfully applied to a wide range of semantic parsing tasks. These include code generation (Ling et al., 2016), question answering (Dong and Lapata, 2016; He and Golub, 2016) and Abstract Meaning Representation parsing (Konstas et al., 2017). Because these models have no intrinsic knowledge of the structure (tree, graph, set) they have to produce, recent work also focused on structured decoding methods, creating neural architectures that always output a graph or a tree (Alvarez-Melis and Jaakkola, 2017; Buys and Blunsom, 2017). These methods often outperform the more general sequence-to-sequence models but are tailored to specific meaning representations. This paper will focus on parsing Discourse Representation Structures (DRSs) proposed in D"
Q18-1043,D17-1151,0,0.0137376,"sequence representation of the natural language utterance, while the decoder produces the sequences of the meaning representation. We apply dropout (Srivastava et al., 2014) between both the recurrent encoding and decoding layers to prevent overfitting, and use general attention (Luong et al., 2015) to selectively give more weight to certain parts of the input sentence. An overview of the general framework of the seq2seq model is shown in Figure 3. During decoding we perform beam search with length normalization, which in neural machine translation (NMT) is crucial to obtaining good results (Britz et al., 2017). We experimented with a wide range of parameter settings, of which the final settings can be found in Table 3. We opted against trying to find the best parameter settings for each individual experiment (next to impossible in terms of computing time necessary, as a single 10-fold CV experiment takes 12 hours on GPU), but selected parameter settings that showed good performance for both the initial character and word-level representations (see §4 for details). The parameter search was performed using 10-fold CV on the training set. Training is stopped when there is no more improvement in perple"
Q18-1043,D13-1146,1,0.834418,"haracter-level or wordlevel score (F-scores between 57 and 68), only coming close when using a small number of merges (which is very close to character-level anyway). Therefore this technique was disregarded for further experiments. 4.2 Tokenization An interesting aspect of the PMB data is the way the input sentences are tokenized. In the data set, multiword expressions are tokenized as single words, for example, “New York” is tokenized to “New∼York.” Unfortunately, most off-the-shelf tokenizers (e.g., the Moses tokenizer) are not equipped to deal with this. We experiment with using Elephant (Evang et al., 2013), a tokenizer that can be (re-)trained on individual data sets, using the tokenized sentences of the published silver and gold PMB data set.7 Simultaneously, we are interested in whether character-level models need tokenization at all, which would be a possible advantage of this type of representing the input text. Results of the experiment are shown in Table 5. None of the two tokenization methods yielded a significant advantage for the character-level models, so they will not be used further. The word-level models, however, did benefit from tokenization, but Elephant did not give us an advan"
Q18-1043,P17-4012,0,0.0449941,"word-representation input. SEP is used as a special character to separate clauses in the output. Parameter Value Parameter Value RNN-type encoder-type optimizer layers nodes min freq source min freq target vector size LSTM brnn sgd 2 300 3 3 300 dropout dropout type bridge learning rate learning rate decay max grad norm beam size length normalization 0.2 naive copy 0.7 0.7 5 10 0.9 Table 3: Parameters explored during training and testing with their final values. All other parameters have default values. bidirectional long short-term memory (LSTM) layers and 300 nodes, implemented in OpenNMT (Klein et al., 2017). The network encodes a sequence representation of the natural language utterance, while the decoder produces the sequences of the meaning representation. We apply dropout (Srivastava et al., 2014) between both the recurrent encoding and decoding layers to prevent overfitting, and use general attention (Luong et al., 2015) to selectively give more weight to certain parts of the input sentence. An overview of the general framework of the seq2seq model is shown in Figure 3. During decoding we perform beam search with length normalization, which in neural machine translation (NMT) is crucial to o"
Q18-1043,P17-1014,0,0.365646,"1 Introduction Semantic parsing is the task of mapping a natural language expression to an interpretable meaning representation. Semantic parsing used to be the domain of symbolic and statistical approaches (Pereira and Shieber, 1987; Zelle and Mooney, 1996; Blackburn and Bos, 2005). Recently, however, neural methods, and in particular sequenceto-sequence models, have been successfully applied to a wide range of semantic parsing tasks. These include code generation (Ling et al., 2016), question answering (Dong and Lapata, 2016; He and Golub, 2016) and Abstract Meaning Representation parsing (Konstas et al., 2017). Because these models have no intrinsic knowledge of the structure (tree, graph, set) they have to produce, recent work also focused on structured decoding methods, creating neural architectures that always output a graph or a tree (Alvarez-Melis and Jaakkola, 2017; Buys and Blunsom, 2017). These methods often outperform the more general sequence-to-sequence models but are tailored to specific meaning representations. This paper will focus on parsing Discourse Representation Structures (DRSs) proposed in Discourse Representation Theory (DRT), a wellstudied formalism developed in formal semant"
Q18-1043,D16-1166,0,0.0210458,"nce. Adding silver training data boosts performance even further. 1 Introduction Semantic parsing is the task of mapping a natural language expression to an interpretable meaning representation. Semantic parsing used to be the domain of symbolic and statistical approaches (Pereira and Shieber, 1987; Zelle and Mooney, 1996; Blackburn and Bos, 2005). Recently, however, neural methods, and in particular sequenceto-sequence models, have been successfully applied to a wide range of semantic parsing tasks. These include code generation (Ling et al., 2016), question answering (Dong and Lapata, 2016; He and Golub, 2016) and Abstract Meaning Representation parsing (Konstas et al., 2017). Because these models have no intrinsic knowledge of the structure (tree, graph, set) they have to produce, recent work also focused on structured decoding methods, creating neural architectures that always output a graph or a tree (Alvarez-Melis and Jaakkola, 2017; Buys and Blunsom, 2017). These methods often outperform the more general sequence-to-sequence models but are tailored to specific meaning representations. This paper will focus on parsing Discourse Representation Structures (DRSs) proposed in Discourse Representati"
Q18-1043,C12-1094,0,0.188791,"ure by connecting two units of discourse by a discourse relation (Asher and Lascarides, 2003). Figure 1: DRS parsing in a nutshell. Given a raw text, a system has to generate a DRS in the clause format, a flat version of the standard box notation. The semantic representation formats are made more readable by using various letters for variables: the letters x, e, s, and t are used for discourse referents denoting individuals, events, states, and time, respectively, and b is used for variables denoting DRS boxes. DRS parsers based on supervised machine learning emerged later (Bos, 2008b; Le and Zuidema, 2012; Bos, 2015; Liu et al., 2018). The objectives of this paper are to apply neural methods to DRS parsing. In particular, we are interested in answers to the following research questions (RQs): 1. Are sequence-to-sequence models able to produce formal meaning representations (DRSs)? 2. What is better for input: sequences of characters or sequences of words; does tokenization help; and what kind of casing is best used? 3. What is the best way of dealing with variables that occur in DRSs? 4. Does adding silver data increase the performance of the neural parser? 5. What parts of semantics are learn"
Q18-1043,L18-1473,0,0.019724,"Missing"
Q18-1043,P16-1057,0,0.0469105,"Missing"
Q18-1043,P16-1002,0,0.0679332,"Missing"
Q18-1043,P18-1040,0,0.443509,"s of discourse by a discourse relation (Asher and Lascarides, 2003). Figure 1: DRS parsing in a nutshell. Given a raw text, a system has to generate a DRS in the clause format, a flat version of the standard box notation. The semantic representation formats are made more readable by using various letters for variables: the letters x, e, s, and t are used for discourse referents denoting individuals, events, states, and time, respectively, and b is used for variables denoting DRS boxes. DRS parsers based on supervised machine learning emerged later (Bos, 2008b; Le and Zuidema, 2012; Bos, 2015; Liu et al., 2018). The objectives of this paper are to apply neural methods to DRS parsing. In particular, we are interested in answers to the following research questions (RQs): 1. Are sequence-to-sequence models able to produce formal meaning representations (DRSs)? 2. What is better for input: sequences of characters or sequences of words; does tokenization help; and what kind of casing is best used? 3. What is the best way of dealing with variables that occur in DRSs? 4. Does adding silver data increase the performance of the neural parser? 5. What parts of semantics are learned and what parts of semantics"
Q18-1043,C86-1156,0,0.697349,"The problem that we try to tackle bears similarities to the recently introduced task of mapping sentences to an Abstract Meaning Representation (AMR; Banarescu et al. 2013). But there are notable differences between DRS and AMR. Firstly, DRSs contain scope, which results in a more linguistically motivated treatment of modals, quantification, and negation. Secondly, DRSs contain a substantially higher number of variable bindings (reentrant nodes in AMR terminology), which are challenging for learning (Damonte et al., 2017). DRS parsing was attempted in the 1980s for small fragments of English (Johnson and Klein, 1986; Wada and Asher, 1986). Wide-coverage 619 Transactions of the Association for Computational Linguistics, vol. 6, pp. 619–634, 2018. Action Editor: Asli Celikyilmaz. Submission batch: 7/2018; Revision batch: 9/2018; Published 12/2018. c 2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. guaranteed by a specially designed checking tool (§3). (b) We compare different methods of representing input and output in §4. (c) We show in §5 that using additional, non-gold standard data can improve performance. (d) We perform a thorough analysis of the produced output a"
Q18-1043,D15-1166,0,0.110008,"Missing"
Q18-1043,L18-1267,1,0.641697,"Missing"
Q18-1043,W17-7306,1,0.904386,"Missing"
Q18-1043,E17-1100,1,0.806981,"mple, there are only three sentences that have two words. 628 systems decreases with sentence length, thus corroborating the trends shown in Figure 6 and (ii) the interaction between parser and sentence length is not significant (i.e., none of the parsers decreases significantly more than any other with sentence length). The fact that the performance of the neural parsers degrades with sentence length is not surprising, because they are based on the seq2seq architecture, and models built on this architecture for other tasks, such as machine translation, have been shown to have the same issue (Toral and Sánchez-Cartagena, 2017). 6.3 Phenomenon Negation & modals Scope ambiguity Pronoun resolution Discourse rel. & imp. Embedded clauses # 73 15 31 33 30 Char Word Boxer 0.90 0.73 0.84 0.64 0.77 0.81 0.57 0.77 0.67 0.70 0.89 0.80 0.90 0.82 0.87 Table 10: Manual evaluation of the output of the three semantic parsers on several semantic phenomena. Reported numbers are accuracies. The results of the semantic evaluation of the parsers on the test set is given in Table 10. The character-level parser performs better than the word-level parser on all the phenomena except one. Even though both our neural parsers clearly outperfo"
Q18-1043,D14-1162,0,0.0822051,"atory categorical grammar parser for syntactic analysis and a compositional semantics based on λ-calculus, followed by pronoun and presupposition resolution (Curran et al., 2007; Bos, 2008b). SPAR is a baseline parser that outputs the same (fixed) default DRS for each input sentence. We implemented a second baseline model, SIM - SPAR, which outputs, for each sentence in the test set, the DRS of the most similar sentence in the training set. This similarity is calculated by taking the cosine similarity of the average word embedding vector (with removed stopwords) based on the GloVe embeddings (Pennington et al., 2014). Table 8 shows the result of the comparison. The neural models comfortably outperform the baselines. We see that both our neural models 8 Note that we cannot apply the manual corrections, so in PMB terminology, these data are bronze instead of silver. 627 Word Boxer All clauses 83.6 83.1 74.3 DRS Operators VerbNet roles WordNet synsets nouns verbs, adverbs, adj. 93.2 84.1 79.7 86.1 65.1 93.3 82.5 79.4 88.5 58.7 88.0 71.4 72.5 82.5 49.3 Oracle sense numbers Oracle synsets Oracle roles 86.7 90.7 87.4 85.7 90.9 87.2 78.1 83.8 82.0 0.80 0.75 0.70 0.65 0.60 Table 9: F-scores of fine-grained evalua"
Q18-1043,C86-1127,0,0.598555,"o tackle bears similarities to the recently introduced task of mapping sentences to an Abstract Meaning Representation (AMR; Banarescu et al. 2013). But there are notable differences between DRS and AMR. Firstly, DRSs contain scope, which results in a more linguistically motivated treatment of modals, quantification, and negation. Secondly, DRSs contain a substantially higher number of variable bindings (reentrant nodes in AMR terminology), which are challenging for learning (Damonte et al., 2017). DRS parsing was attempted in the 1980s for small fragments of English (Johnson and Klein, 1986; Wada and Asher, 1986). Wide-coverage 619 Transactions of the Association for Computational Linguistics, vol. 6, pp. 619–634, 2018. Action Editor: Asli Celikyilmaz. Submission batch: 7/2018; Revision batch: 9/2018; Published 12/2018. c 2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. guaranteed by a specially designed checking tool (§3). (b) We compare different methods of representing input and output in §4. (c) We show in §5 that using additional, non-gold standard data can improve performance. (d) We perform a thorough analysis of the produced output and compare our methods"
Q18-1043,W16-2323,0,0.15191,"t (next to impossible in terms of computing time necessary, as a single 10-fold CV experiment takes 12 hours on GPU), but selected parameter settings that showed good performance for both the initial character and word-level representations (see §4 for details). The parameter search was performed using 10-fold CV on the training set. Training is stopped when there is no more improvement in perplexity on the validation set, which in our case occurred after 13–15 epochs. A powerful, well-known technique in the field of NMT is to use an ensemble of models during decoding (Sutskever et al., 2014; Sennrich et al., 2016a). The resulting model averages over the predictions of the individual models, which can balance out some of the errors. In our experiments, we apply this method when decoding on the test set, but not for our experiments of 10-fold CV (this would take too much computation time). 4 Experiments with Data Representations This section describes the experiments we conduct regarding the data representations of the input (English sentences) and output (a DRS) during training. 4.1 Between Characters and Words We first try two (default) representations: characterlevel and word-level. Most semantic par"
Q18-1043,P16-1162,0,0.333781,"t (next to impossible in terms of computing time necessary, as a single 10-fold CV experiment takes 12 hours on GPU), but selected parameter settings that showed good performance for both the initial character and word-level representations (see §4 for details). The parameter search was performed using 10-fold CV on the training set. Training is stopped when there is no more improvement in perplexity on the validation set, which in our case occurred after 13–15 epochs. A powerful, well-known technique in the field of NMT is to use an ensemble of models during decoding (Sutskever et al., 2014; Sennrich et al., 2016a). The resulting model averages over the predictions of the individual models, which can balance out some of the errors. In our experiments, we apply this method when decoding on the test set, but not for our experiments of 10-fold CV (this would take too much computation time). 4 Experiments with Data Representations This section describes the experiments we conduct regarding the data representations of the input (English sentences) and output (a DRS) during training. 4.1 Between Characters and Words We first try two (default) representations: characterlevel and word-level. Most semantic par"
S12-1040,basile-etal-2012-developing,1,0.848004,"searching for contradictions in texts) is a prime example in natural language understanding. It shouldn’t therefore come as a surprise that detecting negation and adequately representing its Background The semantic representations that are used in this shared task on detecting negation in texts are constructed by means of a pipeline of natural language processing components, of which the backbone is provided by the C&C tools and Boxer (Curran et al., 2007). This tool chain is currently in use semiautomatically for constructing a large semantically annotated corpus, the Groningen Meaning Bank (Basile et al., 2012). The C&C tools are applied for tagging the data with part-of-speech and super tags and for syntactic parsing, using the formalism of Combinatory Categorial Grammar, CCG (Steedman, 2001). The output of the parser, CCG derivations, form the input of Boxer, producing formal semantic representations in the form of Discourse Representation Structures (DRSs), the basic meaning-carrying structures in the framework of Discourse Representation Theory (Kamp and Reyle, 1993). DRT is a widely accepted formal theory of natural language meaning that has been used to study a wide range of linguistic 301 Fir"
S12-1040,W08-2222,1,0.910093,"Missing"
S12-1040,P07-2009,1,0.819669,"central role to determine which truth value is at stake for a given sentence. Negation lies at the heart of deductive inference, of which consistency checking (searching for contradictions in texts) is a prime example in natural language understanding. It shouldn’t therefore come as a surprise that detecting negation and adequately representing its Background The semantic representations that are used in this shared task on detecting negation in texts are constructed by means of a pipeline of natural language processing components, of which the backbone is provided by the C&C tools and Boxer (Curran et al., 2007). This tool chain is currently in use semiautomatically for constructing a large semantically annotated corpus, the Groningen Meaning Bank (Basile et al., 2012). The C&C tools are applied for tagging the data with part-of-speech and super tags and for syntactic parsing, using the formalism of Combinatory Categorial Grammar, CCG (Steedman, 2001). The output of the parser, CCG derivations, form the input of Boxer, producing formal semantic representations in the form of Discourse Representation Structures (DRSs), the basic meaning-carrying structures in the framework of Discourse Representation"
S12-1040,S12-1035,0,0.138648,"Missing"
S12-1040,morante-daelemans-2012-conandoyle,0,0.075599,"bedded ADJP or NP. If one is found, this is annotated as a negated property. Otherwise, the verb is assumed to be a passive auxiliary and the negated event/property is again determined recursively on the basis of the first embedded VP. (iv) In all other cases, the verb itself is annotated as the negated event. To limit the undesired detection of negated events/properties outside of factual statements, the algorithm is not applied to any sentence that contains a question mark. 305 4 Results Here we discuss our results on the Shared Task as compared to the gold standard annotations provided by (Morante and Daelemans, 2012). The output of our two runs will be discussed with respect to Task 1. The first run includes the results of our system without postprocessing steps and in the second run the system is augmented with the postprocessing steps, as discussed in Section 3. During the process of evaluating the results of the training data, an issue with the method of evaluation was discovered. In the first version of the evaluation script precision was calculated using the standard tp formula: tp+fp . However, partial matches are excluded from this calculation (they are only counted as false negatives), which means"
S14-2084,J07-4004,0,0.0447581,"e most important two rules are forward and backward application: (X/Y ):f (>B ) This is motivated by our use of the spatial planner. Without forward composition, we would, e.g., not be able to build a constituent with the semantics (entity : (color : green)(color : red )(type : cube-group)) in the context of a stack consisting of green and red cubes, but no stack consisting exclusively of red cubes – the planner would filter out the intermediate constituent with semantics (entity :(color :red )(type :cube-group)). Finally, we use type-changing rules, which is standard practice in CCG parsing (Clark and Curran, 2007; Zettlemoyer and Collins, 2007). They are automatically extracted from the training data. Some of them account for unary productions within RCL expressions by introducing an additional internal node, such as the destination node in Figure 1. For example: The Lexicon 2.3 (entity/entity):b (∗2 ) Anaphora Anaphora are marked in RCL entity expressions by the subexpression (id : 1) for antecedent entities and (reference-id : 1) for anaphoric entities. The latter have the special type reference, in which case they are typically linked to the word it, or type-reference, in which case they are typica"
S14-2084,E03-1071,0,0.0398708,"Missing"
S14-2084,J07-3004,0,0.0210019,"1 2 2.1 Transforming the Trees RCL expressions are rooted ordered trees whose nodes are labeled with tags. We will write them in the form (t:h) where t is the root tag and h is the sequence of subtrees of the root’s children. Leaves are abbreviated as just their tags. In each training example, each pre-terminal (parent of a leaf) can be aligned to one or more words in the corresponding natural language expression. An example is shown in Figure 1. Since the alignments to words are not crossing, we can interpret the RCL tree as a phrase structure tree for the sentence and use the algorithm of (Hockenmaier and Steedman, 2007) to translate it to CCG. We extend the algorithm with a semantic step that makes sure the derivations would produce the original RCL expressions. Extracting a CCG from RCL CCGs (Steedman, 2001) use a small set of atomic constituent categories such as S (sentence), NP This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 Our code is available at http://www.let.rug. nl/evang/RoBox.zip 482 Proceedings of the 8th International Workshop on S"
S14-2084,D07-1071,0,0.550259,"es are forward and backward application: (X/Y ):f (>B ) This is motivated by our use of the spatial planner. Without forward composition, we would, e.g., not be able to build a constituent with the semantics (entity : (color : green)(color : red )(type : cube-group)) in the context of a stack consisting of green and red cubes, but no stack consisting exclusively of red cubes – the planner would filter out the intermediate constituent with semantics (entity :(color :red )(type :cube-group)). Finally, we use type-changing rules, which is standard practice in CCG parsing (Clark and Curran, 2007; Zettlemoyer and Collins, 2007). They are automatically extracted from the training data. Some of them account for unary productions within RCL expressions by introducing an additional internal node, such as the destination node in Figure 1. For example: The Lexicon 2.3 (entity/entity):b (∗2 ) Anaphora Anaphora are marked in RCL entity expressions by the subexpression (id : 1) for antecedent entities and (reference-id : 1) for anaphoric entities. The latter have the special type reference, in which case they are typically linked to the word it, or type-reference, in which case they are typically linked to the word one, as i"
S14-2084,N10-1069,0,\N,Missing
S14-2114,S14-2001,0,0.127342,"t and contradiction. To increase recall, we used a classifier trained on the output from our similarity task system (see Section 3) to reclassify the “neutrals” into possible entailments. Introduction The recent popularity of employing distributional approaches to semantic interpretation has also lead to interesting questions about the relationship between classic formal semantics (including its computational adaptations) and statistical semantics. A promising way to provide insight into these questions was brought forward as Shared Task 1 in the SemEval-2014 campaign for semantic evaluation (Marelli et al., 2014). In this task, a system is given a set of sentence pairs, and has to predict for each pair whether the sentences are somehow related in meaning. Interestingly, this is done using two different metrics: the first stemming from the formal tradition (contradiction, entailed, neutral), and the second in a distributional fashion (a similarity score between 1 and 5). We participated in this shared task with a system rooted in formal semantics. In particular, we were interested in finding out whether paraphrasing techniques could increase the accuracy of our system, whether meaning representations u"
S14-2114,W08-2222,1,0.883349,"hether the sentences are somehow related in meaning. Interestingly, this is done using two different metrics: the first stemming from the formal tradition (contradiction, entailed, neutral), and the second in a distributional fashion (a similarity score between 1 and 5). We participated in this shared task with a system rooted in formal semantics. In particular, we were interested in finding out whether paraphrasing techniques could increase the accuracy of our system, whether meaning representations used for textual entailment are 2.2 Technicalities The semantic parser that we used is Boxer (Bos, 2008). It is the last component in the pipeline of the C&C tools (Curran et al., 2007), comprising a tokenizer, POS-tagger, lemmatizer (Minnen et 1 To reproduce these results in a linux environment (with SWI Prolog) one needs to install the C&C tools (this includes Boxer and the RTE system), the Vampire theorem prover, the two model builders Paradox and Mace-2, and the PPDB-1.0 XL database. Detailed instructions can be found in the src/scripts/boxer/sick/README folder of the C&C tools. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings"
S14-2114,P07-2009,1,0.612665,"s is done using two different metrics: the first stemming from the formal tradition (contradiction, entailed, neutral), and the second in a distributional fashion (a similarity score between 1 and 5). We participated in this shared task with a system rooted in formal semantics. In particular, we were interested in finding out whether paraphrasing techniques could increase the accuracy of our system, whether meaning representations used for textual entailment are 2.2 Technicalities The semantic parser that we used is Boxer (Bos, 2008). It is the last component in the pipeline of the C&C tools (Curran et al., 2007), comprising a tokenizer, POS-tagger, lemmatizer (Minnen et 1 To reproduce these results in a linux environment (with SWI Prolog) one needs to install the C&C tools (this includes Boxer and the RTE system), the Vampire theorem prover, the two model builders Paradox and Mace-2, and the PPDB-1.0 XL database. Detailed instructions can be found in the src/scripts/boxer/sick/README folder of the C&C tools. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http: //creativecommons.org"
S14-2114,N13-1092,0,0.217137,"works as follows: (i) produce a formal semantic representation for each sentence for a given sentence pair; (ii) translate these semantic representations into first-order logic; (iii) use off-theshelf theorem provers and model builders to check whether the first sentence entails the second, or whether the sentences are contradictory. This is essentially an improved version of the framework introduced by Bos & Markert (2006). To generate background knowledge that could assist in finding a proof we used the lexical database WordNet (Fellbaum, 1998). We also used a large database of paraphrases (Ganitkevitch et al., 2013) to alter the second sentence in case no proof was found at the first attempt, inspired by Bosma & Callison-Burch (2006). The core system reached high precision on entailment and contradiction. To increase recall, we used a classifier trained on the output from our similarity task system (see Section 3) to reclassify the “neutrals” into possible entailments. Introduction The recent popularity of employing distributional approaches to semantic interpretation has also lead to interesting questions about the relationship between classic formal semantics (including its computational adaptations) a"
S16-1182,D15-1198,0,0.0769137,"-of (c / complete-02)) Figure 2: Gold-standard AMR. 1179 Proceedings of SemEval-2016, pages 1179–1184, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics 2.1 Boxer The semantic parser that we employed is Boxer (Bos, 2008; Bos, 2015). It is the last component in the C&C tools pipeline (Curran et al., 2007), comprising a tokenizer (Evang et al., 2013), POStagger, lemmatizer (Minnen et al., 2001), and a robust parser for CCG, Combinatory Categorial Grammar (Steedman, 2001). Overall, this parsing framework shows many points of contact with the recent work by Artzi et al. (2015), who also use CCG coupled with a formal compositional semantics. Boxer produces semantic representations based on Discourse Representation Theory (Kamp and Reyle, 1993), known as Discourse Representation Structures (DRSs), as Figure 1 shows. A DRS is a first-order representation, i.e., expressible with first-order logic. Various notations are possible, but widely used are the box-like representations shown in Figure 1. Boxes display scopes of discourse referents and contain properties of and relations between discourse referents. They are recursive structures, hence a box may contain other bo"
S16-1182,W08-2222,1,0.723603,"_______ _______________ | ||x1 ||e1 s1 || ||............. ||............... || ||equipment(x1)|>|manufacture(e1) || ||_____________ ||Manner(e1,s1) || | |Theme(e1,x1) || | |complete(s1) || | |_______________ || |___________________________________| Figure 1: DRS, as produced by Boxer. (m / manufacture-01 :ARG1 (e2 / equipment :mod (a2 / all)) :ARG1-of (c / complete-02)) Figure 2: Gold-standard AMR. 1179 Proceedings of SemEval-2016, pages 1179–1184, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics 2.1 Boxer The semantic parser that we employed is Boxer (Bos, 2008; Bos, 2015). It is the last component in the C&C tools pipeline (Curran et al., 2007), comprising a tokenizer (Evang et al., 2013), POStagger, lemmatizer (Minnen et al., 2001), and a robust parser for CCG, Combinatory Categorial Grammar (Steedman, 2001). Overall, this parsing framework shows many points of contact with the recent work by Artzi et al. (2015), who also use CCG coupled with a formal compositional semantics. Boxer produces semantic representations based on Discourse Representation Theory (Kamp and Reyle, 1993), known as Discourse Representation Structures (DRSs), as Figure 1 show"
S16-1182,W15-1841,1,0.833012,"____________ | ||x1 ||e1 s1 || ||............. ||............... || ||equipment(x1)|>|manufacture(e1) || ||_____________ ||Manner(e1,s1) || | |Theme(e1,x1) || | |complete(s1) || | |_______________ || |___________________________________| Figure 1: DRS, as produced by Boxer. (m / manufacture-01 :ARG1 (e2 / equipment :mod (a2 / all)) :ARG1-of (c / complete-02)) Figure 2: Gold-standard AMR. 1179 Proceedings of SemEval-2016, pages 1179–1184, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics 2.1 Boxer The semantic parser that we employed is Boxer (Bos, 2008; Bos, 2015). It is the last component in the C&C tools pipeline (Curran et al., 2007), comprising a tokenizer (Evang et al., 2013), POStagger, lemmatizer (Minnen et al., 2001), and a robust parser for CCG, Combinatory Categorial Grammar (Steedman, 2001). Overall, this parsing framework shows many points of contact with the recent work by Artzi et al. (2015), who also use CCG coupled with a formal compositional semantics. Boxer produces semantic representations based on Discourse Representation Theory (Kamp and Reyle, 1993), known as Discourse Representation Structures (DRSs), as Figure 1 shows. A DRS is"
S16-1182,J16-3006,1,0.832421,"otations are possible, but widely used are the box-like representations shown in Figure 1. Boxes display scopes of discourse referents and contain properties of and relations between discourse referents. They are recursive structures, hence a box may contain other boxes. 2.2 Abstract Meaning Representations At first glance, an AMR looks quite different from a DRS. Usually, an AMR is displayed as a directed graph with a unique root (Figure 2). However, it is also possible to view an AMR as a recursive structure, and then DRS and AMR have more in common than one perhaps would initially realize (Bos, 2016). The variables in an AMR correspond to discourse referents in a DRS. The colon-prefixed symbols in an AMR are similar to the two-place relation symbols in a DRS. And the forward slashes in an AMR correspond to one-place predicates in a DRS. So the main commonalities between a DRS (as produced by Boxer) and an AMR (as used at the SEMEVAL2016 shared task) are: • both use a neo-Davidsonian event semantics; • both are recursive meaning representations; • both expect normalization of date expressions. There are also some obvious differences between DRS and AMR. Some of them are theoretical and hav"
S16-1182,P07-2009,1,0.644846,"|| ||equipment(x1)|>|manufacture(e1) || ||_____________ ||Manner(e1,s1) || | |Theme(e1,x1) || | |complete(s1) || | |_______________ || |___________________________________| Figure 1: DRS, as produced by Boxer. (m / manufacture-01 :ARG1 (e2 / equipment :mod (a2 / all)) :ARG1-of (c / complete-02)) Figure 2: Gold-standard AMR. 1179 Proceedings of SemEval-2016, pages 1179–1184, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics 2.1 Boxer The semantic parser that we employed is Boxer (Bos, 2008; Bos, 2015). It is the last component in the C&C tools pipeline (Curran et al., 2007), comprising a tokenizer (Evang et al., 2013), POStagger, lemmatizer (Minnen et al., 2001), and a robust parser for CCG, Combinatory Categorial Grammar (Steedman, 2001). Overall, this parsing framework shows many points of contact with the recent work by Artzi et al. (2015), who also use CCG coupled with a formal compositional semantics. Boxer produces semantic representations based on Discourse Representation Theory (Kamp and Reyle, 1993), known as Discourse Representation Structures (DRSs), as Figure 1 shows. A DRS is a first-order representation, i.e., expressible with first-order logic. Va"
S16-1182,D13-1146,1,0.896308,"__________ ||Manner(e1,s1) || | |Theme(e1,x1) || | |complete(s1) || | |_______________ || |___________________________________| Figure 1: DRS, as produced by Boxer. (m / manufacture-01 :ARG1 (e2 / equipment :mod (a2 / all)) :ARG1-of (c / complete-02)) Figure 2: Gold-standard AMR. 1179 Proceedings of SemEval-2016, pages 1179–1184, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics 2.1 Boxer The semantic parser that we employed is Boxer (Bos, 2008; Bos, 2015). It is the last component in the C&C tools pipeline (Curran et al., 2007), comprising a tokenizer (Evang et al., 2013), POStagger, lemmatizer (Minnen et al., 2001), and a robust parser for CCG, Combinatory Categorial Grammar (Steedman, 2001). Overall, this parsing framework shows many points of contact with the recent work by Artzi et al. (2015), who also use CCG coupled with a formal compositional semantics. Boxer produces semantic representations based on Discourse Representation Theory (Kamp and Reyle, 1993), known as Discourse Representation Structures (DRSs), as Figure 1 shows. A DRS is a first-order representation, i.e., expressible with first-order logic. Various notations are possible, but widely used"
S16-1182,P11-1138,0,0.0451465,"Missing"
S17-2160,W13-2322,0,0.104141,"irectly to the input structure. Especially POS-tags can easily be added as features to the input data, while also providing valuable information. For example, proper nouns in a sentence often occur with the :name relation in the corresponding AMR, while adjectives correlate with the :mod relation. We append the corresponding POS-tag to each word in the sentence (using the C&C POS-tagger by Clark et al. (2003)), creating a new super character for each unique tag. Improvements In this section we describe the methods used to improve the neural semantic parser. Augmentation AMRs, as introduced by Banarescu et al. (2013), are rooted, directed, labeled graphs, in which the different nodes and triples are unordered by definition. However, in our tree representation of AMRs (see Figure 1), there is an order of branches. This means that we are able to permute this order into a more intuitive representation of the sentence, by matching the word order using the AMR-sentence alignments. An example of this method is shown in Figure 2. This approach can also be used to augment the training data, since we are now able generate “new” AMR-sentence pairs that can be added to our training set. However, due to the exponenti"
S17-2160,E17-1035,0,0.0482086,"us, both containing 500 AMRs. HTML-tags are removed from the sentences. Introduction Traditional open-domain semantic parsers often use statistical syntactic parsers to derive syntactic structure on which to build a meaning representation. Recently there have been interesting attempts to view semantic parsing as a translation task, mapping a source language (here: English) to a target language (a logical form of some kind). Dong and Lapata (2016) used sequence-tosequence and sequence-to-tree neural translation models to produce logical forms from sentences, while Barzdins and Gosko (2016) and Peng et al. (2017) used a similar method to produce AMRs. From a purely engineering point of view, these are interesting attempts as complex models of the semantic parsing process can be avoided. Yet little is known about the performance and fine-tuning of such parsers, and whether they can reach performance of traditional semantic parsers, or whether they could contribute to performance in an ensemble setting. In the context of SemEval-2017 Task 9 we aim 2.2 Basic Translation Model We use a seq2seq neural translation model to translate English sentences into AMRs. This is a bi-LSTM model with added attention m"
S17-2160,S16-1176,0,0.374223,"Missing"
S17-2160,N15-1040,0,0.118437,"4 We were able to reproduce the results of the character-level models for neural semantic parsing as proposed by Barzdins and Gosko (2016). Moreover, we showed improvement on their basic setting by using data-augmentation, part-ofspeech as additional input, and using super characters. The latter setting showed that a combination of character and word level input might be optimal for neural semantic parsers. Despite these enhancements, the resulting AMR parser is still outperformed by more traditional, off-the-shelf AMR parsers. Adding our neural semantic parser to an ensemble including CAMR (Wang et al., 2015), a dependency-based parser, yielded no noteworthy improvements on the overall performance. Do these results indicate that neural semantic parsers will never be competitive with more traditional statistical parsers? We don’t think so. We have the feeling that we have just scratched the surface of possibilities that neural semantic parsing can offer us, and how they possibly can complement parsers using different strategies. In future work we will explore these. Table 4: Official results on the evaluation set for both the ensemble and the seq-to-seq neural semantic parser. Figure 3: Comparison"
S17-2160,S16-1182,1,0.697694,"Missing"
S17-2160,P13-2131,0,0.249035,"Missing"
S17-2160,W03-0407,0,0.0181992,"CAS is required for the induction of cell migration” - seq2seq tree representation. 2.3 POS-tagging Character-level models might still be able to benefit from syntactic information, even when this is added directly to the input structure. Especially POS-tags can easily be added as features to the input data, while also providing valuable information. For example, proper nouns in a sentence often occur with the :name relation in the corresponding AMR, while adjectives correlate with the :mod relation. We append the corresponding POS-tag to each word in the sentence (using the C&C POS-tagger by Clark et al. (2003)), creating a new super character for each unique tag. Improvements In this section we describe the methods used to improve the neural semantic parser. Augmentation AMRs, as introduced by Banarescu et al. (2013), are rooted, directed, labeled graphs, in which the different nodes and triples are unordered by definition. However, in our tree representation of AMRs (see Figure 1), there is an order of branches. This means that we are able to permute this order into a more intuitive representation of the sentence, by matching the word order using the AMR-sentence alignments. An example of this met"
S17-2160,E17-1051,0,0.0374294,"e data set. Ultimately, the best ensemble on the test data consisted of three bio-only models, two bio + LDC models and one LDC-only model. This ensemble was used to parse the evaluation set. F-score CAMR retrained on LDC CAMR retrained on bio CAMR retrained on LDC + bio 0.399 0.585 0.582 Ensemble CAMR Ensemble CAMR + seq2seq 0.588 0.589 Table 3: Results of retraining CAMR and results of best ensemble models, tested on the biomedical test data. 3.2 Official Results In Table 4 we see the detailed results of the best seq2seq model and best ensemble on the evaluation data, using the scripts from Damonte et al. (2017).3 While CAMR has similar scores on the 3 Unofficial score for seq2seq negation; due to a mistake, all :polarity nodes were removed in the official submission. This had no influence on the final F-score. 931 test data, the score of the seq2seq model decreases by 0.04. It is interesting to note that seq2seq scores equally well without word sense disambiguation, while there is no separate module that handles this. Setting Smatch Unlabeled No WSD Named Entities Wikification Negation Concepts Reentrancies SRL seq2seq Ensemble 0.460 0.504 0.463 0.512 0.458 0.141 0.630 0.290 0.427 0.576 0.623 0.579"
S19-1027,E17-2039,1,0.846664,"Missing"
S19-1027,P18-2103,0,0.357987,"inferences. Bowman et al. (2015b) proposed an artificial dataset for logical reasoning, whose premise and hypothesis are automatically generated from a simple English-like grammar. Following this line of work, Geiger et al. (2018) presented a method to construct a complex dataset for multiple quantifiers (e.g., Every dwarf licks no rifle ⇒ No ugly dwarf licks some rifle). These datasets contain downward inferences, but they are designed not to require lexical knowledge. There are also NLI datasets which expand lexical knowledge by replacing words using lexical rules (Monz and de Rijke, 2001; Glockner et al., 2018; Naik et al., 2018; Introduction Natural language inference (NLI) has been proposed as a benchmark task for natural language understanding. This task is to determine whether a given statement (premise) semantically entails another statement (hypothesis) (Dagan et al., 2013). Large crowdsourced datasets such as SNLI (Bowman et al., 2015a) and MultiNLI (Williams et al., 2018) have been created from naturally-occurring texts for training and testing neural models on NLI. Recent reports showed that these crowdsourced datasets contain undesired biases that allow prediction of entailment labels onl"
S19-1027,W17-6901,1,0.779854,"iers (dancing w happily dancing), or adding 3.1 Source corpus We use sentences from the Parallel Meaning Bank (PMB, Abzianidze et al., 2017) as a source while creating the inference dataset. The reason behind choosing the PMB is threefold. First, the finegrained annotations in the PMB facilitate our automatic monotonicity-driven construction of inference problems. In particular, semantic tokenization and WordNet (Fellbaum, 1998) senses make narrow and broad concept substitutions easy while the syntactic analyses in Combinatory Categorial Grammar (CCG, Steedman, 2000) format and semantic tags (Abzianidze and Bos, 2017) contribute to monotonicity and polarity detection. Second, the PMB contains lexically and syntactically diverse texts from a wide range of genres. Third, the gold (silver) documents are fully (partially) manually verified, which control noise in the automated generated dataset. To prevent easy inferences, we use the sentences with more than five tokens from 5K gold and 5K silver portions of the PMB. 1 Our dataset and its generation code will be made publicly available at https://github.com/verypluming/HELP. 251 Section Size Up 7784 Down 21192 All [NP kids↓] were [VP dancing on the floor↑] Non"
S19-1027,N18-2017,0,0.0624952,"Missing"
S19-1027,D15-1075,0,0.0783552,"sentence can become longer, yet the inference is valid. FraCaS (Cooper et al., 1994) contains such logically challenging problems as downward inferences. However, it is small in size (only 346 examples) for training neural models, and it covers only simple syntactic patterns with severely restricted vocabularies. The lack of such a dataset on a large scale is due to at least two factors: it is hard to instruct crowd workers without deep knowledge of natural language syntax and semantics, and it is also unfeasible to employ experts to obtain a large number of logically challenging inferences. Bowman et al. (2015b) proposed an artificial dataset for logical reasoning, whose premise and hypothesis are automatically generated from a simple English-like grammar. Following this line of work, Geiger et al. (2018) presented a method to construct a complex dataset for multiple quantifiers (e.g., Every dwarf licks no rifle ⇒ No ugly dwarf licks some rifle). These datasets contain downward inferences, but they are designed not to require lexical knowledge. There are also NLI datasets which expand lexical knowledge by replacing words using lexical rules (Monz and de Rijke, 2001; Glockner et al., 2018; Naik et a"
S19-1027,2014.lilt-9.7,0,0.233344,"ators and syntactic structures. 3 Monotonicity Reasoning Data Creation We address three issues when creating the inference problems: (a) Detect the monotone operators and their arguments; (b) Based on the syntactic structure, induce the polarity of the argument positions; (c) Using lexical knowledge or logical connectives, narrow or broaden the arguments. Monotonicity reasoning is a sort of reasoning based on word replacement. Based on the monotonicity properties of words, it determines whether a certain word replacement results in a sentence entailed from the original one (van Benthem, 1983; Icard and Moss, 2014). A polarity is a characteristic of a word position imposed by monotone operators. Replacements with more general (or specific) phrases in ↑ (or ↓) polarity positions license entailment. Polarities are determined by a function which is always upward monotone (+) (i.e., an order preserving function that licenses entailment from specific to general phrases), always downward monotone (−) (i.e., an order reversing function) or neither, non-monotone. Determiners are modeled as binary operators, taking noun and verb phrases as the first and second arguments, respectively, and they entail sentences w"
S19-1027,W15-4002,0,0.172203,"sentence can become longer, yet the inference is valid. FraCaS (Cooper et al., 1994) contains such logically challenging problems as downward inferences. However, it is small in size (only 346 examples) for training neural models, and it covers only simple syntactic patterns with severely restricted vocabularies. The lack of such a dataset on a large scale is due to at least two factors: it is hard to instruct crowd workers without deep knowledge of natural language syntax and semantics, and it is also unfeasible to employ experts to obtain a large number of logically challenging inferences. Bowman et al. (2015b) proposed an artificial dataset for logical reasoning, whose premise and hypothesis are automatically generated from a simple English-like grammar. Following this line of work, Geiger et al. (2018) presented a method to construct a complex dataset for multiple quantifiers (e.g., Every dwarf licks no rifle ⇒ No ugly dwarf licks some rifle). These datasets contain downward inferences, but they are designed not to require lexical knowledge. There are also NLI datasets which expand lexical knowledge by replacing words using lexical rules (Monz and de Rijke, 2001; Glockner et al., 2018; Naik et a"
S19-1027,P17-1152,0,0.115701,"Missing"
S19-1027,marelli-etal-2014-sick,0,0.0337584,"ho don’t value my time). These problems contain disjunction or modifiers in downward environments where either (i) the premise P contains all words in the hypothesis H yet the inference is invalid or (ii) H contains more words than those in P yet the inference is valid.2 Although HELP contains 21K such problems, the models nevertheless misclassified them. This indicates that the difficulty in learning these non-lexical downward inferences might not come from the lack of training datasets. conjunction, and disjunction sections), (ii) FraCaS (the generalized quantifier section), (iii) the SICK (Marelli et al., 2014) test set, and (iv) MultiNLI matched/mismatched test set. We used the Matthews correlation coefficient (ranging [−1, 1]) as the evaluation metric for GLUE. Regarding other datasets, we used accuracy as the metric. We also check if our data augmentation does not decrease the performance on MultiNLI. 4.2 Results and discussion Table 3 shows that adding HELP to MultiNLI improved the accuracy of all models on GLUE, FraCaS, and SICK. Regarding MultiNLI, note that adding data for downward inference can be harmful for performing upward inference, because lexical replacements work in an opposite way i"
S19-1027,C18-1198,0,0.0606078,"al. (2015b) proposed an artificial dataset for logical reasoning, whose premise and hypothesis are automatically generated from a simple English-like grammar. Following this line of work, Geiger et al. (2018) presented a method to construct a complex dataset for multiple quantifiers (e.g., Every dwarf licks no rifle ⇒ No ugly dwarf licks some rifle). These datasets contain downward inferences, but they are designed not to require lexical knowledge. There are also NLI datasets which expand lexical knowledge by replacing words using lexical rules (Monz and de Rijke, 2001; Glockner et al., 2018; Naik et al., 2018; Introduction Natural language inference (NLI) has been proposed as a benchmark task for natural language understanding. This task is to determine whether a given statement (premise) semantically entails another statement (hypothesis) (Dagan et al., 2013). Large crowdsourced datasets such as SNLI (Bowman et al., 2015a) and MultiNLI (Williams et al., 2018) have been created from naturally-occurring texts for training and testing neural models on NLI. Recent reports showed that these crowdsourced datasets contain undesired biases that allow prediction of entailment labels only from hypothesis s"
S19-1027,W18-5441,0,0.0640217,"Missing"
S19-1027,S18-2023,0,0.0660154,"Missing"
S19-1027,L18-1239,0,0.0559481,"proposed as a benchmark task for natural language understanding. This task is to determine whether a given statement (premise) semantically entails another statement (hypothesis) (Dagan et al., 2013). Large crowdsourced datasets such as SNLI (Bowman et al., 2015a) and MultiNLI (Williams et al., 2018) have been created from naturally-occurring texts for training and testing neural models on NLI. Recent reports showed that these crowdsourced datasets contain undesired biases that allow prediction of entailment labels only from hypothesis sentences (Gururangan et al., 2018; Poliak et al., 2018b; Tsuchiya, 2018). Moreover, these standard datasets come with the so-called 250 Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM), pages 250–255 c Minneapolis, June 6–7, 2019. 2019 Association for Computational Linguistics Poliak et al., 2018a). In these works, however, little attention has been paid to downward inferences. The GLUE leaderboard (Wang et al., 2019) reported that neural models did not perform well on downward inferences, and this leaves us guessing whether the lack of large datasets for such kind of inferences that involve the interaction between lexical a"
S19-1027,N18-1101,0,0.0822646,"These datasets contain downward inferences, but they are designed not to require lexical knowledge. There are also NLI datasets which expand lexical knowledge by replacing words using lexical rules (Monz and de Rijke, 2001; Glockner et al., 2018; Naik et al., 2018; Introduction Natural language inference (NLI) has been proposed as a benchmark task for natural language understanding. This task is to determine whether a given statement (premise) semantically entails another statement (hypothesis) (Dagan et al., 2013). Large crowdsourced datasets such as SNLI (Bowman et al., 2015a) and MultiNLI (Williams et al., 2018) have been created from naturally-occurring texts for training and testing neural models on NLI. Recent reports showed that these crowdsourced datasets contain undesired biases that allow prediction of entailment labels only from hypothesis sentences (Gururangan et al., 2018; Poliak et al., 2018b; Tsuchiya, 2018). Moreover, these standard datasets come with the so-called 250 Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM), pages 250–255 c Minneapolis, June 6–7, 2019. 2019 Association for Computational Linguistics Poliak et al., 2018a). In these works, h"
S19-1027,D18-1007,0,\N,Missing
S19-1027,N19-1423,0,\N,Missing
S19-1027,W19-4810,0,\N,Missing
W03-2123,P93-1008,0,0.0306061,"apply effects(+Effects); (2) the DME agent can call other agents directly, in particular if it is not interested in the results of those requests; (3) the DME agent can use the DME server as a mediating agent, normally when the results are needed for updating the information state of the DME. The advantage of this architecture is the flexibility imposed by it, while at the same time allowing asynchronous interaction of the input/output and supporting agents with the dialogue move engine. 2.4 Supporting Agents OAA itself comes with agents for parsing and generating based on the Gemini system (Dowding et al., 1993). DIPPER provides a further set of agents to deal with natural language understanding, based on Discourse Representation Theory (Kamp and Reyle, 1993). There is an ambiguity resolution agent that resolves underspecified DRSs into fully resolved DRSs, and there is an inference agent that checks consistency of DRSs, using standard firstorder theorem proving techniques, including the theorem prover SPASS (Weidenbach et al., 1999) and the model builder MACE (McCune, 1998). DIPPER also includes a high-level dialogue planning component using O-Plan (Currie and Tate, 1991) which can be used to build"
W03-2123,E03-3005,0,0.0265549,"Missing"
W03-2123,C02-1067,1,\N,Missing
W03-2406,M95-1012,0,\N,Missing
W03-2406,P97-1003,0,\N,Missing
W03-2406,P99-1042,0,\N,Missing
W03-2406,grover-etal-2000-lt,1,\N,Missing
W03-2406,W01-1201,0,\N,Missing
W06-1602,C04-1180,1,0.892698,"Missing"
W06-1602,W02-2024,0,0.0664823,"Missing"
W06-1602,A00-1031,0,0.051983,"er”, and then with the possessive construction, yielding the wrong semantic interpretation. To deal with this, we analyse possessives that interact with the superlative as follows: Rome NP ’s ((NP/N)/(N/N)NP (NP/N)/(N/N) oldest N/N 5.1 church N This analysis yields the correct comparison set for superlative that follow a possessive noun phrase, given the following lexical semantics for the genitive: u ;S(λx.(p(x);n(λy. of(y,x) Superlative Detection Baseline system For superlative detection we generated a baseline that solely relies on part-ofspeech information. The data was tagged using TnT (Brants, 2000), using a model trained on the Wall Street Journal. In the WSJ tagset, superlatives can be marked in two different ways, depending on whether the adjective is inflected or modified by most/least. So, “largest”, for instance, is tagged as JJS, whereas “most beautiful” is a sequence of RBS (most) and JJ (beautiful). We also checked that they are followed by a common or proper noun (NN.*), allowing one word to occur in between. To cover more complex cases, we also considered pre-modification by adjectives (JJ), and cardinals (CD). In summary, we matched on sequences found by the following pattern"
W06-1602,J96-2004,0,0.0125046,"Missing"
W06-1602,P04-1014,0,0.0239456,"Missing"
W06-1602,W05-0619,0,0.0209342,"Missing"
W06-1602,P02-1043,0,0.0128512,"a working system. 4.1 1993)). We follow (Bos et al., 2004; Bos, 2005) in automatically building semantic representation on the basis of CCG derivations in a compositional fashion. We briefly summarise the approach here. The semantic representation for a word is determined by its CCG category, POS-tag, and lemma. Consider the following lexical entries: Combinatory Categorial Grammar (CCG) CCG is a lexicalised theory of grammar (Steedman, 2001). We used Clark & Curran’s widecoverage statistical parser (Clark and Curran, 2004) trained on CCG-bank, which in turn is derived from the Penn-Treebank (Hockenmaier and Steedman, 2002). In CCG-bank, the majority of superlative adjective of cases are analysed as follows: the tallest woman NP/N N/N N the: λp.λq.( tallest: λp.λx.( man: λx. ;p(x);q(x)) ( y y6=x ;p(y))⇒ ;p(x)) taller(x,y) man(x) These lexical entries are combined in a compositional fashion following the CCG derivation, using the λ-calculus as a glue language: N NP most devastating droughts (N/N)/(N/N) N/N N tallest man: λx. N/N man(x) y ⇒ y6=x taller(x,y) man(y) N third largest bank N/N (N/N)(N/N) N/N N N x man(x) y the tallest man: λq.( ⇒ y6=x taller(x,y) man(y) Clark & Curran’s parser outputs besides a CCG de"
W08-2220,W08-2227,0,0.0593038,"Missing"
W08-2220,bos-2008-lets,1,0.876492,"ce, the question as to how to measure the quality of semantic representations output by these systems pops up. This is an important issue for the sake of the field, but difficult to answer. On the one hand one might think that the quality of semantic representations, because they are more abstract than surface and syntactic representations, should be easy to evaluate. On the other hand, however, because there are several “competing” semantic formalisms, and the depth of analysis is arbitrary, it is hard to define a universal theory-neutral gold standard for semantic representations (see, e.g. Bos, 2008a). Partly in response to this situation in the field, a “shared task” was organised as a special event on the STEP 2008 conference. The aim of this shared task was primarily to compare semantic representations for texts as output by state-of-the-art NLP systems. This was seen as a first step for designing evaluation methodologies in computational semantics, with a practical bottom-up strategy: rather than defining theoretical gold standard representations, we look what current systems can actually produce and start working from that. 2 Participants In response to the call for participation se"
W08-2220,W08-2222,1,0.912372,"ce, the question as to how to measure the quality of semantic representations output by these systems pops up. This is an important issue for the sake of the field, but difficult to answer. On the one hand one might think that the quality of semantic representations, because they are more abstract than surface and syntactic representations, should be easy to evaluate. On the other hand, however, because there are several “competing” semantic formalisms, and the depth of analysis is arbitrary, it is hard to define a universal theory-neutral gold standard for semantic representations (see, e.g. Bos, 2008a). Partly in response to this situation in the field, a “shared task” was organised as a special event on the STEP 2008 conference. The aim of this shared task was primarily to compare semantic representations for texts as output by state-of-the-art NLP systems. This was seen as a first step for designing evaluation methodologies in computational semantics, with a practical bottom-up strategy: rather than defining theoretical gold standard representations, we look what current systems can actually produce and start working from that. 2 Participants In response to the call for participation se"
W08-2220,W08-2224,0,0.141529,"System BLUE Boxer GETARUNS LXGram OntoSem TextCap Trips Type of Formalism Logical Form Discourse Representation Theory Situation Semantics Minimal Recursion Semantics Ontological Semantics Semantic Triples Logical Form Authors Clark and Harrison Bos Delmonte Branco and Costa Nirenburg et al. Callaway Allen et al. Pages 263–276 277–286 287–298 299–314 315–326 327–342 343–354 All but one group have NLP systems developed to deal with the English language. One group has an NLP system for Portuguese (LXGram). This made it more difficult to organise the task (the English text had to be translated, Branco and Costa (2008)), Introduction to the Shared Task on Comparing Semantic Representations 259 but also more interesting. After all, it is a reasonable assumption that semantic representations ought to be independent of the source language. Also note that basically all participants adopt different semantic formalisms (Table 1), even though they all claim to do more or less the same thing: computing semantic representations for text. These differences in (formal) background make the shared task only more interesting. 3 The Shared Task Texts All participants were asked to submit an authentic small text, not excee"
W08-2220,W08-2226,0,0.0899138,"of 150 kW. In 2006, commercial, utility-scale turbines are commonly rated at over 1 MW and are available in up to 4 MW capacity. The first text is taken from an AP Physics exam (the fourth sentence is a simplified reformulation of the third sentence) and constitutes a multi-sentence science question (Clark and Harrison, 2008). Text 2 is taken from the Economist, with the third sentence slightly simplified (Bos, 2008b). Text 4 was taken from a Portuguese newspaper and translated into English (Branco and Costa, 2008). Text 6 is also a fragment of a newspaper article, namely the New York Times (Callaway, 2008). Text 7 is an excerpt from http://science.howstuffworks.com. The origin of Text 3 is unknown. 4 Preliminary Results All groups produced semantic representations for the texts using their NLP systems. The results are, for obvious reasons of space, not all listed here, but available at the SIGSEM website http://www.sigsem.org. The papers that follow the current article describe the individual results in detail. It should be noted that two groups created gold standard representations for all seven texts, and already performed a self evaluation (Nirenburg et al., 2008; Allen et al., 2008). The wo"
W08-2220,W08-2221,0,0.0303547,"gy in the United States started to subside, but it picked up again after the U.S. oil shortage in the early 1970s. Over the past 30 years, research and development has fluctuated with federal government interest and tax incentives. In the mid-’80s, wind turbines had a typical maximum power rating of 150 kW. In 2006, commercial, utility-scale turbines are commonly rated at over 1 MW and are available in up to 4 MW capacity. The first text is taken from an AP Physics exam (the fourth sentence is a simplified reformulation of the third sentence) and constitutes a multi-sentence science question (Clark and Harrison, 2008). Text 2 is taken from the Economist, with the third sentence slightly simplified (Bos, 2008b). Text 4 was taken from a Portuguese newspaper and translated into English (Branco and Costa, 2008). Text 6 is also a fragment of a newspaper article, namely the New York Times (Callaway, 2008). Text 7 is an excerpt from http://science.howstuffworks.com. The origin of Text 3 is unknown. 4 Preliminary Results All groups produced semantic representations for the texts using their NLP systems. The results are, for obvious reasons of space, not all listed here, but available at the SIGSEM website http://w"
W08-2220,W08-2223,0,0.0442872,"Missing"
W08-2220,W08-2225,0,0.0577615,"Missing"
W08-2222,J03-2002,1,0.723516,"viously established discourse entities or accommodated on a suitable level of discourse. Van der Sandt’s proposal is cast in DRT, and therefore relatively easy to integrate in Boxer’s semantic formalism. The α-operator indicates information that has to be resolved in the context, and is lexically introduced by anaphoric or presuppositional expressions. A DRS constructed with α resembles the proto-DRS of Van der Sandt’s theory of presupposition (Van der Sandt, 1992) although they are syntactically defined in a slightly different way to overcome problems with free and bound variables, following Bos (2003). Note that the difference between anaphora and presupposition collapses in Van der Sandt’s theory. The types are the ingredients of a typed lambda calculus that is employed to construct DRSs in a bottom-up fashion, compositional way. The language of lambdaWide-Coverage Semantic Analysis with Boxer 281 DRSs is an extension of the language of (U)DRS defined before: &lt;expe > ::= &lt;ref> |&lt;vare > &lt;expt > ::= &lt;udrs> |&lt;vart > &lt;expα > ::= (&lt;exphβ,αi > @ &lt;varβ >) |&lt;varα > &lt;exphα,βi > ::= λ&lt;varα >.&lt;expβ > |&lt;varhα,βi > Hence we define discourse referents as expressions of type e, and DRSs as expressions o"
W08-2222,W08-2220,1,0.734434,"Missing"
W08-2222,C04-1180,1,0.211627,"roper discourse structure. The units of measurement in the last two sentences were not Wide-Coverage Semantic Analysis with Boxer 285 recognised as such. The tricky time expression “mid-80’s” only got a shallow interpretation. 5 Conclusion Boxer is a wide-coverage system for semantic interpretation. It takes as input a CCG derivation of a natural language expression, and produces formally interpretable semantic representations: either in the form of DRSs, or as formulas of first-order logic. The existence of CCGbank (Hockenmaier, 2003) and robust parsers trained on it (Clark and Curran, 2004; Bos et al., 2004) make Boxer a state-of-the-art opendomain tool for deep semantic analysis. Boxer’s performance on the shared task for comparing semantic represtations was promising. It was able to produce DRSs for all texts. We can’t quantify the quality of Boxer’s output, as we don’t have gold standard representations at our disposal. Manually inspecting the output gives us the following impression: • computed predicate argument structure is generally of good quality, including hard constructions involving control or coordination; • discourse structure triggered by conditionals, negation or discourse adverbs"
W08-2222,P04-1014,0,0.0238278,"nce” did not introduce proper discourse structure. The units of measurement in the last two sentences were not Wide-Coverage Semantic Analysis with Boxer 285 recognised as such. The tricky time expression “mid-80’s” only got a shallow interpretation. 5 Conclusion Boxer is a wide-coverage system for semantic interpretation. It takes as input a CCG derivation of a natural language expression, and produces formally interpretable semantic representations: either in the form of DRSs, or as formulas of first-order logic. The existence of CCGbank (Hockenmaier, 2003) and robust parsers trained on it (Clark and Curran, 2004; Bos et al., 2004) make Boxer a state-of-the-art opendomain tool for deep semantic analysis. Boxer’s performance on the shared task for comparing semantic represtations was promising. It was able to produce DRSs for all texts. We can’t quantify the quality of Boxer’s output, as we don’t have gold standard representations at our disposal. Manually inspecting the output gives us the following impression: • computed predicate argument structure is generally of good quality, including hard constructions involving control or coordination; • discourse structure triggered by conditionals, negation o"
W08-2222,P07-2009,1,0.61519,":= &lt;udrs> |&lt;vart > &lt;expα > ::= (&lt;exphβ,αi > @ &lt;varβ >) |&lt;varα > &lt;exphα,βi > ::= λ&lt;varα >.&lt;expβ > |&lt;varhα,βi > Hence we define discourse referents as expressions of type e, and DRSs as expressions of type t. We use @ to indicate function application, and the λ-operator to bind free variables over which we wish to abstract. 3 Practice 3.1 Preprocessing The input text needs to be tokenised with one sentence per line. In the context of this paper, Boxer was put into action after using a combined processing pipeline of the C&C tools consisting of POS-tagging, named entity recognition, and parsing (Curran et al., 2007). The POS tags are used to specify the lexical semantics for ambiguous CCG categories (see below); the named entity tags are transferred to the level of DRSs as well and added as sorts to named discourse referents. An example of a CCG derivation is shown in Figure 2. a virus --[lex] --[lex] by np:nb/n n ---------------------[lex] -----------[fa] Cervical cancer caused ((s:pss
p)(s:pss
p))/np np:nb ---[lex] --[lex] ---[lex] --------------------------------------[fa] n/n n is s:pss
p (s:pss
p)(s:pss
p) ------------[fa] ----------------[lex] -----------------------------------------------["
W09-3705,W06-1602,1,0.915455,"spectively) and arrive at a DRS which meaning can be glossed as “the boy which is taller than every other boy”. In this example, the comparison set of the superlative expression is just the set of boys. Its semantic scope is established in the antecedent of the DRS conditional introduced by the lexical semantics of the superlative. 3 3.1 Superlatives in Genitive Constructions The Problem As shown in the previous section, the analysis given to a superlative adjectives seems to be satisfactory: it can be done in a compositional way, and it yields the desired interpretation. However, as noted in [BN06], this 21 analysis does not easily carry over to superlative expressions that combine with prenominal genitive expressions (constructions marked by the clitic ’s or possessive pronouns), such as “AS Roma’s fastest player”, “his least expected answer”, and “London’s most expensive restaurant”. Why is this the case? Let’s consider the comparison set of AS Roma’s fastest player, which is not just a set of players, but the set of players from AS Roma. It seems here that the superlative adjective out-scopes the possessive noun phrase. But this is not the effect that we get if we follow the standard"
W09-3705,J06-2002,0,0.0708164,"Missing"
W11-2819,P98-1013,0,0.0514121,"al NLG pipeline. • DRT, being a theory of analysing meaning, is by principle language-neutral; 2 • Many linguistic phenomena are studied in the framework provided by DRT; Various semantically annotated corpora of reasonable size exist nowadays: PropBank (Palmer et al., 145 The Groningen SemBank Proceedings of the 13th European Workshop on Natural Language Generation (ENLG), pages 145–150, c Nancy, France, September 2011. 2011 Association for Computational Linguistics Figure 1: Screenshot of SemBank’s visualisation tool for the syntax-semantics interface combining CCG and DRT. 2005), FrameNet (Baker et al., 1998), the Penn Discourse TreeBank (Prasad et al., 2005), and several resources developed for shared tasks such as CoNNL and SemEval. Annotated corpora that combine various levels of annotation into one formalism hardly exist. A notable exception is OntoNotes (Hovy et al., 2006), combining syntax (Penn Treebank style), predicate argument structure (based on PropBank), word senses, and coreference. Yet all of these resources lack a level comprising a formally grounded “deep” semantic representation that combines various layers of linguistic annotation. Filling this gap is exactly the purpose of SemB"
W11-2819,C10-1012,0,0.0331639,"way too costly. But given the developments in (mostly statistical) parsing of the last two decades we are now in a position to use state-ofthe-art tools to semi-automatically produce gold (or nearly gold) standard DRS-annotated corpora. Such a resource could form a good basis to develop (statistical) NLG systems, and this thought is supported by current trends in broad-coverage NLG components (Elhadad and Robin, 1996; White et al., 2007), that take deep semantic representations as starting points for surface realisation. The importance of a multi-level resource for generation is underlined by Bohnet et al. (2010), who feel the lack of such a resource is hampering progress in the field. In this paper we show how we are building such a corpus (SemBank, Section 2), what the exact nature of the DRSs in this corpus is, and what phenomena are covered (Section 3). We also illustrate what challenges it poses upon micro planning and surface realisation (Section 4). Finally, in Section 5, we discuss how generating from DRSs relates to the traditional NLG pipeline. • DRT, being a theory of analysing meaning, is by principle language-neutral; 2 • Many linguistic phenomena are studied in the framework provided by"
W11-2819,W08-2222,1,0.920882,"Missing"
W11-2819,W08-2230,0,0.0168099,"n integration in SemBank Theory/Source Internal DRS Encoding DRT (Kamp and Reyle, 1993) drs(...,...) ACE named(X,’Clinton’,per) VerbNet (Kipper et al., 2008) rel(E,X,’Agent’) WordNet (Fellbaum, 1998) pred(X,loon,n,2) SDRT (Asher and Lascarides, 2003) rel(K1,K2,elaboration) sourcing methods, comprising (i) a group of experts that are able to propose corrections at various levels of annotation in a wiki-based fashion; and (ii) a group of non-experts that provide information for the lower levels of annotation decisions by way of a Game with a Purpose, similar to the successful Phrase Detectives (Chamberlain et al., 2008) and Jeux de Mots (Artignan et al., 2009). 3 Discourse Representation Structures A DRS comprises two parts: a set of discourse referents (the entities introduced in the text), and a set of conditions, describing the properties of the referents and the relations between them. We adopt well-known extensions to the standard theory to include rhetorical relations (Asher, 1993) and presuppositions (Van der Sandt, 1992). DRSs are traditionally visualised as boxes, with the referents placed in the top part, and the DRS conditions in the bottom part. The convention in SemBank is to sort the discourse"
W11-2819,P04-1014,0,0.0443062,"Missing"
W11-2819,W96-0501,0,0.118128,"e are, mostly, purely theoretical considerations. But in order to make DRSs a practical platform for developing NLG systems a large corpus of text annotated with DRSs is required. Doing this manually is way too costly. But given the developments in (mostly statistical) parsing of the last two decades we are now in a position to use state-ofthe-art tools to semi-automatically produce gold (or nearly gold) standard DRS-annotated corpora. Such a resource could form a good basis to develop (statistical) NLG systems, and this thought is supported by current trends in broad-coverage NLG components (Elhadad and Robin, 1996; White et al., 2007), that take deep semantic representations as starting points for surface realisation. The importance of a multi-level resource for generation is underlined by Bohnet et al. (2010), who feel the lack of such a resource is hampering progress in the field. In this paper we show how we are building such a corpus (SemBank, Section 2), what the exact nature of the DRSs in this corpus is, and what phenomena are covered (Section 3). We also illustrate what challenges it poses upon micro planning and surface realisation (Section 4). Finally, in Section 5, we discuss how generating"
W11-2819,N06-2015,0,0.0399782,"he Groningen SemBank Proceedings of the 13th European Workshop on Natural Language Generation (ENLG), pages 145–150, c Nancy, France, September 2011. 2011 Association for Computational Linguistics Figure 1: Screenshot of SemBank’s visualisation tool for the syntax-semantics interface combining CCG and DRT. 2005), FrameNet (Baker et al., 1998), the Penn Discourse TreeBank (Prasad et al., 2005), and several resources developed for shared tasks such as CoNNL and SemEval. Annotated corpora that combine various levels of annotation into one formalism hardly exist. A notable exception is OntoNotes (Hovy et al., 2006), combining syntax (Penn Treebank style), predicate argument structure (based on PropBank), word senses, and coreference. Yet all of these resources lack a level comprising a formally grounded “deep” semantic representation that combines various layers of linguistic annotation. Filling this gap is exactly the purpose of SemBank. It provides a collection of semantically annotated texts with deep rather than shallow semantics. Its goal is to integrate phenomena instead of covering single phenomena into one formalism, and representing texts, not sentences. SemBank is driven by linguistic theory,"
W11-2819,P10-2013,0,0.0324653,"Missing"
W11-2819,J05-1004,0,0.0449776,"Missing"
W11-2819,J06-2002,0,0.0630305,"Missing"
W11-2819,2007.mtsummit-ucnlg.4,0,0.0946826,"oretical considerations. But in order to make DRSs a practical platform for developing NLG systems a large corpus of text annotated with DRSs is required. Doing this manually is way too costly. But given the developments in (mostly statistical) parsing of the last two decades we are now in a position to use state-ofthe-art tools to semi-automatically produce gold (or nearly gold) standard DRS-annotated corpora. Such a resource could form a good basis to develop (statistical) NLG systems, and this thought is supported by current trends in broad-coverage NLG components (Elhadad and Robin, 1996; White et al., 2007), that take deep semantic representations as starting points for surface realisation. The importance of a multi-level resource for generation is underlined by Bohnet et al. (2010), who feel the lack of such a resource is hampering progress in the field. In this paper we show how we are building such a corpus (SemBank, Section 2), what the exact nature of the DRSs in this corpus is, and what phenomena are covered (Section 3). We also illustrate what challenges it poses upon micro planning and surface realisation (Section 4). Finally, in Section 5, we discuss how generating from DRSs relates to"
W11-2819,C98-1013,0,\N,Missing
W13-0122,basile-etal-2012-developing,1,0.910521,"nized as follows. First, a theoretical background on projection phenomena in DRT is provided, focusing on van der Sandt’s (1992) approach to presuppositions. In Section 3 we introduce Projective DRT, describing its preliminaries and how it deals with different types of (projective) content. The interpretation of PDRT is given via a translation to standard DRT, described in Section 4. Finally, Section 5 presents the conclusion and indicates directions for future work, describing an ongoing effort to implement PDRT into a large corpus of semantically annotated texts: the Groningen Meaning Bank (Basile et al., 2012). 2 Background Presuppositions have a long history in the formal semantics and pragmatics literature (see, e.g., Beaver and Geurts, 2011, for an overview). In this paper, we focus on a specific representational theory of presuppositions based on Discourse Representation Theory (DRT; Kamp and Reyle, 1993). 2.1 Presuppositions as anaphora In the theory of van der Sandt (1992), presupposition projection is treated on a par with anaphora resolution. This approach is motivated by the observation that presuppositions and anaphora display similar behaviour, since they both project their content from"
W13-0122,J03-2002,1,0.595501,"orporate other interpretative models, for example a three-valued logic to account for presupposition failure in terms of undefinedness (see, e.g., Krahmer, 1998). 3.3 PDRS composition Most presuppositional theories are lexically driven, i.e., based on the assumption that specific lexical items give rise to presuppositions (so-called ‘presupposition triggers’). Therefore, projected material will be manifested in the lexical semantics of projection triggers. Various authors have proposed a compositional treatment of DRT using basic tools from Montague Grammar and lambda calculus (Muskens, 1996; Bos, 2003; de Groote, 2006). Compositionality in PDRT is realised by providing every lexical item with an (unresolved) semantics in the form of a typed lambda term. In order to combine these unresolved semantics, a merge operation can be applied that combines two PDRSs into one by means of merge-reduction (see, e.g., Bos, 2003). In the current framework, we use different types of merge for asserted, presupposed and conventionally implied material in order to account for their different compositional properties. In PDRT, projected material is not interpreted on a different level than asserted material,"
W13-0122,W08-2222,1,0.91285,"ntermediate accommodation, which are difficult to capture in Schlenker’s account. Moreover, PDRT allows for a more fine-grained analysis, since each referent and condition is associated with an interpretation site, while Schlenker only projects complete propositions. 4 Translation PDRT to DRT The semantics of PDRSs can be described via a translation to standard DRT (Kamp and Reyle, 1993). As described above, PDRT is not strictly limited to this interpretation and may be extended to incorporate other interpretation models. We implemented PDRT as part of the wide-coverage semantic parser Boxer (Bos, 2008), including an automatic translation to standard DRT. Below we only provide a sketch of the algorithmic translation to DRT, as space limitations do not permit a description of the full translation. 4.1 Translation procedure For the translation to DRT we make use of PDRT’s separation of logical structure and linguistic content. Since each referent and condition is associated with a pointer to its accommodation site, it is possible to first separate this content from the embedded PDRS structure and accordingly project each condition to its appointed site. We assume that α-conversion is applied t"
W13-0203,basile-etal-2012-developing,1,0.849463,"al complexity of underspecified representations. At the same time, this approach avoids the need to change the syntactic parsing model. In this paper we show how we applied this technique to the semantic parsing system of Curran et al. (2007). It consists of C&C, a statistical syntactic parser whose output includes predicate-argument structures, and Boxer, a semantic construction component which builds interpretable semantic representations. The resulting enhanced system is being used in constructing the Groningen Meaning Bank (GMB), a large corpus of English text annotated with logical form (Basile et al., 2012). 2 Formal Background Boxer’s lexical-semantic framework (Bos, 2009) uses the output of the C&C parser, which consists of derivations in Combinatory Categorial Grammar (CCG; Steedman, 2001). In these derivations, words and smaller constituents combine into larger constituents using combinatory rules such as forward application, which combines a functor category X/Y : f with an argument category Y : a to yield a new constituent with category X : f @a (1-place function f applied to a). Left of the colon are syntactic λp.( x ; (p@x)) member(x) λp. x ⇒ (p@x) member(x) Figure 1: Generalized quantif"
W13-0203,P07-2009,1,0.672741,"additional layer of tags is a good way. A semantic parser can use these tags to guide its decisions—just like it might use a layer of word sense tags and a layer of named-entity tags, and just like a syntactic parser uses part-of-speech tags. Scope tags can guide the construction of logical forms from the start and thereby avoid the additional computational and representational complexity of underspecified representations. At the same time, this approach avoids the need to change the syntactic parsing model. In this paper we show how we applied this technique to the semantic parsing system of Curran et al. (2007). It consists of C&C, a statistical syntactic parser whose output includes predicate-argument structures, and Boxer, a semantic construction component which builds interpretable semantic representations. The resulting enhanced system is being used in constructing the Groningen Meaning Bank (GMB), a large corpus of English text annotated with logical form (Basile et al., 2012). 2 Formal Background Boxer’s lexical-semantic framework (Bos, 2009) uses the output of the C&C parser, which consists of derivations in Combinatory Categorial Grammar (CCG; Steedman, 2001). In these derivations, words and"
W13-0203,J03-1004,0,0.539336,"of scope-bearing operators such as quantifiers, negation and intensional verbs is crucial for constructing logical form from text that capture the intended meaning. A lot of work has been done to describe the scope alternation behavior especially of quantifiers and to construct underspecified meaning representations from which all (theoretically) possible readings of a sentence containing them can be enumerated (Reyle, 1993; Bos, 1996; Copestake et al., 2005; Egg et al., 2001). The problem of finding the preferred reading in each concrete case has also been addressed, using machine learning (Higgins and Sadock, 2003; Andrew and MacCartney, 2004; Srinivasan and Yates, 2009; Manshadi and Allen, 2011). Despite these efforts, existing wide-coverage semantic parsers do not typically resolve scope. Either the semantic representations they output are shallow and do not use logical quantifiers, which is sufficient for many applications but not for model-theoretic reasoning. Or they leave the output underspecified or always deterministically pick the same scoping behavior for a given quantifier. How to make a semantic parser aware of scope alternation? We argue that an additional layer of tags is a good way. A se"
W13-0203,J07-3004,0,0.0361933,"ition semantics effecting the desired scope behavior are straightforward to implement (Figure 5). For noun phrase modifiers, things are trickier. Firstly, narrow-scope modifiers need to apply to individual properties whereas wide-scope modifiers need to apply to generalized quantifiers. The easiest way to ensure this is to give the preposition the syntactic type ((N N )/NP ) in the former case and ((NP NP )/NP ) in the latter before parsing. Thus, in this case, the syntactic structure is affected. Both categories are well supported in the standard model of the C&C parser trained on CCGbank (Hockenmaier and Steedman, 2007), and although the assignment of lexical categories is integrated with parsing, the parser can be forced using tricks to pick a particular category. Example derivations for both cases are shown in Figures 3 and 4. The second issue with noun phrase modifiers is more severe: specific modifiers of universally quantified noun phrases as in (f) cannot be accounted for in a similar way without heavily modifying the lexical-semantic framework because universally quantified noun phrases keep only their nuclear scope open for modification, not their restrictor. We will return to this limitation in the"
W13-0203,W11-1108,0,0.290004,"crucial for constructing logical form from text that capture the intended meaning. A lot of work has been done to describe the scope alternation behavior especially of quantifiers and to construct underspecified meaning representations from which all (theoretically) possible readings of a sentence containing them can be enumerated (Reyle, 1993; Bos, 1996; Copestake et al., 2005; Egg et al., 2001). The problem of finding the preferred reading in each concrete case has also been addressed, using machine learning (Higgins and Sadock, 2003; Andrew and MacCartney, 2004; Srinivasan and Yates, 2009; Manshadi and Allen, 2011). Despite these efforts, existing wide-coverage semantic parsers do not typically resolve scope. Either the semantic representations they output are shallow and do not use logical quantifiers, which is sufficient for many applications but not for model-theoretic reasoning. Or they leave the output underspecified or always deterministically pick the same scoping behavior for a given quantifier. How to make a semantic parser aware of scope alternation? We argue that an additional layer of tags is a good way. A semantic parser can use these tags to guide its decisions—just like it might use a lay"
W13-0203,D09-1152,0,0.202369,"on and intensional verbs is crucial for constructing logical form from text that capture the intended meaning. A lot of work has been done to describe the scope alternation behavior especially of quantifiers and to construct underspecified meaning representations from which all (theoretically) possible readings of a sentence containing them can be enumerated (Reyle, 1993; Bos, 1996; Copestake et al., 2005; Egg et al., 2001). The problem of finding the preferred reading in each concrete case has also been addressed, using machine learning (Higgins and Sadock, 2003; Andrew and MacCartney, 2004; Srinivasan and Yates, 2009; Manshadi and Allen, 2011). Despite these efforts, existing wide-coverage semantic parsers do not typically resolve scope. Either the semantic representations they output are shallow and do not use logical quantifiers, which is sufficient for many applications but not for model-theoretic reasoning. Or they leave the output underspecified or always deterministically pick the same scoping behavior for a given quantifier. How to make a semantic parser aware of scope alternation? We argue that an additional layer of tags is a good way. A semantic parser can use these tags to guide its decisions—j"
W13-0215,W10-0731,0,0.00705217,"he context of constructing the Groningen Meaning Bank (GMB, Basile et al., 2012), a large semantically annotated corpus, we address this problem by making use of crowdsourcing. The idea of crowdsourcing is that some tasks that are difficult to solve for computers but easy for humans may be outsourced to a number of people across the globe. One of the prime crowdsourcing platforms is Amazon’s Mechanical Turk, where workers get paid small amounts to complete small tasks. Mechanical Turk has already been successfully applied for the purpose of word sense disambiguation and clustering (see, e.g., Akkaya et al., 2010; Rumshisky et al., 2012). Another crowdsourcing technique, “Game with a Purpose” (GWAP), rewards contributors with entertainment rather than money. GWAPs challenge players to score high on specifically designed tasks, thereby contributing their knowledge. GWAPs were successfully pioneered in NLP by initiatives such as ‘Phrase Detectives’ for anaphora resolution (Chamberlain et al., 2008) and ‘JeuxDeMots’ for term relations (Artignan et al., 2009). We have developed an online GWAP platform for semantic annotation, called Wordrobe. In this paper we present the design and the first results of us"
W13-0215,basile-etal-2012-developing,1,0.688884,"ense of each content word from a set of possible senses. In NLP-related research, many models for disambiguating word senses have been proposed. Such models have been evaluated through various public evaluation campaigns, most notably SenseEval (now called SemEval), an international word sense disambiguation competition held already six times since its start in 1998 (Kilgarriff and Rosenzweig, 2000). All disambiguation models rely on gold standard data from human annotators, but this data is timeconsuming and expensive to obtain. In the context of constructing the Groningen Meaning Bank (GMB, Basile et al., 2012), a large semantically annotated corpus, we address this problem by making use of crowdsourcing. The idea of crowdsourcing is that some tasks that are difficult to solve for computers but easy for humans may be outsourced to a number of people across the globe. One of the prime crowdsourcing platforms is Amazon’s Mechanical Turk, where workers get paid small amounts to complete small tasks. Mechanical Turk has already been successfully applied for the purpose of word sense disambiguation and clustering (see, e.g., Akkaya et al., 2010; Rumshisky et al., 2012). Another crowdsourcing technique, “"
W13-0215,J09-4005,0,0.0475904,"Missing"
W13-0215,W08-2230,0,0.091336,"Amazon’s Mechanical Turk, where workers get paid small amounts to complete small tasks. Mechanical Turk has already been successfully applied for the purpose of word sense disambiguation and clustering (see, e.g., Akkaya et al., 2010; Rumshisky et al., 2012). Another crowdsourcing technique, “Game with a Purpose” (GWAP), rewards contributors with entertainment rather than money. GWAPs challenge players to score high on specifically designed tasks, thereby contributing their knowledge. GWAPs were successfully pioneered in NLP by initiatives such as ‘Phrase Detectives’ for anaphora resolution (Chamberlain et al., 2008) and ‘JeuxDeMots’ for term relations (Artignan et al., 2009). We have developed an online GWAP platform for semantic annotation, called Wordrobe. In this paper we present the design and the first results of using Wordrobe for the task of word sense disambiguation. 2 Method Wordrobe1 is a collection of games with a purpose, each targeting a specific level of linguistic annotation. Current games include part-of-speech tagging, named entity tagging, co-reference resolution and 1 http://www.wordrobe.org/ word sense disambiguation. The game used for word sense disambiguation is called Senses. Below"
W13-0215,rumshisky-etal-2012-word,0,0.0292451,"cting the Groningen Meaning Bank (GMB, Basile et al., 2012), a large semantically annotated corpus, we address this problem by making use of crowdsourcing. The idea of crowdsourcing is that some tasks that are difficult to solve for computers but easy for humans may be outsourced to a number of people across the globe. One of the prime crowdsourcing platforms is Amazon’s Mechanical Turk, where workers get paid small amounts to complete small tasks. Mechanical Turk has already been successfully applied for the purpose of word sense disambiguation and clustering (see, e.g., Akkaya et al., 2010; Rumshisky et al., 2012). Another crowdsourcing technique, “Game with a Purpose” (GWAP), rewards contributors with entertainment rather than money. GWAPs challenge players to score high on specifically designed tasks, thereby contributing their knowledge. GWAPs were successfully pioneered in NLP by initiatives such as ‘Phrase Detectives’ for anaphora resolution (Chamberlain et al., 2008) and ‘JeuxDeMots’ for term relations (Artignan et al., 2009). We have developed an online GWAP platform for semantic annotation, called Wordrobe. In this paper we present the design and the first results of using Wordrobe for the task"
W13-0215,D08-1027,0,0.0592114,"Missing"
W13-2101,W08-2227,0,0.181491,"Missing"
W13-2101,W11-2819,1,0.854027,"ous what the output of a natural language generation component should be, namely text, there is little to no agreement on what its input formalism should be (Evans et al., 2002). Since open-domain semantic parsers are able to produce formal semantic representations nowadays (Bos, 2008; Butler and Yoshimoto, 2012), it would be natural to see generation as a reversed process, and consider such semantic representations as input of a surface realization component. The idea of using large text corpora annotated with formal semantic representations for robust generation has been presented recently (Basile and Bos, 2011; Wanner et al., 2012). The need for formal semantic representations as a basis for NLG was expressed already much earlier by Power (1999), who derives semantic networks enriched with scope information from knowledge representations for content planning. In this paper we take 2 Aligning Logic with Text Several different formal semantic representations have been proposed in the literature, and although they might differ in various aspects, they also have a lot in common. Many semantic representations (or logical forms as they are sometimes referred to) are variants of first-order logic and shar"
W13-2101,C12-1094,0,0.0583118,"rm for each of them, and finally composing all of the surface form together into a complete text. We are currently building a large corpus of word-aligned DRSs, and are investigating machine learning methods that could automatically learn the alignments. Surprisingly, given that DRT is one of the best studied formalisms in formal semantics, there isn’t much work on generation from DRSs so far. The contribution of this paper presents a method to align DRSs with surface strings, that paves the way for robust, statistical methods for surface generation from deep semantic representations. (Le and Zuidema, 2012) just because they’re easier and more efficient to process. Packed semantic representations (leaving scope underspecified) also resemble flat representations (Copestake et al., 1995; Reyle, 1993) and can be viewed as graphs, however they show less elaborated reification than the DRGs presented in this paper, and are therefore less suitable for precise alignment with surface strings. 8 Conclusion We presented a formalism to align logical forms, in particular Discourse Representation Structures, with surface text strings. The resulting graph representations (DRGs), make recursion implicit by rei"
W13-2101,basile-etal-2012-developing,1,0.929449,"meaning. This “trick” ensures that we can describe the local order of noun phrases with relative clauses and alike. To wrap things up, the composition operation is used to derive complete surface forms for DRGs. Composition puts together two surface forms, where one of them is complete, and one of them is incomplete. It is formally defined as follows: ρ1 : τ ρ2 : Λ1 ρ1 Λ2 ρ2 : Λ1 τ Λ2 Selected Phenomena We implemented a first prototype using our alignment and realization method and tested it on examples taken from the Groningen Meaning Bank, a large annotated corpus of texts paired with DRSs (Basile et al., 2012). Naturally, we came across phenomena that are notoriously hard to analyze. Most of these we can handle adequately, but some we can’t currently account for and require further work. 6.1 Embedded Clauses In the variant of DRT that we are using, propositional arguments of verbs introduce embedded DRSs associated with a discourse referent. This is a good test for our surface realization formalism, because it would show that it is capable of recursively generating embedded clauses. Figure 9 shows the DRG for the sentence “Michelle thinks that Obama smokes.” (1) referent michelle where ρ1 and ρ2 ar"
W13-2101,W12-1525,0,0.0225199,"s in the DRS). We can account for both interpretations (Figure 10). Note that, interestingly, using the distributive interpretation DRG as input to the surface realization component could result, depending on how words are aligned, in a surface form “fishing occurs and trawling occurs”, or as “fishing and trawling occur”. 6.3 Control Verbs 7 Related work Over the years, several systems have emerged that aim at generate surface forms from different kind of abstract input representations. An overview of the state-of-the-art is showcased by the submissions to the Surface Realization Shared Task (Belz et al., 2012). Bohnet et al. (2010), for instance, employ deep structures derived from the CoNLL 2009 shared task, essentially sentences annotated with shallow semantics, lemmata and dependency trees; as the authors state, these annotations are not made with generation in mind, and they necessitate complex preprocessing steps in order to derive syntactic trees, and ultimately surface forms. The format presented in this work has been especially developed with statistical approaches in mind. Nonetheless, there is very little work on robust, wide-scale generation from DRSs, surprisingly perhaps given the larg"
W13-2101,C10-1012,0,0.0316109,"n account for both interpretations (Figure 10). Note that, interestingly, using the distributive interpretation DRG as input to the surface realization component could result, depending on how words are aligned, in a surface form “fishing occurs and trawling occurs”, or as “fishing and trawling occur”. 6.3 Control Verbs 7 Related work Over the years, several systems have emerged that aim at generate surface forms from different kind of abstract input representations. An overview of the state-of-the-art is showcased by the submissions to the Surface Realization Shared Task (Belz et al., 2012). Bohnet et al. (2010), for instance, employ deep structures derived from the CoNLL 2009 shared task, essentially sentences annotated with shallow semantics, lemmata and dependency trees; as the authors state, these annotations are not made with generation in mind, and they necessitate complex preprocessing steps in order to derive syntactic trees, and ultimately surface forms. The format presented in this work has been especially developed with statistical approaches in mind. Nonetheless, there is very little work on robust, wide-scale generation from DRSs, surprisingly perhaps given the large body of theoretical"
W13-2101,W08-2222,1,0.623053,"precise alignment at the word level, the complete surface form of a meaning representations can be deduced using a simple declarative rule. 1 Introduction Surface Realization is the task of producing fluent text from some kind of formal, abstract representation of meaning (Reiter and Dale, 2000). However, while it is obvious what the output of a natural language generation component should be, namely text, there is little to no agreement on what its input formalism should be (Evans et al., 2002). Since open-domain semantic parsers are able to produce formal semantic representations nowadays (Bos, 2008; Butler and Yoshimoto, 2012), it would be natural to see generation as a reversed process, and consider such semantic representations as input of a surface realization component. The idea of using large text corpora annotated with formal semantic representations for robust generation has been presented recently (Basile and Bos, 2011; Wanner et al., 2012). The need for formal semantic representations as a basis for NLG was expressed already much earlier by Power (1999), who derives semantic networks enriched with scope information from knowledge representations for content planning. In this pa"
W13-2101,C88-2128,0,0.557205,"mple from Figure 7, the complete surface form for the discourse unit k1 is derived by means of composition as formulated in (1) as follows: instance ""Michelle"" 1 ext x1 named Agent int role 1 referent k1 instance ""thinks"" 2 event role think e1 referent 3 int Patient Theme k2 : e1 x2 int 1 role ext referent ""that"" named 1 x1 : A customer e1 : x1 did not pay e1 : A customer did not pay . k2 : A customer did not pay . ext p1 obama instance ""Obama"" referent 1 subordinates:prop punctuation ""."" e2 3 2 instance ""smokes"" event The procedure for generation described here is reminiscent of the work of (Shieber, 1988) who also employs a deductive approach. In particular our composition operation can be seen as a simplified completion. Going back to the example in Section 4, substituting the value of x1 in the incomplete surface form of e1 produces the surface string [A,customer,did,not,pay,.] for e1 . smoke Figure 9: Word-aligned DRG for the sentence “Michelle thinks that Obama smokes.” Here the surface forms of two discourse units (main and embedded) are generated. In order to generate the complete surface form, first the embedded clause is generated, and then composed 6 more than one gold standard surfac"
W13-2101,W12-1506,0,0.0221861,"a natural language generation component should be, namely text, there is little to no agreement on what its input formalism should be (Evans et al., 2002). Since open-domain semantic parsers are able to produce formal semantic representations nowadays (Bos, 2008; Butler and Yoshimoto, 2012), it would be natural to see generation as a reversed process, and consider such semantic representations as input of a surface realization component. The idea of using large text corpora annotated with formal semantic representations for robust generation has been presented recently (Basile and Bos, 2011; Wanner et al., 2012). The need for formal semantic representations as a basis for NLG was expressed already much earlier by Power (1999), who derives semantic networks enriched with scope information from knowledge representations for content planning. In this paper we take 2 Aligning Logic with Text Several different formal semantic representations have been proposed in the literature, and although they might differ in various aspects, they also have a lot in common. Many semantic representations (or logical forms as they are sometimes referred to) are variants of first-order logic and share basic building block"
W13-2101,1995.tmi-1.2,0,0.664519,"ary, and binary; Note that labelling conditions is crucial to distinguish between syntactically equivalent conditions occurring in different (embedded) DRSs. Unlike Power’s scoped semantic network for DRSs, we don’t make the assumption that conditions appear in the DRS in which their discourse referents are introduced (Power, 1999). The example in Figure 6 illustrates that this assumption is not sound: the condition p(x) is in a different DRS than where its discourse referent x is introduced. Further note that our reification procedure yields “flatter” representations than similar formalisms (Copestake et al., 1995; Reyle, 1993), and this makes it more convenient to align surface strings with DRSs with a high granularity, as we will show below. • hC,argument,Ai means that C is a condition with argument A (argument tuples), with the sub-types internal, external, instance, scope, antecedent, and consequence. With the help of a concrete example, it is easy to see that DRGs have the same expressive power as DRSs. Consider for instance a DRS with negation, before and after labelling it (Figure 6): xy xy r(x,y) z ¬ p(x) s(z,y) c1 :r(x,y) z K1 : c2 :¬K2 : c3 :p(x) c4 :s(z,y) 4 Word-Aligned DRGs Figure 6: From"
W13-2101,2007.mtsummit-ucnlg.4,0,0.188522,"machine translation (Schiehlen et al., 2000) and for evaluation purposes (Allen et al., 2008), and semantic parsing Long-Distance Dependencies Cases of extraction, for instance with WHmovement, could be problematic to capture with our formalism. This is in particular an issue when extraction crosses more than one clause boundary, as in “Which car does Bill believe John bought”. Even though these cases are rare in the real world, a complete formalism for surface realization must be able to deal with such cases. The question is whether this is a separate generation task in the domain of syntax (White et al., 2007), or whether the current formalism needs to be adapted to cover such long-distance dependencies. Another range of complications are caused by discontinuous constituents, as in the Dutch sentence “Ik heb kaartjes gekocht voor Berlijn” (literally: “I have tickets bought for Berlin”), where the prepositional phrase “voor Berlijn” is an argument of the noun phrase “kaartjes”. In our formalism the only alignment possible would result in the sentence “Ik heb kaartjes voor Berlijn gekocht”, which is arguably a more fluent realization of the sentence, but doesn’t correspond to the original text. If on"
W13-2101,W02-2119,0,0.0296501,"ical) NLG systems. By reformatting semantic representations as graphs, fine-grained alignment can be obtained. Given a precise alignment at the word level, the complete surface form of a meaning representations can be deduced using a simple declarative rule. 1 Introduction Surface Realization is the task of producing fluent text from some kind of formal, abstract representation of meaning (Reiter and Dale, 2000). However, while it is obvious what the output of a natural language generation component should be, namely text, there is little to no agreement on what its input formalism should be (Evans et al., 2002). Since open-domain semantic parsers are able to produce formal semantic representations nowadays (Bos, 2008; Butler and Yoshimoto, 2012), it would be natural to see generation as a reversed process, and consider such semantic representations as input of a surface realization component. The idea of using large text corpora annotated with formal semantic representations for robust generation has been presented recently (Basile and Bos, 2011; Wanner et al., 2012). The need for formal semantic representations as a basis for NLG was expressed already much earlier by Power (1999), who derives seman"
W13-2101,C08-1038,0,\N,Missing
W15-1832,E12-2019,1,0.932691,"t it can be carried out in (i) a completely data-driven fashion and (ii) using judgments by multiple speakers without linguistic training, thus making it extremely inexpensive and light, yet useful. To comply with (i), we make sure that prepositions are not derived from a fixed precompiled list, but rather acquired automatically, case by case, exploiting Google’s n-grams to generate candidates. The compounds themselves are Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 251 taken from an existing semantically annotated corpus, the Groningen Meaning Bank (Basile et al., 2012). Regarding (ii), we exploit crowd-sourcing and develop a game-with-a-purpose setting to collect data. The acquired data can then be analysed to investigate more closely the use of prepositions for interpreting noun noun compounds and the extent to which different people agree. Moreover, the data can be used to collect descriptive statistics on preposition use in this context that might give new insight into this approach. 2 Method In this section we describe how we selected nounnoun compounds from a corpus (Step 1), generated potential prepositional relations for each compound (Step 2), and t"
W15-1832,dima-etal-2014-tell,0,0.0275558,"eloped an inventory comprising eight different prepositions: of, for, with, in, on, at, about, and from. The third set of attempts views compound interpretation as a paraphrasing task (Nakov, 2007). This would yield interpretations such as “a crime committed during a war” for our earlier example war crime. None of the three approaches show clear advantages. On the one side of the spectrum, the fixedvocabulary-approach faces the problem of being too strict. On the other end of it, paraphrasing is hard to control. Attempts at combining more than one approach for English (Girju, 2009) or German (Dima et al., 2014) still rely heavily on pre-constructed sets of relations/prepositions, the latter advocating a hybrid approach combining a semantic-relation and preposition-based method. Given that the preposition-approach lies somewhere between these other two approaches, and can be taken in such a way that is entirely data driven, this is the approach that we will consider and use in this paper. While we are aware of its expressive limitations (prepositions might not be sufficient, and they might preserve some ambiguity of the compound), we still think it is interesting to test to what extent it can be carr"
W15-1832,J09-2003,0,0.0245038,"pounds (Levi, 1978), developed an inventory comprising eight different prepositions: of, for, with, in, on, at, about, and from. The third set of attempts views compound interpretation as a paraphrasing task (Nakov, 2007). This would yield interpretations such as “a crime committed during a war” for our earlier example war crime. None of the three approaches show clear advantages. On the one side of the spectrum, the fixedvocabulary-approach faces the problem of being too strict. On the other end of it, paraphrasing is hard to control. Attempts at combining more than one approach for English (Girju, 2009) or German (Dima et al., 2014) still rely heavily on pre-constructed sets of relations/prepositions, the latter advocating a hybrid approach combining a semantic-relation and preposition-based method. Given that the preposition-approach lies somewhere between these other two approaches, and can be taken in such a way that is entirely data driven, this is the approach that we will consider and use in this paper. While we are aware of its expressive limitations (prepositions might not be sufficient, and they might preserve some ambiguity of the compound), we still think it is interesting to test"
W15-1832,P95-1007,0,0.0426598,"are mainly three different approaches that deal with this problem. The first family of approaches take a (usually small) fixed inventory of relations and use it to describe compounds based on well-established ontologies. The second line Malvina Nissim Center for Language and Cognition Oude Kijk in ’t Jatstraat 26 University of Groningen m.nissim@rug.nl of research takes a set of English prepositions to describe compounds (in a way similar as we did above). This makes sense, as prepositions naturally describe a relation between two entities. The seminal work following this tradition is Lauer (Lauer, 1995), who, inspired by Levi’s work on fixing a set of possible predicates for interpreting noun-noun compounds (Levi, 1978), developed an inventory comprising eight different prepositions: of, for, with, in, on, at, about, and from. The third set of attempts views compound interpretation as a paraphrasing task (Nakov, 2007). This would yield interpretations such as “a crime committed during a war” for our earlier example war crime. None of the three approaches show clear advantages. On the one side of the spectrum, the fixedvocabulary-approach faces the problem of being too strict. On the other en"
W15-1832,W13-0215,1,0.852943,"expansion plan of the expansion plans of a expansion ... plans for an expansions plans for the expansions In case the number of resulting instances was lower than five, the empty places were filled up with the most frequently used prepositions overall computed for all compounds extracted from the Google n-gram corpus. These were: of, from, on, for and by. The total for a preposition given a compound is the sum of all frequencies obtained for each single query. The third step is using the data generated in Step 2 in a GWAP, a game with a purpose, in order to collect human judgements. Wordrobe (Venhuizen et al., 2013), an existing internet-based GWAP architecture was used to launch a nounnoun compound annotation task in the shape of a game named burgers at www.wordrobe.org. Players of this game, not necessarily knowing anything about linguistic annotation, received a snippet of a text with the relevant noun-noun compound marked up in bold face, and were asked to select the most appropriate prepositions of the five candidates generated in Step 2. They were awarded points relative to the agreement of other players’ choices for the same question (using add1 smoothing initially). Players were instructed to hit"
W15-1841,W13-2101,1,0.822992,"n Structures, the boxes, selected with --semantics drs). Alternatively DRSs can be shown as Projective DRSs (Venhuizen et al., 2013) using --semantics pdrs, where each DRS is labelled with a pointer, and each DRS-condition receives a pointer to the DRS in which it appears. For some applications and users with different mind-sets, Boxer comes with an option to translate DRSs into other types of meaning representation. First of all, with --semantics fol, Boxer supports the well-known translation from boxes to first-order logic (Kamp and Reyle, 1993; Bos, 2004), or to DRSs in the form of graphs (Basile and Bos, 2013), when invoked with --semantics drg. Secondly, the meaning representations can be translated into flat logical forms, as proposed in Jerry Hobbs’s framework (Hobbs, 1991), with --semantics tacitus. Note that not all of these translations are necessary meaningpreserving, becauce of the differences in expressive power between the formalisms. Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 302 4 Meaning Details The devil is in the detail. Indeed, to get the most out of Boxer, it is important to know what features it offers to compute meaning representations."
W15-1841,S12-1040,1,0.850269,"sent the capabilities of the opendomain semantic parser Boxer. Boxer is distributed with the C&C text processing tools (Curran et al., 2007), and its main characteristics were first described in my earlier work (Bos, 2008). The roots of the current version of Boxer go back even further, long before Boxer was officially released to the community (Bos et al., 2004; Bos, 2001). Boxer distinguishes itself from other semantic parsers in that it produces formal meaning representations (compatible with first-order logic) while reaching wide coverage, and is therefore used in a range of applications (Basile et al., 2012; Bjerva et al., 2014). To get an idea of what Boxer does, consider the input and output in Figure 1. John did not go to school . __________________ |x1 | |..................| |named(x1,john,per)| | ____________ | | |e2 x3 || |¬ |............|| | |go(e2) || | |agent(e2,x1)|| | |school(x3) || | |to(e2,x3) || | |____________|| |__________________| Figure 1: Example of Boxer’s input and output. 2 Interface Formats The input of the Boxer system is a syntactic analysis in the form of a derivation of combinatorial categorial grammar, CCG (Steedman, 2001). This input can be augmented in order to inco"
W15-1841,S14-2114,1,0.862023,"Missing"
W15-1841,W15-1832,1,0.818746,"acher). Multiword Expressions Boxer provides two ways to represent compound proper names. With --mwe no a compound name such as Barack Obama is represented by two naming conditions (with the non-logical symbols barack and obama), and with --mwe yes as a single naming condition (with symbol barack~obama). Noun–Noun Compounds Noun–noun compounds are interpreted as two entities that form a certain relation. By default, Boxer picks the generic prepositional of –relation. With --nn true, Boxer attempts to disambiguate noun-noun compound relations by selecting from a set of prepositional relations (Bos and Nissim, 2015). For instance, beach house would be interpreted as: house(x) ∧ beach(y) ∧ at(x,y). Reference Resolution By default Boxer doesn’t resolve pronouns or other referential expressions, but with --resolve true, Boxer attempts to resolve pronouns, proper names and definite descriptions (currently using a rule-based approach). The foundational algorithm to accomplish this is based on Van der Sandt’s theory of presupposition projection (Van der Sandt, 1992). The discourse referents of the selected antecedents are unified with those of the referential expression. Thematic Role Labelling As mentioned ab"
W15-1841,C04-1180,1,0.52755,"to offer, and what this paper contributes (and adds with respect to previous publications) is a finegrained description of the many possibilities that Boxer provides for the formal semantic analysis of text processing. Introduction In this paper I present the capabilities of the opendomain semantic parser Boxer. Boxer is distributed with the C&C text processing tools (Curran et al., 2007), and its main characteristics were first described in my earlier work (Bos, 2008). The roots of the current version of Boxer go back even further, long before Boxer was officially released to the community (Bos et al., 2004; Bos, 2001). Boxer distinguishes itself from other semantic parsers in that it produces formal meaning representations (compatible with first-order logic) while reaching wide coverage, and is therefore used in a range of applications (Basile et al., 2012; Bjerva et al., 2014). To get an idea of what Boxer does, consider the input and output in Figure 1. John did not go to school . __________________ |x1 | |..................| |named(x1,john,per)| | ____________ | | |e2 x3 || |¬ |............|| | |go(e2) || | |agent(e2,x1)|| | |school(x3) || | |to(e2,x3) || | |____________|| |_________________"
W15-1841,W08-2222,1,0.925605,"hat there is a school-going-event e2 that involves the entities x1 (John) and a school (denoted by entity x3 ). But Boxer has a lot more to offer, and what this paper contributes (and adds with respect to previous publications) is a finegrained description of the many possibilities that Boxer provides for the formal semantic analysis of text processing. Introduction In this paper I present the capabilities of the opendomain semantic parser Boxer. Boxer is distributed with the C&C text processing tools (Curran et al., 2007), and its main characteristics were first described in my earlier work (Bos, 2008). The roots of the current version of Boxer go back even further, long before Boxer was officially released to the community (Bos et al., 2004; Bos, 2001). Boxer distinguishes itself from other semantic parsers in that it produces formal meaning representations (compatible with first-order logic) while reaching wide coverage, and is therefore used in a range of applications (Basile et al., 2012; Bjerva et al., 2014). To get an idea of what Boxer does, consider the input and output in Figure 1. John did not go to school . __________________ |x1 | |..................| |named(x1,john,per)| | ____"
W15-1841,M91-1015,0,0.530745,"lled with a pointer, and each DRS-condition receives a pointer to the DRS in which it appears. For some applications and users with different mind-sets, Boxer comes with an option to translate DRSs into other types of meaning representation. First of all, with --semantics fol, Boxer supports the well-known translation from boxes to first-order logic (Kamp and Reyle, 1993; Bos, 2004), or to DRSs in the form of graphs (Basile and Bos, 2013), when invoked with --semantics drg. Secondly, the meaning representations can be translated into flat logical forms, as proposed in Jerry Hobbs’s framework (Hobbs, 1991), with --semantics tacitus. Note that not all of these translations are necessary meaningpreserving, becauce of the differences in expressive power between the formalisms. Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 302 4 Meaning Details The devil is in the detail. Indeed, to get the most out of Boxer, it is important to know what features it offers to compute meaning representations. 4.1 Linguistic Features Copula Notorious among computational semanticists is the analysis of the copula. Boxer gives two options: to interpret the copula as were it an o"
W15-1841,J07-3004,0,0.0217761,"gy components. The output is a meaning representation, produced in a variety of standard formats. 2.1 Input Boxer requires a syntactic analysis of the text in the form of CCG-derivations, every sentence corresponding to one CCG derivation. The derivation itself is represented as a ccg/2 Prolog term, comprising a sentence identifier and a recursively built structure of combinatorial rules (such as fa/3, ba/3, and so on), and terminals (the lexical items). All combinatorial rules of CCG are supported, including the generalized composition rules and the type-changing rules introduced in CCGbank (Hockenmaier and Steedman, 2007). The terminals are captured by a Prolog term consisting of the CCG category (Boxer implements about 600 different lexical category types), the token, its lemma, and part-of-speech. Information of external tools can also be included here, such as word sense disambiguation, thematic role labelling, noun-noun compound interpretation, or reference resolution. Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 301 sem(1,[1001:[tok:’John’,pos:’NNP’,lemma:’John’,namex:’I-PER’], 1002:[tok:did,pos:’VBD’,lemma:do,namex:’O’], 1003:[tok:not,pos:’RB’,lemma:not,namex:’O’"
W15-1841,D14-1107,0,0.0935427,"7:[tok:’.’,pos:’.’,lemma:’.’,namex:’O’]], b2:drs([b1:[]:x1], [b1:[1001]:named(x1,john,per,nam), b2:[1003]:not(b3:drs([b3:[]:e1,b3:[]:x2], [b3:[1004]:pred(e1,go,v,0), b3:[]:role(e1,x1,agent,1), b3:[1006]:pred(x2,school,n,0), b3:[1005]:rel(e1,x2,to,0)]))])). Figure 2: Boxer’s output in Prolog format, for “John does not go to school.” Any parser can be used to support Boxer, as long as it produces CCG derivation in the required Prolog format. The standard parser used in tandem with Boxer is that of the C&C tools (Clark and Curran, 2004). Alternatively, other parsers can be used, such as EasyCCG (Lewis and Steedman, 2014). The lemmas can be provided by offthe-shells tools like morpha (Minnen et al., 2001). 2.2 Output 3.2 The standard output is a meaning representation in the form of a Discourse Representation Structure (Kamp and Reyle, 1993). This output is standard shown in Prolog format, but can also be produced in XML (with the --format xml option). Output can also be suppressed, with --format no, in case only human-readable output is wanted. For the user’s convenience, the meaning can also be displayed in boxed format (with the --box true option), as shown above. In combination with --instantiate true, thi"
W15-1841,W13-0122,1,0.845859,", 1993). Boxer follows the theory closely (--theory drt), except with respect to (i) event semantics, where it adopts a neo-Davidsonian approach, and (ii) the analysis of sentential complements, where Boxer follows an analysis based on modal logic (Bos, 2004). Meaning Translations The meaning representations of Discourse Representation Theory can be shaped in different ways, and Boxer supports several of these possibilities. The standard representations are DRSs (Discourse Representation Structures, the boxes, selected with --semantics drs). Alternatively DRSs can be shown as Projective DRSs (Venhuizen et al., 2013) using --semantics pdrs, where each DRS is labelled with a pointer, and each DRS-condition receives a pointer to the DRS in which it appears. For some applications and users with different mind-sets, Boxer comes with an option to translate DRSs into other types of meaning representation. First of all, with --semantics fol, Boxer supports the well-known translation from boxes to first-order logic (Kamp and Reyle, 1993; Bos, 2004), or to DRSs in the form of graphs (Basile and Bos, 2013), when invoked with --semantics drg. Secondly, the meaning representations can be translated into flat logical"
W15-1841,P04-1014,0,0.0514633,"pos:’TO’,lemma:to,namex:’O’], 1006:[tok:school,pos:’NN’,lemma:school,namex:’O’], 1007:[tok:’.’,pos:’.’,lemma:’.’,namex:’O’]], b2:drs([b1:[]:x1], [b1:[1001]:named(x1,john,per,nam), b2:[1003]:not(b3:drs([b3:[]:e1,b3:[]:x2], [b3:[1004]:pred(e1,go,v,0), b3:[]:role(e1,x1,agent,1), b3:[1006]:pred(x2,school,n,0), b3:[1005]:rel(e1,x2,to,0)]))])). Figure 2: Boxer’s output in Prolog format, for “John does not go to school.” Any parser can be used to support Boxer, as long as it produces CCG derivation in the required Prolog format. The standard parser used in tandem with Boxer is that of the C&C tools (Clark and Curran, 2004). Alternatively, other parsers can be used, such as EasyCCG (Lewis and Steedman, 2014). The lemmas can be provided by offthe-shells tools like morpha (Minnen et al., 2001). 2.2 Output 3.2 The standard output is a meaning representation in the form of a Discourse Representation Structure (Kamp and Reyle, 1993). This output is standard shown in Prolog format, but can also be produced in XML (with the --format xml option). Output can also be suppressed, with --format no, in case only human-readable output is wanted. For the user’s convenience, the meaning can also be displayed in boxed format (wi"
W15-1841,P07-2009,1,\N,Missing
W15-1841,M91-1010,0,\N,Missing
W16-3202,W15-2816,0,0.0166467,"ural Language Processing and Computer Vision has become increasingly popular over the past years. There is an extensive body of work, among others in the following areas: building multimodal models of meaning which take into account both text and image data (Bruni et al., 2012), generating images from textual data (Lazaridou et al., 2015, Coyne et al., 2010), Question Answering on images (Malinowski and Fritz, 2014), and automatic image label generation (Karpathy and Fei-Fei, 2014, Elliott and Keller, 2013, Elliott et al., 2014, Kulkarni et al., 2011, Vinyals et al., 2014, Yang et al., 2011). Belz et al. (2015) present a method for selecting prepositions to describe spatial relationships between objects in images. They use features based on geometrical configurations of bounding boxes as well as prior probabilities of prepositions occurring with objects/class labels. Several approaches have been proposed to reason on spatial information derived from visual input. Neumann and M¨oller (2008) discuss the potential of knowledge representation for high-level scene interpretation. Their focus is on Description Logic (DL), a subset of first-order predicate calculus supporting inferences about various aspec"
W16-3202,D13-1128,0,0.135245,"rediction performance. We find that lexical information is required to accurately predict spatial relations when combined with location information, achieving an F-score of 0.80, compared to a most-frequent-class baseline of 0.62. 1 Introduction In the light of growing amount of digital image data, methods for automatically linking data to language are a great asset. Due to recent advances in the distinct areas of language technology and computer vision, research combining the two fields has become increasingly popular, including automatical generation of captions (Karpathy and Fei-Fei, 2014, Elliott and Keller, 2013, Elliott et al., 2014, Kulkarni et al., 2011, Vinyals et al., 2014, Yang et al., 2011) and translation of text into visual scenes (Coyne et al., 2010). One task which has not yet been extensively researched is the automatic derivation of rich abstract representations from images (Neumann and M¨oller, 2008, Malinowski and Fritz, 2014). A formal representation of an image goes beyond naming the objects that are present; it can also account for some of the structure of the visual scene by including spatial relations between objects. This information could enhance the interface between language a"
W16-3202,C14-1012,0,0.0591268,"find that lexical information is required to accurately predict spatial relations when combined with location information, achieving an F-score of 0.80, compared to a most-frequent-class baseline of 0.62. 1 Introduction In the light of growing amount of digital image data, methods for automatically linking data to language are a great asset. Due to recent advances in the distinct areas of language technology and computer vision, research combining the two fields has become increasingly popular, including automatical generation of captions (Karpathy and Fei-Fei, 2014, Elliott and Keller, 2013, Elliott et al., 2014, Kulkarni et al., 2011, Vinyals et al., 2014, Yang et al., 2011) and translation of text into visual scenes (Coyne et al., 2010). One task which has not yet been extensively researched is the automatic derivation of rich abstract representations from images (Neumann and M¨oller, 2008, Malinowski and Fritz, 2014). A formal representation of an image goes beyond naming the objects that are present; it can also account for some of the structure of the visual scene by including spatial relations between objects. This information could enhance the interface between language and vision. Imagine, fo"
W16-3202,N13-1132,0,0.0284333,"and one blue). They had to choose the statement which they deemed to best describe the relationship between the two objects. To facilitate identification of objects in cluttered pictures, we provided the first WordNet lemma of the synset as a label for each box, prefixing with “A” and “B” for the directed relations part-of and supports. Figure 5 shows an example question as presented in the part-of task. Figure 5: Example question presented to Crowdflower workers on part-of task. Post-processing of the raw annotation results was done using the Multi-Annotator Confidence Estimation tool, MACE (Hovy et al., 2013). MACE is designed to evaluate data from categorical multi-annotator tasks. It provides competence ratings for individual annotators as well as the most probable answer for each item. A subsample of the MACE output was assessed manually and errors found during this inspection were corrected. However, a little bit of noise is likely to remain in the final spatial relation annotations. 4.4 Image Models and Grounding In classical logic, a first-order model M = hD, F i has two components, a non-empty domain D (also called universe) and an interpretation function F (Blackburn and Bos, 2005). The do"
W17-6901,E17-2039,1,0.785737,"Missing"
W17-6901,J16-4007,0,0.0304563,"Missing"
W17-6901,C16-1333,1,0.775246,"med entity classes, fill the gaps in semantic annotation, and represents one unified tagset aiming to facilitate cross-lingual semantic parsing. 3 The Universal Semantic Tagset The universal semantic tagset aims to provide general cross-lingual description for lexical semantics of all sorts of word tokens. It significantly differs from POS tagset, which is not semantically motivated, and generalizes over NE classes as the latter only covers the words of a particular type. The current version of the semantic tagset (v0.7) is given in Table 1, a revised version of the tagset (v0.6) presented in Bjerva et al. (2016).7 The sem-tags are organized into 13 coarse-grained semantic classes each having its own meta-tag. This division is informal as many sem-tags easily qualify for several classes. We designed the tagset in a data-driven fashion while bearing in mind formal semantic properties of tokens. The employed corpus consists of several parallel corpora of various genres spanning over four languages (see Sec. 4). Before we characterize the sem-tags, let us explain how the tags can or cannot be interpreted. A semtag of a token describes a semantic contribution of the token with respect to the meaning of th"
W17-6901,W08-2222,1,0.822992,"ity large-scale semantic lexicon is crucial for the PMB as both projection and derivation of 9 Moreover, some languages like Dutch and German make little grammatical distinction between adverbs and adjectives. It seems unnatural to treat them as predicates and therefore license entailments using the WordNet hypernymy relations: he ran five kilometers ⇒ he ran five metric linear units. 11 http://pmb.let.rug.nl/explorer/ 10 meaning representations starts from lexical items. The semantic tagset plays a crucial role in development and organization of the lexicon. In particular, in the PMB, Boxer (Bos, 2008, 2015) interprets a sem-tag as a mapping from CCG categories (augmented with thematic roles) to a formal semantic schema which is further specified by a token-related predicate/constant symbol and thematic roles (if any). The function behind the EXS tag is partially depicted in (8):          ; r(e) SR1 NP 7→ λP r. P λx. e SYM(e) R1 (e, x) EXS =   e   (SR1 NP )/R2 NP 7→ λQ P r. P λx. Q λy. SYM(e) R (e, x) 1 R2 (e, y)    ; r(e)  (8) In order to collect large semantically annotated data via bootstrapping, we prepared initial silver and gold datasets for system training and te"
W17-6901,W15-1841,1,0.885554,"Missing"
W17-6901,C04-1180,1,0.70448,"to syntactic parsing) make POS tagging a perfect preprocessor for syntactic parsing. But to what extent is POS-tag information useful for semantic parsing—obtaining semantic representations of natural language texts? Trying to answer this question in favor of POS-tags, we take a stand of a semantic parsing approach that heavily relies on them. One of such approaches is the formal compositional semantics driven by syntactic derivations of Combinatory Categorial Grammar (CCG, Steedman 2001), where a meaning representation is derived by composing formal meaning representations of lexical items (Bos et al., 2004; Lewis and Steedman, 2013; Mineshima et al., 2015).1 Since lexical items come with fully fledged semantics, obviously, assigning correct lexical semantics is crucial for this approach. This is the place where POS-tags come into play by providing lexical information helping to determine lexical semantics. For example, given a POS-tag NN (singular or mass noun) or JJ (adjective)2 , it is possible to assign the desired lexical semantics to the modifiers in (1). But there are cases where POS-tags fall short of providing sufficient information for lexical semantics. For instance, regardless of the"
W17-6901,A00-1031,0,0.641606,"Missing"
W17-6901,M98-1028,0,0.54071,"proper names (e.g., Alfred Nobel and European Union), numerical expressions (e.g., ten thousand and 10 000), and function phrases like as well as, each other, and so that. 6 Moreover, there are at least two semantic usages of and one might want to distinguish: distributive and collective readings. For wide-coverage semantic analysis one needs to identify NEs, detect their type, and model their semantics appropriately. The information extraction community has been actively working on the problem of NE classification and designed annotation schemas. For example, the named entity task at MUC-7 (Chinchor and Robinson, 1998) distinguished three general classes of NEs, where each of them contain several types: entity names (person, organization, location), temporal expressions (date and time) and number expressions (money and percentage). These types of NEs are motivated by downstream applications of information extraction. For a fine-grained semantic analyses, one might go beyond this standards. For example, one of such moves, following to Doddington et al. (2004), is to distinguish the locations without political or social groups (e.g., seas, parks and mountains) from those with them, i.e. geo-political entities"
W17-6901,doddington-etal-2004-automatic,0,0.0451897,"Missing"
W17-6901,C16-1056,1,0.824832,"er REL DEF to the PST was EXS earn United States appreciated $ Fenway GPE EXS UOM GEO at QUC 100 AND a BUT but UOM day SUB to PRO we NIL . QUE (4) ? EXS study NOW have CON English NOT n’t EXT got NIL (5) . DIS any NIL . (6) (7) Applications and Results The idea of the universal semantic tagging was originally motivated by the goals of the PMB project (Bos, 2014; Abzianidze et al., 2017): (i) compositionally derive formal meaning representations for widecoverage English text (Bos, 2009), and (ii) project the meaning representations to Dutch, German and Italian translations via word alignments (Evang and Bos, 2016). These requirements challenge semantic competence and cross-lingual scalability of the universal semantic tagging. A high quality large-scale semantic lexicon is crucial for the PMB as both projection and derivation of 9 Moreover, some languages like Dutch and German make little grammatical distinction between adverbs and adjectives. It seems unnatural to treat them as predicates and therefore license entailments using the WordNet hypernymy relations: he ran five kilometers ⇒ he ran five metric linear units. 11 http://pmb.let.rug.nl/explorer/ 10 meaning representations starts from lexical ite"
W17-6901,J93-2004,0,0.0615287,"x p(x) → q(x) (2) Formal semantics of a content word usually involves a symbol corresponding the lemma. This is the case for each lexical item in (1) and (2) except for the quantifiers. But when dependence of lexical semantics on a lemma is beyond a simple substitution, i.e., one needs to verify a lemma to define lexical semantics, then this case fails to generalize across different languages. For example, assigning lexical 1 Similarly, the semantic parsing based on dependency structures (Reddy et al., 2016, 2017) also rely heavily on POS-tags. Throughout the paper the Penn Treebank POS-tags (Marcus et al., 1993), widely accepted in the NLP community, will be assumed unless otherwise stated. 2 semantics to quantifiers based on their lemma does not scale up for multilingual semantics. On the other hand, the treatment of common nouns in (1) and (2) generalizes for a multilingual case by using a simple assignment: [[hw, pos = NN, category = N i]] = λx. SYM(x) (3) where SYM is a lexical predicate, usually a lemma, corresponding to the word w. In order to compensate the shortcomings of POS tagging for semantic parsing, we propose a new NLP task, called Universal Semantic Tagging or Semantic Tagging in shor"
W17-6901,D15-1244,0,0.0421711,"Missing"
W17-6901,L16-1262,0,0.0753915,"Missing"
W17-6901,petrov-etal-2012-universal,0,0.0394473,"on We have proposed a novel NLP task that contributes to wide-coverage cross-lingual semantic parsing. Tagging tokens with universal semantic tags represents an independent task that unifies and generalizes over semantic virtues of POS-tagging and NE recognition. The expressive semantic tagset allows disambiguation of various semantic phenomena. Besides their application in semantic parsing, already demonstrated in the PMB project, sem-tags can contribute to other NLP tasks, e.g. POS tagging, or research lines rooted in compositional semantics. In contrast to POS tagsets (Marcus et al., 1993; Petrov et al., 2012) augmented with morphological/universal features (Sylak-Glassman, 2016; Nivre et al., 2016), the semantic tagset is less expressive from a morphological perspective. On the other hand, the tagset is leaner and models several semantic phenomena, e.g., roles (ROL), subsectives (SST), privatives (PRI), and degrees (DEG), that are beyond morphology. Compared to the standard NE classes (Sang and Meulder, 2003), the named entity class (NAM) of the tagset is broader. The annotations of temporal expressions at TempEval (UzZaman et al., 2013) and MUC-7 (Chinchor and Robinson, 1998) differ from semantic"
W17-6901,Q16-1010,0,0.0392935,"Missing"
W17-6901,D17-1009,0,0.0438156,"Missing"
W17-6901,W03-0419,0,0.209736,"monstrated in the PMB project, sem-tags can contribute to other NLP tasks, e.g. POS tagging, or research lines rooted in compositional semantics. In contrast to POS tagsets (Marcus et al., 1993; Petrov et al., 2012) augmented with morphological/universal features (Sylak-Glassman, 2016; Nivre et al., 2016), the semantic tagset is less expressive from a morphological perspective. On the other hand, the tagset is leaner and models several semantic phenomena, e.g., roles (ROL), subsectives (SST), privatives (PRI), and degrees (DEG), that are beyond morphology. Compared to the standard NE classes (Sang and Meulder, 2003), the named entity class (NAM) of the tagset is broader. The annotations of temporal expressions at TempEval (UzZaman et al., 2013) and MUC-7 (Chinchor and Robinson, 1998) differ from semantic tagging in terms of granularity: they annotate entire time expressions (e.g., August of 2014) while the semantic tagset opts for a more compositional analysis. In future research, we plan to annotate more data with the help of human annotators, automatically tag large monolingual data via bootstrapping, further improve cross-lingual projection of sem-tags, and prepare an annotation guideline for semantic"
W17-6901,S13-2001,0,0.0754993,"Missing"
W17-6905,E17-2039,1,0.79812,"Missing"
W17-6905,W13-0122,1,0.421324,"xicon, by expressions that indicate reported speech. No special, new machinery is required: the λ-calculus can deal with it all. 4 Approach II: Contextualising Indexicals Inside-Out In the second approach, resolution of indexicals is considered to be part and parcel of an existing presupposition projection algorithm. Hence, indexicals are viewed as presupposition triggers and are lexically represented as such. I assume a projection theory similar to that of Van der Sandt (1992) and Hunter (2013). Presuppositional material is separated from assertive information using the ∗ operator, following Venhuizen et al. (2013). Consider now the lexical entries for the presupposition triggers in (12) and (13). (12) [[John]] = λp.( x person(x) named(x, ”John”) ∗ p(x)) isk (13) [[I]]io = λp.( agent(s,i) topic(s,k) ∗ p(i)) The indexical presupposition of the first-person pronoun is here, closely following Hunter (2013), a complex proposition of a speech event, its agent, and its topic. Combining this with the intransitive verb (7) yields a meaning representation (14) with the presupposition that there is some speech act with the speaker co-referring with the agent of the “leaving” event. The question is what role quota"
W17-7306,W13-2322,0,0.296499,"al language sentence into a meaning representation (a logical form). One of the problems a semantic parser has to deal with is co-indexed variables, which arise in antecedent-anaphor relations, proper name co-reference, control constructions and other linguistic phenomena. Examples of such constructions are given in (1)–(4): (1) Bobi likes himselfi . (2) Jacki wants i to buy an ice-cream. (3) Peteri sold hisi car. (4) Suei saw Maryj , but Maryj did not see Suei . In the context of this paper, we represent meanings using the formalism of Abstract Meaning Representation (AMR), as introduced by Banarescu et al. (2013). AMRs can be seen as graphs connecting concepts by relations. Each concept is represented by a named instance. Co-reference is established by re-using these instances. For example, the AMRs corresponding to examples (1) and (2) above are given in Figure 1. Note that, due to the bracketing, the variable b encapsulates the whole entity person :name ""Bob"" and not just person, i.e. b stands for a person with the name Bob. (l / like-01 :ARG0 (b / person :name ""Bob"") :ARG1 b) (w / want-01 :ARG1 (p / person :name ""Jack"") :ARG3 (b / buy-01 :ARG1 p :ARG2 (i / ice-cream))) Figure 1: AMRs representing t"
W17-7306,S16-1176,0,0.0270627,"aining co-indexed var Training Dev Test 36,520 1,368 1,371 17,589 706 749 40,582 1,590 2,033 968,512 46,737 48,252 189,426 8,704 9,686 Silver 100,00 16,235 18,865 3,001,169 109,676 1 Triples are used to evaluate AMR parsing systems, and are explained in Cai and Knight (2013). 3 Method 3.1 Variable-free AMRs The actual values of the variables for AMR instances are unimportant. Hence, one can rename variables in an AMR as long as re-occurrences of variables are preserved. If variables are used only once in an AMR, they can therefore be completely eliminated from an AMR. This insight was used by Barzdins and Gosko (2016) in the first approach to neural semantic parsing of AMRs, because particular names of variables are very hard to learn for a neural model given the limited amount of data available. We present three ways to encode co-reference in variable-free AMRs.2 Method 1A: Baseline Note that if there are co-indexed variables, there is always exactly one instance of the variable that carries semantic information. In our baseline method, similar to Barzdins and Gosko (2016) and Konstas et al. (2017), we simply copy this semantic information, while removing the variables, as is shown below. This method does"
W17-7306,P17-1112,0,0.241601,"can be seen by applying the AMR evaluation suite of Damonte et al. (2017), which calculates nine different metrics to evaluate AMR parsing, reentrancy being one of them. Out of the four systems that made these scores available (all scores reported in van Noord and Bos (2017)), the reentrancy metric obtained the lowest F-score for three of them. Various methods have been proposed to automatically parse AMRs, ranging from syntax-based approaches (e.g. Flanigan et al. (2014); Wang et al. (2015); Pust et al. (2015); Damonte et al. (2017)) to the more recent neural approaches (Peng et al. (2017); Buys and Blunsom (2017); Konstas et al. (2017); Foland and Martin (2017); van Noord and Bos (2017)). Especially the neural approaches are interesting, since they all use some sort of linearization method and therefore need a predefined way to handle reentrancy. Peng et al. (2017) and Buys and Blunsom (2017) use a special character to indicate reentrancy and restore co-referring variables in a post-processing step. Konstas et al. (2017) simply replace reentrancy variables by their co-referring concept in the input and never outputs co-referring nodes. Foland and Martin (2017) and van Noord and Bos (2017) use the same"
W17-7306,P13-2131,0,0.226976,"s explained in van Noord and Bos (2017). The Wikification step simply adds wiki links to :name nodes, since those links were removed in the input. Pruning is used to remove erroneously produced duplicate output. This is a common problem for sequence-to-sequence models, since the model does not keep track of what it has already output. No pre-training or ensemble methods are used for both approaches. 4 Results and Discussion 4.1 Main results The results of applying our three methods on the baseline and best model are shown in Table 3. All reported numbers are F-scores obtained by using SMATCH (Cai and Knight, 2013). All three methods offer an improvement over the baseline, for both the baseline and the best model. Indexing is the highest scoring method, except for the test set of the best model, since Reentrancy Restoring obtains the same F-score there. Explicitly encoding the absolute paths resulted in an increase over the baseline, but did not outperform both Reentrancy Restoring and Indexing, although only by a small margin. Table 3: Results of the different methods in comparison to the char-only and best model. Baseline Reentrancy Restoring Indexing Absolute Paths Char-only Best model Dev Test Dev T"
W17-7306,E17-1051,0,0.398892,"stablished by re-using these instances. For example, the AMRs corresponding to examples (1) and (2) above are given in Figure 1. Note that, due to the bracketing, the variable b encapsulates the whole entity person :name ""Bob"" and not just person, i.e. b stands for a person with the name Bob. (l / like-01 :ARG0 (b / person :name ""Bob"") :ARG1 b) (w / want-01 :ARG1 (p / person :name ""Jack"") :ARG3 (b / buy-01 :ARG1 p :ARG2 (i / ice-cream))) Figure 1: AMRs representing the meaning of examples (1) and (2). That there is a lot to gain in this area can be seen by applying the AMR evaluation suite of Damonte et al. (2017), which calculates nine different metrics to evaluate AMR parsing, reentrancy being one of them. Out of the four systems that made these scores available (all scores reported in van Noord and Bos (2017)), the reentrancy metric obtained the lowest F-score for three of them. Various methods have been proposed to automatically parse AMRs, ranging from syntax-based approaches (e.g. Flanigan et al. (2014); Wang et al. (2015); Pust et al. (2015); Damonte et al. (2017)) to the more recent neural approaches (Peng et al. (2017); Buys and Blunsom (2017); Konstas et al. (2017); Foland and Martin (2017);"
W17-7306,P14-1134,0,0.307844,"buy-01 :ARG1 p :ARG2 (i / ice-cream))) Figure 1: AMRs representing the meaning of examples (1) and (2). That there is a lot to gain in this area can be seen by applying the AMR evaluation suite of Damonte et al. (2017), which calculates nine different metrics to evaluate AMR parsing, reentrancy being one of them. Out of the four systems that made these scores available (all scores reported in van Noord and Bos (2017)), the reentrancy metric obtained the lowest F-score for three of them. Various methods have been proposed to automatically parse AMRs, ranging from syntax-based approaches (e.g. Flanigan et al. (2014); Wang et al. (2015); Pust et al. (2015); Damonte et al. (2017)) to the more recent neural approaches (Peng et al. (2017); Buys and Blunsom (2017); Konstas et al. (2017); Foland and Martin (2017); van Noord and Bos (2017)). Especially the neural approaches are interesting, since they all use some sort of linearization method and therefore need a predefined way to handle reentrancy. Peng et al. (2017) and Buys and Blunsom (2017) use a special character to indicate reentrancy and restore co-referring variables in a post-processing step. Konstas et al. (2017) simply replace reentrancy variables b"
W17-7306,P17-1043,0,0.292471,"e of Damonte et al. (2017), which calculates nine different metrics to evaluate AMR parsing, reentrancy being one of them. Out of the four systems that made these scores available (all scores reported in van Noord and Bos (2017)), the reentrancy metric obtained the lowest F-score for three of them. Various methods have been proposed to automatically parse AMRs, ranging from syntax-based approaches (e.g. Flanigan et al. (2014); Wang et al. (2015); Pust et al. (2015); Damonte et al. (2017)) to the more recent neural approaches (Peng et al. (2017); Buys and Blunsom (2017); Konstas et al. (2017); Foland and Martin (2017); van Noord and Bos (2017)). Especially the neural approaches are interesting, since they all use some sort of linearization method and therefore need a predefined way to handle reentrancy. Peng et al. (2017) and Buys and Blunsom (2017) use a special character to indicate reentrancy and restore co-referring variables in a post-processing step. Konstas et al. (2017) simply replace reentrancy variables by their co-referring concept in the input and never outputs co-referring nodes. Foland and Martin (2017) and van Noord and Bos (2017) use the same input transformation as Konstas et al. (2017), b"
W17-7306,P17-4012,0,0.0587251,"ly that the model will output impossible paths without destination. For example, the model might output :ARG2 instead of :ARG0 in the left example in Figure 6. These impossible paths still need to be replaced by a variable to get a valid AMR. The strategy we opt for here is similar to one used in the Indexing method, namely replacing the path by the variable of the concept in the AMR that most frequently has a referent in the training set. 3.2 Neural model We implement a bidirectional sequence-to-sequence model with general attention that takes characters as input, using the OpenNMT software (Klein et al., 2017). The parameter settings are the same as in van Noord and Bos (2017) and are shown in Table 2. It is trained for 20 epochs, after which the model that performs best on the development set is used to decode the test set. Table 2: Parameter settings of the seq2seq model, as in van Noord and Bos (2017). 3.3 Parameter Value Parameter Value Layers Nodes Epochs Optimizer Learning rate Decay 2 500 20 sgd 0.1 0.7 RNN type Dropout Vocabulary Max length Beam size Replace unk brnn 0.3 100–200 750 5 true Experiments We test the impact of the different methods on two of our earlier models, described in van"
W17-7306,P17-1014,0,0.325,"the AMR evaluation suite of Damonte et al. (2017), which calculates nine different metrics to evaluate AMR parsing, reentrancy being one of them. Out of the four systems that made these scores available (all scores reported in van Noord and Bos (2017)), the reentrancy metric obtained the lowest F-score for three of them. Various methods have been proposed to automatically parse AMRs, ranging from syntax-based approaches (e.g. Flanigan et al. (2014); Wang et al. (2015); Pust et al. (2015); Damonte et al. (2017)) to the more recent neural approaches (Peng et al. (2017); Buys and Blunsom (2017); Konstas et al. (2017); Foland and Martin (2017); van Noord and Bos (2017)). Especially the neural approaches are interesting, since they all use some sort of linearization method and therefore need a predefined way to handle reentrancy. Peng et al. (2017) and Buys and Blunsom (2017) use a special character to indicate reentrancy and restore co-referring variables in a post-processing step. Konstas et al. (2017) simply replace reentrancy variables by their co-referring concept in the input and never outputs co-referring nodes. Foland and Martin (2017) and van Noord and Bos (2017) use the same input transformation a"
W17-7306,E17-1035,0,0.0827884,"to gain in this area can be seen by applying the AMR evaluation suite of Damonte et al. (2017), which calculates nine different metrics to evaluate AMR parsing, reentrancy being one of them. Out of the four systems that made these scores available (all scores reported in van Noord and Bos (2017)), the reentrancy metric obtained the lowest F-score for three of them. Various methods have been proposed to automatically parse AMRs, ranging from syntax-based approaches (e.g. Flanigan et al. (2014); Wang et al. (2015); Pust et al. (2015); Damonte et al. (2017)) to the more recent neural approaches (Peng et al. (2017); Buys and Blunsom (2017); Konstas et al. (2017); Foland and Martin (2017); van Noord and Bos (2017)). Especially the neural approaches are interesting, since they all use some sort of linearization method and therefore need a predefined way to handle reentrancy. Peng et al. (2017) and Buys and Blunsom (2017) use a special character to indicate reentrancy and restore co-referring variables in a post-processing step. Konstas et al. (2017) simply replace reentrancy variables by their co-referring concept in the input and never outputs co-referring nodes. Foland and Martin (2017) and van Noord an"
W17-7306,D15-1136,0,0.0406446,"re 1: AMRs representing the meaning of examples (1) and (2). That there is a lot to gain in this area can be seen by applying the AMR evaluation suite of Damonte et al. (2017), which calculates nine different metrics to evaluate AMR parsing, reentrancy being one of them. Out of the four systems that made these scores available (all scores reported in van Noord and Bos (2017)), the reentrancy metric obtained the lowest F-score for three of them. Various methods have been proposed to automatically parse AMRs, ranging from syntax-based approaches (e.g. Flanigan et al. (2014); Wang et al. (2015); Pust et al. (2015); Damonte et al. (2017)) to the more recent neural approaches (Peng et al. (2017); Buys and Blunsom (2017); Konstas et al. (2017); Foland and Martin (2017); van Noord and Bos (2017)). Especially the neural approaches are interesting, since they all use some sort of linearization method and therefore need a predefined way to handle reentrancy. Peng et al. (2017) and Buys and Blunsom (2017) use a special character to indicate reentrancy and restore co-referring variables in a post-processing step. Konstas et al. (2017) simply replace reentrancy variables by their co-referring concept in the inpu"
W17-7306,N15-1040,0,0.403978,"/ ice-cream))) Figure 1: AMRs representing the meaning of examples (1) and (2). That there is a lot to gain in this area can be seen by applying the AMR evaluation suite of Damonte et al. (2017), which calculates nine different metrics to evaluate AMR parsing, reentrancy being one of them. Out of the four systems that made these scores available (all scores reported in van Noord and Bos (2017)), the reentrancy metric obtained the lowest F-score for three of them. Various methods have been proposed to automatically parse AMRs, ranging from syntax-based approaches (e.g. Flanigan et al. (2014); Wang et al. (2015); Pust et al. (2015); Damonte et al. (2017)) to the more recent neural approaches (Peng et al. (2017); Buys and Blunsom (2017); Konstas et al. (2017); Foland and Martin (2017); van Noord and Bos (2017)). Especially the neural approaches are interesting, since they all use some sort of linearization method and therefore need a predefined way to handle reentrancy. Peng et al. (2017) and Buys and Blunsom (2017) use a special character to indicate reentrancy and restore co-referring variables in a post-processing step. Konstas et al. (2017) simply replace reentrancy variables by their co-referring"
W18-4919,E17-4011,0,0.0222994,"and for any language for which an idiom dictionary exists. Although we use manually created definitions, these can also be acquired and refined automatically, as done by Liu and Hwa (2016). A side benefit of considering both figurative and literal senses is that separate scores can be assigned for both senses. This could be used for detecting difficult cases like dual meanings or puns, since those cases would get high scores for both senses. In future work, our aim is to further improve these cohesion graph-based classifiers by exploring different similarity measures, such as those tested by Ehren (2017) for German. Another promising avenue is to use more compositional representations of contexts and literalisations (see also Gharbieh et al., 2016). This would also allow us to use the information from verbs and modifiers more effectively, as in its current form our method relies on word-to-word comparisons and only nouns contribute to performance. Finally, we find that, for evaluation, using both micro- and macro-averaged metrics is an important way of ensuring balanced performance on both infrequent and frequent PIE types, in addition to using a wide range of corpora. Acknowledgements This w"
W18-4919,J09-1005,0,0.0199713,"ver PIE types, in addition to the harmonic mean of the two. 4 Comparison to Related Work Ideally, we would be able to compare approaches from different papers directly, but this is often impossible. The lack of an established evaluation framework means that reported results for PIE disambiguation are often on different (splits of) datasets, obtained in different ways (cross-validation, leave-oneout) using a range of different metrics (micro- and macro-averaged accuracy and F1-score). For example, Sporleder and Li (2009) report micro-accuracy and micro-F1 scores on the Gigaword corpus, whereas Fazly et al. (2009) report macro-accuracy scores on the VNC-Tokens dataset. A potential solution to this problem was provided by SemEval-2013 Task 5b on PIE disambiguation (Korkontzelos et al., 2013), as results from different participants could be directly compared. However, this dataset does not seem to have been used by the community since. Nevertheless, we compare to two other unsupervised approaches, the canonical form classifier (CForm) by Fazly et al. (2009), and the k-means clustering approach (KMeans) of Gharbieh et al. (2016). The CForm classifier is based on the assumption that idiomatic PIEs show les"
W18-4919,W16-1817,0,0.0736863,"(2009) report micro-accuracy and micro-F1 scores on the Gigaword corpus, whereas Fazly et al. (2009) report macro-accuracy scores on the VNC-Tokens dataset. A potential solution to this problem was provided by SemEval-2013 Task 5b on PIE disambiguation (Korkontzelos et al., 2013), as results from different participants could be directly compared. However, this dataset does not seem to have been used by the community since. Nevertheless, we compare to two other unsupervised approaches, the canonical form classifier (CForm) by Fazly et al. (2009), and the k-means clustering approach (KMeans) of Gharbieh et al. (2016). The CForm classifier is based on the assumption that idiomatic PIEs show less variability than literal ones. It uses a set of canonical forms for each idiom (e.g. make a mark, make one’s mark), and labels all PIEs occurring in a canonical form as idiomatic, and literal otherwise. The KMeans classifier builds vector representations of both the PIE and its immediate context based on word embeddings, and clusters those using the k-means algorithm. It then uses the CForm classifier to label the clusters, and propagates the majority label for each cluster to all PIEs in that cluster. Both are eva"
W18-4919,D17-1263,0,0.0152657,"successful enough to manage, but too successful for help. (British National Corpus (BNC; Burnard, 2007) - doc. ACP - sent. 1209) (2) There was still a dark blob, where it might have hit the wall. (BNC - doc. B2E - sent. 1531) Distinguishing literal and figurative uses is a crucial step towards being able to automatically interpret the meaning of a text containing idiomatic expressions. It has been shown that idiomatic expressions pose a challenge for various NLP applications (Sag et al., 2002), including sentiment analysis (Williams et al., 2015) and machine translation (Salton et al., 2014a; Isabelle et al., 2017). For the latter, it has also been shown that being able to interpret idioms indeed improves performance (Salton et al., 2014b). In this work, we use a method for unsupervised disambiguation that exploits semantic cohesion between the PIE and its context, based on the lexical cohesion approach pioneered by Sporleder and Li (2009). We extend this method and evaluate it on English data in a comprehensive evaluation framework, in order to answer the following research question: Do contexts enriched with literalisations of idioms provide a useful new signal for disambiguation? 2 Approach The disam"
W18-4919,S13-2007,0,0.191292,"ion asks whether literalisations of figurative senses are a useful source of information for improved disambiguation of PIEs. To provide an answer, we test our lexical cohesion graph with and without literalisation on a collection of existing datasets (Section 3.1), and evaluate performance using both micro- and macro-accuracy (Section 3.2). 3.1 Data In order to provide a comprehensive evaluation dataset, we make use of four sizeable corpora containing sense-annotated PIEs:4 the VNC-Tokens Dataset (Cook et al., 2008), the IDIX Corpus (Sporleder et al., 2010), the SemEval-2013 Task 5b dataset (Korkontzelos et al., 2013), and the PIE Corpus.5 An overview of these datasets is provided in Table 1. # Types # Instances # Sense labels Source Corpus VNC-Tokens IDIX SemEval-2013 Task 5b PIE Corpus 53 52 65 278 2,984 4,022 4,350 1,050 3 6 4 3 BNC BNC ukWaC BNC Combined (development) Combined (test) 299 146 8,235 3,073 2 2 BNC & ukWaC BNC & ukWaC Table 1: Overview of existing corpora of sense-annotated PIEs. The source corpus indicates the corpora from which the PIE instances were selected, either the British National Corpus (Burnard, 2007) or ukWaC (Ferraresi et al., 2008). Each corpus has slightly different benefits"
W18-4919,N16-1040,0,0.0132815,"is potential for combining the two types of classification to achieve better performance. Using average similarity differences as confidence values to pick one classifier over the other for a particular instance proved ineffective, but a more advanced setup combining the features of the two classifiers could yield a more effective combination. Moreover, literalisations are cheap to acquire and are available for many PIE types and for any language for which an idiom dictionary exists. Although we use manually created definitions, these can also be acquired and refined automatically, as done by Liu and Hwa (2016). A side benefit of considering both figurative and literal senses is that separate scores can be assigned for both senses. This could be used for detecting difficult cases like dual meanings or puns, since those cases would get high scores for both senses. In future work, our aim is to further improve these cohesion graph-based classifiers by exploring different similarity measures, such as those tested by Ehren (2017) for German. Another promising avenue is to use more compositional representations of contexts and literalisations (see also Gharbieh et al., 2016). This would also allow us to"
W18-4919,D14-1162,0,0.0812033,"Figure 1. In the original approach, though, it is only tested whether the literal sense fits or not, by comparing the full and pruned graph. However, this does not measure whether the figurative sense fits. Ideally, we would like to compare the fit of the literal and figurative senses directly. We do this by introducing and using idiom literalisations (Section 2.2). 2.1 Basic Lexical Cohesion Graph We reimplement the original lexical cohesion graph method with one major modification: instead of Normalized Google Distance we use cosine similarity between 300-dimensional GloVe word embeddings (Pennington et al., 2014). Furthermore, we adapt specifics of the classifier to optimise performance on the development set. We use only nouns to build the contexts, where the part-of-speech of words is determined automatically using the spaCy PoS-tagger3 , instead of both nouns and verbs. As a context window, we use two sentences of additional context on either side of the sentence containing the PIE. We also remove edges between two PIE component words, since those are the same for all instances of the same type and thus uninformative. Finally, PIEs are only classified as literal if average similarity of the pruned"
W18-4919,W14-1007,0,0.0175452,"to British youth: not successful enough to manage, but too successful for help. (British National Corpus (BNC; Burnard, 2007) - doc. ACP - sent. 1209) (2) There was still a dark blob, where it might have hit the wall. (BNC - doc. B2E - sent. 1531) Distinguishing literal and figurative uses is a crucial step towards being able to automatically interpret the meaning of a text containing idiomatic expressions. It has been shown that idiomatic expressions pose a challenge for various NLP applications (Sag et al., 2002), including sentiment analysis (Williams et al., 2015) and machine translation (Salton et al., 2014a; Isabelle et al., 2017). For the latter, it has also been shown that being able to interpret idioms indeed improves performance (Salton et al., 2014b). In this work, we use a method for unsupervised disambiguation that exploits semantic cohesion between the PIE and its context, based on the lexical cohesion approach pioneered by Sporleder and Li (2009). We extend this method and evaluate it on English data in a comprehensive evaluation framework, in order to answer the following research question: Do contexts enriched with literalisations of idioms provide a useful new signal for disambiguat"
W18-4919,W14-0806,0,0.0172588,"to British youth: not successful enough to manage, but too successful for help. (British National Corpus (BNC; Burnard, 2007) - doc. ACP - sent. 1209) (2) There was still a dark blob, where it might have hit the wall. (BNC - doc. B2E - sent. 1531) Distinguishing literal and figurative uses is a crucial step towards being able to automatically interpret the meaning of a text containing idiomatic expressions. It has been shown that idiomatic expressions pose a challenge for various NLP applications (Sag et al., 2002), including sentiment analysis (Williams et al., 2015) and machine translation (Salton et al., 2014a; Isabelle et al., 2017). For the latter, it has also been shown that being able to interpret idioms indeed improves performance (Salton et al., 2014b). In this work, we use a method for unsupervised disambiguation that exploits semantic cohesion between the PIE and its context, based on the lexical cohesion approach pioneered by Sporleder and Li (2009). We extend this method and evaluate it on English data in a comprehensive evaluation framework, in order to answer the following research question: Do contexts enriched with literalisations of idioms provide a useful new signal for disambiguat"
W18-4919,E09-1086,0,0.389799,"iguation of potentially idiomatic expressions involves determining the sense of a potentially idiomatic expression in a given context, e.g. determining that make hay in ‘Investment banks made hay while takeovers shone.’ is used in a figurative sense. This enables automatic interpretation of idiomatic expressions, which is important for applications like machine translation and sentiment analysis. In this work, we present an unsupervised approach for English that makes use of literalisations of idiom senses to improve disambiguation, which is based on the lexical cohesion graph-based method by Sporleder and Li (2009). Experimental results show that, while literalisation carries novel information, its performance falls short of that of state-of-the-art unsupervised methods. 1 Introduction Interpreting potentially idiomatic expressions (PIEs, for short) is the task of determining the meaning of PIEs in context.1 In its most basic form, it consists of distinguishing between the figurative and literal usage of a given expression, as illustrated by hit the wall in Examples (1) and (2), respectively. (1) Melanie hit the wall so familiar to British youth: not successful enough to manage, but too successful for h"
W18-4919,sporleder-etal-2010-idioms,0,0.0243996,"the idiom more concisely. 3 Experiments Our research question asks whether literalisations of figurative senses are a useful source of information for improved disambiguation of PIEs. To provide an answer, we test our lexical cohesion graph with and without literalisation on a collection of existing datasets (Section 3.1), and evaluate performance using both micro- and macro-accuracy (Section 3.2). 3.1 Data In order to provide a comprehensive evaluation dataset, we make use of four sizeable corpora containing sense-annotated PIEs:4 the VNC-Tokens Dataset (Cook et al., 2008), the IDIX Corpus (Sporleder et al., 2010), the SemEval-2013 Task 5b dataset (Korkontzelos et al., 2013), and the PIE Corpus.5 An overview of these datasets is provided in Table 1. # Types # Instances # Sense labels Source Corpus VNC-Tokens IDIX SemEval-2013 Task 5b PIE Corpus 53 52 65 278 2,984 4,022 4,350 1,050 3 6 4 3 BNC BNC ukWaC BNC Combined (development) Combined (test) 299 146 8,235 3,073 2 2 BNC & ukWaC BNC & ukWaC Table 1: Overview of existing corpora of sense-annotated PIEs. The source corpus indicates the corpora from which the PIE instances were selected, either the British National Corpus (Burnard, 2007) or ukWaC (Ferrar"
W19-0504,E17-2039,1,0.867206,"Missing"
W19-0504,W17-6901,1,0.701943,"ijn, 1972). In a post-processing step, the original clause structured is restored.1 To include morphological and syntactic information, we apply a lemmatizer, POS-tagger and dependency parser using Stanford CoreNLP (Manning et al., 2014), similar to Sennrich and Haddow (2016) for machine translation. The lemmas and POS-tags are added as a token after each word. For the dependency parse, we add the incoming arc for each word. We also apply the easyCCG parser of Lewis and Steedman (2014), using the supertags.2 Finally, we exploit semantic information by using semantic tags (Bjerva et al., 2016; Abzianidze and Bos, 2017). Semantic tags are language-neutral semantic categories, which get assigned to a word in a similar fashion as part-of-speech tags. Semantic tags are able to express important semantic distinctions, such as negation, modals and types of quantification. We train a semantic tagger with the TnT tagger (Brants, 2000) on the gold and silver standard data in the PMB release. Examples of the input to the model for each source of information are shown in Table 1. 1 2 See Van Noord, Abzianidze, Toral, and Bos (2018) for a more detailed overview of the representation used. We segment the supertags, e.g."
W19-0504,P17-2021,0,0.0279904,"al., 2017; Dong and Lapata, 2018; Liu et al., 2018; Van Noord, Abzianidze, Toral, and Bos, 2018). This architecture is able to learn meaning representations for a range of semantic phenomena, usually without resorting to any linguistic information such as part-ofspeech or syntax. Though this is an impressive feat in itself, there is no reason to abandon these resources. Even in machine translation, where models can be trained on relatively large data sets, it has been shown that sequence-to-sequence models can benefit from external syntactic and semantic resources (Sennrich and Haddow, 2016; Aharoni and Goldberg, 2017) and a multi-source approach has proved particularly successful for adding syntax (Currey and Heafield, 2018). The current approaches in neural semantic parsing either include (some) linguistic information in a single encoder (POS-tags in Van Noord and Bos 2017a,b, lemmas in Liu et al. 2018), or use multiple encoders to represent multiple languages rather than linguistic knowledge (Duong et al., 2017; Susanto and Lu, 2017). To our knowledge, we are the first to investigate the potential of exploiting linguistic information in a multi-encoder setup for (neural) semantic parsing. Specifically, t"
W19-0504,C16-1333,1,0.810388,"Bruijn index (de Bruijn, 1972). In a post-processing step, the original clause structured is restored.1 To include morphological and syntactic information, we apply a lemmatizer, POS-tagger and dependency parser using Stanford CoreNLP (Manning et al., 2014), similar to Sennrich and Haddow (2016) for machine translation. The lemmas and POS-tags are added as a token after each word. For the dependency parse, we add the incoming arc for each word. We also apply the easyCCG parser of Lewis and Steedman (2014), using the supertags.2 Finally, we exploit semantic information by using semantic tags (Bjerva et al., 2016; Abzianidze and Bos, 2017). Semantic tags are language-neutral semantic categories, which get assigned to a word in a similar fashion as part-of-speech tags. Semantic tags are able to express important semantic distinctions, such as negation, modals and types of quantification. We train a semantic tagger with the TnT tagger (Brants, 2000) on the gold and silver standard data in the PMB release. Examples of the input to the model for each source of information are shown in Table 1. 1 2 See Van Noord, Abzianidze, Toral, and Bos (2018) for a more detailed overview of the representation used. We"
W19-0504,W08-2222,1,0.464715,"similar to only using gold data, with the exception of the semantic tags, which even hurt the performance now. Interestingly, when stacking the linguistic features, there is no improvement over only using the lemma of the source words. We now compare our best models to previous parsers4 (Bos, 2015; Van Noord, Abzianidze, Toral, and Bos, 2018) and two baseline systems, SPAR and SIM - SPAR. As previously indicated, Van Noord, Abzianidze, Toral, and Bos (2018) used a similar sequence-to-sequence model as our current approach, but implemented in OpenNMT and without the linguistic features. Boxer (Bos, 2008, 2015) is a DRS parser that uses a statistical CCG parser for syntactic analysis and a compositional semantics based on λcalculus, followed by pronoun and presupposition resolution. SPAR is a baseline system that outputs the same DRS for each test instance5 , while SIM - SPAR outputs the DRS of the most similar sentence in the training set, based on a simple word embedding metric.6 The results are shown in Table 5. Our model clearly outperforms the previous systems, even when only using gold standard data. When compared to Van Noord, Abzianidze, Toral, and Bos (2018), retrained with the same"
W19-0504,W15-1841,1,0.921824,"observe an improvement for each addition, resulting in a final 2.7 point F-score increase over the baseline. If we also employ silver data, we again observe that the multi-encoder setup is preferable over a single encoder, for both isolating and stacking the linguistic features. On isolation, the results are similar to only using gold data, with the exception of the semantic tags, which even hurt the performance now. Interestingly, when stacking the linguistic features, there is no improvement over only using the lemma of the source words. We now compare our best models to previous parsers4 (Bos, 2015; Van Noord, Abzianidze, Toral, and Bos, 2018) and two baseline systems, SPAR and SIM - SPAR. As previously indicated, Van Noord, Abzianidze, Toral, and Bos (2018) used a similar sequence-to-sequence model as our current approach, but implemented in OpenNMT and without the linguistic features. Boxer (Bos, 2008, 2015) is a DRS parser that uses a statistical CCG parser for syntactic analysis and a compositional semantics based on λcalculus, followed by pronoun and presupposition resolution. SPAR is a baseline system that outputs the same DRS for each test instance5 , while SIM - SPAR outputs the"
W19-0504,A00-1031,0,0.0448471,"are added as a token after each word. For the dependency parse, we add the incoming arc for each word. We also apply the easyCCG parser of Lewis and Steedman (2014), using the supertags.2 Finally, we exploit semantic information by using semantic tags (Bjerva et al., 2016; Abzianidze and Bos, 2017). Semantic tags are language-neutral semantic categories, which get assigned to a word in a similar fashion as part-of-speech tags. Semantic tags are able to express important semantic distinctions, such as negation, modals and types of quantification. We train a semantic tagger with the TnT tagger (Brants, 2000) on the gold and silver standard data in the PMB release. Examples of the input to the model for each source of information are shown in Table 1. 1 2 See Van Noord, Abzianidze, Toral, and Bos (2018) for a more detailed overview of the representation used. We segment the supertags, e.g. (SNP)(SNP) is represented as ( S  NP )  ( S  NP ) Table 1: Example representations for each source of input information. Source Representation Sentence Lemma POS-tags Dependency parse Semantic tags CCG supertags I am not working for Tom . I be not work for Tom . PRP VBP RB VBG IN NNP . nsubj aux neg ROOT c"
W19-0504,P13-2131,0,0.0233128,"300 15 Dropout RNN Dropout src/tgt Batch size Optimization crit Vocab size src/tgt Optimizer 0.2 0.0 12 ce-mean 80/150 adam Learning rate (LR) LR decay LR decay strategy LR decay start Clip normalization 0.002 0.8 epoch 9 3 Beam size Length normalization Label smoothing Skip connections Layer normalization 10 0.9 0.1 True True 2.4 Evaluation Procedure Produced DRSs are compared with the gold standard representations by using COUNTER (Van Noord, Abzianidze, Haagsma, and Bos, 2018). This is a tool that calculates micro precision, recall and F-score over matching clauses, similar to the SMATCH (Cai and Knight, 2013) evaluation tool for AMR parsing. All clauses have the same weight in matching, except for REF clauses, which are ignored. An example of the matching procedure is shown in Figure 1. The produced DRSs go through a strict syntactic and semantic validation process, as described in Van Noord, Abzianidze, Toral, and Bos (2018). If a produced DRS is invalid, it is replaced by a dummy DRS, which gets an F-score of 0.0. We check whether two systems differ significantly by performing approximate randomization (Noreen, 1989), with α = 0.05, R = 1000 and F (model1 ) > F (model2 ) as test statistic for ea"
W19-0504,D18-1327,0,0.076743,"ecture is able to learn meaning representations for a range of semantic phenomena, usually without resorting to any linguistic information such as part-ofspeech or syntax. Though this is an impressive feat in itself, there is no reason to abandon these resources. Even in machine translation, where models can be trained on relatively large data sets, it has been shown that sequence-to-sequence models can benefit from external syntactic and semantic resources (Sennrich and Haddow, 2016; Aharoni and Goldberg, 2017) and a multi-source approach has proved particularly successful for adding syntax (Currey and Heafield, 2018). The current approaches in neural semantic parsing either include (some) linguistic information in a single encoder (POS-tags in Van Noord and Bos 2017a,b, lemmas in Liu et al. 2018), or use multiple encoders to represent multiple languages rather than linguistic knowledge (Duong et al., 2017; Susanto and Lu, 2017). To our knowledge, we are the first to investigate the potential of exploiting linguistic information in a multi-encoder setup for (neural) semantic parsing. Specifically, the aims of this paper are to investigate (i) whether exploiting linguistic information can improve semantic p"
W19-0504,P18-1068,0,0.0211209,"achine translation has shown that employing this information has a lot of potential, especially when using a multi-encoder setup. We employ a range of semantic and syntactic resources to improve performance for the task of Discourse Representation Structure Parsing. We show that (i) linguistic features can be beneficial for neural semantic parsing and (ii) the best method of adding these features is by using multiple encoders. 1 Introduction Sequence-to-sequence neural networks have shown remarkable performance in semantic parsing (Ling et al., 2016; Jia and Liang, 2016; Konstas et al., 2017; Dong and Lapata, 2018; Liu et al., 2018; Van Noord, Abzianidze, Toral, and Bos, 2018). This architecture is able to learn meaning representations for a range of semantic phenomena, usually without resorting to any linguistic information such as part-ofspeech or syntax. Though this is an impressive feat in itself, there is no reason to abandon these resources. Even in machine translation, where models can be trained on relatively large data sets, it has been shown that sequence-to-sequence models can benefit from external syntactic and semantic resources (Sennrich and Haddow, 2016; Aharoni and Goldberg, 2017) and a"
W19-0504,K17-1038,0,0.0139763,"els can be trained on relatively large data sets, it has been shown that sequence-to-sequence models can benefit from external syntactic and semantic resources (Sennrich and Haddow, 2016; Aharoni and Goldberg, 2017) and a multi-source approach has proved particularly successful for adding syntax (Currey and Heafield, 2018). The current approaches in neural semantic parsing either include (some) linguistic information in a single encoder (POS-tags in Van Noord and Bos 2017a,b, lemmas in Liu et al. 2018), or use multiple encoders to represent multiple languages rather than linguistic knowledge (Duong et al., 2017; Susanto and Lu, 2017). To our knowledge, we are the first to investigate the potential of exploiting linguistic information in a multi-encoder setup for (neural) semantic parsing. Specifically, the aims of this paper are to investigate (i) whether exploiting linguistic information can improve semantic parsing and (ii) whether it is better to include this linguistic information in the same encoder or in an additional one. We take as baseline the neural semantic parser for Discourse Representation Structures (DRS, Kamp and Reyle, 1993; Van Noord, Abzianidze, Haagsma, and Bos, 2018) developed b"
W19-0504,N16-1101,0,0.0338903,"ncy parse Semantic tags CCG supertags I am not working for Tom . I be not work for Tom . PRP VBP RB VBG IN NNP . nsubj aux neg ROOT case nmod punct PRO NOW NOT EXG REL PER NIL NP (S[dcl]NP)/(S[ng]NP) (SNP)(SNP) (S[ng]NP)/PP PP/NP N . There are two ways to add the linguistic information; (1) merging all the information (i.e., input text and linguistic information) in a single encoder, or (2) using multiple encoders (i.e., encoding separately the input text and the linguistic information). Multi-source encoders were initially introduced for multilingual translation (Zoph and Knight, 2016; Firat et al., 2016; Libovick´y and Helcl, 2017), but recently were used to introduce syntactic information to the model (Currey and Heafield, 2018). Table 2 shows examples of how the input is structured for using one or more encoders. Table 2: Example representation when using one or two encoders, for either a single source of information (POS) or multiple sources (POS + Sem) for the sentence I am not working for Tom. For readability purposes we show the word-level instead of character-level representation of the source words here. Source Encoder Representation POS - 1 enc Enc 1 I PRP am VBP not RB working VBG"
W19-0504,P16-1002,0,0.0423357,"formance even further. Research in neural machine translation has shown that employing this information has a lot of potential, especially when using a multi-encoder setup. We employ a range of semantic and syntactic resources to improve performance for the task of Discourse Representation Structure Parsing. We show that (i) linguistic features can be beneficial for neural semantic parsing and (ii) the best method of adding these features is by using multiple encoders. 1 Introduction Sequence-to-sequence neural networks have shown remarkable performance in semantic parsing (Ling et al., 2016; Jia and Liang, 2016; Konstas et al., 2017; Dong and Lapata, 2018; Liu et al., 2018; Van Noord, Abzianidze, Toral, and Bos, 2018). This architecture is able to learn meaning representations for a range of semantic phenomena, usually without resorting to any linguistic information such as part-ofspeech or syntax. Though this is an impressive feat in itself, there is no reason to abandon these resources. Even in machine translation, where models can be trained on relatively large data sets, it has been shown that sequence-to-sequence models can benefit from external syntactic and semantic resources (Sennrich and Ha"
W19-0504,P18-4020,0,0.0339798,"NIL Experiments showed that using more than two encoders drastically decreased performance. Therefore, we merge all the linguistic information in a single encoder (see last row of Table 2). 2.3 Neural Architecture We employ a recurrent sequence-to-sequence neural network with attention (Bahdanau et al., 2014) and two bi-LSTM layers, similar to the one used by Van Noord, Abzianidze, Toral, and Bos (2018). However, their model was trained with OpenNMT (Klein et al., 2017), which does not support multiple encoders. Therefore, we switch to the sequence-to-sequence framework implemented in Marian (Junczys-Dowmunt et al., 2018). We use model-type s2s (for a single encoder) or multi-s2s (for multiple encoders). For the latter, this means that the multiple inputs are encoded separately by an identical RNN (without sharing parameters). The encoders share a single decoder, in which the resulting context vectors are concatenated. An attention layer3 is then applied to selectively give more attention to certain parts of the vector (i.e. it can learn that the words themselves are more important than just the POS-tags). A detailed overview of our parameter settings, found after a search on the dev set, can be found in Table"
W19-0504,P17-4012,0,0.0484199,"VBG EXG for IN REL Tom NNP PER . . NIL POS + Sem - 2 enc Enc 1 Enc 2 I am not working for Tom . PRP PRO VBP NOW RB NOT VBG EXG IN REL NNP PER . NIL Experiments showed that using more than two encoders drastically decreased performance. Therefore, we merge all the linguistic information in a single encoder (see last row of Table 2). 2.3 Neural Architecture We employ a recurrent sequence-to-sequence neural network with attention (Bahdanau et al., 2014) and two bi-LSTM layers, similar to the one used by Van Noord, Abzianidze, Toral, and Bos (2018). However, their model was trained with OpenNMT (Klein et al., 2017), which does not support multiple encoders. Therefore, we switch to the sequence-to-sequence framework implemented in Marian (Junczys-Dowmunt et al., 2018). We use model-type s2s (for a single encoder) or multi-s2s (for multiple encoders). For the latter, this means that the multiple inputs are encoded separately by an identical RNN (without sharing parameters). The encoders share a single decoder, in which the resulting context vectors are concatenated. An attention layer3 is then applied to selectively give more attention to certain parts of the vector (i.e. it can learn that the words thems"
W19-0504,P17-1014,0,0.0454551,". Research in neural machine translation has shown that employing this information has a lot of potential, especially when using a multi-encoder setup. We employ a range of semantic and syntactic resources to improve performance for the task of Discourse Representation Structure Parsing. We show that (i) linguistic features can be beneficial for neural semantic parsing and (ii) the best method of adding these features is by using multiple encoders. 1 Introduction Sequence-to-sequence neural networks have shown remarkable performance in semantic parsing (Ling et al., 2016; Jia and Liang, 2016; Konstas et al., 2017; Dong and Lapata, 2018; Liu et al., 2018; Van Noord, Abzianidze, Toral, and Bos, 2018). This architecture is able to learn meaning representations for a range of semantic phenomena, usually without resorting to any linguistic information such as part-ofspeech or syntax. Though this is an impressive feat in itself, there is no reason to abandon these resources. Even in machine translation, where models can be trained on relatively large data sets, it has been shown that sequence-to-sequence models can benefit from external syntactic and semantic resources (Sennrich and Haddow, 2016; Aharoni an"
W19-0504,D14-1107,0,0.0285442,"iable names itself are meaningless, the DRS variables are rewritten to a more general representation, using the De Bruijn index (de Bruijn, 1972). In a post-processing step, the original clause structured is restored.1 To include morphological and syntactic information, we apply a lemmatizer, POS-tagger and dependency parser using Stanford CoreNLP (Manning et al., 2014), similar to Sennrich and Haddow (2016) for machine translation. The lemmas and POS-tags are added as a token after each word. For the dependency parse, we add the incoming arc for each word. We also apply the easyCCG parser of Lewis and Steedman (2014), using the supertags.2 Finally, we exploit semantic information by using semantic tags (Bjerva et al., 2016; Abzianidze and Bos, 2017). Semantic tags are language-neutral semantic categories, which get assigned to a word in a similar fashion as part-of-speech tags. Semantic tags are able to express important semantic distinctions, such as negation, modals and types of quantification. We train a semantic tagger with the TnT tagger (Brants, 2000) on the gold and silver standard data in the PMB release. Examples of the input to the model for each source of information are shown in Table 1. 1 2 S"
W19-0504,P17-2031,0,0.0536274,"Missing"
W19-0504,P16-1057,0,0.0359713,"Missing"
W19-0504,P18-1040,0,0.251877,"shown that employing this information has a lot of potential, especially when using a multi-encoder setup. We employ a range of semantic and syntactic resources to improve performance for the task of Discourse Representation Structure Parsing. We show that (i) linguistic features can be beneficial for neural semantic parsing and (ii) the best method of adding these features is by using multiple encoders. 1 Introduction Sequence-to-sequence neural networks have shown remarkable performance in semantic parsing (Ling et al., 2016; Jia and Liang, 2016; Konstas et al., 2017; Dong and Lapata, 2018; Liu et al., 2018; Van Noord, Abzianidze, Toral, and Bos, 2018). This architecture is able to learn meaning representations for a range of semantic phenomena, usually without resorting to any linguistic information such as part-ofspeech or syntax. Though this is an impressive feat in itself, there is no reason to abandon these resources. Even in machine translation, where models can be trained on relatively large data sets, it has been shown that sequence-to-sequence models can benefit from external syntactic and semantic resources (Sennrich and Haddow, 2016; Aharoni and Goldberg, 2017) and a multi-source appr"
W19-0504,P14-5010,0,0.00241439,"cters. The target DRS is also represented as a sequence of characters, with the exception of DRS operators, thematic roles and DRS variables, which are represented as super characters (Van Noord and Bos, 2017b), i.e. individual tokens. Since the variable names itself are meaningless, the DRS variables are rewritten to a more general representation, using the De Bruijn index (de Bruijn, 1972). In a post-processing step, the original clause structured is restored.1 To include morphological and syntactic information, we apply a lemmatizer, POS-tagger and dependency parser using Stanford CoreNLP (Manning et al., 2014), similar to Sennrich and Haddow (2016) for machine translation. The lemmas and POS-tags are added as a token after each word. For the dependency parse, we add the incoming arc for each word. We also apply the easyCCG parser of Lewis and Steedman (2014), using the supertags.2 Finally, we exploit semantic information by using semantic tags (Bjerva et al., 2016; Abzianidze and Bos, 2017). Semantic tags are language-neutral semantic categories, which get assigned to a word in a similar fashion as part-of-speech tags. Semantic tags are able to express important semantic distinctions, such as negat"
W19-0504,W16-2209,0,0.091739,"and Liang, 2016; Konstas et al., 2017; Dong and Lapata, 2018; Liu et al., 2018; Van Noord, Abzianidze, Toral, and Bos, 2018). This architecture is able to learn meaning representations for a range of semantic phenomena, usually without resorting to any linguistic information such as part-ofspeech or syntax. Though this is an impressive feat in itself, there is no reason to abandon these resources. Even in machine translation, where models can be trained on relatively large data sets, it has been shown that sequence-to-sequence models can benefit from external syntactic and semantic resources (Sennrich and Haddow, 2016; Aharoni and Goldberg, 2017) and a multi-source approach has proved particularly successful for adding syntax (Currey and Heafield, 2018). The current approaches in neural semantic parsing either include (some) linguistic information in a single encoder (POS-tags in Van Noord and Bos 2017a,b, lemmas in Liu et al. 2018), or use multiple encoders to represent multiple languages rather than linguistic knowledge (Duong et al., 2017; Susanto and Lu, 2017). To our knowledge, we are the first to investigate the potential of exploiting linguistic information in a multi-encoder setup for (neural) sema"
W19-0504,P17-2007,0,0.0309244,"n relatively large data sets, it has been shown that sequence-to-sequence models can benefit from external syntactic and semantic resources (Sennrich and Haddow, 2016; Aharoni and Goldberg, 2017) and a multi-source approach has proved particularly successful for adding syntax (Currey and Heafield, 2018). The current approaches in neural semantic parsing either include (some) linguistic information in a single encoder (POS-tags in Van Noord and Bos 2017a,b, lemmas in Liu et al. 2018), or use multiple encoders to represent multiple languages rather than linguistic knowledge (Duong et al., 2017; Susanto and Lu, 2017). To our knowledge, we are the first to investigate the potential of exploiting linguistic information in a multi-encoder setup for (neural) semantic parsing. Specifically, the aims of this paper are to investigate (i) whether exploiting linguistic information can improve semantic parsing and (ii) whether it is better to include this linguistic information in the same encoder or in an additional one. We take as baseline the neural semantic parser for Discourse Representation Structures (DRS, Kamp and Reyle, 1993; Van Noord, Abzianidze, Haagsma, and Bos, 2018) developed by Van Noord, Abzianidze"
W19-0504,L18-1267,1,0.669753,"Missing"
W19-0504,Q18-1043,1,0.801546,"Missing"
W19-0504,W17-7306,1,0.922095,"Missing"
W19-0504,N16-1004,0,0.0191766,"Lemma POS-tags Dependency parse Semantic tags CCG supertags I am not working for Tom . I be not work for Tom . PRP VBP RB VBG IN NNP . nsubj aux neg ROOT case nmod punct PRO NOW NOT EXG REL PER NIL NP (S[dcl]NP)/(S[ng]NP) (SNP)(SNP) (S[ng]NP)/PP PP/NP N . There are two ways to add the linguistic information; (1) merging all the information (i.e., input text and linguistic information) in a single encoder, or (2) using multiple encoders (i.e., encoding separately the input text and the linguistic information). Multi-source encoders were initially introduced for multilingual translation (Zoph and Knight, 2016; Firat et al., 2016; Libovick´y and Helcl, 2017), but recently were used to introduce syntactic information to the model (Currey and Heafield, 2018). Table 2 shows examples of how the input is structured for using one or more encoders. Table 2: Example representation when using one or two encoders, for either a single source of information (POS) or multiple sources (POS + Sem) for the sentence I am not working for Tom. For readability purposes we show the word-level instead of character-level representation of the source words here. Source Encoder Representation POS - 1 enc Enc 1 I PRP am VBP"
W19-1201,E17-2039,1,0.748046,"Missing"
W19-1201,W17-6901,1,0.840403,"e substantially improved by first pre-training on gold and silver data, after which the parser is fine-tuned on only the gold standard data. 6.2 Van Noord (2019) The system of N OORD ET AL .19 is the parser described in Van Noord et al. (2019) and Van Noord (2019), which follows up on their work previously described in Van Noord et al. (2018). They improve on this work in two ways: (i) by switching their sequence-to-sequence framework from OpenNMT (Klein et al., 2017) to Marian (Junczys-Dowmunt et al., 2018) and (ii) by providing the encoder with linguistic information (lemmas, semantic tags (Abzianidze and Bos, 2017), POS-tags, dependency parses and CCG supertags) that are encoded in a separate encoder. 6.3 Liu et al. (2019) L IU ET AL . also follow the approach of Van Noord et al. (2018) in terms of pre- and postprocessing the data, but they improve on it by using the Transformer model (Vaswani et al., 2017), instead of a sequence-to-sequence RNN. Also, they show that employing the bronze standard in addition to the gold and silver standard leads to improved performance. 6.4 Evang (2019) E VANG aim to find a middle-ground between traditional symbolic approaches and the recent neural (sequence-to-sequence"
W19-1201,W08-2227,0,0.110596,"Missing"
W19-1201,W13-2101,1,0.829064,"t producing meaning representations that (i) come with more expressive power than existing ones and (ii) are easily translatable into formal logic, thereby opening the door to applications that require automated forms of inference (Blackburn and Bos, 2005; Dagan et al., 2013). DRSs are meaning representations employed by Discourse Representation Theory (DRT, Kamp and Reyle, 1993). They have been successfully applied for wide-coverage semantic representations (Bos et al., 2004; Bos, 2008), Natural Language Inference (Bos and Markert, 2005; Bjerva et al., 2014), and Natural Language Generation (Basile and Bos, 2013). To the best of our knowledge, there has never been a shared task on scoped meaning representations. The aim of the task is to compare semantic parsing methods and the performance of systems that take as input an English text and provide as output the scoped meaning representation of that text Since a DRS combines logical (negation, quantification and modals), pragmatic (presuppositions) and lexical (word senses and thematic roles) components of semantics in a single meaning representation, the DRS parsing task shares parts of the following NLP tasks: semantic role labeling, reference resolut"
W19-1201,S14-2114,1,0.892524,"Missing"
W19-1201,W08-2222,1,0.858273,"shcovich et al., 2019). The Discourse Representation Structure (DRS) parsing task extends this development by aiming at producing meaning representations that (i) come with more expressive power than existing ones and (ii) are easily translatable into formal logic, thereby opening the door to applications that require automated forms of inference (Blackburn and Bos, 2005; Dagan et al., 2013). DRSs are meaning representations employed by Discourse Representation Theory (DRT, Kamp and Reyle, 1993). They have been successfully applied for wide-coverage semantic representations (Bos et al., 2004; Bos, 2008), Natural Language Inference (Bos and Markert, 2005; Bjerva et al., 2014), and Natural Language Generation (Basile and Bos, 2013). To the best of our knowledge, there has never been a shared task on scoped meaning representations. The aim of the task is to compare semantic parsing methods and the performance of systems that take as input an English text and provide as output the scoped meaning representation of that text Since a DRS combines logical (negation, quantification and modals), pragmatic (presuppositions) and lexical (word senses and thematic roles) components of semantics in a singl"
W19-1201,W15-1841,1,0.897507,"tion and modals), pragmatic (presuppositions) and lexical (word senses and thematic roles) components of semantics in a single meaning representation, the DRS parsing task shares parts of the following NLP tasks: semantic role labeling, reference resolution, scope detection, named entity tagging, word sense disambiguation, predicate-argument structure prediction, and presupposition projection. There are only a few previous approaches to DRS parsing. Traditionally, due to the complexity of the task, it has been the domain of symbolic and statistical approaches (Bos, 2008; Le and Zuidema, 2012; Bos, 2015). Recently, however, neural sequence-to-sequence systems achieved impressive performance on the task (Liu et al., 2018; Van Noord et al., 2018), without relying on any external linguistic resources. 00/3008: He played the piano and she sang. SYSTEM INPUT : Tom isn’t afraid of anything. SYSTEM OUTPUT: b1 b1 b1 b2 b2 b2 b2 b3 b3 b3 b3 b3 b3 b3 REF x1 male &quot;n.02&quot; x1 Name x1 &quot;tom&quot; REF t1 EQU t1 &quot;now&quot; time &quot;n.08&quot; t1 NOT b3 REF s1 Time s1 t1 Experiencer s1 x1 afraid &quot;a.01&quot; s1 Stimulus s1 x2 REF x2 entity &quot;n.01&quot; x2 b6 b2 b2 b1 b1 b1 b1 b3 b3 b1 b1 b1 DRS b1 REF x1 male &quot;n.02&quot; x1 REF e1 play &quot;v.03&quot; e1"
W19-1201,J16-3006,1,0.856426,"= 0.05, R = 1000 and F (model1 ) &gt; F (model2 ) as test statistic for each individual DRS pair. 5.3 Baselines We provide three baseline parsers: SPAR, SIM - SPAR and AMR 2 DRS. SPAR simply outputs a default DRS, which is a DRS that is the most similar to the DRSs in our training set.8 SIM - SPAR outputs the DRS of the most similar sentence in the training set, based on the cosine distance of the average word-embedding vector, calculated using GloVe (Pennington et al., 2014). AMR 2 DRS is a script that converts the output of an AMR parser to a valid DRS by applying a set of rules, described in Bos (2016) and van Noord, Abzianidze, Haagsma, and Bos (2018). We will provide scores on the development, test and evaluation sets by using the AMR parser of van Noord and Bos (2017). 6 Participating Systems We received a total of five submissions in the shared task out of 32 registered participants. Three out of five submitted a system paper. The general characteristics of the participating systems are give in Table 2. Following Nissim et al. (2017), we explicitly encouraged the participants to include ablation experiments and negative results (if any). Note that the authors of the systems N OORD ET AL"
W19-1201,W17-6905,1,0.941078,"ication. A DRS always contains a main labelled box along with an optional set of presupposition DRSs (see Definition 1). For example, the main labelled box in Figure 1 is b0 while b0 is a presupposition. A box can be simple (e.g., the box labelled with b0 in Figure 1) or segmented (e.g., the box labelled with b6 in Figure 2). A simple box consists of a set of discourse referents and a set of conditions. Conditions can be basic or complex. Basic conditions are concept predicates or relations over discourse referents and constants. Indexicals are treated as constants, not as discourse referents Bos (2017), for example, now is one of such indexicals (see Figure 1). Complex conditions are those involving labelled boxes. The examples of complex conditions are ¬b3 in Figure 1 and b3 ⇒ b5 in Figure 3. Finally, a segmented box contains a set of labelled boxes (b1 and 4 in Figure 2) and discourse conditions. A discourse condition is a discourse relations over box labels, e.g., CONTINUATION(b1 , b4 ) in Figure 2. Definition 1: A BNF of DRSs: (possibly empty) sets are denoted with curly brackets as {h elementi}. The string elements for operators and punctuation are in red. hDRSi hlabelled BOXi hBOXi hs"
W19-1201,C04-1180,1,0.577513,"ve Annotation (Hershcovich et al., 2019). The Discourse Representation Structure (DRS) parsing task extends this development by aiming at producing meaning representations that (i) come with more expressive power than existing ones and (ii) are easily translatable into formal logic, thereby opening the door to applications that require automated forms of inference (Blackburn and Bos, 2005; Dagan et al., 2013). DRSs are meaning representations employed by Discourse Representation Theory (DRT, Kamp and Reyle, 1993). They have been successfully applied for wide-coverage semantic representations (Bos et al., 2004; Bos, 2008), Natural Language Inference (Bos and Markert, 2005; Bjerva et al., 2014), and Natural Language Generation (Basile and Bos, 2013). To the best of our knowledge, there has never been a shared task on scoped meaning representations. The aim of the task is to compare semantic parsing methods and the performance of systems that take as input an English text and provide as output the scoped meaning representation of that text Since a DRS combines logical (negation, quantification and modals), pragmatic (presuppositions) and lexical (word senses and thematic roles) components of semantic"
W19-1201,H05-1079,1,0.506309,"presentation Structure (DRS) parsing task extends this development by aiming at producing meaning representations that (i) come with more expressive power than existing ones and (ii) are easily translatable into formal logic, thereby opening the door to applications that require automated forms of inference (Blackburn and Bos, 2005; Dagan et al., 2013). DRSs are meaning representations employed by Discourse Representation Theory (DRT, Kamp and Reyle, 1993). They have been successfully applied for wide-coverage semantic representations (Bos et al., 2004; Bos, 2008), Natural Language Inference (Bos and Markert, 2005; Bjerva et al., 2014), and Natural Language Generation (Basile and Bos, 2013). To the best of our knowledge, there has never been a shared task on scoped meaning representations. The aim of the task is to compare semantic parsing methods and the performance of systems that take as input an English text and provide as output the scoped meaning representation of that text Since a DRS combines logical (negation, quantification and modals), pragmatic (presuppositions) and lexical (word senses and thematic roles) components of semantics in a single meaning representation, the DRS parsing task shar"
W19-1201,P13-2131,0,0.103308,"box in the DRS. All the released clausal forms of the DRSs are valid. We provided the participants with REFEREE in order to help them identify the ill-formed clausal forms produced by their systems. 5.2 Evaluation The evaluation defines to what degree a system output clausal form is similar to the corresponding gold one. To compare the system output and gold representations, we compute the F1-score over the clauses, following Allen et al. (2008). We use the tool COUNTER (van Noord, Abzianidze, Haagsma, and Bos, 2018), which is specifically designed to evaluate DRSs. It is based on the SMATCH Cai and Knight (2013) tool that is used to evaluate AMR parsers. It is essentially a hill-climbing algorithm that finds the best variable mapping between the produced DRS and the gold standard. To avoid local optima, we restart the procedure 10 times. In order to prevent an inflated F-score, before searching the maximal matching, COUNTER discards those REF-clauses which are deemed redundant. A REF-clause hb REF xi is redundant if and only if its discourse referent x occurs with a concept predicate in a basic condition of the same box b – in other words, there exists a clause of the form hb concept &quot;pos.nn&quot; xi.7 An"
W19-1201,N16-1024,0,0.0728764,"Missing"
W19-1201,W19-1202,0,0.131584,"wmunt et al., 2018) and (ii) by providing the encoder with linguistic information (lemmas, semantic tags (Abzianidze and Bos, 2017), POS-tags, dependency parses and CCG supertags) that are encoded in a separate encoder. 6.3 Liu et al. (2019) L IU ET AL . also follow the approach of Van Noord et al. (2018) in terms of pre- and postprocessing the data, but they improve on it by using the Transformer model (Vaswani et al., 2017), instead of a sequence-to-sequence RNN. Also, they show that employing the bronze standard in addition to the gold and silver standard leads to improved performance. 6.4 Evang (2019) E VANG aim to find a middle-ground between traditional symbolic approaches and the recent neural (sequence-to-sequence) models. They employ a transition-based parser that relies on explicit wordmeaning pairs that are found in the training set. Parsing decisions are made based on vector representations of parser states, which are encoded using stack-LSTMs. 6.5 FANCELLU ET AL .9 FANCELLU ET AL . propose a graph decoder that given an input sentence encoded via a bidirectional LSTM generates a DAG (Directed Acyclic Graph) as a sequence of fragments from a graph grammar. These fragments are delexi"
W19-1201,P18-1170,0,0.0198685,"f fragments from a graph grammar. These fragments are delexicalized; predicate names, synset and information on whether the predicate is presupposed or not are predicted in a second step, conditioned on the fragment and the decoding history. Two are the main features of the graph parser: 1) it is agnostic to the underlying semantic formalism and does not need any preprocessing step to deal with variable binding; 2) fragments are aware of the overall graph structure and the graph is built incrementally via a process of non-terminal rewriting. (1) sets this method apart from the graph parser of Groschwitz et al. (2018) where a grammar is extracting via an elaborate pre-processing step, tailored to a specific formalism, whereas (2) allows to leverage neural sequential decoding (stackLSTM, Dyer et al., 2016). The only preprocessing step required is to convert DRSs in clause format into single-rooted, fully instantiated DAGs; we do so by treating both variables 9 The full list of authors: Federico Fancellu, Sorcha Gilroy, Adam Lopez and Mirella Lapata (University of Edinburgh). Since the authors refrained from submitting a system paper due to the ACL policy for submission, we include a slightly extended summar"
W19-1201,S19-2001,0,0.0592394,"alid DRSs, all these makes the DRS parsing a challenging NLP task. The results of the shared task displayed improvements over the existing state-of-the-art parser. 1 Introduction Semantic parsing has been gaining in popularity in the last few years. There have been a series of shared tasks in semantic parsing organized, where each task requires to generate meaning representations of specific types: Broad-Coverage Broad-coverage Semantic Dependencies (Oepen et al., 2014, 2015), Abstract Meaning Representation (May, 2016; May and Priyadarshi, 2017), or Universal Conceptual Cognitive Annotation (Hershcovich et al., 2019). The Discourse Representation Structure (DRS) parsing task extends this development by aiming at producing meaning representations that (i) come with more expressive power than existing ones and (ii) are easily translatable into formal logic, thereby opening the door to applications that require automated forms of inference (Blackburn and Bos, 2005; Dagan et al., 2013). DRSs are meaning representations employed by Discourse Representation Theory (DRT, Kamp and Reyle, 1993). They have been successfully applied for wide-coverage semantic representations (Bos et al., 2004; Bos, 2008), Natural La"
W19-1201,P18-4020,0,0.014391,"and not using character-level representation for DRS roles and operators. Moreover, they show that performance can be substantially improved by first pre-training on gold and silver data, after which the parser is fine-tuned on only the gold standard data. 6.2 Van Noord (2019) The system of N OORD ET AL .19 is the parser described in Van Noord et al. (2019) and Van Noord (2019), which follows up on their work previously described in Van Noord et al. (2018). They improve on this work in two ways: (i) by switching their sequence-to-sequence framework from OpenNMT (Klein et al., 2017) to Marian (Junczys-Dowmunt et al., 2018) and (ii) by providing the encoder with linguistic information (lemmas, semantic tags (Abzianidze and Bos, 2017), POS-tags, dependency parses and CCG supertags) that are encoded in a separate encoder. 6.3 Liu et al. (2019) L IU ET AL . also follow the approach of Van Noord et al. (2018) in terms of pre- and postprocessing the data, but they improve on it by using the Transformer model (Vaswani et al., 2017), instead of a sequence-to-sequence RNN. Also, they show that employing the bronze standard in addition to the gold and silver standard leads to improved performance. 6.4 Evang (2019) E VANG"
W19-1201,P17-4012,0,0.0353615,"feature for uppercase letters and not using character-level representation for DRS roles and operators. Moreover, they show that performance can be substantially improved by first pre-training on gold and silver data, after which the parser is fine-tuned on only the gold standard data. 6.2 Van Noord (2019) The system of N OORD ET AL .19 is the parser described in Van Noord et al. (2019) and Van Noord (2019), which follows up on their work previously described in Van Noord et al. (2018). They improve on this work in two ways: (i) by switching their sequence-to-sequence framework from OpenNMT (Klein et al., 2017) to Marian (Junczys-Dowmunt et al., 2018) and (ii) by providing the encoder with linguistic information (lemmas, semantic tags (Abzianidze and Bos, 2017), POS-tags, dependency parses and CCG supertags) that are encoded in a separate encoder. 6.3 Liu et al. (2019) L IU ET AL . also follow the approach of Van Noord et al. (2018) in terms of pre- and postprocessing the data, but they improve on it by using the Transformer model (Vaswani et al., 2017), instead of a sequence-to-sequence RNN. Also, they show that employing the bronze standard in addition to the gold and silver standard leads to impr"
W19-1201,C12-1094,0,0.0276593,"(negation, quantification and modals), pragmatic (presuppositions) and lexical (word senses and thematic roles) components of semantics in a single meaning representation, the DRS parsing task shares parts of the following NLP tasks: semantic role labeling, reference resolution, scope detection, named entity tagging, word sense disambiguation, predicate-argument structure prediction, and presupposition projection. There are only a few previous approaches to DRS parsing. Traditionally, due to the complexity of the task, it has been the domain of symbolic and statistical approaches (Bos, 2008; Le and Zuidema, 2012; Bos, 2015). Recently, however, neural sequence-to-sequence systems achieved impressive performance on the task (Liu et al., 2018; Van Noord et al., 2018), without relying on any external linguistic resources. 00/3008: He played the piano and she sang. SYSTEM INPUT : Tom isn’t afraid of anything. SYSTEM OUTPUT: b1 b1 b1 b2 b2 b2 b2 b3 b3 b3 b3 b3 b3 b3 REF x1 male &quot;n.02&quot; x1 Name x1 &quot;tom&quot; REF t1 EQU t1 &quot;now&quot; time &quot;n.08&quot; t1 NOT b3 REF s1 Time s1 t1 Experiencer s1 x1 afraid &quot;a.01&quot; s1 Stimulus s1 x2 REF x2 entity &quot;n.01&quot; x2 b6 b2 b2 b1 b1 b1 b1 b3 b3 b1 b1 b1 DRS b1 REF x1 male &quot;n.02&quot; x1 REF e1 pl"
W19-1201,W19-1203,0,0.0833277,"he gold standard data. 6.2 Van Noord (2019) The system of N OORD ET AL .19 is the parser described in Van Noord et al. (2019) and Van Noord (2019), which follows up on their work previously described in Van Noord et al. (2018). They improve on this work in two ways: (i) by switching their sequence-to-sequence framework from OpenNMT (Klein et al., 2017) to Marian (Junczys-Dowmunt et al., 2018) and (ii) by providing the encoder with linguistic information (lemmas, semantic tags (Abzianidze and Bos, 2017), POS-tags, dependency parses and CCG supertags) that are encoded in a separate encoder. 6.3 Liu et al. (2019) L IU ET AL . also follow the approach of Van Noord et al. (2018) in terms of pre- and postprocessing the data, but they improve on it by using the Transformer model (Vaswani et al., 2017), instead of a sequence-to-sequence RNN. Also, they show that employing the bronze standard in addition to the gold and silver standard leads to improved performance. 6.4 Evang (2019) E VANG aim to find a middle-ground between traditional symbolic approaches and the recent neural (sequence-to-sequence) models. They employ a transition-based parser that relies on explicit wordmeaning pairs that are found in th"
W19-1201,P18-1040,0,0.160907,"cs in a single meaning representation, the DRS parsing task shares parts of the following NLP tasks: semantic role labeling, reference resolution, scope detection, named entity tagging, word sense disambiguation, predicate-argument structure prediction, and presupposition projection. There are only a few previous approaches to DRS parsing. Traditionally, due to the complexity of the task, it has been the domain of symbolic and statistical approaches (Bos, 2008; Le and Zuidema, 2012; Bos, 2015). Recently, however, neural sequence-to-sequence systems achieved impressive performance on the task (Liu et al., 2018; Van Noord et al., 2018), without relying on any external linguistic resources. 00/3008: He played the piano and she sang. SYSTEM INPUT : Tom isn’t afraid of anything. SYSTEM OUTPUT: b1 b1 b1 b2 b2 b2 b2 b3 b3 b3 b3 b3 b3 b3 REF x1 male &quot;n.02&quot; x1 Name x1 &quot;tom&quot; REF t1 EQU t1 &quot;now&quot; time &quot;n.08&quot; t1 NOT b3 REF s1 Time s1 t1 Experiencer s1 x1 afraid &quot;a.01&quot; s1 Stimulus s1 x2 REF x2 entity &quot;n.01&quot; x2 b6 b2 b2 b1 b1 b1 b1 b3 b3 b1 b1 b1 DRS b1 REF x1 male &quot;n.02&quot; x1 REF e1 play &quot;v.03&quot; e1 Agent e1 x1 Theme e1 x2 REF x2 piano &quot;n.01&quot; x2 REF t1 time &quot;n.08&quot; t1 TPR t1 &quot;now&quot; x1 b2 male.n.02(x1 ) BOX FORMAT : x"
W19-1201,marelli-etal-2014-sick,0,0.0315567,"tances that were not released previously. They will not be released publicly, but are still available for (blind) scoring via the shared task website.5 However, during the evaluation phase, we asked the participants to provide DRSs for a set of 12,606 short texts. In addition to the raw texts (600) from the evaluation split, this set contained the train (4,597), development (682), 4 5 https://github.com/RikVN/DRS_parsing/tree/master/data/pmb-2.2.0 https://competitions.codalab.org/competitions/20220 and test (650) data from the PMB-2.2.0 release and the sentences (6,077) from the SICK dataset (Marelli et al., 2014). The reason for providing the inflated set of raw texts was three-fold: (i) Disguise the raw texts of the evaluation set to make it hard to tune models on them; (ii) Obtain the complete information about the performance of the systems on the provided training, development and test sets; (iii) Carry out extrinsic evaluation of the participant systems on the natural language inference task. 5 Evaluation Metrics and Baselines Before comparing a system produced clausal form to the gold one, the produced form is checked on validity—whether it represents a DRS. If the clausal form is invalid, it is"
W19-1201,S16-1166,0,0.0584845,", a high number of shared variables among clauses, and highly-constrained format of valid DRSs, all these makes the DRS parsing a challenging NLP task. The results of the shared task displayed improvements over the existing state-of-the-art parser. 1 Introduction Semantic parsing has been gaining in popularity in the last few years. There have been a series of shared tasks in semantic parsing organized, where each task requires to generate meaning representations of specific types: Broad-Coverage Broad-coverage Semantic Dependencies (Oepen et al., 2014, 2015), Abstract Meaning Representation (May, 2016; May and Priyadarshi, 2017), or Universal Conceptual Cognitive Annotation (Hershcovich et al., 2019). The Discourse Representation Structure (DRS) parsing task extends this development by aiming at producing meaning representations that (i) come with more expressive power than existing ones and (ii) are easily translatable into formal logic, thereby opening the door to applications that require automated forms of inference (Blackburn and Bos, 2005; Dagan et al., 2013). DRSs are meaning representations employed by Discourse Representation Theory (DRT, Kamp and Reyle, 1993). They have been succ"
W19-1201,S17-2090,0,0.0453703,"mber of shared variables among clauses, and highly-constrained format of valid DRSs, all these makes the DRS parsing a challenging NLP task. The results of the shared task displayed improvements over the existing state-of-the-art parser. 1 Introduction Semantic parsing has been gaining in popularity in the last few years. There have been a series of shared tasks in semantic parsing organized, where each task requires to generate meaning representations of specific types: Broad-Coverage Broad-coverage Semantic Dependencies (Oepen et al., 2014, 2015), Abstract Meaning Representation (May, 2016; May and Priyadarshi, 2017), or Universal Conceptual Cognitive Annotation (Hershcovich et al., 2019). The Discourse Representation Structure (DRS) parsing task extends this development by aiming at producing meaning representations that (i) come with more expressive power than existing ones and (ii) are easily translatable into formal logic, thereby opening the door to applications that require automated forms of inference (Blackburn and Bos, 2005; Dagan et al., 2013). DRSs are meaning representations employed by Discourse Representation Theory (DRT, Kamp and Reyle, 1993). They have been successfully applied for wide-co"
W19-1201,J17-4007,1,0.888277,"Missing"
W19-1201,S15-2153,0,0.33598,"Missing"
W19-1201,S14-2008,0,0.0999844,"account the rich lexical information, explicit scope marking, a high number of shared variables among clauses, and highly-constrained format of valid DRSs, all these makes the DRS parsing a challenging NLP task. The results of the shared task displayed improvements over the existing state-of-the-art parser. 1 Introduction Semantic parsing has been gaining in popularity in the last few years. There have been a series of shared tasks in semantic parsing organized, where each task requires to generate meaning representations of specific types: Broad-Coverage Broad-coverage Semantic Dependencies (Oepen et al., 2014, 2015), Abstract Meaning Representation (May, 2016; May and Priyadarshi, 2017), or Universal Conceptual Cognitive Annotation (Hershcovich et al., 2019). The Discourse Representation Structure (DRS) parsing task extends this development by aiming at producing meaning representations that (i) come with more expressive power than existing ones and (ii) are easily translatable into formal logic, thereby opening the door to applications that require automated forms of inference (Blackburn and Bos, 2005; Dagan et al., 2013). DRSs are meaning representations employed by Discourse Representation Theo"
W19-1201,D14-1162,0,0.0855607,"the comparison of clausal forms. To calculate whether two systems differ significantly, we perform approximate randomization Noreen (1989), with α = 0.05, R = 1000 and F (model1 ) &gt; F (model2 ) as test statistic for each individual DRS pair. 5.3 Baselines We provide three baseline parsers: SPAR, SIM - SPAR and AMR 2 DRS. SPAR simply outputs a default DRS, which is a DRS that is the most similar to the DRSs in our training set.8 SIM - SPAR outputs the DRS of the most similar sentence in the training set, based on the cosine distance of the average word-embedding vector, calculated using GloVe (Pennington et al., 2014). AMR 2 DRS is a script that converts the output of an AMR parser to a valid DRS by applying a set of rules, described in Bos (2016) and van Noord, Abzianidze, Haagsma, and Bos (2018). We will provide scores on the development, test and evaluation sets by using the AMR parser of van Noord and Bos (2017). 6 Participating Systems We received a total of five submissions in the shared task out of 32 registered participants. Three out of five submitted a system paper. The general characteristics of the participating systems are give in Table 2. Following Nissim et al. (2017), we explicitly encourag"
W19-1201,W19-1204,1,0.892243,"Missing"
W19-1201,L18-1267,1,0.542659,"Missing"
W19-1201,Q18-1043,1,0.77254,"Missing"
W19-1201,W19-0504,1,0.83102,"Missing"
W19-3302,W17-6901,1,0.931343,"ure. In case of the compositional semantics, this also leads to automatic construction of the phrasal semantics. This is because, in a lexicalised grammar, most of the grammar work is done in the lexicon (there is only a dozen general grammar rules), and annotation is just a matter of giving the right information to a word (rather than selecting the correct interpretation from a possibly large set of parse trees). In the PMB a lexicalised grammar is used: Combinatory Categorial Grammar (CCG, Steedman 2001), and the core annotation layers for each word token are a CCG category, a semantic tag (Abzianidze and Bos, 2017), a lemma, and a word sense. Annotating thematic roles (Section 18) is also convenient in a lexicalised grammar environment (Bos et al., 2012). Finally, a lexicalised grammar coupled with compositional semantics facilitates annotation projection for meaning preserving translations and opens the door to multilingual meaning banking (Section 29). Projection of meaning representation from one sentence to another is reduced to word alignment and word-level annotaInclude an issue reporting system Annotators will sooner or later raise issues, have questions about the annotation scheme, or find bugs"
W19-3302,W08-2227,0,0.090136,"Missing"
W19-3302,P98-1013,0,0.459793,"taly produces better wine than France”). This instance of systematic polysemy manifests itself for all classes of GPE, including continents, countries, states, provinces, cities, and so on. Detailed instructions for annotating GPEs can be found in the ACE annotation guidelines (Doddington et al., 2004). ment, Experiencer, Stimulus, Attribute, Value, Location, Destination, Source, Result, and Material. The PMB adopts the thematic roles of VerbNet. FrameNet is organised quite differently. Its starting point is not rooted in linguistics, but rather in real-world situations, classified as frames (Baker et al., 1998). Frames have frame elements that can be realised by linguistic expressions, and they correspond to the PropBank and VerbNet roles. There are more than a thousand different frames, and each frame has its own specific role set (frame elements). For instance, the Buy-Commerce frame has roles Buyer, Goods, Seller, Money, and so on. There are also recent proposals for comprehensive inventories for roles introduced by prepositional and possessive constructions (Schneider et al., 2018). In the PMB, we employ a unified inventory of thematic roles (an extension of the VerbNet roles) that is applicable"
W19-3302,Q14-1029,0,0.0121226,"ses where the gender of a person is not known. The disturbing distribution of male versus female pronouns (or titles) strongly suggests that a female is the least likely choice (Webster et al., 2018). But following this statistical suggestion only causes 18 entities to wikipedia pages, but obviously not every named entity has a wikipedia entry. To our knowledge, no other meaning banks apply wikification. Other interesting applications for symbol grounding are GPS coordinates for toponyms (Leidner, 2008), visualisation of concepts or actions (Navigli and Ponzetto, 2012), or creating timelines (Bamman and Smith, 2014). greater divide. The PMB annotation guidelines for choosing word senses (Secion 15) are such that when it is unclear what sense to pick, the higher sense (thus, the most frequent one), must be selected. This is bad, because systems for word sense disambiguation already show a tendency towards assigning the most frequent sense (Postma et al., 2016). More efforts are needed to reduce bias (Zhao et al., 2017). 17 15 Use existing resources for word senses It seems that in most (if not all) semantically annotated corpora a neo-Davidsonian event semantics is adopted. This means that every event int"
W19-3302,P13-1133,0,0.025502,"rom each other (Lopez de Lacalle and Agirre, 2015). Annotation guidelines are needed for ambiguous cases where syntax doesn’t help to disambiguate: “Swimmming is great fun.” (swimming.n.01 or perhaps swim.v.01?), “Her words were emphasized.” (emphasized.a.01 or emphasize.v.02?). WordNet’s coverage is impressive and substantial, but obviously not all words are listed (example: names of products used as nouns) and sometimes it is inconsistent (for instance, “apple juice” is in WordNet, but “cherry juice” is not). Many WordNets exists for languages other than English (Navigli and Ponzetto, 2012; Bond and Foster, 2013). 16 Adopt neo-Davidsonian events 18 Use existing role labelling inventories A neo-Davidsonian approach presupposes a dictionary of thematic (or semantic) role names. There are three popular sets available: PropBank, VerbNet, and FrameNet. PropBank (Palmer et al., 2005) proposes a set of just six summarising roles: ARG0 (Agent), ARG1 (Patient), ARG2 (Instrument, Benefactive, Attribute), ARG3 (Starting Point), ARG4 (Ending Point), ARGM (Modifier). The interpretation of these roles are in many cases specific to the event in which they participate. The AMR Bank adopts these PropBank roles (Banare"
W19-3302,W13-2322,0,0.370897,"s. The GMB only includes corpora from the public domain (Basile et al., 2012b). Free parallel corpora are also available via OPUS (Skadin¸sˇ et al., 2014). Other researchers take advantage of vague legislation and Look at other meaning banks Other semantic annotation projects can be inspiring, help you to find solutions to hard annotation problems, or to find out where improvements to the state of the art are still needed (Abend and Rappoport, 2017). Good starting points are the English Resource Grammar (Flickinger, 2000, 2011), the Groningen Meaning Bank (GMB, Bos et al. 2017), the AMR Bank (Banarescu et al., 2013), the Parallel Meaning Bank (PMB, Abzianidze et al. 2017), Scope Control Theory (Butler and Yoshimoto, 2012), UCCA (Abend and Rappoport, 2013), Prague Semantic Dependencies 15 Proceedings of the First International Workshop on Designing Meaning Representations, pages 15–27 c Florence, Italy, August 1st, 2019 2019 Association for Computational Linguistics distribute corpora quoting the right of fair use (Postma et al., 2018). Recently, crowd sourcing platforms such as Figure Eight make datasets available, too (“Data For Everyone”), under appropriate licensing. While targeting the public domain"
W19-3302,W15-1832,1,0.84076,"d modal concord, postnominal genitives (“of John’s”), odd punctuation conventions, idioms), and that grammars with high recall and precision are costly to produce (the impressive English Resource Grammar took about several years to develop, but it is restricted to just one language). 7 improvements made in a certain time span (useful for the documentation in data releases). 9 Following the idea of Phrase Detectives (Chamberlain et al., 2008), in the GMB (Bos et al., 2017) a game with a purpose (GWAP) was introduced to annotate parts of speech, antecedents of pronouns, noun compound relations (Bos and Nissim, 2015), and word senses (Venhuizen et al., 2013). The quality of annotations harvested from gamification was generally high, but the amount of annotations relatively low—it would literally take years to annotate the entire GMB corpus. An additional problem with GWAPs is recruiting new players: most players play the game only once, and attempts to make the game addictive could be irresponsible (Andrade F.R.H., 2016). The alternative, engaging people by financially awarding them via crowdsourcing platforms such as Mechanical Turk or Figure Eight, solves the quantity problem (Pustejovsky and Stubbs, 20"
W19-3302,W15-0128,0,0.0198434,"tator pick the correct or most plausible analysis. Similarly, the meaning representations in the GMB are system-produced and partially hand-corrected (Bos et al., 2017), using a CCG parser (Clark and Curran, 2004). Likewise, the meaning representations in the PMB are system-produced with the help of a CCG parser (Lewis and Steedman, 2014) and some of it is completely hand-corrected. In contrast, the meaning representations of the AMR Bank are completely manually manufactured—without the aid of a grammar—with the help of an annotation interface and an extensive manual (Banarescu et al., 2013). Bender et al. (2015) argue that grammarbased meaning banking requires less annotation guidelines, that it provides more consistent anal1 Freezing the corpora already fixes certain data statements for your meaning bank, like curation rationale, language variety, and text characteristics. Communicating these data statements is important from an application point of view (Bender and Friedman, 2018). 16 yses, and that it is more scalable. The downside of grammar-based annotation is that several compound expressions are not always compositional (negative and modal concord, postnominal genitives (“of John’s”), odd punc"
W19-3302,Q18-1041,0,0.0128259,"nd-corrected. In contrast, the meaning representations of the AMR Bank are completely manually manufactured—without the aid of a grammar—with the help of an annotation interface and an extensive manual (Banarescu et al., 2013). Bender et al. (2015) argue that grammarbased meaning banking requires less annotation guidelines, that it provides more consistent anal1 Freezing the corpora already fixes certain data statements for your meaning bank, like curation rationale, language variety, and text characteristics. Communicating these data statements is important from an application point of view (Bender and Friedman, 2018). 16 yses, and that it is more scalable. The downside of grammar-based annotation is that several compound expressions are not always compositional (negative and modal concord, postnominal genitives (“of John’s”), odd punctuation conventions, idioms), and that grammars with high recall and precision are costly to produce (the impressive English Resource Grammar took about several years to develop, but it is restricted to just one language). 7 improvements made in a certain time span (useful for the documentation in data releases). 9 Following the idea of Phrase Detectives (Chamberlain et al.,"
W19-3302,P13-2131,0,0.05621,"the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 77–89, Vancouver, Canada. Association for Computational Linguistics. Measure meaning discrepancies A large part of the users of semantically annotated corpora are from the semantic parsing area, and they need to be able to measure and quantify their output with respect to gold standard meanings. The currently accepted methods are based on precision and recall on the components of the meaning representation by converting them to triples or clauses (Allen et al., 2008; Dridan and Oepen, 2011; Cai and Knight, 2013; Van Noord et al., 2018; Kim and Schubert, 2019). In a parallel corpus setting, such evaluation measures can also be used to compare the meaning representation of a source text and its translation (Saphra and Lopez, 2015). This is done in the PMB, where a nonperfect meaning match between source and target helps the annotator to identify possible culprits. It is important to note that most of these matching techniques check for syntactic equivalence, and don’t take semantic equivalence into account—the same meaning could be expressed by syntactically different representations. The approach by"
W19-3302,C16-1056,1,0.840022,"into account at the same time the result is more likely to be more language-neutral meaning representations. And that’s what meanings should be, they are abstract objects, independent of the language used to expressed them. Of course, there are concepts that can be expressed in certain languages with a single word that other languages are not capable of, but the core of meaning representations should be agnostic to the source language. A good starting point is to work with typologically-related languages. An efficient annotation technique to cover multiple languages is annotation projection (Evang and Bos, 2016; Liu et al., 2018). This requires a parallel corpus and automatic word alignment, and existing semantic annotations for at least one language. Acknowledgments 30 Lasha Abzianidze, Johannes Bjerva, Kilian Evang, Hessel Haagsma, Rik van Noord, Pierre Ludmann, Duc-Duy Nguyen, and Johan Bos. 2017. The parallel meaning bank: Towards a multilingual corpus of translations annotated with compositional meaning representations. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, pages 242–247, Valencia, Spain. We would like to thank the two an"
W19-3302,L18-1202,0,0.0272042,"Missing"
W19-3302,W08-2230,0,0.0104409,"and Friedman, 2018). 16 yses, and that it is more scalable. The downside of grammar-based annotation is that several compound expressions are not always compositional (negative and modal concord, postnominal genitives (“of John’s”), odd punctuation conventions, idioms), and that grammars with high recall and precision are costly to produce (the impressive English Resource Grammar took about several years to develop, but it is restricted to just one language). 7 improvements made in a certain time span (useful for the documentation in data releases). 9 Following the idea of Phrase Detectives (Chamberlain et al., 2008), in the GMB (Bos et al., 2017) a game with a purpose (GWAP) was introduced to annotate parts of speech, antecedents of pronouns, noun compound relations (Bos and Nissim, 2015), and word senses (Venhuizen et al., 2013). The quality of annotations harvested from gamification was generally high, but the amount of annotations relatively low—it would literally take years to annotate the entire GMB corpus. An additional problem with GWAPs is recruiting new players: most players play the game only once, and attempts to make the game addictive could be irresponsible (Andrade F.R.H., 2016). The altern"
W19-3302,P04-1014,0,0.0582907,"ork-based” company could be a new 6 Consider manual annotation Several meaning banks are created with the help of a grammar. The best example here is the sophisticated English Resource Grammar (Flickinger, 2000, 2011) used to produce the treebanks, Redwoods (Oepen et al., 2004) and DeepBank (Flickinger et al., 2012), annotated with English Resource Semantics (ERS) in a compositional way, by letting the annotator pick the correct or most plausible analysis. Similarly, the meaning representations in the GMB are system-produced and partially hand-corrected (Bos et al., 2017), using a CCG parser (Clark and Curran, 2004). Likewise, the meaning representations in the PMB are system-produced with the help of a CCG parser (Lewis and Steedman, 2014) and some of it is completely hand-corrected. In contrast, the meaning representations of the AMR Bank are completely manually manufactured—without the aid of a grammar—with the help of an annotation interface and an extensive manual (Banarescu et al., 2013). Bender et al. (2015) argue that grammarbased meaning banking requires less annotation guidelines, that it provides more consistent anal1 Freezing the corpora already fixes certain data statements for your meaning"
W19-3302,J11-2010,0,0.0244896,"gamification was generally high, but the amount of annotations relatively low—it would literally take years to annotate the entire GMB corpus. An additional problem with GWAPs is recruiting new players: most players play the game only once, and attempts to make the game addictive could be irresponsible (Andrade F.R.H., 2016). The alternative, engaging people by financially awarding them via crowdsourcing platforms such as Mechanical Turk or Figure Eight, solves the quantity problem (Pustejovsky and Stubbs, 2012), but introduces other issues including the question what a proper wage would be (Fort et al., 2011) and dealing with tricksters and cheaters (Buchholz and Latorre, 2011). Make a friendly annotation interface Annotation can be fun (especially if gamification is applied, see Section 9), but it can also be tedious. A good interface helps the annotator to make high-quality annotations, to work efficiently, and to be able to focus on particular linguistic phenomena. An annotation interface should be webbased (i.e., any browser should support it), simple to use, and personalised.2 The latter grants control over annotations of particular users. The “Explorer” (Basile et al., 2012a) introduced in t"
W19-3302,P15-1123,0,0.0342074,"Missing"
W19-3302,doddington-etal-2004-automatic,0,0.0243721,"ined inventory of relations. Princeton WordNet (Fellbaum, 1998) lists several instances of famous persons but obviously the list is incomplete. The AMR Bank includes links from named 19 ments, sport squads that represent them, or the people that live there (and in some case to multiple aspects at the same time, as in “Italy produces better wine than France”). This instance of systematic polysemy manifests itself for all classes of GPE, including continents, countries, states, provinces, cities, and so on. Detailed instructions for annotating GPEs can be found in the ACE annotation guidelines (Doddington et al., 2004). ment, Experiencer, Stimulus, Attribute, Value, Location, Destination, Source, Result, and Material. The PMB adopts the thematic roles of VerbNet. FrameNet is organised quite differently. Its starting point is not rooted in linguistics, but rather in real-world situations, classified as frames (Baker et al., 1998). Frames have frame elements that can be realised by linguistic expressions, and they correspond to the PropBank and VerbNet roles. There are more than a thousand different frames, and each frame has its own specific role set (frame elements). For instance, the Buy-Commerce frame has"
W19-3302,M91-1015,0,0.2564,"Kamp and Reyle, 1993) as a way to add modifiers (e.g., moving from eat(x,y) to eat(e,x,y) for a transitive use of to eat). In most modern meaning representations thematic roles are introduced to reduce the number of arguments of verbal predicates to one, also known as the neo-Davidsonian tradition (Parsons, 1990) (e.g., moving from eat(e,x,y) to eat(e) AGENT(e,x) PATIENT(e,y)). A direct consequence of a neo-Davidsonion design is the need for an inventory of thematic roles. But there is also an alternative, which is given a fixed arity to event predicates, of which some of them may be unused (Hobbs, 1991) when the context does not provide this information (e.g., for the intransive usage of to eat, still maintain eat(e,x,y) where y is left unspecified). The predicate symbols that one finds in meaning representation are usually based on word lemmas. But words have no interpretation, and a link to concepts in an existing ontology (Lenat, 1995; Navigli and Ponzetto, 2012) is something that is needed to make the non-logical symbols in meaning representations interpretable. In the AMR Bank, verbs are disambiguated by OntoNotes senses (Banarescu et al., 2013). In the PMB, nouns, verbs, adjectives and"
W19-3302,W18-4912,0,0.0927595,"es an ontological distinction between entities denoting individuals and entities denoting concepts (kinds). A further question is how tense should be annotated in habitual sentences, as in “Jane used to swim every day” (in some period in the past, Jane swam every day) or “Jane swims every day” (in the current period, Jane swims every day). To our knowledge, none of the existing meaning banks have a satisfactory treatment of generics, even though techniques have been proposed to detect generics (Reiter and Frank, 2010; Friedrich and Pinkal, 2015). Recent proposals try to change this situation (Donatelli et al., 2018). the PMB, following Bos (2003). Yet there are other classes of triggers that are notoriously hard to represent, because they require some ”copying” of large pieces of meaning representation, interact with focus, and require non-trivial semantic composition methods. To these belong implicative verbs (manage), focusing adverbs (only, just), and repetition particles (again, still, yet, another). For instance, although in the PMB a sentence like “The crowd applauded again.” is the presupposition trigger, “again” is semantically tagged as a repetition trigger, for now it doesn’t perform any costly"
W19-3302,P10-2013,0,0.0224763,"ds (Section 4–11), and design of meaning representations (Section 12– 30). We hope this overview will give inspiration and guidance in creating improved meaning banks in the future. 1 2 Select public domain corpora Any text could be protected by copyright law and it is not always easy to find suitable corpora that are free from copyright issues. Indeed, the relationship between copyright of texts and their use in natural language processing is complex (Eckart de Castilho et al., 2018). Nonetheless, it pays off to make some effort by searching for corpora that are free or in the public domain (Ide et al., 2010). This makes it easier for other researchers to work with it, in particular those that are employed by institutes with lesser financial means. The GMB only includes corpora from the public domain (Basile et al., 2012b). Free parallel corpora are also available via OPUS (Skadin¸sˇ et al., 2014). Other researchers take advantage of vague legislation and Look at other meaning banks Other semantic annotation projects can be inspiring, help you to find solutions to hard annotation problems, or to find out where improvements to the state of the art are still needed (Abend and Rappoport, 2017). Good"
W19-3302,ide-romary-2006-representing,0,0.0491184,"a For Everyone”), under appropriate licensing. While targeting the public domain corpora, one might need to bear in mind the coverage of the corpora depending on the objectives of semantic annotation. 3 company based in York, but the other interpretation is more likely. In an NLP-processing pipeline, it is too late for syntax to fix this in a compositional way—the tokenisation needs to be improved. 5 Stand-off annotation is a no-brainer as it offers a lot more flexibility. It enables keeping annotations separate from the original raw text, where ideally each annotation layer has its own file (Ide and Romary, 2006; Pustejovsky and Stubbs, 2012). It is best executed with respect to the character offsets of the raw texts in the corpus (Section 4). A JSON or XML-based annotation file can always be generated from this, should the demand be there. Stand-off annotation is in particular advantageous in a setting where several layers of annotation interact with each other (typically in a pipeline architecture). This was extremely helpful in the GMB (Bos et al., 2017) where the document segmentation (sentence and word boundaries) got improved several times during the project, without having any negative effect"
W19-3302,W11-2927,0,0.0212568,"tion. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 77–89, Vancouver, Canada. Association for Computational Linguistics. Measure meaning discrepancies A large part of the users of semantically annotated corpora are from the semantic parsing area, and they need to be able to measure and quantify their output with respect to gold standard meanings. The currently accepted methods are based on precision and recall on the components of the meaning representation by converting them to triples or clauses (Allen et al., 2008; Dridan and Oepen, 2011; Cai and Knight, 2013; Van Noord et al., 2018; Kim and Schubert, 2019). In a parallel corpus setting, such evaluation measures can also be used to compare the meaning representation of a source text and its translation (Saphra and Lopez, 2015). This is done in the PMB, where a nonperfect meaning match between source and target helps the annotator to identify possible culprits. It is important to note that most of these matching techniques check for syntactic equivalence, and don’t take semantic equivalence into account—the same meaning could be expressed by syntactically different representat"
W19-3302,D13-1146,1,0.475654,"ne annotator is implausible for another. Finally, one needs to be careful, as annotation guidelines that give preference for one particular reading (based on statistical plausbility) have the danger of introducing or even amplifying bias. 13 Try to use language-neutral tools Whenever possible, in machine-assisted annotation, get language technology components that are not tailored to specific languages, because this increases portability of meaning processing components to other languages (Section 29). The statistical tokeniser (for word and sentence segmentation) used in the PMB is Elephant (Evang et al., 2013). The current efforts in multi-lingual POS-tagging, semantic tagging (Abzianidze and Bos, 2017) and dependency parsing are promising (White et al., 2016). In the PMB a categorial grammar is used to cover four languages (English, Dutch, German, and Italian), using the same parser and grammar, but with language-specific statistical models trained for the EasyCCG parser (Lewis and Steedman, 2014). Related are grammatical frameworks designed for parallel grammar writing (Ranta, 2011; Bender et al., 2010). 12 Apply normalisation to symbols Normalising the format of non-logical symbols (the predicat"
W19-3302,L16-1630,0,0.0590878,"Missing"
W19-3302,W19-0402,0,0.0682603,"bzianidze Center for Language and Cognition University of Groningen l.abzianidze@rug.nl Abstract (Hajiˇc et al., 2017) and the ULF Corpus based on Episodic Logic (Kim and Schubert, 2019). The largest differences between these approaches can be found in the expressive power of the meaning representations used. The simplest representations correspond to graphs (Banarescu et al., 2013; Abend and Rappoport, 2013); slightly more expressive ones correspond to first-order logic (Oepen et al., 2016; Bos et al., 2017; Abzianidze et al., 2017; Butler and Yoshimoto, 2012), whereas others go beyond this (Kim and Schubert, 2019). Generally, an increase of expressive power causes a decrease of efficient reasoning (Blackburn and Bos, 2005). Semantic formalisms based on graphs are attractive because of their simplicity, but will face issues when dealing with negation in inference tasks (Section 21). The choice might depend on the application (e.g., if you are not interested in detecting contradictions, coping with negation is less important), but arguably, an open-domain meaning bank ought to be independent of a specific application. Meaning banking—creating a semantically annotated corpus for the purpose of semantic pa"
W19-3302,J05-1004,0,0.0448896,"v.02?). WordNet’s coverage is impressive and substantial, but obviously not all words are listed (example: names of products used as nouns) and sometimes it is inconsistent (for instance, “apple juice” is in WordNet, but “cherry juice” is not). Many WordNets exists for languages other than English (Navigli and Ponzetto, 2012; Bond and Foster, 2013). 16 Adopt neo-Davidsonian events 18 Use existing role labelling inventories A neo-Davidsonian approach presupposes a dictionary of thematic (or semantic) role names. There are three popular sets available: PropBank, VerbNet, and FrameNet. PropBank (Palmer et al., 2005) proposes a set of just six summarising roles: ARG0 (Agent), ARG1 (Patient), ARG2 (Instrument, Benefactive, Attribute), ARG3 (Starting Point), ARG4 (Ending Point), ARGM (Modifier). The interpretation of these roles are in many cases specific to the event in which they participate. The AMR Bank adopts these PropBank roles (Banarescu et al., 2013). VerbNet has a set of about 25 thematic roles independently defined from the verb classes (Kipper et al., 2008). A few examples are: Agent, Patient, Theme, InstruApply symbol grounding Symbol grounding helps to connect abstract representations of meani"
W19-3302,E17-2080,0,0.0494792,"Missing"
W19-3302,S18-1009,0,0.0250119,"Missing"
W19-3302,W15-0114,0,0.0275,"Missing"
W19-3302,L16-1268,0,0.0124095,"knowledge, no other meaning banks apply wikification. Other interesting applications for symbol grounding are GPS coordinates for toponyms (Leidner, 2008), visualisation of concepts or actions (Navigli and Ponzetto, 2012), or creating timelines (Bamman and Smith, 2014). greater divide. The PMB annotation guidelines for choosing word senses (Secion 15) are such that when it is unclear what sense to pick, the higher sense (thus, the most frequent one), must be selected. This is bad, because systems for word sense disambiguation already show a tendency towards assigning the most frequent sense (Postma et al., 2016). More efforts are needed to reduce bias (Zhao et al., 2017). 17 15 Use existing resources for word senses It seems that in most (if not all) semantically annotated corpora a neo-Davidsonian event semantics is adopted. This means that every event introduces its own entity as a variable, and this variable can be used to connect the event to its thematic roles. In the original Davidsonian approach, an event variable was simply added to the predicate introduced by the verb (Davidson, 1967; Kamp and Reyle, 1993) as a way to add modifiers (e.g., moving from eat(x,y) to eat(e,x,y) for a transitive u"
W19-3302,P10-1005,0,0.0345089,"her put constraints on the context. The question, then, is what to do with them in a meaning banking project. Some classes of presupposition triggers, referring expressions including proper names, possessive phrases, and definite descriptions, can be treated in a similar way as pronouns, as is done in the GMB and 21 dog has four legs” is not about a particular lion or dog, nor is it about all dogs or lions. The inventor of “the typewriter” was not the inventor of a particular typewriter, but of the typewriter concept in general. Such generic concepts are also known as kinds in the literature (Reiter and Frank, 2010). It is not impossible to approximate this in first-order logic, but it requires an ontological distinction between entities denoting individuals and entities denoting concepts (kinds). A further question is how tense should be annotated in habitual sentences, as in “Jane used to swim every day” (in some period in the past, Jane swam every day) or “Jane swims every day” (in the current period, Jane swims every day). To our knowledge, none of the existing meaning banks have a satisfactory treatment of generics, even though techniques have been proposed to detect generics (Reiter and Frank, 2010"
W19-3302,D14-1107,0,0.070082,"The best example here is the sophisticated English Resource Grammar (Flickinger, 2000, 2011) used to produce the treebanks, Redwoods (Oepen et al., 2004) and DeepBank (Flickinger et al., 2012), annotated with English Resource Semantics (ERS) in a compositional way, by letting the annotator pick the correct or most plausible analysis. Similarly, the meaning representations in the GMB are system-produced and partially hand-corrected (Bos et al., 2017), using a CCG parser (Clark and Curran, 2004). Likewise, the meaning representations in the PMB are system-produced with the help of a CCG parser (Lewis and Steedman, 2014) and some of it is completely hand-corrected. In contrast, the meaning representations of the AMR Bank are completely manually manufactured—without the aid of a grammar—with the help of an annotation interface and an extensive manual (Banarescu et al., 2013). Bender et al. (2015) argue that grammarbased meaning banking requires less annotation guidelines, that it provides more consistent anal1 Freezing the corpora already fixes certain data statements for your meaning bank, like curation rationale, language variety, and text characteristics. Communicating these data statements is important fro"
W19-3302,L18-1547,0,0.0909492,"not be modelled by simply applying it as a property of an entity. It should be clear—explicit or implicit—what the scope of any negation operator is, i.e. the parts of the meaning representation that are negated. The GMB, PMB and DeepBank (Flickinger et al., 2012) assign proper scope to negation (the latter with the help of underspecification). In AMR Bank negation is modelled with the help of a relation, and this doesn’t always get the required interpretation (Bos, 2016). Negation can be tricky: negation affixes (Section 23) require special care, negative concord (Section 6) and neg raising (Liu et al., 2018) are challenges for compositional approaches to meaning construction. Treat agent nouns differently Agent and recipient nouns (nouns that denote persons performing or receiving some action, such as employee, victim, teacher, mother, cyclist, victim) are intrinsically relational (Booij, 1986). Modelling them like ordinary nouns, i.e., as one-place predicates, can give rise to contradictions for any individual that has been assigned more than one role, because while you want to be able to state that a violin player is not the same thing as a mother, a person could perfectly be a mother and a vio"
W19-3302,E95-1001,0,0.609665,"PMB. 2012). 11 Underspecification is a technique with the aim to free the semantic interpretation component from a disambiguation burden (Reyle, 1993; Bos, 1996; Copestake et al., 2005). In syntactic treebanks, however, the driving force has been to assign the most plausible parse tree to a sentence. This makes sense for the task of statistical (syntactic) parsing. The same applies to (statistical) semantic parsing: a corpus with the most likely interpretation for sentences is required. Moreover, it is not straightforward to draw correct inferences with underspecified meaning representations (Reyle, 1995). So it makes sense, at least from the perspective of semantic annotation, to produce the most plausible interpretation for a given sentence. Consider the following examples. A “sleeping bag” could be a bag that is asleep, but it is very unlikely (even in a Harry Potter setting), so should be annotated as a bag designed to be slept in. In the sentence “Tom kissed his mother”, the possessive pronoun could refer to a third party, but by far the most likely interpretation is that Tom’s mother is kissed by Tom, and that reading should be reflected in the annotation. Genuine scope ambiguities are r"
W19-3302,N15-3008,0,0.0490672,"Missing"
W19-3302,P18-1018,0,0.0132884,"ifferently. Its starting point is not rooted in linguistics, but rather in real-world situations, classified as frames (Baker et al., 1998). Frames have frame elements that can be realised by linguistic expressions, and they correspond to the PropBank and VerbNet roles. There are more than a thousand different frames, and each frame has its own specific role set (frame elements). For instance, the Buy-Commerce frame has roles Buyer, Goods, Seller, Money, and so on. There are also recent proposals for comprehensive inventories for roles introduced by prepositional and possessive constructions (Schneider et al., 2018). In the PMB, we employ a unified inventory of thematic roles (an extension of the VerbNet roles) that is applicable to verbs, adjectives, prepositions, possessives or noun modifiers. 19 21 Sentence meaning is about assigning truth conditions to propositions (Section 23). Negation plays a crucial role here—in fact, the core of semantics is about negation, identifying whether a statement is true of false. Negation is a semantic phenomenon that requires scope, in other words, it cannot be modelled by simply applying it as a property of an entity. It should be clear—explicit or implicit—what the"
W19-3302,skadins-etal-2014-billions,0,0.0514457,"Missing"
W19-3302,L18-1267,1,0.887091,"Missing"
W19-3302,W13-0215,1,0.816665,"“of John’s”), odd punctuation conventions, idioms), and that grammars with high recall and precision are costly to produce (the impressive English Resource Grammar took about several years to develop, but it is restricted to just one language). 7 improvements made in a certain time span (useful for the documentation in data releases). 9 Following the idea of Phrase Detectives (Chamberlain et al., 2008), in the GMB (Bos et al., 2017) a game with a purpose (GWAP) was introduced to annotate parts of speech, antecedents of pronouns, noun compound relations (Bos and Nissim, 2015), and word senses (Venhuizen et al., 2013). The quality of annotations harvested from gamification was generally high, but the amount of annotations relatively low—it would literally take years to annotate the entire GMB corpus. An additional problem with GWAPs is recruiting new players: most players play the game only once, and attempts to make the game addictive could be irresponsible (Andrade F.R.H., 2016). The alternative, engaging people by financially awarding them via crowdsourcing platforms such as Mechanical Turk or Figure Eight, solves the quantity problem (Pustejovsky and Stubbs, 2012), but introduces other issues including"
W19-3302,Q18-1042,0,0.0196752,"tween annotated corpora, it is a good idea to check whether any standards are proposed for normalisation (Pustejovsky and Stubbs, 14 Limit underspecification Beware of annotation bias Assigning the most likely interpretation to a sentence can also give an unfair balance to stereotypes. In the PMB, gender of personal proper names are annotated. In many cases this is a straightforward exercise. But there are sometimes cases where the gender of a person is not known. The disturbing distribution of male versus female pronouns (or titles) strongly suggests that a female is the least likely choice (Webster et al., 2018). But following this statistical suggestion only causes 18 entities to wikipedia pages, but obviously not every named entity has a wikipedia entry. To our knowledge, no other meaning banks apply wikification. Other interesting applications for symbol grounding are GPS coordinates for toponyms (Leidner, 2008), visualisation of concepts or actions (Navigli and Ponzetto, 2012), or creating timelines (Bamman and Smith, 2014). greater divide. The PMB annotation guidelines for choosing word senses (Secion 15) are such that when it is unclear what sense to pick, the higher sense (thus, the most frequ"
W19-3302,D16-1177,0,0.061586,"Missing"
W19-3302,D17-1323,0,0.0118109,"nteresting applications for symbol grounding are GPS coordinates for toponyms (Leidner, 2008), visualisation of concepts or actions (Navigli and Ponzetto, 2012), or creating timelines (Bamman and Smith, 2014). greater divide. The PMB annotation guidelines for choosing word senses (Secion 15) are such that when it is unclear what sense to pick, the higher sense (thus, the most frequent one), must be selected. This is bad, because systems for word sense disambiguation already show a tendency towards assigning the most frequent sense (Postma et al., 2016). More efforts are needed to reduce bias (Zhao et al., 2017). 17 15 Use existing resources for word senses It seems that in most (if not all) semantically annotated corpora a neo-Davidsonian event semantics is adopted. This means that every event introduces its own entity as a variable, and this variable can be used to connect the event to its thematic roles. In the original Davidsonian approach, an event variable was simply added to the predicate introduced by the verb (Davidson, 1967; Kamp and Reyle, 1993) as a way to add modifiers (e.g., moving from eat(x,y) to eat(e,x,y) for a transitive use of to eat). In most modern meaning representations themat"
W19-4005,E17-2039,1,0.906826,"Missing"
W19-4005,D15-1198,0,0.0189754,"rts have been made for Turkish (C ¸ akıcı, 2005), German (Hockenmaier, 2006), English (Hockenmaier and Steedman, 2007), Italian (Bos et al., 2009), Chinese (Tse and Curran, 2010), Arabic (Boxwell and Brew, 2010), Japanese (Uematsu et al., 2013), and Hindi (Ambati et al., 2018). However, all of these treebanks were not directly annotated according to the CCG formalism, but automatically converted from phrase structure or dependency treebanks, which is an error-prone process. Direct annotation in CCG has so far mostly been limited to small datasets for seeding or testing semantic parsers (e.g., Artzi et al., 2015), and no graphical annotation interface is available to support such efforts, making the annotation process difficult to scale. The only exceptions we are aware of are the Groningen Meaning Bank (Bos et al., 2017) and the Parallel Meaning Bank (Abzianidze et al., 2017), two annotation efforts which use a graphical user interface for annotating sentences with CCG derivations and other annotation layers, and which have produced CCG Johan Bos University of Groningen The Netherlands johan.bos@rug.nl treebanks for English, German, Italian, and Dutch. However, these efforts are focused on semantics"
W19-4005,J99-2004,0,0.276761,"as to change, viz. from S[b] NP to (S[b] NP)/ PP. To do this, the annotator clicks on the category and changes it, as shown in the figure. When they hit enter or click somewhere else, the sentence is automatically parsed again in the background, this time with the lexical category constraint that go has category (S[b] NP)/ PP. In many cases, the parser will directly find the desired parse, with there being a PP, and the annotator only has to check it, not make another edit. Span Constraints Although constraining lexical categories is often enough to determine the entire CCG derivation (cf. Bangalore and Joshi, 1999; Lewis and Steedman, 2014), this is not always the case. For example, consider the sentence I want to be a millionaire like my dad. Assuming that like my dad is a verb phrase modifier (category (S  NP)(S  NP)), it could attach to either to be or want, giving very different meanings (cf. Zimmer, 2013). We therefore implemented one other type of edit operation/constraint: span constraints. By simply clicking and dragging across a span of tokens as shown in Figure 2, annotators can constrain this span to be a constituent in the resulting parse. Figure 3: The judge user sees all annotators’ ve"
W19-4005,boxwell-brew-2010-pilot,0,0.0289167,"e guidelines, with 4x100 adjudicated sentences, 10K singleannotator fully corrected sentences, and 82K single-annotator partially corrected sentences. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman, 2000) is a grammar formalism distinguished by its transparent syntax-semantics interface and its elegant handling of coordination. It is a popular tool in semantic parsing, and treebank creation efforts have been made for Turkish (C ¸ akıcı, 2005), German (Hockenmaier, 2006), English (Hockenmaier and Steedman, 2007), Italian (Bos et al., 2009), Chinese (Tse and Curran, 2010), Arabic (Boxwell and Brew, 2010), Japanese (Uematsu et al., 2013), and Hindi (Ambati et al., 2018). However, all of these treebanks were not directly annotated according to the CCG formalism, but automatically converted from phrase structure or dependency treebanks, which is an error-prone process. Direct annotation in CCG has so far mostly been limited to small datasets for seeding or testing semantic parsers (e.g., Artzi et al., 2015), and no graphical annotation interface is available to support such efforts, making the annotation process difficult to scale. The only exceptions we are aware of are the Groningen Meaning Ba"
W19-4005,P06-1064,0,0.0558389,"ion with CCG, for four languages: English, German, Italian, and Dutch. We also release a parallel pilot CCG treebank based on these guidelines, with 4x100 adjudicated sentences, 10K singleannotator fully corrected sentences, and 82K single-annotator partially corrected sentences. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman, 2000) is a grammar formalism distinguished by its transparent syntax-semantics interface and its elegant handling of coordination. It is a popular tool in semantic parsing, and treebank creation efforts have been made for Turkish (C ¸ akıcı, 2005), German (Hockenmaier, 2006), English (Hockenmaier and Steedman, 2007), Italian (Bos et al., 2009), Chinese (Tse and Curran, 2010), Arabic (Boxwell and Brew, 2010), Japanese (Uematsu et al., 2013), and Hindi (Ambati et al., 2018). However, all of these treebanks were not directly annotated according to the CCG formalism, but automatically converted from phrase structure or dependency treebanks, which is an error-prone process. Direct annotation in CCG has so far mostly been limited to small datasets for seeding or testing semantic parsers (e.g., Artzi et al., 2015), and no graphical annotation interface is available to s"
W19-4005,J07-3004,0,0.331564,"ges: English, German, Italian, and Dutch. We also release a parallel pilot CCG treebank based on these guidelines, with 4x100 adjudicated sentences, 10K singleannotator fully corrected sentences, and 82K single-annotator partially corrected sentences. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman, 2000) is a grammar formalism distinguished by its transparent syntax-semantics interface and its elegant handling of coordination. It is a popular tool in semantic parsing, and treebank creation efforts have been made for Turkish (C ¸ akıcı, 2005), German (Hockenmaier, 2006), English (Hockenmaier and Steedman, 2007), Italian (Bos et al., 2009), Chinese (Tse and Curran, 2010), Arabic (Boxwell and Brew, 2010), Japanese (Uematsu et al., 2013), and Hindi (Ambati et al., 2018). However, all of these treebanks were not directly annotated according to the CCG formalism, but automatically converted from phrase structure or dependency treebanks, which is an error-prone process. Direct annotation in CCG has so far mostly been limited to small datasets for seeding or testing semantic parsers (e.g., Artzi et al., 2015), and no graphical annotation interface is available to support such efforts, making the annotation"
W19-4005,P10-1022,1,0.747432,"other annotators in a tabbed interface as shown in Figure 3. In order to enable the judge to easily spot disagreements, categories that annotators disagree on are struck through, and constituents that annotators disagree on are dashed. 3 A Quadrilingual Pilot CCG Treebank To test the viability of creating multilingual CCG treebanks by direct annotation, we conducted an annotation experiment on 110 short sentences from the Tatoeba corpus (Tatoeba, 2019), each in four translations (English, German, Italian, and Dutch). The main annotation guideline was to copy the annotation style of CCGrebank (Honnibal et al., 2010), a CCG treebank adapted from CCGbank (Hockenmaier and Steedman, 2007), which is in turn based on the Penn Treebank (Marcus et al., 1993). Since CCGrebank only covers English and lacks some constructions observed in our corpus, an annotation manual with more specific instructions was needed. We initially annotated ten sentences in four languages and discussed disagreements. The results were recorded in an initial annotation manual, and the initial annotations were discarded. Each of the remaining 4x100 sentences was then annotated independently by at least two of the authors. Table 1 (upper pa"
W19-4005,D14-1107,0,0.1639,"nce takes the annotator to the sentence view. Annotators can also enter arbitrary sentences to annotate, e.g., for experimenting or for producing illustrations. Dynamic Annotation Annotation follows an approach called dynamic annotation (Oepen et al., 2002) or human-aided machine annotation (Bos et al., 2017), in which sentences are automatically analyzed, annotators impose constraints to rule out undesired analyses, sentences are then reanalyzed subject to the constraints, and the process is repeated until only the desired analysis remains. The current system is backed by the EasyCCG parser (Lewis and Steedman, 2014), slightly modified to allow for incorporating constraints, and other CCG parsers could be plugged in with similar modifications. What You See Is What You Get Derivations are rendered in the same graphical format that is used in the literature, representing nodes as horizontal lines placed underneath their children. Annotators directly interact with this graphical representation when annotating, following the WYSIWYG (what you see is what you get) principle. Lexical Category Constraints As an example of editing, consider Figure 1. Suppose that the parser has analyzed there as an adjunct with c"
W19-4005,J93-2004,0,0.0644069,"otators disagree on are struck through, and constituents that annotators disagree on are dashed. 3 A Quadrilingual Pilot CCG Treebank To test the viability of creating multilingual CCG treebanks by direct annotation, we conducted an annotation experiment on 110 short sentences from the Tatoeba corpus (Tatoeba, 2019), each in four translations (English, German, Italian, and Dutch). The main annotation guideline was to copy the annotation style of CCGrebank (Honnibal et al., 2010), a CCG treebank adapted from CCGbank (Hockenmaier and Steedman, 2007), which is in turn based on the Penn Treebank (Marcus et al., 1993). Since CCGrebank only covers English and lacks some constructions observed in our corpus, an annotation manual with more specific instructions was needed. We initially annotated ten sentences in four languages and discussed disagreements. The results were recorded in an initial annotation manual, and the initial annotations were discarded. Each of the remaining 4x100 sentences was then annotated independently by at least two of the authors. Table 1 (upper part) shows the number of nonoverlapping category and span constraints that each annotator created on average per sentence before marking t"
W19-4005,C02-2025,0,0.17741,"t Linux dis1 https://github.com/texttheater/ccgweb Figure 1: Correcting a lexical category. Figure 2: Correcting attachments by selecting a span that need to form a constituent. tribution. It has two main views: the home page shows the list of sentences an annotator is assigned to annotate. Those already done are marked as “marked correct”. Clicking on a sentence takes the annotator to the sentence view. Annotators can also enter arbitrary sentences to annotate, e.g., for experimenting or for producing illustrations. Dynamic Annotation Annotation follows an approach called dynamic annotation (Oepen et al., 2002) or human-aided machine annotation (Bos et al., 2017), in which sentences are automatically analyzed, annotators impose constraints to rule out undesired analyses, sentences are then reanalyzed subject to the constraints, and the process is repeated until only the desired analysis remains. The current system is backed by the EasyCCG parser (Lewis and Steedman, 2014), slightly modified to allow for incorporating constraints, and other CCG parsers could be plugged in with similar modifications. What You See Is What You Get Derivations are rendered in the same graphical format that is used in the"
W19-4005,C10-1122,0,0.0268717,"ilot CCG treebank based on these guidelines, with 4x100 adjudicated sentences, 10K singleannotator fully corrected sentences, and 82K single-annotator partially corrected sentences. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman, 2000) is a grammar formalism distinguished by its transparent syntax-semantics interface and its elegant handling of coordination. It is a popular tool in semantic parsing, and treebank creation efforts have been made for Turkish (C ¸ akıcı, 2005), German (Hockenmaier, 2006), English (Hockenmaier and Steedman, 2007), Italian (Bos et al., 2009), Chinese (Tse and Curran, 2010), Arabic (Boxwell and Brew, 2010), Japanese (Uematsu et al., 2013), and Hindi (Ambati et al., 2018). However, all of these treebanks were not directly annotated according to the CCG formalism, but automatically converted from phrase structure or dependency treebanks, which is an error-prone process. Direct annotation in CCG has so far mostly been limited to small datasets for seeding or testing semantic parsers (e.g., Artzi et al., 2015), and no graphical annotation interface is available to support such efforts, making the annotation process difficult to scale. The only exceptions we are awar"
W19-4005,P13-1103,0,0.0173407,"d sentences, 10K singleannotator fully corrected sentences, and 82K single-annotator partially corrected sentences. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman, 2000) is a grammar formalism distinguished by its transparent syntax-semantics interface and its elegant handling of coordination. It is a popular tool in semantic parsing, and treebank creation efforts have been made for Turkish (C ¸ akıcı, 2005), German (Hockenmaier, 2006), English (Hockenmaier and Steedman, 2007), Italian (Bos et al., 2009), Chinese (Tse and Curran, 2010), Arabic (Boxwell and Brew, 2010), Japanese (Uematsu et al., 2013), and Hindi (Ambati et al., 2018). However, all of these treebanks were not directly annotated according to the CCG formalism, but automatically converted from phrase structure or dependency treebanks, which is an error-prone process. Direct annotation in CCG has so far mostly been limited to small datasets for seeding or testing semantic parsers (e.g., Artzi et al., 2015), and no graphical annotation interface is available to support such efforts, making the annotation process difficult to scale. The only exceptions we are aware of are the Groningen Meaning Bank (Bos et al., 2017) and the Par"
W19-4005,P05-2013,0,0.162952,"Missing"
W19-4804,E17-2039,1,0.838305,"Missing"
W19-4804,D15-1075,0,0.650059,"lity in upward and downward reasoning. 1 Introduction Natural language inference (NLI), also known as recognizing textual entailment (RTE), has been proposed as a benchmark task for natural language understanding. Given a premise P and a hypothesis H, the task is to determine whether the premise semantically entails the hypothesis (Dagan et al., 2013). A number of recent works attempt to test and analyze what type of inferences an NLI model may be performing, focusing on various types of lexical inferences (Glockner et al., 2018; Naik et al., 2018; Poliak et al., 2018) and logical inferences (Bowman et al., 2015b; Evans et al., 2018). Concerning logical inferences, monotonicity reasoning (van Benthem, 1983; Icard and Moss, 2014), which is a type of reasoning based on word replacement, requires the ability to capture the interaction between lexical and syntactic structures. Consider examples in (1) and (2). (1) a. All [ workers ↓] [joined for a French dinner ↑] b. All workers joined for a dinner c. All new workers joined for a French dinner (2) a. Not all [new workers ↑] joined for a dinner b. Not all workers joined for a dinner A context is upward entailing (shown by [... ↑]) that allows an inference"
W19-4804,W15-4002,0,0.598488,"lity in upward and downward reasoning. 1 Introduction Natural language inference (NLI), also known as recognizing textual entailment (RTE), has been proposed as a benchmark task for natural language understanding. Given a premise P and a hypothesis H, the task is to determine whether the premise semantically entails the hypothesis (Dagan et al., 2013). A number of recent works attempt to test and analyze what type of inferences an NLI model may be performing, focusing on various types of lexical inferences (Glockner et al., 2018; Naik et al., 2018; Poliak et al., 2018) and logical inferences (Bowman et al., 2015b; Evans et al., 2018). Concerning logical inferences, monotonicity reasoning (van Benthem, 1983; Icard and Moss, 2014), which is a type of reasoning based on word replacement, requires the ability to capture the interaction between lexical and syntactic structures. Consider examples in (1) and (2). (1) a. All [ workers ↓] [joined for a French dinner ↑] b. All workers joined for a dinner c. All new workers joined for a French dinner (2) a. Not all [new workers ↑] joined for a dinner b. Not all workers joined for a dinner A context is upward entailing (shown by [... ↑]) that allows an inference"
W19-4804,P18-1224,0,0.0386824,"Missing"
W19-4804,P17-1152,0,0.0677033,"Missing"
W19-4804,N19-1423,0,0.0446964,"any, ever, at all, anything, anyone, anymore, anyhow, anywhere) in the hypothesis 4 Results and Discussion 4.1 Baselines To test the difficulty of our dataset, we checked the majority class label and the accuracies of five state-of-the-art NLI models adopting different approaches: BiMPM (Bilateral Multi-Perspective Matching Model; Wang et al., 2017), ESIM (Enhanced Sequential Inference Model; Chen et al., 2017), Decomposable Attention Model (Parikh et al., 2016), KIM (Knowledge-based Inference Model; Chen et al., 2018), and BERT (Bidirectional Encoder Representations from Transformers model; Devlin et al., 2019). Regarding BERT, we checked the performance of a model pretrained on Wikipedia and BookCorpus for language modeling and trained with SNLI and MultiNLI. For other models, we checked the performance trained with SNLI. In agreement with our dataset, we regarded the prediction label contradiction as non-entailment. Table 6 shows that the accuracies of all models were better on upward inferences, in accordance with the reported results of the GLUE leaderboard. The overall accuracy of each model was low. In particular, all models underperformed the majority baseline on downward inferences, despite"
W19-4804,P18-2103,0,0.0768747,"gmentation method showed that these models might be limited in their generalization ability in upward and downward reasoning. 1 Introduction Natural language inference (NLI), also known as recognizing textual entailment (RTE), has been proposed as a benchmark task for natural language understanding. Given a premise P and a hypothesis H, the task is to determine whether the premise semantically entails the hypothesis (Dagan et al., 2013). A number of recent works attempt to test and analyze what type of inferences an NLI model may be performing, focusing on various types of lexical inferences (Glockner et al., 2018; Naik et al., 2018; Poliak et al., 2018) and logical inferences (Bowman et al., 2015b; Evans et al., 2018). Concerning logical inferences, monotonicity reasoning (van Benthem, 1983; Icard and Moss, 2014), which is a type of reasoning based on word replacement, requires the ability to capture the interaction between lexical and syntactic structures. Consider examples in (1) and (2). (1) a. All [ workers ↓] [joined for a French dinner ↑] b. All workers joined for a dinner c. All new workers joined for a French dinner (2) a. Not all [new workers ↑] joined for a dinner b. Not all workers joined f"
W19-4804,N18-2017,0,0.0596308,"Missing"
W19-4804,W18-5446,0,0.0755493,"Missing"
W19-4804,N18-1101,0,0.578916,"xamples by crowdsourcing and well-designed ones from linguistics publications. To enable the evaluation of 1 The dataset will be made publicly available at https://github.com/verypluming/MED. skills required for monotonicity reasoning, we annotate each example in our dataset with linguistic tags associated with monotonicity reasoning. We measure the performance of state-of-the-art NLI models on monotonicity reasoning and investigate their generalization ability in upward and downward reasoning (Section 4). The results show that all models trained with SNLI (Bowman et al., 2015b) and MultiNLI (Williams et al., 2018) perform worse on downward inferences than on upward inferences. In addition, we analyzed the performance of models trained with an automatically created monotonicity dataset, HELP (Yanaka et al., 2019). The analysis with monotonicity data augmentation shows that models tend to perform better in the same direction of monotonicity with the training set, while they perform worse in the opposite direction. This indicates that the accuracy on monotonicity reasoning depends solely on the majority direction in the training set, and models might lack the ability to capture the structural relations be"
W19-4804,S18-2015,0,0.053669,"ght for both tasks. 3.1.1 Premise collection As a resource, we use declarative sentences with more than five tokens from the Parallel Meaning Bank (PMB) (Abzianidze et al., 2017). The PMB contains syntactically correct sentences annotated with its syntactic category in Combinatory Categorial Grammar (CCG; Steedman, 2000) format, which is suitable for our purpose. To get a whole CCG derivation tree, we parse each sentence by the state-of-the-art CCG parser, depccg (Yoshikawa et al., 2017). Then, we add a polarity to every constituent of the CCG tree by the polarity computation system ccg2mono (Hu and Moss, 2018) and make the polarized part a blank field. We ran a trial rephrasing task on 500 examples and detected 17 expressions that were too general and thus difficult to rephrase them in a natural way (e.g., every one, no time). We removed examples involving such expressions. To collect more downward inference examples, we select examples involving determiners in Table 1 and downward operators in Table 2. As a result, we selected 1,485 examples involving expressions having arguments with upward monotonicity and 1,982 examples involving expressions having arguments with downward monotonicity. 3.1.2 Hy"
W19-4804,J16-4012,0,0.0701837,"oriented dataset We also collect monotonicity inference problems from previous manually curated datasets and linguistics publications. The motivation is that previous linguistics publications related to monotonicity reasoning are expected to contain welldesigned inference problems, which might be challenging problems for NLI models. We collected 1,184 examples from 11 linguistics publications (Barwise and Cooper, 1981; Hoeksema, 1986; Heim and Kratzer, 1998; Bonevac et al., 1999; Fyodorov et al., 2003; Geurts, 2003; Geurts and van der Slik, 2005; Zamansky et al., 2006; Szabolcsi et al., 2008; Winter, 2016; Denic et al., 2019). Regarding previous manually-curated datasets, we collected 93 examples for monotonicity reasoning from the GLUE diagnostic dataset, and 37 single-premise problems from FraCaS. Genre Crowd Paper Tags up up: cond up:rev: conj up:lex down:lex down:conj down:cond down up:rev up:disj up:lex: rev non down Premise There is a cat on the chair If you heard her speak English, you would take her for a native American Dogs and cats have all the good qualities of people without at the same time possessing their weaknesses He approached the boy reading a magazine Tom hardly ever liste"
W19-4804,2014.lilt-9.7,0,0.235188,"tual entailment (RTE), has been proposed as a benchmark task for natural language understanding. Given a premise P and a hypothesis H, the task is to determine whether the premise semantically entails the hypothesis (Dagan et al., 2013). A number of recent works attempt to test and analyze what type of inferences an NLI model may be performing, focusing on various types of lexical inferences (Glockner et al., 2018; Naik et al., 2018; Poliak et al., 2018) and logical inferences (Bowman et al., 2015b; Evans et al., 2018). Concerning logical inferences, monotonicity reasoning (van Benthem, 1983; Icard and Moss, 2014), which is a type of reasoning based on word replacement, requires the ability to capture the interaction between lexical and syntactic structures. Consider examples in (1) and (2). (1) a. All [ workers ↓] [joined for a French dinner ↑] b. All workers joined for a dinner c. All new workers joined for a French dinner (2) a. Not all [new workers ↑] joined for a dinner b. Not all workers joined for a dinner A context is upward entailing (shown by [... ↑]) that allows an inference from (1a) to (1b), where French dinner is replaced by a more general concept dinner. On the other hand, a downward ent"
W19-4804,S19-1027,1,0.650772,"red for monotonicity reasoning, we annotate each example in our dataset with linguistic tags associated with monotonicity reasoning. We measure the performance of state-of-the-art NLI models on monotonicity reasoning and investigate their generalization ability in upward and downward reasoning (Section 4). The results show that all models trained with SNLI (Bowman et al., 2015b) and MultiNLI (Williams et al., 2018) perform worse on downward inferences than on upward inferences. In addition, we analyzed the performance of models trained with an automatically created monotonicity dataset, HELP (Yanaka et al., 2019). The analysis with monotonicity data augmentation shows that models tend to perform better in the same direction of monotonicity with the training set, while they perform worse in the opposite direction. This indicates that the accuracy on monotonicity reasoning depends solely on the majority direction in the training set, and models might lack the ability to capture the structural relations between monotonicity operators and their arguments. 2 Monotonicity As an example of a monotonicity inference, consider the example with the determiner every in (3); here the premise P entails the hypothes"
W19-4804,J88-2003,0,0.468338,"Missing"
W19-4804,C18-1198,0,0.039856,"d that these models might be limited in their generalization ability in upward and downward reasoning. 1 Introduction Natural language inference (NLI), also known as recognizing textual entailment (RTE), has been proposed as a benchmark task for natural language understanding. Given a premise P and a hypothesis H, the task is to determine whether the premise semantically entails the hypothesis (Dagan et al., 2013). A number of recent works attempt to test and analyze what type of inferences an NLI model may be performing, focusing on various types of lexical inferences (Glockner et al., 2018; Naik et al., 2018; Poliak et al., 2018) and logical inferences (Bowman et al., 2015b; Evans et al., 2018). Concerning logical inferences, monotonicity reasoning (van Benthem, 1983; Icard and Moss, 2014), which is a type of reasoning based on word replacement, requires the ability to capture the interaction between lexical and syntactic structures. Consider examples in (1) and (2). (1) a. All [ workers ↓] [joined for a French dinner ↑] b. All workers joined for a dinner c. All new workers joined for a French dinner (2) a. Not all [new workers ↑] joined for a dinner b. Not all workers joined for a dinner A conte"
W19-4804,D16-1244,0,0.0952364,"Missing"
W19-4804,S18-2023,0,0.0661357,"Missing"
W19-4804,L18-1239,0,0.100411,"Missing"
W19-4804,P17-1026,0,0.0754829,"rated in (i). Figure 1 summarizes the overview of our human-oriented dataset creation. We used the crowdsourcing platform Figure Eight for both tasks. 3.1.1 Premise collection As a resource, we use declarative sentences with more than five tokens from the Parallel Meaning Bank (PMB) (Abzianidze et al., 2017). The PMB contains syntactically correct sentences annotated with its syntactic category in Combinatory Categorial Grammar (CCG; Steedman, 2000) format, which is suitable for our purpose. To get a whole CCG derivation tree, we parse each sentence by the state-of-the-art CCG parser, depccg (Yoshikawa et al., 2017). Then, we add a polarity to every constituent of the CCG tree by the polarity computation system ccg2mono (Hu and Moss, 2018) and make the polarized part a blank field. We ran a trial rephrasing task on 500 examples and detected 17 expressions that were too general and thus difficult to rephrase them in a natural way (e.g., every one, no time). We removed examples involving such expressions. To collect more downward inference examples, we select examples involving determiners in Table 1 and downward operators in Table 2. As a result, we selected 1,485 examples involving expressions having arg"
