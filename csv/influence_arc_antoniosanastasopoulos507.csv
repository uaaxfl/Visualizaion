2020.acl-main.149,N18-2085,1,0.921528,"of the candidate translation our model actually produces, e.g. under approximate decoding. However, an information-theoretic evaluation 1641 is much more suitable for measuring the more abstract notion of which language pairs are hardest to translate to and from, which is our purpose here. 3 Disentangling Translation Difficulty and Monolingual Complexity We contend that simply reporting cross-entropies is not enough. A second issue in performing a controlled, cross-lingual MT comparison is that the language generation component (without translation) is not equally difficult across languages (Cotterell et al., 2018). We claim that the difficulty of translation corresponds more closely to the mutual information MI(S; T ) between the source and target language, which tells us how much easier it becomes to predict T when S is given (see Figure 1). But what is the appropriate analogue of mutual information for cross-entropy? One such natural generalization is a novel quantity that we term cross-mutual information, defined as: XMI(S → T ) = HqLM (T ) − HqMT (T |S) (3) where HqLM (T ) denotes the cross-entropy of the target sentence T under the model qLM . As in §2, this can, analogously, be approximated by th"
2020.acl-main.149,D18-1312,0,0.025153,"Missing"
2020.acl-main.149,D10-1092,0,0.0433027,"rich language is harder than translating into a morphologically impoverished one. In fact, the only significant correlate of MT difficulty we find is source-side type–token ratio. 2 Cross-Linguistic Comparability through Likelihoods, not BLEU Human evaluation will always be the gold standard of MT evaluation. However, it is both timeconsuming and expensive to perform. To help researchers and practitioners quickly deploy and evaluate new systems, automatic metrics that correlate fairly well with human evaluations have been proposed over the years (Banerjee and Lavie, 2005; Snover et al., 2006; Isozaki et al., 2010; Lo, 2019). BLEU (Papineni et al., 2002), however, has remained the most common metric to report the performance of MT systems. BLEU is a precisionbased metric: a BLEU score is proportional to the geometric average of the number of n-grams in the candidate translation that also appear in the reference translation for 1 ≤ n ≤ 4.1 In the context of our study, we take issue with two shortcomings of BLEU scores that prevent a cross-linguistically comparable study. First, it is not possible to directly compare BLEU scores across languages because different languages might express the same meaning"
2020.acl-main.149,W04-3250,0,0.537126,"Missing"
2020.acl-main.149,2005.mtsummit-papers.11,0,0.165253,"tely estimate the difficulty of translation for a given architecture in a controlled way. In summary, by looking at XMI, we can effectively decouple the language generation component, whose difficulties have been investigated by Cotterell et al. 2018 and Mielke et al. 2019, from the translation component. This gives us a measure of how rich and useful the information extracted from the source language is for the target-language generation component. 4 Experiments In order to measure which pairs of languages are harder to translate to and from, we make use of the latest release v7 of Europarl (Koehn, 2005): a corpus of the proceedings of the European Parliament containing parallel sentences between English (en) and 20 other European languages: Bulgarian (bg), Czech (cs), Danish (da), German (de), Greek (el), Spanish (es), Estonian (et), Finnish (fi), French (fr), Hungarian (hu), Italian (it), Lithuanian (lt), Latvian (lv), Dutch (nl), Polish (pl), Portuguese (pt), Romanian (ro), Slovak (sk), Slovene (sl) and Swedish (sv). Pre-processing steps In order to precisely effect a fully controlled experiment, we enforce a fair comparison by selecting the set of parallel sentences available across all 2"
2020.acl-main.149,P09-5002,0,0.0271803,"but the relation between H(S) and H(T ) can be arbitrary. Right: estimating cross-entropies using models qMT and qLM invalidates relations between bars, except that Hq· (·) ≥ H(·). XMI, our proposed metric, is no longer purely a symmetric measure of language, but now an asymmetric measure that mostly highlights models’ shortcomings. Introduction Machine translation (MT) is one of the core research areas in natural language processing. Current state-of-the-art MT systems are based on neural networks (Sutskever et al., 2014; Bahdanau et al., 2015), which generally surpass phrase-based systems (Koehn, 2009) in a variety of domains and languages (Bentivogli et al., 2016; Toral and S´anchez-Cartagena, 2017; Castilho et al., 2017; Bojar et al., 2018; Barrault et al., 2019). Using phrase-based MT systems, various controlled studies to understand where the translation difficulties lie for different language pairs were conducted (Birch et al., 2008; Koehn et al., 2009). However, comparable studies have yet to be performed for neural machine translation (NMT). As a result, it is still unclear whether all translation directions are equally easy (or hard) to model for NMT. This paper hence aims at fillin"
2020.acl-main.149,2009.mtsummit-papers.7,0,0.0241981,"translation (MT) is one of the core research areas in natural language processing. Current state-of-the-art MT systems are based on neural networks (Sutskever et al., 2014; Bahdanau et al., 2015), which generally surpass phrase-based systems (Koehn, 2009) in a variety of domains and languages (Bentivogli et al., 2016; Toral and S´anchez-Cartagena, 2017; Castilho et al., 2017; Bojar et al., 2018; Barrault et al., 2019). Using phrase-based MT systems, various controlled studies to understand where the translation difficulties lie for different language pairs were conducted (Birch et al., 2008; Koehn et al., 2009). However, comparable studies have yet to be performed for neural machine translation (NMT). As a result, it is still unclear whether all translation directions are equally easy (or hard) to model for NMT. This paper hence aims at filling this gap: Ceteris paribus, is it easier to translate from English into Finnish or into Hungarian? And how much easier is it? Conversely, is it equally hard to translate Finnish and Hungarian into another language? Based on BLEU (Papineni et al., 2002) scores, previous work (Belinkov et al., 2017) suggests that translating into morphologically rich languages,"
2020.acl-main.149,W18-1819,0,0.0425158,"Missing"
2020.acl-main.149,P02-1040,0,\N,Missing
2020.acl-main.149,W05-0909,0,\N,Missing
2020.acl-main.149,D08-1078,0,\N,Missing
2020.acl-main.149,W17-0230,0,\N,Missing
2020.acl-main.149,E17-2002,0,\N,Missing
2020.acl-main.149,W18-6401,0,\N,Missing
2020.acl-main.149,P16-1162,0,\N,Missing
2020.acl-main.149,W19-5358,0,\N,Missing
2020.acl-main.149,2020.acl-main.615,1,\N,Missing
2020.acl-main.764,D08-1078,0,0.368952,"thread is to predict a method’s performance as a function of the training dataset size. Our work belongs in the second thread, but could easily be extended to encompass training time/procedure. In the first thread, Kolachina et al. (2012b) attempt to infer learning curves based on training data features and extrapolate the initial learning curves based on BLEU measurements for statistical machine translation (SMT). By extrapolating the performance of initial learning curves, the predictions on the remainder allows for early termination of a bad run (Domhan et al., 2015). In the second thread, Birch et al. (2008) adopt linear regression to capture the relationship between data features and SMT performance and find that the amount of reordering, the morphological complexity of the target language and the relatedness of the two languages explains the majority of performance variability. More recently, Elsahar and Gallé (2019) use domain shift metrics such as H-divergence based metrics to predict drop in performance under domain-shift. Rosenfeld et al. 8632 RMSE 8 6 4 HIT-SCIR (78.86) UDPipe (76.07) 3 0 1 2 3 4 5 RMSE Phoenix (68.17) 6 5 0 1 2 3 4 5 8 7 6 5 4.5 4 5 4.5 4 4 ICS (75.98) LATTICE (76.07) 0 1"
2020.acl-main.764,D18-1024,0,0.0333568,"Missing"
2020.acl-main.764,D19-1222,0,0.0302189,"rapolate the initial learning curves based on BLEU measurements for statistical machine translation (SMT). By extrapolating the performance of initial learning curves, the predictions on the remainder allows for early termination of a bad run (Domhan et al., 2015). In the second thread, Birch et al. (2008) adopt linear regression to capture the relationship between data features and SMT performance and find that the amount of reordering, the morphological complexity of the target language and the relatedness of the two languages explains the majority of performance variability. More recently, Elsahar and Gallé (2019) use domain shift metrics such as H-divergence based metrics to predict drop in performance under domain-shift. Rosenfeld et al. 8632 RMSE 8 6 4 HIT-SCIR (78.86) UDPipe (76.07) 3 0 1 2 3 4 5 RMSE Phoenix (68.17) 6 5 0 1 2 3 4 5 8 7 6 5 4.5 4 5 4.5 4 4 ICS (75.98) LATTICE (76.07) 0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5 BOUN (66.69) CUNI (66.6) ONLP (61.92) 8 7 6 0 1 2 3 4 5 12 10 0 1 2 3 4 5 0 1 2 3 4 5 Figure 3: RMSE scores of UD task from dataset-wise mean value predictor (the dashed black line in each graph) and predictors trained with experimental records of other models and 0–5 records from a"
2020.acl-main.764,P02-1040,0,\N,Missing
2020.acl-main.764,P12-1003,0,\N,Missing
2020.acl-main.764,D16-1250,0,\N,Missing
2020.acl-main.764,E17-2002,0,\N,Missing
2020.acl-main.764,P17-1042,0,\N,Missing
2020.acl-main.764,D17-1207,0,\N,Missing
2020.acl-main.764,N18-2084,1,\N,Missing
2020.acl-main.764,D18-1268,1,\N,Missing
2020.acl-main.764,Q18-1041,0,\N,Missing
2020.acl-main.764,N19-1188,0,\N,Missing
2020.acl-main.764,P19-1399,0,\N,Missing
2020.acl-main.764,P19-1494,0,\N,Missing
2020.acl-main.764,P19-1308,0,\N,Missing
2020.acl-main.764,K19-1087,0,\N,Missing
2020.acl-main.766,D18-1399,0,0.0338075,"Missing"
2020.acl-main.766,D18-1024,0,0.0146756,"all mappings could be learned jointly, taking advantage of the inter-dependencies between any two language pairs. Importantly, though, there is no closed form solution for learning the joint mapping, hence a solution needs to be approximated with gradientbased methods. The main approaches are: Bilingual Word Embeddings In the supervised BWE setting of Mikolov et al. (2013), given two languages L = {l1 , l2 } and their pre-trained row-aligned embeddings X1 , X2 , respectively, a transformation matrix M is learned such that: • Multilingual adversarial training with pseudorandomized refinement (Chen and Cardie, 2018, MAT+MPSR): a generalization of the adversarial approach of Zhang et al. (2017); Conneau et al. (2018) to multiple languages, also combined with an iterative refinement procedure.4 M = arg min kX1 − M X2 k . M ∈Ω The set Ω can potentially impose a constraint over M , such as the very popular constraint of restricting it to be orthogonal (Xing et al., 2015). Previous work has empirically found that this simple formulation is competitive with other more complicated alternatives (Xing et al., 2015). The orthogonality assumption ensures that there exists a closed-form solution through Singular •"
2020.acl-main.766,P19-1070,0,0.0174121,"to accuracy; we provide P@5 and P@10 results in the Appendix. 3 New LI Evaluation Dictionaries The typically used evaluation dictionaries cover a narrow breadth of the possible language pairs, with the majority of them focusing in pairs with English (as with the MUSE or Dinu et al. (2015) dictionaries) or among high-resource European languages. Glavaš et al. (2019), for instance, highlighted Anglocentricity as an issue, creating and evaluating on 28 dictionaries between 8 languages (Croatian, English, Finnish, French, German, Italian, Russian, Turkish) based on Google Translate. In addition, Czarnowska et al. (2019) focused on the morphology dimension, creating morphologically complete dictionaries for 2 sets of 5 genetically related languages (Romance: French, Spanish, Italian, Portuguese, Catalan; and Slavic: Polish, Czech, Slovak, Russian, Ukrainian). In contrast to these two (very valuable!) works, our method for creating dictionaries 5 Note that Alaux et al. (2019) use the term pivot to refer to what we refer to as the hub language. Pt: En: Cs: prácu trabalho job work praca práca práce pracovní Figure 1: Transitivity example (Portuguese → English → Czech). for low-resource languages (§3.1) leverages"
2020.acl-main.766,L18-1550,0,0.0815163,"rt-Of-Speech (POS) tagging (Zhang et al., 2016), parsing (Ammar et al., 2016a), document classification (Klementiev et al., 2012), and machine translation (Irvine and Callison-Burch, 2013; Artetxe et al., 2018b; Lample et al., 2018). Often, such shared representations are learned with a two-step process, whether under bilingual or multilingual settings (hereinafter BWE and MWE, respectively). First, monolingual word embeddings are learned over 1 Available embeddings. at https://github.com/antonisa/ large swaths of text. Such pre-trained word embeddings, such as the fastText Wikipedia vectors (Grave et al., 2018), are available for many languages and are widely used. Second, a mapping between the languages is learned in one of three ways: in a supervised manner if dictionaries or parallel data are available to be used for supervision (Zou et al., 2013), under minimal supervision e.g. using only identical strings (Smith et al., 2017), or even in an unsupervised fashion (Zhang et al., 2017; Conneau et al., 2018). Both in bilingual and multilingual settings, it is common that one of the language embedding spaces is the target to which all other languages get aligned (hereinafter “the hub&quot;). We outline th"
2020.acl-main.766,N19-1188,0,0.0375994,"Missing"
2020.acl-main.766,W13-2233,0,0.0133612,"pairs that do not include English. 1 Introduction Continuous vectors for representing words (embeddings) (Turian et al., 2010) have become ubiquitous in modern, neural NLP. Cross-lingual representations (Mikolov et al., 2013) additionally represent words from various languages in a shared continuous space, which in turn can be used for Bilingual Lexicon Induction (BLI). BLI is often the first step towards several downstream tasks such as Part-Of-Speech (POS) tagging (Zhang et al., 2016), parsing (Ammar et al., 2016a), document classification (Klementiev et al., 2012), and machine translation (Irvine and Callison-Burch, 2013; Artetxe et al., 2018b; Lample et al., 2018). Often, such shared representations are learned with a two-step process, whether under bilingual or multilingual settings (hereinafter BWE and MWE, respectively). First, monolingual word embeddings are learned over 1 Available embeddings. at https://github.com/antonisa/ large swaths of text. Such pre-trained word embeddings, such as the fastText Wikipedia vectors (Grave et al., 2018), are available for many languages and are widely used. Second, a mapping between the languages is learned in one of three ways: in a supervised manner if dictionaries"
2020.acl-main.766,D19-1328,0,0.0182401,"duction performance (measured with P@1) over 10 languages (90 pairs). In each cell, the superscript denotes the hub language that yields the best result for that language pair. µbest : average using the best hub language. µEn : average using the En as the hub. The lightly shaded cells are the language pairs where a bilingual VecMap system outperforms MAT+MSPR; in heavy shaded cells both MUSEs and VecMap outperform MAT+MSPR. on automated annotations in order to scale to all languages. Our method that uses automatically obtained morphological information combined with the guidelines proposed by Kementchedjhieva et al. (2019) (e.g. removing proper nouns from the evaluation set) scales easily to multiple languages, allowing us to create more than 4 thousand dictionaries. 4 experiments, we use MUSEs14 (MUSE, semisupervised) and VecMap15 systems, and we additionally compare them to MAT+MPSR for completeness. We compare the statistical significance of the performance difference of two systems using paired bootstrap resampling (Koehn, 2004). Generally, a difference of 0.4–0.5 percentage points evaluated over our lexica is significant with p &lt; 0.05. Lexicon Induction Experiments The aim of our LI experiments is two-fold"
2020.acl-main.766,C12-1089,0,0.0314942,"ngual embedding baselines, that extend to language pairs that do not include English. 1 Introduction Continuous vectors for representing words (embeddings) (Turian et al., 2010) have become ubiquitous in modern, neural NLP. Cross-lingual representations (Mikolov et al., 2013) additionally represent words from various languages in a shared continuous space, which in turn can be used for Bilingual Lexicon Induction (BLI). BLI is often the first step towards several downstream tasks such as Part-Of-Speech (POS) tagging (Zhang et al., 2016), parsing (Ammar et al., 2016a), document classification (Klementiev et al., 2012), and machine translation (Irvine and Callison-Burch, 2013; Artetxe et al., 2018b; Lample et al., 2018). Often, such shared representations are learned with a two-step process, whether under bilingual or multilingual settings (hereinafter BWE and MWE, respectively). First, monolingual word embeddings are learned over 1 Available embeddings. at https://github.com/antonisa/ large swaths of text. Such pre-trained word embeddings, such as the fastText Wikipedia vectors (Grave et al., 2018), are available for many languages and are widely used. Second, a mapping between the languages is learned in"
2020.acl-main.766,W04-3250,0,0.178645,"ated annotations in order to scale to all languages. Our method that uses automatically obtained morphological information combined with the guidelines proposed by Kementchedjhieva et al. (2019) (e.g. removing proper nouns from the evaluation set) scales easily to multiple languages, allowing us to create more than 4 thousand dictionaries. 4 experiments, we use MUSEs14 (MUSE, semisupervised) and VecMap15 systems, and we additionally compare them to MAT+MPSR for completeness. We compare the statistical significance of the performance difference of two systems using paired bootstrap resampling (Koehn, 2004). Generally, a difference of 0.4–0.5 percentage points evaluated over our lexica is significant with p &lt; 0.05. Lexicon Induction Experiments The aim of our LI experiments is two-fold. First, the differences in LI performance show the importance of the hub language choice with respect to each evaluation pair. Second, as part of our call for moving beyond Anglo-centric evaluation, we also present LI results on several new language pairs using our triangulated dictionaries. 4.1 Methods and Setup We train and evaluate all models starting with pretrained Wikipedia FastText embeddings for all langua"
2020.acl-main.766,P09-5002,0,0.0332527,"Missing"
2020.acl-main.766,P10-1040,0,0.0868333,"h expand a standard Englishcentered evaluation dictionary collection to include all language pairs using triangulation, and create new dictionaries for under-represented languages.1 Evaluating established methods over all these language pairs sheds light into their suitability for aligning embeddings from distant languages and presents new challenges for the field. Finally, in our analysis we identify general guidelines for strong cross-lingual embedding baselines, that extend to language pairs that do not include English. 1 Introduction Continuous vectors for representing words (embeddings) (Turian et al., 2010) have become ubiquitous in modern, neural NLP. Cross-lingual representations (Mikolov et al., 2013) additionally represent words from various languages in a shared continuous space, which in turn can be used for Bilingual Lexicon Induction (BLI). BLI is often the first step towards several downstream tasks such as Part-Of-Speech (POS) tagging (Zhang et al., 2016), parsing (Ammar et al., 2016a), document classification (Klementiev et al., 2012), and machine translation (Irvine and Callison-Burch, 2013; Artetxe et al., 2018b; Lample et al., 2018). Often, such shared representations are learned w"
2020.acl-main.766,P06-2112,0,0.0592268,"ifica Match M;SG M;PL F;SG M;SG F;SG SG PL Bridged Greek–Italian Lexicon Greek Italian ειρηνικός ειρηνική ειρηνικό ειρηνικά pacifico, pacifici, pacifica pacifica, pacifico, pacifici pacifica, pacifico, pacifici pacifici, pacifica, pacifico Table 1: Triangulation and filtering example on Greek–Italian. All words are valid translations of the English word ‘peaceful’. We also show filtered-out translations. 3.2 Dictionaries for all Language Pairs through Triangulation Our second method for creating new dictionaries is inspired by phrase table triangulation ideas from the pre-neural MT community (Wang et al., 2006; Levinboim and Chiang, 2015). The concept can be easily explained with an example, visualized in Figure 1. Consider the Portuguese (Pt) word trabalho which, according to the MUSE Pt–En dictionary, has the words job and work as possible En translations. In turn, these two En words can be translated to 4 and 5 Czech (Cs) words respectively. By utilizing the transitive property (which translation should exhibit) we can identify the set of 5 possible Cs translations for the Pt word trabalho. Following this simple triangulation approach, we create 4,704 new dictionaries over pairs between the 50 l"
2020.acl-main.766,N15-1104,0,0.0566144,"Missing"
2020.acl-main.766,P17-1179,0,0.391993,"spectively). First, monolingual word embeddings are learned over 1 Available embeddings. at https://github.com/antonisa/ large swaths of text. Such pre-trained word embeddings, such as the fastText Wikipedia vectors (Grave et al., 2018), are available for many languages and are widely used. Second, a mapping between the languages is learned in one of three ways: in a supervised manner if dictionaries or parallel data are available to be used for supervision (Zou et al., 2013), under minimal supervision e.g. using only identical strings (Smith et al., 2017), or even in an unsupervised fashion (Zhang et al., 2017; Conneau et al., 2018). Both in bilingual and multilingual settings, it is common that one of the language embedding spaces is the target to which all other languages get aligned (hereinafter “the hub&quot;). We outline the details in Section 2. Despite all the recent progress in learning crosslingual embeddings, we identify a major shortcoming to previous work: it is by and large English-centric. Notably, most MWE approaches essentially select English as the hub during training by default, aligning all other language spaces to the English one. We argue and empirically show, however, that English"
2020.acl-main.766,N16-1156,0,0.0212573,". Finally, in our analysis we identify general guidelines for strong cross-lingual embedding baselines, that extend to language pairs that do not include English. 1 Introduction Continuous vectors for representing words (embeddings) (Turian et al., 2010) have become ubiquitous in modern, neural NLP. Cross-lingual representations (Mikolov et al., 2013) additionally represent words from various languages in a shared continuous space, which in turn can be used for Bilingual Lexicon Induction (BLI). BLI is often the first step towards several downstream tasks such as Part-Of-Speech (POS) tagging (Zhang et al., 2016), parsing (Ammar et al., 2016a), document classification (Klementiev et al., 2012), and machine translation (Irvine and Callison-Burch, 2013; Artetxe et al., 2018b; Lample et al., 2018). Often, such shared representations are learned with a two-step process, whether under bilingual or multilingual settings (hereinafter BWE and MWE, respectively). First, monolingual word embeddings are learned over 1 Available embeddings. at https://github.com/antonisa/ large swaths of text. Such pre-trained word embeddings, such as the fastText Wikipedia vectors (Grave et al., 2018), are available for many lan"
2020.acl-main.766,N19-1161,1,0.879801,"Missing"
2020.coling-main.471,D19-1091,1,0.831926,"or each segment. Accordingly, we perform the evaluation at the morpheme level: the first two evaluation units from the Figure 1 example would be “you&quot; and &quot;-GEN1&quot;, separated. This setting will provide somewhat of an oracle score, that would be achievable if a linguist or the community provide correct segmentations for the transcriptions, or if a morphological segmentation tool is available for that language. Transliteration Cross-lingual training between typologically related languages has shown promising results in several NLP tasks especially in low-resource settings (McCarthy et al., 2019; Anastasopoulos and Neubig, 2019). Two of our evaluation languages, namely Lezgian and Tsez are fairly similar as they are both members of the Nakho-Daghestanian language family, and as such are ideal for crosslingual transfer. However, Anastasopoulos and Neubig (2019) pointed out that cross-lingual learning can be inversely impeded if the languages do not use the same script even if they are closely genealogically related languages. Lezgian is written in Cyrillic script while Tsez is written in Latin script. To maximally exploit the power of cross-lingual training, we transliterated Lezgian from Cyrillic script to Latin scri"
2020.coling-main.471,D09-1031,0,0.0389939,"word before the correct destination Musa ‘Musa’. Upon inspection, we found that the input word Musaňin is segmented into Mus-aňin after BPE, and the occurrences of aňin in the training set are overwhelmingly combinations of -a and ňin ‘QUOT’, where -a is a verbal suffix. In contrast, Musa ‘Musa’ is a proper name. The model cannot deduce that ňin is its own morpheme and may appear after nouns, when it has only seen it in tandem with a verbal suffix which, obviously, appears only after verbs. 5405 7 Related Work Several works have studied the automated IGT generation task (Palmer et al., 2009; Baldridge and Palmer, 2009; Samardži´c et al., 2015; Moeller and Hulden, 2018; McMillan-Major, 2020). They mainly used machine learning methods such as CRF and SVM to generate gloss and proposed a series of heuristic post-editing algorithms to improve the performance. Among them, Palmer et al. (2009), Baldridge and Palmer (2009) combined machine labeling and active learning for creating IGT. Moeller and Hulden (2018) tested LSTMs to predict the morphological labels within glosses, but underperformed against CRF models in that task. McMillan-Major (2020) exploited parallel information in gloss generation. These models e"
2020.coling-main.471,W14-2206,0,0.0700873,"Missing"
2020.coling-main.471,C14-1096,0,0.0299487,", to our knowledge, the first one to show that modern neural systems are a viable solution for the automatic glossing task, without requiring any additional components or making unrealistic assumptions regarding data or NLP tool availability for low-resource languages. We rely on the observation that parallel corpora with transcription and translation are likely to be available for many low-resource languages, since the knowledge of the two languages is sufficient for translating the corpus without the need of linguistic training. Documentation approaches relying on parallel audio collection (Bird et al., 2014) are in fact already underway in the Americas (Jimerson and Prud’hommeaux, 2018) and Africa (Rialland et al., 2018; Hamlaoui et al., 2018), among other places. An additional advantage of parallel corpora is that they contain rich information that can be beneficial for gloss generation. As Figure 1 outlines, the stems/lemmas in the analysis are often hiding in the translation, while the grammatical tags could be derived from the segments in the transcription. We hypothesize that the information from the translation can further ground the gloss generation, and especially allow a system that prop"
2020.coling-main.471,I17-1004,0,0.0405263,"Missing"
2020.coling-main.471,L18-1533,0,0.0282245,"quiring any additional components or making unrealistic assumptions regarding data or NLP tool availability for low-resource languages. We rely on the observation that parallel corpora with transcription and translation are likely to be available for many low-resource languages, since the knowledge of the two languages is sufficient for translating the corpus without the need of linguistic training. Documentation approaches relying on parallel audio collection (Bird et al., 2014) are in fact already underway in the Americas (Jimerson and Prud’hommeaux, 2018) and Africa (Rialland et al., 2018; Hamlaoui et al., 2018), among other places. An additional advantage of parallel corpora is that they contain rich information that can be beneficial for gloss generation. As Figure 1 outlines, the stems/lemmas in the analysis are often hiding in the translation, while the grammatical tags could be derived from the segments in the transcription. We hypothesize that the information from the translation can further ground the gloss generation, and especially allow a system that properly takes into account to generalize to produce lemmas or stems unseen during training. In this work we propose an automated system which"
2020.coling-main.471,L18-1657,0,0.0629911,"Missing"
2020.coling-main.471,W17-0102,0,0.108775,"tems unseen during training. In this work we propose an automated system which creates the hard-to-obtain gloss from an easyto-obtain parallel corpus. We use deep neural models which have driven recent impressive advances in all facets of modern natural language processing (NLP). Our model for automatic gloss generation uses multi-source transformer models, combining information from the transcription and the translation, significantly outperforming previous state-of-the-art results on three challenging datasets in Lezgian, Tsez, and Arapaho (Arkhangelskiy, 2012; Abdulaev and Abdullaev, 2010; Kazeminejad et al., 2017). Importantly, our approach does not rely on any additional annotations other than plain transcription and translation, also making no assumptions about the gloss tag space. We further extend our training recipes to include necessary improvements that deal with data paucity (utilizing cross-lingual transfer from similar languages) and with the specific characteristics of the glossing task (presenting solutions for output length control). Our contributions are three-fold: 1. We apply multi-source transformers on the gloss generation task and significantly outperform previous state-of-the-art st"
2020.coling-main.471,D19-3019,0,0.0172569,"itten in Cyrillic script while Tsez is written in Latin script. To maximally exploit the power of cross-lingual training, we transliterated Lezgian from Cyrillic script to Latin script, and transliterated Tsez from Latin script to Cyrillic script.5 With the original and the transliterated versions of the training data at hand, we combine them during training into a single training set for the L ANGUAGE T RANSFER Model. The evaluation is of course performed on the original test sets with the original corresponding scripts. 5.1 Implementation We base our implementation on the Joey-NMT toolkit6 (Kreutzer et al., 2019), which we extended to support multi-source transformer models.7 The transcription and translation input sentences can be represented at different granularities: either at the word level or at the more recently popular sub-word level. For simplicity we leave this detail out of the results tables, reporting results with the better-performing option in each case. It is worth noting, though, that for Tsez and Arapaho the sub-word representations (obtained using byte-pair-encoding (BPE)8 ) always lead to better results. For the much smaller Lezgian dataset, we saw no difference between sub-word an"
2020.coling-main.471,W19-4226,0,0.0153372,"correct stems or tags for each segment. Accordingly, we perform the evaluation at the morpheme level: the first two evaluation units from the Figure 1 example would be “you&quot; and &quot;-GEN1&quot;, separated. This setting will provide somewhat of an oracle score, that would be achievable if a linguist or the community provide correct segmentations for the transcriptions, or if a morphological segmentation tool is available for that language. Transliteration Cross-lingual training between typologically related languages has shown promising results in several NLP tasks especially in low-resource settings (McCarthy et al., 2019; Anastasopoulos and Neubig, 2019). Two of our evaluation languages, namely Lezgian and Tsez are fairly similar as they are both members of the Nakho-Daghestanian language family, and as such are ideal for crosslingual transfer. However, Anastasopoulos and Neubig (2019) pointed out that cross-lingual learning can be inversely impeded if the languages do not use the same script even if they are closely genealogically related languages. Lezgian is written in Cyrillic script while Tsez is written in Latin script. To maximally exploit the power of cross-lingual training, we transliterated Lezgian"
2020.coling-main.471,2020.scil-1.42,0,0.106078,"or This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 5397 Proceedings of the 28th International Conference on Computational Linguistics, pages 5397–5408 Barcelona, Spain (Online), December 8-13, 2020 Figure 1: A Tsez IGT example. Combining information from both the transcription and the translation can aid in deriving the information in the analysis. stems (Samardži´c et al., 2015), and using models based on Conditional Random Fields (CRF) integrated with translation and POS-tagging information (McMillan-Major, 2020). In contrast, our approach is, to our knowledge, the first one to show that modern neural systems are a viable solution for the automatic glossing task, without requiring any additional components or making unrealistic assumptions regarding data or NLP tool availability for low-resource languages. We rely on the observation that parallel corpora with transcription and translation are likely to be available for many low-resource languages, since the knowledge of the two languages is sufficient for translating the corpus without the need of linguistic training. Documentation approaches relying"
2020.coling-main.471,D16-1096,0,0.026883,"er text generation tasks where the generated text can be relatively free in word order, the gloss must map to the transcription morpheme-by-morpheme or word-by-word, dependent on the intended granularity. One drawback of using a seq2seq model for the gloss generation task compared to e.g. a CRF-based approach like (McMillan-Major, 2020) is that a hard constraint of “one output per input” is not enforced by or hard-coded in the model. Even though structural biases (Cohn et al., 2016) such as hard monotonic attention (Wu and Cotterell, 2019), or source-side coverage mechanisms (Tu et al., 2016; Mi et al., 2016) could remedy this potential issue, we found that there was little need for them, as a simple mechanism to control the final output length during inference was sufficient.1 The intuition lies in the observation that the length of the output gloss should match that of the input transcription exactly. Hence, during inference we set a minimum desired length of the output sequence, and disallow any candidates shorter than that.2 Alignment between gloss and transcription To ensure fair evaluation against the baseline and other models, we need to be able to produce the exact mapping of the output gl"
2020.coling-main.471,W18-4809,0,0.271751,"lly manifest a yawning gap between the amount of material recorded and archived and the amount of data that is thoroughly analyzed with morphological segmentation and gloss (Seifart et al., 2018). This gap can be filled using automatic approaches, which could at least accelerate the annotation process by providing high-quality first-pass annotations. Previous approaches to automatic gloss generation include manual rule crafting and deep rule-based analysis (Bender et al., 2014; Snoek et al., 2014), treating the glossing task as a classification problem focusing only on the morphological tags (Moeller and Hulden, 2018) and requiring a lexicon for This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 5397 Proceedings of the 28th International Conference on Computational Linguistics, pages 5397–5408 Barcelona, Spain (Online), December 8-13, 2020 Figure 1: A Tsez IGT example. Combining information from both the transcription and the translation can aid in deriving the information in the analysis. stems (Samardži´c et al., 2015), and using models based on Conditional Random Fields (CRF) integrated with translation and"
2020.coling-main.471,W09-1905,0,0.0375284,"BL SUPER . ESS ’, one word before the correct destination Musa ‘Musa’. Upon inspection, we found that the input word Musaňin is segmented into Mus-aňin after BPE, and the occurrences of aňin in the training set are overwhelmingly combinations of -a and ňin ‘QUOT’, where -a is a verbal suffix. In contrast, Musa ‘Musa’ is a proper name. The model cannot deduce that ňin is its own morpheme and may appear after nouns, when it has only seen it in tandem with a verbal suffix which, obviously, appears only after verbs. 5405 7 Related Work Several works have studied the automated IGT generation task (Palmer et al., 2009; Baldridge and Palmer, 2009; Samardži´c et al., 2015; Moeller and Hulden, 2018; McMillan-Major, 2020). They mainly used machine learning methods such as CRF and SVM to generate gloss and proposed a series of heuristic post-editing algorithms to improve the performance. Among them, Palmer et al. (2009), Baldridge and Palmer (2009) combined machine labeling and active learning for creating IGT. Moeller and Hulden (2018) tested LSTMs to predict the morphological labels within glosses, but underperformed against CRF models in that task. McMillan-Major (2020) exploited parallel information in glos"
2020.coling-main.471,P02-1040,0,0.109072,"veraging over all heads. 5400 Language Translation L EZGIAN T SEZ A RAPAHO English English/Russian English Training Examples Test Examples 951 1584 25208 119 198 3151 Table 1: Dataset information for the languages in our experiments. 4 Evaluating Gloss Generation The characteristics of gloss generation require special care rather than blindly using metrics established for other tasks like machine translation. Previous work uses: • Accuracy: percentage of correct (full) analyses for each token. It is the main metric used in previous work (Samardži´c et al., 2015; McMillan-Major, 2020). • BLEU (Papineni et al., 2002): an average of n-gram precision along with a brevity penalty, BLEU is perhaps the most popular reference-based machine translation method. Since our models are inspired from MT, we use it as another indication of quality as it captures accuracy/precision over n-grams, even though the rest of the metrics are more suitable to the automatic glossing task. • Precision/Recall: We further break down the evaluation to focus separately on lemmas and tags. Several previous works prioritize precision over recall, especially by not outputting tags if items are not seen during training, e.g. (Moeller and"
2020.coling-main.471,L18-1674,0,0.0242646,"ossing task, without requiring any additional components or making unrealistic assumptions regarding data or NLP tool availability for low-resource languages. We rely on the observation that parallel corpora with transcription and translation are likely to be available for many low-resource languages, since the knowledge of the two languages is sufficient for translating the corpus without the need of linguistic training. Documentation approaches relying on parallel audio collection (Bird et al., 2014) are in fact already underway in the Americas (Jimerson and Prud’hommeaux, 2018) and Africa (Rialland et al., 2018; Hamlaoui et al., 2018), among other places. An additional advantage of parallel corpora is that they contain rich information that can be beneficial for gloss generation. As Figure 1 outlines, the stems/lemmas in the analysis are often hiding in the translation, while the grammatical tags could be derived from the segments in the transcription. We hypothesize that the information from the translation can further ground the gloss generation, and especially allow a system that properly takes into account to generalize to produce lemmas or stems unseen during training. In this work we propose a"
2020.coling-main.471,2020.sigmorphon-1.21,0,0.0361868,"ansformer for Gloss Generation Problem Formulation and Model Our model is built upon the transformer model (Vaswani et al., 2017), a self-attention-based sequenceto-sequence (seq2seq) neural model. Compared to the CRF model used in McMillan-Major (2020), which can only capture local dependencies, a self-attention model can produce context-sensitive hidden representations that take the whole input into account. Moreover, unlike other recurrent (seq2seq) models such as bidirectional LSTM, the Transformer model shows more robust performance in morphologyrelated tasks under low-resource settings (Ryan and Hulden, 2020). Our architecture choice is also motivated by the promising performance of the model along with its computational efficiency. The original Transformer is composed of a single encoder and a decoder, each with several layers. Each encoder layer consists of a multi-head self-attention layer and a fully connected feed-forward network, while decoder layers are additionally augmented with multi-head attention over the output of the encoder stack. Our model adds a second encoder to create a multi-source transformer similar to (Zoph and Knight, 2016; Anastasopoulos and Chiang, 2018), in order to inco"
2020.coling-main.471,W15-3710,0,0.03569,"Missing"
2020.coling-main.471,P16-1162,0,0.0448655,"Tsez and Arapaho the sub-word representations (obtained using byte-pair-encoding (BPE)8 ) always lead to better results. For the much smaller Lezgian dataset, we saw no difference between sub-word and word-level models, but this lack of difference can be explained by the overall very small size of the vocabulary for the Lezgian dataset. 5 We use the transliterator provided by https://pypi.org/project/transliterate/. https://github.com/joeynmt/joeynmt 7 Our code will be open-sourced at https://github.com/yukiyakiZ/Automatic_Glossing. 8 We use the sentencepiece implementation of the BPE method (Sennrich et al., 2016) with vocab size of 2000 for Lezgian, 2500 for Tsez, and 10000 for Arapaho) 6 5402 For training all Lezgian and Tsez models and the Arapaho model with the subsampled 2,000 training sentences, we use 2 layers for both encoders and the decoder and 2 attention heads. All the embedding and hidden state dimensions are set to 128. We use a batch size of 20. For training the Arapaho model on the original larger dataset, we use 4 layers for all encoders and decoder, with 4 attention heads. The embedding and hidden state dimension are 256, and batch size is 50. For all models, learning rate is initiali"
2020.coling-main.471,W14-2205,0,0.815954,"Missing"
2020.coling-main.471,P16-1008,0,0.0252577,"ontrol Unlike other text generation tasks where the generated text can be relatively free in word order, the gloss must map to the transcription morpheme-by-morpheme or word-by-word, dependent on the intended granularity. One drawback of using a seq2seq model for the gloss generation task compared to e.g. a CRF-based approach like (McMillan-Major, 2020) is that a hard constraint of “one output per input” is not enforced by or hard-coded in the model. Even though structural biases (Cohn et al., 2016) such as hard monotonic attention (Wu and Cotterell, 2019), or source-side coverage mechanisms (Tu et al., 2016; Mi et al., 2016) could remedy this potential issue, we found that there was little need for them, as a simple mechanism to control the final output length during inference was sufficient.1 The intuition lies in the observation that the length of the output gloss should match that of the input transcription exactly. Hence, during inference we set a minimum desired length of the output sequence, and disallow any candidates shorter than that.2 Alignment between gloss and transcription To ensure fair evaluation against the baseline and other models, we need to be able to produce the exact mappin"
2020.coling-main.471,P19-1148,0,0.0226311,"the particular nature of the gloss generation task. Length control Unlike other text generation tasks where the generated text can be relatively free in word order, the gloss must map to the transcription morpheme-by-morpheme or word-by-word, dependent on the intended granularity. One drawback of using a seq2seq model for the gloss generation task compared to e.g. a CRF-based approach like (McMillan-Major, 2020) is that a hard constraint of “one output per input” is not enforced by or hard-coded in the model. Even though structural biases (Cohn et al., 2016) such as hard monotonic attention (Wu and Cotterell, 2019), or source-side coverage mechanisms (Tu et al., 2016; Mi et al., 2016) could remedy this potential issue, we found that there was little need for them, as a simple mechanism to control the final output length during inference was sufficient.1 The intuition lies in the observation that the length of the output gloss should match that of the input transcription exactly. Hence, during inference we set a minimum desired length of the output sequence, and disallow any candidates shorter than that.2 Alignment between gloss and transcription To ensure fair evaluation against the baseline and other m"
2020.coling-main.471,N16-1004,0,0.0285687,"rphologyrelated tasks under low-resource settings (Ryan and Hulden, 2020). Our architecture choice is also motivated by the promising performance of the model along with its computational efficiency. The original Transformer is composed of a single encoder and a decoder, each with several layers. Each encoder layer consists of a multi-head self-attention layer and a fully connected feed-forward network, while decoder layers are additionally augmented with multi-head attention over the output of the encoder stack. Our model adds a second encoder to create a multi-source transformer similar to (Zoph and Knight, 2016; Anastasopoulos and Chiang, 2018), in order to incorporate the secondary information from the translation. A visual depiction of our model is outlined in Figure 2. Let X1 = x11 . . . x1N be a sequence of transcription words, X2 = x21 . . . x2M a sequence of translation words, and Y = y1 . . . yK be a sequence of the target gloss. A single-source gloss generation model attempts to model P (Y |X1 ). A multi-source model can jointly model P (Y |X1 , X2 ), and thus we need two encoders (see Figure 2b). One encoder transforms the input transcription sequence x11 . . . x1N into a sequence of input"
2020.coling-tutorials.7,D19-1091,1,0.837959,") and are actively used in documentation projects (Blachon et al., 2016); automatic speech recognition models have been created to aid with automatic phonetic or orthographic transcriptions focusing in indigenous Australian (Foley et al., 2018) or tonal languages from China and the Americas (Michaud et al., 2018); machine translation for under-represented languages have been presented as new corpora have been collected (Abbott and Martinus, 2018; Abate et al., 2018); cross-lingual transfer has been successfully applied for tagging, morphological analysis and inflection (McCarthy et al., 2019; Anastasopoulos and Neubig, 2019); multitask and active learning are being used for learning from continuous annotations on multiple tasks (Gerstenberger et al., 2017; Anastasopoulos et al., 2018; Chaudhary et al., 2019); approaches dedicated to indigenous polysynthetic languages have been developed (Schwartz et al., 2019; Kann et al., 2018); and computational methods have been used to study or discover typological features from large collections of text (Asgari and Schütze, 2017; Malaviya et al., 2017). In this tutorial, we will outline the language documentation process and revitalization efforts, while also mapping them to"
2020.coling-tutorials.7,C18-1214,1,0.827863,"graphic transcriptions focusing in indigenous Australian (Foley et al., 2018) or tonal languages from China and the Americas (Michaud et al., 2018); machine translation for under-represented languages have been presented as new corpora have been collected (Abbott and Martinus, 2018; Abate et al., 2018); cross-lingual transfer has been successfully applied for tagging, morphological analysis and inflection (McCarthy et al., 2019; Anastasopoulos and Neubig, 2019); multitask and active learning are being used for learning from continuous annotations on multiple tasks (Gerstenberger et al., 2017; Anastasopoulos et al., 2018; Chaudhary et al., 2019); approaches dedicated to indigenous polysynthetic languages have been developed (Schwartz et al., 2019; Kann et al., 2018); and computational methods have been used to study or discover typological features from large collections of text (Asgari and Schütze, 2017; Malaviya et al., 2017). In this tutorial, we will outline the language documentation process and revitalization efforts, while also mapping them to concrete computational tasks. We will then focus on the machine learning approaches tailored to tackle these tasks under this very data-constrained setting. An o"
2020.coling-tutorials.7,W17-0100,0,0.098002,"which typically is language description. The extreme pace of language loss and the urgent needs for language revitalization, however, require that we utilize documentations and go beyond language description: enter 21st century NLP. Himmelmann’s 20-year-old radical vision (Himmelmann, 1998) for a data-centric approach to language documentation (which sparked the creation of modern documentary linguistics) has slowly begun to materialize (McDonnell et al., 2018). For example, the Workshops on the Use of Computational Methods in the Study of Endangered Languages (Comput-EL) (Good et al., 2014; Arppe et al., 2017; Arppe et al., 2019) have provided a small forum for the much-needed discussion between NLP practitioners and documentary and field linguists. Meanwhile, increasingly more focus is dedicated on NLP research and bringing modern technologies to endangered languages. For example, mobile applications have been developed for data collection (Bird et al., 2014; Gauthier et al., 2016) and are actively used in documentation projects (Blachon et al., 2016); automatic speech recognition models have been created to aid with automatic phonetic or orthographic transcriptions focusing in indigenous Austral"
2020.coling-tutorials.7,W19-6000,0,0.0844101,"language description. The extreme pace of language loss and the urgent needs for language revitalization, however, require that we utilize documentations and go beyond language description: enter 21st century NLP. Himmelmann’s 20-year-old radical vision (Himmelmann, 1998) for a data-centric approach to language documentation (which sparked the creation of modern documentary linguistics) has slowly begun to materialize (McDonnell et al., 2018). For example, the Workshops on the Use of Computational Methods in the Study of Endangered Languages (Comput-EL) (Good et al., 2014; Arppe et al., 2017; Arppe et al., 2019) have provided a small forum for the much-needed discussion between NLP practitioners and documentary and field linguists. Meanwhile, increasingly more focus is dedicated on NLP research and bringing modern technologies to endangered languages. For example, mobile applications have been developed for data collection (Bird et al., 2014; Gauthier et al., 2016) and are actively used in documentation projects (Blachon et al., 2016); automatic speech recognition models have been created to aid with automatic phonetic or orthographic transcriptions focusing in indigenous Australian (Foley et al., 20"
2020.coling-tutorials.7,D17-1011,0,0.0128713,"al., 2018); cross-lingual transfer has been successfully applied for tagging, morphological analysis and inflection (McCarthy et al., 2019; Anastasopoulos and Neubig, 2019); multitask and active learning are being used for learning from continuous annotations on multiple tasks (Gerstenberger et al., 2017; Anastasopoulos et al., 2018; Chaudhary et al., 2019); approaches dedicated to indigenous polysynthetic languages have been developed (Schwartz et al., 2019; Kann et al., 2018); and computational methods have been used to study or discover typological features from large collections of text (Asgari and Schütze, 2017; Malaviya et al., 2017). In this tutorial, we will outline the language documentation process and revitalization efforts, while also mapping them to concrete computational tasks. We will then focus on the machine learning approaches tailored to tackle these tasks under this very data-constrained setting. An overview of many of those NLP methods, as applied for language documentation, can be found in co-proposer Anastasopoulos’ PhD dissertation (Anastasopoulos, 2019). Other surveys focus on the state of language technologies within specific geographic areas, such as co-proposer Cox’s overview"
2020.coling-tutorials.7,W14-2201,0,0.0305571,"ked the creation of modern documentary linguistics) has slowly begun to materialize (McDonnell et al., 2018). For example, the Workshops on the Use of Computational Methods in the Study of Endangered Languages (Comput-EL) (Good et al., 2014; Arppe et al., 2017; Arppe et al., 2019) have provided a small forum for the much-needed discussion between NLP practitioners and documentary and field linguists. Meanwhile, increasingly more focus is dedicated on NLP research and bringing modern technologies to endangered languages. For example, mobile applications have been developed for data collection (Bird et al., 2014; Gauthier et al., 2016) and are actively used in documentation projects (Blachon et al., 2016); automatic speech recognition models have been created to aid with automatic phonetic or orthographic transcriptions focusing in indigenous Australian (Foley et al., 2018) or tonal languages from China and the Americas (Michaud et al., 2018); machine translation for under-represented languages have been presented as new corpora have been collected (Abbott and Martinus, 2018; Abate et al., 2018); cross-lingual transfer has been successfully applied for tagging, morphological analysis and inflection ("
2020.coling-tutorials.7,J09-3007,0,0.0190415,"edu gneubig@cs.cmu.edu christopher.cox@carleton.ca hilaria.cruz@louisville.edu 1 Description Computational Linguistics and Natural Language Processing (NLP) have taken immense strides, spearheaded by neural methods and large data collections. The result is ubiquitous language technology and vast amounts of research on new tasks and products. However, the vast majority of the world’s languages have been mostly ignored, including the most vulnerable among them: endangered languages. The lack of communication between the NLP community and the documentary linguistics community is partly to blame (Bird, 2009). Even though field and documentary linguists produce resources and use NLP methods, this is done in isolation, as computational methods are seen as a means towards the final goal, which typically is language description. The extreme pace of language loss and the urgent needs for language revitalization, however, require that we utilize documentations and go beyond language description: enter 21st century NLP. Himmelmann’s 20-year-old radical vision (Himmelmann, 1998) for a data-centric approach to language documentation (which sparked the creation of modern documentary linguistics) has slowly"
2020.coling-tutorials.7,D19-1520,1,0.747607,"ng in indigenous Australian (Foley et al., 2018) or tonal languages from China and the Americas (Michaud et al., 2018); machine translation for under-represented languages have been presented as new corpora have been collected (Abbott and Martinus, 2018; Abate et al., 2018); cross-lingual transfer has been successfully applied for tagging, morphological analysis and inflection (McCarthy et al., 2019; Anastasopoulos and Neubig, 2019); multitask and active learning are being used for learning from continuous annotations on multiple tasks (Gerstenberger et al., 2017; Anastasopoulos et al., 2018; Chaudhary et al., 2019); approaches dedicated to indigenous polysynthetic languages have been developed (Schwartz et al., 2019; Kann et al., 2018); and computational methods have been used to study or discover typological features from large collections of text (Asgari and Schütze, 2017; Malaviya et al., 2017). In this tutorial, we will outline the language documentation process and revitalization efforts, while also mapping them to concrete computational tasks. We will then focus on the machine learning approaches tailored to tackle these tasks under this very data-constrained setting. An overview of many of those"
2020.coling-tutorials.7,W17-0604,0,0.0244958,"automatic phonetic or orthographic transcriptions focusing in indigenous Australian (Foley et al., 2018) or tonal languages from China and the Americas (Michaud et al., 2018); machine translation for under-represented languages have been presented as new corpora have been collected (Abbott and Martinus, 2018; Abate et al., 2018); cross-lingual transfer has been successfully applied for tagging, morphological analysis and inflection (McCarthy et al., 2019; Anastasopoulos and Neubig, 2019); multitask and active learning are being used for learning from continuous annotations on multiple tasks (Gerstenberger et al., 2017; Anastasopoulos et al., 2018; Chaudhary et al., 2019); approaches dedicated to indigenous polysynthetic languages have been developed (Schwartz et al., 2019; Kann et al., 2018); and computational methods have been used to study or discover typological features from large collections of text (Asgari and Schütze, 2017; Malaviya et al., 2017). In this tutorial, we will outline the language documentation process and revitalization efforts, while also mapping them to concrete computational tasks. We will then focus on the machine learning approaches tailored to tackle these tasks under this very d"
2020.coling-tutorials.7,N18-1005,0,0.0209918,"translation for under-represented languages have been presented as new corpora have been collected (Abbott and Martinus, 2018; Abate et al., 2018); cross-lingual transfer has been successfully applied for tagging, morphological analysis and inflection (McCarthy et al., 2019; Anastasopoulos and Neubig, 2019); multitask and active learning are being used for learning from continuous annotations on multiple tasks (Gerstenberger et al., 2017; Anastasopoulos et al., 2018; Chaudhary et al., 2019); approaches dedicated to indigenous polysynthetic languages have been developed (Schwartz et al., 2019; Kann et al., 2018); and computational methods have been used to study or discover typological features from large collections of text (Asgari and Schütze, 2017; Malaviya et al., 2017). In this tutorial, we will outline the language documentation process and revitalization efforts, while also mapping them to concrete computational tasks. We will then focus on the machine learning approaches tailored to tackle these tasks under this very data-constrained setting. An overview of many of those NLP methods, as applied for language documentation, can be found in co-proposer Anastasopoulos’ PhD dissertation (Anastasop"
2020.coling-tutorials.7,C18-1222,1,0.82584,"). In this tutorial, we will outline the language documentation process and revitalization efforts, while also mapping them to concrete computational tasks. We will then focus on the machine learning approaches tailored to tackle these tasks under this very data-constrained setting. An overview of many of those NLP methods, as applied for language documentation, can be found in co-proposer Anastasopoulos’ PhD dissertation (Anastasopoulos, 2019). Other surveys focus on the state of language technologies within specific geographic areas, such as co-proposer Cox’s overview of Canadian languages (Littell et al., 2018) or the one by Mager et al. (2018), focusing on indigenous American languages. The goal of our tutorial will be two-fold. On one hand, we will aim to acquaint the audience with the needs of the documentary linguistics community, and cover the already existing computational research 39 Proceedings of the 28th International Conference on Computational Linguistics: Tutorial Abstracts, pages 39–45 Barcelona, Spain (Online), December 12th, 2020. in the field. On the other hand, we will discuss the capabilities and limitations of current computational approaches, so that the participants will know w"
2020.coling-tutorials.7,C18-1006,0,0.0173374,"the language documentation process and revitalization efforts, while also mapping them to concrete computational tasks. We will then focus on the machine learning approaches tailored to tackle these tasks under this very data-constrained setting. An overview of many of those NLP methods, as applied for language documentation, can be found in co-proposer Anastasopoulos’ PhD dissertation (Anastasopoulos, 2019). Other surveys focus on the state of language technologies within specific geographic areas, such as co-proposer Cox’s overview of Canadian languages (Littell et al., 2018) or the one by Mager et al. (2018), focusing on indigenous American languages. The goal of our tutorial will be two-fold. On one hand, we will aim to acquaint the audience with the needs of the documentary linguistics community, and cover the already existing computational research 39 Proceedings of the 28th International Conference on Computational Linguistics: Tutorial Abstracts, pages 39–45 Barcelona, Spain (Online), December 12th, 2020. in the field. On the other hand, we will discuss the capabilities and limitations of current computational approaches, so that the participants will know when and how to apply NLP methods,"
2020.coling-tutorials.7,D17-1268,1,0.754008,"transfer has been successfully applied for tagging, morphological analysis and inflection (McCarthy et al., 2019; Anastasopoulos and Neubig, 2019); multitask and active learning are being used for learning from continuous annotations on multiple tasks (Gerstenberger et al., 2017; Anastasopoulos et al., 2018; Chaudhary et al., 2019); approaches dedicated to indigenous polysynthetic languages have been developed (Schwartz et al., 2019; Kann et al., 2018); and computational methods have been used to study or discover typological features from large collections of text (Asgari and Schütze, 2017; Malaviya et al., 2017). In this tutorial, we will outline the language documentation process and revitalization efforts, while also mapping them to concrete computational tasks. We will then focus on the machine learning approaches tailored to tackle these tasks under this very data-constrained setting. An overview of many of those NLP methods, as applied for language documentation, can be found in co-proposer Anastasopoulos’ PhD dissertation (Anastasopoulos, 2019). Other surveys focus on the state of language technologies within specific geographic areas, such as co-proposer Cox’s overview of Canadian languages (L"
2020.coling-tutorials.7,W19-4226,0,0.0131412,"; Gauthier et al., 2016) and are actively used in documentation projects (Blachon et al., 2016); automatic speech recognition models have been created to aid with automatic phonetic or orthographic transcriptions focusing in indigenous Australian (Foley et al., 2018) or tonal languages from China and the Americas (Michaud et al., 2018); machine translation for under-represented languages have been presented as new corpora have been collected (Abbott and Martinus, 2018; Abate et al., 2018); cross-lingual transfer has been successfully applied for tagging, morphological analysis and inflection (McCarthy et al., 2019; Anastasopoulos and Neubig, 2019); multitask and active learning are being used for learning from continuous annotations on multiple tasks (Gerstenberger et al., 2017; Anastasopoulos et al., 2018; Chaudhary et al., 2019); approaches dedicated to indigenous polysynthetic languages have been developed (Schwartz et al., 2019; Kann et al., 2018); and computational methods have been used to study or discover typological features from large collections of text (Asgari and Schütze, 2017; Malaviya et al., 2017). In this tutorial, we will outline the language documentation process and revitalization e"
2020.coling-tutorials.7,W19-6012,0,0.0206734,"et al., 2018); machine translation for under-represented languages have been presented as new corpora have been collected (Abbott and Martinus, 2018; Abate et al., 2018); cross-lingual transfer has been successfully applied for tagging, morphological analysis and inflection (McCarthy et al., 2019; Anastasopoulos and Neubig, 2019); multitask and active learning are being used for learning from continuous annotations on multiple tasks (Gerstenberger et al., 2017; Anastasopoulos et al., 2018; Chaudhary et al., 2019); approaches dedicated to indigenous polysynthetic languages have been developed (Schwartz et al., 2019; Kann et al., 2018); and computational methods have been used to study or discover typological features from large collections of text (Asgari and Schütze, 2017; Malaviya et al., 2017). In this tutorial, we will outline the language documentation process and revitalization efforts, while also mapping them to concrete computational tasks. We will then focus on the machine learning approaches tailored to tackle these tasks under this very data-constrained setting. An overview of many of those NLP methods, as applied for language documentation, can be found in co-proposer Anastasopoulos’ PhD dis"
2020.emnlp-main.422,2020.lrec-1.497,0,0.0292855,"Missing"
2020.emnlp-main.475,D11-1033,0,0.195411,"ns. 3 Methods In our setting, we are given two MT models MF E and MEF pretrained on parallel data DF E , and both source and target monolingual corpora DF and DE . The goal is to select and weight samples from the two monolingual corpora for backtranslation, in order to best improve the performance of the two translation models. 3.1 Data Selection Strategies We first describe a commonly used static selection strategy, and then illustrate our dynamic approach. 3.1.1 The Moore and Lewis (2010) Method A common approach for data selection is the Moore and Lewis (2010) method (and extensions, e.g. Axelrod et al. (2011); Duh et al. (2013); Santamar´ıa and Axelrod (2019)), which computes the language model cross-entropy difference for each sentence s in a monolingual corpus: score(s) = HLMin (s) − HLMgen (s), (1) where HLMin (s) and HLMgen (s) represent the cross-entropy scores of s measured with an indomain and a general-domain language model (LM) respectively. Sentences with the highest scores will be selected for training. Typically, the in-domain language model LMin is trained with 5895 (a) score sent 1 sent 2 Representativeness λ(t) Representativeness + (1-λ(t)) Simplicity Simplicity M Representativeness"
2020.emnlp-main.475,P16-1185,0,0.091295,"Missing"
2020.emnlp-main.475,N19-1423,0,0.0205642,"model LMin with indomain monolingual data and compute the score 1 P|s| t=1 log PLMin (st |s<t ) for each sentence s. |s| TF-IDF Scores (TF-IDF). TF-IDF score is another criterion for data selection (Kirchhoff and Bilmes, 2014). For each sentence s, one can compute its term frequency and inverse document frequency for each word. We can thus obtain the TF-IDF vector and calculate the cosine similarity between the TF-IDF vectors of s and each sentence sin in a small in-domain dataset, and treat the maximum value as its representativeness score. 5896 BERT Representation Similarities (BERT). BERT (Devlin et al., 2019) has proven to be effective for sentence representation learning. Following the conclusion of Pires et al. (2019), we feed each sentence to the multilingual BERT model and average the hidden states for all the input tokens except [CLS] and [SEP] at the eighth layer to obtain the sentence representation. We then compute the cosine similarity between representations of sentence s in the monolingual corpus and each sentence sin in a small in-domain set, and the maximum value is treated as the representativeness score. 3.1.4 Simplicity Metrics In our experiments, we study two metrics for measuring"
2020.emnlp-main.475,P13-2119,1,0.845058,"tting, we are given two MT models MF E and MEF pretrained on parallel data DF E , and both source and target monolingual corpora DF and DE . The goal is to select and weight samples from the two monolingual corpora for backtranslation, in order to best improve the performance of the two translation models. 3.1 Data Selection Strategies We first describe a commonly used static selection strategy, and then illustrate our dynamic approach. 3.1.1 The Moore and Lewis (2010) Method A common approach for data selection is the Moore and Lewis (2010) method (and extensions, e.g. Axelrod et al. (2011); Duh et al. (2013); Santamar´ıa and Axelrod (2019)), which computes the language model cross-entropy difference for each sentence s in a monolingual corpus: score(s) = HLMin (s) − HLMgen (s), (1) where HLMin (s) and HLMgen (s) represent the cross-entropy scores of s measured with an indomain and a general-domain language model (LM) respectively. Sentences with the highest scores will be selected for training. Typically, the in-domain language model LMin is trained with 5895 (a) score sent 1 sent 2 Representativeness λ(t) Representativeness + (1-λ(t)) Simplicity Simplicity M Representativeness low high Simplicit"
2020.emnlp-main.475,D18-1045,0,0.0456312,"The Moore and Lewis (2010) data selection strategy for domain adaptation constantly selects the same set of sentences which cannot well represent the target domain. Our approach, instead, selects different subsets of sentences at each epoch and we gradually shift from selecting samples from the general-domain distribution to samples from the target distribution. Introduction Back-translation (Sennrich et al., 2016b) is an effective strategy for improving the performance of neural machine translation (NMT) using monolingual data, delivering impressive gains over already competitive NMT models (Edunov et al., 2018). The strategy is simple: given monolingual data in the target language, one can use a translation model in the opposite of the desired translation direction to back-translate the monolingual data, effectively synthesizing a parallel dataset, which is in turn †: Work completed while at Carnegie Mellon University. Code: https://github.com/zdou0830/ dynamic_select_weight. 1 General Domain (Monolingual) used to train the final translation model. Further improvements can be obtained by iteratively repeating this process (Hoang et al., 2018) in both directions. However, not all monolingual data are"
2020.emnlp-main.475,W19-5401,0,0.023584,"l. (2019), we feed each sentence to the multilingual BERT model and average the hidden states for all the input tokens except [CLS] and [SEP] at the eighth layer to obtain the sentence representation. We then compute the cosine similarity between representations of sentence s in the monolingual corpus and each sentence sin in a small in-domain set, and the maximum value is treated as the representativeness score. 3.1.4 Simplicity Metrics In our experiments, we study two metrics for measuring the simplicity of sentences. Note that in the field of quality estimation for MT (Specia et al., 2010; Fonseca et al., 2019), researchers have proposed several existing techniques to estimate the simplicity of sentences (Turchi et al., 2014; Specia et al., 2015; Shah et al., 2015; Kim and Lee, 2016; Kepler et al., 2019; Zhou et al., 2019; Hou et al., 2019), and here we select a few representative approaches. General-Domain Language Model CrossEntropy (LM-gen). We train a language model LMgen with the one side of the parallel training data DF E . Then, for each sentence s we compute 1 P|s| the score |s| t=1 log PLMgen (st |s<t ). Round-Trip BLEU (R-BLEU). Given two pretrained MT models MF E and MEF , round-trip tran"
2020.emnlp-main.475,W11-2123,0,0.107021,"Missing"
2020.emnlp-main.475,W18-2703,0,0.427658,"ng impressive gains over already competitive NMT models (Edunov et al., 2018). The strategy is simple: given monolingual data in the target language, one can use a translation model in the opposite of the desired translation direction to back-translate the monolingual data, effectively synthesizing a parallel dataset, which is in turn †: Work completed while at Carnegie Mellon University. Code: https://github.com/zdou0830/ dynamic_select_weight. 1 General Domain (Monolingual) used to train the final translation model. Further improvements can be obtained by iteratively repeating this process (Hoang et al., 2018) in both directions. However, not all monolingual data are equally important. An envisioned downstream application is very often characterized by a unique data distribution. In such cases of domain shift, back-translating target domain data can be an effective strategy (Hu et al., 2019) for obtaining a better in-domain translation model. One common strategy is to select samples that are both (1) close to the target distribution and (2) dissimilar to the average generaldomain text (Moore and Lewis, 2010). However, as depicted in Figure 1, this method is not ideal because the second objective co"
2020.emnlp-main.475,W19-5409,0,0.0236587,"between representations of sentence s in the monolingual corpus and each sentence sin in a small in-domain set, and the maximum value is treated as the representativeness score. 3.1.4 Simplicity Metrics In our experiments, we study two metrics for measuring the simplicity of sentences. Note that in the field of quality estimation for MT (Specia et al., 2010; Fonseca et al., 2019), researchers have proposed several existing techniques to estimate the simplicity of sentences (Turchi et al., 2014; Specia et al., 2015; Shah et al., 2015; Kim and Lee, 2016; Kepler et al., 2019; Zhou et al., 2019; Hou et al., 2019), and here we select a few representative approaches. General-Domain Language Model CrossEntropy (LM-gen). We train a language model LMgen with the one side of the parallel training data DF E . Then, for each sentence s we compute 1 P|s| the score |s| t=1 log PLMgen (st |s<t ). Round-Trip BLEU (R-BLEU). Given two pretrained MT models MF E and MEF , round-trip translation first translates a sentence s into another language using MF E and then back-translates the result using MEF , obtaining the reconstructed sentence s0 . The BLEU score between s and s0 is treated as our simplicity metric. Simi"
2020.emnlp-main.475,P19-1286,1,0.709872,"thesizing a parallel dataset, which is in turn †: Work completed while at Carnegie Mellon University. Code: https://github.com/zdou0830/ dynamic_select_weight. 1 General Domain (Monolingual) used to train the final translation model. Further improvements can be obtained by iteratively repeating this process (Hoang et al., 2018) in both directions. However, not all monolingual data are equally important. An envisioned downstream application is very often characterized by a unique data distribution. In such cases of domain shift, back-translating target domain data can be an effective strategy (Hu et al., 2019) for obtaining a better in-domain translation model. One common strategy is to select samples that are both (1) close to the target distribution and (2) dissimilar to the average generaldomain text (Moore and Lewis, 2010). However, as depicted in Figure 1, this method is not ideal because the second objective could bias towards the selection of sentences far from the center of the target distribution, potentially leading to selecting a non-representative set of sentences. Even if we could select all in-domain monolin5894 Proceedings of the 2020 Conference on Empirical Methods in Natural Langua"
2020.emnlp-main.475,W17-5704,0,0.050808,"main Language Model CrossEntropy (LM-gen). We train a language model LMgen with the one side of the parallel training data DF E . Then, for each sentence s we compute 1 P|s| the score |s| t=1 log PLMgen (st |s<t ). Round-Trip BLEU (R-BLEU). Given two pretrained MT models MF E and MEF , round-trip translation first translates a sentence s into another language using MF E and then back-translates the result using MEF , obtaining the reconstructed sentence s0 . The BLEU score between s and s0 is treated as our simplicity metric. Similar ideas have been applied to filter sentences of low quality (Imankulova et al., 2017). For both the representativeness and simplicity scores, it should be noted that they are separately normalized to [0, 1], using the equation score(s)−scoremin scoremax −scoremin , where scoremax and scoremin are the maximum and minimum scores. 3.2 Weighting Strategies Next, we illustrate how we perform data weighting on the back-translated data. 3.2.1 Measuring the Current Quality As general translation models could perform poorly on the in-domain data, we need ways to measure the current quality of the back-translated sentences in order to down-weight examples of poor quality. Encoder Repres"
2020.emnlp-main.475,W18-6478,0,0.260041,"rent Quality As general translation models could perform poorly on the in-domain data, we need ways to measure the current quality of the back-translated sentences in order to down-weight examples of poor quality. Encoder Representation Similarities (Enc). We feed the source sentence x and the target sentence y to the encoders of MF E and MEF respectively, and average the hidden states at the final layer to obtain the representations encF E (x) and encEF (y). The cosine similarity between them is treated as the quality metric. Agreement Between Forward and Backward Models (Agree). Inspired by Junczys-Dowmunt (2018), the second approach utilizes the agreement of the two translation models. For each sentence pair (x, y), we compute the conditional probability HF E (y|x) and HEF (x|y), then exponentiate the absolute value between them exp(−(|HF E (y|x)− HEF (x|y)|)). Intuitively, the back-translated sentences are of poor quality if there are huge disagreements between the two models. 3.2.2 Measuring Quality Improvements In domain adaptation, it is natural that at first the in-domain sentences are poorly translated. As training progresses, however, the quality should be improved. We therefore propose a metr"
2020.emnlp-main.475,W19-5406,0,0.0602812,"Missing"
2020.emnlp-main.475,N16-1059,0,0.0311522,"tence representation. We then compute the cosine similarity between representations of sentence s in the monolingual corpus and each sentence sin in a small in-domain set, and the maximum value is treated as the representativeness score. 3.1.4 Simplicity Metrics In our experiments, we study two metrics for measuring the simplicity of sentences. Note that in the field of quality estimation for MT (Specia et al., 2010; Fonseca et al., 2019), researchers have proposed several existing techniques to estimate the simplicity of sentences (Turchi et al., 2014; Specia et al., 2015; Shah et al., 2015; Kim and Lee, 2016; Kepler et al., 2019; Zhou et al., 2019; Hou et al., 2019), and here we select a few representative approaches. General-Domain Language Model CrossEntropy (LM-gen). We train a language model LMgen with the one side of the parallel training data DF E . Then, for each sentence s we compute 1 P|s| the score |s| t=1 log PLMgen (st |s<t ). Round-Trip BLEU (R-BLEU). Given two pretrained MT models MF E and MEF , round-trip translation first translates a sentence s into another language using MF E and then back-translates the result using MEF , obtaining the reconstructed sentence s0 . The BLEU score"
2020.emnlp-main.475,D14-1014,0,0.106371,"one can view Moore and Lewis (2010) as selecting the most representative and difficult sentences. 3.1.3 Representativeness Metrics We propose three approaches to measure the sentence representativeness. In-Domain Language Model Cross-Entropy (LM-in). As in Axelrod et al. (2011); Duh et al. (2013), we can use HLMin to measure the representativeness of the instances. Concretely, we train a language model LMin with indomain monolingual data and compute the score 1 P|s| t=1 log PLMin (st |s<t ) for each sentence s. |s| TF-IDF Scores (TF-IDF). TF-IDF score is another criterion for data selection (Kirchhoff and Bilmes, 2014). For each sentence s, one can compute its term frequency and inverse document frequency for each word. We can thus obtain the TF-IDF vector and calculate the cosine similarity between the TF-IDF vectors of s and each sentence sin in a small in-domain dataset, and treat the maximum value as its representativeness score. 5896 BERT Representation Similarities (BERT). BERT (Devlin et al., 2019) has proven to be effective for sentence representation learning. Following the conclusion of Pires et al. (2019), we feed each sentence to the multilingual BERT model and average the hidden states for all"
2020.emnlp-main.475,P10-2041,0,0.410281,"used approach and generalize it to a dynamic curriculum learning strategy, which is applied to iterative back-translation models. In addition, we propose weighting strategies based on both the current quality of the sentence and its improvement over the previous iteration. We evaluate our models on domain adaptation, low-resource, and high-resource MT settings and on two language pairs. Experimental results demonstrate that our methods achieve improvements of up to 1.8 BLEU points over competitive baselines. 1 1 Moore-Lewis Target Domain (Monolingual) Ours next epoch next epoch Figure 1: The Moore and Lewis (2010) data selection strategy for domain adaptation constantly selects the same set of sentences which cannot well represent the target domain. Our approach, instead, selects different subsets of sentences at each epoch and we gradually shift from selecting samples from the general-domain distribution to samples from the target distribution. Introduction Back-translation (Sennrich et al., 2016b) is an effective strategy for improving the performance of neural machine translation (NMT) using monolingual data, delivering impressive gains over already competitive NMT models (Edunov et al., 2018). The"
2020.emnlp-main.475,N19-4007,1,0.801237,"Missing"
2020.emnlp-main.475,W18-2710,0,0.0189248,"translation baseline by up to 0.3 BLEU points. Data weighting strategies do not always help, probably because in high-resource settings the back-translated data is already of high quality. In the best case scenario, our method outperforms iterative back-translation by 0.6 BLEU points. 6 Related Work Back-translation (Sennrich et al., 2016a) has proven to be effective and several extensions of it have been proposed (He et al., 2016; Cheng 5901 et al., 2016; Zhang and Zong, 2016; Xia et al., 2019), among which iterative back-translation methods (Cotterell and Kreutzer, 2018; Hoang et al., 2018; Niu et al., 2018; Zheng et al., 2020) have demonstrated strong empirical performance. For domain adaptation, Moore and Lewis (2010) and Kirchhoff and Bilmes (2014) use language model cross entropy differences and TF-IDF to select data that are similar to in-domain text respectively. van der Wees et al. (2017) propose dynamic data selection strategies for machine translation models, and Zhang et al. (2019) extend the idea to curriculum strategies. As for filtering noisy sentences, Junczys-Dowmunt (2018) propose to utilize the agreement between forward and backward translation models and Wang et al. (2019a) pro"
2020.emnlp-main.475,P02-1040,0,0.108023,"iteratively for both directions, with λ increased at each training epoch. 5897 Method WMT LAW MED de-en en-de de-en en-de Baseline Base 31.25 24.44 Back 35.90 26.33 Ite-Back 37.69 27.81 Zhang et al. (2019) 37.70 27.87 Best Selection TF-IDF 38.26* 28.35* Best Curriculum TF-IDF + R-BLEU 39.11* 28.93* Best Weighting Enc 38.20* 28.15* Enc-Imp 38.13* 27.97 Best Curriculum + Best Weighting Curri+Enc 38.87 29.04 Curri+Enc-Imp 38.75 28.89 34.43 42.42 44.08 44.25 26.59 33.98 35.65 36.01 44.26 35.82 44.91* 36.19* 44.28* 44.46* 35.52 35.77 45.46* 45.46* 36.34 36.45* Table 1: Translation accuracy (BLEU (Papineni et al., 2002)) in the domain adaptation setting. The first and second rows list source and target domains respectively. The third row lists the translation directions. We report the best-performing models of only using selection strategies (“Best Selection”), only using curriculum strategies (“Best Curriculum”), only using weighting strategies (“Best Weighting” ) and using both the best curriculum and weighting strategies (“Best Weighting + Best Weighting” ). “Enc-Imp” indicates both the encoder representation similarities and the quality improvement metrics are used for weighting. The highest scores are i"
2020.emnlp-main.475,P19-1493,0,0.0246279,"s. |s| TF-IDF Scores (TF-IDF). TF-IDF score is another criterion for data selection (Kirchhoff and Bilmes, 2014). For each sentence s, one can compute its term frequency and inverse document frequency for each word. We can thus obtain the TF-IDF vector and calculate the cosine similarity between the TF-IDF vectors of s and each sentence sin in a small in-domain dataset, and treat the maximum value as its representativeness score. 5896 BERT Representation Similarities (BERT). BERT (Devlin et al., 2019) has proven to be effective for sentence representation learning. Following the conclusion of Pires et al. (2019), we feed each sentence to the multilingual BERT model and average the hidden states for all the input tokens except [CLS] and [SEP] at the eighth layer to obtain the sentence representation. We then compute the cosine similarity between representations of sentence s in the monolingual corpus and each sentence sin in a small in-domain set, and the maximum value is treated as the representativeness score. 3.1.4 Simplicity Metrics In our experiments, we study two metrics for measuring the simplicity of sentences. Note that in the field of quality estimation for MT (Specia et al., 2010; Fonseca e"
2020.emnlp-main.475,N19-1119,1,0.835906,"nsure the quality of the back-translated data. As the training progresses, the model will become better at translating in-domain sentences, and we will then shift to choosing more representative examples. Formally, at each epoch t, we rank the corpus according to score(s) = λ(t)repr(s) + (1 − λ(t))simp(s), (2) where repr(s) and simp(s) denote the representativeness and simplicity of sentence s respectively, which will be dicussed in the following sections. The term λ(t) balances between the two criteria and is a function of the current epoch t. We adopt the square-root growing function for λ (Platanios et al., 2019) and set r 1 − c20 λ(t) = min(1, t + c20 ), (3) T where c0 is the initial value and T denotes the time after which we solely select representative samples. λ increases relatively quickly at first and then its acceleration will be gradually decreased as the training progresses, which is suitable for our task as at first the sentences are relatively simple and thus we will not need much time on those sentences. Connections to Moore and Lewis (2010). Our proposed criteria generalize Moore and Lewis (2010). The first term of Equation 1, namely HLMin (s), measures the representativeness of data bec"
2020.emnlp-main.475,W15-3041,0,0.0216259,"r to obtain the sentence representation. We then compute the cosine similarity between representations of sentence s in the monolingual corpus and each sentence sin in a small in-domain set, and the maximum value is treated as the representativeness score. 3.1.4 Simplicity Metrics In our experiments, we study two metrics for measuring the simplicity of sentences. Note that in the field of quality estimation for MT (Specia et al., 2010; Fonseca et al., 2019), researchers have proposed several existing techniques to estimate the simplicity of sentences (Turchi et al., 2014; Specia et al., 2015; Shah et al., 2015; Kim and Lee, 2016; Kepler et al., 2019; Zhou et al., 2019; Hou et al., 2019), and here we select a few representative approaches. General-Domain Language Model CrossEntropy (LM-gen). We train a language model LMgen with the one side of the parallel training data DF E . Then, for each sentence s we compute 1 P|s| the score |s| t=1 log PLMgen (st |s<t ). Round-Trip BLEU (R-BLEU). Given two pretrained MT models MF E and MEF , round-trip translation first translates a sentence s into another language using MF E and then back-translates the result using MEF , obtaining the reconstructed sentence"
2020.emnlp-main.475,P15-4020,0,0.0293466,"P] at the eighth layer to obtain the sentence representation. We then compute the cosine similarity between representations of sentence s in the monolingual corpus and each sentence sin in a small in-domain set, and the maximum value is treated as the representativeness score. 3.1.4 Simplicity Metrics In our experiments, we study two metrics for measuring the simplicity of sentences. Note that in the field of quality estimation for MT (Specia et al., 2010; Fonseca et al., 2019), researchers have proposed several existing techniques to estimate the simplicity of sentences (Turchi et al., 2014; Specia et al., 2015; Shah et al., 2015; Kim and Lee, 2016; Kepler et al., 2019; Zhou et al., 2019; Hou et al., 2019), and here we select a few representative approaches. General-Domain Language Model CrossEntropy (LM-gen). We train a language model LMgen with the one side of the parallel training data DF E . Then, for each sentence s we compute 1 P|s| the score |s| t=1 log PLMgen (st |s<t ). Round-Trip BLEU (R-BLEU). Given two pretrained MT models MF E and MEF , round-trip translation first translates a sentence s into another language using MF E and then back-translates the result using MEF , obtaining the reco"
2020.emnlp-main.475,tiedemann-2012-parallel,0,0.121885,"larities and the quality improvement metrics are used for weighting. The highest scores are in bold and ∗ indicates statistical significance compared with the best baseline (p < 0.05). 4 Experiments on Domain Adaptation We first conduct experiments in the domain adaptation setting, where we adapt models from a general domain to a specific domain. 4.1 Setup Datasets. We first train the translation models with (general-domain) WMT-14 German-English dataset, consisting of about 4.5M training sentences, then perform iterative back-translation with (in-domain) law or medical OPUS monolingual data (Tiedemann, 2012). We de-duplicate the law and medical parallel training data, divide them into two halves and obtain 250K and 200K comparable yet non-parallel sentences respectively in both languages to obtain the monolingual corpora. The development and test sets contain 2K sentences in each domain. Byte-pair encoding (Sennrich et al., 2016b) is applied with 32K merge operations. The general-domain and in-domain language models Method Baseline Ite-Back Selection BERT LM-diff LM-in TF-IDF Weighting Enc Enc-Imp Agree Agree-Imp Curriculum LM-in+ LM-gen TF-IDF + LM-gen TF-IDF + R-BLEU WMT LAW MED de-en en-de de-"
2020.emnlp-main.475,P14-1067,1,0.881884,"Missing"
2020.emnlp-main.475,D19-1073,0,0.0414324,"2018; Niu et al., 2018; Zheng et al., 2020) have demonstrated strong empirical performance. For domain adaptation, Moore and Lewis (2010) and Kirchhoff and Bilmes (2014) use language model cross entropy differences and TF-IDF to select data that are similar to in-domain text respectively. van der Wees et al. (2017) propose dynamic data selection strategies for machine translation models, and Zhang et al. (2019) extend the idea to curriculum strategies. As for filtering noisy sentences, Junczys-Dowmunt (2018) propose to utilize the agreement between forward and backward translation models and Wang et al. (2019a) propose uncertainty-based confidence estimation to improve back-translation. Wang et al. (2019b) compose dynamic domain-data selection with dynamic clean-data selection. Our methods generalize previous data selection strategies and our primary focus is to improve iterative back-translation, but our work could be extended to also include training-time dynamic data selection approaches such as the technique of Wang et al. (2020). 7 Conclusion In this paper, we provide a novel insight into a widely-used data selection method (Moore and Lewis, 2010) and generalize it to a curriculum strategy fo"
2020.emnlp-main.475,P19-1123,0,0.0609925,"2018; Niu et al., 2018; Zheng et al., 2020) have demonstrated strong empirical performance. For domain adaptation, Moore and Lewis (2010) and Kirchhoff and Bilmes (2014) use language model cross entropy differences and TF-IDF to select data that are similar to in-domain text respectively. van der Wees et al. (2017) propose dynamic data selection strategies for machine translation models, and Zhang et al. (2019) extend the idea to curriculum strategies. As for filtering noisy sentences, Junczys-Dowmunt (2018) propose to utilize the agreement between forward and backward translation models and Wang et al. (2019a) propose uncertainty-based confidence estimation to improve back-translation. Wang et al. (2019b) compose dynamic domain-data selection with dynamic clean-data selection. Our methods generalize previous data selection strategies and our primary focus is to improve iterative back-translation, but our work could be extended to also include training-time dynamic data selection approaches such as the technique of Wang et al. (2020). 7 Conclusion In this paper, we provide a novel insight into a widely-used data selection method (Moore and Lewis, 2010) and generalize it to a curriculum strategy fo"
2020.emnlp-main.475,2020.acl-main.754,1,0.804561,"to curriculum strategies. As for filtering noisy sentences, Junczys-Dowmunt (2018) propose to utilize the agreement between forward and backward translation models and Wang et al. (2019a) propose uncertainty-based confidence estimation to improve back-translation. Wang et al. (2019b) compose dynamic domain-data selection with dynamic clean-data selection. Our methods generalize previous data selection strategies and our primary focus is to improve iterative back-translation, but our work could be extended to also include training-time dynamic data selection approaches such as the technique of Wang et al. (2020). 7 Conclusion In this paper, we provide a novel insight into a widely-used data selection method (Moore and Lewis, 2010) and generalize it to a curriculum strategy for iterative back-translation. We also propose data weighting methods to down-weight examples of poor quality. Extensive experiments are performed to evaluate the performance of our methods; analyses reveal the selected samples can represent the target domain well and our weighting strategies benefit noisy settings the most. Acknowledgements The authors are grateful to the anonymous reviewers for their constructive comments, and t"
2020.emnlp-main.475,D17-1147,0,0.0580682,"Missing"
2020.emnlp-main.475,P16-1009,0,0.666104,"xperimental results demonstrate that our methods achieve improvements of up to 1.8 BLEU points over competitive baselines. 1 1 Moore-Lewis Target Domain (Monolingual) Ours next epoch next epoch Figure 1: The Moore and Lewis (2010) data selection strategy for domain adaptation constantly selects the same set of sentences which cannot well represent the target domain. Our approach, instead, selects different subsets of sentences at each epoch and we gradually shift from selecting samples from the general-domain distribution to samples from the target distribution. Introduction Back-translation (Sennrich et al., 2016b) is an effective strategy for improving the performance of neural machine translation (NMT) using monolingual data, delivering impressive gains over already competitive NMT models (Edunov et al., 2018). The strategy is simple: given monolingual data in the target language, one can use a translation model in the opposite of the desired translation direction to back-translate the monolingual data, effectively synthesizing a parallel dataset, which is in turn †: Work completed while at Carnegie Mellon University. Code: https://github.com/zdou0830/ dynamic_select_weight. 1 General Domain (Monoli"
2020.emnlp-main.475,P19-1579,1,0.796958,"BLEU points. High-Resource Settings. In high-resource settings, our curriculum strategies improve the iterative back-translation baseline by up to 0.3 BLEU points. Data weighting strategies do not always help, probably because in high-resource settings the back-translated data is already of high quality. In the best case scenario, our method outperforms iterative back-translation by 0.6 BLEU points. 6 Related Work Back-translation (Sennrich et al., 2016a) has proven to be effective and several extensions of it have been proposed (He et al., 2016; Cheng 5901 et al., 2016; Zhang and Zong, 2016; Xia et al., 2019), among which iterative back-translation methods (Cotterell and Kreutzer, 2018; Hoang et al., 2018; Niu et al., 2018; Zheng et al., 2020) have demonstrated strong empirical performance. For domain adaptation, Moore and Lewis (2010) and Kirchhoff and Bilmes (2014) use language model cross entropy differences and TF-IDF to select data that are similar to in-domain text respectively. van der Wees et al. (2017) propose dynamic data selection strategies for machine translation models, and Zhang et al. (2019) extend the idea to curriculum strategies. As for filtering noisy sentences, Junczys-Dowmunt"
2020.emnlp-main.475,P16-1162,0,0.839602,"xperimental results demonstrate that our methods achieve improvements of up to 1.8 BLEU points over competitive baselines. 1 1 Moore-Lewis Target Domain (Monolingual) Ours next epoch next epoch Figure 1: The Moore and Lewis (2010) data selection strategy for domain adaptation constantly selects the same set of sentences which cannot well represent the target domain. Our approach, instead, selects different subsets of sentences at each epoch and we gradually shift from selecting samples from the general-domain distribution to samples from the target distribution. Introduction Back-translation (Sennrich et al., 2016b) is an effective strategy for improving the performance of neural machine translation (NMT) using monolingual data, delivering impressive gains over already competitive NMT models (Edunov et al., 2018). The strategy is simple: given monolingual data in the target language, one can use a translation model in the opposite of the desired translation direction to back-translate the monolingual data, effectively synthesizing a parallel dataset, which is in turn †: Work completed while at Carnegie Mellon University. Code: https://github.com/zdou0830/ dynamic_select_weight. 1 General Domain (Monoli"
2020.emnlp-main.475,D16-1160,0,0.0406354,"ck-translation by 1.8 BLEU points. High-Resource Settings. In high-resource settings, our curriculum strategies improve the iterative back-translation baseline by up to 0.3 BLEU points. Data weighting strategies do not always help, probably because in high-resource settings the back-translated data is already of high quality. In the best case scenario, our method outperforms iterative back-translation by 0.6 BLEU points. 6 Related Work Back-translation (Sennrich et al., 2016a) has proven to be effective and several extensions of it have been proposed (He et al., 2016; Cheng 5901 et al., 2016; Zhang and Zong, 2016; Xia et al., 2019), among which iterative back-translation methods (Cotterell and Kreutzer, 2018; Hoang et al., 2018; Niu et al., 2018; Zheng et al., 2020) have demonstrated strong empirical performance. For domain adaptation, Moore and Lewis (2010) and Kirchhoff and Bilmes (2014) use language model cross entropy differences and TF-IDF to select data that are similar to in-domain text respectively. van der Wees et al. (2017) propose dynamic data selection strategies for machine translation models, and Zhang et al. (2019) extend the idea to curriculum strategies. As for filtering noisy sentenc"
2020.emnlp-main.475,N19-1189,0,0.206999,"ack-Translation. The iterative backtranslation method is rather competitive, as it improves over the unadapted baseline by 9.6 BLEU and simple back-translation by 1.8 BLEU points. Selection Strategies. We can see from the table that the best-performing selection strategies, namely selecting sentences with high TF-IDF scores, is generally effective and can improve the baseline by about 0.5 BLEU points. Curriculum and Weighting Strategies. Both our curriculum and weighting strategies outperform the unadapted and the iterative back-translation models, as well as the curriculum method proposed in Zhang et al. (2019), with our curriculum learning method achieving better performance and improving the strong iterative back-translation baseline by 1.1 BLEU points. Combining curriculum and weighting methods can further improve the performance by up to 0.5 BLEU points, demonstrating the two strategies are complementary to each other. 4.3 law de-en law en-de medical de-en medical en-de 32.5 Choices of Metrics We examine different choices of representativeness and simplicity metrics. The performance of different models is listed in Table 2. Representativeness Metrics. All data selection strategies outperform the"
2020.emnlp-main.475,2020.wmt-1.63,0,0.0770528,"(D0F , DE ). Last, we concatenate back-translated data (D0F , DE ) with the original parallel corpus DF E to train a source-to-target model MF E . The success of back-translation has motivated reAlgorithm 1 Iterative Back-Translation Input: Monolingual corpora DF and DE Output: Translation models MF E and MEF while MF E and MEF have not converged do for all batches (BF , BE ) in (DF , DE ) do Translate BF into B0E using MF E Translate BE into B0F using MEF Train MF E with (B0F , BE ) Train MEF with (B0E , BF ) end for end while searchers to investigate and extend the method (He et al., 2016; Zheng et al., 2020). Hoang et al. (2018) propose to use iterative back-translation and achieve improvements over previous state-of-theart models. As shown in Algorithm 1, at each training step, a batch of monolingual sentences is sampled from one language and back-translated to the other language. The back-translated data is utilized to train the model in the other direction. The process is repeated in both directions. 3 Methods In our setting, we are given two MT models MF E and MEF pretrained on parallel data DF E , and both source and target monolingual corpora DF and DE . The goal is to select and weight sam"
2020.emnlp-main.475,W19-5411,0,0.0245378,"e cosine similarity between representations of sentence s in the monolingual corpus and each sentence sin in a small in-domain set, and the maximum value is treated as the representativeness score. 3.1.4 Simplicity Metrics In our experiments, we study two metrics for measuring the simplicity of sentences. Note that in the field of quality estimation for MT (Specia et al., 2010; Fonseca et al., 2019), researchers have proposed several existing techniques to estimate the simplicity of sentences (Turchi et al., 2014; Specia et al., 2015; Shah et al., 2015; Kim and Lee, 2016; Kepler et al., 2019; Zhou et al., 2019; Hou et al., 2019), and here we select a few representative approaches. General-Domain Language Model CrossEntropy (LM-gen). We train a language model LMgen with the one side of the parallel training data DF E . Then, for each sentence s we compute 1 P|s| the score |s| t=1 log PLMgen (st |s<t ). Round-Trip BLEU (R-BLEU). Given two pretrained MT models MF E and MEF , round-trip translation first translates a sentence s into another language using MF E and then back-translates the result using MEF , obtaining the reconstructed sentence s0 . The BLEU score between s and s0 is treated as our simp"
2020.emnlp-main.478,I17-2007,0,0.0530644,"Missing"
2020.emnlp-main.478,C18-1214,1,0.889388,"Missing"
2020.emnlp-main.478,D19-1091,1,0.881342,"Missing"
2020.emnlp-main.478,P13-1021,0,0.0252568,"of three pages each. For the Yakkha documents, we divide at the paragraph-level, due to the small size of the dataset. We have 33 paragraphs across the three books in our dataset, resulting in 11 segments that contain three paragraphs each. The multi-source results for Yakkha reported in Table 2 use the English translations. Results with Nepali are similar and are included in Appendix A. Metrics We use two metrics for evaluating our systems: character error rate (CER) and word error rate (WER). Both metrics are based on edit distance and are standard for evaluating OCR and OCR postcorrection (Berg-Kirkpatrick et al., 2013; Schulz and Kuhn, 2017). CER is the edit distance between the predicted and the gold transcriptions of the document, divided by the total number of characters in the gold transcription. WER is similar but is calculated at the word level. Methods In our experiments, we compare the performance of our proposed method with the first pass OCR and with two systems from recent work in OCR post-correction. All the post-correction methods have two variants – the single-source model with only the endangered language encoder and the multi-source model that additionally uses the high-resource translation"
2020.emnlp-main.478,2020.lrec-1.356,0,0.0424316,"no and Igbo, and Krishna et al. (2018) show improvements on Romanized Sanksrit OCR by adding a copy mechanism to a neural sequence-to-sequence model. Multi-source encoder-decoder models have been used for various tasks including machine translation (Zoph and Knight, 2016; Libovick´y and Helcl, 2017) and morphological inflection (Kann et al., 2017; Anastasopoulos and Neubig, 2019). Perhaps most relevant to our work is the multi-source model presented by Anastasopoulos and Chiang (2018), which uses high-resource translations to improve speech transcription of lower-resourced languages. Finally, Bustamante et al. (2020) construct corpora for four endangered languages from textbased PDFs using rule-based heuristics. Data creation from such unstructured text files is an important research direction, complementing our method of extracting data from scanned images. 8 Conclusion This work presents a first step towards extracting textual data in endangered languages from scanned images of paper books. We create a benchmark dataset with transcribed images in three endangered languages: Ainu, Griko, and Yakkha. We propose an OCR post-correction method that facilitates learning from small amounts of data, which resul"
2020.emnlp-main.478,P18-1220,0,0.224026,"tual data and transcribed images needed to train state-of-the-art OCR models from scratch are unavailable in the endangered language setting. Instead, we focus on post-correcting the output of an off-the-shelf OCR tool that can handle a variety of scripts. We show that targeted methods for post-correction can significantly improve performance on endangered languages. Although OCR post-correction is relatively wellstudied, most existing methods rely on considerable resources in the target language, including a substantial amount of textual data to train a language model (Schnober et al., 2016; Dong and Smith, 2018; Rigaud et al., 2019) or to create synthetic data (Krishna et al., 2018). While readily available for high-resource languages, these resources are severely limited in endangered languages, preventing the direct application of existing post-correction methods in our setting. As an alternative, we present a method that compounds on previous models for OCR postcorrection, making three improvements tailored to the data-scarce setting. First, we use a multisource model to incorporate information from the high-resource translations that commonly appear in endangered language books. These translatio"
2020.emnlp-main.478,2020.acl-main.560,0,0.0369974,"Missing"
2020.emnlp-main.478,E17-1049,0,0.0353087,"Missing"
2020.emnlp-main.478,H05-1109,0,0.0680367,"7 Related Work Post-correction for OCR is well-studied for highresource languages. Early approaches include lexical methods and weighted finite-state methods (see Schulz and Kuhn (2017) for an overview). Recent work has primarily focused on using neural sequence-to-sequence models. H¨am¨al¨ainen and Hengchen (2019) use a BiLSTM encoder-decoder with attention for historical English post-correction. Similar to our base model, Dong and Smith (2018) use a multi-source model to combine the first pass OCR from duplicate documents in English. There has been little work on lower-resourced languages. Kolak and Resnik (2005) present a probabilistic edit distance based post-correction model applied to Cebuano and Igbo, and Krishna et al. (2018) show improvements on Romanized Sanksrit OCR by adding a copy mechanism to a neural sequence-to-sequence model. Multi-source encoder-decoder models have been used for various tasks including machine translation (Zoph and Knight, 2016; Libovick´y and Helcl, 2017) and morphological inflection (Kann et al., 2017; Anastasopoulos and Neubig, 2019). Perhaps most relevant to our work is the multi-source model presented by Anastasopoulos and Chiang (2018), which uses high-resource t"
2020.emnlp-main.478,K18-1034,0,0.325795,"dels from scratch are unavailable in the endangered language setting. Instead, we focus on post-correcting the output of an off-the-shelf OCR tool that can handle a variety of scripts. We show that targeted methods for post-correction can significantly improve performance on endangered languages. Although OCR post-correction is relatively wellstudied, most existing methods rely on considerable resources in the target language, including a substantial amount of textual data to train a language model (Schnober et al., 2016; Dong and Smith, 2018; Rigaud et al., 2019) or to create synthetic data (Krishna et al., 2018). While readily available for high-resource languages, these resources are severely limited in endangered languages, preventing the direct application of existing post-correction methods in our setting. As an alternative, we present a method that compounds on previous models for OCR postcorrection, making three improvements tailored to the data-scarce setting. First, we use a multisource model to incorporate information from the high-resource translations that commonly appear in endangered language books. These translations are usually in the lingua franca of the region (e.g., Figure 1 (a,b,c)"
2020.emnlp-main.478,2013.mtsummit-papers.10,0,0.0301391,"rst pass OCR and the gold transcription. The operations of each type (insertion, deletion, and replacement) are counted for each character and divided by the number of times that character appears in the first pass OCR. This forms a probability of how often the operation should be applied to that specific character. We use these probabilities to form rules, such as p(replace d with d.) = 0.4 for Griko and p(replace ? with P) = 0.7 for Yakkha. These rules are applied to remove errors from, or “denoise”, the first pass OCR transcription. • Sentence alignment We use Yet Another Sentence Aligner (Lamraoui and Langlais, 2013) for unsupervised alignment of the endangered language and translation on the unannotated pages. Given the aligned first pass OCR for the endangered language text and the translation along with the pseudo-target text, x, t and y ˆ respectively, the pretraining steps, in order, are: • Pretraining the encoders We pretrain both the forward and backward LSTMs of each encoder with a character-level language model objective: the endangered language encoder on x and the translation encoder on t. x x ek,i = v tanh (W1 sk−1 + W2 hi + wg gk,i ) gk is also used to penalize attending to the same locations"
2020.emnlp-main.478,P17-2031,0,0.0297408,"Missing"
2020.emnlp-main.478,D16-1096,0,0.0119481,"rather than generate a character at each time step might lead to better performance (Gu et al., 2016). We incorporate the copy mechanism proposed in See et al. (2017). The mechanism computes a “generation probability” at each time step k, which is used to choose between generating a character (Equation 3) or copying a character from the source x text by sampling from the attention distribution αk . Coverage Given the monotonicity of the postcorrection task, the model should not attend to the same character repeatedly. However, repetition is a common problem for neural encoder-decoder models (Mi et al., 2016; Tu et al., 2016). To account for this problem, we adapt the coverage mechanism from See et al. (2017), which keeps track of the attention distribution over past time steps in a coverage vector. For time step k, the coverage vector k−1 x would be gk = ∑k′ =0 αk′ . gk is used as an extra input to the attention mechanism, ensuring that future attention decisions take the weights from previous time steps into account. Equation 1, with learnable parameter wg , becomes: x x x the edit distance operations between the first pass OCR and the gold transcription. The operations of each type (insertion,"
2020.emnlp-main.478,P16-1008,0,0.0123394,"rate a character at each time step might lead to better performance (Gu et al., 2016). We incorporate the copy mechanism proposed in See et al. (2017). The mechanism computes a “generation probability” at each time step k, which is used to choose between generating a character (Equation 3) or copying a character from the source x text by sampling from the attention distribution αk . Coverage Given the monotonicity of the postcorrection task, the model should not attend to the same character repeatedly. However, repetition is a common problem for neural encoder-decoder models (Mi et al., 2016; Tu et al., 2016). To account for this problem, we adapt the coverage mechanism from See et al. (2017), which keeps track of the attention distribution over past time steps in a coverage vector. For time step k, the coverage vector k−1 x would be gk = ∑k′ =0 αk′ . gk is used as an extra input to the attention mechanism, ensuring that future attention decisions take the weights from previous time steps into account. Equation 1, with learnable parameter wg , becomes: x x x the edit distance operations between the first pass OCR and the gold transcription. The operations of each type (insertion, deletion, and rep"
2020.emnlp-main.478,N16-1004,0,0.0651717,"ari script languages (such as Hindi). 5 OCR Post-Correction Model In this section, we describe our proposed OCR post-correction model. The base architecture of the model is a multi-source sequence-to-sequence 5934 P (y1 . . . yK ) where sk−1 is the decoder state of the previous time x x x step and v , W1 and W2 are trainable parameters. x The encoder hidden states h are weighted by the x attention distribution αk to produce the context x vector ck . We follow a similar procedure for the t second encoder to produce ck . We concatenate the context vectors to combine attention from both sources (Zoph and Knight, 2016): softmax s1 . . . sK decoder c1 . . . cK attention x h1 attention x . . . hN t h1 t . . . hM encoder encoder x1 . . . xN x OCR ck is used by the decoder LSTM to compute the next hidden state sk and subsequently, the probability distribution for predicting the next character yk of the target sequence y: OCR Japanese Ainu Figure 3: The proposed multi-source architecture with the encoder for an endangered language segment (left) and an encoder for the translated segment (right). The input to the encoders is the first pass OCR over the scanned images of each segment. For example, the OCR on the s"
2020.emnlp-main.478,C16-1160,0,0.165497,"he large amounts of textual data and transcribed images needed to train state-of-the-art OCR models from scratch are unavailable in the endangered language setting. Instead, we focus on post-correcting the output of an off-the-shelf OCR tool that can handle a variety of scripts. We show that targeted methods for post-correction can significantly improve performance on endangered languages. Although OCR post-correction is relatively wellstudied, most existing methods rely on considerable resources in the target language, including a substantial amount of textual data to train a language model (Schnober et al., 2016; Dong and Smith, 2018; Rigaud et al., 2019) or to create synthetic data (Krishna et al., 2018). While readily available for high-resource languages, these resources are severely limited in endangered languages, preventing the direct application of existing post-correction methods in our setting. As an alternative, we present a method that compounds on previous models for OCR postcorrection, making three improvements tailored to the data-scarce setting. First, we use a multisource model to incorporate information from the high-resource translations that commonly appear in endangered language b"
2020.emnlp-main.478,D17-1288,0,0.160209,"kkha documents, we divide at the paragraph-level, due to the small size of the dataset. We have 33 paragraphs across the three books in our dataset, resulting in 11 segments that contain three paragraphs each. The multi-source results for Yakkha reported in Table 2 use the English translations. Results with Nepali are similar and are included in Appendix A. Metrics We use two metrics for evaluating our systems: character error rate (CER) and word error rate (WER). Both metrics are based on edit distance and are standard for evaluating OCR and OCR postcorrection (Berg-Kirkpatrick et al., 2013; Schulz and Kuhn, 2017). CER is the edit distance between the predicted and the gold transcriptions of the document, divided by the total number of characters in the gold transcription. WER is similar but is calculated at the word level. Methods In our experiments, we compare the performance of our proposed method with the first pass OCR and with two systems from recent work in OCR post-correction. All the post-correction methods have two variants – the single-source model with only the endangered language encoder and the multi-source model that additionally uses the high-resource translation encoder. • F P -O CR: T"
2020.emnlp-main.478,P17-1099,0,0.0233985,"ower values. The diagonal loss summed over all time steps for a training instance, where N is the length of x, is: x αk = softmax (ek ) x x t ck = [ck ; ck ] t1 . . . tM Ldiag x ck = [Σi αk,i hi ] 5935 N ⎛k−j x x ⎞ = ∑ ⎜ ∑ αk,i + ∑ αk,i ⎟ ⎠ k ⎝ i=1 i=k+j Copy mechanism Table 1 indicates that the first pass OCR predicts a majority of the characters accurately. In this scenario, enabling the model to directly copy characters from the first pass OCR rather than generate a character at each time step might lead to better performance (Gu et al., 2016). We incorporate the copy mechanism proposed in See et al. (2017). The mechanism computes a “generation probability” at each time step k, which is used to choose between generating a character (Equation 3) or copying a character from the source x text by sampling from the attention distribution αk . Coverage Given the monotonicity of the postcorrection task, the model should not attend to the same character repeatedly. However, repetition is a common problem for neural encoder-decoder models (Mi et al., 2016; Tu et al., 2016). To account for this problem, we adapt the coverage mechanism from See et al. (2017), which keeps track of the attention distribution"
2020.emnlp-main.479,D19-1091,1,0.855802,"Missing"
2020.emnlp-main.479,2020.acl-main.421,0,0.0204559,"even outperforming competitive question answering systems relying on external resources (Roberts et al., 2020). Petroni et al. (2020) further shows that LMs can generate even more factual knowledge when augmented with retrieved sentences. Our work builds on these works by expanding to multilingual and multi-token evaluation, and also demonstrates the significant challenges posed by this setting. Multilingual Benchmarks Many multilingual benchmarks have been created to evaluate the performance of multilingual systems on different natural language processing tasks, including question answering (Artetxe et al., 2020; Lewis et al., 2019; Clark et al., 2020), natural language understanding (Conneau et al., 2018; Yang et al., 2019a; Zweigenbaum et al., 2018; Artetxe and Schwenk, 2019), syntactic prediction (Nivre et al., 2018; Pan et al., 2017), and comprehensive benchmarks covering multiple tasks (Hu et al., 2020; Liang et al., 2020). We focus on multilingual factual knowledge retrieval from LMs, which to our knowledge has not been covered by any previous work. 8 Conclusion We examine the intersection of multilinguality and the factual knowledge included in LMs by creating a multilingual and multi-token be"
2020.emnlp-main.479,Q19-1038,0,0.0247568,"nerate even more factual knowledge when augmented with retrieved sentences. Our work builds on these works by expanding to multilingual and multi-token evaluation, and also demonstrates the significant challenges posed by this setting. Multilingual Benchmarks Many multilingual benchmarks have been created to evaluate the performance of multilingual systems on different natural language processing tasks, including question answering (Artetxe et al., 2020; Lewis et al., 2019; Clark et al., 2020), natural language understanding (Conneau et al., 2018; Yang et al., 2019a; Zweigenbaum et al., 2018; Artetxe and Schwenk, 2019), syntactic prediction (Nivre et al., 2018; Pan et al., 2017), and comprehensive benchmarks covering multiple tasks (Hu et al., 2020; Liang et al., 2020). We focus on multilingual factual knowledge retrieval from LMs, which to our knowledge has not been covered by any previous work. 8 Conclusion We examine the intersection of multilinguality and the factual knowledge included in LMs by creating a multilingual and multi-token benchmark XFACTR, and performing experiments comparing and contrasting across languages and LMs. The results demonstrate the difficulty of this task, and that knowledge co"
2020.emnlp-main.479,A88-1019,0,0.130845,"prediction #tokens confidence 2012 1 -1.90 Nueva York 2 -0.61 es outputs EE. UU 3 -1.82 Chicago, Estados Unidos 4 -3.58 2012 Bloomberg L.P 5 -3.06 Figure 1: X-FACTR contains 23 languages, for which the data availability varies dramatically. Prompts get instantiated to produce grammatical sentences with different numbers of mask tokens and are used to obtain predictions for [Y]. In this Spanish example, the verb gerund “fundar” to found is rendered as “fundada” to agree in gender and number with the subject “Bloomberg L.P.”. The final prediction is in bold. Introduction Language models (LMs; (Church, 1988; Kneser and Ney, 1995; Bengio et al., 2003)) learn to model the probability distribution of text, and in doing so capture information about various aspects of the syntax or semantics of the language at hand. Recent works have presented intriguing results demonstrating that modern large-scale LMs also capture a significant amount of factual knowledge (Petroni et al., 2019; Jiang et al., 2020; Poerner et al., 2019). This knowledge is generally probed by having the LM fill in the blanks of cloze-style prompts such as ∗: Work done at Carnegie Mellon University. The first two authors contributed e"
2020.emnlp-main.479,2020.tacl-1.30,0,0.0233529,"swering systems relying on external resources (Roberts et al., 2020). Petroni et al. (2020) further shows that LMs can generate even more factual knowledge when augmented with retrieved sentences. Our work builds on these works by expanding to multilingual and multi-token evaluation, and also demonstrates the significant challenges posed by this setting. Multilingual Benchmarks Many multilingual benchmarks have been created to evaluate the performance of multilingual systems on different natural language processing tasks, including question answering (Artetxe et al., 2020; Lewis et al., 2019; Clark et al., 2020), natural language understanding (Conneau et al., 2018; Yang et al., 2019a; Zweigenbaum et al., 2018; Artetxe and Schwenk, 2019), syntactic prediction (Nivre et al., 2018; Pan et al., 2017), and comprehensive benchmarks covering multiple tasks (Hu et al., 2020; Liang et al., 2020). We focus on multilingual factual knowledge retrieval from LMs, which to our knowledge has not been covered by any previous work. 8 Conclusion We examine the intersection of multilinguality and the factual knowledge included in LMs by creating a multilingual and multi-token benchmark XFACTR, and performing experiment"
2020.emnlp-main.479,D18-1269,0,0.0258777,"s et al., 2020). Petroni et al. (2020) further shows that LMs can generate even more factual knowledge when augmented with retrieved sentences. Our work builds on these works by expanding to multilingual and multi-token evaluation, and also demonstrates the significant challenges posed by this setting. Multilingual Benchmarks Many multilingual benchmarks have been created to evaluate the performance of multilingual systems on different natural language processing tasks, including question answering (Artetxe et al., 2020; Lewis et al., 2019; Clark et al., 2020), natural language understanding (Conneau et al., 2018; Yang et al., 2019a; Zweigenbaum et al., 2018; Artetxe and Schwenk, 2019), syntactic prediction (Nivre et al., 2018; Pan et al., 2017), and comprehensive benchmarks covering multiple tasks (Hu et al., 2020; Liang et al., 2020). We focus on multilingual factual knowledge retrieval from LMs, which to our knowledge has not been covered by any previous work. 8 Conclusion We examine the intersection of multilinguality and the factual knowledge included in LMs by creating a multilingual and multi-token benchmark XFACTR, and performing experiments comparing and contrasting across languages and LMs."
2020.emnlp-main.479,N19-1423,0,0.573261,"uage-related criteria (e.g., require the entities to have translations in all languages we considered). As a result, some entities (either subjects or objects) might not have translations in all languages. The number of facts in different languages in our multilingual multi-token X-FACTR benchmark is shown in Tab. 1. Because many modern pre-trained M-LMs almost invariably use some variety of subword tokenization, the number of tokens an entity contains will depend on the tokenization method used in the LM. We report the statistics based on the WordPiece tokenization used in multilingual BERT (Devlin et al., 2019). The tokenization scheme statistics for the other M-LMs are similar. 3.3 Prompts Some languages we include in the benchmark require additional handling of the prompts to account for their grammar or morphology. For example, (some) named entities inflect for case in languages like Greek, Russian, Hebrew, or Marathi. In some languages syntactic subjects and objects need to be in particular cases. Similarly, languages often require that the verb or other parts of the sentence agree with the subject or the object on some morphological features like person, gender, or number. Our prompts provide t"
2020.emnlp-main.479,L18-1544,0,0.0254733,"arts in another language, and further fine-tune the LM on these codeswitched data (§ 6). We perform experiments on three languages (French, Russian, and Greek, codeswitched with English). Results demonstrate that this code-switching-based learning can successfully improve the knowledge retrieval ability with low-resource language prompts. 2 Retrieving Facts from LMs In this paper we follow the protocol of Petroni et al. (2019)’s English-language LAMA benchmark, which targets factual knowledge expressed in the form of subject-relation-object triples from Wikidata1 curated in the T-REx dataset (ElSahar et al., 2018). The cloze-style prompts used therein are manually created and consist of a sequence of tokens, where [X] and [Y] are placeholders for subjects and objects (e.g. “[X] is a [Y] by profession.”). To assess the existence of a certain fact, [X] is replaced with the actual subject (e.g. “Obama is a hmaski by profession.”) and the model predicts the object in the blank yˆi = argmaxyi p(yi |si:i ), where si:i is the sentence with the i-th token masked out. Finally, the predicted fact is compared to the ground truth. In the next section, we extend this setting to more languages and predict multiple t"
2020.emnlp-main.479,D19-1633,0,0.217076,"Decoding As Tab. 1 shows, many facts involve multi-token entities and thus a LM would need to predict these entities in multiple steps. Generating multiple predictions is straightforward for traditional left-toright LMs (Sundermeyer et al., 2015; Radford et al., 2019), where we can autoregressively decode the next token conditioned on previous tokens. However, many pre-trained LMs such as BERT (Devlin et al., 2019) are masked LMs that predict individual words given left and right contexts, and decoding from such masked LMs remains an open problem (Lawrence et al., 2019; Salazar et al., 2020; Ghazvininejad et al., 2019; Wang and Cho, 2019; Cho, 2019). We systematically examined different multi-token decoding algorithms from three orthogonal perspectives: (1) how the initial predictions are produced, (2) how to refine the predictions, and (3) other commonly used components in neural text generation systems. We assume that the following conditional probability distribution is defined by the masked LM for a sentence with n tokens: p(xk |x01 , ..., x0k−1 , hmaskik , x0k+1 , ..., x0n ), (1) where the subscript of hmaski indicates its position, and the surrounding token x0· can either be an actual word x· or hmas"
2020.emnlp-main.479,2020.tacl-1.28,1,0.932269,"s Spanish example, the verb gerund “fundar” to found is rendered as “fundada” to agree in gender and number with the subject “Bloomberg L.P.”. The final prediction is in bold. Introduction Language models (LMs; (Church, 1988; Kneser and Ney, 1995; Bengio et al., 2003)) learn to model the probability distribution of text, and in doing so capture information about various aspects of the syntax or semantics of the language at hand. Recent works have presented intriguing results demonstrating that modern large-scale LMs also capture a significant amount of factual knowledge (Petroni et al., 2019; Jiang et al., 2020; Poerner et al., 2019). This knowledge is generally probed by having the LM fill in the blanks of cloze-style prompts such as ∗: Work done at Carnegie Mellon University. The first two authors contributed equally. “Obama is a _ by profession.”, where these prompts are invariably written in English. However, it goes without saying that there are many languages of the world other than English, and it is quite conceivable that (1) users may want to query this factual knowledge in other languages, and (2) some facts will be written in non-English languages and thus multilingually trained LMs (here"
2020.emnlp-main.479,D19-1001,0,0.013289,"one of them after lowercasing. 4 Multi-token Decoding As Tab. 1 shows, many facts involve multi-token entities and thus a LM would need to predict these entities in multiple steps. Generating multiple predictions is straightforward for traditional left-toright LMs (Sundermeyer et al., 2015; Radford et al., 2019), where we can autoregressively decode the next token conditioned on previous tokens. However, many pre-trained LMs such as BERT (Devlin et al., 2019) are masked LMs that predict individual words given left and right contexts, and decoding from such masked LMs remains an open problem (Lawrence et al., 2019; Salazar et al., 2020; Ghazvininejad et al., 2019; Wang and Cho, 2019; Cho, 2019). We systematically examined different multi-token decoding algorithms from three orthogonal perspectives: (1) how the initial predictions are produced, (2) how to refine the predictions, and (3) other commonly used components in neural text generation systems. We assume that the following conditional probability distribution is defined by the masked LM for a sentence with n tokens: p(xk |x01 , ..., x0k−1 , hmaskik , x0k+1 , ..., x0n ), (1) where the subscript of hmaski indicates its position, and the surrounding"
2020.emnlp-main.479,D19-1250,0,0.282358,"ctions for [Y]. In this Spanish example, the verb gerund “fundar” to found is rendered as “fundada” to agree in gender and number with the subject “Bloomberg L.P.”. The final prediction is in bold. Introduction Language models (LMs; (Church, 1988; Kneser and Ney, 1995; Bengio et al., 2003)) learn to model the probability distribution of text, and in doing so capture information about various aspects of the syntax or semantics of the language at hand. Recent works have presented intriguing results demonstrating that modern large-scale LMs also capture a significant amount of factual knowledge (Petroni et al., 2019; Jiang et al., 2020; Poerner et al., 2019). This knowledge is generally probed by having the LM fill in the blanks of cloze-style prompts such as ∗: Work done at Carnegie Mellon University. The first two authors contributed equally. “Obama is a _ by profession.”, where these prompts are invariably written in English. However, it goes without saying that there are many languages of the world other than English, and it is quite conceivable that (1) users may want to query this factual knowledge in other languages, and (2) some facts will be written in non-English languages and thus multilingual"
2020.emnlp-main.479,2020.acl-main.240,0,0.149649,"rcasing. 4 Multi-token Decoding As Tab. 1 shows, many facts involve multi-token entities and thus a LM would need to predict these entities in multiple steps. Generating multiple predictions is straightforward for traditional left-toright LMs (Sundermeyer et al., 2015; Radford et al., 2019), where we can autoregressively decode the next token conditioned on previous tokens. However, many pre-trained LMs such as BERT (Devlin et al., 2019) are masked LMs that predict individual words given left and right contexts, and decoding from such masked LMs remains an open problem (Lawrence et al., 2019; Salazar et al., 2020; Ghazvininejad et al., 2019; Wang and Cho, 2019; Cho, 2019). We systematically examined different multi-token decoding algorithms from three orthogonal perspectives: (1) how the initial predictions are produced, (2) how to refine the predictions, and (3) other commonly used components in neural text generation systems. We assume that the following conditional probability distribution is defined by the masked LM for a sentence with n tokens: p(xk |x01 , ..., x0k−1 , hmaskik , x0k+1 , ..., x0n ), (1) where the subscript of hmaski indicates its position, and the surrounding token x0· can either"
2020.emnlp-main.479,W19-2304,0,0.0286623,"any facts involve multi-token entities and thus a LM would need to predict these entities in multiple steps. Generating multiple predictions is straightforward for traditional left-toright LMs (Sundermeyer et al., 2015; Radford et al., 2019), where we can autoregressively decode the next token conditioned on previous tokens. However, many pre-trained LMs such as BERT (Devlin et al., 2019) are masked LMs that predict individual words given left and right contexts, and decoding from such masked LMs remains an open problem (Lawrence et al., 2019; Salazar et al., 2020; Ghazvininejad et al., 2019; Wang and Cho, 2019; Cho, 2019). We systematically examined different multi-token decoding algorithms from three orthogonal perspectives: (1) how the initial predictions are produced, (2) how to refine the predictions, and (3) other commonly used components in neural text generation systems. We assume that the following conditional probability distribution is defined by the masked LM for a sentence with n tokens: p(xk |x01 , ..., x0k−1 , hmaskik , x0k+1 , ..., x0n ), (1) where the subscript of hmaski indicates its position, and the surrounding token x0· can either be an actual word x· or hmaski. We aim to handle"
2020.emnlp-main.479,2020.acl-main.536,0,0.0663478,"Missing"
2020.emnlp-main.479,D19-1382,0,0.0880662,"k Obama is a United1 State2 President3 by profession (c) Confidence: Barack Obama is a minister2 of3 cabinet1 by profession Figure 2: Illustration of three initial prediction and refinement methods. Green boxes are mask tokens to be filled, and subscripts indicate the prediction order. 4.1 Initial Prediction and Refinement Given a sentence with multiple mask tokens, e.g., Eq. 2, we can either generate outputs in parallel independently or one at a time conditioned on the previously generated tokens. These methods are similar to the prediction problems that BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019b) perform in their pre-training stages respectively. We define c ∈ Rn as the probability of each prediction, with details varying by prediction methods. After all mask tokens are replaced with the initial predictions, i.e., sˆi:j = x1 , ..., yˆi , ..., yˆj , ..., xn , we can further refine the predictions by iteratively modifying one token at a time until convergence or until the maximum number of iterations is reached. Here we outline the algorithms with high-level descriptions, and provide concrete details in Appendix C. Independent. For independent initial prediction (Fig. 2a), the mask to"
2020.findings-emnlp.345,L16-1100,0,0.028889,"point of claiming human parity in some high-resource language pairs and in-domain settings (Hassan et al., 2018),1 fine-grained semantic differences become 1 We direct the reader to (Läubli et al., 2018) and (Toral antonis@gmu.edu increasingly important. Negation in particular, with its property of logical reversal, has the potential to cause loss of (or mis-)information if mistranslated. Other linguistic phenomena and analysis axes have gathered significant attention in NMT evaluation studies, including anaphora resolution (Hardmeier et al., 2014; Voita et al., 2018) and pronoun translation (Guillou and Hardmeier, 2016), modality (Baker et al., 2012), ellipsis and deixis (Voita et al., 2019), word sense disambiguation (Tang et al., 2018), and morphological competence (Burlot and Yvon, 2017). Nevertheless, the last comprehensive study of the effect of negation in MT pertains to older, phrase-based models (Fancellu and Webber, 2015). In this work, we set out to study the effect of negation in modern NMT systems. Specifically, we explore: 1. Whether negation affects the quality of the produced translations (it does); 2. Whether the typically-used evaluation datasets include a significant amount of negated examp"
2020.findings-emnlp.345,E17-2002,0,0.0128272,"ine-Y afrl-sys online-A afrl-ewc TartuNLP-u online-X.0 0.161 0.143 0.134 0.133 0.126 0.125 0.108 0.091 0.024 0.022 -0.030 -0.039 -0.096 Q4: Is Translating Negation between Similar Languages Easier? Intuition may lead us to believe that it is easier to translate negation between similar languages. We show the correlation between language similarity and relative differences in Z-scores with and without negation in Figure 2. To calculate similarity between two languages, we follow Zhang and Toral (2019) and Berzak et al. (2017). Briefly, we obtain feature vectors for each language from lang2vec (Littell et al., 2017), and define the similarity between two languages as the cosine similarity between their feature vectors. More specifically, we concatenate 103 morphosyntactic features and 87 language family features (only those relevant to the languages we work with) from the URIEL typological database. We conclude that similarity between languages is only a weak indicator of how difficult it is to translate negation. We revisit this question in Section 5 with an in-depth linguistic discussion. (Z(w/ neg.) - Z(w/o neg.)) / Z(all sentences) Table 3: Rankings of all submissions translating from Russian to Engl"
2020.findings-emnlp.345,W19-6623,0,0.0130524,"ed evaluation datasets include a significant amount of negated examples (they don’t); 3. Whether different systems quantifiably handle negation differently across different settings (they do); and 4. Whether there is a linguistics-grounded explanation of our findings (there is). Our conclusion is that indeed negation still poses an issue for NMT systems in several language pairs, an issue which should be tackled in future NMT systems development. Negation should be taken into consideration especially when deploying realworld systems which might produce incredibly fluent but inadequate output (Martindale et al., 2019). 2 Negation 101 Negation at its most straightforward—simple negation of declarative sentences—involves reversing et al., 2018) for further examination of such claims. 3869 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3869–3885 c November 16 - 20, 2020. 2020 Association for Computational Linguistics the truth value of a sentence. Skies are blue, for example, becomes Skies are not blue. Clausal negation of an existing utterance, defined roughly as negation of the entire clause rather than a subpart, produces a second utterance whose meaning contradicts that of th"
2020.findings-emnlp.345,W17-4770,0,0.0479203,"Missing"
2020.findings-emnlp.345,W18-6319,0,0.0173336,"igating the role of negation WMT competitions, and are preferred to au- in machine translation by only looking at English tomated metrics to assess the quality of MT. negations likely misses valid insights. For examNevertheless, most of the MT community still ple, a Spanish sentence containing negation (e.g., relies on automated metrics for development “El ladrón no estaba preocupado hasta que vino la and system comparisons. Thus, we also work policía”) can be translated into English either with with three automated metrics, in particular negation (“The thief was not worried until the poBLEU (Post, 2018), chrF++ (Popovi´c, 2017), and lice arrived”), or without negation (“The thief only METEOR (Denkowski and Lavie, 2011). worried when the police arrived”). We reserve for future work a more thorough analysis of corresponIn the remainder of the paper we present two dences between negation in source sentences and complementary analyses. First, we investigate the role of negation in machine translation with an em- negation in English reference translations. phasis on numeric evaluation (Section 4). Second, 4 Quantitative Analysis we investigate from a linguistic perspective what makes translating"
2020.lrec-1.344,L18-1530,1,0.86178,"icant data resources, while low-resource languages are left behind. The situation for endangered languages is usually even worse, as the focus of the scientific community mostly relies in language documentation. The typical endangered language documentation process typically includes the creation of language resources in the form of word lists, audio and video recordings, notes, or grammar fragments, with the created resources then stored into large online linguistics archives. This process is often hindered by the so-called ´ Transcription Bottleneck, but recent advances (Cavar et al., 2016; Adams et al., 2018) provide promising directions towards a solution for this issue. However, language documentation and linguistic description, although extremely important itself, does not meaningfully contribute to language conservation, which aims to ensure that the language stays in use. We believe that a major avenue towards continual language use is by further creating language technologies for endangered languages, essentially elevating them to the same level as high-resource, economically or politically stronger languages. The majority of the world’s languages are categorized as synthetic, meaning that t"
2020.lrec-1.344,D19-1091,1,0.908609,"Missing"
2020.lrec-1.344,L16-1632,1,0.846314,"anguages with significant data resources, while low-resource languages are left behind. The situation for endangered languages is usually even worse, as the focus of the scientific community mostly relies in language documentation. The typical endangered language documentation process typically includes the creation of language resources in the form of word lists, audio and video recordings, notes, or grammar fragments, with the created resources then stored into large online linguistics archives. This process is often hindered by the so-called ´ Transcription Bottleneck, but recent advances (Cavar et al., 2016; Adams et al., 2018) provide promising directions towards a solution for this issue. However, language documentation and linguistic description, although extremely important itself, does not meaningfully contribute to language conservation, which aims to ensure that the language stays in use. We believe that a major avenue towards continual language use is by further creating language technologies for endangered languages, essentially elevating them to the same level as high-resource, economically or politically stronger languages. The majority of the world’s languages are categorized as synt"
2020.lrec-1.344,L18-1416,0,0.0341512,"Missing"
2020.lrec-1.344,K17-2001,0,0.219932,"Missing"
2020.lrec-1.344,K18-3001,0,0.156736,"Missing"
2020.lrec-1.344,W19-4226,0,0.0852328,"Missing"
2020.lrec-1.344,W18-4802,0,0.105698,"Missing"
2020.lrec-1.344,L16-1497,0,0.0310382,"0.13 average Levenshtein distance that lemmatization models achieved over 107 treebanks in 66 languages for the SIGMORPHON 2019 shared task (McCarthy et al., 2019). 5. Related Work Our work builds and expands upon previous work on Indigenous languages of the Americas specifically focusing on the complexity of their morphology. Among other works 2829 similar to ours, (Cox et al., 2016) focused on the morphology of Dene verbs, (Moeller et al., 2018) on Arapaho verbs, (Cardenas and Zeman, 2018) on Shipibo-Konibo, and (Chen and Schwartz, 2018) on Saint Lawrence Island and Central Siberian Yupik. (Sylak-Glassman et al., 2016) describe an approach for elicit complete inflection paradigms, with experiments in languages like Nahuatl. Our resource is the first one for SJQ Chatino, but it also provides an exciting new data point in the computational study of morphological analysis, lemmatization, and inflection, as it is the first one in a tonal language with explicit tonal markings in the writing system. In a similar vein, the Oto-Manguean Inflectional Class Database4 (Palancar and Feist, 2015) provides a valuable resource for studying the verbal morphology of Oto-Manguean languages (including a couple of other Chatin"
2020.lrec-1.344,N18-1126,0,\N,Missing
2020.lrec-1.344,K18-3000,0,\N,Missing
2020.lrec-1.344,K18-3006,0,\N,Missing
2020.lrec-1.344,K18-3008,0,\N,Missing
2020.lrec-1.344,W16-2002,0,\N,Missing
2020.lrec-1.350,N19-1009,0,0.0408851,"Missing"
2020.lrec-1.350,L16-1632,0,0.0416335,"Missing"
2020.lrec-1.350,W17-0102,0,0.0279178,") developed hybrid ruleand phrase-based Statistical Machine Translation systems. Naturally, similar works in collecting corpora in Indigenous languages of Latin America are abundant, but very few, if any, have the scale and potential of our resource to be useful in many downstream language-specific and interdisciplinary applications. A general overview of the state of NLP for the under-represented languages of the Americas can be found at (Mager et al., 2018). To name a few of the many notable works, (Monta˜no et al., 2019) created a parallel Mixtec-Spanish corpus for Machine Translation and (Kazeminejad et al., 2017) created lexical resources ´ for Arapaho, while (Cardenas et al., 2018) and (Cavar et al., 2016) focused on building speech corpora for Southern Quechua and Chatino respectively. 4. The Resource 1 The resource is comprised of 142 hours of spoken Mapudungun that was recorded during the AVENUE project (Levin et al., 2002) in 2001 to 2005. The data was recorded under a partnership between the AVENUE project, funded by the US National Science Foundation at Carnegie Mellon University, the Chilean Ministry of Education (Mineduc), and the Instituto de Estudios Ind´ıgenas at Universidad de La Frontera"
2020.lrec-1.350,monson-etal-2004-data,1,0.674738,"Missing"
2020.lrec-1.350,monson-etal-2008-linguistic,1,0.46873,"Missing"
2020.lrec-1.350,W19-3650,0,0.033218,"Missing"
2020.lrec-1.350,P02-1040,0,0.110151,"Missing"
2020.lrec-1.350,W15-3049,0,0.0486746,"Missing"
2020.lrec-1.350,W18-6319,0,0.011561,"ity as well as label smoothing set to 0.1. We train with the Adam optimizer (Kingma and Ba, 2014) for up to 200 epochs using learning decay with a patience of six epochs. The baseline results using different portions of the training set (10k, 50k, 100k, and all (220k) parallel sentences) on both translation directions are presented in Table 3, using detokenized BLEU (Papineni et al., 2002) (a standard MT 2875 metric) and chrF (Popovi´c, 2015) (a metric that we consider to be more appropriate for polysynthetic languages, as it does not rely on word n-grams) computed with the sacreBLEU toolkit (Post, 2018). It it worth noting the difference in quality between the two directions, with translation into Spanish reaching 20.4 (almost 21) BLEU points in the development set, while the opposite direction (translating into Mapudungun) shows about a 7 BLEU points worse performance. This is most likely due to Mapudungun being a polysynthetic language, with its complicated morphology posing a challenge for proper generation. 7. Conclusion With this work we present a resource that will be extremely useful for building language systems in an endangered, under-represented language, Mapudungun. We benchmark N"
2020.lrec-1.350,P16-1162,0,0.0325849,"Missing"
2020.lrec-1.656,L18-1530,1,0.837929,"t al., 2017; Littell et al., 2018). In our experience, the most requested speech technology for very-low-resource languages is transcription acceleration, an application of speech recognition for decreasing the workload of transcribers. Many low-resource and endangered languages do already have extensive untranscribed speech collections, in the form of recorded radio broadcasts, linguists’ field recordings, or other personal recordings. Transcribing these collections is a high priority for many speech communities, as an untranscribed corpus is difficult to use in either research or education (Adams et al., 2018; Foley et al., 2019). AlloVera and Allosaurus were originally and primarily intended for use in transcription acceleration, although we will also be exploring other practical applications. Another priority technology is approximate search of speech databases. While the aforementioned untranscribed speech collections can straightforwardly be made available online, they are not especially accessible as such. A researcher, teacher, or student cannot in practice listen to years’ worth of radio recordings in search of a particular word or topic. AlloVera and Allosaurus, by making an approximate te"
2020.lrec-1.656,W17-4607,1,0.839092,"is approximate search of speech databases. While the aforementioned untranscribed speech collections can straightforwardly be made available online, they are not especially accessible as such. A researcher, teacher, or student cannot in practice listen to years’ worth of radio recordings in search of a particular word or topic. AlloVera and Allosaurus, by making an approximate text representation of the corpus, open up the possibility for efficient approximate phonetic search through otherwise-untranscribed speech databases. Previous work has demonstrated the feasibility of such an approach (Anastasopoulos et al., 2017; Boito et al., 2017), but the quality of the search results can be significantly boosted by improvements in a first-pass phonetic transcription (Ondel et al., 2018). We are also planning on integrating AlloVera and Allosaurus into a language-neutral forced-alignment pipeline. While forced-alignment is a task that is already commonly done in a zero-shot scenario (by manually mapping target-language phones to the vocabulary of a pretrained acoustic model, often an English one), the extensive phonetic vocabulary of AlloVera means that many phones are already covered. This greatly expands the num"
2020.lrec-1.656,2021.eacl-main.58,0,0.114164,"Missing"
2020.lrec-1.656,W17-0106,1,0.837055,"ance over both the shared phoneme model and the private phoneme model substantially. 4. Applications Currently, we intend to integrate AlloVera and Allosaurus (or other future systems trained using AlloVera) into three practical downstream systems for very-low-resource languages, addressing tasks identified as development priori5333 Shared Phoneme PER Private Phoneme PER Allosaurus PER Inuktitut Tusom 94.1 86.2 73.1 93.5 85.8 64.2 Table 5: Comparisons of phone error rates in two unseen languages ties in recent surveys of indigenous and other low-resource language technology (Thieberger, 2016; Levow et al., 2017; Littell et al., 2018). In our experience, the most requested speech technology for very-low-resource languages is transcription acceleration, an application of speech recognition for decreasing the workload of transcribers. Many low-resource and endangered languages do already have extensive untranscribed speech collections, in the form of recorded radio broadcasts, linguists’ field recordings, or other personal recordings. Transcribing these collections is a high priority for many speech communities, as an untranscribed corpus is difficult to use in either research or education (Adams et al"
2020.lrec-1.656,C18-1222,1,0.85747,"hared phoneme model and the private phoneme model substantially. 4. Applications Currently, we intend to integrate AlloVera and Allosaurus (or other future systems trained using AlloVera) into three practical downstream systems for very-low-resource languages, addressing tasks identified as development priori5333 Shared Phoneme PER Private Phoneme PER Allosaurus PER Inuktitut Tusom 94.1 86.2 73.1 93.5 85.8 64.2 Table 5: Comparisons of phone error rates in two unseen languages ties in recent surveys of indigenous and other low-resource language technology (Thieberger, 2016; Levow et al., 2017; Littell et al., 2018). In our experience, the most requested speech technology for very-low-resource languages is transcription acceleration, an application of speech recognition for decreasing the workload of transcribers. Many low-resource and endangered languages do already have extensive untranscribed speech collections, in the form of recorded radio broadcasts, linguists’ field recordings, or other personal recordings. Transcribing these collections is a high priority for many speech communities, as an untranscribed corpus is difficult to use in either research or education (Adams et al., 2018; Foley et al.,"
2020.lrec-1.656,L18-1429,1,0.90023,"For language-specific models and questions, such representations are often adequate and may even be preferable to the alternatives. However, in multilingual models, the language-specific nature of phonemic abstractions can be a liability. The added phonetic realism of even a broad phonetic representation moves transcriptions closer to a universal space where categories transcend the bounds of a particular language. This paper describes AlloVera1 , a resource that maps between the phonemic representations produced by many NLP tools—including grapheme-to-phoneme (G2P) transducers like our own (Mortensen et al., 2018)—and broad phonetic representations. Specifically, it is a database of phonemeallophone pairs (where an allophone is a phonetic realization of a phoneme—see § 1.1. below) for 14 languages. It is designed for notational compatibility with existing G2P systems. The phonetic representations are relatively broad, a consequence of our sources, but they are phonetically realistic enough to improve performance on a speech-to-phone recognition task, as shown in § 3. 1 https://github.com/dmort27/allovera This resource has applications beyond universal speechto-phone recognition, including approximate s"
2020.lrec-1.656,rousseau-etal-2012-ted,0,0.0854531,"Missing"
2020.lrec-1.656,1996.amta-1.36,0,0.808006,"y manually mapping target-language phones to the vocabulary of a pretrained acoustic model, often an English one), the extensive phonetic vocabulary of AlloVera means that many phones are already covered. This greatly expands the number of languages that can be aligned without the need for an extensive transcribed corpus or manual system configuration. 5. Related Work AlloVera builds on work in three major areas: phonetics and theoretical phonology, phonological ontologies, and human language technologies. The term allophone was coined by Benjamin Lee Whorf in the 1920s and was popularized by Trager and Block (1941). However, the idea goes back much further, to Baudoin de Courtenay (1894). The idea of allophony most relevant to our work here comes from American Structuralist linguists like Harris (1951), but we also invoke the concept of the archiphoneme, associated with the Prague Circle (Trubetskoy, 1939). In the 1950s and 1960s, the structuralist notions of the “taxonomic” phoneme and of allophones came under attack by generative linguists (Chomsky and Halle, 1968; Halle, 1962; Halle, 1959), but they have retained their importance both in linguistic practice and linguistic theory. Various resources co"
2020.sigmorphon-1.1,K18-3001,1,0.899071,"logical reinflection, we specifically focus on typological diversity and aim to investigate systems’ ability to generalize across typologically distinct languages many of which are low-resource. For example, if a neural network architecture works well for a sample of IndoEuropean languages, should the same architecture also work well for Tupi–Guarani languages (where nouns are “declined” for tense) or Austronesian languages (where verbal morphology is frequently prefixing)? 2 Task Description The 2020 iteration of our task is similar to CoNLL-SIGMORPHON 2017 (Cotterell et al., 2017) and 2018 (Cotterell et al., 2018) in that participants are required to design a model that learns to generate inflected forms from a lemma and a set of morphosyntactic features that derive the desired target form. For each language we provide a separate training, development, and test set. More historically, all of these tasks resemble the classic “wug”-test that Berko (1958) developed to test child and human knowledge of English nominal morphology. Unlike the task from earlier years, this year’s task proceeds in three phases: a Development Phase, a Generalization Phase, and an Evaluation Phase, in which each phase introduces"
2020.sigmorphon-1.1,K17-2001,1,0.928876,"e SIGMORPHON shared task on morphological reinflection, we specifically focus on typological diversity and aim to investigate systems’ ability to generalize across typologically distinct languages many of which are low-resource. For example, if a neural network architecture works well for a sample of IndoEuropean languages, should the same architecture also work well for Tupi–Guarani languages (where nouns are “declined” for tense) or Austronesian languages (where verbal morphology is frequently prefixing)? 2 Task Description The 2020 iteration of our task is similar to CoNLL-SIGMORPHON 2017 (Cotterell et al., 2017) and 2018 (Cotterell et al., 2018) in that participants are required to design a model that learns to generate inflected forms from a lemma and a set of morphosyntactic features that derive the desired target form. For each language we provide a separate training, development, and test set. More historically, all of these tasks resemble the classic “wug”-test that Berko (1958) developed to test child and human knowledge of English nominal morphology. Unlike the task from earlier years, this year’s task proceeds in three phases: a Development Phase, a Generalization Phase, and an Evaluation Pha"
2020.sigmorphon-1.1,W09-0106,0,0.0385713,"n variably surface as prefixes, suffixes, infixes, or circumfixes (Dryer, 2013). Most Eurasian and Australian languages strongly favor suffixation, and the same holds true, but to a lesser extent, for South American and New Guinean languages (Dryer, 2013). In Mesoamerican languages and African languages spoken below the Sahara, prefixation is dominant instead. These are just three dimensions of variation in morphology, and the cross-linguistic variation is already considerable. Such cross-lingual variation makes the development of natural language processing (NLP) applications challenging. As Bender (2009, 2016) notes, many current architectures and training and tuning algorithms still present language-specific biases. The most commonly used language for developing NLP applications is English. Along the above dimensions, English is productively concatenative, a mixture of analytic and synthetic, and largely suffixing in its inflectional morphology. With respect to languages that exhibit inflectional morphology, English is relatively impoverished.1 Importantly, English is just one morphological system among many. A larger goal of natural language processing is that the system work for any prese"
2020.sigmorphon-1.1,2020.lrec-1.344,1,0.878264,"Missing"
2020.sigmorphon-1.1,2020.sigmorphon-1.15,0,0.0565092,"Missing"
2020.sigmorphon-1.1,2020.sigmorphon-1.14,0,0.0439232,"Missing"
2020.sigmorphon-1.1,L16-1379,0,0.0190826,"Missing"
2020.sigmorphon-1.1,K17-2010,1,0.837279,"l baselines were based on a neural transducer (Wu and Cotterell, 2019), which is essentially a hard monotonic attention model (mono-*). The second baseline is a transformer (Vaswani et al., 2017) adopted for character-level tasks that currently holds the state-of-the-art on the 2017 SIGMORPHON shared task data (Wu et al., 2020, trm-*). Both models take the lemma and morphological tags as input and output the target inflection. The baseline is further expanded to include the data augmentation technique used by Anastasopoulos and Neubig (2019, -aug-) (conceptually similar to the one proposed by Silfverberg et al. (2017)). Relying on a simple characterlevel alignment between lemma and form, this technique replaces shared substrings of length &gt; 3 with random characters from the language’s alphabet, producing hallucinated lemma–tag–form triples. Both neural baselines were trained in mono- (*-single) and multilingual (shared parameters among the same family, *-shared) settings. 6 Many teams based their models on the transformer architecture. NYU-CUBoulder experimented with a vanilla transformer model (NYU-CUBoulder-04-0), a pointer-generator transformer that allows for a copy mechanism (NYU-CUBoulder-02-0), and"
2020.sigmorphon-1.1,2020.sigmorphon-1.4,0,0.0612223,"Missing"
2020.sigmorphon-1.1,W19-4207,0,0.0125147,"c attention model with improved alignment strategy. This model is further improved (flexica-03-1) by introducing a data hallucination technique which is based on phonotactic modelling of extremely low-resource languages (Shcherbakov et al., 2016). LTI focused on their earlier model (Anastasopoulos and Neubig, 2019), a neural multi-source encoder–decoder with two-step attention architecture, training it with hallucinated data, cross-lingual transfer, and romanization of scripts to improve performance on low-resource languages. DeepSpin reimplemented gated sparse two-headed attention model from Peters and Martins (2019) and trained it on all languages at once (massively multilingual). The team experimented with two modifications of the softmax function: sparsemax (Martins and Astudillo, 2016, deepspin-02-1) and 1.5-entmax (Peters et al., 2019, deepspin-01-1). Neural Neural baselines were based on a neural transducer (Wu and Cotterell, 2019), which is essentially a hard monotonic attention model (mono-*). The second baseline is a transformer (Vaswani et al., 2017) adopted for character-level tasks that currently holds the state-of-the-art on the 2017 SIGMORPHON shared task data (Wu et al., 2020, trm-*). Both"
2020.sigmorphon-1.1,P19-1146,0,0.0351308,"Missing"
2020.sigmorphon-1.1,P19-1148,1,0.838088,"lti-source encoder–decoder with two-step attention architecture, training it with hallucinated data, cross-lingual transfer, and romanization of scripts to improve performance on low-resource languages. DeepSpin reimplemented gated sparse two-headed attention model from Peters and Martins (2019) and trained it on all languages at once (massively multilingual). The team experimented with two modifications of the softmax function: sparsemax (Martins and Astudillo, 2016, deepspin-02-1) and 1.5-entmax (Peters et al., 2019, deepspin-01-1). Neural Neural baselines were based on a neural transducer (Wu and Cotterell, 2019), which is essentially a hard monotonic attention model (mono-*). The second baseline is a transformer (Vaswani et al., 2017) adopted for character-level tasks that currently holds the state-of-the-art on the 2017 SIGMORPHON shared task data (Wu et al., 2020, trm-*). Both models take the lemma and morphological tags as input and output the target inflection. The baseline is further expanded to include the data augmentation technique used by Anastasopoulos and Neubig (2019, -aug-) (conceptually similar to the one proposed by Silfverberg et al. (2017)). Relying on a simple characterlevel alignme"
2020.sigmorphon-1.1,2020.sigmorphon-1.5,0,0.0487999,"Missing"
2020.sigmorphon-1.22,D16-1153,0,0.0172123,"nted with data generated from a related highresource language. Among many, for instance, De Gispert and Marino (2006) built a CatalanEnglish MT by bridging through Spanish, while Xia et al. (2019) show that word-level substitutions can convert a high-resource (related) language corpus into a pseudo low-resource one leading to large improvements in MT quality. Such approaches typically operate at the word level, hence they do not need to handle script differences explicitly. NLP models that handle script differences do exist, but focus mostly on analysis tasks such as named entity recognition (Bharadwaj et al., 2016; Chaudhary et al., 2018; Rahimi et al., 2019) or entity linking (Rijhwani et al., 2019), whereas we focus in a generation task. Character-level transliteration was typically incorporated in phrase-based statistical MT systems (Durrani et al., 2014), but was only used to handle named entity translation. Notably, there exist NLP approaches such as the document classification approach of Zhang et al. (2018) showing that indeed shared character-level information can facilitate cross-lingual transfer, but limit their analysis to same-script languages only. Specific to the the morphological inflect"
2020.sigmorphon-1.22,D18-1366,1,0.786098,"from a related highresource language. Among many, for instance, De Gispert and Marino (2006) built a CatalanEnglish MT by bridging through Spanish, while Xia et al. (2019) show that word-level substitutions can convert a high-resource (related) language corpus into a pseudo low-resource one leading to large improvements in MT quality. Such approaches typically operate at the word level, hence they do not need to handle script differences explicitly. NLP models that handle script differences do exist, but focus mostly on analysis tasks such as named entity recognition (Bharadwaj et al., 2016; Chaudhary et al., 2018; Rahimi et al., 2019) or entity linking (Rijhwani et al., 2019), whereas we focus in a generation task. Character-level transliteration was typically incorporated in phrase-based statistical MT systems (Durrani et al., 2014), but was only used to handle named entity translation. Notably, there exist NLP approaches such as the document classification approach of Zhang et al. (2018) showing that indeed shared character-level information can facilitate cross-lingual transfer, but limit their analysis to same-script languages only. Specific to the the morphological inflection task, (Hauer et al.,"
2020.sigmorphon-1.22,K18-3001,0,0.0552652,"y do not need to handle script differences explicitly. NLP models that handle script differences do exist, but focus mostly on analysis tasks such as named entity recognition (Bharadwaj et al., 2016; Chaudhary et al., 2018; Rahimi et al., 2019) or entity linking (Rijhwani et al., 2019), whereas we focus in a generation task. Character-level transliteration was typically incorporated in phrase-based statistical MT systems (Durrani et al., 2014), but was only used to handle named entity translation. Notably, there exist NLP approaches such as the document classification approach of Zhang et al. (2018) showing that indeed shared character-level information can facilitate cross-lingual transfer, but limit their analysis to same-script languages only. Specific to the the morphological inflection task, (Hauer et al., 2019) use cognate projection to augment low-resource data, while (Wiemerslage et al., 2018) explore the inflection task using inputs in phonological space as well as bundles of phonological features from PanPhon (Mortensen et al., 2016), showing improvements for both settings. Our work, in contrast, focuses on better cross-lingual transfer, attempting to combine the phonological a"
2020.sigmorphon-1.22,K17-2001,0,0.0758685,"Missing"
2020.sigmorphon-1.22,W18-2413,0,0.0607071,"Missing"
2020.sigmorphon-1.22,C16-1328,0,0.0583672,"Missing"
2020.sigmorphon-1.22,2020.sigmorphon-1.6,1,0.786101,"nto the test languages, G2P conversion, and romanization), we compute the percentage of times that an inflection with each morphological tag failed. Table 4 reports the tags with the highest difference in these ratios, between the baseline and our models for each method. The higher the number, the larger the improvements for this particular tag. For inflecting Portuguese (top and bottom sets of results), we find it hard to make any conclusions: both noun, adjective, and verb tags appear in the top lists. For inflecting Russian 194 10 In fact, our submission to the SIGMORPHON 2020 Shared Task (Murikinati and Anastasopoulos, 2020) following this approach tied for first for Telugu (Vylomova et al., 2020). (middle set), it is mostly noun/adjective tags pertaining to animacy (ANIM, INAN), gender (MASC) and case (GEN, DAT) that show the largest improvements. We still cannot explain the improvements we see in these language pairs, except for vague hypotheses that either the languages do share some similar inflection processes (besides, they are both Indo-European) or that the harder multi-task training setting regularizes the model leading to better accuracy overall. 5 Conclusion With this work we study whether using transl"
2020.sigmorphon-1.22,W19-4202,0,0.130691,"et al., 2018; Rahimi et al., 2019) or entity linking (Rijhwani et al., 2019), whereas we focus in a generation task. Character-level transliteration was typically incorporated in phrase-based statistical MT systems (Durrani et al., 2014), but was only used to handle named entity translation. Notably, there exist NLP approaches such as the document classification approach of Zhang et al. (2018) showing that indeed shared character-level information can facilitate cross-lingual transfer, but limit their analysis to same-script languages only. Specific to the the morphological inflection task, (Hauer et al., 2019) use cognate projection to augment low-resource data, while (Wiemerslage et al., 2018) explore the inflection task using inputs in phonological space as well as bundles of phonological features from PanPhon (Mortensen et al., 2016), showing improvements for both settings. Our work, in contrast, focuses on better cross-lingual transfer, attempting to combine the phonological and the orthographic space. 2 Quantifying the Issue In Table 1 we offered a few examples from the literature to indicate that differences in script between the transfer and test language in a cross-lingual learning setting"
2020.sigmorphon-1.22,W18-5805,0,0.0362923,"Missing"
2020.sigmorphon-1.22,P18-4003,0,0.131876,"Missing"
2020.sigmorphon-1.22,P19-1493,0,0.0296544,"e offered a few examples from the literature to indicate that differences in script between the transfer and test language in a cross-lingual learning setting can be a potential issue. In this section, we provide additional evidence that this is indeed the case. The intuition behind our analysis is that a model trained cross-lingually can only claim to indeed learn cross-lingually if it ends up sharing the representations of the different inputs, at least to some extent. This observation of a learned shared space has also been noted in massively multilingual models like the multilingual BERT (Pires et al., 2019), or for cross-lingual learning of word-level representations (Wang et al., 2020). For a character-level model, such as the ones typically used for neural morphological inflection, this implies a learned mapping between the characters of the two inputs. Our hypothesis is that such a learned character mapping, and in particular between related languages, should resemble a transliteration mapping, assuming that both languages use a phonographic writing system (such as the Latin or the Cyrillic alphabet and their variations), to use the notation of Faber (1992).2 To verify whether this intuition"
2020.sigmorphon-1.22,W04-3250,0,0.281827,"ed task, but limit ourselves to the language pairs where (1) the two languages use different writing scripts, and (2) we have access to a transliteration model from the transfer to the test language. As a result, we evaluate our approach on the following language pairs: {Hindi,Sanskrit}–Bengali, Kannada–Telugu, {Arabic,Hebrew}–Maltese, Bashkir–Tatar, Bashkir– Crimean Tatar, Armenian–Kabardian, and Russian– Portuguese. We compare our systems’ performance with the baselines using exact match accuracy over the test set. We also perform statistical significance testing using bootstrap resampling (Koehn, 2004).9 4 Experiments and Results We perform experiments both with single-language transfer as well as transfer from multiple related languages, if available. We also perform ablations in two settings, with and without hallucinated data. Transliterating the Transfer into the Test language We first focus on the setting where a 8 We direct the reader to (Anastasopoulos and Neubig, 2019) for further details on the model. 9 We use 10,000 bootstrap samples and a 12 ratio of samples in each iteration. transliteration tool between the transfer and the target language is available (in all cases, the target"
2020.sigmorphon-1.22,P19-1015,0,0.0240897,"urce language. Among many, for instance, De Gispert and Marino (2006) built a CatalanEnglish MT by bridging through Spanish, while Xia et al. (2019) show that word-level substitutions can convert a high-resource (related) language corpus into a pseudo low-resource one leading to large improvements in MT quality. Such approaches typically operate at the word level, hence they do not need to handle script differences explicitly. NLP models that handle script differences do exist, but focus mostly on analysis tasks such as named entity recognition (Bharadwaj et al., 2016; Chaudhary et al., 2018; Rahimi et al., 2019) or entity linking (Rijhwani et al., 2019), whereas we focus in a generation task. Character-level transliteration was typically incorporated in phrase-based statistical MT systems (Durrani et al., 2014), but was only used to handle named entity translation. Notably, there exist NLP approaches such as the document classification approach of Zhang et al. (2018) showing that indeed shared character-level information can facilitate cross-lingual transfer, but limit their analysis to same-script languages only. Specific to the the morphological inflection task, (Hauer et al., 2019) use cognate pro"
2020.sigmorphon-1.22,W18-2411,0,0.0443944,"Missing"
2020.sigmorphon-1.22,W16-2701,0,0.0269806,"Missing"
2020.sigmorphon-1.22,K17-2010,0,0.207307,"Missing"
2020.sigmorphon-1.22,W19-4226,0,0.142719,"s languages are synthetic, meaning they have rich morphology. As a result, modeling morphological inflection computationally can have a significant impact on downstream quality, not only in analysis tasks such as named entity recognition and morphological analysis (Zhu et al., 2019), but also for language generation systems for morphologically-rich languages. In recent years, morphological inflection has been extensively studied in monolingual high resource settings, especially through the recent SIGMORPHON challenges (Cotterell et al., 2016, 2017, 2018). The latest SIGMOPRHON 2019 challenge (McCarthy et al., 2019) focused on lowresource settings and encouraged cross-lingual training, an approach that has been successfully applied in other low-resource tasks such as Machine 1 Our code and data are available at https://github. com/nikim99/Inflection-Transliteration. Translation (MT) or parsing. Cross-lingual learning is a particularly promising direction, due to its potential to utilize similarities across languages (often languages from the same linguistic family, which we will refer to as “related&quot;) in order to overcome the lack of training data. In fact, leveraging data from several related languages"
2020.sigmorphon-1.22,L18-1429,0,0.0392696,"Missing"
2020.sigmorphon-1.22,W18-5818,0,0.0215694,"hereas we focus in a generation task. Character-level transliteration was typically incorporated in phrase-based statistical MT systems (Durrani et al., 2014), but was only used to handle named entity translation. Notably, there exist NLP approaches such as the document classification approach of Zhang et al. (2018) showing that indeed shared character-level information can facilitate cross-lingual transfer, but limit their analysis to same-script languages only. Specific to the the morphological inflection task, (Hauer et al., 2019) use cognate projection to augment low-resource data, while (Wiemerslage et al., 2018) explore the inflection task using inputs in phonological space as well as bundles of phonological features from PanPhon (Mortensen et al., 2016), showing improvements for both settings. Our work, in contrast, focuses on better cross-lingual transfer, attempting to combine the phonological and the orthographic space. 2 Quantifying the Issue In Table 1 we offered a few examples from the literature to indicate that differences in script between the transfer and test language in a cross-lingual learning setting can be a potential issue. In this section, we provide additional evidence that this is"
2020.sigmorphon-1.22,P19-1579,1,0.825508,"ting, we convert both languages into a shared space, using grapheme-to-phoneme (G2P) conversion into the International Phonetic Alphabet (IPA) as well as romanization. We discuss both settings and their effects on morphological inflection in low-resource settings (§3). Our approach bears similarities to pseudo-corpus approaches that have been used in machine translation (MT), where low-resource language data are augmented with data generated from a related highresource language. Among many, for instance, De Gispert and Marino (2006) built a CatalanEnglish MT by bridging through Spanish, while Xia et al. (2019) show that word-level substitutions can convert a high-resource (related) language corpus into a pseudo low-resource one leading to large improvements in MT quality. Such approaches typically operate at the word level, hence they do not need to handle script differences explicitly. NLP models that handle script differences do exist, but focus mostly on analysis tasks such as named entity recognition (Bharadwaj et al., 2016; Chaudhary et al., 2018; Rahimi et al., 2019) or entity linking (Rijhwani et al., 2019), whereas we focus in a generation task. Character-level transliteration was typically"
2020.sigmorphon-1.22,K19-1021,0,0.049411,"Missing"
2020.sigmorphon-1.6,W19-4226,0,0.218151,"Missing"
2020.sigmorphon-1.6,2020.sigmorphon-1.22,1,0.893706,"ate language-specific regimes for each test language, depending on the particular characteristics of the language, on the data availability for the particular test language and the availability of other related language data. As a result, for some high-resource languages we submitted systems without cross-lingual transfer, for some we used a single related high resource language, and for some we used multiple related languages. Last, for a few test languages we augmented our datasets with romanized versions of the training data, an approach that has shown promising results in concurrent work (Murikinati et al., 2020). Introduction Morphological inflection is the process that creates grammatical forms (typically guided by sentence structure) of a lexeme/lemma. As a computational task it is framed as mapping from the lemma and a set of morphological tags to the desired form, which simplifies the task by removing the necessity to infer the form from context. For an example from Asturian, given the lemma aguar and tags V;PRS;2;PL;IND, the task is to create the indicative voice, present tense, 2nd person plural form agu` a. Let X = x1 . . . xN be a character sequence of the lemma, T = t1 . . . tM a set of morp"
2020.sigmorphon-1.6,W19-4207,0,0.0231963,"Missing"
2020.sigmorphon-1.6,D19-1091,1,0.872353,"n University nmurikin@andrew.cmu.edu, aanastas@cs.cmu.edu Abstract shared tasks (Cotterell et al., 2016, 2017, 2018; McCarthy et al., 2019), with the 2019 edition focusing in particularly challenging low-resource scenarios. The 2020 edition (Vylomova et al., 2020) focused on generalization of systems across typologically diverse languages, regardless of data size. This paper describes the CMU-LTI submission to the SIGMORPHON 2020 Shared Task 0 on typologically diverse morphological inflection. The (unrestricted) submission uses the cross-lingual approach of our last year’s winning submission (Anastasopoulos and Neubig, 2019), but adapted to use specific transfer languages for each test language. Our system, with fixed non-tuned hyperparameters, achieved a macro-averaged accuracy of 80.65 ranking 20th among 31 systems, but it was still tied for best system in 25 of the 90 total languages. 1 In our submission we built upon our previous work (Anastasopoulos and Neubig, 2019), utilizing cross-lingual transfer from related languages, data hallucination, and a series of training techniques and regularizers. The defining change was that we attempted to create language-specific regimes for each test language, depending o"
2020.sigmorphon-1.6,K17-2010,0,0.119691,"53.7 90.6 69.0 92.7 90.8 100.0 88.7 70.3 93.0 97.5 74.2 75.1 100.0 91.5 79.0 93.6 97.0 97.4 71.2 68.6 92.6 97.9 sna sot swa swe syc tel tgk tgl tuk udm uig urd uzb vec vep vot vro xno xty zpv zul 100.0 100.0 100.0 95.4 91.6 94.9 93.8 64.0 85.4 97.5 91.9 36.3 51.5 98.8 79.3 77.2 57.3 90.2 90.2 82.9 89.7 Table 1: Accuracy of our system on every language. We highlight the languages where our system was statistically equal to the best system (with p < 0.005). 2 System Description Data Hallucination for tonal languages The data hallucination process of Anastasopoulos and Neubig (2019), inspired by Silfverberg et al. (2017), samples random characters from the language’s alphabet to replace characters in stem-like regions discovered from the training examples through a simple alignment-based heuristic. Our system is the same as the one of Anastasopoulos and Neubig (2019): a neural multi-source encoder-decoder (which reads in the lemma and the tag sequences in a disentangled manner using two separate encoders) with a task-specific attention mechanism. We skip providing further redundant information and we direct the interested reader to (Anastasopoulos and Neubig, 2019) for all details. It is important to note, ho"
2020.sigmorphon-1.6,K18-3001,0,0.184259,"Missing"
2020.sigmorphon-1.6,K17-2001,0,0.0903161,"Missing"
2020.sigmorphon-1.6,2020.lrec-1.344,1,0.832794,"Missing"
2020.sltu-1.48,apresjan-etal-2006-syntactically,0,0.0146223,"Missing"
2020.sltu-1.48,Q17-1010,0,0.0921251,"s appropriate when users are looking for orthographically similar, but not necessarily exactly matched strings. Word embeddings have also been successful in approximate search, finding semantic similarities between words even across languages. A word embedding is typically a vector representation, trained on large amounts (at least 1M tokens) of monolingual text, whose values reflect syntactic and semantic characteristics of the word, based on the contexts in which the word appears. These embeddings can be obtained using different algorithms, such as GloVe (Pennington et al., 2014), fastText (Bojanowski et al., 2017), or BERT (Devlin et al., 2019), among many others. 4.2. Graphical User Interface (GUI) A graphical user interface is essential to the system because it makes it easy for our target users (teachers) to query the corpus. Our GUI’s design is simple and languageindependent. From the user’s perspective, the GUI workflow consists of the following steps: 1. An initial query sentence is entered into the interface 347 From the system’s perspective, this interaction requires the following steps: 1. We compute an embedding for each sentence in the entire corpus and store it along with the sentence’s tex"
2020.sltu-1.48,N19-1423,0,0.0206181,"g for orthographically similar, but not necessarily exactly matched strings. Word embeddings have also been successful in approximate search, finding semantic similarities between words even across languages. A word embedding is typically a vector representation, trained on large amounts (at least 1M tokens) of monolingual text, whose values reflect syntactic and semantic characteristics of the word, based on the contexts in which the word appears. These embeddings can be obtained using different algorithms, such as GloVe (Pennington et al., 2014), fastText (Bojanowski et al., 2017), or BERT (Devlin et al., 2019), among many others. 4.2. Graphical User Interface (GUI) A graphical user interface is essential to the system because it makes it easy for our target users (teachers) to query the corpus. Our GUI’s design is simple and languageindependent. From the user’s perspective, the GUI workflow consists of the following steps: 1. An initial query sentence is entered into the interface 347 From the system’s perspective, this interaction requires the following steps: 1. We compute an embedding for each sentence in the entire corpus and store it along with the sentence’s text. 2. On receiving the user’s f"
2020.sltu-1.48,W17-0102,0,0.0708481,"Missing"
2020.sltu-1.48,P14-2050,0,0.00985133,"-ui.com). Embedding algorithms typically assume that a lot of training data is available, and getting good embeddings with a small corpus has been a challenge. Another challenge is that embeddings tend to capture primarily semantic information with some syntactic information, while we want the reverse. Our team has continued to investigate variations on algorithms that might produce the best results. Approaches under consideration include transfer learning with BERT (Devlin et al., 2019), as well as training using skip-grams over part of speech tags or dependency parses (Mikolov et al., 2013; Levy and Goldberg, 2014). We plan to continue developing embedding strategies that are performant and syntactically rich even when trained with little data, to incorporate fuzzy string matching (possibly augmented with regular expression capabilities) into our system, and to conduct human evaluations that will assess the system’s success as a search interface. 5. Social Media Recently, it has become prevalent for speakers or learners of endangered languages to interact with language on social media (Huaman and Stokes, 2011; Jones and UribeJongbloed, 2012). Previous works on developing extensions for social media incl"
2020.sltu-1.48,D15-1166,0,0.0123109,"f this workshop, including issues discussed, and various conceived and implemented technologies for nine languages: Arapaho, Cayuga, Inuktitut, Irish Gaelic, Kidaw’ida, Kwak’wala, Ojibwe, San Juan Quiahije Chatino, and Seneca. Keywords: Low-resource languages, language documentation, language revitalization 1. Introduction Recently there have been large advances in natural language processing and language technology, leading to usable systems for speech recognition (Hinton et al., 2012; Graves et al., 2013; Hannun et al., 2014; Amodei et al., 2016), machine translation (Bahdanau et al., 2015; Luong et al., 2015; Wu et al., 2016), text-to-speech (van den Oord et al., 2016), and question answering (Seo et al., 2017) for a few of the world’s most-spoken languages, such as English, German, and Chinese. However, there is an urgent need for similar technology for the rest of the world’s languages, particularly those that are threatened or endangered. The rapid documentation and revitalization of these languages is of paramount importance, but all too often language technology plays little role in this process. In August 2019, the first edition of a ‘Workshop on Language Technology for Language Documentati"
2020.sltu-1.48,C16-1328,1,0.858998,"er. The universal speech recognition method has already been deployed as an app that can be used for documentation at https://www.dictate.app, but quantitative testing of its utility in an actual documentation scenario (as in Michaud et al. (2018)) is yet to be performed. Currently, Allosaurus can currently only output a phone that it has been trained to recognize, but as confused sounds frequently occur at the same place of articulation (Ng, 1998), it should be possible to find links between Allosaurus’s inventory and the provided inventory. Future work and possible integration with PanPhon (Mortensen et al., 2016) could allow the tool to adapt the output to the 343 nearest available sound (considering phonological distance) in the language’s inventory. 2.2. Phone to Orthography Decoder Ideally, one would like to convert the phones recognized by Allosaurus to native orthography. If successful, this would provide a speech recognition system that can directly recognize to the native orthography for low-resource languages, with minimal expertise and effort. Many low-resource languages have fairly new orthographies that are adapted from standard scripts. Because the orthographies are new, the phonetic value"
2020.sltu-1.48,D14-1162,0,0.0843609,"Missing"
2020.sltu-1.48,P16-1162,0,0.0098494,"our website. While some dictionaries have morphological information available that will allow for matching queries to stems or suffixes, not all do. One approach to producing automatic alternatives is to use morphological segmentation tools, including unsupervised tools that try to infer morphological boundaries from data. For Kwak’wala, we experimented with two approaches: Morfessor, a probabilistic model for doing unsupervised or semi-supervised morphological segmentation (Virpioja et al., 2013) and byte pair encoding, a compression model that can be applied to automatically segment words (Sennrich et al., 2016). We use these models to split words up into approximate roots and affixes, which are then used for search or to cluster similar words to show to users. Note that there is scope for improvement on this front, including having language speakers determine which automatic systems produce the best segmentations, exploring different ways of visualizing related words, and using these to improve dictionary search. 3.2. Inuktitut Inuktitut is a polysynthetic language spoken in Northern Canada and is an official language of the Territory of Nunavut. Inuktitut words contain many morphemes, which 345 are"
2020.wnut-1.20,N19-1388,0,0.018906,"to Spanish, Italian, French, and Portuguese, with our system achieving improvements of up to 3.1 BLEU points compared to the baselines, establishing a new stateof-the-art on the JFLEG - ES dataset.1 1 Introduction Neural machine translation (NMT) approaches have aided the machine translation field in achieving great advances in the recent years, starting with encoder-decoder models with attention (Bahdanau et al., 2014; Luong et al., 2015), to transformers using self-attention (Vaswani et al., 2018), to massively multilingual models that yield large improvements even in low-resource settings (Aharoni et al., 2019; Zhang et al., 2020). Despite these very encouraging developments, the list of shortcomings of NMT is also quite vast (Koehn and Knowles, 2017), and one of the most crucial shortcomings is the lack of robustness to source-side noise.2 When confronted with inputs that are even slightly different from the inputs that the models were trained on, the quality of the outputs significantly degrades. This observation has been confirmed for noise due to typos or character scrambling (Belinkov and Bisk, 2018), due to faulty speech recognition (Heigold et al., 2018), or due to naturally-occurring errors"
2020.wnut-1.20,W19-4822,1,0.899749,"similar to the mistakes that nonnative English speakers make. To do so, we will utilize state-of-the-art pretrained systems and finetune them using pseudo-references over corpora that include real-world noise. The general outline of our approach is straightforward: 1. Start with a English-to-X NMT system pretrained on any available data. 2. Obtain an English Grammar Error Correction ˜ ) of origidataset, which provides tuples (x, x nal and corrected sentences. 3. Translate the corrected sentences obtaining ˜ = NMT(˜ pseudo-references y x). Notation Throughout this work, we use the notation of Anastasopoulos (2019) to denote different types of data: • x: the original, noisy, potentially ungrammatical English sentence. Its tokens will be denoted as xi . ˜ : the English sentence with the correction • x annotations applied to the original sentence x, which is deemed fluent and grammatical. Again, its tokens will be denoted as x ˜i . ˜ : the output of the NMT system when x ˜ is • y provided as input (tokens: y˜j ). This will be our pseudo-reference for fine-tuning or evaluation. For the sake of readability, we use the terms grammatical errors, noise, or edits interchangeably. In the context of this work, th"
2020.wnut-1.20,N19-1311,1,0.83993,"Missing"
2020.wnut-1.20,W19-5361,0,0.0260083,"show that fine-tuning of a multilingual NMT system on several languages is also advisable, yielding better performance for a subset of the languages. • We also discuss the potential of achieving zero-shot robustness, as long as catastrophic forgetting issues can be overcome. 2 Related Work Our work is inspired by and combines two lines of research: (1) robustness studies in NMT and (2) data augmentation. Robust NMT Making robust models for NMT has recently gained popularity, with Shared Tasks organized in the Conference of Machine Translation (Li et al., 2019) and several solutions put forth (Berard et al., 2019; Helcl et al., 2019; Post and Duh, 2019; Zhou et al., 2019; Zheng et al., 2019, et alia). Liu et al. (2018); Karpukhin et al. (2019) focus on creating black-box methods for making synthetic or natural noises. Ebrahimi et al. (2018) use white-box methods and creates adversarial examples for character-level NMT. Anastasopoulos et al. (2019) show that including noisy synthetic data in the training data can increase the model’s robustness without sacrificing performance on clean data, an approach that Tan et al. (2020) extend to more NLP tasks. While these approaches are indeed meritorious and in"
2020.wnut-1.20,W19-4406,0,0.0220083,"uent and grammatical. Again, its tokens will be denoted as x ˜i . ˜ : the output of the NMT system when x ˜ is • y provided as input (tokens: y˜j ). This will be our pseudo-reference for fine-tuning or evaluation. For the sake of readability, we use the terms grammatical errors, noise, or edits interchangeably. In the context of this work, they will all denote the annotated grammatical errors in the source sentences (x). Data There are many publicly available corpora for non-native English that are annotated with corrections, which have been widely used for the Grammar Error Correction tasks (Bryant et al., 2019). We specifically use NUCLE (Dahlmeier et al., 2013), FCE (Yannakoudakis et al., 2011), and Lang-8 (Tajiri et al., 2012) for creating the pseudo-references. For evaluation we use the JFLEG dataset (Napoles et al., 2017) and its accompanying Spanish translations in the JFLEG-ES dataset (Anastasopoulos et al., 2019). The NUS Corpus of Learner English (NUCLE) contains essays written by Singaporean students. It is generally considered the main benchmark for GEC. This dataset consists of 21.3K sentences. The First Certificate in English corpus (FCE) is also made of essays, written by learners who w"
2020.wnut-1.20,P19-1425,0,0.0349968,"Missing"
2020.wnut-1.20,P18-1163,0,0.062766,"y that robustness is not necessary for low-resource languages; to the contrary! We just focus on high-resource settings first as they are the ones that have the potential to affect a larger number of downstream users. 149 Proceedings of the 2020 EMNLP Workshop W-NUT: The Sixth Workshop on Noisy User-generated Text, pages 149–158 c Online, Nov 19, 2020. 2020 Association for Computational Linguistics works. First, the types of realistic “non-native-like” noise that can be synthesized are limited, covering among others typos or simple morphological or syntactic mistakes (Belinkov and Bisk, 2018; Cheng et al., 2018; Anastasopoulos et al., 2019; Tan et al., 2020, et alia) but not covering the interplay between all these or any other higher level issues (e.g. word choice). Our approach has the potential to handle a larger spectrum of language variation, as it appears in naturally occurring data. Second, our choice of fine-tuning, rather than training from scratch as previous works have opted for, leads to lower training times and lower compute needed for similar improvements on robustness. The main drawback of our approach lies in the need for corrected (or “normalized”) versions of “noisy” non-native sen"
2020.wnut-1.20,W13-1703,0,0.0207233,"enoted as x ˜i . ˜ : the output of the NMT system when x ˜ is • y provided as input (tokens: y˜j ). This will be our pseudo-reference for fine-tuning or evaluation. For the sake of readability, we use the terms grammatical errors, noise, or edits interchangeably. In the context of this work, they will all denote the annotated grammatical errors in the source sentences (x). Data There are many publicly available corpora for non-native English that are annotated with corrections, which have been widely used for the Grammar Error Correction tasks (Bryant et al., 2019). We specifically use NUCLE (Dahlmeier et al., 2013), FCE (Yannakoudakis et al., 2011), and Lang-8 (Tajiri et al., 2012) for creating the pseudo-references. For evaluation we use the JFLEG dataset (Napoles et al., 2017) and its accompanying Spanish translations in the JFLEG-ES dataset (Anastasopoulos et al., 2019). The NUS Corpus of Learner English (NUCLE) contains essays written by Singaporean students. It is generally considered the main benchmark for GEC. This dataset consists of 21.3K sentences. The First Certificate in English corpus (FCE) is also made of essays, written by learners who were sitting the English as Second or Other Language"
2020.wnut-1.20,W14-3348,0,0.0442691,"This model uses 12 layers, 16 attention heads, the embedding dimension is 1024, and positional feed-forward dimension is 4096. Dropout is set to 0.1. We use the same learning rate schedule as in (Vaswani et al., 2017) with 500 warm-up steps but only decay the learning rate until it reaches 3 ∗ 10−5 . We fine-tune our models on a V100 GPU for a maximum of 100 epochs (although best validation set performance is reached around 20 to 25 epochs). For testing we use the model with the best performance on the validation dataset. Our validation check interval is set to 0.2. Evaluation We use METEOR (Denkowski and Lavie, 2014) to calculate the f-METEOR scores. We calculate BLEU and f-BLEU scores using Sacrebleu (Post, 2018). We compute statistical significance with paired bootstrap resampling (Koehn, 2004). We name our models in a way that is convenient to understand. Our models are named as such: dataset language; e.g. the NUCLE ES model will refer to the model fine-tuned on the NUCLE dataset for Spanish language. We will overload the naming convention to also refer to datasets in the same way, e.g. the NUCLE ES dataset. 152 Results on English-Spanish We first discuss the results on the JFLEG - ES test set, which"
2020.wnut-1.20,C18-1055,0,0.0545643,"Missing"
2020.wnut-1.20,N16-1101,0,0.0286926,"as: d(y, y ˜ , y, y ˜) = NR(x, x 4 ˜) ˜) d(y, y 100 − BLEU(y, y = ˜) ˜) d(x, x 100 − BLEU(x, x Experiments and Results Experimental Details All data are tokenized and true-cased using the Moses tools (Koehn et al., 2007).We use the SentencePiece (Kudo and Richardson, 2018) toolkit to split the sentences into sub-words. We use the unigram language model algorithm of the toolkit with 65,000 operations. We filter the fine-tuning dataset so that sentence length is capped at 80 words. Target Side Creation Given the recent success and promise of massively multilingual systems (Johnson et al., 2016; Firat et al., 2016), we use as our original model the OPUS-MT multilingual Romance model5 (Tiedemann and Thottingal, 2020), trained using Marian NMT (JunczysDowmunt et al., 2018) within the HuggingFace’s Transformers library (Wolf et al., 2019). For every dataset we pass source sentences (both original and corrected versions) and obtain target side sentences. Then we use the corrected target side sentences as our ground truth for fine-tuning the same model. Transformer Model Details We use a transformer architecture as they have shown to be much superior to recurrent architectures. We use HugginFace’s Transforme"
2020.wnut-1.20,W18-1807,0,0.042009,"Missing"
2020.wnut-1.20,W19-5364,0,0.0346725,"Missing"
2020.wnut-1.20,Q17-1024,0,0.0794144,"Missing"
2020.wnut-1.20,D19-5506,0,0.0247197,"et of the languages. • We also discuss the potential of achieving zero-shot robustness, as long as catastrophic forgetting issues can be overcome. 2 Related Work Our work is inspired by and combines two lines of research: (1) robustness studies in NMT and (2) data augmentation. Robust NMT Making robust models for NMT has recently gained popularity, with Shared Tasks organized in the Conference of Machine Translation (Li et al., 2019) and several solutions put forth (Berard et al., 2019; Helcl et al., 2019; Post and Duh, 2019; Zhou et al., 2019; Zheng et al., 2019, et alia). Liu et al. (2018); Karpukhin et al. (2019) focus on creating black-box methods for making synthetic or natural noises. Ebrahimi et al. (2018) use white-box methods and creates adversarial examples for character-level NMT. Anastasopoulos et al. (2019) show that including noisy synthetic data in the training data can increase the model’s robustness without sacrificing performance on clean data, an approach that Tan et al. (2020) extend to more NLP tasks. While these approaches are indeed meritorious and indeed improve a model’s robustness, we argue that one needs to use natural noise instead, on account of two phenomena. The first is la"
2020.wnut-1.20,W04-3250,0,0.302018,"Vaswani et al., 2017) with 500 warm-up steps but only decay the learning rate until it reaches 3 ∗ 10−5 . We fine-tune our models on a V100 GPU for a maximum of 100 epochs (although best validation set performance is reached around 20 to 25 epochs). For testing we use the model with the best performance on the validation dataset. Our validation check interval is set to 0.2. Evaluation We use METEOR (Denkowski and Lavie, 2014) to calculate the f-METEOR scores. We calculate BLEU and f-BLEU scores using Sacrebleu (Post, 2018). We compute statistical significance with paired bootstrap resampling (Koehn, 2004). We name our models in a way that is convenient to understand. Our models are named as such: dataset language; e.g. the NUCLE ES model will refer to the model fine-tuned on the NUCLE dataset for Spanish language. We will overload the naming convention to also refer to datasets in the same way, e.g. the NUCLE ES dataset. 152 Results on English-Spanish We first discuss the results on the JFLEG - ES test set, which is the only dataset with human gold references. The performance of our systems on the JFLEG ES test set, as measured by detokenized BLEU is 5 name: Helsinki-NLP/opus-mt-en-ROMANCE Sys"
2020.wnut-1.20,P07-2045,0,0.0113278,"NR) is the ratio between the target- and source-side BLEU score between noisy and corrected sentences. All other measures do not take into consideration how large are the source-side differences. The intuition behind this metric is that if there is minimal perturba˜ ) on the input side then there should be tion d(x, x minimal reflection on the target side perturbation ˜ ) as well. NR is computed as: d(y, y ˜ , y, y ˜) = NR(x, x 4 ˜) ˜) d(y, y 100 − BLEU(y, y = ˜) ˜) d(x, x 100 − BLEU(x, x Experiments and Results Experimental Details All data are tokenized and true-cased using the Moses tools (Koehn et al., 2007).We use the SentencePiece (Kudo and Richardson, 2018) toolkit to split the sentences into sub-words. We use the unigram language model algorithm of the toolkit with 65,000 operations. We filter the fine-tuning dataset so that sentence length is capped at 80 words. Target Side Creation Given the recent success and promise of massively multilingual systems (Johnson et al., 2016; Firat et al., 2016), we use as our original model the OPUS-MT multilingual Romance model5 (Tiedemann and Thottingal, 2020), trained using Marian NMT (JunczysDowmunt et al., 2018) within the HuggingFace’s Transformers lib"
2020.wnut-1.20,W17-3204,0,0.0256506,"blishing a new stateof-the-art on the JFLEG - ES dataset.1 1 Introduction Neural machine translation (NMT) approaches have aided the machine translation field in achieving great advances in the recent years, starting with encoder-decoder models with attention (Bahdanau et al., 2014; Luong et al., 2015), to transformers using self-attention (Vaswani et al., 2018), to massively multilingual models that yield large improvements even in low-resource settings (Aharoni et al., 2019; Zhang et al., 2020). Despite these very encouraging developments, the list of shortcomings of NMT is also quite vast (Koehn and Knowles, 2017), and one of the most crucial shortcomings is the lack of robustness to source-side noise.2 When confronted with inputs that are even slightly different from the inputs that the models were trained on, the quality of the outputs significantly degrades. This observation has been confirmed for noise due to typos or character scrambling (Belinkov and Bisk, 2018), due to faulty speech recognition (Heigold et al., 2018), or due to naturally-occurring errors by second-language non-native speakers (Anastasopoulos et al., 2019). However, this issue can particularly degrade the user experience for mill"
2020.wnut-1.20,D18-2012,0,0.0204413,"e-side BLEU score between noisy and corrected sentences. All other measures do not take into consideration how large are the source-side differences. The intuition behind this metric is that if there is minimal perturba˜ ) on the input side then there should be tion d(x, x minimal reflection on the target side perturbation ˜ ) as well. NR is computed as: d(y, y ˜ , y, y ˜) = NR(x, x 4 ˜) ˜) d(y, y 100 − BLEU(y, y = ˜) ˜) d(x, x 100 − BLEU(x, x Experiments and Results Experimental Details All data are tokenized and true-cased using the Moses tools (Koehn et al., 2007).We use the SentencePiece (Kudo and Richardson, 2018) toolkit to split the sentences into sub-words. We use the unigram language model algorithm of the toolkit with 65,000 operations. We filter the fine-tuning dataset so that sentence length is capped at 80 words. Target Side Creation Given the recent success and promise of massively multilingual systems (Johnson et al., 2016; Firat et al., 2016), we use as our original model the OPUS-MT multilingual Romance model5 (Tiedemann and Thottingal, 2020), trained using Marian NMT (JunczysDowmunt et al., 2018) within the HuggingFace’s Transformers library (Wolf et al., 2019). For every dataset we pass s"
2020.wnut-1.20,W19-5303,1,0.830423,"r errors and input from non-native speakers. • We show that fine-tuning of a multilingual NMT system on several languages is also advisable, yielding better performance for a subset of the languages. • We also discuss the potential of achieving zero-shot robustness, as long as catastrophic forgetting issues can be overcome. 2 Related Work Our work is inspired by and combines two lines of research: (1) robustness studies in NMT and (2) data augmentation. Robust NMT Making robust models for NMT has recently gained popularity, with Shared Tasks organized in the Conference of Machine Translation (Li et al., 2019) and several solutions put forth (Berard et al., 2019; Helcl et al., 2019; Post and Duh, 2019; Zhou et al., 2019; Zheng et al., 2019, et alia). Liu et al. (2018); Karpukhin et al. (2019) focus on creating black-box methods for making synthetic or natural noises. Ebrahimi et al. (2018) use white-box methods and creates adversarial examples for character-level NMT. Anastasopoulos et al. (2019) show that including noisy synthetic data in the training data can increase the model’s robustness without sacrificing performance on clean data, an approach that Tan et al. (2020) extend to more NLP tasks."
2020.wnut-1.20,D15-1166,0,0.0567087,"uts translated using the baseline NMT system) is a promising solution towards systems robust to such type of input variations. We focus on four translation pairs, from English to Spanish, Italian, French, and Portuguese, with our system achieving improvements of up to 3.1 BLEU points compared to the baselines, establishing a new stateof-the-art on the JFLEG - ES dataset.1 1 Introduction Neural machine translation (NMT) approaches have aided the machine translation field in achieving great advances in the recent years, starting with encoder-decoder models with attention (Bahdanau et al., 2014; Luong et al., 2015), to transformers using self-attention (Vaswani et al., 2018), to massively multilingual models that yield large improvements even in low-resource settings (Aharoni et al., 2019; Zhang et al., 2020). Despite these very encouraging developments, the list of shortcomings of NMT is also quite vast (Koehn and Knowles, 2017), and one of the most crucial shortcomings is the lack of robustness to source-side noise.2 When confronted with inputs that are even slightly different from the inputs that the models were trained on, the quality of the outputs significantly degrades. This observation has been"
2020.wnut-1.20,N19-1314,0,0.0783131,"augmentation to address gender bias issues (Zmigrod et al., 2019, et alia). For our purposes, we will focus on data augmentation techniques aimed at increasing NMT robustness. Simple perturbations typically used include the infusion of character-level noise (e.g. character scrambling (Heigold et al., 2018) or typos (Belinkov and Bisk, 2018)) or word order scrambling (Sperber et al., 2017). Cheng et al. (2018, 2019b) propose a gradient based method to attack the translation model with adversarial source examples, but there’s not guarantee that the adversarial attack results in realistic noise (Michel et al., 2019a). Anastasopoulos et al. (2019) add specific types of errors (such as subject-verb agreement or determiner errors) on the source-side of parallel data, while Tan et al. (2020) specifically perturb the inflectional morphology of words to create adversarial examples and show that adversarial finetuning them for a single epoch significantly improves robustness. Our work is highly motivated from these last two works, but instead of creating synthetic perturbed adversarial examples we use real noisy examples. 3 Fine-tuning for Robustness Our goal is to achieve robustness to source-side variations"
2020.wnut-1.20,D18-1050,0,0.0208842,"iations that the models will have to contend with are not static, but rather constantly changing at an ever-increasing pace. Second, and perhaps a partial direct consequence of the first point, one cannot rely on synthetic examples to properly capture the wide variety of naturallyoccurring variations. Besides, if one could properly model noise creation, they could also similarly model the inverse problem adequately, namely remove said noise, in which case a noise-removing preprocessing step would be most likely suffiecient to tackle the issue. On working with real-world noise, the approach of Michel and Neubig (2018) is the most similar to ours. They collected “noisy” English, French, and Japanese sentences from Reddit, created translations, and split their dataset (MTNT) into train, development, and test, ranging from 5 to 36 thousand training examples. To build robust NMT systems, they first train a model on standard clean data and then fine-tune it on the training portion of MTNT using techniques from domain adaptation. The main difference between this worthy effort and our approach is two-fold. First, our approach does not require gold translations of the noisy inputs, which can be expensive and hard"
2020.wnut-1.20,E17-2037,0,0.0918659,"the sake of readability, we use the terms grammatical errors, noise, or edits interchangeably. In the context of this work, they will all denote the annotated grammatical errors in the source sentences (x). Data There are many publicly available corpora for non-native English that are annotated with corrections, which have been widely used for the Grammar Error Correction tasks (Bryant et al., 2019). We specifically use NUCLE (Dahlmeier et al., 2013), FCE (Yannakoudakis et al., 2011), and Lang-8 (Tajiri et al., 2012) for creating the pseudo-references. For evaluation we use the JFLEG dataset (Napoles et al., 2017) and its accompanying Spanish translations in the JFLEG-ES dataset (Anastasopoulos et al., 2019). The NUS Corpus of Learner English (NUCLE) contains essays written by Singaporean students. It is generally considered the main benchmark for GEC. This dataset consists of 21.3K sentences. The First Certificate in English corpus (FCE) is also made of essays, written by learners who were sitting the English as Second or Other Language (ESOL) examinations. We use the publicly available version, which includes 17.6K sentences. Lang-8 is a slightly different dataset than the previous two datasets. This"
2020.wnut-1.20,P02-1040,0,0.107487,"cy-Extended GUG corpus (JFLEG) is a small corpus of only 1.3K sentences, 151 intended only for evaluation. It has an unique character that is different from other datasets, as it contains correction annotations that include extended fluency edits rather than just minimal grammatical ones. The JFLEG corpus was translated into Spanish by Anastasopoulos et al. (2019) to create the JFLEG-ES corpus, which provides gold-standard Spanish translations for every JFLEG sentence. Evaluation In cases where we have access to human references, we can simply evaluate with reference-based metrics (e.g. BLEU (Papineni et al., 2002)). Unfortunately, we only have references for the JFLEG-ES dataset in Spanish. For all other datasets and languages, we treat the translations of the corrected clean English sources as pseudo-references, and use the metrics from (Anastasopoulos, 2019): Robustness Score (RB), f-BLEU, f-METEOR, and Noise Ratio (NR). Robustness Score (RB) is defined as the percentage of translations of noisy sentences that are exactly the same as the translation of the respective corrected sentence. f-BLEU and f-METEOR are slight modification of the popular BLEU and METEOR metrics. The only difference is that the"
2020.wnut-1.20,W18-6319,0,0.012144,"sion is 4096. Dropout is set to 0.1. We use the same learning rate schedule as in (Vaswani et al., 2017) with 500 warm-up steps but only decay the learning rate until it reaches 3 ∗ 10−5 . We fine-tune our models on a V100 GPU for a maximum of 100 epochs (although best validation set performance is reached around 20 to 25 epochs). For testing we use the model with the best performance on the validation dataset. Our validation check interval is set to 0.2. Evaluation We use METEOR (Denkowski and Lavie, 2014) to calculate the f-METEOR scores. We calculate BLEU and f-BLEU scores using Sacrebleu (Post, 2018). We compute statistical significance with paired bootstrap resampling (Koehn, 2004). We name our models in a way that is convenient to understand. Our models are named as such: dataset language; e.g. the NUCLE ES model will refer to the model fine-tuned on the NUCLE dataset for Spanish language. We will overload the naming convention to also refer to datasets in the same way, e.g. the NUCLE ES dataset. 152 Results on English-Spanish We first discuss the results on the JFLEG - ES test set, which is the only dataset with human gold references. The performance of our systems on the JFLEG ES test"
2020.wnut-1.20,P16-1009,0,0.034985,"h-to-transcription-to-translation datasets are 150 4 “tbh” stands for “to be honest” and “smh” for “shake my head”. ˜ ) pairs. 4. Fine-tune the NMT system on (x, y very costly to produce, they use standard speechto-transcription datasets instead. They translate the gold transcription data set to get translation pseudoreferences. Then they jointly train the model on noisy source transcription using the pseudoreference translations as the target. Data Augmentation Data augmentation techniques have become increasingly popular for MT and other NLP tasks, from back-translation of monolingual data (Sennrich et al., 2016) to counterfactual augmentation to address gender bias issues (Zmigrod et al., 2019, et alia). For our purposes, we will focus on data augmentation techniques aimed at increasing NMT robustness. Simple perturbations typically used include the infusion of character-level noise (e.g. character scrambling (Heigold et al., 2018) or typos (Belinkov and Bisk, 2018)) or word order scrambling (Sperber et al., 2017). Cheng et al. (2018, 2019b) propose a gradient based method to attack the translation model with adversarial source examples, but there’s not guarantee that the adversarial attack results i"
2020.wnut-1.20,P12-2039,0,0.0199934,"ded as input (tokens: y˜j ). This will be our pseudo-reference for fine-tuning or evaluation. For the sake of readability, we use the terms grammatical errors, noise, or edits interchangeably. In the context of this work, they will all denote the annotated grammatical errors in the source sentences (x). Data There are many publicly available corpora for non-native English that are annotated with corrections, which have been widely used for the Grammar Error Correction tasks (Bryant et al., 2019). We specifically use NUCLE (Dahlmeier et al., 2013), FCE (Yannakoudakis et al., 2011), and Lang-8 (Tajiri et al., 2012) for creating the pseudo-references. For evaluation we use the JFLEG dataset (Napoles et al., 2017) and its accompanying Spanish translations in the JFLEG-ES dataset (Anastasopoulos et al., 2019). The NUS Corpus of Learner English (NUCLE) contains essays written by Singaporean students. It is generally considered the main benchmark for GEC. This dataset consists of 21.3K sentences. The First Certificate in English corpus (FCE) is also made of essays, written by learners who were sitting the English as Second or Other Language (ESOL) examinations. We use the publicly available version, which in"
2020.wnut-1.20,2020.acl-main.263,0,0.212467,"ce languages; to the contrary! We just focus on high-resource settings first as they are the ones that have the potential to affect a larger number of downstream users. 149 Proceedings of the 2020 EMNLP Workshop W-NUT: The Sixth Workshop on Noisy User-generated Text, pages 149–158 c Online, Nov 19, 2020. 2020 Association for Computational Linguistics works. First, the types of realistic “non-native-like” noise that can be synthesized are limited, covering among others typos or simple morphological or syntactic mistakes (Belinkov and Bisk, 2018; Cheng et al., 2018; Anastasopoulos et al., 2019; Tan et al., 2020, et alia) but not covering the interplay between all these or any other higher level issues (e.g. word choice). Our approach has the potential to handle a larger spectrum of language variation, as it appears in naturally occurring data. Second, our choice of fine-tuning, rather than training from scratch as previous works have opted for, leads to lower training times and lower compute needed for similar improvements on robustness. The main drawback of our approach lies in the need for corrected (or “normalized”) versions of “noisy” non-native sentences, but we take solace in the fact that at"
2020.wnut-1.20,N19-1209,0,0.0198979,"we fine-tuned solely on English-Italian, EnglishFrench, and English-Portuguese. Unfortunately, as outlined in Table 4, this simple approach does not work out-of-the-box. Fine-tuning on a single language pair leads to catastrophic forgetting (French, 1999) of the multilingual abilities of the system. This is a phenomenon commonly observed in continued learning or fine-tuning scenarios (Goodfellow et al., 2013) as well as on MT domain adaptation scenarios in particular (Freitag and AlOnaizan, 2016), for the mitigation of which several approaches have been proposed (Lopez-Paz and Ranzato, 2017; Thompson et al., 2019; Michel et al., 2019b, et alia). As this research direction is beyond the scope of this paper, we leave the application of such approaches for future work. 5 Conclusion In this work, we studied the effect of fine-tuning a NMT model using real source-side noise paired with pseudo-references obtained by translating Grammar Error Correction corpora. We confirmed previous works on the utility of training with source-side noise, as it leads to models more robust to non-native English inputs, but also showed that instead of using synthetically-induced noise, we can (a) use real-user data with pseud"
2020.wnut-1.20,2020.eamt-1.61,0,0.0164238,", x Experiments and Results Experimental Details All data are tokenized and true-cased using the Moses tools (Koehn et al., 2007).We use the SentencePiece (Kudo and Richardson, 2018) toolkit to split the sentences into sub-words. We use the unigram language model algorithm of the toolkit with 65,000 operations. We filter the fine-tuning dataset so that sentence length is capped at 80 words. Target Side Creation Given the recent success and promise of massively multilingual systems (Johnson et al., 2016; Firat et al., 2016), we use as our original model the OPUS-MT multilingual Romance model5 (Tiedemann and Thottingal, 2020), trained using Marian NMT (JunczysDowmunt et al., 2018) within the HuggingFace’s Transformers library (Wolf et al., 2019). For every dataset we pass source sentences (both original and corrected versions) and obtain target side sentences. Then we use the corrected target side sentences as our ground truth for fine-tuning the same model. Transformer Model Details We use a transformer architecture as they have shown to be much superior to recurrent architectures. We use HugginFace’s Transformers’ BartForConditionalGeneration as our model and tokenizer. This model uses 12 layers, 16 attention he"
2020.wnut-1.20,W18-1819,0,0.0280819,"g solution towards systems robust to such type of input variations. We focus on four translation pairs, from English to Spanish, Italian, French, and Portuguese, with our system achieving improvements of up to 3.1 BLEU points compared to the baselines, establishing a new stateof-the-art on the JFLEG - ES dataset.1 1 Introduction Neural machine translation (NMT) approaches have aided the machine translation field in achieving great advances in the recent years, starting with encoder-decoder models with attention (Bahdanau et al., 2014; Luong et al., 2015), to transformers using self-attention (Vaswani et al., 2018), to massively multilingual models that yield large improvements even in low-resource settings (Aharoni et al., 2019; Zhang et al., 2020). Despite these very encouraging developments, the list of shortcomings of NMT is also quite vast (Koehn and Knowles, 2017), and one of the most crucial shortcomings is the lack of robustness to source-side noise.2 When confronted with inputs that are even slightly different from the inputs that the models were trained on, the quality of the outputs significantly degrades. This observation has been confirmed for noise due to typos or character scrambling (Bel"
2020.wnut-1.20,P11-1019,0,0.0189339,"t of the NMT system when x ˜ is • y provided as input (tokens: y˜j ). This will be our pseudo-reference for fine-tuning or evaluation. For the sake of readability, we use the terms grammatical errors, noise, or edits interchangeably. In the context of this work, they will all denote the annotated grammatical errors in the source sentences (x). Data There are many publicly available corpora for non-native English that are annotated with corrections, which have been widely used for the Grammar Error Correction tasks (Bryant et al., 2019). We specifically use NUCLE (Dahlmeier et al., 2013), FCE (Yannakoudakis et al., 2011), and Lang-8 (Tajiri et al., 2012) for creating the pseudo-references. For evaluation we use the JFLEG dataset (Napoles et al., 2017) and its accompanying Spanish translations in the JFLEG-ES dataset (Anastasopoulos et al., 2019). The NUS Corpus of Learner English (NUCLE) contains essays written by Singaporean students. It is generally considered the main benchmark for GEC. This dataset consists of 21.3K sentences. The First Certificate in English corpus (FCE) is also made of essays, written by learners who were sitting the English as Second or Other Language (ESOL) examinations. We use the pu"
2020.wnut-1.20,2020.acl-main.148,0,0.019438,"rench, and Portuguese, with our system achieving improvements of up to 3.1 BLEU points compared to the baselines, establishing a new stateof-the-art on the JFLEG - ES dataset.1 1 Introduction Neural machine translation (NMT) approaches have aided the machine translation field in achieving great advances in the recent years, starting with encoder-decoder models with attention (Bahdanau et al., 2014; Luong et al., 2015), to transformers using self-attention (Vaswani et al., 2018), to massively multilingual models that yield large improvements even in low-resource settings (Aharoni et al., 2019; Zhang et al., 2020). Despite these very encouraging developments, the list of shortcomings of NMT is also quite vast (Koehn and Knowles, 2017), and one of the most crucial shortcomings is the lack of robustness to source-side noise.2 When confronted with inputs that are even slightly different from the inputs that the models were trained on, the quality of the outputs significantly degrades. This observation has been confirmed for noise due to typos or character scrambling (Belinkov and Bisk, 2018), due to faulty speech recognition (Heigold et al., 2018), or due to naturally-occurring errors by second-language n"
2020.wnut-1.20,W19-5367,0,0.0193283,"advisable, yielding better performance for a subset of the languages. • We also discuss the potential of achieving zero-shot robustness, as long as catastrophic forgetting issues can be overcome. 2 Related Work Our work is inspired by and combines two lines of research: (1) robustness studies in NMT and (2) data augmentation. Robust NMT Making robust models for NMT has recently gained popularity, with Shared Tasks organized in the Conference of Machine Translation (Li et al., 2019) and several solutions put forth (Berard et al., 2019; Helcl et al., 2019; Post and Duh, 2019; Zhou et al., 2019; Zheng et al., 2019, et alia). Liu et al. (2018); Karpukhin et al. (2019) focus on creating black-box methods for making synthetic or natural noises. Ebrahimi et al. (2018) use white-box methods and creates adversarial examples for character-level NMT. Anastasopoulos et al. (2019) show that including noisy synthetic data in the training data can increase the model’s robustness without sacrificing performance on clean data, an approach that Tan et al. (2020) extend to more NLP tasks. While these approaches are indeed meritorious and indeed improve a model’s robustness, we argue that one needs to use natural noise"
2020.wnut-1.20,W19-5368,1,0.821277,"languages is also advisable, yielding better performance for a subset of the languages. • We also discuss the potential of achieving zero-shot robustness, as long as catastrophic forgetting issues can be overcome. 2 Related Work Our work is inspired by and combines two lines of research: (1) robustness studies in NMT and (2) data augmentation. Robust NMT Making robust models for NMT has recently gained popularity, with Shared Tasks organized in the Conference of Machine Translation (Li et al., 2019) and several solutions put forth (Berard et al., 2019; Helcl et al., 2019; Post and Duh, 2019; Zhou et al., 2019; Zheng et al., 2019, et alia). Liu et al. (2018); Karpukhin et al. (2019) focus on creating black-box methods for making synthetic or natural noises. Ebrahimi et al. (2018) use white-box methods and creates adversarial examples for character-level NMT. Anastasopoulos et al. (2019) show that including noisy synthetic data in the training data can increase the model’s robustness without sacrificing performance on clean data, an approach that Tan et al. (2020) extend to more NLP tasks. While these approaches are indeed meritorious and indeed improve a model’s robustness, we argue that one needs"
2020.wnut-1.20,P19-1161,0,0.0222847,"and “smh” for “shake my head”. ˜ ) pairs. 4. Fine-tune the NMT system on (x, y very costly to produce, they use standard speechto-transcription datasets instead. They translate the gold transcription data set to get translation pseudoreferences. Then they jointly train the model on noisy source transcription using the pseudoreference translations as the target. Data Augmentation Data augmentation techniques have become increasingly popular for MT and other NLP tasks, from back-translation of monolingual data (Sennrich et al., 2016) to counterfactual augmentation to address gender bias issues (Zmigrod et al., 2019, et alia). For our purposes, we will focus on data augmentation techniques aimed at increasing NMT robustness. Simple perturbations typically used include the infusion of character-level noise (e.g. character scrambling (Heigold et al., 2018) or typos (Belinkov and Bisk, 2018)) or word order scrambling (Sperber et al., 2017). Cheng et al. (2018, 2019b) propose a gradient based method to attack the translation model with adversarial source examples, but there’s not guarantee that the adversarial attack results in realistic noise (Michel et al., 2019a). Anastasopoulos et al. (2019) add specific"
2021.acl-short.16,D18-1366,0,0.371718,"he vocabulary from the MT model, making adaptation easier. Now, to finetune this model to generate TGT, we need TGT embeddings. Since the TGT monolingual corpus is small, training fasttext vectors on this corpus from scratch will lead (as we show) to low-quality embeddings. Leveraging the relatedness of STD and TGT and their vocabulary overlap, we use STD embeddings to transfer knowledge to TGT embeddings: for each character ngram in the TGT corpus, we initialize its embedding with the corresponding STD embedding, if available. We then continue training fasttext on the TGT monolingual corpus (Chaudhary et al., 2018). Last, we use a supervised embedding alignment method (Lample et al., 2018a) to project the learned TGT embeddings in the same space as STD. STD and TGT are expected to have a large lexical overlap, so we use identical tokens in both varieties as supervision for this alignment. The obtained embeddings, due to transfer learning from STD, inject additional knowledge in the model. Finally, to obtain a SRC→TGT model, we finetune f on psuedo-parallel SRC – TGT data. Using a STD→SRC MT model (a back-translation model 111 trained using large STD – SRC parallel data with standard settings) we (back)-"
2021.acl-short.16,2020.emnlp-main.214,0,0.248347,"ing We preprocess raw text using Byte Pair Encoding (BPE, Sennrich et al., 2016) with 24K merge operations on each SRC – STD corpus trained separately on SRC and STD. We use the same BPE model to tokenize the monolingual STD data and learn fasttext embeddings (we consider character n-grams of length 3 to 6).2 Splitting the TGT words with the same STD BPE model will result in heavy segmentation, especially when TGT contains characters not present in STD.3 To counter this, we train a joint BPE model with 24K operations on the concatenation of STD and TGT corpora to tokenize TGT corpus following Chronopoulou et al. (2020). This technique increases the number of shared tokens between STD and TGT, thus enabling better cross-variety transfer while learning embeddings and while finetuning. We follow Chaudhary et al. (2018) to train embeddings on the generated TGT vocabulary where we initialize the character n-gram representations for TGT words with STD’s fasttext model wherever available and finetune them on the TGT corpus. Implementation and Evaluation We modify the standard OpenNMT-py seq2seq models of PyTorch (Klein et al., 2017) to train our model with vMF loss (Kumar and Tsvetkov, 2019). Additional hyperparam"
2021.acl-short.16,2020.emnlp-main.480,0,0.0478527,"Missing"
2021.acl-short.16,W19-6721,0,0.0282889,"i et al., 2016), and 80M MSA sentences from the CoNLL’17 shared task. For Arabic varieties, we use the MADAR corpus (Bouamor et al., 2018) which consists of 12K 6way parallel sentences between English, MSA and the 4 considered varieties. We ignore the English sentences, sample dev/test sets of 1K sentences each, and consider 10K monolingual sentences for each TGT variety. The second set also has English as SRC with Norwegian Bokmål (NO) as STD and its written variety Nynorsk (NN) as TGT. We use 630K EN - NO sentences from WikiMatrix (Schwenk et al., 2021), and 26M NO sentences from ParaCrawl (Esplà et al., 2019) combined with the WikiMatrix NO sentences to train embeddings. We use 310K NN sentences from WikiMatrix, and TED dev/test sets for both varieties (Reimers and Gurevych, 2020). Preprocessing We preprocess raw text using Byte Pair Encoding (BPE, Sennrich et al., 2016) with 24K merge operations on each SRC – STD corpus trained separately on SRC and STD. We use the same BPE model to tokenize the monolingual STD data and learn fasttext embeddings (we consider character n-grams of length 3 to 6).2 Splitting the TGT words with the same STD BPE model will result in heavy segmentation, especially when"
2021.acl-short.16,2020.findings-emnlp.283,0,0.0383765,"Missing"
2021.acl-short.16,D19-1632,0,0.015424,", Belarusian, Nynorsk, and the four Arabic varieties. For reference, note that the EN→RU, EN→MSA, and EN→NO models are relatively strong, yielding BLEU scores of 24.3, 21.2, and 24.9, respectively. Synthetic Setup Considering STD and TGT as the same language is sub-optimal, as is evident from the poor performance of the non-adapted S UP ( SRC→STD ) model. Clearly, special attention ought to be paid to language varieties. Direct unsupervised translation from SRC to TGT performs poorly as well, confirming previously reported results of the ineffectiveness of such methods on unrelated languages (Guzmán et al., 2019). 5 Additional ablation results are listed in Appendix C. Translating SRC to TGT by pivoting through STD achieves much better performance owing to strong U NSUP ( STD→TGT ) models that leverage the similarities between STD and TGT. However, when resources are scarse (e.g., with 10K monolingual sentences as opposed to 1M), this performance gain considerably diminishes. We attribute this drop to overfitting during the pre-training phase on the small TGT monolingual data. Ablation results (Appendix C) also show that in such low-resource settings the learned embeddings are of low quality. Finally,"
2021.acl-short.16,2020.wmt-1.68,0,0.0324819,"Missing"
2021.acl-short.16,2011.eamt-1.19,0,0.178881,"Missing"
2021.acl-short.16,2020.acl-main.448,0,0.0253534,"and aligning the two embedding spaces. (3) The resulting model is finetuned with pseudo-parallel SRC→TGT data. We compare L ANG VAR MT with the following competitive baselines. S UP ( SRC→STD ): train a standard (softmax-based) supervised SRC→STD model, and consider the output of this model as 2 We slightly modify fasttext to not consider BPE token markers “@@” in the character n-grams. 112 3 For example, both RU and UK alphabets consist of 33 letters; RU has the letters Ёё, ъ, ы and Ээ, which are not used in UK. Instead, UK has Ґґ, Єє, Ii and Її. 4 While we recognize the limitations of BLEU (Mathur et al., 2020), more sophisticated embedding-based metrics for MT evaluation (Zhang et al., 2020; Sellam et al., 2020) are unfortunately not available for low-resource language varieties. Size of TGT corpus 10K 100K 1M 10K 100K 1M 300K Arabic Varieties (10K) Doha Beirut Rabat Tunis S UP ( SRC→STD ) U NSUP ( SRC→TGT ) P IVOT S OFTMAX L ANG VAR MT 1.7 0.3 1.5 1.9 6.1 1.7 0.6 8.6 12.7 13.5 1.7 0.9 14.9 15.4 15.3 1.5 0.4 1.15 1.5 2.3 1.5 0.6 3.9 4.5 8.8 1.5 1.4 8.0 7.9 9.8 11.3 2.7 11.9 14.4 16.6 3.7 0.2 1.8 14.5 20.1 UK BE NN 1.8 0.1 2.1 7.4 8.1 2.0 0.1 1.7 4.9 7.4 1.3 0.1 1.1 3.9 4.6 Table 1: BLEU scores on t"
2021.acl-short.16,P12-2059,0,0.0329472,"Missing"
2021.acl-short.16,2020.acl-main.156,0,0.0322545,"GT, but we show that they improve the overall performance. We discuss the implications of this noise in §4. 3 Experimental Setup Datasets We experiment with two setups. In the first (synthetic) setup, we use English (EN) as SRC, Russian (RU) as STD, and Ukrainian (UK) and Belarusian (BE) as TGTs. We sample 10M EN - RU sentences from the WMT’19 shared task (Ma et al., 2019), and 80M RU sentences from the CoNLL’17 shared task to train embeddings. To simulate lowresource scenarios, we sample 10K, 100K and 1M UK sentences from the CoNLL’17 shared task and BE sentences from the OSCAR corpus (Ortiz Suárez et al., 2020). We use TED dev/test sets for both languages pairs (Cettolo et al., 2012). The second (real world) setup has two language sets: the first one defines English as SRC, with Modern Standard Arabic (MSA) as STD and four Arabic varieties spoken in Doha, Beirut, Rabat and Tunis as TGTs. We sample 10M EN - MSA sentences from the UNPC corpus (Ziemski et al., 2016), and 80M MSA sentences from the CoNLL’17 shared task. For Arabic varieties, we use the MADAR corpus (Bouamor et al., 2018) which consists of 12K 6way parallel sentences between English, MSA and the 4 considered varieties. We ignore the Engl"
2021.acl-short.16,P02-1040,0,0.110582,"abling better cross-variety transfer while learning embeddings and while finetuning. We follow Chaudhary et al. (2018) to train embeddings on the generated TGT vocabulary where we initialize the character n-gram representations for TGT words with STD’s fasttext model wherever available and finetune them on the TGT corpus. Implementation and Evaluation We modify the standard OpenNMT-py seq2seq models of PyTorch (Klein et al., 2017) to train our model with vMF loss (Kumar and Tsvetkov, 2019). Additional hyperparameter details are outlined in Appendix B. We evaluate our methods using BLEU score (Papineni et al., 2002) based on the SacreBLEU implementation (Post, 2018).4 For the Arabic varieties, we also report a macro-average. In addition, to measure the expected impact on actual systems’ users, we follow Faisal et al. (2021) in computing a population-weighted macro-average (avgpop ) based on language community populations provided by Ethnologue (Eberhard et al., 2019). 3.1 Experiments Our proposed framework, L ANG VAR MT, consists of three main components: (1) A supervised SRC → STD model is trained to predict continuous STD word embeddings rather than discrete softmax probabilities. (2) Output STD embedd"
2021.acl-short.16,W18-6319,0,0.0133174,"and while finetuning. We follow Chaudhary et al. (2018) to train embeddings on the generated TGT vocabulary where we initialize the character n-gram representations for TGT words with STD’s fasttext model wherever available and finetune them on the TGT corpus. Implementation and Evaluation We modify the standard OpenNMT-py seq2seq models of PyTorch (Klein et al., 2017) to train our model with vMF loss (Kumar and Tsvetkov, 2019). Additional hyperparameter details are outlined in Appendix B. We evaluate our methods using BLEU score (Papineni et al., 2002) based on the SacreBLEU implementation (Post, 2018).4 For the Arabic varieties, we also report a macro-average. In addition, to measure the expected impact on actual systems’ users, we follow Faisal et al. (2021) in computing a population-weighted macro-average (avgpop ) based on language community populations provided by Ethnologue (Eberhard et al., 2019). 3.1 Experiments Our proposed framework, L ANG VAR MT, consists of three main components: (1) A supervised SRC → STD model is trained to predict continuous STD word embeddings rather than discrete softmax probabilities. (2) Output STD embeddings are replaced with TGT embeddings. The TGT embe"
2021.acl-short.16,D17-1266,0,0.0391541,"Missing"
2021.acl-short.16,2020.acl-demos.14,0,0.0283906,"how that in such low-resource settings the learned embeddings are of low quality. Finally, L ANG VAR MT consistently outperforms all baselines. Using 1M UK sentences, it achieves similar performance (for EN→UK) to the softmax ablation of our method, S OFTMAX, and small gains over unsupervised methods. However, in lower resource settings our approach is clearly better than the strongest baselines by over 4 BLEU points for UK (10K) and 3.9 points for BE (100K). To identify potential sources of error in our proposed method, we lemmatize the generated translations and test sets and evaluate BLEU (Qi et al., 2020). Across all data sizes, both UK and BE achieve a substantial increase in BLEU (up to +6 BLEU; see Appendix D for details) compared to that obtained on raw text, indicating morphological errors in the translations. In future work, we will investigate whether we can alleviate this issue by considering TGT embeddings based on morphological features of tokens (Chaudhary et al., 2018). Real-world Setup The effectiveness of L ANG VAR MT is pronounced in this setup with a dramatic improvement of more than 18 BLEU points over unsupervised baselines when translating into Doha Arabic. We hypothesize th"
2021.acl-short.16,2020.emnlp-main.365,0,0.0279739,"arallel sentences between English, MSA and the 4 considered varieties. We ignore the English sentences, sample dev/test sets of 1K sentences each, and consider 10K monolingual sentences for each TGT variety. The second set also has English as SRC with Norwegian Bokmål (NO) as STD and its written variety Nynorsk (NN) as TGT. We use 630K EN - NO sentences from WikiMatrix (Schwenk et al., 2021), and 26M NO sentences from ParaCrawl (Esplà et al., 2019) combined with the WikiMatrix NO sentences to train embeddings. We use 310K NN sentences from WikiMatrix, and TED dev/test sets for both varieties (Reimers and Gurevych, 2020). Preprocessing We preprocess raw text using Byte Pair Encoding (BPE, Sennrich et al., 2016) with 24K merge operations on each SRC – STD corpus trained separately on SRC and STD. We use the same BPE model to tokenize the monolingual STD data and learn fasttext embeddings (we consider character n-grams of length 3 to 6).2 Splitting the TGT words with the same STD BPE model will result in heavy segmentation, especially when TGT contains characters not present in STD.3 To counter this, we train a joint BPE model with 24K operations on the concatenation of STD and TGT corpora to tokenize TGT corpu"
2021.acl-short.16,2021.eacl-main.115,0,0.0357741,"e sample 10M EN - MSA sentences from the UNPC corpus (Ziemski et al., 2016), and 80M MSA sentences from the CoNLL’17 shared task. For Arabic varieties, we use the MADAR corpus (Bouamor et al., 2018) which consists of 12K 6way parallel sentences between English, MSA and the 4 considered varieties. We ignore the English sentences, sample dev/test sets of 1K sentences each, and consider 10K monolingual sentences for each TGT variety. The second set also has English as SRC with Norwegian Bokmål (NO) as STD and its written variety Nynorsk (NN) as TGT. We use 630K EN - NO sentences from WikiMatrix (Schwenk et al., 2021), and 26M NO sentences from ParaCrawl (Esplà et al., 2019) combined with the WikiMatrix NO sentences to train embeddings. We use 310K NN sentences from WikiMatrix, and TED dev/test sets for both varieties (Reimers and Gurevych, 2020). Preprocessing We preprocess raw text using Byte Pair Encoding (BPE, Sennrich et al., 2016) with 24K merge operations on each SRC – STD corpus trained separately on SRC and STD. We use the same BPE model to tokenize the monolingual STD data and learn fasttext embeddings (we consider character n-grams of length 3 to 6).2 Splitting the TGT words with the same STD BP"
2021.acl-short.16,2020.acl-main.704,0,0.0247475,"data. We compare L ANG VAR MT with the following competitive baselines. S UP ( SRC→STD ): train a standard (softmax-based) supervised SRC→STD model, and consider the output of this model as 2 We slightly modify fasttext to not consider BPE token markers “@@” in the character n-grams. 112 3 For example, both RU and UK alphabets consist of 33 letters; RU has the letters Ёё, ъ, ы and Ээ, which are not used in UK. Instead, UK has Ґґ, Єє, Ii and Її. 4 While we recognize the limitations of BLEU (Mathur et al., 2020), more sophisticated embedding-based metrics for MT evaluation (Zhang et al., 2020; Sellam et al., 2020) are unfortunately not available for low-resource language varieties. Size of TGT corpus 10K 100K 1M 10K 100K 1M 300K Arabic Varieties (10K) Doha Beirut Rabat Tunis S UP ( SRC→STD ) U NSUP ( SRC→TGT ) P IVOT S OFTMAX L ANG VAR MT 1.7 0.3 1.5 1.9 6.1 1.7 0.6 8.6 12.7 13.5 1.7 0.9 14.9 15.4 15.3 1.5 0.4 1.15 1.5 2.3 1.5 0.6 3.9 4.5 8.8 1.5 1.4 8.0 7.9 9.8 11.3 2.7 11.9 14.4 16.6 3.7 0.2 1.8 14.5 20.1 UK BE NN 1.8 0.1 2.1 7.4 8.1 2.0 0.1 1.7 4.9 7.4 1.3 0.1 1.1 3.9 4.6 Table 1: BLEU scores on translation from English to Ukrainian, Belarusian, Nynorsk, and Arabic dialects with varying amounts of m"
2021.acl-short.16,P16-1162,0,0.0290828,"ences, sample dev/test sets of 1K sentences each, and consider 10K monolingual sentences for each TGT variety. The second set also has English as SRC with Norwegian Bokmål (NO) as STD and its written variety Nynorsk (NN) as TGT. We use 630K EN - NO sentences from WikiMatrix (Schwenk et al., 2021), and 26M NO sentences from ParaCrawl (Esplà et al., 2019) combined with the WikiMatrix NO sentences to train embeddings. We use 310K NN sentences from WikiMatrix, and TED dev/test sets for both varieties (Reimers and Gurevych, 2020). Preprocessing We preprocess raw text using Byte Pair Encoding (BPE, Sennrich et al., 2016) with 24K merge operations on each SRC – STD corpus trained separately on SRC and STD. We use the same BPE model to tokenize the monolingual STD data and learn fasttext embeddings (we consider character n-grams of length 3 to 6).2 Splitting the TGT words with the same STD BPE model will result in heavy segmentation, especially when TGT contains characters not present in STD.3 To counter this, we train a joint BPE model with 24K operations on the concatenation of STD and TGT corpora to tokenize TGT corpus following Chronopoulou et al. (2020). This technique increases the number of shared tokens"
2021.acl-short.16,W07-0705,0,0.168003,"Missing"
2021.acl-short.79,nordhoff-hammarstrom-2012-glottolog,0,0.0445312,"Missing"
2021.acl-short.79,D16-1244,0,0.0803655,"Missing"
2021.acl-short.79,2020.acl-main.764,1,0.773319,"Missing"
2021.emnlp-main.553,D07-1007,0,0.118219,"r understand the lexical selection process, we train a prediction model which allows us to easily extract such descriptions for each lexical choice vy ∈ trans(vx , tx ). In this paper, we use humanreadable descriptions of the features learned by a linear model, where these features are defined over a set of lexical and semantic features extracted from the source sentences in Dhvx ,tx i . For designing features, we take inspiration from prior work which uses extracted contextual information to improve cross-lingual sense disambiguation in machine translation systems (Garcia-Varea et al., 2001; Carpuat and Wu, 2007b,a). 4.1 Model Features For training a lexical selection model θhvx ,tx i for the focus word hvx , tx i, we construct training data from the source-target sentence pairs Dhvx ,tx i . We focus on features extracted only from the current source sentence, although the framework can be easily extended to include features from the target sentence as well. We represent each source sentence xhvx ,tx i ∈ Dhvx ,tx i with a set of features extracted from the neighborhood of the focus word context relevant to the lexical selection process. This neighborhood includes (1) words from the source sentence th"
2021.emnlp-main.553,N19-1423,0,0.00667338,"trained using the same features 6914 Figure 2: Learning Interface. Rules for the correct answer are displayed to the learner after each question. Individual rules that apply to the given example are highlighted for the convenience of the learner. “wall” here refers to an outside wall and the adjective stone serves as a hint in arriving at the correct answer. as LinearSVM, to validate the choice of SVMs as an interpretable model over other alternatives. Further, we check how our interpretable linear SVM model compares with a “performance skyline”; a less interpretable BERT-based neural model (Devlin et al., 2019) that extracts representations of the source sentence from BERT and trains a classifier to predict the correct lexical choice. 5.1 Setup Data: We experiment with two L2 languages: Spanish and Greek. These languages were chosen due to (1) availability of parallel corpora with which to train models, and (2) availability of linguists and annotators to verify and analyze the data used in our experimental setting. For Spanish we use 10 million English-Spanish parallel sentences from OpenSubtitles (Lison and Tiedemann, 2016), Tatoeba, TED (Tiedemann, 2012), and Europarl (Koehn, 2005). 7 For Greek, w"
2021.emnlp-main.553,2021.eacl-main.181,1,0.689503,"5.1 Setup Data: We experiment with two L2 languages: Spanish and Greek. These languages were chosen due to (1) availability of parallel corpora with which to train models, and (2) availability of linguists and annotators to verify and analyze the data used in our experimental setting. For Spanish we use 10 million English-Spanish parallel sentences from OpenSubtitles (Lison and Tiedemann, 2016), Tatoeba, TED (Tiedemann, 2012), and Europarl (Koehn, 2005). 7 For Greek, we use 31 million English-Greek parallel sentences extracted from OpenSubtitles. For word alignment we use the AWESOME aligner (Dou and Neubig, 2021), for lemmatization we use spaCy (Honnibal et al., 2020), for POS tagging and dependency parsing we use Stanza (Qi et al., 2020), and for English WSD we use EWISER (Bevilacqua and Navigli, 2020).8 7 We use only 1 million sentences from Europarl because we found sentences from Europarl to contain fewer semantic subdivisions owing to the very specific domain of the dataset. 8 POS tagging, dependency parsing and WSD is required only for the source language, here English. Using our automatic pipeline (§3), we identify 157 English words which have fine-grained distinctions in Spanish and 707 Englis"
2021.emnlp-main.553,S13-2029,0,0.0517568,"aid the process of learning a new language. We thus plan to extract the rule set Rvx which governs this lexical selection process in a human- and machine-readable format. 3 Identifying Semantic Subdivisions In this section, we describe in detail the procedure for identifying L1 words that have different lexical manifestations in L2 owing to semantic subdivisions. For the purpose of this work, we refer to these different lexical manifestations in L2 as lexical choices and the corresponding L1 words as focus words. Our work is “loosely inspired” by ContraWSD (Rios et al., 2018) and SemEval2013 (Lefever and Hoste, 2013) which construct a dataset for cross-lingual word sense disambiguation, using a semi-automatic approach combining frequency-based heuristics with human supervision. These datasets are restricted to a subset of manually selected nouns (20 for SemEval-2013 and 70-80 for ContraWSD). In contrast, our approach is fully automated going beyond using just frequency-based filters. Furthermore, we do not restrict to any one word class leading to words being identified across different word classes (nouns, verbs, adjectives, adverbs) for both Spanish and Greek.4 We start with a parallel corpus D = {(x1 ,"
2021.emnlp-main.553,L16-1147,0,0.0107863,"pares with a “performance skyline”; a less interpretable BERT-based neural model (Devlin et al., 2019) that extracts representations of the source sentence from BERT and trains a classifier to predict the correct lexical choice. 5.1 Setup Data: We experiment with two L2 languages: Spanish and Greek. These languages were chosen due to (1) availability of parallel corpora with which to train models, and (2) availability of linguists and annotators to verify and analyze the data used in our experimental setting. For Spanish we use 10 million English-Spanish parallel sentences from OpenSubtitles (Lison and Tiedemann, 2016), Tatoeba, TED (Tiedemann, 2012), and Europarl (Koehn, 2005). 7 For Greek, we use 31 million English-Greek parallel sentences extracted from OpenSubtitles. For word alignment we use the AWESOME aligner (Dou and Neubig, 2021), for lemmatization we use spaCy (Honnibal et al., 2020), for POS tagging and dependency parsing we use Stanza (Qi et al., 2020), and for English WSD we use EWISER (Bevilacqua and Navigli, 2020).8 7 We use only 1 million sentences from Europarl because we found sentences from Europarl to contain fewer semantic subdivisions owing to the very specific domain of the dataset. 8"
2021.emnlp-main.553,P01-1027,0,0.243492,"learners to help them better understand the lexical selection process, we train a prediction model which allows us to easily extract such descriptions for each lexical choice vy ∈ trans(vx , tx ). In this paper, we use humanreadable descriptions of the features learned by a linear model, where these features are defined over a set of lexical and semantic features extracted from the source sentences in Dhvx ,tx i . For designing features, we take inspiration from prior work which uses extracted contextual information to improve cross-lingual sense disambiguation in machine translation systems (Garcia-Varea et al., 2001; Carpuat and Wu, 2007b,a). 4.1 Model Features For training a lexical selection model θhvx ,tx i for the focus word hvx , tx i, we construct training data from the source-target sentence pairs Dhvx ,tx i . We focus on features extracted only from the current source sentence, although the framework can be easily extended to include features from the target sentence as well. We represent each source sentence xhvx ,tx i ∈ Dhvx ,tx i with a set of features extracted from the neighborhood of the focus word context relevant to the lexical selection process. This neighborhood includes (1) words from"
2021.emnlp-main.553,W18-6437,0,0.0634725,"interpretable by humans in order to aid the process of learning a new language. We thus plan to extract the rule set Rvx which governs this lexical selection process in a human- and machine-readable format. 3 Identifying Semantic Subdivisions In this section, we describe in detail the procedure for identifying L1 words that have different lexical manifestations in L2 owing to semantic subdivisions. For the purpose of this work, we refer to these different lexical manifestations in L2 as lexical choices and the corresponding L1 words as focus words. Our work is “loosely inspired” by ContraWSD (Rios et al., 2018) and SemEval2013 (Lefever and Hoste, 2013) which construct a dataset for cross-lingual word sense disambiguation, using a semi-automatic approach combining frequency-based heuristics with human supervision. These datasets are restricted to a subset of manually selected nouns (20 for SemEval-2013 and 70-80 for ContraWSD). In contrast, our approach is fully automated going beyond using just frequency-based filters. Furthermore, we do not restrict to any one word class leading to words being identified across different word classes (nouns, verbs, adjectives, adverbs) for both Spanish and Greek.4"
2021.emnlp-main.553,W17-4702,0,0.0199844,"hose formalism can account for only fixed-length ordered contexts restricting their application. Further, these rules use a combination of only lemma and POS tags while our framework uses more features. Cross-lingual word sense disambiguation CLWSD disambiguates a word in-context by providing appropriate translation across languages. Lefever and Hoste (2010) construct a dataset (25 ambiguous English nouns across six languages) semi-automatically from parallel corpora which are then verified by expert translators. Such lexical choice tasks have been created also for evaluating MT systems (Rios Gonzales et al., 2017; Rios et al., 2018). However, these methods cover a limited set of words (mostly nouns) and require some manual intervention during the data creation process. To the best of our knowledge, our proposed pipeline is 14 This is based on explanations collected from native Spanish speakers, which can also be found in Appendix B.4 the only fully automated one that extracts several ambiguous words across multiple POS tags. 8 Future Work While we have demonstrated the efficacy of our extracted rules in teaching new words for two languages, we plan to apply our framework on much less-resourced languag"
2021.emnlp-main.570,W14-3302,0,0.0824889,"Missing"
2021.emnlp-main.570,W18-6111,0,0.0202721,"ne axis in each plot corresponds to the performance on the clean evaluation set. Our models are more robust on both parsing (LAS) and morphological feature prediction. We report results both over the whole treebank and over only the erroneous tokens. task involves identifying and correcting errors relating to spelling, morphosyntax and word choice. For evaluating L’ AMBRE, we only focus on grammar error identification (GEI) and specifically on identification of morphosyntactic errors. We experiment with two morphologically rich languages, Russian and German. We use the FalkoMERLIN GEC corpus (Boyd, 2018) for German and the RULEC-GEC dataset (Rozovskaya and Roth, 2019) for Russian. We focus on error types related to morphology (see A.3). Evaluation: To evaluate the effectiveness of L’ AMBRE , we run it on the training15 splits of the German and Russian GEC datasets. GEC corpora typically annotate single words or phrases as errors (and provide a correction); in contrast, we only identify errors over a dependency link, which can then be mapped over to either the dependent or head token. This difference is not trivial: a subjectverb agreement error, for instance, could be fixed by modifying eithe"
2021.emnlp-main.570,W18-6433,0,0.154861,"s like misspellings, language model scores or parser scores, and are not indicative of specific grammatical knowledge. In contrast, there has recently been a burgeoning of evaluation techniques based on grammatical *Equal contribution acceptability judgments for both language models 1 Code and data are available at https://github.com/ (Marvin and Linzen, 2018; Warstadt et al., 2019; adithya7/lambre. 2 Gauthier et al., 2020) and MT systems (Sennrich, While grammatical well-formedness is often necessary for fluent text, it is not sufficient (Sakaguchi et al., 2016). 2017; Burlot and Yvon, 2017; Burlot et al., 2018). 7131 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7131–7150 c November 7–11, 2021. 2021 Association for Computational Linguistics However, these methods require an existing model to score two sentences that are carefully crafted to be similar, with one sentence being grammatical and the other not. These techniques are usually tailored towards specific downstream systems. Additionally, they do not consider the interaction between multiple mistakes that may occur in the process of generating text (e.g., an incorrect word early in the sentence ma"
2021.emnlp-main.570,W17-4705,0,0.101209,"mple linguistic features like misspellings, language model scores or parser scores, and are not indicative of specific grammatical knowledge. In contrast, there has recently been a burgeoning of evaluation techniques based on grammatical *Equal contribution acceptability judgments for both language models 1 Code and data are available at https://github.com/ (Marvin and Linzen, 2018; Warstadt et al., 2019; adithya7/lambre. 2 Gauthier et al., 2020) and MT systems (Sennrich, While grammatical well-formedness is often necessary for fluent text, it is not sufficient (Sakaguchi et al., 2016). 2017; Burlot and Yvon, 2017; Burlot et al., 2018). 7131 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7131–7150 c November 7–11, 2021. 2021 Association for Computational Linguistics However, these methods require an existing model to score two sentences that are carefully crafted to be similar, with one sentence being grammatical and the other not. These techniques are usually tailored towards specific downstream systems. Additionally, they do not consider the interaction between multiple mistakes that may occur in the process of generating text (e.g., an incorrect word ea"
2021.emnlp-main.570,D13-1174,0,0.0180435,"ality of L’ AMBRE. In future work, it would be interesting to expand the analysis to datasets from other languages, Czech (Náplava and Straka, 2019) and Ukrainian (Syvokon and Nahorna, 2021). 6 Evaluating NLG: A Machine Translation Case Study Grammaticality measures, including L’ AMBRE, can be useful across NLG tasks. Here, we chose MT due to the wide-spread availability of (humanevaluated) system outputs in many languages. In addition to BLEU, chrF and t-BLEU18 are commonly used to evaluate translation into morphologically-rich languages (Goldwater and McClosky, 2005; Toutanova et al., 2008; Chahuneau et al., 2013; Sennrich et al., 2016). Evaluating the well-formedness of MT outputs has previously been studied (Popovi´c et al., 2006). Recent WMT shared tasks included special test suites to inspect linguistic properties of systems (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), which construct an evaluation set of contrastive source sentence pairs (typically English). While such contrastive pairs are very valuable, they only implicitly evaluate well-formedness and require access to underlying MT models to score the contrastive sentences. In contrast, L’ AMBRE explicitly measures well-forme"
2021.emnlp-main.570,2020.emnlp-main.422,1,0.904164,"ic overview is outlined in Figure 1). Our measure can be used directly on text generated from a black-box NLG system, and allows for decomposing the system performance into individual grammar rules that identify specific areas to improve the model’s grammaticality. L’ AMBRE relies on a grammatical description of the language, similar to those linguists and language educators have been producing for decades when they document a language or create teaching materials. Specifically, we consider rules describing morphosyntax, including agreement, case assignment, and verb form selection. Following Chaudhary et al. (2020), we describe a procedure to automatically extract these rules from existing dependency treebanks (§3) with high precision.3 When evaluating NLG outputs, adherence to these rules can be assessed through dependency parses (Figure 1). However, off-the-shelf dependency parsers are trained on grammatically sound text and are not well-suited for parsing ungrammatical (or noisy) text (Hashemi and Hwa, 2016) such as that generated by NLG systems. We propose a method to train more robust dependency parsers and morphological feature taggers by synthesizing morphosyntactic errors in existing treebanks ("
2021.emnlp-main.570,L16-1102,0,0.0697468,"Missing"
2021.emnlp-main.570,2020.acl-demos.10,0,0.0306615,"n dialogue (Specia et al., 2010; Dušek et al., 2017). With the exception of the grammaticality-based metric of Napoles et al. (GBM; 2016), these metrics are derived from simple linguistic features like misspellings, language model scores or parser scores, and are not indicative of specific grammatical knowledge. In contrast, there has recently been a burgeoning of evaluation techniques based on grammatical *Equal contribution acceptability judgments for both language models 1 Code and data are available at https://github.com/ (Marvin and Linzen, 2018; Warstadt et al., 2019; adithya7/lambre. 2 Gauthier et al., 2020) and MT systems (Sennrich, While grammatical well-formedness is often necessary for fluent text, it is not sufficient (Sakaguchi et al., 2016). 2017; Burlot and Yvon, 2017; Burlot et al., 2018). 7131 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7131–7150 c November 7–11, 2021. 2021 Association for Computational Linguistics However, these methods require an existing model to score two sentences that are carefully crafted to be similar, with one sentence being grammatical and the other not. These techniques are usually tailored towards specific do"
2021.emnlp-main.570,W19-7814,0,0.0276629,"gen- rules as ras (PRON, VERB, subj) → CasePRON = Nom and ras (PRON, VERB, obj) → CasePRON = Acc. 7 Many linguists also produce highly formal accounts of Our hypothesis is that certain syntactic construcgrammatical phenomena. However, many of these formalisms are difficult to implement computationally because they are tions require specific morphological feature selecequivalent (in the most egregious cases) to Turing machines. tion from one of their constituents (e.g., pronoun 8 We use the Surface-Syntactic Universal Dependencies subjects need to be in nominative case, but pronoun (SUD) 2.5 (Gerdes et al., 2019). See A.1 for a comparison of UD and SUD. objects only allow for genitive or accusative case in 7133 Greek).9 This implies that the “local” distribution that a specific construction requires will be different from a “global” distribution of morphological feature values computed over the whole treebank. Figure 2 presents an example for German-GSD. We can automatically discover these rules by finding such cases of distortion. First, we obtain a global distribution (G(fx ) = p(fx )) that captures the empirical distribution of the values of a morphological feature f on POS x over the whole treeban"
2021.emnlp-main.570,H05-1085,0,0.160341,"e Russian and German GEC corpora for evaluating the quality of L’ AMBRE. In future work, it would be interesting to expand the analysis to datasets from other languages, Czech (Náplava and Straka, 2019) and Ukrainian (Syvokon and Nahorna, 2021). 6 Evaluating NLG: A Machine Translation Case Study Grammaticality measures, including L’ AMBRE, can be useful across NLG tasks. Here, we chose MT due to the wide-spread availability of (humanevaluated) system outputs in many languages. In addition to BLEU, chrF and t-BLEU18 are commonly used to evaluate translation into morphologically-rich languages (Goldwater and McClosky, 2005; Toutanova et al., 2008; Chahuneau et al., 2013; Sennrich et al., 2016). Evaluating the well-formedness of MT outputs has previously been studied (Popovi´c et al., 2006). Recent WMT shared tasks included special test suites to inspect linguistic properties of systems (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), which construct an evaluation set of contrastive source sentence pairs (typically English). While such contrastive pairs are very valuable, they only implicitly evaluate well-formedness and require access to underlying MT models to score the contrastive sentences. In c"
2021.emnlp-main.570,D16-1182,0,0.0236581,"arguably would be even more effective if combined with hand-curated descriptions created by linguists. We leave this as an interesting direction for future work. In our code, we provide detailed instructions for adding new rules. 4 Parsing Noisy Text Within our evaluation framework, we rely on parsers to generate the dependency trees of potentially malformed or noisy sentences from NLG systems. However, publicly available parsers are typically trained on clean and grammatical text from UD treebanks, and may not generalize to noisy inputs (Daiber and van der Goot, 2016; Sakaguchi et al., 2017; Hashemi and Hwa, 2016, 2018). Therefore, it is necessary to ensure that parsers are robust to any morphology-related errors in the input 9 This class of rules are also often lexicalized, depending on text. Ideally, the tagger should accurately identify the lexeme of either the head or the dependent. In the example the morphological features of incorrect word forms, S.1 of Figure 1, the object phrase lange Bücher (‘long Book’) while the dependency parser remains robust to such is inflected in the accusative case because of the verb lesen (‘read’). Other constructions might require the object declined noise. To this"
2021.emnlp-main.570,D19-1279,0,0.0232009,"n UniMorph. 4.2 Training Robust Parsers To adapt to the noisy input conditions in practical NLP settings like ours, our proposed solution is to re-train the parsers/taggers directly on noisy UD treebanks. With the procedure described above (§4.1) we also add noise to the train splits of the UD v2.5 treebanks and re-train the lemmatizer, tagger, and dependency parser from scratch.14 To retain the performance on clean inputs, we concatenate the original clean train splits with our noisy ones. We experimented with commonly used multilingual parsers like UDPipe (Straka and Straková, 2017), UDify (Kondratyuk and Straka, 2019), and Stanza (Qi et al., 2020), settling on Stanza for its superior performance in preliminary experiments. We use the standard training procedure that yields stateof-the-art results on most UD languages with the default hyperparameters for each treebank. Given that we are inherently tokenizing the text to add morphology-related noise, we reuse the pre-trained tokenizers instead of retraining them on noisy data. Figure 4 compares the performance of the original and our robust parsers on three treebanks. Overall, we notice significant improvements on both LAS (with similar gains on UAS) and UFe"
2021.emnlp-main.570,2020.acl-main.126,0,0.0166514,"gically-rich languages. 1 1 Introduction A variety of natural language processing (NLP) applications such as machine translation (MT), summarization, and dialogue require natural language generation (NLG). Each of these applications has a different objective and therefore task-specific evaluation metrics are commonly used. For instance, reference-based measures such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and chrF (Popovi´c, 2015) are used to evaluate MT, ROUGE (Lin, 2004) is a metric widely used in summarization, and various task-based metrics are used in dialogue (Liang et al., 2020). Regardless of the downstream application, an important aspect of evaluating language generation systems is measuring the fluency of the generated text. In this paper, we propose a metric that can be used to evaluate the grammatical well-formedness of text produced by NLG systems.2 Our metric number, case, gender agreement number, person agreement case assignment root case assignment comp:aux subj S.1 mod comp:obj PRON AUX ADJ NOUN VERB Ich werde lange Bücher lesen I-NOM .1 SG will-1 SG long-ACC . PL Book-ACC . PL read-PTCP S.2 *Ich werden langen Bücher lesen I-NOM .1 SG will-1 PL long-DAT ."
2021.emnlp-main.570,W04-1013,0,0.0247647,"metric on the task of machine translation through a diachronic study of systems translating into morphologically-rich languages. 1 1 Introduction A variety of natural language processing (NLP) applications such as machine translation (MT), summarization, and dialogue require natural language generation (NLG). Each of these applications has a different objective and therefore task-specific evaluation metrics are commonly used. For instance, reference-based measures such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and chrF (Popovi´c, 2015) are used to evaluate MT, ROUGE (Lin, 2004) is a metric widely used in summarization, and various task-based metrics are used in dialogue (Liang et al., 2020). Regardless of the downstream application, an important aspect of evaluating language generation systems is measuring the fluency of the generated text. In this paper, we propose a metric that can be used to evaluate the grammatical well-formedness of text produced by NLG systems.2 Our metric number, case, gender agreement number, person agreement case assignment root case assignment comp:aux subj S.1 mod comp:obj PRON AUX ADJ NOUN VERB Ich werde lange Bücher lesen I-NOM .1 SG wi"
2021.emnlp-main.570,2020.acl-main.490,0,0.0197465,"s 7131–7150 c November 7–11, 2021. 2021 Association for Computational Linguistics However, these methods require an existing model to score two sentences that are carefully crafted to be similar, with one sentence being grammatical and the other not. These techniques are usually tailored towards specific downstream systems. Additionally, they do not consider the interaction between multiple mistakes that may occur in the process of generating text (e.g., an incorrect word early in the sentence may trigger a grammatical error later in the sentence). Most of these methods, with the exception of Mueller et al. (2020), focus only on English or translation to/from English. In this paper, we propose L’ AMBRE, a metric that both evaluates the grammatical well-formedness of text in a fine-grained fashion and can be applied to text from multiple languages. We use widely available dependency parsers to tag and parse target text, and then compute our metric by identifying language-specific morphosyntactic errors in text (a schematic overview is outlined in Figure 1). Our measure can be used directly on text generated from a black-box NLG system, and allows for decomposing the system performance into individual gr"
2021.emnlp-main.570,W18-6450,0,0.0246572,"idelyt-BLEU (Ataman et al., 2020) measures BLEU on outputs used rule-based proofreading software to detect tagged using a morphological analyzer. 7137 en→ cs 1 de et fi ru tr WMT’18 0.84 all ragree 0.91 ras 0.78 -0.06 0.07 -0.10 0.68 0.83 0.62 0.86 0.96 0.77 0.86 0.71 0.89 0.58 0.64 -0.31 WMT’19 all 0.80 0.89 ragree ras 0.70 0.16 0.14 0.13 - 0.85 0.87 0.82 0.57 0.70 0.45 - 0.95 0.95 0.95 0.93 0.93 0.95 0.95 0.95 0.9 ’15 ’16 ’17 ’18 ’19 ’14 ’15 ’16 ’17 ’18 ’19 : systems average ◦: system •: reference For evaluating MT systems, we use the data from the Metrics Shared Task in WMT 2018 and 2019 (Ma et al., 2018, 2019). This corpus includes outputs from all participating systems on the test sets from the News Translation Shared Task (Bojar et al., 2018; Barrault et al., 2019). Our study focuses on systems that translate from English to morphologically-rich target languages: Czech, Estonian, Finnish, German, Russian, and Turkish. We used all relevant languages from the WMT shared task except for Lithuanian and Kazakh, which lack reasonable quality parsers. Correlation Analysis The MT system outputs are accompanied with human judgment scores, both at the segment and system level. In contrast to the ref"
2021.emnlp-main.570,W19-5302,0,0.0157703,"Analysis The MT system outputs are accompanied with human judgment scores, both at the segment and system level. In contrast to the reference-free nature of human judgments, our scorer is both reference-free and source-free. Following the standard WMT procedure for evaluating MT metrics, we measure the Pearson’s r correlations between L’ AMBRE and human z-scores for systems from WMT18 and WMT19. We follow Mathur et al. (2020) to remove outlier systems, since they tend to significantly boost the correlation scores, making the correlations unreliable, especially for the best performing systems (Ma et al., 2019). Table 3 presents the correlation results for WMT18 and WMT19.19 We generally observe moderate to high correlation with human judgments using both sets of rules across all languages, apart from German (WMT18,19). This confirms that grammatically sound output is an important factor in human evaluation of NLG outputs. The correlation is lower with case assignment and verb form choice rules, with notable negative correlations for German, and See A.5 for the corresponding scatter plots. German 0.95 0.94 0.95 0.94 0.9 ’14 Table 3: With a few exceptions, our grammar-based metrics correlate well wit"
2021.emnlp-main.570,D18-1151,0,0.025453,"ut, limiting their applicability to specific tasks like MT or spoken dialogue (Specia et al., 2010; Dušek et al., 2017). With the exception of the grammaticality-based metric of Napoles et al. (GBM; 2016), these metrics are derived from simple linguistic features like misspellings, language model scores or parser scores, and are not indicative of specific grammatical knowledge. In contrast, there has recently been a burgeoning of evaluation techniques based on grammatical *Equal contribution acceptability judgments for both language models 1 Code and data are available at https://github.com/ (Marvin and Linzen, 2018; Warstadt et al., 2019; adithya7/lambre. 2 Gauthier et al., 2020) and MT systems (Sennrich, While grammatical well-formedness is often necessary for fluent text, it is not sufficient (Sakaguchi et al., 2016). 2017; Burlot and Yvon, 2017; Burlot et al., 2018). 7131 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7131–7150 c November 7–11, 2021. 2021 Association for Computational Linguistics However, these methods require an existing model to score two sentences that are carefully crafted to be similar, with one sentence being grammatical and the ot"
2021.emnlp-main.570,2020.acl-main.448,0,0.0251423,"Estonian, Finnish, German, Russian, and Turkish. We used all relevant languages from the WMT shared task except for Lithuanian and Kazakh, which lack reasonable quality parsers. Correlation Analysis The MT system outputs are accompanied with human judgment scores, both at the segment and system level. In contrast to the reference-free nature of human judgments, our scorer is both reference-free and source-free. Following the standard WMT procedure for evaluating MT metrics, we measure the Pearson’s r correlations between L’ AMBRE and human z-scores for systems from WMT18 and WMT19. We follow Mathur et al. (2020) to remove outlier systems, since they tend to significantly boost the correlation scores, making the correlations unreliable, especially for the best performing systems (Ma et al., 2019). Table 3 presents the correlation results for WMT18 and WMT19.19 We generally observe moderate to high correlation with human judgments using both sets of rules across all languages, apart from German (WMT18,19). This confirms that grammatically sound output is an important factor in human evaluation of NLG outputs. The correlation is lower with case assignment and verb form choice rules, with notable negativ"
2021.emnlp-main.570,D19-5545,0,0.0482666,"Missing"
2021.emnlp-main.570,D16-1228,0,0.158847,"gnment and verb form choice, ras ). Analysis: In both languages, we find agreement rules to be of higher quality than case and verb form assignment ones. This phenomenon is more pronounced in German where many case assignment rules are lexeme-dependent, as discussed in §3. Importantly, our proposed robust parsers lead to clear gains in error identification recall, compared to the pre-trained ones (“Original” vs. “Robust” in Table 2). Given the complexity of the errors present in text from non-native learners and the well-known incompleteness of GEC corpora in listing all possible corrections (Napoles et al., 2016), combined with the prevalence of typos and the dataset’s domain difference compared to the 15 We use the train portion due to its large size, therefore parser’s training data, our error identification modgives a better estimate of our L’ AMBRE performance. Note ule performs quite well. that, in this experiment, we do not aim to compare against state-of-the-art GEI tools. To understand where L’ AMBRE fails, we man7136 ually inspected a sample of false positives. First, we notice that tokens with typos are often erroneously tagged and parsed. Our augmentation is only equipped to handle (correct"
2021.emnlp-main.570,2020.acl-demos.14,0,0.0602168,"Missing"
2021.emnlp-main.570,Q19-1001,0,0.0252876,"e on the clean evaluation set. Our models are more robust on both parsing (LAS) and morphological feature prediction. We report results both over the whole treebank and over only the erroneous tokens. task involves identifying and correcting errors relating to spelling, morphosyntax and word choice. For evaluating L’ AMBRE, we only focus on grammar error identification (GEI) and specifically on identification of morphosyntactic errors. We experiment with two morphologically rich languages, Russian and German. We use the FalkoMERLIN GEC corpus (Boyd, 2018) for German and the RULEC-GEC dataset (Rozovskaya and Roth, 2019) for Russian. We focus on error types related to morphology (see A.3). Evaluation: To evaluate the effectiveness of L’ AMBRE , we run it on the training15 splits of the German and Russian GEC datasets. GEC corpora typically annotate single words or phrases as errors (and provide a correction); in contrast, we only identify errors over a dependency link, which can then be mapped over to either the dependent or head token. This difference is not trivial: a subjectverb agreement error, for instance, could be fixed by modifying either the subject or the verb to agree with the other constituent. To"
2021.emnlp-main.570,Q16-1013,0,0.0163878,"ese metrics are derived from simple linguistic features like misspellings, language model scores or parser scores, and are not indicative of specific grammatical knowledge. In contrast, there has recently been a burgeoning of evaluation techniques based on grammatical *Equal contribution acceptability judgments for both language models 1 Code and data are available at https://github.com/ (Marvin and Linzen, 2018; Warstadt et al., 2019; adithya7/lambre. 2 Gauthier et al., 2020) and MT systems (Sennrich, While grammatical well-formedness is often necessary for fluent text, it is not sufficient (Sakaguchi et al., 2016). 2017; Burlot and Yvon, 2017; Burlot et al., 2018). 7131 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7131–7150 c November 7–11, 2021. 2021 Association for Computational Linguistics However, these methods require an existing model to score two sentences that are carefully crafted to be similar, with one sentence being grammatical and the other not. These techniques are usually tailored towards specific downstream systems. Additionally, they do not consider the interaction between multiple mistakes that may occur in the process of generating tex"
2021.emnlp-main.570,P17-2030,0,0.0584137,"Missing"
2021.emnlp-main.570,2020.acl-main.263,0,0.020855,"os and Neubig (2019), but we leave this for future work. For evaluation, we induce noise into the dev portions of the treebanks and test the robustness of off-the-shelf taggers and parsers from Stanza (Qi et al., 2020) (indicative results on Czech, Greek, and Turkish are shown in Figure 4). Along with the overall scores on the dev set, we also report the results only on the altered word forms (“Altered Forms”). Across the three languages, we notice 12 For each token, we first map the morphological feature annotations in the original UD schema to the UniMorph schema (McCarthy et al., 2018). 13 Tan et al. (2020) follows similar methodology using English-only LemmInflect tool, but our approach is scalable to the large number of languages in UniMorph. 4.2 Training Robust Parsers To adapt to the noisy input conditions in practical NLP settings like ours, our proposed solution is to re-train the parsers/taggers directly on noisy UD treebanks. With the procedure described above (§4.1) we also add noise to the train splits of the UD v2.5 treebanks and re-train the lemmatizer, tagger, and dependency parser from scratch.14 To retain the performance on clean inputs, we concatenate the original clean train spl"
2021.emnlp-main.570,P08-1059,0,0.0600684,"ra for evaluating the quality of L’ AMBRE. In future work, it would be interesting to expand the analysis to datasets from other languages, Czech (Náplava and Straka, 2019) and Ukrainian (Syvokon and Nahorna, 2021). 6 Evaluating NLG: A Machine Translation Case Study Grammaticality measures, including L’ AMBRE, can be useful across NLG tasks. Here, we chose MT due to the wide-spread availability of (humanevaluated) system outputs in many languages. In addition to BLEU, chrF and t-BLEU18 are commonly used to evaluate translation into morphologically-rich languages (Goldwater and McClosky, 2005; Toutanova et al., 2008; Chahuneau et al., 2013; Sennrich et al., 2016). Evaluating the well-formedness of MT outputs has previously been studied (Popovi´c et al., 2006). Recent WMT shared tasks included special test suites to inspect linguistic properties of systems (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), which construct an evaluation set of contrastive source sentence pairs (typically English). While such contrastive pairs are very valuable, they only implicitly evaluate well-formedness and require access to underlying MT models to score the contrastive sentences. In contrast, L’ AMBRE explic"
2021.emnlp-main.570,2020.acl-main.660,0,0.0163098,"igure 5: A diachronic study of grammatical wellformedness of WMT English→X systems’ outputs. The systems in general are becoming more fluent. In the last two years the best systems produce as wellformed outputs as the reference translations. Turkish (WMT18). In the case of German, a significant number of case assignment rules are dependent on the lexeme (as noted in §3) and we expect future work on lexicalized rules to partially address this drawback. In Turkish, the low parser quality plays a significant role and highlights the need for further work on parsing morphologically-rich languages (Tsarfaty et al., 2020). Last, we note that human judgments, unlike L’ AMBRE, incorporate both well-formedness and adequacy (with respect to the source). Therefore, we recommend using L’ AMBRE in tandem with standard MT metrics to obtain a good indication of overall performance, both during model training and evaluation. We additionally perform a correlation analysis of L’ AMBRE with perplexity, BLEU and chrF on the WMT system outputs (A.5 in Appendix). As expected, we see a strong negative correlation with perplexity (low perplexity and high L’ AMBRE). For BLEU and chrF, the results are quite similar to the correla"
2021.emnlp-main.570,Q19-1040,0,0.0140367,"ability to specific tasks like MT or spoken dialogue (Specia et al., 2010; Dušek et al., 2017). With the exception of the grammaticality-based metric of Napoles et al. (GBM; 2016), these metrics are derived from simple linguistic features like misspellings, language model scores or parser scores, and are not indicative of specific grammatical knowledge. In contrast, there has recently been a burgeoning of evaluation techniques based on grammatical *Equal contribution acceptability judgments for both language models 1 Code and data are available at https://github.com/ (Marvin and Linzen, 2018; Warstadt et al., 2019; adithya7/lambre. 2 Gauthier et al., 2020) and MT systems (Sennrich, While grammatical well-formedness is often necessary for fluent text, it is not sufficient (Sakaguchi et al., 2016). 2017; Burlot and Yvon, 2017; Burlot et al., 2018). 7131 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7131–7150 c November 7–11, 2021. 2021 Association for Computational Linguistics However, these methods require an existing model to score two sentences that are carefully crafted to be similar, with one sentence being grammatical and the other not. These techniqu"
2021.emnlp-main.570,E17-2060,0,0.0233458,"rammaticality measures, including L’ AMBRE, can be useful across NLG tasks. Here, we chose MT due to the wide-spread availability of (humanevaluated) system outputs in many languages. In addition to BLEU, chrF and t-BLEU18 are commonly used to evaluate translation into morphologically-rich languages (Goldwater and McClosky, 2005; Toutanova et al., 2008; Chahuneau et al., 2013; Sennrich et al., 2016). Evaluating the well-formedness of MT outputs has previously been studied (Popovi´c et al., 2006). Recent WMT shared tasks included special test suites to inspect linguistic properties of systems (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), which construct an evaluation set of contrastive source sentence pairs (typically English). While such contrastive pairs are very valuable, they only implicitly evaluate well-formedness and require access to underlying MT models to score the contrastive sentences. In contrast, L’ AMBRE explicitly measures well-formedness, without requiring access to trained MT models. Comparison with Other Metrics: We also compare L’ AMBRE to other metrics that capture fluency and/or grammatical well-formedness, namely perplexity as computed by large language mode"
2021.emnlp-main.570,P16-1162,0,0.00537509,"ture work, it would be interesting to expand the analysis to datasets from other languages, Czech (Náplava and Straka, 2019) and Ukrainian (Syvokon and Nahorna, 2021). 6 Evaluating NLG: A Machine Translation Case Study Grammaticality measures, including L’ AMBRE, can be useful across NLG tasks. Here, we chose MT due to the wide-spread availability of (humanevaluated) system outputs in many languages. In addition to BLEU, chrF and t-BLEU18 are commonly used to evaluate translation into morphologically-rich languages (Goldwater and McClosky, 2005; Toutanova et al., 2008; Chahuneau et al., 2013; Sennrich et al., 2016). Evaluating the well-formedness of MT outputs has previously been studied (Popovi´c et al., 2006). Recent WMT shared tasks included special test suites to inspect linguistic properties of systems (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), which construct an evaluation set of contrastive source sentence pairs (typically English). While such contrastive pairs are very valuable, they only implicitly evaluate well-formedness and require access to underlying MT models to score the contrastive sentences. In contrast, L’ AMBRE explicitly measures well-formedness, without requiring"
2021.emnlp-main.570,K17-3009,0,0.0289797,"to the large number of languages in UniMorph. 4.2 Training Robust Parsers To adapt to the noisy input conditions in practical NLP settings like ours, our proposed solution is to re-train the parsers/taggers directly on noisy UD treebanks. With the procedure described above (§4.1) we also add noise to the train splits of the UD v2.5 treebanks and re-train the lemmatizer, tagger, and dependency parser from scratch.14 To retain the performance on clean inputs, we concatenate the original clean train splits with our noisy ones. We experimented with commonly used multilingual parsers like UDPipe (Straka and Straková, 2017), UDify (Kondratyuk and Straka, 2019), and Stanza (Qi et al., 2020), settling on Stanza for its superior performance in preliminary experiments. We use the standard training procedure that yields stateof-the-art results on most UD languages with the default hyperparameters for each treebank. Given that we are inherently tokenizing the text to add morphology-related noise, we reuse the pre-trained tokenizers instead of retraining them on noisy data. Figure 4 compares the performance of the original and our robust parsers on three treebanks. Overall, we notice significant improvements on both LA"
2021.iwslt-1.1,2020.lrec-1.520,0,0.0759372,"Missing"
2021.iwslt-1.1,2020.iwslt-1.3,0,0.0592828,"Missing"
2021.iwslt-1.1,2021.iwslt-1.5,0,0.0628415,"Missing"
2021.iwslt-1.1,N19-1202,1,0.897776,"Missing"
2021.iwslt-1.1,2021.acl-long.224,1,0.769761,"sh-German section of the MuST-C V2 corpus7 and include training, dev, and test (Test Common), in the same structure of the MuST-C V1 corpus (Cattoni et al., 2021) used last year. Since the 2021 test set was processed using the same pipeline applied to create MuST-C V2, the use of the new training resource was strongly recommended. The main differences with respect to MuST-C v1 are: gap with respect to the traditional cascade approach (integrating ASR and MT components in a pipelined architecture). In light of last year’s IWSLT results (Ansari et al., 2020) and of the findings of recent works (Bentivogli et al., 2021) attesting that the gap between the two paradigms has substantially closed, also this year a key element of the evaluation was to set up a shared framework for their comparison. For this reason, and to reliably measure progress with respect to the past rounds, the general evaluation setting was kept unchanged. This stability mainly concerns two aspects: the allowed architectures and the test set provision. On the architecture side, participation was allowed both with cascade and end-to-end (also known as direct) systems. In the latter case, valid submissions had to be obtained by models that:"
2021.iwslt-1.1,2021.iwslt-1.22,0,0.082468,"Missing"
2021.iwslt-1.1,2021.iwslt-1.27,1,0.721453,"to use the same training and development data as in the Offline Speech Translation track. More details are available in §3.2. For the English-Japanese text-to-text track, participants could use the parallel data and monolingual data available for the English-Japanese WMT20 news task (Barrault et al., 2020). For development, participants could use the IWSLT 2017 development sets,2 the IWSLT 2021 development set3 and the simultaneous interpretation transcripts for the IWSLT 2021 development set.4 The simultaneous interpretation was recorded as a part of NAIST Simultaneous Interpretation Corpus (Doi et al., 2021). Systems were evaluated with respect to quality and latency. Quality was evaluated with the standard BLEU metric (Papineni et al., 2002a). Latency was evaluated with metrics developed for simultaneous machine translation, including average proportion (AP), average lagging (AL) and differentiable average lagging (DAL, Cherry and Foster 2019), and later extended to the task of simultaneous speech translation (Ma et al., 2020b). The evaluation was run with the S IMUL E VAL toolkit (Ma et al., 2020a). For the latency measurement of speech input systems, we contrasted computation-aware and non com"
2021.iwslt-1.1,2012.eamt-1.60,1,0.698226,"rently with the 16 kHz sample rate. This difference does not guarantee the fully compatibility between V1 and V2 of MuST-C. Besides MuST-C V2, also this year the allowed training corpora include: • MuST-C V1 (Di Gangi et al., 2019); 3.2 Data and Metrics • CoVoST (Wang et al., 2020); Training and development data. Also this year, participants had the possibility to train their systems using several resources available for ST, ASR and MT. The major novelty on the data front 7 http://ict.fbk.eu/must-c/ http://www.youtube.com/c/TED/videos 9 http://youtube-dl.org/ 10 http://ffmpeg.org/ 8 6 • WIT3 (Cettolo et al., 2012) ; Metrics. Systems’ performance was evaluated with respect to their capability to produce translations similar to the target-language references. Differently from previous rounds, where such similarity was measured in terms of multiple automatic metrics,18 this year only the BLEU metric (computed with SacreBLEU (Post, 2018) with default settings) has been considered. Instead of multiple metrics, the attention focused on considering two different types of target-language references, namely: • Speech-Translation TED corpus11 ; • How2 (Sanabria et al., 2018)12 ; • LibriVoxDeEn (Beilharz and Sun,"
2021.iwslt-1.1,2020.iwslt-1.8,1,0.838546,"Missing"
2021.iwslt-1.1,2021.iwslt-1.12,0,0.0248123,"a et al., 2021) NiuTrans Research, Shenyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; communication and access to multilingual multimedia content in real-time. The goal of this challenge, organized for the second consecutive year, is to examine systems that translate text or audio in a source language into text in a target language from the perspective of both translation quality and latency. • Low-resource speech translation, focusing on resource-scarce settings for translating two S"
2021.iwslt-1.1,L18-1001,0,0.0652124,"Missing"
2021.iwslt-1.1,2021.acl-long.68,1,0.787992,"Missing"
2021.iwslt-1.1,L18-1275,0,0.0286278,"ng two different types of target-language references, namely: • Speech-Translation TED corpus11 ; • How2 (Sanabria et al., 2018)12 ; • LibriVoxDeEn (Beilharz and Sun, 2019)13 ; • Europarl-ST (Iranzo-S´anchez et al., 2020); • TED LIUM v2 (Rousseau et al., 2014) and v3 (Hernandez et al., 2018); • WMT 201914 and 202015 ; • The original TED translations. Since these references come in the form of subtitles, they are subject to compression and omissions to adhere to the TED subtitling guidelines.19 This makes them less literal compared to standard, unconstrained translations; • OpenSubtitles 2018 (Lison et al., 2018); • Augmented LibriSpeech et al., 2018)16 (Kocabiyikoglu • Mozilla Common Voice17 ; • Unconstrained translations. These references were created from scratch20 by adhering to the usual translation guidelines. They are hence exact (more literal) translations, without paraphrasing and with proper punctuation. • LibriSpeech ASR corpus (Panayotov et al., 2015). The list of allowed development data includes the dev set from IWSLT 2010, as well as the test sets used for the 2010, 2013, 2014, 2015 and 2018 IWSLT campaigns. Using other training/development resources was allowed but, in this case, parti"
2021.iwslt-1.1,2020.iwslt-1.9,0,0.0594925,"Missing"
2021.iwslt-1.1,rousseau-etal-2014-enhancing,0,0.0742394,"Missing"
2021.iwslt-1.1,2021.iwslt-1.4,0,0.134226,"essler, Italy (Papi et al., 2021) FAIR Speech Translation (Tang et al., 2021a) Huawei Noah’s Ark Lab, China (Zeng et al., 2021) Huawei Translation Services Center, China University of Stuttgart, Germany (Denisov et al., 2021) Karlsruhe Institute of Technology, Germany (Nguyen et al., 2021; Pham et al., 2021) Desheng Li Nara Institute of Science and Technology, Nara, Japan (Fukuda et al., 2021) NiuTrans Research, Shenyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; communication and acces"
2021.iwslt-1.1,N18-2074,0,0.0499123,"Missing"
2021.iwslt-1.1,2006.amta-papers.25,0,0.370434,"Missing"
2021.iwslt-1.1,W14-3354,0,0.0745687,"Missing"
2021.iwslt-1.1,W18-6319,0,0.0144137,"cipants had the possibility to train their systems using several resources available for ST, ASR and MT. The major novelty on the data front 7 http://ict.fbk.eu/must-c/ http://www.youtube.com/c/TED/videos 9 http://youtube-dl.org/ 10 http://ffmpeg.org/ 8 6 • WIT3 (Cettolo et al., 2012) ; Metrics. Systems’ performance was evaluated with respect to their capability to produce translations similar to the target-language references. Differently from previous rounds, where such similarity was measured in terms of multiple automatic metrics,18 this year only the BLEU metric (computed with SacreBLEU (Post, 2018) with default settings) has been considered. Instead of multiple metrics, the attention focused on considering two different types of target-language references, namely: • Speech-Translation TED corpus11 ; • How2 (Sanabria et al., 2018)12 ; • LibriVoxDeEn (Beilharz and Sun, 2019)13 ; • Europarl-ST (Iranzo-S´anchez et al., 2020); • TED LIUM v2 (Rousseau et al., 2014) and v3 (Hernandez et al., 2018); • WMT 201914 and 202015 ; • The original TED translations. Since these references come in the form of subtitles, they are subject to compression and omissions to adhere to the TED subtitling guideli"
2021.iwslt-1.1,2021.iwslt-1.19,0,0.0280723,", 2021) Fondazione Bruno Kessler, Italy (Papi et al., 2021) FAIR Speech Translation (Tang et al., 2021a) Huawei Noah’s Ark Lab, China (Zeng et al., 2021) Huawei Translation Services Center, China University of Stuttgart, Germany (Denisov et al., 2021) Karlsruhe Institute of Technology, Germany (Nguyen et al., 2021; Pham et al., 2021) Desheng Li Nara Institute of Science and Technology, Nara, Japan (Fukuda et al., 2021) NiuTrans Research, Shenyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; comm"
2021.iwslt-1.1,2020.findings-emnlp.230,0,0.0691772,"Missing"
2021.iwslt-1.1,2021.iwslt-1.7,0,0.0794497,"Missing"
2021.iwslt-1.1,2021.iwslt-1.16,0,0.0332503,"nyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; communication and access to multilingual multimedia content in real-time. The goal of this challenge, organized for the second consecutive year, is to examine systems that translate text or audio in a source language into text in a target language from the perspective of both translation quality and latency. • Low-resource speech translation, focusing on resource-scarce settings for translating two Swahili varieties (Congolese and Co"
2021.iwslt-1.1,2020.lrec-1.517,1,0.846095,"s were finally converted from two (stereo) to one (mono) channel and downsampled from 48 to 16 kHz, using FFmpeg.10 Upon inspection of the spectrograms of the same talks in the two versions of MuST-C, it clearly emerges that the upper limit band in the audios used in MuST-C V1 is 5 kHz, while it is at 8 kHz in the latest version, coherently with the 16 kHz sample rate. This difference does not guarantee the fully compatibility between V1 and V2 of MuST-C. Besides MuST-C V2, also this year the allowed training corpora include: • MuST-C V1 (Di Gangi et al., 2019); 3.2 Data and Metrics • CoVoST (Wang et al., 2020); Training and development data. Also this year, participants had the possibility to train their systems using several resources available for ST, ASR and MT. The major novelty on the data front 7 http://ict.fbk.eu/must-c/ http://www.youtube.com/c/TED/videos 9 http://youtube-dl.org/ 10 http://ffmpeg.org/ 8 6 • WIT3 (Cettolo et al., 2012) ; Metrics. Systems’ performance was evaluated with respect to their capability to produce translations similar to the target-language references. Differently from previous rounds, where such similarity was measured in terms of multiple automatic metrics,18 thi"
2021.iwslt-1.1,2021.iwslt-1.6,0,0.156379,"2021) Desheng Li Nara Institute of Science and Technology, Nara, Japan (Fukuda et al., 2021) NiuTrans Research, Shenyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; communication and access to multilingual multimedia content in real-time. The goal of this challenge, organized for the second consecutive year, is to examine systems that translate text or audio in a source language into text in a target language from the perspective of both translation quality and latency. • Low-resource spe"
2021.mrl-1.18,Q18-1039,0,0.0219181,"020a), b) machine translation followed by align- ments using minimal supervision. Qin et al., 2020 ment (Shah et al., 2010; Yarowsky et al., 2001; allows randomized code-switching to include the Ni et al., 2017), or c) a combination of both (Xu target language, as shown in their Figure 3. In et al., 2020). Before transformer models, effective our context for example, if the target language is approaches included domain adversarial training German, we ensure that there is no code-switching to extract language-agnostic features (Ganin et al., to German during training. We consider this dis2016; Chen et al., 2018) and word alignment meth- tinction essential to evaluate a true zero-shot learnods such as MUSE (Conneau et al., 2017) to align ing scenario and prevent any bias when comparfastText word vectors (Bojanowski et al., 2017). ing with translate-and-train. Yang et al. (2020) Recently, Conneau et al., 2020b show that having present a non-zero-shot approach that performs shared parameters in the top layers of the multi- code-switching to target languages, and Jiang et al. lingual encoders can be used to align different lan- (2020) present a code-switching based method to guages quite effectively on t"
2021.mrl-1.18,2020.acl-main.747,0,0.507787,"e, col2019). In a cross-lingual setting, the model should lected during the Haiti earthquake.1 be able to learn this joint task in one language and transfer knowledge to another (Upadhyay et al., 1 Introduction 2018; Schuster et al., 2019; Xu et al., 2020). This is the premise of our work. A cross-lingual setting is typically described as a Highly effective transformer-based multilingual scenario in which a model trained for a particular models such as mBERT (Devlin et al., 2019) and task in one source language (e.g. English) should be able to generalize well to a different target lan- XLM-R (Conneau et al., 2020a) have found success guage (e.g. Japanese). While semi-supervised so- across several multilingual tasks in recent years. In the zero-shot cross-lingual transfer setting with an lutions (Muis et al., 2018; FitzGerald, 2020, inter alia) assume some target language data or trans- unknown target language, a typical solution is to use pre-trained transformer models and fine-tune to lators are available, a zero-shot solution (Eriguchi the downstream task using the monolingual source 1 Implementation and dataset are available at data (Xu et al., 2020). However, Pires et al. (2019) https://github.com"
2021.mrl-1.18,D18-1269,0,0.0706296,"Missing"
2021.mrl-1.18,N18-2118,0,0.15567,"ily is added as an extra group. In experiments, lset in Algorithm 1 will be assigned languages from a specific family. 2.4 Joint Training Joint training is traditionally used for intent prediction and slot filling to exploit the correlation between the two tasks. This is done by feeding the feature vectors of one model to another or by sharing layers of a neural network followed by training the tasks together. So, a standard joint model loss can be defined as a combination of intent (Li ) and slot (Lsl ) losses. i.e., L = αLi + βLsl , where α and β are corresponding task weights. Prior works (Goo et al., 2018; Schuster et al., 2019; Liu and Lane, 2016; Haihong et al., 2019) that use BiLSTM or RNN are now modified to BERT-based implementations explored in more recent works (Chen et al., 2019; Hardalov et al., 2020; Xu et al., 2020). A standard Joint model consists of BERT outputs from the final hidden state (classification (CLS) token for intent and m word tokens for slots) fed to linear layers to get intent and slot predictions. Assuming hcls represents the CLS token and hm represents a token from the remaining word-level tokens, the BERT model outputs are defined as (Chen et al., 2019; Xu et al.,"
2021.mrl-1.18,2020.acl-main.536,0,0.46228,"e, col2019). In a cross-lingual setting, the model should lected during the Haiti earthquake.1 be able to learn this joint task in one language and transfer knowledge to another (Upadhyay et al., 1 Introduction 2018; Schuster et al., 2019; Xu et al., 2020). This is the premise of our work. A cross-lingual setting is typically described as a Highly effective transformer-based multilingual scenario in which a model trained for a particular models such as mBERT (Devlin et al., 2019) and task in one source language (e.g. English) should be able to generalize well to a different target lan- XLM-R (Conneau et al., 2020a) have found success guage (e.g. Japanese). While semi-supervised so- across several multilingual tasks in recent years. In the zero-shot cross-lingual transfer setting with an lutions (Muis et al., 2018; FitzGerald, 2020, inter alia) assume some target language data or trans- unknown target language, a typical solution is to use pre-trained transformer models and fine-tune to lators are available, a zero-shot solution (Eriguchi the downstream task using the monolingual source 1 Implementation and dataset are available at data (Xu et al., 2020). However, Pires et al. (2019) https://github.com"
2021.mrl-1.18,P19-1544,0,0.122656,"ithm 1 will be assigned languages from a specific family. 2.4 Joint Training Joint training is traditionally used for intent prediction and slot filling to exploit the correlation between the two tasks. This is done by feeding the feature vectors of one model to another or by sharing layers of a neural network followed by training the tasks together. So, a standard joint model loss can be defined as a combination of intent (Li ) and slot (Lsl ) losses. i.e., L = αLi + βLsl , where α and β are corresponding task weights. Prior works (Goo et al., 2018; Schuster et al., 2019; Liu and Lane, 2016; Haihong et al., 2019) that use BiLSTM or RNN are now modified to BERT-based implementations explored in more recent works (Chen et al., 2019; Hardalov et al., 2020; Xu et al., 2020). A standard Joint model consists of BERT outputs from the final hidden state (classification (CLS) token for intent and m word tokens for slots) fed to linear layers to get intent and slot predictions. Assuming hcls represents the CLS token and hm represents a token from the remaining word-level tokens, the BERT model outputs are defined as (Chen et al., 2019; Xu et al., 2020): pi = sof tmax(W i hcls + bi ) sl sl psl m = sof tmax(W hm"
2021.mrl-1.18,N19-1423,0,0.497077,"a new human-annotated tweet dataset of hong et al., 2019; Hardalov et al., 2020; Chen et al., slot filling in English and Haitian Creole, col2019). In a cross-lingual setting, the model should lected during the Haiti earthquake.1 be able to learn this joint task in one language and transfer knowledge to another (Upadhyay et al., 1 Introduction 2018; Schuster et al., 2019; Xu et al., 2020). This is the premise of our work. A cross-lingual setting is typically described as a Highly effective transformer-based multilingual scenario in which a model trained for a particular models such as mBERT (Devlin et al., 2019) and task in one source language (e.g. English) should be able to generalize well to a different target lan- XLM-R (Conneau et al., 2020a) have found success guage (e.g. Japanese). While semi-supervised so- across several multilingual tasks in recent years. In the zero-shot cross-lingual transfer setting with an lutions (Muis et al., 2018; FitzGerald, 2020, inter alia) assume some target language data or trans- unknown target language, a typical solution is to use pre-trained transformer models and fine-tune to lators are available, a zero-shot solution (Eriguchi the downstream task using the"
2021.mrl-1.18,2020.emnlp-main.358,0,0.0180786,"an be used to align different lan- (2020) present a code-switching based method to guages quite effectively on tasks such as XNLI improve the ability of multilingual language mod218 Figure 6: Impact of code-switching on slot labels. els for factual knowledge retrieval. Contemporary work by Tan and Joty, 2021 makes use of both word and phrase-level code-mixing to switch to a set of languages to perform adversarial training for XNLI. Code-switching and other data augmentation techniques have been applied to the pre-training stage in recent works (Chaudhary et al., 2020; Kale and Siddhant, 2021; Dufter and Schütze, 2020). However, pre-training is outside the scope of this work. In addition to studying cross-lingual slot filling and language families, another key distinction of our method is that we completely ignore the target language during training to represent a fully zero-shot scenario. The main advantage is that with enhanced cross-lingual generalizability, it can be deployed out-of-the-box, as our training is conducted independently of the target language. 7 This work was partially supported by U.S. National Science Foundation grants IIS-1815459, IIS1657379, and 2040926. This work was also supported in"
2021.mrl-1.18,N13-1073,0,0.0201562,"guese (pt), French (fr), Italian (it), Romanian (ro) Chinese (zh-cn), Japanese (ja), Korean (ko) Turkish (tr), Azerbaijani (az), Uyghur (ug), Kazakh (kk) Germanic Indo-Aryan Romance Sino-Tibetan, Koreanic, & Japonic Turkic Table 1: Selected language families to evaluate their impact on a target language. number and order of words in the original sentence. At the chunk-level, we use a direct alignment. The BIO-tagged labels are recreated for the translated phrase based on the word tokens. More complex methods could be applied here to improve the alignment of the slot labels such as fast-align (Dyer et al., 2013) or soft-align (Xu et al., 2020), but we leave this for future work. Code-Switching at the word-level essentially translates every word randomly, while at the sentence-level translates the entire sentence. During the experimental evaluation process, to build a language-neutral model using monolingual source (English) data, all eight target languages are excluded from the code-switching procedure to avoid unfair model comparisons, i.e. removing target languages (lT ) from lset in Algorithm 1. Multilingual masked language models, such as mBERT (Devlin et al., 2019), are trained using large datas"
2021.mrl-1.18,2020.aacl-main.57,0,0.0325845,"et al., 2019; Xu et al., 2020). This is the premise of our work. A cross-lingual setting is typically described as a Highly effective transformer-based multilingual scenario in which a model trained for a particular models such as mBERT (Devlin et al., 2019) and task in one source language (e.g. English) should be able to generalize well to a different target lan- XLM-R (Conneau et al., 2020a) have found success guage (e.g. Japanese). While semi-supervised so- across several multilingual tasks in recent years. In the zero-shot cross-lingual transfer setting with an lutions (Muis et al., 2018; FitzGerald, 2020, inter alia) assume some target language data or trans- unknown target language, a typical solution is to use pre-trained transformer models and fine-tune to lators are available, a zero-shot solution (Eriguchi the downstream task using the monolingual source 1 Implementation and dataset are available at data (Xu et al., 2020). However, Pires et al. (2019) https://github.com/jitinkrishnan/ Multilingual-ZeroShot-SlotFilling. showed that existing transformer-based represen211 Proceedings of the 1st Workshop on Multilingual Representation Learning, pages 211–223 November 11, 2021. ©2021 Associat"
2021.mrl-1.18,C16-1234,0,0.0624821,"Missing"
2021.mrl-1.18,N19-1201,0,0.0542364,"Missing"
2021.mrl-1.18,W18-3206,0,0.0314822,"Missing"
2021.mrl-1.18,C18-1007,0,0.0244286,"ion 2018; Schuster et al., 2019; Xu et al., 2020). This is the premise of our work. A cross-lingual setting is typically described as a Highly effective transformer-based multilingual scenario in which a model trained for a particular models such as mBERT (Devlin et al., 2019) and task in one source language (e.g. English) should be able to generalize well to a different target lan- XLM-R (Conneau et al., 2020a) have found success guage (e.g. Japanese). While semi-supervised so- across several multilingual tasks in recent years. In the zero-shot cross-lingual transfer setting with an lutions (Muis et al., 2018; FitzGerald, 2020, inter alia) assume some target language data or trans- unknown target language, a typical solution is to use pre-trained transformer models and fine-tune to lators are available, a zero-shot solution (Eriguchi the downstream task using the monolingual source 1 Implementation and dataset are available at data (Xu et al., 2020). However, Pires et al. (2019) https://github.com/jitinkrishnan/ Multilingual-ZeroShot-SlotFilling. showed that existing transformer-based represen211 Proceedings of the 1st Workshop on Multilingual Representation Learning, pages 211–223 November 11, 20"
2021.mrl-1.18,P17-1135,0,0.0376966,"Missing"
2021.mrl-1.18,P19-2051,0,0.0599232,"Missing"
2021.mrl-1.18,P19-1493,0,0.0216957,"arget lan- XLM-R (Conneau et al., 2020a) have found success guage (e.g. Japanese). While semi-supervised so- across several multilingual tasks in recent years. In the zero-shot cross-lingual transfer setting with an lutions (Muis et al., 2018; FitzGerald, 2020, inter alia) assume some target language data or trans- unknown target language, a typical solution is to use pre-trained transformer models and fine-tune to lators are available, a zero-shot solution (Eriguchi the downstream task using the monolingual source 1 Implementation and dataset are available at data (Xu et al., 2020). However, Pires et al. (2019) https://github.com/jitinkrishnan/ Multilingual-ZeroShot-SlotFilling. showed that existing transformer-based represen211 Proceedings of the 1st Workshop on Multilingual Representation Learning, pages 211–223 November 11, 2021. ©2021 Association for Computational Linguistics Figure 1: t-SNE plot of embeddings across the 12 multi-head attention layers of multilingual BERT. Parallelly translated sentences of MutiATIS++ dataset are still clustered according to the languages: English (black), Chinese (cyan), French (blue), German (green), and Japanese (red). Figure 2: An original example in English"
2021.mrl-1.18,D18-1061,0,0.0280867,"Missing"
2021.mrl-1.18,H90-1020,0,0.272315,"ean, Japanese respectively). We only group them as an additional interesting data point, not because we ascribe to any theories that link them ∑ typologically. 3 L = − n1 n ˆ] i=1 [y log y our baseline for joint training. Our goal will be to show that code-switching on top of joint training improves the performance. The output of Algorithm 1 will be the input used for joint training on BERT for code-switched experiments. 3 Datasets Benchmark Dataset. We use the latest multilingual benchmark dataset of MultiATIS++ (Xu et al., 2020), which was created by manually translating the original ATIS (Price, 1990) dataset from English (en) to 8 other languages: Spanish (es), Portuguese (pt), German (de), French (fr), Chinese (zh), Japanese (ja), Hindi (hi), and Turkish (tr). The dataset consists of utterances for each language with an ‘intent’ label for ‘flight intent’ and ‘slot’ labels for the word tokens in BIO format. A sample datapoint in English is shown in Figure 2. Table 2 presents the dataset statistics for the benchmark dataset of MultiATIS++ as well as for the newly constructed dataset for disaster NLU. New Dataset for Disaster NLU. We construct a new intent and slot filling dataset of tweets"
2021.mrl-1.18,N19-1380,0,0.184129,"strong different families of languages on downstream correlation between the two tasks has made jointly performance. Furthermore, we present an aptrained models successful (Goo et al., 2018; Haiplication of our method for crisis informatics using a new human-annotated tweet dataset of hong et al., 2019; Hardalov et al., 2020; Chen et al., slot filling in English and Haitian Creole, col2019). In a cross-lingual setting, the model should lected during the Haiti earthquake.1 be able to learn this joint task in one language and transfer knowledge to another (Upadhyay et al., 1 Introduction 2018; Schuster et al., 2019; Xu et al., 2020). This is the premise of our work. A cross-lingual setting is typically described as a Highly effective transformer-based multilingual scenario in which a model trained for a particular models such as mBERT (Devlin et al., 2019) and task in one source language (e.g. English) should be able to generalize well to a different target lan- XLM-R (Conneau et al., 2020a) have found success guage (e.g. Japanese). While semi-supervised so- across several multilingual tasks in recent years. In the zero-shot cross-lingual transfer setting with an lutions (Muis et al., 2018; FitzGerald,"
2021.mrl-1.18,W18-3201,0,0.0270453,"ur setting, we assume that target language is unknown during training, so that our model is generalizable across multiple languages. Code-Switching. Linguistic code-switching is a phenomenon where multilingual speakers alternate et al., 2016; Xie et al., 2018), parts-of-speech tag- between languages. Recently, monolingual models ging (Yarowsky et al., 2001; Täckström et al., 2013; have been adapted to code-switched text in entity Plank and Agi´c, 2018), and natural language under- recognition (Aguilar and Solorio, 2019), part-ofstanding (He et al., 2013; Upadhyay et al., 2018; speech tagging (Soto and Hirschberg, 2018; Ball Xu et al., 2020). The methodology for most of the and Garrette, 2018), sentiment analysis (Joshi et al., current approaches for cross-lingual tasks can be 2016) and language identification (Mave et al., categorizes as: a) multilingual representations from 2018; Yirmibe¸so˘glu and Eryi˘git, 2018; Mager et al., pre-trained or fine-tuned models such as mBERT 2019). Recently, KhudaBukhsh et al., 2020 have (Devlin et al., 2019) or XLM-R (Conneau et al., proposed an approach to sample code-mixed docu2020a), b) machine translation followed by align- ments using minimal supervision. Qin et al.,"
2021.mrl-1.18,P18-1029,0,0.0142416,"(Hardalov et al., 2020; Chen et al., 2019) have improved results. On the other hand, cross-lingual versions of this joint task include a low-supervision based approach for Hindi and Turkish (Upadhyay et al., 2018), new datasets for Spanish and Thai (Schuster et al., 2019), and recently Xu et al. (2020) creating MultiATIS++, a comprehensive dataset in 9 languages. The joint task mentioned above in a pure zero-shot setting is one of the motivations for our work. A Zero-shot is the setting where the model sees a new distribution of examples only during test (prediction) time (Xian et al., 2017; Srivastava et al., 2018; Romera-Paredes and Torr, 2015). Thus, in our setting, we assume that target language is unknown during training, so that our model is generalizable across multiple languages. Code-Switching. Linguistic code-switching is a phenomenon where multilingual speakers alternate et al., 2016; Xie et al., 2018), parts-of-speech tag- between languages. Recently, monolingual models ging (Yarowsky et al., 2001; Täckström et al., 2013; have been adapted to code-switched text in entity Plank and Agi´c, 2018), and natural language under- recognition (Aguilar and Solorio, 2019), part-ofstanding (He et al., 2"
2021.mrl-1.18,Q13-1001,0,0.0365956,"s one of the motivations for our work. A Zero-shot is the setting where the model sees a new distribution of examples only during test (prediction) time (Xian et al., 2017; Srivastava et al., 2018; Romera-Paredes and Torr, 2015). Thus, in our setting, we assume that target language is unknown during training, so that our model is generalizable across multiple languages. Code-Switching. Linguistic code-switching is a phenomenon where multilingual speakers alternate et al., 2016; Xie et al., 2018), parts-of-speech tag- between languages. Recently, monolingual models ging (Yarowsky et al., 2001; Täckström et al., 2013; have been adapted to code-switched text in entity Plank and Agi´c, 2018), and natural language under- recognition (Aguilar and Solorio, 2019), part-ofstanding (He et al., 2013; Upadhyay et al., 2018; speech tagging (Soto and Hirschberg, 2018; Ball Xu et al., 2020). The methodology for most of the and Garrette, 2018), sentiment analysis (Joshi et al., current approaches for cross-lingual tasks can be 2016) and language identification (Mave et al., categorizes as: a) multilingual representations from 2018; Yirmibe¸so˘glu and Eryi˘git, 2018; Mager et al., pre-trained or fine-tuned models such a"
2021.mrl-1.18,D18-1034,0,0.0174648,"ing MultiATIS++, a comprehensive dataset in 9 languages. The joint task mentioned above in a pure zero-shot setting is one of the motivations for our work. A Zero-shot is the setting where the model sees a new distribution of examples only during test (prediction) time (Xian et al., 2017; Srivastava et al., 2018; Romera-Paredes and Torr, 2015). Thus, in our setting, we assume that target language is unknown during training, so that our model is generalizable across multiple languages. Code-Switching. Linguistic code-switching is a phenomenon where multilingual speakers alternate et al., 2016; Xie et al., 2018), parts-of-speech tag- between languages. Recently, monolingual models ging (Yarowsky et al., 2001; Täckström et al., 2013; have been adapted to code-switched text in entity Plank and Agi´c, 2018), and natural language under- recognition (Aguilar and Solorio, 2019), part-ofstanding (He et al., 2013; Upadhyay et al., 2018; speech tagging (Soto and Hirschberg, 2018; Ball Xu et al., 2020). The methodology for most of the and Garrette, 2018), sentiment analysis (Joshi et al., current approaches for cross-lingual tasks can be 2016) and language identification (Mave et al., categorizes as: a) multil"
2021.mrl-1.18,2021.naacl-main.282,0,0.0388493,"xt word vectors (Bojanowski et al., 2017). ing with translate-and-train. Yang et al. (2020) Recently, Conneau et al., 2020b show that having present a non-zero-shot approach that performs shared parameters in the top layers of the multi- code-switching to target languages, and Jiang et al. lingual encoders can be used to align different lan- (2020) present a code-switching based method to guages quite effectively on tasks such as XNLI improve the ability of multilingual language mod218 Figure 6: Impact of code-switching on slot labels. els for factual knowledge retrieval. Contemporary work by Tan and Joty, 2021 makes use of both word and phrase-level code-mixing to switch to a set of languages to perform adversarial training for XNLI. Code-switching and other data augmentation techniques have been applied to the pre-training stage in recent works (Chaudhary et al., 2020; Kale and Siddhant, 2021; Dufter and Schütze, 2020). However, pre-training is outside the scope of this work. In addition to studying cross-lingual slot filling and language families, another key distinction of our method is that we completely ignore the target language during training to represent a fully zero-shot scenario. The mai"
2021.mrl-1.18,2020.emnlp-main.410,0,0.0386727,"Missing"
2021.mrl-1.18,K16-1022,0,0.0456584,"Missing"
2021.mrl-1.18,P09-1027,0,0.024919,"ain experiments in Table 3 use k = 5. Figure 4 shows how varying k affects performance. For this analysis, we consider 4 target languages on which code-switching produced significant results in Table 3 on both Intent Accuracy and Slot F1: Chinese, Japanese, 6 Related Work Hindi, and Turkish. Intuitively, we observe that as k increases, too much code-switching becomes Cross-Lingual Transfer. Researchers have studexpensive in terms of runtime, while performance ied cross-lingual tasks in various settings such improvement slowly plateaus. For Slot F1 perfor- as sentiment/sequence classification (Wan, 2009; mance in all four cases, unlike Intent, we observe Eriguchi et al., 2018; Yu et al., 2018), named enan interesting dip when k = 1, which represents tity recognition (Zirikly and Hagiwara, 2015; Tsai 217 Figure 5: Impact of code-switching on intent classes. (Conneau et al., 2018). Monolingual models for joint slot filling and intent prediction have used attention-based RNN (Liu and Lane, 2016) and attention-based BiLSTM with a slot gate (Goo et al., 2018) on benchmark datasets (Price, 1990; Coucke et al., 2018). These methods have shown that a joint method can enhance both tasks and slot fill"
2021.mrl-1.18,2020.emnlp-demos.6,0,0.0166522,"’, ‘food’, ‘water’, ‘shelter’, and ‘other_aid’. Table 2 provides the dataset statistics. 4 Experimental Setup We use the traditional cross-lingual task setting where each experiment consists of a source language and a target language. A model is trained on the source data (English) and evaluated on the target data (8 other languages). For code-switching experiments, the English dataset is augmented with multilingual code-switching before training. Our implementation is in PyTorch (Paszke et al., 2019) and we use the pre-trained bert-base-multilingualuncased with BertForSequenceClassification (Wolf et al., 2020) model. Maximum epochs is set to 25 with an early stopping patience of 5, batch size of 32, and Adam optimizer (Kingma and Ba, 2014) with a learning rate of 5e−5. We select the best 4 https://appen.com/datasets/ combined-disaster-response-data/ 214 Language English Spanish Portuguese German French Chinese Japanese Hindi Turkish English Haitian Creole Utterances Tokens train dev test train dev MultiATIS++ (Xu et al., 2020) 4488 490 893 50755 5445 4488 490 893 55197 5927 4488 490 893 55052 5909 4488 490 893 51111 5517 4488 490 893 55909 5769 4488 490 893 88194 9652 4488 490 893 133890 14416 1440"
2021.mrl-1.18,H01-1035,0,0.432957,"ure zero-shot setting is one of the motivations for our work. A Zero-shot is the setting where the model sees a new distribution of examples only during test (prediction) time (Xian et al., 2017; Srivastava et al., 2018; Romera-Paredes and Torr, 2015). Thus, in our setting, we assume that target language is unknown during training, so that our model is generalizable across multiple languages. Code-Switching. Linguistic code-switching is a phenomenon where multilingual speakers alternate et al., 2016; Xie et al., 2018), parts-of-speech tag- between languages. Recently, monolingual models ging (Yarowsky et al., 2001; Täckström et al., 2013; have been adapted to code-switched text in entity Plank and Agi´c, 2018), and natural language under- recognition (Aguilar and Solorio, 2019), part-ofstanding (He et al., 2013; Upadhyay et al., 2018; speech tagging (Soto and Hirschberg, 2018; Ball Xu et al., 2020). The methodology for most of the and Garrette, 2018), sentiment analysis (Joshi et al., current approaches for cross-lingual tasks can be 2016) and language identification (Mave et al., categorizes as: a) multilingual representations from 2018; Yirmibe¸so˘glu and Eryi˘git, 2018; Mager et al., pre-trained or"
2021.mrl-1.18,W18-6115,0,0.0442365,"Missing"
2021.mrl-1.18,W18-3023,0,0.02775,"e. For this analysis, we consider 4 target languages on which code-switching produced significant results in Table 3 on both Intent Accuracy and Slot F1: Chinese, Japanese, 6 Related Work Hindi, and Turkish. Intuitively, we observe that as k increases, too much code-switching becomes Cross-Lingual Transfer. Researchers have studexpensive in terms of runtime, while performance ied cross-lingual tasks in various settings such improvement slowly plateaus. For Slot F1 perfor- as sentiment/sequence classification (Wan, 2009; mance in all four cases, unlike Intent, we observe Eriguchi et al., 2018; Yu et al., 2018), named enan interesting dip when k = 1, which represents tity recognition (Zirikly and Hagiwara, 2015; Tsai 217 Figure 5: Impact of code-switching on intent classes. (Conneau et al., 2018). Monolingual models for joint slot filling and intent prediction have used attention-based RNN (Liu and Lane, 2016) and attention-based BiLSTM with a slot gate (Goo et al., 2018) on benchmark datasets (Price, 1990; Coucke et al., 2018). These methods have shown that a joint method can enhance both tasks and slot filling can be conditioned on the learned intent. A related approach iteratively learns the rela"
2021.mrl-1.18,P15-2064,0,0.0256925,"icant results in Table 3 on both Intent Accuracy and Slot F1: Chinese, Japanese, 6 Related Work Hindi, and Turkish. Intuitively, we observe that as k increases, too much code-switching becomes Cross-Lingual Transfer. Researchers have studexpensive in terms of runtime, while performance ied cross-lingual tasks in various settings such improvement slowly plateaus. For Slot F1 perfor- as sentiment/sequence classification (Wan, 2009; mance in all four cases, unlike Intent, we observe Eriguchi et al., 2018; Yu et al., 2018), named enan interesting dip when k = 1, which represents tity recognition (Zirikly and Hagiwara, 2015; Tsai 217 Figure 5: Impact of code-switching on intent classes. (Conneau et al., 2018). Monolingual models for joint slot filling and intent prediction have used attention-based RNN (Liu and Lane, 2016) and attention-based BiLSTM with a slot gate (Goo et al., 2018) on benchmark datasets (Price, 1990; Coucke et al., 2018). These methods have shown that a joint method can enhance both tasks and slot filling can be conditioned on the learned intent. A related approach iteratively learns the relationship between the two tasks (Haihong et al., 2019) . Recently, BERT-based approaches (Hardalov et a"
2021.mrl-1.18,2020.acl-main.764,1,0.827084,"ta), as compared to k = 0. Adding the original data to one round of code-switched data (k = 2) leads to big improvements. Overall, we see improvement for both tasks, with Slot F1 plateauing earlier. Table 5 and Figure 10 in Appendix B show the impact of code-switching on training runtime, which increases as k increases. Thus, finding an optimal value of k and specific language groups are essential for downstream applications. Figure 3: Impact of different language groups on the target languages. Figure 4: Performance as k (augmentation rounds) increases (on mBERT). for methods like the one of Xia et al. (2020) that can a priori identify the best helper language or group of languages that can benefit downstream tasks for low resource languages. mBERT versus XLM-R. Additional performance evaluations and benefits of code-switching on XLM-R (Conneau et al., 2020a), a stronger multilingual language model, are provided in Appendix A. Note that XLM-R is trained using Common-Crawl and is likely to be exposed to some code-switched data. Thus, we focus primarily on mBERT which largely remains monolingual at the sentence-level to identify the unbiased impact of code-switching during fine-tuning. Furthermore,"
2021.mrqa-1.14,2021.naacl-main.46,0,0.140682,"available here: https:// github.com/ffaisal93/aligned_qa 2 See (Rogers et al., 2021) for a thorough survey of the field. 3 Joan Clarke was the only female computer scientist at Bletchley Park during the Second World War. Details: https://en.wikipedia.org/wiki/Joan_ Clarke 2021, there is no Wikipedia article for Joane Clarke in Greek or in Bengali. 4 For the QA system to adequately serve these students, it will need to function in a cross-lingual setting, retrieving the English (or any of the other available languages) article and producing an answer given the question in a different language (Asai et al., 2021). Throughout this paper we will refer to a setting where the question and the context are in different languages as “cross-lingual” QA. Multilingually-pretrained language models such as mBERT (Devlin et al., 2019) or XLM-R (Conneau et al., 2020) are widely used as the base of modern QA systems and have shown promise for performing the task in zero-shot (Artetxe et al., 2020) or few-shot manner (Debnath et al., 2021) as well as in cross-lingual settings (Asai et al., 2021). At the same time, these models are not without drawbacks; their pre-training objectives did not explicitly require that th"
2021.mrqa-1.14,D18-1269,0,0.0251871,"ts obtained with fast-align (Dyer et al., 2013). This model aims to decrease the representation distance between words with similar meanings across languages. Zhao et al. (2021) used the fine-tuning process defined by Cao et al. (2020) and further tuned it for low resource languages. After the alignment fine-tuning stage, the authors also perform lastlayer embedding normalization and language specific word-word coordination to further improve on downstream tasks. These contextual representation alignment works are evaluated on tasks designed for cross-lingual and zero-shot transfer like XNLI (Conneau et al., 2018) or RFEVAL (Zhao et al., 2020). Our work is the first to evaluate these methods on the QA task, but also the first to expand the alignment-based fine-tuning to include almost all languages used in the pre-trained models, as opposed to only using the languages on which evaluation is performed. 7 Conclusion and Future Work In this work, we have studied the cross-lingual extractive QA setting where the question and contextto-search are in different languages. Through experiments on synthetic and newly collected data in 4 languages, we have shown that data augmentation along with alignment-based f"
2021.mrqa-1.14,N13-1073,0,0.102299,"Missing"
2021.mrqa-1.14,2020.emnlp-main.480,0,0.0190221,"with both mBERT and XLM-R. Our baselines use the pre-trained models, fine-tuned as above on the QA task (without any intermediate alignment-based fine-tuning). Our models, in turn, first perform alignment-based finetuning on the pre-trained models, and then train them on the QA task. Alignment-Based fine-tuning Due to the lack of n-way parallel corpora, all parallel corpora we use have English as one of the two languages. We obtain word-level alignments using AWESOME-ALIGN (Dou and Neubig, 2021). We use data from WikiMatrix (Schwenk et al., 2021) , Wikimedia (Tiedemann, 2012) and CC-aligned (El-Kishky et al., 2020) to prepare various versions of our parallel corpus. The size of these data varies from 260k to less than 1k for each language based on experiment type. To study the effect of using different language samples and data sizes, we experiment with different models (summarized in Table 1): i=1 The final objective is simply the sum of the two components, similar to the approaches for retrofitting static embeddings (Faruqui et al., 2015): minϕ k ∑ L(cv , fϕ ) + R(cv , fϕ ) v=1 4 Experimental Setup • CAO-HIGH: this is the mBERT model provided by Cao et al. (2020), originally fine-tuned on English-X pa"
2021.mrqa-1.14,2020.tacl-1.30,0,0.0472268,"ngually. In this work we investigate the capabilities of multilingually pretrained language models on cross-lingual QA. We find that explicitly aligning the representations across languages with a post-hoc finetuning step generally leads to improved performance. We additionally investigate the effect of data size as well as the language choice in this fine-tuning step, also releasing a dataset for evaluating cross-lingual QA systems. 1 1 Introduction Information seeking question answering, where a user asks a question to get a related passage or short text as answer, is a widely studied area (Clark et al., 2020; Kwiatkowski et al., 2019; Yang et al., 2015, inter alia) 2 that has been successfully deployed in user-facing applications such as conversational assistants (Gao et al., 2018). For example, an English computer science student that asks Apple Siri, Amazon Alexa, or Google Assistant the question “Where did Joan Clarke work?” 3 will receive the answer “Bletchley Park”, an answer based on the English Wikipedia entry for Joan Clarke automatically retrieved by the system. Now, consider another student, this time based in Greece or Bangladesh, asking effectively the same question, “Πού δούλευε η Jo"
2021.mrqa-1.14,2020.acl-main.747,0,0.0384964,"ikipedia.org/wiki/Joan_ Clarke 2021, there is no Wikipedia article for Joane Clarke in Greek or in Bengali. 4 For the QA system to adequately serve these students, it will need to function in a cross-lingual setting, retrieving the English (or any of the other available languages) article and producing an answer given the question in a different language (Asai et al., 2021). Throughout this paper we will refer to a setting where the question and the context are in different languages as “cross-lingual” QA. Multilingually-pretrained language models such as mBERT (Devlin et al., 2019) or XLM-R (Conneau et al., 2020) are widely used as the base of modern QA systems and have shown promise for performing the task in zero-shot (Artetxe et al., 2020) or few-shot manner (Debnath et al., 2021) as well as in cross-lingual settings (Asai et al., 2021). At the same time, these models are not without drawbacks; their pre-training objectives did not explicitly require that their representations align across languages for semantically similar words/phrases/sentences, with adverse effects especially for languages written in scripts other than the Latin one (Muller et al., 2020). A recent line of work (Cao et al., 2020"
2021.mrqa-1.14,N15-1184,0,0.028755,"Missing"
2021.mrqa-1.14,D16-1264,0,0.0443435,"rks, including MLQA (Lewis et al., 2020), We further analyze the cross-lingual results in Table 6, where we compare the correct-incorrect fre- MKQA (Longpre et al., 2020) and XQuAD (Artetxe et al., 2020). MLQA translates original Enquency of minimal answers for Bengali and Swahili glish questions to 7 other languages to train a multiin two settings: asking questions in the context language or in the English translations (human- lingual QA model. MKQA comprises of questions from 26 diverse languages. XQuAD uses translated recorded, asr transcriptions). Here we only report questions from SQuAD (Rajpurkar et al., 2016) comparison on the answers which are either fully (originally in English) to prepare a widely used and correct or incorrect leaving the partially correct ones. Overall, our models get less instances com- easily adaptable multilingual benchmark of SQuAD baseline. In SD-QA (Faisal et al., 2021) from which pletely wrong. Interestingly, we observe that in a our work is inspired, the authors prepare a naturally number of cases, alignment based fine-tuning helps spoken version of TyDi-QA over 5 languages. In in predicting correct answers in cross lingual setting this work, we further expand the scop"
2021.mrqa-1.14,2021.eacl-main.259,0,0.0481738,"Missing"
2021.mrqa-1.14,2021.eacl-main.115,0,0.0313395,"of a short text span containing the answer). We preform experiments with both mBERT and XLM-R. Our baselines use the pre-trained models, fine-tuned as above on the QA task (without any intermediate alignment-based fine-tuning). Our models, in turn, first perform alignment-based finetuning on the pre-trained models, and then train them on the QA task. Alignment-Based fine-tuning Due to the lack of n-way parallel corpora, all parallel corpora we use have English as one of the two languages. We obtain word-level alignments using AWESOME-ALIGN (Dou and Neubig, 2021). We use data from WikiMatrix (Schwenk et al., 2021) , Wikimedia (Tiedemann, 2012) and CC-aligned (El-Kishky et al., 2020) to prepare various versions of our parallel corpus. The size of these data varies from 260k to less than 1k for each language based on experiment type. To study the effect of using different language samples and data sizes, we experiment with different models (summarized in Table 1): i=1 The final objective is simply the sum of the two components, similar to the approaches for retrofitting static embeddings (Faruqui et al., 2015): minϕ k ∑ L(cv , fϕ ) + R(cv , fϕ ) v=1 4 Experimental Setup • CAO-HIGH: this is the mBERT mode"
2021.mrqa-1.14,tiedemann-2012-parallel,0,0.0120397,"e answer). We preform experiments with both mBERT and XLM-R. Our baselines use the pre-trained models, fine-tuned as above on the QA task (without any intermediate alignment-based fine-tuning). Our models, in turn, first perform alignment-based finetuning on the pre-trained models, and then train them on the QA task. Alignment-Based fine-tuning Due to the lack of n-way parallel corpora, all parallel corpora we use have English as one of the two languages. We obtain word-level alignments using AWESOME-ALIGN (Dou and Neubig, 2021). We use data from WikiMatrix (Schwenk et al., 2021) , Wikimedia (Tiedemann, 2012) and CC-aligned (El-Kishky et al., 2020) to prepare various versions of our parallel corpus. The size of these data varies from 260k to less than 1k for each language based on experiment type. To study the effect of using different language samples and data sizes, we experiment with different models (summarized in Table 1): i=1 The final objective is simply the sum of the two components, similar to the approaches for retrofitting static embeddings (Faruqui et al., 2015): minϕ k ∑ L(cv , fϕ ) + R(cv , fϕ ) v=1 4 Experimental Setup • CAO-HIGH: this is the mBERT model provided by Cao et al. (2020"
2021.mrqa-1.14,2020.emnlp-main.359,0,0.0135082,"R generally yields better performance in terms of F-score. The only exceptions are in the case of Swahili context (Qen , Csw ) for both tasks, where mBERT leads to higher F-score, with XLM-R lagging behind by a couple of percentage points. sults Tables). Aligning the representations of languages other than the ones we evaluate on does not seem to lead to improvements. The CAO-HIGH, ZHAO-LOW, and ZHAO(LOW+HIGH) models generally perform 1-2 percentage points worse than the comparable mBERT or XLM-R baselines using the same task-tuning data (+aug). This is an indication of negative interference (Wang et al., 2020), which we suspect is due to the models using a large amount of data in a limited set of languages that overfits the representations to these languages. Effect of language choices in multilingual alignment The comparison of different multilinguallyaligned checkpoints leads to two main findings. First, that including the evaluation language in the alignment fine-tuning state is important in downstream performance. Across both tasks and all language settings, the highest performing setting is one In contrast, our 111-S and 111-L models, despite where the language pair was included in the fine- i"
2021.naacl-main.38,2020.findings-emnlp.223,0,0.0412259,"ni, Uyghur and Meadow Mari. For instance, transliterating Arabic to the Latin script leads to a drop in performance of 1.5, 4.1 and 6.9 points for POS tagging, parsing and NER respectively.6 Our findings are generally in line with previous work. Transliteration to English specifically (Lin et al., 2016; Durrani et al., 2014) and named entity transliteration (Kundu et al., 2018; Grundkiewicz and Heafield, 2018) has been proven useful for cross-lingual transfer in tasks like NER, entity linking (Rijhwani et al., 2019), morphological inflection (Murikinati et al., 2020), and Machine Translation (Amrhein and Sennrich, 2020). The transliteration approach provides a viable path for rendering large pretrained models like mBERT useful for all languages of the world. Indeed, as reported in Table 4, transliterating both Uyghur and Sorani leads to matching or outper6 Details and complete results on these controlled experiments can be found in Appendix E. Model Arabic Russian Japanese Original Script → Latin Script POS LAS NER 96.4 → 94.9 98.1 → 96.0 97.4 → 95.7 82.9 → 78.8 88.4 → 84.5 88.5 → 86.9 87.8 → 80.9 88.1 → 86.0 61.5 → 55.6 Table 5: mBERT TASK -T UNED on high resource languages for POS tagging, parsing and NER."
2021.naacl-main.38,2020.osact-1.2,0,0.0231597,"cting the abilities of pretrained multilingual language models 1 Introduction to be used for low-resource languages. We dub those categories Easy, Intermediate and Hard. Language models are now a new standard to build state-of-the-art Natural Language ProcessHard languages include both stable and endaning (NLP) systems. In the past year, monolingual gered languages, but they predominantly are lanlanguage models have been released for more than guages of communities that are majorly under20 languages including Arabic, French, German, served by modern NLP. Hence, we direct our attenand Italian (Antoun et al., 2020; Martin et al., 2020; tion to these Hard languages. For those languages, de Vries et al., 2019; Cañete et al., 2020; Kuratov we show that the script they are written in can be and Arkhipov, 2019; Schweter, 2020, inter alia). a critical element in the transfer abilities of preAdditionally, large-scale multilingual models cov- trained multilingual language models. Translitering more than 100 languages are now available erating them leads to large gains in performance (XLM-R by Conneau et al. (2020) and mBERT outperforming non-contextual strong baselines. To by Devlin et al. (2019)). Still, most"
2021.naacl-main.38,E14-4029,0,0.0224963,"trained languages such as Arabic, Russian or Japanese, mBERT is not able to compete with the performance reached when using the script seen during pretraining. Transliterating the Arabic script and the Cyrillic script to Latin does not automatically improve mBERT performance as it does for Sorani, Uyghur and Meadow Mari. For instance, transliterating Arabic to the Latin script leads to a drop in performance of 1.5, 4.1 and 6.9 points for POS tagging, parsing and NER respectively.6 Our findings are generally in line with previous work. Transliteration to English specifically (Lin et al., 2016; Durrani et al., 2014) and named entity transliteration (Kundu et al., 2018; Grundkiewicz and Heafield, 2018) has been proven useful for cross-lingual transfer in tasks like NER, entity linking (Rijhwani et al., 2019), morphological inflection (Murikinati et al., 2020), and Machine Translation (Amrhein and Sennrich, 2020). The transliteration approach provides a viable path for rendering large pretrained models like mBERT useful for all languages of the world. Indeed, as reported in Table 4, transliterating both Uyghur and Sorani leads to matching or outper6 Details and complete results on these controlled experime"
2021.naacl-main.38,W18-2413,0,0.0172977,"compete with the performance reached when using the script seen during pretraining. Transliterating the Arabic script and the Cyrillic script to Latin does not automatically improve mBERT performance as it does for Sorani, Uyghur and Meadow Mari. For instance, transliterating Arabic to the Latin script leads to a drop in performance of 1.5, 4.1 and 6.9 points for POS tagging, parsing and NER respectively.6 Our findings are generally in line with previous work. Transliteration to English specifically (Lin et al., 2016; Durrani et al., 2014) and named entity transliteration (Kundu et al., 2018; Grundkiewicz and Heafield, 2018) has been proven useful for cross-lingual transfer in tasks like NER, entity linking (Rijhwani et al., 2019), morphological inflection (Murikinati et al., 2020), and Machine Translation (Amrhein and Sennrich, 2020). The transliteration approach provides a viable path for rendering large pretrained models like mBERT useful for all languages of the world. Indeed, as reported in Table 4, transliterating both Uyghur and Sorani leads to matching or outper6 Details and complete results on these controlled experiments can be found in Appendix E. Model Arabic Russian Japanese Original Script → Latin S"
2021.naacl-main.38,W19-7803,0,0.0131002,"tion about their scripts, language families, and amount of available raw data can be found in the Appendix in Table 12. 3.1 Raw Data To perform pretraining and fine-tuning on monolingual data, we use the deduplicated datasets from the OSCAR project (Ortiz Suárez et al., 2019). OSCAR is a corpus extracted from a Common Crawl Web snapshot.2 It provides a significant amount of data for all the unseen languages we work with, except for Buryat, Meadow Mari, Erzya and Livvi for which we use Wikipedia dumps and for Narabizi, Naija and Faroese, for which we use data collected by Seddah et al. (2020), Caron et al. (2019) and Biemann et al. (2007) respectively. 3.2 Non-contextual Baselines For parsing and POS tagging, we use the UDPipe future system (Straka, 2018) as our baseline. This model is a LSTM-based (Hochreiter and Schmidhuber, 1997) recurrent architecture trained with pretrained static word embedding (Mikolov et al., 2013) (hence our non-contextual characterization) along with character-level embeddings. This system was ranked in the very first positions for parsing and tagging in the CoNLL shared task 2018 (Zeman and Hajiˇc, 2018). For NER we use the LSTM-CRF model with character and word level embed"
2021.naacl-main.38,D19-1433,0,0.0229135,"z Suárez et al. (2020) showed that pretraining ELMo models (Peters et al., 2018) on less than 1GB of text leads to state-of-the-art performance while Martin et al. (2020) showed that pretraining a BERT model on as few as 4GB of diverse enough data results in state-of-the-art performance. Micheli et al. (2020) further demonstrated that decent performance was achievable with only 100MB of raw text data. Adapting large-scale models for low-resource languages Multilingual language models can be used directly on unseen languages, or they can also be adapted using unsupervised methods. For example, Han and Eisenstein (2019) successfully used unsupervised model adaptation of the English BERT model to Early Modern English for sequence labeling. Instead of fine-tuning the whole model, Pfeiffer et al. (2020) recently showed that adapter layers (Houlsby et al., 2019) can be injected into multilingual language models to provide parameter efficient task and language transfer. Still, as of today, the availability of monolingual or multilingual language models is limited to approximately 120 languages, leaving many languages without access to valuable NLP technology, although some are spoken by millions of people, includ"
2021.naacl-main.38,2020.findings-emnlp.118,0,0.325304,"Missing"
2021.naacl-main.38,2020.acl-main.747,0,0.526302,"Bambara (spoken guage models on a large amount of raw data by around 5 million people in Mali and neighborhas become a new norm to reach state-of-theing countries) are not covered by any available art performance in NLP. Still, it remains unclear language models at the time of writing. how this approach should be applied for unseen languages that are not covered by any available Even if training multilingual models that cover large-scale multilingual language model and more languages and language varieties is tempting, for which only a small amount of raw data is the curse of multilinguality (Conneau et al., 2020) generally available. In this work, by comparmakes it an impractical solution, as it would require ing multilingual and monolingual models, we to train ever larger models. Furthermore, as shown show that such models behave in multiple ways by Wu and Dredze (2020), large-scale multilingual on unseen languages. Some languages greatly language models are sub-optimal for languages that benefit from transfer learning and behave simiare under-sampled during pretraining. larly to closely related high resource languages whereas others apparently do not. Focusing In this paper, we analyze task and lang"
2021.naacl-main.38,2020.acl-main.560,0,0.0441456,"lable at https://github.com/benjami n-mlr/mbert-unseen-languages.git diate and the Easy languages. 448 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 448–462 June 6–11, 2021. ©2021 Association for Computational Linguistics • We show that Hard languages can be better addressed by transliterating them into a betterhandled script (typically Latin), providing a promising direction towards making multilingual language models useful for a new set of unseen languages. 2 Background and Motivation As Joshi et al. (2020) vividly illustrate, there is a large divergence in the coverage of languages by NLP technologies. The majority of the 6500+ of the world’s languages are not studied by the NLP community, since most have few or no annotated datasets, making systems’ development challenging. The development of such models is a matter of high importance for the inclusion of communities, the preservation of endangered languages and more generally to support the rise of tailored NLP ecosystems for such languages (Schmidt and Wiegand, 2017; Stecklow, 2018; Seddah et al., 2020). In that regard, the advent of the Uni"
2021.naacl-main.38,P17-1178,0,0.309625,"ges by NLP technologies. The majority of the 6500+ of the world’s languages are not studied by the NLP community, since most have few or no annotated datasets, making systems’ development challenging. The development of such models is a matter of high importance for the inclusion of communities, the preservation of endangered languages and more generally to support the rise of tailored NLP ecosystems for such languages (Schmidt and Wiegand, 2017; Stecklow, 2018; Seddah et al., 2020). In that regard, the advent of the Universal Dependencies project (Nivre et al., 2016) and the WikiAnn dataset (Pan et al., 2017) have greatly increased the number of covered languages by providing annotated datasets for more than 90 languages for dependency parsing and 282 languages for NER. Regarding modeling approaches, the emergence of multilingual representation models, first with static word embeddings (Ammar et al., 2016) and then with language model-based contextual representations (Devlin et al., 2019; Conneau et al., 2020) enabled transfer from high to low-resource languages, leading to significant improvements in downstream task performance (Rahimi et al., 2019; Kondratyuk and Straka, 2019). Furthermore, in t"
2021.naacl-main.38,N18-1202,0,0.0649653,"it comes to low-resource languages, one direction is to simply train contextualized embedding models on whatever data is available. Another option is to adapt/fine-tune a multilingual pretrained model to the language of interest. We briefly discuss these two options. of pretraining data seems to correlate with downstream task performance (e.g. compare BERT and RoBERTa (Liu et al., 2020)), several attempts have shown that training a model from scratch can be efficient even if the amount of data in that language is limited. Indeed, Ortiz Suárez et al. (2020) showed that pretraining ELMo models (Peters et al., 2018) on less than 1GB of text leads to state-of-the-art performance while Martin et al. (2020) showed that pretraining a BERT model on as few as 4GB of diverse enough data results in state-of-the-art performance. Micheli et al. (2020) further demonstrated that decent performance was achievable with only 100MB of raw text data. Adapting large-scale models for low-resource languages Multilingual language models can be used directly on unseen languages, or they can also be adapted using unsupervised methods. For example, Han and Eisenstein (2019) successfully used unsupervised model adaptation of the"
2021.naacl-main.38,2020.emnlp-main.617,0,0.107921,"Missing"
2021.nlp4prog-1.1,W07-0734,0,0.747682,"nomy of error modes across the various models. We also offer a discussion about differences in errors made across models and suggestions for model improvements. • We offer resources on GitHub2 and Zenodo3 for replicating our experiments, including code and trained models, in addition to all of the data and examples used in our qualitative analysis of model errors. In most works on automated code summarization, the performance of the generated natural language descriptions is evaluated using referencebased metrics adapted from machine translation, e.g., BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007), or text summarization, e.g., ROUGE (Lin, 2004). As such, most researchers make conclusions based on the results obtained using these metrics. However, the code summarization task is a difficult one – due in large part to the sizeable semantic gap between the modalities of source code and natural language. As such, while these metrics provide a general illustration of model efficacy, it can be difficult to determine the specific shortcomings of neural code summarization techniques without a more extensive qualitative investigation into their errors. 2 Background: Deep Learning for Code Summar"
2021.nlp4prog-1.1,N19-1394,0,0.152475,"dalities of source code and natural language. As such, while these metrics provide a general illustration of model efficacy, it can be difficult to determine the specific shortcomings of neural code summarization techniques without a more extensive qualitative investigation into their errors. 2 Background: Deep Learning for Code Summarization This section outlines necessary background regarding our chosen evaluation dataset as well as the three neural code summarization models upon which we focus our empirical investigation. 2.1 Dataset: Funcom In this study we make use of the Funcom dataset (LeClair and McMillan, 2019).4 We selected this dataset primarily for three reasons: (i) this dataset was specifically curated for the task of code summarization, excluding methods more than 100 words and comments with >13 and <3 words or which were auto-generated, (ii) it is currently one of the largest datasets specifically tailored for code summarization, containing over 2.1M Java methods with paired JavaDoc comments, (iii) it targets Java, one of the most popular programming languages.5 In order to make for a feasible training procedure for our various model configurations, and to keep the dataset size in line with p"
2021.nlp4prog-1.1,W04-1013,0,0.412357,"discussion about differences in errors made across models and suggestions for model improvements. • We offer resources on GitHub2 and Zenodo3 for replicating our experiments, including code and trained models, in addition to all of the data and examples used in our qualitative analysis of model errors. In most works on automated code summarization, the performance of the generated natural language descriptions is evaluated using referencebased metrics adapted from machine translation, e.g., BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007), or text summarization, e.g., ROUGE (Lin, 2004). As such, most researchers make conclusions based on the results obtained using these metrics. However, the code summarization task is a difficult one – due in large part to the sizeable semantic gap between the modalities of source code and natural language. As such, while these metrics provide a general illustration of model efficacy, it can be difficult to determine the specific shortcomings of neural code summarization techniques without a more extensive qualitative investigation into their errors. 2 Background: Deep Learning for Code Summarization This section outlines necessary backgrou"
2021.nlp4prog-1.1,C04-1072,0,0.346414,"ue pre-processing constraints, described in detail in Appendix B. Few past studies have examined the failure modes of neural code summarization models as we outline in §6. Therefore, to further explore this topic, in this paper we perform both a qualitative and quantitative empirical comparison of three neural code summarization models. Our quantitative evaluation offers a comparison of three recently proposed models (CodeBERT (Feng et al., 2020), NeuralCodeSum (Ahmad et al., 2020), and code2seq (Alon et al., 2019)) on the Funcom dataset (LeClair and McMillan, 2019) using the smoothed BLEU-4 (Lin and Och, 2004), METEOR (Lavie and Agarwal, 2007), and ROUGEL (Lin, 2004) metrics whereas our qualitative evaluation consists of a rigorous manual categorization of model errors (compared to ground truth captions) based on a procedure adapted from the practice of open coding (Miles et al., 2013). In summary, this paper makes the following contributions: • We offer a quantitative comparative analysis of the CodeBERT, NeuralCodeSum, and code2seq models applied to the task of Java method summarization in the Funcom dataset. The results of this analysis illustrate that the CodeBERT model performs best to a stati"
2021.nlp4prog-1.1,P02-1040,0,0.11028,"ee studied models and derive a taxonomy of error modes across the various models. We also offer a discussion about differences in errors made across models and suggestions for model improvements. • We offer resources on GitHub2 and Zenodo3 for replicating our experiments, including code and trained models, in addition to all of the data and examples used in our qualitative analysis of model errors. In most works on automated code summarization, the performance of the generated natural language descriptions is evaluated using referencebased metrics adapted from machine translation, e.g., BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007), or text summarization, e.g., ROUGE (Lin, 2004). As such, most researchers make conclusions based on the results obtained using these metrics. However, the code summarization task is a difficult one – due in large part to the sizeable semantic gap between the modalities of source code and natural language. As such, while these metrics provide a general illustration of model efficacy, it can be difficult to determine the specific shortcomings of neural code summarization techniques without a more extensive qualitative investigation into their errors. 2 Back"
2021.nlp4prog-1.1,P17-1099,0,0.0233427,"in terms of predicting natural language summaries from Java methods? RQ2 : What types of errors do our studied models make when compared to ground truth captions? NeuralCodeSum The second technique we study is NeuralCodeSum (Ahmad et al., 2020). Here, the authors explored a transformer-based approach to perform the task of code summarization, using a self-attention mechanism to capture the long-term dependencies that are common in source code. In order to enable the model to both copy from already seen source code and to generate new words from its vocabulary, they employed a copy mechanism (See et al., 2017). One important distinction of source code that this model takes into account is that the absolute token position does not necessarily assist in the process of learning effective source code representations (i.e., int a=b+c and int a=c+b; both convey the same meaning). To mitigate this problem, they used the relative positioning of tokens to encode pairwise token relations. Additionally, the authors of this model also explored the integration of an abstract syntax tree (AST)-based source code representation. HowRQ3 : What differences (if any) are there between the errors made by different mode"
2021.nlp4prog-1.1,2021.ccl-1.108,0,0.0429693,"Missing"
2021.nlp4prog-1.1,D16-1096,0,0.0436575,"Missing"
2021.tacl-1.1,C18-1214,1,0.92547,"results both in higher accuracy as well as lower calibration error (SCE). This effect of CVT is much more pronounced in the second experiment, which presents a low-resource scenario and is common in an AL framework. 6 Human Annotation Experiment In this section, we apply our proposed approach on Griko, an endangered language spoken by around 20,000 people in southern Italy, in the Grec`ıa Salentina area southeast of Lecce. The only available online Griko corpus, referred to as UoI (Lekakou et al., 2013),8 consists of 330 utterances by nine native speakers having POS annotations. Additionally, Anastasopoulos et al. (2018) collected, processed, and released 114 stories, of which only the first 10 stories were annotated by experts and have gold-standard annotations. We conduct human annotation experiments on the remaining unannotated stories in order to compare the different active learning methods. 5.4 Effect of Cross-View Training As mentioned in Section § 4.2, we use CVT to not only improve our model overall but also to have a well-calibrated model that can be important for active learning. A model is well-calibrated when a model’s predicted probabilities over the outcomes reflects the true probabilities over"
2021.tacl-1.1,P18-1246,0,0.0562382,"Missing"
2021.tacl-1.1,D19-1520,1,0.892816,"Missing"
2021.tacl-1.1,D18-1217,0,0.0535786,"Missing"
2021.tacl-1.1,D17-1078,0,0.0488873,"Missing"
2021.tacl-1.1,P11-1061,0,0.0734369,"Missing"
2021.tacl-1.1,L18-1293,0,0.0415965,"Missing"
2021.tacl-1.1,P17-2093,0,0.192463,"world’s 7000 languages (Hammarstr¨om et al., 2018). Furthermore, manually annotating large amounts of text with trained experts is an expensive and time-consuming task, even more so when linguists/annotators might not be native speakers of the language. Active Learning (Lewis, 1995; Settles, 2009, AL) is a family of methods that aim to train effective models with less human effort and cost by selecting such a subset of data that maximizes the end model performance. Although many methods have been proposed for AL in sequence labeling (Settles and Craven, 2008; Marcheggiani and Arti`eres, 2014; Fang and Cohn, 2017), through an empirical study across six typologically diverse languages we show that within the same task setup these methods perform inconsistently. Furthermore, even in an oracle scenario, where we have access to the true labels during data selection, existing methods are far from optimal. We posit that the primary reason for this inconsistent performance is that while existing methods consider uncertainty in predictions, they do not consider the direction of the uncertainty with respect to the output labels. For instance, in Figure 1 we consider the German token ‘‘die,’’ which may be either"
2021.tacl-1.1,N13-1014,0,0.0317149,"data which however, is from a slightly different domain, which affects the results somewhat. We observe that QBC has lower WD scores for Linguist-1 and Linguist-2 and UNS for Linguist-3. On further analysis, we find that even though QBC has lower WD, it also has the least coverage of the test data—that is, it has the fewest number of annotated tokens which are present in the test data, as shown in Table 7. We would like to note that a lower WD score does not necessarily translate to 7 Related Work Active Learning for POS Tagging: AL has been widely used for POS tagging. Garrette and Baldridge (2013) use a graph-based label propagation to generalize initial POS annotations to the unlabeled corpus. Further, they find that under a constrained time setting, typelevel annotations prove to be more useful than token-level annotations. In line with this, Fang and Cohn (2017) also select informative word types based on uncertainty sampling for lowresource POS tagging. They also construct a tag dictionary from these type-level annotations and then propagate the labels across the entire unlabeled corpus. However, in our initial analysis on uncertainty sampling, we found that adding label-propagatio"
2021.tacl-1.1,P19-1172,0,0.0440182,"Missing"
2021.tacl-1.1,L16-1262,0,0.058029,"Missing"
2021.tacl-1.1,P16-1101,0,0.0150373,"SCRAL (z )+(1 − pi,t,yˆi, t ) ˆj ← arg maxj ∈ J  {yˆ } pi, t, j 11: i, t 12: OCRAL (z, ˆj ) ← OCRAL (z, ˆj )+ 1 13: XINIT ← b- arg maxz ∈Z SCRAL (z ) 14: for zk ∈ XINIT do 15: jk ← arg maxj ∈ J OCRAL (zk , j ) 16: for xi, t ∈ D s.t. xi, t = zk do 17: cxi, t ← enc(xi, t ) 18: Wxi, t = pi, t, jk ∗ cxi, t 19: 4 Model and Training Regimen Now that we have a method to select data for annotation, we present our POS tagger in Section §4.1, followed by the training algorithm in Section §4.2. 4.1 Model Architecture Our POS tagging model is a hierarchical neural conditional random field (CRF) tagger (Ma and Hovy, 2016; Lample et al., 2016; Yang et al., 2017). Each token (x, t) from the input sequence x is first passed through a character-level Bi-LSTM, followed by a self-attention layer (Vaswani et al., 2017), followed by another Bi-LSTM to capture information about subword structure of the words. Finally, these character-level representations are fed into a token-level Bi-LSTM in order to − → ← − create contextual representations ct = ht : ht , ← − − → where ht and ht are the representations from the forward and backward LSTMs, and ‘‘:’’ denotes the concatenation operation. The encoded representations are"
2021.tacl-1.1,P18-1247,1,0.905155,"Missing"
2021.tacl-1.1,D08-1112,0,0.675088,"resources are not readily available for the majority of the world’s 7000 languages (Hammarstr¨om et al., 2018). Furthermore, manually annotating large amounts of text with trained experts is an expensive and time-consuming task, even more so when linguists/annotators might not be native speakers of the language. Active Learning (Lewis, 1995; Settles, 2009, AL) is a family of methods that aim to train effective models with less human effort and cost by selecting such a subset of data that maximizes the end model performance. Although many methods have been proposed for AL in sequence labeling (Settles and Craven, 2008; Marcheggiani and Arti`eres, 2014; Fang and Cohn, 2017), through an empirical study across six typologically diverse languages we show that within the same task setup these methods perform inconsistently. Furthermore, even in an oracle scenario, where we have access to the true labels during data selection, existing methods are far from optimal. We posit that the primary reason for this inconsistent performance is that while existing methods consider uncertainty in predictions, they do not consider the direction of the uncertainty with respect to the output labels. For instance, in Figure 1 w"
2021.tacl-1.1,H01-1035,0,0.191632,"Missing"
2021.tacl-1.1,D08-1063,0,0.0194508,"θpst that does not look at either the left context or the current token. The token representations ct for each module can be seen as follows: Using the architecture described above, for any given target language we first train a POS model on a group of related high-resource languages. We then fine-tune this pre-trained model on the newly acquired annotations on the target language, as obtained from an AL method. The objective of cross-lingual transfer learning is to warm-start the POS model on the target language. Several methods have been proposed in the past including annotation projection (Zitouni and Florian, 2008), and model transfer using pre-trained models such as m-BERT (Devlin et al., 2019). In this work our primary focus is on designing an active learning method, so we simply pre-train a POS model on a group of related high-resource languages (Cotterell and Heigold, 2017), which is a computationally cheap solution, a crucial requirement for running multiple AL iterations. Furthermore, recent work (Siddhant et al., 2020) has shown the advantage of pre-training using a selected set of related languages over a model pre-trained over all available languages. Following this, for a given target language"
C18-1214,P04-3031,0,0.247291,"Missing"
C18-1214,C14-1096,0,0.0483234,"Missing"
C18-1214,W14-2201,0,0.13972,"Missing"
C18-1214,P11-1061,0,0.107033,"Missing"
C18-1214,P17-2093,0,0.0668892,"Missing"
C18-1214,N13-1014,0,0.117134,"Missing"
C18-1214,P13-1057,0,0.0496625,"Missing"
C18-1214,P07-2045,0,0.00559034,"Missing"
C18-1214,N16-1030,0,0.0245985,"Missing"
C18-1214,petrov-etal-2012-universal,0,0.140452,"Missing"
C18-1214,Q13-1001,0,0.0606958,"Missing"
C18-1214,N03-1033,0,0.165481,"Missing"
C18-1214,N01-1026,0,0.496919,"Missing"
C18-1214,N16-1156,0,0.040947,"Missing"
C18-1214,W02-0109,0,\N,Missing
D16-1133,C14-1096,0,0.147395,"be annotated with translations than with transcriptions. This translated speech is a potentially valuable source of information – for example, for documenting endangered languages or for training speech translation systems. In language documentation, data is usable only if it is interpretable. To make a collection of speech data usable for future studies of the language, something resembling interlinear glossed text (transcription, morphological analysis, word glosses, free translation) would be needed at minimum. New technologies are being developed to facilitate collection of translations (Bird et al., 2014), and there already exist recent examples of parallel speech collection efforts focused on endangered languages (Blachon et al., 2016; Adda et al., 2016). As for the other annotation layers, one might hope that a first pass could be done automatically. A first step towards this goal would be to automatically align spoken words with their translations, capturing information similar to that captured by word glosses. In machine translation, statistical models have traditionally required alignments between the source and target languages as the first step of training. Therefore, producing alignmen"
D16-1133,J93-2003,0,0.0649211,"rforms both a naive but strong baseline and a neural model (Duong et al., 2016). 1 https://www3.nd.edu/∼aanastas/griko/griko-data.tar.gz 1255 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1255–1263, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics 2 Background In this section, we briefly describe the existing models that the two components of our model are based on. In the next section, we will describe how we adapt and combine them to the present task. 2.1 IBM Model 2 and fast_align The IBM translation models (Brown et al., 1993) aim to model the distribution p(e |f) for an English sentence e = e1 · · · el , given a French sentence f = f1 · · · em . They all introduce a hidden variable a = a1 · · · al that gives the position of the French word to which each English word is aligned. The general form of IBM Models 1, 2 and fast_align is p(e, a |f) = p(l) l Y i=1 t(ei |fai ) δ(ai |i, l, m) where t(e |f ) is the probability of translating French word f to English word e, and δ(ai = j |i, l, m) is the probability of aligning the i-th English word with the j-th French word. In Model 1, δ is uniform; in Model 2, it is a cate"
D16-1133,N16-1109,1,0.904362,"2 (Dyer et al., 2013), more commonly known as fast_align, and k-means clustering using Dynamic Time Warping (Berndt and Clifford, 1994) as a distance measure. The two components are trained jointly using expectation-maximization. We experiment on two language pairs. One is Spanish-English, using the CALLHOME and Fisher corpora. The other is Griko-Italian; Griko is an endangered language for which we created (and make freely available)1 gold-standard translations and word alignments (Lekakou et al., 2013). In all cases, our model outperforms both a naive but strong baseline and a neural model (Duong et al., 2016). 1 https://www3.nd.edu/∼aanastas/griko/griko-data.tar.gz 1255 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1255–1263, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics 2 Background In this section, we briefly describe the existing models that the two components of our model are based on. In the next section, we will describe how we adapt and combine them to the present task. 2.1 IBM Model 2 and fast_align The IBM translation models (Brown et al., 1993) aim to model the distribution p(e |f) for an English senten"
D16-1133,N13-1073,0,0.16058,". A first step towards this goal would be to automatically align spoken words with their translations, capturing information similar to that captured by word glosses. In machine translation, statistical models have traditionally required alignments between the source and target languages as the first step of training. Therefore, producing alignments between speech and text would be a natural first step towards MT systems operating directly on speech. We present a model that, in order to learn such alignments, adapts and combines two components: Dyer et al.’s reparameterization of IBM Model 2 (Dyer et al., 2013), more commonly known as fast_align, and k-means clustering using Dynamic Time Warping (Berndt and Clifford, 1994) as a distance measure. The two components are trained jointly using expectation-maximization. We experiment on two language pairs. One is Spanish-English, using the CALLHOME and Fisher corpora. The other is Griko-Italian; Griko is an endangered language for which we created (and make freely available)1 gold-standard translations and word alignments (Lekakou et al., 2013). In all cases, our model outperforms both a naive but strong baseline and a neural model (Duong et al., 2016)."
D16-1133,P12-1018,0,0.0151491,"erparameters p0 , λ, and µ are not learned. We simply set p0 to zero (disallowing unaligned target words) and set µ as described above. For λ we perform a grid search over candidate values to maximize the alignment F-score on the development set. We obtain the best scores with λ = 0.5. 5 Related Work A first step towards modelling parallel speech can be performed by modelling phone-to-word alignment, instead of directly working on continuous speech. For example, Stahlberg et al. (2012) extend IBM Model 3 to align phones to words in order to build cross-lingual pronunciation lexicons. Pialign (Neubig et al., 2012) aligns characters and can be applied equally well to phones. Duong et al. (2016) use an extension of the neural attentional model of Bahdanau et al. (2015) for aligning phones to words and speech to words; we discuss this model below in Section 6.2. There exist several supervised approaches that attempt to integrate speech recognition and machine translation. However, they rely heavily on the abundance of training data, pronunciation lexicons, or language models, and therefore cannot be applied in a low- or zero-resource setting. A task somewhat similar to ours, which operates at a monolingua"
D16-1133,2013.iwslt-papers.14,0,0.0995941,"eline methods, a naive baseline, and the model of Duong et al. (2016). 6.1 Data For each language pair, we require a sentencealigned parallel corpus of source-language speech and target-language text. A subset of these sentences should be annotated with span-to-word alignments for use as a gold standard. 6.1.1 Spanish-English For Spanish-English, we use the Spanish CALLHOME corpus (LDC96S35) and the Fisher corpus 1259 (LDC2010T04), which consist of telephone conversations between Spanish native speakers based in the US and their relatives abroad, together with English translations produced by Post et al. (2013). Spanish is obviously not a low-resource language, but we pretend that it is low-resource by not making use of any Spanish ASR or resources like transcribed speech or pronunciation lexicons. Since there do not exist gold standard alignments between the Spanish speech and English words, we use the “silver” standard alignments produced by Duong et al. (2016) for the CALLHOME corpus, and followed the same procedure for the Fisher corpus as well. In order to obtain them, they first used a forced aligner to align the speech to its transcription, and GIZA++ with the gdfa symmetrization heuristic to"
D16-1133,C96-2141,0,0.758629,"Missing"
D19-1091,K17-2001,0,0.327688,"Missing"
D19-1091,P17-1183,0,0.0482938,"results in the future. ν α π ρ ο σ α ρ τ ή σ ο υ μ ε ax at ax at Figure 3: Attention visualization examples. The inflected form is generated from top to bottom. quently, the lemma attention properly copies the stem, and then the tag attention attends first over PFV and then over PL and 3 in order to construct the correct suffix for perfective and 3rd person plural. 5 Related Work 6 The inflection task in high-resource settings has been extensively studied through the SIGMORPHON shared tasks. Notably, the best models explicitly model copying and hard monotonic attention (Aharoni et al., 2016; Aharoni and Goldberg, 2017) with the previous state-of-the-art forcing strict monotonicity (Wu and Cotterell, 2019). We instead achieve state-of-the-art with a cheaper approach that simply intermixes a copying task which also encourages monotonicity. Data augmentation for inflection has been explored by Bergmanis et al. (2017) and Zhou and Neubig (2017) among others. The work of Silfverberg et al. (2017) is the most similar to ours, but as we already discussed, it has a few shortcomings that our approach addresses. Kann et al. (2017) have identified typology as playing a role for cross-lingual transfer, but they measure"
D19-1091,N16-1077,1,0.862619,"on towards even better results in the future. ν α π ρ ο σ α ρ τ ή σ ο υ μ ε ax at ax at Figure 3: Attention visualization examples. The inflected form is generated from top to bottom. quently, the lemma attention properly copies the stem, and then the tag attention attends first over PFV and then over PL and 3 in order to construct the correct suffix for perfective and 3rd person plural. 5 Related Work 6 The inflection task in high-resource settings has been extensively studied through the SIGMORPHON shared tasks. Notably, the best models explicitly model copying and hard monotonic attention (Aharoni et al., 2016; Aharoni and Goldberg, 2017) with the previous state-of-the-art forcing strict monotonicity (Wu and Cotterell, 2019). We instead achieve state-of-the-art with a cheaper approach that simply intermixes a copying task which also encourages monotonicity. Data augmentation for inflection has been explored by Bergmanis et al. (2017) and Zhou and Neubig (2017) among others. The work of Silfverberg et al. (2017) is the most similar to ours, but as we already discussed, it has a few shortcomings that our approach addresses. Kann et al. (2017) have identified typology as playing a role for cross-lingu"
D19-1091,P17-1182,0,0.0483132,"s explicitly model copying and hard monotonic attention (Aharoni et al., 2016; Aharoni and Goldberg, 2017) with the previous state-of-the-art forcing strict monotonicity (Wu and Cotterell, 2019). We instead achieve state-of-the-art with a cheaper approach that simply intermixes a copying task which also encourages monotonicity. Data augmentation for inflection has been explored by Bergmanis et al. (2017) and Zhou and Neubig (2017) among others. The work of Silfverberg et al. (2017) is the most similar to ours, but as we already discussed, it has a few shortcomings that our approach addresses. Kann et al. (2017) have identified typology as playing a role for cross-lingual transfer, but they measure language similarity using lexical overlap. We attest that this data-based measure is less informative and more suspect to variation, so we instead use the genetic typological information to quantify correlations between performance improvements and language distance. Our novel two-step process decoder architecture bares similarities with multi-source models (Anastasopoulos and Chiang, 2018; Zoph and Knight, 2016) which provide two contexts from two encoded sources to the decoder. A similar disentangled enc"
D19-1091,K17-2002,0,0.0752231,"struct the correct suffix for perfective and 3rd person plural. 5 Related Work 6 The inflection task in high-resource settings has been extensively studied through the SIGMORPHON shared tasks. Notably, the best models explicitly model copying and hard monotonic attention (Aharoni et al., 2016; Aharoni and Goldberg, 2017) with the previous state-of-the-art forcing strict monotonicity (Wu and Cotterell, 2019). We instead achieve state-of-the-art with a cheaper approach that simply intermixes a copying task which also encourages monotonicity. Data augmentation for inflection has been explored by Bergmanis et al. (2017) and Zhou and Neubig (2017) among others. The work of Silfverberg et al. (2017) is the most similar to ours, but as we already discussed, it has a few shortcomings that our approach addresses. Kann et al. (2017) have identified typology as playing a role for cross-lingual transfer, but they measure language similarity using lexical overlap. We attest that this data-based measure is less informative and more suspect to variation, so we instead use the genetic typological information to quantify correlations between performance improvements and language distance. Our novel two-step process decod"
D19-1091,L18-1293,0,0.0523411,"Missing"
D19-1091,Q18-1039,0,0.0271142,"ignments. Briefly, this means that if the i-th source character/word is aligned to the j-th target one (i ← j), then alignments i+1← j+1 or i← j+1 are also quite likely. In a neural architecture this can be approximated by providing the attention weight vector from the previous timestep as input to the function that computes the attention weights. We refer the reader to Cohn et al. (2016) for exact details. Adversarial Language Discriminator When training multilingual systems, encouraging the encoder to learn language-invariant representations can often lead to improvements (Xie et al., 2017; Chen et al., 2018), as it forces the model to truly work in a multilingual setting. We achieve that by introducing a language discriminator (Ganin et al., 2016). This additional component receives the last output of the (bi-directional) intermediate lemma representations hNx and outputs a prediction yl of the source language such that yl =softmax(MLP(hNx )). The discriminator is trained to predict the language by minimizing a standard cross-entropy 3 This heuristic number is largely arbitrary and could potentially be tuned to different values for each language. 986 the length of the region, though allowing for"
D19-1091,E17-2002,0,0.01635,"end of training helps a little more in terms of accuracy, but most importantly it speeds up training. Table 2 also reports the development set accuracy when using hallucinated data. Although the large improvements are also proportionally reflected in the test set, these numbers are not directly comparable to the rest, as the development set data were used in the hallucination process. 4 Analysis We analyze the results over various groupings of languages to elucidate the properties of our models over the 100 quite diverse language pairs. We use typological information from the URIEL database (Littell et al., 2017) in this analysis. Single Language Transfer We first focus on the test languages for which a single transfer language 6 Table 2 includes unpublished results kindly shared by Wu and Cotterell, the authors of the baseline. 989 L2 L1+L2 +Ll +H +Ll + H H turkish persian bashkir uzbek all azeri 81 55 57 47 84 77 63 59 55 71 80 74 66 74 83 81 69 67 70 87 66.7±0.9 urdu sanskrit hindi greek all bengali 42 44 49 42 49 32 38 52 46 50 66 66 67 65 64 67 65 65 67 62 63.7±4.0 turkish bashkir uzbek all crimean tatar 87 59 60 82 80 60 60 81 85 70 72 88 89 69 67 80 71.3±1.1 finnish hungarian estonian all ingri"
D19-1091,W19-6012,0,0.0948557,"Missing"
D19-1091,K17-2010,0,0.393635,"ion characters are sampled uniformly for the alphabet, rather than attempting to sample from a more informed distribution, which has potential for further improvements. Overall, we hallucinated 10,000 examples for each lowresource language, creating an additional hallucinated dataset H. A visualized example of our hallucination process is outlined in Figure 2. Out of the three regions with matching aligned characters (thick lines), we identify two with length equal to three or more. In the hallucinated example (bottom of the figure), we sample random characters for the inside of such regions. Silfverberg et al. (2017) have proposed a data hallucination method conceptually quite similar to ours, which treats the single longest common continuous substring between lemma and form as the stem. Their approach would be effectively similar to ours for languages with affixal steminvariant morphology, but it would likely fail in more complicated morphological phenomena like apophony, stem alternation (conceptually similar to infix morphology) or in the root-and-pattern morphology of semitic languages. Consider the apophony example from the past participle form gschwommen of the lemma schwimmen in Swiss German: our a"
D19-1091,P19-1148,0,0.572689,"k). The lemma and inflected forms are aligned at the character level. The inside of stem-considered parts (highlighted) are substituted with random characters, creating hallucinated triples (bottom). Additional Structural Biases for Attention Incorporating structural biases in the model’s architecture or in the training objective can lead to improvements in performance, especially for tasks where the attention mechanism is expected to behave similarly to an alignment model, like MT. This idea has been successfully applied to the inflection task and is at the core of the state-of-theart model (Wu and Cotterell, 2019). One bias we deem important is coverage of all input characters and tags from the attentions. Intuitively, this entails encouraging the model to “look at” the whole input. We take the approach of Cohn et al. (2016) and add two regularizers over the final attention matrices, encouraging them to also sum to one column-wise: −λ k Σ j atjm − I k2 stem Original triple lemma loss Ll similar to Lample et al. (2018). However, in order to encourage the encoder to learn language-invariant representations, we reverse the gradients flowing from that component into the encoder during back-propagation. 2.2"
D19-1091,K17-2005,1,0.859182,"r perfective and 3rd person plural. 5 Related Work 6 The inflection task in high-resource settings has been extensively studied through the SIGMORPHON shared tasks. Notably, the best models explicitly model copying and hard monotonic attention (Aharoni et al., 2016; Aharoni and Goldberg, 2017) with the previous state-of-the-art forcing strict monotonicity (Wu and Cotterell, 2019). We instead achieve state-of-the-art with a cheaper approach that simply intermixes a copying task which also encourages monotonicity. Data augmentation for inflection has been explored by Bergmanis et al. (2017) and Zhou and Neubig (2017) among others. The work of Silfverberg et al. (2017) is the most similar to ours, but as we already discussed, it has a few shortcomings that our approach addresses. Kann et al. (2017) have identified typology as playing a role for cross-lingual transfer, but they measure language similarity using lexical overlap. We attest that this data-based measure is less informative and more suspect to variation, so we instead use the genetic typological information to quantify correlations between performance improvements and language distance. Our novel two-step process decoder architecture bares simil"
D19-1091,N16-1004,0,0.0422123,"milar to ours, but as we already discussed, it has a few shortcomings that our approach addresses. Kann et al. (2017) have identified typology as playing a role for cross-lingual transfer, but they measure language similarity using lexical overlap. We attest that this data-based measure is less informative and more suspect to variation, so we instead use the genetic typological information to quantify correlations between performance improvements and language distance. Our novel two-step process decoder architecture bares similarities with multi-source models (Anastasopoulos and Chiang, 2018; Zoph and Knight, 2016) which provide two contexts from two encoded sources to the decoder. A similar disentangled encoding was also used by Ács (2018) for their SIGMORPHON 2018 submission. We in fact experimented with this architecture but preConclusion With this work we advance the state-of-the-art for morphological inflection on low-resource languages by 15 points, through a novel architecture, data hallucination, and a variety of training techniques. Our two-step attention decoder follows an intuitive order, also enhancing interpretability. We also suggest that complicated methods for copying and forcing monoton"
D19-1091,D15-1166,0,\N,Missing
D19-1091,W16-2007,0,\N,Missing
D19-1091,D18-1103,1,\N,Missing
D19-1091,K18-3016,0,\N,Missing
D19-1091,W16-2002,0,\N,Missing
D19-1112,N19-1423,0,0.560121,"f fields (He et al., 2017; Vaswani et al., 2017; Povey et al., 2018; Yu et al., 2018). For natural language understanding (NLU) tasks, robust and flexible language representations can be adapted to new tasks or domains efficiently. Aiming at learning representations that are not exclusively tailored to any specific tasks or domains, researchers have proposed several ways to learn general language representations. Recently, there is a trend of learning universal language representations via language model pretraining (Dai and Le, 2015; Peters et al., 2018; Radford et al., 2018). In particular, Devlin et al. (2019) present the BERT model which is based on a bidirectional Transformer (Vaswani et al., 2017). BERT is pre-trained with both masked language model and next sentence prediction objectives and exhibits strong performance on several benchmarks, attracting huge attention from researchers. Another line of research tries to apply multi-task learning to representation learning (Liu et al., 2015; Luong et al., 2015). Multi-task learning allows the model to leverage supervision signals from related tasks and prevents the model from overfitting to a single task. By combining the strengths of both languag"
D19-1112,I05-5002,0,0.0280651,"with three different choices of the task distribution p(T ). Specifically, we propose the following options: • Uniform: sample tasks uniformly. We conduct experiments on the GLUE dataset (Wang et al., 2019) and only on English. Following previous work (Devlin et al., 2019; Liu et al., 2019) we do not train or test models on the WNLI dataset (Levesque et al., 2012). We treat the four high-resource tasks, namely SST-2 (Socher et al., 2013), QQP,1 MNLI (Williams et al., 2018), and QNLI (Rajpurkar et al., 2016), as auxiliary tasks. The other four tasks, namely CoLA (Warstadt et al., 2018), MRPC (Dolan and Brockett, 2005), STS-B (Cera et al., 2017), and RTE (Dagan et al., 2005) are our target tasks. We also evaluate the generalization ability of our approaches on the SciTail dataset (Khot et al., 2018). The details of all datasets are illustrated in Appendix A. We compare our models with two strong baselines: the BERT model (Devlin et al., 2019) and the MT-DNN model (Liu et al., 2019). While the former pre-trains the Transformer model on large amounts of unlabeled dataset, the latter further improves it with multi-task learning. For BERT and MT-DNN, we use their publicly available code to obtain the final resu"
D19-1112,D18-1398,0,0.339511,"algorithm (MAML) and its variants for low-resource NLU tasks. We validate our methods on the GLUE benchmark and show that our proposed models can outperform several strong baselines. We further empirically demonstrate that the learned representations can be adapted to new tasks efficiently and effectively. 1 Figure 1: Differences between multi-task learning and meta learning. Multi-task learning may favor highresource tasks over low-resource ones while metalearning aims at learning a good initialization that can be adapted to any task with minimal training samples. The figure is adapted from Gu et al. (2018). Introduction With the ability to learn rich distributed representations of data in an end-to-end fashion, deep neural networks have achieved the state of the arts in a variety of fields (He et al., 2017; Vaswani et al., 2017; Povey et al., 2018; Yu et al., 2018). For natural language understanding (NLU) tasks, robust and flexible language representations can be adapted to new tasks or domains efficiently. Aiming at learning representations that are not exclusively tailored to any specific tasks or domains, researchers have proposed several ways to learn general language representations. Rece"
D19-1112,P18-1031,0,0.0252888,"when less data are available, especially compared to BERT, suggesting the metalearning algorithms can indeed be helpful for lowresource tasks. 4 Related Work There is a long history of learning general language representations. Previous work on learning general language representations focus on learning word (Mikolov et al., 2013; Pennington et al., 2014) or sentence representations (Le and Mikolov, 2014; Kiros et al., 2015) that are helpful for downstream tasks. Recently, there is a trend of learning contextualized word embeddings (Dai and Le, 2015; McCann et al., 2017; Peters et al., 2018; Howard and Ruder, 2018). One representative approach is the BERT model (Devlin et al., 2019) which learns contextualized word embeddings via bidirectional Transformer models. Another line of research on learning representations focus on multi-task learning (Collobert et al., 2011; Liu et al., 2015). In particular, Liu et al. (2019) propose to combine multi-task learning with language model pre-training and demonstrate the two methods are complementary to each other. Meta-learning algorithms have received lots of attention recently due to their effectiveness (Finn et al., 2017; Fan et al., 2018). However, the potenti"
D19-1112,N15-1092,0,0.0770399,"nguage representations. Recently, there is a trend of learning universal language representations via language model pretraining (Dai and Le, 2015; Peters et al., 2018; Radford et al., 2018). In particular, Devlin et al. (2019) present the BERT model which is based on a bidirectional Transformer (Vaswani et al., 2017). BERT is pre-trained with both masked language model and next sentence prediction objectives and exhibits strong performance on several benchmarks, attracting huge attention from researchers. Another line of research tries to apply multi-task learning to representation learning (Liu et al., 2015; Luong et al., 2015). Multi-task learning allows the model to leverage supervision signals from related tasks and prevents the model from overfitting to a single task. By combining the strengths of both language model pre-training and multi-task learning, Liu et al. (2019) improve the BERT model with multi-task learning and their proposed MT-DNN model successfully achieves state-of-the-art results on several NLU tasks. Although multi-task learning can achieve promising performance, there still exist some potential problems. As shown in Figure 1, multitask learning may favor tasks with signifi"
D19-1112,P19-1441,0,0.221848,"directional Transformer (Vaswani et al., 2017). BERT is pre-trained with both masked language model and next sentence prediction objectives and exhibits strong performance on several benchmarks, attracting huge attention from researchers. Another line of research tries to apply multi-task learning to representation learning (Liu et al., 2015; Luong et al., 2015). Multi-task learning allows the model to leverage supervision signals from related tasks and prevents the model from overfitting to a single task. By combining the strengths of both language model pre-training and multi-task learning, Liu et al. (2019) improve the BERT model with multi-task learning and their proposed MT-DNN model successfully achieves state-of-the-art results on several NLU tasks. Although multi-task learning can achieve promising performance, there still exist some potential problems. As shown in Figure 1, multitask learning may favor tasks with significantly larger amounts of data than others. Liu et al. (2019) alleviate this problem by adding an additional fine-tuning stage after multi-task learning. In this paper, we propose to apply metalearning algorithms in general language represen1192 Proceedings of the 2019 Confe"
D19-1112,P19-1253,0,0.0309714,"arch on learning representations focus on multi-task learning (Collobert et al., 2011; Liu et al., 2015). In particular, Liu et al. (2019) propose to combine multi-task learning with language model pre-training and demonstrate the two methods are complementary to each other. Meta-learning algorithms have received lots of attention recently due to their effectiveness (Finn et al., 2017; Fan et al., 2018). However, the potential of applying meta-learning algorithms in NLU tasks have not been fully investigated yet. Gu et al. (2018) have tried to apply first-order MAML in machine translation and Qian and Yu (2019) propose to address the domain adaptation problem in dialogue generation by using MAML. To the best of our knowledge, the Reptile algorithm, which is simpler than MAML and potentially more useful, has been given less attention. 5 Conclusion In this paper, we investigate three optimizationbased meta-learning algorithms for low-resource NLU tasks. We demonstrate the effectiveness of these algorithms and perform a fair amount of ablation studies. We also show the learned representations can be adapted to new tasks effectively. Our study suggests promising applications of meta-learning algorithms"
D19-1112,D16-1264,0,0.0232324,"chieve competitive or superior performance compared to MAML. 2.4 Choosing the Task Distributions We experiment with three different choices of the task distribution p(T ). Specifically, we propose the following options: • Uniform: sample tasks uniformly. We conduct experiments on the GLUE dataset (Wang et al., 2019) and only on English. Following previous work (Devlin et al., 2019; Liu et al., 2019) we do not train or test models on the WNLI dataset (Levesque et al., 2012). We treat the four high-resource tasks, namely SST-2 (Socher et al., 2013), QQP,1 MNLI (Williams et al., 2018), and QNLI (Rajpurkar et al., 2016), as auxiliary tasks. The other four tasks, namely CoLA (Warstadt et al., 2018), MRPC (Dolan and Brockett, 2005), STS-B (Cera et al., 2017), and RTE (Dagan et al., 2005) are our target tasks. We also evaluate the generalization ability of our approaches on the SciTail dataset (Khot et al., 2018). The details of all datasets are illustrated in Appendix A. We compare our models with two strong baselines: the BERT model (Devlin et al., 2019) and the MT-DNN model (Liu et al., 2019). While the former pre-trains the Transformer model on large amounts of unlabeled dataset, the latter further improves"
D19-1112,D13-1170,0,0.004233,"ps. Despite the simplicity of Reptile, it has been demonstrated to achieve competitive or superior performance compared to MAML. 2.4 Choosing the Task Distributions We experiment with three different choices of the task distribution p(T ). Specifically, we propose the following options: • Uniform: sample tasks uniformly. We conduct experiments on the GLUE dataset (Wang et al., 2019) and only on English. Following previous work (Devlin et al., 2019; Liu et al., 2019) we do not train or test models on the WNLI dataset (Levesque et al., 2012). We treat the four high-resource tasks, namely SST-2 (Socher et al., 2013), QQP,1 MNLI (Williams et al., 2018), and QNLI (Rajpurkar et al., 2016), as auxiliary tasks. The other four tasks, namely CoLA (Warstadt et al., 2018), MRPC (Dolan and Brockett, 2005), STS-B (Cera et al., 2017), and RTE (Dagan et al., 2005) are our target tasks. We also evaluate the generalization ability of our approaches on the SciTail dataset (Khot et al., 2018). The details of all datasets are illustrated in Appendix A. We compare our models with two strong baselines: the BERT model (Devlin et al., 2019) and the MT-DNN model (Liu et al., 2019). While the former pre-trains the Transformer m"
D19-1112,N18-1101,0,0.0306979,"ile, it has been demonstrated to achieve competitive or superior performance compared to MAML. 2.4 Choosing the Task Distributions We experiment with three different choices of the task distribution p(T ). Specifically, we propose the following options: • Uniform: sample tasks uniformly. We conduct experiments on the GLUE dataset (Wang et al., 2019) and only on English. Following previous work (Devlin et al., 2019; Liu et al., 2019) we do not train or test models on the WNLI dataset (Levesque et al., 2012). We treat the four high-resource tasks, namely SST-2 (Socher et al., 2013), QQP,1 MNLI (Williams et al., 2018), and QNLI (Rajpurkar et al., 2016), as auxiliary tasks. The other four tasks, namely CoLA (Warstadt et al., 2018), MRPC (Dolan and Brockett, 2005), STS-B (Cera et al., 2017), and RTE (Dagan et al., 2005) are our target tasks. We also evaluate the generalization ability of our approaches on the SciTail dataset (Khot et al., 2018). The details of all datasets are illustrated in Appendix A. We compare our models with two strong baselines: the BERT model (Devlin et al., 2019) and the MT-DNN model (Liu et al., 2019). While the former pre-trains the Transformer model on large amounts of unlabeled d"
D19-1112,D14-1162,0,0.105788,"ance on these datasets. Figure 2 reveals that our model consistently outperforms the strong MT1195 DNN baseline across different settings, indicating the learned representations are more effective for transfer learning. In particular, the algorithm is more effective when less data are available, especially compared to BERT, suggesting the metalearning algorithms can indeed be helpful for lowresource tasks. 4 Related Work There is a long history of learning general language representations. Previous work on learning general language representations focus on learning word (Mikolov et al., 2013; Pennington et al., 2014) or sentence representations (Le and Mikolov, 2014; Kiros et al., 2015) that are helpful for downstream tasks. Recently, there is a trend of learning contextualized word embeddings (Dai and Le, 2015; McCann et al., 2017; Peters et al., 2018; Howard and Ruder, 2018). One representative approach is the BERT model (Devlin et al., 2019) which learns contextualized word embeddings via bidirectional Transformer models. Another line of research on learning representations focus on multi-task learning (Collobert et al., 2011; Liu et al., 2015). In particular, Liu et al. (2019) propose to combine multi"
D19-1112,N18-1202,0,0.339502,"networks have achieved the state of the arts in a variety of fields (He et al., 2017; Vaswani et al., 2017; Povey et al., 2018; Yu et al., 2018). For natural language understanding (NLU) tasks, robust and flexible language representations can be adapted to new tasks or domains efficiently. Aiming at learning representations that are not exclusively tailored to any specific tasks or domains, researchers have proposed several ways to learn general language representations. Recently, there is a trend of learning universal language representations via language model pretraining (Dai and Le, 2015; Peters et al., 2018; Radford et al., 2018). In particular, Devlin et al. (2019) present the BERT model which is based on a bidirectional Transformer (Vaswani et al., 2017). BERT is pre-trained with both masked language model and next sentence prediction objectives and exhibits strong performance on several benchmarks, attracting huge attention from researchers. Another line of research tries to apply multi-task learning to representation learning (Liu et al., 2015; Luong et al., 2015). Multi-task learning allows the model to leverage supervision signals from related tasks and prevents the model from overfitting"
D19-1147,D11-1033,0,0.0511909,"omain tags to control the output domain, but it still needs a in-domain parallel corpus and our architecture allows more flexible modifications than just adding additional tags. Unsupervised domain adaptation techniques for NMT can be divided into data- and model-centric methods (Chu and Wang, 2018). Data-centric approaches mainly focus on selecting or generating the domain-related data using existing in-domain monolingual data. Both the copy method (Currey et al., 2017) and back-translation (Sennrich et al., 2016a) are representative data-centric methods. In addition, Moore and Lewis (2010); Axelrod et al. (2011); Duh et al. (2013) use LMs to score the outof-domain data, based on which they select data similar to in-domain text. Model-centric methods have not been fully investigated yet. Gulcehre et al. (2015) propose to fuse LMs and NMT models, but their methods require querying two models during inference and have been demonstrated to underperform the data-centric ones (Chu et al., 2018). There are also work on adaptation via retrieving sentences or n-grams in the training data similar to the test set (Farajian et al., 2017; Bapna and Firat, 2019). However, it can be difficult to find similar parall"
D19-1147,N19-1191,0,0.168073,"centric methods. In addition, Moore and Lewis (2010); Axelrod et al. (2011); Duh et al. (2013) use LMs to score the outof-domain data, based on which they select data similar to in-domain text. Model-centric methods have not been fully investigated yet. Gulcehre et al. (2015) propose to fuse LMs and NMT models, but their methods require querying two models during inference and have been demonstrated to underperform the data-centric ones (Chu et al., 2018). There are also work on adaptation via retrieving sentences or n-grams in the training data similar to the test set (Farajian et al., 2017; Bapna and Firat, 2019). However, it can be difficult to find similar parallel sentences in domain adaptation settings. 5 Conclusion In this work, we propose a simple yet effective unsupervised domain adaptation technique for neural machine translation, which adapts the model by domain-aware feature embeddings learned with language modeling. Experimental results demonstrate the effectiveness of the proposed approach across settings. In addition, analysis reveals that our method allows us to control the output domain of translation results. Future work include designing more sophisticated architectures and combinatio"
D19-1147,P18-1008,0,0.0324094,"the performance of the model.1 1 θ Layer N … (N ) task … (N ) θ base (N ) θ domain … … + (1) task (1) θ base (0 ) θ task (0 ) θ base θ Layer 1 (1) θ domain + Word Embeddings Input Task (0 ) θ domain Input Word Input Domain Figure 1: Main architecture of DAFE. Embedding learners generate domain- and task-specific features at each layer, which are then integrated into the output of the base network. Introduction While neural machine translation (NMT) systems have proven to be effective in scenarios where large amounts of in-domain data are available (Gehring et al., 2017; Vaswani et al., 2017; Chen et al., 2018), they have been demonstrated to perform poorly when the test domain does not match the training data (Koehn and Knowles, 2017). Collecting large amounts of parallel data in all possible domains we are interested in is costly, and in many cases impossible. Therefore, it is essential to explore effective methods to train models that generalize well to new domains. 1 Our code is publicly available at: https://github. com/zdou0830/DAFE. Domain adaptation for neural machine translation has attracted much attention in the research community, with the majority of work focusing on the supervised sett"
D19-1147,P17-2061,0,0.0197637,"Knowles, 2017). Collecting large amounts of parallel data in all possible domains we are interested in is costly, and in many cases impossible. Therefore, it is essential to explore effective methods to train models that generalize well to new domains. 1 Our code is publicly available at: https://github. com/zdou0830/DAFE. Domain adaptation for neural machine translation has attracted much attention in the research community, with the majority of work focusing on the supervised setting where a small amount of in-domain data is available (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Chu et al., 2017; Vilar, 2018). An established approach is to use domain tags as additional input, with the domain representations learned over parallel data (Kobus et al., 2017). In this work, we focus on unsupervised adaptation, where there are no in-domain parallel data available. Within this paradigm, Currey et al. (2017) copy the in-domain monolingual data from the target side to the source side and Sennrich et al. (2016a) concatenate backtranslated data with the original corpus. However, these methods learn generic representations for all the text, as the learned representations are shared for all the d"
D19-1147,C18-1111,0,0.0634817,"rk for NMT focus on the setting where a small amount of indomain data is available. Continued training (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016) methods first train an NMT model on outof-domain data and then fine-tune it on the indomain data. Similar to our work, Kobus et al. 1420 (2017) propose to use domain tags to control the output domain, but it still needs a in-domain parallel corpus and our architecture allows more flexible modifications than just adding additional tags. Unsupervised domain adaptation techniques for NMT can be divided into data- and model-centric methods (Chu and Wang, 2018). Data-centric approaches mainly focus on selecting or generating the domain-related data using existing in-domain monolingual data. Both the copy method (Currey et al., 2017) and back-translation (Sennrich et al., 2016a) are representative data-centric methods. In addition, Moore and Lewis (2010); Axelrod et al. (2011); Duh et al. (2013) use LMs to score the outof-domain data, based on which they select data similar to in-domain text. Model-centric methods have not been fully investigated yet. Gulcehre et al. (2015) propose to fuse LMs and NMT models, but their methods require querying two mo"
D19-1147,W17-4715,0,0.347043,"com/zdou0830/DAFE. Domain adaptation for neural machine translation has attracted much attention in the research community, with the majority of work focusing on the supervised setting where a small amount of in-domain data is available (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Chu et al., 2017; Vilar, 2018). An established approach is to use domain tags as additional input, with the domain representations learned over parallel data (Kobus et al., 2017). In this work, we focus on unsupervised adaptation, where there are no in-domain parallel data available. Within this paradigm, Currey et al. (2017) copy the in-domain monolingual data from the target side to the source side and Sennrich et al. (2016a) concatenate backtranslated data with the original corpus. However, these methods learn generic representations for all the text, as the learned representations are shared for all the domains and synthetic and natural data are treated equally. Sharing embeddings may be sub-optimal as data from different domains are inherently different. This problem is exacerbated 1417 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Confe"
D19-1147,P13-2119,1,0.821256,"he output domain, but it still needs a in-domain parallel corpus and our architecture allows more flexible modifications than just adding additional tags. Unsupervised domain adaptation techniques for NMT can be divided into data- and model-centric methods (Chu and Wang, 2018). Data-centric approaches mainly focus on selecting or generating the domain-related data using existing in-domain monolingual data. Both the copy method (Currey et al., 2017) and back-translation (Sennrich et al., 2016a) are representative data-centric methods. In addition, Moore and Lewis (2010); Axelrod et al. (2011); Duh et al. (2013) use LMs to score the outof-domain data, based on which they select data similar to in-domain text. Model-centric methods have not been fully investigated yet. Gulcehre et al. (2015) propose to fuse LMs and NMT models, but their methods require querying two models during inference and have been demonstrated to underperform the data-centric ones (Chu et al., 2018). There are also work on adaptation via retrieving sentences or n-grams in the training data similar to the test set (Farajian et al., 2017; Bapna and Firat, 2019). However, it can be difficult to find similar parallel sentences in dom"
D19-1147,W17-4713,0,0.0627002,"re representative data-centric methods. In addition, Moore and Lewis (2010); Axelrod et al. (2011); Duh et al. (2013) use LMs to score the outof-domain data, based on which they select data similar to in-domain text. Model-centric methods have not been fully investigated yet. Gulcehre et al. (2015) propose to fuse LMs and NMT models, but their methods require querying two models during inference and have been demonstrated to underperform the data-centric ones (Chu et al., 2018). There are also work on adaptation via retrieving sentences or n-grams in the training data similar to the test set (Farajian et al., 2017; Bapna and Firat, 2019). However, it can be difficult to find similar parallel sentences in domain adaptation settings. 5 Conclusion In this work, we propose a simple yet effective unsupervised domain adaptation technique for neural machine translation, which adapts the model by domain-aware feature embeddings learned with language modeling. Experimental results demonstrate the effectiveness of the proposed approach across settings. In addition, analysis reveals that our method allows us to control the output domain of translation results. Future work include designing more sophisticated arch"
D19-1147,P16-1009,0,0.503387,"he research community, with the majority of work focusing on the supervised setting where a small amount of in-domain data is available (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Chu et al., 2017; Vilar, 2018). An established approach is to use domain tags as additional input, with the domain representations learned over parallel data (Kobus et al., 2017). In this work, we focus on unsupervised adaptation, where there are no in-domain parallel data available. Within this paradigm, Currey et al. (2017) copy the in-domain monolingual data from the target side to the source side and Sennrich et al. (2016a) concatenate backtranslated data with the original corpus. However, these methods learn generic representations for all the text, as the learned representations are shared for all the domains and synthetic and natural data are treated equally. Sharing embeddings may be sub-optimal as data from different domains are inherently different. This problem is exacerbated 1417 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1417–1422, c Hong Kong, China, November 3–7, 2019. 2019 As"
D19-1147,P16-1162,0,0.77214,"he research community, with the majority of work focusing on the supervised setting where a small amount of in-domain data is available (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Chu et al., 2017; Vilar, 2018). An established approach is to use domain tags as additional input, with the domain representations learned over parallel data (Kobus et al., 2017). In this work, we focus on unsupervised adaptation, where there are no in-domain parallel data available. Within this paradigm, Currey et al. (2017) copy the in-domain monolingual data from the target side to the source side and Sennrich et al. (2016a) concatenate backtranslated data with the original corpus. However, these methods learn generic representations for all the text, as the learned representations are shared for all the domains and synthetic and natural data are treated equally. Sharing embeddings may be sub-optimal as data from different domains are inherently different. This problem is exacerbated 1417 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1417–1422, c Hong Kong, China, November 3–7, 2019. 2019 As"
D19-1147,tiedemann-2012-parallel,0,0.0526779,"out-of-domain lm in data and {θbase , θtask , θdomain } for in-domain data. Training strategy. Our training strategy is shown in Algorithm 1. The ultimate goal is in mt } to learn a set of parameters {θbase , θdomain , θtask for in-domain machine translation. While out-of-domain parallel data allows us to train out mt }, the monolingual data help {θbase , θdomain , θtask in out the model learn both θdomain and θdomain . 3 Experiments 3.1 Setup Datasets. We validate our models in two different data settings. First, we train on the law, medical and IT datasets of the German-English OPUS corpus (Tiedemann, 2012) and test our methods’ ability to adapt from one domain to another. The dataset contain 2K development and test sentences in each domain, and about 715K, 1M and 337K training sentences respectively. These datasets are relatively small and the domains are quite distant from each other. In the second setting, we adapt models trained on the general-domain WMT-14 datasets into both the TED (Duh, 2018) and law, medical OPUS datasets. For this setting, we consider two language pairs, namely Czech and German to English. The Czech-English and GermanEnglish datasets consist of 1M and 4.5M sentences and"
D19-1147,kobus-etal-2017-domain,0,0.026352,"is essential to explore effective methods to train models that generalize well to new domains. 1 Our code is publicly available at: https://github. com/zdou0830/DAFE. Domain adaptation for neural machine translation has attracted much attention in the research community, with the majority of work focusing on the supervised setting where a small amount of in-domain data is available (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Chu et al., 2017; Vilar, 2018). An established approach is to use domain tags as additional input, with the domain representations learned over parallel data (Kobus et al., 2017). In this work, we focus on unsupervised adaptation, where there are no in-domain parallel data available. Within this paradigm, Currey et al. (2017) copy the in-domain monolingual data from the target side to the source side and Sennrich et al. (2016a) concatenate backtranslated data with the original corpus. However, these methods learn generic representations for all the text, as the learned representations are shared for all the domains and synthetic and natural data are treated equally. Sharing embeddings may be sub-optimal as data from different domains are inherently different. This pro"
D19-1147,N18-2080,0,0.0178104,"ollecting large amounts of parallel data in all possible domains we are interested in is costly, and in many cases impossible. Therefore, it is essential to explore effective methods to train models that generalize well to new domains. 1 Our code is publicly available at: https://github. com/zdou0830/DAFE. Domain adaptation for neural machine translation has attracted much attention in the research community, with the majority of work focusing on the supervised setting where a small amount of in-domain data is available (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Chu et al., 2017; Vilar, 2018). An established approach is to use domain tags as additional input, with the domain representations learned over parallel data (Kobus et al., 2017). In this work, we focus on unsupervised adaptation, where there are no in-domain parallel data available. Within this paradigm, Currey et al. (2017) copy the in-domain monolingual data from the target side to the source side and Sennrich et al. (2016a) concatenate backtranslated data with the original corpus. However, these methods learn generic representations for all the text, as the learned representations are shared for all the domains and syn"
D19-1147,W17-3204,0,0.061295,"(0 ) θ base θ Layer 1 (1) θ domain + Word Embeddings Input Task (0 ) θ domain Input Word Input Domain Figure 1: Main architecture of DAFE. Embedding learners generate domain- and task-specific features at each layer, which are then integrated into the output of the base network. Introduction While neural machine translation (NMT) systems have proven to be effective in scenarios where large amounts of in-domain data are available (Gehring et al., 2017; Vaswani et al., 2017; Chen et al., 2018), they have been demonstrated to perform poorly when the test domain does not match the training data (Koehn and Knowles, 2017). Collecting large amounts of parallel data in all possible domains we are interested in is costly, and in many cases impossible. Therefore, it is essential to explore effective methods to train models that generalize well to new domains. 1 Our code is publicly available at: https://github. com/zdou0830/DAFE. Domain adaptation for neural machine translation has attracted much attention in the research community, with the majority of work focusing on the supervised setting where a small amount of in-domain data is available (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Chu et al., 201"
D19-1147,2015.iwslt-evaluation.11,0,0.417714,"test domain does not match the training data (Koehn and Knowles, 2017). Collecting large amounts of parallel data in all possible domains we are interested in is costly, and in many cases impossible. Therefore, it is essential to explore effective methods to train models that generalize well to new domains. 1 Our code is publicly available at: https://github. com/zdou0830/DAFE. Domain adaptation for neural machine translation has attracted much attention in the research community, with the majority of work focusing on the supervised setting where a small amount of in-domain data is available (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Chu et al., 2017; Vilar, 2018). An established approach is to use domain tags as additional input, with the domain representations learned over parallel data (Kobus et al., 2017). In this work, we focus on unsupervised adaptation, where there are no in-domain parallel data available. Within this paradigm, Currey et al. (2017) copy the in-domain monolingual data from the target side to the source side and Sennrich et al. (2016a) concatenate backtranslated data with the original corpus. However, these methods learn generic representations for all the text, as the"
D19-1147,P10-2041,0,0.128139,"(2017) propose to use domain tags to control the output domain, but it still needs a in-domain parallel corpus and our architecture allows more flexible modifications than just adding additional tags. Unsupervised domain adaptation techniques for NMT can be divided into data- and model-centric methods (Chu and Wang, 2018). Data-centric approaches mainly focus on selecting or generating the domain-related data using existing in-domain monolingual data. Both the copy method (Currey et al., 2017) and back-translation (Sennrich et al., 2016a) are representative data-centric methods. In addition, Moore and Lewis (2010); Axelrod et al. (2011); Duh et al. (2013) use LMs to score the outof-domain data, based on which they select data similar to in-domain text. Model-centric methods have not been fully investigated yet. Gulcehre et al. (2015) propose to fuse LMs and NMT models, but their methods require querying two models during inference and have been demonstrated to underperform the data-centric ones (Chu et al., 2018). There are also work on adaptation via retrieving sentences or n-grams in the training data similar to the test set (Farajian et al., 2017; Bapna and Firat, 2019). However, it can be difficult"
D19-1147,N19-4007,1,0.697209,"Missing"
N16-1109,P10-1010,1,0.84534,"Missing"
N16-1109,C12-2013,1,0.844149,"Missing"
N16-1109,C14-1096,1,0.809603,"Missing"
N16-1109,J93-2003,0,0.0596236,"ement in measured alignment quality. We now give a brief overview of these components. Previous attention. In the basic attentional model, the alignment is calculated based on the source encoding HS and the previous hidden state HTi−1 of the target, αi = Attend(HTi−1 , HS ), where Attend is a function that outputs m attention coefficients. This attention mechanism is overly simplistic, in that it is incapable of capturing patterns in the attention over different positions i. Recognising and exploiting these kinds of patterns has proven critical in traditional word based models of translation (Brown et al., 1993; Vogel et al., 1996; Dyer et al., 2013). For this reason Cohn et al. (2016) include explicit features encoding structural biases from word based models, namely absolute and relative position, Markov conditioning and fertility: 1. previous alignment, αi−1 P 2. sum of previous alignments, i−1 j=1 α j 3. source index vector, (1, 2, 3, . . . , m); and 4. target index vector (i, i, i, . . . , i). These features are concatenated to form a feature matrix β ∈ R4×m , which are added to the alignment calculation, i.e., αi = Attend(HTi−1 , HS , β) . Coverage penalty. The sum over previous alignments fea"
N16-1109,2012.eamt-1.60,0,0.00647996,"that do not require pronunciation lexicons, but train only on speech with text transcriptions (Lee et al., 2013; Maas et al., 2015; Graves et al., 2006). Here, we bypass phonetic transcriptions completely, and rely only on translations. Such data can be found, for example, in subtitled or dubbed movies. Some specific examples of corpora of parallel speech are the European Parliament Plenary Sessions Corpus (Van den Heuvel et al., 2006), which includes parliamentary speeches in the 21 official EU languages, as well as their interpretation into all the other languages; and the TED Talks Corpus (Cettolo et al., 2012), which provides speech in one language (usually English) together with translations into other languages. As mentioned in the introduction, a steppingstone to model parallel speech is to assume a recognizer that can produce a phonetic transcription of the source language, then to model the transformation from transcription to translation. We compare against three previous models that can operate on sequences of phones. The first is simply to run GIZA++ (IBM Model 4) on a phonetic transcription (without word boundaries) of the source side. Stahlberg et al. (2012) present a modification of IBM"
N16-1109,N13-1073,0,0.0289172,"now give a brief overview of these components. Previous attention. In the basic attentional model, the alignment is calculated based on the source encoding HS and the previous hidden state HTi−1 of the target, αi = Attend(HTi−1 , HS ), where Attend is a function that outputs m attention coefficients. This attention mechanism is overly simplistic, in that it is incapable of capturing patterns in the attention over different positions i. Recognising and exploiting these kinds of patterns has proven critical in traditional word based models of translation (Brown et al., 1993; Vogel et al., 1996; Dyer et al., 2013). For this reason Cohn et al. (2016) include explicit features encoding structural biases from word based models, namely absolute and relative position, Markov conditioning and fertility: 1. previous alignment, αi−1 P 2. sum of previous alignments, i−1 j=1 α j 3. source index vector, (1, 2, 3, . . . , m); and 4. target index vector (i, i, i, . . . , i). These features are concatenated to form a feature matrix β ∈ R4×m , which are added to the alignment calculation, i.e., αi = Attend(HTi−1 , HS , β) . Coverage penalty. The sum over previous alignments feature, described above provides a basic f"
N16-1109,P07-2045,0,0.00650321,"Missing"
N16-1109,D13-1019,0,0.00895223,"ity of alignment directly on source-language speech. <s> w1 wi-1 wn Decoder HT Ci Attention 2 wi Background To our knowledge, there has been relatively little research on models that operate directly on parallel speech. Typically, speech is transcribed into a word sequence or lattice using ASR, or at least a phone sequence or lattice using a phone recognizer. This normally requires manually transcribed data and a pronunciation lexicon, which can be costly to create. Recent work has introduced models that do not require pronunciation lexicons, but train only on speech with text transcriptions (Lee et al., 2013; Maas et al., 2015; Graves et al., 2006). Here, we bypass phonetic transcriptions completely, and rely only on translations. Such data can be found, for example, in subtitled or dubbed movies. Some specific examples of corpora of parallel speech are the European Parliament Plenary Sessions Corpus (Van den Heuvel et al., 2006), which includes parliamentary speeches in the 21 official EU languages, as well as their interpretation into all the other languages; and the TED Talks Corpus (Cettolo et al., 2012), which provides speech in one language (usually English) together with translations into"
N16-1109,D15-1166,0,0.176882,"us audio, represented as PLP vectors at 10ms intervals 3 Model We base our approach on the attentional translation model of Cohn et al. (2016), an extension of Bahdanau et al. (2015) which incorporates more fine grained components of the attention mechanism to mimic the structural biases in standard word based translation models. The attentional model encodes a source as a sequence of vectors, then decodes it to generate the output. At each step, it “attends” to different parts of the encoded sequence. This model has been used for translation, image caption generation, and speech recognition (Luong et al., 2015; Xu et al., 2015; Chorowski et al., 2014; Chorowski et al., 2015). Here, we briefly describe the basic attentional model, following Bahdanau et al. (2015), review the extensions for encoding structural biases (Cohn et al., 2016), and then present our novel means for adapting the approach handle parallel speech. 3.1 Base attentional model The model is shown in Figure 1. The speech signal is represented as a sequence of vectors S 1 , S 2 , . . . , S m . For the first set of experiments, each S i is a 128dimensional vector-space embedding of a phone. For the second set of experiments, each S i i"
N16-1109,N15-1038,0,0.0279326,"irectly on source-language speech. <s> w1 wi-1 wn Decoder HT Ci Attention 2 wi Background To our knowledge, there has been relatively little research on models that operate directly on parallel speech. Typically, speech is transcribed into a word sequence or lattice using ASR, or at least a phone sequence or lattice using a phone recognizer. This normally requires manually transcribed data and a pronunciation lexicon, which can be costly to create. Recent work has introduced models that do not require pronunciation lexicons, but train only on speech with text transcriptions (Lee et al., 2013; Maas et al., 2015; Graves et al., 2006). Here, we bypass phonetic transcriptions completely, and rely only on translations. Such data can be found, for example, in subtitled or dubbed movies. Some specific examples of corpora of parallel speech are the European Parliament Plenary Sessions Corpus (Van den Heuvel et al., 2006), which includes parliamentary speeches in the 21 official EU languages, as well as their interpretation into all the other languages; and the TED Talks Corpus (Cettolo et al., 2012), which provides speech in one language (usually English) together with translations into other languages. As"
N16-1109,P11-1064,0,0.00736763,"r languages. As mentioned in the introduction, a steppingstone to model parallel speech is to assume a recognizer that can produce a phonetic transcription of the source language, then to model the transformation from transcription to translation. We compare against three previous models that can operate on sequences of phones. The first is simply to run GIZA++ (IBM Model 4) on a phonetic transcription (without word boundaries) of the source side. Stahlberg et al. (2012) present a modification of IBM Model 3, named Model 3P, designed specifically for phone-to-word alignment. Finally, pialign (Neubig et al., 2011), an unsupervised model for joint phrase alignment and extraction, has been shown to work well at the character level (Neubig et al., 2012) and extends naturally to work on phones. 950 Encoder HS Representation S1 S2 S3 Sm Speech Signal Figure 1: The attentional model as applied to our tasks. We consider two types of input: discrete phone input, or continuous audio, represented as PLP vectors at 10ms intervals 3 Model We base our approach on the attentional translation model of Cohn et al. (2016), an extension of Bahdanau et al. (2015) which incorporates more fine grained components of the att"
N16-1109,P12-1018,0,0.0352532,"ic transcription of the source language, then to model the transformation from transcription to translation. We compare against three previous models that can operate on sequences of phones. The first is simply to run GIZA++ (IBM Model 4) on a phonetic transcription (without word boundaries) of the source side. Stahlberg et al. (2012) present a modification of IBM Model 3, named Model 3P, designed specifically for phone-to-word alignment. Finally, pialign (Neubig et al., 2011), an unsupervised model for joint phrase alignment and extraction, has been shown to work well at the character level (Neubig et al., 2012) and extends naturally to work on phones. 950 Encoder HS Representation S1 S2 S3 Sm Speech Signal Figure 1: The attentional model as applied to our tasks. We consider two types of input: discrete phone input, or continuous audio, represented as PLP vectors at 10ms intervals 3 Model We base our approach on the attentional translation model of Cohn et al. (2016), an extension of Bahdanau et al. (2015) which incorporates more fine grained components of the attention mechanism to mimic the structural biases in standard word based translation models. The attentional model encodes a source as a sequ"
N16-1109,P00-1056,0,0.0780057,"truction of the “silver” standard for evaluation, described below). We also use the English translations produced by Post et al. (2013). We treat the Spanish speech as a sequence of 39dimensional PLP vectors (order 12 with energy and first and second order delta) encoding the power spectrum of the speech signal. We do not have gold standard alignments between the Spanish speech and English words for evaluation, so we produced “silver” standard alignments. We used a forced aligner (Gorman et al., 2011) to align the speech to its transcription, and GIZA++ with the gdfa symmetrization heuristic (Och and Ney, 2000) to align the Spanish transcription to the English translation. We then combined the two alignments to produce “silver” standard alignments between the Spanish speech and the English words. Cleaning and splitting the data based on dialogue turns, resulted in a set of 17,532 Spanish utterances from which we selected 250 for development and 500 testing. For each utterance we have the corresponding English translation, and for each word in the translation we have the corresponding span of Spanish speech. The forced aligner produces the phonetic sequences that correspond to each utterance, which w"
N16-1109,2013.iwslt-papers.14,0,0.326579,"nment smoothing Note that when T = 1 we recover the standard softmax function; we set T = 10 in both experiments. 5 Experimental Setup We work on the Spanish CALLHOME Corpus (LDC96S35), which consists of telephone conversations between Spanish native speakers based in the US and their relatives abroad. While Spanish is not a low-resource language, we pretend that it is by not using any Spanish ASR or resources like transcribed speech or pronunciation lexicons (except in the construction of the “silver” standard for evaluation, described below). We also use the English translations produced by Post et al. (2013). We treat the Spanish speech as a sequence of 39dimensional PLP vectors (order 12 with energy and first and second order delta) encoding the power spectrum of the speech signal. We do not have gold standard alignments between the Spanish speech and English words for evaluation, so we produced “silver” standard alignments. We used a forced aligner (Gorman et al., 2011) to align the speech to its transcription, and GIZA++ with the gdfa symmetrization heuristic (Och and Ney, 2000) to align the Spanish transcription to the English translation. We then combined the two alignments to produce “silve"
N16-1109,van-den-heuvel-etal-2006-tc,0,0.0903295,"Missing"
N16-1109,C96-2141,0,0.222851,"ignment quality. We now give a brief overview of these components. Previous attention. In the basic attentional model, the alignment is calculated based on the source encoding HS and the previous hidden state HTi−1 of the target, αi = Attend(HTi−1 , HS ), where Attend is a function that outputs m attention coefficients. This attention mechanism is overly simplistic, in that it is incapable of capturing patterns in the attention over different positions i. Recognising and exploiting these kinds of patterns has proven critical in traditional word based models of translation (Brown et al., 1993; Vogel et al., 1996; Dyer et al., 2013). For this reason Cohn et al. (2016) include explicit features encoding structural biases from word based models, namely absolute and relative position, Markov conditioning and fertility: 1. previous alignment, αi−1 P 2. sum of previous alignments, i−1 j=1 α j 3. source index vector, (1, 2, 3, . . . , m); and 4. target index vector (i, i, i, . . . , i). These features are concatenated to form a feature matrix β ∈ R4×m , which are added to the alignment calculation, i.e., αi = Attend(HTi−1 , HS , β) . Coverage penalty. The sum over previous alignments feature, described abov"
N18-1008,E17-2076,0,0.233342,"ed by using the output of an ASR system as input to a MT system. For example, Ney (1999) and Matusov et al. (2005) use ASR output lattices as input to translation models, integrating speech recognition uncertainty into the translation model. Recent work has focused more on modelling speech translation without explicit access to transcriptions. Duong et al. (2016) introduced a sequence-to-sequence model for speech translation without transcriptions but only evaluated on alignment, while Anastasopoulos et al. (2016) presented an unsupervised alignment method for speech-to-translation alignment. Bansal et al. (2017) used an unsupervised term discovery system (Jansen et al., 2010) to cluster recurring audio segments into pseudowords Our work is most similar to the work of Weiss et al. (2017). They used sequence-to-sequence models to transcribe Spanish speech and translate it in English, by jointly training the two tasks in a multitask scenario where the decoders share the encoder. In contrast to our work, they use a large corpus for training the model on roughly 163 hours of data, using the Spanish Fisher and CALL89 HOME conversational speech corpora. The parameter number of their model is significantly l"
N18-1008,C14-1096,0,0.0262269,"should provide useful information. Second, we apply regularization that encourages transitivity and invertibility. We show that the application of these notions on jointly trained models improves performance on the tasks of low-resource speech transcription and translation. It also leads to better performance when using attention information for word discovery over unsegmented input. 1 Introduction Recent efforts in endangered language documentation focus on collecting spoken language resources, accompanied by spoken translations in a high resource language to make the resource interpretable (Bird et al., 2014a). For example, the BULB project (Adda et al., 2016) used the LIGAikuma mobile app (Bird et al., 2014b; Blachon et al., 2016) to collect parallel speech corpora between three Bantu languages and French. Since it’s common for speakers of endangered languages to speak one or more additional languages, collection of such a resource is a realistic goal. Speech can be interpreted either by transcription in the original language or translation to another language. Since the size of the data is extremely small, multitask models that jointly train a model for both tasks can take advantage of both sig"
N18-1008,W14-2201,0,0.0129147,"should provide useful information. Second, we apply regularization that encourages transitivity and invertibility. We show that the application of these notions on jointly trained models improves performance on the tasks of low-resource speech transcription and translation. It also leads to better performance when using attention information for word discovery over unsegmented input. 1 Introduction Recent efforts in endangered language documentation focus on collecting spoken language resources, accompanied by spoken translations in a high resource language to make the resource interpretable (Bird et al., 2014a). For example, the BULB project (Adda et al., 2016) used the LIGAikuma mobile app (Bird et al., 2014b; Blachon et al., 2016) to collect parallel speech corpora between three Bantu languages and French. Since it’s common for speakers of endangered languages to speak one or more additional languages, collection of such a resource is a realistic goal. Speech can be interpreted either by transcription in the original language or translation to another language. Since the size of the data is extremely small, multitask models that jointly train a model for both tasks can take advantage of both sig"
N18-1008,P15-1166,0,0.359847,". Note that the context vectors resulting from the two attentions are concatenated, not added. n 3 Finally, the decoder computes a sequence of output states from which a probability distribution over output words can be computed. For compactness, we will write X for the matrix whose rows are the xn , and similarly H, C, and so on. We also write A for the matrix of attention weights: [A]i j = αi j . Let θ be the parameters of our model, which we train on sentence triples (X, Y1 , Y2 ). sm = dec(sm−1 , cm , ym−1 ) P(ym ) = softmax(sm ). In a standard encoder-decoder multitask model (Figure 1b) (Dong et al., 2015; Weiss et al., 2017), we jointly model two output sequences using a shared encoder, but separate attentions and decoders: X α1mn hn c1m = 3.1 score(Y1 , Y2 |X; θ) = λ log P(Y1 |X; θ) + = dec1 (s1m−1 , c1m , y1m−1 ) (1 − λ) log P(Y2 |X, S1 ; θ) = softmax(s1m ) where λ is a parameter that controls the importance of each sub-task. In all our experiments, we set λ to 0.5. We then train the model to maximize X L(θ) = score(Y1 , Y2 |X; θ), and c2m = s2m P(y2m ) X n α2mn hn = dec2 (s2m−1 , c2m , y2m−1 ) = softmax(s2m ). where the summation is over all sentence triples in the training data. We can al"
N18-1008,N16-1109,1,0.929318,"nslations in Japanese and English.3 Since there does not exist a standard train-dev-test split, we employ a cross validation scheme for evaluation purposes. In each fold, one of the 10 narratives becomes the test set, with the previous one (mod 10) becoming the dev set, and the remaining 8 narratives becoming the training set. The models for each of the 10 folds are trained and tested separately. On average, for each fold, we train on about 2000 utterances; the dev and test sets consist of about 270 utterances. 3 4.2 Implementation We employ a 3-layer speech encoding scheme similar to that of Duong et al. (2016). The first bidirectional layer receives the audio sequence in the form of 39-dimensional Perceptual Linear Predictive (PLP) features (Hermansky, 1990) computed over overlapping 25ms-wide windows every 10ms. The second and third layers consist of LSTMs with hidden state sizes of 128 and 512 respectively. Each layer encodes every second output of the previous layer. Thus, the sequence is downsampled by a factor of 4, decreasing the computation load for the attention mechanism and the decoders. In the speech experiments, the decoders 4 The data preprocessing scripts are released with the rest of"
N18-1008,P17-2012,0,0.0236087,"ds the reconstruction models in achiev slightly higher BLEU scores in 3 out of the 6 cases. The transitivity regularizer is even more effective: in 9 out the 12 source-target language combinations, the triangle models achieve higher performance when trained using the regularizer. Some of them are statistical significant improvements, as in the case of French to English where English is the intermediate target language and German is the final target. 7 Multitask learning (Caruana, 1998) has found extensive use across several machine learning and NLP fields. For example, Luong et al. (2016) and Eriguchi et al. (2017) jointly learn to parse and translate; Kim et al. (2017) combine CTC- and attention-based models using multitask models for speech transcription; Dong et al. (2015) use multitask learning for multiple language translation. Toshniwal et al. (2017) apply multitask learning to neural speech recognition in a less traditional fashion: the lower-level outputs of the speech encoder are used for fine-grained auxiliary tasks such as predicting HMM states or phonemes, while the final output of the encoder is passed to a characterlevel decoder. Related Work The speech translation problem has been traditi"
N18-1008,P06-2112,0,0.0174747,"r: X 1 c2m = α12 mm0 sm0 s2m P(y2m ) Maximum likelihood estimation Define the score of a sentence triple to be a loglinear interpolation of the two decoders’ probabilities: n s1m P(y1m ) Learning and Inference 3.2 Regularization We can optionally add a regularization term to the objective function, in order to encourage our attention mechanisms to conform to two intuitive principles of machine translation: transitivity and invertibility. m0 = dec2 (s2m−1 , c2m , y2m−1 ) = softmax(s2m ). Transitivity attention regularizer To a first approximation, the translation relation should be transitive (Wang et al., 2006; Levinboim and Chiang, 2015): If source word xi aligns to target word 1 For simplicity, we have assumed only a single layer for both the encoder and decoder. It is possible to use multiple stacked RNNs; typically, the output of the encoder and decoder (cm and P(ym ), respectively) would be computed from the top layer only. 83 P(y21 · · · y2M2 ) P(y21 · · · y2M2 ) softmax s21 · · · s2M2 P(y1 · · · y M ) softmax s1 · · · s M decoder c1 · · · c M attention h1 · · · hN encoder x1 · · · xN (a) single-task P(y11 · · · y1M1 ) softmax s11 · · · s1M1 P(y21 · · · y2M2 ) decoder c11 · · · c1M1 attention"
N18-1008,N15-1129,1,0.853177,"0 sm0 s2m P(y2m ) Maximum likelihood estimation Define the score of a sentence triple to be a loglinear interpolation of the two decoders’ probabilities: n s1m P(y1m ) Learning and Inference 3.2 Regularization We can optionally add a regularization term to the objective function, in order to encourage our attention mechanisms to conform to two intuitive principles of machine translation: transitivity and invertibility. m0 = dec2 (s2m−1 , c2m , y2m−1 ) = softmax(s2m ). Transitivity attention regularizer To a first approximation, the translation relation should be transitive (Wang et al., 2006; Levinboim and Chiang, 2015): If source word xi aligns to target word 1 For simplicity, we have assumed only a single layer for both the encoder and decoder. It is possible to use multiple stacked RNNs; typically, the output of the encoder and decoder (cm and P(ym ), respectively) would be computed from the top layer only. 83 P(y21 · · · y2M2 ) P(y21 · · · y2M2 ) softmax s21 · · · s2M2 P(y1 · · · y M ) softmax s1 · · · s M decoder c1 · · · c M attention h1 · · · hN encoder x1 · · · xN (a) single-task P(y11 · · · y1M1 ) softmax s11 · · · s1M1 P(y21 · · · y2M2 ) decoder c11 · · · c1M1 attention softmax s21 · · · s2M2 decod"
N18-1008,N15-1063,1,0.839641,"s sake there are dependencies not shown. y1j and y1j aligns to target word y2k , then xi should also probably align to y2k . To encourage the model to preserve this relationship, we add the following transitivity regularizer to the loss function of the triangle models with a small weight λtrans = 0.2: 2 Ltrans = score(Y1 , Y2 ) − λtrans A12 A1 − A2 2 . Corpus Ainu-English Mboshi-French Spanish-English Segments Hours 1 3 240 2,668 5,131 17,394 2.5 4.4 20 Table 1: Statistics on our speech datasets. Invertibility attention regularizer The translation relation also ought to be roughly invertible (Levinboim et al., 2015): if, in the reconstruction version of the cascade model, source word xi aligns to target word y1j , then it stands to reason that y j is likely to align to xi . So, whereas Tu et al. (2017) let the attentions of the translator and the reconstructor be unrelated, we try adding the following invertibility regularizer to encourage the attentions to each be the inverse of the other, again with a weight λinv = 0.2: 2 Linv = score(Y1 , Y2 ) − λinv A1 A12 − I 2 . 3.3 Speakers through beam search a set of candidate transˆ 2 , each with a score P(Y ˆ 2 ). lations Y 3. We then output the combination th"
N18-1008,I17-2050,1,0.804707,"a set of candidate transˆ 2 , each with a score P(Y ˆ 2 ). lations Y 3. We then output the combination that yields the highest total score(Y1 , Y2 ). 3.4 Implementation All our models are implemented in DyNet (Neubig et al., 2017).2 We use a dropout of 0.2, and train using Adam with initial learning rate of 0.0002 for a maximum of 500 epochs. For testing, we select the model with the best performance on dev. At inference time, we use a beam size of 4 for each decoder (due to GPU memory constraints), and the beam scores include length normalization (Wu et al., 2016) with a weight of 0.8, which Nguyen and Chiang (2017) found to work well for lowresource NMT. Decoding Since we have two decoders, we now need to employ a two-phase beam search, following Tu et al. (2017): 1. The first decoder produces, through standard beam search, a set of triples each consistˆ 1 , a score ing of a candidate transcription Y ˆ 1 ), and a hidden state sequence S. ˆ P(Y 4 Speech Transcription and Translation We focus on speech transcription and translation of endangered languages, using three different cor2. For each transcription candidate from the first decoder, the second decoder now produces 2 Our code is available at: https:"
N18-1008,2013.iwslt-papers.14,0,0.268253,"zaville, without standard orthography. We use a corpus (Godard et al., 2017) of 5517 parallel utterances (about 4.4 hours of audio) collected from three native speakers. The corpus provides non-standard grapheme transcriptions (close to the language phonology) produced by linguists, as well as French translations. We sampled 100 segments from the training set to be our dev set, and used the original dev set (514 sentences) as our test set. Data Spanish is, of course, not an endangered language, but the availability of the CALLHOME Spanish Speech dataset (LDC2014T23) with English translations (Post et al., 2013) makes it a convenient language to work with, as has been done in almost all previous work in this area. It consists of telephone conversations between relatives (about 20 total hours of audio) with more than 240 speakers. We use the original train-dev-test split, with the training set comprised of 80 conversations and dev and test of 20 conversations each. Hokkaido Ainu is the sole surviving member of the Ainu language family and is generally considered a language isolate. As of 2007, only ten native speakers were alive. The Glossed Audio Corpus of Ainu Folklore provides 10 narratives with au"
N18-1008,P16-1162,0,0.125632,"gle-task base reverse 7.99 11.31 7.57 11.82 7.78 11.56 7.59 9.29 16.41 14.75 10.38 11.40 8.93 7.42 9.78 10.00 9.33 8.52 8.66 10.46 15.48 16.36 11.02 12.76 reconstruction + 0.2Linv reconstruction + 0.5Linv Table 3: The reconstruction model with the invertibility regularizer produces more informed attentions that result in better word discovery for Mboshi with an Mboshi-French model. Scores reported by previous work are in italics and best scores from our experiments are in bold. (en), French (fr) and German (de). All the sequences are represented by subword units with byte-pair encoding (BPE) (Sennrich et al., 2016) trained on each language with 32000 operations. reproduce the significant gains that were reported when using the reverse model (italicized in Table 3). Also, our version of both the base and reverse singletask models performed better than our reimplementation of the baseline. Furthermore, we found that we were able to obtain even better performance at the type level by combining the attention matrices of a reconstruction model trained with the invertibility regularizer. Boito et al. (2017) reported that combining the attention matrices of a base and a reverse model significantly reduced perf"
N18-1008,D16-1133,1,\N,Missing
N19-1311,Q17-1010,0,0.0238815,"Missing"
N19-1311,P18-1008,0,0.0322033,"errors or typos in English. Don’t try to translate the sentences word for word (e.g. replicate the error in Spanish). Instead, try to translate it as if it was a grammatical sentence, and produce a fluent grammatical Spanish sentence that captures its meaning. 3 Our LSTM models are implemented using DyNet (Neubig et al., 2017), and our transformer models using PyTorch (Paszke et al., 2017). The transformer model uses 6 layers, 8 attention heads, the dimension for embeddings and positional feedforward are 512 and 2048 respectively . The sublayer computation sequence follows the guidelines from Chen et al. (2018). Dropout probability is set to 0.2 (also in the source embeddings, following Sperber et al. (2017)). We use the learning rate schedule in Vaswani et al. (2017) with warm-up steps of 24000 but only decay the learning rate until it reaches 10−5 as inspired by Chen et al. (2018). For testing, we select the model with the best performance on the dev set corresponding to the test set. At inference time, we use a beam size of 4 with length normalization (Wu et al., 2016) with a weight of 0.6. Experiments 3.2 In this section, we provide implementation details and the results of our NMT experiments."
N19-1311,W13-1703,0,0.0884174,"est2013 as dev and test sets, respectively. Furthermore, to test our translation methods on real grammatical errors, we introduce a new collection of Spanish translations of the JFLEG corpus (§2.3). 2.1 Grammar Error Correction Corpora To our knowledge, there are five publicly available corpora of non-native English that are annotated with corrections, which have been widely used for research in Grammar Error Correction (GEC). The NUS Corpus of Learner English (NUCLE) contains essays written by students at the National University of Singapore, corrected by two annotators using 27 error codes (Dahlmeier et al., 2013). It has become the main benchmark for GEC, as it was used in the CoNLL GEC Shared Tasks (Ng et al., 2013, 2014). Other corpora include the Cambridge Learner Corpus First Certificate in English FCE corpus (Yannakoudakis et al., 2011), which is only partially public, the Lang-8 corpus (Tajiri et al., 2012), which was harvested from online corrections, and the AESW 2016 Shared Task corpus, which contains corrections on texts from scientific journals. The last corpus is the JHU FLuency-Extended GUG corpus (JFLEG) (Napoles et al., 2017). This corpus covers a wider range of English proficiency leve"
N19-1311,W18-1807,0,0.416553,"erformance of NMT. Unlike our 3075 Correct article a a an the ∅ all – 0 −4.1 −3.1 −3.8 Substituted article an the ∅ all −2.0 −5.7 – −1.5 −1.5 −2.1 −6.3 −1.8 −1.7 −1.7 0 – −2.2 −3.7 −3.4 −2.1 −7.3 −1.7 – −1.8 Substitution Table 6: Effect of article substitutions in test data (art) relative to clean test data (clean), broken down by substitution type. Different article substitutions have very different impacts on BLEU; changing an indefinite article to definite is especially damaging. work, they primarily focus on a setting where the training set is noisy but the test set is clean. In addition, Heigold et al. (2018) evaluated the robustness of word embeddings against word scrambling noise, and showed that performance in downstream tasks like POS-tagging and MT is especially hurt. Sakaguchi et al. (2017a) studied word scrambling and the Cmabrigde Uinervtisy (Cambridge University) effect, where humans are able to understand the meaning of sentences with scrambled words, performing word recognition (word level spelling correction) with a semi-character RNN system. Focusing only on character-level NMT models, Belinkov and Bisk (2018) showed that they exhibit degraded performance when presented with noisy tes"
N19-1311,N18-1055,0,0.034189,"e-side error types on the behavior of our NMT system, when trained on clean data and tested on the artificial noisy data that we created. Art Errors Table 6 shows the difference of the BLEU scores obtained on the sentences, broken down by the type of article error that was introduced. The first observation is that in all cases the difference is negative, meaning that we get higher BLEU scores when testing on clean data. Encouragingly, there is practically no difference when we substitute ‘a’ with ‘an’ or ‘an’ with ‘a’; the model 3 This model has been recently surpassed by other systems, e.g. (Junczys-Dowmunt et al., 2018), but their outputs are not available online. seems to have learned very similar representations for the two indefinite articles, and as a result such an error has no impact on the produced output. However, we observe larger performance drops when substituting indefinite articles with the definite one and vice versa; since the target language makes the same article distinction as the source language, any article source error is propagated to the produced translation. Prep Errors Due to the large number of prepositions, we cannot present a full analysis of preposition errors, but highlights are"
N19-1311,W18-2709,0,0.0272567,"t preposition or inserting a wrong one leads to performance drops of 1.2 and 0.8 BLEU points for the clean model, but drops of 0.4 and 0.7 for the mix-all model. Nn and Sva Errors We found no significant performance difference between the different nn errors. Incorrectly pluralizing a noun has the same adverse effect as singularizing it, leading to performance reductions of over 4.0 and 3.5 BLEU points respectively. We observe a similar behavior with sva errors: each error type leads to roughly the same performance degradation. 5 Related Work The effect of noise in NMT was recently studied by Khayrallah and Koehn (2018), who explored noisy situations during training due to webcrawled data. This type of noise includes misaligned, mistranslated, or untranslated sentences which, when used during training, significantly degrades the performance of NMT. Unlike our 3075 Correct article a a an the ∅ all – 0 −4.1 −3.1 −3.8 Substituted article an the ∅ all −2.0 −5.7 – −1.5 −1.5 −2.1 −6.3 −1.8 −1.7 −1.7 0 – −2.2 −3.7 −3.4 −2.1 −7.3 −1.7 – −1.8 Substitution Table 6: Effect of article substitutions in test data (art) relative to clean test data (clean), broken down by substitution type. Different article substitutions h"
N19-1311,2005.mtsummit-papers.11,0,0.0798649,"om/ antonis/nmt-grammar-noise 3070 Proceedings of NAACL-HLT 2019, pages 3070–3080 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics source of natural noise. Moreover, since English is probably the most commonly spoken non-native language (Lewis et al., 2009), our work could be directly applicable to several translation applications. Our choice of Spanish as a target language enables us to have access to existing parallel data and easily create new parallel corpora (see below, §2.3). For all experiments, we use the Europarl English-Spanish dataset (Koehn, 2005) as our training set. In the synthetic experiments of Section §2.2, we use the newstest2012 and newstest2013 as dev and test sets, respectively. Furthermore, to test our translation methods on real grammatical errors, we introduce a new collection of Spanish translations of the JFLEG corpus (§2.3). 2.1 Grammar Error Correction Corpora To our knowledge, there are five publicly available corpora of non-native English that are annotated with corrections, which have been widely used for research in Grammar Error Correction (GEC). The NUS Corpus of Learner English (NUCLE) contains essays written by"
N19-1311,W17-3204,0,0.0418515,"rrors. 1 Introduction Neural Machine Translation (NMT) is undeniably a success story: public benchmarks (Bojar et al., 2016) are dominated by neural systems, and neural approaches are the de facto option for industrial systems (Wu et al., 2016; Hassan Awadalla et al., 2018; Crego et al., 2016; Hieber et al., 2018). Even under low-resource conditions, neural models were recently shown to outperform traditional statistical approaches (Nguyen and Chiang, 2018). However, there are still several shortcomings of NMT that need to be addressed: a (nonexhaustive) list of six challenges is discussed by Koehn and Knowles (2017), including outof-domain testing, rare word handling, the widebeam problem, and the large amount of data needed for learning. An additional challenge is robustness to noise, both during training and at inference time. In this paper, we study the effect of a specific type of noise in NMT: grammatical errors. We primarily focus on errors that are made by non-native † Equal contribution. Work performed at the University of Notre Dame. source-language speakers (as opposed to dialectal language, SMS or Twitter language). Not only is this linguistically important, but we believe that it would potent"
N19-1311,E17-2004,0,0.0923684,"al. (2017) proposed a noiseintroduction system reminiscent of WER, based on insertions, deletions, and substitutions. An NMT system tested on correct transcriptions achieves a BLEU score of 55 (4 references), but tested on the ASR transcriptions it only achieves a BLEU score of 35.7. By introducing similar noise in the training data, they were able to make the NMT system slightly more robust. Interestingly, they found that the optimal amount of noise on the training data is smaller than the amount of noise on the test data. The notion of linguistically plausible corruption is also explored by Li et al. (2017), who created adversarial examples with syntactic and semantic noise (reordering and word substitutions respecmodel BLEU difference clean mix-all in→with on→for to→on in→ ∅ ∅→for −6.7 −6.0 −2.9 −1.8 −1.6 −1.7 −0.1 −0.5 −1.9 −0.6 ∅→any any→∅ −1.2 −0.8 −0.4 −0.7 Table 7: Effect of selected preposition substitutions in test data (prep) relative to clean test data (clean), for the clean and mix-all models. The mix-all model handles most errors more efficiently. tively). When training with these noisy datasets, they obtained better performance on several text classification tasks. Furthermore, in a"
N19-1311,E17-2037,0,0.0575645,"ingapore, corrected by two annotators using 27 error codes (Dahlmeier et al., 2013). It has become the main benchmark for GEC, as it was used in the CoNLL GEC Shared Tasks (Ng et al., 2013, 2014). Other corpora include the Cambridge Learner Corpus First Certificate in English FCE corpus (Yannakoudakis et al., 2011), which is only partially public, the Lang-8 corpus (Tajiri et al., 2012), which was harvested from online corrections, and the AESW 2016 Shared Task corpus, which contains corrections on texts from scientific journals. The last corpus is the JHU FLuency-Extended GUG corpus (JFLEG) (Napoles et al., 2017). This corpus covers a wider range of English proficiency levels on the source side, and its correction annotations include extended fluency edits rather than just minimal grammatical ones. That way, the corrected sentence is not just grammatical, but also guaranteed to be fluent. 2.2 Synthetic grammar errors Ideally, we would train a translation model to translate grammatically noisy language by training it on parallel data with grammatically noisy language. Since, to our knowledge, no such data exist in the quantities that would be needed, an alError Type art prep nn sva Confusion Set {a, an"
N19-1311,W14-1701,0,0.0664639,"Missing"
N19-1311,W13-3601,0,0.0382148,"rs, we introduce a new collection of Spanish translations of the JFLEG corpus (§2.3). 2.1 Grammar Error Correction Corpora To our knowledge, there are five publicly available corpora of non-native English that are annotated with corrections, which have been widely used for research in Grammar Error Correction (GEC). The NUS Corpus of Learner English (NUCLE) contains essays written by students at the National University of Singapore, corrected by two annotators using 27 error codes (Dahlmeier et al., 2013). It has become the main benchmark for GEC, as it was used in the CoNLL GEC Shared Tasks (Ng et al., 2013, 2014). Other corpora include the Cambridge Learner Corpus First Certificate in English FCE corpus (Yannakoudakis et al., 2011), which is only partially public, the Lang-8 corpus (Tajiri et al., 2012), which was harvested from online corrections, and the AESW 2016 Shared Task corpus, which contains corrections on texts from scientific journals. The last corpus is the JHU FLuency-Extended GUG corpus (JFLEG) (Napoles et al., 2017). This corpus covers a wider range of English proficiency levels on the source side, and its correction annotations include extended fluency edits rather than just min"
N19-1311,P17-1070,0,0.0335914,"Missing"
N19-1311,N18-1031,1,0.783989,"l errors. We also present a set of Spanish translations of the JFLEG grammar error correction corpus, which allows for testing NMT robustness to real grammatical errors. 1 Introduction Neural Machine Translation (NMT) is undeniably a success story: public benchmarks (Bojar et al., 2016) are dominated by neural systems, and neural approaches are the de facto option for industrial systems (Wu et al., 2016; Hassan Awadalla et al., 2018; Crego et al., 2016; Hieber et al., 2018). Even under low-resource conditions, neural models were recently shown to outperform traditional statistical approaches (Nguyen and Chiang, 2018). However, there are still several shortcomings of NMT that need to be addressed: a (nonexhaustive) list of six challenges is discussed by Koehn and Knowles (2017), including outof-domain testing, rare word handling, the widebeam problem, and the large amount of data needed for learning. An additional challenge is robustness to noise, both during training and at inference time. In this paper, we study the effect of a specific type of noise in NMT: grammatical errors. We primarily focus on errors that are made by non-native † Equal contribution. Work performed at the University of Notre Dame. s"
N19-1311,D16-1161,0,0.0157284,"clean mix-all cor0 Manual correction cor1 cor2 cor3 avg. No corr. Auto corr. 28.4 27.7 28.8 28.1 28.6 27.8 26.2 26.8 27.0 26.7 29.1 28.1 28.2 27.5 Table 5: BLEU scores on the JFLEG-es dev and test datasets. Our proposed mix-all model is slightly behind the clean model on manually corrected input (cor[0–3]). On noisy input (No corr.) the mix-all outperforms the clean model (26.8 > 26.2). Preprocessing the noisy input with a GEC model (Auto corr.) slightly improves results. tem (column Auto corr of Table 5). We used the publicly available JFLEG outputs of the (almost) state-of-the-art model of Junczys-Dowmunt and Grundkiewicz (2016) as inputs to our NMT system.3 This experiment envisions a pipeline where the noisy source is first automatically corrected and then translated. As expected, this helps the clean model (by +1.1 BLEU), but our mixall training helps even further (by another +0.8 BLEU). Interestingly, the automatic GEC system only helps in the test set, while there are no improvements in the dev set. Naturally, since automatic GEC systems are imperfect, the performance of this pipeline still lags behind translating on clean data. 4 Analysis We attempt an in-depth analysis of the impact of the different source-sid"
N19-1311,P02-1040,0,0.106952,"e.g. the mix-all model will refer to the model trained on the mix-all dataset. 3.1 Implementation Details All data are tokenized, truecased, and split into subwords using Byte Pair Encoding (BPE) with 32,000 operations (Sennrich et al., 2016). We filter the training set to only contain sentences up to 80 words. Results We report the results obtained with the transformer model, as they were consistently better than the LSTM one. All the result tables for the LSTM models can be found in the Appendix. The performance of our systems on the synthetic WMT test sets, as measured by detokenized BLEU (Papineni et al., 2002), is summarized in Table 4. When the system is trained only on clean data (first row) and tested on noisy data, it unsurprisingly exhibits degraded performance. We observe significant drops in the range of 1.0–3.6 BLEU. 3073 clean drop En-Es WMT Test Set art prep nn sva clean 33.0 29.6 31.3 32.0 29.3 32.1 31.2 ± 1.5 drop art prep nn sva 31 31.2 30.4 30.4 31.2 30.2 28.4 27.8 27.9 28.7 30.0 30.8 29.3 28.9 30.2 30.0 30.2 30.3 29.5 30.3 28.3 27.7 27.4 29.8 28.2 30.6 30.8 29.9 29.8 30.9 30.0 ± 0.9 29.8 ± 1.4 29.2 ± 1.3 29.4 ± 0.8 29.9 ± 1.2 clean+drop clean+art clean+prep clean+nn clean+sva 32.9 32"
N19-1311,D14-1162,0,0.0810429,"Missing"
N19-1311,P06-1055,0,0.0446059,"of each training sentence, one without errors and one for each error. we obtain the probability of a noun being replaced with its singular or plural form. For sva errors, the probability that a present tense verb is replaced with its third-person-singular (3SG) or not-3SG form. An additional sva error that we included is the confusion between the appropriate form for the verb ‘to be’ in the past tense (‘was’ and ‘were’). The second step involves applying the noiseinducing transformations using our collected statistics as a prior. We obtained parses for each sentence using the Berkeley parser (Petrov et al., 2006). The parse tree allows us to identify candidate error positions in each sentence (for example, the beginning of a noun phrase without a determiner, were one could be inserted). For each error type we introduced exactly one error per sentence, wherever possible, which we believe matches more realistic scenarios than previous work. It also allows for controlled analysis of the behaviour of the NMT system (see Section 4). For each error and each sentence, we first identify candidate positions (based on the error type and the parse tree) and sample one of them based on the specific error distribu"
N19-1311,N18-1057,0,0.0270225,"Missing"
N19-1311,P11-1019,0,0.0713753,"orpora To our knowledge, there are five publicly available corpora of non-native English that are annotated with corrections, which have been widely used for research in Grammar Error Correction (GEC). The NUS Corpus of Learner English (NUCLE) contains essays written by students at the National University of Singapore, corrected by two annotators using 27 error codes (Dahlmeier et al., 2013). It has become the main benchmark for GEC, as it was used in the CoNLL GEC Shared Tasks (Ng et al., 2013, 2014). Other corpora include the Cambridge Learner Corpus First Certificate in English FCE corpus (Yannakoudakis et al., 2011), which is only partially public, the Lang-8 corpus (Tajiri et al., 2012), which was harvested from online corrections, and the AESW 2016 Shared Task corpus, which contains corrections on texts from scientific journals. The last corpus is the JHU FLuency-Extended GUG corpus (JFLEG) (Napoles et al., 2017). This corpus covers a wider range of English proficiency levels on the source side, and its correction annotations include extended fluency edits rather than just minimal grammatical ones. That way, the corrected sentence is not just grammatical, but also guaranteed to be fluent. 2.2 Synthetic"
N19-1311,D10-1094,0,0.224091,"3SG, 2SG-Past, not 2SG-Past} Table 1: Confusion sets for each grammar error type. The art and prep sets include an empty token (∅) allowing for insertions and deletions. SG, PL, 2SG, and 3SG stand for singular, plural, second-person and thirdperson singular respectively. ternative is to add synthetic grammatical noise to clean data. An advantage of this approach is that controlled introduction of errors allows for finegrained analysis. This is a two-step process, similar to the methods used in the GEC literature for creating synthetic data based on confusion matrices (Rozovskaya et al., 2014; Rozovskaya and Roth, 2010; Xie et al., 2016; Sperber et al., 2017). First, we mimic the distribution of errors found in real data, and then introduce errors by applying rulebased transformations on automatic parse trees. The first step involves collecting error statistics on real data. Conveniently, the NUCLE corpus has all corrections annotated with 27 error codes. We focus on five types of errors, with the last four being the most common in the NUCLE corpus: • drop: randomly deleting one character from the sentence.2 • art: article/determiner errors • prep: preposition errors • nn: noun number errors • sva: subject-"
N19-1311,I17-2062,0,0.0405053,"Missing"
N19-1311,E17-3017,0,0.0360748,"Missing"
N19-1311,P16-1162,0,0.117385,"he best performance on the dev set corresponding to the test set. At inference time, we use a beam size of 4 with length normalization (Wu et al., 2016) with a weight of 0.6. Experiments 3.2 In this section, we provide implementation details and the results of our NMT experiments. For convenience, we will refer to each model with the same name as the dataset it was trained on; e.g. the mix-all model will refer to the model trained on the mix-all dataset. 3.1 Implementation Details All data are tokenized, truecased, and split into subwords using Byte Pair Encoding (BPE) with 32,000 operations (Sennrich et al., 2016). We filter the training set to only contain sentences up to 80 words. Results We report the results obtained with the transformer model, as they were consistently better than the LSTM one. All the result tables for the LSTM models can be found in the Appendix. The performance of our systems on the synthetic WMT test sets, as measured by detokenized BLEU (Papineni et al., 2002), is summarized in Table 4. When the system is trained only on clean data (first row) and tested on noisy data, it unsurprisingly exhibits degraded performance. We observe significant drops in the range of 1.0–3.6 BLEU."
N19-1311,P12-2039,0,0.0886316,"English that are annotated with corrections, which have been widely used for research in Grammar Error Correction (GEC). The NUS Corpus of Learner English (NUCLE) contains essays written by students at the National University of Singapore, corrected by two annotators using 27 error codes (Dahlmeier et al., 2013). It has become the main benchmark for GEC, as it was used in the CoNLL GEC Shared Tasks (Ng et al., 2013, 2014). Other corpora include the Cambridge Learner Corpus First Certificate in English FCE corpus (Yannakoudakis et al., 2011), which is only partially public, the Lang-8 corpus (Tajiri et al., 2012), which was harvested from online corrections, and the AESW 2016 Shared Task corpus, which contains corrections on texts from scientific journals. The last corpus is the JHU FLuency-Extended GUG corpus (JFLEG) (Napoles et al., 2017). This corpus covers a wider range of English proficiency levels on the source side, and its correction annotations include extended fluency edits rather than just minimal grammatical ones. That way, the corrected sentence is not just grammatical, but also guaranteed to be fluent. 2.2 Synthetic grammar errors Ideally, we would train a translation model to translate"
P14-1067,W13-2241,0,0.0508695,"used to predict labels1 for new, unseen test items (Specia et al., 2010). In the last couple of years, research in the field received a strong boost by the shared tasks organized within the WMT workshop on SMT,2 which is also the framework of our first experiment in §5. Current approaches to the tasks proposed at WMT have mainly focused on three main directions, namely: i) feature engineering, as in (Hardmeier et al., 2012; de Souza et al., 2013a; de Souza et al., 2013b; Rubino et al., 2013b), ii) model learning with a variety of classification and regression algorithms, as in (Bicici, 2013; Beck et al., 2013; Soricut et al., 2012), and iii) feature selection as a way to overcome sparsity and overfitting issues, as in (Soricut et al., 2012). Being optimized to perform well on specific WMT sub-tasks and datasets, current systems reflect variations along these directions but leave important aspects of the QE problem still partially investigated or totally unexplored.3 Among these, the necessity to model the diversity of human quality judgements and correction strategies (Koponen, 2012; Koponen et al., 2012) calls for solutions that: i) account for annotator-specific behaviour, thus being capable of"
P14-1067,2013.mtsummit-papers.5,0,0.0102042,"ranslated segments and evolve over time incorporating users style and terminology, all components of a CAT tool (the MT engine and the mechanisms to assign quality scores to the suggested translations) should take advantage of translators feedback. On the MT system side, research on adaptive approaches tailored to interactive SMT and CAT scenarios explored the online learning protocol (Littlestone, 1988) to improve various aspects of the decoding process (Cesa-Bianchi et al., 2008; Ortiz-Mart´ınez et al., 2010; Mart´ınez-G´omez et al., 2011; Mart´ınez-G´omez et al., 2012; Mathur et al., 2013; Bertoldi et al., 2013). As regards QE models, our work represents the first investigation on incremental adaptation by exploiting users feedback to provide targeted (system, user, or project specific) quality judgements. 3 Each post-edition brings a wealth of dynamic knowledge about the whole translation process and the involved actors. For instance, adaptive QE components could exploit information about the distance between automatically assigned scores and the quality standards of individual translators (inferred from the amount of their corrections) to “profile” their behaviour. The online learning paradigm fits"
P14-1067,W12-3112,0,0.0061207,"e model to update its predictions for future instances. QE is generally cast as a supervised machine learning task, where a model trained from a collection of (source, target, label) instances is used to predict labels1 for new, unseen test items (Specia et al., 2010). In the last couple of years, research in the field received a strong boost by the shared tasks organized within the WMT workshop on SMT,2 which is also the framework of our first experiment in §5. Current approaches to the tasks proposed at WMT have mainly focused on three main directions, namely: i) feature engineering, as in (Hardmeier et al., 2012; de Souza et al., 2013a; de Souza et al., 2013b; Rubino et al., 2013b), ii) model learning with a variety of classification and regression algorithms, as in (Bicici, 2013; Beck et al., 2013; Soricut et al., 2012), and iii) feature selection as a way to overcome sparsity and overfitting issues, as in (Soricut et al., 2012). Being optimized to perform well on specific WMT sub-tasks and datasets, current systems reflect variations along these directions but leave important aspects of the QE problem still partially investigated or totally unexplored.3 Among these, the necessity to model the diver"
P14-1067,C12-1070,0,0.0195713,"to one of three sets (support, empty, error) maintaining the consistency of a set of conditions known as KarushKuhn Tucker (KKT) conditions. For each new point, OSVR starts a cycle where the samples are moved across the three sets until the KKT conditions are verified and the new point is assigned to one of the sets. If the point is identified as a support vector, the parameters of the model are updated. This allows OSVR to benefit from the prediction capability of -SVR in an online setting. From a practical point of view, providing the best trade off between accuracy and computational time (He and Wang, 2012), PA represents a good solution to meet the demand of efficiency posed by the CAT framework. For each instance i, after emitting a prediction and receiving the true label, PA computes the -insensitive hinge loss function. If its value is larger than the tolerance parameter (), the weights of the model are updated as much as the aggressiveness parameter C allows. In contrast with OSVR, which keeps track of the most important points seen in the past (support vectors), the update of the weights is done without considering the previously processed i-1 instances. Although it makes PA faster than"
P14-1067,W13-2206,0,0.00575961,") instances is used to predict labels1 for new, unseen test items (Specia et al., 2010). In the last couple of years, research in the field received a strong boost by the shared tasks organized within the WMT workshop on SMT,2 which is also the framework of our first experiment in §5. Current approaches to the tasks proposed at WMT have mainly focused on three main directions, namely: i) feature engineering, as in (Hardmeier et al., 2012; de Souza et al., 2013a; de Souza et al., 2013b; Rubino et al., 2013b), ii) model learning with a variety of classification and regression algorithms, as in (Bicici, 2013; Beck et al., 2013; Soricut et al., 2012), and iii) feature selection as a way to overcome sparsity and overfitting issues, as in (Soricut et al., 2012). Being optimized to perform well on specific WMT sub-tasks and datasets, current systems reflect variations along these directions but leave important aspects of the QE problem still partially investigated or totally unexplored.3 Among these, the necessity to model the diversity of human quality judgements and correction strategies (Koponen, 2012; Koponen et al., 2012) calls for solutions that: i) account for annotator-specific behaviour, thu"
P14-1067,2012.amta-wptp.2,0,0.00377347,") model learning with a variety of classification and regression algorithms, as in (Bicici, 2013; Beck et al., 2013; Soricut et al., 2012), and iii) feature selection as a way to overcome sparsity and overfitting issues, as in (Soricut et al., 2012). Being optimized to perform well on specific WMT sub-tasks and datasets, current systems reflect variations along these directions but leave important aspects of the QE problem still partially investigated or totally unexplored.3 Among these, the necessity to model the diversity of human quality judgements and correction strategies (Koponen, 2012; Koponen et al., 2012) calls for solutions that: i) account for annotator-specific behaviour, thus being capable of learning from inherently noisy datasets produced by multiple annotators, and ii) self-adapt to changes in data distribution, learning from user feedback on new, unseen test items. Focusing on the adaptability to user and domain changes, we report the results of comparative experiments with two online algorithms and the standard batch approach. The evaluation is carried out by measuring the global error of each algorithm on test sets featuring different degrees of similarity with the data used for trai"
P14-1067,W12-3102,0,0.0375667,"Missing"
P14-1067,W12-3123,0,0.035032,"imation (QE) systems, additional complexity comes from the difficulty to model user and domain changes. Indeed, the instability of the systems with respect to data coming from different distributions calls for adaptive solutions that react to new operating conditions. To tackle this issue we propose an online framework for adaptive QE that targets reactivity and robustness to user and domain changes. Contrastive experiments in different testing conditions involving user and domain changes demonstrate the effectiveness of our approach. 1 1. The notion of MT output quality is highly subjective (Koponen, 2012; Turchi et al., 2013; Turchi and Negri, 2014). Since the quality standards of individual users may vary considerably (e.g. according to their knowledge of the source and target languages), the estimates of a static QE model trained with data collected from a group of post-editors might not fit with the actual judgements of a new user; Introduction After two decades of steady progress, research in statistical machine translation (SMT) started to cross its path with translation industry with tangible mutual benefit. On one side, SMT research brings to the industry improved output quality and a"
P14-1067,P13-1004,0,0.0213563,"ction from a source–target pair (i.e. one instance at a time instead of processing an entire training set); 2. Emit a prediction for the input instance; 3. Gather user feedback for the instance (i.e. calculating a “true label” based on the amount of user post-editions); 711 These interconnected issues are particularly relevant in the CAT framework, where translation jobs from different domains are routed to professional translators with different idiolect, background and quality standards. The first aspect, modelling annotators’ individual behaviour and interdependences, has been addressed by Cohn and Specia (2013), who explored multi-task Gaussian Processes as a way to jointly learn from the output of multiple annotations. This technique is suitable to cope with the unbalanced distribution of training instances and yields better models when heterogeneous training datasets are available. The second problem, the adaptability of QE models, has not been explored yet. A common trait of all current approaches, in fact, is the reliance on batch learning techniques, which assume a “static” nature of the world where new unseen instances that will be encountered will be similar to the training data.4 However, si"
P14-1067,W13-2243,1,0.645696,"Missing"
P14-1067,W13-2237,0,0.0127973,"incrementally store translated segments and evolve over time incorporating users style and terminology, all components of a CAT tool (the MT engine and the mechanisms to assign quality scores to the suggested translations) should take advantage of translators feedback. On the MT system side, research on adaptive approaches tailored to interactive SMT and CAT scenarios explored the online learning protocol (Littlestone, 1988) to improve various aspects of the decoding process (Cesa-Bianchi et al., 2008; Ortiz-Mart´ınez et al., 2010; Mart´ınez-G´omez et al., 2011; Mart´ınez-G´omez et al., 2012; Mathur et al., 2013; Bertoldi et al., 2013). As regards QE models, our work represents the first investigation on incremental adaptation by exploiting users feedback to provide targeted (system, user, or project specific) quality judgements. 3 Each post-edition brings a wealth of dynamic knowledge about the whole translation process and the involved actors. For instance, adaptive QE components could exploit information about the distance between automatically assigned scores and the quality standards of individual translators (inferred from the amount of their corrections) to “profile” their behaviour. The onlin"
P14-1067,P13-2135,1,0.899634,"Missing"
P14-1067,W12-3122,1,0.413375,"of Athens, Greece {turchi,desouza,negri}@fbk.eu anastasopoulos.ant@gmail.com Abstract The possibility to speed up the translation process and reduce its costs by post-editing goodquality MT output raises interesting research challenges. Among others, these include deciding what to present as a suggestion, and how to do it in the most effective way. In recent years, these issues motivated research on automatic QE, which addresses the problem of estimating the quality of a translated sentence given the source and without access to reference translations (Blatz et al., 2003; Specia et al., 2009; Mehdad et al., 2012). Despite the substantial progress done so far in the field and in successful evaluation campaigns (Callison-Burch et al., 2012; Bojar et al., 2013), focusing on concrete market needs makes possible to further define the scope of research on QE. For instance, moving from controlled lab testing scenarios to real working environments poses additional constraints in terms of adaptability of the QE models to the variable conditions of a translation job. Such variability is due to two main reasons: The automatic estimation of machine translation (MT) output quality is a hard task in which the selec"
P14-1067,turchi-negri-2014-automatic,1,0.822394,"plexity comes from the difficulty to model user and domain changes. Indeed, the instability of the systems with respect to data coming from different distributions calls for adaptive solutions that react to new operating conditions. To tackle this issue we propose an online framework for adaptive QE that targets reactivity and robustness to user and domain changes. Contrastive experiments in different testing conditions involving user and domain changes demonstrate the effectiveness of our approach. 1 1. The notion of MT output quality is highly subjective (Koponen, 2012; Turchi et al., 2013; Turchi and Negri, 2014). Since the quality standards of individual users may vary considerably (e.g. according to their knowledge of the source and target languages), the estimates of a static QE model trained with data collected from a group of post-editors might not fit with the actual judgements of a new user; Introduction After two decades of steady progress, research in statistical machine translation (SMT) started to cross its path with translation industry with tangible mutual benefit. On one side, SMT research brings to the industry improved output quality and a number of appealing solutions useful to increa"
P14-1067,N10-1079,0,0.00997561,"Missing"
P14-1067,W13-2231,1,0.446827,"stems, additional complexity comes from the difficulty to model user and domain changes. Indeed, the instability of the systems with respect to data coming from different distributions calls for adaptive solutions that react to new operating conditions. To tackle this issue we propose an online framework for adaptive QE that targets reactivity and robustness to user and domain changes. Contrastive experiments in different testing conditions involving user and domain changes demonstrate the effectiveness of our approach. 1 1. The notion of MT output quality is highly subjective (Koponen, 2012; Turchi et al., 2013; Turchi and Negri, 2014). Since the quality standards of individual users may vary considerably (e.g. according to their knowledge of the source and target languages), the estimates of a static QE model trained with data collected from a group of post-editors might not fit with the actual judgements of a new user; Introduction After two decades of steady progress, research in statistical machine translation (SMT) started to cross its path with translation industry with tangible mutual benefit. On one side, SMT research brings to the industry improved output quality and a number of appealing s"
P14-1067,C00-2137,0,0.0374506,"2007) on parallel data from the two domains (about 2M sentences for IT and 1.5M for L). Post-editions were collected from eight professional translators (four for each document) operating with the CAT tool in real working conditions. According to the way they are created, the two datasets allow us to evaluate the adaptability of different QE models with respect to user changes 11 Results marked with the “∗ ” symbol are NOT statistically significant compared to the corresponding batch model. The others are always statistically significant at p≤0.005, calculated with approximate randomization (Yeh, 2000). 12 13 715 MateCat – http://www.matecat.com/ http://eur-lex.europa.eu/ user change Train Test rad cons sim1 sim2 cons rad sim2 sim1 Train Test cons rad sim2 sim1 rad cons sim1 sim2 ∆ HTER 20.5 19.4 3.3 3.2 ∆ HTER 12.8 9.6 3.3 1.1 Legal Domain Batch Adaptive MAE MAE Alg. 20.6 14.5 PA 21.3 16.1 PA 12.2 12.6∗ OSVR 13.3 13.9∗ OSVR IT Domain µ Batch Adaptive MAE MAE MAE Alg 19.2 19.8 17.5∗ OSVR 16.8 16.6 15.6 PA 14.7 14.4 15∗ OSVR 15 13.9 14.4∗ OSVR µ MAE 21.4 21.2 14.7 13.4 Empty MAE Alg. 12.5 OSVR 11.3 OSVR 12.9∗ OSVR 15.2∗ OSVR Empty MAE Alg 16.6 OSVR 15.5 OSVR 15.5∗ OSVR 16.1∗ OSVR Table 2: MA"
P14-1067,2013.mtsummit-posters.13,1,0.179685,"Missing"
P14-1067,W13-2227,0,0.0504453,"Missing"
P14-1067,shah-etal-2014-efficient,1,0.715171,"represented by (source, target) pairs; • The prediction p(xi ) is the automatically estimated HTER score; • The true label pˆ(xi ) is the actual HTER score calculated over the target and its post-edition. At each step of the process, the goal of the learner is to exploit user post-editions to reduce the difference between the predicted HTER values and the true labels for the following (source, target) pairs. As depicted in Figure 1, this is done as follows: 1. At step i, an unlabelled (source, target) pair xi is sent to a feature extraction component. To this aim, we used an adapted version (Shah et al., 2014) of the open-source QuEst6 tool (Specia et al., 2013). The tool, which implements a large number of features proposed by participants in the WMT QE shared tasks, has been modified to process one sentence at a time as requested for integration in a CAT environment; Online QE for CAT environments When operating with advanced CAT tools, translators are presented with suggestions (either matching fragments from a translation memory or automatic translations produced by an MT system) for each sentence of a source document. Before being approved and published, translation suggestions may require dif"
P14-1067,2006.amta-papers.25,0,0.104216,"changes in data distribution, learning from user feedback on new, unseen test items. Focusing on the adaptability to user and domain changes, we report the results of comparative experiments with two online algorithms and the standard batch approach. The evaluation is carried out by measuring the global error of each algorithm on test sets featuring different degrees of similarity with the data used for training. Our results 1 Possible label types include post-editing effort scores (e.g. 1-5 Likert scores indicating the estimated percentage of MT output that has to be corrected), HTER values (Snover et al., 2006), and post-editing time (e.g. seconds per word). 2 http://www.statmt.org/wmt13/ 3 For a comprehensive overview of the QE approaches proposed so far we refer the reader to the WMT12 and WMT13 QE shared task reports (Callison-Burch et al., 2012; Bojar et al., 2013). 1. Perform online feature extraction from a source–target pair (i.e. one instance at a time instead of processing an entire training set); 2. Emit a prediction for the input instance; 3. Gather user feedback for the instance (i.e. calculating a “true label” based on the amount of user post-editions); 711 These interconnected issues a"
P14-1067,W12-3118,0,0.0129618,"bels1 for new, unseen test items (Specia et al., 2010). In the last couple of years, research in the field received a strong boost by the shared tasks organized within the WMT workshop on SMT,2 which is also the framework of our first experiment in §5. Current approaches to the tasks proposed at WMT have mainly focused on three main directions, namely: i) feature engineering, as in (Hardmeier et al., 2012; de Souza et al., 2013a; de Souza et al., 2013b; Rubino et al., 2013b), ii) model learning with a variety of classification and regression algorithms, as in (Bicici, 2013; Beck et al., 2013; Soricut et al., 2012), and iii) feature selection as a way to overcome sparsity and overfitting issues, as in (Soricut et al., 2012). Being optimized to perform well on specific WMT sub-tasks and datasets, current systems reflect variations along these directions but leave important aspects of the QE problem still partially investigated or totally unexplored.3 Among these, the necessity to model the diversity of human quality judgements and correction strategies (Koponen, 2012; Koponen et al., 2012) calls for solutions that: i) account for annotator-specific behaviour, thus being capable of learning from inherentl"
P14-1067,2009.eamt-1.5,1,0.289783,"Technical University of Athens, Greece {turchi,desouza,negri}@fbk.eu anastasopoulos.ant@gmail.com Abstract The possibility to speed up the translation process and reduce its costs by post-editing goodquality MT output raises interesting research challenges. Among others, these include deciding what to present as a suggestion, and how to do it in the most effective way. In recent years, these issues motivated research on automatic QE, which addresses the problem of estimating the quality of a translated sentence given the source and without access to reference translations (Blatz et al., 2003; Specia et al., 2009; Mehdad et al., 2012). Despite the substantial progress done so far in the field and in successful evaluation campaigns (Callison-Burch et al., 2012; Bojar et al., 2013), focusing on concrete market needs makes possible to further define the scope of research on QE. For instance, moving from controlled lab testing scenarios to real working environments poses additional constraints in terms of adaptability of the QE models to the variable conditions of a translation job. Such variability is due to two main reasons: The automatic estimation of machine translation (MT) output quality is a hard t"
P14-1067,P13-4014,1,0.049007,"Missing"
P14-1067,P07-2045,0,\N,Missing
P14-1067,C04-1046,0,\N,Missing
P14-1067,W13-2201,0,\N,Missing
P19-1301,Q16-1031,0,0.30865,"question. It has been demonstrated that through cross-lingual transfer, it is possible to leverage one or more similar high-resource languages to improve the performance on the low-resource languages in several NLP tasks, including machine Equal contribution Code, data, and pre-trained models are available at https://github.com/neulab/langrank 1 Ltk: Task Language Transfer Learning Introduction ∗ Ltf,2: Transfer Language 2 ... 1 ... Ltf,1: Transfer Language 1 translation (Zoph et al., 2016; Johnson et al., 2017; Nguyen and Chiang, 2017; Neubig and Hu, 2018), parsing (T¨ackstr¨om et al., 2012; Ammar et al., 2016; Ahmad et al., 2018; Ponti et al., 2018), partof-speech or morphological tagging (T¨ackstr¨om et al., 2013; Cotterell and Heigold, 2017; Malaviya et al., 2018; Plank and Agi´c, 2018), named entity recognition (Zhang et al., 2016; Mayhew et al., 2017; Xie et al., 2018), and entity linking (Tsai and Roth, 2016; Rijhwani et al., 2019). There are many methods for performing this transfer, including joint training (Ammar et al., 2016; Tsai and Roth, 2016; Cotterell and Heigold, 2017; Johnson et al., 2017; Malaviya et al., 2018), annotation projection (T¨ackstr¨om et al., 2012; T¨ackstr¨om et al.,"
P19-1301,D17-1078,0,0.291043,"ource languages to improve the performance on the low-resource languages in several NLP tasks, including machine Equal contribution Code, data, and pre-trained models are available at https://github.com/neulab/langrank 1 Ltk: Task Language Transfer Learning Introduction ∗ Ltf,2: Transfer Language 2 ... 1 ... Ltf,1: Transfer Language 1 translation (Zoph et al., 2016; Johnson et al., 2017; Nguyen and Chiang, 2017; Neubig and Hu, 2018), parsing (T¨ackstr¨om et al., 2012; Ammar et al., 2016; Ahmad et al., 2018; Ponti et al., 2018), partof-speech or morphological tagging (T¨ackstr¨om et al., 2013; Cotterell and Heigold, 2017; Malaviya et al., 2018; Plank and Agi´c, 2018), named entity recognition (Zhang et al., 2016; Mayhew et al., 2017; Xie et al., 2018), and entity linking (Tsai and Roth, 2016; Rijhwani et al., 2019). There are many methods for performing this transfer, including joint training (Ammar et al., 2016; Tsai and Roth, 2016; Cotterell and Heigold, 2017; Johnson et al., 2017; Malaviya et al., 2018), annotation projection (T¨ackstr¨om et al., 2012; T¨ackstr¨om et al., 2013; Zhang et al., 2016; Ponti et al., 2018; Plank and Agi´c, 2018), fine-tuning (Zoph et al., 2016; Neubig and Hu, 2018), data augment"
P19-1301,N18-2085,0,0.030601,"vided a (non-exhaustive) list of examples that employ cross-lingual transfer across several tasks. Other work has performed large-scale studies on the importance of appropriately selecting a transfer language, such as Paul et al. (2009), which performed an extensive search for a “pivot language” in statistical MT, but without attempting to actually learn or predict which pivot language is best. Typologically-informed models are another vein of research that is relevant to our work. The relationship between linguistic typology and statistical modeling has been studied by Gerz et al. (2018) and Cotterell et al. (2018), with a focus on language modeling. Tsvetkov et al. (2016b) used typological information in the target language as additional input to their model for phonetic representation learning. Ammar et al. (2016) and Ahmad et al. (2018) used similar ideas for output: 2 no stf stk yes &gt; 1.61 output: 3 no output: 1 Figure 4: An example of the decision tree learned in the machine translation task for Galician as task language. dependency parsing, incorporating linguisticallyinformed vectors into their models. O’Horan et al. (2016) survey typological resources available and their utility in NLP tasks. Al"
P19-1301,P15-1166,0,0.228228,"018; Rijhwani et al., 2019). The common thread is that data in a high-resource transfer language is used to improve performance on a low-resource task language. However, determining the best transfer language for any particular task language remains an open question – the choice of transfer language has traditionally been done in a heuristic manner, often based on the intuition of the experimenter. A common method of choosing transfer languages involves selecting one that belongs to the same language family or has a small phylogenetic distance in the language family tree to the task language (Dong et al., 2015; Johnson et al., 2017; Cotterell and Heigold, 2017). However, it is not always true that all languages in a single language family share the same linguistic properties (Ahmad et al., 2018). Therefore, another strategy is to select transfer languages based on the typological properties that are relevant to the specific NLP task, such as word ordering for parsing tasks (Ammar et al., 2016; Ahmad et al., 2018). With several heuristics available for selecting a transfer language, it is unclear a priori if any single attribute of a language will be the most reliable criterion in determining whethe"
P19-1301,P81-1022,0,0.264147,"Missing"
P19-1301,D18-1029,0,0.0367306,"Missing"
P19-1301,Q17-1024,0,0.0480571,"Missing"
P19-1301,P18-1007,0,0.0241831,"Missing"
P19-1301,E17-2002,1,0.85781,"lap, and the word overlap is simply the count of the named entities that have exactly the same representations in both transfer and task languages. We also omit subword overlap in the POS and DEP tasks, as some low-resource languages do not have enough data for properly extracting subwords. 3.2 Dataset-independent Features Dataset-independent features are measures of the similarity between a pair of languages based on phylogenetic or typological properties established by linguistic study. Specifically, we leverage six different linguistic distances queried from the URIEL Typological Database (Littell et al., 2017): Geographic distance (dgeo ): The orthodromic distance between the languages on the surface of the earth, divided by the antipodal distance, based primarily on language location descriptions in Glottolog (Hammarstr¨om et al., 2018). Genetic distance (dgen ): The genealogical distance of the languages, derived from the hypothesized tree of language descent in Glottolog. Inventory distance (dinv ): The cosine distance between the phonological feature vectors derived from the PHOIBLE database (Moran et al., 2014), a collection of seven phonological databases. Syntactic distance (dsyn ): The cosi"
P19-1301,P09-5005,0,0.0897376,"Missing"
P19-1301,P16-1101,1,0.690355,"TM encoders, which are trained to maximize the cosine similarity between parallel (i.e., linked) entities (Rijhwani et al., 2019). We use the same dataset as Rijhwani et al. (2019), which contains language-linked Wikipedia article titles from 9 low-resource task languages and 53 potential transfer languages, resulting in 477 task/transfer pairs. We perform training in a zero-shot setting, where we train on corpora only in the transfer language, and test entity linking accuracy on the task language without joint training or fine-tuning. POS Tagging We train a bi-directional LSTMCNNs-CRF model (Ma and Hovy, 2016) on word 3128 sequences without using pre-trained word embeddings. The implementation is based on the NCRF++ toolkit (Yang and Zhang, 2018). We perform training on the Universal Dependencies v2.2 dataset (Nivre et al., 2018), using 26 languages that have the least training data as task languages, and 60 transfer languages,2 resulting in 1,545 pairs of transfer-task languages. Transfer is performed by joint training over the concatenated task and transfer corpora if the task language has training data, and training only with transfer corpora otherwise. The performance is measured by POS tagging"
P19-1301,N09-2056,0,0.0659497,"Missing"
P19-1301,P18-1247,1,0.846408,"e performance on the low-resource languages in several NLP tasks, including machine Equal contribution Code, data, and pre-trained models are available at https://github.com/neulab/langrank 1 Ltk: Task Language Transfer Learning Introduction ∗ Ltf,2: Transfer Language 2 ... 1 ... Ltf,1: Transfer Language 1 translation (Zoph et al., 2016; Johnson et al., 2017; Nguyen and Chiang, 2017; Neubig and Hu, 2018), parsing (T¨ackstr¨om et al., 2012; Ammar et al., 2016; Ahmad et al., 2018; Ponti et al., 2018), partof-speech or morphological tagging (T¨ackstr¨om et al., 2013; Cotterell and Heigold, 2017; Malaviya et al., 2018; Plank and Agi´c, 2018), named entity recognition (Zhang et al., 2016; Mayhew et al., 2017; Xie et al., 2018), and entity linking (Tsai and Roth, 2016; Rijhwani et al., 2019). There are many methods for performing this transfer, including joint training (Ammar et al., 2016; Tsai and Roth, 2016; Cotterell and Heigold, 2017; Johnson et al., 2017; Malaviya et al., 2018), annotation projection (T¨ackstr¨om et al., 2012; T¨ackstr¨om et al., 2013; Zhang et al., 2016; Ponti et al., 2018; Plank and Agi´c, 2018), fine-tuning (Zoph et al., 2016; Neubig and Hu, 2018), data augmentation (Mayhew et al., 2"
P19-1301,D17-1269,0,0.0598619,"Missing"
P19-1301,P10-2041,0,0.0447077,"We consider the following baseline methods: N −1 cess with each language in all N languages as the test language `(tst) , and collect N learned models. We use Normalized Discounted Cumulative 2 For each language, we choose the treebank that has the least number of training instances, which results in 60 languages with training data and 11 without training data. 3129 • Using a single dataset-dependent feature: While dataset-dependent features have not typically been used as criteria for selecting transfer languages, they are a common feature in data selection methods for crossdomain transfer (Moore and Lewis, 2010). In EL POS DEP dataset word overlap ow subword overlap osw size ratio stf /stk type-token ratio dttr 28.6 29.2 3.7 2.5 30.7 – 0.3 – 13.4 – 9.5 7.4 52.3 – 24.8 6.4 genetic dgen syntactic dsyn featural df ea phonological dpho inventory dinv geographic dgeo 24.2 14.8 10.1 3.0 8.5 15.1 50.9 46.4 47.5 4.0 41.3 49.5 14.8 4.1 5.7 9.8 2.4 15.7 32.0 22.9 13.9 43.4 23.5 46.4 L ANG R ANK (all) L ANG R ANK (dataset) L ANG R ANK (URIEL) 51.1 53.7 32.6 63.0 17.0 58.1 28.9 26.5 16.6 65.0 65.0 59.6 1.00 0.95 Max evaluation score MT ling. distance Method view of this, we include selecting the transfer languag"
P19-1301,D18-1103,1,0.843378,"nguages is the lack of training data in the languages in question. It has been demonstrated that through cross-lingual transfer, it is possible to leverage one or more similar high-resource languages to improve the performance on the low-resource languages in several NLP tasks, including machine Equal contribution Code, data, and pre-trained models are available at https://github.com/neulab/langrank 1 Ltk: Task Language Transfer Learning Introduction ∗ Ltf,2: Transfer Language 2 ... 1 ... Ltf,1: Transfer Language 1 translation (Zoph et al., 2016; Johnson et al., 2017; Nguyen and Chiang, 2017; Neubig and Hu, 2018), parsing (T¨ackstr¨om et al., 2012; Ammar et al., 2016; Ahmad et al., 2018; Ponti et al., 2018), partof-speech or morphological tagging (T¨ackstr¨om et al., 2013; Cotterell and Heigold, 2017; Malaviya et al., 2018; Plank and Agi´c, 2018), named entity recognition (Zhang et al., 2016; Mayhew et al., 2017; Xie et al., 2018), and entity linking (Tsai and Roth, 2016; Rijhwani et al., 2019). There are many methods for performing this transfer, including joint training (Ammar et al., 2016; Tsai and Roth, 2016; Cotterell and Heigold, 2017; Johnson et al., 2017; Malaviya et al., 2018), annotation pro"
P19-1301,W18-1818,1,0.738931,"of our research goals is to understand what linguistic or statistical features of a dataset play important roles in transfer learning, so the interpretable nature of the treebased model can provide valuable insights, which we elaborate further in §6.2. 5 5.1 Experimental Settings Testbed Tasks We investigate the performance of L ANG R ANK on four common NLP tasks: MT, EL, POS tagging, and DEPendency parsing. We briefly outline the settings for all four NLP tasks. Machine Translation We train a standard attention-based sequence-to-sequence model (Bahdanau et al., 2015), using the XNMT toolkit (Neubig et al., 2018). We perform training on the multilingual TED talk corpus of Qi et al. (2018), using 54 task and 54 transfer languages, always translating into English, which results in 2,862 task/transfer pairs and 54 single-source training settings. Transfer is performed by joint training over the concatenated task and transfer corpora. Entity Linking The cross-lingual EL task involves linking a named entity mention in the task language to an English knowledge base. We train two character-level LSTM encoders, which are trained to maximize the cosine similarity between parallel (i.e., linked) entities (Rijhw"
P19-1301,I17-2050,0,0.0186119,"niques to low-resource languages is the lack of training data in the languages in question. It has been demonstrated that through cross-lingual transfer, it is possible to leverage one or more similar high-resource languages to improve the performance on the low-resource languages in several NLP tasks, including machine Equal contribution Code, data, and pre-trained models are available at https://github.com/neulab/langrank 1 Ltk: Task Language Transfer Learning Introduction ∗ Ltf,2: Transfer Language 2 ... 1 ... Ltf,1: Transfer Language 1 translation (Zoph et al., 2016; Johnson et al., 2017; Nguyen and Chiang, 2017; Neubig and Hu, 2018), parsing (T¨ackstr¨om et al., 2012; Ammar et al., 2016; Ahmad et al., 2018; Ponti et al., 2018), partof-speech or morphological tagging (T¨ackstr¨om et al., 2013; Cotterell and Heigold, 2017; Malaviya et al., 2018; Plank and Agi´c, 2018), named entity recognition (Zhang et al., 2016; Mayhew et al., 2017; Xie et al., 2018), and entity linking (Tsai and Roth, 2016; Rijhwani et al., 2019). There are many methods for performing this transfer, including joint training (Ammar et al., 2016; Tsai and Roth, 2016; Cotterell and Heigold, 2017; Johnson et al., 2017; Malaviya et al.,"
P19-1301,C16-1123,0,0.056038,"Missing"
P19-1301,D18-1061,0,0.0314953,"Missing"
P19-1301,P11-1157,0,0.0656756,"Missing"
P19-1301,N18-2084,1,0.828574,"a dataset play important roles in transfer learning, so the interpretable nature of the treebased model can provide valuable insights, which we elaborate further in §6.2. 5 5.1 Experimental Settings Testbed Tasks We investigate the performance of L ANG R ANK on four common NLP tasks: MT, EL, POS tagging, and DEPendency parsing. We briefly outline the settings for all four NLP tasks. Machine Translation We train a standard attention-based sequence-to-sequence model (Bahdanau et al., 2015), using the XNMT toolkit (Neubig et al., 2018). We perform training on the multilingual TED talk corpus of Qi et al. (2018), using 54 task and 54 transfer languages, always translating into English, which results in 2,862 task/transfer pairs and 54 single-source training settings. Transfer is performed by joint training over the concatenated task and transfer corpora. Entity Linking The cross-lingual EL task involves linking a named entity mention in the task language to an English knowledge base. We train two character-level LSTM encoders, which are trained to maximize the cosine similarity between parallel (i.e., linked) entities (Rijhwani et al., 2019). We use the same dataset as Rijhwani et al. (2019), which c"
P19-1301,D17-1038,0,0.0282302,"to their model for phonetic representation learning. Ammar et al. (2016) and Ahmad et al. (2018) used similar ideas for output: 2 no stf stk yes &gt; 1.61 output: 3 no output: 1 Figure 4: An example of the decision tree learned in the machine translation task for Galician as task language. dependency parsing, incorporating linguisticallyinformed vectors into their models. O’Horan et al. (2016) survey typological resources available and their utility in NLP tasks. Although not for cross-lingual transfer, there has been prior work on data selection for training models. Tsvetkov et al. (2016a) and Ruder and Plank (2017) use Bayesian optimization for data selection. van der Wees et al. (2017) study the effect of data selection of neural machine translation, as well as propose a dynamic method to select relevant training data that improves translation performance. Plank and van Noord (2011) design a method to automatically select domain-relevant training data for parsing in English and Dutch. 8 7 output: 0 Conclusion We formulate the task of selecting the optimal transfer languages for an NLP task as a ranking problem. For machine translation, entity linking, part-of-speech tagging, and dependency parsing, we"
P19-1301,P16-1162,0,0.00734751,"k consists only of named entities, so the TTR is typically close to 1 for all languages. Therefore, we do not include TTR related features for the EL task. Word overlap and subword overlap: We measure the similarity between the vocabularies of task- and transfer-language corpora by word overlap ow , and subword overlap osw : ow = |Ttf ∩ Ttk | , |Ttf |+ |Ttk | osw = |Stf ∩ Stk | , |Stf |+ |Stk | where Ttf and Ttk are the sets of types in the transfer- and task-language corpora, and Stf and Stk are their sets of subwords. The subwords are obtained by an unsupervised word segmentation algorithm (Sennrich et al., 2016; Kudo, 3127 2018). Note that for EL, we do not consider subword overlap, and the word overlap is simply the count of the named entities that have exactly the same representations in both transfer and task languages. We also omit subword overlap in the POS and DEP tasks, as some low-resource languages do not have enough data for properly extracting subwords. 3.2 Dataset-independent Features Dataset-independent features are measures of the similarity between a pair of languages based on phylogenetic or typological properties established by linguistic study. Specifically, we leverage six differe"
P19-1301,Q13-1001,0,0.0477631,"Missing"
P19-1301,N12-1052,0,0.124907,"Missing"
P19-1301,D18-1034,1,0.865535,"and pre-trained models are available at https://github.com/neulab/langrank 1 Ltk: Task Language Transfer Learning Introduction ∗ Ltf,2: Transfer Language 2 ... 1 ... Ltf,1: Transfer Language 1 translation (Zoph et al., 2016; Johnson et al., 2017; Nguyen and Chiang, 2017; Neubig and Hu, 2018), parsing (T¨ackstr¨om et al., 2012; Ammar et al., 2016; Ahmad et al., 2018; Ponti et al., 2018), partof-speech or morphological tagging (T¨ackstr¨om et al., 2013; Cotterell and Heigold, 2017; Malaviya et al., 2018; Plank and Agi´c, 2018), named entity recognition (Zhang et al., 2016; Mayhew et al., 2017; Xie et al., 2018), and entity linking (Tsai and Roth, 2016; Rijhwani et al., 2019). There are many methods for performing this transfer, including joint training (Ammar et al., 2016; Tsai and Roth, 2016; Cotterell and Heigold, 2017; Johnson et al., 2017; Malaviya et al., 2018), annotation projection (T¨ackstr¨om et al., 2012; T¨ackstr¨om et al., 2013; Zhang et al., 2016; Ponti et al., 2018; Plank and Agi´c, 2018), fine-tuning (Zoph et al., 2016; Neubig and Hu, 2018), data augmentation (Mayhew et al., 2017), or zero-shot transfer (Ahmad et al., 2018; Xie et al., 2018; Neubig and Hu, 3125 Proceedings of the 57th"
P19-1301,P18-4013,0,0.0189533,"e the same dataset as Rijhwani et al. (2019), which contains language-linked Wikipedia article titles from 9 low-resource task languages and 53 potential transfer languages, resulting in 477 task/transfer pairs. We perform training in a zero-shot setting, where we train on corpora only in the transfer language, and test entity linking accuracy on the task language without joint training or fine-tuning. POS Tagging We train a bi-directional LSTMCNNs-CRF model (Ma and Hovy, 2016) on word 3128 sequences without using pre-trained word embeddings. The implementation is based on the NCRF++ toolkit (Yang and Zhang, 2018). We perform training on the Universal Dependencies v2.2 dataset (Nivre et al., 2018), using 26 languages that have the least training data as task languages, and 60 transfer languages,2 resulting in 1,545 pairs of transfer-task languages. Transfer is performed by joint training over the concatenated task and transfer corpora if the task language has training data, and training only with transfer corpora otherwise. The performance is measured by POS tagging accuracy on the task language. Dependency Parsing For the dependency parsing task, we follow the settings of (Ahmad et al., 2018) and util"
P19-1301,C16-1045,0,0.016972,"ng machine Equal contribution Code, data, and pre-trained models are available at https://github.com/neulab/langrank 1 Ltk: Task Language Transfer Learning Introduction ∗ Ltf,2: Transfer Language 2 ... 1 ... Ltf,1: Transfer Language 1 translation (Zoph et al., 2016; Johnson et al., 2017; Nguyen and Chiang, 2017; Neubig and Hu, 2018), parsing (T¨ackstr¨om et al., 2012; Ammar et al., 2016; Ahmad et al., 2018; Ponti et al., 2018), partof-speech or morphological tagging (T¨ackstr¨om et al., 2013; Cotterell and Heigold, 2017; Malaviya et al., 2018; Plank and Agi´c, 2018), named entity recognition (Zhang et al., 2016; Mayhew et al., 2017; Xie et al., 2018), and entity linking (Tsai and Roth, 2016; Rijhwani et al., 2019). There are many methods for performing this transfer, including joint training (Ammar et al., 2016; Tsai and Roth, 2016; Cotterell and Heigold, 2017; Johnson et al., 2017; Malaviya et al., 2018), annotation projection (T¨ackstr¨om et al., 2012; T¨ackstr¨om et al., 2013; Zhang et al., 2016; Ponti et al., 2018; Plank and Agi´c, 2018), fine-tuning (Zoph et al., 2016; Neubig and Hu, 2018), data augmentation (Mayhew et al., 2017), or zero-shot transfer (Ahmad et al., 2018; Xie et al., 2018; Neu"
P19-1301,D16-1163,0,0.0489424,"ng natural language processing (NLP) techniques to low-resource languages is the lack of training data in the languages in question. It has been demonstrated that through cross-lingual transfer, it is possible to leverage one or more similar high-resource languages to improve the performance on the low-resource languages in several NLP tasks, including machine Equal contribution Code, data, and pre-trained models are available at https://github.com/neulab/langrank 1 Ltk: Task Language Transfer Learning Introduction ∗ Ltf,2: Transfer Language 2 ... 1 ... Ltf,1: Transfer Language 1 translation (Zoph et al., 2016; Johnson et al., 2017; Nguyen and Chiang, 2017; Neubig and Hu, 2018), parsing (T¨ackstr¨om et al., 2012; Ammar et al., 2016; Ahmad et al., 2018; Ponti et al., 2018), partof-speech or morphological tagging (T¨ackstr¨om et al., 2013; Cotterell and Heigold, 2017; Malaviya et al., 2018; Plank and Agi´c, 2018), named entity recognition (Zhang et al., 2016; Mayhew et al., 2017; Xie et al., 2018), and entity linking (Tsai and Roth, 2016; Rijhwani et al., 2019). There are many methods for performing this transfer, including joint training (Ammar et al., 2016; Tsai and Roth, 2016; Cotterell and Heigol"
P19-1301,N16-1072,0,0.0284372,"t https://github.com/neulab/langrank 1 Ltk: Task Language Transfer Learning Introduction ∗ Ltf,2: Transfer Language 2 ... 1 ... Ltf,1: Transfer Language 1 translation (Zoph et al., 2016; Johnson et al., 2017; Nguyen and Chiang, 2017; Neubig and Hu, 2018), parsing (T¨ackstr¨om et al., 2012; Ammar et al., 2016; Ahmad et al., 2018; Ponti et al., 2018), partof-speech or morphological tagging (T¨ackstr¨om et al., 2013; Cotterell and Heigold, 2017; Malaviya et al., 2018; Plank and Agi´c, 2018), named entity recognition (Zhang et al., 2016; Mayhew et al., 2017; Xie et al., 2018), and entity linking (Tsai and Roth, 2016; Rijhwani et al., 2019). There are many methods for performing this transfer, including joint training (Ammar et al., 2016; Tsai and Roth, 2016; Cotterell and Heigold, 2017; Johnson et al., 2017; Malaviya et al., 2018), annotation projection (T¨ackstr¨om et al., 2012; T¨ackstr¨om et al., 2013; Zhang et al., 2016; Ponti et al., 2018; Plank and Agi´c, 2018), fine-tuning (Zoph et al., 2016; Neubig and Hu, 2018), data augmentation (Mayhew et al., 2017), or zero-shot transfer (Ahmad et al., 2018; Xie et al., 2018; Neubig and Hu, 3125 Proceedings of the 57th Annual Meeting of the Association for Co"
P19-1301,P16-1013,0,0.0196907,"lingual transfer across several tasks. Other work has performed large-scale studies on the importance of appropriately selecting a transfer language, such as Paul et al. (2009), which performed an extensive search for a “pivot language” in statistical MT, but without attempting to actually learn or predict which pivot language is best. Typologically-informed models are another vein of research that is relevant to our work. The relationship between linguistic typology and statistical modeling has been studied by Gerz et al. (2018) and Cotterell et al. (2018), with a focus on language modeling. Tsvetkov et al. (2016b) used typological information in the target language as additional input to their model for phonetic representation learning. Ammar et al. (2016) and Ahmad et al. (2018) used similar ideas for output: 2 no stf stk yes &gt; 1.61 output: 3 no output: 1 Figure 4: An example of the decision tree learned in the machine translation task for Galician as task language. dependency parsing, incorporating linguisticallyinformed vectors into their models. O’Horan et al. (2016) survey typological resources available and their utility in NLP tasks. Although not for cross-lingual transfer, there has been prio"
P19-1301,N16-1161,1,0.883501,"Missing"
P19-1301,D17-1147,0,0.0412759,"Missing"
P19-1301,D17-1302,0,\N,Missing
P19-1301,P18-1142,0,\N,Missing
P19-1301,N19-1253,1,\N,Missing
P19-1579,N19-1388,0,0.0613493,"790 4 BLEU for X )ENG Training Data AZE (TUR) BEL GLG SLK (RUS) (POR) (CES) 12.89 12.78 18.71 21.73 31.16 30.65 29.16 29.54 11.83 0.47 16.34 0.18 29.51 1.15 28.12 0.75 11.84 12.46 15.72 16.40 29.19 30.07 29.79 30.60 (supervised MT) (unsupervised MT) (word subst.) (modified UMT) 11.92 11.86 14.87 14.72 15.24 15.79 13.83 23.56 23.31 24.25 29.91 29.80 32.02 32.27 32.30 28.52 28.69 29.60 29.55 30.00 (word subst.) (modified UMT) 14.18 13.71 21.74 19.94 31.72 31.39 30.90 30.22 (word subst.) 15.74 24.51 33.16 32.07 15.91 23.69 32.55 31.58 Results from Literature SDE (Wang et al., 2019) many-to-many (Aharoni et al., 2019) Standard NMT 1 {SLE SHE , TLE THE } 2 {ML , ME } (supervised MT) (unsupervised MT) Standard Supervised Back-translation 3 + {SˆEs )L , ME } 4 + {SˆEs )H , ME } Augmentation from HRL-ENG 5 + {SˆHs )L , THE } 6 + {SˆHu )L , THE } 7 + {SˆHw)L , THE } 8 + {SˆHm)L , THE } 9 + {SˆHw)L SˆHm)L , THE THE } Augmention from ENG by pivoting 10 + {SˆEw)H )L , ME } 11 + {SˆEm)H )L , ME } Combinations 12 + {SˆHw)L SˆEw)H )L , THE ME } + {SˆHw)L SˆHm)L , THE THE } 13 + {SˆEw)H )L SˆEm)H )L , ME ME } Table 2: Evaluation of translation performance over four language pairs. Rows 1 and 2 show pre-training BLEU s"
P19-1579,D18-1549,0,0.393947,", THE } and {SˆEw)H )L , ME } where w denotes augmentation with word substitution. 3.2 Augmentation with Unsupervised MT Although we assume LRL and HRL to be similar with regards to word morphology and word order, the simple word-by-word augmentation process will almost certainly be insufficient to completely replicate actual LRL data. A natural next step is to further convert the pseudo-LRL data into a version closer to the real LRL. In order to achieve this in our limited-resource setting, we propose to use unsupervised machine translation (UMT). UMT Unsupervised Neural Machine Translation (Artetxe et al., 2018; Lample et al., 2018a,c) makes it possible to translate between languages without parallel data. This is done by coupling denoising auto-encoding, iterative back-translation, and shared representations of both encoders and decoders, making it possible for the model to extend the initial naive word-to-word mapping into learning to translate longer sentences. Initial studies of UMT have focused on data-rich, morphologically simple languages like English and French. Applying the UMT framework to lowresource and morphologically rich languages is largely unexplored, with the exception of Neubig an"
P19-1579,D19-1632,0,0.0482938,"2018a,c) makes it possible to translate between languages without parallel data. This is done by coupling denoising auto-encoding, iterative back-translation, and shared representations of both encoders and decoders, making it possible for the model to extend the initial naive word-to-word mapping into learning to translate longer sentences. Initial studies of UMT have focused on data-rich, morphologically simple languages like English and French. Applying the UMT framework to lowresource and morphologically rich languages is largely unexplored, with the exception of Neubig and Hu (2018) and Guzmán et al. (2019), showing that UMT performs exceptionally poorly between dissimilar language pairs with BLEU scores lower than 1. The problem is naturally harder for morphologically rich LRLs due to two reasons. First, morphologically rich languages have a higher proportions of infrequent words (Chahuneau et al., 2013). Second, even though still larger than the respective parallel datasets, the size of monolingual datasets in these languages is much smaller compared to HRLs. Modified Initialization As pointed out in Lample et al. (2018c), a good initialization plays a critical role in training NMT in an unsup"
P19-1579,D13-1174,0,0.0307241,"nto learning to translate longer sentences. Initial studies of UMT have focused on data-rich, morphologically simple languages like English and French. Applying the UMT framework to lowresource and morphologically rich languages is largely unexplored, with the exception of Neubig and Hu (2018) and Guzmán et al. (2019), showing that UMT performs exceptionally poorly between dissimilar language pairs with BLEU scores lower than 1. The problem is naturally harder for morphologically rich LRLs due to two reasons. First, morphologically rich languages have a higher proportions of infrequent words (Chahuneau et al., 2013). Second, even though still larger than the respective parallel datasets, the size of monolingual datasets in these languages is much smaller compared to HRLs. Modified Initialization As pointed out in Lample et al. (2018c), a good initialization plays a critical role in training NMT in an unsupervised fashion. Previously explored initialization methods include: 1) word-for-word translation with an induced dictionary to create synthetic sentence pairs for initial training (Lample et al., 2018a; Artetxe et al., 2018); 2) joint Byte-Pair-Encoding (BPE) for both the source and target corpus sides"
P19-1579,P17-1176,0,0.0305798,"ed entity recognition (Xie et al., 2018) or information retrieval (Litschko et al., 2018). Hassan et al. (2017) synthesized data with word embeddings for spoken dialect translation, with a process that requires a LRL - ENG as well as a HRL - LRL dictionary, while our work only uses a HRL - LRL dictionary. Bridging source and target languages through a pivot language was originally proposed for phrasebased MT (De Gispert and Marino, 2006; Cohn and Lapata, 2007). It was later adapted for Neural MT (Levinboim and Chiang, 2015), and Cheng et al. (2017) proposed joint training for pivot-based NMT. Chen et al. (2017) proposed to use an existing pivottarget NMT model to guide the training of sourcetarget model. Lakew et al. (2018) proposed an iterative procedure to realize zero-shot translation by pivoting on a third language. 7 Conclusion We propose a generalized data augmentation framework for low-resource translation, making best use of all available resources. We propose an effective two-step pivoting augmentation method to convert HRL parallel data to LRL . In future work, we will explore methods for controlling the induced dictionary quality to improve word substitution as well as M - UMT . We will a"
P19-1579,P17-4012,0,0.028526,"ntation model for English is trained on English monolingual data only. We set the vocabulary size for each model to 20K. All data are then segmented by their respective segmentation model. We use FastText4 to train word embeddings using ML and MH with a dimension of 256 (used for the dictionary induction step). We also pre-train subword level embeddings on the segmented ML , ˆ and MH with the same dimension. M L 4.3 Model Architecture Supervised NMT We use the self-attention Transformer model (Vaswani et al., 2017). We adapt the implementation from the open-source translation toolkit OpenNMT (Klein et al., 2017). Both encoder and decoder consist of 4 layers, with the word embedding and hidden unit dimensions set to 256. 5 We use a batch size of 8096 tokens. Unsupervised NMT We train unsupervised Transformer models with the UnsupervisedMT toolkit.6 Layer sizes and dimensions are the same as in the supervised NMT model. The parameters of the first three layers of the encoder and the decoder are shared. The embedding layers are initialized with the pre-trained subword embeddings from monolingual data. We set the weight parameters for autodenoising language modeling and iterative back translation as λ1 ="
P19-1579,P17-2061,0,0.0196634,"Hm)L , THE THE } Augmention from ENG by pivoting 10 + {SˆEw)H )L , ME } 11 + {SˆEm)H )L , ME } Combinations 12 + {SˆHw)L SˆEw)H )L , THE ME } + {SˆHw)L SˆHm)L , THE THE } 13 + {SˆEw)H )L SˆEm)H )L , ME ME } Table 2: Evaluation of translation performance over four language pairs. Rows 1 and 2 show pre-training BLEU scores. Rows 3–13 show scores after fine tuning. Statistically significantly best scores are highlighted (p &lt; 0.05). 2016; Nguyen and Chiang, 2017). We first train a base NMT model on the concatenation of {SLE , TLE } and {SHE , THE }. Then we adopt the mixed fine-tuning strategy of Chu et al. (2017), fine-tuning the base model on the concatenation of the base and augmented datasets. For each setting, we perform a sufficient number of updates to reach convergence in terms of development perplexity. We use the performance on the development sets (as provided by the TED corpus) as our criterion for selecting the best model, both for augmentation and final model training. 5 Results and Analysis A collection of our results with the baseline and our proposed methods is shown in Table 2. 5.1 Baselines The performance of the base supervised model (row 1) varies from 11.8 to 29.5 BLEU points. Gen"
P19-1579,P07-1092,0,0.0580969,"018; Zhang et al., 2018). Bilingual dictionaries learned in both supervised and unsupervised ways have been used in lowresource settings for tasks such as named entity recognition (Xie et al., 2018) or information retrieval (Litschko et al., 2018). Hassan et al. (2017) synthesized data with word embeddings for spoken dialect translation, with a process that requires a LRL - ENG as well as a HRL - LRL dictionary, while our work only uses a HRL - LRL dictionary. Bridging source and target languages through a pivot language was originally proposed for phrasebased MT (De Gispert and Marino, 2006; Cohn and Lapata, 2007). It was later adapted for Neural MT (Levinboim and Chiang, 2015), and Cheng et al. (2017) proposed joint training for pivot-based NMT. Chen et al. (2017) proposed to use an existing pivottarget NMT model to guide the training of sourcetarget model. Lakew et al. (2018) proposed an iterative procedure to realize zero-shot translation by pivoting on a third language. 7 Conclusion We propose a generalized data augmentation framework for low-resource translation, making best use of all available resources. We propose an effective two-step pivoting augmentation method to convert HRL parallel data t"
P19-1579,W17-3204,0,0.0280161,"esource language (LRL) and a related high-resource language (HRL), typical data augmentation scenarios use any available parallel data [b] and [c] to back-translate English monolingual data [a] and generate parallel resources ([1] and [2]). We additionally propose scenarios [3] and [4], where we pivot through HRL in order to generate a LRL–ENG resource. Introduction The task of Machine Translation (MT) for low resource languages (LRLs) is notoriously hard due to the lack of the large parallel corpora needed to achieve adequate performance with current Neural Machine Translation (NMT) systems (Koehn and Knowles, 2017). A standard practice to improve training of models for an LRL of interest (e.g. Azerbaijani) is utilizing data from a related high-resource language (HRL, e.g. Turkish). Both transferring from HRL to LRL (Zoph et al., 2016; Nguyen and Chiang, 2017; Gu et al., 2018) and 1 Code is available at https://github.com/ xiamengzhou/DataAugForLRL joint training on HRL and LRL parallel data (Johnson et al., 2017; Neubig and Hu, 2018) have shown to be effective techniques for low-resource NMT. Incorporating data from other languages can be viewed as one form data augmentation, and particularly large impr"
P19-1579,W17-4715,0,0.0482957,"glish. We propose methods to create pseudo-parallel LRL data in this setting. As illustrated in Figure 1, we augment parallel data via two main methods: 1) back-translating from ENG to LRL or HRL; 2) converting the HRL-ENG dataset to a pseudo LRL-ENG dataset. In the first thread, we focus on creating new parallel sentences through back-translation. Backtranslating from the target language to the source (Sennrich et al., 2016) is a common practice in data augmentation, but has also been shown to be less effective in low-resource settings where it is hard to train a good back-translation model (Currey et al., 2017). As a way to ameliorate this problem, we examine methods to instead translate from the target language to a highly-related HRL, which remains unexplored in the context of low-resource NMT. This pseudo-HRL-ENG dataset can then be used for joint training with the LRL-ENG dataset. In the second thread, we focus on converting an HRL - ENG dataset to a pseudo- LRL -to- ENG dataset that better approximates the true LRL data. Converting between HRLs and LRLs also suffers from lack of resources, but because the LRL and HRL are related, this is an easier task that we argue can be done to some extent b"
P19-1579,D15-1126,0,0.0239984,"both supervised and unsupervised ways have been used in lowresource settings for tasks such as named entity recognition (Xie et al., 2018) or information retrieval (Litschko et al., 2018). Hassan et al. (2017) synthesized data with word embeddings for spoken dialect translation, with a process that requires a LRL - ENG as well as a HRL - LRL dictionary, while our work only uses a HRL - LRL dictionary. Bridging source and target languages through a pivot language was originally proposed for phrasebased MT (De Gispert and Marino, 2006; Cohn and Lapata, 2007). It was later adapted for Neural MT (Levinboim and Chiang, 2015), and Cheng et al. (2017) proposed joint training for pivot-based NMT. Chen et al. (2017) proposed to use an existing pivottarget NMT model to guide the training of sourcetarget model. Lakew et al. (2018) proposed an iterative procedure to realize zero-shot translation by pivoting on a third language. 7 Conclusion We propose a generalized data augmentation framework for low-resource translation, making best use of all available resources. We propose an effective two-step pivoting augmentation method to convert HRL parallel data to LRL . In future work, we will explore methods for controlling t"
P19-1579,N18-1032,0,0.151454,"[3] and [4], where we pivot through HRL in order to generate a LRL–ENG resource. Introduction The task of Machine Translation (MT) for low resource languages (LRLs) is notoriously hard due to the lack of the large parallel corpora needed to achieve adequate performance with current Neural Machine Translation (NMT) systems (Koehn and Knowles, 2017). A standard practice to improve training of models for an LRL of interest (e.g. Azerbaijani) is utilizing data from a related high-resource language (HRL, e.g. Turkish). Both transferring from HRL to LRL (Zoph et al., 2016; Nguyen and Chiang, 2017; Gu et al., 2018) and 1 Code is available at https://github.com/ xiamengzhou/DataAugForLRL joint training on HRL and LRL parallel data (Johnson et al., 2017; Neubig and Hu, 2018) have shown to be effective techniques for low-resource NMT. Incorporating data from other languages can be viewed as one form data augmentation, and particularly large improvements can be expected when the HRL shares vocabulary or is syntactically similar with the LRL (Lin et al., 2019). Simple joint training is still not ideal, though, considering that there will still be many words and possibly even syntactic structures that will no"
P19-1579,D18-1103,1,0.939235,"s (LRLs) is notoriously hard due to the lack of the large parallel corpora needed to achieve adequate performance with current Neural Machine Translation (NMT) systems (Koehn and Knowles, 2017). A standard practice to improve training of models for an LRL of interest (e.g. Azerbaijani) is utilizing data from a related high-resource language (HRL, e.g. Turkish). Both transferring from HRL to LRL (Zoph et al., 2016; Nguyen and Chiang, 2017; Gu et al., 2018) and 1 Code is available at https://github.com/ xiamengzhou/DataAugForLRL joint training on HRL and LRL parallel data (Johnson et al., 2017; Neubig and Hu, 2018) have shown to be effective techniques for low-resource NMT. Incorporating data from other languages can be viewed as one form data augmentation, and particularly large improvements can be expected when the HRL shares vocabulary or is syntactically similar with the LRL (Lin et al., 2019). Simple joint training is still not ideal, though, considering that there will still be many words and possibly even syntactic structures that will not be shared between the most highly related languages. There are model-based methods that ameliorate the problem through more expressive source-side representati"
P19-1579,I17-2050,0,0.184273,"ionally propose scenarios [3] and [4], where we pivot through HRL in order to generate a LRL–ENG resource. Introduction The task of Machine Translation (MT) for low resource languages (LRLs) is notoriously hard due to the lack of the large parallel corpora needed to achieve adequate performance with current Neural Machine Translation (NMT) systems (Koehn and Knowles, 2017). A standard practice to improve training of models for an LRL of interest (e.g. Azerbaijani) is utilizing data from a related high-resource language (HRL, e.g. Turkish). Both transferring from HRL to LRL (Zoph et al., 2016; Nguyen and Chiang, 2017; Gu et al., 2018) and 1 Code is available at https://github.com/ xiamengzhou/DataAugForLRL joint training on HRL and LRL parallel data (Johnson et al., 2017; Neubig and Hu, 2018) have shown to be effective techniques for low-resource NMT. Incorporating data from other languages can be viewed as one form data augmentation, and particularly large improvements can be expected when the HRL shares vocabulary or is syntactically similar with the LRL (Lin et al., 2019). Simple joint training is still not ideal, though, considering that there will still be many words and possibly even syntactic struc"
P19-1579,W18-2711,1,0.81174,"Work Our work is related to multilingual and unsupervised translation, bilingual dictionary induction, as well as approaches for triangulation (pivoting). In a low-resource MT scenario, multilingual training that aims at sharing parameters by leveraging parallel datasets of multiple languages is a common practice. Some works target learning a universal representation for all languages either by leveraging semantic sharing between mapped word embeddings (Gu et al., 2018) or by using character n-gram embeddings (Wang et al., 2019) optimizing subword sharing. More related with data augmentation, Nishimura et al. (2018) fill in missing data with a multi-source setting to boost multilingual translation. Unsupervised machine translation enables training NMT models without parallel data (Artetxe et al., 2018; Lample et al., 2018a,c). Recently, multiple methods have been proposed to further improve the framework. By incorporating a statistical MT system as posterior regularization, Ren et al. (2019) achieved state-of-the-art for en-fr and en-de MT. Besides MT, the framework has also been applied to other unsupervised tasks like nonparallel style transfer (Subramanian et al., 2018; Zhang et al., 2018). Bilingual"
P19-1579,N18-2084,1,0.81657,"sier because the two languages are related. A good example is the agglutinative language of Azerbaijiani, where each word may consist of several morphemes and each morpheme could possibly map to an English word itself. Correspondences to (also agglutinative) Turkish, however, are easier to uncover. To give a concrete example, the Azerbijiani word “dü¸süncәlәrim” can be fairly easily aligned to the Turkish word “dü¸süncelerim” while in English it corresponds to the phrase “my thoughts”, which is unlikely to be perfectly aligned. 4 4.1 Experimental Setup Data We use the multilingual TED corpus (Qi et al., 2018) as a test-bed for evaluating the efficacy of each augmentation method. We conduct extensive experiments over four low-resource languages: Azerbaijani (AZE), Belarusian (BEL), Galician (GLG), and Slovak (SLK), along with their highly related languages Turkish (TUR), Russian (RUS), Portuguese (POR), and Czech (CES) respec4.2 Pre-processing We train a joint sentencepiece3 model for each LRL-HRL pair by concatenating the monolingual corpora of the two languages. The segmentation model for English is trained on English monolingual data only. We set the vocabulary size for each model to 20K. All da"
P19-1579,D18-1034,1,0.843215,"without parallel data (Artetxe et al., 2018; Lample et al., 2018a,c). Recently, multiple methods have been proposed to further improve the framework. By incorporating a statistical MT system as posterior regularization, Ren et al. (2019) achieved state-of-the-art for en-fr and en-de MT. Besides MT, the framework has also been applied to other unsupervised tasks like nonparallel style transfer (Subramanian et al., 2018; Zhang et al., 2018). Bilingual dictionaries learned in both supervised and unsupervised ways have been used in lowresource settings for tasks such as named entity recognition (Xie et al., 2018) or information retrieval (Litschko et al., 2018). Hassan et al. (2017) synthesized data with word embeddings for spoken dialect translation, with a process that requires a LRL - ENG as well as a HRL - LRL dictionary, while our work only uses a HRL - LRL dictionary. Bridging source and target languages through a pivot language was originally proposed for phrasebased MT (De Gispert and Marino, 2006; Cohn and Lapata, 2007). It was later adapted for Neural MT (Levinboim and Chiang, 2015), and Cheng et al. (2017) proposed joint training for pivot-based NMT. Chen et al. (2017) proposed to use an ex"
P19-1579,N15-1104,0,0.0828626,"Missing"
P19-1579,D17-1207,0,0.0197792,"that an LRL and its corresponding HRL can be similar in morphology and word order, in the following sections, we propose methods to convert HRL to LRL for data augmentation in a more reliable way. 3 LRL-HRL Translation Methods In this section, we introduce two methods for converting HRL to LRL for data augmentation. 3.1 Augmentation with Word Substitution Mikolov et al. (2013) show that the word embedding spaces share similar innate structure over different languages, making it possible to induce bilingual dictionaries with a limited amount of or even without parallel data (Xing et al., 2015; Zhang et al., 2017; Lample et al., 2018b). Although the capacity of these methods is naturally constrained by the intrinsic properties of the two mapped languages, it’s more likely to create a high-quality bilingual dictionary for two highly-related languages. Given the induced dictionary, we can substitute HRL words with LRL ones and construct a word-by-word translated pseudo-LRL corpus. Dictionary Induction We use a supervised method to obtain a bilingual dictionary between the two highly-related languages. Following Xing et al. (2015), we formulate the task of finding the optimal mapping between the source a"
P19-1579,D16-1163,0,0.0675711,"and [2]). We additionally propose scenarios [3] and [4], where we pivot through HRL in order to generate a LRL–ENG resource. Introduction The task of Machine Translation (MT) for low resource languages (LRLs) is notoriously hard due to the lack of the large parallel corpora needed to achieve adequate performance with current Neural Machine Translation (NMT) systems (Koehn and Knowles, 2017). A standard practice to improve training of models for an LRL of interest (e.g. Azerbaijani) is utilizing data from a related high-resource language (HRL, e.g. Turkish). Both transferring from HRL to LRL (Zoph et al., 2016; Nguyen and Chiang, 2017; Gu et al., 2018) and 1 Code is available at https://github.com/ xiamengzhou/DataAugForLRL joint training on HRL and LRL parallel data (Johnson et al., 2017; Neubig and Hu, 2018) have shown to be effective techniques for low-resource NMT. Incorporating data from other languages can be viewed as one form data augmentation, and particularly large improvements can be expected when the HRL shares vocabulary or is syntactically similar with the LRL (Lin et al., 2019). Simple joint training is still not ideal, though, considering that there will still be many words and poss"
P19-1579,P16-1009,0,0.519111,"ly 28 - August 2, 2019. 2019 Association for Computational Linguistics access to parallel or monolingual data of an LRL of interest, its HRL, and the target language, which we will assume is English. We propose methods to create pseudo-parallel LRL data in this setting. As illustrated in Figure 1, we augment parallel data via two main methods: 1) back-translating from ENG to LRL or HRL; 2) converting the HRL-ENG dataset to a pseudo LRL-ENG dataset. In the first thread, we focus on creating new parallel sentences through back-translation. Backtranslating from the target language to the source (Sennrich et al., 2016) is a common practice in data augmentation, but has also been shown to be less effective in low-resource settings where it is hard to train a good back-translation model (Currey et al., 2017). As a way to ameliorate this problem, we examine methods to instead translate from the target language to a highly-related HRL, which remains unexplored in the context of low-resource NMT. This pseudo-HRL-ENG dataset can then be used for joint training with the LRL-ENG dataset. In the second thread, we focus on converting an HRL - ENG dataset to a pseudo- LRL -to- ENG dataset that better approximates the"
P19-1579,Q17-1024,0,\N,Missing
W17-0123,D16-1263,0,0.23046,"Missing"
W17-0123,D16-1133,1,0.903874,"d corpus interpretable for future studies. New technologies are being developed to facilitate collection of translations (Bird et al., 2014), and there already exist recent examples of parallel speech collection efforts focused on endangered languages (Blachon et al., 2016; Adda et al., 2016). 2 Methodology As a proof-of-concept, we work on the language pair Griko-Italian, for which there exists a sentence-aligned parallel corpus of source-language speech and target-language text (Lekakou et al., 2013). Griko is an endangered minority language spoken in the south of Italy. Using the method of Anastasopoulos et al. (2016), we also obtain speech-to-translation word-level alignments. The corpus that we work on already provides gold-standard transcriptions and speech-totranslation alignments, so it is suitable for conducting a case study that will examine the potential effect of providing the alignments on the crowdsourced transcriptions, as we will be able to compare directly against the gold standard. 170 Proceedings of the 2nd Workshop on the Use of Computational Methods in the Study of Endangered Languages, pages 170–178, c Honolulu, Hawai‘i, USA, March 6–7, 2017. 2017 Association for Computational Linguistic"
W17-0123,C14-1096,0,0.0296799,"ranscribing it, often at a phonetic level, as most of these languages do not have a writing system. This, however, is a costly and slow process, as it could take up to 1 hour for a trained linguist to transcribe the phonemes of 1 minute of speech (Thi-Ngoc-Diep Do and Castelli, 2014). Therefore, speech is more likely to be annotated with translations than with transcriptions. This translated speech is a potentially valuable source of information as it will make the collected corpus interpretable for future studies. New technologies are being developed to facilitate collection of translations (Bird et al., 2014), and there already exist recent examples of parallel speech collection efforts focused on endangered languages (Blachon et al., 2016; Adda et al., 2016). 2 Methodology As a proof-of-concept, we work on the language pair Griko-Italian, for which there exists a sentence-aligned parallel corpus of source-language speech and target-language text (Lekakou et al., 2013). Griko is an endangered minority language spoken in the south of Italy. Using the method of Anastasopoulos et al. (2016), we also obtain speech-to-translation word-level alignments. The corpus that we work on already provides gold-s"
W17-0123,N16-1109,1,0.888718,"Missing"
W17-4607,D16-1263,0,0.0546957,"Missing"
W17-4607,D16-1133,1,0.879375,"Lopez♦ David Chiang♠ ♠ Department of Computer Science and Engineering, University of Notre Dame ♦ School of Informatics, University of Edinburgh Abstract native speakers themselves (Bird et al., 2014; Blachon et al., 2016; Adda et al., 2016). Nevertheless, even translation takes time and language knowledge, so there may still be little translated data relative to the amount of recorded audio. An important goal, then, is to bootstrap language technology from this small parallel corpus in order to provide tools to annotate more data or make the data more searchable. We build on the approach of Anastasopoulos et al. (2016), who developed a system that performs joint inference to identify recurring segments of audio and cluster them while aligning them to words in a text translation. Here, we extend the method to be able to search for new instances of the latent clusters within the unlabeled audio, effectively providing keyword translations for some of the unlabeled speech. We evaluate our method on a Spanish-English corpus used in previous work, and on two datasets from endangered languages (narratives in Arapaho and Ainu). No previous computational methods have been tested on the latter data, to our knowledge."
W18-6313,D11-1033,0,0.0803148,"M 138.6 M De–En Subtitles 22.5 M 171.6 M Ko–En Subtitles 1.4 M 11.5 M Table 1: Number of parameters in each component. Korean, and Russian into English. Our out-ofdomain models are trained on WMT and/or subtitles corpora, and we adapt each model to translate patent abstracts. 2 Sentences Continued training has recently become a standard for domain or cross-lingual adaptation in several neural NLP applications. In PBMT, the most prominent methods focus on adapting the language model component (Moore and Lewis, 2010), and/or the translation model (Matsoukas et al., 2009; Mansour and Ney, 2014; Axelrod et al., 2011), or on interpolating in-domain and out-of-domain models (Lu et al., 2007; Foster et al., 2010; Koehn and Schroeder, 2007). In contrast, the methods employed in NMT tend to utilize continued training, which involves initializing the model with pre-trained weights (trained on out-of-domain data) and training/adapting it to the in-domain data. Among others, Luong and Manning (2015) and Freitag and Al-Onaizan (2016) applied this method for domain adaptation. Chu et al. (2017) mix in-domain and out-of-domain data during continued training in order to adapt to multiple domains. Continued training h"
W18-6313,P10-2016,0,0.057732,"Missing"
W18-6313,P17-2061,0,0.0587364,"model component (Moore and Lewis, 2010), and/or the translation model (Matsoukas et al., 2009; Mansour and Ney, 2014; Axelrod et al., 2011), or on interpolating in-domain and out-of-domain models (Lu et al., 2007; Foster et al., 2010; Koehn and Schroeder, 2007). In contrast, the methods employed in NMT tend to utilize continued training, which involves initializing the model with pre-trained weights (trained on out-of-domain data) and training/adapting it to the in-domain data. Among others, Luong and Manning (2015) and Freitag and Al-Onaizan (2016) applied this method for domain adaptation. Chu et al. (2017) mix in-domain and out-of-domain data during continued training in order to adapt to multiple domains. Continued training has also been applied to cross-lingual transfer learning for NMT, with Zoph et al. (2016) and Nguyen and Chiang (2017) using it for transfer between high- and lowresource language pairs. Continued training is effective on a range of data sizes. In-domain gains have been shown with as few as dozens of in-domain training sentences (Miceli Barone et al., 2017), and recent work has explored continued training on single sentences (Farajian et al., 2017; Kothur et al., 2018). Sim"
W18-6313,2010.iwslt-papers.5,1,0.80886,"ain adaptation methods, because PBMT adaptation methods primarily rely on adapting the language model and phrase table using interpolation or back-off schemes (see §2). Continued training (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016), also referred to as fine-tuning, is one of the most popular methods for NMT adaptation, due to its strong performance. In contrast to the PBMT literature, little research has focused on why continued training is effective or on what happens to NMT models during continued training. Motivated by domain adaptation analysis in PBMT (Haddow and Koehn, 2012; Duh et al., 2010; Irvine et al., 2013), this work proposes a simple freezing subnetworks technique and uses it to gain insight into how the various components of an NMT system behave during continued training. We segment the model into five subnetworks, which we refer to as components, denoted in Figure 1: the source embeddings, encoder, decoder (which includes the attention mechanism), the softmax (used to denote the decoder output embeddings and biases), and the target embeddings. We freeze components one at a time during continued training to see how much the adaptation depends on each component. We also e"
W18-6313,W18-2705,1,0.827312,"Missing"
W18-6313,W17-4713,0,0.124642,"method for domain adaptation. Chu et al. (2017) mix in-domain and out-of-domain data during continued training in order to adapt to multiple domains. Continued training has also been applied to cross-lingual transfer learning for NMT, with Zoph et al. (2016) and Nguyen and Chiang (2017) using it for transfer between high- and lowresource language pairs. Continued training is effective on a range of data sizes. In-domain gains have been shown with as few as dozens of in-domain training sentences (Miceli Barone et al., 2017), and recent work has explored continued training on single sentences (Farajian et al., 2017; Kothur et al., 2018). Similar adaptation techniques are also employed in the field of Automatic Speech Recognition, where continued training has been the basis of 595.9 M 212.4 M 131.8 M 185.8 M 11.9 M In-domain training sets Ru–En WIPO 29 k 620 k De–En WIPO 821 k 19 M Ko–En WIPO 81 k 2.2 M 812 k 23 M 2.0 M In-domain test sets 3k 82 k 3k 132 k 3k 187 k 109 k 162 k 165 k Ru–En WIPO De–En WIPO Ko–En WIPO Related Work Target Table 2: Dataset statistics. The number of tokens is computed before segmentation into subwords. The in-domain development sets (not shown) have similar statistics to the t"
W18-6313,D10-1044,0,0.0664357,"Missing"
W18-6313,W07-0733,1,0.69704,"t. Korean, and Russian into English. Our out-ofdomain models are trained on WMT and/or subtitles corpora, and we adapt each model to translate patent abstracts. 2 Sentences Continued training has recently become a standard for domain or cross-lingual adaptation in several neural NLP applications. In PBMT, the most prominent methods focus on adapting the language model component (Moore and Lewis, 2010), and/or the translation model (Matsoukas et al., 2009; Mansour and Ney, 2014; Axelrod et al., 2011), or on interpolating in-domain and out-of-domain models (Lu et al., 2007; Foster et al., 2010; Koehn and Schroeder, 2007). In contrast, the methods employed in NMT tend to utilize continued training, which involves initializing the model with pre-trained weights (trained on out-of-domain data) and training/adapting it to the in-domain data. Among others, Luong and Manning (2015) and Freitag and Al-Onaizan (2016) applied this method for domain adaptation. Chu et al. (2017) mix in-domain and out-of-domain data during continued training in order to adapt to multiple domains. Continued training has also been applied to cross-lingual transfer learning for NMT, with Zoph et al. (2016) and Nguyen and Chiang (2017) usin"
W18-6313,W12-3154,1,0.843127,"has necessitated new domain adaptation methods, because PBMT adaptation methods primarily rely on adapting the language model and phrase table using interpolation or back-off schemes (see §2). Continued training (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016), also referred to as fine-tuning, is one of the most popular methods for NMT adaptation, due to its strong performance. In contrast to the PBMT literature, little research has focused on why continued training is effective or on what happens to NMT models during continued training. Motivated by domain adaptation analysis in PBMT (Haddow and Koehn, 2012; Duh et al., 2010; Irvine et al., 2013), this work proposes a simple freezing subnetworks technique and uses it to gain insight into how the various components of an NMT system behave during continued training. We segment the model into five subnetworks, which we refer to as components, denoted in Figure 1: the source embeddings, encoder, decoder (which includes the attention mechanism), the softmax (used to denote the decoder output embeddings and biases), and the target embeddings. We freeze components one at a time during continued training to see how much the adaptation depends on each co"
W18-6313,E17-3017,0,0.0649422,"Missing"
W18-6313,Q13-1035,0,0.0453818,"Missing"
W18-6313,W18-2708,1,0.893091,"tation. Chu et al. (2017) mix in-domain and out-of-domain data during continued training in order to adapt to multiple domains. Continued training has also been applied to cross-lingual transfer learning for NMT, with Zoph et al. (2016) and Nguyen and Chiang (2017) using it for transfer between high- and lowresource language pairs. Continued training is effective on a range of data sizes. In-domain gains have been shown with as few as dozens of in-domain training sentences (Miceli Barone et al., 2017), and recent work has explored continued training on single sentences (Farajian et al., 2017; Kothur et al., 2018). Similar adaptation techniques are also employed in the field of Automatic Speech Recognition, where continued training has been the basis of 595.9 M 212.4 M 131.8 M 185.8 M 11.9 M In-domain training sets Ru–En WIPO 29 k 620 k De–En WIPO 821 k 19 M Ko–En WIPO 81 k 2.2 M 812 k 23 M 2.0 M In-domain test sets 3k 82 k 3k 132 k 3k 187 k 109 k 162 k 165 k Ru–En WIPO De–En WIPO Ko–En WIPO Related Work Target Table 2: Dataset statistics. The number of tokens is computed before segmentation into subwords. The in-domain development sets (not shown) have similar statistics to the test sets. 125 cross-li"
W18-6313,W17-2620,0,0.0293601,"field of Automatic Speech Recognition, where continued training has been the basis of 595.9 M 212.4 M 131.8 M 185.8 M 11.9 M In-domain training sets Ru–En WIPO 29 k 620 k De–En WIPO 821 k 19 M Ko–En WIPO 81 k 2.2 M 812 k 23 M 2.0 M In-domain test sets 3k 82 k 3k 132 k 3k 187 k 109 k 162 k 165 k Ru–En WIPO De–En WIPO Ko–En WIPO Related Work Target Table 2: Dataset statistics. The number of tokens is computed before segmentation into subwords. The in-domain development sets (not shown) have similar statistics to the test sets. 125 cross-lingual transfer learning approaches (Gr´ezl et al., 2014; Kunze et al., 2017). Usually, the lower layers of the network, which perform acoustic modeling, are frozen and only the upper layers are updated. In a similar vein, other works (Swietojanski and Renals, 2014; Vilar, 2018) adapt a network to a new domain by learning additional weights that re-scale the hidden units. 3 Data Our experiments are carried out across three language pairs, from Russian, Korean, and German into English. Basic statistics on the datasets used for our experiments are summarized in Table 2. The three languages represent three different domain adaptation scenarios: • In German, both the in- a"
W18-6313,L18-1146,0,0.0799127,"Missing"
W18-6313,L16-1147,0,0.0169409,"fied communication and sharing of information between the project partners enables the transfer of expertise in rural tourism. WIPO The films coated therewith, in particular polycarbonate films coated therewith, have improved properties with regard to scratch resistance, solvent resistance, and reduced oiling effect, said films thus being especially suitable for use in producing plastic parts in film insert molding methods. Table 3: Example sentences to illustrate domain differences. 3.1 Out-of-domain Data For our out-of-domain dataset we utilize the OpenSubtitles2018 corpus (Tiedemann, 2016; Lison and Tiedemann, 2016), which consists of translated movie subtitles.1 For De–En and Ru– En, we also use data from WMT 2017 (Bojar et al., 2017),2 which contains data from several sources: Europarl (parliamentary proceedings) (Koehn, 2005),3 News Commentary (political and economic news commentary),4 Common Crawl (web-crawled parallel corpus), and the EU Press Releases. We use the final 2500 lines of OpenSubtitles2018 for the development set. For German and Russian we also concatenate newstest2016 as part of the development set. newstest2016 consists of translated news articles released by WMT for its shared task. I"
W18-6313,D07-1036,0,0.042488,"Number of parameters in each component. Korean, and Russian into English. Our out-ofdomain models are trained on WMT and/or subtitles corpora, and we adapt each model to translate patent abstracts. 2 Sentences Continued training has recently become a standard for domain or cross-lingual adaptation in several neural NLP applications. In PBMT, the most prominent methods focus on adapting the language model component (Moore and Lewis, 2010), and/or the translation model (Matsoukas et al., 2009; Mansour and Ney, 2014; Axelrod et al., 2011), or on interpolating in-domain and out-of-domain models (Lu et al., 2007; Foster et al., 2010; Koehn and Schroeder, 2007). In contrast, the methods employed in NMT tend to utilize continued training, which involves initializing the model with pre-trained weights (trained on out-of-domain data) and training/adapting it to the in-domain data. Among others, Luong and Manning (2015) and Freitag and Al-Onaizan (2016) applied this method for domain adaptation. Chu et al. (2017) mix in-domain and out-of-domain data during continued training in order to adapt to multiple domains. Continued training has also been applied to cross-lingual transfer learning for NMT, with Zop"
W18-6313,2015.iwslt-evaluation.11,0,0.369812,"t the out-ofdomain model can provide a good generic initialization for the new domain. 1 hands your Wash Decoder Encoder Source Embedding Wasch dir die Hände Figure 1: Visualization of an NMT system segmented into components. Introduction Neural Machine Translation (NMT) has supplanted Phrase-Based Machine Translation (PBMT) as the standard for high-resource machine translation. This has necessitated new domain adaptation methods, because PBMT adaptation methods primarily rely on adapting the language model and phrase table using interpolation or back-off schemes (see §2). Continued training (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016), also referred to as fine-tuning, is one of the most popular methods for NMT adaptation, due to its strong performance. In contrast to the PBMT literature, little research has focused on why continued training is effective or on what happens to NMT models during continued training. Motivated by domain adaptation analysis in PBMT (Haddow and Koehn, 2012; Duh et al., 2010; Irvine et al., 2013), this work proposes a simple freezing subnetworks technique and uses it to gain insight into how the various components of an NMT system behave during continued training. We"
W18-6313,D15-1166,0,0.13368,"Missing"
W18-6313,N18-2080,0,0.103829,"2.2 M 812 k 23 M 2.0 M In-domain test sets 3k 82 k 3k 132 k 3k 187 k 109 k 162 k 165 k Ru–En WIPO De–En WIPO Ko–En WIPO Related Work Target Table 2: Dataset statistics. The number of tokens is computed before segmentation into subwords. The in-domain development sets (not shown) have similar statistics to the test sets. 125 cross-lingual transfer learning approaches (Gr´ezl et al., 2014; Kunze et al., 2017). Usually, the lower layers of the network, which perform acoustic modeling, are frozen and only the upper layers are updated. In a similar vein, other works (Swietojanski and Renals, 2014; Vilar, 2018) adapt a network to a new domain by learning additional weights that re-scale the hidden units. 3 Data Our experiments are carried out across three language pairs, from Russian, Korean, and German into English. Basic statistics on the datasets used for our experiments are summarized in Table 2. The three languages represent three different domain adaptation scenarios: • In German, both the in- and out-of-domain datasets are large. • In Russian, the in-domain dataset is large but the out-of-domain dataset is small. • In Korean, both in- and out-of-domain datasets are small. OpenSubtitles You’re"
W18-6313,2014.eamt-1.6,0,0.0183159,"M 179.8 M De–En WMT 5.8 M 138.6 M De–En Subtitles 22.5 M 171.6 M Ko–En Subtitles 1.4 M 11.5 M Table 1: Number of parameters in each component. Korean, and Russian into English. Our out-ofdomain models are trained on WMT and/or subtitles corpora, and we adapt each model to translate patent abstracts. 2 Sentences Continued training has recently become a standard for domain or cross-lingual adaptation in several neural NLP applications. In PBMT, the most prominent methods focus on adapting the language model component (Moore and Lewis, 2010), and/or the translation model (Matsoukas et al., 2009; Mansour and Ney, 2014; Axelrod et al., 2011), or on interpolating in-domain and out-of-domain models (Lu et al., 2007; Foster et al., 2010; Koehn and Schroeder, 2007). In contrast, the methods employed in NMT tend to utilize continued training, which involves initializing the model with pre-trained weights (trained on out-of-domain data) and training/adapting it to the in-domain data. Among others, Luong and Manning (2015) and Freitag and Al-Onaizan (2016) applied this method for domain adaptation. Chu et al. (2017) mix in-domain and out-of-domain data during continued training in order to adapt to multiple domain"
W18-6313,D16-1163,0,0.0343017,"007; Foster et al., 2010; Koehn and Schroeder, 2007). In contrast, the methods employed in NMT tend to utilize continued training, which involves initializing the model with pre-trained weights (trained on out-of-domain data) and training/adapting it to the in-domain data. Among others, Luong and Manning (2015) and Freitag and Al-Onaizan (2016) applied this method for domain adaptation. Chu et al. (2017) mix in-domain and out-of-domain data during continued training in order to adapt to multiple domains. Continued training has also been applied to cross-lingual transfer learning for NMT, with Zoph et al. (2016) and Nguyen and Chiang (2017) using it for transfer between high- and lowresource language pairs. Continued training is effective on a range of data sizes. In-domain gains have been shown with as few as dozens of in-domain training sentences (Miceli Barone et al., 2017), and recent work has explored continued training on single sentences (Farajian et al., 2017; Kothur et al., 2018). Similar adaptation techniques are also employed in the field of Automatic Speech Recognition, where continued training has been the basis of 595.9 M 212.4 M 131.8 M 185.8 M 11.9 M In-domain training sets Ru–En WIPO"
W18-6313,D09-1074,0,0.0328423,"M Ru–En Subtitles 25.9 M 179.8 M De–En WMT 5.8 M 138.6 M De–En Subtitles 22.5 M 171.6 M Ko–En Subtitles 1.4 M 11.5 M Table 1: Number of parameters in each component. Korean, and Russian into English. Our out-ofdomain models are trained on WMT and/or subtitles corpora, and we adapt each model to translate patent abstracts. 2 Sentences Continued training has recently become a standard for domain or cross-lingual adaptation in several neural NLP applications. In PBMT, the most prominent methods focus on adapting the language model component (Moore and Lewis, 2010), and/or the translation model (Matsoukas et al., 2009; Mansour and Ney, 2014; Axelrod et al., 2011), or on interpolating in-domain and out-of-domain models (Lu et al., 2007; Foster et al., 2010; Koehn and Schroeder, 2007). In contrast, the methods employed in NMT tend to utilize continued training, which involves initializing the model with pre-trained weights (trained on out-of-domain data) and training/adapting it to the in-domain data. Among others, Luong and Manning (2015) and Freitag and Al-Onaizan (2016) applied this method for domain adaptation. Chu et al. (2017) mix in-domain and out-of-domain data during continued training in order to a"
W18-6313,D17-1156,0,0.0510244,"data. Among others, Luong and Manning (2015) and Freitag and Al-Onaizan (2016) applied this method for domain adaptation. Chu et al. (2017) mix in-domain and out-of-domain data during continued training in order to adapt to multiple domains. Continued training has also been applied to cross-lingual transfer learning for NMT, with Zoph et al. (2016) and Nguyen and Chiang (2017) using it for transfer between high- and lowresource language pairs. Continued training is effective on a range of data sizes. In-domain gains have been shown with as few as dozens of in-domain training sentences (Miceli Barone et al., 2017), and recent work has explored continued training on single sentences (Farajian et al., 2017; Kothur et al., 2018). Similar adaptation techniques are also employed in the field of Automatic Speech Recognition, where continued training has been the basis of 595.9 M 212.4 M 131.8 M 185.8 M 11.9 M In-domain training sets Ru–En WIPO 29 k 620 k De–En WIPO 821 k 19 M Ko–En WIPO 81 k 2.2 M 812 k 23 M 2.0 M In-domain test sets 3k 82 k 3k 132 k 3k 187 k 109 k 162 k 165 k Ru–En WIPO De–En WIPO Ko–En WIPO Related Work Target Table 2: Dataset statistics. The number of tokens is computed before segmentatio"
W18-6313,P18-2050,0,0.0137192,"hat is, continued training is able to adapt the overall system to a new domain by modifying only parameters in a single component. This finding goes against the intuitive hypothesis that source embeddings must account for domain changes in the source vocabulary, target embeddings must account for changes in the target vocabulary, etc. We note that the encoder and decoder, despite having the least parameters (3.7M and 6.8M, respectively, out of 56M), perform strongly across all languages. This suggests further work on adapting only a subset of parameters may be warranted (see also Vilar, 2018; Michel and Neubig, 2018). 129 Acknowledgements Softmax Encoder Decoder Source Embed Target Embed 30 20 The authors would like to thank Lane Schwartz and Graham Neubig for their roles in organizing the MT Marathon in the Americas (MTMA), where this work began. The authors would also like to thank Michael Denkowski and David Vilar for assistance with S OCKEYE. This material is based upon work supported in part by the DARPA LORELEI and IARPA MATERIAL programs. Brian Thompson is supported by the Department of Defense through the National Defense Science & Engineering Graduate Fellowship (NDSEG) Program. Antonios Anastaso"
W18-6313,P10-2041,0,0.0713404,"rce Out-of-domain training sets Ru–En WMT 25.2 M 563.9 M Ru–En Subtitles 25.9 M 179.8 M De–En WMT 5.8 M 138.6 M De–En Subtitles 22.5 M 171.6 M Ko–En Subtitles 1.4 M 11.5 M Table 1: Number of parameters in each component. Korean, and Russian into English. Our out-ofdomain models are trained on WMT and/or subtitles corpora, and we adapt each model to translate patent abstracts. 2 Sentences Continued training has recently become a standard for domain or cross-lingual adaptation in several neural NLP applications. In PBMT, the most prominent methods focus on adapting the language model component (Moore and Lewis, 2010), and/or the translation model (Matsoukas et al., 2009; Mansour and Ney, 2014; Axelrod et al., 2011), or on interpolating in-domain and out-of-domain models (Lu et al., 2007; Foster et al., 2010; Koehn and Schroeder, 2007). In contrast, the methods employed in NMT tend to utilize continued training, which involves initializing the model with pre-trained weights (trained on out-of-domain data) and training/adapting it to the in-domain data. Among others, Luong and Manning (2015) and Freitag and Al-Onaizan (2016) applied this method for domain adaptation. Chu et al. (2017) mix in-domain and out-"
W18-6313,I17-2050,0,0.0190351,"0; Koehn and Schroeder, 2007). In contrast, the methods employed in NMT tend to utilize continued training, which involves initializing the model with pre-trained weights (trained on out-of-domain data) and training/adapting it to the in-domain data. Among others, Luong and Manning (2015) and Freitag and Al-Onaizan (2016) applied this method for domain adaptation. Chu et al. (2017) mix in-domain and out-of-domain data during continued training in order to adapt to multiple domains. Continued training has also been applied to cross-lingual transfer learning for NMT, with Zoph et al. (2016) and Nguyen and Chiang (2017) using it for transfer between high- and lowresource language pairs. Continued training is effective on a range of data sizes. In-domain gains have been shown with as few as dozens of in-domain training sentences (Miceli Barone et al., 2017), and recent work has explored continued training on single sentences (Farajian et al., 2017; Kothur et al., 2018). Similar adaptation techniques are also employed in the field of Automatic Speech Recognition, where continued training has been the basis of 595.9 M 212.4 M 131.8 M 185.8 M 11.9 M In-domain training sets Ru–En WIPO 29 k 620 k De–En WIPO 821 k"
W18-6313,P16-1162,0,0.0613931,"based on the WIPO development set perplexity and report results on the WIPO test sets. In-domain Data We perform adaptation into the World International Property Organization (WIPO) COPPA-V2 dataset (Junczys-Dowmunt et al., 2016).5 The WIPO data consist of parallel sentences from international patent application abstracts. We reserve 3000 lines each for the in-domain development and test sets. See Table 3 for an example WIPO sentence. 3.3 mented into words using the KoNLPy wrapper of the Mecab-Ko segmenter.7 As a final preprocessing step, we train Byte Pair Encoding (BPE) segmentation models (Sennrich et al., 2016) on the out-of-domain training corpus. We train separate BPE models for each language, each with a vocabulary size of 30,000. For each language, BPE is trained on the out-of-domain corpus only and then applied to the training, development, and test data for both out-of-domain and in-domain datasets. This mimics the realistic setting where a generic, computationally-expensive-to-train NMT model is trained once. This NMT model is then adapted to new domains as they emerge, without retraining on the out-of-domain corpus. Training BPE on the in-domain data would change the vocabulary and thus requ"
W18-6313,L16-1559,0,0.0130136,"boat. WMT Intensified communication and sharing of information between the project partners enables the transfer of expertise in rural tourism. WIPO The films coated therewith, in particular polycarbonate films coated therewith, have improved properties with regard to scratch resistance, solvent resistance, and reduced oiling effect, said films thus being especially suitable for use in producing plastic parts in film insert molding methods. Table 3: Example sentences to illustrate domain differences. 3.1 Out-of-domain Data For our out-of-domain dataset we utilize the OpenSubtitles2018 corpus (Tiedemann, 2016; Lison and Tiedemann, 2016), which consists of translated movie subtitles.1 For De–En and Ru– En, we also use data from WMT 2017 (Bojar et al., 2017),2 which contains data from several sources: Europarl (parliamentary proceedings) (Koehn, 2005),3 News Commentary (political and economic news commentary),4 Common Crawl (web-crawled parallel corpus), and the EU Press Releases. We use the final 2500 lines of OpenSubtitles2018 for the development set. For German and Russian we also concatenate newstest2016 as part of the development set. newstest2016 consists of translated news articles released b"
W18-6313,2005.mtsummit-papers.11,1,\N,Missing
W19-4822,N19-1311,1,0.857744,"). However, as several works have shown, it has a notable shortcoming (among others, see Koehn and Knowles (2017) for relevant discussion) in dealing with source-side noise, during both training and inference. Heigold et al. (2018) as well as Belinkov and Bisk (2018) pointed out the degraded performance of character- and subword-level NMT models when confronted with synthetic character-level noise –like swaps and scrambling– on French, German, and Czech to English MT. Belinkov and Bisk (2018) and Cheng et al. (2018) also studied synthetic errors from word swaps extracted from Wikipedia edits. Anastasopoulos et al. (2019) focused on a small subset of grammatical errors (article, preposition, noun number, and subject-verb agreement) and evaluated on English-to-Spanish synthetic and natural data. 2 Data and Experimental Settings To our knowledge, there are six publicly available corpora of non-native or erroneous English that are annotated with corrections and which have been widely used for research in GEC. The NUS Corpus of Learner English (NUCLE) contains essays written by students at the National University of Singapore (Dahlmeier et al., 2013). It has become the main benchmark for GEC, as it was used in the"
W19-4822,W13-1704,0,0.0164572,"rpreting Neural Networks for NLP, pages 213–223 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics (ESOL) exams.2 The Lang-8 corpus (Tajiri et al., 2012) was harvested from user-provided corrections in an online learner forum. Both have also been widely used for the GEC Shared Tasks. Another small corpus developed for evaluation purposes is the JHU FLuency-Extended GUG corpus (JFLEG) (Napoles et al., 2017) with correction annotations that include extended fluency edits rather than just minimal grammatical ones. The Cambridge English Write & Improve (W&I) corpus (Andersen et al., 2013) is collected from an online platform where English learners submit text and professional annotators correct them, also assigning a CEFR level of proficiency (of Europe. Council for Cultural Co-operation. Education Committee. Modern Languages Division, 2001). Lastly, we use a portion of the LOCNESS corpus,3 a collection of essays written by native English speakers. 50 essays from LOCNESS were annotated by W&I annotators for grammatical errors, so we will jointly refer to these two corpora as WI+loc. All datasets were consistently annotated for errors with ERRANT (Bryant et al., 2017), an autom"
W19-4822,W05-0909,0,0.0137113,"simple. We use the SOTA NMT system of Edunov et al. (2018) for translating both the original and the corrected English sentences for all our GEC corpora.4 The system achieved the best performance in the WMT 2018 evaluation campaign, using an ensemble of 6 deep transformer models trained with slightly different backtranslated data.5 3 Evaluating NMT Robustness without References When not using human judgments on output fluency and adequacy, Machine Translation is typically evaluated against gold-standard reference translations with automated metrics like BLEU (Papineni et al., 2002) or METEOR (Banerjee and Lavie, 2005). However, in the case of GEC corpora, we do not have access to translations – only monolingual data (potentially with ungrammaticalities) and correction annotations.6 Quality Estimation for MT also operates in a reference-less setting (see Specia et al. (2018) for definitions and an overview of the field) and is hence very related to our work, but is more aimed towards towards predicting the quality of the translation. Our goal instead, is to analyze the behavior of the MT system when confronted with ungrammatical input. Reference-less evaluation has also been proposed for text simplification"
W19-4822,W18-1807,0,0.247462,"also introduce a technique for visualizing the divergence distribution caused by a sourceside error, which allows for additional insights. 1 Introduction Neural Machine Translation (NMT) has become the de facto option for industrial systems in high-resource settings (Wu et al., 2016; Hassan Awadalla et al., 2018; Crego et al., 2016) while dominating public benchmarks (Bojar et al., 2018). However, as several works have shown, it has a notable shortcoming (among others, see Koehn and Knowles (2017) for relevant discussion) in dealing with source-side noise, during both training and inference. Heigold et al. (2018) as well as Belinkov and Bisk (2018) pointed out the degraded performance of character- and subword-level NMT models when confronted with synthetic character-level noise –like swaps and scrambling– on French, German, and Czech to English MT. Belinkov and Bisk (2018) and Cheng et al. (2018) also studied synthetic errors from word swaps extracted from Wikipedia edits. Anastasopoulos et al. (2019) focused on a small subset of grammatical errors (article, preposition, noun number, and subject-verb agreement) and evaluated on English-to-Spanish synthetic and natural data. 2 Data and Experimental Se"
W19-4822,W17-3204,0,0.0485133,"ences, and we use them for extensive analysis of the effects that different grammatical errors have on the NMT output. We also introduce a technique for visualizing the divergence distribution caused by a sourceside error, which allows for additional insights. 1 Introduction Neural Machine Translation (NMT) has become the de facto option for industrial systems in high-resource settings (Wu et al., 2016; Hassan Awadalla et al., 2018; Crego et al., 2016) while dominating public benchmarks (Bojar et al., 2018). However, as several works have shown, it has a notable shortcoming (among others, see Koehn and Knowles (2017) for relevant discussion) in dealing with source-side noise, during both training and inference. Heigold et al. (2018) as well as Belinkov and Bisk (2018) pointed out the degraded performance of character- and subword-level NMT models when confronted with synthetic character-level noise –like swaps and scrambling– on French, German, and Czech to English MT. Belinkov and Bisk (2018) and Cheng et al. (2018) also studied synthetic errors from word swaps extracted from Wikipedia edits. Anastasopoulos et al. (2019) focused on a small subset of grammatical errors (article, preposition, noun number,"
W19-4822,L16-1582,0,0.0179562,"e-level ones. The inclusion of f-METEOR as a robustness metric could partially deal with this issue, as it would not penalize such differences. We do believe it is still interesting, though, that a single source error can cause large perturbations in the output, as in the case of errors with large variance in their divergence distribution. Nevertheless, an extension of our study focusing on the grammatical qualities of the MT output would be exciting and automated tools for such analysis would be invaluable (i.e. MT error labeling and analysis tools extending the works of Zeman et al. (2011), Logacheva et al. (2016), Popovi´c (2018), or Neubig et al. (2019)). A natural next research direction is investigating how to use our reference-less evaluation metrics in order to create a more robust MT system. For instance, one could optimize for f-BLEU or any of the other reference-less measures that we proposed, in the same way that an MT system is optimized for BLEU (either by explicitly using 6 Conclusion In this work, we studied the effects of grammatical errors in NMT. We expanded on findings from previous work, performing analysis on real data with grammatical errors using a SOTA system. With our analysis w"
W19-4822,J93-2003,0,0.0653691,"t tokens, which are by construction more vulnerable to such errors. 4.4 Divergence We introduce a method for computing a divergence distribution. Computing divergence requires a quadruple of (x, x˜ , y, y˜ ). We will focus on instances where x and x˜ differ only with a single edit, as a simple working example. Process: Given a source side sentence pair x and x˜ with a single grammatical error, it is trivial to identify the position i∗ of the correction in x˜ , since we work on corpora pre-annotated with grammatical edits at the token level. Also, using traditional methods like the IBM Models (Brown et al., 1993) and the GIZA++ tool, makes it easy to obtain an alignment between x and y, as well as between x˜ and y˜ . We use the alignment variable α j = i to denote that the target word y j is aligned to source position i, and equivalently the variables α˜ for the corrected source pair. We denote as k∗ the target position that aligns to the source-side correction, such that α˜ k∗ = i∗ . We define the set of divergent tokens Y∗ as the set of tokens of y that do not appear in y˜ : Across all datasets and errors, the distribution of the divergence caused by single errors in the source has a mean µall =0.7,"
W19-4822,N19-1314,0,0.0297372,"assumption, we can now evaluate our system’s robustness by comparing y and y˜ using automated metrics at the corpus or sentence level. Here we list the metrics that we use and briefly discuss their potential shortcomings. NR(x, x˜ , y, y˜ ) = If the average (corpus-level) Noise Ratio score is ˜ Y, Y) ˜ &lt; 1) then we can insmaller than 1 (NR(X, X, fer that the MT system reduces the relative amount of noise, as there is higher relative n-gram overlap between the outputs than the inputs. On the other hand, if it is larger than 1, then the MT system must have introduced even more noise.8 Recently, Michel et al. (2019) proposed a criterion for evaluating adversarial attacks, which requires also having access to the correct translation yˆ . Using a similarity function s(·), they declare an adversarial attack to be successful when: Robustness Percentage (RB): Given a GEC ˜ this corpus-level metric evaluates corpus {X, X}, the percentage at which the system outputs agree at the sentence level: P x)) ˜ cagree (MT(x), MT(˜ x,˜x∈{X,X} RB = , |X|    1 cagree (y, y˜ ) =   0 d(y, y˜ ) 100 − BLEU(y, y˜ ) = . d(x, x˜ ) 100 − BLEU(x, x˜ ) if y = y˜ , otherwise. s(x, x˜ ) + f-BLEU: BLEU is the most standard MT ev"
W19-4822,P17-1074,0,0.0310854,"corpus (Andersen et al., 2013) is collected from an online platform where English learners submit text and professional annotators correct them, also assigning a CEFR level of proficiency (of Europe. Council for Cultural Co-operation. Education Committee. Modern Languages Division, 2001). Lastly, we use a portion of the LOCNESS corpus,3 a collection of essays written by native English speakers. 50 essays from LOCNESS were annotated by W&I annotators for grammatical errors, so we will jointly refer to these two corpora as WI+loc. All datasets were consistently annotated for errors with ERRANT (Bryant et al., 2017), an automatic tool that categorizes correction edits.3 This allows us to consistently aggregate results and analysis across all datasets. 2.1 be equivalent to the number of annotated necessary edits that the source x requires to be deemed grammatical (˜x), as per standard GEC literature. The main focus of our work is the performance analysis of the NMT system, so our experimental design is fairly simple. We use the SOTA NMT system of Edunov et al. (2018) for translating both the original and the corrected English sentences for all our GEC corpora.4 The system achieved the best performance in"
W19-4822,D16-1228,0,0.0208948,"rpora, we do not have access to translations – only monolingual data (potentially with ungrammaticalities) and correction annotations.6 Quality Estimation for MT also operates in a reference-less setting (see Specia et al. (2018) for definitions and an overview of the field) and is hence very related to our work, but is more aimed towards towards predicting the quality of the translation. Our goal instead, is to analyze the behavior of the MT system when confronted with ungrammatical input. Reference-less evaluation has also been proposed for text simplification (Martin et al., 2018) and GEC (Napoles et al., 2016), while the grammaticality of MT systems’ outputs has been evaluated with target-side contrastive pairs (Sennrich, 2017). In this work, the core of our evaluation of a system’s robustness lies in the following observation: a perfectly robust-to-noise MT system would produce the exact same output for the clean and erroneous versions of the same input sentence. Notation and Experimental Settings Throughout this work, we use the following notations: • x: the original, noisy, potentially ungrammatical English sentence. Its tokens will be denoted as xi . • x˜ : the English sentence with the correct"
W19-4822,P18-1163,0,0.163202,"Awadalla et al., 2018; Crego et al., 2016) while dominating public benchmarks (Bojar et al., 2018). However, as several works have shown, it has a notable shortcoming (among others, see Koehn and Knowles (2017) for relevant discussion) in dealing with source-side noise, during both training and inference. Heigold et al. (2018) as well as Belinkov and Bisk (2018) pointed out the degraded performance of character- and subword-level NMT models when confronted with synthetic character-level noise –like swaps and scrambling– on French, German, and Czech to English MT. Belinkov and Bisk (2018) and Cheng et al. (2018) also studied synthetic errors from word swaps extracted from Wikipedia edits. Anastasopoulos et al. (2019) focused on a small subset of grammatical errors (article, preposition, noun number, and subject-verb agreement) and evaluated on English-to-Spanish synthetic and natural data. 2 Data and Experimental Settings To our knowledge, there are six publicly available corpora of non-native or erroneous English that are annotated with corrections and which have been widely used for research in GEC. The NUS Corpus of Learner English (NUCLE) contains essays written by students at the National Univer"
W19-4822,E17-2037,0,0.0241972,"ng the Cambridge Assessment’s English as a Second or Other Language 1 We use the publicly available portion. 213 Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 213–223 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics (ESOL) exams.2 The Lang-8 corpus (Tajiri et al., 2012) was harvested from user-provided corrections in an online learner forum. Both have also been widely used for the GEC Shared Tasks. Another small corpus developed for evaluation purposes is the JHU FLuency-Extended GUG corpus (JFLEG) (Napoles et al., 2017) with correction annotations that include extended fluency edits rather than just minimal grammatical ones. The Cambridge English Write & Improve (W&I) corpus (Andersen et al., 2013) is collected from an online platform where English learners submit text and professional annotators correct them, also assigning a CEFR level of proficiency (of Europe. Council for Cultural Co-operation. Education Committee. Modern Languages Division, 2001). Lastly, we use a portion of the LOCNESS corpus,3 a collection of essays written by native English speakers. 50 essays from LOCNESS were annotated by W&I annot"
W19-4822,N19-4007,0,0.0191346,"robustness metric could partially deal with this issue, as it would not penalize such differences. We do believe it is still interesting, though, that a single source error can cause large perturbations in the output, as in the case of errors with large variance in their divergence distribution. Nevertheless, an extension of our study focusing on the grammatical qualities of the MT output would be exciting and automated tools for such analysis would be invaluable (i.e. MT error labeling and analysis tools extending the works of Zeman et al. (2011), Logacheva et al. (2016), Popovi´c (2018), or Neubig et al. (2019)). A natural next research direction is investigating how to use our reference-less evaluation metrics in order to create a more robust MT system. For instance, one could optimize for f-BLEU or any of the other reference-less measures that we proposed, in the same way that an MT system is optimized for BLEU (either by explicitly using 6 Conclusion In this work, we studied the effects of grammatical errors in NMT. We expanded on findings from previous work, performing analysis on real data with grammatical errors using a SOTA system. With our analysis we were able to identify classes of grammat"
W19-4822,W13-1703,0,0.0782339,"etic errors from word swaps extracted from Wikipedia edits. Anastasopoulos et al. (2019) focused on a small subset of grammatical errors (article, preposition, noun number, and subject-verb agreement) and evaluated on English-to-Spanish synthetic and natural data. 2 Data and Experimental Settings To our knowledge, there are six publicly available corpora of non-native or erroneous English that are annotated with corrections and which have been widely used for research in GEC. The NUS Corpus of Learner English (NUCLE) contains essays written by students at the National University of Singapore (Dahlmeier et al., 2013). It has become the main benchmark for GEC, as it was used in the CoNLL GEC Shared Tasks (Ng et al., 2013, 2014). The Cambridge Learner Corpus First Certificate in English FCE corpus1 (Yannakoudakis et al., 2011) consists of essays collected from learners taking the Cambridge Assessment’s English as a Second or Other Language 1 We use the publicly available portion. 213 Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 213–223 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics (ESOL) exams.2 The Lang-8 cor"
W19-4822,W14-1701,0,0.0660307,"Missing"
W19-4822,W14-3348,0,0.0532637,"parable with any previous work (as we do not use real references) so we denote our metric as faux BLEU (fBLEU) to avoid confusion.7 s(y, yˆ ) − s(˜y, yˆ ) &gt;1 s(y, yˆ ) In our reference-less setting, assuming yˆ ≈ y˜ leads to s(˜y, yˆ ) = 1. Finally, representing the similarity function with a distance function s(·) = 1 − d(·) and simple equation manipulation, the criterion becomes exactly our Target-Source Noise Ratio. We have, hence, arrived at a reference-less criterion for evaluating any kind of adversarial attacks.9 f-METEOR: Same as above, we define fauxMETEOR using the METEOR MT metric (Denkowski and Lavie, 2014) which is more semantically nuanced than BLEU. 4 Analysis We first review the aggregate results across all datasets (§4.1) and with all metrics. We also 8 7 As presented, the NR metric assumes that the length of the input and target sentences are comparable. In the EnglishGerman case, this is more or less correct. A more general implementation could include a discount term based on the average sentence length ratio of the two languages. 9 Indeed, grammatical noise is nothing more than natural occurring adversarial noise. In absolute numbers, we obtain higher scores than the scores of a MT syst"
W19-4822,W13-3601,0,0.0267662,"t of grammatical errors (article, preposition, noun number, and subject-verb agreement) and evaluated on English-to-Spanish synthetic and natural data. 2 Data and Experimental Settings To our knowledge, there are six publicly available corpora of non-native or erroneous English that are annotated with corrections and which have been widely used for research in GEC. The NUS Corpus of Learner English (NUCLE) contains essays written by students at the National University of Singapore (Dahlmeier et al., 2013). It has become the main benchmark for GEC, as it was used in the CoNLL GEC Shared Tasks (Ng et al., 2013, 2014). The Cambridge Learner Corpus First Certificate in English FCE corpus1 (Yannakoudakis et al., 2011) consists of essays collected from learners taking the Cambridge Assessment’s English as a Second or Other Language 1 We use the publicly available portion. 213 Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 213–223 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics (ESOL) exams.2 The Lang-8 corpus (Tajiri et al., 2012) was harvested from user-provided corrections in an online learner forum. Both h"
W19-4822,P02-1040,0,0.107633,"our experimental design is fairly simple. We use the SOTA NMT system of Edunov et al. (2018) for translating both the original and the corrected English sentences for all our GEC corpora.4 The system achieved the best performance in the WMT 2018 evaluation campaign, using an ensemble of 6 deep transformer models trained with slightly different backtranslated data.5 3 Evaluating NMT Robustness without References When not using human judgments on output fluency and adequacy, Machine Translation is typically evaluated against gold-standard reference translations with automated metrics like BLEU (Papineni et al., 2002) or METEOR (Banerjee and Lavie, 2005). However, in the case of GEC corpora, we do not have access to translations – only monolingual data (potentially with ungrammaticalities) and correction annotations.6 Quality Estimation for MT also operates in a reference-less setting (see Specia et al. (2018) for definitions and an overview of the field) and is hence very related to our work, but is more aimed towards towards predicting the quality of the translation. Our goal instead, is to analyze the behavior of the MT system when confronted with ungrammatical input. Reference-less evaluation has also"
W19-4822,D18-1045,0,0.414438,"rammatical errors, so we will jointly refer to these two corpora as WI+loc. All datasets were consistently annotated for errors with ERRANT (Bryant et al., 2017), an automatic tool that categorizes correction edits.3 This allows us to consistently aggregate results and analysis across all datasets. 2.1 be equivalent to the number of annotated necessary edits that the source x requires to be deemed grammatical (˜x), as per standard GEC literature. The main focus of our work is the performance analysis of the NMT system, so our experimental design is fairly simple. We use the SOTA NMT system of Edunov et al. (2018) for translating both the original and the corrected English sentences for all our GEC corpora.4 The system achieved the best performance in the WMT 2018 evaluation campaign, using an ensemble of 6 deep transformer models trained with slightly different backtranslated data.5 3 Evaluating NMT Robustness without References When not using human judgments on output fluency and adequacy, Machine Translation is typically evaluated against gold-standard reference translations with automated metrics like BLEU (Papineni et al., 2002) or METEOR (Banerjee and Lavie, 2005). However, in the case of GEC cor"
W19-4822,E17-2060,0,0.0185244,"tations.6 Quality Estimation for MT also operates in a reference-less setting (see Specia et al. (2018) for definitions and an overview of the field) and is hence very related to our work, but is more aimed towards towards predicting the quality of the translation. Our goal instead, is to analyze the behavior of the MT system when confronted with ungrammatical input. Reference-less evaluation has also been proposed for text simplification (Martin et al., 2018) and GEC (Napoles et al., 2016), while the grammaticality of MT systems’ outputs has been evaluated with target-side contrastive pairs (Sennrich, 2017). In this work, the core of our evaluation of a system’s robustness lies in the following observation: a perfectly robust-to-noise MT system would produce the exact same output for the clean and erroneous versions of the same input sentence. Notation and Experimental Settings Throughout this work, we use the following notations: • x: the original, noisy, potentially ungrammatical English sentence. Its tokens will be denoted as xi . • x˜ : the English sentence with the correction annotations applied to the original sentence x, which is deemed fluent and grammatical. Again, its tokens will be de"
W19-4822,P16-1009,0,0.186768,"anguage Technologies Institute Carnegie Mellon University aanastas@cs.cmu.edu Abstract However, no previous work has extensively studied the behavior of a state-of-the-art (SOTA) model on natural occurring data. Belinkov and Bisk (2018) only trained their systems on about 200K parallel instances, while Heigold et al. (2018) and Anastasopoulos et al. (2019) trained on about 2M parallel sentences from the WMT’16 data. Importantly, though, none of them utilized vast monolingual resources through backtranslation, a technique that has been consistently shown to lead to impressively better results (Sennrich et al., 2016a). In this work, we perform an extensive analysis of the performance of a state-of-the-art EnglishGerman NMT system, with regards to its robustness against real grammatical noise. We propose a method for robustness evaluation without goldstandard translation references, and perform experiments and extensive analysis on all available English Grammar Error Correction (GEC) corpora. Finally, we introduce a visualization technique for performing further analysis. The quality of Neural Machine Translation (NMT) has been shown to significantly degrade when confronted with source-side noise. We pres"
W19-4822,P16-1162,0,0.241945,"anguage Technologies Institute Carnegie Mellon University aanastas@cs.cmu.edu Abstract However, no previous work has extensively studied the behavior of a state-of-the-art (SOTA) model on natural occurring data. Belinkov and Bisk (2018) only trained their systems on about 200K parallel instances, while Heigold et al. (2018) and Anastasopoulos et al. (2019) trained on about 2M parallel sentences from the WMT’16 data. Importantly, though, none of them utilized vast monolingual resources through backtranslation, a technique that has been consistently shown to lead to impressively better results (Sennrich et al., 2016a). In this work, we perform an extensive analysis of the performance of a state-of-the-art EnglishGerman NMT system, with regards to its robustness against real grammatical noise. We propose a method for robustness evaluation without goldstandard translation references, and perform experiments and extensive analysis on all available English Grammar Error Correction (GEC) corpora. Finally, we introduce a visualization technique for performing further analysis. The quality of Neural Machine Translation (NMT) has been shown to significantly degrade when confronted with source-side noise. We pres"
W19-4822,N19-1190,0,0.0542316,"axis is centered around the token y˜ k that aligns to the edit xi *→ x˜ j . The counts describe the caused divergence relative to the expected error’s position. sive amounts of back-translated data, where German monolingual data have been translated into English. The integral part is that English sources were sampled from the De-En model, instead of using beam-search to generate the most likely output. This means that the model was already trained on a fair amount of source-side noise, which would make it more robust to such perturbations (Belinkov and Bisk, 2018; Anastasopoulos et al., 2019; Singh et al., 2019). Although we do not have access to the backtranslated parallel data that Edunov et al. (2018) used, we suspect that translation errors are fairly common and therefore more prevalent in the final training bitext, making the model more robust to such noise. Current English-to-German SOTA systems might not have issues with translating noun phrases, coordinated verbs, or pronoun number, but they still struggle with compound generation, coreference, and verb-future generation (Bojar et al., 2018). source sentence in the target language. However, there are errors where the intended meaning is clear"
W19-4822,P12-2039,0,0.0602547,"has become the main benchmark for GEC, as it was used in the CoNLL GEC Shared Tasks (Ng et al., 2013, 2014). The Cambridge Learner Corpus First Certificate in English FCE corpus1 (Yannakoudakis et al., 2011) consists of essays collected from learners taking the Cambridge Assessment’s English as a Second or Other Language 1 We use the publicly available portion. 213 Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 213–223 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics (ESOL) exams.2 The Lang-8 corpus (Tajiri et al., 2012) was harvested from user-provided corrections in an online learner forum. Both have also been widely used for the GEC Shared Tasks. Another small corpus developed for evaluation purposes is the JHU FLuency-Extended GUG corpus (JFLEG) (Napoles et al., 2017) with correction annotations that include extended fluency edits rather than just minimal grammatical ones. The Cambridge English Write & Improve (W&I) corpus (Andersen et al., 2013) is collected from an online platform where English learners submit text and professional annotators correct them, also assigning a CEFR level of proficiency (of"
W19-4822,P11-1019,0,0.0134342,"luated on English-to-Spanish synthetic and natural data. 2 Data and Experimental Settings To our knowledge, there are six publicly available corpora of non-native or erroneous English that are annotated with corrections and which have been widely used for research in GEC. The NUS Corpus of Learner English (NUCLE) contains essays written by students at the National University of Singapore (Dahlmeier et al., 2013). It has become the main benchmark for GEC, as it was used in the CoNLL GEC Shared Tasks (Ng et al., 2013, 2014). The Cambridge Learner Corpus First Certificate in English FCE corpus1 (Yannakoudakis et al., 2011) consists of essays collected from learners taking the Cambridge Assessment’s English as a Second or Other Language 1 We use the publicly available portion. 213 Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 213–223 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics (ESOL) exams.2 The Lang-8 corpus (Tajiri et al., 2012) was harvested from user-provided corrections in an online learner forum. Both have also been widely used for the GEC Shared Tasks. Another small corpus developed for evaluation purposes"
W19-4822,W18-7005,0,\N,Missing
W19-5303,2012.eamt-1.60,0,0.0358,"indicate the data domain. For data augmentation, they back-translate from a target language to its noisy source. The intuition, also observed by Michel and Neubig (2018), is that the source sentences are noisier than their target translations. They include out-ofdomain clean data during this step and differentiate data types with a special symbol on the target side. In addition, they also run a model ensemble. Training Data In the constrained setting, participants were allowed to use the WMT15 training data3 for Eng↔Fra and any of the KFTT (Neubig, 2011), JESC (Pryzant et al.) and TED talks (Cettolo et al., 2012) corpora for Jpn↔Eng. Additionally, the use of the MTNT corpus (Michel and Neubig, 2018) was allowed in order to adapt models on limited in-domain data. 3.3 Evaluation protocol Test Data The test sets were collected following the same protocol as the MTNT dataset, i.e. collected from 3 http://www.statmt.org/wmt15/ translation-task.html 93 Figure 1: Annotation interface for human evaluations. 94 Eng-Fra Fra-Eng Eng-Jpn Jpn-Eng # samples 1,401 1,233 1,392 1,111 # source tokens # target tokens 20.0k 22.8k 19.8k 19.2k 20.0k 33.6k 18.7k 13.4k Table 1: Statistics of the test sets. model which was al"
W19-5303,P19-1425,0,0.113812,"inkov and Glass (2019) for a categorization of such work. In particular, some have focused on specific variations of naturally-occurring noise, such as grammatical errors produced by non-native speakers (Anastasopoulos et al., 2019) or errors extracted from Wikipedia edits (Belinkov and Bisk, 2018). It has also been shown that adding synthetic noise does not trivially increase robustness to natural noise (Belinkov and Bisk, 2018) and may require specific recipes (Karpukhin et al., 2019). Michel et al. (2019) recently emphasized the importance of meaning-preserving perturbations and along with Cheng et al. (2019) demonstrated the utility of adversarial training without significantly impairing performance on clean data and domain. Durrani et al. (2019) showed that character-based representations are more robust towards noise compared to such learned using BPE-based sub-word units in the task of machine translation. Jpn→Eng. We describe the dataset and the task setup in Section 3. The shared-task attracted a total of 23 submissions from 11 teams. The teams employed a variety of methods to improve robustness. A specific challenge was the small size of the in-domain noisy parallel dataset. We summarize th"
W19-5303,P17-4012,0,0.0548466,"data, improve existing semisupervised approach such as backtranslation. We provide both in-domain (MTNT) and outof-domain (News Commentary, News Crawl, etc) monolingual data. 3.2 Participants and System Descriptions We received 23 submissions from 11 teams. Except two submissions on the Eng-Fra language pair, all systems used the constrained setup. Below we briefly describe the systems from the 8 teams which submitted corresponding system description papers: Baidu & Oregon State University’s submission (Zheng et al., 2019): Their system is based on the Transformer implementation in OpenNMTpy (Klein et al., 2017). The main methods applied in their submission are: domain-sensitive data mixing and data augmentation with backtranslation. For data mixing, they used a special symbol on the source side to indicate the data domain. For data augmentation, they back-translate from a target language to its noisy source. The intuition, also observed by Michel and Neubig (2018), is that the source sentences are noisier than their target translations. They include out-ofdomain clean data during this step and differentiate data types with a special symbol on the target side. In addition, they also run a model ensem"
W19-5303,P18-1163,0,0.189473,"Missing"
W19-5303,W17-3204,1,0.848798,"re efforts from the community in building robust MT models. 2 Related Work The fragility of neural networks (Szegedy et al., 2013) has been shown to extend to neural machine translation models (Belinkov and Bisk, 2018; Heigold et al., 2017) and recent work focused on various aspects of the problem. From the identification of the causes of this brittleness, to the induction of (adversarial) inputs that trigger the unwanted behavior (attacks) and making such models robust against various types of noisy inputs (defenses); improving robustness has been receiving increasing attention in NMT. While Koehn and Knowles (2017) mentioned domain mismatch as a challenge for neural machine translation, Khayrallah and Koehn (2018) addressed noisy training data and focus on the types of noise occurring in web-crawled corpora. Michel and Neubig (2018) proposed a new dataset (MTNT) to test MT models for robustness to the types of noise encountered in the Internet and demonstrated that these challenges cannot be overcome by simple domain adaptation techniques alone. Belinkov and Bisk (2018) and Heigold et al. (2017) showed that NMT systems are very sensitive to slightly perturbed input forms, and hinted at the importance of"
W19-5303,W19-5362,0,0.149193,"this campaign. Unlike other participants, the winning team Naver Labs B´erard et al. (2019) and NTT (Murakami et al., 2019) applied data cleaning techniques in order to filter noisy parallel sentences. They filtered i) identical sentences on source and target side, ii) sentences that belonged to a language other than the source and target language, iii) sentences with length mismatch, and iv) also applied attention-based filtering. Data cleaning gave an improvement of more than 5 BLEU points with substantial reduction in the hallucination of the model for the winning team. NICT’s submission (Dabre and Sumita, 2019): The authors used Transformer models to train their systems and employed two strategies namely: i) mixed fine-tuning and ii) multilingual models for making the systems robust. The former helps as the in-domain data is available in a very small quantity. Using a mix of in-domain and outdomain data for fine-tuning helps overcome the problem of adjusting learning rate, applying better regularization and other complicated strategies. It is not clear how these two methods contributed towards making the models more robust. According to the authors, mixed fine-tuning and multilingual training (bidir"
W19-5303,D18-2012,0,0.020004,"mer-Big architecture, whereas improvements were substantially larger when the base models were RNN-Based MTNT baselines, about 8+ BLEU points. Participants emphasized the importance of their strong Transformer-Big base JHU’s submission (Post and Duh, 2019): This submission participated in the Fra→Eng and Jpn↔Eng tasks. The participants used data dual cross-entropy filtering for reducing the monolingual data, then back-translate these, and train their Transformer models (Vaswani et al., 2017). They compared Moses tokenization+Byte Pair Encoding (BPE) (Sennrich et al., 2016), and sentencepiece (Kudo and Richardson, 2018) (without any pre-processing) and found the two comparable, and that using larger sentence-piece models improved over smaller ones. For Jpn↔Eng (both di4 http://www.statmt.org/wmt19/biomedical-translationtask.html 95 sis, they found that their system performs poorly in translating emojis. The segmentation errors generated by KyTea resulted in further errors in the translation. rections) they first used both in-domain (MTNT) and out-of-domain data (other constrained), and then continued training (fine-tune) using MTNT only. They also reported many results from their hyper-parameter search (albe"
W19-5303,N19-1154,1,0.767588,"ise, such as grammatical errors produced by non-native speakers (Anastasopoulos et al., 2019) or errors extracted from Wikipedia edits (Belinkov and Bisk, 2018). It has also been shown that adding synthetic noise does not trivially increase robustness to natural noise (Belinkov and Bisk, 2018) and may require specific recipes (Karpukhin et al., 2019). Michel et al. (2019) recently emphasized the importance of meaning-preserving perturbations and along with Cheng et al. (2019) demonstrated the utility of adversarial training without significantly impairing performance on clean data and domain. Durrani et al. (2019) showed that character-based representations are more robust towards noise compared to such learned using BPE-based sub-word units in the task of machine translation. Jpn→Eng. We describe the dataset and the task setup in Section 3. The shared-task attracted a total of 23 submissions from 11 teams. The teams employed a variety of methods to improve robustness. A specific challenge was the small size of the in-domain noisy parallel dataset. We summarize the participating systems in Section 4 and the notable methods in Section 5. The contributions were evaluated both automatically and via a huma"
W19-5303,W18-6459,0,0.0413765,"ish (Eng) and French (Fra) and English and Japanese (Jpn), in four translation directions: Eng→Fra, Fra→Eng, Eng→Jpn, and www.reddit.com https://github.com/neulab/compare-mt 91 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 91–102 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics examples are generated with access to the model parameters (Ebrahimi et al., 2018; Cheng et al., 2018a,b, 2019) and ii) black-box attacks, where examples are generated without accessing model internals (Zhao et al., 2018; Lee et al., 2018; ?; Anastasopoulos et al., 2019; Vaibhav et al., 2019); see Belinkov and Glass (2019) for a categorization of such work. In particular, some have focused on specific variations of naturally-occurring noise, such as grammatical errors produced by non-native speakers (Anastasopoulos et al., 2019) or errors extracted from Wikipedia edits (Belinkov and Bisk, 2018). It has also been shown that adding synthetic noise does not trivially increase robustness to natural noise (Belinkov and Bisk, 2018) and may require specific recipes (Karpukhin et al., 2019). Michel et al. (2019) recently emphasized th"
W19-5303,N19-1314,1,0.8427,"ernals (Zhao et al., 2018; Lee et al., 2018; ?; Anastasopoulos et al., 2019; Vaibhav et al., 2019); see Belinkov and Glass (2019) for a categorization of such work. In particular, some have focused on specific variations of naturally-occurring noise, such as grammatical errors produced by non-native speakers (Anastasopoulos et al., 2019) or errors extracted from Wikipedia edits (Belinkov and Bisk, 2018). It has also been shown that adding synthetic noise does not trivially increase robustness to natural noise (Belinkov and Bisk, 2018) and may require specific recipes (Karpukhin et al., 2019). Michel et al. (2019) recently emphasized the importance of meaning-preserving perturbations and along with Cheng et al. (2019) demonstrated the utility of adversarial training without significantly impairing performance on clean data and domain. Durrani et al. (2019) showed that character-based representations are more robust towards noise compared to such learned using BPE-based sub-word units in the task of machine translation. Jpn→Eng. We describe the dataset and the task setup in Section 3. The shared-task attracted a total of 23 submissions from 11 teams. The teams employed a variety of methods to improve ro"
W19-5303,C18-1055,0,0.064971,"n this first iteration, the shared-task used the MTNT dataset (Michel and Neubig, 2018) that contains noisy social media texts and their translations between English (Eng) and French (Fra) and English and Japanese (Jpn), in four translation directions: Eng→Fra, Fra→Eng, Eng→Jpn, and www.reddit.com https://github.com/neulab/compare-mt 91 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 91–102 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics examples are generated with access to the model parameters (Ebrahimi et al., 2018; Cheng et al., 2018a,b, 2019) and ii) black-box attacks, where examples are generated without accessing model internals (Zhao et al., 2018; Lee et al., 2018; ?; Anastasopoulos et al., 2019; Vaibhav et al., 2019); see Belinkov and Glass (2019) for a categorization of such work. In particular, some have focused on specific variations of naturally-occurring noise, such as grammatical errors produced by non-native speakers (Anastasopoulos et al., 2019) or errors extracted from Wikipedia edits (Belinkov and Bisk, 2018). It has also been shown that adding synthetic noise does not trivially increase"
W19-5303,D18-1050,1,0.625161,"erstand the overall challenges in translating social media text and identify major themes of efforts which needs more research from the community. In recent years, Machine Translation (MT) systems have seen great progress, with neural models becoming the de-facto methods and even approaching human quality in news domain (Hassan et al., 2018). However, like other deep learning models, neural machine translation (NMT) models are found to be sensitive to synthetic and natural noise in input, distributional shift, and adversarial 1 2 In this first iteration, the shared-task used the MTNT dataset (Michel and Neubig, 2018) that contains noisy social media texts and their translations between English (Eng) and French (Fra) and English and Japanese (Jpn), in four translation directions: Eng→Fra, Fra→Eng, Eng→Jpn, and www.reddit.com https://github.com/neulab/compare-mt 91 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 91–102 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics examples are generated with access to the model parameters (Ebrahimi et al., 2018; Cheng et al., 2018a,b, 2019) and ii) black-box attacks, where ex"
W19-5303,W19-5363,0,0.0447228,"e of tied multitask learning, where the noisy source sentences are first decoded by a same-language denoising decoder, and both information is passed on to the translation decoder. This approach requires data triples of noisy source, clean source, translation, which they created by data augmentation over the provided data, using tag-informed translation systems trained on either noisy (MTNT) or clean (Europarl) data. As the participants point out though, their performance improvements seems to be attributed to data augmentation and not to the intermediate denoising decoder. FOKUS’ submission (Grozea, 2019): This team participated in three directions: Eng→Fra, Fra→Eng and Jpn→Eng. For the Eng→Fra and Fra→Eng language pairs, the submissions are unconstrained systems, where the model was trained on the medical domain corpus provided by the WMT biomedical shared task 4 . Despite the training data being out-of-domain, removing “lowquality” parallel data such as “Subtitles” as the author hypothesized helped to bring 2 to 4 BLEU points improvement over the baseline models. Their Jpn→Eng submission is a constrained system, using the same model architecture as the Eng→Fra language pair. To improve robus"
W19-5303,N19-4007,1,0.827549,"ype tags (real or backtranslated) for further categorization of the training data. Compared to fine-tuning, adding tags provides them additional flexibility, resulting in a generalized system, robust towards a variety of input data. Human Evaluation The results of human evaluation following the evaluation protocol described in Section 3.4 are outlined in Table 2. Automatic Evaluation The automatic evaluation (BLEU) results of the Shared Task are summarized in Table 3. 6.2 Qualitative Analysis In order to discover salient differences between the methods, we performed analysis using compare-mt (Neubig et al., 2019), and present a few of the salient findings below. Fine-tuning Along with the noisy in-domain MTNT data, general domain data typically made available for WMT campaign was also allowed for this task. Most participants (Murakami et al., 2019; Dabre and Sumita, 2019; Helcl et al., 2019) trained on general domain data and fine-tuned the models towards the task. Murakami et al. (2019) did not see a consistent improvement with finetuning. Due to the small size of the in-domain data, Dabre and Sumita (2019) fine-tuned on a mix of in-domain and a subset of the out-of-domain data. Stronger Submissions"
W19-5303,W19-5364,0,0.144816,"Missing"
W19-5303,P02-1040,0,0.110507,"he translators were presented the original source sentence, the reference and the system output side by side. The order between the reference and the system output was randomized by the user interface. The translators rated both the reference and the translation on a scale from 1 to 100. For both the original source sentence and the reference, the original text was presented except for Eng-Jpn where the Japanese reference tokenized with KyTea was presented in order to be consistent with the systems’ outputs. The user interface for annotation is illustrated in Figure 1. We also evaluated BLEU (Papineni et al., 2002) for each system using SacreBLEU (Post, 2018). For all language pairs except Eng-Jpn, we used the original reference and SacreBLEU with the default options. In the case of Eng-Jpn, we used the reference tokenized with KyTea and the option --tokenize none. Task Setup The task includes two tracks, constrained and unconstrained depending on whether the system is trained on a predefined training datasets or not. The two tracks are evaluated by the same automatic and human evaluation protocol, however, they are compared separately. For the constrained system track, the task specifies two types of t"
W19-5303,D19-5506,0,0.114843,"thout accessing model internals (Zhao et al., 2018; Lee et al., 2018; ?; Anastasopoulos et al., 2019; Vaibhav et al., 2019); see Belinkov and Glass (2019) for a categorization of such work. In particular, some have focused on specific variations of naturally-occurring noise, such as grammatical errors produced by non-native speakers (Anastasopoulos et al., 2019) or errors extracted from Wikipedia edits (Belinkov and Bisk, 2018). It has also been shown that adding synthetic noise does not trivially increase robustness to natural noise (Belinkov and Bisk, 2018) and may require specific recipes (Karpukhin et al., 2019). Michel et al. (2019) recently emphasized the importance of meaning-preserving perturbations and along with Cheng et al. (2019) demonstrated the utility of adversarial training without significantly impairing performance on clean data and domain. Durrani et al. (2019) showed that character-based representations are more robust towards noise compared to such learned using BPE-based sub-word units in the task of machine translation. Jpn→Eng. We describe the dataset and the task setup in Section 3. The shared-task attracted a total of 23 submissions from 11 teams. The teams employed a variety of"
W19-5303,W18-2709,1,0.860168,"networks (Szegedy et al., 2013) has been shown to extend to neural machine translation models (Belinkov and Bisk, 2018; Heigold et al., 2017) and recent work focused on various aspects of the problem. From the identification of the causes of this brittleness, to the induction of (adversarial) inputs that trigger the unwanted behavior (attacks) and making such models robust against various types of noisy inputs (defenses); improving robustness has been receiving increasing attention in NMT. While Koehn and Knowles (2017) mentioned domain mismatch as a challenge for neural machine translation, Khayrallah and Koehn (2018) addressed noisy training data and focus on the types of noise occurring in web-crawled corpora. Michel and Neubig (2018) proposed a new dataset (MTNT) to test MT models for robustness to the types of noise encountered in the Internet and demonstrated that these challenges cannot be overcome by simple domain adaptation techniques alone. Belinkov and Bisk (2018) and Heigold et al. (2017) showed that NMT systems are very sensitive to slightly perturbed input forms, and hinted at the importance of injecting noisy examples during training, also known as adversarial examples. Further research propo"
W19-5303,W18-6319,0,0.0373317,"ce, the reference and the system output side by side. The order between the reference and the system output was randomized by the user interface. The translators rated both the reference and the translation on a scale from 1 to 100. For both the original source sentence and the reference, the original text was presented except for Eng-Jpn where the Japanese reference tokenized with KyTea was presented in order to be consistent with the systems’ outputs. The user interface for annotation is illustrated in Figure 1. We also evaluated BLEU (Papineni et al., 2002) for each system using SacreBLEU (Post, 2018). For all language pairs except Eng-Jpn, we used the original reference and SacreBLEU with the default options. In the case of Eng-Jpn, we used the reference tokenized with KyTea and the option --tokenize none. Task Setup The task includes two tracks, constrained and unconstrained depending on whether the system is trained on a predefined training datasets or not. The two tracks are evaluated by the same automatic and human evaluation protocol, however, they are compared separately. For the constrained system track, the task specifies two types of training data in addition to MTNT train set: •"
W19-5303,P16-1162,0,0.0719206,"on top of the base models with the Transformer-Big architecture, whereas improvements were substantially larger when the base models were RNN-Based MTNT baselines, about 8+ BLEU points. Participants emphasized the importance of their strong Transformer-Big base JHU’s submission (Post and Duh, 2019): This submission participated in the Fra→Eng and Jpn↔Eng tasks. The participants used data dual cross-entropy filtering for reducing the monolingual data, then back-translate these, and train their Transformer models (Vaswani et al., 2017). They compared Moses tokenization+Byte Pair Encoding (BPE) (Sennrich et al., 2016), and sentencepiece (Kudo and Richardson, 2018) (without any pre-processing) and found the two comparable, and that using larger sentence-piece models improved over smaller ones. For Jpn↔Eng (both di4 http://www.statmt.org/wmt19/biomedical-translationtask.html 95 sis, they found that their system performs poorly in translating emojis. The segmentation errors generated by KyTea resulted in further errors in the translation. rections) they first used both in-domain (MTNT) and out-of-domain data (other constrained), and then continued training (fine-tune) using MTNT only. They also reported many"
W19-5303,N19-1190,1,0.747855,"e (Jpn), in four translation directions: Eng→Fra, Fra→Eng, Eng→Jpn, and www.reddit.com https://github.com/neulab/compare-mt 91 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 91–102 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics examples are generated with access to the model parameters (Ebrahimi et al., 2018; Cheng et al., 2018a,b, 2019) and ii) black-box attacks, where examples are generated without accessing model internals (Zhao et al., 2018; Lee et al., 2018; ?; Anastasopoulos et al., 2019; Vaibhav et al., 2019); see Belinkov and Glass (2019) for a categorization of such work. In particular, some have focused on specific variations of naturally-occurring noise, such as grammatical errors produced by non-native speakers (Anastasopoulos et al., 2019) or errors extracted from Wikipedia edits (Belinkov and Bisk, 2018). It has also been shown that adding synthetic noise does not trivially increase robustness to natural noise (Belinkov and Bisk, 2018) and may require specific recipes (Karpukhin et al., 2019). Michel et al. (2019) recently emphasized the importance of meaning-preserving perturbations and al"
W19-5303,D18-1316,0,0.0279519,"ations between English (Eng) and French (Fra) and English and Japanese (Jpn), in four translation directions: Eng→Fra, Fra→Eng, Eng→Jpn, and www.reddit.com https://github.com/neulab/compare-mt 91 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 91–102 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics examples are generated with access to the model parameters (Ebrahimi et al., 2018; Cheng et al., 2018a,b, 2019) and ii) black-box attacks, where examples are generated without accessing model internals (Zhao et al., 2018; Lee et al., 2018; ?; Anastasopoulos et al., 2019; Vaibhav et al., 2019); see Belinkov and Glass (2019) for a categorization of such work. In particular, some have focused on specific variations of naturally-occurring noise, such as grammatical errors produced by non-native speakers (Anastasopoulos et al., 2019) or errors extracted from Wikipedia edits (Belinkov and Bisk, 2018). It has also been shown that adding synthetic noise does not trivially increase robustness to natural noise (Belinkov and Bisk, 2018) and may require specific recipes (Karpukhin et al., 2019). Michel et al. (2019) rece"
W19-5303,W19-5368,1,0.878409,"not experimented. Finally, participants point out one peculiarity they’ve noticed in the train/validation partitioning of the original MTNT dataset; validation source sentences being started with the letter “Y” followed by alphabetically sorted sentences (test partition not effected). The team experimented with the Fra→Eng and Eng→Fra translation directions, obtaining 43.6 and 36.4 BLEU-cased, respectively (3rd place in both). Their ablations show significant benefit from domain-sensitive training (+3 BLEU), with additional improvements from back-translation and ensembling. CMU’s submission (Zhou et al., 2019): This submission only participated in the Fra→Eng direction. They proposed the use of tied multitask learning, where the noisy source sentences are first decoded by a same-language denoising decoder, and both information is passed on to the translation decoder. This approach requires data triples of noisy source, clean source, translation, which they created by data augmentation over the provided data, using tag-informed translation systems trained on either noisy (MTNT) or clean (Europarl) data. As the participants point out though, their performance improvements seems to be attributed to da"
W19-5303,Q19-1004,1,\N,Missing
W19-5303,N19-1311,1,\N,Missing
W19-5303,W19-5366,0,\N,Missing
W19-5368,D18-1050,1,0.760721,"proposed strategy is flexible and it could be used as long as we have at least one element of the T triple. 566 Figure 2: Training data synthesis. Blocks rounded by dash rectangle are synthetic while others are real. lated French sentences are of high quality and thus treat them as clean French text. Depending on which part of triple is available, we select the proper NMT model and synthesize the missing ones. In Figure 2, we show 3 ways that we did this in this work. Note that because we focus on the translation from French to English where the French text mostly consists of MTNTstyle noise (Michel and Neubig, 2018), we specify the source language as fr, the target language as en and the noise style as MTNT; however, our approach could be used for all other language pairs with different noise distributions. Clean fr: To make our back-translation strategy more generalized to settings where the above parallel data is not enough to train the model, we also design a pipeline to utilize monolingual data which is likely to be available most of the time. In this case, we first translate these sentences to English and then translate them back to French. Both NMT models are trained with TED and MTNT data as we de"
W19-5368,W19-4822,1,0.842506,"sing source text and domain adaptation, both of which are popular approaches for designing robust NMT systems. Compared to the baseline vanilla transformer that is trained on clean data only, our proposed model with fine tuning enjoys 7.1 BLEU points improvement on the WMT Robustness shared task French to English dataset. However, this improvement is most likely attributed to the noisy text we add to the training process (hence, due to better domain adaptation), and not due to the denoising multi-task strategy. kinds of noise. Similar findings were outlined in Anastasopoulos et al. (2019) and Anastasopoulos (2019), which evaluated MT systems on natural and natural-like grammatical noise, specifically on English produced by non-native speakers. Natural noise appears to be richer and more complex compared to synthetic noise, making it challenging to manually design a comprehensive set of noise to approximate real world settings. In our work, we follow (Vaibhav et al., 2019) and synthesize the noisy text through back-translation. There is no need to manually control the distribution of noise. In terms of multi-task learning for machine translation, Tu et al. (2017) proposes to add a reconstructor on top o"
W19-5368,N18-1008,1,0.884763,"al. (2015); Vaswani et al. (2017)) systems are known to degrade drastically when confronted with noisy data (Belinkov and Bisk, 2017; Khayrallah and Koehn, 2018; Anastasopoulos et al., 2019). Thus, there is increasing need to build robust NMT systems that are resilient to naturally occurring noise. In this work, we attempt to enhance the robustness of the NMT system through multi-task learning. Our model is a transformer-based model (Vaswani et al., 2017) augmented with two decoders, with each decoder bound to different learning objectives. It has a cascade architecture (Niehues et al., 2016; Anastasopoulos and Chiang, 2018) where the first decoder reads in the output of the encoder and the second decoder reads in the 2 Multi-task Transformer In this section, we describe in detail the architecture of our proposed multi-task transformer. It is a transformer-based (Vaswani et al., 2017) cascade multi-task framework (Niehues et al., 2016; Anastasopoulos and Chiang, 2018). 2.1 Detailed Architecture As illustrated in Figure 1, the model consists of one transformer encoder and two transformer de1 The code is available at https://github.com/ shuyanzhou/multitask_transformer 565 Proceedings of the Fourth Conference on Ma"
W19-5368,C16-1172,0,0.0592808,"Missing"
W19-5368,N19-1311,1,0.775947,"Missing"
W19-5368,N19-1043,0,0.0282892,". In our work, we follow (Vaibhav et al., 2019) and synthesize the noisy text through back-translation. There is no need to manually control the distribution of noise. In terms of multi-task learning for machine translation, Tu et al. (2017) proposes to add a reconstructor on top of the decoder. The auxiliary objective is to reconstruct the source sentence from the hidden layers of the translation decoder. This encourages the decoder to embed complete source information, which helps improve the translation performance. This approach was found to be helpful in low-resource MT scenarios also by Niu et al. (2019). Anastasopoulos and Chiang (2018) proposes a tied multitask learning model architecture to improve the speech translation task. The intuition is that, speech transcription as an intermediate task, should improve the performance of speech translation if the speech translation is based on both the input speech and its transcription. 6 Acknowledgements We thank AWS Educate program for donating computational GPU resources used in this work. We also thank Daniel Clothiaux and Junxian He for their insightful comments. This material is based upon work supported in part by the Defense Advanced Resear"
W19-5368,N19-4009,0,0.169687,"lean English parallel data). Finally, we carry out a case study by comparing the output of our model with the baseline model. 4.1 Data Pre-processing Because of time limitations, we did not use all three kinds of training triples. We only used the first two triples introduced in Section 3. Noisy fr & Clean en: This kind of parallel text can be found in the MTNT training data. Note that even though the manually translated English sentences contain some level of “noise” (e.g. emoji), we treat them as clean English text. In this scenario, we leverage a pre-trained NMT system provided by fairseq (Ott et al., 2019) to translate English sentences back to French. Considering its good performance over other benchmarks (e.g. WMT newstest datasets) we assume that the transClean fr & Clean en: The clean data consists of europarl-v73 and news-commentary-v10 copora.4 We filter out sentences whose length is greater 2 We did not attempt this due to time restrictions. http://www.statmt.org/europarl/v7/ fr-en.tgz 4 http://www.statmt.org/wmt15/ training-parallel-nc-v10.tgz 3 567 than 50. We apply a pretrained Byte Pair Encoding (BPE, Gage (1994)) model with 16k subword units to both source and target sentences. The"
W19-5368,W18-1807,0,0.130833,"is that the first (denoising) decoder does not really properly deal with noise in the desired way, and the translation decoder generally 5 Related Work Here, we discuss how the MT community handles the noise problem. In general, there are mainly two kinds of approaches: the first attempts to denoise text, and the second proposes training with noisy texts. Denoising text: Sakaguchi et al. (2017) proposes semi-character level recurrent neural network (scRNN) to correct words with scrambling characters. Each word is represented as a vector with elements corresponding to the characters’ position. Heigold et al. (2018) investigates the robustness of character-based word embeddings in machine translation against word scrambling and random noise. The experiments show that the noise has a larger influence on character-based models than BPE-based models. To minimize the influence of word structure, Belinkov and Bisk (2017) proposes to represent word as its average character embeddings, which is invariant to these kinds of noise. The proposed method enables the MT system to be more robust to scrambling noise even training the model with clean text. Instead of handling noise at the word level, we try to recover t"
W19-5368,W18-2709,0,0.0189376,"e of 32.8 on the shared task French to English dataset, which is 7.1 BLEU points higher than the baseline vanilla transformer trained with clean text1 . 1 Introduction Real world data, especially in the realm of social media, often contains noise such as mis-spellings, grammar errors, or lexical variations. Even though humans do not have much difficulty in recognizing and translating noisy or ungrammatical sentences, neural machine translation (NMT; Bahdanau et al. (2015); Vaswani et al. (2017)) systems are known to degrade drastically when confronted with noisy data (Belinkov and Bisk, 2017; Khayrallah and Koehn, 2018; Anastasopoulos et al., 2019). Thus, there is increasing need to build robust NMT systems that are resilient to naturally occurring noise. In this work, we attempt to enhance the robustness of the NMT system through multi-task learning. Our model is a transformer-based model (Vaswani et al., 2017) augmented with two decoders, with each decoder bound to different learning objectives. It has a cascade architecture (Niehues et al., 2016; Anastasopoulos and Chiang, 2018) where the first decoder reads in the output of the encoder and the second decoder reads in the 2 Multi-task Transformer In this"
W19-5368,N19-1190,1,0.885147,", the objective of the second decoder, namely the translation decoder, is to correctly translate the sentence to the target language. This framework should be beneficial in two ways: 1) Since the model is trained with noisy text, it should inherently better generalize to noisy text. 2) The translation decoder could potentially take advantage of the recovered clean sentence while maintaining specific varieties of noise (e.g. emoji) by referring to the original noisy sentence. This framework requires triplets of clean and noisy source sentences, along with target translations, so we also follow Vaibhav et al. (2019) and design a back-translation strategy that synthesizes noisy data. Our proposed model outperforms the baseline vanilla transformer trained with clean text by 4.6 BLEU points on the WMT 2019 Robustness shared task (Li et al., 2019) French to English dataset. The fine-tuning process brings an additional 2.5 points improvement. According to our analysis, however, the improvements can mainly be attributed to introducing noisy data during training rather than the multi-task learning objective. While neural machine translation (NMT) achieves remarkable performance on clean, indomain text, performa"
W19-5368,kobus-etal-2017-domain,0,0.0312925,"T data as we describe above. Similarly, in both directions, we add the MTNT tag in the beginning of the sentences. Note that alternatively one could use an off-the-shelf NMT model to generate clean English text.2 Clean fr & Clean en: This is the most common parallel corpus that could be obtained from many existing resources. The only missing text is the noisy French text. In this case, we synthesize the noisy text with the help of the NMT model trained with both TED and MTNT training data. During training, we add a tag showing the source of this pair at the beginning of each English sentence (Kobus et al., 2017; Vaibhav et al., 2019). By adding this tag, the model could potentially better distinguish TED data and MTNT data. To generate the noisy French text, we add an MTNT tag at the beginning of each sentence and feed them to this NMT model. Ideally, besides the inherent noise as a result of imperfect translations, the translated French sentences could also possess a similar noise distribution as MTNT. 4 Experiments In this section, we first describe in detail our data pre-processing scheme, as well as the choice of hyperparameters. Then we compare our system with the baseline model (a vanilla tran"
W19-5368,W19-5303,1,0.878835,"ld inherently better generalize to noisy text. 2) The translation decoder could potentially take advantage of the recovered clean sentence while maintaining specific varieties of noise (e.g. emoji) by referring to the original noisy sentence. This framework requires triplets of clean and noisy source sentences, along with target translations, so we also follow Vaibhav et al. (2019) and design a back-translation strategy that synthesizes noisy data. Our proposed model outperforms the baseline vanilla transformer trained with clean text by 4.6 BLEU points on the WMT 2019 Robustness shared task (Li et al., 2019) French to English dataset. The fine-tuning process brings an additional 2.5 points improvement. According to our analysis, however, the improvements can mainly be attributed to introducing noisy data during training rather than the multi-task learning objective. While neural machine translation (NMT) achieves remarkable performance on clean, indomain text, performance is known to degrade drastically when facing text which is full of typos, grammatical errors and other varieties of noise. In this work, we propose a multitask learning algorithm for transformer-based MT systems that is more resi"
W19-5368,E17-2004,0,0.0311683,"ed models. To minimize the influence of word structure, Belinkov and Bisk (2017) proposes to represent word as its average character embeddings, which is invariant to these kinds of noise. The proposed method enables the MT system to be more robust to scrambling noise even training the model with clean text. Instead of handling noise at the word level, we try to recover the clean text from the noisy one at the sentence level. Besides noise like word scrambling, the sentence level denoising could potentially better deal with more complex noise like grammatical errors. Training with noisy data: Li et al. (2017) designs methods to generate noise in the text, mainly focusing on syntactic noise and semantic noise. (Sperber et al., 2017) proposes a noise model based on automatic speech recognizer (ASR) error types, which consists of substitutions, deletions and insertions. Their noise model samples the positions of words that should be altered in the source sentence. Even training with synthetic noise data brings a large improvement in translating noisy data, Belinkov and Bisk (2017) shows that models mainly perform well on the same kind of noise that is introduced at training time, and they mostly fail"
