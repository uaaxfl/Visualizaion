2021.textgraphs-1.6,Learning Clause Representation from Dependency-Anchor Graph for Connective Prediction,2021,-1,-1,3,1,719,yanjun gao,Proceedings of the Fifteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-15),0,"Semantic representation that supports the choice of an appropriate connective between pairs of clauses inherently addresses discourse coherence, which is important for tasks such as narrative understanding, argumentation, and discourse parsing. We propose a novel clause embedding method that applies graph learning to a data structure we refer to as a dependency-anchor graph. The dependency anchor graph incorporates two kinds of syntactic information, constituency structure, and dependency relations, to highlight the subject and verb phrase relation. This enhances coherence-related aspects of representation. We design a neural model to learn a semantic representation for clauses from graph convolution over latent representations of the subject and verb phrase. We evaluate our method on two new datasets: a subset of a large corpus where the source texts are published novels, and a new dataset collected from students{'} essays. The results demonstrate a significant improvement over tree-based models, confirming the importance of emphasizing the subject and verb phrase. The performance gap between the two datasets illustrates the challenges of analyzing student{'}s written text, plus a potential evaluation task for coherence modeling and an application for suggesting revisions to students."
2021.emnlp-main.487,A Semantic Feature-Wise Transformation Relation Network for Automatic Short Answer Grading,2021,-1,-1,3,0,9687,zhaohui li,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Automatic short answer grading (ASAG) is the task of assessing students{'} short natural language responses to objective questions. It is a crucial component of new education platforms, and could support more wide-spread use of constructed response questions to replace cognitively less challenging multiple choice questions. We propose a Semantic Feature-wise transformation Relation Network (SFRN) that exploits the multiple components of ASAG datasets more effectively. SFRN captures relational knowledge among the questions (Q), reference answers or rubrics (R), and labeled student answers (A). A relation network learns vector representations for the elements of QRA triples, then combines the learned representations using learned semantic feature-wise transformations. We apply translation-based data augmentation to address the two problems of limited training data, and high data skew for multi-class ASAG tasks. Our model has up to 11{\%} performance improvement over state-of-the-art results on the benchmark SemEval-2013 datasets, and surpasses custom approaches designed for a Kaggle challenge, demonstrating its generality."
2021.acl-long.303,{ABCD}: A Graph Framework to Convert Complex Sentences to a Covering Set of Simple Sentences,2021,-1,-1,3,1,719,yanjun gao,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Atomic clauses are fundamental text units for understanding complex sentences. Identifying the atomic sentences within complex sentences is important for applications such as summarization, argument mining, discourse analysis, discourse parsing, and question answering. Previous work mainly relies on rule-based methods dependent on parsing. We propose a new task to decompose each complex sentence into simple sentences derived from the tensed clauses in the source, and a novel problem formulation as a graph edit task. Our neural model learns to Accept, Break, Copy or Drop elements of a graph that combines word adjacency and grammatical dependencies. The full processing pipeline includes modules for graph construction, graph editing, and sentence generation from the output graph. We introduce DeSSE, a new dataset designed to train and evaluate complex sentence decomposition, and MinWiki, a subset of MinWikiSplit. ABCD achieves comparable performance as two parsing baselines on MinWiki. On DeSSE, which has a more even balance of complex sentence types, our model achieves higher accuracy on the number of atomic sentences than an encoder-decoder baseline. Results include a detailed error analysis."
2020.sigdial-1.41,Dialogue Policies for Learning Board Games through Multimodal Communication,2020,-1,-1,6,0,14977,maryam zare,Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"This paper presents MDP policy learning for agents to learn strategic behavior{--}how to play board games{--}during multimodal dialogues. Policies are trained offline in simulation, with dialogues carried out in a formal language. The agent has a temporary belief state for the dialogue, and a persistent knowledge store represented as an extensive-form game tree. How well the agent learns a new game from a dialogue with a simulated partner is evaluated by how well it plays the game, given its dialogue-final knowledge state. During policy training, we control for the simulated dialogue partner{'}s level of informativeness in responding to questions. The agent learns best when its trained policy matches the current dialogue partner{'}s informativeness. We also present a novel data collection for training natural language modules. Human subjects who engaged in dialogues with a baseline system rated the system{'}s language skills as above average. Further, results confirm that human dialogue partners also vary in their informativeness."
W19-4452,Rubric Reliability and Annotation of Content and Argument in Source-Based Argument Essays,2019,0,0,7,1,719,yanjun gao,Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications,0,"We present a unique dataset of student source-based argument essays to facilitate research on the relations between content, argumentation skills, and assessment. Two classroom writing assignments were given to college students in a STEM major, accompanied by a carefully designed rubric. The paper presents a reliability study of the rubric, showing it to be highly reliable, and initial annotation on content and argumentation annotation of the essays."
K19-1038,Automated Pyramid Summarization Evaluation,2019,0,1,3,1,719,yanjun gao,Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),0,"Pyramid evaluation was developed to assess the content of paragraph length summaries of source texts. A pyramid lists the distinct units of content found in several reference summaries, weights content units by how many reference summaries they occur in, and produces three scores based on the weighted content of new summaries. We present an automated method that is more efficient, more transparent, and more complete than previous automated pyramid methods. It is tested on a new dataset of student summaries, and historical NIST data from extractive summarizers."
W18-0531,Automated Content Analysis: A Case Study of Computer Science Student Summaries,2018,0,1,3,1,719,yanjun gao,Proceedings of the Thirteenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"Technology is transforming Higher Education learning and teaching. This paper reports on a project to examine how and why automated content analysis could be used to assess precis writing by university students. We examine the case of one hundred and twenty-two summaries written by computer science freshmen. The texts, which had been hand scored using a teacher-designed rubric, were autoscored using the Natural Language Processing software, PyrEval. Pearson{'}s correlation coefficient and Spearman rank correlation were used to analyze the relationship between the teacher score and the PyrEval score for each summary. Three content models automatically constructed by PyrEval from different sets of human reference summaries led to consistent correlations, showing that the approach is reliable. Also observed was that, in cases where the focus of student assessment centers on formative feedback, categorizing the PyrEval scores by examining the average and standard deviations could lead to novel interpretations of their relationships. It is suggested that this project has implications for the ways in which automated content analysis could be used to help university students improve their summarization skills."
L18-1511,{P}yr{E}val: An Automated Method for Summary Content Analysis,2018,0,0,3,1,719,yanjun gao,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
P15-1153,Abstractive Multi-Document Summarization via Phrase Selection and Merging,2015,48,14,6,0,4058,lidong bing,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"We propose an abstraction-based multi-document summarization framework that can construct new sentences by exploring more fine-grained syntactic units than sentences, namely, noun/verb phrases. Different from existing abstraction-based approaches, our method first constructs a pool of concepts and facts represented by phrases from the input documents. Then new sentences are generated by selecting and merging informative phrases to maximize the salience of phrases and meanwhile satisfy the sentence construction constraints. We employ integer linear optimization for conducting phrase selection and merging simultaneously in order to achieve the global optimal solution for a summary. Experimental results on the benchmark data set TAC 2011 show that our framework outperforms the state-of-the-art models under automated pyramid evaluation metric, and achieves reasonably well results on manual linguistic quality evaluation."
D15-1261,Estimation of Discourse Segmentation Labels from Crowd Data,2015,35,3,3,0,37843,ziheng huang,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"For annotation tasks involving independent judgments, probabilistic models have been used to infer ground truth labels from data where a crowd of many annotators labels the same items. Such models have been shown to produce results superior to taking the majority vote, but have not been applied to sequential data. We present two methods to infer ground truth labels from sequential annotations where we assume judgments are not independent, based on the observation that an annotatorxe2x80x99s segments all tend to be several utterances long. The data consists of crowd labels for annotation of discourse segment boundaries. The new methods extend Hidden Markov Models to relax the independence assumption. The two methods are distinct, so positive labels proposed by both are taken to be ground truth. In addition, results of the models are checked using metrics that test whether an annotatorxe2x80x99s accuracy relative to a given model remains consistent across different conversations."
W14-4330,Aspectual Properties of Conversational Activities,2014,30,1,1,1,721,rebecca passonneau,Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue ({SIGDIAL}),0,"Segmentation of spoken discourse into distinct conversational activities has been applied to broadcast news, meetings, monologs, and two-party dialogs. This paper considers the aspectual properties of discourse segments, meaning how they transpire in time. Classifiers were constructed to distinguish between segment boundaries and non-boundaries, where the sizes of utterance spans to represent data instances were varied, and the locations of segment boundaries relative to these instances. Classifier performance was better for representations that included the end of one discourse segment combined with the beginning of the next. In addition, classification accuracy was better for segments in which speakers accomplish goals with distinctive start and end points."
Q14-1025,The Benefits of a Model of Annotation,2014,18,42,1,1,721,rebecca passonneau,Transactions of the Association for Computational Linguistics,0,"Standard agreement measures for interannotator reliability are neither necessary nor sufficient to ensure a high quality corpus. In a case study of word sense annotation, conventional methods for evaluating labels from trained annotators are contrasted with a probabilistic annotation model applied to crowdsourced data. The annotation model provides far more information, including a certainty measure for each gold standard label; the crowdsourced data was collected at less than half the cost of the conventional approach."
moro-etal-2014-annotating,Annotating the {MASC} Corpus with {B}abel{N}et,2014,28,8,4,0,37216,andrea moro,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In this paper we tackle the problem of automatically annotating, with both word senses and named entities, the MASC 3.0 corpus, a large English corpus covering a wide range of genres of written and spoken text. We use BabelNet 2.0, a multilingual semantic network which integrates both lexicographic and encyclopedic knowledge, as our sense/entity inventory together with its semantic structure, to perform the aforementioned annotation task. Word sense annotated corpora have been around for more than twenty years, helping the development of Word Sense Disambiguation algorithms by providing both training and testing grounds. More recently Entity Linking has followed the same path, with the creation of huge resources containing annotated named entities. However, to date, there has been no resource that contains both kinds of annotation. In this paper we present an automatic approach for performing this annotation, together with its output on the MASC corpus. We use this corpus because its goal of integrating different types of annotations goes exactly in our same direction. Our overall aim is to stimulate research on the joint exploitation and disambiguation of word senses and named entities. Finally, we estimate the quality of our annotations using both manually-tagged named entities and word senses, obtaining an accuracy of roughly 70{\%} for both named entities and word sense annotations."
C14-1054,Biber Redux: Reconsidering Dimensions of Variation in {A}merican {E}nglish,2014,30,6,1,1,721,rebecca passonneau,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Genre classification has been found to improve performance in many applications of statistical NLP, including language modeling for spoken language, domain adaptation of statistical parsers, and machine translation. It has also been found to benefit retrieval of spoken or written documents. At its base, however, classification assumes separability. This paper revisits an assumption that genre variation is continuous along multiple dimensions, and an early use of principal component analysis to find these dimensions. Results on a very heterogeneous corpus of post1990s American English reveal four major dimensions, three of which echo those found in prior work and the fourth depending on features not used in the earlier study. The resulting model can provide a basis for more detailed analysis of sub-genres and the relation between genre and situations of language use, as well as a means to predict distributional properties of new genres."
W13-2323,The Benefits of a Model of Annotation,2013,24,7,1,1,721,rebecca passonneau,Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,0,"This paper presents a case study of a difficult and important categorical annotation task (word sense) to demonstrate a probabilistic annotation model applied to crowdsourced data. It is argued that standard (chance-adjusted) agreement levels are neither necessary nor sufficient to ensure high quality gold standard labels. Compared to conventional agreement measures, application of an annotation model to instances with crowdsourced labels yields higher quality labels at lower cost."
P13-2026,Automated Pyramid Scoring of Summaries using Distributional Semantics,2013,17,21,1,1,721,rebecca passonneau,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"The pyramid method for content evaluation of automated summarizers produces scores that are shown to correlate well with manual scores used in educational assessment of studentsxe2x80x99 summaries. This motivates the development of a more accurate automated method to compute pyramid scores. Of three methods tested here, the one that performs best relies on latent semantics."
P13-1086,Semantic Frames to Predict Stock Price Movement,2013,31,62,2,0,41486,boyi xie,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Semantic frames are a rich linguistic resource. There has been much work on semantic frame parsers, but less that applies them to general NLP problems. We address a task to predict change in stock price from financial news. Semantic frames help to generalize from specific sentences to scenarios, and to detect the (positive or negative) roles of specific companies. We introduce a novel tree representation, and use it to train predictive models with tree kernels using support vector machines. Our experiments test multiple text representations on two binary classification tasks, change of price and polarity. Experiments show that features derived from semantic frame parsing have significantly better performance across years on the polarity task."
N13-1128,Open Dialogue Management for Relational Databases,2013,30,3,2,1,37665,ben hixon,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We present open dialogue management and its application to relational databases. An open dialogue manager generates dialogue states, actions, and default strategies from the semantics of its application domain. We define three open dialogue management tasks. First, vocabulary selection finds the intelligible attributes in each database table. Second, focus discovery selects candidate dialogue foci, tables that have the most potential to address basic user goals. Third, a focus agent is instantiated for each dialogue focus with a default dialogue strategy governed by efficiency. We demonstrate the portability of open dialogue management on three very different databases. Evaluation of our system with simulated users shows that users with realistically limited domain knowledge have dialogues nearly as efficient as those of users with complete domain knowledge."
W12-1635,Semantic Specificity in Spoken Dialogue Requests,2012,5,1,2,1,37665,ben hixon,Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,Ambiguous or open-ended requests to a dialogue system result in more complex dialogues. We present a semantic-specificity metric to gauge this complexity for dialogue systems that access a relational database. An experiment where a simulated user makes requests to a dialogue system shows that semantic specificity correlates with dialogue length.
passonneau-etal-2012-masc,The {MASC} Word Sense Corpus,2012,9,21,1,1,721,rebecca passonneau,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The MASC project has produced a multi-genre corpus with multiple layers of linguistic annotation, together with a sentence corpus containing WordNet 3.1 sense tags for 1000 occurrences of each of 100 words produced by multiple annotators, accompanied by indepth inter-annotator agreement data. Here we give an overview of the contents of MASC and then focus on the word sense sentence corpus, describing the characteristics that differentiate it from other word sense corpora and detailing the inter-annotator agreement studies that have been performed on the annotations. Finally, we discuss the potential to grow the word sense sentence corpus through crowdsourcing and the plan to enhance the content and annotations of MASC through a community-based collaborative effort."
de-melo-etal-2012-empirical,Empirical Comparisons of {MASC} Word Sense Annotations,2012,6,9,4,0,3978,gerard melo,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"We analyze how different conceptions of lexical semantics affect sense annotations and how multiple sense inventories can be compared empirically, based on annotated text. Our study focuses on the MASC project, where data has been annotated using WordNet sense identifiers on the one hand, and FrameNet lexical units on the other. This allows us to compare the sense inventories of these lexical resources empirically rather than just theoretically, based on their glosses, leading to new insights. In particular, we compute contingency matrices and develop a novel measure, the Expected Jaccard Index, that quantifies the agreement between annotations of the same data based on two different resources even when they have different sets of categories."
W11-2027,Embedded Wizardry,2011,33,7,1,1,721,rebecca passonneau,Proceedings of the {SIGDIAL} 2011 Conference,0,"This paper presents a progressively challenging series of experiments that investigate clarification subdialogues to resolve the words in noisy transcriptions of user utterances. We focus on user utterances where the user's specific intent requires little additional inference, given sufficient understanding of the form. We learned decision-making strategies for a dialogue manager from run-time features of our spoken dialogue system and from observation of human wizards we had embedded within it. Results show that noisy ASR can be resolved based on predictions from context about what a user might say, and that dialogue management strategies for clarifications of linguistic form benefit from access to features from spoken language understanding."
W11-2029,Learning to Balance Grounding Rationales for Dialogue Systems,2011,20,8,2,0,44241,joshua gordon,Proceedings of the {SIGDIAL} 2011 Conference,0,This paper reports on an experiment that investigates clarification subdialogues in intentionally noisy speech recognition. The architecture learns weights for mixtures of grounding strategies from examples provided by a human wizard embedded in the system. Results indicate that the architecture learns to eliminate misunderstandings reliably despite high word error rate.
W11-2038,{PARADISE}-style Evaluation of a Human-Human Library Corpus,2011,15,3,1,1,721,rebecca passonneau,Proceedings of the {SIGDIAL} 2011 Conference,0,"We apply a PARADISE-style evaluation to a human-human dialogue corpus that was collected to support the design of a spoken dialogue system for library transactions. The book request dialogue task we investigate is informational in nature: a book request is considered successful if the librarian is able to identify a specific book for the patron. PARADISE assumes that user satisfaction can be modeled as a regression over task success and dialogue costs. The PARADISE model we derive includes features that characterize two types of qualitative features. The first has to do with the specificity of the communicative goals, given a request for an item. The second has to do with the number and location of overlapping turns, which can sometimes signal rapport between the speakers."
W11-0705,Sentiment Analysis of {T}witter Data,2011,15,883,5,0.833333,37109,apoorv agarwal,Proceedings of the Workshop on Language in Social Media ({LSM} 2011),0,"We examine sentiment analysis on Twitter data. The contributions of this paper are: (1) We introduce POS-specific prior polarity features. (2) We explore the use of a tree kernel to obviate the need for tedious feature engineering. The new features (in conjunction with previously proposed features) and the tree kernel perform approximately at the same level, both outperforming the state-of-the-art baseline."
W10-1803,Annotation Scheme for Social Network Extraction from Text,2010,11,13,3,0.833333,37109,apoorv agarwal,Proceedings of the Fourth Linguistic Annotation Workshop,0,"We are interested in extracting social networks from text. We present a novel annotation scheme for a new type of event, called social event, in which two people participate such that at least one of them is cognizant of the other. We compare our scheme in detail to the ACE scheme. We perform a detailed analysis of interannotator agreement, which shows that our annotations are reliable."
W10-1806,{A}nveshan: A Framework for Analysis of Multiple Annotators{'} Labeling Behavior,2010,31,20,2,0,1435,vikas bhardwaj,Proceedings of the Fourth Linguistic Annotation Workshop,0,"Manual annotation of natural language to capture linguistic information is essential for NLP tasks involving supervised machine learning of semantic knowledge. Judgements of meaning can be more or less subjective, in which case instead of a single correct label, the labels assigned might vary among annotators based on the annotators' knowledge, age, gender, intuitions, background, and so on. We introduce a framework Anveshan, where we investigate annotator behavior to find outliers, cluster annotators by behavior, and identify confusable labels. We also investigate the effectiveness of using trained annotators versus a larger number of untrained annotators on a word sense annotation task. The annotation data comes from a word sense disambiguation task for polysemous words, annotated by both trained annotators and untrained annotators from Amazon's Mechanical turk. Our results show that Anveshan is effective in uncovering patterns in annotator behavior, and we also show that trained annotators are superior to a larger number of untrained annotators for this task."
P10-2013,The Manually Annotated Sub-Corpus: A Community Resource for and by the People,2010,15,54,4,0,16303,nancy ide,Proceedings of the {ACL} 2010 Conference Short Papers,0,"The Manually Annotated Sub-Corpus (MASC) project provides data and annotations to serve as the base for a communitywide annotation effort of a subset of the American National Corpus. The MASC infrastructure enables the incorporation of contributed annotations into a single, usable format that can then be analyzed as it is or ported to any of a variety of other formats. MASC includes data from a much wider variety of genres than existing multiply-annotated corpora of English, and the project is committed to a fully open model of distribution, without restriction, for all data and annotations produced or contributed. As such, MASC is the first large-scale, open, community-based effort to create much needed language resources for NLP. This paper describes the MASC project, its corpus and annotations, and serves as a call for contributions of data and annotations from the language processing community."
N10-1126,Learning about Voice Search for Spoken Dialogue Systems,2010,28,14,1,1,721,rebecca passonneau,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"In a Wizard-of-Oz experiment with multiple wizard subjects, each wizard viewed automated speech recognition (ASR) results for utterances whose interpretation is critical to task success: requests for books by title from a library database. To avoid non-understandings, the wizard directly queried the application database with the ASR hypothesis (voice search). To learn how to avoid misunderstandings, we investigated how wizards dealt with uncertainty in voice search results. Wizards were quite successful at selecting the correct title from query results that included a match. The most successful wizard could also tell when the query results did not contain the requested title. Our learned models of the best wizard's behavior combine features available to wizards with some that are not, such as recognition confidence and acoustic model scores."
passonneau-etal-2010-word,Word Sense Annotation of Polysemous Words by Multiple Annotators,2010,27,19,1,1,721,rebecca passonneau,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We describe results of a word sense annotation task using WordNet, involving half a dozen well-trained annotators on ten polysemous words for three parts of speech. One hundred sentences for each word were annotated. Annotators had the same level of training and experience, but interannotator agreement (IA) varied across words. There was some effect of part of speech, with higher agreement on nouns and adjectives, but within the words for each part of speech there was wide variation. This variation in IA does not correlate with number of senses in the inventory, or the number of senses actually selected by annotators. In fact, IA was sometimes quite high for words with many senses. We claim that the IA variation is due to the word meanings, contexts of use, and individual differences among annotators. We find some correlation of IA with sense confusability as measured by a sense confusion threshhold (CT). Data mining for association rules on a flattened data representation indicating each annotator's sense choices identifies outliers for some words, and systematic differences among pairs of annotators on others."
gordon-passonneau-2010-evaluation,An Evaluation Framework for Natural Language Understanding in Spoken Dialogue Systems,2010,19,6,2,0,44241,joshua gordon,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We present an evaluation framework to enable developers of information seeking, transaction based spoken dialogue systems to compare the robustness of natural language understanding (NLU) approaches across varying levels of word error rate and contrasting domains. We develop statistical and semantic parsing based approaches to dialogue act identification and concept retrieval. Voice search is used in each approach to ultimately query the database. Included in the framework is a method for developers to bootstrap a representative pseudo-corpus, which is used to estimate NLU performance in a new domain. We illustrate the relative merits of these NLU techniques by contrasting our statistical NLU approach with a semantic parsing method over two contrasting applications, our CheckItOut library system and the deployed LetÂs Go Public! system, across four levels of word error rate. We find that with respect to both dialogue act identification and concept retrieval, our statistical NLU approach is more likely to robustly accommodate the freer form, less constrained utterances of CheckItOut at higher word error rates than is possible with semantic parsing."
W09-3953,Contrasting the Interaction Structure of an Email and a Telephone Corpus: A Machine Learning Approach to Annotation of Dialogue Function Units,2009,26,15,2,0,34691,jun hu,Proceedings of the {SIGDIAL} 2009 Conference,0,"We present a dialogue annotation scheme for both spoken and written interaction, and use it in a telephone transaction corpus and an email corpus. We train classifiers, comparing regular SVM and structured SVM against a heuristic baseline. We provide a novel application of structured SVM to predicting relations between instance pairs."
W09-2402,Making Sense of Word Sense Variation,2009,31,15,1,1,721,rebecca passonneau,Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions ({SEW}-2009),0,"We present a pilot study of word-sense annotation using multiple annotators, relatively polysemous words, and a heterogenous corpus. Annotators selected senses for words in context, using an annotation interface that presented WordNet senses. Interannotator agreement (IA) results show that annotators agree well or not, depending primarily on the individual words and their general usage properties. Our focus is on identifying systematic differences across words and annotators that can account for IA variation. We identify three lexical use factors: semantic specificity of the context, sense concreteness, and similarity of senses. We discuss systematic differences in sense selection across annotators, and present the use of association rules to mine the data for systematic differences across annotators."
ide-etal-2008-masc,{MASC}: the Manually Annotated Sub-Corpus of {A}merican {E}nglish,2008,14,34,5,0,16303,nancy ide,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"To answer the critical need for sharable, reusable annotated resources with rich linguistic annotations, we are developing a Manually Annotated Sub-Corpus (MASC) including texts from diverse genres and manual annotations or manually-validated annotations for multiple levels, including WordNet senses and FrameNet frames and frame elements, both of which have become significant resources in the international computational linguistics community. To derive maximal benefit from the semantic information provided by these resources, the MASC will also include manually-validated shallow parses and named entities, which will enable linking WordNet senses and FrameNet frames within the same sentences into more complex semantic structures and, because named entities will often be the role fillers of FrameNet frames, enrich the semantic and pragmatic information derivable from the sub-corpus. All MASC annotations will be published with detailed inter-annotator agreement measures. The MASC and its annotations will be freely downloadable from the ANC website, thus providing maximum accessibility for researchers from around the globe."
passonneau-etal-2008-relation,Relation between Agreement Measures on Human Labeling and Machine Learning Performance: Results from an Art History Domain,2008,18,7,1,1,721,rebecca passonneau,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"We discuss factors that affect human agreement on a semantic labeling task in the art history domain, based on the results of four experiments where we varied the number of labels annotators could assign, the number of annotators, the type and amount of training they received, and the size of the text span being labeled. Using the labelings from one experiment involving seven annotators, we investigate the relation between interannotator agreement and machine learning performance. We construct binary classifiers and vary the training and test data by swapping the labelings from the seven annotators. First, we find performance is often quite good despite lower than recommended interannotator agreement. Second, we find that on average, learning performance for a given functional semantic category correlates with the overall agreement among the seven annotators for that category. Third, we find that learning performance on the data from a given annotator does not correlate with the quality of that annotatorÂs labeling. We offer recommendations for the use of labeled data in machine learning, and argue that learners should attempt to accommodate human variation. We also note implications for large scale corpus annotation projects that deal with similarly subjective phenomena."
W07-2312,Measuring Variability in Sentence Ordering for News Summarization,2007,17,23,2,0,16057,nitin madnani,Proceedings of the Eleventh {E}uropean Workshop on Natural Language Generation ({ENLG} 07),0,"The issue of sentence ordering is an important one for natural language tasks such as multi-document summarization, yet there has not been a quantitative exploration of the range of acceptable sentence orderings for short texts. We present results of a sentence reordering experiment with three experimental conditions. Our findings indicate a very high degree of variability in the orderings that the eighteen subjects produce. In addition, the variability of reorderings is significantly greater when the initial ordering seen by subjects is different from the original summary. We conclude that evaluation of sentence ordering should use multiple reference orderings. Our evaluation presents several metrics that might prove useful in assessing against multiple references. We conclude with a deeper set of questions: (a) what sorts of independent assessments of quality of the different reference orderings could be made and (b) whether a large enough test set would obviate the need for such independent means of quality assessment."
passonneau-etal-2006-inter,Inter-annotator Agreement on a Multilingual Semantic Annotation Task,2006,15,40,1,1,721,rebecca passonneau,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"Six sites participated in the Interlingual Annotation of Multilingual Text Corpora (IAMTC) project (Dorr et al., 2004; Farwell et al., 2004; Mitamura et al., 2004). Parsed versions of English translations of news articles in Arabic, French, Hindi, Japanese, Korean and Spanish were annotated by up to ten annotators. Their task was to match open-class lexical items (nouns, verbs, adjectives, adverbs) to one or more concepts taken from the Omega ontology (Philpot et al., 2003), and to identify theta roles for verb arguments. The annotated corpus is intended to be a resource for meaning-based approaches to machine translation. Here we discuss inter-annotator agreement for the corpus. The annotation task is characterized by annotatorsÂ freedom to select multiple concepts or roles per lexical item. As a result, the annotation categories are sets, the number of which is bounded only by the number of distinct annotator-lexical item pairs. We use a reliability metric designed to handle partial agreement between sets. The best results pertain to the part of the ontology derived from WordNet. We examine change over the course of the project, differences among annotators, and differences across parts of speech. Our results suggest a strong learning effect early in the project."
passonneau-2006-measuring,Measuring Agreement on Set-valued Items ({MASI}) for Semantic and Pragmatic Annotation,2006,12,82,1,1,721,rebecca passonneau,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"Annotation projects dealing with complex semantic or pragmatic phenomena face the dilemma of creating annotation schemes that oversimplify the phenomena, or that capture distinctions conventional reliability metrics cannot measure adequately. The solution to the dilemma is to develop metrics that quantify the decisions that annotators are asked to make. This paper discusses MASI, distance metric for comparing sets, and illustrates its use in quantifying the reliability of a specific dataset. Annotations of Summary Content Units (SCUs) generate models referred to as pyramids which can be used to evaluate unseen human summaries or machine summaries. The paper presents reliability results for five pairs of pyramids created for document sets from the 2003 Document Understanding Conference (DUC). The annotators worked independently of each other. Differences between application of MASI to pyramid annotation and its previous application to co-reference annotation are discussed. In addition, it is argued that a paradigmatic reliability study should relate measures of inter-annotator agreement to independent assessments, such as significance tests of the annotated variables with respect to other phenomena. In effect, what counts as sufficiently reliable intera-annotator agreement depends on the use the annotated data will be put to."
passonneau-etal-2006-climb,{CL}i{MB} {T}ool{K}it: A Case Study of Iterative Evaluation in a Multidisciplinary Project,2006,7,0,1,1,721,rebecca passonneau,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"Digital image collections in libraries and other curatorial institutions grow too rapidly to create new descriptive metadata for subject matter search or browsing. CLiMB (Computational Linguistics for Metadata Building) was a project designed to address this dilemma that involved computer scientists, linguists, librarians, and art librarians. The CLiMB project followed an iterative evaluation model: each next phase of the project emerged from the results of an evaluation. After assembling a suite of text processing tools to be used in extracting metada, we conducted a formative evaluation with thirteen participants, using a survey in which we varied the order and type of four conditions under which respondents would propose or select image search terms. Results of the formative evaluation led us to conclude that a CLiMB ToolKit would work best if its main function was to propose terms for users to review. After implementing a prototype ToolKit using a browser interface, we conducted an evaluation with ten experts. Users found the ToolKit very habitable, remained consistently satisfied throughout a lengthy evaluation, and selected a large number of terms per image."
N04-1019,Evaluating Content Selection in Summarization: The Pyramid Method,2004,10,387,2,0,8333,ani nenkova,Proceedings of the Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics: {HLT}-{NAACL} 2004,0,None
passonneau-2004-computing,Computing Reliability for Coreference Annotation,2004,10,81,1,1,721,rebecca passonneau,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,None
P01-1066,Quantitative and Qualitative Evaluation of Darpa Communicator Spoken Dialogue Systems,2001,15,160,2,0,6000,marilyn walker,Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,1,"This paper describes the application of the PARADISE evaluation framework to the corpus of 662 human-computer dialogues collected in the June 2000 Darpa Communicator data collection. We describe results based on the standard logfile metrics as well as results based on additional qualitative metrics derived using the DATE dialogue act tagging scheme. We show that performance models derived via using the standard metrics can account for 37% of the variance in user satisfaction, and that the addition of DATE metrics improved the models by an absolute 5%."
H01-1015,{DATE}: A Dialogue Act Tagging Scheme for Evaluation of Spoken Dialogue Systems,2001,27,50,2,0,6000,marilyn walker,Proceedings of the First International Conference on Human Language Technology Research,0,"This paper describes a dialogue act tagging scheme developed for the purpose of providing finer-grained quantitative dialogue metrics for comparing and evaluating DARPA COMMUNICATOR spoken dialogue systems. We show that these dialogue act metrics can be used to quantify the amount of effort spent in a dialogue maintaining the channel of communication or, establishing the frame for communication, as opposed to actually carrying out the travel planning task that the system is designed to support. We show that the use of these metrics results in a 7% improvement in the fit in models of user satisfaction. We suggest that dialogue act metrics can ultimately support more focused qualitative analysis of the role of various dialogue strategy parameters, e.g. initiative, across dialogue systems, thus clarifying what development paths might be feasible for enhancing user satisfaction in future versions of these systems."
W97-0903,Software Re-Use and Evolution in Text Generation Applications,1997,11,1,2,0,54349,karen kukich,From Research to Commercial Applications: Making {NLP} Work in Practice,0,None
W97-0210,Investigating Complementary Methods for Verb Sense Pruning,1997,22,5,3,0,49187,hongyan jing,"Tagging Text with Lexical Semantics: Why, What, and How?",0,"We present an approach for tagging verb sense that combines a domain-independent method based on subcategorization and alternations with a domain-dependent method utilizing statistically extracted verb clusters. Initial results indicate that verb senses can be pruned for highly polysemous verbs by up to 74% by the first method and by up to 85% by the second method. 1 I n t r o d u c t i o n Much work in natural language processing is predicated on the notion that linguistic usage varies sufficiently across different situations of language use that systems can be tailored to a particular sublanguage variety (Kittredge and Lehrberger, 1982). Biber (1993) presents evidence that a corpus restricted to one or two language registers would exclude much of the English language by narrowing the lexicon, verb tense and aspect, and syntactic complexity. Such observations inform the increasing trend towards analysis of homogeneous corpora to identify linguistic constraints for use in systems intended to understand or generate coherent discourse. Recent work in this vein includes identification of lexical constraints from textual tutorial dialogue (Moser and Moore, 1995), constraints on illocutionary act type from spoken task-oriented dialogue (Allen et al., 1995), prosodic constraints from spoken information-seeking monologues (Hirschberg and Nakatani, 1996), and constraints on referring expressions from spoken narrative monologue (Passonneau, 1996). Related work suggests that constraints of different types are interdependent (Biber, 1993; Passonneau and Litman, forthcoming), hence should be investigated together. Our ultimate goal is to develop methods to tag lexical semantic features in discourse corpora in order to enhance extraction of constraints of the sort just listed. Two types of investigations that would undoubtedly be enhanced are explorations of the interrelation of lexical cohesion and global discourse structure (Morris and Hirst, 1991; Hearst, 1994), and identification of lexicaliza-: tion patterns for domain-specific concepts (Robin, 1994). In this paper, we propose a two-pronged approach to an initial step in lexical semantic tagging, pruning the search space for polysemous verbs. Rather than attempting to identify unique word senses, we aim for the more realistic goal of pruning sense information. We will then incrementally evaluate the utility of tagging corpora with pruned sense sets for different types of discourse. We begin with verbs on the hypothesis that verb sense distinctions correlate with syntactic properties of verbs (Levin, 1993). Our initial results indicate that domain-independent syntactic information reduces potential verb senses for multiply polysemous verbs (five or more WordNet senses) by more than 50%. In Section 2, we outline our first method, based on domain-independent lexical knowledge, presenting results from an analysis of thousands of verbs. In the section following that, we present our complementary method, a technique utilizing verb clusters automatically computed from corpus data. In the conclusion, we discuss how the combination of the two methods increases the performance of our system and enhances the robustness of the final results. 2 E x p l o i t i n g d o m a i n i n d e p e n d e n t s y n t a c t i c c l u e s A given word may have n distinct senses and appear within m different syntactic contexts, but typically, not all n x m combinations are valid. The syntactic context can partly disambiguate the semantic content. For example, when the verb question has a that-clause complement, it cannot have the sense of ask, but rather must have the sense of challenge. To identify such interacting syntactic and semantic constraints at the lexical level, we utilize three knowledge bases for verbs: * The COMLEX database (Grishman et al., 1994; Macleod and Grishman, 1995), which includes detailed subcategorization information for each verb, and some adjectives and nouns. []"
J97-1005,Discourse Segmentation by Human and Automated Means,1997,62,171,1,1,721,rebecca passonneau,Computational Linguistics,0,"The need to model the relation between discourse structure and linguistic features of utterances is almost universally acknowledged in the literature on discourse. However, there is only weak consensus on what the units of discourse structure are, or the criteria for recognizing and generating them. We present quantitative results of a two-part study using a corpus of spontaneous, narrative monologues. The first part of our paper presents a method for empirically validating multitutterance units referred to as discourse segments. We report highly significant results of segmentations performed by naive subjects, where a commonsense notion of speaker intention is the segmentation criterion. In the second part of our study, data abstracted from the subjects' segmentations serve as a target for evaluating two sets of algorithms that use utterance features to perform segmentation. On the first algorithm set, we evaluate and compare the correlation of discourse segmentation with three types of linguistic cues (referential noun phrases, cue words, and pauses). We then develop a second set using two methods: error analysis and machine learning. Testing the new algorithms on a new data set shows that when multiple sources of linguistic knowledge are used concurrently, algorithm performance improves."
J96-2008,Book Reviews: Representing Time in Natural Language: The Dynamic Interpretation of Tense and Aspect,1996,3,0,1,1,721,rebecca passonneau,Computational Linguistics,0,None
P95-1015,Combining Multiple Knowledge Sources for Discourse Segmentation,1995,31,99,2,1,6782,diane litman,33rd Annual Meeting of the Association for Computational Linguistics,1,"We predict discourse segment boundaries from linguistic features of utterances, using a corpus of spoken narratives as data. We present two methods for developing segmentation algorithms from training data: hand tuning and machine learning. When multiple types of features are used, results approach human performance on an independent test set (both methods), and using cross-validation (machine learning)."
H94-1095,Extracting Constraints on Word Usage from Large Text Corpora,1994,-1,-1,2,0.237614,895,kathleen mckeown,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop held at {P}lainsboro, {N}ew {J}ersey, {M}arch 8-11, 1994",0,None
W93-0216,Empirical Evidence for Intention-Based Discourse Segmentation,1993,-1,-1,2,1,6782,diane litman,Intentionality and Structure in Discourse Relations,0,None
P93-1010,Temporal Centering,1993,24,46,2,0,55082,megumi kameyama,31st Annual Meeting of the Association for Computational Linguistics,1,"We present a semantic and pragmatic account of the anaphoric properties of past and perfect that improves on previous work by integrating discourse structure, aspectual type, surface structure and commonsense knowledge. A novel aspect of our account is that we distinguish between two kinds of temporal intervals in the interpretation of temporal operators --- discourse reference intervals and event intervals. This distinction makes it possible to develop an analogy between centering and temporal centering, which operates on discourse reference intervals. Our temporal property-sharing principle is a defeasible inference rule on the logical form. Along with lexical and causal reasoning, it plays a role in incrementally resolving underspecified aspects of the event structure representation of an utterance against the current context."
P93-1020,Intention-Based Segmentation: Human Reliability and Correlation With Linguistic Cues,1993,23,133,1,1,721,rebecca passonneau,31st Annual Meeting of the Association for Computational Linguistics,1,"Certain spans of utterances in a discourse, referred to here as segments, are widely assumed to form coherent units. Further, the segmental structure of discourse has been claimed to constrain and be constrained by many phenomena. However, there is weak consensus on the nature of segments and the criteria for recognizing or generating them. We present quantitative results of a two part study using a corpus of spontaneous, narrative monologues. The first part evaluates the statistical reliability of human segmentation of our corpus, where speaker intention is the segmentation criterion. We then use the subjects' segmentations to evaluate the correlation of discourse segmentation with three linguistic cues (referential noun phrases, cue words, and pauses), using information retrieval metrics."
H93-1085,Extracting Constraints on Word Usage from Large Text Corpora,1993,-1,-1,2,0.271773,895,kathleen mckeown,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993",0,None
H92-1103,Extracting Constraints on Word Usage from Large Text Corpora,1992,0,1,3,0.271773,895,kathleen mckeown,"Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992",0,Our research focuses on the identification of word usage constraints from large text corpora. Such constraints are useful both for the problem of selecting vocabulary for language generation and for disambiguating lexical meaning in interpretation. We are developing systems that can automatically extract such constraints from corpora and empirical methods for analyzing text. Identified constraints will be represented in a lexicon that will be tested computationally as part of a natural language system. We are also identifying lexical constraints for machine translation using the aligned Hansard corpus as training data and are identifying many-to-many word alignments.
P91-1009,"Some Facts About Centers, Indexicals, and Demonstratives",1991,11,7,1,1,721,rebecca passonneau,29th Annual Meeting of the Association for Computational Linguistics,1,"Certain pronoun contexts are argued to establish a local center (LC), i.e., a conventionalized indexical similar to 1st/2nd pers. pronouns. Demonstrative pronouns, also indexicals, are shown to access entities that are not LCs because they lack discourse relevance or because they are not yet in the universe of discourse."
P89-1007,Getting at Discourse Referents,1989,13,23,1,1,721,rebecca passonneau,27th Annual Meeting of the Association for Computational Linguistics,1,"I examine how discourse anaphoric uses of the definite pronoun it contrast with similar uses of the demonstrative pronoun that. Their distinct contexts of use are characterized in terms of two contextual features---persistence of grammatical subject and persistence of grammatical form---which together demonstrate very clearly the interrelation among lexical choice, grammatical choices and the dimension of time in signalling the dynamic attentional state of a discourse."
P88-1002,Sentence Fragments Regular Structures,1988,20,19,4,0,55259,marcia linebarger,26th Annual Meeting of the Association for Computational Linguistics,1,"This paper describes an analysis of telegraphic fragments as regular structures (not errors) handled by minimal extensions to a system designed for processing the standard language. The modular approach which has been implemented in the Unisys natural language processing system PUNDIT is based on a division of labor in which syntax regulates the occurrence and distribution of elided elements, and semantics and pragmatics use the system's standard mechanisms to interpret them."
J88-2005,A Computational Model of the Semantics of Tense and Aspect,1988,29,91,1,1,721,rebecca passonneau,Computational Linguistics,0,"The PUNDIT natural-language system processes references to situations and the intervals over which they hold using an algorithm that integrates the analysis of tense and aspect. For each tensed clause, PUNDIT processes the main verb and its grammatical categories of tense, perfect, and progressive in order to extract three complementary pieces of temporal information. The first is whether a situation has actual time associated with it. Secondly, for each situation that is presumed to take place in actual time, PUNDIT represents its temporal structure as one of three situation types: a state, process, or transition event. The temporal structures of each of these situation types consist of one or more intervals. The intervals are characterized by two features: kinesis, which pertains to their internal structure, and boundedness, which constrains the manner in which they get located in time. Thirdly, the computation of temporal location exploits the three temporal indices proposed in Reichenbach 1947: event time, speech time, and reference time. Here, however, event time is formulated as a single component of the full temporal structure of a situation in order to provide an integrated treatment of tense and aspect."
P87-1003,Situations and Intervals,1987,15,24,1,1,721,rebecca passonneau,25th Annual Meeting of the Association for Computational Linguistics,1,"The PUNDIT system processes natural language descriptions of situations and the intervals over which they hold using an algorithm that integrates aspect and tense logic. It analyses the tense and aspect of the main verb to generate representations of three types of situations---states, processes and events---and to locate the situations with respect to the time at which the text was produced. Each situation type has a distinct temporal structure, represented in terms of one or more intervals. Further, every interval has two features whose different values capture the aspectual differences between the three different situation types. Capturing these differences makes it possible to represent very precisely the times for which predications are asserted to hold."
P87-1019,Nominalizations in {PUNDIT},1987,10,34,3,0,55257,deborah dahl,25th Annual Meeting of the Association for Computational Linguistics,1,"This paper describes the treatment of nominalizations in the PUNDIT text processing system. A single semantic definition is used for both nominalizations and the verbs to which they are related, with the same semantic roles, decompositions, and selectional restrictions on the semantic roles. However, because syntactically nominalizations are noun phrases, the processing which produces the semantic representation is different in several respects from that used for clauses. (1) The rules relating the syntactic positions of the constituents to the roles that they can fill are different. (2) The fact that nominalizations are untensed while clauses normally are tensed means that an alternative treatment of time is required for nominalizations. (3) Because none of the arguments of a nominalization is syntactically obligatory, some differences in the control of the filling of roles are required, in particular, roles can be filled as part of reference resolution for the nominalization. The differences in processing are captured by allowing the semantic interpreter to operate in two different modes, one for clauses, and one for nominalizations. Because many nominalizations are noun-noun compounds, this approach also addresses this problem, by suggesting a way of dealing with one relatively tractable subset of noun-noun compounds."
