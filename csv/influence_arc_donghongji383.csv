2020.aacl-main.13,P15-1039,0,0.0162876,"FNs High-order feature representation ... ... ... w1 i-th refining iteration . Token representation + ... w2 . b . ⊕ + + Score representation Biaffine attention Argument representation Predicate representation Token representation ... ... ... BiLSTM encoder Input representation Figure 1: The overview of the graph-based high-order model for end-to-end SRL. The dotted-line green box is our proposed high-order refining module. 3 parallel and differentiable. We conduct experiments on the dependencybased Chinese SRL datasets, including CoNLL09 (Hajiˇc et al., 2009), and Universal Proposition Bank (Akbik et al., 2015; Akbik and Li, 2016). Results show that the graph-based end-to-end model with our proposed high-order refining consistently brings task improvements, compared with baselines, achieving state-of-the-art results for Chinese end-to-end SRL. 2 Task formulation. Following prior end-to-end SRL work (He et al., 2018a; Li et al., 2019), we treat the task as predicate-argument-role triplets prediction. Given an input sentence S = {w1 , · · · , wn }, the system is expected to output a set of triplets Y ∈ P × A × R, where P = {p1 , · · · , pm } are all possible predicate tokens, A = {a1 , · · · , al } a"
2020.aacl-main.13,P16-4001,0,0.0129893,"re representation ... ... ... w1 i-th refining iteration . Token representation + ... w2 . b . ⊕ + + Score representation Biaffine attention Argument representation Predicate representation Token representation ... ... ... BiLSTM encoder Input representation Figure 1: The overview of the graph-based high-order model for end-to-end SRL. The dotted-line green box is our proposed high-order refining module. 3 parallel and differentiable. We conduct experiments on the dependencybased Chinese SRL datasets, including CoNLL09 (Hajiˇc et al., 2009), and Universal Proposition Bank (Akbik et al., 2015; Akbik and Li, 2016). Results show that the graph-based end-to-end model with our proposed high-order refining consistently brings task improvements, compared with baselines, achieving state-of-the-art results for Chinese end-to-end SRL. 2 Task formulation. Following prior end-to-end SRL work (He et al., 2018a; Li et al., 2019), we treat the task as predicate-argument-role triplets prediction. Given an input sentence S = {w1 , · · · , wn }, the system is expected to output a set of triplets Y ∈ P × A × R, where P = {p1 , · · · , pm } are all possible predicate tokens, A = {a1 , · · · , al } are all associated arg"
2020.aacl-main.13,W09-1206,0,0.124709,"Missing"
2020.aacl-main.13,C18-1233,0,0.315901,"Missing"
2020.aacl-main.13,D15-1112,0,0.0502389,"Missing"
2020.aacl-main.13,P00-1065,0,0.290351,"ate-of-the-art results for Chinese end-to-end SRL. 2 Task formulation. Following prior end-to-end SRL work (He et al., 2018a; Li et al., 2019), we treat the task as predicate-argument-role triplets prediction. Given an input sentence S = {w1 , · · · , wn }, the system is expected to output a set of triplets Y ∈ P × A × R, where P = {p1 , · · · , pm } are all possible predicate tokens, A = {a1 , · · · , al } are all associated argument tokens, and R are the corresponding role labels for each ai , including a null label  indicating no relation between a pair of predicate argument. Related Work Gildea and Jurafsky (2000) pioneer the task of semantic role labeling, as a shallow semantic parsing. Earlier efforts are paid for designing hand-crafted discrete features with machine learning classifiers (Pradhan et al., 2005; Punyakanok et al., 2008; Zhao et al., 2009). Later, a great deal of work takes advantages of neural networks with distributed features (FitzGerald et al., 2015; Roth and Lapata, 2016; Marcheggiani and Titov, 2017; Strubell et al., 2018). On the other hand, many previous work shows that integrating syntactic tree structure can greatly facilitate SRL (Marcheggiani et al., 2017; He et al., 2018b;"
2020.aacl-main.13,P18-2058,0,0.495864,"the shallow semantic parsing aiming to detect the semantic predicates and their argument roles in texts, plays a core role in natural language processing (NLP) community (Pradhan et al., 2005; Zhao et al., 2009; Lei et al., 2015; Xia et al., 2019b). SRL is traditionally handled by two pipeline steps: predicate identification (Scheible, 2010) and argument role labeling (Pradhan et al., 2005). More recently, growing interests are paid for developing end-to-end SRL, achieving both two subtasks, i.e., recognizing all possible predicates together with their arguments jointly, via one single model (He et al., 2018a). The end-to-end joint architecture can greatly alleviate the error propagation problem, thus helping to achieve better task performance. Currently, the end-to-end SRL methods largely are graph-based ∗ Corresponding author. neural models, enumerating all possible predicates and their arguments exhaustively (He et al., 2018a; Cai et al., 2018; Li et al., 2019). However, these first-order models that only consider one predicateargument pair at a time can be limited to short-term features and local decisions, thus being subjective to long-range dependency issues existing at large surface distan"
2020.aacl-main.13,P18-1192,0,0.174786,"the shallow semantic parsing aiming to detect the semantic predicates and their argument roles in texts, plays a core role in natural language processing (NLP) community (Pradhan et al., 2005; Zhao et al., 2009; Lei et al., 2015; Xia et al., 2019b). SRL is traditionally handled by two pipeline steps: predicate identification (Scheible, 2010) and argument role labeling (Pradhan et al., 2005). More recently, growing interests are paid for developing end-to-end SRL, achieving both two subtasks, i.e., recognizing all possible predicates together with their arguments jointly, via one single model (He et al., 2018a). The end-to-end joint architecture can greatly alleviate the error propagation problem, thus helping to achieve better task performance. Currently, the end-to-end SRL methods largely are graph-based ∗ Corresponding author. neural models, enumerating all possible predicates and their arguments exhaustively (He et al., 2018a; Cai et al., 2018; Li et al., 2019). However, these first-order models that only consider one predicateargument pair at a time can be limited to short-term features and local decisions, thus being subjective to long-range dependency issues existing at large surface distan"
2020.aacl-main.13,P82-1020,0,0.685296,"Missing"
2020.aacl-main.13,N15-1121,0,0.0596954,"Missing"
2020.aacl-main.13,D19-1099,0,0.0207091,"Missing"
2020.aacl-main.13,D19-1094,0,0.0237449,"Missing"
2020.aacl-main.13,D19-1544,0,0.0114143,"t architecture can greatly alleviate the error propagation problem, thus helping to achieve better task performance. Currently, the end-to-end SRL methods largely are graph-based ∗ Corresponding author. neural models, enumerating all possible predicates and their arguments exhaustively (He et al., 2018a; Cai et al., 2018; Li et al., 2019). However, these first-order models that only consider one predicateargument pair at a time can be limited to short-term features and local decisions, thus being subjective to long-range dependency issues existing at large surface distances between arguments (Chen et al., 2019; Lyu et al., 2019). This makes it imperative to capture the global interactions between multiple predicates and arguments. In this paper, based on the graph-based model architecture, we propose to further learn the higherorder interaction between all predicate-argument pairs by performing iterative refining for the underlying token representations. Figure 1 illustrates the overall framework of our method. The BiLSTM encoder (Hochreiter and Schmidhuber, 1997) first encodes the inputs into the initial token representations for producing predicate and argument representations, respectively. The"
2020.aacl-main.13,K17-1041,0,0.0160274,"ment. Related Work Gildea and Jurafsky (2000) pioneer the task of semantic role labeling, as a shallow semantic parsing. Earlier efforts are paid for designing hand-crafted discrete features with machine learning classifiers (Pradhan et al., 2005; Punyakanok et al., 2008; Zhao et al., 2009). Later, a great deal of work takes advantages of neural networks with distributed features (FitzGerald et al., 2015; Roth and Lapata, 2016; Marcheggiani and Titov, 2017; Strubell et al., 2018). On the other hand, many previous work shows that integrating syntactic tree structure can greatly facilitate SRL (Marcheggiani et al., 2017; He et al., 2018b; Zhang et al., 2019; Fei et al., 2020b). Prior studies traditionally separate SRL into two individual subtasks, i.e., predicate disambiguation and argument role labeling, mostly conducting only the argument role labeling based on the preFramework 3.1 Baseline Graph-based SRL Model Our baseline SRL model is mostly from He et al. (2018a). First, we obtain the vector representation xw t of each word wt from pre-trained embeddings. We then make use of the part-of-speech (POS) tag for each word, and use its embedding xpos t . A convolutional neural networks (CNNs) is used to enco"
2020.aacl-main.13,P81-1022,0,0.210004,"Missing"
2020.aacl-main.13,D17-1159,0,0.0129306,"e all associated argument tokens, and R are the corresponding role labels for each ai , including a null label  indicating no relation between a pair of predicate argument. Related Work Gildea and Jurafsky (2000) pioneer the task of semantic role labeling, as a shallow semantic parsing. Earlier efforts are paid for designing hand-crafted discrete features with machine learning classifiers (Pradhan et al., 2005; Punyakanok et al., 2008; Zhao et al., 2009). Later, a great deal of work takes advantages of neural networks with distributed features (FitzGerald et al., 2015; Roth and Lapata, 2016; Marcheggiani and Titov, 2017; Strubell et al., 2018). On the other hand, many previous work shows that integrating syntactic tree structure can greatly facilitate SRL (Marcheggiani et al., 2017; He et al., 2018b; Zhang et al., 2019; Fei et al., 2020b). Prior studies traditionally separate SRL into two individual subtasks, i.e., predicate disambiguation and argument role labeling, mostly conducting only the argument role labeling based on the preFramework 3.1 Baseline Graph-based SRL Model Our baseline SRL model is mostly from He et al. (2018a). First, we obtain the vector representation xw t of each word wt from pre-trai"
2020.aacl-main.13,2020.acl-main.627,1,0.895133,"ata. In this study, we focus on the Chinese SRL. We show that our proposed high-order refining mechanism can be especially beneficial for such lower-resource language. Meanwhile, our proposed refining process is fully 100 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 100–105 c December 4 - 7, 2020. 2020 Association for Computational Linguistics identified predicate (Pradhan et al., 2005; Zhao et al., 2009; FitzGerald et al., 2015; He et al., 2018b; Fei et al., 2020a). More recently, several researches consider the end-to-end solution that handles both two subtasks by one single model. All of them employs graph-based neural model, exhaustively enumerating all the possible predicate and argument mentions, as well as their relations (He et al., 2018a; Cai et al., 2018; Li et al., 2019; Xia et al., 2019a). Most of these end-to-end models, however, are first-order, considering merely one predicate-argument pair at a time. In this work, we propose a high-order refining mechanism to reinforce the graph-based end-to-end method. Note that most of the existing SR"
2020.aacl-main.13,P05-1072,0,0.548626,"tion, which are later used to update the original token representations. After several iterations of refinement, the underlying token representations can be enriched with globally interacted features. Our high-order model achieves state-of-the-art results on Chinese SRL data, including CoNLL09 and Universal Proposition Bank, meanwhile relieving the long-range dependency issues. 1 Introduction Semantic role labeling (SRL), as the shallow semantic parsing aiming to detect the semantic predicates and their argument roles in texts, plays a core role in natural language processing (NLP) community (Pradhan et al., 2005; Zhao et al., 2009; Lei et al., 2015; Xia et al., 2019b). SRL is traditionally handled by two pipeline steps: predicate identification (Scheible, 2010) and argument role labeling (Pradhan et al., 2005). More recently, growing interests are paid for developing end-to-end SRL, achieving both two subtasks, i.e., recognizing all possible predicates together with their arguments jointly, via one single model (He et al., 2018a). The end-to-end joint architecture can greatly alleviate the error propagation problem, thus helping to achieve better task performance. Currently, the end-to-end SRL method"
2020.aacl-main.13,J08-2005,0,0.194788,"ence S = {w1 , · · · , wn }, the system is expected to output a set of triplets Y ∈ P × A × R, where P = {p1 , · · · , pm } are all possible predicate tokens, A = {a1 , · · · , al } are all associated argument tokens, and R are the corresponding role labels for each ai , including a null label  indicating no relation between a pair of predicate argument. Related Work Gildea and Jurafsky (2000) pioneer the task of semantic role labeling, as a shallow semantic parsing. Earlier efforts are paid for designing hand-crafted discrete features with machine learning classifiers (Pradhan et al., 2005; Punyakanok et al., 2008; Zhao et al., 2009). Later, a great deal of work takes advantages of neural networks with distributed features (FitzGerald et al., 2015; Roth and Lapata, 2016; Marcheggiani and Titov, 2017; Strubell et al., 2018). On the other hand, many previous work shows that integrating syntactic tree structure can greatly facilitate SRL (Marcheggiani et al., 2017; He et al., 2018b; Zhang et al., 2019; Fei et al., 2020b). Prior studies traditionally separate SRL into two individual subtasks, i.e., predicate disambiguation and argument role labeling, mostly conducting only the argument role labeling based"
2020.aacl-main.13,P16-1113,0,0.0198419,"= {a1 , · · · , al } are all associated argument tokens, and R are the corresponding role labels for each ai , including a null label  indicating no relation between a pair of predicate argument. Related Work Gildea and Jurafsky (2000) pioneer the task of semantic role labeling, as a shallow semantic parsing. Earlier efforts are paid for designing hand-crafted discrete features with machine learning classifiers (Pradhan et al., 2005; Punyakanok et al., 2008; Zhao et al., 2009). Later, a great deal of work takes advantages of neural networks with distributed features (FitzGerald et al., 2015; Roth and Lapata, 2016; Marcheggiani and Titov, 2017; Strubell et al., 2018). On the other hand, many previous work shows that integrating syntactic tree structure can greatly facilitate SRL (Marcheggiani et al., 2017; He et al., 2018b; Zhang et al., 2019; Fei et al., 2020b). Prior studies traditionally separate SRL into two individual subtasks, i.e., predicate disambiguation and argument role labeling, mostly conducting only the argument role labeling based on the preFramework 3.1 Baseline Graph-based SRL Model Our baseline SRL model is mostly from He et al. (2018a). First, we obtain the vector representation xw t"
2020.aacl-main.13,scheible-2010-evaluation,0,0.0317859,"enriched with globally interacted features. Our high-order model achieves state-of-the-art results on Chinese SRL data, including CoNLL09 and Universal Proposition Bank, meanwhile relieving the long-range dependency issues. 1 Introduction Semantic role labeling (SRL), as the shallow semantic parsing aiming to detect the semantic predicates and their argument roles in texts, plays a core role in natural language processing (NLP) community (Pradhan et al., 2005; Zhao et al., 2009; Lei et al., 2015; Xia et al., 2019b). SRL is traditionally handled by two pipeline steps: predicate identification (Scheible, 2010) and argument role labeling (Pradhan et al., 2005). More recently, growing interests are paid for developing end-to-end SRL, achieving both two subtasks, i.e., recognizing all possible predicates together with their arguments jointly, via one single model (He et al., 2018a). The end-to-end joint architecture can greatly alleviate the error propagation problem, thus helping to achieve better task performance. Currently, the end-to-end SRL methods largely are graph-based ∗ Corresponding author. neural models, enumerating all possible predicates and their arguments exhaustively (He et al., 2018a;"
2020.aacl-main.13,D18-1548,0,0.013186,"ns, and R are the corresponding role labels for each ai , including a null label  indicating no relation between a pair of predicate argument. Related Work Gildea and Jurafsky (2000) pioneer the task of semantic role labeling, as a shallow semantic parsing. Earlier efforts are paid for designing hand-crafted discrete features with machine learning classifiers (Pradhan et al., 2005; Punyakanok et al., 2008; Zhao et al., 2009). Later, a great deal of work takes advantages of neural networks with distributed features (FitzGerald et al., 2015; Roth and Lapata, 2016; Marcheggiani and Titov, 2017; Strubell et al., 2018). On the other hand, many previous work shows that integrating syntactic tree structure can greatly facilitate SRL (Marcheggiani et al., 2017; He et al., 2018b; Zhang et al., 2019; Fei et al., 2020b). Prior studies traditionally separate SRL into two individual subtasks, i.e., predicate disambiguation and argument role labeling, mostly conducting only the argument role labeling based on the preFramework 3.1 Baseline Graph-based SRL Model Our baseline SRL model is mostly from He et al. (2018a). First, we obtain the vector representation xw t of each word wt from pre-trained embeddings. We then"
2020.aacl-main.13,D19-1541,0,0.0510887,"resentations. After several iterations of refinement, the underlying token representations can be enriched with globally interacted features. Our high-order model achieves state-of-the-art results on Chinese SRL data, including CoNLL09 and Universal Proposition Bank, meanwhile relieving the long-range dependency issues. 1 Introduction Semantic role labeling (SRL), as the shallow semantic parsing aiming to detect the semantic predicates and their argument roles in texts, plays a core role in natural language processing (NLP) community (Pradhan et al., 2005; Zhao et al., 2009; Lei et al., 2015; Xia et al., 2019b). SRL is traditionally handled by two pipeline steps: predicate identification (Scheible, 2010) and argument role labeling (Pradhan et al., 2005). More recently, growing interests are paid for developing end-to-end SRL, achieving both two subtasks, i.e., recognizing all possible predicates together with their arguments jointly, via one single model (He et al., 2018a). The end-to-end joint architecture can greatly alleviate the error propagation problem, thus helping to achieve better task performance. Currently, the end-to-end SRL methods largely are graph-based ∗ Corresponding author. neura"
2020.aacl-main.13,N19-1075,0,0.0541632,"resentations. After several iterations of refinement, the underlying token representations can be enriched with globally interacted features. Our high-order model achieves state-of-the-art results on Chinese SRL data, including CoNLL09 and Universal Proposition Bank, meanwhile relieving the long-range dependency issues. 1 Introduction Semantic role labeling (SRL), as the shallow semantic parsing aiming to detect the semantic predicates and their argument roles in texts, plays a core role in natural language processing (NLP) community (Pradhan et al., 2005; Zhao et al., 2009; Lei et al., 2015; Xia et al., 2019b). SRL is traditionally handled by two pipeline steps: predicate identification (Scheible, 2010) and argument role labeling (Pradhan et al., 2005). More recently, growing interests are paid for developing end-to-end SRL, achieving both two subtasks, i.e., recognizing all possible predicates together with their arguments jointly, via one single model (He et al., 2018a). The end-to-end joint architecture can greatly alleviate the error propagation problem, thus helping to achieve better task performance. Currently, the end-to-end SRL methods largely are graph-based ∗ Corresponding author. neura"
2020.aacl-main.13,D19-1057,0,0.0106896,"pioneer the task of semantic role labeling, as a shallow semantic parsing. Earlier efforts are paid for designing hand-crafted discrete features with machine learning classifiers (Pradhan et al., 2005; Punyakanok et al., 2008; Zhao et al., 2009). Later, a great deal of work takes advantages of neural networks with distributed features (FitzGerald et al., 2015; Roth and Lapata, 2016; Marcheggiani and Titov, 2017; Strubell et al., 2018). On the other hand, many previous work shows that integrating syntactic tree structure can greatly facilitate SRL (Marcheggiani et al., 2017; He et al., 2018b; Zhang et al., 2019; Fei et al., 2020b). Prior studies traditionally separate SRL into two individual subtasks, i.e., predicate disambiguation and argument role labeling, mostly conducting only the argument role labeling based on the preFramework 3.1 Baseline Graph-based SRL Model Our baseline SRL model is mostly from He et al. (2018a). First, we obtain the vector representation xw t of each word wt from pre-trained embeddings. We then make use of the part-of-speech (POS) tag for each word, and use its embedding xpos t . A convolutional neural networks (CNNs) is used to encode Chinese characters inside a word xc"
2020.aacl-main.13,D09-1004,0,0.472239,"used to update the original token representations. After several iterations of refinement, the underlying token representations can be enriched with globally interacted features. Our high-order model achieves state-of-the-art results on Chinese SRL data, including CoNLL09 and Universal Proposition Bank, meanwhile relieving the long-range dependency issues. 1 Introduction Semantic role labeling (SRL), as the shallow semantic parsing aiming to detect the semantic predicates and their argument roles in texts, plays a core role in natural language processing (NLP) community (Pradhan et al., 2005; Zhao et al., 2009; Lei et al., 2015; Xia et al., 2019b). SRL is traditionally handled by two pipeline steps: predicate identification (Scheible, 2010) and argument role labeling (Pradhan et al., 2005). More recently, growing interests are paid for developing end-to-end SRL, achieving both two subtasks, i.e., recognizing all possible predicates together with their arguments jointly, via one single model (He et al., 2018a). The end-to-end joint architecture can greatly alleviate the error propagation problem, thus helping to achieve better task performance. Currently, the end-to-end SRL methods largely are graph"
2020.acl-main.397,W13-2322,0,0.401941,"Missing"
2020.acl-main.397,P19-1284,0,0.383702,"in Figure 1, AMR prefers to select the coordination (i.e. “and”) as the root, which is different from syntactic dependencies (i.e. “came”). Given the above observations, we investigate the effectiveness of latent syntactic dependencies for 4306 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4306–4319 c July 5 - 10, 2020. 2020 Association for Computational Linguistics AMR parsing. Different from existing work (Wang et al., 2016), which uses a dependency parser to provide explicit syntactic structures, we make use of a two-parameter distribution (Bastings et al., 2019) to induce latent graphs, which is differentiable under reparameterization (Kingma and Welling, 2014). We thus build a end-to-end model for AMR parsing with induced latent dependency structures as a middle layer, which is tuned in AMR training and thus can be more aligned to the need of AMR structure. For better investigating the correlation between induced and gold syntax, and better combine the strengths, we additionally consider fusing gold and induced structural dependencies into an align-free AMR parser (Zhang et al., 2019a). Specifically, we first obtain the input sentence’s syntactic de"
2020.acl-main.397,E17-1051,0,0.182802,"sent semantic relations. AMR introduces re-entrance relation to depict the node reuse in the graphs. It has been adopted in downstream NLP tasks, including text summarization (Liu et al., 2015; Dohare and Karnick, 2017), question answering (Mitra and Baral, 2016) and machine translation (Jones et al., 2012; Song et al., 2019). AMR parsing aims to transform natural language sentences into AMR semantic graphs. Similar to constituent parsing and dependency parsing (Nivre, 2008; Dozat and Manning, 2017), AMR parsers mainly employ two parsing techniques: transitionbased parsing (Wang et al., 2016; Damonte et al., 2017; Wang and Xue, 2017; Liu et al., 2018; Guo and Lu, 2018) use a sequence of transition actions † cc nsubj Part of work was done when the author was visiting Westlake University * Corresponding author. to incrementally construct the graph, while graphbased parsing (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a; Cai and Lam, 2019) divides the task into concept identification and relation extraction stages and then generate a full AMR graph with decoding algorithms such as greedy and maximum spanning tree (MST). Additionally, reinforcement learning (Naseem et al., 2019) and sequ"
2020.acl-main.397,P14-1134,0,0.0497626,"on (Jones et al., 2012; Song et al., 2019). AMR parsing aims to transform natural language sentences into AMR semantic graphs. Similar to constituent parsing and dependency parsing (Nivre, 2008; Dozat and Manning, 2017), AMR parsers mainly employ two parsing techniques: transitionbased parsing (Wang et al., 2016; Damonte et al., 2017; Wang and Xue, 2017; Liu et al., 2018; Guo and Lu, 2018) use a sequence of transition actions † cc nsubj Part of work was done when the author was visiting Westlake University * Corresponding author. to incrementally construct the graph, while graphbased parsing (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a; Cai and Lam, 2019) divides the task into concept identification and relation extraction stages and then generate a full AMR graph with decoding algorithms such as greedy and maximum spanning tree (MST). Additionally, reinforcement learning (Naseem et al., 2019) and sequence-to-sequence (Konstas et al., 2017) have been exploited in AMR parsing as well. Previous works (Wang et al., 2016; Artzi et al., 2015) shows that structural information can bring benefit to AMR parsing. Illustrated by Figure 1, for example syntactic dependencies can convey the main"
2020.acl-main.397,D18-1198,0,0.169396,"to depict the node reuse in the graphs. It has been adopted in downstream NLP tasks, including text summarization (Liu et al., 2015; Dohare and Karnick, 2017), question answering (Mitra and Baral, 2016) and machine translation (Jones et al., 2012; Song et al., 2019). AMR parsing aims to transform natural language sentences into AMR semantic graphs. Similar to constituent parsing and dependency parsing (Nivre, 2008; Dozat and Manning, 2017), AMR parsers mainly employ two parsing techniques: transitionbased parsing (Wang et al., 2016; Damonte et al., 2017; Wang and Xue, 2017; Liu et al., 2018; Guo and Lu, 2018) use a sequence of transition actions † cc nsubj Part of work was done when the author was visiting Westlake University * Corresponding author. to incrementally construct the graph, while graphbased parsing (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a; Cai and Lam, 2019) divides the task into concept identification and relation extraction stages and then generate a full AMR graph with decoding algorithms such as greedy and maximum spanning tree (MST). Additionally, reinforcement learning (Naseem et al., 2019) and sequence-to-sequence (Konstas et al., 2017) have been exploit"
2020.acl-main.397,P19-1024,0,0.0188529,"ng et al. (2019a) extend a pointer generator (See et al., 2017), which can generate a node multiple times without alignment through the copy mechanism. With regards to latent structure, Naradowsky et al. (2012) couples syntactically-oriented NLP tasks to combinatorially constrained hidden syntactic representations. Bowman et al. (2016); Yogatama et al. (2017) and Choi et al. (2018) generate unsupervised constituent tree for text classification. The latent constituent trees are shallower than human annotated, and it can boost the performance of downstream NLP tasks (e.g., text classification). Guo et al. (2019) and Ji et al. (2019) employ selfattention and bi-affine attention mechanism respectively to generate soft connected graphs, and then adopt GNNs to encode the soft structure to take advantage from the structural information to their works. GCN and its variants are increasingly applied in embedding syntactic and semantic structures in NLP tasks (Kipf and Welling, 2017; Marcheggiani and Titov, 2017; Damonte and Cohen, 2019). Syntactic-GCN tries to alleviate the error propagation in external parsers with gates mechanism, it encodes both relations and labels with the gates, and filters the output"
2020.acl-main.397,C12-1083,0,0.0603499,"Missing"
2020.acl-main.397,P17-1014,0,0.211142,"17; Liu et al., 2018; Guo and Lu, 2018) use a sequence of transition actions † cc nsubj Part of work was done when the author was visiting Westlake University * Corresponding author. to incrementally construct the graph, while graphbased parsing (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a; Cai and Lam, 2019) divides the task into concept identification and relation extraction stages and then generate a full AMR graph with decoding algorithms such as greedy and maximum spanning tree (MST). Additionally, reinforcement learning (Naseem et al., 2019) and sequence-to-sequence (Konstas et al., 2017) have been exploited in AMR parsing as well. Previous works (Wang et al., 2016; Artzi et al., 2015) shows that structural information can bring benefit to AMR parsing. Illustrated by Figure 1, for example syntactic dependencies can convey the main predicate-argument structure. However, dependency structural information may be noisy due to the error propagation of external parsers. Moreover, AMR concentrates on semantic relations, which can be different from syntactic dependencies. For instance, in Figure 1, AMR prefers to select the coordination (i.e. “and”) as the root, which is different fro"
2020.acl-main.397,P19-1232,0,0.0340383,"Missing"
2020.acl-main.397,P82-1020,0,0.807868,"Missing"
2020.acl-main.397,P19-1450,0,0.0798049,"Missing"
2020.acl-main.397,P19-1237,0,0.0164503,"nd a pointer generator (See et al., 2017), which can generate a node multiple times without alignment through the copy mechanism. With regards to latent structure, Naradowsky et al. (2012) couples syntactically-oriented NLP tasks to combinatorially constrained hidden syntactic representations. Bowman et al. (2016); Yogatama et al. (2017) and Choi et al. (2018) generate unsupervised constituent tree for text classification. The latent constituent trees are shallower than human annotated, and it can boost the performance of downstream NLP tasks (e.g., text classification). Guo et al. (2019) and Ji et al. (2019) employ selfattention and bi-affine attention mechanism respectively to generate soft connected graphs, and then adopt GNNs to encode the soft structure to take advantage from the structural information to their works. GCN and its variants are increasingly applied in embedding syntactic and semantic structures in NLP tasks (Kipf and Welling, 2017; Marcheggiani and Titov, 2017; Damonte and Cohen, 2019). Syntactic-GCN tries to alleviate the error propagation in external parsers with gates mechanism, it encodes both relations and labels with the gates, and filters the output of each GCN layer ove"
2020.acl-main.397,N15-1114,0,0.0536212,"sentence “The boy came and left”. The dashed lines denote the connected relations in the syntactic dependencies but not appear in the AMR graph. Introduction Abstract Meaning Representations (AMRs) (Banarescu et al., 2013) model sentence level semantics as rooted, directed, acyclic graphs. Nodes in the graph are concepts which represent the events, objects and features of the input sentence, and edges between nodes represent semantic relations. AMR introduces re-entrance relation to depict the node reuse in the graphs. It has been adopted in downstream NLP tasks, including text summarization (Liu et al., 2015; Dohare and Karnick, 2017), question answering (Mitra and Baral, 2016) and machine translation (Jones et al., 2012; Song et al., 2019). AMR parsing aims to transform natural language sentences into AMR semantic graphs. Similar to constituent parsing and dependency parsing (Nivre, 2008; Dozat and Manning, 2017), AMR parsers mainly employ two parsing techniques: transitionbased parsing (Wang et al., 2016; Damonte et al., 2017; Wang and Xue, 2017; Liu et al., 2018; Guo and Lu, 2018) use a sequence of transition actions † cc nsubj Part of work was done when the author was visiting Westlake Univer"
2020.acl-main.397,D18-1264,0,0.479564,"-entrance relation to depict the node reuse in the graphs. It has been adopted in downstream NLP tasks, including text summarization (Liu et al., 2015; Dohare and Karnick, 2017), question answering (Mitra and Baral, 2016) and machine translation (Jones et al., 2012; Song et al., 2019). AMR parsing aims to transform natural language sentences into AMR semantic graphs. Similar to constituent parsing and dependency parsing (Nivre, 2008; Dozat and Manning, 2017), AMR parsers mainly employ two parsing techniques: transitionbased parsing (Wang et al., 2016; Damonte et al., 2017; Wang and Xue, 2017; Liu et al., 2018; Guo and Lu, 2018) use a sequence of transition actions † cc nsubj Part of work was done when the author was visiting Westlake University * Corresponding author. to incrementally construct the graph, while graphbased parsing (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a; Cai and Lam, 2019) divides the task into concept identification and relation extraction stages and then generate a full AMR graph with decoding algorithms such as greedy and maximum spanning tree (MST). Additionally, reinforcement learning (Naseem et al., 2019) and sequence-to-sequence (Konstas et al., 2017"
2020.acl-main.397,P18-1037,0,0.710402,"Song et al., 2019). AMR parsing aims to transform natural language sentences into AMR semantic graphs. Similar to constituent parsing and dependency parsing (Nivre, 2008; Dozat and Manning, 2017), AMR parsers mainly employ two parsing techniques: transitionbased parsing (Wang et al., 2016; Damonte et al., 2017; Wang and Xue, 2017; Liu et al., 2018; Guo and Lu, 2018) use a sequence of transition actions † cc nsubj Part of work was done when the author was visiting Westlake University * Corresponding author. to incrementally construct the graph, while graphbased parsing (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a; Cai and Lam, 2019) divides the task into concept identification and relation extraction stages and then generate a full AMR graph with decoding algorithms such as greedy and maximum spanning tree (MST). Additionally, reinforcement learning (Naseem et al., 2019) and sequence-to-sequence (Konstas et al., 2017) have been exploited in AMR parsing as well. Previous works (Wang et al., 2016; Artzi et al., 2015) shows that structural information can bring benefit to AMR parsing. Illustrated by Figure 1, for example syntactic dependencies can convey the main predicate-argument s"
2020.acl-main.397,P14-5010,0,0.0123999,"interpret the probabilistic relations between the input words in AMR parsing by generating the latent graph2 . 2 Baseline: Align-Free AMR Parsing We adopt the parser of Zhang et al. (2019a) as our baseline, which treats AMR parsing as sequenceto-graph transduction. 2.1 Task Formalization Our baseline splits AMR parsing into a two-stage procedure: concept identification and edge prediction. The first task aims to identify the concepts (nodes) in AMR graph from input tokens, and the second task is designed to predict semantic relations between identified concepts. 1 We employ Stanford CoreNLP (Manning et al., 2014) to get the dependencies. 2 Our code will be available at: https://github. com/zhouqiji/ACL2020_AMR_Parsing. Formally, for a given input sequence of words w = hw1 , ..., wn i, the goal of concept identification in our baseline is sequentially predicting the concept nodes u = hu1 , ..., um i in the output AMR graph, and deterministically assigning corresponding indices d = hd1 , ..., dm i. P (u) = m Y P (ui |u<i , d<i , w), i=1 After identifying the concept nodes c and their corresponding indices d, we predict the semantic relations in the searching space R(u). X Predict(u) = arg max score(ui ,"
2020.acl-main.397,D17-1159,0,0.244453,"d wi at the l − th layer is: n X (l) (l−1) hi = σ( A˜ij W (l) hj /di + b(l) ), Training Similar to our baseline (Zhang et al., 2019a), we linearize the AMR concepts nodes by a pre-order traversal over the training dataset. We obtain gradient estimates of E(φ, θ) through Monte Carlo sampling from: E(φ, θ) = EU (0,I) [log P (node|ut , gφ (u, w), θ) + log Pt (head|uk , gφ (u, w), θ) j=1 ˜ = A + I with the n × n identity matrix where A Pn ˜ I, di = j=1 Aij is the degree of word wi in the graph for normalizing the activation to avoid the word representation with significantly different magnitudes (Marcheggiani and Titov, 2017; Kipf and Welling, 2017), and σ is a nonlinear activation function. In order to take benefits from both explicit and latent structural information in AMR parsing, we extend the Syntactic-GCN (Marcheggiani and Titov, 2017; Zhang et al., 2018b) with a graph fusion layer and omit labels in the graph (i.e. we only consider the connected relation in GCN). Specifically, we propose to merge the parsed syntactic dependencies and sampled latent graph through a graph fusion layer: F = πL + (1 − π)D where π is trainable gate variables are calculated via the sigmoid function, D and L are the parsed synta"
2020.acl-main.397,J08-4003,0,0.0123942,"Nodes in the graph are concepts which represent the events, objects and features of the input sentence, and edges between nodes represent semantic relations. AMR introduces re-entrance relation to depict the node reuse in the graphs. It has been adopted in downstream NLP tasks, including text summarization (Liu et al., 2015; Dohare and Karnick, 2017), question answering (Mitra and Baral, 2016) and machine translation (Jones et al., 2012; Song et al., 2019). AMR parsing aims to transform natural language sentences into AMR semantic graphs. Similar to constituent parsing and dependency parsing (Nivre, 2008; Dozat and Manning, 2017), AMR parsers mainly employ two parsing techniques: transitionbased parsing (Wang et al., 2016; Damonte et al., 2017; Wang and Xue, 2017; Liu et al., 2018; Guo and Lu, 2018) use a sequence of transition actions † cc nsubj Part of work was done when the author was visiting Westlake University * Corresponding author. to incrementally construct the graph, while graphbased parsing (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a; Cai and Lam, 2019) divides the task into concept identification and relation extraction stages and then generate a full AMR grap"
2020.acl-main.397,P17-1186,0,0.0948125,"Missing"
2020.acl-main.397,D14-1162,0,0.0858128,"ching space R(u). X Predict(u) = arg max score(ui , uj ), r∈R(u) (u ,u )∈r i j where r = {(ui , uj ) |1 ≤ i, j ≤ m} is a set of directed relations between concept nodes. 2.2 Align-Free Concept Identification Our baseline extends the pointer-generator network with self-copy mechanism for concept identification (See et al., 2017; Zhang et al., 2018a). The extended model can copy the nodes not only from the source text, but also from the previously generated list of nodes on the target side. The concept identifier firstly encodes the input sentence into concatenated vector embeddings with GloVe (Pennington et al., 2014), BERT (Devlin et al., 2019), POS (part-of-speech) and characterlevel (Kim et al., 2016) embeddings. Subsequently, we encode the embedded sentence by a two-layer bidirectional LSTM (Schuster and Paliwal, 1997; Hochreiter and Schmidhuber, 1997): → − ← −l l−1 l l hli = [ f l (hl−1 i , hi−1 ); f (hi , hi+1 )], where hli is the l-th layer encoded hidden state at the time step i and h0i is the embedded token wi . Different from the encoding stage, the decoder does not use pre-trained BERT embeddings, but employs a two-layer LSTM to generate the decoding hidden state slt at each time step: l slt = f"
2020.acl-main.397,D15-1136,0,0.0514325,"Missing"
2020.acl-main.397,P17-1099,0,0.648157,"cept nodes u = hu1 , ..., um i in the output AMR graph, and deterministically assigning corresponding indices d = hd1 , ..., dm i. P (u) = m Y P (ui |u<i , d<i , w), i=1 After identifying the concept nodes c and their corresponding indices d, we predict the semantic relations in the searching space R(u). X Predict(u) = arg max score(ui , uj ), r∈R(u) (u ,u )∈r i j where r = {(ui , uj ) |1 ≤ i, j ≤ m} is a set of directed relations between concept nodes. 2.2 Align-Free Concept Identification Our baseline extends the pointer-generator network with self-copy mechanism for concept identification (See et al., 2017; Zhang et al., 2018a). The extended model can copy the nodes not only from the source text, but also from the previously generated list of nodes on the target side. The concept identifier firstly encodes the input sentence into concatenated vector embeddings with GloVe (Pennington et al., 2014), BERT (Devlin et al., 2019), POS (part-of-speech) and characterlevel (Kim et al., 2016) embeddings. Subsequently, we encode the embedded sentence by a two-layer bidirectional LSTM (Schuster and Paliwal, 1997; Hochreiter and Schmidhuber, 1997): → − ← −l l−1 l l hli = [ f l (hl−1 i , hi−1 ); f (hi , hi+1"
2020.acl-main.397,Q19-1002,1,0.831473,"AMR graph. Introduction Abstract Meaning Representations (AMRs) (Banarescu et al., 2013) model sentence level semantics as rooted, directed, acyclic graphs. Nodes in the graph are concepts which represent the events, objects and features of the input sentence, and edges between nodes represent semantic relations. AMR introduces re-entrance relation to depict the node reuse in the graphs. It has been adopted in downstream NLP tasks, including text summarization (Liu et al., 2015; Dohare and Karnick, 2017), question answering (Mitra and Baral, 2016) and machine translation (Jones et al., 2012; Song et al., 2019). AMR parsing aims to transform natural language sentences into AMR semantic graphs. Similar to constituent parsing and dependency parsing (Nivre, 2008; Dozat and Manning, 2017), AMR parsers mainly employ two parsing techniques: transitionbased parsing (Wang et al., 2016; Damonte et al., 2017; Wang and Xue, 2017; Liu et al., 2018; Guo and Lu, 2018) use a sequence of transition actions † cc nsubj Part of work was done when the author was visiting Westlake University * Corresponding author. to incrementally construct the graph, while graphbased parsing (Flanigan et al., 2014; Lyu and Titov, 2018"
2020.acl-main.397,D12-1074,0,0.0294963,"ls. Lyu and Titov (2018) treat the alignments as an latent variable for their probabilistic model, which jointly obtains the concepts, relations and alignments variables. Sequence-to-sequence AMR parsers transform AMR graphs into serialized sequences by external traversal rules, and then restore the generated the AMR sequence to avoid aligning issue (Konstas et al., 2017; van Noord and Bos, 2017). Moreover, Zhang et al. (2019a) extend a pointer generator (See et al., 2017), which can generate a node multiple times without alignment through the copy mechanism. With regards to latent structure, Naradowsky et al. (2012) couples syntactically-oriented NLP tasks to combinatorially constrained hidden syntactic representations. Bowman et al. (2016); Yogatama et al. (2017) and Choi et al. (2018) generate unsupervised constituent tree for text classification. The latent constituent trees are shallower than human annotated, and it can boost the performance of downstream NLP tasks (e.g., text classification). Guo et al. (2019) and Ji et al. (2019) employ selfattention and bi-affine attention mechanism respectively to generate soft connected graphs, and then adopt GNNs to encode the soft structure to take advantage f"
2020.acl-main.397,P19-1451,0,0.414534,"., 2016; Damonte et al., 2017; Wang and Xue, 2017; Liu et al., 2018; Guo and Lu, 2018) use a sequence of transition actions † cc nsubj Part of work was done when the author was visiting Westlake University * Corresponding author. to incrementally construct the graph, while graphbased parsing (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a; Cai and Lam, 2019) divides the task into concept identification and relation extraction stages and then generate a full AMR graph with decoding algorithms such as greedy and maximum spanning tree (MST). Additionally, reinforcement learning (Naseem et al., 2019) and sequence-to-sequence (Konstas et al., 2017) have been exploited in AMR parsing as well. Previous works (Wang et al., 2016; Artzi et al., 2015) shows that structural information can bring benefit to AMR parsing. Illustrated by Figure 1, for example syntactic dependencies can convey the main predicate-argument structure. However, dependency structural information may be noisy due to the error propagation of external parsers. Moreover, AMR concentrates on semantic relations, which can be different from syntactic dependencies. For instance, in Figure 1, AMR prefers to select the coordination"
2020.acl-main.397,D17-1129,0,0.197665,"s. AMR introduces re-entrance relation to depict the node reuse in the graphs. It has been adopted in downstream NLP tasks, including text summarization (Liu et al., 2015; Dohare and Karnick, 2017), question answering (Mitra and Baral, 2016) and machine translation (Jones et al., 2012; Song et al., 2019). AMR parsing aims to transform natural language sentences into AMR semantic graphs. Similar to constituent parsing and dependency parsing (Nivre, 2008; Dozat and Manning, 2017), AMR parsers mainly employ two parsing techniques: transitionbased parsing (Wang et al., 2016; Damonte et al., 2017; Wang and Xue, 2017; Liu et al., 2018; Guo and Lu, 2018) use a sequence of transition actions † cc nsubj Part of work was done when the author was visiting Westlake University * Corresponding author. to incrementally construct the graph, while graphbased parsing (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a; Cai and Lam, 2019) divides the task into concept identification and relation extraction stages and then generate a full AMR graph with decoding algorithms such as greedy and maximum spanning tree (MST). Additionally, reinforcement learning (Naseem et al., 2019) and sequence-to-sequence (Ko"
2020.acl-main.397,P19-1009,0,0.367829,"Missing"
2020.acl-main.397,D19-1392,0,0.175275,"Missing"
2020.acl-main.397,D18-1194,0,0.0605107,"Missing"
2020.acl-main.397,D18-1244,0,0.0942822,", ..., um i in the output AMR graph, and deterministically assigning corresponding indices d = hd1 , ..., dm i. P (u) = m Y P (ui |u<i , d<i , w), i=1 After identifying the concept nodes c and their corresponding indices d, we predict the semantic relations in the searching space R(u). X Predict(u) = arg max score(ui , uj ), r∈R(u) (u ,u )∈r i j where r = {(ui , uj ) |1 ≤ i, j ≤ m} is a set of directed relations between concept nodes. 2.2 Align-Free Concept Identification Our baseline extends the pointer-generator network with self-copy mechanism for concept identification (See et al., 2017; Zhang et al., 2018a). The extended model can copy the nodes not only from the source text, but also from the previously generated list of nodes on the target side. The concept identifier firstly encodes the input sentence into concatenated vector embeddings with GloVe (Pennington et al., 2014), BERT (Devlin et al., 2019), POS (part-of-speech) and characterlevel (Kim et al., 2016) embeddings. Subsequently, we encode the embedded sentence by a two-layer bidirectional LSTM (Schuster and Paliwal, 1997; Hochreiter and Schmidhuber, 1997): → − ← −l l−1 l l hli = [ f l (hl−1 i , hi−1 ); f (hi , hi+1 )], where hli is th"
2020.acl-main.588,S14-2135,0,0.0268622,"f our knowledge, the proposed DGEDT is the first work that jointly considers the flat textual knowledge and dependency graph empowered knowledge in a unified framework. Furthermore, unlike other aspect-based GCN models, we aggregate the aspect embeddings from multiple aspect spans which share the same mentioned aspect before feeding these embeddings into submodules. We also introduce an aspect-modified dependency graph in DGEDT. 2 Related Work Employing modern neural networks for aspectbased sequence-level sentiment classification task, such as CNNs (Kim, 2014; Johnson and Zhang, 2015), RNNs (Castellucci et al., 2014; Tang et al., 2016a), Recurrent Convolutional Neural Networks (RCNNs) (Lai et al., 2015), have already achieved excellent performance in several sentiment analysis tasks. Many attention-based RNN or CNN methods (Yang et al., 2017; Zhang and Liu, 2017; Zeng et al., 2019) are also proposed to handle sequence classification tasks. Tai et al. (2015) proposed a tree-LSTM structure which is enhanced with dependency trees or constituency trees, which outperforms traditional LSTM. Dong et al. (2014) proposed an adaptive recursive neural network using dependency trees. Since being firstly introduced i"
2020.acl-main.588,P19-1052,0,0.415174,"ressing the graph structure representation in Natural Language Processing (NLP) field. Marcheggiani and Titov (2017) proposed a GCN-based model for semantic role labeling. Vashishth et al. (2018) and Zhang et al. 6579 (2018) used GCN over dependency trees in document dating and relation classification, respectively. Yao et al. (2019) introduced GCN to text classification task with the guidance of document-word and word-word relations. Furthermore, Zhang et al. (2019) introduced aspect-based GCN to cope with aspect-level sentiment classification task using dependency graphs. On the other hand, Chen and Qian (2019) introduced and adapted Capsule Networks along with transfer learning to improve the performance of aspect-level sentiment classification. Gao et al. (2019) introduced BERT into a target-based method, and Sun et al. (2019) constructed BERT-based auxiliary sentences to further improve the performance. Attention Module Classify Max-Pooling Aspect Representation Aspect Representation Dual-transformer Structure SUM SUM Aspect Span 3 Since Transformer (Vaswani et al., 2017) and GCN are two crucial sub-modules in DGEDT, here we briefly introduce these two networks and illustrate the fact that GCN ca"
2020.acl-main.588,N19-1423,0,0.217609,"t. Note that for non-aspect words, spans involved in the computation are their original positions with the length as one. Graph (with flat) Add&Norm Add&Norm Mutual Biaffine Add&Norm Feed Forward T Add&Norm Add&Norm Self Attention sj = SU M ([hspanj ]), BiGCN Input Embedding Figure 3: A simplified demonstration of dualtransformer structure, which consists of two submodules, one is a standard transformer, another is a transformer-like structure implemented by BiGCN with the supervision of dependency graph. first utilize BiLSTM or Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) as the aspect-based encoder to extract hidden contextual representations. Then these hidden representations are fed into our proposed dual-transformer structure, with the guidance of aspect-modified dependency graph. At last, we aggregate all the aspect representations via maxpooling and apply an attention module to align contextual words and the target aspect. In this way, the model can automatically select relevant aspectsensitive contextual words with the dependency information for sentiment classification. 4.1 Aspect-based Encoder We use wk to represent the k-th word embedding. Bidirectio"
2020.acl-main.588,P14-2009,0,0.826223,"air (sentence, aspect). Aspects in our study are mostly noun phrases appearing in the ∗ Corresponding author. input sentence. As shown in Figure 1, where the comment is about the laptop review, the sentiment polarities of two aspects battery life and memory are positive and negative, respectively. Giving a specific aspect is crucial for sentiment classification owing to the situation that one sentence sometimes contains several aspects, and these aspects may have different sentiment polarities. Modern neural methods such as Recurrent Neural Networks (RNN), Convolutional Neural Networks (CNN) (Dong et al., 2014; Vo and Zhang, 2015) have already been widely applied to aspectbased sentiment classification. Inspired by the work (Tang et al., 2016a) which demonstrates the importance of modeling the semantic connection between contextual words and aspects, RNN augmented by attention mechanism (Bahdanau et al., 2015; Luong et al., 2015; Xu et al., 2015) is widely utilized in recent methods for exploring the potentially relevant words with respect to the given aspect (Yang et al., 2017; Zhang and Liu, 2017; Zeng et al., 2019; Wang et al., 2016). CNN based attention methods (Xue and Li, 2018; Li et al., 201"
2020.acl-main.588,S16-1055,0,0.0362502,"Missing"
2020.acl-main.588,D14-1181,0,0.00521618,"tate-of-the-art alternatives. To the best of our knowledge, the proposed DGEDT is the first work that jointly considers the flat textual knowledge and dependency graph empowered knowledge in a unified framework. Furthermore, unlike other aspect-based GCN models, we aggregate the aspect embeddings from multiple aspect spans which share the same mentioned aspect before feeding these embeddings into submodules. We also introduce an aspect-modified dependency graph in DGEDT. 2 Related Work Employing modern neural networks for aspectbased sequence-level sentiment classification task, such as CNNs (Kim, 2014; Johnson and Zhang, 2015), RNNs (Castellucci et al., 2014; Tang et al., 2016a), Recurrent Convolutional Neural Networks (RCNNs) (Lai et al., 2015), have already achieved excellent performance in several sentiment analysis tasks. Many attention-based RNN or CNN methods (Yang et al., 2017; Zhang and Liu, 2017; Zeng et al., 2019) are also proposed to handle sequence classification tasks. Tai et al. (2015) proposed a tree-LSTM structure which is enhanced with dependency trees or constituency trees, which outperforms traditional LSTM. Dong et al. (2014) proposed an adaptive recursive neural networ"
2020.acl-main.588,S15-2082,0,0.115857,"s. We also introduce an aspect-modified dependency graph in DGEDT. 2 Related Work Employing modern neural networks for aspectbased sequence-level sentiment classification task, such as CNNs (Kim, 2014; Johnson and Zhang, 2015), RNNs (Castellucci et al., 2014; Tang et al., 2016a), Recurrent Convolutional Neural Networks (RCNNs) (Lai et al., 2015), have already achieved excellent performance in several sentiment analysis tasks. Many attention-based RNN or CNN methods (Yang et al., 2017; Zhang and Liu, 2017; Zeng et al., 2019) are also proposed to handle sequence classification tasks. Tai et al. (2015) proposed a tree-LSTM structure which is enhanced with dependency trees or constituency trees, which outperforms traditional LSTM. Dong et al. (2014) proposed an adaptive recursive neural network using dependency trees. Since being firstly introduced in (Kipf and Welling, 2017), GCN has recently shown a great ability on addressing the graph structure representation in Natural Language Processing (NLP) field. Marcheggiani and Titov (2017) proposed a GCN-based model for semantic role labeling. Vashishth et al. (2018) and Zhang et al. 6579 (2018) used GCN over dependency trees in document dating"
2020.acl-main.588,P18-1087,0,0.119454,"l., 2019) are also proposed to handle sequence classification tasks. Tai et al. (2015) proposed a tree-LSTM structure which is enhanced with dependency trees or constituency trees, which outperforms traditional LSTM. Dong et al. (2014) proposed an adaptive recursive neural network using dependency trees. Since being firstly introduced in (Kipf and Welling, 2017), GCN has recently shown a great ability on addressing the graph structure representation in Natural Language Processing (NLP) field. Marcheggiani and Titov (2017) proposed a GCN-based model for semantic role labeling. Vashishth et al. (2018) and Zhang et al. 6579 (2018) used GCN over dependency trees in document dating and relation classification, respectively. Yao et al. (2019) introduced GCN to text classification task with the guidance of document-word and word-word relations. Furthermore, Zhang et al. (2019) introduced aspect-based GCN to cope with aspect-level sentiment classification task using dependency graphs. On the other hand, Chen and Qian (2019) introduced and adapted Capsule Networks along with transfer learning to improve the performance of aspect-level sentiment classification. Gao et al. (2019) introduced BERT in"
2020.acl-main.588,D15-1166,0,0.045031,"is crucial for sentiment classification owing to the situation that one sentence sometimes contains several aspects, and these aspects may have different sentiment polarities. Modern neural methods such as Recurrent Neural Networks (RNN), Convolutional Neural Networks (CNN) (Dong et al., 2014; Vo and Zhang, 2015) have already been widely applied to aspectbased sentiment classification. Inspired by the work (Tang et al., 2016a) which demonstrates the importance of modeling the semantic connection between contextual words and aspects, RNN augmented by attention mechanism (Bahdanau et al., 2015; Luong et al., 2015; Xu et al., 2015) is widely utilized in recent methods for exploring the potentially relevant words with respect to the given aspect (Yang et al., 2017; Zhang and Liu, 2017; Zeng et al., 2019; Wang et al., 2016). CNN based attention methods (Xue and Li, 2018; Li et al., 2018) are also proposed to enhance the phrase-level representation and achieved encouraging results. Although attention-based models have achieved promising performance on several tasks, the limitation is still obvious because attention module may highlight the irrelevant words owing to the syntactical absence. For example, gi"
2020.acl-main.588,D17-1159,0,0.231273,"g of the Association for Computational Linguistics, pages 6578–6588 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Aspect: memory Sentiment: Negative Aspect: battery life Sentiment: Positive Figure 1: A typical utterance sample of aspect-based sentiment classification task with a proper dependency tree, notice that different aspects may have different sentiment polarities. Welling, 2017) to encode the information conveyed by a dependency tree has already been investigated in several fields, e.g., modeling document-word relationships (Yao et al., 2019) and tree structures (Marcheggiani and Titov, 2017; Zhang et al., 2018). As shown in Figure 1, an annotated dependency tree of original sentence is provided, and we can observe that word-aspect pairs (bad, memory) and (great, battery life) are well established. Direct application of dependency tree has two obvious shortcomings: (1) the noisy information is inevitably introduced through the dependency tree, due to imperfect parsing performance and the casualness of input sentence; (2) GCN would be inherently inferior in modeling long-distance or disconnected words in the dependency tree. It is reported that lower performance is achieved even w"
2020.acl-main.588,D14-1162,0,0.0905359,"Missing"
2020.acl-main.588,N19-1035,0,0.0423149,"used GCN over dependency trees in document dating and relation classification, respectively. Yao et al. (2019) introduced GCN to text classification task with the guidance of document-word and word-word relations. Furthermore, Zhang et al. (2019) introduced aspect-based GCN to cope with aspect-level sentiment classification task using dependency graphs. On the other hand, Chen and Qian (2019) introduced and adapted Capsule Networks along with transfer learning to improve the performance of aspect-level sentiment classification. Gao et al. (2019) introduced BERT into a target-based method, and Sun et al. (2019) constructed BERT-based auxiliary sentences to further improve the performance. Attention Module Classify Max-Pooling Aspect Representation Aspect Representation Dual-transformer Structure SUM SUM Aspect Span 3 Since Transformer (Vaswani et al., 2017) and GCN are two crucial sub-modules in DGEDT, here we briefly introduce these two networks and illustrate the fact that GCN can be considered as a specialized Transformer. Assume that there are three input matrices Q ∈ n×d R k , K ∈ Rm×dk , V ∈ Rm×dv , which represent the queries, keys and values respectively. n and m are the length of two inputs"
2020.acl-main.588,P15-1150,0,0.106087,"Missing"
2020.acl-main.588,C16-1311,0,0.823941,"in Figure 1, where the comment is about the laptop review, the sentiment polarities of two aspects battery life and memory are positive and negative, respectively. Giving a specific aspect is crucial for sentiment classification owing to the situation that one sentence sometimes contains several aspects, and these aspects may have different sentiment polarities. Modern neural methods such as Recurrent Neural Networks (RNN), Convolutional Neural Networks (CNN) (Dong et al., 2014; Vo and Zhang, 2015) have already been widely applied to aspectbased sentiment classification. Inspired by the work (Tang et al., 2016a) which demonstrates the importance of modeling the semantic connection between contextual words and aspects, RNN augmented by attention mechanism (Bahdanau et al., 2015; Luong et al., 2015; Xu et al., 2015) is widely utilized in recent methods for exploring the potentially relevant words with respect to the given aspect (Yang et al., 2017; Zhang and Liu, 2017; Zeng et al., 2019; Wang et al., 2016). CNN based attention methods (Xue and Li, 2018; Li et al., 2018) are also proposed to enhance the phrase-level representation and achieved encouraging results. Although attention-based models have"
2020.acl-main.588,D16-1021,0,0.783206,"in Figure 1, where the comment is about the laptop review, the sentiment polarities of two aspects battery life and memory are positive and negative, respectively. Giving a specific aspect is crucial for sentiment classification owing to the situation that one sentence sometimes contains several aspects, and these aspects may have different sentiment polarities. Modern neural methods such as Recurrent Neural Networks (RNN), Convolutional Neural Networks (CNN) (Dong et al., 2014; Vo and Zhang, 2015) have already been widely applied to aspectbased sentiment classification. Inspired by the work (Tang et al., 2016a) which demonstrates the importance of modeling the semantic connection between contextual words and aspects, RNN augmented by attention mechanism (Bahdanau et al., 2015; Luong et al., 2015; Xu et al., 2015) is widely utilized in recent methods for exploring the potentially relevant words with respect to the given aspect (Yang et al., 2017; Zhang and Liu, 2017; Zeng et al., 2019; Wang et al., 2016). CNN based attention methods (Xue and Li, 2018; Li et al., 2018) are also proposed to enhance the phrase-level representation and achieved encouraging results. Although attention-based models have"
2020.acl-main.588,P18-1149,0,0.0117792,", 2017; Zeng et al., 2019) are also proposed to handle sequence classification tasks. Tai et al. (2015) proposed a tree-LSTM structure which is enhanced with dependency trees or constituency trees, which outperforms traditional LSTM. Dong et al. (2014) proposed an adaptive recursive neural network using dependency trees. Since being firstly introduced in (Kipf and Welling, 2017), GCN has recently shown a great ability on addressing the graph structure representation in Natural Language Processing (NLP) field. Marcheggiani and Titov (2017) proposed a GCN-based model for semantic role labeling. Vashishth et al. (2018) and Zhang et al. 6579 (2018) used GCN over dependency trees in document dating and relation classification, respectively. Yao et al. (2019) introduced GCN to text classification task with the guidance of document-word and word-word relations. Furthermore, Zhang et al. (2019) introduced aspect-based GCN to cope with aspect-level sentiment classification task using dependency graphs. On the other hand, Chen and Qian (2019) introduced and adapted Capsule Networks along with transfer learning to improve the performance of aspect-level sentiment classification. Gao et al. (2019) introduced BERT in"
2020.acl-main.588,D16-1058,0,0.0952734,"ent Neural Networks (RNN), Convolutional Neural Networks (CNN) (Dong et al., 2014; Vo and Zhang, 2015) have already been widely applied to aspectbased sentiment classification. Inspired by the work (Tang et al., 2016a) which demonstrates the importance of modeling the semantic connection between contextual words and aspects, RNN augmented by attention mechanism (Bahdanau et al., 2015; Luong et al., 2015; Xu et al., 2015) is widely utilized in recent methods for exploring the potentially relevant words with respect to the given aspect (Yang et al., 2017; Zhang and Liu, 2017; Zeng et al., 2019; Wang et al., 2016). CNN based attention methods (Xue and Li, 2018; Li et al., 2018) are also proposed to enhance the phrase-level representation and achieved encouraging results. Although attention-based models have achieved promising performance on several tasks, the limitation is still obvious because attention module may highlight the irrelevant words owing to the syntactical absence. For example, given the sentence “it has a bad memory but a great battery life.” and aspect “battery life”, attention module may still assign a large weight to word “bad” rather than “great”, which adversely leads to a wrong sen"
2020.acl-main.588,D19-1464,0,0.640422,"l representation and achieved encouraging results. Although attention-based models have achieved promising performance on several tasks, the limitation is still obvious because attention module may highlight the irrelevant words owing to the syntactical absence. For example, given the sentence “it has a bad memory but a great battery life.” and aspect “battery life”, attention module may still assign a large weight to word “bad” rather than “great”, which adversely leads to a wrong sentiment polarity prediction. To take advantages of syntactical information among aspects and contextual words, Zhang et al. (2019) proposed a novel aspect-based GCN method which incorporates dependency tree into the attention models. Actually, using GCN (Kipf and 6578 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6578–6588 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Aspect: memory Sentiment: Negative Aspect: battery life Sentiment: Positive Figure 1: A typical utterance sample of aspect-based sentiment classification task with a proper dependency tree, notice that different aspects may have different sentiment polarities. Welling, 2017) to encode th"
2020.acl-main.588,E17-2091,0,0.270018,", a dualtransformer structure is devised in DGEDT to support mutual reinforcement between the flat representation learning and graph-based representation learning. The idea is to allow the dependency graph to guide the representation learning of the transformer encoder and vice versa. The results on five datasets demonstrate that the proposed DGEDT outperforms all state-of-the-art alternatives with a large margin. 1 Introduction Aspect-based or aspect-level sentiment classification is a popular task with the purpose of identifying the sentiment polarity of the given aspect (Yang et al., 2017; Zhang and Liu, 2017; Zeng et al., 2019). The goal is to predict the sentiment polarity of a given pair (sentence, aspect). Aspects in our study are mostly noun phrases appearing in the ∗ Corresponding author. input sentence. As shown in Figure 1, where the comment is about the laptop review, the sentiment polarities of two aspects battery life and memory are positive and negative, respectively. Giving a specific aspect is crucial for sentiment classification owing to the situation that one sentence sometimes contains several aspects, and these aspects may have different sentiment polarities. Modern neural method"
2020.acl-main.588,D18-1244,0,0.0339878,"tational Linguistics, pages 6578–6588 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Aspect: memory Sentiment: Negative Aspect: battery life Sentiment: Positive Figure 1: A typical utterance sample of aspect-based sentiment classification task with a proper dependency tree, notice that different aspects may have different sentiment polarities. Welling, 2017) to encode the information conveyed by a dependency tree has already been investigated in several fields, e.g., modeling document-word relationships (Yao et al., 2019) and tree structures (Marcheggiani and Titov, 2017; Zhang et al., 2018). As shown in Figure 1, an annotated dependency tree of original sentence is provided, and we can observe that word-aspect pairs (bad, memory) and (great, battery life) are well established. Direct application of dependency tree has two obvious shortcomings: (1) the noisy information is inevitably introduced through the dependency tree, due to imperfect parsing performance and the casualness of input sentence; (2) GCN would be inherently inferior in modeling long-distance or disconnected words in the dependency tree. It is reported that lower performance is achieved even with the golden depend"
2020.acl-main.588,P18-1234,0,0.0549201,"etworks (CNN) (Dong et al., 2014; Vo and Zhang, 2015) have already been widely applied to aspectbased sentiment classification. Inspired by the work (Tang et al., 2016a) which demonstrates the importance of modeling the semantic connection between contextual words and aspects, RNN augmented by attention mechanism (Bahdanau et al., 2015; Luong et al., 2015; Xu et al., 2015) is widely utilized in recent methods for exploring the potentially relevant words with respect to the given aspect (Yang et al., 2017; Zhang and Liu, 2017; Zeng et al., 2019; Wang et al., 2016). CNN based attention methods (Xue and Li, 2018; Li et al., 2018) are also proposed to enhance the phrase-level representation and achieved encouraging results. Although attention-based models have achieved promising performance on several tasks, the limitation is still obvious because attention module may highlight the irrelevant words owing to the syntactical absence. For example, given the sentence “it has a bad memory but a great battery life.” and aspect “battery life”, attention module may still assign a large weight to word “bad” rather than “great”, which adversely leads to a wrong sentiment polarity prediction. To take advantages"
2020.acl-main.627,C10-1081,0,\N,Missing
2020.acl-main.627,D07-1002,0,\N,Missing
2020.acl-main.627,kingsbury-palmer-2002-treebank,0,\N,Missing
2020.acl-main.627,W11-1608,0,\N,Missing
2020.acl-main.627,N12-1052,0,\N,Missing
2020.acl-main.627,H01-1035,0,\N,Missing
2020.acl-main.627,W09-1206,0,\N,Missing
2020.acl-main.627,P11-2052,0,\N,Missing
2020.acl-main.627,petrov-etal-2012-universal,0,\N,Missing
2020.acl-main.627,J05-1004,0,\N,Missing
2020.acl-main.627,P15-2111,0,\N,Missing
2020.acl-main.627,D15-1039,0,\N,Missing
2020.acl-main.627,P15-1039,0,\N,Missing
2020.acl-main.627,P15-2115,0,\N,Missing
2020.acl-main.627,W15-1824,0,\N,Missing
2020.acl-main.627,P11-2051,0,\N,Missing
2020.acl-main.627,P16-4001,0,\N,Missing
2020.acl-main.627,P17-1044,0,\N,Missing
2020.acl-main.627,P18-1192,0,\N,Missing
2020.acl-main.627,C18-1324,0,\N,Missing
2020.acl-main.627,D18-1039,0,\N,Missing
2020.acl-main.627,D18-1262,0,\N,Missing
2020.acl-main.627,D18-1269,0,\N,Missing
2020.acl-main.627,D18-1498,0,\N,Missing
2020.acl-main.627,W19-0417,0,\N,Missing
2020.acl-main.627,P19-1299,0,\N,Missing
2020.acl-main.627,N19-1423,0,\N,Missing
2020.acl-main.627,Q18-1039,0,\N,Missing
2020.acl-main.627,D19-1575,0,\N,Missing
2020.acl-main.627,D19-1092,1,\N,Missing
2020.acl-main.627,K19-1029,0,\N,Missing
2020.acl-main.627,W14-1614,0,\N,Missing
2020.acl-main.627,N13-1073,0,\N,Missing
2020.coling-main.263,W99-0611,0,0.451022,"ord Distance The distance between the two separation words should be an important factor in coreference recognition. Intuitively, as the distance increases, the difficulty should be also increased greatly. Here we conduct analysis to verify this intuition. Figure 5(b) shows the comparison results, which is consistent with our supposition. In addition, we can see that our final end-to-end model behaves much better, with relatively smaller decreases as the distance increases. 5 Related Work Coreference recognition has been investigated extensively in NLP for decades (McCarthy and Lehnert, 1995; Cardie and Wagstaff, 1999; Ng and Cardie, 2002; Elango, 2005). Lexical fusion can be regarded as one kind of coreference, however, it has received little attention in the NLP community. Our proposed models are inspired by the work of neural coreference resolution (Fern´andez-Gallego et al., 2016; Clark and Manning, 2016; Xu et al., 2017; Lee et al., 2017; Zhang et al., 2018). We adapt these models by considering task-specific features of Chinese lexical fusion, for example, enhancing the encoder with a GAT module for structural sememe information. Another most closely-related topic is abbreviation (Zhong, 1985). There"
2020.coling-main.263,P18-1045,0,0.0161733,"hat our final model is effective and competitive for the task. Detailed analysis is offered for comprehensively understanding the new task and our proposed model. 1 Introduction Coreference is one important topic in linguistics (Gordon and Hendrick, 1998; Pinillos, 2011), and coreference recognition has been extensively researched in the natural language processing (NLP) community (Ng and Cardie, 2002; Lee et al., 2017; Qi et al., 2012; Fei et al., 2019). There are different kinds of coreferences, such as pronoun anaphora and abbreviation (Mitkov, 1998; Mitkov, 1999; Mu˜noz and Montoyo, 2002; Choubey and Huang, 2018). Here we examine the phenomenon of Chinese lexical fusion, also called separable coreference (Chen and Ren, 2020), where two closely related words in a paragraph are united by a fusion form of the same meaning, and the fusion word can be seen as the coreference of the two separation words. Since the fusion words are always out-of-vocabulary (OOV) words in downstream paragraph-level tasks such as reading comprehension, summarization, machine translation and etc., which can hinder the overall understanding and lead to degraded performance, the new task can offer informative knowledge to these t"
2020.coling-main.263,P16-1061,0,0.0292208,"which is consistent with our supposition. In addition, we can see that our final end-to-end model behaves much better, with relatively smaller decreases as the distance increases. 5 Related Work Coreference recognition has been investigated extensively in NLP for decades (McCarthy and Lehnert, 1995; Cardie and Wagstaff, 1999; Ng and Cardie, 2002; Elango, 2005). Lexical fusion can be regarded as one kind of coreference, however, it has received little attention in the NLP community. Our proposed models are inspired by the work of neural coreference resolution (Fern´andez-Gallego et al., 2016; Clark and Manning, 2016; Xu et al., 2017; Lee et al., 2017; Zhang et al., 2018). We adapt these models by considering task-specific features of Chinese lexical fusion, for example, enhancing the encoder with a GAT module for structural sememe information. Another most closely-related topic is abbreviation (Zhong, 1985). There have been several studies on abbreviation prediction, recovery and dictionary construction (Sun et al., 2008; Li and Yarowsky, 2008; Liu et al., 2009; Zhang and Sun, 2018). Lexical fusion is different from abbreviation in many points. For example, abbreviation always refers to one inseparable m"
2020.coling-main.263,N19-1423,0,0.619902,"h is referred to as mention detection. Second, coreference recognition is performed over the detected mentions, linking each character in the fusion words to their coreferences, respectively. By the second step, full lexical fusion coreferences are also recognized concurrently. The two steps can be conducted jointly in a single end-to-end model (Lee et al., 2017), which helps avoid the error propagation problem, and meanwhile, enable the two subtasks with full interaction. In this paper, we present a competitive end-to-end model for lexical fusion recognition. Contextual BERT representations (Devlin et al., 2019) are adopted as encoder inputs as they have achieved great success in a number of NLP tasks (Tian et al., 2019; Zhou et al., 2019; Xu et al., 2019). For mention detection, a CRF decoder (Huang et al., 2015) is exploited to detect all mention words, including both the fusion and the separation words. Further, we use a BiAffine decoder for coreference recognition (Zhang et al., 2017; Bekoulis et al., 2018), determining a given mention pair either to be a coreference or not. Since our task is semantic oriented, we use the sememe knowledge provided in HowNet (Dong and Dong, 2003) to help capturing"
2020.coling-main.263,D19-1033,0,0.221852,"oder (Huang et al., 2015) is exploited to detect all mention words, including both the fusion and the separation words. Further, we use a BiAffine decoder for coreference recognition (Zhang et al., 2017; Bekoulis et al., 2018), determining a given mention pair either to be a coreference or not. Since our task is semantic oriented, we use the sememe knowledge provided in HowNet (Dong and Dong, 2003) to help capturing the semantic similarity between the characters and the separation words. HowNet has achieved success in many Chinese NLP tasks in recent years (Duan et al., 2007; Gu et al., 2018; Ding et al., 2019; Li et al., 2019). Both Chinese characters and words are defined by senses of sememe graphs in it, and we exploit graph attention networks (GAT) (Velickovic et al., 2018) to model the sememe graphs to enhance our encoder. Finally, we manually construct a high-quality dataset to evaluate our models. The dataset consists of 7,271 cases of the lexical fusion, which are all exploited as the test instances. To train our proposed models, we construct a pseudo dataset automatically from the web resource. Experimental results show that the auto-constructed training dataset is highly effective for our"
2020.coling-main.263,P19-1064,0,0.0229756,"wledge from HowNet by graph attention networks. We manually annotate a benchmark dataset for the task and then conduct experiments on it. Results demonstrate that our final model is effective and competitive for the task. Detailed analysis is offered for comprehensively understanding the new task and our proposed model. 1 Introduction Coreference is one important topic in linguistics (Gordon and Hendrick, 1998; Pinillos, 2011), and coreference recognition has been extensively researched in the natural language processing (NLP) community (Ng and Cardie, 2002; Lee et al., 2017; Qi et al., 2012; Fei et al., 2019). There are different kinds of coreferences, such as pronoun anaphora and abbreviation (Mitkov, 1998; Mitkov, 1999; Mu˜noz and Montoyo, 2002; Choubey and Huang, 2018). Here we examine the phenomenon of Chinese lexical fusion, also called separable coreference (Chen and Ren, 2020), where two closely related words in a paragraph are united by a fusion form of the same meaning, and the fusion word can be seen as the coreference of the two separation words. Since the fusion words are always out-of-vocabulary (OOV) words in downstream paragraph-level tasks such as reading comprehension, summarizati"
2020.coling-main.263,D18-1493,0,0.175235,"ection, a CRF decoder (Huang et al., 2015) is exploited to detect all mention words, including both the fusion and the separation words. Further, we use a BiAffine decoder for coreference recognition (Zhang et al., 2017; Bekoulis et al., 2018), determining a given mention pair either to be a coreference or not. Since our task is semantic oriented, we use the sememe knowledge provided in HowNet (Dong and Dong, 2003) to help capturing the semantic similarity between the characters and the separation words. HowNet has achieved success in many Chinese NLP tasks in recent years (Duan et al., 2007; Gu et al., 2018; Ding et al., 2019; Li et al., 2019). Both Chinese characters and words are defined by senses of sememe graphs in it, and we exploit graph attention networks (GAT) (Velickovic et al., 2018) to model the sememe graphs to enhance our encoder. Finally, we manually construct a high-quality dataset to evaluate our models. The dataset consists of 7,271 cases of the lexical fusion, which are all exploited as the test instances. To train our proposed models, we construct a pseudo dataset automatically from the web resource. Experimental results show that the auto-constructed training dataset is highl"
2020.coling-main.263,D19-1588,0,0.0194355,"et al., 2009; Zhang and Sun, 2018). Lexical fusion is different from abbreviation in many points. For example, abbreviation always refers to one inseparable mention, which is not necessary for lexical fusion. Besides, lexical fusion should abide by certain word construction rules, while abbreviation is for free. BERT and its variations have achieved the leading performances for GLUE benchmark datasets (Devlin et al., 2019; Liu et al., 2019a; Liu et al., 2019b). For the close tasks such as coreference resolution and relation extraction, BERT representations have also shown competitive results (Joshi et al., 2019; Lin et al., 2019), which inspires our work by using it as basic inputs aiming for competitive performance. The sense and sememe information has been demonstrated effective for several semantic-oriented NLP tasks (Niu et al., 2017; Gu et al., 2018; Zeng et al., 2018; Ding et al., 2019; Qi et al., 2019). HowNet offers a large knowledge base of sememe-based (Dong and Dong, 2003), which has been adopted for sememe extraction. We encode the sememes by the form of a graph naturally to enhance our task encoder. 2943 6 Conclusion In this work, we introduced the task of lexical fusion recognition in"
2020.coling-main.263,P18-2063,0,0.0410707,"Missing"
2020.coling-main.263,D17-1018,0,0.395671,"urther enhanced with the sememe knowledge from HowNet by graph attention networks. We manually annotate a benchmark dataset for the task and then conduct experiments on it. Results demonstrate that our final model is effective and competitive for the task. Detailed analysis is offered for comprehensively understanding the new task and our proposed model. 1 Introduction Coreference is one important topic in linguistics (Gordon and Hendrick, 1998; Pinillos, 2011), and coreference recognition has been extensively researched in the natural language processing (NLP) community (Ng and Cardie, 2002; Lee et al., 2017; Qi et al., 2012; Fei et al., 2019). There are different kinds of coreferences, such as pronoun anaphora and abbreviation (Mitkov, 1998; Mitkov, 1999; Mu˜noz and Montoyo, 2002; Choubey and Huang, 2018). Here we examine the phenomenon of Chinese lexical fusion, also called separable coreference (Chen and Ren, 2020), where two closely related words in a paragraph are united by a fusion form of the same meaning, and the fusion word can be seen as the coreference of the two separation words. Since the fusion words are always out-of-vocabulary (OOV) words in downstream paragraph-level tasks such a"
2020.coling-main.263,P08-1049,0,0.0342123,"↔下调 (reduce)” and “息↔利率 (interest rate)”. Lexical fusion is used frequently in the Chinese language (Chen and Ren, 2020). Moreover, the fusion words are usually rarer words compared with their separation words coreferred, which are more difficult to be handled by NLP models (Zhang and Yang, 2018; Gui et al., 2019). Luckily, the meaning of the fusion words can be derived from that of the separation words. Thus recognizing the lexical fusion would be beneficial for downstream paragraph (or document)-level NLP applications such as machine translation, information extraction, summarization, etc. (Li and Yarowsky, 2008; Ferreira et al., 2013; Kundu et al., 2018). For example, for deep semantic parsing or translation, the fusion words “受访” (UNK) can be substituted directly by the separation words “接受” (accept) and “访问” (interview), as the same fusion words are rarely occurred in other paragraphs. The recognition of lexical fusion can be accomplished by two subtasks. Given one paragraph, the fusion words, as well as the separation words should be detected as the first step, which is referred to as mention detection. Second, coreference recognition is performed over the detected mentions, linking each characte"
2020.coling-main.263,P19-1430,0,0.0822444,"2015) is exploited to detect all mention words, including both the fusion and the separation words. Further, we use a BiAffine decoder for coreference recognition (Zhang et al., 2017; Bekoulis et al., 2018), determining a given mention pair either to be a coreference or not. Since our task is semantic oriented, we use the sememe knowledge provided in HowNet (Dong and Dong, 2003) to help capturing the semantic similarity between the characters and the separation words. HowNet has achieved success in many Chinese NLP tasks in recent years (Duan et al., 2007; Gu et al., 2018; Ding et al., 2019; Li et al., 2019). Both Chinese characters and words are defined by senses of sememe graphs in it, and we exploit graph attention networks (GAT) (Velickovic et al., 2018) to model the sememe graphs to enhance our encoder. Finally, we manually construct a high-quality dataset to evaluate our models. The dataset consists of 7,271 cases of the lexical fusion, which are all exploited as the test instances. To train our proposed models, we construct a pseudo dataset automatically from the web resource. Experimental results show that the auto-constructed training dataset is highly effective for our task. The end-to-"
2020.coling-main.263,W19-1908,0,0.0186543,"and Sun, 2018). Lexical fusion is different from abbreviation in many points. For example, abbreviation always refers to one inseparable mention, which is not necessary for lexical fusion. Besides, lexical fusion should abide by certain word construction rules, while abbreviation is for free. BERT and its variations have achieved the leading performances for GLUE benchmark datasets (Devlin et al., 2019; Liu et al., 2019a; Liu et al., 2019b). For the close tasks such as coreference resolution and relation extraction, BERT representations have also shown competitive results (Joshi et al., 2019; Lin et al., 2019), which inspires our work by using it as basic inputs aiming for competitive performance. The sense and sememe information has been demonstrated effective for several semantic-oriented NLP tasks (Niu et al., 2017; Gu et al., 2018; Zeng et al., 2018; Ding et al., 2019; Qi et al., 2019). HowNet offers a large knowledge base of sememe-based (Dong and Dong, 2003), which has been adopted for sememe extraction. We encode the sememes by the form of a graph naturally to enhance our task encoder. 2943 6 Conclusion In this work, we introduced the task of lexical fusion recognition in Chinese and then pr"
2020.coling-main.263,P19-1441,0,0.0245254,"s abbreviation (Zhong, 1985). There have been several studies on abbreviation prediction, recovery and dictionary construction (Sun et al., 2008; Li and Yarowsky, 2008; Liu et al., 2009; Zhang and Sun, 2018). Lexical fusion is different from abbreviation in many points. For example, abbreviation always refers to one inseparable mention, which is not necessary for lexical fusion. Besides, lexical fusion should abide by certain word construction rules, while abbreviation is for free. BERT and its variations have achieved the leading performances for GLUE benchmark datasets (Devlin et al., 2019; Liu et al., 2019a; Liu et al., 2019b). For the close tasks such as coreference resolution and relation extraction, BERT representations have also shown competitive results (Joshi et al., 2019; Lin et al., 2019), which inspires our work by using it as basic inputs aiming for competitive performance. The sense and sememe information has been demonstrated effective for several semantic-oriented NLP tasks (Niu et al., 2017; Gu et al., 2018; Zeng et al., 2018; Ding et al., 2019; Qi et al., 2019). HowNet offers a large knowledge base of sememe-based (Dong and Dong, 2003), which has been adopted for sememe extractio"
2020.coling-main.263,D15-1166,0,0.0603718,"The second part is obtained straightforwardly by the embedding of the position offset of the sense’s source word. The position offset is denoted by [s, e], where s and e indicate the relative position of the start and end characters of the source word to the current character, which has been illustrated in Figure 3. We use the position offset as a single unit for embedding. Following, we concatenate the two parts, resulting in the sense representation: Pm hsmi hsn = i=1 ⊕ e[s,t] , (10) M where ⊕ denotes vector concatenation. Finally, we aggregate all sense representations by global attention (Luong et al., 2015) with the guide of the BERT outputs to obtain character-level representations. Let {sn1 , · · · , snN } denote the set of sense representations for one character ci , the sememe-enhanced representation for character ci can be computed by:  exp tanh(v[hi ⊕ hsnj ]) aj = PN , exp tanh(v[h ⊕ h ]) i sn k k=1 (11) N X hsem = aj · hsnj , i j=1 sem where v is a model parameter for attention calculation, and hsem 1 · · · hn are the desired outputs which is used instead of the BERT outputs h1 · · · hn for decoder. 4 Experiments 4.1 Dataset Test Data We build a lexical fusion dataset manually in this w"
2020.coling-main.263,P16-1101,0,0.0944677,"Missing"
2020.coling-main.263,P09-1113,0,0.0143127,"two words in the same paragraph contain these characters each, or if they appear in the dictionary definition of the characters, then we obtain one context-independent triple. Finally, we collect 1,608 well-formed triples of lexical fusions and treat them as seeds to construct pseudo training instances. Note that the fusion words of these triples are currently acceptable and widely used by users, such as “停 车 (parking vehicles)↔停放 (parking)/车辆 (vehicle)”. Then, we search for the paragraphs containing all three words of a certain triple, regarding them as valid cases of Chinese lexical fusion (Mintz et al., 2009). Finally, we obtain 11,034 paragraphs, which are divided into training and development sections for model parameter learning and hyper-parameter tuning, respectively. Table 2 summarizes the overall data statistics of the training, development and testing sections, including the number of cases (lexical fusion), the averaged paragraph length (by character count) and the number of unique triples, respectively. Training Development Testing #Case #U.Triple #A.Length #N.Case #N.U.Triple 7,400 2,213 7,271 1,531 1,001 1,661 106 105 91 94 3959 77 957 Table 2: Data statistics of our corpus, where A. ,"
2020.coling-main.263,P98-2143,0,0.0204688,"then conduct experiments on it. Results demonstrate that our final model is effective and competitive for the task. Detailed analysis is offered for comprehensively understanding the new task and our proposed model. 1 Introduction Coreference is one important topic in linguistics (Gordon and Hendrick, 1998; Pinillos, 2011), and coreference recognition has been extensively researched in the natural language processing (NLP) community (Ng and Cardie, 2002; Lee et al., 2017; Qi et al., 2012; Fei et al., 2019). There are different kinds of coreferences, such as pronoun anaphora and abbreviation (Mitkov, 1998; Mitkov, 1999; Mu˜noz and Montoyo, 2002; Choubey and Huang, 2018). Here we examine the phenomenon of Chinese lexical fusion, also called separable coreference (Chen and Ren, 2020), where two closely related words in a paragraph are united by a fusion form of the same meaning, and the fusion word can be seen as the coreference of the two separation words. Since the fusion words are always out-of-vocabulary (OOV) words in downstream paragraph-level tasks such as reading comprehension, summarization, machine translation and etc., which can hinder the overall understanding and lead to degraded pe"
2020.coling-main.263,P02-1014,0,0.0884658,"the encoder, and is further enhanced with the sememe knowledge from HowNet by graph attention networks. We manually annotate a benchmark dataset for the task and then conduct experiments on it. Results demonstrate that our final model is effective and competitive for the task. Detailed analysis is offered for comprehensively understanding the new task and our proposed model. 1 Introduction Coreference is one important topic in linguistics (Gordon and Hendrick, 1998; Pinillos, 2011), and coreference recognition has been extensively researched in the natural language processing (NLP) community (Ng and Cardie, 2002; Lee et al., 2017; Qi et al., 2012; Fei et al., 2019). There are different kinds of coreferences, such as pronoun anaphora and abbreviation (Mitkov, 1998; Mitkov, 1999; Mu˜noz and Montoyo, 2002; Choubey and Huang, 2018). Here we examine the phenomenon of Chinese lexical fusion, also called separable coreference (Chen and Ren, 2020), where two closely related words in a paragraph are united by a fusion form of the same meaning, and the fusion word can be seen as the coreference of the two separation words. Since the fusion words are always out-of-vocabulary (OOV) words in downstream paragraph-"
2020.coling-main.263,P17-1187,0,0.0186764,"should abide by certain word construction rules, while abbreviation is for free. BERT and its variations have achieved the leading performances for GLUE benchmark datasets (Devlin et al., 2019; Liu et al., 2019a; Liu et al., 2019b). For the close tasks such as coreference resolution and relation extraction, BERT representations have also shown competitive results (Joshi et al., 2019; Lin et al., 2019), which inspires our work by using it as basic inputs aiming for competitive performance. The sense and sememe information has been demonstrated effective for several semantic-oriented NLP tasks (Niu et al., 2017; Gu et al., 2018; Zeng et al., 2018; Ding et al., 2019; Qi et al., 2019). HowNet offers a large knowledge base of sememe-based (Dong and Dong, 2003), which has been adopted for sememe extraction. We encode the sememes by the form of a graph naturally to enhance our task encoder. 2943 6 Conclusion In this work, we introduced the task of lexical fusion recognition in Chinese and then presented an end-to-end model for the new task. BERT representation was exploited as the basic input for the models, and the model is further enhanced with the sememe knowledge from HowNet by graph attention networ"
2020.coling-main.263,W12-4304,0,0.073625,"Missing"
2020.coling-main.263,P19-1571,0,0.0145635,"r free. BERT and its variations have achieved the leading performances for GLUE benchmark datasets (Devlin et al., 2019; Liu et al., 2019a; Liu et al., 2019b). For the close tasks such as coreference resolution and relation extraction, BERT representations have also shown competitive results (Joshi et al., 2019; Lin et al., 2019), which inspires our work by using it as basic inputs aiming for competitive performance. The sense and sememe information has been demonstrated effective for several semantic-oriented NLP tasks (Niu et al., 2017; Gu et al., 2018; Zeng et al., 2018; Ding et al., 2019; Qi et al., 2019). HowNet offers a large knowledge base of sememe-based (Dong and Dong, 2003), which has been adopted for sememe extraction. We encode the sememes by the form of a graph naturally to enhance our task encoder. 2943 6 Conclusion In this work, we introduced the task of lexical fusion recognition in Chinese and then presented an end-to-end model for the new task. BERT representation was exploited as the basic input for the models, and the model is further enhanced with the sememe knowledge from HowNet by graph attention networks. We manually annotated a benchmark dataset for the task, which was use"
2020.coling-main.263,P17-1114,0,0.0238973,"our supposition. In addition, we can see that our final end-to-end model behaves much better, with relatively smaller decreases as the distance increases. 5 Related Work Coreference recognition has been investigated extensively in NLP for decades (McCarthy and Lehnert, 1995; Cardie and Wagstaff, 1999; Ng and Cardie, 2002; Elango, 2005). Lexical fusion can be regarded as one kind of coreference, however, it has received little attention in the NLP community. Our proposed models are inspired by the work of neural coreference resolution (Fern´andez-Gallego et al., 2016; Clark and Manning, 2016; Xu et al., 2017; Lee et al., 2017; Zhang et al., 2018). We adapt these models by considering task-specific features of Chinese lexical fusion, for example, enhancing the encoder with a GAT module for structural sememe information. Another most closely-related topic is abbreviation (Zhong, 1985). There have been several studies on abbreviation prediction, recovery and dictionary construction (Sun et al., 2008; Li and Yarowsky, 2008; Liu et al., 2009; Zhang and Sun, 2018). Lexical fusion is different from abbreviation in many points. For example, abbreviation always refers to one inseparable mention, which is"
2020.coling-main.263,N19-1242,0,0.0552695,"Missing"
2020.coling-main.263,L18-1325,0,0.0366173,"mmunity. Our proposed models are inspired by the work of neural coreference resolution (Fern´andez-Gallego et al., 2016; Clark and Manning, 2016; Xu et al., 2017; Lee et al., 2017; Zhang et al., 2018). We adapt these models by considering task-specific features of Chinese lexical fusion, for example, enhancing the encoder with a GAT module for structural sememe information. Another most closely-related topic is abbreviation (Zhong, 1985). There have been several studies on abbreviation prediction, recovery and dictionary construction (Sun et al., 2008; Li and Yarowsky, 2008; Liu et al., 2009; Zhang and Sun, 2018). Lexical fusion is different from abbreviation in many points. For example, abbreviation always refers to one inseparable mention, which is not necessary for lexical fusion. Besides, lexical fusion should abide by certain word construction rules, while abbreviation is for free. BERT and its variations have achieved the leading performances for GLUE benchmark datasets (Devlin et al., 2019; Liu et al., 2019a; Liu et al., 2019b). For the close tasks such as coreference resolution and relation extraction, BERT representations have also shown competitive results (Joshi et al., 2019; Lin et al., 20"
2020.coling-main.263,P18-1144,0,0.0433728,"Missing"
2020.coling-main.263,E17-1063,0,0.027293,"Missing"
2020.coling-main.263,P19-1328,0,0.0563967,"Missing"
2020.coling-main.263,C98-2138,0,\N,Missing
2020.coling-main.263,E12-2021,0,\N,Missing
2020.emnlp-main.168,P17-2021,0,0.0158617,"lated context-sensitive features (Peters et al., 2018). In this work, we follow the line of Transformer-based (Vaswani et al., 2017) LM (e.g., BERT), considering its prominence. Structure induction. The idea of introducing tree structures into deep models for structure-aware language modeling has long been explored by supervised structure learning, which generally relies on annotated parse trees during training and maximizes the joint likelihood of sentence-tree pairs (Socher et al., 2010, 2013; Tai et al., 2015; Yazdani and Henderson, 2015; Dyer et al., 2016; AlvarezMelis and Jaakkola, 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Wang et al., 2018; G¯u et al., 2018). There has been much attention paid to unsupervised grammar induction task (Williams et al., 2017; Shen et al., 2018a,b; Kuncoro et al., 2018; Kim et al., 2019a; Luo et al., 2019; Drozdov et al., 2019; Kim et al., 2019b). For example, PRPN (Shen et al., 2018a) computes the syntactic distance of word pairs. On-LSTM (Shen et al., 2018b) allows hidden neurons to learn long-term or shortterm information by a gate mechanism. URNNG (Kim et al., 2019b) applies amortized variational inference, encouraging the decoder to generate reasonable"
2020.emnlp-main.168,P19-1030,0,0.09386,"been shown (Conneau et al., 2018; Tenney et al., 2019) that besides rich semantic information, implicit language structure knowledge can be captured by a deep BERT (Vig and Belinkov, 2019; Jawahar et al., 2019; Goldberg, 2019). However, such structure features learnt via the vanilla Transformer LM are insufficient for those NLP tasks that heavily rely on syntactic or linguistic knowledge (Hao et al., 2019). Some effort devote to improved the ability of structure Corresponding author. ytask ystruc Introduction ∗ ytask learning in Transformer LM by installing novel syntax-attention mechanisms (Ahmed et al., 2019; Wang et al., 2019). Nevertheless, several limitations can be observed. First, according to the recent findings by probing tasks (Conneau et al., 2018; Tenney et al., 2019; Goldberg, 2019), the syntactic structure representations are best retained right at the middle layers (Vig and Belinkov, 2019; Jawahar et al., 2019). Nevertheless, existing tree Transformers employ traditional full-scale training over the whole deep Transformer architecture (as shown in Figure 1(a)), consequently weakening the upper-layer semantic learning that can be crucial for end tasks. Second, these tree Transformer m"
2020.emnlp-main.168,P18-1198,0,0.153794,"ture-aware Transformer LM (right). Natural language models (LM) can generate fluent text and encode factual knowledge (Mikolov et al., 2013; Pennington et al., 2014; Merity et al., 2017). Recently, pre-trained contextualized language models have given remarkable improvements on various NLP tasks (Peters et al., 2018; Radford et al., 2018; Howard and Ruder, 2018; Yang et al., 2019; Devlin et al., 2019; Dai et al., 2019). Among such methods, the Transformer-based (Vaswani et al., 2017) BERT has become a most popular encoder for obtaining state-of-the-art NLP task performance. It has been shown (Conneau et al., 2018; Tenney et al., 2019) that besides rich semantic information, implicit language structure knowledge can be captured by a deep BERT (Vig and Belinkov, 2019; Jawahar et al., 2019; Goldberg, 2019). However, such structure features learnt via the vanilla Transformer LM are insufficient for those NLP tasks that heavily rely on syntactic or linguistic knowledge (Hao et al., 2019). Some effort devote to improved the ability of structure Corresponding author. ytask ystruc Introduction ∗ ytask learning in Transformer LM by installing novel syntax-attention mechanisms (Ahmed et al., 2019; Wang et al.,"
2020.emnlp-main.168,P19-1285,0,0.0582409,"Missing"
2020.emnlp-main.168,N19-1423,0,0.178855,"t improvements for both semantic- and syntactic-dependent tasks. 1 (a) Full-layer (b) Middle-layer Figure 1: Full-layer multi-task learning for structural training (left), and the middle-layer training for deep structure-aware Transformer LM (right). Natural language models (LM) can generate fluent text and encode factual knowledge (Mikolov et al., 2013; Pennington et al., 2014; Merity et al., 2017). Recently, pre-trained contextualized language models have given remarkable improvements on various NLP tasks (Peters et al., 2018; Radford et al., 2018; Howard and Ruder, 2018; Yang et al., 2019; Devlin et al., 2019; Dai et al., 2019). Among such methods, the Transformer-based (Vaswani et al., 2017) BERT has become a most popular encoder for obtaining state-of-the-art NLP task performance. It has been shown (Conneau et al., 2018; Tenney et al., 2019) that besides rich semantic information, implicit language structure knowledge can be captured by a deep BERT (Vig and Belinkov, 2019; Jawahar et al., 2019; Goldberg, 2019). However, such structure features learnt via the vanilla Transformer LM are insufficient for those NLP tasks that heavily rely on syntactic or linguistic knowledge (Hao et al., 2019). Some"
2020.emnlp-main.168,N19-1116,0,0.0775566,"e-aware language modeling has long been explored by supervised structure learning, which generally relies on annotated parse trees during training and maximizes the joint likelihood of sentence-tree pairs (Socher et al., 2010, 2013; Tai et al., 2015; Yazdani and Henderson, 2015; Dyer et al., 2016; AlvarezMelis and Jaakkola, 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Wang et al., 2018; G¯u et al., 2018). There has been much attention paid to unsupervised grammar induction task (Williams et al., 2017; Shen et al., 2018a,b; Kuncoro et al., 2018; Kim et al., 2019a; Luo et al., 2019; Drozdov et al., 2019; Kim et al., 2019b). For example, PRPN (Shen et al., 2018a) computes the syntactic distance of word pairs. On-LSTM (Shen et al., 2018b) allows hidden neurons to learn long-term or shortterm information by a gate mechanism. URNNG (Kim et al., 2019b) applies amortized variational inference, encouraging the decoder to generate reasonable tree structures. DIORA (Drozdov et al., 2019) uses inside-outside dynamic programming to compose latent representations from all possible binary trees. PCFG (Kim et al., 2019a) achieves grammar induction by probabilistic context-free grammar. Unlike these recurr"
2020.emnlp-main.168,N16-1024,0,0.0260721,"om language models can help to give the most task-related context-sensitive features (Peters et al., 2018). In this work, we follow the line of Transformer-based (Vaswani et al., 2017) LM (e.g., BERT), considering its prominence. Structure induction. The idea of introducing tree structures into deep models for structure-aware language modeling has long been explored by supervised structure learning, which generally relies on annotated parse trees during training and maximizes the joint likelihood of sentence-tree pairs (Socher et al., 2010, 2013; Tai et al., 2015; Yazdani and Henderson, 2015; Dyer et al., 2016; AlvarezMelis and Jaakkola, 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Wang et al., 2018; G¯u et al., 2018). There has been much attention paid to unsupervised grammar induction task (Williams et al., 2017; Shen et al., 2018a,b; Kuncoro et al., 2018; Kim et al., 2019a; Luo et al., 2019; Drozdov et al., 2019; Kim et al., 2019b). For example, PRPN (Shen et al., 2018a) computes the syntactic distance of word pairs. On-LSTM (Shen et al., 2018b) allows hidden neurons to learn long-term or shortterm information by a gate mechanism. URNNG (Kim et al., 2019b) applies amortized variation"
2020.emnlp-main.168,P17-2012,0,0.0153027,"ures (Peters et al., 2018). In this work, we follow the line of Transformer-based (Vaswani et al., 2017) LM (e.g., BERT), considering its prominence. Structure induction. The idea of introducing tree structures into deep models for structure-aware language modeling has long been explored by supervised structure learning, which generally relies on annotated parse trees during training and maximizes the joint likelihood of sentence-tree pairs (Socher et al., 2010, 2013; Tai et al., 2015; Yazdani and Henderson, 2015; Dyer et al., 2016; AlvarezMelis and Jaakkola, 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Wang et al., 2018; G¯u et al., 2018). There has been much attention paid to unsupervised grammar induction task (Williams et al., 2017; Shen et al., 2018a,b; Kuncoro et al., 2018; Kim et al., 2019a; Luo et al., 2019; Drozdov et al., 2019; Kim et al., 2019b). For example, PRPN (Shen et al., 2018a) computes the syntactic distance of word pairs. On-LSTM (Shen et al., 2018b) allows hidden neurons to learn long-term or shortterm information by a gate mechanism. URNNG (Kim et al., 2019b) applies amortized variational inference, encouraging the decoder to generate reasonable tree structures. DIORA"
2020.emnlp-main.168,D18-1037,0,0.0387803,"Missing"
2020.emnlp-main.168,D19-1424,0,0.0798043,"19; Devlin et al., 2019; Dai et al., 2019). Among such methods, the Transformer-based (Vaswani et al., 2017) BERT has become a most popular encoder for obtaining state-of-the-art NLP task performance. It has been shown (Conneau et al., 2018; Tenney et al., 2019) that besides rich semantic information, implicit language structure knowledge can be captured by a deep BERT (Vig and Belinkov, 2019; Jawahar et al., 2019; Goldberg, 2019). However, such structure features learnt via the vanilla Transformer LM are insufficient for those NLP tasks that heavily rely on syntactic or linguistic knowledge (Hao et al., 2019). Some effort devote to improved the ability of structure Corresponding author. ytask ystruc Introduction ∗ ytask learning in Transformer LM by installing novel syntax-attention mechanisms (Ahmed et al., 2019; Wang et al., 2019). Nevertheless, several limitations can be observed. First, according to the recent findings by probing tasks (Conneau et al., 2018; Tenney et al., 2019; Goldberg, 2019), the syntactic structure representations are best retained right at the middle layers (Vig and Belinkov, 2019; Jawahar et al., 2019). Nevertheless, existing tree Transformers employ traditional full-sca"
2020.emnlp-main.168,S10-1006,0,0.0273867,"Missing"
2020.emnlp-main.168,P18-1031,0,0.122,"fine-tuning, our model achieves significant improvements for both semantic- and syntactic-dependent tasks. 1 (a) Full-layer (b) Middle-layer Figure 1: Full-layer multi-task learning for structural training (left), and the middle-layer training for deep structure-aware Transformer LM (right). Natural language models (LM) can generate fluent text and encode factual knowledge (Mikolov et al., 2013; Pennington et al., 2014; Merity et al., 2017). Recently, pre-trained contextualized language models have given remarkable improvements on various NLP tasks (Peters et al., 2018; Radford et al., 2018; Howard and Ruder, 2018; Yang et al., 2019; Devlin et al., 2019; Dai et al., 2019). Among such methods, the Transformer-based (Vaswani et al., 2017) BERT has become a most popular encoder for obtaining state-of-the-art NLP task performance. It has been shown (Conneau et al., 2018; Tenney et al., 2019) that besides rich semantic information, implicit language structure knowledge can be captured by a deep BERT (Vig and Belinkov, 2019; Jawahar et al., 2019; Goldberg, 2019). However, such structure features learnt via the vanilla Transformer LM are insufficient for those NLP tasks that heavily rely on syntactic or lingu"
2020.emnlp-main.168,P19-1356,0,0.240197,"l., 2017). Recently, pre-trained contextualized language models have given remarkable improvements on various NLP tasks (Peters et al., 2018; Radford et al., 2018; Howard and Ruder, 2018; Yang et al., 2019; Devlin et al., 2019; Dai et al., 2019). Among such methods, the Transformer-based (Vaswani et al., 2017) BERT has become a most popular encoder for obtaining state-of-the-art NLP task performance. It has been shown (Conneau et al., 2018; Tenney et al., 2019) that besides rich semantic information, implicit language structure knowledge can be captured by a deep BERT (Vig and Belinkov, 2019; Jawahar et al., 2019; Goldberg, 2019). However, such structure features learnt via the vanilla Transformer LM are insufficient for those NLP tasks that heavily rely on syntactic or linguistic knowledge (Hao et al., 2019). Some effort devote to improved the ability of structure Corresponding author. ytask ystruc Introduction ∗ ytask learning in Transformer LM by installing novel syntax-attention mechanisms (Ahmed et al., 2019; Wang et al., 2019). Nevertheless, several limitations can be observed. First, according to the recent findings by probing tasks (Conneau et al., 2018; Tenney et al., 2019; Goldberg, 2019), t"
2020.emnlp-main.168,P19-1228,0,0.0346835,"Missing"
2020.emnlp-main.168,N19-1114,0,0.0635536,"uctures into deep models for structure-aware language modeling has long been explored by supervised structure learning, which generally relies on annotated parse trees during training and maximizes the joint likelihood of sentence-tree pairs (Socher et al., 2010, 2013; Tai et al., 2015; Yazdani and Henderson, 2015; Dyer et al., 2016; AlvarezMelis and Jaakkola, 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Wang et al., 2018; G¯u et al., 2018). There has been much attention paid to unsupervised grammar induction task (Williams et al., 2017; Shen et al., 2018a,b; Kuncoro et al., 2018; Kim et al., 2019a; Luo et al., 2019; Drozdov et al., 2019; Kim et al., 2019b). For example, PRPN (Shen et al., 2018a) computes the syntactic distance of word pairs. On-LSTM (Shen et al., 2018b) allows hidden neurons to learn long-term or shortterm information by a gate mechanism. URNNG (Kim et al., 2019b) applies amortized variational inference, encouraging the decoder to generate reasonable tree structures. DIORA (Drozdov et al., 2019) uses inside-outside dynamic programming to compose latent representations from all possible binary trees. PCFG (Kim et al., 2019a) achieves grammar induction by probabilistic"
2020.emnlp-main.168,P18-1249,0,0.0415675,"Missing"
2020.emnlp-main.168,D19-1445,0,0.0180145,"uraging the decoder to generate reasonable tree structures. DIORA (Drozdov et al., 2019) uses inside-outside dynamic programming to compose latent representations from all possible binary trees. PCFG (Kim et al., 2019a) achieves grammar induction by probabilistic context-free grammar. Unlike these recurrent network based structure-aware LM, our work focuses on structure learning for a deep Transformer LM. Structure-aware Transformer language model. Some efforts have been paid for the Transformerbased pre-trained language models (e.g. BERT) by visualizing the attention (Vig and Belinkov, 2019; Kovaleva et al., 2019; Hao et al., 2019) or probing tasks (Jawahar et al., 2019; Goldberg, 2019). They find that the latent language structure knowledge is best retained at the middle-layer in BERT (Vig and Belinkov, 2019; Jawahar et al., 2019; Goldberg, 2019). Ahmed et al. (2019) employ a decomposable attention mechanism for recursively learn the tree structure for Transformer. Wang et al. (2019) integrate tree structures into Transformer via constituency-attention. However, these Transformer LMs suffer from the full-scale structural training and monotonous types of the structure, limiting the performance of stru"
2020.emnlp-main.168,P18-1132,0,0.0538032,"Missing"
2020.emnlp-main.168,P19-1144,0,0.264906,"odels for structure-aware language modeling has long been explored by supervised structure learning, which generally relies on annotated parse trees during training and maximizes the joint likelihood of sentence-tree pairs (Socher et al., 2010, 2013; Tai et al., 2015; Yazdani and Henderson, 2015; Dyer et al., 2016; AlvarezMelis and Jaakkola, 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Wang et al., 2018; G¯u et al., 2018). There has been much attention paid to unsupervised grammar induction task (Williams et al., 2017; Shen et al., 2018a,b; Kuncoro et al., 2018; Kim et al., 2019a; Luo et al., 2019; Drozdov et al., 2019; Kim et al., 2019b). For example, PRPN (Shen et al., 2018a) computes the syntactic distance of word pairs. On-LSTM (Shen et al., 2018b) allows hidden neurons to learn long-term or shortterm information by a gate mechanism. URNNG (Kim et al., 2019b) applies amortized variational inference, encouraging the decoder to generate reasonable tree structures. DIORA (Drozdov et al., 2019) uses inside-outside dynamic programming to compose latent representations from all possible binary trees. PCFG (Kim et al., 2019a) achieves grammar induction by probabilistic context-free gramma"
2020.emnlp-main.168,J93-2004,0,0.073061,"Missing"
2020.emnlp-main.168,D14-1162,0,0.0980918,"me. Experimental results show that the retrofitted structure-aware Transformer language model achieves improved perplexity, meanwhile inducing accurate syntactic phrases. By performing structure-aware fine-tuning, our model achieves significant improvements for both semantic- and syntactic-dependent tasks. 1 (a) Full-layer (b) Middle-layer Figure 1: Full-layer multi-task learning for structural training (left), and the middle-layer training for deep structure-aware Transformer LM (right). Natural language models (LM) can generate fluent text and encode factual knowledge (Mikolov et al., 2013; Pennington et al., 2014; Merity et al., 2017). Recently, pre-trained contextualized language models have given remarkable improvements on various NLP tasks (Peters et al., 2018; Radford et al., 2018; Howard and Ruder, 2018; Yang et al., 2019; Devlin et al., 2019; Dai et al., 2019). Among such methods, the Transformer-based (Vaswani et al., 2017) BERT has become a most popular encoder for obtaining state-of-the-art NLP task performance. It has been shown (Conneau et al., 2018; Tenney et al., 2019) that besides rich semantic information, implicit language structure knowledge can be captured by a deep BERT (Vig and Bel"
2020.emnlp-main.168,N18-1202,0,0.0955703,"ctic phrases. By performing structure-aware fine-tuning, our model achieves significant improvements for both semantic- and syntactic-dependent tasks. 1 (a) Full-layer (b) Middle-layer Figure 1: Full-layer multi-task learning for structural training (left), and the middle-layer training for deep structure-aware Transformer LM (right). Natural language models (LM) can generate fluent text and encode factual knowledge (Mikolov et al., 2013; Pennington et al., 2014; Merity et al., 2017). Recently, pre-trained contextualized language models have given remarkable improvements on various NLP tasks (Peters et al., 2018; Radford et al., 2018; Howard and Ruder, 2018; Yang et al., 2019; Devlin et al., 2019; Dai et al., 2019). Among such methods, the Transformer-based (Vaswani et al., 2017) BERT has become a most popular encoder for obtaining state-of-the-art NLP task performance. It has been shown (Conneau et al., 2018; Tenney et al., 2019) that besides rich semantic information, implicit language structure knowledge can be captured by a deep BERT (Vig and Belinkov, 2019; Jawahar et al., 2019; Goldberg, 2019). However, such structure features learnt via the vanilla Transformer LM are insufficient for those NLP"
2020.emnlp-main.168,P17-1105,0,0.156253,")), consequently weakening the upper-layer semantic learning that can be crucial for end tasks. Second, these tree Transformer methods encode either standalone constituency or dependency structure, while different tasks can depend on varying types of structural knowledge. The constituent and dependency representation for syntactic structure share underlying linguistic characteristics, while the former focuses on disclosing phrasal continuity and the latter aims at indicating dependency relations among elements. For example, semantic parsing tasks are more dependent on the dependency features (Rabinovich et al., 2017; Xia et al., 2019), while constituency information is much needed for sentiment classification (Socher et al., 2013). In this paper, we aim to retrofit structure-aware 2151 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2151–2161, c November 16–20, 2020. 2020 Association for Computational Linguistics Transformer LM for facilitating end tasks. • On the one hand, we propose a structure learning module for Transformer LM, meanwhile exploiting syntactic distance as the measurement for encoding both the phrasal constituency and the dependency connecti"
2020.emnlp-main.168,D13-1170,0,0.0611168,"former methods encode either standalone constituency or dependency structure, while different tasks can depend on varying types of structural knowledge. The constituent and dependency representation for syntactic structure share underlying linguistic characteristics, while the former focuses on disclosing phrasal continuity and the latter aims at indicating dependency relations among elements. For example, semantic parsing tasks are more dependent on the dependency features (Rabinovich et al., 2017; Xia et al., 2019), while constituency information is much needed for sentiment classification (Socher et al., 2013). In this paper, we aim to retrofit structure-aware 2151 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2151–2161, c November 16–20, 2020. 2020 Association for Computational Linguistics Transformer LM for facilitating end tasks. • On the one hand, we propose a structure learning module for Transformer LM, meanwhile exploiting syntactic distance as the measurement for encoding both the phrasal constituency and the dependency connection. • On the other hand, as illustrated in Figure 1, to better coordinate the structural learning and semantic learni"
2020.emnlp-main.168,P15-1150,0,0.149305,"Missing"
2020.emnlp-main.168,W19-4808,0,0.281548,"t al., 2014; Merity et al., 2017). Recently, pre-trained contextualized language models have given remarkable improvements on various NLP tasks (Peters et al., 2018; Radford et al., 2018; Howard and Ruder, 2018; Yang et al., 2019; Devlin et al., 2019; Dai et al., 2019). Among such methods, the Transformer-based (Vaswani et al., 2017) BERT has become a most popular encoder for obtaining state-of-the-art NLP task performance. It has been shown (Conneau et al., 2018; Tenney et al., 2019) that besides rich semantic information, implicit language structure knowledge can be captured by a deep BERT (Vig and Belinkov, 2019; Jawahar et al., 2019; Goldberg, 2019). However, such structure features learnt via the vanilla Transformer LM are insufficient for those NLP tasks that heavily rely on syntactic or linguistic knowledge (Hao et al., 2019). Some effort devote to improved the ability of structure Corresponding author. ytask ystruc Introduction ∗ ytask learning in Transformer LM by installing novel syntax-attention mechanisms (Ahmed et al., 2019; Wang et al., 2019). Nevertheless, several limitations can be observed. First, according to the recent findings by probing tasks (Conneau et al., 2018; Tenney et al., 20"
2020.emnlp-main.168,D18-1509,0,0.0220353,"18). In this work, we follow the line of Transformer-based (Vaswani et al., 2017) LM (e.g., BERT), considering its prominence. Structure induction. The idea of introducing tree structures into deep models for structure-aware language modeling has long been explored by supervised structure learning, which generally relies on annotated parse trees during training and maximizes the joint likelihood of sentence-tree pairs (Socher et al., 2010, 2013; Tai et al., 2015; Yazdani and Henderson, 2015; Dyer et al., 2016; AlvarezMelis and Jaakkola, 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Wang et al., 2018; G¯u et al., 2018). There has been much attention paid to unsupervised grammar induction task (Williams et al., 2017; Shen et al., 2018a,b; Kuncoro et al., 2018; Kim et al., 2019a; Luo et al., 2019; Drozdov et al., 2019; Kim et al., 2019b). For example, PRPN (Shen et al., 2018a) computes the syntactic distance of word pairs. On-LSTM (Shen et al., 2018b) allows hidden neurons to learn long-term or shortterm information by a gate mechanism. URNNG (Kim et al., 2019b) applies amortized variational inference, encouraging the decoder to generate reasonable tree structures. DIORA (Drozdov et al., 20"
2020.emnlp-main.168,D19-1098,0,0.183116,"et al., 2018; Tenney et al., 2019) that besides rich semantic information, implicit language structure knowledge can be captured by a deep BERT (Vig and Belinkov, 2019; Jawahar et al., 2019; Goldberg, 2019). However, such structure features learnt via the vanilla Transformer LM are insufficient for those NLP tasks that heavily rely on syntactic or linguistic knowledge (Hao et al., 2019). Some effort devote to improved the ability of structure Corresponding author. ytask ystruc Introduction ∗ ytask learning in Transformer LM by installing novel syntax-attention mechanisms (Ahmed et al., 2019; Wang et al., 2019). Nevertheless, several limitations can be observed. First, according to the recent findings by probing tasks (Conneau et al., 2018; Tenney et al., 2019; Goldberg, 2019), the syntactic structure representations are best retained right at the middle layers (Vig and Belinkov, 2019; Jawahar et al., 2019). Nevertheless, existing tree Transformers employ traditional full-scale training over the whole deep Transformer architecture (as shown in Figure 1(a)), consequently weakening the upper-layer semantic learning that can be crucial for end tasks. Second, these tree Transformer methods encode either"
2020.emnlp-main.168,N19-1075,0,0.113981,"g the upper-layer semantic learning that can be crucial for end tasks. Second, these tree Transformer methods encode either standalone constituency or dependency structure, while different tasks can depend on varying types of structural knowledge. The constituent and dependency representation for syntactic structure share underlying linguistic characteristics, while the former focuses on disclosing phrasal continuity and the latter aims at indicating dependency relations among elements. For example, semantic parsing tasks are more dependent on the dependency features (Rabinovich et al., 2017; Xia et al., 2019), while constituency information is much needed for sentiment classification (Socher et al., 2013). In this paper, we aim to retrofit structure-aware 2151 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2151–2161, c November 16–20, 2020. 2020 Association for Computational Linguistics Transformer LM for facilitating end tasks. • On the one hand, we propose a structure learning module for Transformer LM, meanwhile exploiting syntactic distance as the measurement for encoding both the phrasal constituency and the dependency connection. • On the other"
2020.emnlp-main.168,K15-1015,0,0.0176936,"alized word representation from language models can help to give the most task-related context-sensitive features (Peters et al., 2018). In this work, we follow the line of Transformer-based (Vaswani et al., 2017) LM (e.g., BERT), considering its prominence. Structure induction. The idea of introducing tree structures into deep models for structure-aware language modeling has long been explored by supervised structure learning, which generally relies on annotated parse trees during training and maximizes the joint likelihood of sentence-tree pairs (Socher et al., 2010, 2013; Tai et al., 2015; Yazdani and Henderson, 2015; Dyer et al., 2016; AlvarezMelis and Jaakkola, 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Wang et al., 2018; G¯u et al., 2018). There has been much attention paid to unsupervised grammar induction task (Williams et al., 2017; Shen et al., 2018a,b; Kuncoro et al., 2018; Kim et al., 2019a; Luo et al., 2019; Drozdov et al., 2019; Kim et al., 2019b). For example, PRPN (Shen et al., 2018a) computes the syntactic distance of word pairs. On-LSTM (Shen et al., 2018b) allows hidden neurons to learn long-term or shortterm information by a gate mechanism. URNNG (Kim et al., 2019b) applies"
2020.findings-emnlp.18,D15-1092,0,0.0248234,"tion in natural language processing (NLP), which has been used for a wide range of end tasks, such as sentiment analysis (SA) (Nguyen and Shirai, 2015; Teng and Zhang, 2017; Looks et al., 2017; Zhang and Zhang, 2019), neural machine translation (NMT) (Cho et al., 2014; Garmash and Monz, 2015; G¯u et al., 2018), language modeling (Yazdani and Henderson, 2015; Zhang et al., 2016; Zhou et al., 2017), semantic role labeling (SRL) (Marcheggiani and Titov, 2017; Strubell et al., 2018; Fei et al., 2020c), natural language inference (NLI) (Tai et al., 2015a; Liu et al., 2018) and text classification (Chen et al., 2015; Zhang et al., 2018b). Despite the usefulness of structure knowledge, most existing models use only a single syntactic tree, such as a constituency or a dependency tree. Constituent and dependency representation for syntactic structure share underlying linguistic and computational characteristics, while differ also in various aspects. For example, the former focuses ∗ NP Corresponding author. on revealing the continuity of phrases, while the latter is more effective in representing the dependencies among elements. By integrating the two representations from heterogeneous trees, the mutual ben"
2020.findings-emnlp.18,D15-1075,0,0.0183489,"Missing"
2020.findings-emnlp.18,A00-2018,0,0.732263,"2014; Nguyen and Shirai, 2015; Looks et al., 2017; Liu et al., 2018; Zhang and Zhang, 2019; Fei et al., 2020b). Generally, these methods consider injecting either standalone constituency tree or dependency tree by tree encoders such as TreeLSTM (Socher et al., 2013; Tai et al., 2015a) or GCN (Kipf and Welling, 2017). Based on the assumption that the dependency and constituency representation can be disentangled and coexist in one shared model, existing efforts are paid for joint constituent and dependency parsing, verifying the mutual benefit of these heterogeneous structures (Collins, 1997; Charniak, 2000; Charniak and Johnson, 2005; Farkas et al., 2011; Ren et al., 2013; Yoshikawa et al., 2017; Strzyz et al., 2019; Kato and Matsubara, 2019; Zhou and Zhao, 2019). However, little attention is paid for facilitating the syntax-dependent tasks via integrating Dependency Teachers Student Constituency Teachers Gold One-hot GCN Tree LSTM GCN     Tree LSTM Sequential Word Input Dependency Tree Input ,: Output Distill ,: Feature Distill Constituency Tree Input Figure 2: Overall framework of the proposed model. heterogeneous syntactic trees. Although the integration from heterogeneous trees can"
2020.findings-emnlp.18,P18-1198,0,0.0124361,"lla BERT). 4.3 SST Figure 3: Heterogeneous syntax distribution. The predominance of dependency syntax is above 0.5, otherwise for constituency. Table 2: Ablation results on distilled student. ‘Tea.Anl.’ refers to teacher annealing. In ‘Semantics’, we replace semantic learning Lsem with pre-trained contextualized word representations. Constituency 28.31 19.11 68.65 66.30 62.61 53.20 NLI Rel Heterogeneous Tree Structure Upper-bound of heterogeneous structures. We explore to what extent the distilled student can manage to capture heterogeneous tree structure information. Following previous work (Conneau et al., 2018), we consider employing two syntactic probing tasks, including 1) Constituent labeling, which assigns a non-terminal label for text spans within the phrase-structure (e.g., Verb, Noun, etc.), and 2) Dependency labeling, which predicts the relationship (edge) between two tokens (e.g., subject-object etc.). We take the last-layer output representation as the probing objective. We compare the student model with four teacher tree encoders, separately, based on the SRL task. As shown in Table 3, the student LSTM gives slightly lower score than one of the best tree models (i.e., GCN+dep. for depende"
2020.findings-emnlp.18,N19-1423,0,0.0666568,"Missing"
2020.findings-emnlp.18,W11-2924,0,0.612312,"use only a single syntactic tree, such as a constituency or a dependency tree. Constituent and dependency representation for syntactic structure share underlying linguistic and computational characteristics, while differ also in various aspects. For example, the former focuses ∗ NP Corresponding author. on revealing the continuity of phrases, while the latter is more effective in representing the dependencies among elements. By integrating the two representations from heterogeneous trees, the mutual benefit has been explored for joint parsing tasks (Collins, 1997; Charniak and Johnson, 2005; Farkas et al., 2011; Yoshikawa et al., 2017; Zhou and Zhao, 2019). Intuitively, complementary advantages from heterogeneous trees can facilitate a range of NLP tasks, especially syntax-dependent ones such as SRL and NLI. Taking the sentence of Figure 1 as example, where an example is shown from SRL1 task. In this case, the dependency links can locate the relations between arguments and predicates more efficiently, while the constituency structure can aggregate the phrasal spans for arguments, and guide the global path to the predicate. Integrating the features of two structures can better guide the model to focu"
2020.findings-emnlp.18,P19-1337,0,0.0976502,"lo et al., 2018), and existing methods are divided into two categories: 1) output distillation, which makes a teacher model output logits as a student model training objective (Kim and Rush, 2016; Vyas and Carpuat, 2019; Clark et al., 2019), 2) feature distillation, which allows a student to learn from a teacher’s intermediate feature representations (Zagoruyko and Komodakis, 2017; Sun et al., 2019). In this paper, we enhance the distillation of heterogeneous structures via both output and feature distillations by employing a sequential LSTM as the student. Our work is also closely related to Kuncoro et al., (2019), who distill syntactic structure knowledge to a student LSTM model. The difference lies in that they focus on transferring tree knowledge from syntax-aware language model for achieving scalable unsupervised syntax induction, while we aim at integrating heterogeneous syntax for improving downstream tasks. 3 Method As shown in Figure 2, the overall architecture consists of a sequential LSTM (Hochreiter and Schmidhuber, 1997) student, and several tree teachers for dependency and constituency structures. 184 3.1 Tree Encoder Teachers Different tree models can encode the same tree structure, resul"
2020.findings-emnlp.18,P17-1001,0,0.020349,"0.2 and 0.5, respectively. The training iteration thresholds G1 and G2 are set as 300 and 128, respectively. These values achieve the best performance in the development experiments. Baselines systems. We compare the following baselines. 1) Sequential encoders: LSTM, attention-based LSTM (ATTLSTM) and Transformer (Vaswani et al., 2017), sentence-state LSTM (S-LSTM) (Zhang et al., 2018a); 2) Tree encoders introduced in §2; 3) Ensemble models: ensembling learning (EnSem) (Wolpert, 1992; Ju et al., 2019), multi-task method (MTL) (Liu et al., 2016; Chen et al., 2018), adversarial training (AdvT) (Liu et al., 2017) and tree communication model (TCM) (Zhang and Zhang, 2019). For EnSem, we only concatenate the output representations of tree encodes. For MTL, we use an underlying shared LSTM for parameter sharing for tree encodes. For 187 Rel • Sequential Encoder LSTM 80.5 ATTLSTM 82.3 Transformer 84.7 S-LSTM 85.0 • Standalone Tree Model TreeLSTM+dep. 85.2 GCN+dep. 85.9 TreeLSTM+con. 85.0 GCN+con. 84.8 Avg. 85.3 • Tree Ensemble EnSem 85.5 MTL 84.9 AdvT 86.4 TCM 85.7 Avg. 85.9 • Distilled Student Best 89.2∗ • Others ESIM LG-SANs 85.6 BERT 91.3 AdvT, we adopt the shared-private architecture (Liu et al., 2017"
2020.findings-emnlp.18,D15-1287,0,0.0273453,"mutual benefit of constituency and dependency tree structures. (1) refers to the constituency tree structure, (2) indicates the semantic role labels, (3) refers to the example sentence, (4) represents the dependency tree structure. Introduction Integrating syntactic information into neural networks has received increasing attention in natural language processing (NLP), which has been used for a wide range of end tasks, such as sentiment analysis (SA) (Nguyen and Shirai, 2015; Teng and Zhang, 2017; Looks et al., 2017; Zhang and Zhang, 2019), neural machine translation (NMT) (Cho et al., 2014; Garmash and Monz, 2015; G¯u et al., 2018), language modeling (Yazdani and Henderson, 2015; Zhang et al., 2016; Zhou et al., 2017), semantic role labeling (SRL) (Marcheggiani and Titov, 2017; Strubell et al., 2018; Fei et al., 2020c), natural language inference (NLI) (Tai et al., 2015a; Liu et al., 2018) and text classification (Chen et al., 2015; Zhang et al., 2018b). Despite the usefulness of structure knowledge, most existing models use only a single syntactic tree, such as a constituency or a dependency tree. Constituent and dependency representation for syntactic structure share underlying linguistic and comput"
2020.findings-emnlp.18,D18-1037,0,0.0516081,"Missing"
2020.findings-emnlp.18,S10-1006,0,0.0143545,"Missing"
2020.findings-emnlp.18,P82-1020,0,0.780247,"Missing"
2020.findings-emnlp.18,P19-1530,0,0.203638,"e methods consider injecting either standalone constituency tree or dependency tree by tree encoders such as TreeLSTM (Socher et al., 2013; Tai et al., 2015a) or GCN (Kipf and Welling, 2017). Based on the assumption that the dependency and constituency representation can be disentangled and coexist in one shared model, existing efforts are paid for joint constituent and dependency parsing, verifying the mutual benefit of these heterogeneous structures (Collins, 1997; Charniak, 2000; Charniak and Johnson, 2005; Farkas et al., 2011; Ren et al., 2013; Yoshikawa et al., 2017; Strzyz et al., 2019; Kato and Matsubara, 2019; Zhou and Zhao, 2019). However, little attention is paid for facilitating the syntax-dependent tasks via integrating Dependency Teachers Student Constituency Teachers Gold One-hot GCN Tree LSTM GCN     Tree LSTM Sequential Word Input Dependency Tree Input ,: Output Distill ,: Feature Distill Constituency Tree Input Figure 2: Overall framework of the proposed model. heterogeneous syntactic trees. Although the integration from heterogeneous trees can be achieved via widely employed approaches, such as ensemble learning (Wolpert, 1992; Ju et al., 2019) and multi-task training (Liu et al."
2020.findings-emnlp.18,D16-1139,0,0.427745,"n better guide the model to focus on the most suitable phrasal granularity (as circled by the dotted box), and also ensure the route consistency between the semantic objective pairs. In this paper, we investigate the Knowledge Distillation (KD) method, which has been shown to be 1 We consider the span-based SRL, which aims to annotate the phrasal span of all semantic arguments. 183 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 183–193 c November 16 - 20, 2020. 2020 Association for Computational Linguistics effective for knowledge ensembling (Hinton et al., 2015; Kim and Rush, 2016; Furlanello et al., 2018), for heterogeneous structure integration. Specifically, we employ a sequential LSTM as the student for distilling heterogeneous syntactic structures from various teacher tree encoders, such as GCN (Kipf and Welling, 2017) and TreeLSTM (Tai et al., 2015a). We consider output distillation, syntactic feature injection and semantic learning. In addition, we introduce an alternative structure injection strategy to enhance the ability of heterogeneous syntactic representations within the shared sequential model. The distilled structure-aware student model can make inferenc"
2020.findings-emnlp.18,P18-1249,0,0.0337042,"Missing"
2020.findings-emnlp.18,D18-1184,0,0.0343641,"Missing"
2020.findings-emnlp.18,D17-1159,0,0.33711,"to the example sentence, (4) represents the dependency tree structure. Introduction Integrating syntactic information into neural networks has received increasing attention in natural language processing (NLP), which has been used for a wide range of end tasks, such as sentiment analysis (SA) (Nguyen and Shirai, 2015; Teng and Zhang, 2017; Looks et al., 2017; Zhang and Zhang, 2019), neural machine translation (NMT) (Cho et al., 2014; Garmash and Monz, 2015; G¯u et al., 2018), language modeling (Yazdani and Henderson, 2015; Zhang et al., 2016; Zhou et al., 2017), semantic role labeling (SRL) (Marcheggiani and Titov, 2017; Strubell et al., 2018; Fei et al., 2020c), natural language inference (NLI) (Tai et al., 2015a; Liu et al., 2018) and text classification (Chen et al., 2015; Zhang et al., 2018b). Despite the usefulness of structure knowledge, most existing models use only a single syntactic tree, such as a constituency or a dependency tree. Constituent and dependency representation for syntactic structure share underlying linguistic and computational characteristics, while differ also in various aspects. For example, the former focuses ∗ NP Corresponding author. on revealing the continuity of phrases, while"
2020.findings-emnlp.18,D15-1298,0,0.0481694,"Missing"
2020.findings-emnlp.18,N18-1202,0,0.0179919,"Missing"
2020.findings-emnlp.18,D13-1170,0,0.0260954,"-dependent tasks, including semantic role labeling, relation classification, natural language inference and sentiment classification. Results show that the distilled student outperforms tree encoders, verifying the advantage of integrating heterogeneous structures. The proposed method also outperforms existing ensemble methods and strong baseline systems, demonstrating its high effectiveness on structure information integration. 2 2.1 Related Work Syntactic Structures for Text Modeling Previous work shows that integrating syntactic structure knowledge can improve the performance of NLP tasks (Socher et al., 2013; Cho et al., 2014; Nguyen and Shirai, 2015; Looks et al., 2017; Liu et al., 2018; Zhang and Zhang, 2019; Fei et al., 2020b). Generally, these methods consider injecting either standalone constituency tree or dependency tree by tree encoders such as TreeLSTM (Socher et al., 2013; Tai et al., 2015a) or GCN (Kipf and Welling, 2017). Based on the assumption that the dependency and constituency representation can be disentangled and coexist in one shared model, existing efforts are paid for joint constituent and dependency parsing, verifying the mutual benefit of these heterogeneous structures (Co"
2020.findings-emnlp.18,D18-1548,0,0.0528092,"represents the dependency tree structure. Introduction Integrating syntactic information into neural networks has received increasing attention in natural language processing (NLP), which has been used for a wide range of end tasks, such as sentiment analysis (SA) (Nguyen and Shirai, 2015; Teng and Zhang, 2017; Looks et al., 2017; Zhang and Zhang, 2019), neural machine translation (NMT) (Cho et al., 2014; Garmash and Monz, 2015; G¯u et al., 2018), language modeling (Yazdani and Henderson, 2015; Zhang et al., 2016; Zhou et al., 2017), semantic role labeling (SRL) (Marcheggiani and Titov, 2017; Strubell et al., 2018; Fei et al., 2020c), natural language inference (NLI) (Tai et al., 2015a; Liu et al., 2018) and text classification (Chen et al., 2015; Zhang et al., 2018b). Despite the usefulness of structure knowledge, most existing models use only a single syntactic tree, such as a constituency or a dependency tree. Constituent and dependency representation for syntactic structure share underlying linguistic and computational characteristics, while differ also in various aspects. For example, the former focuses ∗ NP Corresponding author. on revealing the continuity of phrases, while the latter is more eff"
2020.findings-emnlp.18,P19-1531,0,0.156042,"Missing"
2020.findings-emnlp.18,D19-1441,0,0.0930925,"ional complexity. 2.2 Knowledge Distillation Our work is related to knowledge distillation techniques. It has been shown that KD is very effective and scalable for knowledge ensembling (Hinton et al., 2015; Furlanello et al., 2018), and existing methods are divided into two categories: 1) output distillation, which makes a teacher model output logits as a student model training objective (Kim and Rush, 2016; Vyas and Carpuat, 2019; Clark et al., 2019), 2) feature distillation, which allows a student to learn from a teacher’s intermediate feature representations (Zagoruyko and Komodakis, 2017; Sun et al., 2019). In this paper, we enhance the distillation of heterogeneous structures via both output and feature distillations by employing a sequential LSTM as the student. Our work is also closely related to Kuncoro et al., (2019), who distill syntactic structure knowledge to a student LSTM model. The difference lies in that they focus on transferring tree knowledge from syntax-aware language model for achieving scalable unsupervised syntax induction, while we aim at integrating heterogeneous syntax for improving downstream tasks. 3 Method As shown in Figure 2, the overall architecture consists of a seq"
2020.findings-emnlp.18,P15-1150,0,0.842782,"Missing"
2020.findings-emnlp.18,Q17-1012,0,0.0168857,"j nmod dobj IN NN AM-LOC at school case nmod NP JJ NN AM-TMP last week amod nmod Figure 1: An example illustrating the mutual benefit of constituency and dependency tree structures. (1) refers to the constituency tree structure, (2) indicates the semantic role labels, (3) refers to the example sentence, (4) represents the dependency tree structure. Introduction Integrating syntactic information into neural networks has received increasing attention in natural language processing (NLP), which has been used for a wide range of end tasks, such as sentiment analysis (SA) (Nguyen and Shirai, 2015; Teng and Zhang, 2017; Looks et al., 2017; Zhang and Zhang, 2019), neural machine translation (NMT) (Cho et al., 2014; Garmash and Monz, 2015; G¯u et al., 2018), language modeling (Yazdani and Henderson, 2015; Zhang et al., 2016; Zhou et al., 2017), semantic role labeling (SRL) (Marcheggiani and Titov, 2017; Strubell et al., 2018; Fei et al., 2020c), natural language inference (NLI) (Tai et al., 2015a; Liu et al., 2018) and text classification (Chen et al., 2015; Zhang et al., 2018b). Despite the usefulness of structure knowledge, most existing models use only a single syntactic tree, such as a constituency or a d"
2020.findings-emnlp.18,D19-1532,0,0.0179148,"semble learning (Wolpert, 1992; Ju et al., 2019) and multi-task training (Liu et al., 2016; Chen et al., 2018; Fei et al., 2020a), they usually suffer from low-efficiency and high computational complexity. 2.2 Knowledge Distillation Our work is related to knowledge distillation techniques. It has been shown that KD is very effective and scalable for knowledge ensembling (Hinton et al., 2015; Furlanello et al., 2018), and existing methods are divided into two categories: 1) output distillation, which makes a teacher model output logits as a student model training objective (Kim and Rush, 2016; Vyas and Carpuat, 2019; Clark et al., 2019), 2) feature distillation, which allows a student to learn from a teacher’s intermediate feature representations (Zagoruyko and Komodakis, 2017; Sun et al., 2019). In this paper, we enhance the distillation of heterogeneous structures via both output and feature distillations by employing a sequential LSTM as the student. Our work is also closely related to Kuncoro et al., (2019), who distill syntactic structure knowledge to a student LSTM model. The difference lies in that they focus on transferring tree knowledge from syntax-aware language model for achieving scalable un"
2020.findings-emnlp.18,P19-1295,0,0.030022,"Missing"
2020.findings-emnlp.18,K15-1015,0,0.0160677,". (1) refers to the constituency tree structure, (2) indicates the semantic role labels, (3) refers to the example sentence, (4) represents the dependency tree structure. Introduction Integrating syntactic information into neural networks has received increasing attention in natural language processing (NLP), which has been used for a wide range of end tasks, such as sentiment analysis (SA) (Nguyen and Shirai, 2015; Teng and Zhang, 2017; Looks et al., 2017; Zhang and Zhang, 2019), neural machine translation (NMT) (Cho et al., 2014; Garmash and Monz, 2015; G¯u et al., 2018), language modeling (Yazdani and Henderson, 2015; Zhang et al., 2016; Zhou et al., 2017), semantic role labeling (SRL) (Marcheggiani and Titov, 2017; Strubell et al., 2018; Fei et al., 2020c), natural language inference (NLI) (Tai et al., 2015a; Liu et al., 2018) and text classification (Chen et al., 2015; Zhang et al., 2018b). Despite the usefulness of structure knowledge, most existing models use only a single syntactic tree, such as a constituency or a dependency tree. Constituent and dependency representation for syntactic structure share underlying linguistic and computational characteristics, while differ also in various aspects. For"
2020.findings-emnlp.18,P17-1026,0,0.422109,"ntactic tree, such as a constituency or a dependency tree. Constituent and dependency representation for syntactic structure share underlying linguistic and computational characteristics, while differ also in various aspects. For example, the former focuses ∗ NP Corresponding author. on revealing the continuity of phrases, while the latter is more effective in representing the dependencies among elements. By integrating the two representations from heterogeneous trees, the mutual benefit has been explored for joint parsing tasks (Collins, 1997; Charniak and Johnson, 2005; Farkas et al., 2011; Yoshikawa et al., 2017; Zhou and Zhao, 2019). Intuitively, complementary advantages from heterogeneous trees can facilitate a range of NLP tasks, especially syntax-dependent ones such as SRL and NLI. Taking the sentence of Figure 1 as example, where an example is shown from SRL1 task. In this case, the dependency links can locate the relations between arguments and predicates more efficiently, while the constituency structure can aggregate the phrasal spans for arguments, and guide the global path to the predicate. Integrating the features of two structures can better guide the model to focus on the most suitable p"
2020.findings-emnlp.18,N16-1035,0,0.0249307,"ncy tree structure, (2) indicates the semantic role labels, (3) refers to the example sentence, (4) represents the dependency tree structure. Introduction Integrating syntactic information into neural networks has received increasing attention in natural language processing (NLP), which has been used for a wide range of end tasks, such as sentiment analysis (SA) (Nguyen and Shirai, 2015; Teng and Zhang, 2017; Looks et al., 2017; Zhang and Zhang, 2019), neural machine translation (NMT) (Cho et al., 2014; Garmash and Monz, 2015; G¯u et al., 2018), language modeling (Yazdani and Henderson, 2015; Zhang et al., 2016; Zhou et al., 2017), semantic role labeling (SRL) (Marcheggiani and Titov, 2017; Strubell et al., 2018; Fei et al., 2020c), natural language inference (NLI) (Tai et al., 2015a; Liu et al., 2018) and text classification (Chen et al., 2015; Zhang et al., 2018b). Despite the usefulness of structure knowledge, most existing models use only a single syntactic tree, such as a constituency or a dependency tree. Constituent and dependency representation for syntactic structure share underlying linguistic and computational characteristics, while differ also in various aspects. For example, the former"
2020.findings-emnlp.18,P19-1342,0,0.140815,"od NP JJ NN AM-TMP last week amod nmod Figure 1: An example illustrating the mutual benefit of constituency and dependency tree structures. (1) refers to the constituency tree structure, (2) indicates the semantic role labels, (3) refers to the example sentence, (4) represents the dependency tree structure. Introduction Integrating syntactic information into neural networks has received increasing attention in natural language processing (NLP), which has been used for a wide range of end tasks, such as sentiment analysis (SA) (Nguyen and Shirai, 2015; Teng and Zhang, 2017; Looks et al., 2017; Zhang and Zhang, 2019), neural machine translation (NMT) (Cho et al., 2014; Garmash and Monz, 2015; G¯u et al., 2018), language modeling (Yazdani and Henderson, 2015; Zhang et al., 2016; Zhou et al., 2017), semantic role labeling (SRL) (Marcheggiani and Titov, 2017; Strubell et al., 2018; Fei et al., 2020c), natural language inference (NLI) (Tai et al., 2015a; Liu et al., 2018) and text classification (Chen et al., 2015; Zhang et al., 2018b). Despite the usefulness of structure knowledge, most existing models use only a single syntactic tree, such as a constituency or a dependency tree. Constituent and dependency r"
2020.findings-emnlp.18,P18-1030,0,0.0942262,"guage processing (NLP), which has been used for a wide range of end tasks, such as sentiment analysis (SA) (Nguyen and Shirai, 2015; Teng and Zhang, 2017; Looks et al., 2017; Zhang and Zhang, 2019), neural machine translation (NMT) (Cho et al., 2014; Garmash and Monz, 2015; G¯u et al., 2018), language modeling (Yazdani and Henderson, 2015; Zhang et al., 2016; Zhou et al., 2017), semantic role labeling (SRL) (Marcheggiani and Titov, 2017; Strubell et al., 2018; Fei et al., 2020c), natural language inference (NLI) (Tai et al., 2015a; Liu et al., 2018) and text classification (Chen et al., 2015; Zhang et al., 2018b). Despite the usefulness of structure knowledge, most existing models use only a single syntactic tree, such as a constituency or a dependency tree. Constituent and dependency representation for syntactic structure share underlying linguistic and computational characteristics, while differ also in various aspects. For example, the former focuses ∗ NP Corresponding author. on revealing the continuity of phrases, while the latter is more effective in representing the dependencies among elements. By integrating the two representations from heterogeneous trees, the mutual benefit has been explor"
2020.findings-emnlp.18,D18-1244,0,0.129814,"guage processing (NLP), which has been used for a wide range of end tasks, such as sentiment analysis (SA) (Nguyen and Shirai, 2015; Teng and Zhang, 2017; Looks et al., 2017; Zhang and Zhang, 2019), neural machine translation (NMT) (Cho et al., 2014; Garmash and Monz, 2015; G¯u et al., 2018), language modeling (Yazdani and Henderson, 2015; Zhang et al., 2016; Zhou et al., 2017), semantic role labeling (SRL) (Marcheggiani and Titov, 2017; Strubell et al., 2018; Fei et al., 2020c), natural language inference (NLI) (Tai et al., 2015a; Liu et al., 2018) and text classification (Chen et al., 2015; Zhang et al., 2018b). Despite the usefulness of structure knowledge, most existing models use only a single syntactic tree, such as a constituency or a dependency tree. Constituent and dependency representation for syntactic structure share underlying linguistic and computational characteristics, while differ also in various aspects. For example, the former focuses ∗ NP Corresponding author. on revealing the continuity of phrases, while the latter is more effective in representing the dependencies among elements. By integrating the two representations from heterogeneous trees, the mutual benefit has been explor"
2020.findings-emnlp.18,P19-1230,0,0.291363,"constituency or a dependency tree. Constituent and dependency representation for syntactic structure share underlying linguistic and computational characteristics, while differ also in various aspects. For example, the former focuses ∗ NP Corresponding author. on revealing the continuity of phrases, while the latter is more effective in representing the dependencies among elements. By integrating the two representations from heterogeneous trees, the mutual benefit has been explored for joint parsing tasks (Collins, 1997; Charniak and Johnson, 2005; Farkas et al., 2011; Yoshikawa et al., 2017; Zhou and Zhao, 2019). Intuitively, complementary advantages from heterogeneous trees can facilitate a range of NLP tasks, especially syntax-dependent ones such as SRL and NLI. Taking the sentence of Figure 1 as example, where an example is shown from SRL1 task. In this case, the dependency links can locate the relations between arguments and predicates more efficiently, while the constituency structure can aggregate the phrasal spans for arguments, and guide the global path to the predicate. Integrating the features of two structures can better guide the model to focus on the most suitable phrasal granularity (as"
2020.findings-emnlp.8,N19-1115,0,0.0120717,"show that our model outperforms strong baselines by a large margin. In-depth analysis indicates that our method is highly effective in composing sentence semantics. 1 (a) The same syntactic structure but different semantics. A. They forced Tom to fake his history B. Tom cooked his own past under threat Similar: · Syntactic × · Semantic √ (b) The similar semantics but different syntactic structures. Figure 1: Comparisons of syntax and semantics in sentences. The same color indicates the same (similar) semantic objective. bring improved performance for syntax-dependent tasks (Shi et al., 2016; Havrylov et al., 2019), such as semantic role labeling (SRL) (Wang et al., 2019) and natural language inference (NLI) (Chen et al., 2017; Liu et al., 2018, 2019), etc. Intuitively, sequential semantic models and syntactic tree models play different roles in text modeling. Sequential semantic models learn the representation via adjacency neighborhood, while syntactic tree models encode texts through structural connections. Taking the two sentence pairs from the NLI task in Figure 1 as example, sentence A and B in example (a) share the same dependency structure but have irrelevant semantics, and tree models are more"
2020.findings-emnlp.8,P82-1020,0,0.784723,"Missing"
2020.findings-emnlp.8,N19-1254,0,0.0207978,"ing the importance of integrating syntax and semantics for text understanding. 2 ℎ????,? ℎ0???,? ℎ????? ,? ???? ,? ? ? ????? ,? ? ??? ,? ℎ? ℎ0???? ,? ??? ,? ℎ0 ??? ,? Figure 2: Overall architecture of the proposed model. In recent years, exploring the correlation between syntax and semantics has become a hot research topic. Previous work has shown a strong correlation between syntax and semantics, and proven that integrating syntactic tree models with sequential models could improve the performance of end tasks (Swayamdipta et al., 2016; Shi et al., 2016; Looks et al., 2017; Liu et al., 2018; Chen et al., 2019). For example, Shi et al. (2016) simultaneously conducted syntax parsing and semantic role labeling via multi-task training strategy. Swayamdipta et al. (2018) incorporated syntactic features into semantic parsing tasks by multi-task learning. Vashishth et al. (2019) concatenated the contextualized semantic representations with syntactic tree representations for improving the ability of word embeddings. More recently, Liu et al. (2019) added a multi-layer BiLSTM with shortcut connections to the Pairwise Word Interaction model for capturing semantics and syntactic structure of sentences. Howeve"
2020.findings-emnlp.8,D15-1189,0,0.0164291,"i i hall,t i = cti hall,t−1 i + (1 − cti ) + b) (27) all,t hi (28) all,t where hi is the ungated value from the concatenation of hseq,t and htree,t . The context gate cti for i i the node wi controls the contribution proportion of history representation and current representation during each step t. 3.3 (19) Decoding and Training We use a softmax classifier as the decoding layer: where xseq,t is the neighbor node representation i y = softmax(r) 87 (29) Task, Dataset and Evaluation. We conduct experiments on typical syntax-dependent tasks. 1) EFP, event factuality prediction on the UW dataset (Lee et al., 2015). EFP evaluates the performance of different methods with Pearson correlation coefficient (r). 2) Rel, relation classification for drug-drug interaction (Segura Bedmar et al., 2013). 3) SRL, semantic role labeling on the CoNLL08 WSJ dataset (Surdeanu et al., 2008). Rel and SRL use the F1 score to measure the performance of different models. 4) NLI, natural language inference, which also can be modeled as a sentence pair classification, and we investigate NLI on three benchmarks: QNLI (Rajpurkar et al., 2016), SICK (Marelli et al., 2014) and RTE (Bentivogli et al., 2009). For NLI, we use the ac"
2020.findings-emnlp.8,P17-1152,0,0.425329,"hly effective in composing sentence semantics. 1 (a) The same syntactic structure but different semantics. A. They forced Tom to fake his history B. Tom cooked his own past under threat Similar: · Syntactic × · Semantic √ (b) The similar semantics but different syntactic structures. Figure 1: Comparisons of syntax and semantics in sentences. The same color indicates the same (similar) semantic objective. bring improved performance for syntax-dependent tasks (Shi et al., 2016; Havrylov et al., 2019), such as semantic role labeling (SRL) (Wang et al., 2019) and natural language inference (NLI) (Chen et al., 2017; Liu et al., 2018, 2019), etc. Intuitively, sequential semantic models and syntactic tree models play different roles in text modeling. Sequential semantic models learn the representation via adjacency neighborhood, while syntactic tree models encode texts through structural connections. Taking the two sentence pairs from the NLI task in Figure 1 as example, sentence A and B in example (a) share the same dependency structure but have irrelevant semantics, and tree models are more suitable and effective for capturing the semantic difference than sequential models in this case. In example (b),"
2020.findings-emnlp.8,D19-1114,0,0.250688,"ree models with sequential models could improve the performance of end tasks (Swayamdipta et al., 2016; Shi et al., 2016; Looks et al., 2017; Liu et al., 2018; Chen et al., 2019). For example, Shi et al. (2016) simultaneously conducted syntax parsing and semantic role labeling via multi-task training strategy. Swayamdipta et al. (2018) incorporated syntactic features into semantic parsing tasks by multi-task learning. Vashishth et al. (2019) concatenated the contextualized semantic representations with syntactic tree representations for improving the ability of word embeddings. More recently, Liu et al. (2019) added a multi-layer BiLSTM with shortcut connections to the Pairwise Word Interaction model for capturing semantics and syntactic structure of sentences. However, these methods only use shallow integration of syntax and semantics, limiting the performance of end tasks. Our model is inspired by Zhang et al. (2019), who introduce a novel method allowing the sufficient communication between different tree models for sentiment analysis. Unlike their work, this paper is dedicated to realizing a deep communication between syntactic tree model and sequential semantic model for improving text underst"
2020.findings-emnlp.8,D18-1184,0,0.0360396,"Missing"
2020.findings-emnlp.8,N19-1423,0,0.0170672,"different syntactic structures. Therefore, two types of models should interact closely in learning compositional representations for better understanding of the texts. However, existing efforts integrate tree and sequential models through a straightforward way such as representations concatenation (Chen et al., 2017; Vashishth et al., 2019) or multi-task learning (Shi et al., 2016; Swayamdipta et al., 2018; Chen Introduction Neural sequential models such as LSTM (Hochreiter and Schmidhuber, 1997), GRU (Cho et al., 2014), Transformer (Vaswani et al., 2017), ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have been extensively applied for encoding the semantics of texts in natural language processing (NLP) (Sundermeyer et al., 2012; Bahdanau et al., 2015; Dozat and Manning, 2017; Yuan et al., 2019). On the other hand, hierarchical tree models, such as TreeLSTM (Socher et al., 2013) and GCN (Kipf and Welling, 2017), have been introduced to enrich sequence encoding with syntactic information, bringing further strengths in text modeling. Such external syntactic structure knowledge provides enhanced features, which can facilitate a broad range of NLP tasks (Tai et al., 2015; Looks et al., 2017; Z"
2020.findings-emnlp.8,P15-2047,0,0.0156757,"anau et al., 2015), question answering (Yuan et al., 2019), etc. On the other hand, some efforts devote to develop hierarchical tree models such as TreeLSTM and GCN, based on syntactic structures (e.g., dependency tree). Such tree encoders equipped with external syntactic knowledge can bring further improvements for some NLP tasks, especially syntax-dependent ones (Tai et al., 2015; Looks et al., 2017; Zhang and Zhang, 2019; Fei et al., 2020b,a), such as SRL (Swayamdipta et al., 2016; Wang et al., 2019; Fei et al., 2020c), NLI (Chen et al., 2017; Liu et al., 2019) and relation classification (Liu et al., 2015; Tran et al., 2019), etc. 85 3.1.2 Tree Encoder We employ the dependency tree as the underlying structure, where all the nodes are input words and connected with directed edges, as the sentences shown in Figure 1. We use two typical tree models for encoding the structure, including TreeLSTM and GCN, both under a bidirectional setting. The standard TreeLSTM encodes each node j with its corresponding head word representation as input xj . For the bottom-up TreeLSTM: X ↑ ↑ hk (8) hj = more effective in composing semantic information of texts. 3 Model In this paper, we propose a deep neural commu"
2020.findings-emnlp.8,J93-2004,0,0.0704019,"Missing"
2020.findings-emnlp.8,marelli-etal-2014-sick,0,0.016451,"Missing"
2020.findings-emnlp.8,N19-1286,0,0.0180876,", question answering (Yuan et al., 2019), etc. On the other hand, some efforts devote to develop hierarchical tree models such as TreeLSTM and GCN, based on syntactic structures (e.g., dependency tree). Such tree encoders equipped with external syntactic knowledge can bring further improvements for some NLP tasks, especially syntax-dependent ones (Tai et al., 2015; Looks et al., 2017; Zhang and Zhang, 2019; Fei et al., 2020b,a), such as SRL (Swayamdipta et al., 2016; Wang et al., 2019; Fei et al., 2020c), NLI (Chen et al., 2017; Liu et al., 2019) and relation classification (Liu et al., 2015; Tran et al., 2019), etc. 85 3.1.2 Tree Encoder We employ the dependency tree as the underlying structure, where all the nodes are input words and connected with directed edges, as the sentences shown in Figure 1. We use two typical tree models for encoding the structure, including TreeLSTM and GCN, both under a bidirectional setting. The standard TreeLSTM encodes each node j with its corresponding head word representation as input xj . For the bottom-up TreeLSTM: X ↑ ↑ hk (8) hj = more effective in composing semantic information of texts. 3 Model In this paper, we propose a deep neural communication model betwe"
2020.findings-emnlp.8,D14-1162,0,0.0834542,"• Sequential semantic models, including BiLSTM, attention-based BiLSTM, Transformer and sentence-state LSTM (S-LSTM) (Zhang et al., 2018). • Syntactic tree models, including standalone TreeLSTM or GCN encoder introduced in § 3.1.2. Experiments Experimental Setups • Syntax and semantics ensemble models, including ensembling learning (Wolpert, 1992; Ju et al., 2019) and multi-task method (MTL) (Liu et al., 2016). Hyperparameters. For BiLSTM, TreeLSTM and GCN, we all use a 2-layer version. The dimension of word embeddings is set to 300, which is initialized with the pre-trained GloVe embedding (Pennington et al., 2014). All the hidden sizes in neural networks are set to 350. We adopt the Adam optimizer with an initial learning rate in [1e-5, 2e-5, 1e-6], and L2 weight decay of 0.01. We use the mini-batch in [16, 32, 64] based on the tasks, and apply 0.5 dropout ratio for word embeddings. λ is fine-tuned according to specific tasks. For ensemble models, we concatenate the output representations of tree encoder TreeLSTM and sequential model BiLSTM. For MTL, we use the underlying shared structure for parameter sharing for TreeLSTM and BiLSTM. For the NLI task, we additionally compare the syntax-semantics integ"
2020.findings-emnlp.8,N18-1202,0,0.0291342,"ery similar semantics but have different syntactic structures. Therefore, two types of models should interact closely in learning compositional representations for better understanding of the texts. However, existing efforts integrate tree and sequential models through a straightforward way such as representations concatenation (Chen et al., 2017; Vashishth et al., 2019) or multi-task learning (Shi et al., 2016; Swayamdipta et al., 2018; Chen Introduction Neural sequential models such as LSTM (Hochreiter and Schmidhuber, 1997), GRU (Cho et al., 2014), Transformer (Vaswani et al., 2017), ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have been extensively applied for encoding the semantics of texts in natural language processing (NLP) (Sundermeyer et al., 2012; Bahdanau et al., 2015; Dozat and Manning, 2017; Yuan et al., 2019). On the other hand, hierarchical tree models, such as TreeLSTM (Socher et al., 2013) and GCN (Kipf and Welling, 2017), have been introduced to enrich sequence encoding with syntactic information, bringing further strengths in text modeling. Such external syntactic structure knowledge provides enhanced features, which can facilitate a broad range of NLP tasks (Tai et a"
2020.findings-emnlp.8,D16-1264,0,0.0378694,"Missing"
2020.findings-emnlp.8,P19-1529,0,0.0614667,"argin. In-depth analysis indicates that our method is highly effective in composing sentence semantics. 1 (a) The same syntactic structure but different semantics. A. They forced Tom to fake his history B. Tom cooked his own past under threat Similar: · Syntactic × · Semantic √ (b) The similar semantics but different syntactic structures. Figure 1: Comparisons of syntax and semantics in sentences. The same color indicates the same (similar) semantic objective. bring improved performance for syntax-dependent tasks (Shi et al., 2016; Havrylov et al., 2019), such as semantic role labeling (SRL) (Wang et al., 2019) and natural language inference (NLI) (Chen et al., 2017; Liu et al., 2018, 2019), etc. Intuitively, sequential semantic models and syntactic tree models play different roles in text modeling. Sequential semantic models learn the representation via adjacency neighborhood, while syntactic tree models encode texts through structural connections. Taking the two sentence pairs from the NLI task in Figure 1 as example, sentence A and B in example (a) share the same dependency structure but have irrelevant semantics, and tree models are more suitable and effective for capturing the semantic differen"
2020.findings-emnlp.8,S13-2056,0,0.0206154,"Missing"
2020.findings-emnlp.8,D19-1280,0,0.0595237,"Missing"
2020.findings-emnlp.8,D16-1098,0,0.136084,"ax-dependent tasks show that our model outperforms strong baselines by a large margin. In-depth analysis indicates that our method is highly effective in composing sentence semantics. 1 (a) The same syntactic structure but different semantics. A. They forced Tom to fake his history B. Tom cooked his own past under threat Similar: · Syntactic × · Semantic √ (b) The similar semantics but different syntactic structures. Figure 1: Comparisons of syntax and semantics in sentences. The same color indicates the same (similar) semantic objective. bring improved performance for syntax-dependent tasks (Shi et al., 2016; Havrylov et al., 2019), such as semantic role labeling (SRL) (Wang et al., 2019) and natural language inference (NLI) (Chen et al., 2017; Liu et al., 2018, 2019), etc. Intuitively, sequential semantic models and syntactic tree models play different roles in text modeling. Sequential semantic models learn the representation via adjacency neighborhood, while syntactic tree models encode texts through structural connections. Taking the two sentence pairs from the NLI task in Figure 1 as example, sentence A and B in example (a) share the same dependency structure but have irrelevant semantics, a"
2020.findings-emnlp.8,P19-1342,0,0.0718435,"), have been extensively applied for encoding the semantics of texts in natural language processing (NLP) (Sundermeyer et al., 2012; Bahdanau et al., 2015; Dozat and Manning, 2017; Yuan et al., 2019). On the other hand, hierarchical tree models, such as TreeLSTM (Socher et al., 2013) and GCN (Kipf and Welling, 2017), have been introduced to enrich sequence encoding with syntactic information, bringing further strengths in text modeling. Such external syntactic structure knowledge provides enhanced features, which can facilitate a broad range of NLP tasks (Tai et al., 2015; Looks et al., 2017; Zhang and Zhang, 2019). Recent studies show that integrating syntactic tree models with sequential semantic models can ∗ Similar: · Syntactic √ · Semantic × Corresponding author. 84 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 84–93 c November 16 - 20, 2020. 2020 Association for Computational Linguistics et al., 2019), limiting the performance of end tasks. We believe that a better integration can be achieved when adequate interactions between sequential semantic encoder and syntactic tree encoder can take place during learning, improving the performance of end tasks, and also allevi"
2020.findings-emnlp.8,D13-1170,0,0.00627064,"s concatenation (Chen et al., 2017; Vashishth et al., 2019) or multi-task learning (Shi et al., 2016; Swayamdipta et al., 2018; Chen Introduction Neural sequential models such as LSTM (Hochreiter and Schmidhuber, 1997), GRU (Cho et al., 2014), Transformer (Vaswani et al., 2017), ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have been extensively applied for encoding the semantics of texts in natural language processing (NLP) (Sundermeyer et al., 2012; Bahdanau et al., 2015; Dozat and Manning, 2017; Yuan et al., 2019). On the other hand, hierarchical tree models, such as TreeLSTM (Socher et al., 2013) and GCN (Kipf and Welling, 2017), have been introduced to enrich sequence encoding with syntactic information, bringing further strengths in text modeling. Such external syntactic structure knowledge provides enhanced features, which can facilitate a broad range of NLP tasks (Tai et al., 2015; Looks et al., 2017; Zhang and Zhang, 2019). Recent studies show that integrating syntactic tree models with sequential semantic models can ∗ Similar: · Syntactic √ · Semantic × Corresponding author. 84 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 84–93 c November 16 - 20,"
2020.findings-emnlp.8,P18-1030,0,0.111,"ion model for capturing semantics and syntactic structure of sentences. However, these methods only use shallow integration of syntax and semantics, limiting the performance of end tasks. Our model is inspired by Zhang et al. (2019), who introduce a novel method allowing the sufficient communication between different tree models for sentiment analysis. Unlike their work, this paper is dedicated to realizing a deep communication between syntactic tree model and sequential semantic model for improving text understanding. The idea of sentence-level propagation in our work is partially related to Zhang et al. (2018), who propose a novel LSTM architecture where a set of global states are used for sentence-level propagation along recurrent steps, rather than incremental reading of a sequence of words in vanilla sequential LSTM. Compared with their model, our model is Related Work Neural sequential models have been widely used for encoding texts in the NLP community, due to their effectiveness on capturing semantics. Representative models such as LSTM, GRU, Transformer, ELMo and BERT, have been employed for various NLP tasks, including language modeling (Sundermeyer et al., 2012), machine translation (Bahda"
2020.findings-emnlp.8,K16-1019,0,0.121351,"icates that our method is highly effective in composing sentence semantics, verifying the importance of integrating syntax and semantics for text understanding. 2 ℎ????,? ℎ0???,? ℎ????? ,? ???? ,? ? ? ????? ,? ? ??? ,? ℎ? ℎ0???? ,? ??? ,? ℎ0 ??? ,? Figure 2: Overall architecture of the proposed model. In recent years, exploring the correlation between syntax and semantics has become a hot research topic. Previous work has shown a strong correlation between syntax and semantics, and proven that integrating syntactic tree models with sequential models could improve the performance of end tasks (Swayamdipta et al., 2016; Shi et al., 2016; Looks et al., 2017; Liu et al., 2018; Chen et al., 2019). For example, Shi et al. (2016) simultaneously conducted syntax parsing and semantic role labeling via multi-task training strategy. Swayamdipta et al. (2018) incorporated syntactic features into semantic parsing tasks by multi-task learning. Vashishth et al. (2019) concatenated the contextualized semantic representations with syntactic tree representations for improving the ability of word embeddings. More recently, Liu et al. (2019) added a multi-layer BiLSTM with shortcut connections to the Pairwise Word Interactio"
2020.findings-emnlp.8,D18-1412,0,0.110917,"irrelevant semantics, and tree models are more suitable and effective for capturing the semantic difference than sequential models in this case. In example (b), two sentences convey very similar semantics but have different syntactic structures. Therefore, two types of models should interact closely in learning compositional representations for better understanding of the texts. However, existing efforts integrate tree and sequential models through a straightforward way such as representations concatenation (Chen et al., 2017; Vashishth et al., 2019) or multi-task learning (Shi et al., 2016; Swayamdipta et al., 2018; Chen Introduction Neural sequential models such as LSTM (Hochreiter and Schmidhuber, 1997), GRU (Cho et al., 2014), Transformer (Vaswani et al., 2017), ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have been extensively applied for encoding the semantics of texts in natural language processing (NLP) (Sundermeyer et al., 2012; Bahdanau et al., 2015; Dozat and Manning, 2017; Yuan et al., 2019). On the other hand, hierarchical tree models, such as TreeLSTM (Socher et al., 2013) and GCN (Kipf and Welling, 2017), have been introduced to enrich sequence encoding with syntactic informa"
2020.findings-emnlp.8,P15-1150,0,0.226666,"., 2018) and BERT (Devlin et al., 2019), have been extensively applied for encoding the semantics of texts in natural language processing (NLP) (Sundermeyer et al., 2012; Bahdanau et al., 2015; Dozat and Manning, 2017; Yuan et al., 2019). On the other hand, hierarchical tree models, such as TreeLSTM (Socher et al., 2013) and GCN (Kipf and Welling, 2017), have been introduced to enrich sequence encoding with syntactic information, bringing further strengths in text modeling. Such external syntactic structure knowledge provides enhanced features, which can facilitate a broad range of NLP tasks (Tai et al., 2015; Looks et al., 2017; Zhang and Zhang, 2019). Recent studies show that integrating syntactic tree models with sequential semantic models can ∗ Similar: · Syntactic √ · Semantic × Corresponding author. 84 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 84–93 c November 16 - 20, 2020. 2020 Association for Computational Linguistics et al., 2019), limiting the performance of end tasks. We believe that a better integration can be achieved when adequate interactions between sequential semantic encoder and syntactic tree encoder can take place during learning, improving t"
2021.acl-long.372,W18-2501,0,0.0220696,"Missing"
2021.acl-long.372,P19-1024,0,0.127751,"are entity fragments. Following, fragment relations are predicted by another classifier to determine whether two specific fragments involve a certain relation. We define two relations for our goal: Overlapping or Succession, which are used for overlapped and discontinuous entities, respectively. In essence, the joint model can be regarded as one kind of relation extraction models, which is adapted for our goal. To enhance our model, we utilize the syntax information as well by using a dependency-guided graph convolutional network (Kipf and Welling, 2017; Zhang et al., 2018; Jie and Lu, 2019; Guo et al., 2019). We evaluate our proposed model on several benchmark datasets which includes both overlapped and discontinuous entities (e.g., CLEF (Suominen et al., 2013)). The results show that our model outperforms the hypergraph (Muis and Lu, 2016; Wang and Lu, 2019) and transition-based models (Dai et al., 2020). Besides, we conduct experiments on two benchmark datasets including only overlapped entities (i.e., GENIA (Kim et al., 2003) and ACE05). Experimental results show that our model can also obtain comparable performances with the state-of-the-art models (Luan et al., 2019; Wadden et al., 2019; Str"
2021.acl-long.372,H05-1048,0,0.127879,"enhanced by dependencyguided graph convolutional network (GCN) (Zhang et al., 2018; Guo et al., 2019). To our knowledge, syntax information is commonly neglected in most previous work for overlapped or discontinuous NER, except Finkel and Manning (2009). The work employs a constituency parser to transform a sentence into a nested entity tree, and syntax information is used naturally to facilitate NER. By contrast, syntax information has been utilized in some studies for traditional regular NER. Under the traditional statistical setting, syntax information is used by manually-crafted features (Hacioglu et al., 2005; Ling and Weld, 2012) or auxiliary tasks (Florian et al., 2006) for NER. Recently, Jie et al. (2017) build a semi-CRF model based on dependency information to optimize the research space of NER recognition. Jie and Lu (2019) stack the dependency-guided graph convolutional network (Zhang et al., 2018; Guo et al., 2019) on top of the BiLSTM layer. These studies have demonstrated that syntax information could be an effective feature source for NER. 3 Method The key idea of our model includes two mechanisms. First, our model enumerates all possible text spans in a sentence and then exploits a mul"
2021.acl-long.372,N16-1030,0,0.0422792,"dered as a sequence labeling problem (Liu et al., 2018; Lin et al., 2019b; Cao et al., 2019). With well-designed features, CRF-based models have achieved the leading performance (Lafferty et al., 2001; Finkel et al., 2005; Liu et al., 2011). Recently, neural network models have been exploited for feature representations (Chen and Manning, 2014; Zhou et al., 2015). Moreover, contextualized word representations such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018) and BERT (Devlin et al., 2019) have also achieved great success. As for NER, the end-to-end bi-directional LSTM CRF models (Lample et al., 2016; Ma and Hovy, 2016; Yang et al., 2018) is one representative architecture. These models are only capable of recognizing regular named entities. For overlapped NER, the earliest model to our knowledge is proposed by Finkel and Manning (2009), where they convert overlapped NER as a parsing task. Lu and Roth (2015) propose a hypergraph model to recognize overlapped entities and lead to a number of extensions (Muis and Lu, 2017; Katiyar and Cardie, 2018; Wang and Lu, 2018). Moreover, recurrent neural networks (RNNs) are also used for overlapped NER (Ju et al., 2018; Wang et al., 2018). Other appr"
2021.acl-long.372,2020.acl-main.519,0,0.0666574,"Missing"
2021.acl-long.372,P19-1511,0,0.0365435,"Missing"
2021.acl-long.372,P19-1016,0,0.064587,"be overlapping or succession. In this way, we can recognize not only discontinuous entities, and meanwhile doubly check the overlapped entities. As a whole, our model can be regarded as a relation extraction paradigm essentially. Experimental results on multiple benchmark datasets (i.e., CLEF, GENIA and ACE05) show that our model is highly competitive for overlapped and discontinuous NER. 1 Introduction Named entity recognition (NER) (Sang and De Meulder, 2003) is one fundamental task for natural language processing (NLP), due to its wide application in information extraction and data mining (Lin et al., 2019b; Cao et al., 2019). Traditionally, NER is presented as a sequence labeling problem and widely solved by conditional random field (CRF) based models (Lafferty et al., 2001). However, this framework is difficult to handle overlapped and discontinuous entities (Lu and Roth, 2015; Muis and Lu, 2016), which we illustrate using two examples as shown in Figure 1. The two entities “Pennsylvania” and “Pennsylvania radio station” are nested with each other,1 and the sec∗ 1 Corresponding author. We consider “nested” as a special case of “overlapped”. ond example shows a discontinuous entity “mitral lea"
2021.acl-long.372,D16-1008,0,0.31922,"ENIA and ACE05) show that our model is highly competitive for overlapped and discontinuous NER. 1 Introduction Named entity recognition (NER) (Sang and De Meulder, 2003) is one fundamental task for natural language processing (NLP), due to its wide application in information extraction and data mining (Lin et al., 2019b; Cao et al., 2019). Traditionally, NER is presented as a sequence labeling problem and widely solved by conditional random field (CRF) based models (Lafferty et al., 2001). However, this framework is difficult to handle overlapped and discontinuous entities (Lu and Roth, 2015; Muis and Lu, 2016), which we illustrate using two examples as shown in Figure 1. The two entities “Pennsylvania” and “Pennsylvania radio station” are nested with each other,1 and the sec∗ 1 Corresponding author. We consider “nested” as a special case of “overlapped”. ond example shows a discontinuous entity “mitral leaflets thickened” involving three fragments. There have been several studies to investigate overlapped or discontinuous entities (Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017; Katiyar and Cardie, 2018; Wang and Lu, 2018; Ju et al., 2018; Wang et al., 2018; Fisher and Vlachos, 2019"
2021.acl-long.372,D17-1276,0,0.14249,"However, this framework is difficult to handle overlapped and discontinuous entities (Lu and Roth, 2015; Muis and Lu, 2016), which we illustrate using two examples as shown in Figure 1. The two entities “Pennsylvania” and “Pennsylvania radio station” are nested with each other,1 and the sec∗ 1 Corresponding author. We consider “nested” as a special case of “overlapped”. ond example shows a discontinuous entity “mitral leaflets thickened” involving three fragments. There have been several studies to investigate overlapped or discontinuous entities (Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017; Katiyar and Cardie, 2018; Wang and Lu, 2018; Ju et al., 2018; Wang et al., 2018; Fisher and Vlachos, 2019; Luan et al., 2019; Wang and Lu, 2019). The majority of them focus on overlapped NER, with only several exceptions to the best of our knowledge. Muis and Lu (2016) present a hypergraph model that is capable of handling both overlapped and discontinuous entities. Wang and Lu (2019) extend the hypergraph model with long short-term memories (LSTMs) (Hochreiter and Schmidhuber, 1997). Dai et al. (2020) proposed a transition-based neural model for discontinuous NER. By using these models, NER"
2021.acl-long.372,D14-1162,0,0.090372,"ethods without contextualized representations. 5.4 Result Analysis based on Entity Types Comparing with BiLSTM-CRF To show the necessity of building one model to recognize regular, overlapped and discontinuous entities simultaneously, we analyze the predicted entities in the CLEF-Dis dataset and classify them based on their types, as shown in Figure 4. In addition, we compare our model with BiLSTM-CRF (Lample et al., 2016; Ma and Hovy, 2016; Yang et al., 2018), to show our model does not influence the performance of regular NER significantly. For a fair comparison, we replace BERT with Glove (Pennington et al., 2014) and keep the setting of our model the same with the setting of the BiLSTM-CRF model used in previous work (Yang et al., 2018). As seen, if only considering regular entities, the 8 Many discontinuous entities are also overlapped, but we do not count them as overlapped entities in this figure. r+o r+d r+o+d Figure 5: Result analysis based on entity types on the CLEF-Dis dataset, comparing with Dai et al. (2020) (blue). BiLSTM-CRF model can achieve a better performance compared with our model, especially the precision value is much higher. One likely reason might be that the BiLSTM-CRF model is"
2021.acl-long.372,P17-1001,0,0.0576066,"Missing"
2021.acl-long.372,N18-1202,0,0.00874374,"odel enhancement are effective in the benchmark datasets. Our code is available at https://github.com/foxlf823/sodner. 2 Related Work In the NLP domain, NER is usually considered as a sequence labeling problem (Liu et al., 2018; Lin et al., 2019b; Cao et al., 2019). With well-designed features, CRF-based models have achieved the leading performance (Lafferty et al., 2001; Finkel et al., 2005; Liu et al., 2011). Recently, neural network models have been exploited for feature representations (Chen and Manning, 2014; Zhou et al., 2015). Moreover, contextualized word representations such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018) and BERT (Devlin et al., 2019) have also achieved great success. As for NER, the end-to-end bi-directional LSTM CRF models (Lample et al., 2016; Ma and Hovy, 2016; Yang et al., 2018) is one representative architecture. These models are only capable of recognizing regular named entities. For overlapped NER, the earliest model to our knowledge is proposed by Finkel and Manning (2009), where they convert overlapped NER as a parsing task. Lu and Roth (2015) propose a hypergraph model to recognize overlapped entities and lead to a number of extensions (Muis and Lu, 2017"
2021.acl-long.372,P11-1037,0,0.0585102,"that our model can also obtain comparable performances with the state-of-the-art models (Luan et al., 2019; Wadden et al., 2019; Strakov´a et al., 2019). In addition, we observe that our approaches for model enhancement are effective in the benchmark datasets. Our code is available at https://github.com/foxlf823/sodner. 2 Related Work In the NLP domain, NER is usually considered as a sequence labeling problem (Liu et al., 2018; Lin et al., 2019b; Cao et al., 2019). With well-designed features, CRF-based models have achieved the leading performance (Lafferty et al., 2001; Finkel et al., 2005; Liu et al., 2011). Recently, neural network models have been exploited for feature representations (Chen and Manning, 2014; Zhou et al., 2015). Moreover, contextualized word representations such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018) and BERT (Devlin et al., 2019) have also achieved great success. As for NER, the end-to-end bi-directional LSTM CRF models (Lample et al., 2016; Ma and Hovy, 2016; Yang et al., 2018) is one representative architecture. These models are only capable of recognizing regular named entities. For overlapped NER, the earliest model to our knowledge is proposed by Finke"
2021.acl-long.372,S14-2007,0,0.0634049,"Missing"
2021.acl-long.372,W03-0419,0,0.258164,"Missing"
2021.acl-long.372,P19-1527,0,0.034235,"Missing"
2021.acl-long.372,P19-1138,0,0.0386649,"Missing"
2021.acl-long.372,C18-1327,0,0.0176749,"u et al., 2018; Lin et al., 2019b; Cao et al., 2019). With well-designed features, CRF-based models have achieved the leading performance (Lafferty et al., 2001; Finkel et al., 2005; Liu et al., 2011). Recently, neural network models have been exploited for feature representations (Chen and Manning, 2014; Zhou et al., 2015). Moreover, contextualized word representations such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018) and BERT (Devlin et al., 2019) have also achieved great success. As for NER, the end-to-end bi-directional LSTM CRF models (Lample et al., 2016; Ma and Hovy, 2016; Yang et al., 2018) is one representative architecture. These models are only capable of recognizing regular named entities. For overlapped NER, the earliest model to our knowledge is proposed by Finkel and Manning (2009), where they convert overlapped NER as a parsing task. Lu and Roth (2015) propose a hypergraph model to recognize overlapped entities and lead to a number of extensions (Muis and Lu, 2017; Katiyar and Cardie, 2018; Wang and Lu, 2018). Moreover, recurrent neural networks (RNNs) are also used for overlapped NER (Ju et al., 2018; Wang et al., 2018). Other approaches include multi-grained detection"
2021.acl-long.372,P16-1040,1,0.825641,"ER could be conducted universally without any assumption to exclude overlapped or discontinuous entities, which could be more practical in real applications. The hypergraph (Muis and Lu, 2016; Wang and Lu, 2019) and transition-based models (Dai et al., 2020) are flexible to be adapted for different tasks, achieving great successes for overlapped or discontinuous NER. However, these models need to manually define graph nodes, edges and transition actions. Moreover, these models build graphs or generate transitions along the words in the sentences gradually, which may lead to error propagation (Zhang et al., 2016). In contrast, the spanbased scheme might be a good alternative, which is much simpler including only span-level classification. Thus, it needs less manual intervention and meanwhile span-level classification can be fully parallelized without error propagation. Recently, Luan et al. (2019) utilized the span-based model for information extraction effectively. In this work, we propose a novel span-based joint model to recognize overlapped and discon4814 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural L"
2021.acl-long.372,D18-1244,0,0.360802,"t al., 2019), classifying whether they are entity fragments. Following, fragment relations are predicted by another classifier to determine whether two specific fragments involve a certain relation. We define two relations for our goal: Overlapping or Succession, which are used for overlapped and discontinuous entities, respectively. In essence, the joint model can be regarded as one kind of relation extraction models, which is adapted for our goal. To enhance our model, we utilize the syntax information as well by using a dependency-guided graph convolutional network (Kipf and Welling, 2017; Zhang et al., 2018; Jie and Lu, 2019; Guo et al., 2019). We evaluate our proposed model on several benchmark datasets which includes both overlapped and discontinuous entities (e.g., CLEF (Suominen et al., 2013)). The results show that our model outperforms the hypergraph (Muis and Lu, 2016; Wang and Lu, 2019) and transition-based models (Dai et al., 2020). Besides, we conduct experiments on two benchmark datasets including only overlapped entities (i.e., GENIA (Kim et al., 2003) and ACE05). Experimental results show that our model can also obtain comparable performances with the state-of-the-art models (Luan e"
2021.acl-long.372,D19-1034,0,0.0371452,"Missing"
2021.acl-long.372,P15-1117,0,0.0176781,"2019; Strakov´a et al., 2019). In addition, we observe that our approaches for model enhancement are effective in the benchmark datasets. Our code is available at https://github.com/foxlf823/sodner. 2 Related Work In the NLP domain, NER is usually considered as a sequence labeling problem (Liu et al., 2018; Lin et al., 2019b; Cao et al., 2019). With well-designed features, CRF-based models have achieved the leading performance (Lafferty et al., 2001; Finkel et al., 2005; Liu et al., 2011). Recently, neural network models have been exploited for feature representations (Chen and Manning, 2014; Zhou et al., 2015). Moreover, contextualized word representations such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018) and BERT (Devlin et al., 2019) have also achieved great success. As for NER, the end-to-end bi-directional LSTM CRF models (Lample et al., 2016; Ma and Hovy, 2016; Yang et al., 2018) is one representative architecture. These models are only capable of recognizing regular named entities. For overlapped NER, the earliest model to our knowledge is proposed by Finkel and Manning (2009), where they convert overlapped NER as a parsing task. Lu and Roth (2015) propose a hypergraph model to r"
2021.acl-long.372,D19-1585,0,0.143846,"2019; Guo et al., 2019). We evaluate our proposed model on several benchmark datasets which includes both overlapped and discontinuous entities (e.g., CLEF (Suominen et al., 2013)). The results show that our model outperforms the hypergraph (Muis and Lu, 2016; Wang and Lu, 2019) and transition-based models (Dai et al., 2020). Besides, we conduct experiments on two benchmark datasets including only overlapped entities (i.e., GENIA (Kim et al., 2003) and ACE05). Experimental results show that our model can also obtain comparable performances with the state-of-the-art models (Luan et al., 2019; Wadden et al., 2019; Strakov´a et al., 2019). In addition, we observe that our approaches for model enhancement are effective in the benchmark datasets. Our code is available at https://github.com/foxlf823/sodner. 2 Related Work In the NLP domain, NER is usually considered as a sequence labeling problem (Liu et al., 2018; Lin et al., 2019b; Cao et al., 2019). With well-designed features, CRF-based models have achieved the leading performance (Lafferty et al., 2001; Finkel et al., 2005; Liu et al., 2011). Recently, neural network models have been exploited for feature representations (Chen and Manning, 2014; Zhou"
2021.acl-long.372,D18-1019,0,0.0168956,"le overlapped and discontinuous entities (Lu and Roth, 2015; Muis and Lu, 2016), which we illustrate using two examples as shown in Figure 1. The two entities “Pennsylvania” and “Pennsylvania radio station” are nested with each other,1 and the sec∗ 1 Corresponding author. We consider “nested” as a special case of “overlapped”. ond example shows a discontinuous entity “mitral leaflets thickened” involving three fragments. There have been several studies to investigate overlapped or discontinuous entities (Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017; Katiyar and Cardie, 2018; Wang and Lu, 2018; Ju et al., 2018; Wang et al., 2018; Fisher and Vlachos, 2019; Luan et al., 2019; Wang and Lu, 2019). The majority of them focus on overlapped NER, with only several exceptions to the best of our knowledge. Muis and Lu (2016) present a hypergraph model that is capable of handling both overlapped and discontinuous entities. Wang and Lu (2019) extend the hypergraph model with long short-term memories (LSTMs) (Hochreiter and Schmidhuber, 1997). Dai et al. (2020) proposed a transition-based neural model for discontinuous NER. By using these models, NER could be conducted universally without any a"
2021.acl-long.372,D19-1644,0,0.354757,"using two examples as shown in Figure 1. The two entities “Pennsylvania” and “Pennsylvania radio station” are nested with each other,1 and the sec∗ 1 Corresponding author. We consider “nested” as a special case of “overlapped”. ond example shows a discontinuous entity “mitral leaflets thickened” involving three fragments. There have been several studies to investigate overlapped or discontinuous entities (Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017; Katiyar and Cardie, 2018; Wang and Lu, 2018; Ju et al., 2018; Wang et al., 2018; Fisher and Vlachos, 2019; Luan et al., 2019; Wang and Lu, 2019). The majority of them focus on overlapped NER, with only several exceptions to the best of our knowledge. Muis and Lu (2016) present a hypergraph model that is capable of handling both overlapped and discontinuous entities. Wang and Lu (2019) extend the hypergraph model with long short-term memories (LSTMs) (Hochreiter and Schmidhuber, 1997). Dai et al. (2020) proposed a transition-based neural model for discontinuous NER. By using these models, NER could be conducted universally without any assumption to exclude overlapped or discontinuous entities, which could be more practical in real appl"
2021.acl-long.372,D18-1124,0,0.0405263,"Missing"
2021.findings-acl.117,P17-1162,0,0.0156285,"block, and predict the relation of a pair of entities using local and global features simultaneously. • Our model achieves the state-of-the-art performances on three benchmark datasets for document-level RE. We also conduct extensive analyses of our model to better understand its working mechanism.2 2 Codes are publicly available at https://github. com/ljynlp/MRN 2 Related Work Relation extraction (RE), including sentence-level RE and document-level RE, plays a crucial role in a wide variety of knowledge-based applications, such as question answering (Hixon et al., 2015), dialogue generation (He et al., 2017), etc. Recent studies largely focus on sentence-level RE by various neural network methods, such as CNN (Zeng et al., 2014; dos Santos et al., 2015), BiLSTM (Zhang et al., 2015; Cai et al., 2016), attention mechanism (Wang et al., 2016; Lin et al., 2016), and neural graph models (Zhang et al., 2018; Zhu et al., 2019). However, in practice, many relational facts need to be inferred across multiple sentences in a document, so researchers have shown a growing interest in document-level RE. Compared with sentence-level RE, documentlevel RE needs to consider the complicated interactions between ent"
2021.findings-acl.117,N15-1086,0,0.0313414,"ork in concert with the mention reasoning block, and predict the relation of a pair of entities using local and global features simultaneously. • Our model achieves the state-of-the-art performances on three benchmark datasets for document-level RE. We also conduct extensive analyses of our model to better understand its working mechanism.2 2 Codes are publicly available at https://github. com/ljynlp/MRN 2 Related Work Relation extraction (RE), including sentence-level RE and document-level RE, plays a crucial role in a wide variety of knowledge-based applications, such as question answering (Hixon et al., 2015), dialogue generation (He et al., 2017), etc. Recent studies largely focus on sentence-level RE by various neural network methods, such as CNN (Zeng et al., 2014; dos Santos et al., 2015), BiLSTM (Zhang et al., 2015; Cai et al., 2016), attention mechanism (Wang et al., 2016; Lin et al., 2016), and neural graph models (Zhang et al., 2018; Zhu et al., 2019). However, in practice, many relational facts need to be inferred across multiple sentences in a document, so researchers have shown a growing interest in document-level RE. Compared with sentence-level RE, documentlevel RE needs to consider t"
2021.findings-acl.117,2020.coling-main.136,0,0.215222,"Intuitively, the former can benefit the identification of nearer (e.g., intra-sentence) relations, while the latter is more useful for distant (e.g., inter-sentence) relations. For document-level RE, such context information is closely related to the ubiquitous mentions in a document, so mention-based reasoning with different context granularities is highly important for the task. However, most previous studies have not distinguished local and global reasoning explicitly (Peng et al., 2017; Christopoulou et al., 2019; Sahu et al., 2019; Nan et al., 2020; Zhou et al., 2020; Zeng et al., 2020; Li et al., 2020; Zhang et al., 2020). Recently Wang et al. (2020) investigate local and global contexts for documentlevel RE by performing global and local reasoning consecutively. However, their pipeline method can be problematic because it ignores the interactions and communications of local and global contexts,1 which limits the performance of the task. 1 From Figure 1, it is shown that the intra-sentence relations P 27 and P 102 can help to identify the inter-sentence relation P 17, since ‘U.S.’ and ‘Democratic’ are linked through ‘Finnegan’. 1359 Findings of the Association for Computational Linguistics"
2021.findings-acl.117,P16-1200,0,0.0397431,"judge in 1939. ... P570 [S12] He continued to serve until his death in 1971. ... P17: country object P27: country of citizenship object P102: member of political party P570: date of death Figure 1: An example of document-level RE from the DocRED dataset. We use the same color to denote the mentions of the same entity. Introduction Relation extraction (RE), identifying the semantic relations among target entities in the text, has long been a fundamental task in the natural language processing (NLP) community (Zeng et al., 2014; Xu et al., 2015). Prior efforts largely focus on sentencelevel RE (Lin et al., 2016; Zhang et al., 2018). However, recent studies reveal that a large number of relations can actually be expressed through multiple sentences, which necessitates document-level RE (Yao et al., 2019). Compared with sentencelevel RE, the entities for document-level relations may be mentioned in multiple sentences across a document. Therefore, document-level RE requires capturing the complex interactions between all enti∗ Corresponding author. subject: intra-sentence relation subject: inter-sentence relation ties in the entire document (Nan et al., 2020; Zeng et al., 2020; Wang et al., 2020). It is"
2021.findings-acl.117,P16-1072,0,0.0999689,"ent-level RE. We also conduct extensive analyses of our model to better understand its working mechanism.2 2 Codes are publicly available at https://github. com/ljynlp/MRN 2 Related Work Relation extraction (RE), including sentence-level RE and document-level RE, plays a crucial role in a wide variety of knowledge-based applications, such as question answering (Hixon et al., 2015), dialogue generation (He et al., 2017), etc. Recent studies largely focus on sentence-level RE by various neural network methods, such as CNN (Zeng et al., 2014; dos Santos et al., 2015), BiLSTM (Zhang et al., 2015; Cai et al., 2016), attention mechanism (Wang et al., 2016; Lin et al., 2016), and neural graph models (Zhang et al., 2018; Zhu et al., 2019). However, in practice, many relational facts need to be inferred across multiple sentences in a document, so researchers have shown a growing interest in document-level RE. Compared with sentence-level RE, documentlevel RE needs to consider the complicated interactions between entities across multiple sentences. With this in mind, researchers begin to use graph neural networks to reason intra- and inter-sentence relations and make certain progress in extracting inter-sent"
2021.findings-acl.117,2020.acl-main.141,0,0.32052,"or efforts largely focus on sentencelevel RE (Lin et al., 2016; Zhang et al., 2018). However, recent studies reveal that a large number of relations can actually be expressed through multiple sentences, which necessitates document-level RE (Yao et al., 2019). Compared with sentencelevel RE, the entities for document-level relations may be mentioned in multiple sentences across a document. Therefore, document-level RE requires capturing the complex interactions between all enti∗ Corresponding author. subject: intra-sentence relation subject: inter-sentence relation ties in the entire document (Nan et al., 2020; Zeng et al., 2020; Wang et al., 2020). It is well known that local and global contexts are two key performance enhancers for the task. Intuitively, the former can benefit the identification of nearer (e.g., intra-sentence) relations, while the latter is more useful for distant (e.g., inter-sentence) relations. For document-level RE, such context information is closely related to the ubiquitous mentions in a document, so mention-based reasoning with different context granularities is highly important for the task. However, most previous studies have not distinguished local and global reasonin"
2021.findings-acl.117,W16-2922,0,0.0678618,"Missing"
2021.findings-acl.117,W18-2314,0,0.0328795,"Missing"
2021.findings-acl.117,D19-1498,0,0.106129,"2020). It is well known that local and global contexts are two key performance enhancers for the task. Intuitively, the former can benefit the identification of nearer (e.g., intra-sentence) relations, while the latter is more useful for distant (e.g., inter-sentence) relations. For document-level RE, such context information is closely related to the ubiquitous mentions in a document, so mention-based reasoning with different context granularities is highly important for the task. However, most previous studies have not distinguished local and global reasoning explicitly (Peng et al., 2017; Christopoulou et al., 2019; Sahu et al., 2019; Nan et al., 2020; Zhou et al., 2020; Zeng et al., 2020; Li et al., 2020; Zhang et al., 2020). Recently Wang et al. (2020) investigate local and global contexts for documentlevel RE by performing global and local reasoning consecutively. However, their pipeline method can be problematic because it ignores the interactions and communications of local and global contexts,1 which limits the performance of the task. 1 From Figure 1, it is shown that the intra-sentence relations P 27 and P 102 can help to identify the inter-sentence relation P 17, since ‘U.S.’ and ‘Democratic’ a"
2021.findings-acl.117,N19-1423,0,0.0604824,"fine classifier are leveraged for jointly reasoning the relations between subject and object entities. 3.1 Encoding Layer We first map each word wi into a vector, and concatenate it with its corresponding entity type ti :3 3 ti denotes the type of entity mention that contains this word (e.g. if an entity type is Person, its mention word type is also Person). t xi = [xw i ; xi ] . (1) Then, we adopt BiLSTM to encode the vectorial word representations into contextualized word representations: hi = BiLSTM(xi ) , (2) where hi is the token hidden representation. Note that we also can use the BERT (Devlin et al., 2019) as an alternative to improve performances. Based on hi , we can obtain the mention representation: i mi = Max{hj }bj=a , i (3) where ai and bi are the start and end positions of the i-th mention, respectively. 3.2 Interactive Mention-based Reasoning Layer As we argued earlier, local and global context information is closely related to the ubiquitous mentions in a document. We thus propose a mention-based module for multi-hop reasoning among the relationships of all the mentions. Considering that there exist overlapping relations where multiple relations share the same mention, we distinguish"
2021.findings-acl.117,Q17-1008,0,0.145644,"2020; Wang et al., 2020). It is well known that local and global contexts are two key performance enhancers for the task. Intuitively, the former can benefit the identification of nearer (e.g., intra-sentence) relations, while the latter is more useful for distant (e.g., inter-sentence) relations. For document-level RE, such context information is closely related to the ubiquitous mentions in a document, so mention-based reasoning with different context granularities is highly important for the task. However, most previous studies have not distinguished local and global reasoning explicitly (Peng et al., 2017; Christopoulou et al., 2019; Sahu et al., 2019; Nan et al., 2020; Zhou et al., 2020; Zeng et al., 2020; Li et al., 2020; Zhang et al., 2020). Recently Wang et al. (2020) investigate local and global contexts for documentlevel RE by performing global and local reasoning consecutively. However, their pipeline method can be problematic because it ignores the interactions and communications of local and global contexts,1 which limits the performance of the task. 1 From Figure 1, it is shown that the intra-sentence relations P 27 and P 102 can help to identify the inter-sentence relation P 17, sin"
2021.findings-acl.117,D14-1162,0,0.0838777,"Missing"
2021.findings-acl.117,P19-1423,0,0.121508,"t local and global contexts are two key performance enhancers for the task. Intuitively, the former can benefit the identification of nearer (e.g., intra-sentence) relations, while the latter is more useful for distant (e.g., inter-sentence) relations. For document-level RE, such context information is closely related to the ubiquitous mentions in a document, so mention-based reasoning with different context granularities is highly important for the task. However, most previous studies have not distinguished local and global reasoning explicitly (Peng et al., 2017; Christopoulou et al., 2019; Sahu et al., 2019; Nan et al., 2020; Zhou et al., 2020; Zeng et al., 2020; Li et al., 2020; Zhang et al., 2020). Recently Wang et al. (2020) investigate local and global contexts for documentlevel RE by performing global and local reasoning consecutively. However, their pipeline method can be problematic because it ignores the interactions and communications of local and global contexts,1 which limits the performance of the task. 1 From Figure 1, it is shown that the intra-sentence relations P 27 and P 102 can help to identify the inter-sentence relation P 17, since ‘U.S.’ and ‘Democratic’ are linked through ‘"
2021.findings-acl.117,P15-1061,0,0.0108408,"performances on three benchmark datasets for document-level RE. We also conduct extensive analyses of our model to better understand its working mechanism.2 2 Codes are publicly available at https://github. com/ljynlp/MRN 2 Related Work Relation extraction (RE), including sentence-level RE and document-level RE, plays a crucial role in a wide variety of knowledge-based applications, such as question answering (Hixon et al., 2015), dialogue generation (He et al., 2017), etc. Recent studies largely focus on sentence-level RE by various neural network methods, such as CNN (Zeng et al., 2014; dos Santos et al., 2015), BiLSTM (Zhang et al., 2015; Cai et al., 2016), attention mechanism (Wang et al., 2016; Lin et al., 2016), and neural graph models (Zhang et al., 2018; Zhu et al., 2019). However, in practice, many relational facts need to be inferred across multiple sentences in a document, so researchers have shown a growing interest in document-level RE. Compared with sentence-level RE, documentlevel RE needs to consider the complicated interactions between entities across multiple sentences. With this in mind, researchers begin to use graph neural networks to reason intra- and inter-sentence relations and"
2021.findings-acl.117,D17-1188,0,0.0155994,"etrics, where Ign F1 is calculated by excluding the common relation facts shared by the training, development and test sets. Depending on whether relation arguments occur within one sentence or not, F1 can be further split into intra-F1 and inter-F1. The results of our model are presented after a significant test (p≤0.03). 4.2 Baselines We make comparisons with the current state-of-theart systems, including two categories. 1) Sequencebased methods, which use different neural architectures to encode the entire document, include CNN (Zeng et al., 2014), BiLSTM (Cai et al., 2016), Context-Aware (Sorokin and Gurevych, 2017) and HIN (Tang et al., 2020). 2) Graphbased methods, which construct homogeneous or heterogeneous graphs based on the whole document, include GAT (Velickovic et al., 2018), GCNN (Sahu et al., 2019), EoG (Christopoulou et al., 2019), LSR (Nan et al., 2020), GAIN (Zeng et al., 2020) and GLRE (Wang et al., 2020). Besides, some models leverage BERT (Devlin et al., 2019) for task improvements, including twophase+BERT (Wang et al., 2019) and Coref+BERT 1363 Dev Test Ign F1 F1 Intra-F1 Inter-F1 Ign F1 F1 CNN (Yao et al., 2019) BiLSTM (Yao et al., 2019) Context-Aware (Yao et al., 2019) HIN-GloVe (Tang"
2021.findings-acl.117,2020.emnlp-main.127,0,0.425903,"focus on sentencelevel RE (Lin et al., 2016; Zhang et al., 2018). However, recent studies reveal that a large number of relations can actually be expressed through multiple sentences, which necessitates document-level RE (Yao et al., 2019). Compared with sentencelevel RE, the entities for document-level relations may be mentioned in multiple sentences across a document. Therefore, document-level RE requires capturing the complex interactions between all enti∗ Corresponding author. subject: intra-sentence relation subject: inter-sentence relation ties in the entire document (Nan et al., 2020; Zeng et al., 2020; Wang et al., 2020). It is well known that local and global contexts are two key performance enhancers for the task. Intuitively, the former can benefit the identification of nearer (e.g., intra-sentence) relations, while the latter is more useful for distant (e.g., inter-sentence) relations. For document-level RE, such context information is closely related to the ubiquitous mentions in a document, so mention-based reasoning with different context granularities is highly important for the task. However, most previous studies have not distinguished local and global reasoning explicitly (Peng"
2021.findings-acl.117,Y15-1009,0,0.0210289,"k datasets for document-level RE. We also conduct extensive analyses of our model to better understand its working mechanism.2 2 Codes are publicly available at https://github. com/ljynlp/MRN 2 Related Work Relation extraction (RE), including sentence-level RE and document-level RE, plays a crucial role in a wide variety of knowledge-based applications, such as question answering (Hixon et al., 2015), dialogue generation (He et al., 2017), etc. Recent studies largely focus on sentence-level RE by various neural network methods, such as CNN (Zeng et al., 2014; dos Santos et al., 2015), BiLSTM (Zhang et al., 2015; Cai et al., 2016), attention mechanism (Wang et al., 2016; Lin et al., 2016), and neural graph models (Zhang et al., 2018; Zhu et al., 2019). However, in practice, many relational facts need to be inferred across multiple sentences in a document, so researchers have shown a growing interest in document-level RE. Compared with sentence-level RE, documentlevel RE needs to consider the complicated interactions between entities across multiple sentences. With this in mind, researchers begin to use graph neural networks to reason intra- and inter-sentence relations and make certain progress in ex"
2021.findings-acl.117,N18-1080,0,0.0476993,"Missing"
2021.findings-acl.117,D18-1244,0,0.0621305,"P570 [S12] He continued to serve until his death in 1971. ... P17: country object P27: country of citizenship object P102: member of political party P570: date of death Figure 1: An example of document-level RE from the DocRED dataset. We use the same color to denote the mentions of the same entity. Introduction Relation extraction (RE), identifying the semantic relations among target entities in the text, has long been a fundamental task in the natural language processing (NLP) community (Zeng et al., 2014; Xu et al., 2015). Prior efforts largely focus on sentencelevel RE (Lin et al., 2016; Zhang et al., 2018). However, recent studies reveal that a large number of relations can actually be expressed through multiple sentences, which necessitates document-level RE (Yao et al., 2019). Compared with sentencelevel RE, the entities for document-level relations may be mentioned in multiple sentences across a document. Therefore, document-level RE requires capturing the complex interactions between all enti∗ Corresponding author. subject: intra-sentence relation subject: inter-sentence relation ties in the entire document (Nan et al., 2020; Zeng et al., 2020; Wang et al., 2020). It is well known that loca"
2021.findings-acl.117,2020.coling-main.143,0,0.0446798,"Missing"
2021.findings-acl.117,2020.emnlp-main.303,0,0.215865,"evel RE (Lin et al., 2016; Zhang et al., 2018). However, recent studies reveal that a large number of relations can actually be expressed through multiple sentences, which necessitates document-level RE (Yao et al., 2019). Compared with sentencelevel RE, the entities for document-level relations may be mentioned in multiple sentences across a document. Therefore, document-level RE requires capturing the complex interactions between all enti∗ Corresponding author. subject: intra-sentence relation subject: inter-sentence relation ties in the entire document (Nan et al., 2020; Zeng et al., 2020; Wang et al., 2020). It is well known that local and global contexts are two key performance enhancers for the task. Intuitively, the former can benefit the identification of nearer (e.g., intra-sentence) relations, while the latter is more useful for distant (e.g., inter-sentence) relations. For document-level RE, such context information is closely related to the ubiquitous mentions in a document, so mention-based reasoning with different context granularities is highly important for the task. However, most previous studies have not distinguished local and global reasoning explicitly (Peng et al., 2017; Christ"
2021.findings-acl.117,2020.coling-main.461,0,0.230448,"ey performance enhancers for the task. Intuitively, the former can benefit the identification of nearer (e.g., intra-sentence) relations, while the latter is more useful for distant (e.g., inter-sentence) relations. For document-level RE, such context information is closely related to the ubiquitous mentions in a document, so mention-based reasoning with different context granularities is highly important for the task. However, most previous studies have not distinguished local and global reasoning explicitly (Peng et al., 2017; Christopoulou et al., 2019; Sahu et al., 2019; Nan et al., 2020; Zhou et al., 2020; Zeng et al., 2020; Li et al., 2020; Zhang et al., 2020). Recently Wang et al. (2020) investigate local and global contexts for documentlevel RE by performing global and local reasoning consecutively. However, their pipeline method can be problematic because it ignores the interactions and communications of local and global contexts,1 which limits the performance of the task. 1 From Figure 1, it is shown that the intra-sentence relations P 27 and P 102 can help to identify the inter-sentence relation P 17, since ‘U.S.’ and ‘Democratic’ are linked through ‘Finnegan’. 1359 Findings of the Assoc"
2021.findings-acl.117,P19-1128,0,0.0114456,"icly available at https://github. com/ljynlp/MRN 2 Related Work Relation extraction (RE), including sentence-level RE and document-level RE, plays a crucial role in a wide variety of knowledge-based applications, such as question answering (Hixon et al., 2015), dialogue generation (He et al., 2017), etc. Recent studies largely focus on sentence-level RE by various neural network methods, such as CNN (Zeng et al., 2014; dos Santos et al., 2015), BiLSTM (Zhang et al., 2015; Cai et al., 2016), attention mechanism (Wang et al., 2016; Lin et al., 2016), and neural graph models (Zhang et al., 2018; Zhu et al., 2019). However, in practice, many relational facts need to be inferred across multiple sentences in a document, so researchers have shown a growing interest in document-level RE. Compared with sentence-level RE, documentlevel RE needs to consider the complicated interactions between entities across multiple sentences. With this in mind, researchers begin to use graph neural networks to reason intra- and inter-sentence relations and make certain progress in extracting inter-sentence relations with document-level graph convolutional neural network (Peng et al., 2017; Velickovic et al., 2018; Christop"
2021.findings-acl.117,P16-1123,0,0.0567967,"Missing"
2021.findings-acl.117,D15-1062,0,0.0197666,"atic nomination P102 for the position of a Chicago municipal court judge in 1939. ... P570 [S12] He continued to serve until his death in 1971. ... P17: country object P27: country of citizenship object P102: member of political party P570: date of death Figure 1: An example of document-level RE from the DocRED dataset. We use the same color to denote the mentions of the same entity. Introduction Relation extraction (RE), identifying the semantic relations among target entities in the text, has long been a fundamental task in the natural language processing (NLP) community (Zeng et al., 2014; Xu et al., 2015). Prior efforts largely focus on sentencelevel RE (Lin et al., 2016; Zhang et al., 2018). However, recent studies reveal that a large number of relations can actually be expressed through multiple sentences, which necessitates document-level RE (Yao et al., 2019). Compared with sentencelevel RE, the entities for document-level relations may be mentioned in multiple sentences across a document. Therefore, document-level RE requires capturing the complex interactions between all enti∗ Corresponding author. subject: intra-sentence relation subject: inter-sentence relation ties in the entire docum"
2021.findings-acl.117,P19-1074,0,0.168675,"1: An example of document-level RE from the DocRED dataset. We use the same color to denote the mentions of the same entity. Introduction Relation extraction (RE), identifying the semantic relations among target entities in the text, has long been a fundamental task in the natural language processing (NLP) community (Zeng et al., 2014; Xu et al., 2015). Prior efforts largely focus on sentencelevel RE (Lin et al., 2016; Zhang et al., 2018). However, recent studies reveal that a large number of relations can actually be expressed through multiple sentences, which necessitates document-level RE (Yao et al., 2019). Compared with sentencelevel RE, the entities for document-level relations may be mentioned in multiple sentences across a document. Therefore, document-level RE requires capturing the complex interactions between all enti∗ Corresponding author. subject: intra-sentence relation subject: inter-sentence relation ties in the entire document (Nan et al., 2020; Zeng et al., 2020; Wang et al., 2020). It is well known that local and global contexts are two key performance enhancers for the task. Intuitively, the former can benefit the identification of nearer (e.g., intra-sentence) relations, while"
2021.findings-acl.117,2020.emnlp-main.582,0,0.0370662,"Missing"
2021.findings-acl.117,C14-1220,0,0.425882,"y sought the Democratic nomination P102 for the position of a Chicago municipal court judge in 1939. ... P570 [S12] He continued to serve until his death in 1971. ... P17: country object P27: country of citizenship object P102: member of political party P570: date of death Figure 1: An example of document-level RE from the DocRED dataset. We use the same color to denote the mentions of the same entity. Introduction Relation extraction (RE), identifying the semantic relations among target entities in the text, has long been a fundamental task in the natural language processing (NLP) community (Zeng et al., 2014; Xu et al., 2015). Prior efforts largely focus on sentencelevel RE (Lin et al., 2016; Zhang et al., 2018). However, recent studies reveal that a large number of relations can actually be expressed through multiple sentences, which necessitates document-level RE (Yao et al., 2019). Compared with sentencelevel RE, the entities for document-level relations may be mentioned in multiple sentences across a document. Therefore, document-level RE requires capturing the complex interactions between all enti∗ Corresponding author. subject: intra-sentence relation subject: inter-sentence relation ties i"
2021.findings-acl.49,W13-3820,0,0.0277389,"rmances, meanwhile bringing explainable task improvements. 1 (3) (4) PP A0 V A1 AM-LOC She met her sister in the pub nsubj root ROOT det nmod case obj obl Figure 1: The mutual benefit to integrate both the (1) syntactic constituency and (4) dependency structures for (2) SRL, based on (3) an example sentence. Semantic role labeling (SRL) aims to disclose the predicate-argument structure of a given sentence. Such shallow semantic structures have been shown highly useful for a wide range of downstream tasks in natural language processing (NLP), such as information extraction (Fader et al., 2011; Bastianelli et al., 2013), machine translation (Xiong et al., 2012; Shi et al., 2016) and question answering (Maqsud et al., 2014; Xu et al., 2020). Based on whether to recognize the constituent phrasal span or the syntactic dependency head token of an argument, prior works categorize SRL into two types: the span-based SRL popularized in CoNLL05/12 shared tasks (Carreras and M`arquez, 2005; Pradhan et al., 2013), and the dependency-based SRL introduced in CoNLL08/09 shared tasks (Surdeanu Corresponding author. NP NP (2) Introduction ∗ VP (1) et al., 2008; Hajiˇc et al., 2009). By adopting various neural network method"
2021.findings-acl.49,A00-2018,0,0.66,"ic dependent feature is more preferred to be injected into the dependency-based SRL (Roth and Lapata, 2016; Marcheggiani and Titov, 2017; He et al., 2018; Kasai et al., 2019), while other consider the constituent syntax for the span-based SRL (Wang 550 et al., 2019; Marcheggiani and Titov, 2020). Actually, the constituent and dependency syntax depict the structural features from different angles, while they can share close linguistic relevance. Related works have revealed the mutual benefits on integrating these two heterogeneous syntactic representations for various NLP tasks (Collins, 1997; Charniak, 2000; Charniak and Johnson, 2005; Farkas et al., 2011; Yoshikawa et al., 2017; Zhou and Zhao, 2019; Strzyz et al., 2019; Kato and Matsubara, 2019). Unfortunately, there are very limited explorations for SRL. For example, Li et al. (2010) construct discrete heterogeneous syntactic features for SRL. More recent work in Fei et al. (2020a) leverage knowledge distillation method to inject the heterogeneous syntax representations from various tree encoders into one model for enhancing the span-based SRL. In this work, we consider an explicit integration of these two syntactic structures via two neural s"
2021.findings-acl.49,P97-1003,0,0.400606,"ly, the syntactic dependent feature is more preferred to be injected into the dependency-based SRL (Roth and Lapata, 2016; Marcheggiani and Titov, 2017; He et al., 2018; Kasai et al., 2019), while other consider the constituent syntax for the span-based SRL (Wang 550 et al., 2019; Marcheggiani and Titov, 2020). Actually, the constituent and dependency syntax depict the structural features from different angles, while they can share close linguistic relevance. Related works have revealed the mutual benefits on integrating these two heterogeneous syntactic representations for various NLP tasks (Collins, 1997; Charniak, 2000; Charniak and Johnson, 2005; Farkas et al., 2011; Yoshikawa et al., 2017; Zhou and Zhao, 2019; Strzyz et al., 2019; Kato and Matsubara, 2019). Unfortunately, there are very limited explorations for SRL. For example, Li et al. (2010) construct discrete heterogeneous syntactic features for SRL. More recent work in Fei et al. (2020a) leverage knowledge distillation method to inject the heterogeneous syntax representations from various tree encoders into one model for enhancing the span-based SRL. In this work, we consider an explicit integration of these two syntactic structures"
2021.findings-acl.49,D11-1142,0,0.0186384,"of-the-art SRL performances, meanwhile bringing explainable task improvements. 1 (3) (4) PP A0 V A1 AM-LOC She met her sister in the pub nsubj root ROOT det nmod case obj obl Figure 1: The mutual benefit to integrate both the (1) syntactic constituency and (4) dependency structures for (2) SRL, based on (3) an example sentence. Semantic role labeling (SRL) aims to disclose the predicate-argument structure of a given sentence. Such shallow semantic structures have been shown highly useful for a wide range of downstream tasks in natural language processing (NLP), such as information extraction (Fader et al., 2011; Bastianelli et al., 2013), machine translation (Xiong et al., 2012; Shi et al., 2016) and question answering (Maqsud et al., 2014; Xu et al., 2020). Based on whether to recognize the constituent phrasal span or the syntactic dependency head token of an argument, prior works categorize SRL into two types: the span-based SRL popularized in CoNLL05/12 shared tasks (Carreras and M`arquez, 2005; Pradhan et al., 2013), and the dependency-based SRL introduced in CoNLL08/09 shared tasks (Surdeanu Corresponding author. NP NP (2) Introduction ∗ VP (1) et al., 2008; Hajiˇc et al., 2009). By adopting va"
2021.findings-acl.49,W11-2924,0,0.187288,"ency representations for facilitating the span-based SRL (Wang et al., 2019; Marcheggiani and Titov, 2020). Yet almost all the syntax-based SRL methods use one standalone syntactic tree, i.e., either dependency or constituency tree. Constituent and dependency syntax actually depict the syntactic structure from different perspectives, and integrat549 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 549–559 August 1–6, 2021. ©2021 Association for Computational Linguistics ing these two heterogeneous representations can intuitively bring complementary advantages (Farkas et al., 2011; Yoshikawa et al., 2017; Zhou and Zhao, 2019). As exemplified in Figure 1, the dependency edges represent the inter-relations between arguments and predicates, while the constituency structure1 locates more about phrase boundaries of argument spans, and then directs the paths to the predicate globally. Interacting these two structures can better guide the system to focus on the most proper granularity of phrasal spans (as circled by the dotted box), while also ensuring the route consistency between predicate-argument pairs. Unfortunately, we find that there are very limited explorations of th"
2021.findings-acl.49,2020.findings-emnlp.18,1,0.535196,"ile the constituency structure1 locates more about phrase boundaries of argument spans, and then directs the paths to the predicate globally. Interacting these two structures can better guide the system to focus on the most proper granularity of phrasal spans (as circled by the dotted box), while also ensuring the route consistency between predicate-argument pairs. Unfortunately, we find that there are very limited explorations of the heterogeneous syntax integration in SRL. For instance, Li et al. (2010) manually craft two types of discrete syntax features for statistical model, and recently Fei et al. (2020a) implicitly distill two heterogeneous syntactic representations into one unified neural model. In this paper, we present two innovative neural methods for explicitly integrating two kinds of syntactic features for SRL. As shown in Figure 2, in our framework, the syntactic constituent and dependency encoders are built jointly as a unified block (i.e., Heterogeneous Syntax Fuser, namely HeSyFu), and work closely with each other. In the first architecture of HeSyFu (cf. Figure 3), we take two separate TreeLSTMs as the structure encoders for two syntactic trees. Based on our framework, we try to"
2021.findings-acl.49,2020.acl-main.627,1,0.621134,"ile the constituency structure1 locates more about phrase boundaries of argument spans, and then directs the paths to the predicate globally. Interacting these two structures can better guide the system to focus on the most proper granularity of phrasal spans (as circled by the dotted box), while also ensuring the route consistency between predicate-argument pairs. Unfortunately, we find that there are very limited explorations of the heterogeneous syntax integration in SRL. For instance, Li et al. (2010) manually craft two types of discrete syntax features for statistical model, and recently Fei et al. (2020a) implicitly distill two heterogeneous syntactic representations into one unified neural model. In this paper, we present two innovative neural methods for explicitly integrating two kinds of syntactic features for SRL. As shown in Figure 2, in our framework, the syntactic constituent and dependency encoders are built jointly as a unified block (i.e., Heterogeneous Syntax Fuser, namely HeSyFu), and work closely with each other. In the first architecture of HeSyFu (cf. Figure 3), we take two separate TreeLSTMs as the structure encoders for two syntactic trees. Based on our framework, we try to"
2021.findings-acl.49,D15-1112,0,0.0322692,"Missing"
2021.findings-acl.49,2020.findings-emnlp.102,0,0.0386554,"Missing"
2021.findings-acl.49,P00-1065,0,0.348756,"2. the improvement for span-based SRL is more obvious than dependency-based one; IA3. GCN performs better than TreeLSTM; IA4. syntactic labels are quite helpful for SRL; IA5. SRL and both kinds of syntactic structures have strong associations and should be exploited for mutual benefits. In our experiments, our SRL framework with two proposed HeSyFu encoders achieves better results than current best-performing systems, and yield more explainable task improvements. 2 Related Work The SRL task, uncovering the shallow semantic structure (i.e. ‘who did what to whom where and when’) is pioneered by Gildea and Jurafsky (2000), and popularized from PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998). SRL is typically divided into the span-based one and dependency-based one on the basis of the granularity of arguments (e.g., phrasal spans or dependency heads). Earlier efforts focus on designing hand-crafted features with machine learning methods (Pradhan et al., 2005; Punyakanok et al., 2008; Zhao et al., 2009b,a). Later, SRL works mostly employ neural networks with distributed features for the task improvements (FitzGerald et al., 2015; Roth and Lapata, 2016; Marcheggiani and Titov, 2017; Strubell et a"
2021.findings-acl.49,2021.ccl-1.108,0,0.0561943,"Missing"
2021.findings-acl.49,P17-1044,0,0.368695,"014; Xu et al., 2020). Based on whether to recognize the constituent phrasal span or the syntactic dependency head token of an argument, prior works categorize SRL into two types: the span-based SRL popularized in CoNLL05/12 shared tasks (Carreras and M`arquez, 2005; Pradhan et al., 2013), and the dependency-based SRL introduced in CoNLL08/09 shared tasks (Surdeanu Corresponding author. NP NP (2) Introduction ∗ VP (1) et al., 2008; Hajiˇc et al., 2009). By adopting various neural network methods, two types of SRL have achieved significant performances in recent years (FitzGerald et al., 2015; He et al., 2017; Fei et al., 2021a) Syntactic features have been extensively verified to be highly effective for SRL (Pradhan et al., 2005; Punyakanok et al., 2008; Marcheggiani and Titov, 2017; Strubell et al., 2018; Zhang et al., 2019). In particular, syntactic dependency features have gained a majority of attention, especially for the dependency-based SRL, considering their close relevance with the dependency structure (Roth and Lapata, 2016; He et al., 2018; Xia et al., 2019; Fei et al., 2021b). Most existing works focus on designing various methods for modeling the dependency representations into the SR"
2021.findings-acl.49,D19-1538,0,0.0247447,"Missing"
2021.findings-acl.49,P18-1192,0,0.0664259,"2009). By adopting various neural network methods, two types of SRL have achieved significant performances in recent years (FitzGerald et al., 2015; He et al., 2017; Fei et al., 2021a) Syntactic features have been extensively verified to be highly effective for SRL (Pradhan et al., 2005; Punyakanok et al., 2008; Marcheggiani and Titov, 2017; Strubell et al., 2018; Zhang et al., 2019). In particular, syntactic dependency features have gained a majority of attention, especially for the dependency-based SRL, considering their close relevance with the dependency structure (Roth and Lapata, 2016; He et al., 2018; Xia et al., 2019; Fei et al., 2021b). Most existing works focus on designing various methods for modeling the dependency representations into the SRL learning, such as TreeLSTM (Li et al., 2018; Xia et al., 2019) and graph convolutional networks (GCN) (Marcheggiani and Titov, 2017; Li et al., 2018). On the other hand, some efforts try to encode the constituency representations for facilitating the span-based SRL (Wang et al., 2019; Marcheggiani and Titov, 2020). Yet almost all the syntax-based SRL methods use one standalone syntactic tree, i.e., either dependency or constituency tree. Consti"
2021.findings-acl.49,N19-1075,0,0.170163,"roblem with BIO tagging scheme for both two types of SRL (He et al., 2017; Ouchi et al., 2018; Fei et al., 2020c,b). On the other hand, syntactic features are a highly effective SRL performance enhancer, according to numbers of empirical verification in prior works (Marcheggiani et al., 2017; He et al., 2018; Swayamdipta et al., 2018; Zhang et al., 2019), as intuitively SRL shares much underlying structure with syntax. Basically, the syntactic dependent feature is more preferred to be injected into the dependency-based SRL (Roth and Lapata, 2016; Marcheggiani and Titov, 2017; He et al., 2018; Kasai et al., 2019), while other consider the constituent syntax for the span-based SRL (Wang 550 et al., 2019; Marcheggiani and Titov, 2020). Actually, the constituent and dependency syntax depict the structural features from different angles, while they can share close linguistic relevance. Related works have revealed the mutual benefits on integrating these two heterogeneous syntactic representations for various NLP tasks (Collins, 1997; Charniak, 2000; Charniak and Johnson, 2005; Farkas et al., 2011; Yoshikawa et al., 2017; Zhou and Zhao, 2019; Strzyz et al., 2019; Kato and Matsubara, 2019). Unfortunately, t"
2021.findings-acl.49,P19-1530,0,0.0147914,"2017; He et al., 2018; Kasai et al., 2019), while other consider the constituent syntax for the span-based SRL (Wang 550 et al., 2019; Marcheggiani and Titov, 2020). Actually, the constituent and dependency syntax depict the structural features from different angles, while they can share close linguistic relevance. Related works have revealed the mutual benefits on integrating these two heterogeneous syntactic representations for various NLP tasks (Collins, 1997; Charniak, 2000; Charniak and Johnson, 2005; Farkas et al., 2011; Yoshikawa et al., 2017; Zhou and Zhao, 2019; Strzyz et al., 2019; Kato and Matsubara, 2019). Unfortunately, there are very limited explorations for SRL. For example, Li et al. (2010) construct discrete heterogeneous syntactic features for SRL. More recent work in Fei et al. (2020a) leverage knowledge distillation method to inject the heterogeneous syntax representations from various tree encoders into one model for enhancing the span-based SRL. In this work, we consider an explicit integration of these two syntactic structures via two neural solutions. To our knowledge, we are the first attempt performing thorough investigations on the impacts of the heterogeneous syntax combination"
2021.findings-acl.49,C10-2076,0,0.0985694,"ified in Figure 1, the dependency edges represent the inter-relations between arguments and predicates, while the constituency structure1 locates more about phrase boundaries of argument spans, and then directs the paths to the predicate globally. Interacting these two structures can better guide the system to focus on the most proper granularity of phrasal spans (as circled by the dotted box), while also ensuring the route consistency between predicate-argument pairs. Unfortunately, we find that there are very limited explorations of the heterogeneous syntax integration in SRL. For instance, Li et al. (2010) manually craft two types of discrete syntax features for statistical model, and recently Fei et al. (2020a) implicitly distill two heterogeneous syntactic representations into one unified neural model. In this paper, we present two innovative neural methods for explicitly integrating two kinds of syntactic features for SRL. As shown in Figure 2, in our framework, the syntactic constituent and dependency encoders are built jointly as a unified block (i.e., Heterogeneous Syntax Fuser, namely HeSyFu), and work closely with each other. In the first architecture of HeSyFu (cf. Figure 3), we take t"
2021.findings-acl.49,2020.acl-main.744,0,0.0211576,"Missing"
2021.findings-acl.49,C14-2018,0,0.0736986,"Missing"
2021.findings-acl.49,K17-1041,0,0.0155973,"et al., 2005; Punyakanok et al., 2008; Zhao et al., 2009b,a). Later, SRL works mostly employ neural networks with distributed features for the task improvements (FitzGerald et al., 2015; Roth and Lapata, 2016; Marcheggiani and Titov, 2017; Strubell et al., 2018). Most high-performing systems model the task as a sequence labeling problem with BIO tagging scheme for both two types of SRL (He et al., 2017; Ouchi et al., 2018; Fei et al., 2020c,b). On the other hand, syntactic features are a highly effective SRL performance enhancer, according to numbers of empirical verification in prior works (Marcheggiani et al., 2017; He et al., 2018; Swayamdipta et al., 2018; Zhang et al., 2019), as intuitively SRL shares much underlying structure with syntax. Basically, the syntactic dependent feature is more preferred to be injected into the dependency-based SRL (Roth and Lapata, 2016; Marcheggiani and Titov, 2017; He et al., 2018; Kasai et al., 2019), while other consider the constituent syntax for the span-based SRL (Wang 550 et al., 2019; Marcheggiani and Titov, 2020). Actually, the constituent and dependency syntax depict the structural features from different angles, while they can share close linguistic relevance"
2021.findings-acl.49,D17-1159,0,0.136058,"nto two types: the span-based SRL popularized in CoNLL05/12 shared tasks (Carreras and M`arquez, 2005; Pradhan et al., 2013), and the dependency-based SRL introduced in CoNLL08/09 shared tasks (Surdeanu Corresponding author. NP NP (2) Introduction ∗ VP (1) et al., 2008; Hajiˇc et al., 2009). By adopting various neural network methods, two types of SRL have achieved significant performances in recent years (FitzGerald et al., 2015; He et al., 2017; Fei et al., 2021a) Syntactic features have been extensively verified to be highly effective for SRL (Pradhan et al., 2005; Punyakanok et al., 2008; Marcheggiani and Titov, 2017; Strubell et al., 2018; Zhang et al., 2019). In particular, syntactic dependency features have gained a majority of attention, especially for the dependency-based SRL, considering their close relevance with the dependency structure (Roth and Lapata, 2016; He et al., 2018; Xia et al., 2019; Fei et al., 2021b). Most existing works focus on designing various methods for modeling the dependency representations into the SRL learning, such as TreeLSTM (Li et al., 2018; Xia et al., 2019) and graph convolutional networks (GCN) (Marcheggiani and Titov, 2017; Li et al., 2018). On the other hand, some e"
2021.findings-acl.49,2020.emnlp-main.322,0,0.199634,"ity of attention, especially for the dependency-based SRL, considering their close relevance with the dependency structure (Roth and Lapata, 2016; He et al., 2018; Xia et al., 2019; Fei et al., 2021b). Most existing works focus on designing various methods for modeling the dependency representations into the SRL learning, such as TreeLSTM (Li et al., 2018; Xia et al., 2019) and graph convolutional networks (GCN) (Marcheggiani and Titov, 2017; Li et al., 2018). On the other hand, some efforts try to encode the constituency representations for facilitating the span-based SRL (Wang et al., 2019; Marcheggiani and Titov, 2020). Yet almost all the syntax-based SRL methods use one standalone syntactic tree, i.e., either dependency or constituency tree. Constituent and dependency syntax actually depict the syntactic structure from different perspectives, and integrat549 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 549–559 August 1–6, 2021. ©2021 Association for Computational Linguistics ing these two heterogeneous representations can intuitively bring complementary advantages (Farkas et al., 2011; Yoshikawa et al., 2017; Zhou and Zhao, 2019). As exemplified in Figure 1, the depende"
2021.findings-acl.49,D18-1191,0,0.0156342,"ased one on the basis of the granularity of arguments (e.g., phrasal spans or dependency heads). Earlier efforts focus on designing hand-crafted features with machine learning methods (Pradhan et al., 2005; Punyakanok et al., 2008; Zhao et al., 2009b,a). Later, SRL works mostly employ neural networks with distributed features for the task improvements (FitzGerald et al., 2015; Roth and Lapata, 2016; Marcheggiani and Titov, 2017; Strubell et al., 2018). Most high-performing systems model the task as a sequence labeling problem with BIO tagging scheme for both two types of SRL (He et al., 2017; Ouchi et al., 2018; Fei et al., 2020c,b). On the other hand, syntactic features are a highly effective SRL performance enhancer, according to numbers of empirical verification in prior works (Marcheggiani et al., 2017; He et al., 2018; Swayamdipta et al., 2018; Zhang et al., 2019), as intuitively SRL shares much underlying structure with syntax. Basically, the syntactic dependent feature is more preferred to be injected into the dependency-based SRL (Roth and Lapata, 2016; Marcheggiani and Titov, 2017; He et al., 2018; Kasai et al., 2019), while other consider the constituent syntax for the span-based SRL (Wang"
2021.findings-acl.49,J05-1004,0,0.371031,"dependency-based one; IA3. GCN performs better than TreeLSTM; IA4. syntactic labels are quite helpful for SRL; IA5. SRL and both kinds of syntactic structures have strong associations and should be exploited for mutual benefits. In our experiments, our SRL framework with two proposed HeSyFu encoders achieves better results than current best-performing systems, and yield more explainable task improvements. 2 Related Work The SRL task, uncovering the shallow semantic structure (i.e. ‘who did what to whom where and when’) is pioneered by Gildea and Jurafsky (2000), and popularized from PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998). SRL is typically divided into the span-based one and dependency-based one on the basis of the granularity of arguments (e.g., phrasal spans or dependency heads). Earlier efforts focus on designing hand-crafted features with machine learning methods (Pradhan et al., 2005; Punyakanok et al., 2008; Zhao et al., 2009b,a). Later, SRL works mostly employ neural networks with distributed features for the task improvements (FitzGerald et al., 2015; Roth and Lapata, 2016; Marcheggiani and Titov, 2017; Strubell et al., 2018). Most high-performing systems model the tas"
2021.findings-acl.49,P05-1072,0,0.202366,"en of an argument, prior works categorize SRL into two types: the span-based SRL popularized in CoNLL05/12 shared tasks (Carreras and M`arquez, 2005; Pradhan et al., 2013), and the dependency-based SRL introduced in CoNLL08/09 shared tasks (Surdeanu Corresponding author. NP NP (2) Introduction ∗ VP (1) et al., 2008; Hajiˇc et al., 2009). By adopting various neural network methods, two types of SRL have achieved significant performances in recent years (FitzGerald et al., 2015; He et al., 2017; Fei et al., 2021a) Syntactic features have been extensively verified to be highly effective for SRL (Pradhan et al., 2005; Punyakanok et al., 2008; Marcheggiani and Titov, 2017; Strubell et al., 2018; Zhang et al., 2019). In particular, syntactic dependency features have gained a majority of attention, especially for the dependency-based SRL, considering their close relevance with the dependency structure (Roth and Lapata, 2016; He et al., 2018; Xia et al., 2019; Fei et al., 2021b). Most existing works focus on designing various methods for modeling the dependency representations into the SRL learning, such as TreeLSTM (Li et al., 2018; Xia et al., 2019) and graph convolutional networks (GCN) (Marcheggiani and T"
2021.findings-acl.49,J08-2005,0,0.381296,"or works categorize SRL into two types: the span-based SRL popularized in CoNLL05/12 shared tasks (Carreras and M`arquez, 2005; Pradhan et al., 2013), and the dependency-based SRL introduced in CoNLL08/09 shared tasks (Surdeanu Corresponding author. NP NP (2) Introduction ∗ VP (1) et al., 2008; Hajiˇc et al., 2009). By adopting various neural network methods, two types of SRL have achieved significant performances in recent years (FitzGerald et al., 2015; He et al., 2017; Fei et al., 2021a) Syntactic features have been extensively verified to be highly effective for SRL (Pradhan et al., 2005; Punyakanok et al., 2008; Marcheggiani and Titov, 2017; Strubell et al., 2018; Zhang et al., 2019). In particular, syntactic dependency features have gained a majority of attention, especially for the dependency-based SRL, considering their close relevance with the dependency structure (Roth and Lapata, 2016; He et al., 2018; Xia et al., 2019; Fei et al., 2021b). Most existing works focus on designing various methods for modeling the dependency representations into the SRL learning, such as TreeLSTM (Li et al., 2018; Xia et al., 2019) and graph convolutional networks (GCN) (Marcheggiani and Titov, 2017; Li et al., 20"
2021.findings-acl.49,P16-1113,0,0.122788,"., 2008; Hajiˇc et al., 2009). By adopting various neural network methods, two types of SRL have achieved significant performances in recent years (FitzGerald et al., 2015; He et al., 2017; Fei et al., 2021a) Syntactic features have been extensively verified to be highly effective for SRL (Pradhan et al., 2005; Punyakanok et al., 2008; Marcheggiani and Titov, 2017; Strubell et al., 2018; Zhang et al., 2019). In particular, syntactic dependency features have gained a majority of attention, especially for the dependency-based SRL, considering their close relevance with the dependency structure (Roth and Lapata, 2016; He et al., 2018; Xia et al., 2019; Fei et al., 2021b). Most existing works focus on designing various methods for modeling the dependency representations into the SRL learning, such as TreeLSTM (Li et al., 2018; Xia et al., 2019) and graph convolutional networks (GCN) (Marcheggiani and Titov, 2017; Li et al., 2018). On the other hand, some efforts try to encode the constituency representations for facilitating the span-based SRL (Wang et al., 2019; Marcheggiani and Titov, 2020). Yet almost all the syntax-based SRL methods use one standalone syntactic tree, i.e., either dependency or constitu"
2021.findings-acl.49,D18-1262,0,0.198701,"features have been extensively verified to be highly effective for SRL (Pradhan et al., 2005; Punyakanok et al., 2008; Marcheggiani and Titov, 2017; Strubell et al., 2018; Zhang et al., 2019). In particular, syntactic dependency features have gained a majority of attention, especially for the dependency-based SRL, considering their close relevance with the dependency structure (Roth and Lapata, 2016; He et al., 2018; Xia et al., 2019; Fei et al., 2021b). Most existing works focus on designing various methods for modeling the dependency representations into the SRL learning, such as TreeLSTM (Li et al., 2018; Xia et al., 2019) and graph convolutional networks (GCN) (Marcheggiani and Titov, 2017; Li et al., 2018). On the other hand, some efforts try to encode the constituency representations for facilitating the span-based SRL (Wang et al., 2019; Marcheggiani and Titov, 2020). Yet almost all the syntax-based SRL methods use one standalone syntactic tree, i.e., either dependency or constituency tree. Constituent and dependency syntax actually depict the syntactic structure from different perspectives, and integrat549 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages"
2021.findings-acl.49,P16-1212,0,0.0696299,"Missing"
2021.findings-acl.49,D18-1548,0,0.250314,"SRL popularized in CoNLL05/12 shared tasks (Carreras and M`arquez, 2005; Pradhan et al., 2013), and the dependency-based SRL introduced in CoNLL08/09 shared tasks (Surdeanu Corresponding author. NP NP (2) Introduction ∗ VP (1) et al., 2008; Hajiˇc et al., 2009). By adopting various neural network methods, two types of SRL have achieved significant performances in recent years (FitzGerald et al., 2015; He et al., 2017; Fei et al., 2021a) Syntactic features have been extensively verified to be highly effective for SRL (Pradhan et al., 2005; Punyakanok et al., 2008; Marcheggiani and Titov, 2017; Strubell et al., 2018; Zhang et al., 2019). In particular, syntactic dependency features have gained a majority of attention, especially for the dependency-based SRL, considering their close relevance with the dependency structure (Roth and Lapata, 2016; He et al., 2018; Xia et al., 2019; Fei et al., 2021b). Most existing works focus on designing various methods for modeling the dependency representations into the SRL learning, such as TreeLSTM (Li et al., 2018; Xia et al., 2019) and graph convolutional networks (GCN) (Marcheggiani and Titov, 2017; Li et al., 2018). On the other hand, some efforts try to encode th"
2021.findings-acl.49,P19-1531,0,0.0578958,"Missing"
2021.findings-acl.49,D18-1412,0,0.0181416,"et al., 2009b,a). Later, SRL works mostly employ neural networks with distributed features for the task improvements (FitzGerald et al., 2015; Roth and Lapata, 2016; Marcheggiani and Titov, 2017; Strubell et al., 2018). Most high-performing systems model the task as a sequence labeling problem with BIO tagging scheme for both two types of SRL (He et al., 2017; Ouchi et al., 2018; Fei et al., 2020c,b). On the other hand, syntactic features are a highly effective SRL performance enhancer, according to numbers of empirical verification in prior works (Marcheggiani et al., 2017; He et al., 2018; Swayamdipta et al., 2018; Zhang et al., 2019), as intuitively SRL shares much underlying structure with syntax. Basically, the syntactic dependent feature is more preferred to be injected into the dependency-based SRL (Roth and Lapata, 2016; Marcheggiani and Titov, 2017; He et al., 2018; Kasai et al., 2019), while other consider the constituent syntax for the span-based SRL (Wang 550 et al., 2019; Marcheggiani and Titov, 2020). Actually, the constituent and dependency syntax depict the structural features from different angles, while they can share close linguistic relevance. Related works have revealed the mutual be"
2021.findings-acl.49,P15-1150,0,0.15042,"Missing"
2021.findings-acl.49,P19-1529,0,0.0161468,"have gained a majority of attention, especially for the dependency-based SRL, considering their close relevance with the dependency structure (Roth and Lapata, 2016; He et al., 2018; Xia et al., 2019; Fei et al., 2021b). Most existing works focus on designing various methods for modeling the dependency representations into the SRL learning, such as TreeLSTM (Li et al., 2018; Xia et al., 2019) and graph convolutional networks (GCN) (Marcheggiani and Titov, 2017; Li et al., 2018). On the other hand, some efforts try to encode the constituency representations for facilitating the span-based SRL (Wang et al., 2019; Marcheggiani and Titov, 2020). Yet almost all the syntax-based SRL methods use one standalone syntactic tree, i.e., either dependency or constituency tree. Constituent and dependency syntax actually depict the syntactic structure from different perspectives, and integrat549 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 549–559 August 1–6, 2021. ©2021 Association for Computational Linguistics ing these two heterogeneous representations can intuitively bring complementary advantages (Farkas et al., 2011; Yoshikawa et al., 2017; Zhou and Zhao, 2019). As exemp"
2021.findings-acl.49,2020.coling-main.266,0,0.0396221,"Missing"
2021.findings-acl.49,P12-1095,0,0.0301197,"rovements. 1 (3) (4) PP A0 V A1 AM-LOC She met her sister in the pub nsubj root ROOT det nmod case obj obl Figure 1: The mutual benefit to integrate both the (1) syntactic constituency and (4) dependency structures for (2) SRL, based on (3) an example sentence. Semantic role labeling (SRL) aims to disclose the predicate-argument structure of a given sentence. Such shallow semantic structures have been shown highly useful for a wide range of downstream tasks in natural language processing (NLP), such as information extraction (Fader et al., 2011; Bastianelli et al., 2013), machine translation (Xiong et al., 2012; Shi et al., 2016) and question answering (Maqsud et al., 2014; Xu et al., 2020). Based on whether to recognize the constituent phrasal span or the syntactic dependency head token of an argument, prior works categorize SRL into two types: the span-based SRL popularized in CoNLL05/12 shared tasks (Carreras and M`arquez, 2005; Pradhan et al., 2013), and the dependency-based SRL introduced in CoNLL08/09 shared tasks (Surdeanu Corresponding author. NP NP (2) Introduction ∗ VP (1) et al., 2008; Hajiˇc et al., 2009). By adopting various neural network methods, two types of SRL have achieved signifi"
2021.findings-acl.49,2020.emnlp-main.537,0,0.0186178,"T det nmod case obj obl Figure 1: The mutual benefit to integrate both the (1) syntactic constituency and (4) dependency structures for (2) SRL, based on (3) an example sentence. Semantic role labeling (SRL) aims to disclose the predicate-argument structure of a given sentence. Such shallow semantic structures have been shown highly useful for a wide range of downstream tasks in natural language processing (NLP), such as information extraction (Fader et al., 2011; Bastianelli et al., 2013), machine translation (Xiong et al., 2012; Shi et al., 2016) and question answering (Maqsud et al., 2014; Xu et al., 2020). Based on whether to recognize the constituent phrasal span or the syntactic dependency head token of an argument, prior works categorize SRL into two types: the span-based SRL popularized in CoNLL05/12 shared tasks (Carreras and M`arquez, 2005; Pradhan et al., 2013), and the dependency-based SRL introduced in CoNLL08/09 shared tasks (Surdeanu Corresponding author. NP NP (2) Introduction ∗ VP (1) et al., 2008; Hajiˇc et al., 2009). By adopting various neural network methods, two types of SRL have achieved significant performances in recent years (FitzGerald et al., 2015; He et al., 2017; Fei"
2021.findings-acl.49,P17-1026,0,0.0482143,"for facilitating the span-based SRL (Wang et al., 2019; Marcheggiani and Titov, 2020). Yet almost all the syntax-based SRL methods use one standalone syntactic tree, i.e., either dependency or constituency tree. Constituent and dependency syntax actually depict the syntactic structure from different perspectives, and integrat549 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 549–559 August 1–6, 2021. ©2021 Association for Computational Linguistics ing these two heterogeneous representations can intuitively bring complementary advantages (Farkas et al., 2011; Yoshikawa et al., 2017; Zhou and Zhao, 2019). As exemplified in Figure 1, the dependency edges represent the inter-relations between arguments and predicates, while the constituency structure1 locates more about phrase boundaries of argument spans, and then directs the paths to the predicate globally. Interacting these two structures can better guide the system to focus on the most proper granularity of phrasal spans (as circled by the dotted box), while also ensuring the route consistency between predicate-argument pairs. Unfortunately, we find that there are very limited explorations of the heterogeneous syntax i"
2021.findings-acl.49,D19-1057,0,0.213142,"L05/12 shared tasks (Carreras and M`arquez, 2005; Pradhan et al., 2013), and the dependency-based SRL introduced in CoNLL08/09 shared tasks (Surdeanu Corresponding author. NP NP (2) Introduction ∗ VP (1) et al., 2008; Hajiˇc et al., 2009). By adopting various neural network methods, two types of SRL have achieved significant performances in recent years (FitzGerald et al., 2015; He et al., 2017; Fei et al., 2021a) Syntactic features have been extensively verified to be highly effective for SRL (Pradhan et al., 2005; Punyakanok et al., 2008; Marcheggiani and Titov, 2017; Strubell et al., 2018; Zhang et al., 2019). In particular, syntactic dependency features have gained a majority of attention, especially for the dependency-based SRL, considering their close relevance with the dependency structure (Roth and Lapata, 2016; He et al., 2018; Xia et al., 2019; Fei et al., 2021b). Most existing works focus on designing various methods for modeling the dependency representations into the SRL learning, such as TreeLSTM (Li et al., 2018; Xia et al., 2019) and graph convolutional networks (GCN) (Marcheggiani and Titov, 2017; Li et al., 2018). On the other hand, some efforts try to encode the constituency repres"
2021.findings-acl.49,W09-1209,0,0.0580167,"systems, and yield more explainable task improvements. 2 Related Work The SRL task, uncovering the shallow semantic structure (i.e. ‘who did what to whom where and when’) is pioneered by Gildea and Jurafsky (2000), and popularized from PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998). SRL is typically divided into the span-based one and dependency-based one on the basis of the granularity of arguments (e.g., phrasal spans or dependency heads). Earlier efforts focus on designing hand-crafted features with machine learning methods (Pradhan et al., 2005; Punyakanok et al., 2008; Zhao et al., 2009b,a). Later, SRL works mostly employ neural networks with distributed features for the task improvements (FitzGerald et al., 2015; Roth and Lapata, 2016; Marcheggiani and Titov, 2017; Strubell et al., 2018). Most high-performing systems model the task as a sequence labeling problem with BIO tagging scheme for both two types of SRL (He et al., 2017; Ouchi et al., 2018; Fei et al., 2020c,b). On the other hand, syntactic features are a highly effective SRL performance enhancer, according to numbers of empirical verification in prior works (Marcheggiani et al., 2017; He et al., 2018; Swayamdipta e"
2021.findings-acl.49,D09-1004,0,0.0407938,"systems, and yield more explainable task improvements. 2 Related Work The SRL task, uncovering the shallow semantic structure (i.e. ‘who did what to whom where and when’) is pioneered by Gildea and Jurafsky (2000), and popularized from PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998). SRL is typically divided into the span-based one and dependency-based one on the basis of the granularity of arguments (e.g., phrasal spans or dependency heads). Earlier efforts focus on designing hand-crafted features with machine learning methods (Pradhan et al., 2005; Punyakanok et al., 2008; Zhao et al., 2009b,a). Later, SRL works mostly employ neural networks with distributed features for the task improvements (FitzGerald et al., 2015; Roth and Lapata, 2016; Marcheggiani and Titov, 2017; Strubell et al., 2018). Most high-performing systems model the task as a sequence labeling problem with BIO tagging scheme for both two types of SRL (He et al., 2017; Ouchi et al., 2018; Fei et al., 2020c,b). On the other hand, syntactic features are a highly effective SRL performance enhancer, according to numbers of empirical verification in prior works (Marcheggiani et al., 2017; He et al., 2018; Swayamdipta e"
2021.findings-acl.49,P19-1230,0,0.100817,"n-based SRL (Wang et al., 2019; Marcheggiani and Titov, 2020). Yet almost all the syntax-based SRL methods use one standalone syntactic tree, i.e., either dependency or constituency tree. Constituent and dependency syntax actually depict the syntactic structure from different perspectives, and integrat549 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 549–559 August 1–6, 2021. ©2021 Association for Computational Linguistics ing these two heterogeneous representations can intuitively bring complementary advantages (Farkas et al., 2011; Yoshikawa et al., 2017; Zhou and Zhao, 2019). As exemplified in Figure 1, the dependency edges represent the inter-relations between arguments and predicates, while the constituency structure1 locates more about phrase boundaries of argument spans, and then directs the paths to the predicate globally. Interacting these two structures can better guide the system to focus on the most proper granularity of phrasal spans (as circled by the dotted box), while also ensuring the route consistency between predicate-argument pairs. Unfortunately, we find that there are very limited explorations of the heterogeneous syntax integration in SRL. For"
C12-1075,W04-3247,0,0.0858619,"ng techniques. In unsupervised methods, feature-based ranking methods are usually based on a combination of linguistic and statistical features such as term frequency, sentence position, cue words, stigma words, lexical chains, rhetorical structure, topic signatures (Luhn, 1969; Lin and Hovy, 2000), etc. Clustering-based methods usually select one or more representative sentences from each subtopic to produce a summary with minimized redundancy and maximized coverage (Nomoto and Matsumoto, 2001). Graph-based methods have been shown to work well and are becoming more and more popular. LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) are representative methods adopting models like PageRank and HITS to estimate the importance of sentences via the computation of the stationary distribution of a Markov chain or a mutual reinforcement process (Zha, 2002). For supervised methods, summarization is often regarded as a classification task or a sequence labeling task at sentence level, and many supervised learning algorithms have been investigated including Hidden Markov Models (Conroy and O'leary, 2001), Support Vector Regression (You et al., 2011), Factor Graph Model (Yang et al., 2011), e"
C12-1075,I11-1054,1,0.850648,"luding Hidden Markov Models (Conroy and O'leary, 2001), Support Vector Regression (You et al., 2011), Factor Graph Model (Yang et al., 2011), etc. However, such a supervised learning paradigm often requires a large amount of labeled data, which are not available in most cases. With the rapid growth of online information, some work has began to employ context to aid summarization, such as contents from external documents (Wan and Yang, 2007) or cited papers (Mei and Zhai, 2008; Qazvinian and Radev, 2010), click-through data or search logs (Sun et al., 2005), and social tags (Qu and Chen, 2009; Hu et al., 2011), comments (Hu et al., 2008) or discussing tweets (Yang et al., 2011), etc. However, such methods so far are usually designed for generic summarization and do not take into account the impact of users’ interests on summary generation. Besides, in the existing studies, personalized summarization is often conducted with the help of a query (Sun, 2008; You et al., 2011) or a static user profile (Díaz and Gervás, 2007), and most studies only use the local content from target document(s) or the user profile, with little attention paid to the rich social contextual information affiliated with them."
C12-1075,C00-1072,0,0.199611,"Missing"
C12-1075,N03-1020,0,0.236445,"Missing"
C12-1075,P08-1093,0,0.0202719,"as a classification task or a sequence labeling task at sentence level, and many supervised learning algorithms have been investigated including Hidden Markov Models (Conroy and O'leary, 2001), Support Vector Regression (You et al., 2011), Factor Graph Model (Yang et al., 2011), etc. However, such a supervised learning paradigm often requires a large amount of labeled data, which are not available in most cases. With the rapid growth of online information, some work has began to employ context to aid summarization, such as contents from external documents (Wan and Yang, 2007) or cited papers (Mei and Zhai, 2008; Qazvinian and Radev, 2010), click-through data or search logs (Sun et al., 2005), and social tags (Qu and Chen, 2009; Hu et al., 2011), comments (Hu et al., 2008) or discussing tweets (Yang et al., 2011), etc. However, such methods so far are usually designed for generic summarization and do not take into account the impact of users’ interests on summary generation. Besides, in the existing studies, personalized summarization is often conducted with the help of a query (Sun, 2008; You et al., 2011) or a static user profile (Díaz and Gervás, 2007), and most studies only use the local content"
C12-1075,P10-1057,0,0.0226546,"task or a sequence labeling task at sentence level, and many supervised learning algorithms have been investigated including Hidden Markov Models (Conroy and O'leary, 2001), Support Vector Regression (You et al., 2011), Factor Graph Model (Yang et al., 2011), etc. However, such a supervised learning paradigm often requires a large amount of labeled data, which are not available in most cases. With the rapid growth of online information, some work has began to employ context to aid summarization, such as contents from external documents (Wan and Yang, 2007) or cited papers (Mei and Zhai, 2008; Qazvinian and Radev, 2010), click-through data or search logs (Sun et al., 2005), and social tags (Qu and Chen, 2009; Hu et al., 2011), comments (Hu et al., 2008) or discussing tweets (Yang et al., 2011), etc. However, such methods so far are usually designed for generic summarization and do not take into account the impact of users’ interests on summary generation. Besides, in the existing studies, personalized summarization is often conducted with the help of a query (Sun, 2008; You et al., 2011) or a static user profile (Díaz and Gervás, 2007), and most studies only use the local content from target document(s) or t"
C12-1075,Y09-2005,0,0.0185252,"en investigated including Hidden Markov Models (Conroy and O'leary, 2001), Support Vector Regression (You et al., 2011), Factor Graph Model (Yang et al., 2011), etc. However, such a supervised learning paradigm often requires a large amount of labeled data, which are not available in most cases. With the rapid growth of online information, some work has began to employ context to aid summarization, such as contents from external documents (Wan and Yang, 2007) or cited papers (Mei and Zhai, 2008; Qazvinian and Radev, 2010), click-through data or search logs (Sun et al., 2005), and social tags (Qu and Chen, 2009; Hu et al., 2011), comments (Hu et al., 2008) or discussing tweets (Yang et al., 2011), etc. However, such methods so far are usually designed for generic summarization and do not take into account the impact of users’ interests on summary generation. Besides, in the existing studies, personalized summarization is often conducted with the help of a query (Sun, 2008; You et al., 2011) or a static user profile (Díaz and Gervás, 2007), and most studies only use the local content from target document(s) or the user profile, with little attention paid to the rich social contextual information affi"
C12-1075,D11-1124,0,0.056335,"er passive consumers of web contents. They can create contents and add metadata. Similarly, web documents no longer exist on their own and they are naturally associated with other documents and diverse users. All these information can be considered as the potential data source for document understanding and personalization. For generating a personalized summary, traditional methods usually require that a user explicitly provides his interest aspects, such as specifying the categories he prefers (Díaz and Gervás, 2007) or clicking a subset of sentences in a document according to his interests (Yan et al., 2011). However, most users are reluctant to provide such information, thus it is more meaningful to infer a user’s interests implicitly. To address these concerns, we present an unsupervised approach for personalized summarization. The underlying assumption is that it is beneficial to understand both a single document and a single user better if appropriate social context can be leveraged under some constraints. In this work, the expanded social context used to infer users' interests and enrich document’s content is highly selective, which comes from the most similar users and documents. We explore"
C14-1152,S07-1002,0,0.023677,"007) uses hypergraph model for WSI, in which co-occurrences of two or more words are represented by using weighted hyperedges. This model fully exploits the existence of collocations or terms consisting of more than two words. In fact, the method converts the sense induction problem to the clustering of the contextual words, and the result relies on local word co-occurrence frequency. Our hypergraph model is constructed from a global perspective, where the whole context instance is regarded as a node. WSI evaluation also is an important issue in WSI tasks. Previous WSI evaluations in SemEval (Agirre and Soroa, 2007; Manandhar et al., 2010) have approached sense induction in terms of finding the single most salient sense of a target word given its context. However, as shown in Erk and McCarthy (2009), multiple senses of the target word may be perceived by readers from different angles and a graded notion of sense labeling may be considered as the most appropriate. The SemEval-2013 WSI evaluation is designed to explore the possibility of finding all perceived senses of a target word in a single context instance. Our model is evaluated and verified on the SemEval-2013 WSI task. 1602 Algorithm 1. lexical ch"
C14-1152,W97-0703,0,0.344048,"Missing"
C14-1152,S13-2050,0,0.0190586,"uch as co-occurrence of words within phrases (Pantel and Lin, 2002; Dorow and Widdows, 2003), parts of speeches (Purandare and Pedersen, 2004), and grammatical relations (Pantel and Lin, 2002; Dorow and Widdows, 2003). The size of the context window also varies, such as two words before and after the target word, the sentence or even larger paragraph within which contains the target word. Most of the work in WSI is the vector space model, such as context-based vector algorithm (Schütze, 1998; Ide et al., 2001; Van de Cruys et al., 2011), substitute-based vector algorithm (Yatbaz et al., 2012; Baskaya et al., 2013). In this model, the context of each instance of a target word is represented as a vector of features based on frequency statistics or probability distributions (e.g., firstorder or second-order vector). These vectors are clustered by various algorithms and the resulting clusters represent the induced senses. Another family of employed approach is graph-based methods, which have been successfully applied in the sense induction task with some better results achieved. In this framework words are represented as nodes in the graph and vertices are drawn between the target word and its cooccurrence"
C14-1152,E09-1013,0,0.0222013,"oss of sense information since this setting does not require any sense mapping procedure to convert induced senses to WordNet senses. Table 2 shows the performance of our systems, benchmarks and baselines. It shows that the NMImeasure is biased towards the one sense per instance baseline and the FBC-measure one sense baseline. However, systems are capable of performing well in both the Fuzzy NMI and Fuzzy B-Cubed measures, thereby avoiding the extreme performance of either baseline. Generally, the performance of our model gets balanced scores. 4.5 Discussion Topic models, such as LDA and HDP (Brody and Lapata, 2009; Lau et al., 2012), have been successfully adopted for WSI, in which one topic is viewed as one sense. Our work is motivated by lexical chain that represents the intrinsic semantic relatedness among context instances on the viewpoint of linguistics. Topic model is used to find lexical chains which are interpreted as topics. We have compared the Unimelb (Lau et al., 2013), a HDP topic model, with our model in the experiments. Additionally, we also follow Lau et al. (2012) to train a LDA model with a fixed number of topics based on our training data for WSI3. Table 3 shows the supervised result"
C14-1152,E06-1018,0,0.0247542,"ers represent the induced senses. Another family of employed approach is graph-based methods (Widdows and Dorow, 2002; Véronis, 2004; Agirre et al., 2006; Klapaftis and Manandhar, 2007; Di Marco and Navigli, 2011; Hope and Keller, 2013), which have been recently explored successfully to some extent. Graph-based methods are considering the notion of a co-occurrence graph, assuming a binary relatedness between cooccurring words. One of the key challenges in WSI is learning the higher-order semantic relatedness among multiple context instances. Previous approaches (Klapaftis and Manandhar. 2007; Bordag, 2006) for WSI are used to construct higher-order relatedness by counting co-occurrence frequency or collocation of mutiwords, regardless of global semantic similarity. Lexical chain (Morris and Hirst, 1991) is defined as a sequence of semantically related words in text and provides important clues about the text structure and topic. It can be viewed as a global counterpart of the measures of semantic similarity (Navigli, 2009). For example, Figure 1 gives three context instances containing Apple. * Corresponding author E-mail: dhji@whu.edu.cn. This work is licensed under a Creative Commons Attribut"
C14-1152,E03-1020,0,0.0319779,"13). The remainder of this paper is structured as follows. Section 2 presents an overview of the related work. Section 3 describes our model in details. Section 4 provides a quantitative evaluation and comparison with other algorithms in the SemEval-2013 word sense induction task. Finally, section 5 draws conclusions and lays out some future research directions. 2 2.1 Related Work Word sense induction A number of diverse approaches to WSI have been proposed so far. Context features are often represented in a variety of forms such as co-occurrence of words within phrases (Pantel and Lin, 2002; Dorow and Widdows, 2003), parts of speeches (Purandare and Pedersen, 2004), and grammatical relations (Pantel and Lin, 2002; Dorow and Widdows, 2003). The size of the context window also varies, such as two words before and after the target word, the sentence or even larger paragraph within which contains the target word. Most of the work in WSI is the vector space model, such as context-based vector algorithm (Schütze, 1998; Ide et al., 2001; Van de Cruys et al., 2011), substitute-based vector algorithm (Yatbaz et al., 2012; Baskaya et al., 2013). In this model, the context of each instance of a target word is repre"
C14-1152,P09-1002,0,0.0541808,"Missing"
C14-1152,ide-suderman-2004-american,0,0.0340932,"a relatively high degree of semantic similarity. High cohesion is defined as greater than average cohesion of all clusters. Low separation is defined as a reciprocal relationship between two clusters: if a cluster Ci has the lowest separations to a cluster Cj and Cj has the lowest separation to Ci, then the two (high cohesion) clusters are merged. This merging process is iterated until it converges. 4 Experiment and Evaluation 4.1 Dataset Our WSI evaluation is based on the dataset provided by the SemEval-2013 shared 13th task. Test data was drawn from the Open American National Corpus (OANC) (Ide and Suderman, 2004) across a variety of genres and from both the spoken and written portions of the corpus. It consists of 4,806 instances of 50 target words: 20 verbs, 20 nouns and 10 adjectives. Due to the unsupervised nature of the task, participants were not provided with sense-labeled training data. However, WSI systems were provided with the ukWac corpus (Baroni et al., 2009) to use in inducing senses. Additionally, we used the SemEval-2013 lexical trial data sets as development sets to tune parameters. 4.2 Implementation details The training data is extracted from uKWac corpus. For each target word, we ex"
C14-1152,S13-2049,0,0.0337128,"Missing"
C14-1152,S07-1092,0,0.0201496,"e instances into classes, each corresponding to an induced sense. Traditional methods in WSI tend to adopt the vector space model, in which the context of each instance of a target word is represented as a vector of features based on frequency statistics and probability distributions, e.g., first-order or second-order vector (Schütze, 1998; Purandare and Pedersen, 2004; Cruys et al., 2011). These vectors are clustered and the resulting clusters represent the induced senses. Another family of employed approach is graph-based methods (Widdows and Dorow, 2002; Véronis, 2004; Agirre et al., 2006; Klapaftis and Manandhar, 2007; Di Marco and Navigli, 2011; Hope and Keller, 2013), which have been recently explored successfully to some extent. Graph-based methods are considering the notion of a co-occurrence graph, assuming a binary relatedness between cooccurring words. One of the key challenges in WSI is learning the higher-order semantic relatedness among multiple context instances. Previous approaches (Klapaftis and Manandhar. 2007; Bordag, 2006) for WSI are used to construct higher-order relatedness by counting co-occurrence frequency or collocation of mutiwords, regardless of global semantic similarity. Lexical"
C14-1152,S13-2051,0,0.024406,"both the Fuzzy NMI and Fuzzy B-Cubed measures, thereby avoiding the extreme performance of either baseline. Generally, the performance of our model gets balanced scores. 4.5 Discussion Topic models, such as LDA and HDP (Brody and Lapata, 2009; Lau et al., 2012), have been successfully adopted for WSI, in which one topic is viewed as one sense. Our work is motivated by lexical chain that represents the intrinsic semantic relatedness among context instances on the viewpoint of linguistics. Topic model is used to find lexical chains which are interpreted as topics. We have compared the Unimelb (Lau et al., 2013), a HDP topic model, with our model in the experiments. Additionally, we also follow Lau et al. (2012) to train a LDA model with a fixed number of topics based on our training data for WSI3. Table 3 shows the supervised result compared to the Schype. These experiments show promising performance for our model, which captures richer semantic relatedness by using lexical chains. Lexical chains play a key role for the performance of our model. Intuitively, when lexical chains are too long, the higher-order relatedness would be mixed with some noises, while when lexical chains are too short, some h"
C14-1152,E12-1060,0,0.0153844,"since this setting does not require any sense mapping procedure to convert induced senses to WordNet senses. Table 2 shows the performance of our systems, benchmarks and baselines. It shows that the NMImeasure is biased towards the one sense per instance baseline and the FBC-measure one sense baseline. However, systems are capable of performing well in both the Fuzzy NMI and Fuzzy B-Cubed measures, thereby avoiding the extreme performance of either baseline. Generally, the performance of our model gets balanced scores. 4.5 Discussion Topic models, such as LDA and HDP (Brody and Lapata, 2009; Lau et al., 2012), have been successfully adopted for WSI, in which one topic is viewed as one sense. Our work is motivated by lexical chain that represents the intrinsic semantic relatedness among context instances on the viewpoint of linguistics. Topic model is used to find lexical chains which are interpreted as topics. We have compared the Unimelb (Lau et al., 2013), a HDP topic model, with our model in the experiments. Additionally, we also follow Lau et al. (2012) to train a LDA model with a fixed number of topics based on our training data for WSI3. Table 3 shows the supervised result compared to the Sc"
C14-1152,C94-2121,0,0.346625,"; semantic threshold γ. Output: lexical chain set S 1 θ,φ,Z LDA (D) 2 for each topic z 3 lc ="""" // lc denotes a lexical chain 4 for each doc d 5 for each word w in doc d 6 if ( zw = z and p(w,d|z) > γ ) 7 lc.add (w) 8 S.add (lc) 9 return S 2.2 Lexical chain extraction The Lexical chain method is an important technique in natural language processing. A lexical chain is a sequence of semantic related words in text and provides important clues about the text structure and topic. It has formed a theoretically well-founded building block in a lot of applications, such as word sense disambiguation (Manabu and Takeo, 1994), malapropism detection and correction (Hirst and StOnge, 1998), summarization (Barzilay et al., 1997), topic tracking (Carthy, 2004), text segmentation (Stokes et al., 2004), and others. There are mainly two approaches for lexical chain extraction. One focuses on the use of knowledge resources like WordNet (Hirst and St-Onge, 1998) or thesauri (Morris and Hirst, 1991) as background information in order to quantify semantic relations between words. A major disadvantage of this strategy is that it relies on the resource, which has a direct impact on the quality of lexical chains. Another approa"
C14-1152,S10-1011,0,0.0189897,"el for WSI, in which co-occurrences of two or more words are represented by using weighted hyperedges. This model fully exploits the existence of collocations or terms consisting of more than two words. In fact, the method converts the sense induction problem to the clustering of the contextual words, and the result relies on local word co-occurrence frequency. Our hypergraph model is constructed from a global perspective, where the whole context instance is regarded as a node. WSI evaluation also is an important issue in WSI tasks. Previous WSI evaluations in SemEval (Agirre and Soroa, 2007; Manandhar et al., 2010) have approached sense induction in terms of finding the single most salient sense of a target word given its context. However, as shown in Erk and McCarthy (2009), multiple senses of the target word may be perceived by readers from different angles and a graded notion of sense labeling may be considered as the most appropriate. The SemEval-2013 WSI evaluation is designed to explore the possibility of finding all perceived senses of a target word in a single context instance. Our model is evaluated and verified on the SemEval-2013 WSI task. 1602 Algorithm 1. lexical chains extraction algorithm"
C14-1152,J91-1002,0,0.959054,"rco and Navigli, 2011; Hope and Keller, 2013), which have been recently explored successfully to some extent. Graph-based methods are considering the notion of a co-occurrence graph, assuming a binary relatedness between cooccurring words. One of the key challenges in WSI is learning the higher-order semantic relatedness among multiple context instances. Previous approaches (Klapaftis and Manandhar. 2007; Bordag, 2006) for WSI are used to construct higher-order relatedness by counting co-occurrence frequency or collocation of mutiwords, regardless of global semantic similarity. Lexical chain (Morris and Hirst, 1991) is defined as a sequence of semantically related words in text and provides important clues about the text structure and topic. It can be viewed as a global counterpart of the measures of semantic similarity (Navigli, 2009). For example, Figure 1 gives three context instances containing Apple. * Corresponding author E-mail: dhji@whu.edu.cn. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1601 Proceedings of COLING 2014, the 25th In"
C14-1152,W04-2406,0,0.168602,"a word. Word sense induction is generally considered as an unsupervised clustering problem. The input for the clustering algorithm is context instances of a target word, represented by word bags or cooccurrence vectors, and the output is a grouping of these instances into classes, each corresponding to an induced sense. Traditional methods in WSI tend to adopt the vector space model, in which the context of each instance of a target word is represented as a vector of features based on frequency statistics and probability distributions, e.g., first-order or second-order vector (Schütze, 1998; Purandare and Pedersen, 2004; Cruys et al., 2011). These vectors are clustered and the resulting clusters represent the induced senses. Another family of employed approach is graph-based methods (Widdows and Dorow, 2002; Véronis, 2004; Agirre et al., 2006; Klapaftis and Manandhar, 2007; Di Marco and Navigli, 2011; Hope and Keller, 2013), which have been recently explored successfully to some extent. Graph-based methods are considering the notion of a co-occurrence graph, assuming a binary relatedness between cooccurring words. One of the key challenges in WSI is learning the higher-order semantic relatedness among multip"
C14-1152,N13-1119,0,0.271632,"he Apple Company. We can directly group three instances by the lexical chain: iPod-iTunes-hardware and software product-Inc. This lexical chain represents a higher-order semantic relatedness among the three instances. In this paper, we propose a hypergraph model from a global perspective, in which nodes represent instances of contexts where a target word occurs and hyperedges denote higher-order semantic relatedness among instances. A lexical chain based method is used for identifying the hyperedges. This method for lexical chain extraction is a knowledge-free method based on LDA topic model (Remus and Biemann, 2013). The remainder of this paper is structured as follows. Section 2 presents an overview of the related work. Section 3 describes our model in details. Section 4 provides a quantitative evaluation and comparison with other algorithms in the SemEval-2013 word sense induction task. Finally, section 5 draws conclusions and lays out some future research directions. 2 2.1 Related Work Word sense induction A number of diverse approaches to WSI have been proposed so far. Context features are often represented in a variety of forms such as co-occurrence of words within phrases (Pantel and Lin, 2002; Dor"
C14-1152,W12-0703,0,0.0163124,"me paragraph or text, whose topic distributions are identical. However, in our experiment the context instances of a target word for WSI are derived from different articles, whose topic distributions are varied. Therefore both lexical and contextual topics are modeled. After training the LDA model, we use the information of the per-document topic distribution θd= p(z|d), the per-topic word distribution φ w=p(w|z) and the sampling topic of a word zw. The key work lies in how to assign a word to a topic in training LDA model. Since single samples of topics per word may exhibit a large variance (Riedl and Biemann, 2012), we sample several times and use the mode (most frequently assigned) topic ID per word as the topic assignment. 1603 The extraction algorithm is shown in algorithm 1. In order to improve the quality of identified lexical chains, a threshold γ is set to filter those invalid words whose generating probability of sampling topics in the document is lower than γ. p ( w, d |z ) ≈ p ( z |d ) p ( w |z ) > γ (1) The threshold γ is essential for the quality of lexical chains, which directly impacts on the performance of the model. Detailed analysis for the threshold γ will be given in section 4.5. 3.2"
C14-1152,J98-1004,0,0.734871,"ible senses for a word. Word sense induction is generally considered as an unsupervised clustering problem. The input for the clustering algorithm is context instances of a target word, represented by word bags or cooccurrence vectors, and the output is a grouping of these instances into classes, each corresponding to an induced sense. Traditional methods in WSI tend to adopt the vector space model, in which the context of each instance of a target word is represented as a vector of features based on frequency statistics and probability distributions, e.g., first-order or second-order vector (Schütze, 1998; Purandare and Pedersen, 2004; Cruys et al., 2011). These vectors are clustered and the resulting clusters represent the induced senses. Another family of employed approach is graph-based methods (Widdows and Dorow, 2002; Véronis, 2004; Agirre et al., 2006; Klapaftis and Manandhar, 2007; Di Marco and Navigli, 2011; Hope and Keller, 2013), which have been recently explored successfully to some extent. Graph-based methods are considering the notion of a co-occurrence graph, assuming a binary relatedness between cooccurring words. One of the key challenges in WSI is learning the higher-order sem"
C14-1152,P11-1148,0,0.0281054,"Missing"
C14-1152,C02-1114,0,0.26147,"or cooccurrence vectors, and the output is a grouping of these instances into classes, each corresponding to an induced sense. Traditional methods in WSI tend to adopt the vector space model, in which the context of each instance of a target word is represented as a vector of features based on frequency statistics and probability distributions, e.g., first-order or second-order vector (Schütze, 1998; Purandare and Pedersen, 2004; Cruys et al., 2011). These vectors are clustered and the resulting clusters represent the induced senses. Another family of employed approach is graph-based methods (Widdows and Dorow, 2002; Véronis, 2004; Agirre et al., 2006; Klapaftis and Manandhar, 2007; Di Marco and Navigli, 2011; Hope and Keller, 2013), which have been recently explored successfully to some extent. Graph-based methods are considering the notion of a co-occurrence graph, assuming a binary relatedness between cooccurring words. One of the key challenges in WSI is learning the higher-order semantic relatedness among multiple context instances. Previous approaches (Klapaftis and Manandhar. 2007; Bordag, 2006) for WSI are used to construct higher-order relatedness by counting co-occurrence frequency or collocati"
C14-1152,P95-1026,0,0.567297,"hypergraph model in which nodes represent instances of contexts where a target word occurs and hyperedges represent higher-order semantic relatedness among instances. A lexical chain based method is used for discovering the hyperedges, and hypergraph clustering methods are used for finding word senses among the context instances. Experiments show that this model outperforms other methods in supervised evaluation and achieves comparable performance with other methods in unsupervised evaluation. 1 Introduction Word sense induction (WSI) aims to automatically find senses of a given target word (Yarowsky, 1995) from large scale texts. Compared with existing manual word sense resources, WSI techniques use clustering algorithms to determine the possible senses for a word. Word sense induction is generally considered as an unsupervised clustering problem. The input for the clustering algorithm is context instances of a target word, represented by word bags or cooccurrence vectors, and the output is a grouping of these instances into classes, each corresponding to an induced sense. Traditional methods in WSI tend to adopt the vector space model, in which the context of each instance of a target word is"
C14-1152,D12-1086,0,0.0276048,"a variety of forms such as co-occurrence of words within phrases (Pantel and Lin, 2002; Dorow and Widdows, 2003), parts of speeches (Purandare and Pedersen, 2004), and grammatical relations (Pantel and Lin, 2002; Dorow and Widdows, 2003). The size of the context window also varies, such as two words before and after the target word, the sentence or even larger paragraph within which contains the target word. Most of the work in WSI is the vector space model, such as context-based vector algorithm (Schütze, 1998; Ide et al., 2001; Van de Cruys et al., 2011), substitute-based vector algorithm (Yatbaz et al., 2012; Baskaya et al., 2013). In this model, the context of each instance of a target word is represented as a vector of features based on frequency statistics or probability distributions (e.g., firstorder or second-order vector). These vectors are clustered by various algorithms and the resulting clusters represent the induced senses. Another family of employed approach is graph-based methods, which have been successfully applied in the sense induction task with some better results achieved. In this framework words are represented as nodes in the graph and vertices are drawn between the target wo"
C14-1152,W06-1669,0,\N,Missing
C16-1235,D13-1172,0,0.0256999,"of threshold, which is in line with intuitively understanding. For obtaining enough negative samples, we chosen 0.3 as the similarity threshold. 1.90 0.56 0.54 1.89 0.52 1.88 0.50 Purity 0.48 1.87 Entropy 0.46 1.86 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Figure 3: Influence of the similarity threshold η 4 Related Work Our work is related to aspect-level sentiment analysis, metric learning, and deep learning. For aspect-level sentiment analysis, there are many methods on clustering aspect phrases. Some topic-model-based approaches jointly extract aspect phrases and group them at the same time (Chen et al., 2013; Moghaddam and Ester, 2012; Lu et al., 2011; Jo and Oh, 2011; Zhao et al., 2010; Lin and He, 2009). Those methods tend to discover coarse-grained and grouped aspect phrases, but not specific opinionated aspect phrase themselves. In addition, Zhai et al. (2011a) showed that they did not perform well even considering pre-existing knowledge. Some other work focuses on grouping aspect phrases. Guo et al. (2009) grouped aspect phrases using multi-level LaSA, which exploits the virtual 2499 context documents and semantic structure of aspect phrase. Zhai et al. (2010) used an EM-based semisupervised"
C16-1235,J81-4005,0,0.707261,"Missing"
C16-1235,N15-1184,0,0.0185674,"to the success of our method is the finding of a proper training algorithm for the neural network model. Inspired by word embedding training methods (Collobert et al., 2011; Mikolov et al., 2013), we take a negative sampling approach. In particular, we take pairs of sentences that contain the same aspect phrase as positive training examples, and pairs of sentences that contain incompatible aspect phrase as negative training examples, maximizing a score margin between positive and negative examples. Here two aspect phrases are incompatible if the distance based on a semantic lexicon is large (Faruqui et al., 2015; Yu and Dredze, 2014). To find a better vector space representation, we add two nonlinear transformation layers, as shown in h(1) and h(2) in Figure 1. This method is similar to the Mahalanobis distance metric learning for face verification (Hu et al., 2014). Model training is performed by back-propagation over all neural nodes. With such vector space being learned, direct K-means clustering can be used to group aspect phrases. Results on a standard benchmark show that our neural network significantly outperforms traditional models. The average results on 4 domains reached 0.51 (Purity) and 1"
C16-1235,P14-1062,0,0.0146159,"ectly concatenated to form feature vectors. In this paper, we adapt this method to the aspect phrase grouping task, and provide an extra attention-based semantic composite model to obtain feature vectors based on word vectors of aspect phrase and its context. Our work is related to word embedding and deep learning. Prior research (Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013; Tang et al., 2014; Ren et al., 2016b) presented different models to improve the performance of word embedding training, and our training is inspried by negative sampling. Deep learning methods (Kalchbrenner et al., 2014; Kim, 2014; Socher et al., 2013; Vo and Zhang, 2015; Zhang et al., 2015; Zhang et al., 2016; Ren et al., 2016a) have been applied to many tasks related to sentiment analysis. In this paper, we explore attention (Luong et al., 2015; Rush et al., 2015; Ling et al., 2015) with a MLP network to tackle the aspect phrase grouping problem. 5 Conclusion We studied distance metric learning for aspect phrase grouping, exploring a novel deep neural network framework. By leveraging semantic relations between aspect phrase and their contexts, our approach give better performance to strong baselines which"
C16-1235,W06-0301,0,0.0309999,"Liu, 2004; Pang and Lee, 2008), aspect identification from the corpus is a necessary step. Here aspect is the name of a feature of the product, while an aspect phrase is a word or phrase that actually appears in a sentence to indicate the aspect. Different aspect phrases can be used to describe the same aspect. For example, “picture quality” could be referred to “photo”, “image” and “picture”. All aspect phrases in the same group indicate the same aspect. In this paper, we assume that all aspect phrases have been identified by using existing methods (Jin et al., 2009; Kobayashi et al., 2007; Kim and Hovy, 2006), and focus on grouping domain synonymous aspect phrases. Most existing work employed unsupervised methods, exploiting lexical similarity from semantic dictionary as well as context environments (Zhao et al., 2014; Zhai et al., 2011a; Guo et al., 2009). The context for an aspect phrase is formed by aggregating related sentences that mention the same aspect phrase. Thereafter, aspect phrase and context environment are represented using bag-of-word (BoW) models separately, and integrated into a unified learning framework. One limitation of the existing methods is that they do not model the inter"
C16-1235,D14-1181,0,0.00493363,"feature vectors. In this paper, we adapt this method to the aspect phrase grouping task, and provide an extra attention-based semantic composite model to obtain feature vectors based on word vectors of aspect phrase and its context. Our work is related to word embedding and deep learning. Prior research (Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013; Tang et al., 2014; Ren et al., 2016b) presented different models to improve the performance of word embedding training, and our training is inspried by negative sampling. Deep learning methods (Kalchbrenner et al., 2014; Kim, 2014; Socher et al., 2013; Vo and Zhang, 2015; Zhang et al., 2015; Zhang et al., 2016; Ren et al., 2016a) have been applied to many tasks related to sentiment analysis. In this paper, we explore attention (Luong et al., 2015; Rush et al., 2015; Ling et al., 2015) with a MLP network to tackle the aspect phrase grouping problem. 5 Conclusion We studied distance metric learning for aspect phrase grouping, exploring a novel deep neural network framework. By leveraging semantic relations between aspect phrase and their contexts, our approach give better performance to strong baselines which achieve the"
C16-1235,D07-1114,0,0.0338543,"ntiment analysis (Hu and Liu, 2004; Pang and Lee, 2008), aspect identification from the corpus is a necessary step. Here aspect is the name of a feature of the product, while an aspect phrase is a word or phrase that actually appears in a sentence to indicate the aspect. Different aspect phrases can be used to describe the same aspect. For example, “picture quality” could be referred to “photo”, “image” and “picture”. All aspect phrases in the same group indicate the same aspect. In this paper, we assume that all aspect phrases have been identified by using existing methods (Jin et al., 2009; Kobayashi et al., 2007; Kim and Hovy, 2006), and focus on grouping domain synonymous aspect phrases. Most existing work employed unsupervised methods, exploiting lexical similarity from semantic dictionary as well as context environments (Zhao et al., 2014; Zhai et al., 2011a; Guo et al., 2009). The context for an aspect phrase is formed by aggregating related sentences that mention the same aspect phrase. Thereafter, aspect phrase and context environment are represented using bag-of-word (BoW) models separately, and integrated into a unified learning framework. One limitation of the existing methods is that they d"
C16-1235,D15-1161,0,0.149789,"ight and sharp and the sound is good”, the words “clear”, “bright” and “sharp” are related to the aspect phrase “picture”, while the word “good” is related to the aspect phrase “sound”. By the traditional model, these words are not differentiated when they are taken for the context, thereby causing noise in the grouping of the two aspect phrases. To address this issue, we propose a novel neural network structure that automatically learns the relative importance of each context word with respect to a target/aspect phrase, by leveraging an attention model (Luong et al., 2015; Rush et al., 2015; Ling et al., 2015). As shown in Figure 1, given a sentence that contains an aspect phrase, we use a neural network to find a vector representation of the aspect phrase and its context. For the grouping of a certain aspect phrase, we concatenate all the occurrences of the aspect phrase in a corpus to find its vector form. Thus, the problem of aspect phrase grouping is transformed into a clustering problem in the resulting vector space. Different from traditional methods, which leverage ∗ corresponding author This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org"
C16-1235,D15-1166,0,0.198834,"in the review “the picture is clear, bright and sharp and the sound is good”, the words “clear”, “bright” and “sharp” are related to the aspect phrase “picture”, while the word “good” is related to the aspect phrase “sound”. By the traditional model, these words are not differentiated when they are taken for the context, thereby causing noise in the grouping of the two aspect phrases. To address this issue, we propose a novel neural network structure that automatically learns the relative importance of each context word with respect to a target/aspect phrase, by leveraging an attention model (Luong et al., 2015; Rush et al., 2015; Ling et al., 2015). As shown in Figure 1, given a sentence that contains an aspect phrase, we use a neural network to find a vector representation of the aspect phrase and its context. For the grouping of a certain aspect phrase, we concatenate all the occurrences of the aspect phrase in a corpus to find its vector form. Thus, the problem of aspect phrase grouping is transformed into a clustering problem in the resulting vector space. Different from traditional methods, which leverage ∗ corresponding author This work is licenced under a Creative Commons Attribution 4.0 Int"
C16-1235,N10-1047,0,0.0295388,"95 We exploit lexical similarity to obtain incompatible aspect phrases, which have low similarity in semantic lexicon. In particular, we choose WordNet as the semantic lexicon. Two aspect phrase are incompatible when the WordNet similarity between them is smaller than a threshold η. And the WordNet similarity is calculated by Equation (12) Res(w1 , w2 ) = IC(LCS(w1 , w2 )) (10) IC(w) = −logP (w) (11) Jcn(w1 , w2 ) = 1 IC(w1 ) + IC(w2 ) − 2 × Res(w1 , w2 ) (12) where LCS (lease common subsumer) is the most specific concept that is a shared ancestor of the two concepts represented by the words (Pedersen, 2010). P (w) is the probability of the concept word w. In our experiments, the threshold is set to 0.85. Traditional methods (Zhai et al., 2010; Zhai et al., 2011b) exploit lexical knowledge to provide soft constraint for clustering aspect phrases. They assume that the aspect phrases that have high similarity in semantic lexicon, are likely to belong to the same group. In this cause, our method uses a similar assumption. For obtaining the training data, we apply an extra sample pair generation process. The generated sample aspect phrase pairs are fed into left and right sub neural network of Figure"
C16-1235,D14-1162,0,0.0804157,"Missing"
C16-1235,D15-1044,0,0.220843,"icture is clear, bright and sharp and the sound is good”, the words “clear”, “bright” and “sharp” are related to the aspect phrase “picture”, while the word “good” is related to the aspect phrase “sound”. By the traditional model, these words are not differentiated when they are taken for the context, thereby causing noise in the grouping of the two aspect phrases. To address this issue, we propose a novel neural network structure that automatically learns the relative importance of each context word with respect to a target/aspect phrase, by leveraging an attention model (Luong et al., 2015; Rush et al., 2015; Ling et al., 2015). As shown in Figure 1, given a sentence that contains an aspect phrase, we use a neural network to find a vector representation of the aspect phrase and its context. For the grouping of a certain aspect phrase, we concatenate all the occurrences of the aspect phrase in a corpus to find its vector form. Thus, the problem of aspect phrase grouping is transformed into a clustering problem in the resulting vector space. Different from traditional methods, which leverage ∗ corresponding author This work is licenced under a Creative Commons Attribution 4.0 International License."
C16-1235,D13-1170,0,0.00785405,"tors. In this paper, we adapt this method to the aspect phrase grouping task, and provide an extra attention-based semantic composite model to obtain feature vectors based on word vectors of aspect phrase and its context. Our work is related to word embedding and deep learning. Prior research (Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013; Tang et al., 2014; Ren et al., 2016b) presented different models to improve the performance of word embedding training, and our training is inspried by negative sampling. Deep learning methods (Kalchbrenner et al., 2014; Kim, 2014; Socher et al., 2013; Vo and Zhang, 2015; Zhang et al., 2015; Zhang et al., 2016; Ren et al., 2016a) have been applied to many tasks related to sentiment analysis. In this paper, we explore attention (Luong et al., 2015; Rush et al., 2015; Ling et al., 2015) with a MLP network to tackle the aspect phrase grouping problem. 5 Conclusion We studied distance metric learning for aspect phrase grouping, exploring a novel deep neural network framework. By leveraging semantic relations between aspect phrase and their contexts, our approach give better performance to strong baselines which achieve the best results in stan"
C16-1235,P14-1146,0,0.0378009,"e pair. However, these methods not perform nonlinear transformation. Hu et al. (2014) employed a MLP-based nonlinear transformation, but its input is the given image descriptor, which can be directly concatenated to form feature vectors. In this paper, we adapt this method to the aspect phrase grouping task, and provide an extra attention-based semantic composite model to obtain feature vectors based on word vectors of aspect phrase and its context. Our work is related to word embedding and deep learning. Prior research (Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013; Tang et al., 2014; Ren et al., 2016b) presented different models to improve the performance of word embedding training, and our training is inspried by negative sampling. Deep learning methods (Kalchbrenner et al., 2014; Kim, 2014; Socher et al., 2013; Vo and Zhang, 2015; Zhang et al., 2015; Zhang et al., 2016; Ren et al., 2016a) have been applied to many tasks related to sentiment analysis. In this paper, we explore attention (Luong et al., 2015; Rush et al., 2015; Ling et al., 2015) with a MLP network to tackle the aspect phrase grouping problem. 5 Conclusion We studied distance metric learning for aspect ph"
C16-1235,P15-1098,0,0.0334699,"Missing"
C16-1235,W15-1509,0,0.0120389,"mputational Linguistics: Technical Papers, pages 2492–2502, Osaka, Japan, December 11-17 2016. y Distance Metric h(2) 1 h(2) 2 h(1) 1 h(1) 2 x1 x2 c~ 1 c~ 2 a1 a2 p1 p2 c1 c2 Figure 1: Architecture of the proposed method. For a given pair of aspect phrases p1 and p2 , with their contexts c1 and c2 respectively, two vectors x1 and x2 are obtained via attention-based semantic (2) (2) combination, and then mapped into the same feature subspace as h1 and h2 . a bag-of-word feature space, our vector space considers not only words, but also semantic similarities between aspect phrases and contexts (Xu et al., 2015). One challenge to the success of our method is the finding of a proper training algorithm for the neural network model. Inspired by word embedding training methods (Collobert et al., 2011; Mikolov et al., 2013), we take a negative sampling approach. In particular, we take pairs of sentences that contain the same aspect phrase as positive training examples, and pairs of sentences that contain incompatible aspect phrase as negative training examples, maximizing a score margin between positive and negative examples. Here two aspect phrases are incompatible if the distance based on a semantic lex"
C16-1235,P14-2089,0,0.0214248,"method is the finding of a proper training algorithm for the neural network model. Inspired by word embedding training methods (Collobert et al., 2011; Mikolov et al., 2013), we take a negative sampling approach. In particular, we take pairs of sentences that contain the same aspect phrase as positive training examples, and pairs of sentences that contain incompatible aspect phrase as negative training examples, maximizing a score margin between positive and negative examples. Here two aspect phrases are incompatible if the distance based on a semantic lexicon is large (Faruqui et al., 2015; Yu and Dredze, 2014). To find a better vector space representation, we add two nonlinear transformation layers, as shown in h(1) and h(2) in Figure 1. This method is similar to the Mahalanobis distance metric learning for face verification (Hu et al., 2014). Model training is performed by back-propagation over all neural nodes. With such vector space being learned, direct K-means clustering can be used to group aspect phrases. Results on a standard benchmark show that our neural network significantly outperforms traditional models. The average results on 4 domains reached 0.51 (Purity) and 1.74 (Entropy), better"
C16-1235,C10-1143,0,0.388385,"choose WordNet as the semantic lexicon. Two aspect phrase are incompatible when the WordNet similarity between them is smaller than a threshold η. And the WordNet similarity is calculated by Equation (12) Res(w1 , w2 ) = IC(LCS(w1 , w2 )) (10) IC(w) = −logP (w) (11) Jcn(w1 , w2 ) = 1 IC(w1 ) + IC(w2 ) − 2 × Res(w1 , w2 ) (12) where LCS (lease common subsumer) is the most specific concept that is a shared ancestor of the two concepts represented by the words (Pedersen, 2010). P (w) is the probability of the concept word w. In our experiments, the threshold is set to 0.85. Traditional methods (Zhai et al., 2010; Zhai et al., 2011b) exploit lexical knowledge to provide soft constraint for clustering aspect phrases. They assume that the aspect phrases that have high similarity in semantic lexicon, are likely to belong to the same group. In this cause, our method uses a similar assumption. For obtaining the training data, we apply an extra sample pair generation process. The generated sample aspect phrase pairs are fed into left and right sub neural network of Figure 1, respectively. Specifically, each training sentence is utilized with its labelled aspect phrase as a gold sample. Then, we combine each"
C16-1235,D15-1073,1,0.821773,"to the aspect phrase grouping task, and provide an extra attention-based semantic composite model to obtain feature vectors based on word vectors of aspect phrase and its context. Our work is related to word embedding and deep learning. Prior research (Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013; Tang et al., 2014; Ren et al., 2016b) presented different models to improve the performance of word embedding training, and our training is inspried by negative sampling. Deep learning methods (Kalchbrenner et al., 2014; Kim, 2014; Socher et al., 2013; Vo and Zhang, 2015; Zhang et al., 2015; Zhang et al., 2016; Ren et al., 2016a) have been applied to many tasks related to sentiment analysis. In this paper, we explore attention (Luong et al., 2015; Rush et al., 2015; Ling et al., 2015) with a MLP network to tackle the aspect phrase grouping problem. 5 Conclusion We studied distance metric learning for aspect phrase grouping, exploring a novel deep neural network framework. By leveraging semantic relations between aspect phrase and their contexts, our approach give better performance to strong baselines which achieve the best results in standard benchmark. Our method can be applie"
C16-1235,D10-1006,0,0.0420326,"ugh negative samples, we chosen 0.3 as the similarity threshold. 1.90 0.56 0.54 1.89 0.52 1.88 0.50 Purity 0.48 1.87 Entropy 0.46 1.86 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Figure 3: Influence of the similarity threshold η 4 Related Work Our work is related to aspect-level sentiment analysis, metric learning, and deep learning. For aspect-level sentiment analysis, there are many methods on clustering aspect phrases. Some topic-model-based approaches jointly extract aspect phrases and group them at the same time (Chen et al., 2013; Moghaddam and Ester, 2012; Lu et al., 2011; Jo and Oh, 2011; Zhao et al., 2010; Lin and He, 2009). Those methods tend to discover coarse-grained and grouped aspect phrases, but not specific opinionated aspect phrase themselves. In addition, Zhai et al. (2011a) showed that they did not perform well even considering pre-existing knowledge. Some other work focuses on grouping aspect phrases. Guo et al. (2009) grouped aspect phrases using multi-level LaSA, which exploits the virtual 2499 context documents and semantic structure of aspect phrase. Zhai et al. (2010) used an EM-based semisupervised learning method for clustering aspect phrases, in which the lexical knowledge i"
C16-1235,D14-1169,0,0.0762796,"sentence to indicate the aspect. Different aspect phrases can be used to describe the same aspect. For example, “picture quality” could be referred to “photo”, “image” and “picture”. All aspect phrases in the same group indicate the same aspect. In this paper, we assume that all aspect phrases have been identified by using existing methods (Jin et al., 2009; Kobayashi et al., 2007; Kim and Hovy, 2006), and focus on grouping domain synonymous aspect phrases. Most existing work employed unsupervised methods, exploiting lexical similarity from semantic dictionary as well as context environments (Zhao et al., 2014; Zhai et al., 2011a; Guo et al., 2009). The context for an aspect phrase is formed by aggregating related sentences that mention the same aspect phrase. Thereafter, aspect phrase and context environment are represented using bag-of-word (BoW) models separately, and integrated into a unified learning framework. One limitation of the existing methods is that they do not model the interaction between aspect phrases and their contexts explicitly. For example, in the review “the picture is clear, bright and sharp and the sound is good”, the words “clear”, “bright” and “sharp” are related to the as"
C16-1235,P14-1030,0,\N,Missing
C98-1095,H93-1036,0,\N,Missing
C98-1095,C92-2070,0,\N,Missing
C98-1095,P93-1034,0,\N,Missing
D14-1055,P12-2034,0,\N,Missing
D14-1055,P11-1032,0,\N,Missing
D15-1211,P06-2005,0,0.187504,"et al., 2011; Gimpel et al., 2011; Han and Baldwin, 2011). One of the major challenges for microblog processing is the issue of informal words. For example, “tmrw” has been frequently used in tweets for “tomorrow”, causing OOV problems. Text normalization has been introduced as a pre-processing step for microblog processing, which transforms informal words into their standard forms. Most work in the literature focuses on English microblog normalization, treating it as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c), and training models based on words. Lack of annotated corpora, text normalization is more challenging for Chinese. Unlike English, Chinese informal words are more difficult ∗ corresponding author to mechanically normalize for two main reasons. First, Chinese does not have word delimiters. Second, Chinese informal words manifest diversity, such as abbreviations, neologisms, unconventional spellings and phonetic substitutions. Intuitively, there is mutual dependency between Chinese word segmentation and normalization, and therefo"
D15-1211,P15-1168,0,0.0201398,"Gimpel et al. (2011) and Foster et al. (2011) annotate English microblog posts with POS tags. Han and Baldwin (2011) release a microblog corpus annotated with normalized words. Duan et al. (2012) develop a Chinese microblog corpus annotated with segmentation for SIGHAN bakeoff. Wang et al. (2013) release a Chinese microblog corpus for word segmentation and informal word detection. However, there are no microblog corpora annotated Chinese word segmentation, POS tags, and normalized sentences. Our work is alse related to the work of word segmentation (Zhang and Clark, 2007; Zhang et al., 2013; Chen et al., 2015) and joint word segmentation and POS-tagging (Jiang et al., 2008; Zhang and Clark, 2010). A comprehensive survey is out of the scope of this paper, but interested readers can refer to Pei et al. (Pei et al., 2014) for a recent literature review of the fields. To evaluate our model, we develop an annotated microblog corpus with word segmentation, POS tags, and normalization. Furthermore, we train our model by using a standard segmented and POS tagged corpus. We also present a comprehensive evaluation in terms of precision and recall on our 1844 microblog test corpus. Such an evaluation has not"
D15-1211,W09-2010,0,0.201583,"as shown that off-the-shelf NLP tools can perform poorly on microblogs (Foster et al., 2011; Gimpel et al., 2011; Han and Baldwin, 2011). One of the major challenges for microblog processing is the issue of informal words. For example, “tmrw” has been frequently used in tweets for “tomorrow”, causing OOV problems. Text normalization has been introduced as a pre-processing step for microblog processing, which transforms informal words into their standard forms. Most work in the literature focuses on English microblog normalization, treating it as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c), and training models based on words. Lack of annotated corpora, text normalization is more challenging for Chinese. Unlike English, Chinese informal words are more difficult ∗ corresponding author to mechanically normalize for two main reasons. First, Chinese does not have word delimiters. Second, Chinese informal words manifest diversity, such as abbreviations, neologisms, unconventional spellings and phonetic substitutions. Intuitively, there is mutual depen"
D15-1211,P11-2008,0,0.121397,"Missing"
D15-1211,W11-0704,0,0.029173,"cular, the precision in our SNT model improves upon the baselines significantly. The main reason is that our model is based on global features over whole sentences, while the two baselines based on local windows features. 7 Related Work There has much work on text normalization. The task is generally treated as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013; Sonmez and Ozgur, 2014) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c). For English, most recent work (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) uses two-step unsupervised approaches to first detect and then normalize informal words. They aim to produce and use informal/formal word lexicons and mappings. In processing Chinese informal text, Wong and Xia (2008) address the problem of informal words in bulletin board system (BBS) chats by employing pattern matching. Xia et al. (2005) also use SVM-based classification to recognize Chinese informal sentences chats. Both methods have their advantages: the learning-based method does better on recall, while the pattern matching performs better on precision. Li and Yarowsky"
D15-1211,P11-1038,0,0.32066,"three systems. In particular, the precision in our SNT model improves upon the baselines significantly. The main reason is that our model is based on global features over whole sentences, while the two baselines based on local windows features. 7 Related Work There has much work on text normalization. The task is generally treated as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013; Sonmez and Ozgur, 2014) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c). For English, most recent work (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) uses two-step unsupervised approaches to first detect and then normalize informal words. They aim to produce and use informal/formal word lexicons and mappings. In processing Chinese informal text, Wong and Xia (2008) address the problem of informal words in bulletin board system (BBS) chats by employing pattern matching. Xia et al. (2005) also use SVM-based classification to recognize Chinese informal sentences chats. Both methods have their advantages: the learning-based method does better on recall, while the pattern matching performs better on precis"
D15-1211,D12-1039,0,0.402557,"ization, which can be expensive. In this paper, we propose a joint model for Chinese text normalization, word-segmentation and POS tagging, which can be trained using standard segmentation and POS tagging annotation, overcoming the lack of an annotated corpus on Chinese microblogs. Our model is based on Zhang and Clark (2010), with an extended set of transition actions to handle joint normalization. In our model, word segmentation and POS tagging are based on normalized text transformed from informal text. Assuming that the majority of informal words can be normalized into formal equivalents (Han et al., 2012; Li and Yarowsky, 2008), we seek standard forms of informal words from an automatically constructed normalization dictionary. To evaluate our model, we developed an annotated corpus of microblog texts. Results show that our model achieves the best performances on three tasks compared with several baseline systems. 2 Text Normalization Text normalization is a relatively new research topic. There are no precise definitions of a text 1837 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1837–1846, c Lisbon, Portugal, 17-21 September 2015. 2015 Associa"
D15-1211,P08-1102,0,0.0537527,"Missing"
D15-1211,D14-1011,0,0.0548928,"start of a new word in a state, SEPS operates SEP and replaces the last word by a possible standard form. 3.4 Features In the experiments, we use the segmentation feature templates of Zhang and Clark (2011). These features are effective for segmentation on formal text. However, for text normalization, these features contain insufficient information. Our experiments show that by using Zhang and Clark’s features, the F-Score on normalization is only 0.4207. Prior work has shown that the language statistic information is important for text normalization (Wang et al., 2013; Li and Yarowsky, 2008; Kaji and Kitsuregawa, 2014). As a result, we extract language model features by using word-based language model learned from a large quantity of standard texts. In particular, 1-gram, 2-gram, 3-gram features are extracted. Every type of n-gram is divided into ten probability ranges. For example, if the probability of the word bigram: “ ‹›- '” (high pressure) is in the 2nd range, the feature is represented as “word-2-gram=2”. In our experiments, language models are trained on the Gigaword corpus1 with SRILM tools2 . To train a word-based language model, we segmented the corpus using our re-implementation of Zhang and Cla"
D15-1211,P09-1058,0,0.0310177,"ttp://www.speech.sri.com/projects/srilm/ Text wR_ðwŸ (Overseas returnees is also referred to as turtles.) õ Ø b † } º i Ùõ ëØp ØIpf (A tree, seemingly a little high, fails a lot of people. Well, this tree is called high number (advanced mathematics)) mance of text normalization, but also increases the performance of word-segmentation. 4 4.1 Extension for Joint Segmentation, Normalization and POS tagging Joint Segmentation and POS Tagging In order to reduce the error propagation of word segmentation, joint models have been applied to some NLP tasks, such as POS tagging (Zhang and Clark, 2010; Kruengkrai et al., 2009) and Parsing (Zhang et al., 2014a; Qian and Liu, 2012; Zhang et al., 2014b). We take the joint word segmentation and POS tagging model of Zhang and Clark (2010) as the joint baseline. It extends from transition-based segmenter, adding POS arguments to the original actions. In Figure 1, when the current character ci is processing, the transition system for ST would operate as follows : (1) APP(ci ), removing ci from Q, and appending it to the last (partial) word in S with the same POS tag, . (2) SEP(ci , pos), removing ci from Q, making the last word in S as completed, and adding ci as a new pa"
D15-1211,D08-1108,0,0.117583,"be expensive. In this paper, we propose a joint model for Chinese text normalization, word-segmentation and POS tagging, which can be trained using standard segmentation and POS tagging annotation, overcoming the lack of an annotated corpus on Chinese microblogs. Our model is based on Zhang and Clark (2010), with an extended set of transition actions to handle joint normalization. In our model, word segmentation and POS tagging are based on normalized text transformed from informal text. Assuming that the majority of informal words can be normalized into formal equivalents (Han et al., 2012; Li and Yarowsky, 2008), we seek standard forms of informal words from an automatically constructed normalization dictionary. To evaluate our model, we developed an annotated corpus of microblog texts. Results show that our model achieves the best performances on three tasks compared with several baseline systems. 2 Text Normalization Text normalization is a relatively new research topic. There are no precise definitions of a text 1837 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1837–1846, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational L"
D15-1211,P14-1028,0,0.0258405,"microblog corpus annotated with segmentation for SIGHAN bakeoff. Wang et al. (2013) release a Chinese microblog corpus for word segmentation and informal word detection. However, there are no microblog corpora annotated Chinese word segmentation, POS tags, and normalized sentences. Our work is alse related to the work of word segmentation (Zhang and Clark, 2007; Zhang et al., 2013; Chen et al., 2015) and joint word segmentation and POS-tagging (Jiang et al., 2008; Zhang and Clark, 2010). A comprehensive survey is out of the scope of this paper, but interested readers can refer to Pei et al. (Pei et al., 2014) for a recent literature review of the fields. To evaluate our model, we develop an annotated microblog corpus with word segmentation, POS tags, and normalization. Furthermore, we train our model by using a standard segmented and POS tagged corpus. We also present a comprehensive evaluation in terms of precision and recall on our 1844 microblog test corpus. Such an evaluation has not been conducted in previous work due to the lack of annotated corpora for Chinese microblogs. 8 Conclusion We proposed a joint model of word segmentation, POS tagging and normalization, in which the three tasks ben"
D15-1211,D12-1046,0,0.0377294,"eas returnees is also referred to as turtles.) õ Ø b † } º i Ùõ ëØp ØIpf (A tree, seemingly a little high, fails a lot of people. Well, this tree is called high number (advanced mathematics)) mance of text normalization, but also increases the performance of word-segmentation. 4 4.1 Extension for Joint Segmentation, Normalization and POS tagging Joint Segmentation and POS Tagging In order to reduce the error propagation of word segmentation, joint models have been applied to some NLP tasks, such as POS tagging (Zhang and Clark, 2010; Kruengkrai et al., 2009) and Parsing (Zhang et al., 2014a; Qian and Liu, 2012; Zhang et al., 2014b). We take the joint word segmentation and POS tagging model of Zhang and Clark (2010) as the joint baseline. It extends from transition-based segmenter, adding POS arguments to the original actions. In Figure 1, when the current character ci is processing, the transition system for ST would operate as follows : (1) APP(ci ), removing ci from Q, and appending it to the last (partial) word in S with the same POS tag, . (2) SEP(ci , pos), removing ci from Q, making the last word in S as completed, and adding ci as a new partial word with a POS tag “pos”. Given the sentence “"
D15-1211,J11-1005,1,0.920973,"alization dictionary, avoiding diversity on informal words. 3 3.1 Joint Segmentation and Normalization Transition-based Segmentation We adapt the segmenter of Zhang and Clark (2007) as our baseline segmenter. Given an input sentence x, the baseline segmenter finds a segmentation by maximizing: F (x) = argmax Score(y) yGen(x) (1) where Gen(x) denotes the set of all possible segmentations for an input sentence. Zhang and Clark (2007) proposed a graphbased scoring model, with features based on complete words and word sequences. We adapt their method slightly, under a transition-based framework (Zhang and Clark, 2011), which gives us a consistent way of defining all models in this paper. Stack ... S2 S1 S0 Queue C1 ... Cn Figure 1: A state of transition-based model. Here a transition model is defined as a quadruple M = (C, T, W, Ct ), where C is a state space, T is a set of transitions, each of which is a function: C → C, W is an input sentence c1 ... cn , Ct is a set of terminal states. A model scores the output by scoring the corresponding transition sequence. As shown in Figure 1, a state is a tuple ST = (S, Q), where S contains partially segmented sequences, and Q = (ci , ci+1 , ..., cn ) is the sequen"
D15-1211,D13-1031,0,0.0249816,"guistic information. Gimpel et al. (2011) and Foster et al. (2011) annotate English microblog posts with POS tags. Han and Baldwin (2011) release a microblog corpus annotated with normalized words. Duan et al. (2012) develop a Chinese microblog corpus annotated with segmentation for SIGHAN bakeoff. Wang et al. (2013) release a Chinese microblog corpus for word segmentation and informal word detection. However, there are no microblog corpora annotated Chinese word segmentation, POS tags, and normalized sentences. Our work is alse related to the work of word segmentation (Zhang and Clark, 2007; Zhang et al., 2013; Chen et al., 2015) and joint word segmentation and POS-tagging (Jiang et al., 2008; Zhang and Clark, 2010). A comprehensive survey is out of the scope of this paper, but interested readers can refer to Pei et al. (Pei et al., 2014) for a recent literature review of the fields. To evaluate our model, we develop an annotated microblog corpus with word segmentation, POS tags, and normalization. Furthermore, we train our model by using a standard segmented and POS tagged corpus. We also present a comprehensive evaluation in terms of precision and recall on our 1844 microblog test corpus. Such an"
D15-1211,P14-1125,1,0.925763,"ne of the major challenges for microblog processing is the issue of informal words. For example, “tmrw” has been frequently used in tweets for “tomorrow”, causing OOV problems. Text normalization has been introduced as a pre-processing step for microblog processing, which transforms informal words into their standard forms. Most work in the literature focuses on English microblog normalization, treating it as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c), and training models based on words. Lack of annotated corpora, text normalization is more challenging for Chinese. Unlike English, Chinese informal words are more difficult ∗ corresponding author to mechanically normalize for two main reasons. First, Chinese does not have word delimiters. Second, Chinese informal words manifest diversity, such as abbreviations, neologisms, unconventional spellings and phonetic substitutions. Intuitively, there is mutual dependency between Chinese word segmentation and normalization, and therefore two tasks should be solved jointly. Wang and Kan (2013) prop"
D15-1211,E14-1062,1,0.939348,"ne of the major challenges for microblog processing is the issue of informal words. For example, “tmrw” has been frequently used in tweets for “tomorrow”, causing OOV problems. Text normalization has been introduced as a pre-processing step for microblog processing, which transforms informal words into their standard forms. Most work in the literature focuses on English microblog normalization, treating it as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c), and training models based on words. Lack of annotated corpora, text normalization is more challenging for Chinese. Unlike English, Chinese informal words are more difficult ∗ corresponding author to mechanically normalize for two main reasons. First, Chinese does not have word delimiters. Second, Chinese informal words manifest diversity, such as abbreviations, neologisms, unconventional spellings and phonetic substitutions. Intuitively, there is mutual dependency between Chinese word segmentation and normalization, and therefore two tasks should be solved jointly. Wang and Kan (2013) prop"
D15-1211,D14-1037,0,0.0801416,"periments can partly reflect some conclusions. Table 7 shows the results of normalization by different systems. The performance of our model is the best among the three systems. In particular, the precision in our SNT model improves upon the baselines significantly. The main reason is that our model is based on global features over whole sentences, while the two baselines based on local windows features. 7 Related Work There has much work on text normalization. The task is generally treated as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013; Sonmez and Ozgur, 2014) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c). For English, most recent work (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) uses two-step unsupervised approaches to first detect and then normalize informal words. They aim to produce and use informal/formal word lexicons and mappings. In processing Chinese informal text, Wong and Xia (2008) address the problem of informal words in bulletin board system (BBS) chats by employing pattern matching. Xia et al. (2005) also use SVM-based classification to recognize Chinese in"
D15-1211,P13-1072,0,0.0766895,"2012; Zhang et al., 2014c), and training models based on words. Lack of annotated corpora, text normalization is more challenging for Chinese. Unlike English, Chinese informal words are more difficult ∗ corresponding author to mechanically normalize for two main reasons. First, Chinese does not have word delimiters. Second, Chinese informal words manifest diversity, such as abbreviations, neologisms, unconventional spellings and phonetic substitutions. Intuitively, there is mutual dependency between Chinese word segmentation and normalization, and therefore two tasks should be solved jointly. Wang and Kan (2013) proposed a joint model to process word segmentation and informal word detection. However, text normalization was not included in the joint model. Kaji et al (2014) proposed a joint model for word segmentation, POS tagging and normalization for Japanese Microblogs, which was trained on a partially annotated microblog corpus. Their method requires special annotation for text normalization, which can be expensive. In this paper, we propose a joint model for Chinese text normalization, word-segmentation and POS tagging, which can be trained using standard segmentation and POS tagging annotation,"
D15-1211,W12-2106,0,0.0705481,"Missing"
D15-1211,I13-1015,0,0.511944,"discourse-level normalization. In this paper we focus on lexical-level normalization, which aims to transform informal words into their standard forms. Lexical normalization can be regarded as a spelling correction problem. However, researches on spelling correction focus on typographic and cognitive/orthographic errors (Kukich, 1992), while text normalization focuses on lexical variants, such as phonetic substitutions, abbreviation and paraphrases. Unlike English, for which informal words are detected according to whether they are out of vocabulary, Chinese informal words manifest diversity. Wang et al. (2013) divided informal words into three types: phonetic substitutions, abbreviations and neologisms. Li and Yarowsky (2008) classified them into four types: homophone, abbreviation, transliteration and others. Due to variant characteristics, they normalise informal words by training a model per type, leading to increased system complexity. Research reveals that most lexical variants have an unambiguous standard form (Han et al., 2012; Li and Yarowsky, 2008). The validity of this assumption is also empirically assessed on our corpus annotation in Section 6.1. Based on this assumption, we seek standa"
D15-1211,I05-3013,0,0.0401119,"ook and Stevenson, 2009; Yang and Eisenstein, 2013; Sonmez and Ozgur, 2014) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c). For English, most recent work (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) uses two-step unsupervised approaches to first detect and then normalize informal words. They aim to produce and use informal/formal word lexicons and mappings. In processing Chinese informal text, Wong and Xia (2008) address the problem of informal words in bulletin board system (BBS) chats by employing pattern matching. Xia et al. (2005) also use SVM-based classification to recognize Chinese informal sentences chats. Both methods have their advantages: the learning-based method does better on recall, while the pattern matching performs better on precision. Li and Yarowsky (2008) tackle the problem of identifying informal/formal Chinese word pairs by generating candidates from Baidu search engine and ranking using a conditional log-linear model. Zhang et al. (2014c) analyze the phenomena of mixed text in Chinese microblogs, proposing a two-stage method to normalise mixed texts. However, their models employ pipelined words segm"
D15-1211,D13-1007,0,0.151872,"f NLP tools can perform poorly on microblogs (Foster et al., 2011; Gimpel et al., 2011; Han and Baldwin, 2011). One of the major challenges for microblog processing is the issue of informal words. For example, “tmrw” has been frequently used in tweets for “tomorrow”, causing OOV problems. Text normalization has been introduced as a pre-processing step for microblog processing, which transforms informal words into their standard forms. Most work in the literature focuses on English microblog normalization, treating it as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c), and training models based on words. Lack of annotated corpora, text normalization is more challenging for Chinese. Unlike English, Chinese informal words are more difficult ∗ corresponding author to mechanically normalize for two main reasons. First, Chinese does not have word delimiters. Second, Chinese informal words manifest diversity, such as abbreviations, neologisms, unconventional spellings and phonetic substitutions. Intuitively, there is mutual dependency between Chinese word s"
D15-1211,P07-1106,1,0.905444,"to variant characteristics, they normalise informal words by training a model per type, leading to increased system complexity. Research reveals that most lexical variants have an unambiguous standard form (Han et al., 2012; Li and Yarowsky, 2008). The validity of this assumption is also empirically assessed on our corpus annotation in Section 6.1. Based on this assumption, we seek standard forms of informal words from a constructed normalization dictionary, avoiding diversity on informal words. 3 3.1 Joint Segmentation and Normalization Transition-based Segmentation We adapt the segmenter of Zhang and Clark (2007) as our baseline segmenter. Given an input sentence x, the baseline segmenter finds a segmentation by maximizing: F (x) = argmax Score(y) yGen(x) (1) where Gen(x) denotes the set of all possible segmentations for an input sentence. Zhang and Clark (2007) proposed a graphbased scoring model, with features based on complete words and word sequences. We adapt their method slightly, under a transition-based framework (Zhang and Clark, 2011), which gives us a consistent way of defining all models in this paper. Stack ... S2 S1 S0 Queue C1 ... Cn Figure 1: A state of transition-based model. Here a"
D15-1211,D10-1082,1,0.862042,"r, text normalization was not included in the joint model. Kaji et al (2014) proposed a joint model for word segmentation, POS tagging and normalization for Japanese Microblogs, which was trained on a partially annotated microblog corpus. Their method requires special annotation for text normalization, which can be expensive. In this paper, we propose a joint model for Chinese text normalization, word-segmentation and POS tagging, which can be trained using standard segmentation and POS tagging annotation, overcoming the lack of an annotated corpus on Chinese microblogs. Our model is based on Zhang and Clark (2010), with an extended set of transition actions to handle joint normalization. In our model, word segmentation and POS tagging are based on normalized text transformed from informal text. Assuming that the majority of informal words can be normalized into formal equivalents (Han et al., 2012; Li and Yarowsky, 2008), we seek standard forms of informal words from an automatically constructed normalization dictionary. To evaluate our model, we developed an annotated corpus of microblog texts. Results show that our model achieves the best performances on three tasks compared with several baseline sys"
D15-1211,W12-6307,0,\N,Missing
D15-1211,C10-2022,0,\N,Missing
D19-5723,W09-1401,0,0.112371,"Missing"
D19-5723,W13-2003,0,0.0504214,"Missing"
D19-5723,D17-1018,0,0.0327263,"al for BB-rel extraction, the entity representations Me and He are also used, given by: produced in layer 1, ..., l − 1: (l) (1) (l−1) hj = [xj ; gj , ..., gj ], (3) Each densely connected layer has L sub-layers. The dimensions of these sub-layers dhidden are decided by L and the input feature dimension d. In our model, we use dhidden = d/L. Then we use N separate dense connection layers to modify the computation of each layer as follows (for the t-th matrix A˜(t) ): gtli = ρ( n X A˜(t) Wtl hli + blt ), gm = f (gm1 , ..., gmn ), gh = f (gh1 , ..., ghn ). (7) Inspired by (Santoro et al., 2017; Lee et al., 2017), we obtained the final feature for BB-rel extraction by feeding the sequence and entity representations into a multi-layer perceptron (MLP): gf inal = M LP ([gseque ; gm ; gh ]), (4) (8) where “[]” denotes the concatenation operation. Finally, gf inal is fed into a softmax layer to compute the probability distribution over all classes. During training, our model uses the cross-entropy loss: j=1 where t = 1, ..., N and t selects the weight matrix and bias term associated with the attention guided adjacency matrix A˜(t) . The column dimension of the weight matrix increases by dhidden per (l) su"
D19-5723,D19-5719,0,0.206356,"Missing"
D19-5723,W16-3005,0,0.0328146,"Missing"
D19-5723,W13-2024,0,0.172202,"hold to 0.5. Figure 3: An example of the dependency graph and its corresponding adjacent matrix. If there is a dependency relation between the node i and j in the dependency graph, the value of the element Mij in the adjacent matrix is 1. automatically extracted lexical chains using statistical methods . Another approach (Li et al., 2017) is based on semantic word vectors. In this paper, we assume that lexical relationships can be captured by calculating the similarity of their semantic vectors. To compute similarities, we use 200dimensional pre-trained word vectors released by Pyysalo et al. (2013). Moreover, we only consider nouns for constructing the lexical chains since they usually contain relevant information. Given a sentence, we first use the Stanford CoreNLP toolkit (Manning et al., 2014) to obtain POS tags for each word. Then we pick those words whose POS tags belonging to N= (NN,NNP,NNS) as candidates for chain construction. We take one candidate at a time and check where it should be placed. Assuming that C is the set of lexical chains, we add each candidate w to C according to the following steps (Figure 2): 2.3 Dependency Graph Construction In this section, we propose an ap"
D19-5723,I17-2060,0,0.0244835,"relationship between words. The second approach (Remus and Biemann, 2013) 159 Figure 2: Process of lexical chain construction. Orange words denote nouns. C is the set of lexical chains. The similarity here refers to the cosine similarity between word vectors. We set the threshold to 0.5. Figure 3: An example of the dependency graph and its corresponding adjacent matrix. If there is a dependency relation between the node i and j in the dependency graph, the value of the element Mij in the adjacent matrix is 1. automatically extracted lexical chains using statistical methods . Another approach (Li et al., 2017) is based on semantic word vectors. In this paper, we assume that lexical relationships can be captured by calculating the similarity of their semantic vectors. To compute similarities, we use 200dimensional pre-trained word vectors released by Pyysalo et al. (2013). Moreover, we only consider nouns for constructing the lexical chains since they usually contain relevant information. Given a sentence, we first use the Stanford CoreNLP toolkit (Manning et al., 2014) to obtain POS tags for each word. Then we pick those words whose POS tags belonging to N= (NN,NNP,NNS) as candidates for chain cons"
D19-5723,H05-1091,0,0.240922,"Missing"
D19-5723,W16-3002,0,0.274602,"Missing"
D19-5723,P16-1200,0,0.0395763,"tion (Zhou et al., 2015; Miwa and Bansal, 2016). Bunescu et al. (2005) demonstrated that the relationship of an entity pair can be captured along their shortest dependency path in the dependency graph because the words on the shortest dependency path concentrate the most relevant information and diminish redundant information. Following this observation, several studies (Xu et al., 2015; Liu et al., 2015) achieved outstanding performance by combining shortest dependency paths with various neural networks. As deep learning develops, some attention-based neural architectures (Zhou et al., 2016; Lin et al., 2016) have been proposed for relation classification and show the state-of-the-art performance. But with a few exceptions, almost all related work only focused on intra-sentence relation extraction, without considering the inter-sentence relations. Recent work has explored some approaches to consider inter-sentence relations, such as Graph LSTMs (Peng et al., 2017), self-attention (Verga et al., 2018), Graph CNNs (Sahu et al., 2019). However, none of these work investigated lexical chains for inter-sentence relation extraction. In the future, we will evaluate our approach on some large-scale datase"
D19-5723,P15-2047,0,0.0204973,"ing the relations within one sentence, and ignored the relations beyond one sentence. In the NLP community, it has proven to be effective to combine linguistic features with neural networks for relation extraction (Zhou et al., 2015; Miwa and Bansal, 2016). Bunescu et al. (2005) demonstrated that the relationship of an entity pair can be captured along their shortest dependency path in the dependency graph because the words on the shortest dependency path concentrate the most relevant information and diminish redundant information. Following this observation, several studies (Xu et al., 2015; Liu et al., 2015) achieved outstanding performance by combining shortest dependency paths with various neural networks. As deep learning develops, some attention-based neural architectures (Zhou et al., 2016; Lin et al., 2016) have been proposed for relation classification and show the state-of-the-art performance. But with a few exceptions, almost all related work only focused on intra-sentence relation extraction, without considering the inter-sentence relations. Recent work has explored some approaches to consider inter-sentence relations, such as Graph LSTMs (Peng et al., 2017), self-attention (Verga et al"
D19-5723,P14-5010,0,0.0127244,"y incorporating dependency graphs and lexical chains into the neural network. As shown in Figure 1, intersentence relations are usually expressed in interrelated sentences, and these sentences may contain semantically-related words which can form lexical chains. We utilize these lexical chains and dependency graphs to build an inter-sentence dependency graph for inter-sentence relation extraction. Specifically, we utilize word embedding to find the semantic relationships of words that occur in different sentences for building reliable lexical chains. Then, we use the Stanford CoreNLP toolkit (Manning et al., 2014) to obtain sentence-level dependency and part-of-speech (POS) information, and build an inter-sentence dependency graph based on these information and lexical chains. 2.1 Relation Candidate Generation In the BB-rel dataset, if all candidate pairs (bacteria and habitat or phenotype) that occur in the document are enlisted as candidate training examples, the positive and negative examples will become very unbalanced because most entity pairs located beyond one sentence do not have any relation. Based on our observations, most entity pairs spanning more than two sentences have no relations betwee"
D19-5723,P19-1024,0,0.0301125,"Missing"
D19-5723,W17-4813,0,0.0138505,"onships Intra-sentence relationships Inter-sentence relationships Train 715 281 996 885 111 Dev 395 138 533 467 66 Table 1: BB-rel data statistics on the training and development set. A lexical chain (Morris and Hirst, 1991) is a sequence of words which are semantically-similar or related. These words are related sequentially in the text, defining the topic of the text segment that they cover and establishing associations between sentences. Following this observation, some researchers have obtained success in many NLP tasks such as word sense induction(Tao et al., 2014) , machine translation (Mascarell, 2017) and text (Stokes et al., 2004) segmentation. In the BB-rel dataset, the sentences where inter-sentence relations occur usually express the same topic or have semantic associations each other. These features usually appear as some related words which can form lexical chains. Following this observation, we propose a novel approach to build an inter-sentence dependency graph based on lexical chains. tures are fed into a multi-layer perceptron (MLP) to classify the relation between an entity pair. Our approach has two advantages. First, it is capable of extracting both intra-sentence and intersen"
D19-5723,Q19-1019,0,0.114479,"the graph convolution of each layer, each node gathers all the information of its neighboring nodes in the graph. After the L-layer graph convolution operation, we transform the original dependency graph into a fully connected edge-weighted graph by constructing N (N is a hyper-parameter) attentionguided adjacency matrix. Each attention-guided adjacency matrix A˜ corresponds to a completely connected graph. In this paper, we use the multi← concatenated, formalized as hi = [hi hi ]. 2.4.2 (1) Attention-Guided GCNN Layer We employ the attention-guided graph convolutional neural network (AGCNN) (Guo et al., 2019a) to incorporate the dependency information into word representations, which is composed of M identical blocks. Each block has three types of layers: attention-guided layer, densely connected layer, linear combination layer. 161 2.4.3 Output Layer We treat the BB-rel task as a classification task. S = [s1 , ..., sn ] denotes a sequence, si is the ith token, Me and He denote Microorganism and Habitat or Phenotype entities. The entities may consist of several tokens, namely [se1 , ..., sen ] and [sh1 , ..., shn ]. The goal of the BB-rel task is to predict whether there is a ”Live in” or ”Exhibi"
D19-5723,W16-3009,0,0.037259,"Missing"
D19-5723,P16-1105,0,0.0280212,", the Major Projects of the National Social Science Foundation of China (No. 11&ZD189), the National Key Research, Development Program of China (No. 2017YFC1200500). In the natural language processing community, there are a number of related competitions and tasks (Wei et al., 2015; N´edellec et al., 2013; Del´eger et al., 2016). Most prior work focused on extracting the relations within one sentence, and ignored the relations beyond one sentence. In the NLP community, it has proven to be effective to combine linguistic features with neural networks for relation extraction (Zhou et al., 2015; Miwa and Bansal, 2016). Bunescu et al. (2005) demonstrated that the relationship of an entity pair can be captured along their shortest dependency path in the dependency graph because the words on the shortest dependency path concentrate the most relevant information and diminish redundant information. Following this observation, several studies (Xu et al., 2015; Liu et al., 2015) achieved outstanding performance by combining shortest dependency paths with various neural networks. As deep learning develops, some attention-based neural architectures (Zhou et al., 2016; Lin et al., 2016) have been proposed for relati"
D19-5723,J91-1002,0,0.842411,", not all the relations occur between two entities with the same sentence. In the preprocessing step, we found that there exist about one fourth of all relations whose argument entities are located in different sentences. Therefore, we need to build a model that does not only consider the entity relationship within one sentence, but also beyond the sentence boundary. Lives In Exhibits Total relatonships Intra-sentence relationships Inter-sentence relationships Train 715 281 996 885 111 Dev 395 138 533 467 66 Table 1: BB-rel data statistics on the training and development set. A lexical chain (Morris and Hirst, 1991) is a sequence of words which are semantically-similar or related. These words are related sequentially in the text, defining the topic of the text segment that they cover and establishing associations between sentences. Following this observation, some researchers have obtained success in many NLP tasks such as word sense induction(Tao et al., 2014) , machine translation (Mascarell, 2017) and text (Stokes et al., 2004) segmentation. In the BB-rel dataset, the sentences where inter-sentence relations occur usually express the same topic or have semantic associations each other. These features"
D19-5723,W13-2001,0,0.0653393,"Missing"
D19-5723,W11-1801,0,0.0799302,"Missing"
D19-5723,D15-1206,0,0.0304605,"ocused on extracting the relations within one sentence, and ignored the relations beyond one sentence. In the NLP community, it has proven to be effective to combine linguistic features with neural networks for relation extraction (Zhou et al., 2015; Miwa and Bansal, 2016). Bunescu et al. (2005) demonstrated that the relationship of an entity pair can be captured along their shortest dependency path in the dependency graph because the words on the shortest dependency path concentrate the most relevant information and diminish redundant information. Following this observation, several studies (Xu et al., 2015; Liu et al., 2015) achieved outstanding performance by combining shortest dependency paths with various neural networks. As deep learning develops, some attention-based neural architectures (Zhou et al., 2016; Lin et al., 2016) have been proposed for relation classification and show the state-of-the-art performance. But with a few exceptions, almost all related work only focused on intra-sentence relation extraction, without considering the inter-sentence relations. Recent work has explored some approaches to consider inter-sentence relations, such as Graph LSTMs (Peng et al., 2017), self-att"
D19-5723,Q17-1008,0,0.0191942,"ral studies (Xu et al., 2015; Liu et al., 2015) achieved outstanding performance by combining shortest dependency paths with various neural networks. As deep learning develops, some attention-based neural architectures (Zhou et al., 2016; Lin et al., 2016) have been proposed for relation classification and show the state-of-the-art performance. But with a few exceptions, almost all related work only focused on intra-sentence relation extraction, without considering the inter-sentence relations. Recent work has explored some approaches to consider inter-sentence relations, such as Graph LSTMs (Peng et al., 2017), self-attention (Verga et al., 2018), Graph CNNs (Sahu et al., 2019). However, none of these work investigated lexical chains for inter-sentence relation extraction. In the future, we will evaluate our approach on some large-scale datasets for intra- and inter-sentence relation extraction (Yao et al., 2019). 5 Conclusion In this paper, we describe our approach used for participating the Bacteria Biotope task at BioNLPOST 2019. Our approach achieved very competitive performance in the official evaluation. We found that the idea using lexical chains to build inter-sentence dependency graphs is"
D19-5723,P19-1074,0,0.0225255,"ow the state-of-the-art performance. But with a few exceptions, almost all related work only focused on intra-sentence relation extraction, without considering the inter-sentence relations. Recent work has explored some approaches to consider inter-sentence relations, such as Graph LSTMs (Peng et al., 2017), self-attention (Verga et al., 2018), Graph CNNs (Sahu et al., 2019). However, none of these work investigated lexical chains for inter-sentence relation extraction. In the future, we will evaluate our approach on some large-scale datasets for intra- and inter-sentence relation extraction (Yao et al., 2019). 5 Conclusion In this paper, we describe our approach used for participating the Bacteria Biotope task at BioNLPOST 2019. Our approach achieved very competitive performance in the official evaluation. We found that the idea using lexical chains to build inter-sentence dependency graphs is effective. Moreover, ensemble training and inference can improve the performance of our model. The attention-guided graph convolution neural network performs well in extracting Bacteria Biotope relations. However, our approach is not specific to Bacteria Biotope relation extraction, and it can be applied to"
D19-5723,W11-1815,0,0.0601843,"Missing"
D19-5723,D18-1244,0,0.0144812,"j in the dependency graph, the element Mij in the adjacent matrix is assigned with 1. • Step 3: if the current candidate w cannot be attached to any existing lexical chain, we will create a new lexical chain for it. 160 Figure 4: The architecture of our model. The input sentence is “MRSA were isolated by oxacillin screening agar” with a Microorganism entity “MRSA” and a Habitat entity “oxacillin screening agar”. M denotes the adjacency matrix. 2.4 2.4.1 Neural Network Model In the attention guided layer, we first update the representation of the node using a graph convolution network (GCNN) (Zhang et al., 2018). For an L-layer GCNN, we denotes the inputs in the (0) (0) first layer as g1 , ..., gn and the outputs in the last (L) (L) (l) layer as g1 , ..., gn . The gi denotes the output vectors of the node i in the l-th layer. The convolution operation in the l-th layer can be written as: BiLSTM Layer Figure 4 shows the neural network architecture of our model. It uses the words and POS tags as input. We adopt the 200-dimensional word embeddings and 20-dimensional POS tag embeddings. The final representation for the token is the concatenation xi of the word embedding si and the POS tag embedding pi ."
D19-5723,P15-1117,0,0.0302328,"hina (No. 61772378), the Major Projects of the National Social Science Foundation of China (No. 11&ZD189), the National Key Research, Development Program of China (No. 2017YFC1200500). In the natural language processing community, there are a number of related competitions and tasks (Wei et al., 2015; N´edellec et al., 2013; Del´eger et al., 2016). Most prior work focused on extracting the relations within one sentence, and ignored the relations beyond one sentence. In the NLP community, it has proven to be effective to combine linguistic features with neural networks for relation extraction (Zhou et al., 2015; Miwa and Bansal, 2016). Bunescu et al. (2005) demonstrated that the relationship of an entity pair can be captured along their shortest dependency path in the dependency graph because the words on the shortest dependency path concentrate the most relevant information and diminish redundant information. Following this observation, several studies (Xu et al., 2015; Liu et al., 2015) achieved outstanding performance by combining shortest dependency paths with various neural networks. As deep learning develops, some attention-based neural architectures (Zhou et al., 2016; Lin et al., 2016) have"
D19-5723,N13-1119,0,0.0120578,"in two sentences as the candidates to generate training examples. The statistics of our dataset are summarized in Table 1. 2.2 After that, we employ a neural network model which consists of the bidirectional long shortterm memories and attention-guided graph convolutional neural networks to extract features from the inter-sentence dependency graph. The feaLexical Chain Construction In previous work, there are mainly three approaches for constructing lexical chains. The first one utilized WordNet (Hirst and St-Onge, 1997) to capture the semantic relationship between words. The second approach (Remus and Biemann, 2013) 159 Figure 2: Process of lexical chain construction. Orange words denote nouns. C is the set of lexical chains. The similarity here refers to the cosine similarity between word vectors. We set the threshold to 0.5. Figure 3: An example of the dependency graph and its corresponding adjacent matrix. If there is a dependency relation between the node i and j in the dependency graph, the value of the element Mij in the adjacent matrix is 1. automatically extracted lexical chains using statistical methods . Another approach (Li et al., 2017) is based on semantic word vectors. In this paper, we ass"
D19-5723,P16-2034,0,0.0298244,"for relation extraction (Zhou et al., 2015; Miwa and Bansal, 2016). Bunescu et al. (2005) demonstrated that the relationship of an entity pair can be captured along their shortest dependency path in the dependency graph because the words on the shortest dependency path concentrate the most relevant information and diminish redundant information. Following this observation, several studies (Xu et al., 2015; Liu et al., 2015) achieved outstanding performance by combining shortest dependency paths with various neural networks. As deep learning develops, some attention-based neural architectures (Zhou et al., 2016; Lin et al., 2016) have been proposed for relation classification and show the state-of-the-art performance. But with a few exceptions, almost all related work only focused on intra-sentence relation extraction, without considering the inter-sentence relations. Recent work has explored some approaches to consider inter-sentence relations, such as Graph LSTMs (Peng et al., 2017), self-attention (Verga et al., 2018), Graph CNNs (Sahu et al., 2019). However, none of these work investigated lexical chains for inter-sentence relation extraction. In the future, we will evaluate our approach on some"
D19-5723,P19-1423,0,0.0390426,"performance by combining shortest dependency paths with various neural networks. As deep learning develops, some attention-based neural architectures (Zhou et al., 2016; Lin et al., 2016) have been proposed for relation classification and show the state-of-the-art performance. But with a few exceptions, almost all related work only focused on intra-sentence relation extraction, without considering the inter-sentence relations. Recent work has explored some approaches to consider inter-sentence relations, such as Graph LSTMs (Peng et al., 2017), self-attention (Verga et al., 2018), Graph CNNs (Sahu et al., 2019). However, none of these work investigated lexical chains for inter-sentence relation extraction. In the future, we will evaluate our approach on some large-scale datasets for intra- and inter-sentence relation extraction (Yao et al., 2019). 5 Conclusion In this paper, we describe our approach used for participating the Bacteria Biotope task at BioNLPOST 2019. Our approach achieved very competitive performance in the official evaluation. We found that the idea using lexical chains to build inter-sentence dependency graphs is effective. Moreover, ensemble training and inference can improve the"
D19-5723,C14-1152,1,0.851498,"e boundary. Lives In Exhibits Total relatonships Intra-sentence relationships Inter-sentence relationships Train 715 281 996 885 111 Dev 395 138 533 467 66 Table 1: BB-rel data statistics on the training and development set. A lexical chain (Morris and Hirst, 1991) is a sequence of words which are semantically-similar or related. These words are related sequentially in the text, defining the topic of the text segment that they cover and establishing associations between sentences. Following this observation, some researchers have obtained success in many NLP tasks such as word sense induction(Tao et al., 2014) , machine translation (Mascarell, 2017) and text (Stokes et al., 2004) segmentation. In the BB-rel dataset, the sentences where inter-sentence relations occur usually express the same topic or have semantic associations each other. These features usually appear as some related words which can form lexical chains. Following this observation, we propose a novel approach to build an inter-sentence dependency graph based on lexical chains. tures are fed into a multi-layer perceptron (MLP) to classify the relation between an entity pair. Our approach has two advantages. First, it is capable of ext"
D19-5723,N18-1080,0,0.0144417,"al., 2015) achieved outstanding performance by combining shortest dependency paths with various neural networks. As deep learning develops, some attention-based neural architectures (Zhou et al., 2016; Lin et al., 2016) have been proposed for relation classification and show the state-of-the-art performance. But with a few exceptions, almost all related work only focused on intra-sentence relation extraction, without considering the inter-sentence relations. Recent work has explored some approaches to consider inter-sentence relations, such as Graph LSTMs (Peng et al., 2017), self-attention (Verga et al., 2018), Graph CNNs (Sahu et al., 2019). However, none of these work investigated lexical chains for inter-sentence relation extraction. In the future, we will evaluate our approach on some large-scale datasets for intra- and inter-sentence relation extraction (Yao et al., 2019). 5 Conclusion In this paper, we describe our approach used for participating the Bacteria Biotope task at BioNLPOST 2019. Our approach achieved very competitive performance in the official evaluation. We found that the idea using lexical chains to build inter-sentence dependency graphs is effective. Moreover, ensemble trainin"
H05-1114,C02-1039,0,0.0170041,"learning algorithms (Leacock et al., 1998; Towel and Voorheest, 1998), weakly supervised learning algorithms (Dagan and Itai, 1994; Li and Li, 2004; Mihalcea, 2004; Niu et al., 2005; Park et al., 2000; Standard dimentionality reduction techniques include (1) supervised feature selection and supervised feature clustering when given labeled data, (2) unsupervised feature selection, latent semantic indexing, and unsupervised feature clustering when only unlabeled data is available. Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision tree over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002), but feature selection does not improve SVM and Adaboost over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002) for word sense disambiguation. Latent semantic indexing (LSI) studied in (Sch¨ utze, 1998) improves the performance of sense discrimination, while unsupervised feature selection also improves the performance of word sense discrimination (Niu et al., 2004). But little work is done on using feature clustering to conduct dimensionality reduction for WSD. This paper will describe an application of feat"
H05-1114,W04-2405,0,0.0123837,"ace requires large amount of memory and CPU time, which limits the scalability of WSD model to very large datasets or incorporation of WSD model into natural language processing systems. Introduction This paper deals with word sense disambiguation (WSD) problem, which is to assign an appropriate sense to an occurrence of a word in a given context. Many corpus based statistical methods have been proposed to solve this problem, including supervised learning algorithms (Leacock et al., 1998; Towel and Voorheest, 1998), weakly supervised learning algorithms (Dagan and Itai, 1994; Li and Li, 2004; Mihalcea, 2004; Niu et al., 2005; Park et al., 2000; Standard dimentionality reduction techniques include (1) supervised feature selection and supervised feature clustering when given labeled data, (2) unsupervised feature selection, latent semantic indexing, and unsupervised feature clustering when only unlabeled data is available. Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision tree over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002), but feature selection does not improve SVM and Adaboost over"
H05-1114,J94-4003,0,0.0118157,"ta lying in high-dimensional feature space requires large amount of memory and CPU time, which limits the scalability of WSD model to very large datasets or incorporation of WSD model into natural language processing systems. Introduction This paper deals with word sense disambiguation (WSD) problem, which is to assign an appropriate sense to an occurrence of a word in a given context. Many corpus based statistical methods have been proposed to solve this problem, including supervised learning algorithms (Leacock et al., 1998; Towel and Voorheest, 1998), weakly supervised learning algorithms (Dagan and Itai, 1994; Li and Li, 2004; Mihalcea, 2004; Niu et al., 2005; Park et al., 2000; Standard dimentionality reduction techniques include (1) supervised feature selection and supervised feature clustering when given labeled data, (2) unsupervised feature selection, latent semantic indexing, and unsupervised feature clustering when only unlabeled data is available. Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision tree over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002), but feature selection does n"
H05-1114,P04-1080,1,0.843703,"nly unlabeled data is available. Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision tree over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002), but feature selection does not improve SVM and Adaboost over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002) for word sense disambiguation. Latent semantic indexing (LSI) studied in (Sch¨ utze, 1998) improves the performance of sense discrimination, while unsupervised feature selection also improves the performance of word sense discrimination (Niu et al., 2004). But little work is done on using feature clustering to conduct dimensionality reduction for WSD. This paper will describe an application of feature 907 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 907–914, Vancouver, October 2005. 2005 Association for Computational Linguistics clustering technique to WSD task. Feature clustering has been extensively studied for the benefit of text categorization and document clustering. In the context of text categorization, supervised feature clustering algorithms"
H05-1114,J98-1006,0,0.025447,"which is far from optimal for many classification algorithms. Furthermore, processing data lying in high-dimensional feature space requires large amount of memory and CPU time, which limits the scalability of WSD model to very large datasets or incorporation of WSD model into natural language processing systems. Introduction This paper deals with word sense disambiguation (WSD) problem, which is to assign an appropriate sense to an occurrence of a word in a given context. Many corpus based statistical methods have been proposed to solve this problem, including supervised learning algorithms (Leacock et al., 1998; Towel and Voorheest, 1998), weakly supervised learning algorithms (Dagan and Itai, 1994; Li and Li, 2004; Mihalcea, 2004; Niu et al., 2005; Park et al., 2000; Standard dimentionality reduction techniques include (1) supervised feature selection and supervised feature clustering when given labeled data, (2) unsupervised feature selection, latent semantic indexing, and unsupervised feature clustering when only unlabeled data is available. Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision"
H05-1114,W02-1006,0,0.487307,"vised learning algorithms (Dagan and Itai, 1994; Li and Li, 2004; Mihalcea, 2004; Niu et al., 2005; Park et al., 2000; Standard dimentionality reduction techniques include (1) supervised feature selection and supervised feature clustering when given labeled data, (2) unsupervised feature selection, latent semantic indexing, and unsupervised feature clustering when only unlabeled data is available. Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision tree over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002), but feature selection does not improve SVM and Adaboost over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002) for word sense disambiguation. Latent semantic indexing (LSI) studied in (Sch¨ utze, 1998) improves the performance of sense discrimination, while unsupervised feature selection also improves the performance of word sense discrimination (Niu et al., 2004). But little work is done on using feature clustering to conduct dimensionality reduction for WSD. This paper will describe an application of feature 907 Proceedings of Human Language Technology Conference and Conference on Empirica"
H05-1114,J04-1001,0,0.0119965,"sional feature space requires large amount of memory and CPU time, which limits the scalability of WSD model to very large datasets or incorporation of WSD model into natural language processing systems. Introduction This paper deals with word sense disambiguation (WSD) problem, which is to assign an appropriate sense to an occurrence of a word in a given context. Many corpus based statistical methods have been proposed to solve this problem, including supervised learning algorithms (Leacock et al., 1998; Towel and Voorheest, 1998), weakly supervised learning algorithms (Dagan and Itai, 1994; Li and Li, 2004; Mihalcea, 2004; Niu et al., 2005; Park et al., 2000; Standard dimentionality reduction techniques include (1) supervised feature selection and supervised feature clustering when given labeled data, (2) unsupervised feature selection, latent semantic indexing, and unsupervised feature clustering when only unlabeled data is available. Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision tree over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002), but feature selection does not improve SVM an"
H05-1114,P04-1036,0,0.0890695,"Missing"
H05-1114,P05-1049,1,0.707284,"ge amount of memory and CPU time, which limits the scalability of WSD model to very large datasets or incorporation of WSD model into natural language processing systems. Introduction This paper deals with word sense disambiguation (WSD) problem, which is to assign an appropriate sense to an occurrence of a word in a given context. Many corpus based statistical methods have been proposed to solve this problem, including supervised learning algorithms (Leacock et al., 1998; Towel and Voorheest, 1998), weakly supervised learning algorithms (Dagan and Itai, 1994; Li and Li, 2004; Mihalcea, 2004; Niu et al., 2005; Park et al., 2000; Standard dimentionality reduction techniques include (1) supervised feature selection and supervised feature clustering when given labeled data, (2) unsupervised feature selection, latent semantic indexing, and unsupervised feature clustering when only unlabeled data is available. Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision tree over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002), but feature selection does not improve SVM and Adaboost over SENSEVAL-1 and SEN"
H05-1114,W04-0849,0,0.0440224,"Missing"
H05-1114,P00-1069,0,0.0198546,"y and CPU time, which limits the scalability of WSD model to very large datasets or incorporation of WSD model into natural language processing systems. Introduction This paper deals with word sense disambiguation (WSD) problem, which is to assign an appropriate sense to an occurrence of a word in a given context. Many corpus based statistical methods have been proposed to solve this problem, including supervised learning algorithms (Leacock et al., 1998; Towel and Voorheest, 1998), weakly supervised learning algorithms (Dagan and Itai, 1994; Li and Li, 2004; Mihalcea, 2004; Niu et al., 2005; Park et al., 2000; Standard dimentionality reduction techniques include (1) supervised feature selection and supervised feature clustering when given labeled data, (2) unsupervised feature selection, latent semantic indexing, and unsupervised feature clustering when only unlabeled data is available. Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision tree over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002), but feature selection does not improve SVM and Adaboost over SENSEVAL-1 and SENSEVAL-2 data (Lee a"
H05-1114,W97-0322,0,0.05447,"Missing"
H05-1114,J98-1004,0,0.170863,"Missing"
H05-1114,J98-1005,0,0.0258029,"imal for many classification algorithms. Furthermore, processing data lying in high-dimensional feature space requires large amount of memory and CPU time, which limits the scalability of WSD model to very large datasets or incorporation of WSD model into natural language processing systems. Introduction This paper deals with word sense disambiguation (WSD) problem, which is to assign an appropriate sense to an occurrence of a word in a given context. Many corpus based statistical methods have been proposed to solve this problem, including supervised learning algorithms (Leacock et al., 1998; Towel and Voorheest, 1998), weakly supervised learning algorithms (Dagan and Itai, 1994; Li and Li, 2004; Mihalcea, 2004; Niu et al., 2005; Park et al., 2000; Standard dimentionality reduction techniques include (1) supervised feature selection and supervised feature clustering when given labeled data, (2) unsupervised feature selection, latent semantic indexing, and unsupervised feature clustering when only unlabeled data is available. Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision tree over SENSEVAL-1 and SEN"
H05-1114,P95-1026,0,0.197927,"Missing"
H05-1114,W04-0807,0,\N,Missing
I05-1035,P04-1053,0,\N,Missing
I05-1035,P97-1009,0,\N,Missing
I05-1035,P03-1029,0,\N,Missing
I05-1035,W02-1010,0,\N,Missing
I05-1035,A00-1039,0,\N,Missing
I05-2045,P03-1029,0,0.0175592,"Missing"
I05-2045,P97-1009,0,0.009886,"ˆ= the mapping procedure can be formulated as:Ω P|T C| arg maxΩ j=1 tΩ(j),j , where Ω(j) is the index of the estimated cluster associated with the j-th class. Given the result of one-to-one mapping, we can define the evaluation measure as follows: P tΩ(j),j ˆ j Accuracy(P ) = P t . Intuitively, it reflects i,j i,j the accuracy of the clustering result. 3.3 Evaluation method for relation labelling For evaluation of the relation labeling, we need to explore the relatedness between the identified labels and the pre-defined relation names. To do this, we use one information-content based measure (Lin, 1997), which is provided in WordnetSimilarity package (Pedersen et al., 2004) to evaluate the similarity between two concepts in Wordnet. Intuitively, the relatedness between two concepts in Wordnet is captured by the information content of their lowest common subsumer (lcs) and the information content of the two concepts themselves , which can be formalized as follows: 1 ,c2 )) Relatednesslin (c1 , c2 ) = 2×IC(lcs(c IC(c1 )+IC(c2 ) . This measure depends upon the corpus to estimate information content. We carried out the experiments using the British National Corpus (BNC) as the source of informat"
I05-2045,N04-3012,0,0.0415364,"axΩ j=1 tΩ(j),j , where Ω(j) is the index of the estimated cluster associated with the j-th class. Given the result of one-to-one mapping, we can define the evaluation measure as follows: P tΩ(j),j ˆ j Accuracy(P ) = P t . Intuitively, it reflects i,j i,j the accuracy of the clustering result. 3.3 Evaluation method for relation labelling For evaluation of the relation labeling, we need to explore the relatedness between the identified labels and the pre-defined relation names. To do this, we use one information-content based measure (Lin, 1997), which is provided in WordnetSimilarity package (Pedersen et al., 2004) to evaluate the similarity between two concepts in Wordnet. Intuitively, the relatedness between two concepts in Wordnet is captured by the information content of their lowest common subsumer (lcs) and the information content of the two concepts themselves , which can be formalized as follows: 1 ,c2 )) Relatednesslin (c1 , c2 ) = 2×IC(lcs(c IC(c1 )+IC(c2 ) . This measure depends upon the corpus to estimate information content. We carried out the experiments using the British National Corpus (BNC) as the source of information content. 3.4 Experiments and Results For comparison of the effect of"
I05-2045,P04-1053,0,\N,Missing
I08-2103,H01-1065,0,0.424524,"Missing"
I08-2103,P03-1069,0,0.63859,"ased on the order of themes. The idea of this method is reasonable since the summary of multi-documents usually covers several topics in source documents to achieve representative, and the theme ordering can suggest sentence ordering somehow. However, there are two challenges for this method. One is how to cluster sentences into topics, and the other is how to order sentences belonging to the same topic. Barzilay et al. (2002) combined topic relatedness and chronological ordering together to order sentences. Besides chronological ordering, Probabilistic model was also used to order sentences. Lapata (2003) ordered sentences based on conditional probabilities of sentence pairs. The conditional probabilities of sentence pairs were learned from a training corpus. With conditional probability of each sentence pairs, the approximate optimal global ordering was achieved with a simple greedy algorithm. The conditional probability of a pair of sentences was calculated by conditional probability of feature pairs occurring in the two sentences. The experiment results show that it gets significant improvement compared with randomly sentence ranking. Bollegala et al. (2005) combined chronological ordering,"
I08-2103,C04-1129,0,0.0752962,"Missing"
I08-2103,I05-1055,0,\N,Missing
I11-1054,W04-3247,0,0.345155,"ures, while the latter usually makes use of advanced natural language understanding or generation technique to fuse, compress or reformulate information. In this paper, we focus on the extractive approach. Much work has been done on extractive summarization including classification-based methods (Conroy and O&apos;leary, 2001), regressionbased methods (You et al., 2011), NMF-based methods (Lee et al., 2009), MMR-based methods (Carbonell and Goldstein, 1998), clusteringbased methods (Nomoto and Matsumoto, 2001), etc. Recently, graph-based ranking methods are becoming more and more popular. LexRank (Erkan and Radev, 2004), TextRank (Mihalcea and Tarau, 2004), mutual reinforcement based ranking (Zha, 2002), and manifold ranking (Wan et al., 2007) are such methods using algorithms similar to PageRank and HITS to compute sentence&apos;s significance. When summarizing a specified document, most methods employ only the information contained in the document while ignore its context. One exception is the collaborative approach proposed by Wan and Yang (2007), which improves news document summarization by use of the content from neighboring documents. This motivates us to further consider how social context knowledge might"
I11-1054,P10-1057,0,0.0695541,"ost methods employ only the information contained in the document while ignore its context. One exception is the collaborative approach proposed by Wan and Yang (2007), which improves news document summarization by use of the content from neighboring documents. This motivates us to further consider how social context knowledge might be incorporated in the sentence evaluation process to improve the performance of traditional summarization systems. To date, much work on summarization tends to focus on a specific type of document, such as news articles (McKeown and Radev, 1995), academic papers (Qazvinian and Radev, 2010) or medical records (Afantenos et al., 2005). With the rapid growth of documents over the Internet, a large number of web documents need to be summarized. However, the content contained in a web document is observed to be sparser and noisier, and it is difficult for the traditional summarization methods that only focus on the local content of a document to capture the true meaning of web documents in a richer context environment. So it is more reasonable to summarize the web document by taking advantage of its social context (i.e. document context and user context). Relevant work on web docume"
I11-1054,W03-0502,0,0.0786377,"Missing"
I11-1054,W04-3252,0,\N,Missing
I11-1054,N04-1019,0,\N,Missing
I11-1054,N03-1020,0,\N,Missing
ji-etal-2004-building,J93-2004,0,\N,Missing
ji-etal-2004-building,C00-2099,0,\N,Missing
ji-etal-2004-building,C96-1020,0,\N,Missing
L16-1138,P14-2131,0,0.0464055,"Missing"
L16-1138,W06-0130,0,0.242518,"mentation errors can affect the quality, it has been shown that such word embeddings can improve Chinese NLP tasks such as parsing (Wu et al., 2013). On the other hand, character information can also be used for NLP (Zhang et al., 2014a). Zhang et al. (2013) show that parsing can be improved by taking characters as inputs, jointly performing segmentation and syntax parsing. They show that character features can significantly improve the accuracies. Character features are also central to other Chinese NLP tasks such as character-based segmentation (Xue, 2003) and named entity recognition (NER (Chen et al., 2006)). It is an interesting research question whether character embeddings can be useful for Chinese NLP. However, relatively little work has been reported on embedding Chinese characters, and limited effect has been observed (Sun et al., 2014). One possible reason is that the number of characters is much smaller compared to the number of words (104 vs 106 ), and the effect of character embedding on reducing sparsity can be limited. Also due to this reason, it has been shown that unsupervised clustering of characters does not improve in-domain segmentation (Liang, 2005). Another possible reason, h"
L16-1138,D14-1110,0,0.0161911,"del significantly. 2. Multi-prototype Character Embedding Most Chinese characters have multiple senses, but take only one sense in a given context. The task of multi-prototype character embedding is to find a continuous vector representation for each Chinese character sense. We develop a multi-prototype word model by adapting an existing model for word embedding in the literature, making significant changes for the character embedding task. There has been a line of work in training multi-prototype word embeddings (Huang et al., 2012; Guo et al., 2014a; Frey and Dueck, 2007; Tian et al., 2014; Chen et al., 2014). We adapt the Multi-Sense Skip-gram (MSSG) model (Neelakantan et al., 2014), which is based on the Skip-gram model (Mikolov et al., 2013). The Skip-gram model works by predicting a word vector using its context. To accommodate for multiple word senses, the MSSG model works by predicting the sense of the current word given its context, and then updating the current word sense vector and context vector. In particular, each word w is associated with two types of vector representations, one being the output embedding v(w), and the other being the context v(w). Given a special context, the sense o"
L16-1138,C14-1048,0,0.0186181,"mbeddings improves the accuracy of a state-of-the-art CRF model significantly. 2. Multi-prototype Character Embedding Most Chinese characters have multiple senses, but take only one sense in a given context. The task of multi-prototype character embedding is to find a continuous vector representation for each Chinese character sense. We develop a multi-prototype word model by adapting an existing model for word embedding in the literature, making significant changes for the character embedding task. There has been a line of work in training multi-prototype word embeddings (Huang et al., 2012; Guo et al., 2014a; Frey and Dueck, 2007; Tian et al., 2014; Chen et al., 2014). We adapt the Multi-Sense Skip-gram (MSSG) model (Neelakantan et al., 2014), which is based on the Skip-gram model (Mikolov et al., 2013). The Skip-gram model works by predicting a word vector using its context. To accommodate for multiple word senses, the MSSG model works by predicting the sense of the current word given its context, and then updating the current word sense vector and context vector. In particular, each word w is associated with two types of vector representations, one being the output embedding v(w), and the othe"
L16-1138,D14-1012,0,0.0185787,"mbeddings improves the accuracy of a state-of-the-art CRF model significantly. 2. Multi-prototype Character Embedding Most Chinese characters have multiple senses, but take only one sense in a given context. The task of multi-prototype character embedding is to find a continuous vector representation for each Chinese character sense. We develop a multi-prototype word model by adapting an existing model for word embedding in the literature, making significant changes for the character embedding task. There has been a line of work in training multi-prototype word embeddings (Huang et al., 2012; Guo et al., 2014a; Frey and Dueck, 2007; Tian et al., 2014; Chen et al., 2014). We adapt the Multi-Sense Skip-gram (MSSG) model (Neelakantan et al., 2014), which is based on the Skip-gram model (Mikolov et al., 2013). The Skip-gram model works by predicting a word vector using its context. To accommodate for multiple word senses, the MSSG model works by predicting the sense of the current word given its context, and then updating the current word sense vector and context vector. In particular, each word w is associated with two types of vector representations, one being the output embedding v(w), and the othe"
L16-1138,P12-1092,0,0.0522135,"ER task, character embeddings improves the accuracy of a state-of-the-art CRF model significantly. 2. Multi-prototype Character Embedding Most Chinese characters have multiple senses, but take only one sense in a given context. The task of multi-prototype character embedding is to find a continuous vector representation for each Chinese character sense. We develop a multi-prototype word model by adapting an existing model for word embedding in the literature, making significant changes for the character embedding task. There has been a line of work in training multi-prototype word embeddings (Huang et al., 2012; Guo et al., 2014a; Frey and Dueck, 2007; Tian et al., 2014; Chen et al., 2014). We adapt the Multi-Sense Skip-gram (MSSG) model (Neelakantan et al., 2014), which is based on the Skip-gram model (Mikolov et al., 2013). The Skip-gram model works by predicting a word vector using its context. To accommodate for multiple word senses, the MSSG model works by predicting the sense of the current word given its context, and then updating the current word sense vector and context vector. In particular, each word w is associated with two types of vector representations, one being the output embedding"
L16-1138,P14-1062,0,0.0177772,"Missing"
L16-1138,D14-1113,0,0.0223256,"e characters have multiple senses, but take only one sense in a given context. The task of multi-prototype character embedding is to find a continuous vector representation for each Chinese character sense. We develop a multi-prototype word model by adapting an existing model for word embedding in the literature, making significant changes for the character embedding task. There has been a line of work in training multi-prototype word embeddings (Huang et al., 2012; Guo et al., 2014a; Frey and Dueck, 2007; Tian et al., 2014; Chen et al., 2014). We adapt the Multi-Sense Skip-gram (MSSG) model (Neelakantan et al., 2014), which is based on the Skip-gram model (Mikolov et al., 2013). The Skip-gram model works by predicting a word vector using its context. To accommodate for multiple word senses, the MSSG model works by predicting the sense of the current word given its context, and then updating the current word sense vector and context vector. In particular, each word w is associated with two types of vector representations, one being the output embedding v(w), and the other being the context v(w). Given a special context, the sense of a word is predicted as the center 855 top K one fix cilin one pos fix pos"
L16-1138,C14-1016,0,0.0204472,"e-of-the-art CRF model significantly. 2. Multi-prototype Character Embedding Most Chinese characters have multiple senses, but take only one sense in a given context. The task of multi-prototype character embedding is to find a continuous vector representation for each Chinese character sense. We develop a multi-prototype word model by adapting an existing model for word embedding in the literature, making significant changes for the character embedding task. There has been a line of work in training multi-prototype word embeddings (Huang et al., 2012; Guo et al., 2014a; Frey and Dueck, 2007; Tian et al., 2014; Chen et al., 2014). We adapt the Multi-Sense Skip-gram (MSSG) model (Neelakantan et al., 2014), which is based on the Skip-gram model (Mikolov et al., 2013). The Skip-gram model works by predicting a word vector using its context. To accommodate for multiple word senses, the MSSG model works by predicting the sense of the current word given its context, and then updating the current word sense vector and context vector. In particular, each word w is associated with two types of vector representations, one being the output embedding v(w), and the other being the context v(w). Given a special"
L16-1138,P10-1040,0,0.153108,"Missing"
L16-1138,W13-5708,0,0.0145802,". The advantages of embeddings are two-fold. First, they are useful for reducing sparseness compared with discrete words and n-grams. Second, they contain automatically induced features. Written as continuous sequences of characters, Chinese sentences do not have explicit word delimitation. As a result, similar to other Chinese NLP tasks, it is possible to create word embeddings based on automatically segmented Chinese sentences (Zhang et al., 2014b). Although segmentation errors can affect the quality, it has been shown that such word embeddings can improve Chinese NLP tasks such as parsing (Wu et al., 2013). On the other hand, character information can also be used for NLP (Zhang et al., 2014a). Zhang et al. (2013) show that parsing can be improved by taking characters as inputs, jointly performing segmentation and syntax parsing. They show that character features can significantly improve the accuracies. Character features are also central to other Chinese NLP tasks such as character-based segmentation (Xue, 2003) and named entity recognition (NER (Chen et al., 2006)). It is an interesting research question whether character embeddings can be useful for Chinese NLP. However, relatively little w"
L16-1138,O03-4002,0,0.102594,"sentences (Zhang et al., 2014b). Although segmentation errors can affect the quality, it has been shown that such word embeddings can improve Chinese NLP tasks such as parsing (Wu et al., 2013). On the other hand, character information can also be used for NLP (Zhang et al., 2014a). Zhang et al. (2013) show that parsing can be improved by taking characters as inputs, jointly performing segmentation and syntax parsing. They show that character features can significantly improve the accuracies. Character features are also central to other Chinese NLP tasks such as character-based segmentation (Xue, 2003) and named entity recognition (NER (Chen et al., 2006)). It is an interesting research question whether character embeddings can be useful for Chinese NLP. However, relatively little work has been reported on embedding Chinese characters, and limited effect has been observed (Sun et al., 2014). One possible reason is that the number of characters is much smaller compared to the number of words (104 vs 106 ), and the effect of character embedding on reducing sparsity can be limited. Also due to this reason, it has been shown that unsupervised clustering of characters does not improve in-domain"
L16-1138,P13-1013,1,0.852115,"iscrete words and n-grams. Second, they contain automatically induced features. Written as continuous sequences of characters, Chinese sentences do not have explicit word delimitation. As a result, similar to other Chinese NLP tasks, it is possible to create word embeddings based on automatically segmented Chinese sentences (Zhang et al., 2014b). Although segmentation errors can affect the quality, it has been shown that such word embeddings can improve Chinese NLP tasks such as parsing (Wu et al., 2013). On the other hand, character information can also be used for NLP (Zhang et al., 2014a). Zhang et al. (2013) show that parsing can be improved by taking characters as inputs, jointly performing segmentation and syntax parsing. They show that character features can significantly improve the accuracies. Character features are also central to other Chinese NLP tasks such as character-based segmentation (Xue, 2003) and named entity recognition (NER (Chen et al., 2006)). It is an interesting research question whether character embeddings can be useful for Chinese NLP. However, relatively little work has been reported on embedding Chinese characters, and limited effect has been observed (Sun et al., 2014)"
L16-1138,P14-1125,1,0.891171,"Missing"
N06-2007,P04-1054,0,0.0347615,"in this paper we propose a label propagation (LP) based semi-supervised learning algorithm for relation extraction task to learn from both labeled and unlabeled data. Evaluation on the ACE corpus showed when only a few labeled examples are available, our LP based relation extraction can achieve better performance than SVM and another bootstrapping method. 1 Introduction Relation extraction is the task of finding relationships between two entities from text. For the task, many machine learning methods have been proposed, including supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), semisupervised methods (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004), and unsupervised method (Hasegawa et al., 2004). Supervised relation extraction achieves good performance, but it requires a large amount of manually labeled relation instances. Unsupervised methods do not need the definition of relation types and manually labeled data, but it is difficult to evaluate the clustering result since there is no relation type label for each instance in clusters. Therefore, semisupervised learning has received attention, which can minimize corpus ann"
N06-2007,P04-1053,0,0.0183029,"Evaluation on the ACE corpus showed when only a few labeled examples are available, our LP based relation extraction can achieve better performance than SVM and another bootstrapping method. 1 Introduction Relation extraction is the task of finding relationships between two entities from text. For the task, many machine learning methods have been proposed, including supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), semisupervised methods (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004), and unsupervised method (Hasegawa et al., 2004). Supervised relation extraction achieves good performance, but it requires a large amount of manually labeled relation instances. Unsupervised methods do not need the definition of relation types and manually labeled data, but it is difficult to evaluate the clustering result since there is no relation type label for each instance in clusters. Therefore, semisupervised learning has received attention, which can minimize corpus annotation requirement. Current works on semi-supervised resolution for relation extraction task mostly use the bootstrapping algorithm, which is based on a local consi"
N06-2007,A00-2030,0,0.0345428,"for supervised relation extraction methods, in this paper we propose a label propagation (LP) based semi-supervised learning algorithm for relation extraction task to learn from both labeled and unlabeled data. Evaluation on the ACE corpus showed when only a few labeled examples are available, our LP based relation extraction can achieve better performance than SVM and another bootstrapping method. 1 Introduction Relation extraction is the task of finding relationships between two entities from text. For the task, many machine learning methods have been proposed, including supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), semisupervised methods (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004), and unsupervised method (Hasegawa et al., 2004). Supervised relation extraction achieves good performance, but it requires a large amount of manually labeled relation instances. Unsupervised methods do not need the definition of relation types and manually labeled data, but it is difficult to evaluate the clustering result since there is no relation type label for each instance in clusters. Therefore, semisupervised learning has"
N06-2007,P95-1026,0,0.225738,"Missing"
N06-2007,W02-1010,0,0.0279954,"on extraction methods, in this paper we propose a label propagation (LP) based semi-supervised learning algorithm for relation extraction task to learn from both labeled and unlabeled data. Evaluation on the ACE corpus showed when only a few labeled examples are available, our LP based relation extraction can achieve better performance than SVM and another bootstrapping method. 1 Introduction Relation extraction is the task of finding relationships between two entities from text. For the task, many machine learning methods have been proposed, including supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), semisupervised methods (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004), and unsupervised method (Hasegawa et al., 2004). Supervised relation extraction achieves good performance, but it requires a large amount of manually labeled relation instances. Unsupervised methods do not need the definition of relation types and manually labeled data, but it is difficult to evaluate the clustering result since there is no relation type label for each instance in clusters. Therefore, semisupervised learning has received attention, wh"
N06-2007,A00-2018,0,\N,Missing
P04-1080,E03-1020,0,0.0518221,"feature subset, which is not good enough to provide improvement when 0.9 Table 5: Average accuracy of three procedures with various Table 6: Automatically determined mixture component number. Word hard interest line serve Context window size 1 5 15 25 all 1 5 15 25 all 1 5 15 25 all 1 5 15 25 all Model order with χ2 3 2 2 2 2 5 3 4 4 3 5 4 5 5 3 3 3 3 3 2 Model order with f req 4 2 3 3 3 4 4 6 6 4 6 3 4 4 4 3 4 3 3 4 context window size is no less than 5. 4 Related Work Besides the two works (Pantel and Lin, 2002; Sch¨ utze, 1998), there are other related efforts on word sense discrimination (Dorow and Widdows, 2003; Fukumoto and Suzuki, 1999; Pedersen and Bruce, 1997). In (Pedersen and Bruce, 1997), they described an experimental comparison of three clustering algorithms for word sense discrimination. Their feature sets included morphology of target word, part of speech of contextual words, absence or presence of particular contextual words, and collocation of freAccuracy 0.5 0.7 0.6 0.4 01 0.4 0.3 0.5 5 15 25 0.2 01 all 5 Hard dataset 0.7 0.6 0.6 0.55 Accuracy 0.554 0.404 0.407 0.409 0.513 0.512 0.508 0.512 0.451 0.437 0.447 0.502 0.498 0.485 Accuracy Average accuracy Accuracy Feature weighting method"
P04-1080,E99-1028,0,0.134083,"not good enough to provide improvement when 0.9 Table 5: Average accuracy of three procedures with various Table 6: Automatically determined mixture component number. Word hard interest line serve Context window size 1 5 15 25 all 1 5 15 25 all 1 5 15 25 all 1 5 15 25 all Model order with χ2 3 2 2 2 2 5 3 4 4 3 5 4 5 5 3 3 3 3 3 2 Model order with f req 4 2 3 3 3 4 4 6 6 4 6 3 4 4 4 3 4 3 3 4 context window size is no less than 5. 4 Related Work Besides the two works (Pantel and Lin, 2002; Sch¨ utze, 1998), there are other related efforts on word sense discrimination (Dorow and Widdows, 2003; Fukumoto and Suzuki, 1999; Pedersen and Bruce, 1997). In (Pedersen and Bruce, 1997), they described an experimental comparison of three clustering algorithms for word sense discrimination. Their feature sets included morphology of target word, part of speech of contextual words, absence or presence of particular contextual words, and collocation of freAccuracy 0.5 0.7 0.6 0.4 01 0.4 0.3 0.5 5 15 25 0.2 01 all 5 Hard dataset 0.7 0.6 0.6 0.55 Accuracy 0.554 0.404 0.407 0.409 0.513 0.512 0.508 0.512 0.451 0.437 0.447 0.502 0.498 0.485 Accuracy Average accuracy Accuracy Feature weighting method binary binary idf tf · idf"
P04-1080,J98-1006,0,0.455411,"Missing"
P04-1080,W97-0322,0,0.375187,"improvement when 0.9 Table 5: Average accuracy of three procedures with various Table 6: Automatically determined mixture component number. Word hard interest line serve Context window size 1 5 15 25 all 1 5 15 25 all 1 5 15 25 all 1 5 15 25 all Model order with χ2 3 2 2 2 2 5 3 4 4 3 5 4 5 5 3 3 3 3 3 2 Model order with f req 4 2 3 3 3 4 4 6 6 4 6 3 4 4 4 3 4 3 3 4 context window size is no less than 5. 4 Related Work Besides the two works (Pantel and Lin, 2002; Sch¨ utze, 1998), there are other related efforts on word sense discrimination (Dorow and Widdows, 2003; Fukumoto and Suzuki, 1999; Pedersen and Bruce, 1997). In (Pedersen and Bruce, 1997), they described an experimental comparison of three clustering algorithms for word sense discrimination. Their feature sets included morphology of target word, part of speech of contextual words, absence or presence of particular contextual words, and collocation of freAccuracy 0.5 0.7 0.6 0.4 01 0.4 0.3 0.5 5 15 25 0.2 01 all 5 Hard dataset 0.7 0.6 0.6 0.55 Accuracy 0.554 0.404 0.407 0.409 0.513 0.512 0.508 0.512 0.451 0.437 0.447 0.502 0.498 0.485 Accuracy Average accuracy Accuracy Feature weighting method binary binary idf tf · idf binary idf tf · idf binary"
P04-1080,J98-1004,0,0.413017,"Missing"
P04-1080,N03-1036,0,0.132457,"many language applications such as machine translation, information retrieval, and speech processing (Ide and V´ eronis, 1998). Almost all of sense disambiguation methods are heavily dependant on manually compiled lexical resources. However these lexical resources often miss domain specific word senses, even many new words are not included inside. Learning word senses from free text will help us dispense of outside knowledge source for defining sense by only discriminating senses of words. Another application of word sense learning is to help enriching or even constructing semantic lexicons (Widdows, 2003). The solution of word sense learning is closely related to the interpretation of word senses. Different interpretations of word senses result in different solutions to word sense learning. One interpretation strategy is to treat a word sense as a set of synonyms like synset in WordNet. The committee based word sense discovery algorithm (Pantel and Lin, 2002) followed this strategy, which treated senses as clusters of words occurring in similar contexts. Their algorithm initially discovered tight clusters called committees by grouping top n words similar with target word using averageChew-Lim"
P04-1080,J98-1001,0,\N,Missing
P05-1049,P02-1033,0,0.049407,"Missing"
P05-1049,J98-1002,0,0.0296964,"saurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al., 2004; Seo et al., 2004; Yarowsky, 1992), (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003), (3) bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al., 2000; Yarowsky, 1995). As a commonly used semi-supervised learning method for WSD, bootstrapping algorithm works by iteratively classifying unlabeled examples and adding confidently classified examples into labeled dataset using a model learned from augmented labeled dataset in previous iteration. It can be found that the affinity information among unlabeled examples is not fully explored in this bootstrapping process. Bootstrapping is based on a local consistency assumption: examples close to labeled examples within same class will have same labels, which is als"
P05-1049,J98-1006,0,0.145626,"consistency assumption: similar examples should have similar labels. Our experimental results on benchmark corpora indicate that it consistently outperforms SVM when only very few labeled examples are available, and its performance is also better than monolingual bootstrapping, and comparable to bilingual bootstrapping. 1 Introduction In this paper, we address the problem of word sense disambiguation (WSD), which is to assign an appropriate sense to an occurrence of a word in a given context. Many methods have been proposed to deal with this problem, including supervised learning algorithms (Leacock et al., 1998), semi-supervised learning algorithms (Yarowsky, 1995), and unsupervised learning algorithms (Sch¨ utze, 1998). Supervised sense disambiguation has been very successful, but it requires a lot of manually sensetagged data and can not utilize raw unannotated data that can be cheaply acquired. Fully unsupervised methods do not need the definition of senses and manually sense-tagged data, but their sense clustering results can not be directly used in many NLP tasks since there is no sense tag for each instance in clusters. Considering both the availability of a large amount of unlabelled data and"
P05-1049,W02-1006,0,0.267713,"Missing"
P05-1049,J04-1001,0,0.163025,"ry for target words. They roughly fall into three categories according to what is used for supervision in learning process: (1) using external resources, e.g., thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al., 2004; Seo et al., 2004; Yarowsky, 1992), (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003), (3) bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al., 2000; Yarowsky, 1995). As a commonly used semi-supervised learning method for WSD, bootstrapping algorithm works by iteratively classifying unlabeled examples and adding confidently classified examples into labeled dataset using a model learned from augmented labeled dataset in previous iteration. It can be found that the affinity information among unlabeled examples is not fully explored in thi"
P05-1049,P97-1009,0,0.039132,"National University of Singapore 3 Science Drive 2 117543 Singapore tancl@comp.nus.edu.sg senses, semi-supervised learning methods have received great attention recently. Semi-supervised methods for WSD are characterized in terms of exploiting unlabeled data in learning procedure with the requirement of predefined sense inventory for target words. They roughly fall into three categories according to what is used for supervision in learning process: (1) using external resources, e.g., thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al., 2004; Seo et al., 2004; Yarowsky, 1992), (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003), (3) bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al., 2000; Yarowsky, 1995). As a commonly used semi-supervised learn"
P05-1049,P04-1036,0,0.0479931,"iversity of Singapore 3 Science Drive 2 117543 Singapore tancl@comp.nus.edu.sg senses, semi-supervised learning methods have received great attention recently. Semi-supervised methods for WSD are characterized in terms of exploiting unlabeled data in learning procedure with the requirement of predefined sense inventory for target words. They roughly fall into three categories according to what is used for supervision in learning process: (1) using external resources, e.g., thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al., 2004; Seo et al., 2004; Yarowsky, 1992), (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003), (3) bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al., 2000; Yarowsky, 1995). As a commonly used semi-supervised learning method for WSD, boo"
P05-1049,W04-2405,0,0.0556268,"sambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al., 2004; Seo et al., 2004; Yarowsky, 1992), (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003), (3) bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al., 2000; Yarowsky, 1995). As a commonly used semi-supervised learning method for WSD, bootstrapping algorithm works by iteratively classifying unlabeled examples and adding confidently classified examples into labeled dataset using a model learned from augmented labeled dataset in previous iteration. It can be found that the affinity information among unlabeled examples is not fully explored in this bootstrapping process. Bootstrapping is based on a local consistency assumption: examples close to labeled examples within same class will have same labels, which is also the assumption"
P05-1049,P03-1058,0,0.0514187,"ds. They roughly fall into three categories according to what is used for supervision in learning process: (1) using external resources, e.g., thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al., 2004; Seo et al., 2004; Yarowsky, 1992), (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003), (3) bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al., 2000; Yarowsky, 1995). As a commonly used semi-supervised learning method for WSD, bootstrapping algorithm works by iteratively classifying unlabeled examples and adding confidently classified examples into labeled dataset using a model learned from augmented labeled dataset in previous iteration. It can be found that the affinity information among unlabeled examples is not fully explored in this bootstrapping pr"
P05-1049,J98-1004,0,0.324855,"Missing"
P05-1049,P95-1026,0,0.916316,"r labels. Our experimental results on benchmark corpora indicate that it consistently outperforms SVM when only very few labeled examples are available, and its performance is also better than monolingual bootstrapping, and comparable to bilingual bootstrapping. 1 Introduction In this paper, we address the problem of word sense disambiguation (WSD), which is to assign an appropriate sense to an occurrence of a word in a given context. Many methods have been proposed to deal with this problem, including supervised learning algorithms (Leacock et al., 1998), semi-supervised learning algorithms (Yarowsky, 1995), and unsupervised learning algorithms (Sch¨ utze, 1998). Supervised sense disambiguation has been very successful, but it requires a lot of manually sensetagged data and can not utilize raw unannotated data that can be cheaply acquired. Fully unsupervised methods do not need the definition of senses and manually sense-tagged data, but their sense clustering results can not be directly used in many NLP tasks since there is no sense tag for each instance in clusters. Considering both the availability of a large amount of unlabelled data and direct use of word Chew Lim Tan Department of Computer"
P05-1049,P91-1034,0,0.242418,"arning procedure with the requirement of predefined sense inventory for target words. They roughly fall into three categories according to what is used for supervision in learning process: (1) using external resources, e.g., thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al., 2004; Seo et al., 2004; Yarowsky, 1992), (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003), (3) bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al., 2000; Yarowsky, 1995). As a commonly used semi-supervised learning method for WSD, bootstrapping algorithm works by iteratively classifying unlabeled examples and adding confidently classified examples into labeled dataset using a model learned from augmented labeled dataset in previous iteration. It can be found that the affinity inf"
P05-1049,J98-1005,0,\N,Missing
P05-1049,W04-0807,0,\N,Missing
P05-1049,W97-0322,0,\N,Missing
P05-1049,P00-1069,0,\N,Missing
P05-1049,C92-2070,0,\N,Missing
P05-1049,J94-4003,0,\N,Missing
P05-1049,P00-1000,0,\N,Missing
P05-1049,A00-2000,0,\N,Missing
P06-1017,P95-1026,0,0.365851,"Missing"
P06-1017,W02-1010,0,0.0138167,"isfy two constraints: 1) it should be fixed on the labeled nodes, 2) it should be smooth on the whole graph. Experiment results on the ACE corpus showed that this LP algorithm achieves better performance than SVM when only very few labeled examples are available, and it also performs better than bootstrapping for the relation extraction task. 1 Introduction Relation extraction is the task of detecting and classifying relationships between two entities from text. Many machine learning methods have been proposed to address this problem, e.g., supervised learning algorithms (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi-supervised learning algorithms (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004), and unsupervised learning algorithms (Hasegawa et al., 2004). Supervised methods for relation extraction perform well on the ACE Data, but they require a large DIPRE (Dual Iterative Pattern Relation Expansion) (Brin, 1998) is a bootstrapping-based system that used a pattern matching system as classifier to exploit the duality between sets of patterns and relations. Snowball (Agichtein and Gravano, 2000) is another system that used bootstr"
P06-1017,P05-1053,0,0.0899144,"Missing"
P06-1017,P04-1054,0,0.0747713,"1) it should be fixed on the labeled nodes, 2) it should be smooth on the whole graph. Experiment results on the ACE corpus showed that this LP algorithm achieves better performance than SVM when only very few labeled examples are available, and it also performs better than bootstrapping for the relation extraction task. 1 Introduction Relation extraction is the task of detecting and classifying relationships between two entities from text. Many machine learning methods have been proposed to address this problem, e.g., supervised learning algorithms (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi-supervised learning algorithms (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004), and unsupervised learning algorithms (Hasegawa et al., 2004). Supervised methods for relation extraction perform well on the ACE Data, but they require a large DIPRE (Dual Iterative Pattern Relation Expansion) (Brin, 1998) is a bootstrapping-based system that used a pattern matching system as classifier to exploit the duality between sets of patterns and relations. Snowball (Agichtein and Gravano, 2000) is another system that used bootstrapping techniques for extra"
P06-1017,P04-1053,0,0.0703311,"n only very few labeled examples are available, and it also performs better than bootstrapping for the relation extraction task. 1 Introduction Relation extraction is the task of detecting and classifying relationships between two entities from text. Many machine learning methods have been proposed to address this problem, e.g., supervised learning algorithms (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi-supervised learning algorithms (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004), and unsupervised learning algorithms (Hasegawa et al., 2004). Supervised methods for relation extraction perform well on the ACE Data, but they require a large DIPRE (Dual Iterative Pattern Relation Expansion) (Brin, 1998) is a bootstrapping-based system that used a pattern matching system as classifier to exploit the duality between sets of patterns and relations. Snowball (Agichtein and Gravano, 2000) is another system that used bootstrapping techniques for extracting relations from unstructured text. Snowball shares much in common with DIPRE, including the employment of the bootstrapping framework as well as the use of pattern matching to extract ne"
P06-1017,A00-2030,0,0.28514,"Missing"
P06-1017,A00-2018,0,\N,Missing
P06-2012,P04-1054,0,0.12114,"ating eigenvectors of an adjacency graph’s Laplacian to recover a submanifold of data from a high dimensionality space and then performing cluster number estimation on the eigenvectors. Experiment results on ACE corpora show that this spectral clustering based approach outperforms the other clustering methods. 1 Introduction In this paper, we address the task of relation extraction, which is to find relationships between name entities in a given context. Many methods have been proposed to deal with this task, including supervised learning algorithms (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi-supervised learning algorithms (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004), and unsupervised learning algorithm (Hasegawa et al., 2004). Among these methods, supervised learning is usually more preferred when a large amount of labeled training data is available. However, it is time-consuming and labor-intensive to manually tag a large amount of training data. Semi-supervised learning methods have been put forward to minimize the corpus annotation requirement. Most of semi-supervised methods employ the bootstrapping framework, which only ne"
P06-2012,A00-2030,0,0.0643241,"Missing"
P06-2012,W02-1010,0,0.0297307,"ts. It works by calculating eigenvectors of an adjacency graph’s Laplacian to recover a submanifold of data from a high dimensionality space and then performing cluster number estimation on the eigenvectors. Experiment results on ACE corpora show that this spectral clustering based approach outperforms the other clustering methods. 1 Introduction In this paper, we address the task of relation extraction, which is to find relationships between name entities in a given context. Many methods have been proposed to deal with this task, including supervised learning algorithms (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi-supervised learning algorithms (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004), and unsupervised learning algorithm (Hasegawa et al., 2004). Among these methods, supervised learning is usually more preferred when a large amount of labeled training data is available. However, it is time-consuming and labor-intensive to manually tag a large amount of training data. Semi-supervised learning methods have been put forward to minimize the corpus annotation requirement. Most of semi-supervised methods employ the bootstrappi"
P06-2012,P05-1053,0,0.0468321,"Missing"
P06-2012,A00-2018,0,\N,Missing
P06-2012,P04-1053,0,\N,Missing
P15-1045,C10-1037,0,0.205831,"w(ei ) = salext (s) (2) where salabs (·) denotes the word salience score of an abstractive model, salext (·) denotes the sentence salience score of an extractive model, and Sen(ei ) denotes the sentence set where ei is extracted from. We exploit our baseline sentence ranking method, SentRank, to obtain the sentence salience score, and use our baseline phrase ranking method, PhraseRank, to obtain the phrase salience score. 3.2 Headline Generation We use a graph-based multi-sentence compression (MSC) model to generate the final title for the proposed event-driven model. The model is inspired by Filippova (2010). First, a weighted directed acyclic word graph is built, with a start node and an end node in the graph. A headline can be obtained by any path from the start node to the end node. We measure each candidate path by a scoring function. Based on the measurement, we exploit a beam-search algorithm to find the optimum path. (1) i=1 (lj ,ei )∈Gbi X s∈Sen(ei ) rij × sal(lj ) rij × sal(ei ) Events w(lj ) · w(ei ) A 3.2.1 Word-Graph Construction Given a set of candidate events CE, we extract all the sentences that contain the events. In particular, we add two artificial words, hSi and hEi, to the sta"
P15-1045,P13-1122,0,0.38366,"e #K Multi-Sentence Compression Headline Headline Generation Figure 1: System framework. Zajic et al., 2005). Abstractive models choose a set of informative phrases for candidate extraction, and then exploit sentence synthesis techniques for headline generation (Soricut and Marcu, 2007; Woodsend et al., 2010; Xu et al., 2010). Extractive HG and abstractive HG have their respective advantages and disadvantages. Extractive models can generate more readable headlines, because the final title is derived by tailoring human-written sentences. However, extractive models give less informative titles (Alfonseca et al., 2013), because sentences are very sparse, making high-recall candidate extraction difficult. In contrast, abstractive models use phrases as the basic processing units, which are much less sparse. However, it is more difficult for abstractive HG to ensure the grammaticality of the generated titles, given that sentence synthesis is still very inaccurate based on a set of phrases with little grammatical information (Zhang, 2013). In this paper, we propose an event-driven model for headline generation, which alleviates the Introduction Headline generation (HG) is a text summarization task, which aims t"
P15-1045,N10-1131,0,0.0313243,"rade-off between sentences and phrases, avoiding sparsity and structureless problems. In particular, our event-driven model can interact with sentences and phrases, thus is a light combination for two traditional models. The event-driven model is mainly inspired by Alfonseca et al. (2013), who exploit events for multi-document headline generation. They leverage titles of sub-documents for supervised training. In contrast, we generate a title for a single document using an unsupervised model. We use novel approaches for event ranking and title generation. In recent years, sentence compression (Galanis and Androutsopoulos, 2010; Yoshikawa and Iida, 2012; Wang et al., 2013; Li et al., 2014; Thadani, 2014) has received much attention. Some methods can be directly applied for multidocument summarization (Wang et al., 2013; Li et al., 2014). To our knowledge, few studies have been explored on applying them in headline generation. Multi-sentence compression based on word graph was first proposed by Filippova (2010). Some subsequent work was presented recently. Boudin and Morin (2013) propose that the key phrase is helpful to sentence generation. The key phrases are extracted according to syntactic pattern and introduced"
P15-1045,W97-0703,0,0.110622,"extracting event arguments. Event arguments that have the same predicate are merged into one event, represented by tuple (Subject, Predicate, Object). For example, given the sentence, “the Keenans could demand the Aryan Nations’ assets”, Figure 2 present its partial parsing tree. Based on the parsing results, two event arguments are obtained: nsubj(demand, Keenans) and dobj(demand, assets). The two event arguments are merged into one event: (Keenans, demand, assets). 3.1.2 Extracting Lexical Chains Lexical chains are used to link semanticallyrelated words and phrases (Morris and Hirst, 1991; Barzilay and Elhadad, 1997). A lexical chain is analogous to a semantic synset. Compared with words, lexical chains are less sparse for event ranking. Given a text, we follow Boudin and Morin (2013) to construct lexical chains based on the following principles: 1. All words that are identical after stemming are treated as one word; 2. All NPs with the same head word fall into one lexical chain;2 3.1.1 Extracting Events We apply an open-domain event extraction approach. Different from traditional event extraction, for which types and arguments are predefined, open event extraction does not have a closed set of entities a"
P15-1045,J95-2003,0,0.490343,"Missing"
P15-1045,J05-3002,0,0.0528138,"of EventRank is better, capturing the major event in the reference title. 469 a word graph are treated equally, and the edges in the graph are constructed according to the adjacent order in original sentence. Our MSC model is also inspired by Filippova (2010). Our approach is more aggressive than their approach, generating compressions with arbitrary length by using a different edge construction strategy. In addition, our search algorithm is also different from theirs. Our graph-based MSC model is also similar in spirit to sentence fusion, which has been used for multi-document summarization (Barzilay and McKeown, 2005; Elsner and Santhanam, 2011). abstractive models have the problem of poor grammaticality. The event-driven mothod can alleviate both issues since event offer a trade-off between sentence and phrase. 5 Related Work Our event-driven model is different from traditional extractive (Dorr et al., 2003; Erkan and Radev, 2004; Alfonseca et al., 2013) and abstractive models (Zajic et al., 2005; Soricut and Marcu, 2007; Woodsend et al., 2010; Xu et al., 2010) in that events are used as the basic processing units instead of sentences and phrases. As mentioned above, events are a trade-off between senten"
P15-1045,D13-1036,0,0.142968,"Missing"
P15-1045,N13-1030,0,0.043263,"Missing"
P15-1045,D14-1076,0,0.0195508,"lems. In particular, our event-driven model can interact with sentences and phrases, thus is a light combination for two traditional models. The event-driven model is mainly inspired by Alfonseca et al. (2013), who exploit events for multi-document headline generation. They leverage titles of sub-documents for supervised training. In contrast, we generate a title for a single document using an unsupervised model. We use novel approaches for event ranking and title generation. In recent years, sentence compression (Galanis and Androutsopoulos, 2010; Yoshikawa and Iida, 2012; Wang et al., 2013; Li et al., 2014; Thadani, 2014) has received much attention. Some methods can be directly applied for multidocument summarization (Wang et al., 2013; Li et al., 2014). To our knowledge, few studies have been explored on applying them in headline generation. Multi-sentence compression based on word graph was first proposed by Filippova (2010). Some subsequent work was presented recently. Boudin and Morin (2013) propose that the key phrase is helpful to sentence generation. The key phrases are extracted according to syntactic pattern and introduced to identify shortest path in their work. Mehdad et al. (2013;"
P15-1045,J10-3005,0,0.05359,"Missing"
P15-1045,W04-1013,0,0.0153886,"ne sentence. 6 466 http://www.speech.sri.com/projects/srilm/ Input: G ← (V, E), LM, B Output: best candidates ← { {hSi} } loop do beam ← { } for each candidate in candidates if candidate endwith hEi A DD T O B EAM(beam, candidate) continue for each Vi in V candidate ← A DDV ERTEX(candidate, Vi ) C OMPUTE S CORE(candidate, LM) A DD T O B EAM(beam, candidate) end for end for candidates ← T OP -K(beam, B) if candidates all endwith hEi : break end loop best ← B EST(candidates) conducted using the Stanford NLP tools (Marneffe and Manning, 2008). The MRP iteration number is set to 10. We use ROUGE (Lin, 2004) to automatically measure the model performance, which has been widely used in summarization tasks (Wang et al., 2013; Ng et al., 2014). We focus on Rouge1 and Rouge2 scores, following Xu et al. (2010). In addition, we conduct human evaluations, using the same method as Woodsend et al. (2010). Four participants are asked to rate the generated headlines by three criteria: informativeness (how much important information in the article does the headline describe?), fluency (is it fluent to read?) and coherence (does it capture the topic of article?). Each headline is given a subjective score from"
P15-1045,P04-1015,0,0.038794,"m language model is trained using SRILM6 on English Gigaword (LDC2011T07). e∈CE where sal(e) is the salience score of an event from the candidate extraction step, Vi .w denotes the word of vertex Vi , and dist(w, e) denotes the distance from the word w to the event e, which are defined by the minimum distance from w to all the related words of e in a sentence by the dependency path5 between them. Intuitively, equation 3 demonstrates that a vertex is salient when its corresponding word is close to salient 3.2.3 Beam Search Beam search has been widely used aiming to find the sub optimum result (Collins and Roark, 2004; Zhang and Clark, 2011), when exact inference is extremely difficult. Assuming our word graph has a vertex size of n, the worst computation complexity is O(n4 ) when using a trigram language model, which is time consuming. 5 The distance is +∞ when e and w are not in one sentence. 6 466 http://www.speech.sri.com/projects/srilm/ Input: G ← (V, E), LM, B Output: best candidates ← { {hSi} } loop do beam ← { } for each candidate in candidates if candidate endwith hEi A DD T O B EAM(beam, candidate) continue for each Vi in V candidate ← A DDV ERTEX(candidate, Vi ) C OMPUTE S CORE(candidate, LM) A"
P15-1045,W09-1801,0,0.0753187,"Missing"
P15-1045,D14-1148,1,0.812388,"POS NNS the Keenans could demand the Aryan Nations ’ assets Figure 2: Dependency tree for the sentence “the Keenans could demand the Aryan Nations’ assets”. Candidate Extraction We exploit events as the basic units for candidate extraction. Here an event is a tuple (S, P, O), where S is the subject, P is the predicate and O is the object. For example, for the sentence “Ukraine Delays Announcement of New Government”, the event is (Ukraine, Delays, Announcement). This type of event structures has been used in open information extraction (Fader et al., 2011), and has a range of NLP applications (Ding et al., 2014; Ng et al., 2014). A sentence is a well-formed structure with complete syntactic information, but can contain redundant information for text summarization, which makes sentences very sparse. Phrases can be used to avoid the sparsity problem, but with little syntactic information between phrases, fluent headline generation is difficult. Events can be regarded as a trade-off between sentences and phrases. They are meaningful structures without redundant components, less sparse than sentences and containing more syntactic information than phrases. In our system, candidate event extraction is per"
P15-1045,W13-2117,0,0.0238691,"2013; Li et al., 2014; Thadani, 2014) has received much attention. Some methods can be directly applied for multidocument summarization (Wang et al., 2013; Li et al., 2014). To our knowledge, few studies have been explored on applying them in headline generation. Multi-sentence compression based on word graph was first proposed by Filippova (2010). Some subsequent work was presented recently. Boudin and Morin (2013) propose that the key phrase is helpful to sentence generation. The key phrases are extracted according to syntactic pattern and introduced to identify shortest path in their work. Mehdad et al. (2013; Mehdad et al. (2014) introduce the MSC based on word graph into meeting summarization. Tzouridis et al. (2014) cast multi-sentence compression as a structured predication problem. They use a largemargin approach to adapt parameterised edge weights to the data in order to acquire the shortest path. In their work, the sentences introduced to 6 Conclusion and Future Work We proposed an event-driven model headline generation, introducing a graph-based MSC model to generate the final title, based on a set of events. Our event-driven model can incorporate sentence and phrase salience, which has be"
P15-1045,W03-0501,0,0.414777,"nd McKeown, 2004). This task is challenging in not only informativeness and readability, which are challenges to common summarization tasks, but also the length reduction, which is unique for headline generation. Previous headline generation models fall into two main categories, namely extractive HG and abstractive HG (Woodsend et al., 2010; Alfonseca et al., 2013). Both consist of two steps: candidate extraction and headline generation. Extractive models choose a set of salient sentences in candidate extraction, and then exploit sentence compression techniques to achieve headline generation (Dorr et al., 2003; 462 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 462–472, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics sentence ranking method. In this paper, we use SentRank to denote this method. disadvantages of both extractive and abstractive HG. The framework of the proposed model is shown in Figure 1. In particular, we use events as the basic processing units for candidate extraction. We use structured tuples to represent the subject, predica"
P15-1045,P14-1115,0,0.0355339,"Missing"
P15-1045,W11-1607,0,0.0240977,"uring the major event in the reference title. 469 a word graph are treated equally, and the edges in the graph are constructed according to the adjacent order in original sentence. Our MSC model is also inspired by Filippova (2010). Our approach is more aggressive than their approach, generating compressions with arbitrary length by using a different edge construction strategy. In addition, our search algorithm is also different from theirs. Our graph-based MSC model is also similar in spirit to sentence fusion, which has been used for multi-document summarization (Barzilay and McKeown, 2005; Elsner and Santhanam, 2011). abstractive models have the problem of poor grammaticality. The event-driven mothod can alleviate both issues since event offer a trade-off between sentence and phrase. 5 Related Work Our event-driven model is different from traditional extractive (Dorr et al., 2003; Erkan and Radev, 2004; Alfonseca et al., 2013) and abstractive models (Zajic et al., 2005; Soricut and Marcu, 2007; Woodsend et al., 2010; Xu et al., 2010) in that events are used as the basic processing units instead of sentences and phrases. As mentioned above, events are a trade-off between sentences and phrases, avoiding spa"
P15-1045,J91-1002,0,0.655589,"ons, nsubj and dobj, for extracting event arguments. Event arguments that have the same predicate are merged into one event, represented by tuple (Subject, Predicate, Object). For example, given the sentence, “the Keenans could demand the Aryan Nations’ assets”, Figure 2 present its partial parsing tree. Based on the parsing results, two event arguments are obtained: nsubj(demand, Keenans) and dobj(demand, assets). The two event arguments are merged into one event: (Keenans, demand, assets). 3.1.2 Extracting Lexical Chains Lexical chains are used to link semanticallyrelated words and phrases (Morris and Hirst, 1991; Barzilay and Elhadad, 1997). A lexical chain is analogous to a semantic synset. Compared with words, lexical chains are less sparse for event ranking. Given a text, we follow Boudin and Morin (2013) to construct lexical chains based on the following principles: 1. All words that are identical after stemming are treated as one word; 2. All NPs with the same head word fall into one lexical chain;2 3.1.1 Extracting Events We apply an open-domain event extraction approach. Different from traditional event extraction, for which types and arguments are predefined, open event extraction does not ha"
P15-1045,W08-1301,0,0.0796257,": 1. All words that are identical after stemming are treated as one word; 2. All NPs with the same head word fall into one lexical chain;2 3.1.1 Extracting Events We apply an open-domain event extraction approach. Different from traditional event extraction, for which types and arguments are predefined, open event extraction does not have a closed set of entities and relations (Fader et al., 2011). We follow Hu’s work (Hu et al., 2013) to extract events. Given a text, we first use the Stanford dependency parser1 to obtain the Stanford typed dependency structures of the sentences (Marneffe and Manning, 2008). Then we focus on 1 poss aux 3. A pronoun is added to the corresponding lexical chain if it refers to a word in the chain (The coreference resolution is performed using the Stanford Coreference Resolution system);3 4. Lexical chains are merged if their main words are in the same synset of WordNet.4 2 NPs are extracted according to the dependency relations nn and amod. As shown in Figure 2, we can extract the noun phrase Aryan Nations according to the dependency relation nn(Nations, Aryan). 3 http://nlp.stanford.edu/software/dcoref.shtml 4 http://wordnet.princeton.edu/ http://nlp.stanford.edu/"
P15-1045,P14-1087,0,0.148337,"could demand the Aryan Nations ’ assets Figure 2: Dependency tree for the sentence “the Keenans could demand the Aryan Nations’ assets”. Candidate Extraction We exploit events as the basic units for candidate extraction. Here an event is a tuple (S, P, O), where S is the subject, P is the predicate and O is the object. For example, for the sentence “Ukraine Delays Announcement of New Government”, the event is (Ukraine, Delays, Announcement). This type of event structures has been used in open information extraction (Fader et al., 2011), and has a range of NLP applications (Ding et al., 2014; Ng et al., 2014). A sentence is a well-formed structure with complete syntactic information, but can contain redundant information for text summarization, which makes sentences very sparse. Phrases can be used to avoid the sparsity problem, but with little syntactic information between phrases, fluent headline generation is difficult. Events can be regarded as a trade-off between sentences and phrases. They are meaningful structures without redundant components, less sparse than sentences and containing more syntactic information than phrases. In our system, candidate event extraction is performed on a bipart"
P15-1045,D11-1142,0,0.474229,"ternational Joint Conference on Natural Language Processing, pages 462–472, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics sentence ranking method. In this paper, we use SentRank to denote this method. disadvantages of both extractive and abstractive HG. The framework of the proposed model is shown in Figure 1. In particular, we use events as the basic processing units for candidate extraction. We use structured tuples to represent the subject, predicate and object of an event. This form of event representation is widely used in open information extraction (Fader et al., 2011; Qiu and Zhang, 2014). Intuitively, events can be regarded as a trade-off between sentences and phrases. Events are meaningful structures, containing necessary grammatical information, and yet are much less sparse than sentences. We use salience measures of both sentences and phrases for event extraction, and thus our model can be regarded as a combination of extractive and abstractive HG. During the headline generation step, A graphbased multi-sentence compression (MSC) model is proposed to generate a final title, given multiple events. First a directed acyclic word graph is constructed base"
P15-1045,D14-1201,1,0.841711,"nference on Natural Language Processing, pages 462–472, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics sentence ranking method. In this paper, we use SentRank to denote this method. disadvantages of both extractive and abstractive HG. The framework of the proposed model is shown in Figure 1. In particular, we use events as the basic processing units for candidate extraction. We use structured tuples to represent the subject, predicate and object of an event. This form of event representation is widely used in open information extraction (Fader et al., 2011; Qiu and Zhang, 2014). Intuitively, events can be regarded as a trade-off between sentences and phrases. Events are meaningful structures, containing necessary grammatical information, and yet are much less sparse than sentences. We use salience measures of both sentences and phrases for event extraction, and thus our model can be regarded as a combination of extractive and abstractive HG. During the headline generation step, A graphbased multi-sentence compression (MSC) model is proposed to generate a final title, given multiple events. First a directed acyclic word graph is constructed based on the extracted eve"
P15-1045,J11-1005,1,0.849206,"ed using SRILM6 on English Gigaword (LDC2011T07). e∈CE where sal(e) is the salience score of an event from the candidate extraction step, Vi .w denotes the word of vertex Vi , and dist(w, e) denotes the distance from the word w to the event e, which are defined by the minimum distance from w to all the related words of e in a sentence by the dependency path5 between them. Intuitively, equation 3 demonstrates that a vertex is salient when its corresponding word is close to salient 3.2.3 Beam Search Beam search has been widely used aiming to find the sub optimum result (Collins and Roark, 2004; Zhang and Clark, 2011), when exact inference is extremely difficult. Assuming our word graph has a vertex size of n, the worst computation complexity is O(n4 ) when using a trigram language model, which is time consuming. 5 The distance is +∞ when e and w are not in one sentence. 6 466 http://www.speech.sri.com/projects/srilm/ Input: G ← (V, E), LM, B Output: best candidates ← { {hSi} } loop do beam ← { } for each candidate in candidates if candidate endwith hEi A DD T O B EAM(beam, candidate) continue for each Vi in V candidate ← A DDV ERTEX(candidate, Vi ) C OMPUTE S CORE(candidate, LM) A DD T O B EAM(beam, candi"
P15-1045,P14-1117,0,0.0129353,"ar, our event-driven model can interact with sentences and phrases, thus is a light combination for two traditional models. The event-driven model is mainly inspired by Alfonseca et al. (2013), who exploit events for multi-document headline generation. They leverage titles of sub-documents for supervised training. In contrast, we generate a title for a single document using an unsupervised model. We use novel approaches for event ranking and title generation. In recent years, sentence compression (Galanis and Androutsopoulos, 2010; Yoshikawa and Iida, 2012; Wang et al., 2013; Li et al., 2014; Thadani, 2014) has received much attention. Some methods can be directly applied for multidocument summarization (Wang et al., 2013; Li et al., 2014). To our knowledge, few studies have been explored on applying them in headline generation. Multi-sentence compression based on word graph was first proposed by Filippova (2010). Some subsequent work was presented recently. Boudin and Morin (2013) propose that the key phrase is helpful to sentence generation. The key phrases are extracted according to syntactic pattern and introduced to identify shortest path in their work. Mehdad et al. (2013; Mehdad et al. (2"
P15-1045,C14-1155,0,0.0185322,"multidocument summarization (Wang et al., 2013; Li et al., 2014). To our knowledge, few studies have been explored on applying them in headline generation. Multi-sentence compression based on word graph was first proposed by Filippova (2010). Some subsequent work was presented recently. Boudin and Morin (2013) propose that the key phrase is helpful to sentence generation. The key phrases are extracted according to syntactic pattern and introduced to identify shortest path in their work. Mehdad et al. (2013; Mehdad et al. (2014) introduce the MSC based on word graph into meeting summarization. Tzouridis et al. (2014) cast multi-sentence compression as a structured predication problem. They use a largemargin approach to adapt parameterised edge weights to the data in order to acquire the shortest path. In their work, the sentences introduced to 6 Conclusion and Future Work We proposed an event-driven model headline generation, introducing a graph-based MSC model to generate the final title, based on a set of events. Our event-driven model can incorporate sentence and phrase salience, which has been used in extractive and abstractive HG models. The proposed graph-based MSC model is not limited to our event-"
P15-1045,C04-1079,0,0.160236,"ce realization preferences jointly. Previous extractive and abstractive models take two main steps, namely candidate extraction and headline generation. Here, we introduce these two types of models according to the two steps. 2.1 Abstractive Headline Generation Extractive Headline Generation Candidate Extraction. Extractive models exploit sentences as the basic processing units in this step. Sentences are ranked by their salience according to specific strategies (Dorr et al., 2003; Erkan and Radev, 2004; Zajic et al., 2005). One of the stateof-the-art approaches is the work of Erkan and Radev (2004), which exploits centroid, position and length features to compute sentence salience. We re-implemented this method as our baseline 463 3 Our Model dobj Similar to extractive and abstractive models, the proposed event-driven model consists of two steps, namely candidate extraction and headline generation. 3.1 nsubj det nn DT NNPS MD VB DT NNP NNP POS NNS the Keenans could demand the Aryan Nations ’ assets Figure 2: Dependency tree for the sentence “the Keenans could demand the Aryan Nations’ assets”. Candidate Extraction We exploit events as the basic units for candidate extraction. Here an ev"
P15-1045,P13-1136,0,0.317702,"also from the proposed graph-based MSC model. Both our candidate extraction and headline generation methods outperform competitive baseline methods, and our model achieves the best results compared with previous state-of-the-art systems. 2 Headline Generation. Given a set of sentences, extractive models exploit sentence compression techniques to generate a final title. Most previous work exploits single-sentence compression (SSC) techniques. Dorr et al. (2003) proposed the Hedge Trimmer algorithm to compress a sentence by making use of handcrafted linguistically-based rules. Alfonseca et al. (2013) introduce a multi-sentence compression (MSC) model into headline generation, using it as a baseline in their work. They indicated that the most important information is distributed across several sentences in the text. 2.2 Candidate Extraction. Different from extractive models, abstractive models exploit phrases as the basic processing units. A set of salient phrases are selected according to specific principles during candidate extraction (Schwartz, 01; Soricut and Marcu, 2007; Xu et al., 2010; Woodsend et al., 2010). Xu et al. (2010) propose to rank phrases using background knowledge extrac"
P15-1045,D10-1050,0,0.138899,"adline generation, combining the advantages of both methods using event structures. Standard evaluation shows that our model achieves the best performance compared with previous state-of-the-art systems. 1 Candidate Extraction Events Phrases Sentences Candidate Ranking Candidate #1 ... Candidate #i ... Candidate #K Multi-Sentence Compression Headline Headline Generation Figure 1: System framework. Zajic et al., 2005). Abstractive models choose a set of informative phrases for candidate extraction, and then exploit sentence synthesis techniques for headline generation (Soricut and Marcu, 2007; Woodsend et al., 2010; Xu et al., 2010). Extractive HG and abstractive HG have their respective advantages and disadvantages. Extractive models can generate more readable headlines, because the final title is derived by tailoring human-written sentences. However, extractive models give less informative titles (Alfonseca et al., 2013), because sentences are very sparse, making high-recall candidate extraction difficult. In contrast, abstractive models use phrases as the basic processing units, which are much less sparse. However, it is more difficult for abstractive HG to ensure the grammaticality of the generated"
P15-1045,P12-2068,0,0.0198855,"ses, avoiding sparsity and structureless problems. In particular, our event-driven model can interact with sentences and phrases, thus is a light combination for two traditional models. The event-driven model is mainly inspired by Alfonseca et al. (2013), who exploit events for multi-document headline generation. They leverage titles of sub-documents for supervised training. In contrast, we generate a title for a single document using an unsupervised model. We use novel approaches for event ranking and title generation. In recent years, sentence compression (Galanis and Androutsopoulos, 2010; Yoshikawa and Iida, 2012; Wang et al., 2013; Li et al., 2014; Thadani, 2014) has received much attention. Some methods can be directly applied for multidocument summarization (Wang et al., 2013; Li et al., 2014). To our knowledge, few studies have been explored on applying them in headline generation. Multi-sentence compression based on word graph was first proposed by Filippova (2010). Some subsequent work was presented recently. Boudin and Morin (2013) propose that the key phrase is helpful to sentence generation. The key phrases are extracted according to syntactic pattern and introduced to identify shortest path"
P15-1045,P14-2054,0,\N,Missing
P15-1045,R13-1033,0,\N,Missing
P98-1098,H93-1036,0,\N,Missing
P98-1098,C92-2070,0,\N,Missing
P98-1098,P93-1034,0,\N,Missing
S07-1037,S07-1002,0,0.0373784,"asks for evaluation exercise, covering word sense disambiguation, word sense discrimination, semantic role labeling, and sense disambiguation for information retrieval, and other topics in NLP. We participated three tasks in SemEval-2007, which are task 2 (Evaluating Word Sense Induction and Discrimination Systems), task 5 (Multilingual Chinese-English Lexical Sample Task) and the first subtask at task 17 (English Lexical Sample, English Semantic Role Labeling and English All-Words Tasks). The goal for SemEval-2007 task 2 (Evaluating Word Sense Induction and Discrimination Systems)(Agirre and Soroa, 2007) is to automatically discriminate the senses of English target words by the use of only untagged data. Here we address this word sense discrimination problem by (1) estimating the number of word senses of a target word in untagged data using a stability criterion, and then (2) grouping the instances of this target word into the estimated number of clusters according to the similarity of contexts of the instances. No sense-tagged data is used to help the clustering process. The goal of task 5 (Chinese Word Sense Disambiguation) is to create a framework for the evaluation of word sense disambigu"
S07-1037,W02-1006,0,0.053136,"n of data sets at task 2, task 5 and task 17. Then, we will present the experimental results of our systems at the three tasks in section 6. Finally we will give a conclusion of our work in section 7. 2 Feature Set In task 2, task 5 and task 17, we used three types of features to capture contextual information: part-ofspeech of neighboring words (no more than threeword distance) with position information, unordered single words in topical context (all the contextual sentences), and local collocations (including 11 collocations). The feature set used here is as same as the feature set used in (Lee and Ng, 2002) except that we did not use syntactic relations. 3 The Word Sense Discrimination Method for Task 2 Word sense discrimination is to automatically discriminate the senses of target words by the use of only untagged data. So we can employ clustering algorithms to address this problem. Another problem is that there is no sense inventories for target words. So the clustering algorithms should have the ability to automatically estimate the sense number of a target word. Here we used the sequential Information Bottleneck algorithm (sIB) (Slonim, et al., 2002) to estimate cluster structure, which meas"
S07-1037,P05-1049,1,0.817613,"y of contexts of the instances. No sense-tagged data is used to help the clustering process. The goal of task 5 (Chinese Word Sense Disambiguation) is to create a framework for the evaluation of word sense disambiguation in Chinese-English machine translation systems. Each participates of this task will be provided with sense tagged training data and untagged test data for 40 Chinese polysemous words. The ”sense tags” for the ambiguous Chinese target words are given in the form of their English translations. Here we used a semisupervised classification algorithm (label propagation algorithm) (Niu, et al., 2005) to address this Chinese word sense disambiguation problem. The lexical sample subtask of task 17 (English Word Sense Disambiguation) provides sense-tagged training data and untagged test data for 35 nouns and 65 verbs. This data includes, for each target word: OntoNotes sense tags (these are groupings of WordNet senses that are more coarse-grained than tradi177 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 177–182, c Prague, June 2007. 2007 Association for Computational Linguistics tional WN entries), as well as the sense inventory for these lemma"
S16-1141,attardi-etal-2010-resource,0,0.517645,"Missing"
S16-1141,W13-2322,0,0.0320077,"rdNet or Brown clusters which are allowed in the supervised closed condition. Experimental results show that much further work on feature engineering and model optimization needs to be explored. 1 Introduction In the community of natural language processing, multiword expressions (MWEs) detection (Schneider et al., 2014b; Schneider et al., 2014a) and supersense tagging (Ciaramita and Johnson, 2003; Ciaramita and Altun, 2006) have received much research attention due to their various applications such as syntactic parsing (Candito and Constant, 2014; Bengoetxea et al., 2014), semantic parsing (Banarescu et al., 2013), and machine translation (Carpuat and Diab, 2010). However, not much attention has been paid to the relationship between MWEs and supersenses (Piao et al., 2005; Schneider and Smith, 2015). 918 Input: SecurityN OU N increasedV ERB inADP MumbaiP ROP N amidADP terrorN OU N threatsN OU N aheadADP ofADP GaneshotsavP ROP N Output: Securityn.state increasedv.change in Mumbain.location amid terror threatsn.communication ahead of Ganeshotsavn.event Figure 1: An DiMSUM Example. Given a tokenized and POS-tagged sentence, outputs will be a representation annotated with MWEs and supersenses. Noun and ver"
S16-1141,W13-1016,0,0.0420829,"Missing"
S16-1141,J92-4003,0,0.490974,"nce labeling task using first-order CRFs; second, supersense tagging is treated as a multi-classification task using Maximum Entropy Algorithm. We focus on the supervised closed condition, so only the training set are used for training both submodels separately. Then results generated on the test set are submitted for official evaluation. The evaluation results show that our system performance are not as good as those of other teams, since we leverage only some simple features such as words, POS, etc. Syntactic features and semantic resources such as WordNet (Miller, 1995) and Brown clusters (Brown et al., 1992) are not used. This suggests that further work needs to be done on feature engineering and model optimization. 2 Multiword Expression Detection In the training set, sentences are tokenized into words, and every word has been annotated with POS and lemma. We formalize MWE detection as a sequence labeling problem, so Mallet (McCallum, 2002), Off-the-shelf implementation of CRFs, is used to handle this task. All the labels of our CRF model, extracted from the training set, are listed as follows: • O, which indicates that the current word does not belong to a MWE. 919 1 2 3 4 5 6 7 8 9 current wor"
S16-1141,P14-1070,0,0.0187184,"ags from the training set, and avoid using external resources such as WordNet or Brown clusters which are allowed in the supervised closed condition. Experimental results show that much further work on feature engineering and model optimization needs to be explored. 1 Introduction In the community of natural language processing, multiword expressions (MWEs) detection (Schneider et al., 2014b; Schneider et al., 2014a) and supersense tagging (Ciaramita and Johnson, 2003; Ciaramita and Altun, 2006) have received much research attention due to their various applications such as syntactic parsing (Candito and Constant, 2014; Bengoetxea et al., 2014), semantic parsing (Banarescu et al., 2013), and machine translation (Carpuat and Diab, 2010). However, not much attention has been paid to the relationship between MWEs and supersenses (Piao et al., 2005; Schneider and Smith, 2015). 918 Input: SecurityN OU N increasedV ERB inADP MumbaiP ROP N amidADP terrorN OU N threatsN OU N aheadADP ofADP GaneshotsavP ROP N Output: Securityn.state increasedv.change in Mumbain.location amid terror threatsn.communication ahead of Ganeshotsavn.event Figure 1: An DiMSUM Example. Given a tokenized and POS-tagged sentence, outputs will"
S16-1141,N10-1029,0,0.022348,"upervised closed condition. Experimental results show that much further work on feature engineering and model optimization needs to be explored. 1 Introduction In the community of natural language processing, multiword expressions (MWEs) detection (Schneider et al., 2014b; Schneider et al., 2014a) and supersense tagging (Ciaramita and Johnson, 2003; Ciaramita and Altun, 2006) have received much research attention due to their various applications such as syntactic parsing (Candito and Constant, 2014; Bengoetxea et al., 2014), semantic parsing (Banarescu et al., 2013), and machine translation (Carpuat and Diab, 2010). However, not much attention has been paid to the relationship between MWEs and supersenses (Piao et al., 2005; Schneider and Smith, 2015). 918 Input: SecurityN OU N increasedV ERB inADP MumbaiP ROP N amidADP terrorN OU N threatsN OU N aheadADP ofADP GaneshotsavP ROP N Output: Securityn.state increasedv.change in Mumbain.location amid terror threatsn.communication ahead of Ganeshotsavn.event Figure 1: An DiMSUM Example. Given a tokenized and POS-tagged sentence, outputs will be a representation annotated with MWEs and supersenses. Noun and verb supersenses start with “n.” and “v.”, respective"
S16-1141,2012.eamt-1.60,0,0.0309604,"Missing"
S16-1141,W06-1670,0,0.151113,"Missing"
S16-1141,W03-1022,0,0.0786505,"cation problem solved by Maximum Entropy Algorithm. To carry out our pilot study quickly, we extract some simple features such as words or part-of-speech tags from the training set, and avoid using external resources such as WordNet or Brown clusters which are allowed in the supervised closed condition. Experimental results show that much further work on feature engineering and model optimization needs to be explored. 1 Introduction In the community of natural language processing, multiword expressions (MWEs) detection (Schneider et al., 2014b; Schneider et al., 2014a) and supersense tagging (Ciaramita and Johnson, 2003; Ciaramita and Altun, 2006) have received much research attention due to their various applications such as syntactic parsing (Candito and Constant, 2014; Bengoetxea et al., 2014), semantic parsing (Banarescu et al., 2013), and machine translation (Carpuat and Diab, 2010). However, not much attention has been paid to the relationship between MWEs and supersenses (Piao et al., 2005; Schneider and Smith, 2015). 918 Input: SecurityN OU N increasedV ERB inADP MumbaiP ROP N amidADP terrorN OU N threatsN OU N aheadADP ofADP GaneshotsavP ROP N Output: Securityn.state increasedv.change in Mumbain.loc"
S16-1141,P12-1022,0,0.0155292,"k descriptions, we consider the concepts of minimal semantic units and semantic classes are identical to those of MWEs and supersenses, respectively. Figure 1 shows an input example and its corresponding outputs of DiMSUM task. Prior work on MWE detection using unsupervised methods includes lexicon lookup (Bejˇcek et al., 2013), statistical association measures (Ramisch et al., 2012), parallel corpora (Tsvetkov and Wintner, 2010), or hybrid methods (Tsvetkov and Wintner, 2011). More sophisticated methods use supervised techniques such as conditional random fields (CRFs) (Shigeto et al., 2013; Constant et al., 2012; Vincze et al., 2013) or structured perceptron Proceedings of SemEval-2016, pages 918–924, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics (Schneider et al., 2014a), and usually achieve better performance. Compared to most aforementioned systems, MWEs in the DiMSUM task may be not contiguous or restricted by syntactic construction, which increases the detection difficulty. Supersense tagging has been studied on diverse languages such as English (Ciaramita and Johnson, 2003; Ciaramita and Altun, 2006; Johannsen et al., 2014; Schneider and Smith, 2015),"
S16-1141,S14-1001,0,0.149351,"(CRFs) (Shigeto et al., 2013; Constant et al., 2012; Vincze et al., 2013) or structured perceptron Proceedings of SemEval-2016, pages 918–924, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics (Schneider et al., 2014a), and usually achieve better performance. Compared to most aforementioned systems, MWEs in the DiMSUM task may be not contiguous or restricted by syntactic construction, which increases the detection difficulty. Supersense tagging has been studied on diverse languages such as English (Ciaramita and Johnson, 2003; Ciaramita and Altun, 2006; Johannsen et al., 2014; Schneider and Smith, 2015), Italian (Attardi et al., 2010), Chinese (Qiu et al., 2011) and Arabic (Schneider et al., 2012). It is usually formalized as a multi-classification problem solved by supervised approaches such as perceptron. In the DiMSUM task, both single-word and multiword expressions that holistically function as noun or verb, can be considered as units for supersense tagging. Following prior work using supervised approaches, we divide DiMSUM task into two subtasks: first, MWEs detection is treated as a sequence labeling task using first-order CRFs; second, supersense tagging is"
S16-1141,D14-1108,0,0.081987,"Missing"
S16-1141,W12-3301,0,0.425246,"Missing"
S16-1141,N15-1177,0,0.603783,"xplored. 1 Introduction In the community of natural language processing, multiword expressions (MWEs) detection (Schneider et al., 2014b; Schneider et al., 2014a) and supersense tagging (Ciaramita and Johnson, 2003; Ciaramita and Altun, 2006) have received much research attention due to their various applications such as syntactic parsing (Candito and Constant, 2014; Bengoetxea et al., 2014), semantic parsing (Banarescu et al., 2013), and machine translation (Carpuat and Diab, 2010). However, not much attention has been paid to the relationship between MWEs and supersenses (Piao et al., 2005; Schneider and Smith, 2015). 918 Input: SecurityN OU N increasedV ERB inADP MumbaiP ROP N amidADP terrorN OU N threatsN OU N aheadADP ofADP GaneshotsavP ROP N Output: Securityn.state increasedv.change in Mumbain.location amid terror threatsn.communication ahead of Ganeshotsavn.event Figure 1: An DiMSUM Example. Given a tokenized and POS-tagged sentence, outputs will be a representation annotated with MWEs and supersenses. Noun and verb supersenses start with “n.” and “v.”, respectively. “ ” joins tokens within a MWE. The DiMSUM shared task (Schneider et al., 2016) at SemEval 2016 aims to predict a broad-coverage represe"
S16-1141,P12-2050,0,0.368278,"016, pages 918–924, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics (Schneider et al., 2014a), and usually achieve better performance. Compared to most aforementioned systems, MWEs in the DiMSUM task may be not contiguous or restricted by syntactic construction, which increases the detection difficulty. Supersense tagging has been studied on diverse languages such as English (Ciaramita and Johnson, 2003; Ciaramita and Altun, 2006; Johannsen et al., 2014; Schneider and Smith, 2015), Italian (Attardi et al., 2010), Chinese (Qiu et al., 2011) and Arabic (Schneider et al., 2012). It is usually formalized as a multi-classification problem solved by supervised approaches such as perceptron. In the DiMSUM task, both single-word and multiword expressions that holistically function as noun or verb, can be considered as units for supersense tagging. Following prior work using supervised approaches, we divide DiMSUM task into two subtasks: first, MWEs detection is treated as a sequence labeling task using first-order CRFs; second, supersense tagging is treated as a multi-classification task using Maximum Entropy Algorithm. We focus on the supervised closed condition, so onl"
S16-1141,Q14-1016,0,0.333849,"solved by first-order CRFs, and the latter one is formalized as a classification problem solved by Maximum Entropy Algorithm. To carry out our pilot study quickly, we extract some simple features such as words or part-of-speech tags from the training set, and avoid using external resources such as WordNet or Brown clusters which are allowed in the supervised closed condition. Experimental results show that much further work on feature engineering and model optimization needs to be explored. 1 Introduction In the community of natural language processing, multiword expressions (MWEs) detection (Schneider et al., 2014b; Schneider et al., 2014a) and supersense tagging (Ciaramita and Johnson, 2003; Ciaramita and Altun, 2006) have received much research attention due to their various applications such as syntactic parsing (Candito and Constant, 2014; Bengoetxea et al., 2014), semantic parsing (Banarescu et al., 2013), and machine translation (Carpuat and Diab, 2010). However, not much attention has been paid to the relationship between MWEs and supersenses (Piao et al., 2005; Schneider and Smith, 2015). 918 Input: SecurityN OU N increasedV ERB inADP MumbaiP ROP N amidADP terrorN OU N threatsN OU N aheadADP of"
S16-1141,schneider-etal-2014-comprehensive,0,0.298503,"solved by first-order CRFs, and the latter one is formalized as a classification problem solved by Maximum Entropy Algorithm. To carry out our pilot study quickly, we extract some simple features such as words or part-of-speech tags from the training set, and avoid using external resources such as WordNet or Brown clusters which are allowed in the supervised closed condition. Experimental results show that much further work on feature engineering and model optimization needs to be explored. 1 Introduction In the community of natural language processing, multiword expressions (MWEs) detection (Schneider et al., 2014b; Schneider et al., 2014a) and supersense tagging (Ciaramita and Johnson, 2003; Ciaramita and Altun, 2006) have received much research attention due to their various applications such as syntactic parsing (Candito and Constant, 2014; Bengoetxea et al., 2014), semantic parsing (Banarescu et al., 2013), and machine translation (Carpuat and Diab, 2010). However, not much attention has been paid to the relationship between MWEs and supersenses (Piao et al., 2005; Schneider and Smith, 2015). 918 Input: SecurityN OU N increasedV ERB inADP MumbaiP ROP N amidADP terrorN OU N threatsN OU N aheadADP of"
S16-1141,W13-1021,0,0.015516,"sses. Based on the task descriptions, we consider the concepts of minimal semantic units and semantic classes are identical to those of MWEs and supersenses, respectively. Figure 1 shows an input example and its corresponding outputs of DiMSUM task. Prior work on MWE detection using unsupervised methods includes lexicon lookup (Bejˇcek et al., 2013), statistical association measures (Ramisch et al., 2012), parallel corpora (Tsvetkov and Wintner, 2010), or hybrid methods (Tsvetkov and Wintner, 2011). More sophisticated methods use supervised techniques such as conditional random fields (CRFs) (Shigeto et al., 2013; Constant et al., 2012; Vincze et al., 2013) or structured perceptron Proceedings of SemEval-2016, pages 918–924, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics (Schneider et al., 2014a), and usually achieve better performance. Compared to most aforementioned systems, MWEs in the DiMSUM task may be not contiguous or restricted by syntactic construction, which increases the detection difficulty. Supersense tagging has been studied on diverse languages such as English (Ciaramita and Johnson, 2003; Ciaramita and Altun, 2006; Johannsen et al., 2014; Schn"
S16-1141,C10-2144,0,0.0268478,"giving an English sentence. This representation consists of two facets: a segmentation into minimal semantic units, and a labeling of some of those units with semantic classes. Based on the task descriptions, we consider the concepts of minimal semantic units and semantic classes are identical to those of MWEs and supersenses, respectively. Figure 1 shows an input example and its corresponding outputs of DiMSUM task. Prior work on MWE detection using unsupervised methods includes lexicon lookup (Bejˇcek et al., 2013), statistical association measures (Ramisch et al., 2012), parallel corpora (Tsvetkov and Wintner, 2010), or hybrid methods (Tsvetkov and Wintner, 2011). More sophisticated methods use supervised techniques such as conditional random fields (CRFs) (Shigeto et al., 2013; Constant et al., 2012; Vincze et al., 2013) or structured perceptron Proceedings of SemEval-2016, pages 918–924, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics (Schneider et al., 2014a), and usually achieve better performance. Compared to most aforementioned systems, MWEs in the DiMSUM task may be not contiguous or restricted by syntactic construction, which increases the detection diffi"
S16-1141,D11-1077,0,0.026134,"consists of two facets: a segmentation into minimal semantic units, and a labeling of some of those units with semantic classes. Based on the task descriptions, we consider the concepts of minimal semantic units and semantic classes are identical to those of MWEs and supersenses, respectively. Figure 1 shows an input example and its corresponding outputs of DiMSUM task. Prior work on MWE detection using unsupervised methods includes lexicon lookup (Bejˇcek et al., 2013), statistical association measures (Ramisch et al., 2012), parallel corpora (Tsvetkov and Wintner, 2010), or hybrid methods (Tsvetkov and Wintner, 2011). More sophisticated methods use supervised techniques such as conditional random fields (CRFs) (Shigeto et al., 2013; Constant et al., 2012; Vincze et al., 2013) or structured perceptron Proceedings of SemEval-2016, pages 918–924, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics (Schneider et al., 2014a), and usually achieve better performance. Compared to most aforementioned systems, MWEs in the DiMSUM task may be not contiguous or restricted by syntactic construction, which increases the detection difficulty. Supersense tagging has been studied on di"
S16-1141,P14-2106,0,\N,Missing
S16-1141,S16-1084,0,\N,Missing
tang-etal-2004-model,P98-1013,0,\N,Missing
tang-etal-2004-model,C98-1013,0,\N,Missing
W04-0847,C02-1143,0,0.157972,"y of class ci given feature set F , 1 ≤ i ≤ L. Assuming the independence between features, the classification procedure can be formulated as: ˆi = arg max 1≤i≤L Introduction Word sense disambiguation (WSD) is to assign appropriate meaning to a given ambiguous word in a text. Corpus based method is one of the successful lines of research on WSD. Many supervised learning algorithms have been applied for WSD, ex. Bayesian learning (Leacock et al., 1998), exemplar based learning (Ng and Lee, 1996), decision list (Yarowsky, 2000), neural network (Towel and Voorheest, 1998), maximum entropy method (Dang et al., 2002), etc.. In this paper, we employ Naive Bayes classifier to perform WSD. Resolving the ambiguity of words usually relies on the contexts of their occurrences. The feature set used for context representation consists of local and topical features. Local features include part of speech tags of words within local context, morphological information of target word, local collocations, and syntactic relations between contextual words and target word, etc.. Topical features are bag of words occurred within topical context. Contextual features play an important role in providing discrimination informat"
W04-0847,J98-1006,0,0.0901114,"ent in the context of target word, otherwise 0. In classification process, the Naive Bayes classifier tries to find the class that maximizes P (ci |F ), the probability of class ci given feature set F , 1 ≤ i ≤ L. Assuming the independence between features, the classification procedure can be formulated as: ˆi = arg max 1≤i≤L Introduction Word sense disambiguation (WSD) is to assign appropriate meaning to a given ambiguous word in a text. Corpus based method is one of the successful lines of research on WSD. Many supervised learning algorithms have been applied for WSD, ex. Bayesian learning (Leacock et al., 1998), exemplar based learning (Ng and Lee, 1996), decision list (Yarowsky, 2000), neural network (Towel and Voorheest, 1998), maximum entropy method (Dang et al., 2002), etc.. In this paper, we employ Naive Bayes classifier to perform WSD. Resolving the ambiguity of words usually relies on the contexts of their occurrences. The feature set used for context representation consists of local and topical features. Local features include part of speech tags of words within local context, morphological information of target word, local collocations, and syntactic relations between contextual words and t"
W04-0847,W96-0208,0,0.0531647,"Missing"
W04-0847,P96-1006,0,0.0525118,"In classification process, the Naive Bayes classifier tries to find the class that maximizes P (ci |F ), the probability of class ci given feature set F , 1 ≤ i ≤ L. Assuming the independence between features, the classification procedure can be formulated as: ˆi = arg max 1≤i≤L Introduction Word sense disambiguation (WSD) is to assign appropriate meaning to a given ambiguous word in a text. Corpus based method is one of the successful lines of research on WSD. Many supervised learning algorithms have been applied for WSD, ex. Bayesian learning (Leacock et al., 1998), exemplar based learning (Ng and Lee, 1996), decision list (Yarowsky, 2000), neural network (Towel and Voorheest, 1998), maximum entropy method (Dang et al., 2002), etc.. In this paper, we employ Naive Bayes classifier to perform WSD. Resolving the ambiguity of words usually relies on the contexts of their occurrences. The feature set used for context representation consists of local and topical features. Local features include part of speech tags of words within local context, morphological information of target word, local collocations, and syntactic relations between contextual words and target word, etc.. Topical features are bag o"
W04-0847,N01-1011,0,0.0336835,"Missing"
W04-0847,J98-1005,0,0.143236,"the class that maximizes P (ci |F ), the probability of class ci given feature set F , 1 ≤ i ≤ L. Assuming the independence between features, the classification procedure can be formulated as: ˆi = arg max 1≤i≤L Introduction Word sense disambiguation (WSD) is to assign appropriate meaning to a given ambiguous word in a text. Corpus based method is one of the successful lines of research on WSD. Many supervised learning algorithms have been applied for WSD, ex. Bayesian learning (Leacock et al., 1998), exemplar based learning (Ng and Lee, 1996), decision list (Yarowsky, 2000), neural network (Towel and Voorheest, 1998), maximum entropy method (Dang et al., 2002), etc.. In this paper, we employ Naive Bayes classifier to perform WSD. Resolving the ambiguity of words usually relies on the contexts of their occurrences. The feature set used for context representation consists of local and topical features. Local features include part of speech tags of words within local context, morphological information of target word, local collocations, and syntactic relations between contextual words and target word, etc.. Topical features are bag of words occurred within topical context. Contextual features play an importa"
W04-1018,C00-1072,0,0.107966,"od without discourse structure analysis. Yet when the writing style of a document is rather free and the distribution of the themes is variable, that is the same theme can be distributed in several paragraphs not adjacent to each other, then the use of this method can’t be equally effective. To deal with a lot of Chinese documents which have free style of writing and flexible themes, a sentence-extraction summarization method created by detecting thematic areas is tried following such work as (Nomoto and Matsumoto, 2001; Salton et al., 1996; Salton et al., 1997; Carbonell and Goldstein, 1998; Lin and Hovy, 2000). The thematic areas detection in a document is obtained through the adaptive clustering of paragraphs (cf. Moens et al. 1999), so it can overcome in a certain degree the defects of the above methods in dealing with the documents with rather flexible theme distribution. 3 The Algorithm In this section, the proposed method will be introduced in detail. The method consists of the following three main stages: Stage 1: Find the different thematic areas in the document through paragraph clustering and clustering analysis. Stage 2: Select the most suitable sentence from each thematic area as the rep"
W04-1018,W00-0403,0,0.0422498,"d K-medoid paragraph clustering process, and the weight of Pj is calculated by formula (2). Put the objective function in K clustering results corresponding 3.1.4 Step 4: Thematic Area Detection Output the complete information table of each thematic area in the form of the representative paragraph and all the paragraphs and sentences covered by the thematic area. 3.2 Stage 2: Selection of the Thematic Representative Sentences To select a most suitable representative sentence from each thematic area, the author proposes the following method. This is in contrast with a method proposed by Radev (Radev et al., 2000 ), where the centroid of a cluster is selected as the representative one. Method: select the sentence which is most similar to the thematic area semantically as representative one. Before carrying out the method in detail, there are two problems to be solved: 1) The vector representation of sentence and thematic area The vector representation of sentence and thematic area is similar to that of paragraph introduced before. We only need to change the weight calculation field of the terms from the interior of paragraph to the interior of sentence or thematic area. Accordingly, we can describe th"
W04-1117,P98-1013,0,0.0266983,"Missing"
W04-1117,J02-3001,0,0.0259425,"Missing"
W04-1117,C02-1062,0,\N,Missing
W04-1117,C02-1091,0,\N,Missing
W06-0125,C04-1004,1,0.905303,"Missing"
W06-0125,I05-1047,1,0.897369,"Missing"
W06-0125,P02-1060,1,\N,Missing
W06-1649,J98-1006,0,0.112705,"Missing"
W06-1649,W02-1006,0,0.0248906,"cted graphs as follows: two instances u, v will be connected by an edge if u is among v’s 10 nearest neighbors, or if v is among u’s 10 nearest neighbors as measured by cosine or JS distance measure (following (Zhu and Ghahramani, 2002)). We used three types of features to capture the information in all the contextual sentences of target words in SENSEVAL-3 data for all the four algorithms: part-of-speech of neighboring words with position information, words in topical context without position information (after removing stop words), and local collocations (as same as the feature set used in (Lee and Ng, 2002) except that we did not use syntactic relations). We removed the features with occurrence frequency (counted in both training set and test set) less than 3 times. If the estimated sense number is more than the sense number in the initial tagged corpus XL , then the results from order identification based methods will consist of the instances from clusters of unknown classes. When assessing the agreement between these classification results and the known results on official test set, we will encounter the problem that there is no sense tag for each instance in unknown classes. Slonim and Tishby"
W06-1649,P05-1049,1,0.741051,"ion problem. Therefore the labeled data of other classes cannot be used when determining the positive labeled data for current class. ELP can use the labeled data of all the known classes to determine the seeds of unknown classes. It may explain why LPU’s performance is worse than ELP based sense disambiguation although LPU can correctly estimate the sense number in XL+U We used Jensen-Shannon (JS) divergence (Lin, 1991) as distance measure for semi-supervised clustering and ELP, since plain LP with JS divergence achieves better performance than that with cosine similarity on SENSEVAL-3 data (Niu et al., 2005). For the LP process in ELP algorithm, we constructed connected graphs as follows: two instances u, v will be connected by an edge if u is among v’s 10 nearest neighbors, or if v is among u’s 10 nearest neighbors as measured by cosine or JS distance measure (following (Zhu and Ghahramani, 2002)). We used three types of features to capture the information in all the contextual sentences of target words in SENSEVAL-3 data for all the four algorithms: part-of-speech of neighboring words with position information, words in topical context without position information (after removing stop words), a"
W06-1649,J98-1004,0,0.254649,"Missing"
W06-1649,P95-1026,0,0.146635,"se disambiguation (Yarowsky, 1995). Supervised methods usually rely on the information from previously sense tagged corpora to determine the senses of words in unseen texts. Semi-supervised methods for WSD are characterized in terms of exploiting unlabeled data in the learning procedure with the need of predefined sense inventories for target words. The information for semi-supervised sense disambiguation is usually obtained from bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994), or sense-tagged seed examples (Yarowsky, 1995). Some observations can be made on the previous supervised and semi-supervised methods. They always rely on hand-crafted lexicons (e.g., WordNet) as sense inventories. But these resources may miss domain-specific senses, which leads to incomplete sense tagged corpus. Therefore, sense taggers trained on the incomplete tagged corpus will misclassify some instances if the senses of these instances are not defined in sense inventories. For example, one performs WSD in information technology related texts using WordNet 2 as sense inventory. When disambiguating the word “boot” in the phrase “boot se"
W06-1649,P91-1034,0,0.304957,"vised sense disambiguation (Leacock et al., 1998), and semi-supervised sense disambiguation (Yarowsky, 1995). Supervised methods usually rely on the information from previously sense tagged corpora to determine the senses of words in unseen texts. Semi-supervised methods for WSD are characterized in terms of exploiting unlabeled data in the learning procedure with the need of predefined sense inventories for target words. The information for semi-supervised sense disambiguation is usually obtained from bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994), or sense-tagged seed examples (Yarowsky, 1995). Some observations can be made on the previous supervised and semi-supervised methods. They always rely on hand-crafted lexicons (e.g., WordNet) as sense inventories. But these resources may miss domain-specific senses, which leads to incomplete sense tagged corpus. Therefore, sense taggers trained on the incomplete tagged corpus will misclassify some instances if the senses of these instances are not defined in sense inventories. For example, one performs WSD in information technology related texts using WordNet 2 as sens"
W06-1649,J94-4003,0,0.0276465,"uation (Leacock et al., 1998), and semi-supervised sense disambiguation (Yarowsky, 1995). Supervised methods usually rely on the information from previously sense tagged corpora to determine the senses of words in unseen texts. Semi-supervised methods for WSD are characterized in terms of exploiting unlabeled data in the learning procedure with the need of predefined sense inventories for target words. The information for semi-supervised sense disambiguation is usually obtained from bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994), or sense-tagged seed examples (Yarowsky, 1995). Some observations can be made on the previous supervised and semi-supervised methods. They always rely on hand-crafted lexicons (e.g., WordNet) as sense inventories. But these resources may miss domain-specific senses, which leads to incomplete sense tagged corpus. Therefore, sense taggers trained on the incomplete tagged corpus will misclassify some instances if the senses of these instances are not defined in sense inventories. For example, one performs WSD in information technology related texts using WordNet 2 as sense inventory. When disam"
W06-1649,W04-0807,0,\N,Missing
W06-1667,A00-2030,0,0.046813,"Missing"
W06-1667,P04-1054,0,0.10059,"egawa et al. (2004)’s hierarchical clustering: no consideration of manifold structure in data, and requirement to provide cluster number by users. Experiment results on ACE corpora show that this spectral clustering based approach outperforms Hasegawa et al. (2004)’s hierarchical clustering method and a plain k-means clustering method. 1 Introduction The task of relation extraction is to identify various semantic relations between name entities from text. Prior work on automatic relation extraction come in three kinds: supervised learning algorithms (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi-supervised learning algorithms (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004), and unsupervised learning algorithm (Hasegawa et al., 2004). Among these methods, supervised learning is usually more preferred when a large amount of laCompared with supervised and semi-supervised methods, Hasegawa et al. (2004)’s unsupervised approach for relation extraction can overcome the difficulties on requirement of a large amount of labeled data and enumeration of all class labels. Hasegawa et al. (2004)’s method is to use a hierarchical clustering method"
W06-1667,W02-1010,0,0.0158005,"ties encoutered in Hasegawa et al. (2004)’s hierarchical clustering: no consideration of manifold structure in data, and requirement to provide cluster number by users. Experiment results on ACE corpora show that this spectral clustering based approach outperforms Hasegawa et al. (2004)’s hierarchical clustering method and a plain k-means clustering method. 1 Introduction The task of relation extraction is to identify various semantic relations between name entities from text. Prior work on automatic relation extraction come in three kinds: supervised learning algorithms (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi-supervised learning algorithms (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004), and unsupervised learning algorithm (Hasegawa et al., 2004). Among these methods, supervised learning is usually more preferred when a large amount of laCompared with supervised and semi-supervised methods, Hasegawa et al. (2004)’s unsupervised approach for relation extraction can overcome the difficulties on requirement of a large amount of labeled data and enumeration of all class labels. Hasegawa et al. (2004)’s method is to use a hier"
W06-1667,P05-1053,0,0.0362499,"Missing"
W06-1667,A00-2018,0,\N,Missing
W06-1667,P04-1053,0,\N,Missing
W09-1215,burchardt-etal-2006-salsa,0,0.031753,"Missing"
W09-1215,C96-1058,0,0.0333578,"f syntactic and semantic dependency parsing contains two folds: (1) identifying the syntactic head of each word and assigning the dependency relationship between the word and its head; (2) identifying predicates with proper senses and labeling semantic dependencies for them. For data-driven syntactic dependency parsing, many approaches are based on supervised learning using treebank or annotated datasets. Currently, graph-based and transition-based algorithms are two dominating approaches that are employed by many researchers, especially in previous CoNLL shared tasks. Graph-based algorithms (Eisner, 1996; McDonald et al., 2005) assume a series of dependency tree candidates for a sentence and the goal is to find the dependency tree with highest score. Transition-based algorithms (Yamada and Matsumoto, 2003; Nivre et al., 2004) utilize transition histories learned from dependencies within sentences to predict next state transition and build the optimal transition sequence. Although different strategies were considered, two approaches yielded comparable results at previous tasks. Semantic role labeling contains two problems: identification and labeling. Identification is a binary classification"
W09-1215,C04-1186,0,0.0942727,"and Matsumoto, 2003; Nivre et al., 2004) utilize transition histories learned from dependencies within sentences to predict next state transition and build the optimal transition sequence. Although different strategies were considered, two approaches yielded comparable results at previous tasks. Semantic role labeling contains two problems: identification and labeling. Identification is a binary classification problem, and the goal is to identify annotated units in a sentence; while labeling is a multi-class classification problem, which is to assign arguments with appropriate semantic roles. Hacioglu (2004) utilized predicate-argument structure and map dependency relations to semantic roles. Liu et al. (2005) combined two problems into a classification one, avoiding some annotated units being excluded due to some incorrect identification results. In addition, various features are also selected to improve accuracy of SRL. In this paper, we propose a pipelined approach for CoNLL-09 shared task on joint learning of syntactic and semantic dependencies, and describe our system that can handle multiple languages. In the system, we handle syntactic dependency parsing with a transition-based approach. F"
W09-1215,kawahara-etal-2002-construction,0,0.0173341,"Missing"
W09-1215,P05-1012,0,0.0664978,"d semantic dependency parsing contains two folds: (1) identifying the syntactic head of each word and assigning the dependency relationship between the word and its head; (2) identifying predicates with proper senses and labeling semantic dependencies for them. For data-driven syntactic dependency parsing, many approaches are based on supervised learning using treebank or annotated datasets. Currently, graph-based and transition-based algorithms are two dominating approaches that are employed by many researchers, especially in previous CoNLL shared tasks. Graph-based algorithms (Eisner, 1996; McDonald et al., 2005) assume a series of dependency tree candidates for a sentence and the goal is to find the dependency tree with highest score. Transition-based algorithms (Yamada and Matsumoto, 2003; Nivre et al., 2004) utilize transition histories learned from dependencies within sentences to predict next state transition and build the optimal transition sequence. Although different strategies were considered, two approaches yielded comparable results at previous tasks. Semantic role labeling contains two problems: identification and labeling. Identification is a binary classification problem, and the goal is"
W09-1215,W04-2407,0,0.0348489,"oper senses and labeling semantic dependencies for them. For data-driven syntactic dependency parsing, many approaches are based on supervised learning using treebank or annotated datasets. Currently, graph-based and transition-based algorithms are two dominating approaches that are employed by many researchers, especially in previous CoNLL shared tasks. Graph-based algorithms (Eisner, 1996; McDonald et al., 2005) assume a series of dependency tree candidates for a sentence and the goal is to find the dependency tree with highest score. Transition-based algorithms (Yamada and Matsumoto, 2003; Nivre et al., 2004) utilize transition histories learned from dependencies within sentences to predict next state transition and build the optimal transition sequence. Although different strategies were considered, two approaches yielded comparable results at previous tasks. Semantic role labeling contains two problems: identification and labeling. Identification is a binary classification problem, and the goal is to identify annotated units in a sentence; while labeling is a multi-class classification problem, which is to assign arguments with appropriate semantic roles. Hacioglu (2004) utilized predicate-argum"
W09-1215,W04-0308,0,0.0303155,"Missing"
W09-1215,W05-0627,0,0.0536644,"Missing"
W09-1215,J08-2004,0,0.0189949,"m the word to its predicate. · FAMILY: Relationship between the word and its predicate, including Child, Parent, Descendant, Ancestor, Sibling, Self and Null. · PRED_CHD_POS, PRED_CHD_DEPREL: Part-of-speech and dependency relations of all children of the word’s predicate. For different languages, some features mentioned above are invalid and should be removed, and some extended features could improve the performance of the classifier. In our system we mainly focus on Chinese, therefore, WORD and VOICE should be removed when processing Chinese data set. We also adopt some features proposed by (Xue, 2008): · POS_PATH_BA, POS_PATH_SB, POS_ PATH_LB: BA and BEI are functional words that impact the order of arguments. In PropBank, BA words have the POS tag BA, and BEI words have two POS tags: SB (short BEI) and LB (long BEI). in MaltParser, such as assigned parameters for LIBSVM and combined prediction strategy, and utilize improved approaches mentioned in section 2. For semantic dependency training and parsing, we choose the count of iteration as 100 and cutoff value as 10 for the ME model. Table 1 shows the training time for syntactic and semantic dependency of all languages. Parsing time for sy"
W09-1215,W03-3023,0,0.0650693,"entifying predicates with proper senses and labeling semantic dependencies for them. For data-driven syntactic dependency parsing, many approaches are based on supervised learning using treebank or annotated datasets. Currently, graph-based and transition-based algorithms are two dominating approaches that are employed by many researchers, especially in previous CoNLL shared tasks. Graph-based algorithms (Eisner, 1996; McDonald et al., 2005) assume a series of dependency tree candidates for a sentence and the goal is to find the dependency tree with highest score. Transition-based algorithms (Yamada and Matsumoto, 2003; Nivre et al., 2004) utilize transition histories learned from dependencies within sentences to predict next state transition and build the optimal transition sequence. Although different strategies were considered, two approaches yielded comparable results at previous tasks. Semantic role labeling contains two problems: identification and labeling. Identification is a binary classification problem, and the goal is to identify annotated units in a sentence; while labeling is a multi-class classification problem, which is to assign arguments with appropriate semantic roles. Hacioglu (2004) uti"
W09-1215,W08-2121,0,\N,Missing
W09-1215,W09-1201,0,\N,Missing
W09-1215,taule-etal-2008-ancora,0,\N,Missing
W97-0321,P96-1006,0,0.0754483,"s definitions with those of the words in the clusters. 1. Introduction Word sense disambiguation has long been one of the major concerns in natural language processing area (e.g., Bruce et al., 1994; Choueka et al., 1985; Gale et al., 1993; McRoy, 1992; Yarowsky 1992, 1994, 1995), whose aim is to identify the correct sense of a word in a particular context, among all of its senses defined in a dictionary or a thesaurus. Undoubtedly, effective disambiguation techniques are of great use in many natural language processing tasks, e.g., machine translation and information retrieving (Allen, 1995; Ng and Lee, 1996; Resnik, 1995), etc. Previous strategies for word sense disambiguation mainly fall into two categories: statistics-based method and exemplar-based method. Statistics-based method often requires large-scale corpora (e.g., Hirst, 1987; Luk, 1995), sense-tagging or not, monolingual or aligned bilingual, as training data to specify significant clues for each word sense. The method generally suffers from the problem of data sparseness. Moreover, huge corpora, especially sense-tagged or aligned ones, are not generally available in all domains for all languages. Exemplar-based method makes use of ty"
W97-0321,W95-0105,0,0.0219613,"those of the words in the clusters. 1. Introduction Word sense disambiguation has long been one of the major concerns in natural language processing area (e.g., Bruce et al., 1994; Choueka et al., 1985; Gale et al., 1993; McRoy, 1992; Yarowsky 1992, 1994, 1995), whose aim is to identify the correct sense of a word in a particular context, among all of its senses defined in a dictionary or a thesaurus. Undoubtedly, effective disambiguation techniques are of great use in many natural language processing tasks, e.g., machine translation and information retrieving (Allen, 1995; Ng and Lee, 1996; Resnik, 1995), etc. Previous strategies for word sense disambiguation mainly fall into two categories: statistics-based method and exemplar-based method. Statistics-based method often requires large-scale corpora (e.g., Hirst, 1987; Luk, 1995), sense-tagging or not, monolingual or aligned bilingual, as training data to specify significant clues for each word sense. The method generally suffers from the problem of data sparseness. Moreover, huge corpora, especially sense-tagged or aligned ones, are not generally available in all domains for all languages. Exemplar-based method makes use of typical contexts"
W97-0321,P93-1034,0,0.0813666,"Missing"
W97-0321,C92-2070,0,0.0296879,"algorithm to find the nodes (sense clusters) corresponding with sets of similar senses in the dendrogram. Given a word in a particular context&apos; the context would activate some clusters in the dendrogram, based on its similarity with the contexts of the words in the clusters, then the correct sense of the word could be determined by comparing its definitions with those of the words in the clusters. 1. Introduction Word sense disambiguation has long been one of the major concerns in natural language processing area (e.g., Bruce et al., 1994; Choueka et al., 1985; Gale et al., 1993; McRoy, 1992; Yarowsky 1992, 1994, 1995), whose aim is to identify the correct sense of a word in a particular context, among all of its senses defined in a dictionary or a thesaurus. Undoubtedly, effective disambiguation techniques are of great use in many natural language processing tasks, e.g., machine translation and information retrieving (Allen, 1995; Ng and Lee, 1996; Resnik, 1995), etc. Previous strategies for word sense disambiguation mainly fall into two categories: statistics-based method and exemplar-based method. Statistics-based method often requires large-scale corpora (e.g., Hirst, 1987; Luk, 1995), sens"
W97-0321,P94-1013,0,0.0123697,"r each word sense. The method generally suffers from the problem of data sparseness. Moreover, huge corpora, especially sense-tagged or aligned ones, are not generally available in all domains for all languages. Exemplar-based method makes use of typical contexts (exemplars) of a word sense, e.g., verbnoun collocations or adjective-noun collocations, and identifies the correct sense of a word in a particular context by comparing the context with the exemplars (Ng and Lee, 1996). Recently, some kinds of learning techniques have been applied to cumulatively acquire exemplars form large corpora (Yarowsky, 1994, 1995). But ideal resources from which to learn exemplars are not generally available for any languages. Moreover, the effectiveness of this method on disambiguating words in large-scale corpora into fine-grained sense distinctions needs to be further investigated (Ng and Lee, 1996). *The work is supportedby National ScienceFoundationof China. 187 both semantic space based on the contexts o f the monoapproaches is that neighboring words provide strong sense words, and structure the senses in the space as A common assumption held by and consistent clues for the correct sense o f a target a den"
W97-0321,P95-1026,0,0.0373827,"Missing"
W97-0321,P92-1032,0,\N,Missing
W97-0321,P95-1025,0,\N,Missing
W97-1004,P89-1010,0,0.140052,"similar ones, while in bound compositions, words cannot be replaced freely(Benson 1990). Free compositions are predictable, i.e., their reasonableness can be determined according to the syntactic and semantic properties of the words in them. While bound compositions are not predictable, i.e., their reasonableness cannot be derived from the syntactic and semantic properties of the words in them(Smadja 1993). Now with the availability of large-scale corpus, automatic acquisition of word compositions, especially word collocations from them have been extensively studied(e.g., Choueka et al. 1988; Church and Hanks 1989; Smadja 1993). The key of their methods is to make use of some statistical means, e.g., frequencies or mutual information, to quantify the compositional strength between words. These methods are more appropriate for retrieving bound compositions, while less appropriate for retrieving free ones. This is because in free compositions, words are related with each other in a more loose way, which may result in the invalidity of mutual information and other statistical means in distinguishing reasonable compositions from unreasonable ones. In this paper, we start from a different point to explore t"
W97-1004,W89-0240,0,0.0606214,"Missing"
W97-1004,P93-1024,0,0.0526087,"ned with another one to form a meaningful phrase, the words similar to t h e m in meaning can also be combined with each other. But it has been shown t h a t the similarity between words in meaning doesn&apos;t correspond to the similarity in compositional ability(Zhu 1982). So adopting semantic classes to construct compositional frames will result in considerable redundancy. An alternative to semantic class is word cluster based on distributional environment (Brown et al., 1992), which in general refers to the surrounding words distributed around certain word (e.g., Hatzivassiloglou et al., 1993; Pereira et al., 1993), or the classes of them(Bensch et al., 1995), or more complex statistical means (Dagan et al., 1993). According to the properties of the clusters in compositional frames, the clusters should be based on the environment, which, however, is narrowed in the given compositions. Because the given compositions are listed by hand, it is impossible to make use of statistical means to form the environment, the remaining choices are surrounding words or classes of them. Pereira et a1.(1993) put forward a method to cluster nouns in V-N compositions, taking the verbs which can combine with a noun as its"
W97-1004,J93-1007,0,0.053469,"Missing"
W97-1004,J90-1003,0,\N,Missing
W97-1004,H89-2012,0,\N,Missing
W97-1004,P95-1026,0,\N,Missing
W97-1004,P93-1023,0,\N,Missing
Y09-1021,P02-1045,0,0.0668292,"Missing"
Y09-1021,W00-0405,0,0.0532996,"ems. The remainder of the paper is organized as follows. Section 2 discusses related work. The proposed summarization approach is described in Section 3. The details of the experimentation are shown in Section 4. Section 5 presents our conclusion and future work. 2 Related Work Most of multi-document summarization methods can be categorized into two main paradigms, i.e. extractive and abstractive summarization. Extractive summarization often directly extracts important sentences in supervised, unsupervised or semi-supervised way based on the combination of a few implicit or explicit features (Goldstein et al., 2000; Radev et al., 2004), while abstractive summarization usually makes use of deep natural language understanding or generation technology to fuse or reformulate information (Knight and Marcu, 2000). In this paper, we focus on extractive summarization approach. Compared to single-document summarization, it is more likely for multi-document summarization to have repetitive contents and diverse subtopics across documents, so maximizing content salience and minimizing content redundancy has been recognized as one of the major difficulties. Many methods have been proposed to achieve this goal. Maxim"
Y09-1021,N03-1020,0,0.152635,"Missing"
Y09-1021,N01-1023,0,0.0154138,"ink structure analysis, stability-based random walk, global consistency or smoothness-based label propagation on the graph. Topic-sensitive LexRank (Haveliwala, 2003) extended the traditional LexRank algorithm by integrating the similarity between sentences and the given query. Wan et al. (2007) adopted a manifold-ranking algorithm to rank sentences by considering global information and emphasizing the high biased information richness in a score propagation process. Recently co-training algorithm has been successfully used in many natural language processing applications (Muller et al., 2002; Sarkar, 2001). Wong et al. (2008) applied co191 training algorithm to generic multi-document summarization, which trains two different classifiers on the same feature space to evaluate the importance of a sentence and needs a few manually labeled examples as training data. However, there is little research in applying cotraining based learning algorithm to query-focused multi-document summarization especially when manually labeled information is absent. In our approach, the major point of concern is how to employ the co-training algorithm to support better choosing query relevant sentences from two differe"
Y09-1021,C08-1124,0,0.0199088,"nalysis, stability-based random walk, global consistency or smoothness-based label propagation on the graph. Topic-sensitive LexRank (Haveliwala, 2003) extended the traditional LexRank algorithm by integrating the similarity between sentences and the given query. Wan et al. (2007) adopted a manifold-ranking algorithm to rank sentences by considering global information and emphasizing the high biased information richness in a score propagation process. Recently co-training algorithm has been successfully used in many natural language processing applications (Muller et al., 2002; Sarkar, 2001). Wong et al. (2008) applied co191 training algorithm to generic multi-document summarization, which trains two different classifiers on the same feature space to evaluate the importance of a sentence and needs a few manually labeled examples as training data. However, there is little research in applying cotraining based learning algorithm to query-focused multi-document summarization especially when manually labeled information is absent. In our approach, the major point of concern is how to employ the co-training algorithm to support better choosing query relevant sentences from two different but abundant feat"
Y09-1021,N07-1013,0,0.335833,"atural language understanding or generation technology to fuse or reformulate information (Knight and Marcu, 2000). In this paper, we focus on extractive summarization approach. Compared to single-document summarization, it is more likely for multi-document summarization to have repetitive contents and diverse subtopics across documents, so maximizing content salience and minimizing content redundancy has been recognized as one of the major difficulties. Many methods have been proposed to achieve this goal. Maximum marginal relevance (MMR) (Carbonell and Goldstein, 1998), GRASSHOPPER ranking (Zhu et al., 2007), diversity penalty (Zhang et al., 2005) and mixture models (Zhang et al., 2002) are commonly used approaches incorporating information salience and diversity into the ranking process. Among these approaches, GRASSHOPPER ranking tries to encourage the balance of salience and diversity in a unified framework, while other approaches deal with them separately. Inspired by PageRank and HITS algorithm, much focus has been put on adopting graphbased ranking algorithm like LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) to multi-document summarization. These algorithms general"
Y09-1021,W04-3252,0,\N,Missing
Y09-2006,W09-1215,1,0.824739,"eb knowledge bases. For semantic similarity of definitions, we consider the similarity of the core semantic roles, which primarily profile the features of definitions. We choose the verb based labeling architecture derived from PropBank in which ‘predicate’, ‘subject’ and ‘object’ are the core roles for a sentence. For each sentence in definition set we only label PRED, A0 and A1 and combine them to a triple. For example, the triple of Answer 1 in Table 1 is: {is |PRED, Greenhouse gas |A 0, gas |A1} For labeling of semantic roles, we utilize a method which we implemented in CoNLL shared task (Ren et al., 2009) to extract triples. They handle syntactic dependency parsing with a transition-based approach and utilize MaltParser 4 as the base model. We also utilize a Maximum Entropy model to identify predicate senses and classify arguments. Although definitions summarized from web knowledge bases are more precious, there still have some sentences that are not definitions. Since a definition sentence quite probably has the target of the definition, triples can only be acquired by these sentences. Also, co-reference resolution is made use of to help find actual definition sentences in web knowledge bases"
Y09-2006,P09-1023,0,0.0312389,"ments. 3.1 Definition Summarization Although web knowledge bases are cleaned up and generalized by manual work, they still permeate insignificant information that may decrease the performance of document retrieval or answering ranking. To solve this problem, some systems make use of summarizing methods to acquire important portions from definitions. But when definitions are not rich, the snippets summarizing from them are also sparse. Since some web knowledges such as Wikipedia provide links to combine enormous concepts, documents linked by these links allows us to obtain more reliable texts. Ye et al. (2009) proposed a novel approach that can produces summaries with various length. By building an extended document concept lattice model, concepts and non-textual features such as wiki article, infobox and outline are combined. Experiments showed that system performance outperformed not only traditional summarizing methods but also some soft-pattern approaches. In this paper, we utilize this approach to perform definitions in Wikipedia. Definitions summarized are put into basic definition set. Although most definitions can be found in Wikipedia, we still utilize other two web knowledges as a supplem"
Y09-2006,I05-1044,0,\N,Missing
Y09-2037,C04-1069,1,0.698313,"d to broad coverage task typically associated with IR. Kemps (2004) proposed a method to re-order retrieved documents by making use of manually assigned controlled vocabularies in documents. And it is reported that this re-ranking strategy significantly improves retrieved effectiveness on their experiments on German GIRT and French Amaryllis collections. Balinski and Danilowicz (2005) put forward a 1 2 http://www.wikipedia.org http://aclia.lti.cs.cmu.edu/wiki/TaskDefinition#Format 749 document re-ranking method that uses the distances between documents for modifying initial relevance weights. Yang et al. (2004, 2005) used query terms which occur in both query and top N (N<=30) retrieved documents to re-rank documents. Many research efforts have been made on how to apply clustering to get better retrieved results. Lee et al. (2001) proposed a model of information retrieval system that is based on a document re-ranking method using document clusters mentioned above. Anick and Vaithyanathan (1997) exploited the synergy between document clustering and phrasal analysis for the purpose of automatically constructing a context-based retrieval system. In their system, a context consists of two components, c"
