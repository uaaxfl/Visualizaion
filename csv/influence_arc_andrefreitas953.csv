2020.acl-main.657,P16-1084,0,0.0686134,"Missing"
2020.acl-main.657,2020.lrec-1.266,1,0.766707,"statements using natural language. The mathematical discourse is composed of a particular combination of words and mathematical terms, where terms follow a different set of syntactic rules and entail a specific lexicon. Nonetheless, words and mathematical terms are interdependent in the context of mathematical discourse. This phenomenon is exclusive to mathematical language, not found in any other natural, or artificial, language (Ganesalingam, 2013), providing a unique and challenging application for semantic evaluation and natural language processing. The natural language premise selection (Ferreira and Freitas, 2020) task is defined as: Definition (Natural language premise selection): Given a set of premises (or supporting facts) P in a mathematical corpus (containing both natural language and formulae) and a new conjecture c proposed by a user, predict those premises from P that will most likely be useful for generating a proof for c (i.e. partially entails c). A premise is considered relevant if the knowledge it provides can be reused for generating a proof for a given conjecture. We propose an approach to solve the natural premise selection task, representing all conjectures and premises as nodes and t"
2020.acl-main.657,D19-1534,0,0.0454235,"Missing"
2020.acl-main.657,D18-1132,0,0.063281,"Missing"
2020.acl-main.657,D19-1536,0,0.0541147,"Missing"
2020.lrec-1.266,D19-1371,0,0.0179037,"omparing number of hops needed for obtaining premises. We also verify on how state-of-the-art embedding models perform with such specific dataset. BERT (Devlin et al., 2019) is reported to have performed in different NLP tasks, including understanding numeracy (Wallace et al., 2019). In order to use BERT, we formulate the problem as a pairwise relevance classification problem, where we aim to classify if one mathematical text is connected to another. We do not perform any pre-processing for the expressions. For this experiment, we used the pre-trained BERT model bert-base-uncased and SciBERT (Beltagy et al., 2019) model scibert-scivocab-uncased, fine-tuning for our task with a sequence classifier, adding a linear layer on top of the transformer vectors. The results are presented in Table 6.. Even though BERT is not pre-trained using a mathematical corpus, it performs better than TF-IDF and PV-DBOW. SciBERT perform slightly better than BERT, since it was trained in a scientific corpus, but not in a mathematical corpus. This hints that BERT trained from scratch in a mathematical corpus could have even better results, however, this is outside the scope of this work. 2180 Model MAP SciBERT BERT 0.383 0.377"
2020.lrec-1.266,P16-1055,0,0.0182779,"also for evaluating semantic representations for mathematical discourse (including embeddings), textual entailment for mathematics and natural language inference in the context of mathematical texts. The contributions of this paper can be summarised as follows: • Proposal of a new NLP task: natural language premise selection. • A novel dataset, NL-PS, to support the evaluation of premise selection methods using natural language corpora. • Comparison of different baselines for the natural premise selection task. 2. Related Work NLP has been applied before in the context of general Mathematics. Chaganty and Liang (2016) proposes a new 1 2175 https://proofwiki.org/wiki/Main Page task for semantic analysis, the task of perspective generation, i.e., generating description to numerical values using other values as reference. Huang et al. (2016) analyze different approaches to solve mathematical word problems and concludes that it is still an unsolved challenge. Ganesalingam and Gowers (2017) propose a program that solves elementary mathematical problems, mainly in metric space theory, and presents solutions similar to the ones proposed by humans. The authors recognize that their system is operating at a disadvan"
2020.lrec-1.266,N19-1423,0,0.0827677,"be used to evaluate different approaches for the natural premise selection task. Using different baselines, we demonstrate the underlying interpretation challenges associated with the task. Keywords: mathematical text, mathematical language processing, mathematical text analysis 1. Introduction Comprehending mathematical text requires evaluating the semantics of its mathematical structures (such as expressions) and connecting its internal components with the respective definitions or premises (Greiner-Petter et al., 2019). State-of-the-art models for natural language processing, such as BERT (Devlin et al., 2019), have high scores for several tasks, such as entity recognition, textual entailment and machine translation, but they do not encode the intricate mathematical background knowledge needed to reason over mathematical discourse. The language of mathematics is composed of a combination of words and symbols, where symbols follow a different set of rules and have a specific alphabet. Nonetheless, word and symbols are interdependent in the context of mathematical discourse. This phenomenon is exclusive to mathematical language, not found in any other natural, or artificial, language (Ganesalingam, 2"
2020.lrec-1.266,P16-1084,0,0.10707,"can be summarised as follows: • Proposal of a new NLP task: natural language premise selection. • A novel dataset, NL-PS, to support the evaluation of premise selection methods using natural language corpora. • Comparison of different baselines for the natural premise selection task. 2. Related Work NLP has been applied before in the context of general Mathematics. Chaganty and Liang (2016) proposes a new 1 2175 https://proofwiki.org/wiki/Main Page task for semantic analysis, the task of perspective generation, i.e., generating description to numerical values using other values as reference. Huang et al. (2016) analyze different approaches to solve mathematical word problems and concludes that it is still an unsolved challenge. Ganesalingam and Gowers (2017) propose a program that solves elementary mathematical problems, mainly in metric space theory, and presents solutions similar to the ones proposed by humans. The authors recognize that their system is operating at a disadvantage because human language involves several constraints that rule out many sound and effective tactics for generating proofs. Wang et al. (2018) propose an approach to automatically formalize informal mathematics using stati"
2020.lrec-1.266,D19-1534,0,0.0608604,"even more challenging, as we present in Table 6., where we consider the transitivity with two and three hops of distance. From the results, we notice that the more hops needed to obtain the premise, the worse our baselines perform. 1-hop premises 2-hop premises 3-hop premises TFIDF PV-DBOW 0.089 0.052 0.038 0.073 0.047 0.031 Table 4: Comparing number of hops needed for obtaining premises. We also verify on how state-of-the-art embedding models perform with such specific dataset. BERT (Devlin et al., 2019) is reported to have performed in different NLP tasks, including understanding numeracy (Wallace et al., 2019). In order to use BERT, we formulate the problem as a pairwise relevance classification problem, where we aim to classify if one mathematical text is connected to another. We do not perform any pre-processing for the expressions. For this experiment, we used the pre-trained BERT model bert-base-uncased and SciBERT (Beltagy et al., 2019) model scibert-scivocab-uncased, fine-tuning for our task with a sequence classifier, adding a linear layer on top of the transformer vectors. The results are presented in Table 6.. Even though BERT is not pre-trained using a mathematical corpus, it performs bet"
2020.lrec-1.660,D13-1160,0,0.0498879,"Missing"
2020.lrec-1.660,W18-2607,0,0.0229215,"plausible answers, when multiple expected answers contradict each other, or an answer is not specific enough with respect to the question and a more specific answer is present. We annotate an answer as Wrong when it is factually wrong and a correct answer is present in the context. Required Reasoning It is important to understand what types of reasoning the benchmark evaluates, in order to be able to accredit various reasoning capabilities to the models it evaluates. Our proposed reasoning categories are inspired by those found in scientific question answering literature (Jansen et al., 2016; Boratko et al., 2018), as research in this area focuses on understanding the required reasoning capabilities. We include reasoning about the Temporal succession of events, Spatial reasoning about directions and environment, and Causal reasoning about the cause-effect relationship between events. We further annotate (multiplechoice) answers that can only be answered By Exclusion of every other alternative. We further extend the reasoning categories by operational logic, similar to those required in semantic parsing tasks (Berant et al., 2013), as solving those tasks typically requires “multi-hop” reasoning (Yang et"
2020.lrec-1.660,N19-1405,0,0.128342,"s like a complex question that requires the synthesis of different information across multiple documents, the keyword “2010” appears in the question and only in the sentence that answers it, considerably simplifying the search. Full example with 10 passages can be seen in Supplementary Materials C. ducing novel metrics (Dodge et al., 2019), surprisingly little is done to establish the quality of the data itself. Additionally, recent research arrived at worrisome findings: the data of those gold standards, which is usually gathered involving a crowd-sourcing step, suffers from flaws in design (Chen and Durrett, 2019a) or contains overly specific keywords (Jia and Liang, 2017). Furthermore, these gold standards contain “annotation artefacts”, cues that lead models into focusing on superficial aspects of text, such as lexical overlap and word order, instead of actual language understanding (McCoy et al., 2019; Gururangan et al., 2018). These weaknesses cast some doubt on whether the data can reliably evaluate the reading comprehension performance of the models they evaluate, i.e. if the models are indeed being assessed for their capability to read. 5359 Figure 1 shows an example from H OTPOT QA (Yang et al"
2020.lrec-1.660,N19-1423,0,0.0257318,"i et al., 2017; Sukhbaatar et al., 2015) that allow for efficient optimisation of neural networks consisting of multiple layers, hardware designed for deep learning purposes12 and software frameworks (Abadi et al., 2016; Paszke et al., 2017) that allow efficient development and testing of novel approaches. These factors enable researchers to produce models that are pre-trained on large scale corpora and provide contextualised word representations (Peters et al., 2018) that are shown to be a vital component towards solutions for a variety of natural language understanding tasks, including MRC (Devlin et al., 2019). Another important factor that led to the recent success in MRC-related tasks is the widespread availability of various large datasets, e.g., SQuAD (Rajpurkar et al., 2016), that provide sufficient examples for optimising statistical models. The combination of these factors yields notable results, even surpassing human performance (Lan et al., 2020). MRC is a generic task format that can be used to probe for various natural language understanding capabilities (Gardner et al., 2019). Therefore it is crucially important to establish a rigorous evaluation methodology in order to be able to draw"
2020.lrec-1.660,D19-1224,0,0.0146718,"] As of the 2010 census, the city had a population of 51,271. The U.S. Census Bureau estimated the population in 2013 to be 53,438. [. . . ] Question: What is the 2010 population of the city 2.1 miles southwest of Marietta Air Force Station? Figure 1: While initially this looks like a complex question that requires the synthesis of different information across multiple documents, the keyword “2010” appears in the question and only in the sentence that answers it, considerably simplifying the search. Full example with 10 passages can be seen in Supplementary Materials C. ducing novel metrics (Dodge et al., 2019), surprisingly little is done to establish the quality of the data itself. Additionally, recent research arrived at worrisome findings: the data of those gold standards, which is usually gathered involving a crowd-sourcing step, suffers from flaws in design (Chen and Durrett, 2019a) or contains overly specific keywords (Jia and Liang, 2017). Furthermore, these gold standards contain “annotation artefacts”, cues that lead models into focusing on superficial aspects of text, such as lexical overlap and word order, instead of actual language understanding (McCoy et al., 2019; Gururangan et al., 2"
2020.lrec-1.660,N19-1246,0,0.0319083,"the linguistic reasoning capabilities probed by a gold standard, therefore we include the appropriate category used by Wang et al. (2019). Specifically, we annotate occurrences that require understanding of Negation, Quantifiers (such as “every”, “some”, or “all”), Conditional (“if . . . then”) statements and the logical implications of Con-/Disjunction (i.e. “and” and “or”) in order to derive the expected answer. Finally, we investigate whether arithmetic reasoning re5361 quirements emerge in MRC gold standards as this can probe for reasoning that is not evaluated by simple answer retrieval (Dua et al., 2019). To this end, we annotate the presence of of Addition and Subtraction, answers that require Ordering of numerical values, Counting and Other occurrences of simple mathematical operations. An example can exhibit multiple forms of reasoning. Notably, we do not annotate any of the categories mentioned above if the expected answer is directly stated in the passage. For example, if the question asks “How many total points were scored in the game?” and the passage contains a sentence similar to “The total score of the game was 51 points”, it does not require any reasoning, in which case we annotate"
2020.lrec-1.660,N18-2017,0,0.136604,"(Dodge et al., 2019), surprisingly little is done to establish the quality of the data itself. Additionally, recent research arrived at worrisome findings: the data of those gold standards, which is usually gathered involving a crowd-sourcing step, suffers from flaws in design (Chen and Durrett, 2019a) or contains overly specific keywords (Jia and Liang, 2017). Furthermore, these gold standards contain “annotation artefacts”, cues that lead models into focusing on superficial aspects of text, such as lexical overlap and word order, instead of actual language understanding (McCoy et al., 2019; Gururangan et al., 2018). These weaknesses cast some doubt on whether the data can reliably evaluate the reading comprehension performance of the models they evaluate, i.e. if the models are indeed being assessed for their capability to read. 5359 Figure 1 shows an example from H OTPOT QA (Yang et al., 2018), a dataset that exhibits the last kind of weakness mentioned above, i.e., the presence of unique keywords in both the question and the passage (in close proximity to the expected answer). An evaluation methodology is vital to the fine-grained understanding of challenges associated with a single gold standard, in"
2020.lrec-1.660,C16-1278,0,0.16487,"ge features multiple plausible answers, when multiple expected answers contradict each other, or an answer is not specific enough with respect to the question and a more specific answer is present. We annotate an answer as Wrong when it is factually wrong and a correct answer is present in the context. Required Reasoning It is important to understand what types of reasoning the benchmark evaluates, in order to be able to accredit various reasoning capabilities to the models it evaluates. Our proposed reasoning categories are inspired by those found in scientific question answering literature (Jansen et al., 2016; Boratko et al., 2018), as research in this area focuses on understanding the required reasoning capabilities. We include reasoning about the Temporal succession of events, Spatial reasoning about directions and environment, and Causal reasoning about the cause-effect relationship between events. We further annotate (multiplechoice) answers that can only be answered By Exclusion of every other alternative. We further extend the reasoning categories by operational logic, similar to those required in semantic parsing tasks (Berant et al., 2013), as solving those tasks typically requires “multi-"
2020.lrec-1.660,D17-1215,0,0.273632,"ent information across multiple documents, the keyword “2010” appears in the question and only in the sentence that answers it, considerably simplifying the search. Full example with 10 passages can be seen in Supplementary Materials C. ducing novel metrics (Dodge et al., 2019), surprisingly little is done to establish the quality of the data itself. Additionally, recent research arrived at worrisome findings: the data of those gold standards, which is usually gathered involving a crowd-sourcing step, suffers from flaws in design (Chen and Durrett, 2019a) or contains overly specific keywords (Jia and Liang, 2017). Furthermore, these gold standards contain “annotation artefacts”, cues that lead models into focusing on superficial aspects of text, such as lexical overlap and word order, instead of actual language understanding (McCoy et al., 2019; Gururangan et al., 2018). These weaknesses cast some doubt on whether the data can reliably evaluate the reading comprehension performance of the models they evaluate, i.e. if the models are indeed being assessed for their capability to read. 5359 Figure 1 shows an example from H OTPOT QA (Yang et al., 2018), a dataset that exhibits the last kind of weakness m"
2020.lrec-1.660,P19-1262,0,0.195738,"Missing"
2020.lrec-1.660,D18-1546,0,0.0169594,"ed method to generate adversary examples to exploit that weakness. This method was further adapted to be fully automated (Wang and Bansal, 2018) and applied to different gold standards (Jiang and Bansal, 2019). Our proposed approach differs in that we aim to provide qualitative justifications for those quantitatively measured issues. Sanity Baselines Another line of research establishes sane baselines to provide more meaningful context to the raw performance scores of evaluated models. When removing integral parts of the task formulation such as question, the textual passage or parts thereof (Kaushik and Lipton, 2018) or restricting model complexity by design in order to suppress some required form of reasoning (Chen and Durrett, 2019b), models are still able to perform comparably to the state-of-the-art. This raises concerns about the perceived benchmark complexity and is related to our work in a broader sense as one of our goals is to estimate the complexity of benchmarks. Benchmark evaluation in NLP Beyond MRC, efforts similar to ours that pursue the goal of analysing the evaluation of established datasets exist in Natural Language Inference (Gururangan et al., 2018; McCoy et al., 2019). Their analyses"
2020.lrec-1.660,N18-1023,0,0.0646088,"Missing"
2020.lrec-1.660,W04-1013,0,0.0198724,"tric. Typical performance metrics are exact match (EM) or accuracy, i.e. the percentage of exactly predicted answers, and the F1 score – the harmonic mean between the precision and the recall of the predicted tokens compared to expected answer tokens. The overall F1 score can either be computed by averaging the F1 scores for every instance or by first averaging the precision and recall and then computing the F1 score from those averages (macro F1). Free-text answers, meanwhile, are evaluated by means of text generation and summarisation metrics such as BLEU (Papineni et al., 2001) or ROUGE-L (Lin, 2004). 2.2. Dimensions of Interest In this section we describe a methodology to categorise gold standards according to linguistic complexity, required reasoning and background knowledge, and their factual correctness. Specifically, we use those dimensions as highlevel categories of a qualitative annotation schema for annotating question, expected answer and the corresponding context. We further enrich the qualitative annotations by a metric based on lexical cues in order to approximate a lower bound for the complexity of the reading comprehen5360 Annotation Schema Supporting Fact Answer Type Paraph"
2020.lrec-1.660,N19-1112,0,0.203079,"e a common evaluation methodology for MRC gold standards and the first across-the-board qualitative evaluation of MRC datasets with respect to the proposed categories. 2. 2.1. Framework for MRC Gold Standard Analysis Problem definition We define the task of machine reading comprehension, the target application of the proposed methodology as follows: Given a paragraph P that consists of tokens (words) p1 , . . . , pnP and a question Q that consists of tokens q1 . . . qnQ , the goal is to retrieve an answer A with tokens a1 . . . anA . A is commonly constrained to be one of the following cases (Liu et al., 2019b), illustrated in Figure 2: • Multiple choice, where the goal is to predict A from a given set of choices A. • Cloze-style, where S is a sentence, and A and Q are obtained by removing a sequence of words such that Q = S − A. The task is to fill in the resulting gap in Q with the expected answer A to form S. • Span, where is a continuous subsequence of tokens from the paragraph (A ⊆ P ). Flavours include multiple spans as the correct answer or A ⊆ Q. Passage The Pats win the AFC East for the 9th straight year. The Patriots trailed 24-16 at the end of the third quarter. They scored on a 46-yard"
2020.lrec-1.660,P19-1334,0,0.136419,"ucing novel metrics (Dodge et al., 2019), surprisingly little is done to establish the quality of the data itself. Additionally, recent research arrived at worrisome findings: the data of those gold standards, which is usually gathered involving a crowd-sourcing step, suffers from flaws in design (Chen and Durrett, 2019a) or contains overly specific keywords (Jia and Liang, 2017). Furthermore, these gold standards contain “annotation artefacts”, cues that lead models into focusing on superficial aspects of text, such as lexical overlap and word order, instead of actual language understanding (McCoy et al., 2019; Gururangan et al., 2018). These weaknesses cast some doubt on whether the data can reliably evaluate the reading comprehension performance of the models they evaluate, i.e. if the models are indeed being assessed for their capability to read. 5359 Figure 1 shows an example from H OTPOT QA (Yang et al., 2018), a dataset that exhibits the last kind of weakness mentioned above, i.e., the presence of unique keywords in both the question and the passage (in close proximity to the expected answer). An evaluation methodology is vital to the fine-grained understanding of challenges associated with a"
2020.lrec-1.660,P19-1416,0,0.0868423,"Missing"
2020.lrec-1.660,2001.mtsummit-papers.68,0,0.0395985,", . . . , m} under a performance metric. Typical performance metrics are exact match (EM) or accuracy, i.e. the percentage of exactly predicted answers, and the F1 score – the harmonic mean between the precision and the recall of the predicted tokens compared to expected answer tokens. The overall F1 score can either be computed by averaging the F1 scores for every instance or by first averaging the precision and recall and then computing the F1 score from those averages (macro F1). Free-text answers, meanwhile, are evaluated by means of text generation and summarisation metrics such as BLEU (Papineni et al., 2001) or ROUGE-L (Lin, 2004). 2.2. Dimensions of Interest In this section we describe a methodology to categorise gold standards according to linguistic complexity, required reasoning and background knowledge, and their factual correctness. Specifically, we use those dimensions as highlevel categories of a qualitative annotation schema for annotating question, expected answer and the corresponding context. We further enrich the qualitative annotations by a metric based on lexical cues in order to approximate a lower bound for the complexity of the reading comprehen5360 Annotation Schema Supporting"
2020.lrec-1.660,N18-1202,0,0.0308671,"MRC). This is mostly due to wide-spread success of advances in various facets of deep learning related research, such as novel architectures (Vaswani et al., 2017; Sukhbaatar et al., 2015) that allow for efficient optimisation of neural networks consisting of multiple layers, hardware designed for deep learning purposes12 and software frameworks (Abadi et al., 2016; Paszke et al., 2017) that allow efficient development and testing of novel approaches. These factors enable researchers to produce models that are pre-trained on large scale corpora and provide contextualised word representations (Peters et al., 2018) that are shown to be a vital component towards solutions for a variety of natural language understanding tasks, including MRC (Devlin et al., 2019). Another important factor that led to the recent success in MRC-related tasks is the widespread availability of various large datasets, e.g., SQuAD (Rajpurkar et al., 2016), that provide sufficient examples for optimising statistical models. The combination of these factors yields notable results, even surpassing human performance (Lan et al., 2020). MRC is a generic task format that can be used to probe for various natural language understanding"
2020.lrec-1.660,D16-1264,0,0.167831,"Missing"
2020.lrec-1.660,P18-2124,0,0.0224211,"ppears in a sentence. The resulting taxonomy of the framework is shown in Figure 3. The full catalogue of features, their description, detailed annotation guideline as well as illustrating examples can be found in Supplementary Material A.3 3. 3.1. Application of the Framework Candidate Datasets We select contemporary MRC benchmarks to represent all four commonly used problem definitions (Liu et al., 2019b). In selecting relevant datasets, we do not consider those that are considered “solved”, i.e. where the state of the art performance surpasses human performance, as is the case with SQ UAD (Rajpurkar et al., 2018; Lan et al., 2020). Concretely, we selected gold standards that fit our problem definition and were published in the years 2016 to 2019, have at least (2019 − publication year) × 20 citations, and bucket them according to the answer selection styles as described in Section 2.1. We randomly draw one from each bucket and add two randomly drawn datasets from the candidate pool. This leaves us with the datasets described in Table 1. For a more detailed description, we refer to Supplementary Material D and the respective publications accompanying the datasets. 3.2. Annotation Task We randomly sele"
2020.lrec-1.660,N18-2002,0,0.0361376,"Missing"
2020.lrec-1.660,P19-1452,0,0.0133897,"the reliance on factual knowledge, that is (Geo)political/Legal, Cultural/Historic, Technical/Scientific and Other Domain Specific knowledge about the world that can be expressed as a set of facts. On the other hand, we denote Intuitive knowledge requirements, which is challenging to express as a set of facts, such as the knowledge that a parenthetic numerical expression next to a person’s name in a biography usually denotes his life span. Linguistic Complexity Another dimension of interest is the evaluation of various linguistic capabilities of MRC models (Goldberg, 2019; Liu et al., 2019a; Tenney et al., 2019). We aim to establish which linguistic phenomena are probed by gold standards and to which degree. To that end, we draw inspiration from the annotation schema used by Wang et al. (2019), and adapt it around lexical semantics and syntax. More specifically, we annotate features that introduce variance between the supporting facts and the question. With regard to lexical semantics, we focus on the use of redundant words that do not alter the meaning of a sentence for the task of retrieving the expected answer (Redundancy), requirements on the understanding of words’ semantic fields (Lexical Entai"
2020.lrec-1.660,W17-2623,0,0.0265288,"ed further in the complexity analysis. Factual Correctness An important factor for the quality of a benchmark is its factual correctness, because on the one hand, the presence of factually wrong or debatable examples introduces an upper bound for the achievable performance of models on those gold standards. On the other hand, it is hard to draw conclusions about the correctness of answers produced by a model that is evaluated on partially incorrect data. One way by which developers of modern crowd-sourced gold standards ensure quality is by having the same entry annotated by multiple workers (Trischler et al., 2017) and keeping only those with high agreement. We investigate whether this method is enough to establish a sound ground truth answer that is unambiguously correct. Concretely we annotate an answer as Debatable when the passage features multiple plausible answers, when multiple expected answers contradict each other, or an answer is not specific enough with respect to the question and a more specific answer is present. We annotate an answer as Wrong when it is factually wrong and a correct answer is present in the context. Required Reasoning It is important to understand what types of reasoning t"
2020.lrec-1.660,N18-2091,0,0.0250057,"ugh not as prominent as the research on novel architecture, there has been steady progress in critically investigating the data and evaluation aspects of NLP and machine learning in general and MRC in particular. Adversarial Evaluation The authors of the A DD S ENT algorithm (Jia and Liang, 2017) show that MRC models trained and evaluated on the SQ UAD dataset pay too little attention to details that might change the semantics of a sentence, and propose a crowd-sourcing based method to generate adversary examples to exploit that weakness. This method was further adapted to be fully automated (Wang and Bansal, 2018) and applied to different gold standards (Jiang and Bansal, 2019). Our proposed approach differs in that we aim to provide qualitative justifications for those quantitatively measured issues. Sanity Baselines Another line of research establishes sane baselines to provide more meaningful context to the raw performance scores of evaluated models. When removing integral parts of the task formulation such as question, the textual passage or parts thereof (Kaushik and Lipton, 2018) or restricting model complexity by design in order to suppress some required form of reasoning (Chen and Durrett, 2019"
2020.lrec-1.660,Q18-1042,0,0.0241979,"yond MRC, efforts similar to ours that pursue the goal of analysing the evaluation of established datasets exist in Natural Language Inference (Gururangan et al., 2018; McCoy et al., 2019). Their analyses reveal the existence of biases in training and evaluation data that can be approximated with simple majoritybased heuristics. Because of these biases, trained models fail to extract the semantics that are required for the correct inference. Furthermore, a fair share of work was done to reveal gender bias in coreference resolution datasets and models (Rudinger et al., 2018; Zhao et al., 2018; Webster et al., 2018). Annotation Taxonomies Finally, related to our framework are works that introduce annotation categories for gold standards evaluation. Concretely, we build our annotation framework around linguistic features that were introduced in the GLUE suite (Wang et al., 2019) and the reasoning categories introduced in the W ORLD T REE dataset (Jansen et al., 2016). A qualitative analysis complementary to ours, with focus on the unanswerability patterns in datasets that feature unanswerable questions was done by Yatskar (2019). 5. Conclusion In this paper, we introduce a novel framework to characterise"
2020.lrec-1.660,Q18-1021,0,0.0206467,"h in this area focuses on understanding the required reasoning capabilities. We include reasoning about the Temporal succession of events, Spatial reasoning about directions and environment, and Causal reasoning about the cause-effect relationship between events. We further annotate (multiplechoice) answers that can only be answered By Exclusion of every other alternative. We further extend the reasoning categories by operational logic, similar to those required in semantic parsing tasks (Berant et al., 2013), as solving those tasks typically requires “multi-hop” reasoning (Yang et al., 2018; Welbl et al., 2018). When an answer can only be obtained by combining information from different sentences joined by mentioning a common entity, concept, date, fact or event (from here on called entity), we annotate it as Bridge. We further annotate the cases, when the answer is a concrete entity that satisfies a Constraint specified in the question, when it is required to draw a Comparison of multiple entities’ properties or when the expected answer is an Intersection of their properties (e.g. “What do Person A and Person B have in common?”) We are interested in the linguistic reasoning capabilities probed by a"
2020.lrec-1.660,D19-1260,0,0.0237727,"Missing"
2020.lrec-1.660,D18-1259,0,0.142707,"rett, 2019a) or contains overly specific keywords (Jia and Liang, 2017). Furthermore, these gold standards contain “annotation artefacts”, cues that lead models into focusing on superficial aspects of text, such as lexical overlap and word order, instead of actual language understanding (McCoy et al., 2019; Gururangan et al., 2018). These weaknesses cast some doubt on whether the data can reliably evaluate the reading comprehension performance of the models they evaluate, i.e. if the models are indeed being assessed for their capability to read. 5359 Figure 1 shows an example from H OTPOT QA (Yang et al., 2018), a dataset that exhibits the last kind of weakness mentioned above, i.e., the presence of unique keywords in both the question and the passage (in close proximity to the expected answer). An evaluation methodology is vital to the fine-grained understanding of challenges associated with a single gold standard, in order to understand in greater detail which capabilities of MRC models it evaluates. More importantly, it allows to draw comparisons between multiple gold standards and between the results of respective state-of-the-art models that are evaluated on them. In this work, we take a step b"
2020.lrec-1.660,N19-1241,0,0.049246,"te between Span, where an answer is a continuous span taken from the passage, Paraphrasing, where the answer is a paraphrase of a text span, Unanswerable, where there is no answer present in the context, and Generated, if it does not fall into any of the other categories. It is not sufficient for an answer to restate the question or combine multiple Span or Paraphrasing answers to be annotated as Generated. It is worth mentioning that we focus our investigations on answerable questions. For a complementary qualitative analysis that categorises unanswerable questions, the reader is referred to Yatskar (2019). Furthermore, we mark a sentence as Supporting Fact if it contains evidence required to produce the expected answer, as they are used further in the complexity analysis. Factual Correctness An important factor for the quality of a benchmark is its factual correctness, because on the one hand, the presence of factually wrong or debatable examples introduces an upper bound for the achievable performance of models on those gold standards. On the other hand, it is hard to draw conclusions about the correctness of answers produced by a model that is evaluated on partially incorrect data. One way b"
2020.lrec-1.660,N18-2003,0,0.0267103,"valuation in NLP Beyond MRC, efforts similar to ours that pursue the goal of analysing the evaluation of established datasets exist in Natural Language Inference (Gururangan et al., 2018; McCoy et al., 2019). Their analyses reveal the existence of biases in training and evaluation data that can be approximated with simple majoritybased heuristics. Because of these biases, trained models fail to extract the semantics that are required for the correct inference. Furthermore, a fair share of work was done to reveal gender bias in coreference resolution datasets and models (Rudinger et al., 2018; Zhao et al., 2018; Webster et al., 2018). Annotation Taxonomies Finally, related to our framework are works that introduce annotation categories for gold standards evaluation. Concretely, we build our annotation framework around linguistic features that were introduced in the GLUE suite (Wang et al., 2019) and the reasoning categories introduced in the W ORLD T REE dataset (Jansen et al., 2016). A qualitative analysis complementary to ours, with focus on the unanswerability patterns in datasets that feature unanswerable questions was done by Yatskar (2019). 5. Conclusion In this paper, we introduce a novel fra"
2020.lrec-1.660,P02-1040,0,\N,Missing
2021.acl-demo.23,N19-1423,0,0.510728,"s the application of probing methods to the user’s inputs. 1 Introduction Recent interest in investigating the intermediate features present in neural models’ representations has led to the use of structural analysis methods such as probing. At its simplest, probing1 is the training of an external classifier model (a “probe”) to determine the extent to which a set of auxiliary target feature labels can be predicted from the internal model representations. For example, probing studies have been carried out to determine whether word and sentence representations generated by models such as BERT (Devlin et al., 2019) capture intermediate syntactic and semantic features such as parts of speech and dependency labels (Hewitt and Manning, 2019b; Tenney et al., 2019b) and lexical relations (Vuli´c et al., 2020). Various problems can arise when performing probing experiments (Hewitt and Liang, 2019), such as achieving a high probing accuracy without being due to a high mutual information between the representation and the auxiliary task labels. This has prompted much recent work on establishing more reliable methodologies for probing (Hewitt and Liang, 2019; Voita and Titov, 2020; Pimentel et al., 2020b,a). The"
2021.acl-demo.23,D19-1275,0,0.0733101,"ning of an external classifier model (a “probe”) to determine the extent to which a set of auxiliary target feature labels can be predicted from the internal model representations. For example, probing studies have been carried out to determine whether word and sentence representations generated by models such as BERT (Devlin et al., 2019) capture intermediate syntactic and semantic features such as parts of speech and dependency labels (Hewitt and Manning, 2019b; Tenney et al., 2019b) and lexical relations (Vuli´c et al., 2020). Various problems can arise when performing probing experiments (Hewitt and Liang, 2019), such as achieving a high probing accuracy without being due to a high mutual information between the representation and the auxiliary task labels. This has prompted much recent work on establishing more reliable methodologies for probing (Hewitt and Liang, 2019; Voita and Titov, 2020; Pimentel et al., 2020b,a). These approaches introduce various steps such as controlling and varying model complexity and structure, including randomized control tasks and incorporating more informative metrics such as selectivity (Hewitt and Liang, 2019) and minimum description length (Voita and Titov, 2020). T"
2021.acl-demo.23,N19-1419,0,0.443887,"te features present in neural models’ representations has led to the use of structural analysis methods such as probing. At its simplest, probing1 is the training of an external classifier model (a “probe”) to determine the extent to which a set of auxiliary target feature labels can be predicted from the internal model representations. For example, probing studies have been carried out to determine whether word and sentence representations generated by models such as BERT (Devlin et al., 2019) capture intermediate syntactic and semantic features such as parts of speech and dependency labels (Hewitt and Manning, 2019b; Tenney et al., 2019b) and lexical relations (Vuli´c et al., 2020). Various problems can arise when performing probing experiments (Hewitt and Liang, 2019), such as achieving a high probing accuracy without being due to a high mutual information between the representation and the auxiliary task labels. This has prompted much recent work on establishing more reliable methodologies for probing (Hewitt and Liang, 2019; Voita and Titov, 2020; Pimentel et al., 2020b,a). These approaches introduce various steps such as controlling and varying model complexity and structure, including randomized co"
2021.acl-demo.23,2021.ccl-1.108,0,0.0701607,"Missing"
2021.acl-demo.23,J93-2004,0,0.0757901,"by prior visualizations and hypothesized relationships. • As far as possible, stick to comparing representations of the same sizes. Lowerdimensional representations may reach their maximum accuracy at lower probe complexity values; as such they may give the “appearance” of superior probe accuracy scores to larger representations. For this reason, it is also important that you investigate a sufficiently large range of model complexities. 5 Case Study To demonstrate the Probe-Ably system, we include an implementation of a Part-Of-Speech tagging auxiliary task based on the Penn Treebank corpus (Marcus et al., 1993). It has been used multiple times in works on probing methodology (Hewitt and Liang, 2019; Voita and Titov, 2020; Pimentel et al., 2020b). We use the custom control task from (Hewitt and Liang, 2019). Using linear models as probes, we compare the probing results for different layers of BERT (bert-base-uncased) 199 pre-trained on the masked language modelling task (Devlin et al., 2019), across 50 probing runs. The results are consistent with observations in (Tenney et al., 2019a), which note that syntactic features (such as part of speech tags) are more prevalent in earlier layers of BERT. This"
2021.acl-demo.23,2020.acl-main.420,0,0.069973,"as BERT (Devlin et al., 2019) capture intermediate syntactic and semantic features such as parts of speech and dependency labels (Hewitt and Manning, 2019b; Tenney et al., 2019b) and lexical relations (Vuli´c et al., 2020). Various problems can arise when performing probing experiments (Hewitt and Liang, 2019), such as achieving a high probing accuracy without being due to a high mutual information between the representation and the auxiliary task labels. This has prompted much recent work on establishing more reliable methodologies for probing (Hewitt and Liang, 2019; Voita and Titov, 2020; Pimentel et al., 2020b,a). These approaches introduce various steps such as controlling and varying model complexity and structure, including randomized control tasks and incorporating more informative metrics such as selectivity (Hewitt and Liang, 2019) and minimum description length (Voita and Titov, 2020). To make these methods more accessible and quick to implement for any user wishing to probe the representations of their neural models in line with the evolving suggested methodologies, we introduce Probe-Ably: an extendable probing framework which supports and automates the application of suggested best pract"
2021.acl-demo.23,P19-1452,0,0.320678,"al models’ representations has led to the use of structural analysis methods such as probing. At its simplest, probing1 is the training of an external classifier model (a “probe”) to determine the extent to which a set of auxiliary target feature labels can be predicted from the internal model representations. For example, probing studies have been carried out to determine whether word and sentence representations generated by models such as BERT (Devlin et al., 2019) capture intermediate syntactic and semantic features such as parts of speech and dependency labels (Hewitt and Manning, 2019b; Tenney et al., 2019b) and lexical relations (Vuli´c et al., 2020). Various problems can arise when performing probing experiments (Hewitt and Liang, 2019), such as achieving a high probing accuracy without being due to a high mutual information between the representation and the auxiliary task labels. This has prompted much recent work on establishing more reliable methodologies for probing (Hewitt and Liang, 2019; Voita and Titov, 2020; Pimentel et al., 2020b,a). These approaches introduce various steps such as controlling and varying model complexity and structure, including randomized control tasks and incorp"
2021.acl-demo.23,2020.emnlp-demos.15,0,0.0356937,"sistent with observations in (Tenney et al., 2019a), which note that syntactic features (such as part of speech tags) are more prevalent in earlier layers of BERT. This case study is available as a ready-to-run example. 6 providing the infrastructure to run our experiments. Thanks to Alber Santos for the helpful discussions. Related Work Previous interpretability tools for neural models have focused on gradient-based methods (Wallace et al., 2019), the visualization of attention weights (Vig, 2019) and other tools focusing on NLP model explainability and interpretability (Wexler et al., 2020; Tenney et al., 2020). The ongoing discussion on probing, auxiliary tasks and the surrounding best practices can be traced back to the early definitions in (Alain and Bengio, 2018), where it was first described as diagnostic classification. Early probing studies in NLP include (Zhang and Bowman, 2018) and (Tenney et al., 2019c), the former being an early example of the importance of comparing with randomized representations or labels. Further discussion has introduced control tasks and the selectivity metric (Hewitt and Liang, 2019), formalized notions of ease of extraction (Voita and Titov, 2020) and described ot"
2021.acl-demo.23,2020.emnlp-main.14,0,0.277795,"enerated by models such as BERT (Devlin et al., 2019) capture intermediate syntactic and semantic features such as parts of speech and dependency labels (Hewitt and Manning, 2019b; Tenney et al., 2019b) and lexical relations (Vuli´c et al., 2020). Various problems can arise when performing probing experiments (Hewitt and Liang, 2019), such as achieving a high probing accuracy without being due to a high mutual information between the representation and the auxiliary task labels. This has prompted much recent work on establishing more reliable methodologies for probing (Hewitt and Liang, 2019; Voita and Titov, 2020; Pimentel et al., 2020b,a). These approaches introduce various steps such as controlling and varying model complexity and structure, including randomized control tasks and incorporating more informative metrics such as selectivity (Hewitt and Liang, 2019) and minimum description length (Voita and Titov, 2020). To make these methods more accessible and quick to implement for any user wishing to probe the representations of their neural models in line with the evolving suggested methodologies, we introduce Probe-Ably: an extendable probing framework which supports and automates the application"
2021.acl-demo.23,2020.emnlp-main.586,0,0.0279423,"Missing"
2021.acl-demo.23,D19-3002,0,0.0177566,"for different layers of BERT (bert-base-uncased) 199 pre-trained on the masked language modelling task (Devlin et al., 2019), across 50 probing runs. The results are consistent with observations in (Tenney et al., 2019a), which note that syntactic features (such as part of speech tags) are more prevalent in earlier layers of BERT. This case study is available as a ready-to-run example. 6 providing the infrastructure to run our experiments. Thanks to Alber Santos for the helpful discussions. Related Work Previous interpretability tools for neural models have focused on gradient-based methods (Wallace et al., 2019), the visualization of attention weights (Vig, 2019) and other tools focusing on NLP model explainability and interpretability (Wexler et al., 2020; Tenney et al., 2020). The ongoing discussion on probing, auxiliary tasks and the surrounding best practices can be traced back to the early definitions in (Alain and Bengio, 2018), where it was first described as diagnostic classification. Early probing studies in NLP include (Zhang and Bowman, 2018) and (Tenney et al., 2019c), the former being an early example of the importance of comparing with randomized representations or labels. Further discu"
2021.acl-demo.23,W18-5448,0,0.0185762,"nks to Alber Santos for the helpful discussions. Related Work Previous interpretability tools for neural models have focused on gradient-based methods (Wallace et al., 2019), the visualization of attention weights (Vig, 2019) and other tools focusing on NLP model explainability and interpretability (Wexler et al., 2020; Tenney et al., 2020). The ongoing discussion on probing, auxiliary tasks and the surrounding best practices can be traced back to the early definitions in (Alain and Bengio, 2018), where it was first described as diagnostic classification. Early probing studies in NLP include (Zhang and Bowman, 2018) and (Tenney et al., 2019c), the former being an early example of the importance of comparing with randomized representations or labels. Further discussion has introduced control tasks and the selectivity metric (Hewitt and Liang, 2019), formalized notions of ease of extraction (Voita and Titov, 2020) and described other strategies for taking model complexity into account (Pimentel et al., 2020a). 7 Conclusion While probing can be used to explore hypotheses about linguistic (or general) features present in model representations, there are various pitfalls that can lead to premature or incorrec"
2021.eacl-main.15,D19-5311,0,0.508511,"a multi-hop inference problem, where multiple pieces of evidence have to be aggregated to arrive at the final answer (Thayaparan et al., 2020; Khashabi et al., 2018; Khot et al., 2017; Jansen et al., 2017). Aggregation methods based on lexical overlaps and explicit constraints suffer from semantic drift (Khashabi et al., 2019; Fried et al., 2015) – i.e. the tendency of composing spurious inference chains leading to wrong conclusions. One way to contain semantic drift is to leverage common explanatory patterns in explanationcentred corpora (Jansen et al., 2018). Transformers (Das et al., 2019; Chia et al., 2019) represent the state-of-the-art for explanation reconstruction in this setting (Jansen and Ustalov, 2019). However, these models require high computational resources that prevent their applicability to large corpora. On the other hand, approaches based on IR techniques are readily scalable. The approach described in this paper preserves the scalability of IR methods, obtaining, at the same time, performances competitive with Transformers. Thanks to this feature, the framework can be flexibly applied in combination with downstream question answering models. Our findings are in line with previou"
2021.eacl-main.15,D19-5313,0,0.0596662,"Missing"
2021.eacl-main.15,N19-1423,0,0.22891,"(Jansen and Ustalov, 2019), the performances of the models are evaluated via Mean Average Precision (MAP) of the explanation ranking produced for a given question qj and its correct answer aj . Table 1 illustrates the score achieved by our best implementation compared to state-of-the-art approaches in the literature. Previous approaches are grouped into four categories: Transformers, Information Retrieval with re-ranking, One-step Information Retrieval, and Feature-based models. Transformers. This class of approaches employs the gold explanations in the corpus to train a BERT language model (Devlin et al., 2019). The best-performing system (Das et al., 2019) adopts a multi-step retrieval strategy. In the first step, it returns the top K sentences ranked by a TF-IDF model. In the second step, BERT is used to rerank the paths composed of all the facts that are within 1-hop from the first retrieved set. Similarly, other approaches adopt BERT to re-rank each fact 204 MAP Model MAP Model All Central Grounding Lexical Glue RS TF-IDF RS BM25 42.8 46.1 43.4 46.6 25.4 23.3 8.2 10.7 US TF-IDF US BM25 21.6 21.9 16.9 18.1 22.0 16.7 RS TF-IDF + US TF-IDF RS TF-IDF + US BM25 RS BM25 + US TF-IDF RS BM25 + US BM25 4"
2021.eacl-main.15,D19-5312,0,0.0355075,"Missing"
2021.eacl-main.15,Q15-1015,0,0.622629,"tion 200 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 200–211 April 19 - 23, 2021. ©2021 Association for Computational Linguistics contains a set of concrete sentences that are conceptually connected with q and a (f1 ,f2 and f3 ), along with a set of abstract facts that require multi-hop inference (f4 and f5 ). Previous work has shown that constructing long explanations is challenging due to semantic drift – i.e. the tendency of composing out-ofcontext inference chains as the number of hops increases (Khashabi et al., 2019; Fried et al., 2015). While existing approaches build explanations considering each question in isolation (Khashabi et al., 2018; Khot et al., 2017), we hypothesise that semantic drift can be tackled by leveraging explanatory patterns emerging in clusters of similar questions. In Science, a given statement is considered explanatory to the extent it performs unification (Friedman, 1974; Kitcher, 1981, 1989), that is showing how a set of initially disconnected phenomena are the expression of the same regularity. An example of unification is Newton’s law of universal gravitation, which unifies the motion of planets"
2021.eacl-main.15,C16-1278,0,0.147871,"US), resulting in a larger improvement on many-hops explanations (6+ facts). Similarly, Figure 2b illustrates the Precision@K. As shown in the graph, the drop in precision for the US model exhibits the slowest degradation. Similarly to what observed for many-hops explanations, the US score contributes to the robustness of the RS + US model, making it able to reconstruct more precise explanations. As discussed in section 4.4, this feature has a positive impact on question answering. Semantic drift. Science questions in the Worldtree corpus require an average of six facts in their explanations (Jansen et al., 2016). Long explanations typically include sentences that share few terms with question and answer, increasing the probability of semantic drift. Therefore, to test the impact of the Unification Score on the robustness of the model, we measure the performance in the reconstruction of many-hops explanations. Figure 2a shows the change in MAP score for the RS + US, RS and US models (BM25) with increasing explanation length. The fast drop in performance for the Relevance Score reflects the complexity of the task. This drop occurs because the RS model is not able to rank abstract explanatory facts. Con"
2021.eacl-main.15,J17-2005,0,0.217257,"Missing"
2021.eacl-main.15,L18-1433,0,0.777796,"† , Andr´e Freitas†‡ Department of Computer Science, University of Manchester, United Kingdom† Idiap Research Institute, Switzerland‡ {marco.valentino,mokanarangan.thayaparan,andre.freitas} @manchester.ac.uk Abstract only to measure the performance on answer prediction, but also the ability of a QA system to provide explanations for the underlying reasoning process. The need for explainability and a quantitative methodology for its evaluation have conducted to the creation of shared tasks on explanation reconstruction (Jansen and Ustalov, 2019) using corpora of explanations such as Worldtree (Jansen et al., 2018, 2016). Given a science question, explanation reconstruction consists in regenerating the gold explanation that supports the correct answer through the combination of a series of atomic facts. While most of the existing benchmarks for multi-hop QA require the composition of only 2 supporting sentences or paragraphs (e.g. QASC (Khot et al., 2019), HotpotQA (Yang et al., 2018)), the explanation reconstruction task requires the aggregation of an average of 6 facts (and as many as ≈20), making it particularly hard for multi-hop reasoning models. Moreover, the structure of the explanations affects"
2021.eacl-main.15,H89-1033,0,0.426349,"Missing"
2021.eacl-main.15,P17-2049,0,0.0890006,"11 April 19 - 23, 2021. ©2021 Association for Computational Linguistics contains a set of concrete sentences that are conceptually connected with q and a (f1 ,f2 and f3 ), along with a set of abstract facts that require multi-hop inference (f4 and f5 ). Previous work has shown that constructing long explanations is challenging due to semantic drift – i.e. the tendency of composing out-ofcontext inference chains as the number of hops increases (Khashabi et al., 2019; Fried et al., 2015). While existing approaches build explanations considering each question in isolation (Khashabi et al., 2018; Khot et al., 2017), we hypothesise that semantic drift can be tackled by leveraging explanatory patterns emerging in clusters of similar questions. In Science, a given statement is considered explanatory to the extent it performs unification (Friedman, 1974; Kitcher, 1981, 1989), that is showing how a set of initially disconnected phenomena are the expression of the same regularity. An example of unification is Newton’s law of universal gravitation, which unifies the motion of planets and falling bodies on Earth showing that all bodies with mass obey the same law. Since the explanatory power of a given statemen"
2021.eacl-main.15,D19-5306,1,0.846786,"g, at the same time, performances competitive with Transformers. Thanks to this feature, the framework can be flexibly applied in combination with downstream question answering models. Our findings are in line with previous work in different QA settings (Rajani et al., 2019; Yadav et al., 2019), which highlights the positive impact of explanations and supporting facts on the final answer prediction task. 201 In parallel with Science QA, the development of models for explanation generation is being explored in different NLP tasks, ranging from open domain question answering (Yang et al., 2018; Thayaparan et al., 2019), to textual entailment (Camburu et al., 2018) and natural language premise selection (Ferreira and Freitas, 2020b,a). Scientific Explanation and AI. The field of Artificial Intelligence has been historically inspired by models of explanation in Philosophy of Science (Thagard and Litt, 2008). The deductivenomological model proposed by Hempel (Hempel, 1965) constitutes the philosophical foundation for explainable models based on logical deduction, such as Expert Systems (Lacave and Diez, 2004; Wick and Thompson, 1992) and Explanationbased Learning (Mitchell et al., 1986). Similarly, the inheren"
2021.eacl-main.15,D18-1260,0,0.0338364,"Missing"
2021.eacl-main.15,D19-1260,0,0.139204,"truction in this setting (Jansen and Ustalov, 2019). However, these models require high computational resources that prevent their applicability to large corpora. On the other hand, approaches based on IR techniques are readily scalable. The approach described in this paper preserves the scalability of IR methods, obtaining, at the same time, performances competitive with Transformers. Thanks to this feature, the framework can be flexibly applied in combination with downstream question answering models. Our findings are in line with previous work in different QA settings (Rajani et al., 2019; Yadav et al., 2019), which highlights the positive impact of explanations and supporting facts on the final answer prediction task. 201 In parallel with Science QA, the development of models for explanation generation is being explored in different NLP tasks, ranging from open domain question answering (Yang et al., 2018; Thayaparan et al., 2019), to textual entailment (Camburu et al., 2018) and natural language premise selection (Ferreira and Freitas, 2020b,a). Scientific Explanation and AI. The field of Artificial Intelligence has been historically inspired by models of explanation in Philosophy of Science (Th"
2021.eacl-main.15,D18-1259,0,0.202526,"or explainability and a quantitative methodology for its evaluation have conducted to the creation of shared tasks on explanation reconstruction (Jansen and Ustalov, 2019) using corpora of explanations such as Worldtree (Jansen et al., 2018, 2016). Given a science question, explanation reconstruction consists in regenerating the gold explanation that supports the correct answer through the combination of a series of atomic facts. While most of the existing benchmarks for multi-hop QA require the composition of only 2 supporting sentences or paragraphs (e.g. QASC (Khot et al., 2019), HotpotQA (Yang et al., 2018)), the explanation reconstruction task requires the aggregation of an average of 6 facts (and as many as ≈20), making it particularly hard for multi-hop reasoning models. Moreover, the structure of the explanations affects the complexity of the reconstruction task. Explanations for science questions are typically composed of two main parts: a grounding part, containing knowledge about concrete concepts in the question, and a core scientific part, including general scientific statements and laws. Consider the following question and answer pair from Worldtree (Jansen et al., 2018): This paper pr"
2021.eacl-main.15,P19-1487,0,0.0161543,"or explanation reconstruction in this setting (Jansen and Ustalov, 2019). However, these models require high computational resources that prevent their applicability to large corpora. On the other hand, approaches based on IR techniques are readily scalable. The approach described in this paper preserves the scalability of IR methods, obtaining, at the same time, performances competitive with Transformers. Thanks to this feature, the framework can be flexibly applied in combination with downstream question answering models. Our findings are in line with previous work in different QA settings (Rajani et al., 2019; Yadav et al., 2019), which highlights the positive impact of explanations and supporting facts on the final answer prediction task. 201 In parallel with Science QA, the development of models for explanation generation is being explored in different NLP tasks, ranging from open domain question answering (Yang et al., 2018; Thayaparan et al., 2019), to textual entailment (Camburu et al., 2018) and natural language premise selection (Ferreira and Freitas, 2020b,a). Scientific Explanation and AI. The field of Artificial Intelligence has been historically inspired by models of explanation in Phil"
2021.eacl-main.15,N16-3020,0,0.0768699,"Missing"
2021.eacl-main.282,2020.lrec-1.266,1,0.8568,"st likely will be relevant to prove a particular statement. We found that STAR not only outperforms baselines that do not distinguish between natural language and mathematical elements, but it also achieves better performance than state-ofthe-art models. 1 Introduction Natural language understanding has been applied to several different tasks and areas, from question answering to visual grounding. Even though Mathematics is a well-established field with immense importance for most areas of science, applications of NLP in this field are still limited. Natural language premise selection (NLPS) (Ferreira and Freitas, 2020a) is a task that requires the combination of natural language reasoning and mathematical reasoning. Given a certain conjecture (a mathematical statement written in natural language) that needs to be proven, we attempt to recommend useful premises that can be relevant for developing that mathematical argument. Mathematical statements have a particular discourse structure that makes it challenging to use traditional NLP techniques. Some of its distinctive features are: (1) Entangled dual lexical spaces for the mathematical elements (ME) and natural language (NL); (2) Distinct syntactic phenomen"
2021.eacl-main.282,2020.acl-main.657,1,0.863605,"st likely will be relevant to prove a particular statement. We found that STAR not only outperforms baselines that do not distinguish between natural language and mathematical elements, but it also achieves better performance than state-ofthe-art models. 1 Introduction Natural language understanding has been applied to several different tasks and areas, from question answering to visual grounding. Even though Mathematics is a well-established field with immense importance for most areas of science, applications of NLP in this field are still limited. Natural language premise selection (NLPS) (Ferreira and Freitas, 2020a) is a task that requires the combination of natural language reasoning and mathematical reasoning. Given a certain conjecture (a mathematical statement written in natural language) that needs to be proven, we attempt to recommend useful premises that can be relevant for developing that mathematical argument. Mathematical statements have a particular discourse structure that makes it challenging to use traditional NLP techniques. Some of its distinctive features are: (1) Entangled dual lexical spaces for the mathematical elements (ME) and natural language (NL); (2) Distinct syntactic phenomen"
2021.eacl-main.282,D19-1630,0,0.0478951,"Missing"
2021.eacl-main.282,P14-1026,0,0.0114197,".578 .664 .605 .704 .714 .627 .663 Table 5: Testing how different mathematical areas are transportable to other areas. The areas considered here are Abstract Algebra(AA), Topology (TP) and Set Theory (ST). For these experiments, we use random examples with n = 1. Val Test F1 P R F1 P R BERT MathSum Self-attention + BiLSTM .886 .644 .871 .512 .901 .869 .877 .459 .925 .562 .834 .388 .651 .550 .796 .631 .573 .703 STAR .885 .854 .917 .882 .865 .899 Table 6: Comparison of our model with other baselines, using n=1 and random examples. matical problem written in natural language (Zhang et al., 2020; Kushman et al., 2014; Ran et al., 2019). These problems are usually self-contained and are structured in a didactic and straightforward manner, not containing complex mathematical expressions. Some contributions focus on the representation of mathematical text and mathematical elements. Zinn (2004) proposes a representation for mathematical proofs using Discourse Representation Theory. Similarly, Ganesalingam (2013) introduces a grammar for representing informal mathematical text, while Pease et al. (2017) presents this style of text using Argumentation Theory. Such explicit representations are relevant for repre"
2021.eacl-main.282,D19-1251,0,0.0272652,"14 .627 .663 Table 5: Testing how different mathematical areas are transportable to other areas. The areas considered here are Abstract Algebra(AA), Topology (TP) and Set Theory (ST). For these experiments, we use random examples with n = 1. Val Test F1 P R F1 P R BERT MathSum Self-attention + BiLSTM .886 .644 .871 .512 .901 .869 .877 .459 .925 .562 .834 .388 .651 .550 .796 .631 .573 .703 STAR .885 .854 .917 .882 .865 .899 Table 6: Comparison of our model with other baselines, using n=1 and random examples. matical problem written in natural language (Zhang et al., 2020; Kushman et al., 2014; Ran et al., 2019). These problems are usually self-contained and are structured in a didactic and straightforward manner, not containing complex mathematical expressions. Some contributions focus on the representation of mathematical text and mathematical elements. Zinn (2004) proposes a representation for mathematical proofs using Discourse Representation Theory. Similarly, Ganesalingam (2013) introduces a grammar for representing informal mathematical text, while Pease et al. (2017) presents this style of text using Argumentation Theory. Such explicit representations are relevant for representing the reasoni"
2021.eacl-main.282,N18-1028,0,0.0282218,"This type of representation (Fraser et al., 2018; Zanibbi et al., 2016) often removes the expression for its original discourse, losing the textual context that can help to find a semantic representation. In this work, we focus on creating a representation that can integrate both of these aspects, natural language and mathematical elements. Similar to our work, Yuan et al. (2019) uses self-attention for mathematical elements in order to generate headlines for mathematical questions. Other relevant tasks for NLP applied to Mathematics include typing variables according to its surrounding text (Stathopoulos et al., 2018), obtaining the units of mathematical elements (Schubotz et al., 2016) and generating equations on a given topic (Yasunaga and Lafferty, 2019). Representations of mathematical elements are often used in the context of Mathematical Information Retrieval, used, for example, for obtaining a particular equation or expression, given a specific query. Tangent-CFT (Mansouri et al., 2019) is an embedding model that uses the subparts an expresPremise selection is a well-defined task in the field of Automated Theorem Proving (ATP), where proofs are encoded using a formal logical representation. Given a"
2021.eacl-main.282,2020.acl-main.362,0,0.0138601,".535 .591 .684 .769 .578 .664 .605 .704 .714 .627 .663 Table 5: Testing how different mathematical areas are transportable to other areas. The areas considered here are Abstract Algebra(AA), Topology (TP) and Set Theory (ST). For these experiments, we use random examples with n = 1. Val Test F1 P R F1 P R BERT MathSum Self-attention + BiLSTM .886 .644 .871 .512 .901 .869 .877 .459 .925 .562 .834 .388 .651 .550 .796 .631 .573 .703 STAR .885 .854 .917 .882 .865 .899 Table 6: Comparison of our model with other baselines, using n=1 and random examples. matical problem written in natural language (Zhang et al., 2020; Kushman et al., 2014; Ran et al., 2019). These problems are usually self-contained and are structured in a didactic and straightforward manner, not containing complex mathematical expressions. Some contributions focus on the representation of mathematical text and mathematical elements. Zinn (2004) proposes a representation for mathematical proofs using Discourse Representation Theory. Similarly, Ganesalingam (2013) introduces a grammar for representing informal mathematical text, while Pease et al. (2017) presents this style of text using Argumentation Theory. Such explicit representations"
2021.eval4nlp-1.21,W19-5301,0,0.0649937,"Missing"
2021.eval4nlp-1.21,J17-4007,0,0.0376186,"Missing"
2021.eval4nlp-1.21,W17-1608,0,0.0149548,"t studies on shared tasks in given domain, 209 Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems (Eval4NLP 2021), pages 209–229 c November 10, 2021. 2021 Association for Computational Linguistics specially in clinical application of NLP (Filannino and Uzuner, 2018), (Chapman et al., 2011). However, they refer to tasks outside the SemEval and are more result oriented rather than task organization. Some studies discuss ethical issues in the organisation and participation of shared tasks. An overview focusing on task competitive nature and fairness can be found in Parra Escartín et al. (2017). In Nissim et al. (2017) authors also relate to these issues, yet giving the priority to advancing the field over fair competition. Comparatively, this paper covers a wider range of NLP topics, and compares sentiment analysis and semantic similarity as well as other task types/groups in a systematic manner. To the best to our knowledge this is the first systematic analysis on SemEval. 3 Analysis methodology We build a corpus based on the ACL anthology archive from the SemEval workshops between the years 2012-2019. Reference material included ACL anthology papers covering the task description,"
2021.findings-acl.1,N19-1423,0,0.127805,"erate plausible explanations for answer prediction; (3) Our model demonstrates better robustness towards semantic drift when compared to transformerbased and multi-hop approaches. 1 Introduction Answering science questions remain a fundamental challenge in Natural Language Processing and AI as it requires complex forms of inference, including causal, model-based and example-based reasoning (Jansen, 2018; Clark et al., 2018; Jansen et al., 2016; Clark et al., 2013). Current state-of-theart (SOTA) approaches for answering questions in the science domain are dominated by transformerbased models (Devlin et al., 2019; Sun et al., 2019). Despite remarkable performance on answer prediction, these approaches are black-box by nature, 1 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1–12 August 1–6, 2021. ©2021 Association for Computational Linguistics For each Candidate Hypothesis: Question(Q): Fact Graph Construction: What is an example of force producing heat? Candidate Answer (C1): FG1 Two sticks getting warm when rubbed together Hypothesis (H1): Two sticks getting warm when rubbed together is an example of force producing heat Grounding Facts: [✓] a stick is an object: F"
2021.findings-acl.1,2020.acl-main.657,1,0.734529,"The other class of approaches that provide explanations are graph-based approaches. Graphbased approaches have been successfully applied for open-domain question answering (Fang et al., 2020; Qiu et al., 2019; Thayaparan et al., 2019) where the question only requires only two hops. PathNet (Kundu et al., 2019) operates within the same design principles and has been applied on OpenbookQA science dataset. As indicated in the empirical evaluation, it struggles with longchain explanations since it relies only on two facts. Graph-based approaches have also been employed for mathematical reasoning (Ferreira and Freitas, 2020a,b) and textual entailment (Silva et al., 2019, 2018). The third category of partially explainable approaches employs black-box neural models in combination with a retrieval approach. The SOTA model for Science Question (Khashabi et al., 2020) answering is pretrained across multiple datasets and is not explainable. The current partially explainable SOTA approach that does not rely on external resource (Yadav et al., 2019b) employs a large parameter BERT model for question answering resulting. In contrast, with a low number of parameters, we have introduced a model that demonstrates competitiv"
2021.findings-acl.1,Q15-1015,0,0.0213219,"t al., 2018; Jansen et al., 2017; Khot et al., 2017a; Khashabi et al., 2016) employ this form of reasoning for multiple-choice science questions to build a set of plausible explanations for each candidate answer and select the one with the best explanation as the final answer. XSQA solvers typically treat explanation generation as a multi-hop graph traversal problem. Here, the solver attempts to compose multiple facts that connect the question to a candidate answer. These multi-hop approaches have shown diminishing returns with an increasing number of hops (Jansen et al., 2018; Jansen, 2018). Fried et al. (2015) conclude that this phenomenon is due to semantic drift – i.e., as the number of aggregated facts increases, so does the probability of drifting out of context. Khashabi et al. (2019) propose a theoretical framework, empirically supported by Jansen et al. (2018); Fried et al. (2015), attesting that ongoing efforts with very long multi-hop reasoning chains are unlikely to succeed, emphasising the need for a richer representation with fewer hops and higher importance to abstraction and grounding mechanisms. Consider the example in Figure 1A where the central concept the question examines is the"
2021.findings-acl.1,P17-2049,0,0.292965,"yaparan et al., 2020; Miller, 2019; Biran and Cotton, 2017; Jansen et al., 2016). Explainable Science Question Answering (XSQA) is often framed as a natural language abductive reasoning problem (Khashabi et al., 2018; Jansen et al., 2017). Abductive reasoning represents a distinct inference process, known as inference to the best explanation (Peirce, 1960; Lipton, 2017), which starts from a set of complete or incomplete observations to find the hypothesis, from a set of plausible alternatives, that best explains the observations. Several approaches (Khashabi et al., 2018; Jansen et al., 2017; Khot et al., 2017a; Khashabi et al., 2016) employ this form of reasoning for multiple-choice science questions to build a set of plausible explanations for each candidate answer and select the one with the best explanation as the final answer. XSQA solvers typically treat explanation generation as a multi-hop graph traversal problem. Here, the solver attempts to compose multiple facts that connect the question to a candidate answer. These multi-hop approaches have shown diminishing returns with an increasing number of hops (Jansen et al., 2018; Jansen, 2018). Fried et al. (2015) conclude that this phenomenon i"
2021.findings-acl.1,C16-1278,0,0.0153469,"performance when compared to transformer-based and multi-hop approaches despite having a significantly lower number of parameters; (2) We show that our model is able to generate plausible explanations for answer prediction; (3) Our model demonstrates better robustness towards semantic drift when compared to transformerbased and multi-hop approaches. 1 Introduction Answering science questions remain a fundamental challenge in Natural Language Processing and AI as it requires complex forms of inference, including causal, model-based and example-based reasoning (Jansen, 2018; Clark et al., 2018; Jansen et al., 2016; Clark et al., 2013). Current state-of-theart (SOTA) approaches for answering questions in the science domain are dominated by transformerbased models (Devlin et al., 2019; Sun et al., 2019). Despite remarkable performance on answer prediction, these approaches are black-box by nature, 1 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1–12 August 1–6, 2021. ©2021 Association for Computational Linguistics For each Candidate Hypothesis: Question(Q): Fact Graph Construction: What is an example of force producing heat? Candidate Answer (C1): FG1 Two sticks gettin"
2021.findings-acl.1,P19-1263,0,0.0819427,"hoices (i.e., predict 0). During inference, we select the choice with the highest prediction score as the correct answer. BERT baselines are further enhanced with explanatory facts retrieved by the retrieval models. BERT + BM25 and BERT + UR, is fine-tuned for binary classification by complementing the question-answer pair with grounding and abstract facts selected by BM25 and Unification retrieval, respectively. Similarly, the second model BERT + UR complements the question-answer pair with grounding and abstract facts selected using BM25 and Unification retrieval, respectively. (2) PathNet (Kundu et al., 2019): PathNet is a neural approach that constructs a single linear path composed of two facts connected via entity pairs for reasoning. PathNet also can explain its reasoning via explicit reasoning paths. They have exhibited strong performance for multiple-choice science questions by composing two facts. Similar to Bert-based models, we employ PathNET with the top k facts retrieved utilizing Unification (PathNet + UR) and BM25 (PathNet + BM25) retrieval. We concatenate the facts retrieved for each candidate answer and provide as supporting facts. Further details regarding the hyperparameters and c"
2021.findings-acl.1,J17-2005,0,0.0203571,"heir predictions (Thayaparan et al., 2020; Miller, 2019; Biran and Cotton, 2017; Jansen et al., 2016). Explainable Science Question Answering (XSQA) is often framed as a natural language abductive reasoning problem (Khashabi et al., 2018; Jansen et al., 2017). Abductive reasoning represents a distinct inference process, known as inference to the best explanation (Peirce, 1960; Lipton, 2017), which starts from a set of complete or incomplete observations to find the hypothesis, from a set of plausible alternatives, that best explains the observations. Several approaches (Khashabi et al., 2018; Jansen et al., 2017; Khot et al., 2017a; Khashabi et al., 2016) employ this form of reasoning for multiple-choice science questions to build a set of plausible explanations for each candidate answer and select the one with the best explanation as the final answer. XSQA solvers typically treat explanation generation as a multi-hop graph traversal problem. Here, the solver attempts to compose multiple facts that connect the question to a candidate answer. These multi-hop approaches have shown diminishing returns with an increasing number of hops (Jansen et al., 2018; Jansen, 2018). Fried et al. (2015) conclude tha"
2021.findings-acl.1,L18-1433,0,0.222506,"ions. Several approaches (Khashabi et al., 2018; Jansen et al., 2017; Khot et al., 2017a; Khashabi et al., 2016) employ this form of reasoning for multiple-choice science questions to build a set of plausible explanations for each candidate answer and select the one with the best explanation as the final answer. XSQA solvers typically treat explanation generation as a multi-hop graph traversal problem. Here, the solver attempts to compose multiple facts that connect the question to a candidate answer. These multi-hop approaches have shown diminishing returns with an increasing number of hops (Jansen et al., 2018; Jansen, 2018). Fried et al. (2015) conclude that this phenomenon is due to semantic drift – i.e., as the number of aggregated facts increases, so does the probability of drifting out of context. Khashabi et al. (2019) propose a theoretical framework, empirically supported by Jansen et al. (2018); Fried et al. (2015), attesting that ongoing efforts with very long multi-hop reasoning chains are unlikely to succeed, emphasising the need for a richer representation with fewer hops and higher importance to abstraction and grounding mechanisms. Consider the example in Figure 1A where the central c"
2021.findings-acl.1,N19-1030,0,0.0399616,"Missing"
2021.findings-acl.1,W18-1703,0,0.0825284,"r. XSQA solvers typically treat explanation generation as a multi-hop graph traversal problem. Here, the solver attempts to compose multiple facts that connect the question to a candidate answer. These multi-hop approaches have shown diminishing returns with an increasing number of hops (Jansen et al., 2018; Jansen, 2018). Fried et al. (2015) conclude that this phenomenon is due to semantic drift – i.e., as the number of aggregated facts increases, so does the probability of drifting out of context. Khashabi et al. (2019) propose a theoretical framework, empirically supported by Jansen et al. (2018); Fried et al. (2015), attesting that ongoing efforts with very long multi-hop reasoning chains are unlikely to succeed, emphasising the need for a richer representation with fewer hops and higher importance to abstraction and grounding mechanisms. Consider the example in Figure 1A where the central concept the question examines is the understanding of friction. Here, an inference solver’s We propose an explainable inference approach for science questions by reasoning on grounding and abstract inference chains. This paper frames question answering as a natural language abductive reasoning prob"
2021.findings-acl.1,P19-1617,0,0.0286404,"y have exhibited good performance with no supervision, the performance tends to be lower when answer8 Acknowledgements ing complex questions requiring long explanatory chains. In contrast, our approach performs inference over unstructured text by imposing structural constraints via grounding-abstract chains, lowering the hops, and also combine parametric optimisation to extract the best performing model. The other class of approaches that provide explanations are graph-based approaches. Graphbased approaches have been successfully applied for open-domain question answering (Fang et al., 2020; Qiu et al., 2019; Thayaparan et al., 2019) where the question only requires only two hops. PathNet (Kundu et al., 2019) operates within the same design principles and has been applied on OpenbookQA science dataset. As indicated in the empirical evaluation, it struggles with longchain explanations since it relies only on two facts. Graph-based approaches have also been employed for mathematical reasoning (Ferreira and Freitas, 2020a,b) and textual entailment (Silva et al., 2019, 2018). The third category of partially explainable approaches employs black-box neural models in combination with a retrieval approac"
2021.findings-acl.1,D19-1410,0,0.0230888,"ned as follows: (1) Relevance: We promote the inclusion of highly relevant facts in the explanations by encouraging the selection of sentences with higher lexical relevance and semantic similarity with the hypothesis. We use the following scores to measure the relevance and the semantic similarity of the facts: Lexical Relevance score (L): Obtained from the upstream facts retrieval model (e.g: BM25 score/ Unification score (Valentino et al., 2021)). Semantic Similarity score (S): Cosine similarity obtained from neural sentence representation models. For our experiments, we adopt SentenceBERT (Reimers et al., 2019) since it shows state-of-the-art performance in semantic textual similarity tasks.   θgg D(vj , vk )     θaa D(vj , vk ) ωe (vj , vk ; θ1 ) = θga C(vj , vk )    θqg C(vj , vk )     θqa C(vj , vk ) ( ωv (vihi ; θ2 ) 3 = vj , vk ∈ FGhi vj , vk ∈ FAhi vj ∈ FGhi , vk ∈ FAhi vj ∈ FGhi , vk = hi vj ∈ FAhi , vk = hi θlr L(vj , hi ) + θss S(vj , hi ) 0 0 vj ∈ FAhi vi ∈ FGhi vi = hi where θgg , θaa , θga , θgq , θqa ∈ θ1 and θlr , θss ∈ θ2 . 2.3 In order to emulate the grounding-abstract inference chains and obtain a valid subgraph, we impose the set constraints described in Table 1 for"
2021.findings-acl.1,N19-1274,0,0.143172,"42.72 43.45 42.17 43.72 7 PathNet + BM25 (k=20) 8 PathNet + UR (k=15) 43.32 47.64 36.42 33.55 9 Ours + BM25 (k=30) 10 Ours + UR (k=30) 63.82 66.23 48.24 50.15 BERTBase + BM25 (k=10) BERTLarge + BM25 (k=10) BERTBase + UR (k=10) BERTLarge + UR (k=10) Table 2: Accuracy on Easy (764) and Challenge split (313) of WorldTree test-set corpus from the best performing k of each model # Model 1 BERTLarge 2 3 4 5 6 7 8 IR Solver (Clark et al., 2016) TupleInf (Khot et al., 2017b) TableILP (Khashabi et al., 2016) DGEM (Clark et al., 2016) KGˆ2 (Zhang et al., 2018) ET-RR (Ni et al., 2019) Unsupervised AHE (Yadav et al., 2019a) 9 Supervised AHE (Yadav et al., 2019a) 10 AutoRocc (Yadav et al., 2019b) 11 Ours + BM25 (k=40) 12 Ours + UR (k=40) Explainable Accuracy No 35.11 Yes Yes Yes Partial Partial Partial Partial 20.26 23.83 26.97 27.11 31.70 36.61 33.87 Partial 34.47 Partial 41.24 Yes Yes 40.21 39.84 Table 3: ARC challenge scores compared with other Fully or Partially explainable approaches trained only on the ARC dataset. best performing setting in Table 2. We draw the following conclusions: (1) Despite having a smaller number of parameters to train (BERTBase : 110M parameters, BERTLarge : 340M parameters, Expla"
2021.findings-acl.1,D19-1260,0,0.114545,"42.72 43.45 42.17 43.72 7 PathNet + BM25 (k=20) 8 PathNet + UR (k=15) 43.32 47.64 36.42 33.55 9 Ours + BM25 (k=30) 10 Ours + UR (k=30) 63.82 66.23 48.24 50.15 BERTBase + BM25 (k=10) BERTLarge + BM25 (k=10) BERTBase + UR (k=10) BERTLarge + UR (k=10) Table 2: Accuracy on Easy (764) and Challenge split (313) of WorldTree test-set corpus from the best performing k of each model # Model 1 BERTLarge 2 3 4 5 6 7 8 IR Solver (Clark et al., 2016) TupleInf (Khot et al., 2017b) TableILP (Khashabi et al., 2016) DGEM (Clark et al., 2016) KGˆ2 (Zhang et al., 2018) ET-RR (Ni et al., 2019) Unsupervised AHE (Yadav et al., 2019a) 9 Supervised AHE (Yadav et al., 2019a) 10 AutoRocc (Yadav et al., 2019b) 11 Ours + BM25 (k=40) 12 Ours + UR (k=40) Explainable Accuracy No 35.11 Yes Yes Yes Partial Partial Partial Partial 20.26 23.83 26.97 27.11 31.70 36.61 33.87 Partial 34.47 Partial 41.24 Yes Yes 40.21 39.84 Table 3: ARC challenge scores compared with other Fully or Partially explainable approaches trained only on the ARC dataset. best performing setting in Table 2. We draw the following conclusions: (1) Despite having a smaller number of parameters to train (BERTBase : 110M parameters, BERTLarge : 340M parameters, Expla"
2021.findings-acl.1,N19-1270,0,0.056958,"Missing"
2021.findings-acl.1,2021.eacl-main.15,1,0.849422,"Bayesian Optimisation to obtain the best possible combination with the highest accuracy for answer selection. To the best of our knowledge, we are the first to combine a parameter optimisation method with Linear Programming for inference. The rest of this section describes the model in detail. 2.1 Relevant facts retrival Given a question (Q) and candidate answers C = {c1 , c2 , c3 , ..., cn } we convert them to hypotheses {h1 , h2 , h3 , ..., hn } using the approach proposed by Demszky et al. (2018). For each hypothesis hi we adopt fact retrieval approaches (e.g: BM25, Unification-retrieval (Valentino et al., 2021)) to select the top m relevant abstract facts hi } from a knowlFAhi = {f1hi , f2hi , f3hi , ..., fm edge base containing abstract facts (Abstract Facts KB) and top l relevant grounding facts FGhi = {f1hi , f2hi , f3hi , ..., flhi } from a knowledge base containing grounding facts (Grounding Facts KB) that at least connects one abstract fact with the hypothesis, such that F hi = FAhi ∪FGhi and l+m = k. 2.2 C(fjhi , fkhi ) = |t(fjhi ) ∩ t(fkhi )| max(|t(fjhi )|, |t(fkhi )|) Therefore, the higher the number of term overlaps, the higher the cohesion score. (3) Diversity: While maximizing relevance"
2021.findings-emnlp.301,2020.acl-main.9,0,0.034047,"ce an upper bound of mutual information, showing its benefits in fair classification. Differently from all these approaches, we model linguistic features as discrete variables, which allows us to enforce control on the encodings’ dimensions. Discrete Latent Variables. Various approaches present discrete encodings for language generation. Shu et al. (2020) enhance control and diversity in generation with latent spaces, while Guo et al. (2020) leverage textual evidence to guide the generation. Both methods are based on the VQVAE (van den Oord et al., 2017), which we consider in our experiments. Bao et al. (2020) encodes discrete latent variables into Transformer blocks for dialogue generation. Differently from these methods, we leverage discrete variables to optimize disentanglement. 5 Experiments In this section, we evaluate the disentanglement of the proposed model with qualitative and quantitative methods, against several baselines. Furthermore, the benefits of our model’s encodings are demonstrated in the downstream task of text style transfer. 5.1 Qualitative Evaluation Latent Traversals. After training our model, we can evaluate the disentanglement quality of the representations by analysing th"
2021.findings-emnlp.301,K16-1002,0,0.581156,"gn the first VAE-based architecture (shown in Figure 1), where linguistic features are encoded as discrete latent variables via the GumbelSoftmax trick (Jang et al., 2016) and disentanglement is jointly enforced in the objective function. We derive a decomposition of the VAE evidence lower bound (ELBO), where independence between variables is encouraged by fine-tuning the Total Correlation term, with the goal of achieving disentanglement. At the same time, an information channel mechanism is introduced, and increased incrementally during training, to avoid the issue of the posterior collapse (Bowman et al., 2016). 2) We provide, to our knowledge, the first extensive evaluation of the text representation, under the lenses of contemporary disentanglement methods. We propose to probe the quality of the representation by traversing and decoding each latent, expecting a disentangled representation to show only a single dimensional change (for example in tense, as in Figure 1). In addition, we show that the proposed model outperforms numerous baselines on three quantitative disentanglement metrics from the Representation Learning literature. Finally, we show the beneficial effect of the proposed representat"
2021.findings-emnlp.301,2020.acl-main.673,0,0.441238,"ester.ac.uk name.surname@manchester.ac.uk x ˆ F IGURE 1: Overview of the proposed Discrete Controlled Total Correlation (DCTC) model: The KL decomposition encourages independence between variables, which are encoded as discrete latents, to capture the dimensions of linguistic features. The representation is probed with latent traversals and quantitative metrics. Introduction ∗ pθ π3 eter which controls the KL divergence in a Variational AutoEncoder (VAE) (Kingma and Welling, 2014). In the NLP domain, John et al. (2019) use adversarial losses to separate the style and content embeddings, while Cheng et al. (2020) disentangle style and content embeddings by minimizing their mutual information. After a thorough review of the literature, we find that there are currently two main issues in the area of disentanglement in NLP. First, the mentioned approaches operate using Gaussian distributions in a continuous space. Although a continuous representation may be suitable to encode images, for text data one should rather specifically consider discrete distributions of the feature set. In fact, generative features of a sentence mostly belong to a discrete domain, for example, one would encode the gender feature"
2021.findings-emnlp.301,2021.acl-long.511,0,0.0353714,"lement with it represents a generalization of the mutual infor- adversarial losses for style transfer, while Sha and mation, which measures the dependence between Lukasiewicz (2021) propose to improve the trainvariables. Our variation of the TC allows us to ing stability, using multiple non-adversarial losses. control the amount of information encoded in the 2) Information-theoretic: Cheng et al. (2020) prodiscrete channel, thus avoiding the collapse of the pose to disentangle style and content by minimizterm. ing the mutual information between the latent and 3550 the observed variable, while Colombo et al. (2021) introduce an upper bound of mutual information, showing its benefits in fair classification. Differently from all these approaches, we model linguistic features as discrete variables, which allows us to enforce control on the encodings’ dimensions. Discrete Latent Variables. Various approaches present discrete encodings for language generation. Shu et al. (2020) enhance control and diversity in generation with latent spaces, while Guo et al. (2020) leverage textual evidence to guide the generation. Both methods are based on the VQVAE (van den Oord et al., 2017), which we consider in our exper"
2021.findings-emnlp.301,2020.acl-main.544,0,0.0428489,"the pose to disentangle style and content by minimizterm. ing the mutual information between the latent and 3550 the observed variable, while Colombo et al. (2021) introduce an upper bound of mutual information, showing its benefits in fair classification. Differently from all these approaches, we model linguistic features as discrete variables, which allows us to enforce control on the encodings’ dimensions. Discrete Latent Variables. Various approaches present discrete encodings for language generation. Shu et al. (2020) enhance control and diversity in generation with latent spaces, while Guo et al. (2020) leverage textual evidence to guide the generation. Both methods are based on the VQVAE (van den Oord et al., 2017), which we consider in our experiments. Bao et al. (2020) encodes discrete latent variables into Transformer blocks for dialogue generation. Differently from these methods, we leverage discrete variables to optimize disentanglement. 5 Experiments In this section, we evaluate the disentanglement of the proposed model with qualitative and quantitative methods, against several baselines. Furthermore, the benefits of our model’s encodings are demonstrated in the downstream task of tex"
2021.findings-emnlp.301,P19-1041,0,0.280635,"16; Burgess et al., 2018; Kim and Mnih, 2018) fine-tune the param† name.surname@postgrad.manchester.ac.uk name.surname@manchester.ac.uk x ˆ F IGURE 1: Overview of the proposed Discrete Controlled Total Correlation (DCTC) model: The KL decomposition encourages independence between variables, which are encoded as discrete latents, to capture the dimensions of linguistic features. The representation is probed with latent traversals and quantitative metrics. Introduction ∗ pθ π3 eter which controls the KL divergence in a Variational AutoEncoder (VAE) (Kingma and Welling, 2014). In the NLP domain, John et al. (2019) use adversarial losses to separate the style and content embeddings, while Cheng et al. (2020) disentangle style and content embeddings by minimizing their mutual information. After a thorough review of the literature, we find that there are currently two main issues in the area of disentanglement in NLP. First, the mentioned approaches operate using Gaussian distributions in a continuous space. Although a continuous representation may be suitable to encode images, for text data one should rather specifically consider discrete distributions of the feature set. In fact, generative features of"
2021.findings-emnlp.301,N13-1090,0,0.0396687,"AE 0.79 0.71 0.23 0.83 0.84 βVAE 0.88 0.87 0.32 0.91 0.75 βTC-VAE 0.92 0.90 0.27 0.91 0.92 FactorVAE 0.91 0.92 0.18 0.92 0.92 CGT 0.78 0.63 0.13 0.77 0.66 ST-VAE 0.82 0.67 0.24 0.84 0.72 VQVAE 0.77 0.74 0.27 0.75 0.76 JointVAE 0.89 0.81 0.35 0.90 0.95 DCTC 0.94 0.91 0.43 0.94 0.92 MIG 0.25 0.30 0.29 0.27 0.18 0.26 0.17 0.33 0.49 TABLE 4: Disentanglement metrics results. to disentangle the style embedding of sentences, while ST-VAE focuses on disentangling style from content. 5.3 Text Style Transfer Arithmetics for Latent Factors. In this experiment, we take inspiration from previous work from Mikolov et al. (2013), which showed that word embeddings can capture semantic relations via vector arithmetics, (for example, king - man + woman = queen). More specifically, we consider text generative factors (e.g. negation), and investigate sentence-level embeddings arithmetic in the task of text style transfer (extrinsic evaluation of the models). The style transfer protocol of our experiment is performed as follows. We first select a factor (e.g. negation) and extract two lists of sentences containing two specific values (e.g. negative, and affirmative), that we name respectively sn and sa . The extraction is"
2021.findings-emnlp.301,D19-1407,0,0.0372894,"Missing"
2021.findings-emnlp.301,2020.acl-demos.14,0,0.0239445,"gories. Following this intuition, a simple solution for being able to measure disentanglement in a text representation, is to have a pre-processing step, where generative factors are extracted, before utilizing this information to compute the quantitative metrics. Data Preparation. In our experiment, we focus on the Yelp reviews dataset (Shen et al., 2017), which is composed by 600,000 review sentences, and we define and extract 5 generative factors, namely: gender, tense, negation, subject number, and object number. Using the part-of-speech (POS) engine provided by the Stanza python package (Qi et al., 2020), we extract: 1) the gender factor from the pronouns and 2) the number factor from subjects and objects. Similarly, the tense is obtained from the verb using Stanza’s lemmatizer, while negation is determined from the presence of negation attributes in the parsed metadata. Samples of decoded sentences from the traversals for tense and subject-number are displayed in Table 3. DCTC correctly disentangles the tense into present, future and past, while the other factors Experimental Setup. We follow the setup of the are fixed, and similarly it disentangles the subject- previous qualitative experime"
2021.findings-emnlp.301,2020.findings-emnlp.339,0,0.0292362,"mation-theoretic: Cheng et al. (2020) prodiscrete channel, thus avoiding the collapse of the pose to disentangle style and content by minimizterm. ing the mutual information between the latent and 3550 the observed variable, while Colombo et al. (2021) introduce an upper bound of mutual information, showing its benefits in fair classification. Differently from all these approaches, we model linguistic features as discrete variables, which allows us to enforce control on the encodings’ dimensions. Discrete Latent Variables. Various approaches present discrete encodings for language generation. Shu et al. (2020) enhance control and diversity in generation with latent spaces, while Guo et al. (2020) leverage textual evidence to guide the generation. Both methods are based on the VQVAE (van den Oord et al., 2017), which we consider in our experiments. Bao et al. (2020) encodes discrete latent variables into Transformer blocks for dialogue generation. Differently from these methods, we leverage discrete variables to optimize disentanglement. 5 Experiments In this section, we evaluate the disentanglement of the proposed model with qualitative and quantitative methods, against several baselines. Furthermo"
C16-2036,P15-1034,0,0.0761117,"ain-independent discovery of relations extracted from text by not depending on any relation-specific human input. Generally, state-of-the-art Open RE systems identify relationships between entities in a sentence by matching patterns over either its POS tags, e. g. (Banko et al., 2007; Fader et al., 2011; Merhav et al., 2012), or its dependency tree, e. g. (Nakashole et al., 2012; Mausam et al., 2012; Xu et al., 2013; Mesquita et al., 2013). However, particularly in long and syntactically complex sentences, relevant relations often span several clauses or are presented in a non-canonical form (Angeli et al., 2015), thus posing a challenge for current Open RE approaches which are prone to make incorrect extractions while missing others - when operating on sentences with an intricate structure. To achieve a higher accuracy on Open RE tasks, we have developed a framework for simplifying the linguistic structure of NL sentences. It identifies components of a sentence which usually provide supplementary information that may be easily extracted without losing essential information. By applying a set of hand-crafted grammar rules that have been defined in the course of a rule engineering process based on ling"
C16-2036,P08-1004,0,0.0724424,"ss for subsequently applied Open RE systems. 1 Introduction Relation Extraction (RE) is the task of recognizing the assertion of relationships between two or more entities in NL text. Traditional RE systems have concentrated on identifying and extracting relations of interest by taking as input the target relations, along with hand-crafted extraction patterns or patterns learned from hand-labeled training examples. Consequently, shifting to a new domain requires to first specify the target relations and then to manually create new extraction rules or to annotate new training examples by hand (Banko and Etzioni, 2008). As this manual labor scales linearly with the number of target relations, this supervised approach does not scale to large, heterogeneous corpora which are likely to contain a variety of unanticipated relations (Schmidek and Barbosa, 2014). To tackle this issue, Banko and Etzioni (2008) introduced a new extraction paradigm named ’Open RE’ that facilitates domain-independent discovery of relations extracted from text by not depending on any relation-specific human input. Generally, state-of-the-art Open RE systems identify relationships between entities in a sentence by matching patterns over"
C16-2036,D11-1142,0,0.150231,"umber of target relations, this supervised approach does not scale to large, heterogeneous corpora which are likely to contain a variety of unanticipated relations (Schmidek and Barbosa, 2014). To tackle this issue, Banko and Etzioni (2008) introduced a new extraction paradigm named ’Open RE’ that facilitates domain-independent discovery of relations extracted from text by not depending on any relation-specific human input. Generally, state-of-the-art Open RE systems identify relationships between entities in a sentence by matching patterns over either its POS tags, e. g. (Banko et al., 2007; Fader et al., 2011; Merhav et al., 2012), or its dependency tree, e. g. (Nakashole et al., 2012; Mausam et al., 2012; Xu et al., 2013; Mesquita et al., 2013). However, particularly in long and syntactically complex sentences, relevant relations often span several clauses or are presented in a non-canonical form (Angeli et al., 2015), thus posing a challenge for current Open RE approaches which are prone to make incorrect extractions while missing others - when operating on sentences with an intricate structure. To achieve a higher accuracy on Open RE tasks, we have developed a framework for simplifying the ling"
C16-2036,D12-1048,0,0.175451,"a which are likely to contain a variety of unanticipated relations (Schmidek and Barbosa, 2014). To tackle this issue, Banko and Etzioni (2008) introduced a new extraction paradigm named ’Open RE’ that facilitates domain-independent discovery of relations extracted from text by not depending on any relation-specific human input. Generally, state-of-the-art Open RE systems identify relationships between entities in a sentence by matching patterns over either its POS tags, e. g. (Banko et al., 2007; Fader et al., 2011; Merhav et al., 2012), or its dependency tree, e. g. (Nakashole et al., 2012; Mausam et al., 2012; Xu et al., 2013; Mesquita et al., 2013). However, particularly in long and syntactically complex sentences, relevant relations often span several clauses or are presented in a non-canonical form (Angeli et al., 2015), thus posing a challenge for current Open RE approaches which are prone to make incorrect extractions while missing others - when operating on sentences with an intricate structure. To achieve a higher accuracy on Open RE tasks, we have developed a framework for simplifying the linguistic structure of NL sentences. It identifies components of a sentence which usually provide sup"
C16-2036,D13-1043,0,0.457426,"y of unanticipated relations (Schmidek and Barbosa, 2014). To tackle this issue, Banko and Etzioni (2008) introduced a new extraction paradigm named ’Open RE’ that facilitates domain-independent discovery of relations extracted from text by not depending on any relation-specific human input. Generally, state-of-the-art Open RE systems identify relationships between entities in a sentence by matching patterns over either its POS tags, e. g. (Banko et al., 2007; Fader et al., 2011; Merhav et al., 2012), or its dependency tree, e. g. (Nakashole et al., 2012; Mausam et al., 2012; Xu et al., 2013; Mesquita et al., 2013). However, particularly in long and syntactically complex sentences, relevant relations often span several clauses or are presented in a non-canonical form (Angeli et al., 2015), thus posing a challenge for current Open RE approaches which are prone to make incorrect extractions while missing others - when operating on sentences with an intricate structure. To achieve a higher accuracy on Open RE tasks, we have developed a framework for simplifying the linguistic structure of NL sentences. It identifies components of a sentence which usually provide supplementary information that may be easily"
C16-2036,D12-1104,0,0.07757,"ge, heterogeneous corpora which are likely to contain a variety of unanticipated relations (Schmidek and Barbosa, 2014). To tackle this issue, Banko and Etzioni (2008) introduced a new extraction paradigm named ’Open RE’ that facilitates domain-independent discovery of relations extracted from text by not depending on any relation-specific human input. Generally, state-of-the-art Open RE systems identify relationships between entities in a sentence by matching patterns over either its POS tags, e. g. (Banko et al., 2007; Fader et al., 2011; Merhav et al., 2012), or its dependency tree, e. g. (Nakashole et al., 2012; Mausam et al., 2012; Xu et al., 2013; Mesquita et al., 2013). However, particularly in long and syntactically complex sentences, relevant relations often span several clauses or are presented in a non-canonical form (Angeli et al., 2015), thus posing a challenge for current Open RE approaches which are prone to make incorrect extractions while missing others - when operating on sentences with an intricate structure. To achieve a higher accuracy on Open RE tasks, we have developed a framework for simplifying the linguistic structure of NL sentences. It identifies components of a sentence whic"
C16-2036,C16-2036,1,0.0512975,"Missing"
C16-2036,schmidek-barbosa-2014-improving,0,0.236408,"g and extracting relations of interest by taking as input the target relations, along with hand-crafted extraction patterns or patterns learned from hand-labeled training examples. Consequently, shifting to a new domain requires to first specify the target relations and then to manually create new extraction rules or to annotate new training examples by hand (Banko and Etzioni, 2008). As this manual labor scales linearly with the number of target relations, this supervised approach does not scale to large, heterogeneous corpora which are likely to contain a variety of unanticipated relations (Schmidek and Barbosa, 2014). To tackle this issue, Banko and Etzioni (2008) introduced a new extraction paradigm named ’Open RE’ that facilitates domain-independent discovery of relations extracted from text by not depending on any relation-specific human input. Generally, state-of-the-art Open RE systems identify relationships between entities in a sentence by matching patterns over either its POS tags, e. g. (Banko et al., 2007; Fader et al., 2011; Merhav et al., 2012), or its dependency tree, e. g. (Nakashole et al., 2012; Mausam et al., 2012; Xu et al., 2013; Mesquita et al., 2013). However, particularly in long and"
C16-2036,N13-1107,0,0.0659048,"contain a variety of unanticipated relations (Schmidek and Barbosa, 2014). To tackle this issue, Banko and Etzioni (2008) introduced a new extraction paradigm named ’Open RE’ that facilitates domain-independent discovery of relations extracted from text by not depending on any relation-specific human input. Generally, state-of-the-art Open RE systems identify relationships between entities in a sentence by matching patterns over either its POS tags, e. g. (Banko et al., 2007; Fader et al., 2011; Merhav et al., 2012), or its dependency tree, e. g. (Nakashole et al., 2012; Mausam et al., 2012; Xu et al., 2013; Mesquita et al., 2013). However, particularly in long and syntactically complex sentences, relevant relations often span several clauses or are presented in a non-canonical form (Angeli et al., 2015), thus posing a challenge for current Open RE approaches which are prone to make incorrect extractions while missing others - when operating on sentences with an intricate structure. To achieve a higher accuracy on Open RE tasks, we have developed a framework for simplifying the linguistic structure of NL sentences. It identifies components of a sentence which usually provide supplementary inform"
C18-1195,P15-1034,0,0.157264,"ng a simple linguistic structure is derived from the input. Then, one or more predicate-argument extractions are generated for each clause. In the same vein, Schmidek and Barbosa (2014) propose a strategy to break down structurally complex sentences into simpler ones by decomposing the original sentence into its basic building blocks via chunking. The dependencies of each two chunks are then determined using dependency parsing or a Naive Bayes classifier. Depending on their relationships, chunks are combined into simplified sentences, upon which the task of relation extraction is carried out. Angeli et al. (2015) present Stanford Open IE, an approach in which a classifier is learned for splitting a sentence into a set of logically entailed shorter utterances which are then maximally shortened by running natural logic inference over them. In the end, a small set of hand-crafted patterns are used to extract a predicate-argument triple from each utterance. On the basis of such re-arrangement strategies for decomposing a complex input sentence into a set of self-contained clauses that present a linguistic structure that is easier to process for Open IE systems, we have developed an Open IE pipeline which"
C18-1195,D11-1142,0,0.0629464,"over shallow linguistic features (though, at the cost of extraction speed). O LLIE follows the idea of bootstrap learning of patterns based on dependency parse paths. However, while WOE relies on Wikipedia-based bootstrapping, O LLIE applies a set of high precision seed tuples from its predecessor system R E V ERB to bootstrap a large training set. Moreover, O LLIE is the first Open IE approach to identify not only verb-based relations, but also noun-mediated ones. Rule-based approaches. The second category of Open IE systems make use of hand-crafted extraction rules. This includes R E V ERB (Fader et al., 2011), a shallow extractor that applies a set of lexical and syntactic constraints that are expressed in terms of POS-based regular expressions. In that way, the amount of incoherent, uninformative and overspecified relation phrases is reduced. While previously mentioned Open IE systems focus on the extraction of binary relations, K RAKE N (Akbik and L¨oser, 2012) is the first approach to be specifically built for capturing complete facts from sentences by gathering the full set of arguments for each relation phrase within a sentence, thus producing tuples of arbitrary arity. The identification of"
C18-1195,D15-1076,0,0.0394693,"we integrate Graphene into Stanovsky and Dagan (2016)’s Open IE benchmark framework, which was created from a QA-SRL dataset where every verbal predicate was considered as constituting an own extraction. Since Graphene’s structured output is not designed to yield extractions for every occurrence of a verbal predicate, we use an alternative relation extraction implementation which is able to produce more than one extraction from a simplified core sentence. To match extractions to reference propositions from the gold standard, we use the method described in Stanovsky and Dagan (2016), following He et al. (2015) by matching an extraction with a gold proposition if both agree on the grammatical head of both the relational phrase and its arguments. Since n-ary relational tuples with possibly more than two arguments have to be compared, we assign a positive match if the relational phrase and at least two of their arguments match. Since the gold standard is composed of n-ary relations, we add all contexts (S) of an extraction as additional arguments besides arg1 and arg2 . We assess the performance of our system together with the state-of-the-art systems ClausIE, O LLIE, OpenIE-4 (Mausam, 2016), PropS (S"
C18-1195,levy-andrew-2006-tregex,0,0.0551609,"y have been observed in 1664 by Robert Hooke. core This is disputed. Figure 4: Example for subordinated clauses with closing subordinative clauses. 2305 framework can be found online1 . 3.1.3 Transformation Rules The set of hand-crafted transformation rules used in the simplification process are based on syntactic and lexical features that can be obtained from a sentence’s phrase structure, which we generated with the help of Stanford’s pre-trained lexicalized parser (Socher et al., 2013). These rules make use of regular expressions over the parse trees encoded in the form of Tregex patterns (Levy and Andrew, 2006). They were heuristically determined in a rule engineering process whose main goal was to provide a best-effort set of rules, targeting the challenge of being applied in a recursive fashion and to overcome biased or erroneous parse trees. During our experiments, we developed a fixed execution order of rules which achieved the highest F1 -score in the evaluation setting. Each rule pattern accepts a sentence’s phrasal parse tree as an input and encodes a certain parse tree pattern that, in case of a match, will extract textual parts out of the tree that are used to produce the following informat"
C18-1195,D12-1048,0,0.135741,"output of such systems (see Figure 1), we observed three common shortcomings. First, relations often span over long nested structures or are presented in a non-canonical form that cannot be easily captured by a small set of extraction patterns. Therefore, such relations are commonly missed by state-of-the-art approaches. Consider for example the first sentence in Figure 1 which asserts that hSonia Sotomayor; became; the first Supreme Court Justice of Hispanic descenti. This information is encoded in a complex participial construction that is omitted by both reference Open IE systems, O LLIE (Mausam et al., 2012) and ClausIE (Del Corro and Gemulla, 2013). Second, current Open IE systems tend to extract propositions with long argument phrases that can be further decomposed into meaningful propositions, with each of them representing a separate fact. Overly specific constituents that mix multiple - potentially semantically unrelated - propositions are difficult This work is licenced under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 2300 Proceedings of the 27th International Conference on Computational Linguistics, pages 2300–231"
C18-1195,D13-1043,0,0.0489239,"In that way, the amount of incoherent, uninformative and overspecified relation phrases is reduced. While previously mentioned Open IE systems focus on the extraction of binary relations, K RAKE N (Akbik and L¨oser, 2012) is the first approach to be specifically built for capturing complete facts from sentences by gathering the full set of arguments for each relation phrase within a sentence, thus producing tuples of arbitrary arity. The identification of relation phrases and their corresponding arguments is based on hand-written extraction rules over typed dependency information. E XEMPLAR (Mesquita et al., 2013) applies a similar approach for extracting n-ary relations, as it uses hand-crafted patterns based on dependency parse trees to detect a relation trigger and the arguments connected to it. Clause-based approaches. Aiming to improve the accuracy of Open IE approaches, more recent work is based on the idea of incorporating a sentence re-structuring stage whose goal is to transform the original sentence into a set of independent clauses that are easy to segment into Open IE tuples. An 2302 example of such a paraphrase-based Open IE approach is ClausIE, which exploits linguistic knowledge about th"
C18-1195,C16-2036,1,0.914975,"cture that is easier to process for Open IE systems, we have developed an Open IE pipeline which will be presented in the following section. 3 Proposed Open IE Approach We propose a method for facilitating the task of Open IE on sentences that present a complex linguistic structure. It is based on the idea of disembedding clausal and phrasal constituents out of a source sentence. In doing so, our approach identifies and retains the semantic connections between the individual components, thus generating a novel lightweight semantic Open IE representation. We build upon the concept presented in Niklaus et al. (2016), who distinguish between core and contextual information in the context of sentence simplification. This is done by disembedding and transforming supplementary material expressed in phrases (e.g. spatial or temporal information) into standalone context sentences, thus reducing the input sentences to their key information (core sentences). In our work, we now port this idea to a broader scope by targeting clausal disembedding techniques for complex, nested structures. Furthermore, by converting the whole simplification process into a recursive process, we are able to generate a hierarchical re"
C18-1195,schmidek-barbosa-2014-improving,0,0.10761,"rating a sentence re-structuring stage whose goal is to transform the original sentence into a set of independent clauses that are easy to segment into Open IE tuples. An 2302 example of such a paraphrase-based Open IE approach is ClausIE, which exploits linguistic knowledge about the grammar of the English language to map the dependency relations of an input sentence to clause constituents. In that way, a set of coherent clauses presenting a simple linguistic structure is derived from the input. Then, one or more predicate-argument extractions are generated for each clause. In the same vein, Schmidek and Barbosa (2014) propose a strategy to break down structurally complex sentences into simpler ones by decomposing the original sentence into its basic building blocks via chunking. The dependencies of each two chunks are then determined using dependency parsing or a Naive Bayes classifier. Depending on their relationships, chunks are combined into simplified sentences, upon which the task of relation extraction is carried out. Angeli et al. (2015) present Stanford Open IE, an approach in which a classifier is learned for splitting a sentence into a set of logically entailed shorter utterances which are then m"
C18-1195,P13-1045,0,0.0333359,"ved by Robert Hooke , SBAR IN although S NP VP DT is disputed this Extraction: “although” → Contrast core The Great Red Spot may have been observed in 1664 by Robert Hooke. core This is disputed. Figure 4: Example for subordinated clauses with closing subordinative clauses. 2305 framework can be found online1 . 3.1.3 Transformation Rules The set of hand-crafted transformation rules used in the simplification process are based on syntactic and lexical features that can be obtained from a sentence’s phrase structure, which we generated with the help of Stanford’s pre-trained lexicalized parser (Socher et al., 2013). These rules make use of regular expressions over the parse trees encoded in the form of Tregex patterns (Levy and Andrew, 2006). They were heuristically determined in a rule engineering process whose main goal was to provide a best-effort set of rules, targeting the challenge of being applied in a recursive fashion and to overcome biased or erroneous parse trees. During our experiments, we developed a fixed execution order of rules which achieved the highest F1 -score in the evaluation setting. Each rule pattern accepts a sentence’s phrasal parse tree as an input and encodes a certain parse"
C18-1195,D16-1252,0,0.270735,"ted 2009 on nominated 2009 be nominated Sonia Sotomayor on August 6, 2009 May 26 2009 May 26 May 26 Sonia Sotomayor May 26 Ollie (using framework): (8) #1 0 he nominated Sonia Sotomayor (&quot;a) S:PURPOSE to replace David Souter . (&quot;b) S:TEMPORAL on May 26 , 2009 . (9) #2 0 she was becoming the first Supreme Court Justice of Hispanic descent Figure 6: Comparison of the output of O LLIE alone and with the framework. 4 Evaluation In order to evaluate the performance of our Open IE reference implementation Graphene, we conduct an automatic evaluation using the Open IE benchmark framework proposed in Stanovsky and Dagan (2016), which is based on a QA-Semantic Role Labeling (SRL) corpus with more than 10,000 extractions over 3,200 sentences from Wikipedia and the Wall Street Journal3 . This benchmark allows us to compare our system with a variety of current Open IE approaches in recall and precision. Moreover, we investigate whether our two-layered transformation process of clausal and phrasal disembedding improves the performance of state-of-the-art Open IE systems when applied as a preprocessing step. 3 available under https://github.com/gabrielStanovsky/oie-benchmark 2307 4.1 Experimental Setup To conduct an auto"
C18-1195,P10-1013,0,0.060624,"roblems such as sentiment analysis, coreference resolution or text summarization. 2 Related Work Learning-based approaches. The line of work on Open IE begins with T EXT RUNNER (Banko et al., 2007), a self-supervised learning approach which uses a Naive Bayes classifier to train a model of relations over examples of extraction tuples that are heuristically generated from sentences in the Penn Treebank using unlexicalized POS and NP chunk features. The system then applies the learned extractor to label each word between a candidate pair of NP arguments as part of a relation phrase or not. WOE (Wu and Weld, 2010) also learns an open information extractor without direct supervision. It makes use of Wikipedia as a source of training data by bootstrapping from entries in Wikipedia infoboxes to learn extraction patterns on both POS tags (WOEpos ) and dependency parses (WOEparse ). By comparing their two approaches, Wu and Weld (2010) show that the use of dependency features results in an increase in precision and recall over shallow linguistic features (though, at the cost of extraction speed). O LLIE follows the idea of bootstrap learning of patterns based on dependency parse paths. However, while WOE re"
C18-1326,P15-1034,0,0.102954,"transitive verb, Vct : Complex-transitive verb plex sentences into simpler ones by decomposing the original sentence into its basic building blocks via chunking. The dependencies of each two chunks are then determined (one of ”connected”, ”disconnected” or ”dependent”) using either manually defined rules over dependency paths between words in different chunks or a Naive Bayes classifier trained on shallow features, such as POS tags and the distance between chunks. Depending on their relationships, chunks are combined into simplified sentences, upon which the extraction process is carried out. Angeli et al. (2015) present Stanford Open IE, an approach in which a classifier is learned for splitting a sentence into a set of logically entailed shorter utterances by recursively traversing its dependency tree and predicting at each step whether an edge should yield an independent clause or not. In order to increase the usefulness of the extracted propositions for downstream applications, each self-contained clause is then maximally shortened by running natural logic inference over it. In the end, a small set of 14 hand-crafted patterns are used to extract a predicate-argument triple from each utterance. An"
C18-1326,D16-1006,0,0.0290058,"rs are directly assigned to the corresponding relational tuples, Bast and Haussmann (2013) represent contextual information in the form of separate, linked propositions. To do so, each extraction is given a unique identifier that can be used in the argument position of an extraction for a later substitution with the corresponding fact by a downstream application. An example for an attribution is shown below (Bast and Haussmann, 2013): #1: hThe Embassy; said; that #2i #2: h6,700 Americans; were; in Pakistan.i Another current approach that captures inter-proposition relationships is proposed by Bhutani et al. (2016), who present a nested representation for Open IE that is able to capture high-level dependencies, allowing for a more accurate representation of the meaning of an input sentence. Their system N EST IE uses bootstrapping over a dataset for textual entailment to learn both binary and nested triple representations for n-ary relations over dependency parse trees. These patterns can take on the form of binary triples harg1 ; rel; arg2 i or nested triples such as hharg1 ; rel; arg2 i; rel2 ; arg3 i for n-ary relations. Using a set of manually defined rules, contextual links between extracted propos"
C18-1326,C18-1195,1,0.907966,"ssibility [believes]) #2: hSuperman; was born on; beautiful Kryptoni Annotation: factuality, (- [not], certainty), attribution (Pinocchio, +, possibility [believes]) #3: hSuperman; ”is”; heroi Annotation: factuality, (+, certainty) with + and - signifying positive and negative polarity, respectively. In that way, the output generated by MinIE is further reduced to its core constituents, producing maximally shortened, semantically enriched extractions. To further enhance the expressiveness of extracted propositions and sustain their interpretability in downstream artificial intelligence tasks, Cetto et al. (2018) propose Graphene, an Open IE framework that uses a set of hand-crafted simplification rules to transform complex natural language sentences into clean, compact structures by removing clauses and phrases that present no central information from the input and converting them into stand-alone sentences. In that way, a source sentence is transformed into a hierarchical representation in the form of core facts and accompanying contexts (Niklaus et al., 2016). In addition, inspired by the work on Rhetorical Structure Theory (Mann and Thompson, 1988), a set of syntactic and lexical patterns is used"
C18-1326,W10-0907,0,0.213812,"nt and uninforphrase within a sentence, thus producing tuples of arbimative extractions. trary arity. The identification of relational phrases and their corresponding arguments is based on hand-written extraction rules over typed dependency parses. E XEMPLAR (Mesquita et al., 2013) applies a similar approach for extracting n-ary relations, using hand-crafted patterns based on dependency parse trees to detect a relation trigger and the arguments connected to it. Based on the task of Semantic Role Labeling (SRL), whose key idea is to classify semantic constituents into different semantic roles (Christensen et al., 2010), it assigns each argument its corresponding role (such as subject, direct object or prepositional object). A more abstract approach, P ROP S, was suggested by Stanovsky et al. (2016), who argue that it is hard to read out from a dependency parse the complete structure of a sentence’s propositions, since, amongst others, different predications are represented in a non-uniform manner and proposition boundaries are not easy to detect. Therefore, they introduce a more semantically-oriented sentence representation that is generated by transforming a dependency parse tree into a directed graph whic"
C18-1326,D11-1142,0,0.553187,"fail to increase [...] hCongress; fail; to increase [...]i President Bush fail to increase [...] hPresident Bush; fail; to increase [...]i Condition context The funding will be delayed. Coordination List core Condition List core President Bush fail to Congress fail to increase the Treasury’s increase the Treasury’s borrowing capacity. borrowing capacity. Figure 5: Graphene’s extraction workflow for an example sentence (Cetto et al., 2018). performance of different systems is widely agreed upon. As can be seen in Table 2, the corpora compiled by Del Corro and Gemulla (2013), Xu et al. (2013), Fader et al. (2011) and Banko et al. (2007) are occasionally re-used. However, new datasets are still collected, hindering a fair comparison of the proposed approaches. Besides, although Open IE methods are targeted at being domain independent and able to cope with heterogeneous datasets, the corpora used in the evaluation process are restricted to the news, Wikipedia and Web domains for the most part. Accordingly, no clear statement about the portability of the approaches to various genres of text is possible. In addition, most evaluation procedures described in the literature focus on precision-oriented metric"
C18-1326,D16-1086,0,0.0578169,"mong different Open IE systems in a large-scale, objective and reproducible fashion. Instead, most approaches use proprietary datasets over small, domain-dependent corpora. Stanovsky and Dagan (2016) and Schneider et al. (2017) recently made the first move to standardize the evaluation of Open IE by proposing benchmark frameworks that operate on a larger scale. However, apart from Cetto et al. (2018), neither benchmarking toolkit has been adopted yet in the Open IE community. Moreover, most Open IE approaches focus on the English language, leaving aside other languages. Notable exceptions are Falke et al. (2016), who investigate the portability of the P ROP S system to German, and Gamallo and Garcia (2015), who propose a multilingual system covering English, Spanish, Portuguese and Galician. Besides, due to the use of language-agnostic patterns over UD parses, PredPatt works across languages, though its performance was only evaluated on English texts. Hence, the applicability and transferability of previously proposed Open IE approaches to other languages than English represents an interesting direction for future work. Finally, the problem of canonicalizing relational phrases and arguments has been"
C18-1326,D17-1278,0,0.108163,"the corresponding propositions using identifiers, e.g. (Bhutani et al., 2016): #1: hbody; appeared to have been thrown; ∅i #2: h#1; from; vehiclei or another example based on the following sentence: 1 2 https://github.com/dair-iitd/OpenIE-standalone https://github.com/swarnaHub/OpenIEListExtractor 3871 “After giving 5,000 people a second chance at life, doctors are celebrating the 25th anniversary of Britain’s first heart transplant.” #1: hdoctors; are celebrating; the 25th anniversary of Britain’s first heart transplanti #2: hdoctors; giving; second chance at lifei #3: h#1; after; #2i MinIE (Gashteovski et al., 2017), another recent Open IE system, is built on top of ClausIE, a system that was found to often produce overly specific extractions. Such overly specific constituents that combine multiple, potentially semantically unrelated propositions in a single relational or argument phrase generally hurt the performance of downstream semantic applications, such as question answering or textual entailment. In fact, those approaches benefit from extractions that are as compact as possible. Therefore, MinIE aims to minimize both relational and argument phrases by identifying and removing parts that are consid"
C18-1326,D15-1076,0,0.0153897,"2013) Table 3: Extrinsic evaluation approaches. 3874 relations from heterogeneous corpora, instead of only extracting a set of pre-specified classes of relations. The majority of current Open IE systems realize this challenge by considering all possible verbs as potential relations. Accordingly, their scope is often limited to the extraction of verbal predicates, while ignoring relations mediated by more complex syntactic constructs, such as nouns or adjectives. Realizing that above-mentioned requirements are subsumed by the task of Question Answering (QA) driven Semantic Role Labeling (SRL) (He et al., 2015), Stanovsky and Dagan (2016) converted the annotations of a QA-SRL dataset to an Open IE corpus, resulting in more than 10,000 extractions over 3,200 sentences from Wikipedia and the Wall Street Journal. In addition, Schneider et al. (2017) presented RelVis, another benchmark framework for Open IE that allows for a large-scale comparative analysis of Open IE approaches. Besides Stanovsky and Dagan (2016)’s benchmark suite, it comprises the n-ary news dataset proposed in Mesquita et al. (2013), Banko et al. (2007)’s Web corpus and the Penn sentences from Xu et al. (2013). Similar to the toolkit"
C18-1326,D08-1008,0,0.064893,"Missing"
C18-1326,de-marneffe-etal-2014-universal,0,0.0608784,"Missing"
C18-1326,D12-1048,0,0.820742,"e number of sentences from which each extraction was found, thus exploiting the redundancy of information in Web text and assigning higher confidence to extractions that occur multiple times. 3867 WOE (Wu and Weld, 2010) also learns an open information extractor without direct supervision. It makes use of Wikipedia as a source of training data by bootstrapping from entries in Wikipedia infoboxes, i.e. by heuristically matching infobox attribute-value pairs with corresponding sentences in the article. This data is then used to learn extraction patterns on Figure 2: OLLIE’s system architecture (Mausam et al., 2012). both POS tags (WOEpos ) and deOLLIE begins with seed tuples from R E V ERB, uses them to pendency parses (WOEparse ). Forbuild a bootstrap learning set, and learns open pattern templates. mer extractor utilizes a linear-chain These are applied to individual sentences at extraction time. Conditional Random Field (CRF) to train a model of relations on shallow features which outputs certain text between two NPs when it denotes a relation. Latter approach, in contrast, makes use of dependency trees to build a classifier that decides whether the shortest dependency path between two NPs indicates"
C18-1326,D13-1043,0,0.31748,"ding to extraction errors such as incomplete, uninformative or erroneous propositions, K RAKE N (Akbik and L¨oser, 2012) is the first approach to be specifically built for capturing complete facts from sentences by Figure 3: ReVerb’s POS-based regular exgathering the full set of arguments for each relational pression for reducing incoherent and uninforphrase within a sentence, thus producing tuples of arbimative extractions. trary arity. The identification of relational phrases and their corresponding arguments is based on hand-written extraction rules over typed dependency parses. E XEMPLAR (Mesquita et al., 2013) applies a similar approach for extracting n-ary relations, using hand-crafted patterns based on dependency parse trees to detect a relation trigger and the arguments connected to it. Based on the task of Semantic Role Labeling (SRL), whose key idea is to classify semantic constituents into different semantic roles (Christensen et al., 2010), it assigns each argument its corresponding role (such as subject, direct object or prepositional object). A more abstract approach, P ROP S, was suggested by Stanovsky et al. (2016), who argue that it is hard to read out from a dependency parse the comple"
C18-1326,P09-1113,0,0.0135714,"it expands the syntactic scope of relational phrases to cover a wider range of relation expressions, resulting in a much higher yield (at comparable precision) as compared to previous systems. More recently, Yahya et al. (2014) proposed ReNoun, an Open IE system that entirely focuses on the extraction of noun-mediated relations. Starting with a small set of high-precision seed facts relying on manually specified lexical patterns that are specifically tailored for NPs, a set of dependency parse patterns for the extraction of noun-based relations is learned with the help of distant supervision (Mintz et al., 2009). These patterns are then applied to generate a set of candidate extractions which are assigned a confidence score based on the frequency and coherence of the patterns producing them. 2.2 Rule-based Systems The second category of Open IE systems make use of hand-crafted extraction rules. This includes R E V ERB (Fader et al., 2011), a shallow extractor that addresses three common errors of hitherto existing Open IE systems: the output of such systems frequently contains a great many of uninformative extractions (i.e. extractions that omit critical information), incoherent extractions (i.e. ext"
C18-1326,D12-1104,0,0.109501,"Missing"
C18-1326,C16-2036,1,0.788841,"To further enhance the expressiveness of extracted propositions and sustain their interpretability in downstream artificial intelligence tasks, Cetto et al. (2018) propose Graphene, an Open IE framework that uses a set of hand-crafted simplification rules to transform complex natural language sentences into clean, compact structures by removing clauses and phrases that present no central information from the input and converting them into stand-alone sentences. In that way, a source sentence is transformed into a hierarchical representation in the form of core facts and accompanying contexts (Niklaus et al., 2016). In addition, inspired by the work on Rhetorical Structure Theory (Mann and Thompson, 1988), a set of syntactic and lexical patterns is used to identify the rhetorical relations by which core sentences and their associated contexts are connected in order to preserve their semantic relationships and return a set of semantically typed and interconnected relational tuples (see extractions (15-17) in Figure 1). Graphene’s extraction workflow is illustrated in Figure 5. 3 Evaluation Though a multitude of systems for Open IE have been developed over the last decade, a clear formal specification of"
C18-1326,W16-1307,0,0.0411378,"fied by matching patterns with the dependency parse of the sentence. Clausal modifiers are determined by an adverbial-clause edge and filtered lexically (the first word of the clause must match a list of cue terms, e.g. if, when, or although), while attribution modifiers are identified by a clausal-complement edge whose context verb must match one of the terms given in VerbNet’s list of common verbs of communication and cognition (Mausam et al., 2012). A similar output is produced by OLLIES’s successor O PEN IE4 (Mausam, 2016), which combines S RL IE (Christensen et al., 2010) and R EL N OUN (Pal and Mausam, 2016). Former is a system that converts the output of a SRL system into an Open IE extraction by treating the verb as the relational phrase, while taking its role-labeled arguments as the Open IE argument phrases related to the relation. Latter, in contrast, represents a rule-based Open IE system that extracts noun-mediated relations, thereby paying special attention to demonyms and compound relational nouns. In addition, O PEN IE4 marks temporal and spatial arguments by assigning them a T or S label, respectively. Lately, its successor O PEN IE 5.0 was released1 . It integrates BONIE (Saha et al.,"
C18-1326,D13-1020,0,0.0848796,"Missing"
C18-1326,P17-2050,0,0.0235409,"ausam, 2016). Former is a system that converts the output of a SRL system into an Open IE extraction by treating the verb as the relational phrase, while taking its role-labeled arguments as the Open IE argument phrases related to the relation. Latter, in contrast, represents a rule-based Open IE system that extracts noun-mediated relations, thereby paying special attention to demonyms and compound relational nouns. In addition, O PEN IE4 marks temporal and spatial arguments by assigning them a T or S label, respectively. Lately, its successor O PEN IE 5.0 was released1 . It integrates BONIE (Saha et al., 2017) and OpenIEListExtractor2 . While the former focuses on extracting tuples where one of the arguments is a number or a quantity-unit phrase, the latter targets the extraction of propositions from conjunctive sentences. Similar to OLLIE, Bast and Haussmann (2013), who explore the use of contextual sentence decomposition (CSD) for Open IE, advocate to further specify propositions with information on which they depend. Their system CSD-IE is based on the idea of paraphrasing-based approaches described in section 2.3. Using a set of hand-crafted rules over the output of a constituent parser, a sent"
C18-1326,schmidek-barbosa-2014-improving,0,0.0181316,"anguage to map the dependency relations of an input sentence to clause constituents. In that way, a set of coherent clauses presenting a simple linguistic structure is derived from the input. Then, the type of each clause is determined by combining knowledge of properties of verbs (with the help of domain-independent lexica) with knowledge about the structure of input clauses. Finally, based on its type, one or more propositions are generated from each clause, each representing different pieces of information. The basic set of patterns used for this task is shown in Table 1. In the same vein, Schmidek and Barbosa (2014) propose a strategy to break down structurally com3869 S1 : S2 : S3 : S4 : S5 : S6 : S7 : Pattern SVi SVe A SVc C SVmt O SVdt Oi O SVct OA SVct OC Clause type SV SVA SVC SVO SVOO SVOA SVOC Example AE died. AE remained in Princeton. AE is smart. AE has won the Nobel Prize. RSAS gave AE the Nobel Prize. The doorman showed AE to his office. AE declared the meeting open. Derived clauses (AE, died) (AE, remained, in Princeton) (AE, is, smart) (AE, has won, the Nobel Prize) (RSAS, gave, AE, the Nobel Prize) (The doorman, showed, AE, to his office) (AE, declared, the meeting, open) Table 1: Basic pat"
C18-1326,W17-5402,0,0.0894295,"Missing"
C18-1326,D16-1252,0,0.523337,"total number of extractions labeled as correct, or coverage, i.e. the percentage of text from the input that is contained in at least one of the extractions. Hence, the absence of a standard evaluation procedure makes it hard to replicate and compare the performance of different Open IE systems. Table 2 provides a detailed overview of both the datasets and measures used for intrinsically evaluating the various approaches described above, while Table 3 shows the tasks that were used for an extrinsic evaluation of a small set of Open IE systems. In order to address aforementioned difficulties, Stanovsky and Dagan (2016) recently made a first attempt in standardizing the Open IE evaluation by providing a large gold benchmark corpus. It is based on a set of consensual guiding principles that underly most Open IE approaches proposed so far, as they have identified. Those principles cover the core aspects of the task of Open IE, allowing for a clearer formulation of the problem to be solved. The three key features to consider are the following: Assertedness. The assertedness principle states that extracted propositions should be asserted by the original sentence. Usually, instead of inferring propositions out of"
C18-1326,P03-1002,0,0.292541,"Missing"
C18-1326,D16-1177,0,0.0624167,"Missing"
C18-1326,P10-1013,0,0.138695,"and then heuristically designating each word in between as part of a relation phrase or not. Next, each candidate extraction is presented to the classifier, whereupon only those labeled as trustworthy are kept. Restricting to the use of shallow features in this step makes T EXT RUNNER highly efficient. Finally, a redundancy-based assessor assigns a probability to each retained tuple based on the number of sentences from which each extraction was found, thus exploiting the redundancy of information in Web text and assigning higher confidence to extractions that occur multiple times. 3867 WOE (Wu and Weld, 2010) also learns an open information extractor without direct supervision. It makes use of Wikipedia as a source of training data by bootstrapping from entries in Wikipedia infoboxes, i.e. by heuristically matching infobox attribute-value pairs with corresponding sentences in the article. This data is then used to learn extraction patterns on Figure 2: OLLIE’s system architecture (Mausam et al., 2012). both POS tags (WOEpos ) and deOLLIE begins with seed tuples from R E V ERB, uses them to pendency parses (WOEparse ). Forbuild a bootstrap learning set, and learns open pattern templates. mer extrac"
C18-1326,N13-1107,0,0.124044,"elayed; i Congress fail to increase [...] hCongress; fail; to increase [...]i President Bush fail to increase [...] hPresident Bush; fail; to increase [...]i Condition context The funding will be delayed. Coordination List core Condition List core President Bush fail to Congress fail to increase the Treasury’s increase the Treasury’s borrowing capacity. borrowing capacity. Figure 5: Graphene’s extraction workflow for an example sentence (Cetto et al., 2018). performance of different systems is widely agreed upon. As can be seen in Table 2, the corpora compiled by Del Corro and Gemulla (2013), Xu et al. (2013), Fader et al. (2011) and Banko et al. (2007) are occasionally re-used. However, new datasets are still collected, hindering a fair comparison of the proposed approaches. Besides, although Open IE methods are targeted at being domain independent and able to cope with heterogeneous datasets, the corpora used in the evaluation process are restricted to the news, Wikipedia and Web domains for the most part. Accordingly, no clear statement about the portability of the approaches to various genres of text is possible. In addition, most evaluation procedures described in the literature focus on prec"
C18-1326,D14-1038,0,0.0396155,"esentation by adding attribution and clausal modifiers, if necessary, and thus increasing the precision of the system (see extractions (1) and (2) in Figure 1; for details, see section 2.4). Moreover, OLLIE is the first Open IE approach to identify not only verb-based relations, but also relationships mediated by nouns and adjectives (see extractions (3) and (4) in Figure 1). In that way, it expands the syntactic scope of relational phrases to cover a wider range of relation expressions, resulting in a much higher yield (at comparable precision) as compared to previous systems. More recently, Yahya et al. (2014) proposed ReNoun, an Open IE system that entirely focuses on the extraction of noun-mediated relations. Starting with a small set of high-precision seed facts relying on manually specified lexical patterns that are specifically tailored for NPs, a set of dependency parse patterns for the extraction of noun-based relations is learned with the help of distant supervision (Mintz et al., 2009). These patterns are then applied to generate a set of candidate extractions which are assigned a confidence score based on the frequency and coherence of the patterns producing them. 2.2 Rule-based Systems T"
C18-1326,W17-6944,0,0.0247139,"Missing"
C18-2021,P15-1034,0,0.0268786,"ting a semantic relation between them: harg1; rel; arg2i. Unlike traditional IE methods, Open IE is not limited to a small set of target relations known in advance, but rather extracts all types of relations found in a text. In that way, it facilitates the domain-independent discovery of relations extracted from text and scales to large, heterogeneous corpora such as the Web. Since its introduction by Banko et al. (2007), a large body of work on the task of Open IE has been described. By analyzing the output of state-of-the-art systems (e.g., (Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015)), we observed three common shortcomings. First, relations often span over long nested structures or are presented in a non-canonical form that cannot be easily captured by a small set of extraction patterns. Therefore, such relations are commonly missed by state-of-the-art approaches. Second, current Open IE systems tend to extract propositions with long argument phrases that can be further decomposed into meaningful propositions, with each of them representing a separate fact. Overly specific constituents that mix multiple - potentially semantically unrelated - propositions are difficult to"
C18-2021,C18-1195,1,0.802051,"Missing"
C18-2021,D12-1048,0,0.0495211,"consisting of a set of arguments and a phrase denoting a semantic relation between them: harg1; rel; arg2i. Unlike traditional IE methods, Open IE is not limited to a small set of target relations known in advance, but rather extracts all types of relations found in a text. In that way, it facilitates the domain-independent discovery of relations extracted from text and scales to large, heterogeneous corpora such as the Web. Since its introduction by Banko et al. (2007), a large body of work on the task of Open IE has been described. By analyzing the output of state-of-the-art systems (e.g., (Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015)), we observed three common shortcomings. First, relations often span over long nested structures or are presented in a non-canonical form that cannot be easily captured by a small set of extraction patterns. Therefore, such relations are commonly missed by state-of-the-art approaches. Second, current Open IE systems tend to extract propositions with long argument phrases that can be further decomposed into meaningful propositions, with each of them representing a separate fact. Overly specific constituents that mix multiple - potentially sema"
C18-2021,C16-2036,1,0.803041,"layer. Phrasal Disembedding. After recursively dividing multi-clause sentences into stand-alone sentences that contain one clause each, they are further simplified on a phrasal level. For this purpose, sentences are processed separately and transformed into simpler structures by extracting the following phrasal components from the input: prepositional phrases, participial phrases, adjectival/adverbial phrases, appositive phrases, lead noun phrases, coordinations of verb phrases, enumerations of noun phrases and purposes. This task is assisted by the sentence simplification system described in Niklaus et al. (2016). 2.2 Relation Extraction After the transformation stage, RE is performed by using the simplified sentences as an input. The framework is designed to accept any type of RE implementation which is able to extract relational tuples from single sentences. The identified rhetorical relations from the transformation stage are then mapped to the corresponding relational tuples in the form of simple and linked contextual arguments (see Section 3). As a result, different approaches for RE can be complemented with contextual information that further specifies the extracted relational tuples. In that wa"
C18-2021,D16-1252,0,0.0375865,"Missing"
D19-1440,S18-1161,0,0.0634398,"Missing"
D19-1440,S18-1155,0,0.0245003,"vector models (JoBimText), dense word-vector (DWV) models (W2V, GloVe) and supervised/ unsupervised machine learning approaches (SVM, MLP, Ensemble Methods). With regard to interpretability and explainability we can classify IDA approaches into three categories. Frequency-based models over textbased features, heavily relying on textual features and frequency-based methods (Gamallo, 2018; Gonz´alez et al., 2018) ; ML over Textual features (Dumitru et al., 2018; Sommerauer et al., 2018; King et al., 2018; Mao et al., 2018) and ML over dense vectors and textual features (Brychc´ın et al., 2018; Attia et al., 2018; Wu et al., 2018; Dumitru et al., 2018; Arroyo-Fern´andez et al., 2018; Speer and Lowry-Duda, 2018; Santus et al., 2018; Grishin, 2018; Zhou et al., 2018; Vinayan et al., 2018; Kulmizev et al., 2018; Zhang and Carpuat, 2018; Shiue et al., 2018). While the first category concentrates on models with higher interpretability, none of these models provide explanations. Comparatively, this work focuses on the creation of an explicit word vector space model (EWVM), with an associated explanation, evaluating the performance of different types of lexicosemantic resources in the context of the task of"
D19-1440,S18-1153,0,0.045045,"Missing"
D19-1440,S18-1158,0,0.0571221,"Missing"
D19-1440,S18-1156,0,0.0306745,"and (ii) definition-based word vector space models. Existing approaches have explored combinations of linguistic and data resources (WordNet, ConceptNet, Wikipedia), linguistic features (syntactic dependencies), sparse word vector models (JoBimText), dense word-vector (DWV) models (W2V, GloVe) and supervised/ unsupervised machine learning approaches (SVM, MLP, Ensemble Methods). With regard to interpretability and explainability we can classify IDA approaches into three categories. Frequency-based models over textbased features, heavily relying on textual features and frequency-based methods (Gamallo, 2018; Gonz´alez et al., 2018) ; ML over Textual features (Dumitru et al., 2018; Sommerauer et al., 2018; King et al., 2018; Mao et al., 2018) and ML over dense vectors and textual features (Brychc´ın et al., 2018; Attia et al., 2018; Wu et al., 2018; Dumitru et al., 2018; Arroyo-Fern´andez et al., 2018; Speer and Lowry-Duda, 2018; Santus et al., 2018; Grishin, 2018; Zhou et al., 2018; Vinayan et al., 2018; Kulmizev et al., 2018; Zhang and Carpuat, 2018; Shiue et al., 2018). While the first category concentrates on models with higher interpretability, none of these models provide explanations. Comp"
D19-1440,S18-1159,0,0.0495258,"Missing"
D19-1440,S18-1164,0,0.0195351,"Ensemble Methods). With regard to interpretability and explainability we can classify IDA approaches into three categories. Frequency-based models over textbased features, heavily relying on textual features and frequency-based methods (Gamallo, 2018; Gonz´alez et al., 2018) ; ML over Textual features (Dumitru et al., 2018; Sommerauer et al., 2018; King et al., 2018; Mao et al., 2018) and ML over dense vectors and textual features (Brychc´ın et al., 2018; Attia et al., 2018; Wu et al., 2018; Dumitru et al., 2018; Arroyo-Fern´andez et al., 2018; Speer and Lowry-Duda, 2018; Santus et al., 2018; Grishin, 2018; Zhou et al., 2018; Vinayan et al., 2018; Kulmizev et al., 2018; Zhang and Carpuat, 2018; Shiue et al., 2018). While the first category concentrates on models with higher interpretability, none of these models provide explanations. Comparatively, this work focuses on the creation of an explicit word vector space model (EWVM), with an associated explanation, evaluating the performance of different types of lexicosemantic resources in the context of the task of identification of discriminative attributes (IDA). 3 3.1 Identifying and Explaining Discriminative Attributes Problem Definition This p"
D19-1440,S18-1167,0,0.0503224,"Missing"
D19-1440,S18-1118,0,0.0667544,"Missing"
D19-1440,S18-1169,0,0.0314397,"WordNet, ConceptNet, Wikipedia), linguistic features (syntactic dependencies), sparse word vector models (JoBimText), dense word-vector (DWV) models (W2V, GloVe) and supervised/ unsupervised machine learning approaches (SVM, MLP, Ensemble Methods). With regard to interpretability and explainability we can classify IDA approaches into three categories. Frequency-based models over textbased features, heavily relying on textual features and frequency-based methods (Gamallo, 2018; Gonz´alez et al., 2018) ; ML over Textual features (Dumitru et al., 2018; Sommerauer et al., 2018; King et al., 2018; Mao et al., 2018) and ML over dense vectors and textual features (Brychc´ın et al., 2018; Attia et al., 2018; Wu et al., 2018; Dumitru et al., 2018; Arroyo-Fern´andez et al., 2018; Speer and Lowry-Duda, 2018; Santus et al., 2018; Grishin, 2018; Zhou et al., 2018; Vinayan et al., 2018; Kulmizev et al., 2018; Zhang and Carpuat, 2018; Shiue et al., 2018). While the first category concentrates on models with higher interpretability, none of these models provide explanations. Comparatively, this work focuses on the creation of an explicit word vector space model (EWVM), with an associated explanation, evaluating th"
D19-1440,S18-1163,0,0.0224175,"pproaches (SVM, MLP, Ensemble Methods). With regard to interpretability and explainability we can classify IDA approaches into three categories. Frequency-based models over textbased features, heavily relying on textual features and frequency-based methods (Gamallo, 2018; Gonz´alez et al., 2018) ; ML over Textual features (Dumitru et al., 2018; Sommerauer et al., 2018; King et al., 2018; Mao et al., 2018) and ML over dense vectors and textual features (Brychc´ın et al., 2018; Attia et al., 2018; Wu et al., 2018; Dumitru et al., 2018; Arroyo-Fern´andez et al., 2018; Speer and Lowry-Duda, 2018; Santus et al., 2018; Grishin, 2018; Zhou et al., 2018; Vinayan et al., 2018; Kulmizev et al., 2018; Zhang and Carpuat, 2018; Shiue et al., 2018). While the first category concentrates on models with higher interpretability, none of these models provide explanations. Comparatively, this work focuses on the creation of an explicit word vector space model (EWVM), with an associated explanation, evaluating the performance of different types of lexicosemantic resources in the context of the task of identification of discriminative attributes (IDA). 3 3.1 Identifying and Explaining Discriminative Attributes Problem De"
D19-1440,S18-1171,0,0.0247543,"o three categories. Frequency-based models over textbased features, heavily relying on textual features and frequency-based methods (Gamallo, 2018; Gonz´alez et al., 2018) ; ML over Textual features (Dumitru et al., 2018; Sommerauer et al., 2018; King et al., 2018; Mao et al., 2018) and ML over dense vectors and textual features (Brychc´ın et al., 2018; Attia et al., 2018; Wu et al., 2018; Dumitru et al., 2018; Arroyo-Fern´andez et al., 2018; Speer and Lowry-Duda, 2018; Santus et al., 2018; Grishin, 2018; Zhou et al., 2018; Vinayan et al., 2018; Kulmizev et al., 2018; Zhang and Carpuat, 2018; Shiue et al., 2018). While the first category concentrates on models with higher interpretability, none of these models provide explanations. Comparatively, this work focuses on the creation of an explicit word vector space model (EWVM), with an associated explanation, evaluating the performance of different types of lexicosemantic resources in the context of the task of identification of discriminative attributes (IDA). 3 3.1 Identifying and Explaining Discriminative Attributes Problem Definition This paper provides an explainable word vector space model (EWVM) for detecting whether a given term is a discrimina"
D19-1440,S18-1168,0,0.0516305,"Missing"
D19-1440,S18-1117,0,0.192975,"and Pantel, 2010), (Freitas, 2015). Additionally, the simplicity entailed by the vector space abstraction makes it an engineering-friendly representation, also explaining its widespread adoption and use (Freitas, 2015). However, the latent features (dense vectors) at the center of most of the best-performing models have limited their application to two main uses: (i) computing semantic similarity and relatedness measures and (ii) performing vocabulary generalization as an input layer on Machine Learning (ML) models. The identification of discriminative attributes (IDA), recently introduced by Krebs et al. (2018), can motivate the development of word vector models which can support types of operations with finer semantics, going beyond the computation of semantic similarity and relatedness scores, with potential applications in fine-grained semantic inference tasks. Concretely, using the example provided by (Krebs et al., 2018), given a pair of target terms apple and banana, the IDA task seeks to answer if the term red is a discriminative attribute for apple in comparison to banana. According to Krebs et al. semantic difference is a ternary relation between two concepts (apple, banana) and a discrimin"
D19-1440,W16-5323,1,0.823298,"hese roles aim to transform natural definitions into definition knowledge graphs, in order to facilitate natural language inference tasks (Silva et al., 2018b), (Silva et al., 2018a). The set of semantic roles includes: supertype, differentia quality, differentia event, event location, purpose, accessory determiner, origin location. Figure 1 (DBM) depicts an example of a classified definition. The semantic roles are assigned by building a recurrent neural network (RNN) Definition Role Labeling (DRL) classifier using POS-tags, and pre-trained word vectors as features using the configuration of Silva et al. (2016). After the semantic roles are assigned, they are used as an input to build the supporting word vector space. For each definition segment, all tokens are lemmatized and stop-words removed. Afterwards, an inverse document frequency (idf) weighting scheme is applied, in order to support the computation of semantic similarity and relatedness scores within the model (despite not being the target use of the model). Ad4315 ditionally, for each target term, we take into account its upward taxonomic chain, i.e. it inherits the definition attributes from the parent terms linked by the detected supertyp"
D19-1440,S18-1154,0,0.0255166,"binations of linguistic and data resources (WordNet, ConceptNet, Wikipedia), linguistic features (syntactic dependencies), sparse word vector models (JoBimText), dense word-vector (DWV) models (W2V, GloVe) and supervised/ unsupervised machine learning approaches (SVM, MLP, Ensemble Methods). With regard to interpretability and explainability we can classify IDA approaches into three categories. Frequency-based models over textbased features, heavily relying on textual features and frequency-based methods (Gamallo, 2018; Gonz´alez et al., 2018) ; ML over Textual features (Dumitru et al., 2018; Sommerauer et al., 2018; King et al., 2018; Mao et al., 2018) and ML over dense vectors and textual features (Brychc´ın et al., 2018; Attia et al., 2018; Wu et al., 2018; Dumitru et al., 2018; Arroyo-Fern´andez et al., 2018; Speer and Lowry-Duda, 2018; Santus et al., 2018; Grishin, 2018; Zhou et al., 2018; Vinayan et al., 2018; Kulmizev et al., 2018; Zhang and Carpuat, 2018; Shiue et al., 2018). While the first category concentrates on models with higher interpretability, none of these models provide explanations. Comparatively, this work focuses on the creation of an explicit word vector space model (EWVM), with an"
D19-1440,S18-1162,0,0.0237688,"upervised machine learning approaches (SVM, MLP, Ensemble Methods). With regard to interpretability and explainability we can classify IDA approaches into three categories. Frequency-based models over textbased features, heavily relying on textual features and frequency-based methods (Gamallo, 2018; Gonz´alez et al., 2018) ; ML over Textual features (Dumitru et al., 2018; Sommerauer et al., 2018; King et al., 2018; Mao et al., 2018) and ML over dense vectors and textual features (Brychc´ın et al., 2018; Attia et al., 2018; Wu et al., 2018; Dumitru et al., 2018; Arroyo-Fern´andez et al., 2018; Speer and Lowry-Duda, 2018; Santus et al., 2018; Grishin, 2018; Zhou et al., 2018; Vinayan et al., 2018; Kulmizev et al., 2018; Zhang and Carpuat, 2018; Shiue et al., 2018). While the first category concentrates on models with higher interpretability, none of these models provide explanations. Comparatively, this work focuses on the creation of an explicit word vector space model (EWVM), with an associated explanation, evaluating the performance of different types of lexicosemantic resources in the context of the task of identification of discriminative attributes (IDA). 3 3.1 Identifying and Explaining Discriminative"
D19-1440,S18-1166,0,0.0212238,"interpretability and explainability we can classify IDA approaches into three categories. Frequency-based models over textbased features, heavily relying on textual features and frequency-based methods (Gamallo, 2018; Gonz´alez et al., 2018) ; ML over Textual features (Dumitru et al., 2018; Sommerauer et al., 2018; King et al., 2018; Mao et al., 2018) and ML over dense vectors and textual features (Brychc´ın et al., 2018; Attia et al., 2018; Wu et al., 2018; Dumitru et al., 2018; Arroyo-Fern´andez et al., 2018; Speer and Lowry-Duda, 2018; Santus et al., 2018; Grishin, 2018; Zhou et al., 2018; Vinayan et al., 2018; Kulmizev et al., 2018; Zhang and Carpuat, 2018; Shiue et al., 2018). While the first category concentrates on models with higher interpretability, none of these models provide explanations. Comparatively, this work focuses on the creation of an explicit word vector space model (EWVM), with an associated explanation, evaluating the performance of different types of lexicosemantic resources in the context of the task of identification of discriminative attributes (IDA). 3 3.1 Identifying and Explaining Discriminative Attributes Problem Definition This paper provides an explainable word vector"
D19-1440,S18-1157,0,0.0277944,"mText), dense word-vector (DWV) models (W2V, GloVe) and supervised/ unsupervised machine learning approaches (SVM, MLP, Ensemble Methods). With regard to interpretability and explainability we can classify IDA approaches into three categories. Frequency-based models over textbased features, heavily relying on textual features and frequency-based methods (Gamallo, 2018; Gonz´alez et al., 2018) ; ML over Textual features (Dumitru et al., 2018; Sommerauer et al., 2018; King et al., 2018; Mao et al., 2018) and ML over dense vectors and textual features (Brychc´ın et al., 2018; Attia et al., 2018; Wu et al., 2018; Dumitru et al., 2018; Arroyo-Fern´andez et al., 2018; Speer and Lowry-Duda, 2018; Santus et al., 2018; Grishin, 2018; Zhou et al., 2018; Vinayan et al., 2018; Kulmizev et al., 2018; Zhang and Carpuat, 2018; Shiue et al., 2018). While the first category concentrates on models with higher interpretability, none of these models provide explanations. Comparatively, this work focuses on the creation of an explicit word vector space model (EWVM), with an associated explanation, evaluating the performance of different types of lexicosemantic resources in the context of the task of identification of"
D19-1440,S18-1170,0,0.0247101,"assify IDA approaches into three categories. Frequency-based models over textbased features, heavily relying on textual features and frequency-based methods (Gamallo, 2018; Gonz´alez et al., 2018) ; ML over Textual features (Dumitru et al., 2018; Sommerauer et al., 2018; King et al., 2018; Mao et al., 2018) and ML over dense vectors and textual features (Brychc´ın et al., 2018; Attia et al., 2018; Wu et al., 2018; Dumitru et al., 2018; Arroyo-Fern´andez et al., 2018; Speer and Lowry-Duda, 2018; Santus et al., 2018; Grishin, 2018; Zhou et al., 2018; Vinayan et al., 2018; Kulmizev et al., 2018; Zhang and Carpuat, 2018; Shiue et al., 2018). While the first category concentrates on models with higher interpretability, none of these models provide explanations. Comparatively, this work focuses on the creation of an explicit word vector space model (EWVM), with an associated explanation, evaluating the performance of different types of lexicosemantic resources in the context of the task of identification of discriminative attributes (IDA). 3 3.1 Identifying and Explaining Discriminative Attributes Problem Definition This paper provides an explainable word vector space model (EWVM) for detecting whether a given"
D19-1440,S18-1165,0,0.0235499,"s). With regard to interpretability and explainability we can classify IDA approaches into three categories. Frequency-based models over textbased features, heavily relying on textual features and frequency-based methods (Gamallo, 2018; Gonz´alez et al., 2018) ; ML over Textual features (Dumitru et al., 2018; Sommerauer et al., 2018; King et al., 2018; Mao et al., 2018) and ML over dense vectors and textual features (Brychc´ın et al., 2018; Attia et al., 2018; Wu et al., 2018; Dumitru et al., 2018; Arroyo-Fern´andez et al., 2018; Speer and Lowry-Duda, 2018; Santus et al., 2018; Grishin, 2018; Zhou et al., 2018; Vinayan et al., 2018; Kulmizev et al., 2018; Zhang and Carpuat, 2018; Shiue et al., 2018). While the first category concentrates on models with higher interpretability, none of these models provide explanations. Comparatively, this work focuses on the creation of an explicit word vector space model (EWVM), with an associated explanation, evaluating the performance of different types of lexicosemantic resources in the context of the task of identification of discriminative attributes (IDA). 3 3.1 Identifying and Explaining Discriminative Attributes Problem Definition This paper provides an ex"
D19-5306,C18-1326,1,0.89159,"Missing"
D19-5306,C18-1195,1,0.887438,"Missing"
D19-5306,P19-1333,1,0.880615,"Missing"
D19-5306,P17-1171,0,0.0230149,"(see Baseline Replication in Table 2). This observation is perfectly in line with the literature. (Hashimoto et al., 2016) report improvements on 4 Related Work State-of-the-art approaches for Open-Domain Question Answering over large collections of documents employ a combination of character-level models, self-attention (Wang et al., 2017), and biattention (Seo et al., 2016) to operate over unstructured paragraphs without exploiting any structured text representation. Despite these methods have demonstrated impressive results reaching in some cases super-human performances (Seo et al., 2016; Chen et al., 2017; Yu et al., 2018), recent studies have raised important concerns related to generalisation (Wiese et al., 2017; Dhingra et al., 2017) complex reasoning (Welbl et al., 2018) and explainability (Yang et al., 2018). Specifically, the lack of structured representation makes it hard for current Machine Comprehension models to find meaningful patterns in large corpora, generalise beyond the training domain and justify the answer. Research efforts towards the creation of message-passing architectures with relational inductive bias (Battaglia et al., 2018) have enabled machine learning algorithms to"
D19-5306,D14-1162,0,0.0819748,"Missing"
D19-5306,N19-1240,0,0.0394828,"Missing"
D19-5306,P18-2124,0,0.0706673,"Missing"
D19-5306,C18-1014,0,0.0545115,"Missing"
D19-5306,D18-1546,0,0.0313132,"s in reading comprehension tasks where the context is represented by a single paragraph (Rajpurkar et al., 2018). However, when it comes to answering complex questions on large document collections, it is unlikely that a single passage can provide sufficient evidence to support the answer. Complex QA typically requires multi-hop reasoning, i.e. the ability of combining multiple information fragments from different sources. Moreover, recent studies have raised concerns on inference capabilities, generalisation and interpretability of current MC models (Wiese et al., 2017; Dhingra et al., 2017; Kaushik and Lipton, 2018), leading to the creation of novel datasets that propose multi-hop reading comprehension as a benchmark for evaluating complex reasoning and explainability (Yang et al., 2018). : equal contribution 42 Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13), pages 42–51 c Hong Kong, November 4, 2019. 2019 Association for Computational Linguistics MC performance on supporting facts identification. The experiments show that DGN is able to obtain improvements in F1 score when compared to a MC baseline that adopts a sequential reading strategy."
D19-5306,D18-1455,0,0.0173753,"eady demonstrated remarkable results in a large set of applications ranging from Computer Vision, Physical Systems and Protein-Protein In48 model is based on a state-of-the-art MC model (Seo et al., 2016) that adopts a sequential reading strategy to identifying supporting facts from large collections of documents. teraction (Zhou et al., 2018). Our research is in line with recent trends in Question Answering prone to explore messagepassing architectures over graph-structured representation of documents to enhance performance and overcome challenges involved in dealing with unstructured text. (Sun et al., 2018) fuse text corpus with manually-curated knowledge bases to create heterogeneous graphs of KB facts and text sentences. Their model, GRAFTNet, built upon Graph Convolutional Networks (Schlichtkrull et al., 2018), is used to propagate information between heterogeneous nodes in the graph and perform binary classification on entity nodes to select the answer. Differently from the proposed approach, the latter work focuses on links between whole paragraphs and external entities in a Knowledge Base. Moreover, GRAFTNet is designed for single-hop Question Answering, assuming that the question is alway"
D19-5306,P17-1018,0,0.0700263,"Missing"
D19-5306,Q18-1021,0,0.0254475,"he-art approaches for Open-Domain Question Answering over large collections of documents employ a combination of character-level models, self-attention (Wang et al., 2017), and biattention (Seo et al., 2016) to operate over unstructured paragraphs without exploiting any structured text representation. Despite these methods have demonstrated impressive results reaching in some cases super-human performances (Seo et al., 2016; Chen et al., 2017; Yu et al., 2018), recent studies have raised important concerns related to generalisation (Wiese et al., 2017; Dhingra et al., 2017) complex reasoning (Welbl et al., 2018) and explainability (Yang et al., 2018). Specifically, the lack of structured representation makes it hard for current Machine Comprehension models to find meaningful patterns in large corpora, generalise beyond the training domain and justify the answer. Research efforts towards the creation of message-passing architectures with relational inductive bias (Battaglia et al., 2018) have enabled machine learning algorithms to incorporate graphical structures in their training process. These models, trained over explicit entities and relations, have the potential to boost generalisation, interpret"
D19-5306,K17-1029,0,0.0320475,"Missing"
D19-5306,D18-1259,0,0.0934934,"ent collections, it is unlikely that a single passage can provide sufficient evidence to support the answer. Complex QA typically requires multi-hop reasoning, i.e. the ability of combining multiple information fragments from different sources. Moreover, recent studies have raised concerns on inference capabilities, generalisation and interpretability of current MC models (Wiese et al., 2017; Dhingra et al., 2017; Kaushik and Lipton, 2018), leading to the creation of novel datasets that propose multi-hop reading comprehension as a benchmark for evaluating complex reasoning and explainability (Yang et al., 2018). : equal contribution 42 Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13), pages 42–51 c Hong Kong, November 4, 2019. 2019 Association for Computational Linguistics MC performance on supporting facts identification. The experiments show that DGN is able to obtain improvements in F1 score when compared to a MC baseline that adopts a sequential reading strategy. The obtained results confirm the value of pursuing research towards the definition of novel MC architectures, which are able to incorporate structure as an integral part of th"
D19-5322,C18-1195,1,0.914497,"nguistics Figure 1: Architecture and Workflow overview: the overall architecture (blue) supports the implementation of the motivational scenario (green). cial reports, blog articles and recent news only exist in the form of unstructured text. The engineer also anticipates the benefits of integrating structured Knowledge Graphs such as DBpedia (Auer et al., 2007), integrating KGs to the textual data sources. Realizing the importance of his application to be able to generate traceable and explainable answers, he decides to use an explicit internal representation, such as the graph-based RDF-NL (Cetto et al., 2018). With the associated chain of classifiers and extractors available at DBee, he performs openinformation extraction (OIE), Coreference Resolution (CR), Entity Linking (EL) and Rhetorical Structure Classification (RSC) to obtain the graph from a chosen set of documents. After the extraction, the graph is indexed to ensure efficiency for different types of queries over the graph representation. In order to support semantic approximation during the queries, he associates two pre-trained word embedding models to the KG using the DBee API and uses the provided set of query primitives to query the k"
D19-5322,P14-5010,0,0.00319745,"extracted from the unstructured text, in the form of a set of inter-linked subject-predicateobject triples, thus yielding a graph. With coreference resolution, the graph is further enriched semantically, linking nodes that refer to the same 6 6.1 Implementation DBee was conceptualised and implemented as an extensible Python library. We use the HOCON2 format to enable for easy generator chain definition and persistence. We provide a pre-defined chain featuring the contextualised open information extraction tool Graphene (Cetto et al., 2018), the Stanford CoreNLP coreference resolution system (Manning et al., 2014) and the entity linker Spotlight (Mendes et al., 2011) that links recognised entities to DBpedia resources. Furthermore, we use ElasticSearch3 as the full-text search engine and Annoy4 to index the embeddings for the kNN queries. Note, that following the design goals of extensibility and scalability, the software is not constrained to use those specific tools. Even the choice of generators and storage types is not fixed, as it requires low effort to add a new generator or storage type, such as a relational database to perform joins more efficiently, for instance. Table 1: Library of pre-define"
D19-5322,P19-1333,1,0.85769,"Missing"
D19-5322,D16-1264,0,0.0187938,"e the infrastructure to extract, represent and query structured and unstructured data (with an emphasis on KGs from text and associated embeddings). Early efforts in a similar direction include (Sales et al., 2018), that present a uniform service-based API for storing, querying and comparing word embeddings, pre-computed with varying models and on different datasets. Another information management tool for unstructured data is Apache UIMA 1. Contemporary machine comprehension systems based on neural architectures have targeted evaluation settings which have limited document scale (e.g. SQUAD (Rajpurkar et al., 2016)). Different works explored the connection be1 178 http://uima.apache.org 5. Scalability: Operating over large-scale data sources, large knowledge graphs and embeddings require principled query processing strategies. DBee inherits indexing strategies from databases and kNN embedding queries in order to support scaling to large datasets, memory footprints and storage space requirements. tween distributional semantics and structured Knowledge Graph representations in the context of semantic parsing over large-scale RDF graphs (Freitas and Curry, 2014; Freitas, 2015; Sales et al., 2016) and appro"
D19-5322,S16-2025,1,0.845335,"QUAD (Rajpurkar et al., 2016)). Different works explored the connection be1 178 http://uima.apache.org 5. Scalability: Operating over large-scale data sources, large knowledge graphs and embeddings require principled query processing strategies. DBee inherits indexing strategies from databases and kNN embedding queries in order to support scaling to large datasets, memory footprints and storage space requirements. tween distributional semantics and structured Knowledge Graph representations in the context of semantic parsing over large-scale RDF graphs (Freitas and Curry, 2014; Freitas, 2015; Sales et al., 2016) and approximative abductive reasoning over commonsense KBs (Freitas et al., 2014, 2013). Comparatively, DBee focuses on explicit semantic representation models (Knowledge Graphs) extracted from text. 4 Proposed Framework To satisfy the emerging need to work with unstructured text representations, as depicted in the introductory part of this work, we propose a framework that supports the extraction and management of both explicit and latent text representation models and facilitates the integration with downstream machine learning based models. DBee was designed to deliver the following featur"
D19-5322,L18-1211,1,0.855366,"osing their user to the definition of neural architectures, abstracting away the computation details of automatic differentiation - or trying to learn even those (Jin et al., 2018) - with TensorFlow (Abadi et al., 2016) being the most complete suite providing assistance from data streaming, over training to model serving. Our approach can be seen as complementary to these efforts since we aim to provide the infrastructure to extract, represent and query structured and unstructured data (with an emphasis on KGs from text and associated embeddings). Early efforts in a similar direction include (Sales et al., 2018), that present a uniform service-based API for storing, querying and comparing word embeddings, pre-computed with varying models and on different datasets. Another information management tool for unstructured data is Apache UIMA 1. Contemporary machine comprehension systems based on neural architectures have targeted evaluation settings which have limited document scale (e.g. SQUAD (Rajpurkar et al., 2016)). Different works explored the connection be1 178 http://uima.apache.org 5. Scalability: Operating over large-scale data sources, large knowledge graphs and embeddings require principled que"
D19-5322,P17-1088,0,0.0303595,"Knowledge Graphs and Embeddings Viktor Schlegel, Andr´e Freitas Department of Computer Science University of Manchester {viktor.schlegel,andre.freitas}@manchester.ac.uk Abstract latent models. Word embeddings (Mikolov et al., 2013) and Knowledge Graphs (lexico-semantic graphs) (Bollacker et al., 2008) are becoming the de-facto representation models within different AI tasks. Moreover, they have complementary properties, where word embeddings provide more coarsegrained semantics which are complemented by the fine-grained semantics of KGs being commonly used in coordination (Silva et al., 2018; Xie et al., 2017). This paper describes DBee, a database for creating, querying and consuming embeddings and knowledge graphs. DBee aims to be a database designed for satisfying recurring demands from AI applications. DBee provides a seamless layer to jointly query knowledge graphs and embeddings, simultaneously exploiting the semantic properties of both resources, taking into account performance and scalability aspects. At the centre of the proposed database is the goal of bridging the gap between data, representation, learning and inference algorithms, where classifiers and extractors directly interface with"
L16-1330,S15-1002,0,0.0120685,"nsor Network(Socher et al., 2013b). This Recursive NN uses layers of a tensor V to compose each dimension of the parent vector. 3.3. 5 C C X X 1 W( wt−i + wt+i ) 2C i=1 i=1 Recurrent Neural Network Recurrent NNs (Figure 3), just like the Recursive NN, can handle variable sized inputs. The idea behind this kind of 2082 network is to keep some kind of memory of previous results as features for the next input. And just like the Recursive NN, this NN is a good way to capture semantic aspects of sentences. This kind of network has great results with language models(Mikolov, 2012), compositionality(Le and Zuidema, 2015) and more. The simplest case for Recurrent NN is when the recurrence function is: ht = σ(Wh ht−1 + Wx xt + b) where Wh and Wx are weight matrices, ht−1 is the result of the previous recurrence and xt is the current input. Other more complicated recurrence functions exists, the Long Short Term Memory(Hochreiter and Schmidhuber, 1997) being a particularly popular choice. 4. NNBlocks Two things that all of models presented above have in common are: (a) they have some very different aspects from common Multilayer Perceptrons and (b) they all have a lot of architectural variations depending on the"
L16-1330,D12-1110,0,0.0177716,"ing the tree structure. This kind of NN has great results in compositionality tasks. The simplest Recursive NN is the one where a parent vector p is   c p = σ(W 1 + b) (1) c2 where c1 and c2 are child vectors, W is a weight matrix, b is a bias vector and σ is an activation function, e.g. the sigmoid function. The result of this recursive process, the last parent vector, can then be used as a semantic feature of the sentence. These kind of compositional vectors have great results in parsing(Socher et al., 2013a), paraphrase detection(Socher et al., 2011), entities relationship classification(Socher et al., 2012), sentiment detection(Socher et al., 2013b) and more. Normally, the simple composition function in (1) is not enough to fully capture semantic relationships between words or phrases. One Recursive NN that tries to overcome this is the Recursive Neural Tensor Network(Socher et al., 2013b). This Recursive NN uses layers of a tensor V to compose each dimension of the parent vector. 3.3. 5 C C X X 1 W( wt−i + wt+i ) 2C i=1 i=1 Recurrent Neural Network Recurrent NNs (Figure 3), just like the Recursive NN, can handle variable sized inputs. The idea behind this kind of 2082 network is to keep some ki"
L16-1330,P13-1045,0,0.0134537,"ees of sentences and word vectors as leafs. The model recursively applies a composition function following the tree structure. This kind of NN has great results in compositionality tasks. The simplest Recursive NN is the one where a parent vector p is   c p = σ(W 1 + b) (1) c2 where c1 and c2 are child vectors, W is a weight matrix, b is a bias vector and σ is an activation function, e.g. the sigmoid function. The result of this recursive process, the last parent vector, can then be used as a semantic feature of the sentence. These kind of compositional vectors have great results in parsing(Socher et al., 2013a), paraphrase detection(Socher et al., 2011), entities relationship classification(Socher et al., 2012), sentiment detection(Socher et al., 2013b) and more. Normally, the simple composition function in (1) is not enough to fully capture semantic relationships between words or phrases. One Recursive NN that tries to overcome this is the Recursive Neural Tensor Network(Socher et al., 2013b). This Recursive NN uses layers of a tensor V to compose each dimension of the parent vector. 3.3. 5 C C X X 1 W( wt−i + wt+i ) 2C i=1 i=1 Recurrent Neural Network Recurrent NNs (Figure 3), just like the Recu"
L16-1330,D13-1170,0,0.00537085,"ees of sentences and word vectors as leafs. The model recursively applies a composition function following the tree structure. This kind of NN has great results in compositionality tasks. The simplest Recursive NN is the one where a parent vector p is   c p = σ(W 1 + b) (1) c2 where c1 and c2 are child vectors, W is a weight matrix, b is a bias vector and σ is an activation function, e.g. the sigmoid function. The result of this recursive process, the last parent vector, can then be used as a semantic feature of the sentence. These kind of compositional vectors have great results in parsing(Socher et al., 2013a), paraphrase detection(Socher et al., 2011), entities relationship classification(Socher et al., 2012), sentiment detection(Socher et al., 2013b) and more. Normally, the simple composition function in (1) is not enough to fully capture semantic relationships between words or phrases. One Recursive NN that tries to overcome this is the Recursive Neural Tensor Network(Socher et al., 2013b). This Recursive NN uses layers of a tensor V to compose each dimension of the parent vector. 3.3. 5 C C X X 1 W( wt−i + wt+i ) 2C i=1 i=1 Recurrent Neural Network Recurrent NNs (Figure 3), just like the Recu"
L18-1211,L18-1618,1,0.876437,"Missing"
L18-1211,P13-4006,0,0.0148024,"tive-based models. D EEP L EARNING 4J is also written in JAVA and its API contains methods to access word vectors and to find nearest neighbours (kNN). G EN S IM is one of the most popular word-embedding toolkits, mainly credited to its efficient implementation of nearˇ uˇrek and Sojka, 2010). G EN est neighbours function (Reh˚ S IM is written in P YTHON and apart from its kNN function, it supports the generation of predictive-based models and methods to access word vectors. Following a different motivation, DISSECT (DIStributional SEmantics Composition Tookit) focuses on vector compositions (Dinu et al., 2013). DISSECT is a P YTHON library containing methods to generate vector representation of sentences from the vector of its constituting words. DISSECT partially supports the generation of count-based models and brings an integrated baseline framework for evaluation purposes. J O B IM T EXT is a semantic similarity tool that implements its own algorithm named JoBim (Biemann et al., 2013). The tool supports the construction of the JoBim model and 1326 1 https://deeplearning4j.org/ Features I NDRA GenSim DeepLearning4J Word Embeddings S-Space JoBim DISSECT C JoBim C* Simple vectors Composed vectors"
L18-1211,P10-4006,0,0.2112,"he experimentation and customisation of multilingual WEMs, allowing end-users and applications to consume and operate over multiple word embedding spaces as a service or library. I NDRA is available from two repositories (github.com/ Lambda-3/Indra and github.com/Lambda-3/ IndraIndexer) both licensed as open-source software. Additionally, I NDRA also provides a Python client (pyindra) available via pip and from github.com/ Lambda-3/pyindra. 2. Related Work S-S PACE is a library to support the construction of countbased distributional methods unifying different approaches in a common JAVA API (Jurgens and Stevens, 2010). D EEP L EARNING 4J1 , on the other hand, is a library which concentrates predictive-based models. D EEP L EARNING 4J is also written in JAVA and its API contains methods to access word vectors and to find nearest neighbours (kNN). G EN S IM is one of the most popular word-embedding toolkits, mainly credited to its efficient implementation of nearˇ uˇrek and Sojka, 2010). G EN est neighbours function (Reh˚ S IM is written in P YTHON and apart from its kNN function, it supports the generation of predictive-based models and methods to access word vectors. Following a different motivation, DISSE"
L18-1211,Q14-1041,0,0.185682,"shares more than 65 pre-computed models in 14 languages. Keywords: word embedding server, semantic relatedness server, semantic toolkit, corpus pre-processor 1. Introduction Word embedding is a popular semantic model which represents words and sentences in computational linguistics systems and machine learning models. In recent years a large set of algorithms for both generating and consuming word embedding models (WEMs) have been proposed, which includes corpus pre-processing strategies, WEM algorithms or weighting schemes, vector compositions and distance measures (Turney and Pantel, 2010; Lapesa and Evert, 2014; Mitchell and Lapata, 2010). Determining the optimal set of strategies for a given problem demands the support of a tool that facilitates the exploration of the configuration space of parameters. Furthermore, given the applicability and maturity achieved by these systems and models, they have been promoted from academic prototypes to industry-level applications (Loebbecke and Picot, 2015; Hengstler et al., 2016; Moro et al., 2015). In this new production scenario, a candidate tool should be able to scale to large number of requests and to the construction of models from large corpora, making"
L18-1211,D14-1162,0,0.0968573,"Missing"
L18-1211,S16-2025,1,0.465171,"3.1.4. Vector Server As a primary use, I NDRA acts as a central repository of WEMs, serving vectors for terms in different languages and models. The set of pre-processed models allows the user to experiment different WEMs configurations as a one-stopshop fashion. I NDRA can act as a central server in an enterprise context, or as a local library in more constrained environments. 3.1.5. Semantic Relatedness Natural language understanding systems use semantic relatedness in fine-grained tasks such as word disambiguation (Freitas et al., 2013) or more coarse-grained such as paraphrase detection (Sales et al., 2016; Silva et al., 2018), semantic parsing (Sales et al., 2018) and question answering (Freitas, 2015). I NDRA implements two semantic relatedness methods. The first is the pair-wise semantic relatedness in which the user provides pairs of terms to calculate their semantic relatedness. The other option is integrated to the nearest neighbours function which returns the relatedness of the k closest terms. Additionally Indra can support the application of various distance or correlation measures (Lapesa and Evert, 2014). Currently I NDRA supports more than ten different distance and correlation func"
L18-1398,S16-2025,1,0.678647,"ation that sums up the vector’s components using the resulting vector to calculate the cosine similarity. We developed the experiments with the support of Indra (a distributional semantics tool) (Sales et al., 2018b; Freitas et al., 2016). The evaluation is applied in two scenarios. The first considers the Top-10 results of each execution and the second considers the Top-20. This assumption makes precision a redundant indicator since it can be derived from recall. So, the analysis measures recall and mean reciprocal rank (MRR). This methodology of evaluation follows the same strategy used in (Sales et al., 2016) and (Sales et al., 2018a). Table 4 shows the recall and MRR considering only the highly relevant categories. This is the preferable experimental setting, since it evaluates the ability to identify the paraphrases proposed by the volunteers, ignoring any other potential relevant result. 2507 Query Set Size (# of Queries) Existing Query Sets INEX-XER (Demartini et al., 2010) TREC Entity (Balog et al., 2009) SemSearch ES (Blanco et al., 2011; Halpin et al., 2010) SemSearch LS (Blanco et al., 2011) QALD-2 (Lopez et al., 2013) INEX-LD (Wang et al., 2012) Proposed Query Sets Entity Categories Engli"
L18-1398,L18-1211,1,0.820018,"The baselines applied the Skip-gram (Mikolov et al., 2013) vector space model generated from the Wikipedia 2014 corpora. We pre-processed the corpora lower-casing and stemming each token using the Porter Algorithm (Porter, 1997) and generated the distributional model using the default parameters. Our baseline is the sum-algebraic-based method, where entity categories are compared by an algebraic operation that sums up the vector’s components using the resulting vector to calculate the cosine similarity. We developed the experiments with the support of Indra (a distributional semantics tool) (Sales et al., 2018b; Freitas et al., 2016). The evaluation is applied in two scenarios. The first considers the Top-10 results of each execution and the second considers the Top-20. This assumption makes precision a redundant indicator since it can be derived from recall. So, the analysis measures recall and mean reciprocal rank (MRR). This methodology of evaluation follows the same strategy used in (Sales et al., 2016) and (Sales et al., 2018a). Table 4 shows the recall and MRR considering only the highly relevant categories. This is the preferable experimental setting, since it evaluates the ability to identi"
L18-1423,J08-4004,0,0.0656643,"Missing"
L18-1423,J93-2004,0,0.0612691,"SA and finance. Section 3. details the data collection method put in place to ensure representative samples. Section 4. describes the annotation process and quality analysis results are presented in Section 5.. Finally, we conclude in Section 6.. 2. Related work A large body of literature exists in the domain of Sentiment Analysis. Since our problem was to identify multilingual 2671 financial resources for use in supervised learning methods, we narrow our review to annotated corpora of the financial domain. Work on financial corpora has a long history, starting with the Penn Treebank corpus (Marcus et al., 1993) which includes Wall Street Journal articles in American English. More recent work includes corpora targeting various types of texts and different languages. A number of relevant financial corpora exists in English. (O’Hare et al., 2009) analysed financial blogs. To do so, they developed a corpus of financial blogs. The annotation scheme targets 500 specific companies and is applied at document and paragraph level, not sentence level. Annotators successively used a 3 and a 2 point polarity scale of sentiment. An annotation tool was used by seven trained raters who annotated 979 documents. Inte"
L18-1423,momtazi-2012-fine,0,0.032452,"r agreement results are given. The Emotiblog corpus (Boldrini et al., 2009) includes a 30,000 word Spanish component. It includes message blogs that focus on topical news such as the Kyoto protocol but not on financial topics. The fine-grained annotation scheme includes a mix of syntactic and semantic features. Average interannotator agreement across all categories is 0.68. In German, there are two projects of interest. Scholtz et al (Scholz et al., 2012) published a corpus of news-related texts. It includes 15,089 sentences and includes 3-level polarity scale (Cohen’s Kappa = 0.88). Momtazi (Momtazi, 2012) published another German corpus. It focuses on blog messages and includes 500 short texts from social media. Messages are annotated at document level with four polarity scores. Fleiss’ Kappa scores range from 0.5 to 0.65. This short review indicates that microblog annotated corpora focused on the financial sector are scarce. In addition, very few of these resources provide polarity information at entity level. The SSIX GS corpus addresses this lack by providing polarity annotation, on a continuous scale, for financial entities mentioned in microblogs. 3. Data collection The gathering process"
L18-1423,takala-etal-2014-gold,0,0.0587227,"Missing"
L18-1542,Q15-1038,0,0.0309883,"y definitions, but in the adopted conceptual model there are only three types of edges, numbered from 0 to 2: the 0-edge represents unary predicates and the 1 and 2-edges connects binary predicates to their arguments. In common, most approaches work at the word-level, converting each single word in the definition into a different attribute or node. In the graph knowledge base scenario, this can increase the information retrieval complexity, given that it may be necessary to concatenate the contents of several nodes to obtain meaningful enough information about an entity. The work proposed by (Bovi et al., 2015) go beyond the word-level representation, being able to identify multiword expressions. They perform a syntactic-semantic analysis of textual definitions for Open Information Extraction 3438 Role Supertype (OIE). Although they generate a syntactic-semantic graph representation of the definitions, the resulting graphs are used only as an intermediary resource for the final goal of extracting semantic relations between the entities present in the definition. 3. Differentia quality Differentia event Graph Conceptual Model To build the definition graph, we adopted the conceptual model proposed by"
L18-1542,W91-0217,0,0.627863,"phrase which contains a piece of self-contained information about the definiendum. Following this methodology, we processed the whole noun and verb databases of WordNet (Fellbaum, 1998) and built the WordNetGraph, and then used this knowledge graph to recognize text entailments in an interpretable way, providing concise justifications for the entailment decisions. Related Work The construction of structured databases from dictionary definitions has been largely explored, and most approaches rely on syntactic parsers for the identification of patterns that point to relationships between words (Calzolari, 1991; Vossen, 1991; Vossen, 1992; Vossen and Copestake, 1994). Among early efforts, it is remarkable the creation of LKB, a Lexical Knowledge Base (Copestake, 1991) based on typed-feature structures that can be seen as a set of attributes for a given concept, such as “origin”, “color”, “smell”, “taste” and “temperature” for the concept drink, for example. The definitions from a machine-readable dictionary are parsed to extract the definiendum’s genus and differentiae, and the values represented by the differentiae will fill in the feature structures for that genus. Since the features, that is, the"
L18-1542,P14-5010,0,0.00525702,"Missing"
L18-1542,J08-2001,0,0.0849046,"Missing"
L18-1542,L16-1417,0,0.0301737,"e dictionary are parsed to extract the definiendum’s genus and differentiae, and the values represented by the differentiae will fill in the feature structures for that genus. Since the features, that is, the relevant attributes for a given entity, must be defined in advance, only a restricted domain was considered in their approach. Dolan et al. (1993) also describe an automated strategy to build a structured lexical knowledge base but, instead of the entity-attributes structure, they use syntactic parsing to identify semantic relations such as is-a, part-of, etc., to build a directed graph. Recski (2016) also derives a graph representation from dictionary definitions, but in the adopted conceptual model there are only three types of edges, numbered from 0 to 2: the 0-edge represents unary predicates and the 1 and 2-edges connects binary predicates to their arguments. In common, most approaches work at the word-level, converting each single word in the definition into a different attribute or node. In the graph knowledge base scenario, this can increase the information retrieval complexity, given that it may be necessary to concatenate the contents of several nodes to obtain meaningful enough"
L18-1542,W16-5323,1,0.812489,"go beyond the word-level representation, being able to identify multiword expressions. They perform a syntactic-semantic analysis of textual definitions for Open Information Extraction 3438 Role Supertype (OIE). Although they generate a syntactic-semantic graph representation of the definitions, the resulting graphs are used only as an intermediary resource for the final goal of extracting semantic relations between the entities present in the definition. 3. Differentia quality Differentia event Graph Conceptual Model To build the definition graph, we adopted the conceptual model proposed by Silva et al. (2016). This model extends the genus-differentia definition pattern from Aristotle’s classic theory of definition (Berg, 1982; Lloyd, 1962; Granger, 1984) by defining a set of entity-centered semantic roles for lexical definitions. Differently from the commonly used event-centered semantic roles, which define the semantic relations holding among a predicate (the main verb in a clause) and its associated participants and properties (M`arquez et al., 2008), definition’s semantic roles express the part played by an expression in a definition, showing how it relates to the definiendum, that is, the enti"
L18-1618,N09-1003,0,0.103459,"Missing"
L18-1618,S17-2002,0,0.0429491,"Simlex-999 1.15 WS-353 8.0 Table 1: Semantic Similarity vs Semantic Relatedness • Creating semantic similarity and relatedness models which work on languages not having a high-volume supporting corpora. This paper is organized as follows: Section 2. describes the state-of-the-art in existing gold-standards for semantic similarity and relatedness computations as well as their language variants; Section 3. describes the English goldstandards which were used as a reference for the machine translation process; Section 4. describes the SemR-11 goldstandard and its creation process. 2. Related Work Camacho-Collados et al. (2017) developed a multi-lingual gold-standard which includes 518 word pairs for five languages (English, German, Italian, Spanish and Persian). It is composed of nominal pairs of multi-word expressions, domain-specific terms and named entities that are manually scored between 0 to 4 where 0 indicates that they are completely dissimilar and 4 denotes that the two words are synonymous. This dataset (Camacho-Collados et al., 2017) focuses on semantic similarity. Bruni et al. (2014) introduced a test collection containing 3000 word pairs. The MEN dataset obtained by crowdsourcing using Amazon Mechanica"
L18-1618,P14-5004,0,0.0206759,"(Finkelstein et al., 2001), evolving from capturing semantic similarity to semantic relatedness behavior. More recently, the creation of neural/predictive word embedding models (Mikolov et al., 2013; Pennington et al., 2014) pushed semantic similarity and relatedness gold-standards to evolve in the direction of quantifying more fine-grained semantic relations (Hill et al., 2015). Currently, most of the existing gold-standards for evaluating semantic similarity and relatedness have focused on the English language, with some initiatives providing initial gold-standards for few other languages (Faruqui and Dyer, 2014). This paper describes SemR-11, a multi-lingual gold-standard which aims at generalizing existing semantic similarity and relatedness gold-standards to 11 languages (German, French, Russian, Italian, Dutch, Chinese, Portuguese, Swedish, Spanish, Arabic and Persian). The resource is built using a principled translation method over four reference gold-standards: Miller & Charles (Miller and Charles, 1991), Rubenstein & Goodenough (Rubenstein and Goodenough, 1965), WS-353 (Finkelstein et al., 2001) and Simlex-999 (Leviant and Reichart, 2015). The final resource contains in total 15,917 word pairs"
L18-1618,J15-4004,0,0.0446801,"in most cases, a vector space representation of meaning. As distributional semantic models can induce modes with a more comprehensive underlying vocabulary and also capture a broader set of semantic relations, new gold-standards emerged (Finkelstein et al., 2001), evolving from capturing semantic similarity to semantic relatedness behavior. More recently, the creation of neural/predictive word embedding models (Mikolov et al., 2013; Pennington et al., 2014) pushed semantic similarity and relatedness gold-standards to evolve in the direction of quantifying more fine-grained semantic relations (Hill et al., 2015). Currently, most of the existing gold-standards for evaluating semantic similarity and relatedness have focused on the English language, with some initiatives providing initial gold-standards for few other languages (Faruqui and Dyer, 2014). This paper describes SemR-11, a multi-lingual gold-standard which aims at generalizing existing semantic similarity and relatedness gold-standards to 11 languages (German, French, Russian, Italian, Dutch, Chinese, Portuguese, Swedish, Spanish, Arabic and Persian). The resource is built using a principled translation method over four reference gold-standar"
L18-1618,D14-1162,0,0.0799168,"Missing"
L18-1618,S16-2025,1,0.846565,"butional Semantic Models (DSM) are consolidating themselves as fundamental components for supporting automatic semantic interpretation in different application scenarios in natural language processing. From question answering systems, to semantic search and text entailment, distributional semantic models support a scalable approach for representing the meaning of words, which can automatically capture comprehensive associative commonsense information by analysing word-context patterns in largescale corpora in an unsupervised or semi-supervised fashion (Freitas, 2015; Turney and Pantel, 2010b; Sales et al., 2016). The SemR-11 test collection was used by Freitas et al.(2016), Sales et al.(2018) and Barzegar et al.(2018) to evaluate how different distributional semantic models built from corpora in different languages and with different sizes, perform in computing semantic relatedness similarity and relatedness tasks. Additionally, SemR-11 was used to analyze the role of machine translation approaches to support the construction of high-quality distributional vectors and computing semantic similarity & relatedness measures for other languages. 6. Acknowledgments This publication has emanated from resear"
L18-1618,L18-1211,1,0.876105,"Missing"
L18-1618,P94-1019,0,\N,Missing
P19-1333,D18-1080,0,0.518084,"es an even smaller set of 11 hand-written rules to perform sentence splitting and deletion of irrelevant sentences or sentence parts with an unsupervised lexical simplifier based on word embeddings (Glavaˇs and ˇ Stajner, 2015). plex sentence to a sequence of structurally simplified sentences. Aharoni and Goldberg (2018) further explore this idea, augmenting the presented neural models with a copy mechanism. Though outperforming the models used in Narayan et al. (2017), they still perform poorly compared to previous state-of-the-art rule-based syntactic simplification approaches. In addition, Botha et al. (2018) observed that the sentences from the WebSplit corpus contain fairly unnatural linguistic expressions using only a small vocabulary. To overcome this limitation, they present a scalable, languageagnostic method for mining training data from Wikipedia edit histories, providing a rich and varied vocabulary over naturally expressed sentences and their extracted splits. When training the best-performing model of Aharoni and Goldberg (2018) on this new split-and-rephrase dataset, they achieve a strong improvement over prior best results from Aharoni and Goldberg (2018). However, due to the uniform"
P19-1333,C18-1195,1,0.765887,"Missing"
P19-1333,C96-2183,0,0.754909,"013) or deaf persons (Inui et al., 2003). However, not only human readers may benefit from TS. Previous work has established that applying TS as a preprocessing step can improve the performance of a variety of natural language processing (NLP) tasks, such as Open IE (Saha and Mausam, 2018; Cetto ˇ et al., 2018), MT (Stajner and Popovic, 2016, 2018), Relation Extraction (Miwa et al., 2010), Semantic Role Labeling (Vickrey and Koller, 2008), Text Summarization (Siddharthan et al., 2004; Bouayad-Agha et al., 2009), Question Generation (Heilman and Smith, 2010; Bernhard et al., 2012), or Parsing (Chandrasekar et al., 1996; Jonnalagadda et al., 2009). Linguistic complexity stems from the use of either a difficult vocabulary or sentence structure. Therefore, TS is classified into two categories: lexical simplification and syntactic simplification. Through substituting a difficult word or phrase with a more comprehensible synonym, the former primarily addresses a human audience. Most NLP systems, on the contrary, derive greater benefit from syntactic simplification, which focuses on identifying grammatical complexities in a sentence and converting these structures into simpler ones, using a set of text-to-text re"
P19-1333,P11-2117,0,0.0487919,"-the-art syntactic simplification approaches are rule-based (Siddharthan and Mandya, 2014; Ferr´es et al., 2016; Saggion et al., 2015), providing more grammatical output and covering a wider range of syn3415 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3415–3427 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics tactic transformation operations, however, at the cost of being very conservative, often to the extent of not making any changes at all. Acknowledging that existing TS corpora (Zhu et al., 2010; Coster and Kauchak, 2011; Xu et al., 2015) are inappropriate for learning to decompose sentences into shorter, syntactically simplified components, as they contain only a small number of split examples, Narayan et al. (2017) lately compiled the first TS dataset that explicitly addresses the task of sentence splitting. Using this corpus, several encoderdecoder models (Bahdanau et al., 2014) are proposed for breaking down a complex source into a set of sentences with a simplified structure. Aharoni and Goldberg (2018) further explore this idea, augmenting the presented neural models with a copy mechanism (Gu et al., 20"
P19-1333,D11-1142,0,0.185528,"Missing"
P19-1333,P15-2011,0,0.0607083,"Missing"
P19-1333,P16-1154,0,0.0291369,"Kauchak, 2011; Xu et al., 2015) are inappropriate for learning to decompose sentences into shorter, syntactically simplified components, as they contain only a small number of split examples, Narayan et al. (2017) lately compiled the first TS dataset that explicitly addresses the task of sentence splitting. Using this corpus, several encoderdecoder models (Bahdanau et al., 2014) are proposed for breaking down a complex source into a set of sentences with a simplified structure. Aharoni and Goldberg (2018) further explore this idea, augmenting the presented neural models with a copy mechanism (Gu et al., 2016; See et al., 2017). conservatism exhibited by state-of-the-art syntactic TS approaches, i.e. their tendency to retain the input sentence rather than transforming it. For this purpose, we decompose each source sentence into minimal semantic units and turn them into self-contained propositions. In that way, we provide a fine-grained output that is easy to process for subsequently applied NLP tools. Another major drawback of the structural TS approaches described so far is that they do not preserve the semantic links between the individual split components, resulting in a set of incoherent utter"
P19-1333,W03-1602,0,0.462995,"Missing"
P19-1333,N09-2045,0,0.0414786,"et al., 2003). However, not only human readers may benefit from TS. Previous work has established that applying TS as a preprocessing step can improve the performance of a variety of natural language processing (NLP) tasks, such as Open IE (Saha and Mausam, 2018; Cetto ˇ et al., 2018), MT (Stajner and Popovic, 2016, 2018), Relation Extraction (Miwa et al., 2010), Semantic Role Labeling (Vickrey and Koller, 2008), Text Summarization (Siddharthan et al., 2004; Bouayad-Agha et al., 2009), Question Generation (Heilman and Smith, 2010; Bernhard et al., 2012), or Parsing (Chandrasekar et al., 1996; Jonnalagadda et al., 2009). Linguistic complexity stems from the use of either a difficult vocabulary or sentence structure. Therefore, TS is classified into two categories: lexical simplification and syntactic simplification. Through substituting a difficult word or phrase with a more comprehensible synonym, the former primarily addresses a human audience. Most NLP systems, on the contrary, derive greater benefit from syntactic simplification, which focuses on identifying grammatical complexities in a sentence and converting these structures into simpler ones, using a set of text-to-text rewriting operations. Sentence"
P19-1333,levy-andrew-2006-tregex,0,0.0837755,"Missing"
P19-1333,D12-1048,0,0.0472263,"Missing"
P19-1333,C10-1089,0,0.0205075,"o improve the readability of a text, making information easier to comprehend for people with reduced literacy, such as non-native speakers (Paetzold and Specia, 2016), aphasics (Carroll et al., 1998), dyslexics (Rello et al., 2013) or deaf persons (Inui et al., 2003). However, not only human readers may benefit from TS. Previous work has established that applying TS as a preprocessing step can improve the performance of a variety of natural language processing (NLP) tasks, such as Open IE (Saha and Mausam, 2018; Cetto ˇ et al., 2018), MT (Stajner and Popovic, 2016, 2018), Relation Extraction (Miwa et al., 2010), Semantic Role Labeling (Vickrey and Koller, 2008), Text Summarization (Siddharthan et al., 2004; Bouayad-Agha et al., 2009), Question Generation (Heilman and Smith, 2010; Bernhard et al., 2012), or Parsing (Chandrasekar et al., 1996; Jonnalagadda et al., 2009). Linguistic complexity stems from the use of either a difficult vocabulary or sentence structure. Therefore, TS is classified into two categories: lexical simplification and syntactic simplification. Through substituting a difficult word or phrase with a more comprehensible synonym, the former primarily addresses a human audience. Most"
P19-1333,P14-1041,0,0.413907,"ts, resulting in a much more fine-grained output where each proposition represents a minimal semantic unit that is typically composed of a simple subject-predicate-object structure. Though tackling a larger set of linguistic constructs, our framework operates on a much smaller set of only 35 manually defined rules as compared to existing syntax-driven rule-based approaches. Approaches based on Semantic Parsing While the TS approaches described above are based on syntactic information, there are a variety of methods that use semantic structures for sentence splitting. These include the work of Narayan and Gardent (2014) and Narayan and Gardent (2016), who propose a framework that takes semantically-shared elements as the basis for splitting and rephrasing a sentence. It first generates a semantic representation of the input to identify splitting points in the sentence. In a second step, the split components are then rephrased by completing them with missing elements in order to reconstruct grammatically sound sentences. Lately, with DSS, Sulem et al. (2018c) describe another semantic-based structural simplification framework that follows a similar approach. 2.3 Data-driven Approaches More recently, data-driv"
P19-1333,W16-6620,0,0.0603679,"ne-grained output where each proposition represents a minimal semantic unit that is typically composed of a simple subject-predicate-object structure. Though tackling a larger set of linguistic constructs, our framework operates on a much smaller set of only 35 manually defined rules as compared to existing syntax-driven rule-based approaches. Approaches based on Semantic Parsing While the TS approaches described above are based on syntactic information, there are a variety of methods that use semantic structures for sentence splitting. These include the work of Narayan and Gardent (2014) and Narayan and Gardent (2016), who propose a framework that takes semantically-shared elements as the basis for splitting and rephrasing a sentence. It first generates a semantic representation of the input to identify splitting points in the sentence. In a second step, the split components are then rephrased by completing them with missing elements in order to reconstruct grammatically sound sentences. Lately, with DSS, Sulem et al. (2018c) describe another semantic-based structural simplification framework that follows a similar approach. 2.3 Data-driven Approaches More recently, data-driven approaches for the task of s"
P19-1333,D17-1064,0,0.721954,"3415 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3415–3427 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics tactic transformation operations, however, at the cost of being very conservative, often to the extent of not making any changes at all. Acknowledging that existing TS corpora (Zhu et al., 2010; Coster and Kauchak, 2011; Xu et al., 2015) are inappropriate for learning to decompose sentences into shorter, syntactically simplified components, as they contain only a small number of split examples, Narayan et al. (2017) lately compiled the first TS dataset that explicitly addresses the task of sentence splitting. Using this corpus, several encoderdecoder models (Bahdanau et al., 2014) are proposed for breaking down a complex source into a set of sentences with a simplified structure. Aharoni and Goldberg (2018) further explore this idea, augmenting the presented neural models with a copy mechanism (Gu et al., 2016; See et al., 2017). conservatism exhibited by state-of-the-art syntactic TS approaches, i.e. their tendency to retain the input sentence rather than transforming it. For this purpose, we decompose"
P19-1333,P17-2014,0,0.277185,"entence and converting these structures into simpler ones, using a set of text-to-text rewriting operations. Sentence splitting plays a major role here: it divides a sentence into several shorter components, with each of them presenting a simpler and more regular structure that is easier to process for downstream applications. Many different methods for addressing the task of TS have been presented so far. As noted in ˇ Stajner and Glavaˇs (2017), data-driven approaches outperform rule-based systems in the area of lexiˇ cal simplification (Glavaˇs and Stajner, 2015; Paetzold and Specia, 2016; Nisioi et al., 2017; Zhang and Lapata, 2017). In contrast, the state-of-the-art syntactic simplification approaches are rule-based (Siddharthan and Mandya, 2014; Ferr´es et al., 2016; Saggion et al., 2015), providing more grammatical output and covering a wider range of syn3415 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3415–3427 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics tactic transformation operations, however, at the cost of being very conservative, often to the extent of not making any changes at all. Acknow"
P19-1333,W16-4912,0,0.0356045,"matical complexities in a sentence and converting these structures into simpler ones, using a set of text-to-text rewriting operations. Sentence splitting plays a major role here: it divides a sentence into several shorter components, with each of them presenting a simpler and more regular structure that is easier to process for downstream applications. Many different methods for addressing the task of TS have been presented so far. As noted in ˇ Stajner and Glavaˇs (2017), data-driven approaches outperform rule-based systems in the area of lexiˇ cal simplification (Glavaˇs and Stajner, 2015; Paetzold and Specia, 2016; Nisioi et al., 2017; Zhang and Lapata, 2017). In contrast, the state-of-the-art syntactic simplification approaches are rule-based (Siddharthan and Mandya, 2014; Ferr´es et al., 2016; Saggion et al., 2015), providing more grammatical output and covering a wider range of syn3415 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3415–3427 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics tactic transformation operations, however, at the cost of being very conservative, often to the extent of not making any c"
P19-1333,P02-1040,0,0.105417,"Missing"
P19-1333,C18-1194,0,0.0145596,"anguage (NL) text by utilizing a more readily accessible vocabulary and sentence structure. Its goal is to improve the readability of a text, making information easier to comprehend for people with reduced literacy, such as non-native speakers (Paetzold and Specia, 2016), aphasics (Carroll et al., 1998), dyslexics (Rello et al., 2013) or deaf persons (Inui et al., 2003). However, not only human readers may benefit from TS. Previous work has established that applying TS as a preprocessing step can improve the performance of a variety of natural language processing (NLP) tasks, such as Open IE (Saha and Mausam, 2018; Cetto ˇ et al., 2018), MT (Stajner and Popovic, 2016, 2018), Relation Extraction (Miwa et al., 2010), Semantic Role Labeling (Vickrey and Koller, 2008), Text Summarization (Siddharthan et al., 2004; Bouayad-Agha et al., 2009), Question Generation (Heilman and Smith, 2010; Bernhard et al., 2012), or Parsing (Chandrasekar et al., 1996; Jonnalagadda et al., 2009). Linguistic complexity stems from the use of either a difficult vocabulary or sentence structure. Therefore, TS is classified into two categories: lexical simplification and syntactic simplification. Through substituting a difficult wo"
P19-1333,P17-1099,0,0.0161714,"et al., 2015) are inappropriate for learning to decompose sentences into shorter, syntactically simplified components, as they contain only a small number of split examples, Narayan et al. (2017) lately compiled the first TS dataset that explicitly addresses the task of sentence splitting. Using this corpus, several encoderdecoder models (Bahdanau et al., 2014) are proposed for breaking down a complex source into a set of sentences with a simplified structure. Aharoni and Goldberg (2018) further explore this idea, augmenting the presented neural models with a copy mechanism (Gu et al., 2016; See et al., 2017). conservatism exhibited by state-of-the-art syntactic TS approaches, i.e. their tendency to retain the input sentence rather than transforming it. For this purpose, we decompose each source sentence into minimal semantic units and turn them into self-contained propositions. In that way, we provide a fine-grained output that is easy to process for subsequently applied NLP tools. Another major drawback of the structural TS approaches described so far is that they do not preserve the semantic links between the individual split components, resulting in a set of incoherent utterances. Consequently"
P19-1333,E14-1076,0,0.555967,"s a major role here: it divides a sentence into several shorter components, with each of them presenting a simpler and more regular structure that is easier to process for downstream applications. Many different methods for addressing the task of TS have been presented so far. As noted in ˇ Stajner and Glavaˇs (2017), data-driven approaches outperform rule-based systems in the area of lexiˇ cal simplification (Glavaˇs and Stajner, 2015; Paetzold and Specia, 2016; Nisioi et al., 2017; Zhang and Lapata, 2017). In contrast, the state-of-the-art syntactic simplification approaches are rule-based (Siddharthan and Mandya, 2014; Ferr´es et al., 2016; Saggion et al., 2015), providing more grammatical output and covering a wider range of syn3415 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3415–3427 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics tactic transformation operations, however, at the cost of being very conservative, often to the extent of not making any changes at all. Acknowledging that existing TS corpora (Zhu et al., 2010; Coster and Kauchak, 2011; Xu et al., 2015) are inappropriate for learning to decompose se"
P19-1333,C04-1129,0,0.305698,"h reduced literacy, such as non-native speakers (Paetzold and Specia, 2016), aphasics (Carroll et al., 1998), dyslexics (Rello et al., 2013) or deaf persons (Inui et al., 2003). However, not only human readers may benefit from TS. Previous work has established that applying TS as a preprocessing step can improve the performance of a variety of natural language processing (NLP) tasks, such as Open IE (Saha and Mausam, 2018; Cetto ˇ et al., 2018), MT (Stajner and Popovic, 2016, 2018), Relation Extraction (Miwa et al., 2010), Semantic Role Labeling (Vickrey and Koller, 2008), Text Summarization (Siddharthan et al., 2004; Bouayad-Agha et al., 2009), Question Generation (Heilman and Smith, 2010; Bernhard et al., 2012), or Parsing (Chandrasekar et al., 1996; Jonnalagadda et al., 2009). Linguistic complexity stems from the use of either a difficult vocabulary or sentence structure. Therefore, TS is classified into two categories: lexical simplification and syntactic simplification. Through substituting a difficult word or phrase with a more comprehensible synonym, the former primarily addresses a human audience. Most NLP systems, on the contrary, derive greater benefit from syntactic simplification, which focuse"
P19-1333,P13-1045,0,0.14132,"Missing"
P19-1333,W16-3411,0,0.045906,"ssible vocabulary and sentence structure. Its goal is to improve the readability of a text, making information easier to comprehend for people with reduced literacy, such as non-native speakers (Paetzold and Specia, 2016), aphasics (Carroll et al., 1998), dyslexics (Rello et al., 2013) or deaf persons (Inui et al., 2003). However, not only human readers may benefit from TS. Previous work has established that applying TS as a preprocessing step can improve the performance of a variety of natural language processing (NLP) tasks, such as Open IE (Saha and Mausam, 2018; Cetto ˇ et al., 2018), MT (Stajner and Popovic, 2016, 2018), Relation Extraction (Miwa et al., 2010), Semantic Role Labeling (Vickrey and Koller, 2008), Text Summarization (Siddharthan et al., 2004; Bouayad-Agha et al., 2009), Question Generation (Heilman and Smith, 2010; Bernhard et al., 2012), or Parsing (Chandrasekar et al., 1996; Jonnalagadda et al., 2009). Linguistic complexity stems from the use of either a difficult vocabulary or sentence structure. Therefore, TS is classified into two categories: lexical simplification and syntactic simplification. Through substituting a difficult word or phrase with a more comprehensible synonym, the f"
P19-1333,Q15-1021,0,0.166552,"ication approaches are rule-based (Siddharthan and Mandya, 2014; Ferr´es et al., 2016; Saggion et al., 2015), providing more grammatical output and covering a wider range of syn3415 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3415–3427 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics tactic transformation operations, however, at the cost of being very conservative, often to the extent of not making any changes at all. Acknowledging that existing TS corpora (Zhu et al., 2010; Coster and Kauchak, 2011; Xu et al., 2015) are inappropriate for learning to decompose sentences into shorter, syntactically simplified components, as they contain only a small number of split examples, Narayan et al. (2017) lately compiled the first TS dataset that explicitly addresses the task of sentence splitting. Using this corpus, several encoderdecoder models (Bahdanau et al., 2014) are proposed for breaking down a complex source into a set of sentences with a simplified structure. Aharoni and Goldberg (2018) further explore this idea, augmenting the presented neural models with a copy mechanism (Gu et al., 2016; See et al., 20"
P19-1333,Q16-1029,0,0.0281234,"Treasury’s President. borrowing capacity. Figure 3: Final discourse tree of the example sentence. 4 Experimental Setup To compare the performance of our TS approach with state-of-the-art syntactic simplification systems, we evaluated D IS S IM with respect to the sentence splitting task (subtask 1). The evaluation of the rhetorical structures (subtasks 2 and 3) will be subject of future work. Corpora. We conducted experiments on three commonly used simplification corpora from two different domains. The first dataset we used was Wikilarge, which consists of 359 sentences from the PWKP corpus (Xu et al., 2016). Moreover, to demonstrate domain independence, we compared the output generated by our TS approach with that of the various baseline systems on the Newsela corpus (Xu et al., 2015), which is composed of 1077 sentences from newswire articles. In addition, we assessed the performance of our simplification system using the 5000 test sentences from the WikiSplit benchmark (Botha et al., 2018), which was mined from Wikipedia edit histories. Baselines. We compared our D IS S IM approach against several state-of-the-art baseline systems that have a strong focus on syntactic transformations through e"
P19-1333,D17-1062,0,0.0151544,"g these structures into simpler ones, using a set of text-to-text rewriting operations. Sentence splitting plays a major role here: it divides a sentence into several shorter components, with each of them presenting a simpler and more regular structure that is easier to process for downstream applications. Many different methods for addressing the task of TS have been presented so far. As noted in ˇ Stajner and Glavaˇs (2017), data-driven approaches outperform rule-based systems in the area of lexiˇ cal simplification (Glavaˇs and Stajner, 2015; Paetzold and Specia, 2016; Nisioi et al., 2017; Zhang and Lapata, 2017). In contrast, the state-of-the-art syntactic simplification approaches are rule-based (Siddharthan and Mandya, 2014; Ferr´es et al., 2016; Saggion et al., 2015), providing more grammatical output and covering a wider range of syn3415 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3415–3427 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics tactic transformation operations, however, at the cost of being very conservative, often to the extent of not making any changes at all. Acknowledging that existing TS"
P19-1333,W18-7006,0,0.1653,"Missing"
P19-1333,C10-1152,0,0.219344,"rast, the state-of-the-art syntactic simplification approaches are rule-based (Siddharthan and Mandya, 2014; Ferr´es et al., 2016; Saggion et al., 2015), providing more grammatical output and covering a wider range of syn3415 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3415–3427 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics tactic transformation operations, however, at the cost of being very conservative, often to the extent of not making any changes at all. Acknowledging that existing TS corpora (Zhu et al., 2010; Coster and Kauchak, 2011; Xu et al., 2015) are inappropriate for learning to decompose sentences into shorter, syntactically simplified components, as they contain only a small number of split examples, Narayan et al. (2017) lately compiled the first TS dataset that explicitly addresses the task of sentence splitting. Using this corpus, several encoderdecoder models (Bahdanau et al., 2014) are proposed for breaking down a complex source into a set of sentences with a simplified structure. Aharoni and Goldberg (2018) further explore this idea, augmenting the presented neural models with a cop"
P19-1333,D16-1252,0,0.052813,"Missing"
P19-1333,D18-1081,0,0.0611544,"Missing"
P19-1333,N18-1063,0,0.178228,"Missing"
P19-1333,P18-1016,0,0.133998,"are based on syntactic information, there are a variety of methods that use semantic structures for sentence splitting. These include the work of Narayan and Gardent (2014) and Narayan and Gardent (2016), who propose a framework that takes semantically-shared elements as the basis for splitting and rephrasing a sentence. It first generates a semantic representation of the input to identify splitting points in the sentence. In a second step, the split components are then rephrased by completing them with missing elements in order to reconstruct grammatically sound sentences. Lately, with DSS, Sulem et al. (2018c) describe another semantic-based structural simplification framework that follows a similar approach. 2.3 Data-driven Approaches More recently, data-driven approaches for the task of sentence splitting emerged. Narayan et al. (2017) propose a set of sequence-to-sequence models trained on the WebSplit corpus, a dataset of over one million tuples that map a single com3 Recursive Sentence Splitting 1 The source code of our framework is available under https://github.com/Lambda-3/ DiscourseSimplification. 3417 With the help of the transformation patterns that we specified, source sentences that"
P19-1333,P08-1040,0,\N,Missing
P19-1333,P15-1034,0,\N,Missing
S16-2025,P14-1023,0,0.439707,"ar than with Texas. To avoid this kind of misinterpretation, spatial expressions are compared using their names, acronyms, and demonyms. Because of the numerical and ordered nature of temporal references, temporal specialisations are represented as year intervals. By this representation, two expressions of time are compared by computing the interval intersection. We consider them as semantically related if the intersection is not empty. From a finite set of words, it is possible to express unlimited utterances and ideas. This property is credited to the principle of semantic compositionality (Baroni et al., 2014a). Distributional semantics is based on the hypothesis that words co-occurring in similar contexts tend to have similar meaning (Harris, 1954; Turney and Pantel, 2010). Distributional semantics supports the automatic construction of semantic models from large-scale unstructured corpora, using vector space models to represent the meaning of a word. The process to construct distributional models ranges from statistical methods to models based on machine learning (Dumais et al., 1988; Mikolov et al., 2013; Jeffrey Pennington, 2014). Distributional semantics allows measuring the semantic composit"
S16-2025,N09-1003,0,0.0266522,"Missing"
S16-2025,S15-1003,0,0.01884,"Conclusion This work proposes a compositional-distributional model to recognise paraphrases of entity categories. Distributional semantics in combination with the proposed compositional model supports a search strategy with robust semantic approximation capabilities, largely outperforming string and WordNet-based approaches in recall and mean reciprocal rank. The proposed compositional strategy also outperforms the tradional vector-sum method. This work also provides additional evidence to reinforce (i) the suitability of distributional models to cross the semantic gap (Freitas et al., 2012; Aletras and Stevenson, 2015; Agirre et al., 2009; Freitas et al., 2015) and (ii) suggest that prediction methods generate better semantic vectors when compared to count-based approaches. Considering the controversies about the comparisons between predictive-based and count-based distributional models (Baroni et al., 2014b; Lebret and Collobert, 2015; Levy and Goldberg, 2014), this evidence is restricted to the distributional models involved in the experiment and cannot be generalised. In the context of our work, we conjecture that the better performance is credited to the fact that our problem comprises much more paradi"
S16-2025,W15-0133,1,0.834181,"tributional model to recognise paraphrases of entity categories. Distributional semantics in combination with the proposed compositional model supports a search strategy with robust semantic approximation capabilities, largely outperforming string and WordNet-based approaches in recall and mean reciprocal rank. The proposed compositional strategy also outperforms the tradional vector-sum method. This work also provides additional evidence to reinforce (i) the suitability of distributional models to cross the semantic gap (Freitas et al., 2012; Aletras and Stevenson, 2015; Agirre et al., 2009; Freitas et al., 2015) and (ii) suggest that prediction methods generate better semantic vectors when compared to count-based approaches. Considering the controversies about the comparisons between predictive-based and count-based distributional models (Baroni et al., 2014b; Lebret and Collobert, 2015; Levy and Goldberg, 2014), this evidence is restricted to the distributional models involved in the experiment and cannot be generalised. In the context of our work, we conjecture that the better performance is credited to the fact that our problem comprises much more paradig206 matic than syntagmatic relations. Addit"
S16-2025,2014.lilt-9.5,0,0.0214974,"ar than with Texas. To avoid this kind of misinterpretation, spatial expressions are compared using their names, acronyms, and demonyms. Because of the numerical and ordered nature of temporal references, temporal specialisations are represented as year intervals. By this representation, two expressions of time are compared by computing the interval intersection. We consider them as semantically related if the intersection is not empty. From a finite set of words, it is possible to express unlimited utterances and ideas. This property is credited to the principle of semantic compositionality (Baroni et al., 2014a). Distributional semantics is based on the hypothesis that words co-occurring in similar contexts tend to have similar meaning (Harris, 1954; Turney and Pantel, 2010). Distributional semantics supports the automatic construction of semantic models from large-scale unstructured corpora, using vector space models to represent the meaning of a word. The process to construct distributional models ranges from statistical methods to models based on machine learning (Dumais et al., 1988; Mikolov et al., 2013; Jeffrey Pennington, 2014). Distributional semantics allows measuring the semantic composit"
S16-2025,P10-4006,0,0.0524452,"rey Pennington, 2014): GloVe aims to conciliate the statistical cooccurrence knowledge present in the whole corpus with the local pattern analysis (proposed by the skip-gram model) applying a hybrid approach of conditional probability and machine learning techniques. DINFRA (Barzegar et al., 2015), a SaaS distributional infrastructure, provided the distributional vectors. We generated all five ditributional models using the English Wikipedia 2014 dump as a reference corpus, stemming by the Porter algorithm (Porter, 1997) and removing stopwords. For LSA, RI and ESA, we used the SSpace Package (Jurgens and Stevens, 2010), while W2V and GloVe were generated by the code shared by the respec204 Approaches Lucene Core-Oriented Segmentation Sum-algebraic-based method with LSA with RI with ESA with W2V with GloVe Our proposed method with LSA with RI with ESA with W2V with GloVe Recall Top 10 0.0904 0.0985 0.1126 0.0630 0.0540 0.2657 0.2702 0.3545 0.3073 0.2818 0.3727 0.3727 Top 20 0.1040 0.1126 0.1621 0.0945 0.0900 0.3333 0.3558 0.4000 0.3743 0.3182 0.4364 0.4090 Top 50 0.1357 0.1361 0.2117 0.1216 0.1486 0.3963 0.4324 0.4590 0.4078 0.4000 0.4909 0.4500 MRR Top 10 0.0410 0.0613 0.0595 0.0348 0.0271 0.1356 0.1417 0.1"
S16-2025,W14-1618,0,0.205932,"be identified as the paraphrasing of Wold War I and the Soviet Union or even other variations, what is not available in our model. based query expansion. By applying either sum-algebraic-based method and our proposed method, most of the distributional models present significant performance improvement in comparison to non-distributional methods. It is also important to stress that Word2Vec and GloVe consistently deliver better results for the test collection. Apart the controversies about predictive-based and count-based distributional models (Baroni et al., 2014b; Lebret and Collobert, 2015; Levy and Goldberg, 2014), in the context of this work, these results suggest that predictive-based distributional models outperform count-based methods (despite the proximity of LSA results). Regarding the compositional method, the results of the core-oriented strategy combined with the named entity recognition exceeded all results delivered by the sum-algebraic-based method when comparing the same distributional model. The performance increases not only in the recall, which represents more entity categories retrieved but also in the mean reciprocal rank, reflecting that the target categories are better positioned in"
S16-2025,C08-1075,0,0.0882425,"Missing"
S16-2025,D14-1162,0,\N,Missing
S17-2089,S17-2141,0,0.11802,"Missing"
S17-2089,S17-2144,0,0.0510415,"Missing"
S17-2089,S17-2139,0,0.10068,"Missing"
S17-2089,S17-2154,0,0.102128,"Missing"
S17-2089,S15-2080,0,0.0474544,"Missing"
S17-2089,S17-2152,0,0.227443,"Missing"
S17-2089,S17-2089,1,0.124405,"ith the latter performing better. On the other hand, Cabanski et al. (2017) implemented two-hybrid techniques, where the Hybrid (DL, Lex) approach produced their best result for this track , same as for track 1. The systems that ranked first (Mansar et al., 2017) and second (Kar et al., 2017) both adopted a Hybrid (DL, Lex) technique, whereas an ML technique was used by the system in rank three. The Machine Learning-based techniques made use of the following algorithms: • Multi-Kernel Gaussian Process (MKGP) adopted by Deborah et al. (2017) • XGBoost Regressor (XGB) - adopted by Nasim (2017); John and Vechtomova (2017); Jiang et al. (2017) • Boosted Decision Tree Regression (BDTR) adopted by Symeonidis et al. (2017) • AdaBoost Regressor (ABR) - adopted by Jiang et al. (2017) • Bagging Regressor (BR) - adopted by Jiang et al. (2017) • Gradient Boosting Regressor (GBR) - adopted by Jiang et al. (2017) • Least Absolute Shrinkage and Selection Operator (LASSO) - adopted by Jiang et al. (2017) As can be seen above, the most common ML technique used within the systems was SVR by 9 participants. This was used by the system that ranked third for this track (Rotim et al., 2017). The Deep Learning-based techniques ma"
S17-2089,S17-2150,0,0.0829236,"Missing"
S17-2089,S17-2153,0,0.0826142,"Missing"
S17-2089,S17-2143,0,0.0501667,"Missing"
S17-2089,S15-2082,0,0.0390947,"Missing"
S17-2089,S17-2138,0,0.0787738,"Missing"
S17-2089,S17-2095,0,0.194537,"(six participants). However, the techniques were more balanced in this track, with six participants adopting a Machine Learning-based approach. It is worth noting that one of the systems used a Machine Learning and Ontology-based Hybrid approach, which technique is unique in both tracks. In this system,Schouten et al. (2017) used the SVR algorithm with ontology features (including features derived from ontology reasoning), which ontology was self-designed by the authors. Multiple techniques were used by some authors in order to find the best one to use in this competition within their system. Moore and Rayson (2017) experimented with an ML and DL algorithm respectively, with the latter performing better. On the other hand, Cabanski et al. (2017) implemented two-hybrid techniques, where the Hybrid (DL, Lex) approach produced their best result for this track , same as for track 1. The systems that ranked first (Mansar et al., 2017) and second (Kar et al., 2017) both adopted a Hybrid (DL, Lex) technique, whereas an ML technique was used by the system in rank three. The Machine Learning-based techniques made use of the following algorithms: • Multi-Kernel Gaussian Process (MKGP) adopted by Deborah et al. (20"
S17-2089,S17-2140,0,0.0513398,"Missing"
S17-2089,C16-2036,1,0.815626,"Missing"
S17-2089,S14-2004,0,0.0622893,"Missing"
S17-2089,S14-2009,0,0.0291623,"Missing"
S17-2089,S15-2078,0,0.0213314,"n the stock price. Each phrase was scored by between five and eight annotators. In our proposed task, the sentiment was also assigned with a view to the stock price or market development. However, our annotation is more fine-grained, ranging on a scale from -1 to 1. Furthermore, we annotate at the target (stock or company entity) rather than the phrase-level. Over the years, many shared tasks in SemEval have focused on Sentiment Analysis, exploring various angles within the field. A series of tasks have concentrated on Sentiment Analysis in Twitter (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal and Stoyanov, 2015). They have covered tasks such as Polarity Disambiguation, documentand topic-level Polarity Classification, and topicbased Sentiment Aggregation. These tasks targeted open domains, with topics being determined using Named Entity Recognition (e.g. celebrities, places, sports clubs). The sentiment was assigned on two-point (positive, negative), three-point (positive, negative, neutral) or five-point (strongly positive, weakly positive, neutral, weakly negative, strongly negative) scales. In contrast, our proposed task aims to detect fine-grained sentiment, a scoring company- and stock-level sent"
S17-2089,S17-2148,0,0.0883525,"Missing"
S17-2089,S17-2155,0,0.104424,"Missing"
S17-2089,S16-2025,1,0.655932,"Missing"
S17-2089,S17-2151,0,0.106732,"Missing"
S17-2089,S17-2146,0,0.0717444,"Missing"
S17-2089,S17-2147,0,0.0545313,"Missing"
S17-2089,takala-etal-2014-gold,0,0.174581,"Missing"
S17-2089,J09-3003,0,0.178774,"Missing"
S17-2089,S13-2067,0,0.16453,"Missing"
S17-2089,S17-2142,0,0.0553326,"Missing"
S17-2089,W13-1204,0,\N,Missing
S17-2089,S13-2052,0,\N,Missing
S17-2092,S14-2006,0,0.0138234,"s, 2015), information retrieval approaches (Sales et al., 2016). Additionally, pre-processing techniques such as clausal disembedding (Niklaus et al., 2016) and co-reference resolution are central components within the task. While approaches and test collections emphasising the shallow parsing aspect of the problem are more present in the literature (Section 3), others focusing on a semantic matching process involving a broader vocabulary gap (Furnas et al., 1987) are less prevalent. Part of this can be explained by the domain-specific nature of previous works (e.g. focus on spatial commands (Dukes, 2014)). In contrast, this task emphasises the creation of a test collection targeting an open domain scenario, with a large-scale set of target actions, assessing the ability of command interpretation approaches to address a larger vocabulary gap. This scenario aims to instantiate a real use case for end3 Similar Initiatives Most of the applications related to the parsing of natural language commands are within the context of human-robot interaction. The Human Robot Interaction Corpus (HuRIC) describes a list of spoken commands between humans and robots. It is composed of three datasets which were"
S17-2092,C16-2036,1,0.829302,"er natural language programming, since the action knowledge base used in the test collection maps to real-world APIs and so a semantic interpreter developed over this test collection can become a concrete end-user programming environment. The task can be addressed using different semantic interpretation abstractions: shallow parsing, lambda-calculus-based semantic parsing (Artzi et al., 2014), compositional-distributional models (Freitas and Curry, 2014; Freitas, 2015), information retrieval approaches (Sales et al., 2016). Additionally, pre-processing techniques such as clausal disembedding (Niklaus et al., 2016) and co-reference resolution are central components within the task. While approaches and test collections emphasising the shallow parsing aspect of the problem are more present in the literature (Section 3), others focusing on a semantic matching process involving a broader vocabulary gap (Furnas et al., 1987) are less prevalent. Part of this can be explained by the domain-specific nature of previous works (e.g. focus on spatial commands (Dukes, 2014)). In contrast, this task emphasises the creation of a test collection targeting an open domain scenario, with a large-scale set of target actio"
S17-2092,P15-1085,0,0.0194811,"photo of Paris. Attach the photo in an email, write the quote and send to maria@hotmail.com. Get the translation of the hashtag #sqn. Convert it to a QR code and send to my Skype account. within a spatial region. The proposed task can be contrasted with these previous initiatives in the following dimensions: (i) more comprehensive knowledge base of actions, (ii) generic (open domain) user programming scenarios and (iii) exploration of the interaction between actions and user personal information (Section 4). The work that has more similarity with this test collection is the problem defined by Quirk et al. (2015) under the ifttt.com platform, which targets the creation of an if-then receipt from a natural language description provided by the user. The first difference between the two tasks is the fact that, while the program structure is limited to ifthen recipes in Quirk et al., other more complex structures are supported in this task. Secondly, in the case of Quirk et al., the task requires only the mapping of the actions that comprise the recipe, keeping aside the instantiation of the parameter values, while our proposed task emphasises both. Finally, the presence of these two characteristics intro"
S17-2092,S16-2025,1,0.796401,"d] PARAMS: 557 Figure 1: An overview of the task. from: “USD” to: “JPY” from amount: 475 user natural language programming, since the action knowledge base used in the test collection maps to real-world APIs and so a semantic interpreter developed over this test collection can become a concrete end-user programming environment. The task can be addressed using different semantic interpretation abstractions: shallow parsing, lambda-calculus-based semantic parsing (Artzi et al., 2014), compositional-distributional models (Freitas and Curry, 2014; Freitas, 2015), information retrieval approaches (Sales et al., 2016). Additionally, pre-processing techniques such as clausal disembedding (Niklaus et al., 2016) and co-reference resolution are central components within the task. While approaches and test collections emphasising the shallow parsing aspect of the problem are more present in the literature (Section 3), others focusing on a semantic matching process involving a broader vocabulary gap (Furnas et al., 1987) are less prevalent. Part of this can be explained by the domain-specific nature of previous works (e.g. focus on spatial commands (Dukes, 2014)). In contrast, this task emphasises the creation o"
S17-2092,J82-2002,0,0.768718,"the content of the message. This first level of interpretation of the command would generate an output such as: • Statistical and ontology-based semantic matching techniques; • Compositional models for natural language command interpretation (NLCI); • Machine learning models for NLCI; • API/Service composition and associated planning techniques; • Linguistic aspects of user action intents. 2 Commands & Programming in Natural Language The use of natural language to instruct robots and computational systems, in general, is an active research area since the 70’s and 80’s (Maas and Suppes, 1985; Guida and Tasso, 1982) (and within references). Initiatives vary over a large spectrum of application domains including operating system’s functions (Manaris and Dominick, 1993), web services choreography (Englmeier et al., 2006), mobile programming by voice (Amos Azaria, 2016), domain-specific natural programming languages (Pane and Myers, 2006), industrial robots (Stenmark and Nugues, 2013) and home care assistants. The variability of domains translates into a wide number of research communities comprising different foci and being expressed by distinct terms such as natural language interfaces, end-user developme"
S17-2092,S17-2175,0,0.142996,"2: The scenario creation workflow. percentage of the total number of scenarios which were addressed. Participating teams were allowed to use external linguistic resources and external tools such as taggers and parsers. “If an issue is created, send its content to the Tech Manager.” This excerpt shows both cases. The bold its makes reference to an issue, while Tech Manager is a metonymy for the Tech Manager’s email (sandra@andrade.com.br according to the user KB). 6 7 Participants and Results Initially, nine teams demonstrated interest in the tasks, but only one participated in the challenge. Kubis et al. (2017) proposed the EUDAMU system, which implements an action ranking model based on TF/IDF and a type matching system. The EUDAMU system is composed of a pipeline divided into six steps. It starts by preprocessing the dataset using three tools (NLTK, Core-NLP and SyntaxNet). In the pre-processing step, natural language commands are tokenized and each token is enriched with its lemma, partof-speech and named entity labels. Additionally, it also adds the constituent and dependency structures associated with the commands. The final pre-processing step annotates the commands with types which supports t"
S17-2092,bastianelli-etal-2014-huric,0,\N,Missing
S17-2092,D14-1134,0,\N,Missing
W15-0133,W97-0207,0,0.366968,"the query-database semantic matching problem, the relation between wi and wj can easily transcend the synonym relation, expressing a broader semantic relatedness relationship. Semantic relatedness will include both taxonomic (including different abstraction levels) and non-taxonomic relationships. As an absolute number of relationships cannot be enumerated, approximate entropy measures can be used to estimate the terminological entropy of a term. Figure 2 and Figure 3(6,10) depicts Hterm . One example of approximate terminological entropy measures is the translational entropy (Melamed, 1996) [10] which uses the coherence in the translation of a word (translational distribution) as an entropy measure. Given a set of word pairs of a set of ordered word pairs (s, t), respectively coming from a source language and a target language, an iterative process is used to determine the frequency F (s, t) in which a word s is translated to a word t where F (s) is the absolute frequency of the source word in the text. The probability that s translates to t is defined as P (t|s) = F (s, t)/F (s). The notion of probability is defined by the translational distribution, the term H(T |s) is generated, c"
W15-0133,W00-0107,0,0.307471,"ughter of”,P) = 0.181 Hstruct(:Chelsea_Clinton) = 5 10 Hterm(“married to”) = 5.730 married to :birthPlace ... :spouse :spouse 12 11 Hmatching(“married to”,P) = 0.193 Hstruct(:Chelsea_Clinton, :spouse) = 0 :Mark_Mezvinsky Figure 3: Instantiation of the query-entropy model for an example query. 4.1.2 Structural Entropy (Hstruct ) The structural entropy defines the complexity of a database based on the possible propositions that can be encoded under its schema. It provides a numerical description of the amount of information expressed in the database, independent of the query. Pollard & Biermann [11] proposed a structural entropy measure to quantify the entropy of a structured database. The entropy is computed by taking into account the number of predicates and constants and their syntactic combination. Figure 2 and Figure 3(5,8,12) depicts Hstruct . The entropy of a constant c or a predicate π are defined as a function of the cardinality of the set of tuples in which the constant or the predicate is inserted. Further details are available in [11]. 4.1.3 Terminological Entropy (Hterm ) The terminological entropy focuses on quantifying an estimate on the amount of synonymy and vagueness fo"
W16-5305,W11-2501,0,0.0252842,"ely, ranging from binary classification (Lapata, 2002) to 35 classes (Moldovan et al., 2004) to open (inference-based) approaches (Sabou et al., 2008). There are several test collections for Semantic Relation Classification. Task 8 in SemEval 2010 (Hendrickx et al., 2009) focuses on multi-way semantic relation classification between pairs of nouns. Nine relations with broad coverage were selected1 , with a focus on practical interest. Patterns were used to collect relation candidates from the web, which were then classified by two annotators. In the context of Distributional Semantics, BLESS (Baroni and Lenci, 2011) is a test collection which is designed to evaluate Distributional Semantic Models (DSMs) on the task of Semantic Relation Classification. BLESS provides a benchmark for evaluating the lexical semantic capabilities of DSMs: it provides concept, relation, relatum triples for a large range of common concepts. There are five lexical semantic relations (co-hyponym, hypernym, meronym, attribute and event) and three random relations (random-noun, random-verb, random-adjective), which provide additional value for discriminativeness assessments. Some work has been done on SRC for specific domains, wit"
W16-5305,W09-2415,0,0.15411,"practices for creating relation inventories have been subject to much discussion (O’Seaghdha, 2007). Inventories can either be organised under a hierarchical (Rosario and Hearst, 2001), (Nastase and Szpakowicz, 2003), (Masolo et al., 2003) or under a flattened approach (Moldovan et al., 2004). The number of relations in a given inventory varies widely, ranging from binary classification (Lapata, 2002) to 35 classes (Moldovan et al., 2004) to open (inference-based) approaches (Sabou et al., 2008). There are several test collections for Semantic Relation Classification. Task 8 in SemEval 2010 (Hendrickx et al., 2009) focuses on multi-way semantic relation classification between pairs of nouns. Nine relations with broad coverage were selected1 , with a focus on practical interest. Patterns were used to collect relation candidates from the web, which were then classified by two annotators. In the context of Distributional Semantics, BLESS (Baroni and Lenci, 2011) is a test collection which is designed to evaluate Distributional Semantic Models (DSMs) on the task of Semantic Relation Classification. BLESS provides a benchmark for evaluating the lexical semantic capabilities of DSMs: it provides concept, rela"
W16-5305,J02-3004,0,0.0438773,"d by the conclusions and future work in Section 5. 2 The Semantic Relation Classification Task Semantic Relation Classification is usually framed under the context of a supervised classification problem. Best practices for creating relation inventories have been subject to much discussion (O’Seaghdha, 2007). Inventories can either be organised under a hierarchical (Rosario and Hearst, 2001), (Nastase and Szpakowicz, 2003), (Masolo et al., 2003) or under a flattened approach (Moldovan et al., 2004). The number of relations in a given inventory varies widely, ranging from binary classification (Lapata, 2002) to 35 classes (Moldovan et al., 2004) to open (inference-based) approaches (Sabou et al., 2008). There are several test collections for Semantic Relation Classification. Task 8 in SemEval 2010 (Hendrickx et al., 2009) focuses on multi-way semantic relation classification between pairs of nouns. Nine relations with broad coverage were selected1 , with a focus on practical interest. Patterns were used to collect relation candidates from the web, which were then classified by two annotators. In the context of Distributional Semantics, BLESS (Baroni and Lenci, 2011) is a test collection which is"
W16-5305,W04-2609,0,0.0491025,"cribes the relations provided by the foundational ontology DOLCE. Section 4 describes the corpus-based analysis, followed by the conclusions and future work in Section 5. 2 The Semantic Relation Classification Task Semantic Relation Classification is usually framed under the context of a supervised classification problem. Best practices for creating relation inventories have been subject to much discussion (O’Seaghdha, 2007). Inventories can either be organised under a hierarchical (Rosario and Hearst, 2001), (Nastase and Szpakowicz, 2003), (Masolo et al., 2003) or under a flattened approach (Moldovan et al., 2004). The number of relations in a given inventory varies widely, ranging from binary classification (Lapata, 2002) to 35 classes (Moldovan et al., 2004) to open (inference-based) approaches (Sabou et al., 2008). There are several test collections for Semantic Relation Classification. Task 8 in SemEval 2010 (Hendrickx et al., 2009) focuses on multi-way semantic relation classification between pairs of nouns. Nine relations with broad coverage were selected1 , with a focus on practical interest. Patterns were used to collect relation candidates from the web, which were then classified by two annota"
W16-5305,P16-1144,0,0.0187677,"d and built under conceptually well grounded methodologies. Complementarily, the semantic relation classification task provides a corpus-based analysis on the incidence of these semantic relations on discourse, providing the fine-grained semantic context in which these abstractions are instantiated. However, when projecting these semantic relations back to the corpora-level, it can be observed that the majority of the words within a text does not have a direct semantic relationship connecting them. Recent semantic interpretation tasks targeting word prediction over broader discourse contexts (Paperno et al., 2016) may require the detection of broader and complex semantic relations. Addressing these interpretation tasks may strongly benefit from relating terms expressed into the sentence using compositions of semantic relations. This work aims at improving the description and the formalisation of the semantic relation classification task by grounding it with a foundational ontology and by introducing the concept of composite semantic relations, in which the relations between terms within a text can be expressed using the composition of one or more relations. This work is licenced under a Creative Common"
W16-5305,W01-0511,0,0.326476,"semantic relation annotation task. Section 3 presents an analysis of current sets of semantic relations, and describes the relations provided by the foundational ontology DOLCE. Section 4 describes the corpus-based analysis, followed by the conclusions and future work in Section 5. 2 The Semantic Relation Classification Task Semantic Relation Classification is usually framed under the context of a supervised classification problem. Best practices for creating relation inventories have been subject to much discussion (O’Seaghdha, 2007). Inventories can either be organised under a hierarchical (Rosario and Hearst, 2001), (Nastase and Szpakowicz, 2003), (Masolo et al., 2003) or under a flattened approach (Moldovan et al., 2004). The number of relations in a given inventory varies widely, ranging from binary classification (Lapata, 2002) to 35 classes (Moldovan et al., 2004) to open (inference-based) approaches (Sabou et al., 2008). There are several test collections for Semantic Relation Classification. Task 8 in SemEval 2010 (Hendrickx et al., 2009) focuses on multi-way semantic relation classification between pairs of nouns. Nine relations with broad coverage were selected1 , with a focus on practical inter"
W16-5305,P04-1055,0,0.0644925,"rge range of common concepts. There are five lexical semantic relations (co-hyponym, hypernym, meronym, attribute and event) and three random relations (random-noun, random-verb, random-adjective), which provide additional value for discriminativeness assessments. Some work has been done on SRC for specific domains, with a focus on the medical domain. Stephens et al. (2001) distinguish 17 relations holding between genes. Rosario and Hearst (2001) classify relations between noun compounds in the medical domain, while Rosario et al. (2002) undertake a similar endeavour using the MeSH hierarchy. Rosario and Hearst (2004) explore SRC for biomedical texts, focusing on relations between treatments and diseases such as “prevents”, “cures” or less specific effects. 3 3.1 A Critique of Existing Sets of Semantic Relations SemEval-2010 Task 8 Although the Semeval-2010 Task 8 semantic relations set was developed with the aim of covering “real word” situations (Hendrickx et al., 2009), some of the constraints imposed to overcome the structural and lexical factors that can affect the truth of a relation, described next, can bring considerable limitations. In those cases, it is necessary to identify other classes of sema"
W16-5305,P02-1032,0,0.078664,"capabilities of DSMs: it provides concept, relation, relatum triples for a large range of common concepts. There are five lexical semantic relations (co-hyponym, hypernym, meronym, attribute and event) and three random relations (random-noun, random-verb, random-adjective), which provide additional value for discriminativeness assessments. Some work has been done on SRC for specific domains, with a focus on the medical domain. Stephens et al. (2001) distinguish 17 relations holding between genes. Rosario and Hearst (2001) classify relations between noun compounds in the medical domain, while Rosario et al. (2002) undertake a similar endeavour using the MeSH hierarchy. Rosario and Hearst (2004) explore SRC for biomedical texts, focusing on relations between treatments and diseases such as “prevents”, “cures” or less specific effects. 3 3.1 A Critique of Existing Sets of Semantic Relations SemEval-2010 Task 8 Although the Semeval-2010 Task 8 semantic relations set was developed with the aim of covering “real word” situations (Hendrickx et al., 2009), some of the constraints imposed to overcome the structural and lexical factors that can affect the truth of a relation, described next, can bring considera"
W16-5323,W91-0217,0,0.876612,"mental principles, such as the essential and non-essential property differentiation, when defining a new ontology concept by means of axioms, that is, in a structured way rather than in natural language. He concludes that Description Logic (DL), Unified Modeling Language (UML) and Object Role Modeling (ORM) present limitations to deal with some issues, and proposes a set of definitional tags to address those points. The information extraction from definitions has also been widely explored with the aim of constructing structured knowledge bases from machine readable dictionaries (Vossen, 1992; Calzolari, 1991; Copestake, 1991; Vossen, 1991; Vossen and Copestake, 1994). The development of a Lexical Knowledge Base (LKB) also used to take into account both semantic and syntactic information from lexical definitions, which were processed to extract the definiendum’s genus and differentiae. To populate the LKB, typed-feature structures were used to store the information from the differentiae, which were, in turn, transmitted by inheritance based on the genus information. A feature structure can be seen as a set of attributes for a given concept, such as “origin”, “color”, “smell”, “taste” and “temperat"
W16-5323,J00-4006,0,0.00800337,"ole. 3 Words that are not part of the largest sequence in the NP found as an entry in WN 181 5 Related Work The task described in this work is a form of Semantic Role Labeling (SRL), but centered on entities instead of events. Typically, SRL has as primary goal to identify what semantic relations hold among a predicate (the main verb in a clause) and its associated participants and properties (M`arquez et al., 2008). Focusing on determining “who” did “what” to “whom”, “where”, “when”, and “how”, the labels defined for this task include agent, theme, force, result and instrument, among others (Jurafsky and Martin, 2000). Liu and Ng (2007) perform SRL focusing on nouns instead of verbs, but most noun predicates in NomBank, which were used in the task, are verb nominalizations. This leads to the same event-centered role labeling, and the same principles and labels used for verbs apply. Kordjamshidi et al. (2010) describe a non-event-centered semantic role labeling task. They focus on spatial relations between objects, defining roles such as trajectory, landmark, region, path, motion, direction and frame of reference, and develop an approach to annotate sentences containing spatial descriptions, extracting topo"
W16-5323,kordjamshidi-etal-2010-spatial,0,0.0411865,"Missing"
W16-5323,P07-1027,0,0.0346158,"rt of the largest sequence in the NP found as an entry in WN 181 5 Related Work The task described in this work is a form of Semantic Role Labeling (SRL), but centered on entities instead of events. Typically, SRL has as primary goal to identify what semantic relations hold among a predicate (the main verb in a clause) and its associated participants and properties (M`arquez et al., 2008). Focusing on determining “who” did “what” to “whom”, “where”, “when”, and “how”, the labels defined for this task include agent, theme, force, result and instrument, among others (Jurafsky and Martin, 2000). Liu and Ng (2007) perform SRL focusing on nouns instead of verbs, but most noun predicates in NomBank, which were used in the task, are verb nominalizations. This leads to the same event-centered role labeling, and the same principles and labels used for verbs apply. Kordjamshidi et al. (2010) describe a non-event-centered semantic role labeling task. They focus on spatial relations between objects, defining roles such as trajectory, landmark, region, path, motion, direction and frame of reference, and develop an approach to annotate sentences containing spatial descriptions, extracting topological, directiona"
W16-5323,P14-5010,0,0.00745846,"Missing"
W16-5323,J08-2001,0,0.320465,"Missing"
W16-5323,W05-0639,0,0.107039,"Missing"
W19-8615,P18-2114,0,0.222506,"Missing"
W19-8615,D18-1080,0,0.162295,"Missing"
W19-8615,C18-1195,1,0.885812,"Missing"
W19-8615,W16-3411,0,0.0482218,"Missing"
W19-8615,P11-2117,0,0.0284994,"Sentence Splitting Corpora All of the TS approaches mentioned above make use of a set of hand-crafted transformation rules to decompose complex sentences into a sequence of structurally simplified components, requiring a complex rule engineering process. To overcome this expensive manual effort, Narayan et al. (2017) presented a first attempt at modelling a data-driven sentence splitting approach where simplification rewrites are learned automatically from examples of aligned complex source and simplified target sentences. Since previously compiled TS corpora (PWKP (Zhu et al., 2010), EW-SEW (Coster and Kauchak, 2011), and Newsela (Xu et al., 2015)) contain only a small number of split examples, they are ill-suited for learning to decompose sentences into shorter, syntactically simplified components. Therefore, Narayan et al. (2017) gathered a new dataset, W EB S PLIT, which is the first TS corpus that explicitly addresses the task of sentence splitting, while abstracting away from deletion-based and lexical simplification operaIntroduction Sentences that present a complex linguistic structure can be hard to comprehend by human readers, as well as difficult to analyze by semantic applications (Saggion, 201"
W19-8615,W18-7006,0,0.0412345,"Missing"
W19-8615,D18-1081,0,0.0879207,"Missing"
W19-8615,D17-1064,0,0.270904,"how to transform sentences with a complex linguistic structure into a fine-grained representation of short sentences that present a simple and more regular structure which is easier to process for downstream applications and thus facilitates and improves their performance. 1 2 Limitations of Existing Sentence Splitting Corpora All of the TS approaches mentioned above make use of a set of hand-crafted transformation rules to decompose complex sentences into a sequence of structurally simplified components, requiring a complex rule engineering process. To overcome this expensive manual effort, Narayan et al. (2017) presented a first attempt at modelling a data-driven sentence splitting approach where simplification rewrites are learned automatically from examples of aligned complex source and simplified target sentences. Since previously compiled TS corpora (PWKP (Zhu et al., 2010), EW-SEW (Coster and Kauchak, 2011), and Newsela (Xu et al., 2015)) contain only a small number of split examples, they are ill-suited for learning to decompose sentences into shorter, syntactically simplified components. Therefore, Narayan et al. (2017) gathered a new dataset, W EB S PLIT, which is the first TS corpus that ex"
W19-8615,N18-1063,0,0.116107,"Missing"
W19-8615,P19-1333,1,0.792209,"not doing anything wrong, and after Pearlman’s bankruptcy, the company emerged unscathed and was sold to a Canadian company. The Assistant Attorney in Orlando investigated the modeling company, and decided that they were not doing anything wrong. After Pearlman’s bankruptcy, the modeling company emerged unscathed and was sold to a Canadian company. 4 Corpus Construction M IN W IKI S PLIT is a large-scale sentence splitting corpus consisting of 203K complex source sentences and their simplified counterparts in the form of a sequence of minimal propositions. It was created by running D IS S IM (Niklaus et al., 2019), a syntactic TS framework, over the one million complex input sentences from the W IKI S PLIT corpus. D IS S IM applies a small set of 35 hand-written transformation rules to decompose a wide range of linguistic constructs, including both clausal components (coordinations, adverbial clauses, relative clauses and reported speech) and phrasal elements (appositions, prepositional phrases, adverbial/adjectival phrases and coordinate noun phrases). In that way, a fine-grained output in the form of a sequence of minimal, selfcontained propositions is produced. Some example instances are shown in Ta"
W19-8615,Q15-1021,0,0.0465848,"approaches mentioned above make use of a set of hand-crafted transformation rules to decompose complex sentences into a sequence of structurally simplified components, requiring a complex rule engineering process. To overcome this expensive manual effort, Narayan et al. (2017) presented a first attempt at modelling a data-driven sentence splitting approach where simplification rewrites are learned automatically from examples of aligned complex source and simplified target sentences. Since previously compiled TS corpora (PWKP (Zhu et al., 2010), EW-SEW (Coster and Kauchak, 2011), and Newsela (Xu et al., 2015)) contain only a small number of split examples, they are ill-suited for learning to decompose sentences into shorter, syntactically simplified components. Therefore, Narayan et al. (2017) gathered a new dataset, W EB S PLIT, which is the first TS corpus that explicitly addresses the task of sentence splitting, while abstracting away from deletion-based and lexical simplification operaIntroduction Sentences that present a complex linguistic structure can be hard to comprehend by human readers, as well as difficult to analyze by semantic applications (Saggion, 2017). Identifying such grammatica"
W19-8615,P02-1040,0,0.106631,"ructural simplicity of the instances contained in M IN W IKI S PLIT, we calculated the SAMSA and SAMSAabl scores of both the complex source and the simplified output sentences (Sulem et al., 2018b). They are the first metrics that explicitly target syntactic aspects 5.2 Manual Analysis In a second step, we randomly selected a subset of 300 sentences from M IN W IKI S PLIT, on which we conducted a manual analysis in order to get some deeper insights into the quality of the simplified sentences. Each input-output pair was rated by 2 Prior work on syntactic TS commonly also reports average BLEU (Papineni et al., 2002) scores. However, Sulem et al. (2018a) recently demonstrated that this score is inappropriate for the evaluation of TS approaches when sentence splitting is involved. Therefore, we refrain from calculating BLEU scores. 121 sentences that present a simple and more regular structure. The thus generated output may serve as an intermediate representation that is easier to process for downstream semantic applications and may thus lead to a better performance of those tools. We intend to train a sentence simplification model on M IN W IKI S PLIT and compare it to previously proposed systems trained"
W19-8615,C10-1152,0,0.0942465,"2 Limitations of Existing Sentence Splitting Corpora All of the TS approaches mentioned above make use of a set of hand-crafted transformation rules to decompose complex sentences into a sequence of structurally simplified components, requiring a complex rule engineering process. To overcome this expensive manual effort, Narayan et al. (2017) presented a first attempt at modelling a data-driven sentence splitting approach where simplification rewrites are learned automatically from examples of aligned complex source and simplified target sentences. Since previously compiled TS corpora (PWKP (Zhu et al., 2010), EW-SEW (Coster and Kauchak, 2011), and Newsela (Xu et al., 2015)) contain only a small number of split examples, they are ill-suited for learning to decompose sentences into shorter, syntactically simplified components. Therefore, Narayan et al. (2017) gathered a new dataset, W EB S PLIT, which is the first TS corpus that explicitly addresses the task of sentence splitting, while abstracting away from deletion-based and lexical simplification operaIntroduction Sentences that present a complex linguistic structure can be hard to comprehend by human readers, as well as difficult to analyze by"
W19-8615,E14-1076,0,0.230187,"Missing"
W19-8615,C04-1129,0,\N,Missing
W19-8662,D17-1064,0,0.125133,"accompanying contexts, while identifying the rhetorical relations that hold between them. In that way, we preserve the coherence structure of the input and, hence, its interpretability for downstream tasks. 1 Introduction We developed a syntactic text simplification (TS) approach that can be used as a preprocessing step to facilitate and improve the performance of a wide range of artificial intelligence (AI) tasks, such as Machine Translation, Information Extraction (IE) or Text Summarization. Since shorter sentences are generally better processed by natural language processing (NLP) systems (Narayan et al., 2017), the goal of our approach is to break down a complex source sentence into a set of minimal propositions, i.e. a sequence of sound, self-contained utterances, with each of them presenting a minimal semantic unit that cannot be further decomposed into meaningful propositions (Bast and Haussmann, 2013). However, any sound and coherent text is not simply a loose arrangement of self-contained units, but rather a logical structure of utterances that are semantically connected (Siddharthan, 2014). Consequently, when carrying out syntactic simplification operations without considering discourse impli"
W19-8662,C18-1326,1,0.870893,"Missing"
W19-8662,P19-1333,1,0.66159,"Missing"
W19-8662,P13-1045,0,0.0190543,"sion and 29 rules for the German approach.2 These patterns were heuristically determined in a comprehensive linguistic analysis and encode syntactic and lexical features that can be derived from a sentence’s parse tree.3 Each rule 1 The source code of our framework is available under https://github.com/Lambda-3/ DiscourseSimplification. 2 For reproducibility purposes, the complete set of transformation patterns is available under https://github. com/Lambda-3/DiscourseSimplification/ tree/master/supplemental_material. 3 For the English version, we use Stanford’s pre-trained lexicalized parser (Socher et al., 2013) to create a sentence’s phrasal parse tree. For the German approach, we apply dependency parse structures generated by the spaCy parser (https://spacy.io/). 504 Proceedings of The 12th International Conference on Natural Language Generation, pages 504–507, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics Figure 1: D IS S IM’s browser-based user interface. The simplified output is displayed in the form of a directed graph where the split sentences are connected by arrows whose labels denote the semantic relationship that holds between a pair of simplified sen"
W19-8662,N18-1081,0,0.0570697,"Missing"
