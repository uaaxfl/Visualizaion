1988.tmi-1.4,P84-1107,0,0.0223256,"ortant when it is difficult to constrain the types of output in generation, and, consequently, when the lexicon becomes large. Machine translation and automatic text summarization are among applications that by nature require a wide range of outputs and have to use a sizable lexicon. Note that of these two, the former does not involve utterance planning and concentrates on lexical and syntactic realization. In the natural language generation community, attention to the task of lexical selection attention has recently grown. Of course, it has always been recognized as an important problem (cf. Danlos, 1984; Jacobs, 1985; Bienkowski, 1986; and the survey Cumming, 1986) and was addressed in a well-known early generation project (Goldman, 1975). However, most major NLG efforts of the early 1980s (e.g., Mann and Matthiessen, 1985; McDonald, 1983; McKeown, 1985, Appelt, 1985) concentrated more on syntactic and stylistic realization as well as to text planning than to lexical selection. Among those who dealt with lexical selection was the SEMSYN project (Rösner, 1986). Currently, work on lexical selection is going on at the University of Pennsylvania (e.g., Marcus, 1987) and at University of Californ"
1988.tmi-1.4,C86-1131,0,0.012376,"namely, the selection of open-class lexical items to realize the meanings of object, event and property tokens in the input. Thus, the output of the generation module described here is a lexical unit or a pronoun in the target language. Our approach (and especially the expected input) to text generation is similar to that of the SEMSYN project (e.g., Rösner, 1986). Lexical selection is not, however, an immediate concern of and is not discussed at any length in SEMSYN descriptions (see Laubsch et al., 1984, p. 492), and a published analysis of practical difficulties encountered by the project (Hanakata et al., 1986) does not address this issue at all. Furthermore, since until very recently that project had to generate sentence-length texts (article titles), the problem of definite descriptions, pronominalization and ellipsis did not become acutely important. 3 Why is it a difficult task? Lexical choice is not a straightforward task. Suppose we have to express in English the meaning 'a person whose sex is male and whose age is between 13 and 15 years.' What knowledge do people use in order to come up with an appropriate choice out of such candidate realizations as those listed in (1). (1) boy, kid, teenag"
1988.tmi-1.4,P84-1105,0,0.0215193,"agmatic content (see Section 4 for more details). In this paper we deal with a subset of the generation task, namely, the selection of open-class lexical items to realize the meanings of object, event and property tokens in the input. Thus, the output of the generation module described here is a lexical unit or a pronoun in the target language. Our approach (and especially the expected input) to text generation is similar to that of the SEMSYN project (e.g., Rösner, 1986). Lexical selection is not, however, an immediate concern of and is not discussed at any length in SEMSYN descriptions (see Laubsch et al., 1984, p. 492), and a published analysis of practical difficulties encountered by the project (Hanakata et al., 1986) does not address this issue at all. Furthermore, since until very recently that project had to generate sentence-length texts (article titles), the problem of definite descriptions, pronominalization and ellipsis did not become acutely important. 3 Why is it a difficult task? Lexical choice is not a straightforward task. Suppose we have to express in English the meaning 'a person whose sex is male and whose age is between 13 and 15 years.' What knowledge do people use in order to co"
1988.tmi-1.4,C86-1031,0,0.0204453,"onstrain without any filtering actually achieved. (If there is no such provision there is a danger of a deadlock when all the processes will be in the waiting mode.) This eventuality is taken care of by rigging the probabilities of the two outcomes in toss-a-coin2. Heuristically, the choice of the main verb (proposition head) is the most independent, followed by the choice of noun phrase heads. Collocationally-constrain uses the collocation information in the lexicon entries to match 2 This mechanism is similar to the one used in the Friendly-Neighbors algorithm of the PARPAR parallel parser (Lozinskii and Nirenburg, 1986). and filter the candidate sets. If the residual set has cardinality one, the result is posted on the LSBB. If there exist more than one candidate, the function select-best (i.e., the Matcher) is called to perform context-independent lexical selection based on the quality of the match between the meaning pattern of the ILT frame on the one hand and the weighted meaning patterns of the GL entries in the candidate set on the other. The predicate modifier-realization-indicated returns t if there exist properties in the ILT frame that were not accounted for in the head realization of the lexical i"
1988.tmi-1.4,T87-1046,0,0.0264698,"d as an important problem (cf. Danlos, 1984; Jacobs, 1985; Bienkowski, 1986; and the survey Cumming, 1986) and was addressed in a well-known early generation project (Goldman, 1975). However, most major NLG efforts of the early 1980s (e.g., Mann and Matthiessen, 1985; McDonald, 1983; McKeown, 1985, Appelt, 1985) concentrated more on syntactic and stylistic realization as well as to text planning than to lexical selection. Among those who dealt with lexical selection was the SEMSYN project (Rösner, 1986). Currently, work on lexical selection is going on at the University of Pennsylvania (e.g., Marcus, 1987) and at University of California at Berkeley (Ward, 1988). Still, the set of problems facing this field is significant. One motivation for our research was that we agree with Marcus (1987, p. 211) that at present 'most generation systems don't use words at all,' and we believe that the quality of generation output will improve significantly once an adequate lexical selection component becomes a standard part of a NLG system. 2 The Task Research reported in this paper was performed within the DIOGENES project (Nirenburg, 1987), whose objective is to provide a high-quality generator for a knowle"
1988.tmi-1.4,C86-1148,1,0.816188,"be generated (the first application of DIOGENES is, for example, in the domain of computer hardware manuals) • a generation lexicon that links (sub)world concepts (or, more accurately, their instances) with particular lexical units of the target language. The above description is necessarily incomplete. See Nirenburg, 1987 for an extensive specification of all the facets of DIOGENES. The input to DIOGENES, known as interlingua text (ILT), is a set of FrameKit frames representing the propositional and non-propositional meanings of the source language text input into the translation system (see Nirenburg et al., 1986, 1987b; Nirenburg and Carbonell, 1987 for a detailed description). The ILT is produced during the analysis stage of the MT process by the parser, the semantic interpreter and, if needed, with human help. This latter stage is needed because our requirements for the input to generation are such that no current analysis system can produce them in a completely automatic fashion. The following is a sample InterLingua Text. (make-frame clausel (clauseid (value clausel)) (propositionid (value propositionl)) (speechactid (value speech-actl))) (make-frame propositionl (propositionid (value proposition"
1988.tmi-1.4,C88-2149,0,0.013119,"Bienkowski, 1986; and the survey Cumming, 1986) and was addressed in a well-known early generation project (Goldman, 1975). However, most major NLG efforts of the early 1980s (e.g., Mann and Matthiessen, 1985; McDonald, 1983; McKeown, 1985, Appelt, 1985) concentrated more on syntactic and stylistic realization as well as to text planning than to lexical selection. Among those who dealt with lexical selection was the SEMSYN project (Rösner, 1986). Currently, work on lexical selection is going on at the University of Pennsylvania (e.g., Marcus, 1987) and at University of California at Berkeley (Ward, 1988). Still, the set of problems facing this field is significant. One motivation for our research was that we agree with Marcus (1987, p. 211) that at present 'most generation systems don't use words at all,' and we believe that the quality of generation output will improve significantly once an adequate lexical selection component becomes a standard part of a NLG system. 2 The Task Research reported in this paper was performed within the DIOGENES project (Nirenburg, 1987), whose objective is to provide a high-quality generator for a knowledge-based interlingual machine translation system. The in"
1993.mtsummit-1.18,1988.tmi-1.15,0,0.0200055,"ly hand-crafted) grammars and lexicons of each language involved. The other group of researchers argues that this acquisition task is not realistic. Taking heart in numerous observations that deep analysis is not always needed for translation (for instance, that the polysemous Spanish noun centro is translated into German as zentrum no matter which of the senses of centro was used in the source language text), they opt for a simpler analysis and use of more direct source-target language substitutions in place of involved meaning analysis. A typical formulation of this position is given by Ben Ari et al. (1988, 2): ""It must be kept in mind that the translation process does not necessarily require full understanding of the text. Many ambiguities may be preserved during translation ..., and thus should not be presented to the user (human translator) for resolution."" 190 Similarly, Isabelle and Bourbeau (1985, 21) contend that, ""Sometimes, it is possible to ignore certain ambiguities, in the hope that the same ambiguities will carry over in translation. This is particularly true in systems like TAUM-AVIATION that deal with only one pair of closely related languages within a severely restricted subdoma"
1993.mtsummit-1.18,1992.tmi-1.12,0,0.0453738,"the area of machine translation. All the latest methodological novelties in this field are essentially technology-oriented and do not aim at advancing our knowledge about either basic mechanisms of text comprehension and production or computer models simulating such mechanisms. The two most recently popular technological paradigms in machine translation — examplebased translation (EBMT) and statistics-based translation (SBMT) — require knowledge about language only as an afterthought. While the representatives of the above paradigms are still at the stage of either building toy systems (e.g., Furuse and Iida, 1992; McLean, 1992, Jones, 1992, Maruyama and Watanabe, 1992) or struggling with the natural constraints of approaches that eschew the study of language as such (e.g., Brown et al., 1990), a number of proposals have come up for some hybridization of MT. In some such approaches, corpus analysis is used for tuning analysis and transfer grammars (e.g., Su and Chang, 1992). In others, a standard transfer-based approach (TBMT) is followed using traditional analysis and generation techniques but having transfer component based on aligned bilingual corpora (Grishman and Kosaka, 1992). Still in others, it"
1993.mtsummit-1.18,1992.tmi-1.23,0,0.0172098,"ilding toy systems (e.g., Furuse and Iida, 1992; McLean, 1992, Jones, 1992, Maruyama and Watanabe, 1992) or struggling with the natural constraints of approaches that eschew the study of language as such (e.g., Brown et al., 1990), a number of proposals have come up for some hybridization of MT. In some such approaches, corpus analysis is used for tuning analysis and transfer grammars (e.g., Su and Chang, 1992). In others, a standard transfer-based approach (TBMT) is followed using traditional analysis and generation techniques but having transfer component based on aligned bilingual corpora (Grishman and Kosaka, 1992). Still in others, it was suggested that statistical information be used as the source of preference assignment during text disambiguation (e.g., the outline presented in Lehmann and Ott, 1992). Indeed, hybrid MT systems were a central topic of the latest Conference on Theoretical and Methodological Issues in MT. It is important to recognize, though, that most of these hybridization proposals were driven essentially by technological concerns. Of course, machine translation is an applied field and it is quite appropriate that the impetus for progress comes in a large part from extra-scientific"
1993.mtsummit-1.18,1992.tmi-1.14,0,0.0328048,"the latest methodological novelties in this field are essentially technology-oriented and do not aim at advancing our knowledge about either basic mechanisms of text comprehension and production or computer models simulating such mechanisms. The two most recently popular technological paradigms in machine translation — examplebased translation (EBMT) and statistics-based translation (SBMT) — require knowledge about language only as an afterthought. While the representatives of the above paradigms are still at the stage of either building toy systems (e.g., Furuse and Iida, 1992; McLean, 1992, Jones, 1992, Maruyama and Watanabe, 1992) or struggling with the natural constraints of approaches that eschew the study of language as such (e.g., Brown et al., 1990), a number of proposals have come up for some hybridization of MT. In some such approaches, corpus analysis is used for tuning analysis and transfer grammars (e.g., Su and Chang, 1992). In others, a standard transfer-based approach (TBMT) is followed using traditional analysis and generation techniques but having transfer component based on aligned bilingual corpora (Grishman and Kosaka, 1992). Still in others, it was suggested that statist"
1993.mtsummit-1.18,1992.tmi-1.21,0,0.0283009,"s such (e.g., Brown et al., 1990), a number of proposals have come up for some hybridization of MT. In some such approaches, corpus analysis is used for tuning analysis and transfer grammars (e.g., Su and Chang, 1992). In others, a standard transfer-based approach (TBMT) is followed using traditional analysis and generation techniques but having transfer component based on aligned bilingual corpora (Grishman and Kosaka, 1992). Still in others, it was suggested that statistical information be used as the source of preference assignment during text disambiguation (e.g., the outline presented in Lehmann and Ott, 1992). Indeed, hybrid MT systems were a central topic of the latest Conference on Theoretical and Methodological Issues in MT. It is important to recognize, though, that most of these hybridization proposals were driven essentially by technological concerns. Of course, machine translation is an applied field and it is quite appropriate that the impetus for progress comes in a large part from extra-scientific sources. However, machine translation is a special application. Unlike most other areas, it is a very good 1 Many thanks to David Farwell and Yorick Wilks for extensive discussions and critique"
1993.mtsummit-1.18,1992.tmi-1.15,0,0.0429267,"thodological novelties in this field are essentially technology-oriented and do not aim at advancing our knowledge about either basic mechanisms of text comprehension and production or computer models simulating such mechanisms. The two most recently popular technological paradigms in machine translation — examplebased translation (EBMT) and statistics-based translation (SBMT) — require knowledge about language only as an afterthought. While the representatives of the above paradigms are still at the stage of either building toy systems (e.g., Furuse and Iida, 1992; McLean, 1992, Jones, 1992, Maruyama and Watanabe, 1992) or struggling with the natural constraints of approaches that eschew the study of language as such (e.g., Brown et al., 1990), a number of proposals have come up for some hybridization of MT. In some such approaches, corpus analysis is used for tuning analysis and transfer grammars (e.g., Su and Chang, 1992). In others, a standard transfer-based approach (TBMT) is followed using traditional analysis and generation techniques but having transfer component based on aligned bilingual corpora (Grishman and Kosaka, 1992). Still in others, it was suggested that statistical information be used as th"
1993.mtsummit-1.18,1992.tmi-1.4,0,0.0386735,"nslation. All the latest methodological novelties in this field are essentially technology-oriented and do not aim at advancing our knowledge about either basic mechanisms of text comprehension and production or computer models simulating such mechanisms. The two most recently popular technological paradigms in machine translation — examplebased translation (EBMT) and statistics-based translation (SBMT) — require knowledge about language only as an afterthought. While the representatives of the above paradigms are still at the stage of either building toy systems (e.g., Furuse and Iida, 1992; McLean, 1992, Jones, 1992, Maruyama and Watanabe, 1992) or struggling with the natural constraints of approaches that eschew the study of language as such (e.g., Brown et al., 1990), a number of proposals have come up for some hybridization of MT. In some such approaches, corpus analysis is used for tuning analysis and transfer grammars (e.g., Su and Chang, 1992). In others, a standard transfer-based approach (TBMT) is followed using traditional analysis and generation techniques but having transfer component based on aligned bilingual corpora (Grishman and Kosaka, 1992). Still in others, it was suggested"
1993.mtsummit-1.18,1992.tmi-1.22,0,0.0334135,"d translation (EBMT) and statistics-based translation (SBMT) — require knowledge about language only as an afterthought. While the representatives of the above paradigms are still at the stage of either building toy systems (e.g., Furuse and Iida, 1992; McLean, 1992, Jones, 1992, Maruyama and Watanabe, 1992) or struggling with the natural constraints of approaches that eschew the study of language as such (e.g., Brown et al., 1990), a number of proposals have come up for some hybridization of MT. In some such approaches, corpus analysis is used for tuning analysis and transfer grammars (e.g., Su and Chang, 1992). In others, a standard transfer-based approach (TBMT) is followed using traditional analysis and generation techniques but having transfer component based on aligned bilingual corpora (Grishman and Kosaka, 1992). Still in others, it was suggested that statistical information be used as the source of preference assignment during text disambiguation (e.g., the outline presented in Lehmann and Ott, 1992). Indeed, hybrid MT systems were a central topic of the latest Conference on Theoretical and Methodological Issues in MT. It is important to recognize, though, that most of these hybridization pr"
1993.mtsummit-1.18,J85-1002,0,\N,Missing
1993.mtsummit-1.18,C88-1016,0,\N,Missing
1993.mtsummit-1.18,J90-2002,0,\N,Missing
1993.tmi-1.4,1992.tmi-1.23,0,0.026613,"Missing"
1993.tmi-1.4,1992.tmi-1.14,0,0.0405059,"nvolved theories in physics and other natural sciences by relying on commonsense reasoning and ended up with a set of theories whose formulation was arguably even more intricate and difficult to use for reasoning than the original ones. For EBMT to succeed, it should be shown not to rely on an extensive apparatus of linguistic and domain-oriented language analysis, which forms the basis of the ""traditional"" rule-based MT, which EBMT set out to supplant. 5.2. Related Work The only detailed description of a proposal for solving the partitioning problem has been reported by Maruyama and Watanabe (1992). They describe an algorithm for determining the best partitioning (""cover"") of a source language sentence in an environment which, though not strictly EBMT, is closely related to EBMT. The input to their algorithm is a) a source language sentence and b) a set of pairs consisting of a substring of the input sentence and the translation of this substring into a target language. The output of their algorithm is essentially, a string of source language substrings which completely covers the entire input sentence. (The substitution of translations for these substrings is, then, a trivial step.) Th"
1993.tmi-1.4,1992.tmi-1.15,0,0.226311,"et al., 1992 who, in order to implement an EBMT algorithm based on a pre-acquired set of translation equivalence patterns, had to forgo the flexibility of matching expressions of the kind suggested by Sato and Nagao, 1990). Indeed, a major hurdle introduced by using passages shorter than a complete sentence is the necessity of finding the optimum cover of an entire passage by a string of matched fragments (see, e.g., Sato and Nagao, 1990, where the passage length parameter is made a component of the similarity metric, see below). This task is exacerbated if there are many possible covers see Maruyama and Watanabe, 1992). The simplest solution, therefore, is to select a single external criterion for partitioning the text into passages. For instance, sentence boundaries can be declared as passage delimiters. In this paper we report experiments on matching both on complete sentences and on passages whose length is determined dynamically. Turning to the subject of matching an input string with an archive, we first observe that the simplest similarity metric is a complete match, with a Boolean outcome. If an S completely matches an S', then T' is used, otherwise, failure is announced. Even this kind of a system c"
1993.tmi-1.4,1992.tmi-1.4,0,0.0245896,"ation. A possible set of match relaxation criteria (that is. the composition of the equivalence class for comparisons) is presented in Table 1. The table lists: a) equivalence classes for matching, b) an a priori set of penalty factors assigned to matches against elements of a given class (see Section 3 for a discussion of the experimental verification of these factors) and c) the background knowledge and tools necessary to support a particular matching criterion. 1 For reasons of immediate inapplicability, we disregard both statistical and connectionist methods of distance determination. See McLean, 1992, for an experiment on connectionist determination of the distance. 49 50 The computational price of using equivalence sets 4, 6, 8 and 10 is prohibitive (if one already has a semantic analyzer, it can be put to better use than just supporting EBMT!). The utility of comparing part-of-speech symbol strings (set 12) is very limited (what use would be the match of ""Queen of England"" with ""rate of exchange,"" both N Prep N?) even though this heuristic was used by Furuse and Iida. 1992 and Sumita and Iida (1991), as was synonymy and hyperonymy based on a thesaurus-based hierarchy of semantic markers"
1993.tmi-1.4,A92-1045,1,0.820149,"urrently give the common investor the opportunity for a very high acquisition on an initial investment of a few thousand dollars. We intermittently used additions, deletions and substitutions of synonyms, hyperonyms and hyponyms. A central task was to devise a method of determining the ""canonical"" ranking order of possible matches. As our original desire was to see how EBMT can fit into a practical MT environment, we decided to use an independently motivated ""control"" metric for checking the quality of the EBMT metric we use. To do so, we decided to use the CMU Translator's Workstation (TWS) (Nirenburg et al., 1992). This TWS includes, among other facilities, a post-editing editor which makes it easy for the user to substitute a word or a phrase in the text by their synonyms, hyperonyms or hyponyms, as well as delete and move text passages using at most two mouse clicks; see Figure 1 for an illustration. The ""control"" metric was put together as follows: for each sentence in the example set, we calculated the number of keystrokes it required in the TWS editor to make it completely match the input sentence. Every word deletion was counted as 3 keystrokes; every word substitution using a TWS editor-supporte"
1993.tmi-1.4,C92-2107,0,0.0361671,"hosen in the hope that the resulting systems will have fewer practical shortcomings than the pure rule-based systems (a high complexity of processing plus a high price of knowledge acquisition) or the pure EBMT systems (a very ungraceful degradation curve when matches are bad). Among the crucial tasks in EBMT are a) selecting the most appropriate length of the passages S and S' and b) establishing the metric of similarity between S and S'. In what follows we analyze these tasks, in turn. The longer the matched passages, the lower the probability of a complete match (see also the discussion in Nomiyama, 1992. p. 715) . The shorter the passages, the greater the probability of ambiguity (one and the same S' can correspond to more than one passage T') and the greater the danger that the resulting translation will be of low quality, due to passage boundary friction and incorrect chunking. This is easy to see when a passage is exactly one word long (the minimum reasonable length), since words are typically ambiguous, and word-for-word translation has been repeatedly shown to be inadequate in many ways. In practice, if passage length in a particular EBMT system is user-settable, the optimum passage len"
1993.tmi-1.4,P91-1036,0,0.0147707,"several ways of dealing with this problem: • biting the bullet and going through a massive knowledge acquisition effort, either general-purpose (e.g., the CYC project, Lenat et al., 1990) or domain-specific (e.g., the KBMT-89 project, Goodman and Nirenburg, 1992 • seeking ways of bringing down the price of knowledge acquisition by studying ways of automatically or semi automatically extracting relevant information from machine-readable dictionaries (morphological, syntactic and some semantic information, e.g., in the work of Wilks et al., 1990) or text corpora (for instance, collocations cf. Smadja, 1991) • seeking ways of avoiding the need for massive knowledge acquisition by rejecting the entire established NLP paradigm in favor of knowledge-free, linguistics- and AI-independent approaches. This last option has been energetically promulgated in the important NLP application of machine translation (MT). The two basic ""non-traditional"" approaches to MT are • statistical MT, which seeks to carry out translation based on complex cooccurrence and distribution probability calculations over very large aligned bilingual text corpora, and • a more modest approach, called example-based MT. which is th"
1993.tmi-1.4,1992.tmi-1.13,0,0.0250262,", aligned with their translations into a target language, passages T', S is compared with the source-language ""side"" of the archive. The ""closest"" match for passage S' is selected and the translation of this closest match, the passage T' is accepted as the translation of S. The appeal of the basic idea of EBMT is so high that it has been suggested as the basis for tackling additional tasks such as source language analysis (e.g., Jones. 1992; Furuse and Iida, 1992), source-to-target language transfer (e.g., Grishman and Kosaka. 1992; Furuse and Iida, 1992; Watanabe. 1992) and generation (e.g., Somers, 1992). This marks the advent of hybrid rule-based and example-based MT systems. The hybridization route is chosen in the hope that the resulting systems will have fewer practical shortcomings than the pure rule-based systems (a high complexity of processing plus a high price of knowledge acquisition) or the pure EBMT systems (a very ungraceful degradation curve when matches are bad). Among the crucial tasks in EBMT are a) selecting the most appropriate length of the passages S and S' and b) establishing the metric of similarity between S and S'. In what follows we analyze these tasks, in turn. The"
1993.tmi-1.4,P91-1024,0,0.322376,"licability, we disregard both statistical and connectionist methods of distance determination. See McLean, 1992, for an experiment on connectionist determination of the distance. 49 50 The computational price of using equivalence sets 4, 6, 8 and 10 is prohibitive (if one already has a semantic analyzer, it can be put to better use than just supporting EBMT!). The utility of comparing part-of-speech symbol strings (set 12) is very limited (what use would be the match of ""Queen of England"" with ""rate of exchange,"" both N Prep N?) even though this heuristic was used by Furuse and Iida. 1992 and Sumita and Iida (1991), as was synonymy and hyperonymy based on a thesaurus-based hierarchy of semantic markers. The thesaurus-based definition of synonymy is more relaxed than the one intended in equivalence class 4 (it might include pen, pencil and calligraphy, as having the marker writing, in an equivalence set, whereas the latter would not list calligraphy), though it might be weakly more selective than the one used in equivalence class 5, which might include both pencil and pigsty as members of the synonym class of pen. 2. The Matching Metric We decided to use a matching metric based on string composition disc"
1993.tmi-1.4,C92-2115,0,0.193736,"who, in order to implement an EBMT algorithm based on a pre-acquired set of translation equivalence patterns, had to forgo the flexibility of matching expressions of the kind suggested by Sato and Nagao, 1990). Indeed, a major hurdle introduced by using passages shorter than a complete sentence is the necessity of finding the optimum cover of an entire passage by a string of matched fragments (see, e.g., Sato and Nagao, 1990, where the passage length parameter is made a component of the similarity metric, see below). This task is exacerbated if there are many possible covers see Maruyama and Watanabe, 1992). The simplest solution, therefore, is to select a single external criterion for partitioning the text into passages. For instance, sentence boundaries can be declared as passage delimiters. In this paper we report experiments on matching both on complete sentences and on passages whose length is determined dynamically. Turning to the subject of matching an input string with an archive, we first observe that the simplest similarity metric is a complete match, with a Boolean outcome. If an S completely matches an S', then T' is used, otherwise, failure is announced. Even this kind of a system c"
1994.amta-1.10,H93-1038,1,0.792898,"Missing"
1994.amta-1.10,1993.tmi-1.4,1,0.733597,"Missing"
1995.tmi-1.8,H93-1036,0,0.0147809,". Indeed, the seven entries of dejar seem to offer a near ideal example of such a semantics-free approach1 to disambiguation: it turns out that an analyzer will be able to determine which of the meanings is used in the text by using syntactic, morphological and other non-semantic clues from the various lexicon entry zones which we, due to space constraints, did not show in the example. 109 In some other projects, the legitimate desire to avoid acquisition difficulties has led to a redefinition of the notion of ontology as an hierarchically organized set of word senses within one language (see Knight, 1993; and, to some extent, Farwell et al., 1993), which are connected directly to corresponding word senses in other languages. There is no essential difference between this approach (which claims to be interlingual) and the transfer approach because, in the former, the hierarchical structures of the language-dependent ontologies of word senses (whose nodes contain little information other than subsumption and translation equivalents) end up being essentially unused. One example of the benefits of ontology-based description is in resolution of referential ambiguities, a notoriously difficult probl"
1996.amta-1.10,H92-1030,0,0.0203237,"Missing"
1996.amta-1.10,J93-1002,0,0.0148156,"available for that language, in particular the hand-corrected parsed text of the Penn Tree Bank (Marcus et al., 1993). See Carroll and Charniak (1992) and Lari and Young (1990) for examples of this approach. In our experiments, a small, core grammar for the languages under development (in our case, Russian and Serbo-Croatian) is used instead of the information provided by the parsed Penn Tree Bank corpus. This 98 grammar will &quot;seed&apos;&apos; the development of a robust, large-scale grammar for these languages. This approach has been discussed, for example, in the work of Pereira and Schabes (1992) or Briscoe and Carroll (1993). We plan to combine this non-automatic top-down approach with any number of bottom-up organizing approaches, for instance, N-gram analyses of corpus which extend the methods applied by Finch and Chater (1991). 4 Rapid Lexical Acquisition MT-related lexicons at CRL include both large bilingual terminological glossaries and deep-coverage computational-semantic lexicons to support various MT engines. For rapid development, acquisition of all these resources must be enhanced. 4.1 Corpus-Based Bilingual Glossary Acquisition Bilingual glossaries support the glossary-based translation engine in the"
1996.amta-1.10,1994.amta-1.8,0,0.0103157,"work continues to advance the framework of the Pangloss MT project (e.g., Nirenburg (ed.), 1994), in which the word-for-word, glossary-based, transfer-based, knowledge-based and example-based translation engines have been used in a variety of configurations. 2.1 Reuse of Off-The-Shelf Linguistic Components In order to develop working prototypes of lower-end modules for the MT engines in a short time, offthe-shelf linguistic components were used whenever possible, in the spirit of, for example, work at Cm in Montreal (e.g., Isabelle, 1992). For example, the SPOST Spanish part-of-speech tagger (Farwell et al., 1994), the Juman Japanese morphological analyzer (Matsumoto et al., 1993) and the Penman English morphological generator (Penman, 1988) were integrated. Often, off-the-shelf resources were used as input for a semi-automatic process of knowledge acquisition. Thus, in our experiments Spanish, Japanese and Arabic bilingual dictionaries were derived from corresponding machine-readable versions of paper dictionaries using the LexBase approach (Guthrie et al., 1993). In reusing off-the-shelf components, a major problem has been the lack of compatibility between linguistic representations used by the vari"
1996.amta-1.10,A94-1016,1,0.838025,"oduces generation lexicons from analysis lexicons. 6. Automatic testing methods enable rapid validation of automatically acquired resources. 2 The Core Environment The Computing Research Laboratory (CRL) has developed a multi-engine architecture designed for rapid development of machine translation functionalities in a translator&apos;s workstation. This architecture is a descendant of the approach to multi-engine human-aided machine translation (HAMT) developed by Sergei Nirenburg and his associates at CMU CMT (Nirenburg et al., 1992; 1994; Frederking et al., 1993; Nirenburg and Frederking, 1994; Frederking and Nirenburg, 1994). At present, it serves as the environment for HAMT from Japanese, Arabic, Russian and Spanish to English. The system includes a general multilingual document handling environment supporting the translator&apos;s workstation and a basic multilingual machine translation architecture. Emphasis is made on reusing off-the-shelf linguistic components and on empirical, corpus-based acquisition of bilingual glossaries. The translator&apos;s workstation is designed to be used concurrently with information extraction and retrieval tools. To this end, the Temple Translator&apos;s Workstation, which provides an integra"
1996.amta-1.10,H93-1038,1,0.771772,"ion of manually acquired lexicons. 5. Lexicon reversal produces generation lexicons from analysis lexicons. 6. Automatic testing methods enable rapid validation of automatically acquired resources. 2 The Core Environment The Computing Research Laboratory (CRL) has developed a multi-engine architecture designed for rapid development of machine translation functionalities in a translator&apos;s workstation. This architecture is a descendant of the approach to multi-engine human-aided machine translation (HAMT) developed by Sergei Nirenburg and his associates at CMU CMT (Nirenburg et al., 1992; 1994; Frederking et al., 1993; Nirenburg and Frederking, 1994; Frederking and Nirenburg, 1994). At present, it serves as the environment for HAMT from Japanese, Arabic, Russian and Spanish to English. The system includes a general multilingual document handling environment supporting the translator&apos;s workstation and a basic multilingual machine translation architecture. Emphasis is made on reusing off-the-shelf linguistic components and on empirical, corpus-based acquisition of bilingual glossaries. The translator&apos;s workstation is designed to be used concurrently with information extraction and retrieval tools. To this en"
1996.amta-1.10,P91-1025,0,0.0146306,"destroy will have the semantics of an EVENT, as will the noun destruction (naturally, with a different linking in the syntax-semantics interface). Similarly, destroyer (as a person) would be represented using the same event with the addition of a HUMAN as a filler of the agent case-role. This built-in transcategoriality strongly facilitates applications such as interlingual MT, as it renders vacuous many problems connected with category mismatches and misalignments that plague those paradigms in MT that do not rely on extracting language-neutral text meaning representations (e.g., Dorr 1995; Kameyama, Ochitani, and Peters, 1991). The following phenomena seem to be appropriate for treatment with LRs: • Inflected Forms - Specifically, those inflectional phenomena which accompany changes in subcategorization frame (passivization, dative alternation, etc.). • Word Formation - The production of derived forms by LR is illustrated in a case study below, and includes formation of deverbal nominals (destruction, running), agentive nouns (catcher). Typically involving a shift in syntactic category, these LRs are often less productive than inflection-oriented ones. Consequently, derivational LRs are even more prone to overgene"
1996.amta-1.10,J93-2004,0,0.0303024,"sers for any language (Brill, 1994; Brill and Marcus, 1994), though they do not yet provide a mature enough technology for general use. CRL is working on semi-automatic development of robust grammars for languages with limited on-line resources. Limiting the resources is vital since the languages for which rapid MT development is most important are frequently those where electronic resources are scarce. Results reported to date refer to developing grammars for English using the extensive resources available for that language, in particular the hand-corrected parsed text of the Penn Tree Bank (Marcus et al., 1993). See Carroll and Charniak (1992) and Lari and Young (1990) for examples of this approach. In our experiments, a small, core grammar for the languages under development (in our case, Russian and Serbo-Croatian) is used instead of the information provided by the parsed Penn Tree Bank corpus. This 98 grammar will &quot;seed&apos;&apos; the development of a robust, large-scale grammar for these languages. This approach has been discussed, for example, in the work of Pereira and Schabes (1992) or Briscoe and Carroll (1993). We plan to combine this non-automatic top-down approach with any number of bottom-up orga"
1996.amta-1.10,C94-1101,0,0.0552497,"Missing"
1996.amta-1.10,C92-3149,0,0.0929712,"entations) enable rapid expansion of manually acquired lexicons. 5. Lexicon reversal produces generation lexicons from analysis lexicons. 6. Automatic testing methods enable rapid validation of automatically acquired resources. 2 The Core Environment The Computing Research Laboratory (CRL) has developed a multi-engine architecture designed for rapid development of machine translation functionalities in a translator&apos;s workstation. This architecture is a descendant of the approach to multi-engine human-aided machine translation (HAMT) developed by Sergei Nirenburg and his associates at CMU CMT (Nirenburg et al., 1992; 1994; Frederking et al., 1993; Nirenburg and Frederking, 1994; Frederking and Nirenburg, 1994). At present, it serves as the environment for HAMT from Japanese, Arabic, Russian and Spanish to English. The system includes a general multilingual document handling environment supporting the translator&apos;s workstation and a basic multilingual machine translation architecture. Emphasis is made on reusing off-the-shelf linguistic components and on empirical, corpus-based acquisition of bilingual glossaries. The translator&apos;s workstation is designed to be used concurrently with information extraction"
1996.amta-1.10,H94-1026,1,0.817895,"lexicons. 5. Lexicon reversal produces generation lexicons from analysis lexicons. 6. Automatic testing methods enable rapid validation of automatically acquired resources. 2 The Core Environment The Computing Research Laboratory (CRL) has developed a multi-engine architecture designed for rapid development of machine translation functionalities in a translator&apos;s workstation. This architecture is a descendant of the approach to multi-engine human-aided machine translation (HAMT) developed by Sergei Nirenburg and his associates at CMU CMT (Nirenburg et al., 1992; 1994; Frederking et al., 1993; Nirenburg and Frederking, 1994; Frederking and Nirenburg, 1994). At present, it serves as the environment for HAMT from Japanese, Arabic, Russian and Spanish to English. The system includes a general multilingual document handling environment supporting the translator&apos;s workstation and a basic multilingual machine translation architecture. Emphasis is made on reusing off-the-shelf linguistic components and on empirical, corpus-based acquisition of bilingual glossaries. The translator&apos;s workstation is designed to be used concurrently with information extraction and retrieval tools. To this end, the Temple Translator&apos;s Works"
1996.amta-1.10,P92-1017,0,0.0125483,"using the extensive resources available for that language, in particular the hand-corrected parsed text of the Penn Tree Bank (Marcus et al., 1993). See Carroll and Charniak (1992) and Lari and Young (1990) for examples of this approach. In our experiments, a small, core grammar for the languages under development (in our case, Russian and Serbo-Croatian) is used instead of the information provided by the parsed Penn Tree Bank corpus. This 98 grammar will &quot;seed&apos;&apos; the development of a robust, large-scale grammar for these languages. This approach has been discussed, for example, in the work of Pereira and Schabes (1992) or Briscoe and Carroll (1993). We plan to combine this non-automatic top-down approach with any number of bottom-up organizing approaches, for instance, N-gram analyses of corpus which extend the methods applied by Finch and Chater (1991). 4 Rapid Lexical Acquisition MT-related lexicons at CRL include both large bilingual terminological glossaries and deep-coverage computational-semantic lexicons to support various MT engines. For rapid development, acquisition of all these resources must be enhanced. 4.1 Corpus-Based Bilingual Glossary Acquisition Bilingual glossaries support the glossary-ba"
1996.amta-1.10,J91-4003,0,0.131982,"ontaining entries corresponding to word senses, without reference to their part of speech (the senses of both the noun and the verb &quot;walk,&quot; for example, will be listed inside the same superentry). Each word meaning is identified by a unique identifier, or lexeme (Mel&apos;cuk et al., 1984; Onyshkevych and Nirenburg, 1995). No distinction is made between homonyms and polysemous words, and all homonyms and all meaning shifts of polysemous words are under one single superentry, adopting Fillmore&apos;s (1971) rather than Weinreich&apos;s approach (1964). Moreover, for &quot;logically polysemous&quot; words as defined by Pustejovsky (1991; 1995), we keep one entry (as opposed to two in most approaches). The meaning of a lexical entry is encoded in a (lexical) semantic representation language whose primitives are predominantly terms in an independently motivated world model, or ontology (see Carlson and Nirenburg, 1990; Mahesh and Nirenburg, 1995; Mahesh, 1996). The information contained inside a lexeme is divided into zones corresponding to various levels of lexical information. The different zones include, among others, category, morphology, syntactic structure, semantic mapping, lexical relations and rules and stylistics. 4."
1996.amta-1.10,P96-1005,1,0.647079,"shifts as +count to - count or - common to +common. While shifts and modulations are important, we find that the main significance of LRs is in their promise to aid the task of massive lexical acquisition by allowing automatic generation of entries from a set of &quot;core&quot; entries acquired manually. The major problem in using LRs is overgeneration: LR generators suggest inappropriate forms which need to be weeded out by humans. This problem is usually overlooked if the goal of studying the LRs is theoretical (as is the case in many of the above-mentioned approaches). In practice, as we argue in (Viegas et al., 1996b), the costs of using LRs in knowledge acquisition must be carefully weighed against the benefits. The lexicon for which our LRs are introduced is intended mainly to support the computational specification and use of text meaning representations used in the KBMT engine, though the syntactic, morphological, pragmatic, stylistic and other information stored there will also aid other MT engines (see, e.g., Onyshkevych and Nirenburg, 1995). The acquisition of such a lexicon, with or without the LR mechanism, involves a substantial investment of resources. The LR processor applies to each word sen"
1997.tmi-1.1,C92-2070,0,0.0744322,"Missing"
1997.tmi-1.18,P96-1005,1,0.792955,"Missing"
1997.tmi-1.18,C92-2070,0,0.0485992,"s remarkably, they demand detailed knowledge of possible scenarios. It is often prohibitively expensive to acquire such knowledge for general purpose, domain independent NLP. None of these methods used a large scale ontology (because none was available). Nor did they show that they can resolve sense ambiguities in entire texts and at the same time produce complete meaning representations for the texts. We believe that Mikrokosmos is the first successful application of a knowledge-based method for large scale word sense disambiguation and text meaning representation. Statistical methods (e.g., Yarowsky (1992)), work well on carefully chosen domains and training corpora. However, they are not as effective for processing texts from a wide variety of domains in general. Moreover, statistical methods are attractive for solving individual problems such as word sense disambiguation or part of speech tagging. They do not explain why certain meanings were chosen or how the chosen meanings together provide a meaning for a whole sentence or text, something that is often required to carry out further processing (e.g., to generate the meaning in a target language for machine translation). 6 Conclusions Resolv"
1999.mtsummit-1.47,C88-1016,0,0.0604212,"nd Lehnert 1996) that an MT system providing adequate performance even for a single type of text should be considered useful. If an MT system - 324- uses a restricted sublanguage—and. thus, can operate with smaller-scale static knowledge sources—the scope of acquisition and development effort will decrease correspondingly. Indeed, practically all MT systems for special domains are usually (see, e.g.. Kukich 1983; Kittredge et al., 1986) built to conform to the constraints of a sublanguage. Massive attempts have been made in the past ten years or so to make MT systems fully automatic (e.g.. P. Brown et al., 1988). In practice, the state of the art in NLP suggests a mixture of automatic and manual methods for any realistic comprehensive application (e.g.. Paris et al., 1995: Nirenburg et al., 1996). Several modes of human-computer cooperation have been used in practice over the years. Our approach conforms to the human-aided machine translation (HAMT) paradigm (e.g., Kay 1973, see also Hutchins and Somers 1992 for a definition). We would also like to stress an additional important parameter of human-computer interaction in HAMT: initiative. Human-computer interaction in HAMT system could be initiated e"
1999.mtsummit-1.47,C86-1132,0,0.144061,"Missing"
1999.mtsummit-1.47,1996.amta-1.10,1,0.674619,"thus, can operate with smaller-scale static knowledge sources—the scope of acquisition and development effort will decrease correspondingly. Indeed, practically all MT systems for special domains are usually (see, e.g.. Kukich 1983; Kittredge et al., 1986) built to conform to the constraints of a sublanguage. Massive attempts have been made in the past ten years or so to make MT systems fully automatic (e.g.. P. Brown et al., 1988). In practice, the state of the art in NLP suggests a mixture of automatic and manual methods for any realistic comprehensive application (e.g.. Paris et al., 1995: Nirenburg et al., 1996). Several modes of human-computer cooperation have been used in practice over the years. Our approach conforms to the human-aided machine translation (HAMT) paradigm (e.g., Kay 1973, see also Hutchins and Somers 1992 for a definition). We would also like to stress an additional important parameter of human-computer interaction in HAMT: initiative. Human-computer interaction in HAMT system could be initiated either by the system or by the human (sometimes both modes are present in a single application). In our model, the initiative is predominantly, though not exclusively, with the system. In t"
1999.mtsummit-1.47,P98-2160,1,0.892189,"Missing"
1999.mtsummit-1.47,C98-2155,1,\N,Missing
1999.mtsummit-1.61,H92-1046,1,0.858726,"Missing"
1999.mtsummit-1.61,X96-1024,0,\N,Missing
1999.mtsummit-1.61,H92-1045,0,\N,Missing
2002.tmi-papers.4,P98-1012,0,0.027466,"em of finding relevant information concerning the state of a patient across several online medical documents. In a later work, McKeown, et. al., (1999) presented a more general approach which integrated other disciplines such as machine learning and statistical techniques combined with some linguistic features and also information fusion techniques, to select relevant phrases from the documents so they can be included in the final summary. Another approach to cross-document summarization uses cross document co-reference resolution to produce a summary out of a collection of related documents (Bagga and Baldwin, 1998). Figure 4. An Arabic text and its translation into English using a simple translation system. The current implementation of the system takes as input a person&apos;s name in English, Spanish, and/or Russian. At present the user must supply the names (and morphological variants) in each language. Additional search terms can be added to further constrain the search. A search is then carried out on a selected web search engine and the user can see the type of documents being found. If the search is successful then the user initiates generation of the personal profile. The main experiment described be"
2002.tmi-papers.4,C98-1012,0,\N,Missing
2005.mtsummit-papers.9,C94-1012,0,0.0333909,"m. For example, in the text box in Figure 1, notice that the subject “you” is included in the imperative. For content questions, an appropriate pronoun such as “who” or “where” is placed in the clause constituent that is being questioned. Every event is propositionalized. We eliminate most figurative language, except when it has theological implications. Specifically, we eliminate most instances of metonymy, synecdoche, euphemisms and idioms, and metaphors are converted to similes and the point of similarity is supplied. Other standard restrictions (like those described for the Kant system in Baker et al., 1994 and Mitamura, 1991) are employed, such as disallowing reduced relative clauses. Target Language Text Generation In this section we discuss the target language knowledge acquisition process along with a brief overview of the generation process. The knowledge acquisition interface and the text generator are integrated into The Bible Translator’s Assistant (TBTA). TBTA has been tested for English, Korean, Jula (spoken in West Africa) and Kewa (a clause chaining language spoken in Papua New Guinea). Korean, Jula and Kewa differ conceptually and structurally from English, yet in all cases the gene"
2005.mtsummit-papers.9,1991.mtsummit-papers.9,0,\N,Missing
A92-1045,P87-1005,0,0.0276231,"achine Translation of Carnegie Mellon University, capable of supporting a number of dedicated workstation configurations. Among the types of end users whom this system will benefit are technical writers, text revisors and translators. In the framework of NLP system development this tool supports dictionary and ontology acquisition. A number of separate functionalities included in this system have been developed and used either in commercial word processing software packages or in NLP projects (e.g., the translator&apos;s tools described in Macklovitch, 1989, ; and the developer environments IRACQ (Ayuso et al., 1987), LUKE (Wrobiewski and Rich, 1988) or ONTOS (Monarch and Nirenburg, 1989), among many others). Our system allows a merge of the two directions in the tool development. One direct reason to put the two previously separate kinds of functionality into a single system was to support the knowledge-based machineaided translation environment which involves an interactive human editor who uses an interface to help the machine understand the source text. A standard Unix- and X-windows-based workstation platform has been selected for our system, whose working name is Tws, for ""Translator&apos;s Workstation."""
A92-1045,A88-1026,0,0.0302977,"Missing"
A94-1016,J90-2002,0,0.0615918,"Missing"
A94-1016,H93-1038,1,0.74872,"has produced. It uses dynamic programming to efficiently compare weighted averages of sets of adjacent scored component translations. The current system operates primarily in a human-aided MT mode. The translation delivery system and its associated post-editing aide are briefly described, as is an initial evaluation of the usefulness of this method. Individual M T engines will be reported separately and are not, therefore, described in detail here. 2 INTEGRATING MULTI-ENGINE OUTPUT In our experiment we used three MT engines: * a knowledge-based MT (KBMT) system, the mainline Pangloss engine (Frederking et al., 1993b); • an example-based MT (EBMT) system (see (Nirenburg et al., 1993; Nirenburg et al., 1994b); the original idea is due to Nagao (Nagao, 1984)); and • a lexical transfer system, fortified with morphological analysis and synthesis modules and relying on a number of databases - - a 95 machine-readable dictionary (the Collins Spanish/English), the lexicons used by the K B M T modules, a large set of user-generated bilingual glossaries as well as a gazetteer and a list of proper and organization names. on average thirteen years o[ flight (time). This is a sentence from one of the 1993 ARPA MT eva"
A94-1016,E93-1062,1,0.743153,"has produced. It uses dynamic programming to efficiently compare weighted averages of sets of adjacent scored component translations. The current system operates primarily in a human-aided MT mode. The translation delivery system and its associated post-editing aide are briefly described, as is an initial evaluation of the usefulness of this method. Individual M T engines will be reported separately and are not, therefore, described in detail here. 2 INTEGRATING MULTI-ENGINE OUTPUT In our experiment we used three MT engines: * a knowledge-based MT (KBMT) system, the mainline Pangloss engine (Frederking et al., 1993b); • an example-based MT (EBMT) system (see (Nirenburg et al., 1993; Nirenburg et al., 1994b); the original idea is due to Nagao (Nagao, 1984)); and • a lexical transfer system, fortified with morphological analysis and synthesis modules and relying on a number of databases - - a 95 machine-readable dictionary (the Collins Spanish/English), the lexicons used by the K B M T modules, a large set of user-generated bilingual glossaries as well as a gazetteer and a list of proper and organization names. on average thirteen years o[ flight (time). This is a sentence from one of the 1993 ARPA MT eva"
A94-1016,1993.tmi-1.4,1,0.751309,"Missing"
C00-2147,X98-1023,1,\N,Missing
C00-2168,H90-1055,0,0.0290194,"it on the example of the syntax module of the Boas knowledge elicitation system for a quick ramp up of a standard transfer-based machine translation system from any language into English (Nirenburg 1998). This work is a part of an ongoing project devoted to the creation of resources for NLP by eliciting knowledge from informants. 1 Other Work on Syntax Acquisition Experiments in “single-step” automatic acquisition of knowledge have been among the most fashionable topics in NLP over the past decade. One can mention work on automatic acquisition of phrase structure using distribution analysis (Brill et al 1990). The problems with Sergei NIRENBURG Computing Research Laboratory New Mexico State University Las Cruces, NM, USA 88003 sergei@crl.nmsu.edu the current fully automatic corpus-based approaches include difficulties of maintaining any system based on them, due to the opaqueness of the method and the data to the language engineer. At the present time, the most promising NLP systems include elements of both corpus-based and human knowledge-based methods. One example is acquisition of Twisted Pair Grammar (Jones and Havrilla 1998) for a pair of English and a source language (SL). Another example of"
C00-2168,J90-2002,0,0.0965686,"approach (Chomsky 1981) and its successors (Chomsky 1995). Widely devised as the basis of universal grammar, the principlesand-parameters approach has focused on the universality of certain formal grammatical rules within that particular approach rather on the substantive and exhaustive list of universal parameters, a subset of which is applicable to each natural language, along with their corresponding sets of values, such as a parameter set of nominal cases. In some other approaches, parameters and parameter values are either not sought out or are expected to be obtained automatically (e.g. Brown et al. 1990; Goldstein 1998), and, while holding promise for the future as a potential component of an elicitation system, cannot, at this time, form the basis of an entire system of this kind. In order to ensure uniformity and systematicity of operation of a language knowledge elicitation system, such as Boas, it is desirable to come up with a comprehensive list of all possible parameters in natural languages and, for each such parameter, to create a cumulative list of its possible values in all the languages that Boas can expect as SLs. Three basic methodological approaches are used in Boas. Expectatio"
C00-2168,jones-havrilla-1998-twisted,0,\N,Missing
C00-2168,J01-2001,0,\N,Missing
C86-1043,1985.tmi-1.15,1,0.446152,"Missing"
C86-1043,C86-1148,1,0.878293,"Missing"
C86-1043,C86-1079,1,\N,Missing
C86-1043,J78-3021,0,\N,Missing
C86-1043,T78-1014,0,\N,Missing
C86-1079,1985.tmi-1.17,1,0.882073,"Missing"
C86-1148,1985.tmi-1.4,0,0.0177672,"accommodated by this philosophy, but only as a temporary measure. Interactive modules will be phlgged into the system pendhlg the develop ment of autonlatic modules for perfbrnling tile various tasks as well as more powerful inference engines and representation schemata. This is a device that facilitates early testing of a system Even tlefbre all the modules are actually built. Another advantage of this strategy is that the systnlu becomes ' d y n a m i c ' , in the sense that its knowledge is growing with use. This strategy is an exteusion of one of the approaches discussed, for example, in Carbonell and Tomita (1985) since it implies knowledge acquisition during the exploitation stage a/~d also involves a broader class of texts as its inlnlt. Johnson and Whitelock (1985) are also proponents of the interactive approach, lint their motivation is different, in that they perceive the human to be an integral part of their system even in its final incarnation. In any case, interactlvity is not tile central design feature of TRANSLATOR. Before proceeding to describe the knowledge chlsters in TItANSI.A.. ""fOR we would like to colnnlent very briefly on a n u m b e r of methodologi cal points concerning M T researc"
C88-2100,P84-1107,0,0.0496614,"Missing"
C88-2100,P84-1105,0,0.0649958,"Missing"
C88-2100,C86-1148,1,\N,Missing
C88-2100,C86-1131,0,\N,Missing
C88-2100,H86-1023,0,\N,Missing
C88-2100,C86-1154,0,\N,Missing
C90-3039,J87-3007,1,\N,Missing
C90-3039,C88-2100,1,\N,Missing
C94-1019,H94-1019,0,0.0600941,"Missing"
C94-1019,1992.tmi-1.12,0,0.10997,"Missing"
C94-1019,J85-1002,0,0.0514431,"Missing"
C94-1019,1992.tmi-1.14,0,0.0240526,".,ricnted and do not aim ill advltnch]g our knowledge itl&apos;Joul either basic tnechaI l i S l l l S o f text coralschess;on and production O f C O I l l ] ) t l l e l + models shnHhlling stich i11e.challiSlllS. Tile lwo lllOSl recently popular techncd o~,{ical parndit, ms ill machine t r a n s l a t i o n - - - e×ample-I&apos;~ased Iranslalion (EBMT) and stalisliCSdlased transhlthm (SIIM&apos;f) - - - r e quire linguistic knowledge only :is an aflerlhollghl. While the represenlatives of the above paradigms are still al lhc stage, of e.ilher building toy systems (e.g., Furuse and litht, 1992; McLean, 1992,Jones, 1992, Maruyama and Wltlan,aim, 1992) or struggling with tile natural constraints olal&gt; proaches that eschew the Sttldy ol"" langual.,e ;is such (e.g., Brown et at., 1990), .it number of llropi`&apos;sals have come up lor some hybridization oF M&apos;I: [n some such .aplnO&apos;,tches, tJ I( Corl)llS analysis is tised (&apos;Ill"" ltlllhl]:{ analysis ;lid Ii.{uIsfcr grammars (e.t;., Su and (&apos;hang, 1992). Ill olhcrs, a standlu&apos;d tr:msfcx-I&apos;~L~ed aPl/rtmch (TBMT) is followed usiny, hadilh/nal analysis and generalhm technhlueS bul havin!,, a IranslEr component Imscd on aligned I,ilingual corp(ira ((lrishnmn and Kosnka, I"")"
C94-1019,1992.tmi-1.21,0,0.0239964,"Missing"
C94-1019,1992.tmi-1.15,0,0.0217843,"Missing"
C94-1019,1992.tmi-1.4,0,0.0189808,"Missing"
C94-1019,1992.tmi-1.22,0,0.0307214,"Missing"
C94-1019,C88-1016,0,\N,Missing
C94-1019,J90-2002,0,\N,Missing
C94-1019,1992.tmi-1.23,0,\N,Missing
C94-1057,W91-0202,1,\N,Missing
C94-1057,J94-4004,0,\N,Missing
C96-1016,X93-1018,0,0.0261149,"Missing"
C98-2155,1993.tmi-1.14,0,0.0348354,"er values have an associated set of realization options in each language. For instance, the parameter of gender in Ukrainian is described as follows: both the nature of the parameters it would be using and their inventory has to be developed in-house. In order to define a set of parameters for Boas, it is essential to distinguish among the language phenomena that should be accorded the status of parameter and those that should be understood as parameter values or their realizations. Still other phenomena may remain, at least for the task at hand, outside the parameter system. We believe, with Dorr (1993), that parameters may be understood as building blocks of an interlingua in MT. We reserve judgment about whether every component of an interlingua is by definition parametric 3. language: Ukrainian parameter: gender domain: nouns, adjectives, possessives (head agreement), verbs in past tense range (parameter values): masculine, feminine, neuter realization: [gender markers in lexicon for nouns; inflection paradigms for adjectives, possessives and verbs in past tense] For comparison, the Hebrew gender is described differently: Thus, the parameter ""lexical category"" has a range of values {V, N,"
C98-2211,P89-1010,0,0.0506968,"Missing"
C98-2211,W94-0311,1,0.90354,"e Computational Semantic Approach 3 In order to account for the continuum we find in natural languages, we argue for a continuum perspective, spanning the range fl'oln free-combining words to idioms, with semantic collocations and idiosyncrasies in between as defined in (Viegas and B o u i l h m , 1994): • • i d i o s y n c r a s i e s (large coke; green jealousy) • i d i o m s (to kick the (proverbial) bucket) Formally, we go from a purely compositional approach in ""free-combining words"" to a noncompositional approach in idioms, In between, a (semi-)compositional approach is still possible. (Viegas and Bouillon, 1994) showed t h a t we can reduce the set of what are conventionally considered as idiosyncrasies by difl>rentiating ""true"" idiosyncrasies (difficult to derive or calculate) from expressions which have well-defined calculi, being compositional in nature, and t h a t have been called semantic collocations. In this paper, we further distinguish their idiosyncrasies into: • • co-occ'arrcncc L o c - i n ( ( l i s t a n c e ) = at a distance The M T T approach is very interesting as it provides a model of production well suited for generation with its different s t r a t a and also a lot of lexiealsema"
C98-2211,P96-1005,1,0.833826,"ween different classes of cooccurrences (modulo presence of derived forms in the lexicon with same or subsumed semantics). Looking at the following example, A bitter heavy big + N <=> resentment smoker eater V resent smoke eat + V + Adv <=> Adv + oppose strongly strongly oblige morally morally Adv bitterly heavily *bigly Adj-ed opposed obliged we see that after having acquired with human intervention co-occurrences belonging to tile A + N class, we can use lexical rules to deriw; the V + Adv class and also Adv + Adj-ed class. Lexical rules are a useful conceptual tool to extend a dictionary. (Viegas et al., 1996) used derivational lexical rules to extend a Spanish lexicon. We apply their approach to the production of restricted semantic co-occurrences. Note t h a t eat bi91y will be produced but then rejected, as the form bigly does not exist in a dictionary, The rules overgenerate cooccurrences. This is a minor problem for analysis than for generation. To use these derived restricted co-occurrences in generation, the output of the lexical rule processor nmst be checked. This can be (ton(; in different ways: dictionary check, corpus check and ultimately human check. Other classes, such as the ones bel"
C98-2211,J93-1007,0,\N,Missing
C98-2211,C88-2100,1,\N,Missing
E93-1062,A92-1045,1,0.792096,"Missing"
H92-1052,P88-1018,0,0.0503226,"Missing"
H92-1052,J89-3002,0,\N,Missing
H93-1038,1991.mtsummit-papers.13,0,0.1541,"r o d u c t i o n Fully automated machine translation of unconstrained texts is beyond the state of the art today. The need for mechanizing the translation process is, however, very urgent. It is desirable, therefore, to seek ways o f both speeding up the process of translating texts and making it less expensive. In this paper we describe an environment that facilitates the integration of automatic machine translation (MT) and machine-aided translation (MAT). station provides the user interface and the integration platform. It is similar in spirit to systems such as the Translator's Workbench[3]. The processing in PANGLOSSgoes as follows: 1. an input passage is broken into sentences; 2. a fully-automated translation o f each full sentence is attempted; if it fails, then 3. a fully-automated translation o f smaller chunks of text is attempted (currently, these are noun phrases); . the material that does not get covered by noun phrases is treated in a &quot;word-for-word&quot; mode, whereby translation suggestions for each word (or phrase) are sought in the system's MT lexicons, an online bilingual dictionary, and a set o f user-Supplied glossaries; . The resulting list o f translated noun phras"
H93-1038,A92-1045,1,0.826242,"ranslation o f smaller chunks of text is attempted (currently, these are noun phrases); . the material that does not get covered by noun phrases is treated in a &quot;word-for-word&quot; mode, whereby translation suggestions for each word (or phrase) are sought in the system's MT lexicons, an online bilingual dictionary, and a set o f user-Supplied glossaries; . The resulting list o f translated noun phrases and translation suggestions for words and phrases is displayed in a special editor window, where the human user finalizes the translation. This environment, called the Translator's Workstation (TWS)[5], has been developed in the framework of the PANGLOSS machine translation project. 1 The main goal of this project is to develop a system that will, from the very beginning, produce high-quality output. This can only be attained currently by keeping the human being in the translation loop. The main measure of progress in the development of the Pangloss system is the gradual increase in the level of automation. This entire process can be viewed as helping a human translator, by doing parts of the job automatically and making the rest less time-consuming. We have designed and implemented an inte"
H94-1026,E93-1062,1,0.856241,"e 7: Chart-walk algorithm who that whom which ""were have"" have ""have got"" ""were possess"" possess ""were hold"" hold ""hold on to"" ""hold up"" ""were grasp"" in on onto at by average middle mid-point NIL years of from about for by flight ""to dash off"" ""to clear off"" ""to leave the parental nest"" ""spread one&apos;s wings"" ""to overhear sth in passing"" ""to catch on immediately"" ""get it at once"" ""to be pretty smart"" &apos;flight feathers"" calculated as a weighted average of the scores in the row to its left, in the column below it and the previous contents of the array cell for its position. So to calculate element (1,4), we compare the combined scores of the best walks over (1,1) and (2,4), (1,2) and (3,4), and (1,3) and (4,4) with the scores of any chart edges going from 1 to 4, and take the maximum. When the score in the top-right comer is produced, the algorithm is finished, and the associated set of edges is the final chart-walk result. It may seem that the scores should increase towards the top-right comer. In our experiment, howevel~ this has not generally been the case. Indeed, the system suggested a number of high-scoring short edges, but many low-scoring edges had to be included to span the entire i"
H94-1026,1993.tmi-1.4,1,0.813047,"g simultaneously on the same text, the overall quality will improve. Using this novel approach to MT in the latest version of the Pangloss MT project, we submit an input text to a battery of machine translation systems (engines), collect their (possibly, incomplete) results in a joint chart-like data structure and select the overall best translation using a set of simple heuristics. 2. INTEGRATING MULTI-ENGINE OUTPUT The MT configuration in our experiment used three MT engines: 147 • a knowledge-based MT (K.BMT) system, the mainline Pangloss engine[l]; • an example-based MT (EBMT) system (see [2, 3]; the original idea is due to Nagao[4]); and • a lexical transfer system, fortified with morphological analysis and synthesis modules and relying on a number of databases - a machine-readable dictionary (the Collins Spanish/English), the lexicons used by the KBMT modules, a large set of usergenerated bilingual glossaries as well as a gazetteer and a List of proper and organization names. The results (target language words and phrases) were recorded in a chart whose initial edges corresponded to words in the source language input. As a result of the operation of each of the MT engines, new edge"
H94-1026,2002.tmi-tutorials.1,0,0.0310161,"g simultaneously on the same text, the overall quality will improve. Using this novel approach to MT in the latest version of the Pangloss MT project, we submit an input text to a battery of machine translation systems (engines), collect their (possibly, incomplete) results in a joint chart-like data structure and select the overall best translation using a set of simple heuristics. 2. INTEGRATING MULTI-ENGINE OUTPUT The MT configuration in our experiment used three MT engines: 147 • a knowledge-based MT (K.BMT) system, the mainline Pangloss engine[l]; • an example-based MT (EBMT) system (see [2, 3]; the original idea is due to Nagao[4]); and • a lexical transfer system, fortified with morphological analysis and synthesis modules and relying on a number of databases - a machine-readable dictionary (the Collins Spanish/English), the lexicons used by the KBMT modules, a large set of usergenerated bilingual glossaries as well as a gazetteer and a List of proper and organization names. The results (target language words and phrases) were recorded in a chart whose initial edges corresponded to words in the source language input. As a result of the operation of each of the MT engines, new edge"
H94-1026,H93-1038,1,0.886372,"ntal substring order differences. Following a venerable tradition in MT, we used a target language-dependent set of postprocessing rules to alleviate this problem (e.g., by switching the order of adjectives and nouns in a noun phrase if it was produced by the word-for-word engine). 3. TRANSLATION DELIVERY SYSTEM Results of multi-engine MT were fed in our experiment into a translator&apos;s workstation (TWS)[5], through which a translator either approved the system&apos;s output or modified it. The main option for human interaction in TWS currently is the Component Machine-Aided Translation (CMAT) editor[6]. A view of this editor is presented in Figure 9. (The user can see the original source language text in another editor window.) The user can use menus, function keys and mouse clicks to change the system&apos;s initially chosen candidate trans0 0 i 2 3 4 5 6 7 8 9 tO 11 12 13 14 15 16 17 18 19 20 21 22 5 1 I0 2.5 2 3 4 5 7.3 6.75 6.4 5.6 2.25 3.16 3.62 3.3 2 3.5 4.0 3.5 5 5.0 4.0 5 3.5 2 6 7 8 9 5.57 3.58 3.8 4.25 4.0 3.5 5 5.5 3.78 4.0 4.4 4.25 4.0 5.0 5 5.1 3.56 3.71 4.0 3.8 3.5 4.0 3.5 2 5.1 3.72 3.87 4.14 4.0 3.8 4.25 4.0 3.5 5 I0 ii 6.0 5.66 4.85 4.59 5 . 1 1 4.8 5.5 5.11 5.57 5.13 5.66 5.14"
J87-3007,P85-1033,0,0.065608,"large-scale applications. We describe the interactive concept lexicon acquisition module of the LMS for TRANSLATOR, a knowledge-based, sublanguage-oriented machine translation project. This project belongs to two of the fastest growing fields of computational linguistics and artificial intelligence in general: the lexicon and knowledge acquisition for AI systems. The work in lexicon has centered on a) studies concerned with the utilization of conventional humanoriented dictionaries, newly available in machine-readable form, for computational tasks (e.g., Amsler 1984a,b; Chodorow et al. 1985; Ahlswede 1985; Markowitz et al. 1986) improving the ancillary capabilities for lexicon systems, such as, for instance, morphological processors and descriptions (e.g., Nirenburg and Ben Asher 1984; Byrd et al. 1986; Boguraev et al. 1987); c) hand-building of lexicons necessary for natural language systems, often with considerations about extensibility (e.g., Zernik and Dyer 1985; Bessemer and Jacobs 1987). An interesting perspective on the field is given in Miller (1985). Knowledge acquisition is a central topic in AI and expert systems. A number of systems exist for assistance in acquiring specialized kno"
J87-3007,P84-1094,0,0.0258231,"s necessary for lexicon acquisition in large-scale applications. We describe the interactive concept lexicon acquisition module of the LMS for TRANSLATOR, a knowledge-based, sublanguage-oriented machine translation project. This project belongs to two of the fastest growing fields of computational linguistics and artificial intelligence in general: the lexicon and knowledge acquisition for AI systems. The work in lexicon has centered on a) studies concerned with the utilization of conventional humanoriented dictionaries, newly available in machine-readable form, for computational tasks (e.g., Amsler 1984a,b; Chodorow et al. 1985; Ahlswede 1985; Markowitz et al. 1986) improving the ancillary capabilities for lexicon systems, such as, for instance, morphological processors and descriptions (e.g., Nirenburg and Ben Asher 1984; Byrd et al. 1986; Boguraev et al. 1987); c) hand-building of lexicons necessary for natural language systems, often with considerations about extensibility (e.g., Zernik and Dyer 1985; Bessemer and Jacobs 1987). An interesting perspective on the field is given in Miller (1985). Knowledge acquisition is a central topic in AI and expert systems. A number of systems exist for"
J87-3007,P84-1093,0,0.0254209,"s necessary for lexicon acquisition in large-scale applications. We describe the interactive concept lexicon acquisition module of the LMS for TRANSLATOR, a knowledge-based, sublanguage-oriented machine translation project. This project belongs to two of the fastest growing fields of computational linguistics and artificial intelligence in general: the lexicon and knowledge acquisition for AI systems. The work in lexicon has centered on a) studies concerned with the utilization of conventional humanoriented dictionaries, newly available in machine-readable form, for computational tasks (e.g., Amsler 1984a,b; Chodorow et al. 1985; Ahlswede 1985; Markowitz et al. 1986) improving the ancillary capabilities for lexicon systems, such as, for instance, morphological processors and descriptions (e.g., Nirenburg and Ben Asher 1984; Byrd et al. 1986; Boguraev et al. 1987); c) hand-building of lexicons necessary for natural language systems, often with considerations about extensibility (e.g., Zernik and Dyer 1985; Bessemer and Jacobs 1987). An interesting perspective on the field is given in Miller (1985). Knowledge acquisition is a central topic in AI and expert systems. A number of systems exist for"
J87-3007,P87-1005,0,0.0136227,"cal processors and descriptions (e.g., Nirenburg and Ben Asher 1984; Byrd et al. 1986; Boguraev et al. 1987); c) hand-building of lexicons necessary for natural language systems, often with considerations about extensibility (e.g., Zernik and Dyer 1985; Bessemer and Jacobs 1987). An interesting perspective on the field is given in Miller (1985). Knowledge acquisition is a central topic in AI and expert systems. A number of systems exist for assistance in acquiring specialized knowledge for natural language processing. Thus, TEAM (Grosz et al. 1985), TEL1 (Ballard and Stumberger, 1986), IRACQ (Ayuso et al. 1987) and some others are devoted to facilitating customizations in the framework of database query systems; INKA (Phillips et al. 1986) and CYC (Lenat et al. 1986) are examples of knowledge acquisition systems aimed at expert system design. We would like to address some important methodological and strategic points in providing the lexicon support in the context of a text processing system, such as the machine translation system TRANSLATOR (Nirenburg et al. 1987). 1. NEED H E L P BUILDING LEXICON A natural language lexicon is a necessary part of any natural language processing (NLP) system. Buildi"
J87-3007,P87-1026,0,0.0261973,"con has centered on a) studies concerned with the utilization of conventional humanoriented dictionaries, newly available in machine-readable form, for computational tasks (e.g., Amsler 1984a,b; Chodorow et al. 1985; Ahlswede 1985; Markowitz et al. 1986) improving the ancillary capabilities for lexicon systems, such as, for instance, morphological processors and descriptions (e.g., Nirenburg and Ben Asher 1984; Byrd et al. 1986; Boguraev et al. 1987); c) hand-building of lexicons necessary for natural language systems, often with considerations about extensibility (e.g., Zernik and Dyer 1985; Bessemer and Jacobs 1987). An interesting perspective on the field is given in Miller (1985). Knowledge acquisition is a central topic in AI and expert systems. A number of systems exist for assistance in acquiring specialized knowledge for natural language processing. Thus, TEAM (Grosz et al. 1985), TEL1 (Ballard and Stumberger, 1986), IRACQ (Ayuso et al. 1987) and some others are devoted to facilitating customizations in the framework of database query systems; INKA (Phillips et al. 1986) and CYC (Lenat et al. 1986) are examples of knowledge acquisition systems aimed at expert system design. We would like to address"
J87-3007,P87-1027,0,0.0484173,"Missing"
J87-3007,P86-1019,0,0.0194646,"belongs to two of the fastest growing fields of computational linguistics and artificial intelligence in general: the lexicon and knowledge acquisition for AI systems. The work in lexicon has centered on a) studies concerned with the utilization of conventional humanoriented dictionaries, newly available in machine-readable form, for computational tasks (e.g., Amsler 1984a,b; Chodorow et al. 1985; Ahlswede 1985; Markowitz et al. 1986) improving the ancillary capabilities for lexicon systems, such as, for instance, morphological processors and descriptions (e.g., Nirenburg and Ben Asher 1984; Byrd et al. 1986; Boguraev et al. 1987); c) hand-building of lexicons necessary for natural language systems, often with considerations about extensibility (e.g., Zernik and Dyer 1985; Bessemer and Jacobs 1987). An interesting perspective on the field is given in Miller (1985). Knowledge acquisition is a central topic in AI and expert systems. A number of systems exist for assistance in acquiring specialized knowledge for natural language processing. Thus, TEAM (Grosz et al. 1985), TEL1 (Ballard and Stumberger, 1986), IRACQ (Ayuso et al. 1987) and some others are devoted to facilitating customizations in the"
J87-3007,P84-1095,0,0.0459119,"1960; Grimes 1970; Kucera 1969; Venezky 1973). A more feasible scenario, however, is partial computational assistance for lexicology, probably along the lines suggested in this paper, as well as utilizing such new and promising resources as on-line dictionaries. Recent work on machine-readable dictionaries offers new and interesting possibilities both for the computerassisted lexicology (see Walker 1984) and for constructing lexical databases derived from the definitions in machine-readable dictionaries and utilized in NLP along with other fields (see Amsler 1982, 1984a; Walker, Amsler 1986, Calzolari 1984a,b). The premises and goals of these efforts are fully compatible with our belief, first, that no AI system is ready to make the kind of decisions that lexicon building requires and, second, that 'simply having an online version of an encyclopedia [or a dictionary] would be of little use, as there is practically nothing that current AI could draw from the raw text. Rather, we must carefully re-represent the encyclopedia's knowledge - - by hand - - into some more structured form' (Lenat et al. 1986:75). Such re-representation would be necessary for Amsler's (1984a:458) 'lexical knowledge base"
J87-3007,P85-1008,0,0.0138305,"types and concept tokens. Such 280 a distinction is common in certain knowledge representation languages (see Brachman and Schmolze 1985; Nirenburg et al. 1986). Concept types belong in the lexicon; concept tokens are instantiations of concept types obtained as a result of analyzing natural language inputs. One kind of knowledge representation can be used for both types and tokens, a frame-based notation being one of them. (Of course, due to a number of possible reasons, a complete knowledge representation system can use one type of representation for the types and another for the tokens, cf. Hobbs 1985 for a discussion of' such a position.) The frames for a type and its token will not, however, be identical in structure. The semantic content of slots in a lexicon (type) frame is different fi:om that of the corresponding slots in the text (token) frame. Concept tokens have their slots occupied by actual values of properties; if information about a property is not forthcoming, then the default value (if any) is inherited from the corresponding type representations. For example, the frame for a verbal action type and a verbal action token can have a slot named 'agent.' However, in the former c"
J87-3007,P86-1018,0,0.0118598,"plications. We describe the interactive concept lexicon acquisition module of the LMS for TRANSLATOR, a knowledge-based, sublanguage-oriented machine translation project. This project belongs to two of the fastest growing fields of computational linguistics and artificial intelligence in general: the lexicon and knowledge acquisition for AI systems. The work in lexicon has centered on a) studies concerned with the utilization of conventional humanoriented dictionaries, newly available in machine-readable form, for computational tasks (e.g., Amsler 1984a,b; Chodorow et al. 1985; Ahlswede 1985; Markowitz et al. 1986) improving the ancillary capabilities for lexicon systems, such as, for instance, morphological processors and descriptions (e.g., Nirenburg and Ben Asher 1984; Byrd et al. 1986; Boguraev et al. 1987); c) hand-building of lexicons necessary for natural language systems, often with considerations about extensibility (e.g., Zernik and Dyer 1985; Bessemer and Jacobs 1987). An interesting perspective on the field is given in Miller (1985). Knowledge acquisition is a central topic in AI and expert systems. A number of systems exist for assistance in acquiring specialized knowledge for natural langu"
J87-3007,P85-1038,0,0.023492,"umanoriented dictionaries, newly available in machine-readable form, for computational tasks (e.g., Amsler 1984a,b; Chodorow et al. 1985; Ahlswede 1985; Markowitz et al. 1986) improving the ancillary capabilities for lexicon systems, such as, for instance, morphological processors and descriptions (e.g., Nirenburg and Ben Asher 1984; Byrd et al. 1986; Boguraev et al. 1987); c) hand-building of lexicons necessary for natural language systems, often with considerations about extensibility (e.g., Zernik and Dyer 1985; Bessemer and Jacobs 1987). An interesting perspective on the field is given in Miller (1985). Knowledge acquisition is a central topic in AI and expert systems. A number of systems exist for assistance in acquiring specialized knowledge for natural language processing. Thus, TEAM (Grosz et al. 1985), TEL1 (Ballard and Stumberger, 1986), IRACQ (Ayuso et al. 1987) and some others are devoted to facilitating customizations in the framework of database query systems; INKA (Phillips et al. 1986) and CYC (Lenat et al. 1986) are examples of knowledge acquisition systems aimed at expert system design. We would like to address some important methodological and strategic points in providing th"
J87-3007,1985.tmi-1.15,1,0.721427,"ords and phrases. In reality, AI systems deal not with the entire world but typically with a well-specified subworld of it. In other words, in any practical application, the world concept lexicon will, in fact, be a subworld concept lexicon. In the area of machine translation the analysis and generation lexicons involve two different natural languages, which creates the task of building such resources as, say, an E n g l i s h - subworld and subworld - - Russian lexicons, as is the case, for example, in TRANSLATOR, the knowledge-based machine translation project for the computer subworld (see Nirenburg et al. 1985, 1986, 1987). 2. WORLD FIRST, WORD LATER We assert that of the three lexicons described above, the first to be built must be the subworld concept lexicon. The availability of such a lexicon is a s i n e q u a n o n for any subsequent lexical work in NLP. The recognized necessity to describe the concept lexicon prior to dealing with natural languages is a feature which clearly distinguishes our approach from Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 277 Sergei Nirenburg and Victor Raskin The Subworld Concept Lexicon and the Lexicon Management System other work on le"
J87-3007,C86-1148,1,0.795448,"y any number of members of the corresponding domain, and the logical operators and, or, and not can be used to augment the expressive power. Also, in every case, the semantics of the constraints in the lexicon is that of default knowledge: the contents of a slot are understood as typically constraining the meaning of the concept. The semantic character of a slot in a concept lexicon frame underscores a careful distinction that should be made between concept types and concept tokens. Such 280 a distinction is common in certain knowledge representation languages (see Brachman and Schmolze 1985; Nirenburg et al. 1986). Concept types belong in the lexicon; concept tokens are instantiations of concept types obtained as a result of analyzing natural language inputs. One kind of knowledge representation can be used for both types and tokens, a frame-based notation being one of them. (Of course, due to a number of possible reasons, a complete knowledge representation system can use one type of representation for the types and another for the tokens, cf. Hobbs 1985 for a discussion of' such a position.) The frames for a type and its token will not, however, be identical in structure. The semantic content of slot"
J87-3007,P85-1035,0,0.0218136,"tems. The work in lexicon has centered on a) studies concerned with the utilization of conventional humanoriented dictionaries, newly available in machine-readable form, for computational tasks (e.g., Amsler 1984a,b; Chodorow et al. 1985; Ahlswede 1985; Markowitz et al. 1986) improving the ancillary capabilities for lexicon systems, such as, for instance, morphological processors and descriptions (e.g., Nirenburg and Ben Asher 1984; Byrd et al. 1986; Boguraev et al. 1987); c) hand-building of lexicons necessary for natural language systems, often with considerations about extensibility (e.g., Zernik and Dyer 1985; Bessemer and Jacobs 1987). An interesting perspective on the field is given in Miller (1985). Knowledge acquisition is a central topic in AI and expert systems. A number of systems exist for assistance in acquiring specialized knowledge for natural language processing. Thus, TEAM (Grosz et al. 1985), TEL1 (Ballard and Stumberger, 1986), IRACQ (Ayuso et al. 1987) and some others are devoted to facilitating customizations in the framework of database query systems; INKA (Phillips et al. 1986) and CYC (Lenat et al. 1986) are examples of knowledge acquisition systems aimed at expert system desig"
J87-3007,P85-1037,0,\N,Missing
J87-3007,P86-1005,0,\N,Missing
J87-3007,P84-1036,0,\N,Missing
mcshane-etal-2004-meaning,W03-0904,1,\N,Missing
nirenburg-etal-2004-rationale,W98-0703,0,\N,Missing
nirenburg-etal-2004-rationale,W98-0707,0,\N,Missing
nirenburg-etal-2004-rationale,W98-0705,0,\N,Missing
nirenburg-etal-2004-rationale,W03-0904,1,\N,Missing
nirenburg-etal-2004-rationale,P98-1013,0,\N,Missing
nirenburg-etal-2004-rationale,C98-1013,0,\N,Missing
nirenburg-etal-2004-rationale,P03-2030,0,\N,Missing
nirenburg-etal-2004-rationale,W98-0701,0,\N,Missing
P17-3019,W12-4705,0,0.0230232,"ary provenance (syntax, reference, coherence, etc.) in its scoring mechanism for disambiguation and to steer the search process towards the promising region of the search space. This allows it to focus on exploring conceptually plausible interpretations while being less sensitive to upstream noise. The algorithm is opportunistic as the overlapping lattice representation allows it to simultaneously pursue multiple hypotheses, while the explicit evaluation of the candidates is deferred until a promising solution is found. Incremental semantic parsing has come back to the forefront of NLU tasks (Peldszus and Schlangen, 2012), (Zhou et al., 2016). Accounting for incrementality helps to make dialogues more human-like by accommodating sporadic asynchronous feedback as well as corrections, wedged-in questions, etc. (Skantze and Schlangen, 2009). In addition, incrementality of meaning representation construction coupled with sensory grounding has been shown to help prune the search space and reduce ambiguity during semantic parsing (Kruijff et al., 2007). The presented algorithm is incremental as it proceeds via a series of operations gradually extending and deepening the lattice structure containing candidate solutio"
P17-3019,roark-etal-2006-sparseval,0,0.0314981,"in the input. It is shown in the table both as a tree and as a highlighted fragment of the solution lattice. 3 or production rules to cover all possible realizations). Second and more importantly, the feasibility of a semantic interpretation becomes conditional on the well-formedness of the language input and the correctness of the corresponding syntactic parse. This requirement seems unnecessarily strong considering a) human outstanding ability to make sense of ungrammatical and fragmented speech (Kong et al., 2014) and b) still considerably high error rates of ASR transcription and parsing (Roark et al., 2006). The algorithm we present, by contrast, operates directly over the meaning representation space, while relying on heuristics of arbitrary provenance (syntax, reference, coherence, etc.) in its scoring mechanism for disambiguation and to steer the search process towards the promising region of the search space. This allows it to focus on exploring conceptually plausible interpretations while being less sensitive to upstream noise. The algorithm is opportunistic as the overlapping lattice representation allows it to simultaneously pursue multiple hypotheses, while the explicit evaluation of the"
P17-3019,E09-1085,0,0.028073,"tually plausible interpretations while being less sensitive to upstream noise. The algorithm is opportunistic as the overlapping lattice representation allows it to simultaneously pursue multiple hypotheses, while the explicit evaluation of the candidates is deferred until a promising solution is found. Incremental semantic parsing has come back to the forefront of NLU tasks (Peldszus and Schlangen, 2012), (Zhou et al., 2016). Accounting for incrementality helps to make dialogues more human-like by accommodating sporadic asynchronous feedback as well as corrections, wedged-in questions, etc. (Skantze and Schlangen, 2009). In addition, incrementality of meaning representation construction coupled with sensory grounding has been shown to help prune the search space and reduce ambiguity during semantic parsing (Kruijff et al., 2007). The presented algorithm is incremental as it proceeds via a series of operations gradually extending and deepening the lattice structure containing candidate solutions. The algorithm constantly re-evaluates its partial solutions and potentially produces an actionable TMR without exhaustive search over the entire input. Discussion & Related Work Syntax plays a formidable role in the"
P17-3019,W13-4065,0,0.0277636,"Missing"
P17-3019,D16-1065,0,0.0228245,"e, coherence, etc.) in its scoring mechanism for disambiguation and to steer the search process towards the promising region of the search space. This allows it to focus on exploring conceptually plausible interpretations while being less sensitive to upstream noise. The algorithm is opportunistic as the overlapping lattice representation allows it to simultaneously pursue multiple hypotheses, while the explicit evaluation of the candidates is deferred until a promising solution is found. Incremental semantic parsing has come back to the forefront of NLU tasks (Peldszus and Schlangen, 2012), (Zhou et al., 2016). Accounting for incrementality helps to make dialogues more human-like by accommodating sporadic asynchronous feedback as well as corrections, wedged-in questions, etc. (Skantze and Schlangen, 2009). In addition, incrementality of meaning representation construction coupled with sensory grounding has been shown to help prune the search space and reduce ambiguity during semantic parsing (Kruijff et al., 2007). The presented algorithm is incremental as it proceeds via a series of operations gradually extending and deepening the lattice structure containing candidate solutions. The algorithm con"
P17-3019,D14-1108,0,\N,Missing
P87-1028,J82-2002,0,0.0767905,"Missing"
P87-1028,W98-1400,0,0.130925,"Missing"
P87-1028,E85-1027,1,0.88243,"Missing"
P87-1028,J85-4002,0,\N,Missing
P96-1005,P91-1025,0,0.052559,"syntactic category of the input form is changed; however, in our model, the semantic category is preserved in many of these LRs. For example, the verb destroy may be represented by an EVENT, as will the noun destruction (naturally, with a different linking in the syntax-semantics interface). Similarly, destroyer (as a person) would be represented using the same event with the addition of a HUMAN as a filler of the agent case role. This built-in transcategoriality strongly facilitates applications such as interlingual MT, as it renders vacuous many problems connected with category mismatches (Kameyama et al., 1991) and misalignments or divergences (Dorr, 1995), (Held, 1993) that plague those paradigms in MT which do not rely on extracting language-neutral text meaning representations. This transcategoriality is supported by LRs. 2.2.2 W h e n S h o u l d L R s Be A p p l i e d ? Once LRs are defined in a computational scenario, a decision is required about the time of application of those rules. In a particular system, LRs can be applied at acquisition time, at lexicon load time and at run time. • Acquisition Time - The major advantage of this strategy is that the results of any LR expansion can be chec"
P96-1005,C86-1079,1,0.776205,"Missing"
P96-1005,J91-4003,0,0.0188382,"alternation, etc.). • Word F o r m a t i o n - The production of derived forms by LR is illustrated in a case study below, and includes formation of deverbal nominals (destruction, running), agentive nouns (catcher). Typically involving a shift in syntactic category, these LRs are often less productive than inflection-oriented ones. Consequently, derivational LRs are even more prone to overgeneration than inflectional LRs. • Regular Polysemy - This set of phenomena includes regular polysemies or regular nonmetaphoric and non-metonymic alternations such as those described in (Apresjan, 1974), (Pustejovsky, 1991, 1995), (Ostler and htkins, 1992) and others. The nature of the links in the lexicon to the ontology is critical to &apos;the entire issue of LRs. Representations of lexical meaning may be defined in terms of any number of ontological primitives, called con= cepts. Any of the concepts in the ontology may be used (singly or in combination) in a lexical meaning representation. No necessary correlation is expected between syntactic category and properties and semantic or ontological classification and properties (and here we definitely part company with syntax-driven semanticssee, for example, (Levin"
P96-1005,W91-0202,1,\N,Missing
P96-1005,W91-0209,0,\N,Missing
P96-1005,W91-0208,0,\N,Missing
P98-2160,1993.tmi-1.14,0,0.0359958,"comprehensive inventory of universal grammar parameters or even those for particular languages o] language families. For Project Boas, it means tha~. both the nature of the parameters it would be using and their inventory has to be developed in-house. In order to define a set of parameters for Boas, it is essential to distinguish among the language phenomena that should be accorded the status of parameter and those that should be understood as parameter values or their realizations. Still other phenomena may remain, at least for the task at hand, outside the parameter system. We believe, with Dorr (1993), that parameters may be understood as building blocks of an interlingua in MT. We reserve judgment about whether every component of an interlingua is by definition parametric 3. Thus, the parameter ""lexical category"" has a range of values {V, N, Adj, Adv, ... }. Any of these values may itself be considered a parameter. If viewed within a single language, their values are, ultimately, all words in the language which belong to the respective lexical categories. The realizations of these values are the specific forms of these words, which appear in text decorated with realizations of appropriate"
P98-2216,P89-1010,0,0.0308486,"the co-occurrence is semicompositional between the base and the collocate (strong coffee, pay attention, heavy smoker, ...) • r e s t r i c t e d l e x i c a l c o - o c c u r r e n c e , where the meaning of the collocate is compositional but has a lexical idiosyncratic behavior (lecture ... student; rancid butter; sour milk). co-occurrence L o c - i n ( d i s t a n c e ) = at a distance The M T T approach is very interesting as it provides a model of production well suited for generation with its different strata and also a lot of lexicalsemantic information. It seems nevertheless that all 2Church and Hanks (1989), Smadja (1993) use statistics in their algorithms to extract collocations from texts. 3See (Iordanskaja et al., 1991) and (Ramos et al., 1994) for their use of LFs in M T T and NLG respectively. 4(Held, 1989) contrasts Hausman&apos;s base and collate to Mel&apos;tuk&apos;s keyword and LF values. 5There are about 60 LFs listed said to be universal; the lexicographic approach of Mel&apos;tuk and Zolkovsky has been applied among other languages to Russian, French, German and English. f r e e - c o m b i n i n g w o r d s (the girl ate candies) * s e m a n t i c c o l l o c a t i o n s (fast car; long book) 6 • dist"
P98-2216,W94-0311,1,0.895093,"igning LFs. They distinguish four types of syntagmatic LFs: the collocational information is listed in a static way. We believe that one of the main drawbacks of the approach is the lack of any predictable calculi on the possible expressions which can collocate with each other s e m a n t i c a l l y . 3 • • i d i o s y n c r a s i e s (large coke; green jealousy) • i d i o m s (to kick the (proverbial) bucket) Formally, we go from a purely compositional approach in &quot;free-combining words&quot; to a noncompositional approach in idioms. In between, a (semi-)compositional approach is still possible. (Viegas and Bouillon, 1994) showed that we can reduce the set of what are conventionally considered as idiosyncrasies by differentiating &quot;true&quot; idiosyncrasies (difficult to derive or calculate) from expressions which have well-defined calculi, being compositional in nature, and that have been called semantic collocations. In this paper, we further distinguish their idiosyncrasies into: • r e s t r i c t e d s e m a n t i c c o - o c c u r r e n c e , where the meaning of the co-occurrence is semicompositional between the base and the collocate (strong coffee, pay attention, heavy smoker, ...) • r e s t r i c t e d l e x"
P98-2216,P96-1005,1,0.842501,"ifferent classes of cooccurrences (modulo presence of derived forms in the lexicon with same or subsumed semantics). Looking at the following example, h + N &lt;=&gt; V bitter resentment resent heavy big smoker eater smoke eat v + hdv &lt;=&gt; Adv + oppose strongly strongly morally oblige morally + Adv bitterly heavily *bigly Adj-ed opposed obliged we see t h a t after having acquired with h u m a n intervention co-occurrences belonging to the A + N class, we can use lexical rules to derive the V + Adv class and also Adv + Adj-ed class. Lexical rules are a useful conceptual tool to extend a dictionary. (Viegas et al., 1996) used derivational lexical rules to extend a Spanish lexicon. We apply their approach to the production of restricted semantic co-occurrences. Note t h a t eat bigly will be produced but then rejected, as the form bigly does not exist in a dictionary. The rules overgenerate cooccurrences. This is a minor problem for analysis than for generation. To use these derived restricted co-occurrences in generation, the output of the lexical rule processor must be checked. This can be done in different ways: dictionary check, corpus check and ultimately h u m a n check. Other classes, such as the ones b"
P98-2216,J93-1007,0,\N,Missing
P98-2216,C88-2100,1,\N,Missing
sheremetyeva-nirenburg-2000-towards,jones-havrilla-1998-twisted,0,\N,Missing
sheremetyeva-nirenburg-2000-towards,J90-2002,0,\N,Missing
sheremetyeva-nirenburg-2000-towards,J01-2001,0,\N,Missing
sheremetyeva-nirenburg-2000-towards,P98-2160,1,\N,Missing
sheremetyeva-nirenburg-2000-towards,C98-2155,1,\N,Missing
W02-1303,P98-2160,1,0.756471,"gely constitutive in nature in the sense that they would probably not exist if NLP could not offer the know-how to implement them. 3.2.1 MT for Encryption Inspired by the most obvious connection between encryption and NL, the largely apocryphal World War II episode, when instead of an elaborate code, the American and British General Headquarters in Europe used the native speakers of Navajo (Shawnee, in another version, involving the Pacific theater) to communicate in open, uncoded language and were never “decoded,” the idea was to use a family of existing or rapidly deployable MT systems (see Nirenburg and Raskin 1998) to add a level of encryption in an “exotic” language. Raskin et al.—Page 2 Once proposed (Raskin et al. 2001), the idea failed to catch and has never been implemented, partially because there was no research challenge in that, but also because it would involve the “security by obscurity” principle disdained by IAS: one should assume that the adversary is at least as smart and knowledgeable as we, the good guys, are. Also, an MT system, even if publicly available, is too long and messy a “key,” another IAS no-no. 3.2.2 Mnemonics for Random-Generated Passwords Passwords are sometimes dismissed"
W02-1303,C98-2155,1,\N,Missing
W03-0904,1997.tmi-1.1,1,0.677897,"our example, selectional restrictions on the theme of the proposition head matched successfully: indeed, tools are artifacts. As to the restrictions on the agent, they have been found to be too weak to resolve the ambiguity completely: both the lastname and the first-name (not shown) sense of Patrick fit the selectional restrictions on the proposition head (indeed, Alex Patrick may be also be a double first name). Additional disambiguation means are required in this case. We have developed two general methods for additional sense disambiguation: dynamic tightening of selectional restrictions (Mahesh et al., 1997) and determining weighted distances among ontological concepts activated in the input (using the Ontosearch procedure, e.g., Onyshkevych, 1997). None of these methods will, incidentally, help in our example, so that additional heuristic procedures will have to be built for this type of ambiguity. Incidentally, such heuristic procedures could include evidence from a wide variety of sources, including text corpora. Supporting semantic analysis in this way should become an important direction of work in corpus-oriented computational linguistics (see further discussion below). Residual ambiguity i"
W03-0905,moreno-ortiz-etal-2002-new,1,0.820314,"well as the discovery process for, and format of, scripts for the purposes of processing coreference and inferencing which are required, for example, in high-end Q&A and IE applications. 1 Introduction A spate of advanced new applications has called for a massive effort in script acquisition. Conceptualized as complex events, they have been provided for in the ontology since its inception (see Carlson and Nirenburg, 1990) and their format has always been reasonably welldefined as well as constantly adjusted to the consecutive releases (see Nirenburg and Raskin, 2003, Section 7.1.5; cf. Moreno Ortiz et al. 2002). Throughout the early and mid-1990s, however, lower-end NLP applications, such as knowledge- and meaning-based MT, did not necessitate a heavy use of scripts. The new generation of higher-end Q&A and similar IE applications make it necessary to recognize individual events and their effects as part of scripts, both because humans do and because such recognition is necessary for establishing (co)reference relations. Thus, in the following text, only the availability of the BANKRUPTCY script can relate (i) and (ii) (and thus determine whose bankruptcy it is in the latter), which may be immediate"
W04-0904,mcshane-etal-2004-meaning,1,0.926534,"language-independent ontology, • • • • • which is written using a metalanguage of description and currently contains around 5,500 concepts, each of which is described by an average of 16 properties. In all, the ontology contains hundreds of properties (which cover the same territory as the Qualia plus much more). Fillers for properties can be other ontological concepts or literals. An OntoSem lexicon for each language processed, which contains syntactic and semantic zones (linked using variables) as well as calls to “meaning procedures” (i.e., programs that carry out procedural semantics, see McShane et al. 2004a) when applicable. The semantic zone most frequently refers to ontological concepts, either directly or with property-based modifications, but can also describe word meaning extraontologically, for example, in terms of modality, aspect, time, etc. The current English lexicon contains approximately 12K senses, including all closed-class items and the most frequent verbs, as indicated by corpus analysis. This English lexicon took less than 1 person year to build and can (as described below) be ported to other languages. An onomasticon, or lexicon of proper names, which contains approximately 35"
W04-0904,W04-0905,1,0.552183,"eflects the meaning of the sentence He asked the UN to authorize the war, is as follows: REQUEST-ACTION-69 AGENT THEME BENEFICIARY SOURCE-ROOT-WORD TIME ACCEPT-70 THEME THEME-OF SOURCE-ROOT-WORD ORGANIZATION-71 HAS-NAME BENEFICIARY-OF SOURCE-ROOT-WORD HUMAN-72 HAS-NAME AGENT-OF SOURCE-ROOT-WORD WAR-73 THEME-OF SOURCE-ROOT-WORD HUMAN-72 ACCEPT-70 ORGANIZATION-71 ask (< (FIND-ANCHOR-TIME)) WAR-73 REQUEST-ACTION-69 authorize UNITED-NATIONS REQUEST-ACTION-69 UN COLIN POWELL REQUEST-ACTION-69 he ; ref. resolution done ACCEPT-70 war Details of this approach to text processing can be found, e.g., in Nirenburg et al. 2004a,b. The ontology itself, a brief ontology tutorial, and an extensive lexicon tutorial can be viewed at http://ilit.umbc.edu. OntoSem has been used with languages including English, Spanish, Chinese, Arabic and Persian, to varying degrees of lexical coverage (e.g., earlier, less fine-grained English and Spanish lexicons contained 40K entries and were used for MT in the Mikrokosmos project). What makes OntoSem amenable to efficient cross-linguistic usage is that many of the resources are either fully language independent (the ontology, the fact repository, the TMR metalanguage) or parameterizab"
W04-0905,W03-0904,1,\N,Missing
W04-0905,J02-3001,0,\N,Missing
W04-0906,J02-3001,0,0.00827318,"ully automatic way. At this point, we rely on TMRs that are obtained automatically but improved through human interaction (see Nirenburg et al. 2004 for details). Note that fully automatic methods for creating structured knowledge of a quality even remotely approaching that needed to support realistic QA do not at this point exist. Few of the numerous current and recent machine learning and statistical processing experiments in NLP deal with the analysis of meaning at all; and those that do address partial tasks (e.g., determining case role fillers in terms of undisambiguated text elements in Gildea and Jurafsky 2002) in a rather “knowledge-lean” manner. The results are very far away indeed from either good quality or good coverage, either in terms of phenomena and text. We believe that our approach, using as it does statistical as well as recorded-knowledge evidence for extracting, representing and manipulating meaning is the most practical and holds the most promise for the future. Indeed, it is not even as expensive as many people believe. 3 The Knowledge Support Infrastructure The process of deriving TMRs from text is implemented in our Ontosem text analyzer. Semantic analysis in OntoSem is described i"
W04-0906,C00-1072,0,0.0423847,"nce. The expressive power of the ontology and the TMR is enhanced by multivalued fillers for properties, implemented using the “facets” DEFAULT, SEM, VALUE, and RELAXABLE-TO, among others. At the time of this writing, the ontology contains about 6,000 concepts (events, objects and properties), with, on average, 16 properties each. Temporally and causally related events are encoded as values of a complex event’s HAS-EVENT-AS-PART property. These are essentially scripts that provide information that is very useful in general reasoning as well as reasoning for NLP (e.g., Schank and Abelson 1977, Lin and Hovy 2000, Clark and Porter 2000). We use scripts in the answer content determination module of the question answering system. Figure 4 illustrates a rather simple script that supports reasoning for our example question answering session. The OntoSem lexicon contains not only semantic information, it also supports morphological and syntactic analysis. Semantically, it specifies what concept, concepts, property or properties of concepts defined in the ontology must be instantiated in the TMR to account for the meaning of a given lexical unit of input. At the time of writing, the latest version of the En"
W04-0906,1997.tmi-1.1,1,0.657351,"or good coverage, either in terms of phenomena and text. We believe that our approach, using as it does statistical as well as recorded-knowledge evidence for extracting, representing and manipulating meaning is the most practical and holds the most promise for the future. Indeed, it is not even as expensive as many people believe. 3 The Knowledge Support Infrastructure The process of deriving TMRs from text is implemented in our Ontosem text analyzer. Semantic analysis in OntoSem is described in some detail in Nirenburg and Raskin 2004; Nirenburg et al., 2004; Beale et al. 1995, 1996, 2003; Mahesh et al. 1997. Our description here will be necessarily brief. Also note that the analysis process is described here as if it were a strict pipeline architecture; in reality, semantic analysis is used to inform and disambiguate syntactic analysis, for example, in cases of prepositional phrase attachment. Text analysis in OntoSem relies on the results of a battery of pre-semantic text processing modules. The preprocessor module deals with mark-up in the input text, finds boundaries of sentences and words, recognizes dates, numbers, named entities and acronyms and performs morphological analysis. Once the mo"
W04-0906,W04-0905,1,\N,Missing
W04-2601,P97-1051,0,0.032261,"– has been widely studied in computational (not to mention other branches of) linguistics, largely because accounting for missing syntactic elements is a crucial aspect of achieving a full parse, and parsing is required for many approaches to NLP.1 Much less attention has been devoted to what we will call semantic ellipsis, or the non-expression of elements that, while not syntactically obligatory, are required for a full semantic interpretation of a text.2 Naturally, semantic ellipsis is important only in truly knowledge-rich ap1 Examples of NLP efforts to resolve syntactic ellipsis include Hobbs and Kehler 1997; Kehler and Shieber 1997; and Lappin 1992, among many others. 2 Some of the types of semantic underspecification treated here are described in the literature (e.g., Pustejovsky 1995) in theoretical terms, not as heuristic algorithms. This is due, in large part, to a lack of knowledge sources for semantic reasoning in those contributions. proaches to NLP, which few current non-toy systems pursue. All definitions of ellipsis derive from a stated or implied notion of completeness. Taking, again, the example of syntactic ellipsis, this means that obligatory verbal arguments must be overt, auxilia"
W04-2601,J97-3005,0,0.0167995,"ied in computational (not to mention other branches of) linguistics, largely because accounting for missing syntactic elements is a crucial aspect of achieving a full parse, and parsing is required for many approaches to NLP.1 Much less attention has been devoted to what we will call semantic ellipsis, or the non-expression of elements that, while not syntactically obligatory, are required for a full semantic interpretation of a text.2 Naturally, semantic ellipsis is important only in truly knowledge-rich ap1 Examples of NLP efforts to resolve syntactic ellipsis include Hobbs and Kehler 1997; Kehler and Shieber 1997; and Lappin 1992, among many others. 2 Some of the types of semantic underspecification treated here are described in the literature (e.g., Pustejovsky 1995) in theoretical terms, not as heuristic algorithms. This is due, in large part, to a lack of knowledge sources for semantic reasoning in those contributions. proaches to NLP, which few current non-toy systems pursue. All definitions of ellipsis derive from a stated or implied notion of completeness. Taking, again, the example of syntactic ellipsis, this means that obligatory verbal arguments must be overt, auxiliary verbs must have comple"
W04-2601,mcshane-etal-2004-meaning,1,0.875495,"Missing"
W04-2601,W03-0904,1,0.817439,"example of a TMR, reflecting the meaning of the sentence The US won the war, is as follows: WIN-3 AGENT NATION-213 THEME WAR-ACTIVITY-7 This TMR is headed by a WIN event – in fact, it is the 3rd instantiation of the concept WIN (WIN-3) in the world “snapshot” being built during the processing of the given text(s). Its agent is NATION-213, which refers to the United States of America in our fact repository. The theme of the event is the 7th instantiation of WARACTIVITY in this text. Details of this approach to text processing can be found, e.g., in Nirenburg and Raskin 2004, Beale et al 2003, Nirenburg et al 2003a,b. The ontology itself, a brief ontology tutorial, and an extensive lexicon tutorial can be viewed at http://ilit.umbc.edu. Since OntoSem text processing attempts to do it all – meaning that any phenomenon in any language we are processing is within the purview of our approach – work on any given problem is carried out in spiral fashion: first at a rough grain size, then at a finer grain size with each iterative improvement of the system. In order both to drive and to organize work, we develop a “microtheory” for each aspect of text processing we treat: e.g., we have microtheories of mood, t"
W04-2601,W04-0905,1,\N,Missing
W05-0310,brants-2000-inter,0,0.0186284,"propriate properties are linked to their heads is quite simple, whereas writing a program for this non-trivial case of reference resolution is not. Thus, in some cases we push through gold standard TMR production while keeping track of – and developing as time permits – the more difficult aspects of text processing that will enhance TMR output in the future. The gold standard TMR for the sentence discussed at length here was produced with only a few manual corrections: changing two part of speech tags and selecting the correct sense for one word. Work took less than the 10 minutes reported by Brants 2000 for their non-semantic tagging. 5 Porting to Other Languages Recently the need for tagged corpora for less commonly taught languages has received much attention. While our group is not currently pursuing such languages, it has in the past: TMRs have been automatically generated for languages such as Chinese, Georgian, Arabic and Persian. We take a short tangent to explain how OntoSem/DEKADE can be extended, at relatively low cost, to the annotation of other languages – showing yet another way in which this approach to annotation reaches beyond the results for any given text or corpus. Whereas"
W05-0310,J02-3001,0,0.00894706,"e at 10 minutes, by the time two taggers carry out the task, their results are compared, difficult issues are resolved, and taggers are trained in the first place. Notably, however, this effort used students as taggers, not professionals. We, by contrast, use professionals to check and correct TMRs and thus reduce to practically zero the training time, the need for multiple annotators (provided the size of a typical annotation task is commensurate with those in current projects), and costly correction of errors. Among past projects that have addressed semantic annotation are the following: 1. Gildea and Jurafsky (2002) created a stochastic system that labels case roles of predicates with either abstract (e.g., AGENT, THEME) or domainspecific (e.g., MESSAGE, TOPIC) roles. The system trained on 50,000 words of hand-annotated text (produced by the FrameNet project). When tasked to segment constituents and identify their semantic roles (with fillers being undisambiguated textual strings) the system scored in the 60’s in precision and recall. Limitations of the system include its reliance on hand-annotated data, and its reliance on prior knowledge of the predicate frame type (i.e., it lacks the capacity to disam"
W05-0310,W04-0904,1,0.829972,"eing treated compositionally.) We do not intend to trivialize the fact that creating a new lexicon is a lot of work. It is, however, compelling to consider that a new lexicon of the same quality of our OntoSem English one could be created with little more work than would be required to build a typical translation dictionary. In fact, we recently carried out an experiment on porting the English lexicon to Polish and found that a) much of it could be done semi-automatically and b) the manual work for a second language is considerably less than for the first language (for further discussion, see McShane et al. 2004). To sum up, the OntoSem ontology and the DEKADE environment are equally suited to any language, and the OntoSem English lexicon and analyzer can be configured to new languages with much less work required than for their initial development. In short, semantic-rich tagging through TMR creation could be a realistic option for languages other than English. 6 Discussion Lack of interannotator agreement presents a significant problem in annotation efforts (see, e.g., Marcus et al. 1993). With the OntoSem semiautomated approach, there is far less possibility of interannotator disagreement since peo"
W05-0310,W04-0905,1,0.834969,"mber of mistakes at each stage of analysis, and the effect that the correction of output at one stage has on the next stage. No methods or standards for such evaluation are readily available since no work of this type has ever been carried out. In the face of the usual pressures of time and manpower, we have made the programmatic decision not to focus on all types of evaluation but, rather, to concentrate our evaluation metrics on the correctness of the automated output of the system, the extent to which manual correction is needed, and the depth and robustness of our knowledge resources (see Nirenburg et al. 2004 for our first evaluation effort). We do not deny the ultimate 74 desirability of additional aspects of evaluation in the future. The main source of variation among knowledge engineers within our approach lies not in reviewing/editing annotations as such, but in building the knowledge sources that give rise to them. To take an actual example we encountered: one member of our group described the phrase weapon of mass destruction in the lexicon as BIOLOGICAL-WEAPON or CHEMICAL-WEAPON, while another described it as a WEAPON with the potential to kill a very large number of people/animals. While b"
W05-0310,W05-0311,0,0.0842338,"Missing"
W05-0310,H01-1035,0,0.0962925,"Missing"
W05-0310,J93-2004,0,\N,Missing
W08-2214,mcshane-etal-2004-meaning,1,0.843877,"berry ice cream!” • the pronoun I being used outside of a quotation and the writer of the text being available in metadata: &lt;title&gt;Understanding Your Finances&lt;/title&gt; &lt;author&gt;Mary Smith&lt;/author&gt; . . . I believe that the only way to understand your finances is to consult a financial advisor. In short, using the combination of the information in the sem-struc and meaningprocedures zones we arm the analyzer with the types of the information a person would use to both understand the idiom and to resolve all implied references. (For a more detailed description of meaning procedures in OntoSem, see McShane et al. (2004).) 3.3 Subjectless Constructions We conclude our example-based discussion with one category of phenomena in which idiom processing is actually much simpler than the processing of structurally similar compositional language since it permits preemptive disambiguation. The disambiguation in question regards subjects, which in Russian can be overt, elided or completely missing. Completely missing (uninsertable) subjects occur in the following constructions: McShane and Nirenburg 174 • In the indefinite personal construction a 3rd person plural verb form is used without a subject to indicate an uns"
W08-2214,W04-0411,0,0.0308582,"were part of an idiom: textual coreferents — recorded as semantic entities in TMR — are sought and, whether or not they are found, the referring expression is anchored in the fact repository. If we look at what is The Idiom–Reference Connection 171 special about processing the reference in idioms, then, there are only two aspects: (1) ensuring that productive syntactic processes are permitted only if applicable, and (2) ensuring that the correct inventory of referring expressions — understood as semantic structures — is generated. Let us compare this treatment of idioms to the one proposed by Villavicencio et al. (2004). They treat the potential variability of idioms using the notion of semantic decomposition. If an idiom can be paraphrased in a syntactically parallel way, it is decomposable (spill the beans → reveal a secret), even though non-standard meanings need to be assigned to each component. The fundamental differences between their approach and ours relate to semantic encoding and reference resolution. For Villavicencio et al., the semantics of idioms is conveyed by paraphrases with other linguistic elements (spill → reveal, beans → secret). For us, semantics is formulated using the ontologically gr"
W08-2215,P01-1008,0,0.107062,"Missing"
W08-2215,P04-2006,0,0.0663533,"Missing"
W08-2215,W03-1606,0,0.0443524,"Missing"
W08-2215,W03-1608,0,0.0183392,"of contextually elastic adjectives (such as fast, which means different things in fast highway and fast eater) by semiautomatically constructing paraphrases for phrases that include such adjectives. These paraphrases use the original noun and the adjective (or any of its synonyms, taken from a hand-constructed list) in its adverbial form and add a corpus-derived candidate verb intended to explain the meaning of the adjective. Results are evaluated by human judgments of whether a paraphrase (e.g., highway travel quickly) is appropriate as an explanation of the meaning of fast in fast highway. Ibrahim et al. (2003) pursue the more immediate goal of supporting a questionanswering system. Creating paraphrases for questions helps to expand the queries to the textual resources that are mined for answers. In an early version of this system, such paraphrase rules — which included a combination of lexical and syntactic transformations — were created by hand (Katz and Levin, 1988). The new approach follows the methodology of Lin and Pantel (2001) for dynamically determining paraphrases in a corpus by measuring the similarity of paths between nodes in syntactic deResolving Paraphrases to Support Modeling Languag"
W08-2215,C88-1065,0,0.145448,"candidate verb intended to explain the meaning of the adjective. Results are evaluated by human judgments of whether a paraphrase (e.g., highway travel quickly) is appropriate as an explanation of the meaning of fast in fast highway. Ibrahim et al. (2003) pursue the more immediate goal of supporting a questionanswering system. Creating paraphrases for questions helps to expand the queries to the textual resources that are mined for answers. In an early version of this system, such paraphrase rules — which included a combination of lexical and syntactic transformations — were created by hand (Katz and Levin, 1988). The new approach follows the methodology of Lin and Pantel (2001) for dynamically determining paraphrases in a corpus by measuring the similarity of paths between nodes in syntactic deResolving Paraphrases to Support Modeling Language Perception 181 pendency trees. This method was applied to pairs of sentences from different English translations of the same text. (The idea of using a monolingual “sentence-aligned” corpus is due to Barzilay and McKeown (2001).) Ibrahim et al. (2003) then suggest a set of heuristics for the subsentential-level matching of nouns and pronouns which leads to the"
W08-2215,N01-1009,0,0.0327825,"ent NLP work. As a result, most contributions devoted to paraphrase can be described as syntactic or “light semantic.” In some contributions, processing semantics is constrained to finding synonyms, hyponyms, etc., in a manually constructed word net, like WordNet or any of its progeny. Some others do not rely on a manually constructed knowledge resource but, rather, aim to determine distributional clustering of similar words in corpora (see, e.g. Pereira et al. (1993) or Lin (2001)). A few approaches to dealing with paraphrase actually go beyond the detection and use of synonyms. For example, Lapata (2001) seeks to interpret the meanings of contextually elastic adjectives (such as fast, which means different things in fast highway and fast eater) by semiautomatically constructing paraphrases for phrases that include such adjectives. These paraphrases use the original noun and the adjective (or any of its synonyms, taken from a hand-constructed list) in its adverbial form and add a corpus-derived candidate verb intended to explain the meaning of the adjective. Results are evaluated by human judgments of whether a paraphrase (e.g., highway travel quickly) is appropriate as an explanation of the m"
W08-2215,1997.tmi-1.1,1,0.531795,"EVENT). This preference is the inverse of the lexical choice in text generation off of text meaning representations (for details see Nirenburg and Nirenburg, 1988). OntoSem can yield either of the above basic text meaning representations. In many applications — for example, in interlingua-based machine translation — this would be quite benign. However, it is possible to create extended meaning representations such that the above variability is eliminated. The method we use for this purpose relies on the dynamic tightening or relaxation of selectional restrictions and is described in detail in Mahesh et al. (1997). Note that different paraphrases will still be produced for inputs that, while referring to the same event instance, describe it with a different degree of vagueness or underspecificity (see Section 2.1 above). The fact that the two meaning representations above are paraphrases of one another can be automatically detected using a fairly simple heuristic: the ontological description of AERIAL - MOTION - EVENT includes the following property-value pairs: AERIAL - MOTION - EVENT IS - A MOTION - EVENT INSTRUMENT AIRPLANE HELICOPTER BALLOON Since the head of one of the MRs is an ancestor of the ot"
W08-2215,C88-2100,1,0.424028,"concepts stand in a Nirenburg, McShane, and Beale 188 direct subsumption relation. If one chooses to use a concept that is higher in the ontological hierarchy, one may have to add further overt constraints to the meaning representation (like the one about the INSTRUMENT of the MOTION - EVENT above). If one chooses the lower-level, narrower ontological concept to start with, such constraints may be inherent in its definition (as is the case with AERIAL - MOTION - EVENT). This preference is the inverse of the lexical choice in text generation off of text meaning representations (for details see Nirenburg and Nirenburg, 1988). OntoSem can yield either of the above basic text meaning representations. In many applications — for example, in interlingua-based machine translation — this would be quite benign. However, it is possible to create extended meaning representations such that the above variability is eliminated. The method we use for this purpose relies on the dynamic tightening or relaxation of selectional restrictions and is described in detail in Mahesh et al. (1997). Note that different paraphrases will still be produced for inputs that, while referring to the same event instance, describe it with a differ"
W08-2215,P93-1024,0,0.0618378,"ficult problem: at its deepest, it centrally involves semantics, which, due to its inherent complexity, can be addressed only in limited ways in current NLP work. As a result, most contributions devoted to paraphrase can be described as syntactic or “light semantic.” In some contributions, processing semantics is constrained to finding synonyms, hyponyms, etc., in a manually constructed word net, like WordNet or any of its progeny. Some others do not rely on a manually constructed knowledge resource but, rather, aim to determine distributional clustering of similar words in corpora (see, e.g. Pereira et al. (1993) or Lin (2001)). A few approaches to dealing with paraphrase actually go beyond the detection and use of synonyms. For example, Lapata (2001) seeks to interpret the meanings of contextually elastic adjectives (such as fast, which means different things in fast highway and fast eater) by semiautomatically constructing paraphrases for phrases that include such adjectives. These paraphrases use the original noun and the adjective (or any of its synonyms, taken from a hand-constructed list) in its adverbial form and add a corpus-derived candidate verb intended to explain the meaning of the adjecti"
W08-2215,P04-1019,0,0.0301646,"- EVENT INSTRUMENT AIRPLANE HELICOPTER BALLOON Since the head of one of the MRs is an ancestor of the other, and the property-value pairs in the ancestor-based MR unify with the ontological definition of the descendant (the head of the other MR), these two structures are deemed to be paraphrases. As we see from this example, world knowledge stored in the ontology is leveraged to carry out the reasoning needed to detect that the abovementioned formal structures are paraphrases. Such situations are somewhat similar to “bridging references” in the literature devoted to reference resolution (e.g. Poesio et al., 2004) because a knowledge bridge is needed to aid in the reference resolution of the entity. A common source of this type of paraphrase derives from decisions about how to build the ontology. Ontology building is a complex task with “the lesser of the evils” decisions to be made at every turn. Two ontologies can be equally valid and yet look quite different. One of the most difficult aspects of ontology building is deciding when a new concept is needed. Let us continue with the example of taking a trip. A small excerpt from the MOTION - EVENT subtree of our ontology is as follows: MOTION - EVENT AE"
W08-2225,mcshane-etal-2004-meaning,1,0.852745,"Missing"
W08-2225,W04-0905,1,0.812574,"Missing"
W16-3805,P14-5010,0,0.00304216,"ns is good enough. For example, whereas kick the bucket can mean ‘to die’, kick the buckets (plural) cannot. So we do not want our agents to assume that all recorded constraints are relaxable – they have to be smarter about making related judgments. Returning to the non-canonical “managed close the door”, let us assume that the agent already processed the canonical input Charlie managed to close the door and stored the results in the TMR repository. Let’s assume further that the new input is The fire chief managed close the door, for which the external parser we use, from the CoreNLP toolset (Manning et al. 2014), does not recognize that close the door is intended to be an xcomp. So our agent cannot directly align the parser output with the xcomp expected in the lexical sense for manage. As before, the agent can use the TMR repository as a search space and look for approximate string-level matches of “managed close the door”. If it finds “managed to close the door,” it can judge the similarity between the stored and new text strings and, if close enough, use the stored analysis to guide the new analysis. The natural extension is to relax the notion of similarity beyond surface string matching. The fir"
W16-3805,W05-0310,1,0.722961,"Missing"
W90-0120,J87-3007,1,\N,Missing
W91-0202,J89-3002,0,0.0259686,"Missing"
W91-0202,W91-0221,0,0.121105,"Missing"
W91-0202,P90-1032,0,0.0478417,"Missing"
W96-0310,J91-1001,0,0.0253751,"graphic descriptions of adjectives (and other categories) are rare, is bound to change rapidly. As computational semantics moves to large-scale systems serving non-toy domains, the need for large lexicons with entries of all lexical categories in them is becoming increasingly acute, and the attention of computational semanticists and lexicographers is turning more towards such previously neglected or avoided categories as the adjectives. Recently, there have appeared some first indications of this attention--see, for instance, Smadja (1991), Beckwith et al. (1991), Bouillon and Viegas (1994), Justeson and Katz (1991, 1995), Pustejovsky (1995: 20-23). This research is a step in the same direction. 90 2. The Ontology-Based Approach to Adjectival Semantics and Lexicology In this section, we briefly review the basis of our approach to adjectival meaning and illustrate it on three examples of adjectival lexicon entries, i.e., one each for the three major classes of adjectives. 2.1 The Ontological Approach Our work on adjectives forms a microtheory used by the MikroKosmos semantic analyzer. The architecture of MikroKosmos is described in Onyshkevych and Nirenburg 1994 and Beale et al. 1995. The MikroKosmos pro"
W96-0310,J95-1001,0,0.0244091,"Missing"
W96-0310,C96-2142,1,0.809508,"Types 2.2.1 Scalar Adjectives Our microtheory associates the meaning of a typical truly scalar adjective with a region on a scale which is defined as the range of an ontological property. The contribution that the adjective makes to the construction of a semantic dependency structure (TMR) typically consists of inserting its meaning (a property-value pair) as a slot in a frame representing the meaning of the noun which this adjective syntactically modifies. Thus, in big house, big will assign a high value as the filler of the property slot SIZE of the frame for the meaning of house (see also Raskin and Nirenburg 1996). (1) is a partial lexical entry for big, with just two of the 13 lexical zones represented: (1) (big (big-Adj~CAT adj) 91 (SYN-STRUC (1 ((root Svarl) (cat n) (mods ((root $var0))))) (2 ((root $var0) (cat adj) (subj ((root $varl) (cat n)))))) (SEM-STRUC (LEX-MAP ((1 2) (size-attribute (domain (value ^$varl) (sem physical-object)) (range (value (&gt; 0.75)) (relaxable-to (value (&gt; 0.6)))))))))) SIZE is an ontological c o n c e p t o f the SCALAR-PHYSICAL-OBJECT-A&apos;FrRIBUTE-PROPERTY type, with the term &apos;scalar&apos; used here, as it is customarily, primarily in the sense o f &apos;gradable.&apos; B i g is, of cour"
W96-0310,P96-1005,1,0.799788,"cept on which the lexical entry should be based. While a relatively small class of true scalars is more or less easily associated with certain property concepts, there are many more process and object concepts, and it is much easier to relate the meaning of a verb or a noun to one of those. Besides, it is precisely the denominal and deverbal adjectives which are very hard to relate to a property concept directly, so the LRs come in quite handy. Several of the LRs discovered manually in the pilot project turned out to be &quot;large,&quot; in the sense of their massive productiveness (see, for instance, Viegas et al. 1996), such as the comparativedegree LR, applicable to every one of the 4,000-plus gradable adjectives. Given the paucity of relative adjectives:in English (because of the adjectival use of nouns), the denominal adjective LR (12i) produces around 300 entries in our English corpus. The deverbal adjective LR falls in between, with an output of almost 1,000 entries. We have foundlvirtually no LR to be exception-free, and that reduces, of course, the degree to which a large LR can be used fully automatically, thus raising the cost of their application. 4. Deverbal Adjectives: A Full(er) View In this se"
W96-0407,C86-1132,0,0.0218462,"s can be a difficult task even for a patent expert, let alone an inventor who is typically an engineer and not a technical writer. Note that the difficulty of the task is not constrained to syntax and style. A claim must be composed so as to make patent infringement difficult. We have developed a system which helps an inventor to compose patent claims. The system has an interactive and an automatic components. Knowledge about the invention is elicited from the inventor interactively. Most of text planning and realization is carried out automatically. specification modules (e.g., Kukich, 1983; Kittredge et al. 1986), our system relies on an authoring workstation environment equipped with a knowledge elicitation scenario for joint humancomputer content specification (see Sheremetyeva et al., submitted 1996, for the details of the knowledge elicitation scenario). Lexical selection and some other text planning tasks are interleaved with the process of content specification. The latter results in the production of a ""draft"" claim. This draft, while not yet an English text, is a list of proposition-level structures (""templates"") specifying the proposition head and case role values filled by POS-tagged word st"
W96-0407,J93-1009,0,0.107362,"nui et al., 1992). Using the set of distinctions by Robin, our approach is content-preserving (no extra content is added) and performs revisions on a shallow representation. The realization stage linearizes the plan and takes care of the ellipsis, conjoined structures, punctuation and morphological forms. The architecture of the system is illustrated in Figure 2. In what follows we describe each stage of our system in turn and illustrate it with a single example of generating the claim of Figure 1. 2 Content Specification The input to our system is quite unlike the inputs to other generators. McDonald (1993) lists several kinds of possible inputs -- numerical data, structured objects used by a reasoning system or logical formulae based on lexical predicates (p. 191). A large part of our input is, in fact, in the mind of the inventor. The system simply helps the inventor express this input. In this intent, this system is similar to the DRAFTER system (e.g., Paris et al., 1995). Superficially, the architecture of our system conforms to the standard emerged in natural language generation (NLG) (as expressed, for instance in Reiter, 1994) in that it includes the stages of content specification, text"
W96-0407,W94-0319,0,0.0318985,"Missing"
W98-0713,W96-0305,0,0.0280598,"corporating work done at NMSU (Bruce and Guthrie, 1991) that identifies and disaxabiguates the head nouns in the definJtion texts. The hierarchy is used to guide the deterzaination of nontrivial matches by providing local cozLtext in which senses can be considered unambiguo as by filtering out the other senses not applicable to either subhierarchy. It also allows for matching the ~arents of words from an existing match. Note that r,his mapping is facilitated by the target and source domains being the same: namely, English words. Therefore, the problem of assessing correspondence is minimized. Chang and Chen (1996) describe an algorithm for augmenting LDO£&apos;E with information from Longman&apos;s Lexicon of Contemporary English (LLOCE). LLOCE is basically a thesaurus, with word lists arranged under 14 subjects and 129 topics. These topic identifiers are used as a coarse form of sense division. The mat~ zing algorithm works by computing a similarity scol&apos;e for the degree of overlap in the list of words for each LDOCE sense compared to the list of words from t[ e LLOCE topics that contain the headword (expanded to include cross-references). Other work is l,~s directly related. Lehmann (1995) describes a methodol"
W98-0713,W95-0105,0,0.0285882,"heuristics 95 that exploit the WordNet ontology hierarchy. One specifically assigns the highest weight to the potential synset/concept mapping which has the highest degree of overlap between the respective is-a hierarchies. For instance, when mapping to the concept DOG, the canine sense of ""dog"", which goes through M A M M A L and to ENTITY, is preferred over the scoundrel sense, which goes through PERSON to ENTITY. The other heuristic uses synset frequencies, estimated from a corpus of Wall Street Journal articles. This is based on a technique for disambiguating noun groups using WordNet by Resnik (1995). For each pair of defining words that share a common ancestor, the support for the match is increased by an amount inversely proportional to the frequency of the ancestor, because unrelated words only have common ancestors at the top level of the hierarchy. Two other heuristics exploit the ontology but in a more localized manner. One assigns a weight based on the degree of overlap among the children for each concepts. The other does the same thing for the siblings of each. The final heuristic computes the degree of overlap among the words in the definition texts. This was meant mainly as a we"
W98-0713,P96-1005,1,0.838376,"en for humans not computers. Atkins (1995) estimates that it would take 100 person-years to properly develop a semantic lexical database comparable in scope to a standard college dictionary. Lexicons are a key component of machine translation systems (Onyshkevych and Nirenburg, 1994). The Mikrokosmos (/~K) project at NMSU&apos;s Computing Research Laboratory is developing Spanish, Chinese and Japanese lexicons to support knowledgebased machine translation (KBMT). The following table indicates the amount of effort that was re94 quired for developing the initial Spanish lexicon entries from scratch (Viegas et al., 1996): 6798 word-sense entries (as of 29 Mar 1996) Average of 1.2 meaning per word form Acquisition rate: 45 entries/day per person Acquisition effort: 4 person years Like many research centers, we don&apos;t have the human resources to construct the entire lexicons manually, so we are investigating several different ways to automate lexicon acquisition. Viegas et al. (1996) discuss one approach at this through the use of lexical rules, such as for generating the morpho-semantic derivatives of Spanish verbs. A natural solution would be to take advantage of machine readable dictionaries (MILD&apos;s), such as"
W98-1406,J97-2001,0,0.19888,"Missing"
W98-1406,W96-0403,0,0.019266,"Missing"
W98-1406,1996.amta-1.10,1,0.774247,"Missing"
W98-1406,W94-0315,0,0.0593217,"Missing"
W98-1406,P96-1005,1,0.871835,"Missing"
W98-1406,W96-0401,1,0.844947,"Missing"
W98-1406,A92-1006,0,\N,Missing
W99-0703,J95-4004,0,0.07504,"Missing"
W99-0703,P84-1070,0,0.184046,"Missing"
W99-0703,J96-1003,1,0.854985,"Missing"
W99-0703,J94-3001,0,0.0205025,"Missing"
W99-0703,W98-1308,0,0.0212845,"Missing"
W99-0703,P97-1057,0,0.028827,"Missing"
W99-0703,C92-1025,0,0.0243393,"Missing"
W99-0703,A97-1016,0,0.140673,"Missing"
W99-0703,C94-1066,0,0.0321245,"Missing"
W99-0703,P98-2160,1,0.855506,"Missing"
W99-0703,C98-2155,1,\N,Missing
