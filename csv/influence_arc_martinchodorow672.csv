2020.cmcl-1.9,macwhinney-2012-morphosyntactic,0,0.0136545,"urpose of the study is not to provide a model to explain children’s grammatical case acquisition, but to examine if the distributional 3 3.1 Methods Corpus Summary Following Mintz (2003) and Clair et al. (2010), we used the same six corpora of child-directed speech from CHILDES (MacWhinney, 2014): Anne and Aran (Theakston et al., 2001), Eve (Brown, 1973), Naomi (Sachs, 1983), Nina (Suppes, 1974), Peter (Bloom et al., 1974). We analyzed the utterances in the files where the child is younger than 2;6 years old. The pronouns were extracted with the part-of-speech tags assigned by the MOR parser (MacWhinney, 2012) in CHILDES: pro:sub for nominative pronouns, pro:obj for accusative pronouns and det:poss for genitive pronouns. Case-ambiguous pronouns ‘you’ and ‘it’ were excluded from the study since they were tagged as pro:per in all argument positions. The pronoun ‘her’ was included since it was tagged as pro:obj for its accusative use and det:poss for its genitive use. Each pronoun was extracted with its aXb context. Table 1 summarizes the number of tokens of all pronouns and each case, and the number of types for aX, Xb and aXb. Figure 1 shows the token frequencies of the pronouns produced by the chil"
2021.scil-1.2,N16-1163,0,0.0128082,"ls speaking different pairs of languages. Future studies should test the generalizability of this claim. The quality of the translated equivalents could be evaluated and enhanced in the future by using a professional bilingual dictionary, asking bilinguals to evaluate the translations, and considering word context. In the present study, although we used WordNet to find the best matching translation in general, we could not guarantee that those translations were correct given the context in the sentence. One possible way of tackling this problem is to apply sense-specific word representations (Ettinger et al., 2016; Upadhyay et al., 2017) to represent and match the particular senses of words in the two languages. Our study provides a novel approach to investigating code-switching in speech, which further sheds light on the possibility that bilingual word retrieval is influenced by the interconnection and the interaction between words. Nevertheless, the pattern observed in our study, where a CS word tends to have lower C than its translated equivalent, does not directly address the proposed mechanism that the activation of the lower C word ‘stands out’ among its neighbors and therefore is selected over i"
2021.scil-1.2,W14-3902,0,0.0121196,"ched word and its translated equivalent. The results showed that words where language is switched have lower C than their translated equivalents in the other language, suggesting that the structures of the lexicons in the two languages play an important role in bilingual code-switching speech. 1 Background Code-switching is defined as “the alternation between two (or more) languages within a single discourse, sentence, or constituent” (Poplack, 2000). The phenomenon has been widely observed in bilinguals with a variety of different language pairs (Poplack, 2000; Santorini and Mahootian, 1995; Barman et al., 2014). Understanding the mechanism behind bilingual code-switching speech is of particular importance to psycholinguistics and natural language processing. In psycholinguistics, it provides key clues to bilinguals’ coordination between the representations of two or more languages from structural, psychological and social perspectives (Bullock and Toribio, 2009). In natural language processing, on the other hand, its mechanisms could provide a strong foundation for tasks, such as language identification (Lyu et al., 2006; Barman et al., 2014) and syntactic parsing (Broersma, 2009), among others. Thi"
2021.scil-1.2,Q17-1010,0,0.00514616,"glish-Chinese word-translation pairs and 898 Chinese-English word-translation pairs being kept. 3.3 Embeddings Data We used preprocessed fastText embeddings of English and Chinese (Grave et al., 2018)1 . Each fastText model contains vectors of 2M unique words. The fastText embedding model is a neural network model that learns vector representations of words from text. Words that appear in similar contexts are closer in vector space. Because semantically-related words tend to exist in similar contexts (e.g., ‘king’ and ‘queen’), fastText can well capture the semantic association between words (Bojanowski et al., 2017). For example, it is able to tell that the semantic association between ‘king’ and ‘man’ is analogous to the association between ‘queen’ and ‘woman’. Moreover, fastText outperforms other similar embedding models in semantic representations (Bojanowski et al., 2017; Grave et al., 2018), because it also has embedded fine-grained sublexical information such as morphology in English and characters in Chinese. 刚才 我 不是 跟 你 讲 我 apply 那个 job “Haven’t I told you just now that I applied for that job?” 3.2 Translated equivalents We used the Princeton WordNet of English (Miller, 1995) and the Chinese Open"
2021.scil-1.2,P13-1133,0,0.0263255,"between ‘king’ and ‘man’ is analogous to the association between ‘queen’ and ‘woman’. Moreover, fastText outperforms other similar embedding models in semantic representations (Bojanowski et al., 2017; Grave et al., 2018), because it also has embedded fine-grained sublexical information such as morphology in English and characters in Chinese. 刚才 我 不是 跟 你 讲 我 apply 那个 job “Haven’t I told you just now that I applied for that job?” 3.2 Translated equivalents We used the Princeton WordNet of English (Miller, 1995) and the Chinese Open WordNet (Wang and Bond, 2013) from Open Multilingual WordNet (Bond and Foster, 2013) to translate the CS words. The two WordNets use common word senses. Therefore, if a word in one WordNet shares the same word sense(s) with a word in the other WordNet, the two words are considered translated equivalents of each other (See Figure 3 for an example). Since a word can have multiple translated 1 21 https://fasttext.cc/docs/en/crawl-vectors.html 3.4 Semantic networks A weighted semantic network was built for each language. Because analyzing large weighted semantic networks can be computationally expensive, we constructed the semantic network with the CS words and the translated equ"
2021.scil-1.2,L18-1550,0,0.0178269,"shed through multilingual WordNet. The Chinese word ‘工作’ is a translated equivalent of the English word ‘job’, as they share three senses. Due to the size difference between the English and the Chinese WordNets, not all CS words had their translated equivalents. To maintain the quality of the translation, we only kept the CS words whose translated equivalents could be found in the WordNets. This resulted in 3,453 English-Chinese word-translation pairs and 898 Chinese-English word-translation pairs being kept. 3.3 Embeddings Data We used preprocessed fastText embeddings of English and Chinese (Grave et al., 2018)1 . Each fastText model contains vectors of 2M unique words. The fastText embedding model is a neural network model that learns vector representations of words from text. Words that appear in similar contexts are closer in vector space. Because semantically-related words tend to exist in similar contexts (e.g., ‘king’ and ‘queen’), fastText can well capture the semantic association between words (Bojanowski et al., 2017). For example, it is able to tell that the semantic association between ‘king’ and ‘man’ is analogous to the association between ‘queen’ and ‘woman’. Moreover, fastText outperf"
2021.scil-1.2,W17-2613,0,0.0118005,"airs of languages. Future studies should test the generalizability of this claim. The quality of the translated equivalents could be evaluated and enhanced in the future by using a professional bilingual dictionary, asking bilinguals to evaluate the translations, and considering word context. In the present study, although we used WordNet to find the best matching translation in general, we could not guarantee that those translations were correct given the context in the sentence. One possible way of tackling this problem is to apply sense-specific word representations (Ettinger et al., 2016; Upadhyay et al., 2017) to represent and match the particular senses of words in the two languages. Our study provides a novel approach to investigating code-switching in speech, which further sheds light on the possibility that bilingual word retrieval is influenced by the interconnection and the interaction between words. Nevertheless, the pattern observed in our study, where a CS word tends to have lower C than its translated equivalent, does not directly address the proposed mechanism that the activation of the lower C word ‘stands out’ among its neighbors and therefore is selected over its translated equivalent"
2021.scil-1.2,W13-4302,0,0.0172412,"ple, it is able to tell that the semantic association between ‘king’ and ‘man’ is analogous to the association between ‘queen’ and ‘woman’. Moreover, fastText outperforms other similar embedding models in semantic representations (Bojanowski et al., 2017; Grave et al., 2018), because it also has embedded fine-grained sublexical information such as morphology in English and characters in Chinese. 刚才 我 不是 跟 你 讲 我 apply 那个 job “Haven’t I told you just now that I applied for that job?” 3.2 Translated equivalents We used the Princeton WordNet of English (Miller, 1995) and the Chinese Open WordNet (Wang and Bond, 2013) from Open Multilingual WordNet (Bond and Foster, 2013) to translate the CS words. The two WordNets use common word senses. Therefore, if a word in one WordNet shares the same word sense(s) with a word in the other WordNet, the two words are considered translated equivalents of each other (See Figure 3 for an example). Since a word can have multiple translated 1 21 https://fasttext.cc/docs/en/crawl-vectors.html 3.4 Semantic networks A weighted semantic network was built for each language. Because analyzing large weighted semantic networks can be computationally expensive, we constructed the se"
A00-2019,W95-0104,0,0.0356988,"extual cues and selecting the most similar word sense model (e.g., Leacock, Chodorow and Miller (1998), Yarowsky (1993)). A major objective of this research is to avoid the laborious and costly process of collecting errors (or negative evidence) for each word that we wish to evaluate. Instead, we train ALEK on a general corpus of English and on edited text containing example uses of the target word. The system identifies inappropriate usage based on differences between the word's local context cues in an essay and the models of context it has derived from the corpora of well-formed sentences. Golding (1995) showed how methods used for WSD (decision lists and Bayesian classifiers) could be adapted to detect errors resulting from 140 common spelling confusions among sets such as there, their, and they 're. He extracted contexts from correct usage of each confusable word in a training corpus and then identified a new occurrence as an error when it matched the wrong context. ALEK infers negative evidence from the contextual cues that do not co-occur with the target word - either in the word specific corpus or in the general English one. It uses two kinds o f contextual cues in a +2 word window aroun"
A00-2019,J98-1006,1,0.722536,"Missing"
A00-2019,P98-2196,0,0.022538,"ow of one or two words around it. Statistically-based computer programs have been able to do the same with a high level of accuracy (Kilgarriff and Palmer, 2000). The goal of our work is to automatically identify inappropriate usage of specific vocabulary words in essays by looking at the local contextual cues around a target word. We have developed a statistical system, ALEK (Assessing Le____xicalKnowledge), that uses statistical analysis for this purpose. 1 Background Approaches to detecting errors by non-native writers typically produce grammars that look for specific expected error types (Schneider and McCoy, 1998; Park, Palmer and Washburn, 1997). Under this approach, essays written by ESL students are collected and examined for errors. Parsers are then adapted to identify those error types that were found in the essay collection. We take a different approach, initially viewing error detection as an extension of the word sense disambiguation (WSD) problem. Corpus-based WSD systems identify the intended sense of a polysemous word by (1) collecting a set of example sentences for each of its various senses and (2) extracting salient contextual cues from these sets to (3) build a statistical model for eac"
A00-2019,H93-1052,0,0.0261636,"in the essay collection. We take a different approach, initially viewing error detection as an extension of the word sense disambiguation (WSD) problem. Corpus-based WSD systems identify the intended sense of a polysemous word by (1) collecting a set of example sentences for each of its various senses and (2) extracting salient contextual cues from these sets to (3) build a statistical model for each sense. They identify the intended sense of a word in a novel sentence by extracting its contextual cues and selecting the most similar word sense model (e.g., Leacock, Chodorow and Miller (1998), Yarowsky (1993)). A major objective of this research is to avoid the laborious and costly process of collecting errors (or negative evidence) for each word that we wish to evaluate. Instead, we train ALEK on a general corpus of English and on edited text containing example uses of the target word. The system identifies inappropriate usage based on differences between the word's local context cues in an essay and the models of context it has derived from the corpora of well-formed sentences. Golding (1995) showed how methods used for WSD (decision lists and Bayesian classifiers) could be adapted to detect err"
A00-2019,C98-2191,0,\N,Missing
A88-1020,J87-3005,0,0.0705859,"Missing"
A88-1020,P85-1037,1,0.847396,"nals and archives have the plural records; and diary has the phrase daily record. We believe that the CT-synonyms are nontransitive for many of these same reasons. Sense Disanzbiguation Is it possible to reach any noun in CT by following the synonym links to and from any other noun? The answer is NO, but almost. Computing the transitive closure over the synonyms of the noun house, where we include the words listed in the entry for house and the 4 s The computational tool we have used for computing the transitive closure over synonymy is a program known as S P R O U T . It was originally used (Chodorow, et al., 1985) to generate taxonomic trees from the h y p o n y m relation as extracted from Webster's Seventh Collegiate Dictionary (Merriam 1963). S P R O U T starts with a root node and retrieves from a designated file (in this case, a D A M fde) the words that bear the given relation to the root. These words are the ftrst-level descendents (daughters) of the root. S P R O U T then applies recursively to each of the daughter nodes, generating their daughters, etc. In this way, the tree is generated in a breadth-first fashion. The process is complete when the only nodes that remain open are either termina"
A88-1020,A88-1012,0,0.126001,"plausible one. If the two nouns are semantically distant, the alternate attachment is more plausible. An automatic measure of semantic distance can assist information retrieval systems as well. One can conceive of a system which will retrieve documents containing synonyms of the key word by first searching for a very restrictive set of synonyms (first-level synonyms perhaps). If not enough documents are retrieved, words that are more distant semantically can be searched for as well. Another application for which a sprouted synonym tree is useful is third-generation on-line dictionary systems (Neff, et al., 1988). Among other things, these systems display synonyms to users who are editing natural language texts. The list of synonyms presented by the system can be arranged according to the semantic distance between the word interrogated and the words on the synonym list. It should be noted, however, that for this application, words need to be arranged according to additional parameters as well. Synonyms that are polysemous or rare may be poor substitution candidates in a general text. Finally, we are now investigating the possible use of our tools by lexicographers who wish to update and revise an exis"
C08-1109,P96-1042,0,0.0113352,"produce an overall Hit rate (0.08). Overall rates for FPs and Misses are calculated in a similar manner. 7. Using the values from step 6, calculate Precision (Hits/(Hits + FP)) and Recall (Hits/(Hits + Misses)). These are shown in the last two rows of Table 3. Hits FP Misses Precision Recall Estimated Overall Rates Sample Proportion * Sub-Corpus Proportion 0.80 * 0.10 = 0.08 0.20 * 0.10 = 0.02 0.30 * 0.90 = 0.27 0.08/(0.08 + 0.02) = 0.80 0.08/(0.08 + 0.27) = 0.23 Table 3: Sampling Calculations (Hypothetical) This method is similar in spirit to active learning ((Dagan and Engelson, 1995) and (Engelson and Dagan, 1996)), which has been used to iteratively build up an annotated corpus, but it differs from active learning applications in that there are no iterative loops between the system and the human annotator(s). In addition, while our methodology is used for evaluating a system, active learning is commonly used for training a system. 4.2 Application Next, we tested whether our proposed sampling approach provides good estimates of a system’s performance. For this task, we used the +Combo:word model to separate a large corpus of student essays into the “Error” and “OK” subcorpora. The original corpus total"
C08-1109,I08-1059,0,0.655628,"Missing"
C08-1109,C94-1042,0,0.332691,"ng 2008), pages 865–872 Manchester, August 2008 development and evaluation of such a system. First, we describe a machine learning system that detects preposition errors in essays of ESL writers. To date there have been relatively few attempts to address preposition error detection, though the sister task of detecting determiner errors has been the focus of more research. Our system performs comparably with other leading systems. We extend our previous work (Chodorow et al., 2007) by experimenting with combination features, as well as features derived from the Google N-Gram corpus and Comlex (Grishman et al., 1994). Second, we discuss drawbacks in current methods of annotating ESL data and evaluating error detection systems, which are not limited to preposition errors. While the need for annotation by multiple raters has been well established in NLP tasks (Carletta, 1996), most previous work in error detection has surprisingly relied on only one rater to either create an annotated corpus of learner errors, or to check the system’s output. Some grammatical errors, such as number disagreement between subject and verb, no doubt show very high reliability, but others, such as usage errors involving preposit"
C08-1109,P03-2026,0,0.798728,"Missing"
C08-1109,izumi-etal-2004-overview,0,0.148294,"Missing"
C08-1109,W00-0708,0,0.0557462,"Missing"
C08-1109,P06-1031,0,0.132405,"Missing"
C08-1109,J96-2004,0,0.024601,"ror detection, though the sister task of detecting determiner errors has been the focus of more research. Our system performs comparably with other leading systems. We extend our previous work (Chodorow et al., 2007) by experimenting with combination features, as well as features derived from the Google N-Gram corpus and Comlex (Grishman et al., 1994). Second, we discuss drawbacks in current methods of annotating ESL data and evaluating error detection systems, which are not limited to preposition errors. While the need for annotation by multiple raters has been well established in NLP tasks (Carletta, 1996), most previous work in error detection has surprisingly relied on only one rater to either create an annotated corpus of learner errors, or to check the system’s output. Some grammatical errors, such as number disagreement between subject and verb, no doubt show very high reliability, but others, such as usage errors involving prepositions or determiners are likely to be much less reliable. Our results show that relying on one rater for system evaluation can be problematic, and we provide a sampling approach which can facilitate using multiple raters for this task. In the next section, we des"
C08-1109,A00-2019,1,0.860565,"Missing"
C08-1109,W08-1205,1,0.547726,"Missing"
C08-1109,W07-1604,1,0.733173,"r detection system. This paper addresses both the 865 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 865–872 Manchester, August 2008 development and evaluation of such a system. First, we describe a machine learning system that detects preposition errors in essays of ESL writers. To date there have been relatively few attempts to address preposition error detection, though the sister task of detecting determiner errors has been the focus of more research. Our system performs comparably with other leading systems. We extend our previous work (Chodorow et al., 2007) by experimenting with combination features, as well as features derived from the Google N-Gram corpus and Comlex (Grishman et al., 1994). Second, we discuss drawbacks in current methods of annotating ESL data and evaluating error detection systems, which are not limited to preposition errors. While the need for annotation by multiple raters has been well established in NLP tasks (Carletta, 1996), most previous work in error detection has surprisingly relied on only one rater to either create an annotated corpus of learner errors, or to check the system’s output. Some grammatical errors, such"
C08-1109,N07-2045,0,0.0614255,"Missing"
C12-1038,boyd-2010-eagle,0,0.0439876,"of string distance, there is no way to talk about true negatives. Errors can be of variable sizes, but this means that the size of non-errors is, in a sense, undefined. Thus, alternative measures like κ cannot be calculated. The issue of unit size becomes more complicated when error types interact. In (3), for instance, if a learner’s preposition in should be corrected to to, then the article can be dropped. If, however, the preposition is not changed, then dropping the article makes the sentence worse. This is because one correction leads to another, as noted by many doing error annotation (Boyd, 2010; Hana et al., 2010; Lee et al., 2012; Dickinson and Ledbetter, 2012). As far as we aware, no system evaluation accounts for this problem, and we do not solve it, either. (3) Every day we go { in the school → to school } Recommendations How one treats positives depends upon one’s purpose(s). If matching to a correct string and P/R are all that is required, EDMs have several excellent properties, including being able to account for errors of variable type and size (above the level of the base unit). If feedback is desired, EDMs are not a good choice because each error type must be taken into ac"
C12-1038,D11-1010,0,0.0149267,"12: Technical Papers, pages 611–628, COLING 2012, Mumbai, December 2012. 611 1 Introduction With hundreds of millions of people worldwide learning second, or even third, languages (Leacock et al., 2010), there is a large and growing need for NLP systems that automatically detect and correct the grammar and word usage errors that learners make. In response to this need, NLP researchers have developed tools to target errors involving articles (Han et al., 2006), prepositions (Tetreault and Chodorow, 2008b), particles (Dickinson et al., 2011), verb forms (Lee and Seneff, 2008), and collocations (Dahlmeier and Ng, 2011). Research in this field has surged over the last few years, culminating notably in two recent “Helping Our Own” (HOO) Shared Tasks (Dale and Kilgarriff, 2010) and (Dale et al., 2012), which are concerned with automated error correction in texts authored by non-native speakers of English working in NLP. However, despite this high level of activity and interest, there is relatively little consensus on how best to evaluate grammatical error detection/correction systems. This makes it hard to measure performance and compare systems, which is essential for progress in the field. The goal of this p"
C12-1038,N12-1067,0,0.0368454,"a string-based definition can refer to both words as a single error. (2) a. The book in my class inspire me. b. The book in my class inspires me. c. The books in my class inspire me. There are several issues involved in choosing the unit size. First, the definition of unit size affects whether a system is given credit for finding all and only the errors, as these examples illustrate. Identifying inspire as an error may or may not be sufficient; identifying both words may be overkill. Defining error detection metrics in terms of edit distance mitigates this problem for evaluation comparisons (Dahlmeier and Ng, 2012), as correcting inspire to inspires is handled the same as book ... inspire corrected to book ... inspires. In essence, edit distance measures (EDMs) compare the system output to the correct string, ignoring exactly how it was derived. 619 Moreover, EDMs naturally handle multiple, overlapping errors, a problem for systems that target only a specific error type (Gamon, 2010; Rozovskaya and Roth, 2010a). Taking an example from Dahlmeier and Ng (2012), the sequence ... development set similar with test set ... can be corrected as a preposition selection error with → to and an adjacent article omi"
C12-1038,W12-2006,0,0.47295,"et al., 2010), there is a large and growing need for NLP systems that automatically detect and correct the grammar and word usage errors that learners make. In response to this need, NLP researchers have developed tools to target errors involving articles (Han et al., 2006), prepositions (Tetreault and Chodorow, 2008b), particles (Dickinson et al., 2011), verb forms (Lee and Seneff, 2008), and collocations (Dahlmeier and Ng, 2011). Research in this field has surged over the last few years, culminating notably in two recent “Helping Our Own” (HOO) Shared Tasks (Dale and Kilgarriff, 2010) and (Dale et al., 2012), which are concerned with automated error correction in texts authored by non-native speakers of English working in NLP. However, despite this high level of activity and interest, there is relatively little consensus on how best to evaluate grammatical error detection/correction systems. This makes it hard to measure performance and compare systems, which is essential for progress in the field. The goal of this paper is to draw attention to the many evaluation issues in error detection that have largely been overlooked in the past and which make it hard to draw meaningful comparisons between"
C12-1038,W10-4236,0,0.0232724,"r even third, languages (Leacock et al., 2010), there is a large and growing need for NLP systems that automatically detect and correct the grammar and word usage errors that learners make. In response to this need, NLP researchers have developed tools to target errors involving articles (Han et al., 2006), prepositions (Tetreault and Chodorow, 2008b), particles (Dickinson et al., 2011), verb forms (Lee and Seneff, 2008), and collocations (Dahlmeier and Ng, 2011). Research in this field has surged over the last few years, culminating notably in two recent “Helping Our Own” (HOO) Shared Tasks (Dale and Kilgarriff, 2010) and (Dale et al., 2012), which are concerned with automated error correction in texts authored by non-native speakers of English working in NLP. However, despite this high level of activity and interest, there is relatively little consensus on how best to evaluate grammatical error detection/correction systems. This makes it hard to measure performance and compare systems, which is essential for progress in the field. The goal of this paper is to draw attention to the many evaluation issues in error detection that have largely been overlooked in the past and which make it hard to draw meaning"
C12-1038,dale-narroway-2012-framework,0,0.0329995,"Missing"
C12-1038,W11-1410,1,0.932988,"ror detection, system evaluation, evaluation metrics. Proceedings of COLING 2012: Technical Papers, pages 611–628, COLING 2012, Mumbai, December 2012. 611 1 Introduction With hundreds of millions of people worldwide learning second, or even third, languages (Leacock et al., 2010), there is a large and growing need for NLP systems that automatically detect and correct the grammar and word usage errors that learners make. In response to this need, NLP researchers have developed tools to target errors involving articles (Han et al., 2006), prepositions (Tetreault and Chodorow, 2008b), particles (Dickinson et al., 2011), verb forms (Lee and Seneff, 2008), and collocations (Dahlmeier and Ng, 2011). Research in this field has surged over the last few years, culminating notably in two recent “Helping Our Own” (HOO) Shared Tasks (Dale and Kilgarriff, 2010) and (Dale et al., 2012), which are concerned with automated error correction in texts authored by non-native speakers of English working in NLP. However, despite this high level of activity and interest, there is relatively little consensus on how best to evaluate grammatical error detection/correction systems. This makes it hard to measure performance and com"
C12-1038,dickinson-ledbetter-2012-annotating,1,0.843068,"t true negatives. Errors can be of variable sizes, but this means that the size of non-errors is, in a sense, undefined. Thus, alternative measures like κ cannot be calculated. The issue of unit size becomes more complicated when error types interact. In (3), for instance, if a learner’s preposition in should be corrected to to, then the article can be dropped. If, however, the preposition is not changed, then dropping the article makes the sentence worse. This is because one correction leads to another, as noted by many doing error annotation (Boyd, 2010; Hana et al., 2010; Lee et al., 2012; Dickinson and Ledbetter, 2012). As far as we aware, no system evaluation accounts for this problem, and we do not solve it, either. (3) Every day we go { in the school → to school } Recommendations How one treats positives depends upon one’s purpose(s). If matching to a correct string and P/R are all that is required, EDMs have several excellent properties, including being able to account for errors of variable type and size (above the level of the base unit). If feedback is desired, EDMs are not a good choice because each error type must be taken into account separately (which is largely done in Dale and Narroway (2011))."
C12-1038,N10-1019,0,0.0422643,"se in real world applications. 3.4 Variety in Reporting Evaluation Measures As a result of the many issues involved in evaluating grammatical error correction systems, it is perhaps unsurprising that there is little consensus about which evaluation metric to use. For example, Leacock et al. (2010) describe how, in 2008, there were three papers on preposition error detection and each used slightly different evaluation metrics making comparison impossible. More recently, Tetreault et al. (2010) and Dickinson et al. (2011) report P and R for work done on prepositions and particles, respectively. Gamon (2010) also used P and R, while Rozovskaya and Roth (2010b) and Rozovskaya and Roth (2011) reported A of usage in a test corpus before and after corrections were made by their system. Sometimes, different metrics have been used for two aspects of the same task, as in Han et al. (2010), where A was reported for determining the presence vs. absence of a preposition, but P and R were used to evaluate performance in detecting when an incorrect preposition was used by the writer. Some researchers have even looked towards more holistic, sentence-level metrics like BLEU and METEOR (see, e.g. Park and Levy,"
C12-1038,W11-1422,0,0.0290787,"2011) reported A of usage in a test corpus before and after corrections were made by their system. Sometimes, different metrics have been used for two aspects of the same task, as in Han et al. (2010), where A was reported for determining the presence vs. absence of a preposition, but P and R were used to evaluate performance in detecting when an incorrect preposition was used by the writer. Some researchers have even looked towards more holistic, sentence-level metrics like BLEU and METEOR (see, e.g. Park and Levy, 2011), or modifying measures like P and R to consider entire sentences, as in Gamon (2011). In the two HOO Shared Tasks, P, R and F1 were all reported, but no preference was given to which one to use to rank the systems in the end. 618 Recommendations It is clear that no single measure of performance is best for all purposes; each one has its own set of advantages and disadvantages. However, all of the measures, aside from BLEU and METEOR, are based on the same four values, the counts for TP, FP, FN, and TN. We recommend reporting these four in addition to any metrics derived from them. This will enable readers to calculate other measures that the authors of a particular paper did"
C12-1038,han-etal-2010-using,1,0.627698,"e, Leacock et al. (2010) describe how, in 2008, there were three papers on preposition error detection and each used slightly different evaluation metrics making comparison impossible. More recently, Tetreault et al. (2010) and Dickinson et al. (2011) report P and R for work done on prepositions and particles, respectively. Gamon (2010) also used P and R, while Rozovskaya and Roth (2010b) and Rozovskaya and Roth (2011) reported A of usage in a test corpus before and after corrections were made by their system. Sometimes, different metrics have been used for two aspects of the same task, as in Han et al. (2010), where A was reported for determining the presence vs. absence of a preposition, but P and R were used to evaluate performance in detecting when an incorrect preposition was used by the writer. Some researchers have even looked towards more holistic, sentence-level metrics like BLEU and METEOR (see, e.g. Park and Levy, 2011), or modifying measures like P and R to consider entire sentences, as in Gamon (2011). In the two HOO Shared Tasks, P, R and F1 were all reported, but no preference was given to which one to use to rank the systems in the end. 618 Recommendations It is clear that no single"
C12-1038,W10-1802,0,0.053335,"istance, there is no way to talk about true negatives. Errors can be of variable sizes, but this means that the size of non-errors is, in a sense, undefined. Thus, alternative measures like κ cannot be calculated. The issue of unit size becomes more complicated when error types interact. In (3), for instance, if a learner’s preposition in should be corrected to to, then the article can be dropped. If, however, the preposition is not changed, then dropping the article makes the sentence worse. This is because one correction leads to another, as noted by many doing error annotation (Boyd, 2010; Hana et al., 2010; Lee et al., 2012; Dickinson and Ledbetter, 2012). As far as we aware, no system evaluation accounts for this problem, and we do not solve it, either. (3) Every day we go { in the school → to school } Recommendations How one treats positives depends upon one’s purpose(s). If matching to a correct string and P/R are all that is required, EDMs have several excellent properties, including being able to account for errors of variable type and size (above the level of the base unit). If feedback is desired, EDMs are not a good choice because each error type must be taken into account separately (w"
C12-1038,N12-1029,1,0.845321,"ccuracy (A), Precision (P), Recall (R), true-negative rate (TNR), and F-score (F1 ), as shown in Figure 2. System prediction Comma (+) No comma (-) Annotation (Gold Standard) Comma (+) No comma (-) TP FP FN TN Figure 1: The basis for typical NLP system evaluation T P+T N T P+T N +F P+F N TP T P+F P TP T P+F N Accuracy (A) = Precision (P)= Recall (R)= True Negative Rate (TNR) = F-measure (F1 ) = 2 · P·R P+R TN T N +F P Figure 2: Evaluation metrics 3.2 Error Detection and the Three-Way Contingency Table Now consider a task that is similar to comma restoration, the task of comma error detection (Israel et al., 2012), in which a system seeks to find and correct errors in the writer’s usage of commas. For this task, the positive class is not the presence of a comma but rather an error of the writer’s that involves a comma. Therefore, it is necessary to compare what the writer has written to an annotator’s judgment, and only if there is a mismatch between the two do we have an error (the positive class); when writer and annotator agree, the case is a non-error (the negative class). The traditional 2x2 table is no longer sufficient to represent all of the contingencies, which must instead be laid out in the"
C12-1038,P08-1021,0,0.0160292,"luation metrics. Proceedings of COLING 2012: Technical Papers, pages 611–628, COLING 2012, Mumbai, December 2012. 611 1 Introduction With hundreds of millions of people worldwide learning second, or even third, languages (Leacock et al., 2010), there is a large and growing need for NLP systems that automatically detect and correct the grammar and word usage errors that learners make. In response to this need, NLP researchers have developed tools to target errors involving articles (Han et al., 2006), prepositions (Tetreault and Chodorow, 2008b), particles (Dickinson et al., 2011), verb forms (Lee and Seneff, 2008), and collocations (Dahlmeier and Ng, 2011). Research in this field has surged over the last few years, culminating notably in two recent “Helping Our Own” (HOO) Shared Tasks (Dale and Kilgarriff, 2010) and (Dale et al., 2012), which are concerned with automated error correction in texts authored by non-native speakers of English working in NLP. However, despite this high level of activity and interest, there is relatively little consensus on how best to evaluate grammatical error detection/correction systems. This makes it hard to measure performance and compare systems, which is essential fo"
C12-1038,W12-3617,1,0.849254,"o way to talk about true negatives. Errors can be of variable sizes, but this means that the size of non-errors is, in a sense, undefined. Thus, alternative measures like κ cannot be calculated. The issue of unit size becomes more complicated when error types interact. In (3), for instance, if a learner’s preposition in should be corrected to to, then the article can be dropped. If, however, the preposition is not changed, then dropping the article makes the sentence worse. This is because one correction leads to another, as noted by many doing error annotation (Boyd, 2010; Hana et al., 2010; Lee et al., 2012; Dickinson and Ledbetter, 2012). As far as we aware, no system evaluation accounts for this problem, and we do not solve it, either. (3) Every day we go { in the school → to school } Recommendations How one treats positives depends upon one’s purpose(s). If matching to a correct string and P/R are all that is required, EDMs have several excellent properties, including being able to account for errors of variable type and size (above the level of the base unit). If feedback is desired, EDMs are not a good choice because each error type must be taken into account separately (which is largely do"
C12-1038,W03-0209,0,0.0191481,"task the system is used for, so we begin with a brief overview of applications of grammatical error detection. 2 Applications It is not surprising that most applications of grammatical error detection are in education, where it has been used for student assessment and to support language learning. A more recent set of applications focuses on improving systems within the domain of NLP itself. Automatically scoring essays Error detection is a fundamental component in most systems which perform automated essay scoring (Yannakoudakis et al., 2011; Attali and Burstein, 2006; Burstein et al., 2003; Lonsdale and Strong-Krause, 2003). The goal here is to find aspects of grammar and word usage related to overall text quality so that a holistic score, usually on a 5or 6-point scale, can be generated. Essay scoring systems also measure the range of vocabulary, discourse structure, and the mechanics of writing (e.g., spelling) as predictors of the writing score. These systems are used for large scale high-stakes tests taken by native and non-native speakers, such as the Graduate Record Exam (GRE), and for tests of non-native proficiency, such as the Test of English as a Foreign Language (TOEFL). Improving writing quality To d"
C12-1038,P11-2089,1,0.69726,"Missing"
C12-1038,P11-1094,0,0.0174037,". Gamon (2010) also used P and R, while Rozovskaya and Roth (2010b) and Rozovskaya and Roth (2011) reported A of usage in a test corpus before and after corrections were made by their system. Sometimes, different metrics have been used for two aspects of the same task, as in Han et al. (2010), where A was reported for determining the presence vs. absence of a preposition, but P and R were used to evaluate performance in detecting when an incorrect preposition was used by the writer. Some researchers have even looked towards more holistic, sentence-level metrics like BLEU and METEOR (see, e.g. Park and Levy, 2011), or modifying measures like P and R to consider entire sentences, as in Gamon (2011). In the two HOO Shared Tasks, P, R and F1 were all reported, but no preference was given to which one to use to rank the systems in the end. 618 Recommendations It is clear that no single measure of performance is best for all purposes; each one has its own set of advantages and disadvantages. However, all of the measures, aside from BLEU and METEOR, are based on the same four values, the counts for TP, FP, FN, and TN. We recommend reporting these four in addition to any metrics derived from them. This will e"
C12-1038,W11-2111,1,0.821062,"n order to properly model behavior (Amaral and Meurers, 2007). For both purposes, error detection is needed to detect errors, suggest corrections, and provide information about the linguistic properties of the writer’s mistakes. Applications within NLP Grammatical error detection can be a useful component in correcting and evaluating text generated in various NLP applications. Among other applications, it can be used to detect errors in machine translation (MT) output (such as in Knight and Chander (1994) and Peng and Araki (2005)) and can even be incorporated in quality metrics to assess MT (Parton et al., 2011). In these contexts, an NLP system takes the place of the writer, but the goal is similar, namely to produce more error-free language. 613 3 Measuring Performance 3.1 Traditional Evaluation Measures To illustrate the traditional measures of NLP system evaluation, consider as an example the task of comma restoration (see, e.g. Shieber and Tao, 2003), in which the commas are removed from a well-edited text (the gold standard) and a system attempts to restore them by predicting their locations. For evaluations involving a binary distinction such as this one (the presence vs. absence of a comma),"
C12-1038,E12-1035,0,0.325261,"ects (TP) to the total number of errors in the Annotated gold standard (TP+FN); P compares TP to the total number of errors that the System reports (TP+FP); and F1 is the harmonic mean of R and P. Unfortunately, all three measures are affected by the proportion of cases that are annotated as errors in the gold standard (referred to as the prevalence of the errors, which is equal to (TP+FN)/N, where N is the total number of cases, i.e., N = TP+TN+FP+FN) and by the proportion of cases that are reported by the System as errors (referred to as the bias of the System, which is equal to (TP+FP)/N). Powers (2012) demonstrates how a system that performs no better than chance will nonetheless show an increase in R when prevalence increases and an increase in P when bias increases. To understand this behavior, we must consider what it means to perform at chance. If the class labels Error and No Error are assigned to cases independently by the Annotator and the System, then these labels are expected to match a proportion of the time by chance alone a proportion equal to the product of their probabilities. For example, the expected proportion of TP matches is equal to the product of the proportion of cases"
C12-1038,W10-1004,0,0.314583,"iety in Reporting Evaluation Measures As a result of the many issues involved in evaluating grammatical error correction systems, it is perhaps unsurprising that there is little consensus about which evaluation metric to use. For example, Leacock et al. (2010) describe how, in 2008, there were three papers on preposition error detection and each used slightly different evaluation metrics making comparison impossible. More recently, Tetreault et al. (2010) and Dickinson et al. (2011) report P and R for work done on prepositions and particles, respectively. Gamon (2010) also used P and R, while Rozovskaya and Roth (2010b) and Rozovskaya and Roth (2011) reported A of usage in a test corpus before and after corrections were made by their system. Sometimes, different metrics have been used for two aspects of the same task, as in Han et al. (2010), where A was reported for determining the presence vs. absence of a preposition, but P and R were used to evaluate performance in detecting when an incorrect preposition was used by the writer. Some researchers have even looked towards more holistic, sentence-level metrics like BLEU and METEOR (see, e.g. Park and Levy, 2011), or modifying measures like P and R to consi"
C12-1038,N10-1018,0,0.101343,"iety in Reporting Evaluation Measures As a result of the many issues involved in evaluating grammatical error correction systems, it is perhaps unsurprising that there is little consensus about which evaluation metric to use. For example, Leacock et al. (2010) describe how, in 2008, there were three papers on preposition error detection and each used slightly different evaluation metrics making comparison impossible. More recently, Tetreault et al. (2010) and Dickinson et al. (2011) report P and R for work done on prepositions and particles, respectively. Gamon (2010) also used P and R, while Rozovskaya and Roth (2010b) and Rozovskaya and Roth (2011) reported A of usage in a test corpus before and after corrections were made by their system. Sometimes, different metrics have been used for two aspects of the same task, as in Han et al. (2010), where A was reported for determining the presence vs. absence of a preposition, but P and R were used to evaluate performance in detecting when an incorrect preposition was used by the writer. Some researchers have even looked towards more holistic, sentence-level metrics like BLEU and METEOR (see, e.g. Park and Levy, 2011), or modifying measures like P and R to consi"
C12-1038,P11-1093,0,0.0376992,"sures As a result of the many issues involved in evaluating grammatical error correction systems, it is perhaps unsurprising that there is little consensus about which evaluation metric to use. For example, Leacock et al. (2010) describe how, in 2008, there were three papers on preposition error detection and each used slightly different evaluation metrics making comparison impossible. More recently, Tetreault et al. (2010) and Dickinson et al. (2011) report P and R for work done on prepositions and particles, respectively. Gamon (2010) also used P and R, while Rozovskaya and Roth (2010b) and Rozovskaya and Roth (2011) reported A of usage in a test corpus before and after corrections were made by their system. Sometimes, different metrics have been used for two aspects of the same task, as in Han et al. (2010), where A was reported for determining the presence vs. absence of a preposition, but P and R were used to evaluate performance in detecting when an incorrect preposition was used by the writer. Some researchers have even looked towards more holistic, sentence-level metrics like BLEU and METEOR (see, e.g. Park and Levy, 2011), or modifying measures like P and R to consider entire sentences, as in Gamon"
C12-1038,N03-1029,0,0.0228136,"ed in various NLP applications. Among other applications, it can be used to detect errors in machine translation (MT) output (such as in Knight and Chander (1994) and Peng and Araki (2005)) and can even be incorporated in quality metrics to assess MT (Parton et al., 2011). In these contexts, an NLP system takes the place of the writer, but the goal is similar, namely to produce more error-free language. 613 3 Measuring Performance 3.1 Traditional Evaluation Measures To illustrate the traditional measures of NLP system evaluation, consider as an example the task of comma restoration (see, e.g. Shieber and Tao, 2003), in which the commas are removed from a well-edited text (the gold standard) and a system attempts to restore them by predicting their locations. For evaluations involving a binary distinction such as this one (the presence vs. absence of a comma), a comparison between the system’s output and the annotator’s judgments (the gold standard) can be organized as a two-by-two contingency table, shown in Figure 1. Presence of a comma is the target or positive class, and absence is the negative class. Positions in the text where both the system and the gold standard indicate that there should be a co"
C12-1038,W08-1205,1,0.759732,"e to move forward. KEYWORDS: grammatical error detection, system evaluation, evaluation metrics. Proceedings of COLING 2012: Technical Papers, pages 611–628, COLING 2012, Mumbai, December 2012. 611 1 Introduction With hundreds of millions of people worldwide learning second, or even third, languages (Leacock et al., 2010), there is a large and growing need for NLP systems that automatically detect and correct the grammar and word usage errors that learners make. In response to this need, NLP researchers have developed tools to target errors involving articles (Han et al., 2006), prepositions (Tetreault and Chodorow, 2008b), particles (Dickinson et al., 2011), verb forms (Lee and Seneff, 2008), and collocations (Dahlmeier and Ng, 2011). Research in this field has surged over the last few years, culminating notably in two recent “Helping Our Own” (HOO) Shared Tasks (Dale and Kilgarriff, 2010) and (Dale et al., 2012), which are concerned with automated error correction in texts authored by non-native speakers of English working in NLP. However, despite this high level of activity and interest, there is relatively little consensus on how best to evaluate grammatical error detection/correction systems. This makes"
C12-1038,C08-1109,1,0.657801,"e to move forward. KEYWORDS: grammatical error detection, system evaluation, evaluation metrics. Proceedings of COLING 2012: Technical Papers, pages 611–628, COLING 2012, Mumbai, December 2012. 611 1 Introduction With hundreds of millions of people worldwide learning second, or even third, languages (Leacock et al., 2010), there is a large and growing need for NLP systems that automatically detect and correct the grammar and word usage errors that learners make. In response to this need, NLP researchers have developed tools to target errors involving articles (Han et al., 2006), prepositions (Tetreault and Chodorow, 2008b), particles (Dickinson et al., 2011), verb forms (Lee and Seneff, 2008), and collocations (Dahlmeier and Ng, 2011). Research in this field has surged over the last few years, culminating notably in two recent “Helping Our Own” (HOO) Shared Tasks (Dale and Kilgarriff, 2010) and (Dale et al., 2012), which are concerned with automated error correction in texts authored by non-native speakers of English working in NLP. However, despite this high level of activity and interest, there is relatively little consensus on how best to evaluate grammatical error detection/correction systems. This makes"
C12-1038,P10-2065,1,0.845196,"use, as shown above, it does not reflect A, P, or R. This limits its value for decisions about whether a system is ready for use in real world applications. 3.4 Variety in Reporting Evaluation Measures As a result of the many issues involved in evaluating grammatical error correction systems, it is perhaps unsurprising that there is little consensus about which evaluation metric to use. For example, Leacock et al. (2010) describe how, in 2008, there were three papers on preposition error detection and each used slightly different evaluation metrics making comparison impossible. More recently, Tetreault et al. (2010) and Dickinson et al. (2011) report P and R for work done on prepositions and particles, respectively. Gamon (2010) also used P and R, while Rozovskaya and Roth (2010b) and Rozovskaya and Roth (2011) reported A of usage in a test corpus before and after corrections were made by their system. Sometimes, different metrics have been used for two aspects of the same task, as in Han et al. (2010), where A was reported for determining the presence vs. absence of a preposition, but P and R were used to evaluate performance in detecting when an incorrect preposition was used by the writer. Some resear"
C12-1038,P11-1019,0,0.0332827,"uation. Certain metrics are more or less appropriate depending on the type of task the system is used for, so we begin with a brief overview of applications of grammatical error detection. 2 Applications It is not surprising that most applications of grammatical error detection are in education, where it has been used for student assessment and to support language learning. A more recent set of applications focuses on improving systems within the domain of NLP itself. Automatically scoring essays Error detection is a fundamental component in most systems which perform automated essay scoring (Yannakoudakis et al., 2011; Attali and Burstein, 2006; Burstein et al., 2003; Lonsdale and Strong-Krause, 2003). The goal here is to find aspects of grammar and word usage related to overall text quality so that a holistic score, usually on a 5or 6-point scale, can be generated. Essay scoring systems also measure the range of vocabulary, discourse structure, and the mechanics of writing (e.g., spelling) as predictors of the writing score. These systems are used for large scale high-stakes tests taken by native and non-native speakers, such as the Graduate Record Exam (GRE), and for tests of non-native proficiency, such"
C12-1158,N12-1033,0,0.0956259,"Missing"
C12-1158,brooke-hirst-2012-measuring,0,0.126261,"ich features actually perform best. A second problem is that there is no consensus on the scope of the evaluation. The ICLE contains English essays written by native speakers of 16 languages. Typically a subset of 7 languages is used in the evaluations, although more recently some work has reported results for a larger set. Moreover, when researchers report results for 7 languages, they are not always reporting on the same 7 languages. For example, in the work of Wong and Dras (2011) the 7 native languages (L1s) are Bulgarian, Chinese, Czech, French, Japanese, Russian, and Spanish. Whereas in Brooke and Hirst (2012), Italian and Polish are used instead of Bulgarian and Czech. In addition, different researchers have split the corpus in different ways when training and evaluating their systems, making it even more difficult to compare results across experiments. 1 2 Note that Kochmar (2011) used a subsection of the Cambridge Learner Corpus. Throughout this paper, we will refer to ICLE version 2 as ICLE. In this paper, we first provide an automatic method for extracting data from the ICLE corpus to remove some of the corpus-specific idiosyncracies that automatic Native Language Identification classifiers cu"
C12-1158,D11-1010,0,0.020331,"sification, Corpora. 1 Introduction One growing NLP field is that of Native Language Identification (NLI), which is the task of automatically identifying a speaker’s first language based solely on the speaker’s writing in another language. NLI can be useful for a number of applications. Native language is often used as a feature in machine learning approaches to authorship profiling (Estival et al., 2007), which is frequently used in forensic linguistics. NLI can also be used in educational settings to provide more targeted feedback to language learners about their errors (Chang et al., 2008; Dahlmeier and Ng, 2011; Rozovskaya and Roth, 2011). It is well known that speakers of different languages make different kinds of errors when learning a language (Swan and Smith, 2001). For example, a French speaker learning English might write sentence (1), which contains a verb tense error. On the other hand, a Japanese speaker learning English might make the verb tense error shown in (2). A writing tutor system which can detect the native language of the learner will be able to tailor the feedback about the error and contrast it with common properties of the learner’s language. (1) She knows that she hasn’t achi"
C12-1158,de-marneffe-etal-2006-generating,0,0.0145892,"Missing"
C12-1158,P11-2038,0,0.0920066,"Missing"
C12-1158,P09-2012,0,0.0654302,"Missing"
C12-1158,P11-1093,0,0.0289759,"ntroduction One growing NLP field is that of Native Language Identification (NLI), which is the task of automatically identifying a speaker’s first language based solely on the speaker’s writing in another language. NLI can be useful for a number of applications. Native language is often used as a feature in machine learning approaches to authorship profiling (Estival et al., 2007), which is frequently used in forensic linguistics. NLI can also be used in educational settings to provide more targeted feedback to language learners about their errors (Chang et al., 2008; Dahlmeier and Ng, 2011; Rozovskaya and Roth, 2011). It is well known that speakers of different languages make different kinds of errors when learning a language (Swan and Smith, 2001). For example, a French speaker learning English might write sentence (1), which contains a verb tense error. On the other hand, a Japanese speaker learning English might make the verb tense error shown in (2). A writing tutor system which can detect the native language of the learner will be able to tailor the feedback about the error and contrast it with common properties of the learner’s language. (1) She knows that she hasn’t achieve it completely. (2) They"
C12-1158,1996.amta-1.36,0,0.411296,"Missing"
C12-1158,P12-2038,0,0.285243,"the verb tense error shown in (2). A writing tutor system which can detect the native language of the learner will be able to tailor the feedback about the error and contrast it with common properties of the learner’s language. (1) She knows that she hasn’t achieve it completely. (2) They also said to have great curiosity. There has been a great deal of work on NLI in recent years. The methods employed have ranged from some combination of lexical, part-of-speech and n-gram features (Koppel et al., 2005), to syntactic features (Wong and Dras, 2011) including Tree Substitution Grammars (TSGs) (Swanson and Charniak, 2012), to topic models (Wong et al., 2011). Despite these research efforts, it has been somewhat hard to compare different approaches for a number of reasons. The first difficulty is with the evaluation data set. Evaluating an NLI system requires a corpus containing texts in a language other than the native language of the writer. Because of a scarcity of such corpora, most work1 has used the ICLEv22 for training and evaluation since it contains several hundred essays written by college-level English language learners. However, this corpus is quite small for training and testing statistical systems"
C12-1158,C08-1109,1,0.893112,"Missing"
C12-1158,N03-1033,0,0.070628,"Missing"
C12-1158,W07-0602,0,0.367516,"Missing"
C12-1158,U09-1008,0,0.299281,"Missing"
C12-1158,D11-1148,0,0.390776,"On the other hand, a Japanese speaker learning English might make the verb tense error shown in (2). A writing tutor system which can detect the native language of the learner will be able to tailor the feedback about the error and contrast it with common properties of the learner’s language. (1) She knows that she hasn’t achieve it completely. (2) They also said to have great curiosity. There has been a great deal of work on NLI in recent years. The methods employed have ranged from some combination of lexical, part-of-speech and n-gram features (Koppel et al., 2005), to syntactic features (Wong and Dras, 2011) including Tree Substitution Grammars (TSGs) (Swanson and Charniak, 2012), to topic models (Wong et al., 2011). Despite these research efforts, it has been somewhat hard to compare different approaches for a number of reasons. The first difficulty is with the evaluation data set. Evaluating an NLI system requires a corpus containing texts in a language other than the native language of the writer. Because of a scarcity of such corpora, most work1 has used the ICLEv22 for training and evaluation since it contains several hundred essays written by college-level English language learners. However"
C12-1158,U11-1015,0,0.0665164,"tutor system which can detect the native language of the learner will be able to tailor the feedback about the error and contrast it with common properties of the learner’s language. (1) She knows that she hasn’t achieve it completely. (2) They also said to have great curiosity. There has been a great deal of work on NLI in recent years. The methods employed have ranged from some combination of lexical, part-of-speech and n-gram features (Koppel et al., 2005), to syntactic features (Wong and Dras, 2011) including Tree Substitution Grammars (TSGs) (Swanson and Charniak, 2012), to topic models (Wong et al., 2011). Despite these research efforts, it has been somewhat hard to compare different approaches for a number of reasons. The first difficulty is with the evaluation data set. Evaluating an NLI system requires a corpus containing texts in a language other than the native language of the writer. Because of a scarcity of such corpora, most work1 has used the ICLEv22 for training and evaluation since it contains several hundred essays written by college-level English language learners. However, this corpus is quite small for training and testing statistical systems which makes it difficult to tell whe"
C12-1158,D12-1064,0,0.18574,"Missing"
C14-1090,W97-0703,0,0.0498831,"y is influenced by many factors including, but not limited to, ordering of information, such as text unity, detailing and organization. Higgins et al. (2004) implemented a genre-dependent system to predict discourse coherence quality in essays. Their approach, however, was reliant on organizational structures particular to expository and persuasive essays, such as thesis statement and conclusion. 5.2 Lexical Chaining and Cohesion Lexical chaining has been used in a number of applications such as news segmentation (Stokes, 2003), question-answering (Moldovan and Novischi, 2002), summarization (Barzilay and Elhadad, 1997), detection and correction of malapropisms (Hirst and St-Onge, 1995), topic detection (Hatch et al., 2000), topic tracking (Carthy and Sherwood-Smith, 2002), and keyword extraction (Ercan and Cicekli, 2007). In a closely related study, Feng et al. (2009) use lexical chains to measure readability. Lexical chain features are employed to indicate the number of entities/concepts that a reader must keep in mind while reading a document, and two of their features (number of chains in the document and average length of chains) overlap with our LEX-1 features. Our work also differs from systems using"
C14-1090,J08-1001,0,0.467967,"ne Features A review by Burstein et al. (2013a) describes the several systems that measure discourse coherence quality across various text genres including test-taker essays. Features used to evaluate the discourse coherence quality systems in this study include those previously discussed in Burstein et al. (described below). In addition to comparing our features with previously explored features, our goal is to see if the state-of-the-art feature set can be extended with the use of lexical chaining features. Entity-grid transition probabilities (entity). Entity-grid transition probabilities (Barzilay and Lapata, 2008) are intended to address unity, progression and coherence by tracking nouns and pronouns in text. An entity grid is constructed in which all entities (nouns and pronouns) are represented by their syntactic roles in a sentence (i.e., Subject, Object, Other). Entity grid transitions track how the same word appears in a syntactic role across adjacent sentences. Type/Token Ratios for Entities (type/token). These are modified entity-grid transition probabilities. While the entity grid only captures, for example “Subject-Subject” transitions, type/token ratios capture the proportion of unique words"
C14-1090,D12-1091,0,0.0212038,"ality. Surprisingly, maxLSA features, which have the same underlying principle of cohesion in text as lexical chains, are in some of the top-performing feature combinations (at ranks 4 and 5), indicating that, in addition to how ideas and themes are presented throughout the essay, the re-introduction of topics is also important. We tested the statistical significance of the performance differences between our best system (gramErr + LEX-2+ LEX-1+ maxLSA+ program+ RST, at rank 1 in Table 3) and three other systems (Baseline-1, Baseline-2 and gramErr+program) by drawing 10,000 bootstrap samples (Berg-Kirkpatrick et al., 2012) from our manually scored essays. For each sample, QWKs were calcuated between the human scores and the predictions of our best system, and between the human scores and each of the other three systems’ predictions. For each sample, the differences in QWKs were recorded, and the distributions of differences were used for significance testing. Results show that our best performing system is significantly better than Baseline-1 (p < 0.001) and Baseline-2 (p < 0.01), and it marginally outperformed the system with gramErr+program features (p < 0.06). These results show that lexical chaining informa"
C14-1090,W98-0303,1,0.783029,"the nature of the links. Examples of these features are: number and percentage of each link type, number (and percentage) of links of each type in large chains as well as in small chains. Corresponding to each feature that uses counts (e.g. total number of chains) we also created normalized versions of the numbers to account for the essay length. LEX-1 has a total of 38 features. 2.1.3 LEX-2 feature set LEX-2 features capture the interactions between discourse transitions, indicated by explicit discourse cues, and lexical chaining patterns. For this, we use a discourse cue tagger described in Burstein et al. (1998) that was specifically developed for tagging discourse cues in the essay genre. Using patterns and syntactic rules, the tagger automatically identifies words and phrases used as discourse cues, and assigns them a discourse tag. Each tag has a primary component, indicating whether an argument (or topic) is being initialized (arg-init) or developed (arg-dev), and a secondary component indicating the specific type of discourse initialization (e.g. CLAIM, SUMMARY), or development (e.g. CLAIM, CONTRAST). Examples of the discourse tags and their cues are: arg-init:SUMMARY (e.g. all in all, in conclu"
C14-1090,P08-2011,0,0.0166085,"ces how chains or thematic threads are organized with respect to the discourse. Our approach also differs from models that measure local coherence between adjacent sentences (Foltz et al., 1998), in that lexical chains can run though the length of the entire text, and hence the features derived from them are able to capture aggregate thematic properties of the entire text such as number, distribution and elaboration of topics. Discourse coherence models have been previously employed for the task of information-ordering in well-formed texts (e.g., (Soricut and Marcu, 2006; Elsner et al., 2007; Elsner and Charniak, 2008)). In our tasks, discourse coherence quality is influenced by many factors including, but not limited to, ordering of information, such as text unity, detailing and organization. Higgins et al. (2004) implemented a genre-dependent system to predict discourse coherence quality in essays. Their approach, however, was reliant on organizational structures particular to expository and persuasive essays, such as thesis statement and conclusion. 5.2 Lexical Chaining and Cohesion Lexical chaining has been used in a number of applications such as news segmentation (Stokes, 2003), question-answering (Mo"
C14-1090,N07-1055,0,0.01241,"ork, in contrast, traces how chains or thematic threads are organized with respect to the discourse. Our approach also differs from models that measure local coherence between adjacent sentences (Foltz et al., 1998), in that lexical chains can run though the length of the entire text, and hence the features derived from them are able to capture aggregate thematic properties of the entire text such as number, distribution and elaboration of topics. Discourse coherence models have been previously employed for the task of information-ordering in well-formed texts (e.g., (Soricut and Marcu, 2006; Elsner et al., 2007; Elsner and Charniak, 2008)). In our tasks, discourse coherence quality is influenced by many factors including, but not limited to, ordering of information, such as text unity, detailing and organization. Higgins et al. (2004) implemented a genre-dependent system to predict discourse coherence quality in essays. Their approach, however, was reliant on organizational structures particular to expository and persuasive essays, such as thesis statement and conclusion. 5.2 Lexical Chaining and Cohesion Lexical chaining has been used in a number of applications such as news segmentation (Stokes, 2"
C14-1090,E09-1027,0,0.0131011,"wever, was reliant on organizational structures particular to expository and persuasive essays, such as thesis statement and conclusion. 5.2 Lexical Chaining and Cohesion Lexical chaining has been used in a number of applications such as news segmentation (Stokes, 2003), question-answering (Moldovan and Novischi, 2002), summarization (Barzilay and Elhadad, 1997), detection and correction of malapropisms (Hirst and St-Onge, 1995), topic detection (Hatch et al., 2000), topic tracking (Carthy and Sherwood-Smith, 2002), and keyword extraction (Ercan and Cicekli, 2007). In a closely related study, Feng et al. (2009) use lexical chains to measure readability. Lexical chain features are employed to indicate the number of entities/concepts that a reader must keep in mind while reading a document, and two of their features (number of chains in the document and average length of chains) overlap with our LEX-1 features. Our work also differs from systems using cohesion to measure writing quality (e.g., (Witte and Faigley, 1981; Flor et al., 2013)), in that we focus on predicting the quality of discourse coherence. 6 Conclusion In this paper, we investigated the use of lexical chaining for measuring discourse c"
C14-1090,W13-1504,0,0.0395045,"topic detection (Hatch et al., 2000), topic tracking (Carthy and Sherwood-Smith, 2002), and keyword extraction (Ercan and Cicekli, 2007). In a closely related study, Feng et al. (2009) use lexical chains to measure readability. Lexical chain features are employed to indicate the number of entities/concepts that a reader must keep in mind while reading a document, and two of their features (number of chains in the document and average length of chains) overlap with our LEX-1 features. Our work also differs from systems using cohesion to measure writing quality (e.g., (Witte and Faigley, 1981; Flor et al., 2013)), in that we focus on predicting the quality of discourse coherence. 6 Conclusion In this paper, we investigated the use of lexical chaining for measuring discourse coherence quality. Based on intuitions about what makes a text coherent, we extracted two sets of features from lexical chains, one encoding how topical themes and cohesive elements are addressed in the text, and another 958 encoding how the topical themes interact with explicit discourse organizational cues. We performed detailed experiments which showed that lexical chaining features are useful for predicting discourse coherence"
C14-1090,J95-2003,0,0.247799,"l chaining features (topical detailing, variety and organization) are already provided for the writer in the source document and the audio lecture, i.e., the materials that are to be referred to in writing this type of essay. Thus, other features play a more prominent role, such as the RST features that capture local discourse organization which is needed, for example, when drawing a contrast between two sources of conflicting information. 957 5 5.1 Related Work Discourse coherence quality A number of models for measuring the quality of discourse coherence have been based on Centering Theory (Grosz et al., 1995). For example, Barzilay and Lapata (2008) construct entity grids based on syntactic subjects and objects. Their algorithm keeps track of the distribution of entity transitions between adjacent sentences and computes a value for all transition types based on their proportion of occurrence in a text. The algorithm has been evaluated with three tasks using well-formed newspaper corpora: text ordering, summary coherence evaluation, and readability assessment. Along similar lines, Rus and Niraula (2012) find centered paragraphs based on prominent syntactic roles. Similarly, Miltsakaki and Kukich (2"
C14-1090,N04-1024,1,0.442974,"exical chains can run though the length of the entire text, and hence the features derived from them are able to capture aggregate thematic properties of the entire text such as number, distribution and elaboration of topics. Discourse coherence models have been previously employed for the task of information-ordering in well-formed texts (e.g., (Soricut and Marcu, 2006; Elsner et al., 2007; Elsner and Charniak, 2008)). In our tasks, discourse coherence quality is influenced by many factors including, but not limited to, ordering of information, such as text unity, detailing and organization. Higgins et al. (2004) implemented a genre-dependent system to predict discourse coherence quality in essays. Their approach, however, was reliant on organizational structures particular to expository and persuasive essays, such as thesis statement and conclusion. 5.2 Lexical Chaining and Cohesion Lexical chaining has been used in a number of applications such as news segmentation (Stokes, 2003), question-answering (Moldovan and Novischi, 2002), summarization (Barzilay and Elhadad, 1997), detection and correction of malapropisms (Hirst and St-Onge, 1995), topic detection (Hatch et al., 2000), topic tracking (Carthy"
C14-1090,P11-1100,0,0.134473,"e Treebank (Prasad et al., 2008), and vocabulary and length features. Wang, Harrington, and White (2012) combine the approaches from Barzilay and Lapata (2008), and Miltsakaki and Kukich (2000) to detect coherence breakdown points. The biggest difference between our approach and the approaches based on Centering Theory is that we do not use syntactically prominent items or try to establish a center. Instead, multiple concurrent thematic chains can “flow” through the paragraph, and their length, density, and interaction with discourse markers are used to model coherence. In other related work, Lin et al. (2011) use discourse relations from Discourse Lexicalized Tree Adjoining Grammar (D-LTAG) and compile sub-sequences of discourse role transitions to see how the discourse role of a term varies through the progression of the text. Our work, in contrast, traces how chains or thematic threads are organized with respect to the discourse. Our approach also differs from models that measure local coherence between adjacent sentences (Foltz et al., 1998), in that lexical chains can run though the length of the entire text, and hence the features derived from them are able to capture aggregate thematic prope"
C14-1090,P98-2127,0,0.0304044,"the words in any chain that is 951 no farther away than the previous six sentences in the text; the search ends as soon as a strong relation is found. Finally, if no relationship has yet been found, medium-strong relations are sought with the search scope limited to words in chains that are no farther away than the previous three sentences. If the node cannot be added to any existing chains, it forms its own single-node chain. In this work, nouns are the focus of the lexical chains. Nouns, adjective-noun and noun-noun structures are identified as potential chain participants. Lin’s thesaurus (Lin, 1998) is used to measure similarity between words and phrases. Candidate pairs receiving similarity scores greater than 0.8 are considered to have an extra-strong relationship (word repetition receives a similarity score of 1), pairs with similarity greater than 0.172 are considered to have a strong relation, and pairs with similarity scores greater than 0.099 are considered to have a medium-strong relation. These thresholds were chosen after qualitative inspection of a separate development data set of essays, and are also based on a previous finding (Burstein et al., 2012) that 0.172 is the mean s"
C14-1090,C02-1167,0,0.0425806,"8)). In our tasks, discourse coherence quality is influenced by many factors including, but not limited to, ordering of information, such as text unity, detailing and organization. Higgins et al. (2004) implemented a genre-dependent system to predict discourse coherence quality in essays. Their approach, however, was reliant on organizational structures particular to expository and persuasive essays, such as thesis statement and conclusion. 5.2 Lexical Chaining and Cohesion Lexical chaining has been used in a number of applications such as news segmentation (Stokes, 2003), question-answering (Moldovan and Novischi, 2002), summarization (Barzilay and Elhadad, 1997), detection and correction of malapropisms (Hirst and St-Onge, 1995), topic detection (Hatch et al., 2000), topic tracking (Carthy and Sherwood-Smith, 2002), and keyword extraction (Ercan and Cicekli, 2007). In a closely related study, Feng et al. (2009) use lexical chains to measure readability. Lexical chain features are employed to indicate the number of entities/concepts that a reader must keep in mind while reading a document, and two of their features (number of chains in the document and average length of chains) overlap with our LEX-1 feature"
C14-1090,J91-1002,0,0.66278,"hain features is better than that of previous discourse features used for this task, and that the best system performance is achieved when combining lexical chaining features with complementary discourse features, such as those provided by a discourse parser based on rhetorical structure theory, and features that reflect errors in grammar, word usage, and mechanics. 1 Introduction Coherence, the reader’s ability to construct meaning from a document, is greatly influenced by the presence and organization of cohesive elements in the text (Halliday and Hasan, 1976; Moe, 1979). The lexical chain (Morris and Hirst, 1991) is one such element. It consists of a sequence of related words that contribute to the continuity of meaning based on word repetition, synonymy and similarity. In this paper we explore how lexical chains can be employed to measure coherence in essays. Specifically, our goal is to investigate how attributes of lexical chains can encode discourse coherence quality, such as adherence to the essay topic, elaboration, usage of varied vocabulary, and sound organization of thoughts and ideas. To do this, we build lexical chains and extract linguistically-motivated features from them. The number of c"
C14-1090,D08-1020,0,0.0239027,"s between adjacent sentences and computes a value for all transition types based on their proportion of occurrence in a text. The algorithm has been evaluated with three tasks using well-formed newspaper corpora: text ordering, summary coherence evaluation, and readability assessment. Along similar lines, Rus and Niraula (2012) find centered paragraphs based on prominent syntactic roles. Similarly, Miltsakaki and Kukich (2000) use manually marked centering information and find that higher numbers of Rough Shifts within paragraphs are indicative of a lack of coherence. Using well-formed texts, Pitler and Nenkova (2008) show that a text coherence detection system yields the best performance when it includes features using the Barzilay and Lapata (2008) entity grids, syntactic features, discourse relations from the Penn Discourse Treebank (Prasad et al., 2008), and vocabulary and length features. Wang, Harrington, and White (2012) combine the approaches from Barzilay and Lapata (2008), and Miltsakaki and Kukich (2000) to detect coherence breakdown points. The biggest difference between our approach and the approaches based on Centering Theory is that we do not use syntactically prominent items or try to estab"
C14-1090,prasad-etal-2008-penn,0,0.0294283,"luation, and readability assessment. Along similar lines, Rus and Niraula (2012) find centered paragraphs based on prominent syntactic roles. Similarly, Miltsakaki and Kukich (2000) use manually marked centering information and find that higher numbers of Rough Shifts within paragraphs are indicative of a lack of coherence. Using well-formed texts, Pitler and Nenkova (2008) show that a text coherence detection system yields the best performance when it includes features using the Barzilay and Lapata (2008) entity grids, syntactic features, discourse relations from the Penn Discourse Treebank (Prasad et al., 2008), and vocabulary and length features. Wang, Harrington, and White (2012) combine the approaches from Barzilay and Lapata (2008), and Miltsakaki and Kukich (2000) to detect coherence breakdown points. The biggest difference between our approach and the approaches based on Centering Theory is that we do not use syntactically prominent items or try to establish a center. Instead, multiple concurrent thematic chains can “flow” through the paragraph, and their length, density, and interaction with discourse markers are used to model coherence. In other related work, Lin et al. (2011) use discourse"
C14-1090,P06-2103,0,0.184684,"ession of the text. Our work, in contrast, traces how chains or thematic threads are organized with respect to the discourse. Our approach also differs from models that measure local coherence between adjacent sentences (Foltz et al., 1998), in that lexical chains can run though the length of the entire text, and hence the features derived from them are able to capture aggregate thematic properties of the entire text such as number, distribution and elaboration of topics. Discourse coherence models have been previously employed for the task of information-ordering in well-formed texts (e.g., (Soricut and Marcu, 2006; Elsner et al., 2007; Elsner and Charniak, 2008)). In our tasks, discourse coherence quality is influenced by many factors including, but not limited to, ordering of information, such as text unity, detailing and organization. Higgins et al. (2004) implemented a genre-dependent system to predict discourse coherence quality in essays. Their approach, however, was reliant on organizational structures particular to expository and persuasive essays, such as thesis statement and conclusion. 5.2 Lexical Chaining and Cohesion Lexical chaining has been used in a number of applications such as news se"
C14-1090,N03-3009,0,0.021824,"al., 2007; Elsner and Charniak, 2008)). In our tasks, discourse coherence quality is influenced by many factors including, but not limited to, ordering of information, such as text unity, detailing and organization. Higgins et al. (2004) implemented a genre-dependent system to predict discourse coherence quality in essays. Their approach, however, was reliant on organizational structures particular to expository and persuasive essays, such as thesis statement and conclusion. 5.2 Lexical Chaining and Cohesion Lexical chaining has been used in a number of applications such as news segmentation (Stokes, 2003), question-answering (Moldovan and Novischi, 2002), summarization (Barzilay and Elhadad, 1997), detection and correction of malapropisms (Hirst and St-Onge, 1995), topic detection (Hatch et al., 2000), topic tracking (Carthy and Sherwood-Smith, 2002), and keyword extraction (Ercan and Cicekli, 2007). In a closely related study, Feng et al. (2009) use lexical chains to measure readability. Lexical chain features are employed to indicate the number of entities/concepts that a reader must keep in mind while reading a document, and two of their features (number of chains in the document and averag"
C14-1090,miltsakaki-etal-2004-penn,0,\N,Missing
C14-1090,C98-2122,0,\N,Missing
C92-4177,J88-2003,0,0.807507,"Missing"
C92-4177,J80-1003,0,\N,Missing
C98-1032,W97-0713,0,0.0360494,"Missing"
C98-1032,P84-1055,0,0.152518,"Missing"
C98-1032,J93-3003,0,0.0168896,"Missing"
C98-1032,J95-1002,0,\N,Missing
D11-1119,P10-1089,0,0.400416,"y slightly. 1 Introduction The function of context-sensitive text correction is to identify word-choice errors in text (Bergsma et al., 2009). It can be viewed as a lexical disambiguation task (Lapata and Keller, 2005), where a system selects from a predefined confusion word set, such as {affect, effect} or {complement, compliment}, and provides the most appropriate word choice given the context. Typically, one determines if a word has been used correctly based on lexical, syntactic and semantic information from the context of the word. One of the top performing models of spelling correction (Bergsma et al., 2010) is based on web-scale n-gram counts, which reflect both syntax and meaning. However, even with a large-scale n-gram corpus, data sparsity can hurt performance in two ways. ∗ This work was done when the first author was an intern for Educational Testing Service. Take a sentence from The New York Times (NYT) for example: “‘This fellow’s won a war,’ the dean of the capital’s press corps, David Broder, announced on ‘Meet the Press’ after complimenting the president on the ‘great sense of authority and command’ he exhibited in a flight suit.” Unfortunately, neither the phrase “complementing the pr"
D11-1119,W07-1604,1,0.878565,"antic 1292 similarity/distance measures in WordNet. Both systems report performance that is lower than systems developed more recently. A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), transformation-based learning (Mangu and Brill, 1997), augmented mixture models (Cucerzan and Yarowsky, 2002) and maximum entropy classifiers (Izumi et al., 2003; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Felice and Pulman, 2008). Despite their differences, these approaches mainly use contextual features to capture the lexical, semantic and/or syntactic environment of the target word. The use of distributional similarity measures for spelling correction has been previously explored in (Mohammad and Hist, 2006). In our work, distributional similarity is not the primary contribution but we show the impact it can have when used in conjunction with a large scale n-gram model and with parse features, which allows the system to select words outside the local window for"
D11-1119,W02-1005,0,0.0287773,"udanitsky and Hirst (2001) investigated the effectiveness of predicting words based on different semantic 1292 similarity/distance measures in WordNet. Both systems report performance that is lower than systems developed more recently. A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), transformation-based learning (Mangu and Brill, 1997), augmented mixture models (Cucerzan and Yarowsky, 2002) and maximum entropy classifiers (Izumi et al., 2003; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Felice and Pulman, 2008). Despite their differences, these approaches mainly use contextual features to capture the lexical, semantic and/or syntactic environment of the target word. The use of distributional similarity measures for spelling correction has been previously explored in (Mohammad and Hist, 2006). In our work, distributional similarity is not the primary contribution but we show the impact it can have when used in conjunction with a large scale n-gram model"
D11-1119,de-marneffe-etal-2006-generating,0,0.0106983,"Missing"
D11-1119,P08-2008,0,0.0201236,"ey focus on using a parser as a filter to discriminate between possible real-world corrections where the part-ofspeech differs. In our work, we show that parse features are effective when used directly in the classification mode (as opposed to as a final filter) to select the best correction regardless of whether or not the part-of-speech of the choices differ. Statistical parsers have also seen limited use in the sister tasks of preposition and article error detection (Hermet et al., 2008; Lee and Knutsson, 2008; Felice and Pulman, 2009; Tetreault et al., 2010) and verb sense disambiguation (Dligach and Palmer, 2008). In those instances where parsers have been used, they have mainly provided shallow analyses or relations involving specific target words, such as a preposition or verb. Unlike preposition errors, spelling errors can occur in any word. In this paper, we propose a novel way to incorporate the parse into spelling correction, applying the parser to sentences filled by each candidate word equivalently and extracting salient features. This overcomes two problem in the existing methods: 1) the parse trees of the same sentence filled by different confusion words can be different. However, in the tes"
D11-1119,P97-1067,0,0.0409743,"mplimenting the president” exists in the web-scale Google N-gram corpus (Brants and Franz, 2006). The n-gram models decide solely based on the frequency of the bi-grams “after comple(i)menting” and “comple(i)menting the”, which are common usages for both words. The real question is whether we are more likely to “compliment” or “complement” a person, the “president”. Several clues could help us answer that question. A dependency parser can identify the word “president” as the subject of “compliment” or “complement” which also may be the case in some of the training data. Lexical co-occurrence (Edmonds, 1997) and semantic word relatedness measurements, such as Random Indexing (Sahlgren, 2006), could provide evidence that “compliment” is more likely to co-occur with “president” than “complement”. Fur1291 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1291–1300, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics thermore, some important clues can be quite distant from the target word, e.g. outside the 9-word context window Bergsma et al. (2010) and Carlson (2007) used. Consider another sentence in the NYT corpus,"
D11-1119,P98-1059,0,0.0306819,"sight, site}, {peace, piece} and {raise, rise}. They reported that the SVM model with NG features outperformed its unsupervised version, sumLM. However, the limited confusion word sets they evaluated may not comprehensively represent the word usage errors that writers typically make. In this paper, we test nine additional commonly confused word pairs to expand the scope of the evaluation. These words were selected based on their lower frequencies compared to the five pairs in the above work (as shown later in Table 2). 1293 3 Enhanced N-gram Models with Parse Features To our knowledge, only (Elmi and Evans, 1998) have used parsing for spell correction. They focus on using a parser as a filter to discriminate between possible real-world corrections where the part-ofspeech differs. In our work, we show that parse features are effective when used directly in the classification mode (as opposed to as a final filter) to select the best correction regardless of whether or not the part-of-speech of the choices differ. Statistical parsers have also seen limited use in the sister tasks of preposition and article error detection (Hermet et al., 2008; Lee and Knutsson, 2008; Felice and Pulman, 2009; Tetreault et"
D11-1119,C08-1022,0,0.0241586,"Missing"
D11-1119,I08-1059,0,0.0242171,"use of distributional similarity measures for spelling correction has been previously explored in (Mohammad and Hist, 2006). In our work, distributional similarity is not the primary contribution but we show the impact it can have when used in conjunction with a large scale n-gram model and with parse features, which allows the system to select words outside the local window for distributional similarity. In the prior work, the words for distributional similarity are constrained to the local window, and positional information of the words is not encoded. Recent work (Carlson and Fette, 2007; Gamon et al., 2008; Bergsma et al., 2009) has demonstrated that large-scale language modeling is extremely helpful for contextual spelling correction and other lexical disambiguation tasks. These systems make the word choice depending on how frequently each candidate word has been seen in the given context in web-scale data. As n-gram data has become more readily available, such as the Google N-gram Corpus, the likelihood of a word being used in a certain context can be better estimated. Bergsma et al. (2009; 2010) presented a series of simple but powerful models which relied heavily on web-scale n-gram counts."
D11-1119,W95-0104,0,0.419802,"e intended word is more likely to be semantically coherent with the context than is a spelling error. Jones and Martin (1997) made use of the semantic similarity produced by Latent Semantic Analysis. Budanitsky and Hirst (2001) investigated the effectiveness of predicting words based on different semantic 1292 similarity/distance measures in WordNet. Both systems report performance that is lower than systems developed more recently. A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), transformation-based learning (Mangu and Brill, 1997), augmented mixture models (Cucerzan and Yarowsky, 2002) and maximum entropy classifiers (Izumi et al., 2003; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Felice and Pulman, 2008). Despite their differences, these approaches mainly use contextual features to capture the lexical, semantic and/or syntactic environment of the target word. The use of distributional similarity measures for spelling correction has be"
D11-1119,hermet-etal-2008-using,0,0.0660695,"Missing"
D11-1119,P03-2026,0,0.0382898,"redicting words based on different semantic 1292 similarity/distance measures in WordNet. Both systems report performance that is lower than systems developed more recently. A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), transformation-based learning (Mangu and Brill, 1997), augmented mixture models (Cucerzan and Yarowsky, 2002) and maximum entropy classifiers (Izumi et al., 2003; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Felice and Pulman, 2008). Despite their differences, these approaches mainly use contextual features to capture the lexical, semantic and/or syntactic environment of the target word. The use of distributional similarity measures for spelling correction has been previously explored in (Mohammad and Hist, 2006). In our work, distributional similarity is not the primary contribution but we show the impact it can have when used in conjunction with a large scale n-gram model and with parse features, which allows the system to"
D11-1119,A97-1025,0,0.0775056,"and how our approach differs from these approaches. In Sections 3 and 4, we discuss our methods for using parse features and word co-occurrence information. In Section 5, we present experimental results and analysis. 2 Related Work A variety of approaches have been proposed for context-sensitive spelling correction ranging from semantic methods to machine learning classifiers to large-scale n-gram models. Some semantics-based systems have been developed based on an intuitive assumption that the intended word is more likely to be semantically coherent with the context than is a spelling error. Jones and Martin (1997) made use of the semantic similarity produced by Latent Semantic Analysis. Budanitsky and Hirst (2001) investigated the effectiveness of predicting words based on different semantic 1292 similarity/distance measures in WordNet. Both systems report performance that is lower than systems developed more recently. A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), trans"
D11-1119,W06-1605,0,0.0690266,"Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), transformation-based learning (Mangu and Brill, 1997), augmented mixture models (Cucerzan and Yarowsky, 2002) and maximum entropy classifiers (Izumi et al., 2003; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Felice and Pulman, 2008). Despite their differences, these approaches mainly use contextual features to capture the lexical, semantic and/or syntactic environment of the target word. The use of distributional similarity measures for spelling correction has been previously explored in (Mohammad and Hist, 2006). In our work, distributional similarity is not the primary contribution but we show the impact it can have when used in conjunction with a large scale n-gram model and with parse features, which allows the system to select words outside the local window for distributional similarity. In the prior work, the words for distributional similarity are constrained to the local window, and positional information of the words is not encoded. Recent work (Carlson and Fette, 2007; Gamon et al., 2008; Bergsma et al., 2009) has demonstrated that large-scale language modeling is extremely helpful for conte"
D11-1119,N10-1018,0,0.0119634,"ny word. In this paper, we propose a novel way to incorporate the parse into spelling correction, applying the parser to sentences filled by each candidate word equivalently and extracting salient features. This overcomes two problem in the existing methods: 1) the parse trees of the same sentence filled by different confusion words can be different. However, in the test phase, we do not know which word should be put in the sentences to create parse features for test examples. Previous studies (Tetreault et al., 2010) failed to discuss this issue. 2) Some existing work (Whitelaw et al., 2009; Rozovskaya and Roth, 2010) in the text correction field introduced artificial errors into training data to adapt the system to better handle ill-formed text. But this method will encounter serious data sparsity problems when facing rare words. 3.1 Baseline System We chose one of the leading spelling correction systems, (Bergsma et al., 2010), as our primary baseline. As noted earlier, it is an SVM-based system combining web-scale n-gram counts (NG) and contextual words (LEX) as features. To simplify the explanation, throughout the paper, we will only consider the situation with two confusion words. The problem with mor"
D11-1119,C08-1109,1,0.808685,"istance measures in WordNet. Both systems report performance that is lower than systems developed more recently. A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), transformation-based learning (Mangu and Brill, 1997), augmented mixture models (Cucerzan and Yarowsky, 2002) and maximum entropy classifiers (Izumi et al., 2003; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Felice and Pulman, 2008). Despite their differences, these approaches mainly use contextual features to capture the lexical, semantic and/or syntactic environment of the target word. The use of distributional similarity measures for spelling correction has been previously explored in (Mohammad and Hist, 2006). In our work, distributional similarity is not the primary contribution but we show the impact it can have when used in conjunction with a large scale n-gram model and with parse features, which allows the system to select words outside the local window for distributional similarity. In"
D11-1119,P10-2065,1,0.802253,"Evans, 1998) have used parsing for spell correction. They focus on using a parser as a filter to discriminate between possible real-world corrections where the part-ofspeech differs. In our work, we show that parse features are effective when used directly in the classification mode (as opposed to as a final filter) to select the best correction regardless of whether or not the part-of-speech of the choices differ. Statistical parsers have also seen limited use in the sister tasks of preposition and article error detection (Hermet et al., 2008; Lee and Knutsson, 2008; Felice and Pulman, 2009; Tetreault et al., 2010) and verb sense disambiguation (Dligach and Palmer, 2008). In those instances where parsers have been used, they have mainly provided shallow analyses or relations involving specific target words, such as a preposition or verb. Unlike preposition errors, spelling errors can occur in any word. In this paper, we propose a novel way to incorporate the parse into spelling correction, applying the parser to sentences filled by each candidate word equivalently and extracting salient features. This overcomes two problem in the existing methods: 1) the parse trees of the same sentence filled by differ"
D11-1119,D09-1093,0,0.0288853,"g errors can occur in any word. In this paper, we propose a novel way to incorporate the parse into spelling correction, applying the parser to sentences filled by each candidate word equivalently and extracting salient features. This overcomes two problem in the existing methods: 1) the parse trees of the same sentence filled by different confusion words can be different. However, in the test phase, we do not know which word should be put in the sentences to create parse features for test examples. Previous studies (Tetreault et al., 2010) failed to discuss this issue. 2) Some existing work (Whitelaw et al., 2009; Rozovskaya and Roth, 2010) in the text correction field introduced artificial errors into training data to adapt the system to better handle ill-formed text. But this method will encounter serious data sparsity problems when facing rare words. 3.1 Baseline System We chose one of the leading spelling correction systems, (Bergsma et al., 2010), as our primary baseline. As noted earlier, it is an SVM-based system combining web-scale n-gram counts (NG) and contextual words (LEX) as features. To simplify the explanation, throughout the paper, we will only consider the situation with two confusion"
D11-1119,C98-1057,0,\N,Missing
H94-1046,H93-1061,1,0.620215,"uter-readable form. More recently, the editors of Collins COBUILD Dictionary of the English Language [6] made use of the 20,000,000-word COBUILD corpus of written English to insure that the most commonly used words were included. Entries in this dictionary are organized in such a way that, whenever possible, the first sense of a polysemous word is both common and central to the meaning of the word. Again, however, sense-frequencies do not seem to be generally available in a computer-readable form. At the ARPA Human Language Technology Workshop in March 1993, Miller, Leacock, Tengi, and Bunker [7] described a semantic concordance that combines passages from the Brown Corpus [3] with the WordNet lexical d~t_~base [2] in such a way that every open-class word in the text (every noun, verb, adjective, or adverb) carries both a syntactic tag and a semantic tag pointing to the appropriate sense of that word in WordNet. The version of this semantic concordance that existed in August 1993, incorporating 103 of the 500 passages in the Brown Corpus, was made publicly available, along with version 1.4 of WordNet to which the passages were tagged. 1 Passages in the Brown Corpus are approximately 2"
H94-1046,H93-1051,1,0.752374,"Missing"
H94-1046,H93-1052,0,0.0910433,"Missing"
H94-1046,P92-1032,0,\N,Missing
han-etal-2004-detecting,W96-0213,0,\N,Missing
han-etal-2004-detecting,W00-0708,0,\N,Missing
han-etal-2004-detecting,C94-1002,0,\N,Missing
han-etal-2004-detecting,P98-1085,0,\N,Missing
han-etal-2004-detecting,C98-1082,0,\N,Missing
J87-3003,P85-1034,1,0.853071,"Missing"
J87-3003,P86-1019,1,0.893026,"Missing"
J87-3003,P84-1036,1,0.60533,"Missing"
J87-3003,P84-1095,1,0.870618,"Missing"
J87-3003,P85-1037,1,0.785797,"ist of one or more &quot; g e n u s &quot; terms, which identify superordinate categories of the defined word, and &quot;differentia&quot; which distinguish this instance of the superordinate categories from other instances. By manually extracting and disambiguating genus terms for a pocket dictionary, Amsler demonstrated the feasibility of generating semantic hierarchies. It has been our goal to automate the genus extraction and disambiguation processes so that hierarchies can be generated from full-sized dictionaries, such as Webster's Seventh New Collegiate Dictionary. These efforts are described in detail in Chodorow, et al. (1985). They begin with Head Finding. In the definition of car as &quot; a vehicle moving on wheels&quot;, the word vehicle serves as the genus term, while &quot;moving on wheels&quot; differentiates cars from some other types of vehicles. Taken as a group, all of the word/genus pairs contained in a normal dictionary for words o f a given part-of-speech form what Amsler(1980) calls a &quot;tangled hierarchy&quot;. In this hierarchy, each word constitutes a node whose subordinate Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 Roy J. Byrd, Nicoletta Calzolari, Martin S. Chodorow, Judith L. Klavans, Mary S. N"
J87-3003,C86-1051,0,0.0305603,"Missing"
J87-3003,J83-3002,0,0.0390277,"Missing"
J87-3003,P86-1018,0,0.0795249,"ord search technique turned up 527 more candidate words. Subtracting the 153 words c o m m o n to both lists, there remained 844 words to be checked. Half of these passed the criteria in all senses, so a total of 422 words are marked with the [+unit] feature. In addition, a subset of unit nouns (n=128) were marked with a feature [+currency], of which 26 have irregular plural forms, yielding a total of 154 tokens marked. This was an unexpected but useful result particularly for judging the grammaticality of constructions like: Rice is four dollars a pound. 4 . 1 . 3 . ACTIVE AND STATIVE VERBS. Markowitz et al. (1986) present linguistic motivations for extracting an &quot; a c t i v e &quot; and &quot; s t a t i v e &quot; feature for verbs, and suggest methods for using MRD's as sources for finding active/stative information for verbs. We decided to try to carry out their suggestions using our analysis tools and our morphological analyzer on our dictionary resources. Our attempt was both a success 232 Tools and Methods and a failure. The success arises from the design and use of our tools. We achieved clear results, although the results were not what we had expected. The disappointment arises from the fact that the active/st"
J87-3003,A88-1012,1,0.901385,"Missing"
J98-1006,H94-1047,0,0.0246031,"Missing"
J98-1006,P94-1020,0,0.0141895,"Missing"
J98-1006,J95-3002,0,0.0151887,"Missing"
J98-1006,J94-4003,0,0.300246,"biguate a noun, a verb, and an adjective. A knowledge base in the form of WordNet&apos;s lexical relations is used to automatically locate training examples in a general text corpus. Test results are compared with those from manually tagged training examples. 1. Introduction An impressive array of statistical methods have been developed for word sense identification. They range from dictionary-based approaches that rely on definitions (V~ronis and Ide 1990; Wilks et al. 1993) to corpus-based approaches that use only word cooccurrence frequencies extracted from large textual corpora (Sch~itze 1995; Dagan and Itai 1994). We have drawn on these two traditions, using corpus-based co-occurrence and the lexical knowledge base that is embodied in the WordNet lexicon. The two traditions complement each other. Corpus-based approaches have the advantage of being generally applicable to new texts, domains, and corpora without needing costly and perhaps error-prone parsing or semantic analysis. They require only training corpora in which the sense distinctions have been marked, but therein lies their weakness. Obtaining training materials for statistical methods is costly and timeconsuming--it is a &quot;knowledge acquisit"
J98-1006,H92-1045,0,0.144589,"Missing"
J98-1006,W95-0104,0,0.0247899,"e. Testing consists of taking a new example of the polysemous word and computing the most probable sense, based on the cues present in the context of the new item. A comparison is made to the sense assigned by a human judge, and the classifier&apos;s decision is scored as correct or incorrect. TLC uses a Bayesian approach to find the sense si that is the most probable given the cues q contained in a context window of ±k positions around the polysemous target word. For each si, the probability is computed with Bayes&apos; rule: p(si I C-k . . . . . Ck) ~- P(C-k . . . . &quot;Ck [ si)p(si) p(C-k, . . .,Ck) As Golding (1995) points out, the term p(c-k,..., Ck I Si) is difficult to estimate because of the sparse data problem, but if we assume, as is often done, that the occurrence of each cue is independent of the others, then this term can be replaced with: k p(C-k .... &apos;Ck l Si) = H p(Cj I si) j=-k In TLC, we have made this assumption and have estimated p(cj I si) from the training. Of course, the sparse data problem affects these probabilities too, and so TLC uses the Good-Turing formula (Good 1953; Chiang, Lin, and Su 1995), to smooth the values of p(cj I si), including providing probabilities for cues that di"
J98-1006,H93-1051,1,0.426369,"Missing"
J98-1006,C90-2067,0,0.0595564,"Missing"
J98-1006,C92-2070,0,0.059424,"ions, using corpus-based co-occurrence and the lexical knowledge base that is embodied in the WordNet lexicon. The two traditions complement each other. Corpus-based approaches have the advantage of being generally applicable to new texts, domains, and corpora without needing costly and perhaps error-prone parsing or semantic analysis. They require only training corpora in which the sense distinctions have been marked, but therein lies their weakness. Obtaining training materials for statistical methods is costly and timeconsuming--it is a &quot;knowledge acquisition bottleneck&quot; (Gale, Church, and Yarowsky 1992a). To open this bottleneck, we use WordNet&apos;s lexical relations to locate unsupervised training examples. Section 2 describes a statistical classifier, TLC (Topical/Local Classifier), that uses topical context (the open-class words that co-occur with a particular sense), local context (the open- and closed-class items that occur within a small window around a word), or a combination of the two. The results of combining the two types of context to disambiguate a noun (line), a verb (serve), and an adjective (hard) are presented. The following questions are discussed: When is topical context sup"
J98-1006,H93-1052,0,0.186756,"Missing"
J98-1006,P94-1013,0,0.202766,"Missing"
J98-1006,P95-1026,0,0.369829,"Missing"
N12-1003,W01-1605,0,0.103125,"nent’s claims (e.g., “The argument states that”) • to evaluate an opponent’s claims (e.g., “It may seem reasonable at first glance, but actually, there are some logical mistakes in it”) • to present evidence and relate it to specific claims (e.g., “To illustrate my point, I will now give the example of”) There are many ways of analyzing discourse. The most relevant is perhaps rhetorical structure theory (RST) (Mann and Thompson, 1988). To our knowledge, the RST parser from Marcu (2000) is the only RST parser readily available for experimentation. The parser is trained to model the RST corpus (Carlson et al., 2001), which treats complete clauses (i.e., clauses with their obligatory complements) as the elementary units of analysis. Thus, the parser treats the first sentence in example 1 as a single unit and does not differentiate between the main and subordinate clauses. In contrast, our approach distinguishes 21 the sequence “The argument states that . . . ” as shell (which is used here to restate the external claim). Furthermore, we identify the entire second sentence as shell (here, used to evaluate the external claim), whereas the RST parser splits the sentence into two clauses, “It may seem . . .” a"
N12-1003,1993.eamt-1.1,0,0.348313,"Missing"
N12-1003,P11-1099,0,0.0677393,"mber of relationships. On the other hand, shell can capture longer sequences that express more complex relationships between the components of an argumentative discourse (e.g., “But let’s get back to the core issue here” signals that the following point is more important than the previous one). Acknowledgments There are also various other approaches to analyzing arguments. Notably, much recent theoretical research on argumentation has focused on argumentation schemes (Walton et al., 2008), which are high-level strategies for constructing arguments (e.g., argument from consequences). Recently, Feng and Hirst (2011) developed automated methods for classifying texts by argumentation scheme. In similar work, Anand et al. (2011) use argumentation schemes to identify tactics in blog posts (e.g., moral appeal, social generalization, appeals to external authorities etc.). Although shell language can certainly be found in persuasive writing, it is used to organize the persuader’s tactics and claims rather than to express them. For example, consider the following sentence: “It must be the case that this diet works since it was recommended by someone who lost 20 pounds on it.” In shell detection, we focus on the"
N12-1003,P04-1087,0,0.0886083,"e opponents claims, connect ones own claims, etc., may be seen as determining what Grosz and Sidener call “discourse segment purposes” (i.e., the intentions underlying the segments containing the shell spans). We can also view shell detection as the task of identifying phrases that indicate certain types of speech acts (Searle, 1975). In particular, we aim to identify markers of assertive speech acts, which declare that the speaker believes a certain proposition, and expressive speech acts, which express attitudes toward propositions. Shell also overlaps with the concept of discourse markers (Hutchinson, 2004), such as “however” or TP L INCOLN (L) — D OUGLAS (D) DEBATES L: Now, I say that there is no charitable way to look at that statement, except to conclude that he is actually crazy. L: The first thing I see fit to notice is the fact that . . . FP D: He became noted as the author of the scheme to . . . D: . . . such amendments were to be made to it as would render it useless and inefficient . . . FN D: I wish to impress it upon you, that every man who voted for those resolutions . . . L: That statement he makes, too, in the teeth of the knowledge that I had made the stipulation to come down here"
N12-1003,J86-3001,0,\N,Missing
N12-1019,P08-1007,0,0.058236,"a syntactically-aware metric designed to focus on structural n-grams with long surface spans that cannot be captured efficiently with surface ngram metrics. Like BLEU, it is a precisionbased metric and requires a length penalty to minimize the effects of length. 7. BADGER (Parker, 2008) is a language independent metric based on compression and information theory. It computes a compression distance between the two sentences that utilizes the Burrows Wheeler Transformation (BWT). The BWT enables taking into account common sentence contexts with no limit on the size of these contexts. 8. MAXSIM (Chan and Ng, 2008) treats the problem as one of bipartite graph matching and maps each word in one sentence to at most one word in the other sentence. It allows the use of arbitrary similarity functions between words.2 Our choice of metrics was based on their popularity in the MT community, their performance in open competitions such as the NIST MetricsMATR challenge (NIST, 2008) and the WMT shared evaluation task (Callison-Burch et al., 2010), their availability, and their relative complementarity. 3.3 Datasets In this section, we describe the two datasets that we used to evaluate our approach. 3.3.1 Microsoft"
N12-1019,P09-1053,0,0.648384,"based on LCS, skip n-grams and WordNet with a meta-classifier composed of SVM, k-nearest neighbor and maximum entropy classifiers. Islam and Inkpen (2007) measure semantic similarity using a corpus-based measure and a modified version of the Longest Common Subsequence (LCS) algorithm. Rus et al. (2008) take a graph-based approach originally developed for recognizing textual entailment and adapt it for paraphrase identification. Fernando and Stevenson (2008) construct a matrix of word similarities between all pairs of words in both sentences instead of relying only on the maximal similarities. Das and Smith (2009) use an explicit model of alignment between the corresponding parts of two paraphrastic sentences and combine it with a logistic regression classifier built from n-gram overlap features. Most recently, Socher et al. (2011) employ a joint model that incorporates the similarities between both single word features as well as multi-word phrases extracted from the parse trees of the two sentences. We compare our results to those from all the approaches described in this section later in §3.4. 3 Classifying with MT Metrics In this section, we first describe our overall approach to paraphrase identif"
N12-1019,N10-1031,0,0.034254,"ording to its informativeness as indicated by its frequency. We use NIST1 through NIST5 as 5 different features for our classifier (hereafter NIST(1-5)). 3. TER (Snover et al., 2006) is defined as the number of edits needed to “fix” the translation output so that it matches the reference. TER differs from WER in that it includes a heuristic algorithm to deal with shifts in addition to insertions, deletions and substitutions. 4. TERp (TER-Plus) (Snover et al., 2009) builds upon the core TER algorithm by providing additional edit operations based on stemming, synonymy and paraphrase. 5. METEOR (Denkowski and Lavie, 2010) uses a combination of both precision and recall unlike BLEU which focuses on precision. Furthermore, it incorporates stemming, synonymy (via WordNet) and paraphrase (via a lookup table). 6. SEPIA (Habash and El Kholy, 2008) is a syntactically-aware metric designed to focus on structural n-grams with long surface spans that cannot be captured efficiently with surface ngram metrics. Like BLEU, it is a precisionbased metric and requires a length penalty to minimize the effects of length. 7. BADGER (Parker, 2008) is a language independent metric based on compression and information theory. It com"
N12-1019,C04-1051,0,0.850577,"anslation hypothesis produced by a system is semantically equivalent to the source sentence that was translated. However, cross-lingual semantic equivalence is even harder to assess than monolingual, therefore, most MT metrics instead try to measure whether the hypothesis is semantically equivalent to a human-authored reference translation of the same source sentence. Using such automated metrics as This paper describes such a re-examination. We employ 8 different MT metrics for identifying paraphrases across two different datasets - the well-known Microsoft Research paraphrase corpus (MSRP) (Dolan et al., 2004) and the plagiarism detection corpus (PAN) from the 2010 Uncovering Plagiarism, Authorship and Social Software Misuse shared task (Potthast et al., 2010). We include both MSRP and PAN in our study because they represent two very different sources of paraphrased text. The creation of MSRP relied on the massive redundancy of news articles on the web and extracted sentential paraphrases from different stories written about the same topic. In the case of PAN, humans consciously paraphrased existing text to generate new, 182 2012 Conference of the North American Chapter of the Association for Compu"
N12-1019,I05-5003,0,0.777794,"olely on MT metrics. We release both the new dataset and the error analysis annotations for use by the community. 1 In the last 5 years, several shared tasks and competitions have led to the development of increasingly sophisticated metrics that go beyond the computation of n-gram overlaps (BLEU, NIST) or edit distances (TER, WER, PER etc.). Note that the task of an MT metric is essentially one of identifying whether the translation produced by a system is a paraphrase of the reference translation. Although the notion of using MT metrics for the task of paraphrase identification is not novel (Finch et al., 2005; Wan et al., 2006), it merits a re-examination in the light of the development of these novel MT metrics for which we can ask “How much better, if at all, do these newer metrics perform for the task of paraphrase identification?” Introduction One of the most important reasons for the recent advances made in Statistical Machine Translation (SMT) has been the development of automated metrics for evaluation of translation quality. The goal of any such metric is to assess whether the translation hypothesis produced by a system is semantically equivalent to the source sentence that was translated."
N12-1019,P11-2089,1,0.745416,"1 indicating the degree to which two pairs are paraphrastic is more suitable for most approaches. However, rather than asking annotators to rate pairs on a scale, a better idea might be to show the sentence pairs to a large number of Turkers (≥ 20) on Amazon Mechanical Turk and ask them to classify it as either a paraphrase or a non-paraphrase. A simple estimate of the degree of semantic equivalence of the pair is simply the proportion of the Turkers who classified the pair as paraphrastic. An example of such an approach, as applied to the task of grammatical error detection, can be found in (Madnani et al., 2011).3 Second, we believe that the PAN corpus— with Turker simulated plagiarism—contains much more realistic examples of paraphrase and should be incorporated into future evaluations of paraphrase identification. In order to encourage this, we are releasing our PAN dataset containing 13,000 sentence pairs. We are also releasing our error analysis data (100 pairs for MSRP and 100 pairs for PAN) since they might prove useful to other researchers as well. Note that the annotations for this analysis were produced by the authors themselves and, although, they attempted to accurately identify all error"
N12-1019,P02-1040,0,0.0953774,"r BLEU(1-4)). Our best system utilized a classifier combination approach. We used a simple meta-classifier that uses the average of the unweighted probability estimates from the constituent classifiers to make its final decision. We used three constituent classifiers: Logistic regression, the SMO implementation of a support vector machine (Platt, 1999; Keerthi et al., 2001) and a lazy, instance-based classifier that extends the nearest neighbor algorithm (Aha et al., 1991). We used the WEKA machine learning toolkit to perform our experiments (Hall et al., 2009). 1 3.2 MT metrics used 1. BLEU (Papineni et al., 2002) is the most commonly used metric for MT evaluation. It is computed as the amount of n-gram overlap— for different values of n—between the system output and the reference translation, tempered by a penalty for translations that might be too short. BLEU relies on exact matching and has no concept of synonymy or paraphrasing. We use BLEU1 through BLEU4 as 4 different fea1 These constituent classifiers were chosen since they were the top 3 performers in 5-fold cross-validation experiments conducted on both MSRP and PAN training sets. The metaclassifier was chosen similarly once the constituent cl"
N12-1019,C10-2115,0,0.0351299,"Missing"
N12-1019,W06-1603,0,0.304937,"rase and can prove useful to the 183 community for future evaluations of paraphrase identification. BLEU-based features were also employed by Wan et al. (2006) who use them in combination with several other features based on dependency relations and tree edit-distance inside an SVM. There are several other supervised approaches to paraphrase identification that do not use any features based on MT metrics. Mihalcea et al. (2006) combine pointwise mutual information, latent semantic analysis and WordNet-based measures of word semantic similarity into an arbitrary text-to-text similarity metric. Qiu et al. (2006) build a framework that detects dissimilarities between sentences and makes its paraphrase judgment based on the significance of such dissimilarities. Kozareva and Montoyo (2006) use features based on LCS, skip n-grams and WordNet with a meta-classifier composed of SVM, k-nearest neighbor and maximum entropy classifiers. Islam and Inkpen (2007) measure semantic similarity using a corpus-based measure and a modified version of the Longest Common Subsequence (LCS) algorithm. Rus et al. (2008) take a graph-based approach originally developed for recognizing textual entailment and adapt it for par"
N12-1019,2006.amta-papers.25,0,0.0825834,"s 4 different fea1 These constituent classifiers were chosen since they were the top 3 performers in 5-fold cross-validation experiments conducted on both MSRP and PAN training sets. The metaclassifier was chosen similarly once the constituent classifiers had been chosen. 184 2. NIST (Doddington, 2002) is a variant of BLEU that uses the arithmetic mean of n-gram overlaps, rather than the geometric mean. It also weights each n-gram according to its informativeness as indicated by its frequency. We use NIST1 through NIST5 as 5 different features for our classifier (hereafter NIST(1-5)). 3. TER (Snover et al., 2006) is defined as the number of edits needed to “fix” the translation output so that it matches the reference. TER differs from WER in that it includes a heuristic algorithm to deal with shifts in addition to insertions, deletions and substitutions. 4. TERp (TER-Plus) (Snover et al., 2009) builds upon the core TER algorithm by providing additional edit operations based on stemming, synonymy and paraphrase. 5. METEOR (Denkowski and Lavie, 2010) uses a combination of both precision and recall unlike BLEU which focuses on precision. Furthermore, it incorporates stemming, synonymy (via WordNet) and p"
N12-1019,U06-1019,0,0.930952,"We release both the new dataset and the error analysis annotations for use by the community. 1 In the last 5 years, several shared tasks and competitions have led to the development of increasingly sophisticated metrics that go beyond the computation of n-gram overlaps (BLEU, NIST) or edit distances (TER, WER, PER etc.). Note that the task of an MT metric is essentially one of identifying whether the translation produced by a system is a paraphrase of the reference translation. Although the notion of using MT metrics for the task of paraphrase identification is not novel (Finch et al., 2005; Wan et al., 2006), it merits a re-examination in the light of the development of these novel MT metrics for which we can ask “How much better, if at all, do these newer metrics perform for the task of paraphrase identification?” Introduction One of the most important reasons for the recent advances made in Statistical Machine Translation (SMT) has been the development of automated metrics for evaluation of translation quality. The goal of any such metric is to assess whether the translation hypothesis produced by a system is semantically equivalent to the source sentence that was translated. However, cross-lin"
N12-1019,O97-1002,0,\N,Missing
N12-1029,P06-2001,0,0.0611946,"Missing"
N12-1029,1995.iwpt-1.8,0,0.277359,"Missing"
N12-1029,P11-1092,0,0.0405999,"Missing"
N12-1029,W10-4236,0,0.0979335,"Missing"
N12-1029,C08-1022,0,0.0517037,"Missing"
N12-1029,W11-1410,1,0.721123,"ster task of restoring commas in well-formed text. For both tasks, we show that the use of novel features which encode long-distance information improves upon the more lexically-driven features used in prior work. 1 Introduction Automatically detecting and correcting grammatical errors in learner language is a growing sub-field of Natural Language Processing. As the field has progressed, we have seen research focusing on a range of grammatical phenomena including English articles and prepositions (c.f. Tetreault et al., 2010; De Felice and Pulman, 2008), particles in Korean and Japanese (c.f. Dickinson et al., 2011; Oyama, 2010), and broad approaches that aim to find multiple error types (c.f Rozovskaya et al., 2011; Gamon, 2011). However, to the best of our knowledge, there has not been any research published specifically on correcting erroneous comma usage in English (though there have been efforts such as the MS Word grammar checker, and products like Grammarly and White Smoke that include comma checking). There are a variety of reasons that motivate our interest in attempting to correct comma errors. First of all, a review of error typologies in Leacock et al. (2010) reveals that comma usage errors"
N12-1029,N10-1019,0,0.0386973,"e POS features abstract away from the words and avoid the problem of data sparseness by allowing the classifier to focus on the categories of the words, rather than the lexical items themselves. The combination (combo) feature is a unigram of the word+pos for every word in the sliding window. It reinforces the relationship between the lexical items and their POS tags, further strengthening the evidence of entries like then RB. All of these features have been used in previous grammatical error detection tasks which target particle, article, and preposition errors (c.f., Dickinson et al., 2011; Gamon, 2010; Tetreault and Chodorow, 2008). The first combo feature keeps track of the first combination feature of the sentence so that it can be referred to by the classifier throughout processing the entire sentence. This feature is helpful when an introductory phrase is longer than the classifier’s five word window. Figure 1 provides a good example of the utility of this feature, as If the teacher easily gets mad is so long that by the time the window has moved to the target position of the space following mad, the first word and POS, If RB, which can often indicate an introductory phrase, is beyond"
N12-1029,W11-1422,0,0.0692504,"tance information improves upon the more lexically-driven features used in prior work. 1 Introduction Automatically detecting and correcting grammatical errors in learner language is a growing sub-field of Natural Language Processing. As the field has progressed, we have seen research focusing on a range of grammatical phenomena including English articles and prepositions (c.f. Tetreault et al., 2010; De Felice and Pulman, 2008), particles in Korean and Japanese (c.f. Dickinson et al., 2011; Oyama, 2010), and broad approaches that aim to find multiple error types (c.f Rozovskaya et al., 2011; Gamon, 2011). However, to the best of our knowledge, there has not been any research published specifically on correcting erroneous comma usage in English (though there have been efforts such as the MS Word grammar checker, and products like Grammarly and White Smoke that include comma checking). There are a variety of reasons that motivate our interest in attempting to correct comma errors. First of all, a review of error typologies in Leacock et al. (2010) reveals that comma usage errors are the fourth most common error type among non-native writers in the Cambridge Learner Corpus (Nicholls, 1999), whic"
N12-1029,C94-1069,0,0.286386,"Missing"
N12-1029,D10-1018,0,0.0213926,"words and POS tags in a sliding 5 word window (target word, +/- 2 words). The lexical items help to encode any idiosyncratic relationships between words and commas that might not be exploited through the examination of more in-depth linguistic features. For example, then is a special case of an adverb (RB) that is often preceded by a comma, even if other adverbs are not, so POS tags might not capture this relationship. The lexical items also provide an approximation of a language model or hidden event language model approach, which has proven to be useful in comma restoration tasks (see e.g. Lu and Ng, 2010). The POS features abstract away from the words and avoid the problem of data sparseness by allowing the classifier to focus on the categories of the words, rather than the lexical items themselves. The combination (combo) feature is a unigram of the word+pos for every word in the sliding window. It reinforces the relationship between the lexical items and their POS tags, further strengthening the evidence of entries like then RB. All of these features have been used in previous grammatical error detection tasks which target particle, article, and preposition errors (c.f., Dickinson et al., 20"
N12-1029,W96-0213,0,0.35576,"omma error correction, and have not, as far as we know, been utilized in previous research. 5 Comma Restoration Before applying our system to the task of error correction, we tested its utility in restoring commas in newswire texts. Specifically, we evaluate on section 23 of the WSJ, training on sections 02-22. Here, the task is straightforward: we remove all commas from the test data and performance is measured on the system’s ability to put the commas back in the right places. After stripping all commas from our test data, the text is tokenized and POS tagged using a maximum entropy tagger (Ratnaparkhi, 1996) and every token is considered by the classifier as either requiring a following comma or not. Out of 53,640 tokens, 3062 should be followed by a comma. We provide accuracy, precision, recall, F1 -score, and sentence accuracy (S Acc.) for these tests, along with results from Gravano et al. (2009) and Shieber and Tao (2003) in Table 2. The first system (LexSyn) includes only the lexical and syntactic features from Figure 1; the second (LexSyn+Dist) includes all of the features. System LexSyn LexSyn+Dist Shieber & Tao Gravano et al. Acc. 97.4 97.5 97.0 N.A. P 85.8 85.8 79.7 57 R 64.9 66.3 62.6 6"
N12-1029,W11-2843,0,0.0477105,"res which encode long-distance information improves upon the more lexically-driven features used in prior work. 1 Introduction Automatically detecting and correcting grammatical errors in learner language is a growing sub-field of Natural Language Processing. As the field has progressed, we have seen research focusing on a range of grammatical phenomena including English articles and prepositions (c.f. Tetreault et al., 2010; De Felice and Pulman, 2008), particles in Korean and Japanese (c.f. Dickinson et al., 2011; Oyama, 2010), and broad approaches that aim to find multiple error types (c.f Rozovskaya et al., 2011; Gamon, 2011). However, to the best of our knowledge, there has not been any research published specifically on correcting erroneous comma usage in English (though there have been efforts such as the MS Word grammar checker, and products like Grammarly and White Smoke that include comma checking). There are a variety of reasons that motivate our interest in attempting to correct comma errors. First of all, a review of error typologies in Leacock et al. (2010) reveals that comma usage errors are the fourth most common error type among non-native writers in the Cambridge Learner Corpus (Nicholl"
N12-1029,N03-1029,0,0.301522,"Missing"
N12-1029,C08-1109,1,0.866655,"s abstract away from the words and avoid the problem of data sparseness by allowing the classifier to focus on the categories of the words, rather than the lexical items themselves. The combination (combo) feature is a unigram of the word+pos for every word in the sliding window. It reinforces the relationship between the lexical items and their POS tags, further strengthening the evidence of entries like then RB. All of these features have been used in previous grammatical error detection tasks which target particle, article, and preposition errors (c.f., Dickinson et al., 2011; Gamon, 2010; Tetreault and Chodorow, 2008). The first combo feature keeps track of the first combination feature of the sentence so that it can be referred to by the classifier throughout processing the entire sentence. This feature is helpful when an introductory phrase is longer than the classifier’s five word window. Figure 1 provides a good example of the utility of this feature, as If the teacher easily gets mad is so long that by the time the window has moved to the target position of the space following mad, the first word and POS, If RB, which can often indicate an introductory phrase, is beyond the scope of the sliding window"
N12-1029,P10-2065,1,0.918,"Missing"
N12-1029,W08-1703,0,\N,Missing
P01-1014,P98-1032,1,0.889356,"Missing"
P01-1014,E99-1015,0,\N,Missing
P01-1014,C98-1032,1,\N,Missing
P01-1014,J86-3001,0,\N,Missing
P10-2065,W07-1604,1,0.645202,"nd Knutsson (2008) show that knowledge of the PP attachment site helps in the task of preposition selection by comparing a classifier trained on lexical features (the verb before the preposition, the noun between the verb and the preposition, if any, and the noun after the preposition) to a classifier trained on attachment features which explicitly state whether the preposition is attached to the preceding noun or verb. They also argue that a parser which is capable of distinguishing between arguments and adjuncts is useful for generating the correct preposition. 3 Baseline System The work of Chodorow et al. (2007) and T&C08 treat the tasks of preposition selection and error detection as a classification problem. That is, given the context around a preposition and a model of correct usage, a classifier determines which of the 34 prepositions covered by the model is most appropriate for the context. A model of correct preposition usage is constructed by training a Maximum Entropy classifier (Ratnaparkhi, 1998) on millions of preposition contexts from well-formed text. A context is represented by 25 lexical features and 4 combination features: Lexical Token and POS n-grams in a 2 word window around the pr"
P10-2065,C08-1022,0,0.602123,"Missing"
P10-2065,W08-1301,0,0.00717771,"Missing"
P10-2065,de-marneffe-etal-2006-generating,0,0.028425,"Missing"
P10-2065,I08-1059,0,0.0586698,"modeling preposition usage in well-formed text and learner text? • We demonstrate that parse features have a significant impact on preposition selection in well-formed text. We also show which features have the greatest effect on performance. • We show that, despite the noisiness of learner text, parse features can actually make small, albeit non-significant, improvements to the performance of a state-of-the-art preposition error detection system. • We evaluate the accuracy of parsing and especially preposition attachment in learner texts. 2 Related Work T&C08, De Felice and Pulman (2008) and Gamon et al. (2008) describe very similar preposition error detection systems in which a model of correct prepositional usage is trained from wellformed text and a writer’s preposition is compared with the predictions of this model. It is difficult to directly compare these systems since they are trained and tested on different data sets 353 Proceedings of the ACL 2010 Conference Short Papers, pages 353–358, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics 3.1 but they achieve accuracy in a similar range. Of these systems, only the DAPPER system (De Felice and Pulman, 2008; De F"
P10-2065,hermet-etal-2008-using,0,0.0431457,"Missing"
P10-2065,P09-2079,0,0.0107573,"Missing"
P10-2065,P03-1054,0,0.00558171,"Detection Joel Tetreault Educational Testing Service Princeton NJ, USA Jennifer Foster NCLT Dublin City University Ireland Martin Chodorow Hunter College of CUNY New York, NY, USA JTetreault@ets.org jfoster@computing.dcu.ie @hunter.cuny.edu Abstract We recreate a state-of-the-art preposition usage system (Tetreault and Chodorow (2008), henceforth T&C08) originally trained with lexical features and augment it with parser output features. We employ the Stanford parser in our experiments because it consists of a competitive phrase structure parser and a constituent-to-dependency conversion tool (Klein and Manning, 2003a; Klein and Manning, 2003b; de Marneffe et al., 2006; de Marneffe and Manning, 2008). We compare the original model with the parser-augmented model on the tasks of preposition selection in wellformed text (fluent writers) and preposition error detection in learner texts (ESL writers). This paper makes the following contributions: We evaluate the effect of adding parse features to a leading model of preposition usage. Results show a significant improvement in the preposition selection task on native speaker text and a modest increment in precision and recall in an ESL error detection task. Ana"
P10-2065,P08-1006,0,0.014116,"Missing"
P10-2065,C08-1109,1,0.709664,"t test sets and also training sizes. Given the time required to train large models, we report here experiments with a relatively small model. 355 Model T&C08 +Phrase Structure Only +Dependency Only +Parse +head-tag+comp-tag +left +grandparent +head-token+comp-tag +head-tag +head-token +head-tag+comp-token Accuracy 65.2 67.1 68.2 68.5 66.9 66.8 66.6 66.6 66.5 66.4 66.1 Method T&C08 +Parse native speakers for the Test of English as a Foreign R Language (TOEFL ). The prepositions were judged by two trained annotators and checked by the authors using the preposition annotation scheme described in Tetreault and Chodorow (2008b). 4,881 of the prepositions were judged to be correct and the remaining 302 were judged to be incorrect. The writer’s preposition is flagged as an error by the system if its likelihood according to the model satisfied a set of criteria (e.g., the difference between the probability of the system’s choice and the writer’s preposition is 0.8 or higher). Unlike the selection task where we use accuracy as the metric, we use precision and recall with respect to error detection. To date, performance figures that have been reported in the literature have been quite low, reflecting the difficulty of"
P10-2065,W08-1205,1,0.791661,"t test sets and also training sizes. Given the time required to train large models, we report here experiments with a relatively small model. 355 Model T&C08 +Phrase Structure Only +Dependency Only +Parse +head-tag+comp-tag +left +grandparent +head-token+comp-tag +head-tag +head-token +head-tag+comp-token Accuracy 65.2 67.1 68.2 68.5 66.9 66.8 66.6 66.6 66.5 66.4 66.1 Method T&C08 +Parse native speakers for the Test of English as a Foreign R Language (TOEFL ). The prepositions were judged by two trained annotators and checked by the authors using the preposition annotation scheme described in Tetreault and Chodorow (2008b). 4,881 of the prepositions were judged to be correct and the remaining 302 were judged to be incorrect. The writer’s preposition is flagged as an error by the system if its likelihood according to the model satisfied a set of criteria (e.g., the difference between the probability of the system’s choice and the writer’s preposition is 0.8 or higher). Unlike the selection task where we use accuracy as the metric, we use precision and recall with respect to error detection. To date, performance figures that have been reported in the literature have been quite low, reflecting the difficulty of"
P10-2065,J07-4004,0,\N,Missing
P10-2065,J03-4003,0,\N,Missing
P11-2089,W10-0731,0,0.0164388,"lice and Pulman (2008) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourcOne of the fastest growing areas in need of NLP tools is the field of grammatical error detection for learners of English as a Second Language (ESL). According to Guo and Beckett (2007), “over a billion people speak English as their second or foreign language.” This high demand has resulted in"
P11-2089,D09-1030,0,0.0334174,"ferent groups have been compared directly within the field primarily because there is no common corpus or shared task—both commonly found in other NLP areas such as machine translation.1 For example, Tetreault and Chodorow (2008), Gamon et al. (2008) and Felice and Pulman (2008) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourcOne of the fastest growing areas in ne"
P11-2089,W10-0708,0,0.00908944,"e there is no common corpus or shared task—both commonly found in other NLP areas such as machine translation.1 For example, Tetreault and Chodorow (2008), Gamon et al. (2008) and Felice and Pulman (2008) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourcOne of the fastest growing areas in need of NLP tools is the field of grammatical error detection for learners of"
P11-2089,C08-1022,0,0.0718184,"Missing"
P11-2089,W10-0713,0,0.00614558,"s the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourcOne of the fastest growing areas in need of NLP tools is the field of grammatical error detection for learners of English as a Second Language (ESL). According to Guo and Beckett (2007), “over a billion people speak English as their second or foreign language.” This high demand has resulted in many NLP research papers on the topic, a Synthesis Series book (Leacock et al., 2010) and a recurring workshop (Tetreault et al., 2010a), all in the last five years. In this year’s A"
P11-2089,I08-1059,0,0.0380613,"of crowdsourcing. 1 Motivation and Contributions to have different judgments of usage. Therefore, an appropriate evaluation should take this into account by not only enlisting multiple human judges but also aggregating these judgments in a graded manner. Second, systems are hardly ever compared to each other. In fact, to our knowledge, no two systems developed by different groups have been compared directly within the field primarily because there is no common corpus or shared task—both commonly found in other NLP areas such as machine translation.1 For example, Tetreault and Chodorow (2008), Gamon et al. (2008) and Felice and Pulman (2008) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambigu"
P11-2089,N10-1019,0,0.0860787,"Missing"
P11-2089,W10-0717,0,0.0141234,"ated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourcOne of the fastest growing areas in need of NLP tools is the field of grammatical error detection for learners of English as a Second Language (ESL). According to Guo and Beckett (2007), “over a billion people speak English as their second or foreign language.” This high demand has resulted in many NLP research papers on the topic, a Synthesis Series book (Leacock et al., 2010) a"
P11-2089,N10-1024,0,0.0278613,"orpus or shared task—both commonly found in other NLP areas such as machine translation.1 For example, Tetreault and Chodorow (2008), Gamon et al. (2008) and Felice and Pulman (2008) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourcOne of the fastest growing areas in need of NLP tools is the field of grammatical error detection for learners of English as a Second Language (ESL)."
P11-2089,W10-1004,1,0.378088,"Missing"
P11-2089,D10-1094,1,0.419692,"Missing"
P11-2089,C08-1109,1,0.664593,"ies, both based on a novel use of crowdsourcing. 1 Motivation and Contributions to have different judgments of usage. Therefore, an appropriate evaluation should take this into account by not only enlisting multiple human judges but also aggregating these judgments in a graded manner. Second, systems are hardly ever compared to each other. In fact, to our knowledge, no two systems developed by different groups have been compared directly within the field primarily because there is no common corpus or shared task—both commonly found in other NLP areas such as machine translation.1 For example, Tetreault and Chodorow (2008), Gamon et al. (2008) and Felice and Pulman (2008) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009),"
P11-2089,W10-1006,1,0.836109,"allison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourcOne of the fastest growing areas in need of NLP tools is the field of grammatical error detection for learners of English as a Second Language (ESL). According to Guo and Beckett (2007), “over a billion people speak English as their second or foreign language.” This high demand has resulted in many NLP research papers on the topic, a Synthesis Series book (Leacock et al., 2010) and a recurring workshop (Tetreault et al., 2010a), all in the last five years. In this year’s ACL conference, there are four long papers devoted to this topic. Despite the growing interest, two major factors encumber the growth of this subfield. First, the lack of consistent and appropriate score reporting is an issue. Most work reports results in the form of precision and recall as measured against the judgment of a single human rater. This is problematic because most usage errors (such as those in article and preposition usage) are a matter of degree rather than sim1 ple rule violations such as number agreement. As a There has been a rec"
P11-2089,W10-0725,0,0.0169367,"rent evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourcOne of the fastest growing areas in need of NLP tools is the field of grammatical error detection for learners of English as a Second Language (ESL). According to Guo and Beckett (2007), “over a billion people speak English as their second or foreign language.” This high demand has resulted in many NLP research papers on the topic, a Synthesis Series book (Leacock et al., 2010) and a recurring workshop (Tetreault et al., 20"
P11-2089,N10-1057,0,0.046017,"Missing"
P11-2089,W10-4236,0,\N,Missing
P85-1037,P84-1036,0,\N,Missing
P98-1032,W97-0713,0,0.0386549,"Missing"
P98-1032,P84-1055,0,0.16725,"Missing"
P98-1032,J93-3003,0,0.0169937,"Missing"
P98-1032,J95-1002,0,\N,Missing
Q18-1007,I13-1171,0,0.0842246,"Elsner (2012) proposed a rich representation of story-characters for the purpose of summarizing and representing novels. Bamman et al. (2014) automatically inferred latent character types in English novels. VallsVargas et al. (2014) extracted characters and roles from Russian folk tales, based on their actions. Chaturvedi et al. (2015) analyzed short stories for characters’ desires and built a system to recognize desire fulfillment, using textual entailment. Researchers have also studied social networks and have modeled relationships in stories (Elson et al., 2010; Celikyilmaz et al., 2010). Agarwal et al. (2013) modeled character interactions from Alice in Wonderland for the purpose of social network analysis. Chaturvedi et al. (2016) modeled character relationships in novels, using structured prediction. Wiebe (1994) proposed a method for tracking psychological points of view in narratives, looking at private states and subjective sentences. Ovesdotter Alm and Sproat (2005) studied emotional sequencing and trajectories in 22 Grimm’s fairy tales. Ware et al. (2011) analyzed dimensions of conflict in four simple, constructed stories, with the goal of evaluating story content. Similarly, Swanson et al."
Q18-1007,D13-1178,0,0.0226642,"sed for modeling and understanding narrative structures (Finlayson, 2012; Elson, 2012). See Finlayson (2013) and Mani (2012) for detailed literature surveys. One important aspect of a narrative is that it conveys a sequence of events (Fludernik, 2009; Almeida, 1995). Chambers and Jurafsky (2009; 2008) presented techniques for the automatic acquisition of event chains and event schemas (Chambers, 2013), which are related to earlier notions of scripts as prepackaged chunks of knowledge (Schank and Abelson, 1977). This line of research has received a great deal of attention (Nguyen et al., 2015; Balasubramanian et al., 2013; Jans et al., 2012; McIntyre and Lapata, 2010). For narratives, Ouyang and McKeown (2015) focused on automatic detection of compelling events. Bogel et al. (2014) worked on extraction and temporal ordering of events in narratives. Based on the ‘Narrative Cloze Test’ (Chambers and Jurafsky, 2008), Mostafazadeh et al. (2016) presented a framework for evaluating story understanding algorithms, the ‘Story Cloze Test’, whose goal is to predict a held-out continuation of a short story. Our research differs significantly from previous work. We aim to evaluate, on an integer scale, the quality of nar"
Q18-1007,P14-1035,0,0.0334529,".2 Narrative Analysis in Computational Linguistics Research on narratives in Computational Linguistics has employed fables, fairy tales, and literary texts, aiming at representing, understanding and extracting information, e.g., Charniak (1972). Goyal et al. (2010) analyzed Aesop’s fables, producing automatic plot-unit representations (Lehnert, 1981) with a task-specific knowledge base of affect. Character traits and personas in stories have also been analyzed. For example, Elsner (2012) proposed a rich representation of story-characters for the purpose of summarizing and representing novels. Bamman et al. (2014) automatically inferred latent character types in English novels. VallsVargas et al. (2014) extracted characters and roles from Russian folk tales, based on their actions. Chaturvedi et al. (2015) analyzed short stories for characters’ desires and built a system to recognize desire fulfillment, using textual entailment. Researchers have also studied social networks and have modeled relationships in stories (Elson et al., 2010; Celikyilmaz et al., 2010). Agarwal et al. (2013) modeled character interactions from Alice in Wonderland for the purpose of social network analysis. Chaturvedi et al. (2"
Q18-1007,P14-2041,1,0.792516,"its three language arts genres. With the increased focus on automated evaluation of student writing in educational settings (Adams, 2014), automated methods for evaluating narrative essays at scale are becoming increasingly important. Automated scoring of narrative essays is a challenging area, and one that has not been explored extensively in NLP research. Previous work on automated essay scoring has focused on informational, argumentative, persuasive and source-based writing constructs (Stab and Gurevych, 2017; Nguyen and Litman, 2016; Farra et al., 2015; Somasundaran et al., 2014; Beigman Klebanov et al., 2014; Shermis and Burstein, 2013). Similarly, operational essay scoring engines (Attali and Burstein, 2006; Elliot, 2003) are geared towards evaluating language proficiency in general. In this work, we lay the groundwork and present the first results for automated scoring of narrative essays, focusing on narrative quality. One of the challenges in narrative quality analysis is the scarcity of scored essays in this genre. We describe a detailed manual annotation study on scoring student essays along multiple dimensions of narrative quality, such as narrative development and narrative organization."
Q18-1007,D12-1091,0,0.0231317,"7.2 Results We experimented with Linear Regression, Support Vector Regression (RBF kernel), Random Forests, and Elastic Net learners from the scikitlearn toolkit (Pedregosa et al., 2011), with 10-fold cross-validation on 942 essays. As Linear Regression results were consistently better, both for Baseline and for our features, we only report results from this learner. Trimming of the predicted linear regression output was performed; that is, if the predicted score was above the max score, or below the min score, it was assigned the max or the min score, respectively. Bootstrapping experiments (Berg-Kirkpatrick et al., 2012; Efron and Tibshirani, 1994) were performed to test for statistical significance (we used 1000 bootstrap samples). For each trait-scoring experiment, we extracted all the features (described in Section 6) from the essays and used the corresponding human trait scores for training and testing. Thus, the input essays and their features are the same across all experiments. What varies is the trait to be predicted and, consequently, the performance of feature sets as well as the best feature combination. Table 4 shows the performance of Baseline, the individual features, all features, and the best"
Q18-1007,bogel-etal-2014-computational,0,0.0249172,"aspect of a narrative is that it conveys a sequence of events (Fludernik, 2009; Almeida, 1995). Chambers and Jurafsky (2009; 2008) presented techniques for the automatic acquisition of event chains and event schemas (Chambers, 2013), which are related to earlier notions of scripts as prepackaged chunks of knowledge (Schank and Abelson, 1977). This line of research has received a great deal of attention (Nguyen et al., 2015; Balasubramanian et al., 2013; Jans et al., 2012; McIntyre and Lapata, 2010). For narratives, Ouyang and McKeown (2015) focused on automatic detection of compelling events. Bogel et al. (2014) worked on extraction and temporal ordering of events in narratives. Based on the ‘Narrative Cloze Test’ (Chambers and Jurafsky, 2008), Mostafazadeh et al. (2016) presented a framework for evaluating story understanding algorithms, the ‘Story Cloze Test’, whose goal is to predict a held-out continuation of a short story. Our research differs significantly from previous work. We aim to evaluate, on an integer scale, the quality of narratives in student-generated essays. Insights from previous work on narrative analysis can be useful for our purposes if they capture narrative techniques employed"
Q18-1007,P08-1090,0,0.623993,"2008) presented techniques for the automatic acquisition of event chains and event schemas (Chambers, 2013), which are related to earlier notions of scripts as prepackaged chunks of knowledge (Schank and Abelson, 1977). This line of research has received a great deal of attention (Nguyen et al., 2015; Balasubramanian et al., 2013; Jans et al., 2012; McIntyre and Lapata, 2010). For narratives, Ouyang and McKeown (2015) focused on automatic detection of compelling events. Bogel et al. (2014) worked on extraction and temporal ordering of events in narratives. Based on the ‘Narrative Cloze Test’ (Chambers and Jurafsky, 2008), Mostafazadeh et al. (2016) presented a framework for evaluating story understanding algorithms, the ‘Story Cloze Test’, whose goal is to predict a held-out continuation of a short story. Our research differs significantly from previous work. We aim to evaluate, on an integer scale, the quality of narratives in student-generated essays. Insights from previous work on narrative analysis can be useful for our purposes if they capture narrative techniques employed by student writers, and if they correlate with scores representing narrative quality. It is still an open question whether an elabora"
Q18-1007,P09-1068,0,0.0474924,"ur simple, constructed stories, with the goal of evaluating story content. Similarly, Swanson et al. (2014) analyzed blog narratives for narrative clause sub-types such as orientation, action and evaluation. Reagan et al. (2016) used sentiment analysis to generate emotional profiles for English novels. NLP methods have also been used for modeling and understanding narrative structures (Finlayson, 2012; Elson, 2012). See Finlayson (2013) and Mani (2012) for detailed literature surveys. One important aspect of a narrative is that it conveys a sequence of events (Fludernik, 2009; Almeida, 1995). Chambers and Jurafsky (2009; 2008) presented techniques for the automatic acquisition of event chains and event schemas (Chambers, 2013), which are related to earlier notions of scripts as prepackaged chunks of knowledge (Schank and Abelson, 1977). This line of research has received a great deal of attention (Nguyen et al., 2015; Balasubramanian et al., 2013; Jans et al., 2012; McIntyre and Lapata, 2010). For narratives, Ouyang and McKeown (2015) focused on automatic detection of compelling events. Bogel et al. (2014) worked on extraction and temporal ordering of events in narratives. Based on the ‘Narrative Cloze Test’"
Q18-1007,D13-1185,0,0.0233395,"narratives for narrative clause sub-types such as orientation, action and evaluation. Reagan et al. (2016) used sentiment analysis to generate emotional profiles for English novels. NLP methods have also been used for modeling and understanding narrative structures (Finlayson, 2012; Elson, 2012). See Finlayson (2013) and Mani (2012) for detailed literature surveys. One important aspect of a narrative is that it conveys a sequence of events (Fludernik, 2009; Almeida, 1995). Chambers and Jurafsky (2009; 2008) presented techniques for the automatic acquisition of event chains and event schemas (Chambers, 2013), which are related to earlier notions of scripts as prepackaged chunks of knowledge (Schank and Abelson, 1977). This line of research has received a great deal of attention (Nguyen et al., 2015; Balasubramanian et al., 2013; Jans et al., 2012; McIntyre and Lapata, 2010). For narratives, Ouyang and McKeown (2015) focused on automatic detection of compelling events. Bogel et al. (2014) worked on extraction and temporal ordering of events in narratives. Based on the ‘Narrative Cloze Test’ (Chambers and Jurafsky, 2008), Mostafazadeh et al. (2016) presented a framework for evaluating story underst"
Q18-1007,J90-1003,0,0.278329,"the following set of (typed dependency) arguments: nsubj, dobj, nsubjpass, xsubj, csubj, csubjpass. To estimate event cohesion, we extract all event pairs from an essay after pre-processing it with the Stanford Core NLP toolkit (Manning et al., 2014). Event tokens from the essay are linked into pairs when they share a filler in their arguments. For essays, we use Stanford co-reference resolution for matching fillers of verb-argument slots. For all event pairs extracted from an essay, we query the events database to retrieve the pair association value (we use the point-wise mutual information (Church and Hanks, 1990)). We define three quantitative measures to encode event cohesion: (1) total count of event pairs in the essay; (2) proportion of in-essay event-pairs that are actually found in the events database; (3) proportion of in-essay event-pairs that have substantial association (we use P M I ≥ 2). We also capture aspects of coherent event sequencing. For this, we compute event chains, which are defined as sequences of events that share the same actor or object, in subject or direct object role (Chambers and Jurafsky, 2008). Specifically, we encode the following additional features in the Events featu"
Q18-1007,W08-1301,0,0.0341659,"Missing"
Q18-1007,E12-1065,0,0.106009,"tated oral retellings of the same story on three consecutive days in order to study and model children’s comprehension. 2.2 Narrative Analysis in Computational Linguistics Research on narratives in Computational Linguistics has employed fables, fairy tales, and literary texts, aiming at representing, understanding and extracting information, e.g., Charniak (1972). Goyal et al. (2010) analyzed Aesop’s fables, producing automatic plot-unit representations (Lehnert, 1981) with a task-specific knowledge base of affect. Character traits and personas in stories have also been analyzed. For example, Elsner (2012) proposed a rich representation of story-characters for the purpose of summarizing and representing novels. Bamman et al. (2014) automatically inferred latent character types in English novels. VallsVargas et al. (2014) extracted characters and roles from Russian folk tales, based on their actions. Chaturvedi et al. (2015) analyzed short stories for characters’ desires and built a system to recognize desire fulfillment, using textual entailment. Researchers have also studied social networks and have modeled relationships in stories (Elson et al., 2010; Celikyilmaz et al., 2010). Agarwal et al."
Q18-1007,P10-1015,0,0.0898577,"n stories have also been analyzed. For example, Elsner (2012) proposed a rich representation of story-characters for the purpose of summarizing and representing novels. Bamman et al. (2014) automatically inferred latent character types in English novels. VallsVargas et al. (2014) extracted characters and roles from Russian folk tales, based on their actions. Chaturvedi et al. (2015) analyzed short stories for characters’ desires and built a system to recognize desire fulfillment, using textual entailment. Researchers have also studied social networks and have modeled relationships in stories (Elson et al., 2010; Celikyilmaz et al., 2010). Agarwal et al. (2013) modeled character interactions from Alice in Wonderland for the purpose of social network analysis. Chaturvedi et al. (2016) modeled character relationships in novels, using structured prediction. Wiebe (1994) proposed a method for tracking psychological points of view in narratives, looking at private states and subjective sentences. Ovesdotter Alm and Sproat (2005) studied emotional sequencing and trajectories in 22 Grimm’s fairy tales. Ware et al. (2011) analyzed dimensions of conflict in four simple, constructed stories, with the goal of e"
Q18-1007,W15-0608,1,0.777534,"in grades K12, employs literature/narratives as one of its three language arts genres. With the increased focus on automated evaluation of student writing in educational settings (Adams, 2014), automated methods for evaluating narrative essays at scale are becoming increasingly important. Automated scoring of narrative essays is a challenging area, and one that has not been explored extensively in NLP research. Previous work on automated essay scoring has focused on informational, argumentative, persuasive and source-based writing constructs (Stab and Gurevych, 2017; Nguyen and Litman, 2016; Farra et al., 2015; Somasundaran et al., 2014; Beigman Klebanov et al., 2014; Shermis and Burstein, 2013). Similarly, operational essay scoring engines (Attali and Burstein, 2006; Elliot, 2003) are geared towards evaluating language proficiency in general. In this work, we lay the groundwork and present the first results for automated scoring of narrative essays, focusing on narrative quality. One of the challenges in narrative quality analysis is the scarcity of scored essays in this genre. We describe a detailed manual annotation study on scoring student essays along multiple dimensions of narrative quality,"
Q18-1007,2009.mtsummit-wpt.10,0,0.195535,"yzed dimensions of conflict in four simple, constructed stories, with the goal of evaluating story content. Similarly, Swanson et al. (2014) analyzed blog narratives for narrative clause sub-types such as orientation, action and evaluation. Reagan et al. (2016) used sentiment analysis to generate emotional profiles for English novels. NLP methods have also been used for modeling and understanding narrative structures (Finlayson, 2012; Elson, 2012). See Finlayson (2013) and Mani (2012) for detailed literature surveys. One important aspect of a narrative is that it conveys a sequence of events (Fludernik, 2009; Almeida, 1995). Chambers and Jurafsky (2009; 2008) presented techniques for the automatic acquisition of event chains and event schemas (Chambers, 2013), which are related to earlier notions of scripts as prepackaged chunks of knowledge (Schank and Abelson, 1977). This line of research has received a great deal of attention (Nguyen et al., 2015; Balasubramanian et al., 2013; Jans et al., 2012; McIntyre and Lapata, 2010). For narratives, Ouyang and McKeown (2015) focused on automatic detection of compelling events. Bogel et al. (2014) worked on extraction and temporal ordering of events in na"
Q18-1007,D10-1008,0,0.215174,"Missing"
Q18-1007,P06-1108,0,0.0440737,"t Procedure (Strong et al., 1998). The Index of Narrative Complexity (Petersen et al., 2008) scores oral narratives on several dimensions and is used to study the effectiveness of clinical interventions. Olinghouse and Leaird (2009) used pictureprompts for eliciting narratives from about 200 students at the 2nd and 4th grade levels. The stories were evaluated for organization, development and creative vocabulary, but the study focused on vocabulary characteristics at different grade levels. McKeough et al. (2006) studied 150 student narratives in order to compare talented and average writers. Halpin and Moore (2006) analyzed students’ retelling of exemplar stories. They focused on event extraction, with the final goal of providing advice in an interactive story-telling environment. Passonneau 92 et al. (2007) annotated oral retellings of the same story on three consecutive days in order to study and model children’s comprehension. 2.2 Narrative Analysis in Computational Linguistics Research on narratives in Computational Linguistics has employed fables, fairy tales, and literary texts, aiming at representing, understanding and extracting information, e.g., Charniak (1972). Goyal et al. (2010) analyzed Ae"
Q18-1007,E12-1034,0,0.014015,"ding narrative structures (Finlayson, 2012; Elson, 2012). See Finlayson (2013) and Mani (2012) for detailed literature surveys. One important aspect of a narrative is that it conveys a sequence of events (Fludernik, 2009; Almeida, 1995). Chambers and Jurafsky (2009; 2008) presented techniques for the automatic acquisition of event chains and event schemas (Chambers, 2013), which are related to earlier notions of scripts as prepackaged chunks of knowledge (Schank and Abelson, 1977). This line of research has received a great deal of attention (Nguyen et al., 2015; Balasubramanian et al., 2013; Jans et al., 2012; McIntyre and Lapata, 2010). For narratives, Ouyang and McKeown (2015) focused on automatic detection of compelling events. Bogel et al. (2014) worked on extraction and temporal ordering of events in narratives. Based on the ‘Narrative Cloze Test’ (Chambers and Jurafsky, 2008), Mostafazadeh et al. (2016) presented a framework for evaluating story understanding algorithms, the ‘Story Cloze Test’, whose goal is to predict a held-out continuation of a short story. Our research differs significantly from previous work. We aim to evaluate, on an integer scale, the quality of narratives in student-"
Q18-1007,P14-5010,0,0.00249648,"aword distribution (Napoles et al., 2012), which has been automatically annotated with typed dependency information (de Marneffe and Manning, 2008). Following Chambers and Jurafsky (2008), we define events as verbs in a text (excluding be/have/do) and pairs of events are defined as those verbs that share arguments in the text. In the present work we limit our scope to the following set of (typed dependency) arguments: nsubj, dobj, nsubjpass, xsubj, csubj, csubjpass. To estimate event cohesion, we extract all event pairs from an essay after pre-processing it with the Stanford Core NLP toolkit (Manning et al., 2014). Event tokens from the essay are linked into pairs when they share a filler in their arguments. For essays, we use Stanford co-reference resolution for matching fillers of verb-argument slots. For all event pairs extracted from an essay, we query the events database to retrieve the pair association value (we use the point-wise mutual information (Church and Hanks, 1990)). We define three quantitative measures to encode event cohesion: (1) total count of event pairs in the essay; (2) proportion of in-essay event-pairs that are actually found in the events database; (3) proportion of in-essay e"
Q18-1007,P10-1158,0,0.0313519,"ctures (Finlayson, 2012; Elson, 2012). See Finlayson (2013) and Mani (2012) for detailed literature surveys. One important aspect of a narrative is that it conveys a sequence of events (Fludernik, 2009; Almeida, 1995). Chambers and Jurafsky (2009; 2008) presented techniques for the automatic acquisition of event chains and event schemas (Chambers, 2013), which are related to earlier notions of scripts as prepackaged chunks of knowledge (Schank and Abelson, 1977). This line of research has received a great deal of attention (Nguyen et al., 2015; Balasubramanian et al., 2013; Jans et al., 2012; McIntyre and Lapata, 2010). For narratives, Ouyang and McKeown (2015) focused on automatic detection of compelling events. Bogel et al. (2014) worked on extraction and temporal ordering of events in narratives. Based on the ‘Narrative Cloze Test’ (Chambers and Jurafsky, 2008), Mostafazadeh et al. (2016) presented a framework for evaluating story understanding algorithms, the ‘Story Cloze Test’, whose goal is to predict a held-out continuation of a short story. Our research differs significantly from previous work. We aim to evaluate, on an integer scale, the quality of narratives in student-generated essays. Insights f"
Q18-1007,N16-1167,0,0.023662,"QA neutral words. 6.4 Detailing Feature Set Providing specific details, such as names to characters, and describing the story elements, helps in developing the narrative and providing depth to the story. Proper nouns, adjectives and adverbs come into play when a writer provides descriptions. Thus, we create a Details feature set comprised of a total of 6 features encoding, separately, the presence (a binary feature) and the count of proper nouns, adjectives and adverbs. 6.5 Graph Feature Set Graph statistics have been reported to be effective for capturing development and coherence in essays (Mesgar and Strube, 2016; Somasundaran et al., 2016). We closely follow the implementation and features described in Somasundaran et al. (2016) for capturing narrative development (due to space constraints we refer the reader to the original paper). Graphs were constructed from essays by representing each content word (word type) in a sentence as a node in the graph. Links were drawn between words belonging to adjacent sentences. Features based on connectivity, shape and PageRank were extracted, giving a total of 19 Graph features. Specifically, the features used were: percentage of nodes with degrees one, two and th"
Q18-1007,N16-1098,0,0.0154622,"the automatic acquisition of event chains and event schemas (Chambers, 2013), which are related to earlier notions of scripts as prepackaged chunks of knowledge (Schank and Abelson, 1977). This line of research has received a great deal of attention (Nguyen et al., 2015; Balasubramanian et al., 2013; Jans et al., 2012; McIntyre and Lapata, 2010). For narratives, Ouyang and McKeown (2015) focused on automatic detection of compelling events. Bogel et al. (2014) worked on extraction and temporal ordering of events in narratives. Based on the ‘Narrative Cloze Test’ (Chambers and Jurafsky, 2008), Mostafazadeh et al. (2016) presented a framework for evaluating story understanding algorithms, the ‘Story Cloze Test’, whose goal is to predict a held-out continuation of a short story. Our research differs significantly from previous work. We aim to evaluate, on an integer scale, the quality of narratives in student-generated essays. Insights from previous work on narrative analysis can be useful for our purposes if they capture narrative techniques employed by student writers, and if they correlate with scores representing narrative quality. It is still an open question whether an elaborate representation and unders"
Q18-1007,W12-3018,0,0.0308542,"Missing"
Q18-1007,P15-1019,0,0.0139205,"hods have also been used for modeling and understanding narrative structures (Finlayson, 2012; Elson, 2012). See Finlayson (2013) and Mani (2012) for detailed literature surveys. One important aspect of a narrative is that it conveys a sequence of events (Fludernik, 2009; Almeida, 1995). Chambers and Jurafsky (2009; 2008) presented techniques for the automatic acquisition of event chains and event schemas (Chambers, 2013), which are related to earlier notions of scripts as prepackaged chunks of knowledge (Schank and Abelson, 1977). This line of research has received a great deal of attention (Nguyen et al., 2015; Balasubramanian et al., 2013; Jans et al., 2012; McIntyre and Lapata, 2010). For narratives, Ouyang and McKeown (2015) focused on automatic detection of compelling events. Bogel et al. (2014) worked on extraction and temporal ordering of events in narratives. Based on the ‘Narrative Cloze Test’ (Chambers and Jurafsky, 2008), Mostafazadeh et al. (2016) presented a framework for evaluating story understanding algorithms, the ‘Story Cloze Test’, whose goal is to predict a held-out continuation of a short story. Our research differs significantly from previous work. We aim to evaluate, on an int"
Q18-1007,D15-1257,0,0.0921626,"inlayson (2013) and Mani (2012) for detailed literature surveys. One important aspect of a narrative is that it conveys a sequence of events (Fludernik, 2009; Almeida, 1995). Chambers and Jurafsky (2009; 2008) presented techniques for the automatic acquisition of event chains and event schemas (Chambers, 2013), which are related to earlier notions of scripts as prepackaged chunks of knowledge (Schank and Abelson, 1977). This line of research has received a great deal of attention (Nguyen et al., 2015; Balasubramanian et al., 2013; Jans et al., 2012; McIntyre and Lapata, 2010). For narratives, Ouyang and McKeown (2015) focused on automatic detection of compelling events. Bogel et al. (2014) worked on extraction and temporal ordering of events in narratives. Based on the ‘Narrative Cloze Test’ (Chambers and Jurafsky, 2008), Mostafazadeh et al. (2016) presented a framework for evaluating story understanding algorithms, the ‘Story Cloze Test’, whose goal is to predict a held-out continuation of a short story. Our research differs significantly from previous work. We aim to evaluate, on an integer scale, the quality of narratives in student-generated essays. Insights from previous work on narrative analysis can"
Q18-1007,prasad-etal-2008-penn,0,0.0316723,"acteristics. Each set is 96 Org. Dev. Conv. Nar. Total Org. 1.00 Dev. 0.88 1.00 Conv. 0.40 0.42 1.00 Nar. 0.97 0.97 0.42 1.00 Tot. 0.93 0.94 0.64 0.97 1.00 Table 3: Score correlations for traits, Narrative and Total. described in detail in the following sections. 6.1 Transition Feature Set Effective organization of ideas and events is typically achieved with the use of discourse markers. In order to encode effective transitioning, we compiled a transition-cue lexicon, and constructed features based on it. We compiled a list of 234 discourse cues from the Penn Discourse Treebank (PDTB) manual (Prasad et al., 2008), and we manually collected a list of transition cues from the web by mining websites that provide tips on good essay/narrative writing. The latter, with a total of 484 unigrams and multi-word expressions, is more focused on cues that are used commonly to write stories (e.g., cues that provide locational or temporal connections) than the former. Using the lexicon, we extracted two features from each essay: the number of cues in the essay and that number divided by the essay length. These two features form the Transition feature set. 6.2 Event-oriented Feature Set Events are the building blocks"
Q18-1007,C14-1090,1,0.870801,"oys literature/narratives as one of its three language arts genres. With the increased focus on automated evaluation of student writing in educational settings (Adams, 2014), automated methods for evaluating narrative essays at scale are becoming increasingly important. Automated scoring of narrative essays is a challenging area, and one that has not been explored extensively in NLP research. Previous work on automated essay scoring has focused on informational, argumentative, persuasive and source-based writing constructs (Stab and Gurevych, 2017; Nguyen and Litman, 2016; Farra et al., 2015; Somasundaran et al., 2014; Beigman Klebanov et al., 2014; Shermis and Burstein, 2013). Similarly, operational essay scoring engines (Attali and Burstein, 2006; Elliot, 2003) are geared towards evaluating language proficiency in general. In this work, we lay the groundwork and present the first results for automated scoring of narrative essays, focusing on narrative quality. One of the challenges in narrative quality analysis is the scarcity of scored essays in this genre. We describe a detailed manual annotation study on scoring student essays along multiple dimensions of narrative quality, such as narrative developme"
Q18-1007,W15-0605,1,0.759213,"uch in a personal story about a travel experience. To the best of our knowledge, this work makes a first attempt at investigating the evaluation of narrative quality using automated methods. 2.3 Automated essay scoring There are a number of automated essay scoring (AES) systems, many of which are used operationally, such as e-raterr (Attali and Burstein, 2006), Intellimetric (Elliot, 2003), the Intelligent Essay Assessor (Landauer et al., 2003) and Project Essay Grade (Page, 1994). However, these previous studies have not been focused on narratives. 93 In a somewhat related study to this one, Somasundaran et al. (2015) scored oral narratives that were generated by international students in response to a series of pictures. Some of the features used in that study overlap with our work due to the overlap in the genre; however, their focus was on scoring the response for language proficiency. Graph features, which we have used in this work, have been shown to be effective in capturing idea development in essays (Somasundaran et al., 2016). This work also employs graph features, but it is one of the many we explore for encoding the various linguistic phenomena that characterize good narratives. 3 Data Our data"
Q18-1007,C16-1148,1,0.91997,"(Landauer et al., 2003) and Project Essay Grade (Page, 1994). However, these previous studies have not been focused on narratives. 93 In a somewhat related study to this one, Somasundaran et al. (2015) scored oral narratives that were generated by international students in response to a series of pictures. Some of the features used in that study overlap with our work due to the overlap in the genre; however, their focus was on scoring the response for language proficiency. Graph features, which we have used in this work, have been shown to be effective in capturing idea development in essays (Somasundaran et al., 2016). This work also employs graph features, but it is one of the many we explore for encoding the various linguistic phenomena that characterize good narratives. 3 Data Our data comprises narrative essays written by R school students in the Criterion program1 , an online writing evaluation service from Educational Testing Service. It is a web-based, instructor-led writing tool that helps students plan, write and revise their essays. Narrative essays were obtained from grade levels 7, 10 and 12. Each essay was written in response to one of 18 story-telling prompts related to personal experiences,"
Q18-1007,E17-1092,0,0.0595168,"es that details requirements for student knowledge in grades K12, employs literature/narratives as one of its three language arts genres. With the increased focus on automated evaluation of student writing in educational settings (Adams, 2014), automated methods for evaluating narrative essays at scale are becoming increasingly important. Automated scoring of narrative essays is a challenging area, and one that has not been explored extensively in NLP research. Previous work on automated essay scoring has focused on informational, argumentative, persuasive and source-based writing constructs (Stab and Gurevych, 2017; Nguyen and Litman, 2016; Farra et al., 2015; Somasundaran et al., 2014; Beigman Klebanov et al., 2014; Shermis and Burstein, 2013). Similarly, operational essay scoring engines (Attali and Burstein, 2006; Elliot, 2003) are geared towards evaluating language proficiency in general. In this work, we lay the groundwork and present the first results for automated scoring of narrative essays, focusing on narrative quality. One of the challenges in narrative quality analysis is the scarcity of scored essays in this genre. We describe a detailed manual annotation study on scoring student essays alo"
Q18-1007,W14-4323,0,0.0185981,"et al. (2013) modeled character interactions from Alice in Wonderland for the purpose of social network analysis. Chaturvedi et al. (2016) modeled character relationships in novels, using structured prediction. Wiebe (1994) proposed a method for tracking psychological points of view in narratives, looking at private states and subjective sentences. Ovesdotter Alm and Sproat (2005) studied emotional sequencing and trajectories in 22 Grimm’s fairy tales. Ware et al. (2011) analyzed dimensions of conflict in four simple, constructed stories, with the goal of evaluating story content. Similarly, Swanson et al. (2014) analyzed blog narratives for narrative clause sub-types such as orientation, action and evaluation. Reagan et al. (2016) used sentiment analysis to generate emotional profiles for English novels. NLP methods have also been used for modeling and understanding narrative structures (Finlayson, 2012; Elson, 2012). See Finlayson (2013) and Mani (2012) for detailed literature surveys. One important aspect of a narrative is that it conveys a sequence of events (Fludernik, 2009; Almeida, 1995). Chambers and Jurafsky (2009; 2008) presented techniques for the automatic acquisition of event chains and e"
Q18-1007,J94-2004,0,0.888941,"et al. (2014) extracted characters and roles from Russian folk tales, based on their actions. Chaturvedi et al. (2015) analyzed short stories for characters’ desires and built a system to recognize desire fulfillment, using textual entailment. Researchers have also studied social networks and have modeled relationships in stories (Elson et al., 2010; Celikyilmaz et al., 2010). Agarwal et al. (2013) modeled character interactions from Alice in Wonderland for the purpose of social network analysis. Chaturvedi et al. (2016) modeled character relationships in novels, using structured prediction. Wiebe (1994) proposed a method for tracking psychological points of view in narratives, looking at private states and subjective sentences. Ovesdotter Alm and Sproat (2005) studied emotional sequencing and trajectories in 22 Grimm’s fairy tales. Ware et al. (2011) analyzed dimensions of conflict in four simple, constructed stories, with the goal of evaluating story content. Similarly, Swanson et al. (2014) analyzed blog narratives for narrative clause sub-types such as orientation, action and evaluation. Reagan et al. (2016) used sentiment analysis to generate emotional profiles for English novels. NLP me"
Q18-1007,H05-1044,0,0.0285092,"of scores for all chains in the essay. For each of the features 4-10, we also produce a feature that is normalized by the log of the essay length (log wordcount). 6.3 Subjectivity-based Feature Set Evaluative and subjective language is used to describe characters (e.g., foolish, smart), situations (e.g., grand, impoverished) and characters’ private states (e.g., thoughts, beliefs, happiness, sadness) (Wiebe, 1994). These are evidenced when characters are described and story-lines are developed. We use two lexicons for detecting sentiment and subjective words: the MPQA subjectivity lexicon 97 (Wilson et al., 2005) and a sentiment lexicon, ASSESS, developed for essay scoring (Beigman Klebanov et al., 2012). MPQA associates a positive/negative/neutral polarity category to its entries, while ASSESS assigns a positive/negative/neutral polarity probability to its entries. We consider a term from ASSESS to be polar if the sum of positive and negative probabilities is greater than 0.65 (based on manual inspection of the lexicon). The neutral category in MPQA comprises subjective terms that indicate speech acts and private states (e.g., view, assess, believe), which is valuable for our purposes. The neutral ca"
W07-1604,P03-2026,0,0.608351,"Missing"
W07-1604,izumi-etal-2004-overview,0,0.173212,"Missing"
W07-1604,Y04-1032,0,0.0517139,"Missing"
W08-1205,P96-1042,0,0.06125,"FPs in the sample from the “Error” sub-corpus. For the hypothetical data in Figure 1, these values are 600/750 = 0.80 for Hits, and 150/750 = 0.20 for FPs. Calculate the proportion of Misses in the sample from the “OK” subcorpus. For the hypothetical data, this is 450/1500 = 0.30 for Misses. Estimated Overall Rates Sample Proportion * Sub-Corpus Proportion 0.80 * 0.10 = 0.08 0.20 * 0.10 = 0.02 0.30 * 0.90 = 0.27 0.08/(0.08 + 0.02) = 0.80 0.08/(0.08 + 0.27) = 0.23 Table 5: Sampling Calculations (Hypothetical) This method is similar in spirit to active learning ((Dagan and Engelson, 1995) and (Engelson and Dagan, 1996)), which has been used to iteratively build up an annotated corpus, but it differs from active learning applications in that there are no iterative loops between the system and the human annotator(s). In addition, while our methodology is used for evaluating a system, active learning is commonly used for training a system. 6. The values computed in step 5 are conditional proportions based on the sub-corpora. To calculate the overall proportions in the test corpus, it is necessary to multiply each value by the relative size of its sub-corpus. This is shown in Table 5, where the proportion of Hi"
W08-1205,W07-1607,0,0.161831,"Missing"
W08-1205,I08-1059,0,0.506964,"notator disagreements about discourse and semantics as being quite common, our studies show that judgments of preposition usage, which is largely lexically driven, can be just as contentious. As a result, this unreliability poses a serious issue for the development and evaluation of NLP tools in the task of automatically detecting preposition usage errors in the writing of non-native speakers of English. To date, single human annotation has typically been the gold standard for grammatical error detection, such as in the work of (Izumi et al., 2004), (Han et al., 2006), (Nagata et al., 2006), (Gamon et al., 2008)1 . Although there are several learner cor• Sampling Approach Multiple annotation can be very costly and time-consuming, which may explain why previous work employed only one rater. As an alternative to the standard exhaustive annotation, we propose a sampling approach in which estimates of the rates of hits, false positives, and misses are derived from random samples of the system’s output, and then precision and recall of the system can be calculated. We show that estimates of system performance derived c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unp"
W08-1205,izumi-etal-2004-overview,0,0.436773,"ments in rating preposition usage. While one tends to think of annotator disagreements about discourse and semantics as being quite common, our studies show that judgments of preposition usage, which is largely lexically driven, can be just as contentious. As a result, this unreliability poses a serious issue for the development and evaluation of NLP tools in the task of automatically detecting preposition usage errors in the writing of non-native speakers of English. To date, single human annotation has typically been the gold standard for grammatical error detection, such as in the work of (Izumi et al., 2004), (Han et al., 2006), (Nagata et al., 2006), (Gamon et al., 2008)1 . Although there are several learner cor• Sampling Approach Multiple annotation can be very costly and time-consuming, which may explain why previous work employed only one rater. As an alternative to the standard exhaustive annotation, we propose a sampling approach in which estimates of the rates of hits, false positives, and misses are derived from random samples of the system’s output, and then precision and recall of the system can be calculated. We show that estimates of system performance derived c 2008. Licensed under t"
W08-1205,P06-1031,0,0.082225,"ne tends to think of annotator disagreements about discourse and semantics as being quite common, our studies show that judgments of preposition usage, which is largely lexically driven, can be just as contentious. As a result, this unreliability poses a serious issue for the development and evaluation of NLP tools in the task of automatically detecting preposition usage errors in the writing of non-native speakers of English. To date, single human annotation has typically been the gold standard for grammatical error detection, such as in the work of (Izumi et al., 2004), (Han et al., 2006), (Nagata et al., 2006), (Gamon et al., 2008)1 . Although there are several learner cor• Sampling Approach Multiple annotation can be very costly and time-consuming, which may explain why previous work employed only one rater. As an alternative to the standard exhaustive annotation, we propose a sampling approach in which estimates of the rates of hits, false positives, and misses are derived from random samples of the system’s output, and then precision and recall of the system can be calculated. We show that estimates of system performance derived c 2008. Licensed under the Creative Commons Attribution-Noncommerci"
W08-1205,A00-2019,1,0.851772,"Missing"
W08-1205,W07-1604,1,0.675429,"Missing"
W08-1205,C08-1109,1,0.46915,"ns, determiners and collocations. In the work discussed here, we target preposition usage errors, specifically those of incorrect selection (“we arrived to the station”) and 3 Automatically Detecting Preposition Usage Errors In this section, we give a description of our system and compare its performance to other systems. Although the focus of this paper is on human judgments in the task of error detection, we describe our system to show that variability in human judgments can impact the evaluation of a system in this task. A full description of our system and its performance can be found in (Tetreault and Chodorow, 2008). 3.1 System Our approach treats preposition error detection as a classification problem: that is, given a context of two words before and two words after the writer’s preposition, what is the best preposition to use? 4 There is a third error type, omission (“we are fond null beer”), that is a topic for our future research. 25 Prep in for of on to with at by as from about An error is marked when the system’s suggestion differs from the writer’s by a certain threshold amount. We have used a maximum entropy (ME) classifier (Ratnaparkhi, 1998) to select the most probable preposition for a given c"
W08-1205,P03-2026,0,\N,Missing
W09-3010,N04-2006,1,0.842388,"non-native writing is difficult because of the possibility of multiple correct answers and the variability in human agreement. This paper seeks to improve the best practice of such evaluation by analyzing the frequency of multiple correct answers and identifying factors that influence agreement levels in judging the usage of articles and noun number. 1 Introduction In recent years, systems have been developed with the long-term goal of detecting, in the writing of non-native speakers, usage errors involving articles, prepositions and noun number (Knight and Chander, 1994; Minnen et al., 2000; Lee, 2004; Han et al., 2005; Peng and Araki, 2005; Brockett et al., 2006; Turner and Charniak, 2007). These systems should, ideally, be evaluated on a corpus of learners’ writing, annotated with acceptable corrections. However, since such corpora are expensive to compile, many researchers have instead resorted to measuring the accuracy of predicting what a native writer originally wrote in well-formed text. This type of evaluation effectively makes the assumption that there is one correct form of native usage per context, which may not always be the case. Two studies have already challenged this “singl"
W09-3010,W00-0708,0,0.300286,"Missing"
W09-3010,H94-1048,0,0.0215294,"ld be annotated. Using this annotation scheme, however, raises two questions that have not yet been thoroughly researched: (1) what is the human agreement level on such annotation? (2) what factors might influence the agreement level? In this paper, we consider two factors: the context of a word, and the variability of its usage. In the two studies cited above, the human judges were shown only the target sentence and did not take into account any constraint on the choice of word that might be imposed by the larger context. For PP attachment, human performance improves when given more context (Ratnaparkhi et al., 1994). For other linguistic phenomena, such as article/number selection for nouns, a larger context window of at least several sentences may be required, even though some automatic methods for exploiting context have not been shown to boost performance (Han et al., 2005). The second factor, variability of usage, may be 60 Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 60–63, c Suntec, Singapore, 6-7 August 2009. 2009 ACL and AFNLP null Three years ago John Small, a sheep farmer in the Mendip Hills, read an editorial in his local newspaper which claimed that foxes ne"
W09-3010,N07-2045,0,0.134007,"Missing"
W09-3010,W08-1205,1,\N,Missing
W09-3010,P06-1032,0,\N,Missing
W10-1006,D09-1030,0,0.0747838,"Missing"
W10-1006,I08-1059,0,0.0420656,"but at a fraction of the time and cost. Based on these results, we propose a new evaluation method which makes it feasible to compare two error detection systems tested on different learner data sets. 1 Introduction The last few years have seen an explosion in the development of NLP tools to detect and correct errors made by learners of English as a Second Language (ESL). While there has been considerable emphasis placed on the system development aspect of the field, with researchers tackling some of the toughest ESL errors such as those involving articles (Han et al., 2006) and prepositions (Gamon et al., 2008), (Felice and Pullman, 2009), there has been a woeful lack of attention paid to developing best practices for annotation and evaluation. Annotation in the field of ESL error detection has typically relied on just one trained rater, and that rater’s judgments then become the gold standard for evaluating a system. So it is very rare that inter-rater reliability is reported, although, in other NLP subfields, reporting reliability is the norm. Time and cost are probably the two most important reasons why past work has relied on only one rater because using multiple annotators on the same ESL texts"
W10-1006,D08-1027,0,0.245786,"Missing"
W10-1006,C08-1109,1,0.85996,"ts of what is acceptable. One way to address this is by aggregating a multitude of judgments for each preposition and treating this as the gold standard, however such a tactic has been impractical due to time and cost limitations. While annotation is a problem in this field, comparing one system to another has also been a major issue. To date, none of the preposition and article error detection systems in the literature have been evaluated on the same corpus. This is mostly due to the fact that learner corpora are difficult to acquire (and then annotate), but also to the fact that they are 1 (Tetreault and Chodorow, 2008b) report that it would take 80hrs for one of their trained raters to find and mark 1,000 preposition errors. Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 45–48, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics • In terms of cost and time, AMT is an effective alternative to trained raters on the tasks of preposition selection in well-formed text and preposition error annotation in ESL text. • With AMT, it is possible to efficiently collect multiple judgments for a target construction."
W10-1006,W08-1205,1,0.867989,"ts of what is acceptable. One way to address this is by aggregating a multitude of judgments for each preposition and treating this as the gold standard, however such a tactic has been impractical due to time and cost limitations. While annotation is a problem in this field, comparing one system to another has also been a major issue. To date, none of the preposition and article error detection systems in the literature have been evaluated on the same corpus. This is mostly due to the fact that learner corpora are difficult to acquire (and then annotate), but also to the fact that they are 1 (Tetreault and Chodorow, 2008b) report that it would take 80hrs for one of their trained raters to find and mark 1,000 preposition errors. Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 45–48, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics • In terms of cost and time, AMT is an effective alternative to trained raters on the tasks of preposition selection in well-formed text and preposition error annotation in ESL text. • With AMT, it is possible to efficiently collect multiple judgments for a target construction."
W11-2111,P07-1111,0,0.0171842,"tions using language model (LM) and syntactic features. Kulesza and Shieber (2004) attempt the same task using an support vector machine (SVM) classifier and features derived from reference-based MT metrics such as WER, PER, BLEU and NIST. They also claim that the confidence score for the classifier being used, if available, may be taken as an estimate of translation quality. Quirk (2004) took a different approach and examined whether it is possible to explicitly compute a confidence measure for each translated sentence by using features derived from both the source and target language sides. Albrecht and Hwa (2007a) expanded on this idea and conducted a larger scale study to show the viability of regression as a sentence-level metric of MT quality. They used features derived from several other reference-driven MT metrics. In other work (Albrecht and Hwa, 2007b), they showed that one could substitute translations from other MT systems for human-authored reference translations and derive the regression features from them. Gamon et al. (2005) build a classifier to distinguish machine-generated translations from human 108 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 108–115, c"
W11-2111,P07-1038,0,0.0169553,"tions using language model (LM) and syntactic features. Kulesza and Shieber (2004) attempt the same task using an support vector machine (SVM) classifier and features derived from reference-based MT metrics such as WER, PER, BLEU and NIST. They also claim that the confidence score for the classifier being used, if available, may be taken as an estimate of translation quality. Quirk (2004) took a different approach and examined whether it is possible to explicitly compute a confidence measure for each translated sentence by using features derived from both the source and target language sides. Albrecht and Hwa (2007a) expanded on this idea and conducted a larger scale study to show the viability of regression as a sentence-level metric of MT quality. They used features derived from several other reference-driven MT metrics. In other work (Albrecht and Hwa, 2007b), they showed that one could substitute translations from other MT systems for human-authored reference translations and derive the regression features from them. Gamon et al. (2005) build a classifier to distinguish machine-generated translations from human 108 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 108–115, c"
W11-2111,W10-1703,0,0.313827,"ver et al. (2009) were used to produce scores tuned for fluency and adequacy. 3. METEOR (Lavie and Denkowski, 2009): Meteor scores were produced using Meteor-next v1.2. All types of matches were allowed (exact, stem, synonym and paraphrase) and scores tuned specifically to rank, HTER and adequacy were produced using the “-t” flag in the tool. We also implemented features closely related to or inspired by other MT metrics. The set of these auxiliary features is referred to as “Aux”. 1. Character-level statistics: Based on the success of the i-letter-BLEU and i-letter-recall metrics from WMT10 (Callison-Burch et al., 2010), we added the harmonic mean of precision (or recall) for character n-grams (from 1 to 10) as features. 2. Raw n-gram matches: We calculated the precision and precision for word n-grams (up to n=6) and added each as a separate feature (for a total of 12). Although these statistics are also calculated as part of the MT metrics above, breaking them into separate features gives the model more information. 3. Length ratios: The ratio between the lengths of the MT output and the reference translation was calculated on a character level and a word level. These ratios were also calculated between the"
W11-2111,A00-2019,1,0.607784,"ary automated essay scoring system developed by Educational Testing Service (ETS) to assess writing quality.1 The system has been used operationally for over 10 years in highstakes exams such as the GRE and TOEFL given its speed, reliability and high agreement with human raters. E-rater combines 8 main features using linear regression to produce a numerical score for an essay. These features are grammar, usage, mechanics, style, organization, development, lexical complexity and vocabulary usage. The grammar feature covers errors such as sentence fragments, verb form errors and pronoun errors (Chodorow and Leacock, 2000). The usage feature detects errors related to articles (Han et al., 2006), prepositions (Tetreault and Chodorow, 2008) and collocations (Futagi et al., 2008). The mechanics feature checks for spelling, punctuation and capitalization errors. The style feature checks for passive constructions and word repetition, among others. Organization and development tabulate the presence or absence of discourse elements and the length of each element. Finally, the lexical complexity feature details how complex the writer’s words are based on frequency indices and writing scales, and the vocabulary feature"
W11-2111,P01-1020,0,0.105398,"compare translation hypotheses to a set of human-authored reference translations. However, there has also been some work on methods that are not dependent on human-authored translations. One subset of such methods is task-based in that the methods determine the quality of a translation in terms of how well it serves the need of an extrinsic task. These tasks can either be downstream NLP Besides extrinsic evaluation, there is another set of methods that attempt to “learn” what makes a good translation and then predict the quality of new translations without comparing to reference translations. Corston-Oliver et al. (2001) proposed the idea of building a decision tree classifier to simply distinguish between machine and human translations using language model (LM) and syntactic features. Kulesza and Shieber (2004) attempt the same task using an support vector machine (SVM) classifier and features derived from reference-based MT metrics such as WER, PER, BLEU and NIST. They also claim that the confidence score for the classifier being used, if available, may be taken as an estimate of translation quality. Quirk (2004) took a different approach and examined whether it is possible to explicitly compute a confidenc"
W11-2111,W08-0331,0,0.0463365,"cs above, breaking them into separate features gives the model more information. 3. Length ratios: The ratio between the lengths of the MT output and the reference translation was calculated on a character level and a word level. These ratios were also calculated between the MT output and the source sentence. 4. OOV heuristic: The percentage of tokens in the MT that match the source sentence. This is a low-precision heuristic for counting out of vocabulary (OOV) words, since it also counts named entities and words that happen to be the same in different languages. 4.3 Ranking Model Following (Duh, 2008), we represent sentence-level MT evaluation as a ranking problem. For a particular source sentence, there are N machine translations and one reference translation. A feature vector is extracted from each {source, reference, MT} tuple. The training data consists of sets of translations that have been annotated with relative ranks. During training, all ranked sets are converted to sets of feature vectors, where the label for each feature vector is the rank. The ranking model is a linear SVM that predicts a relative score for each feature vector, and is implemented by SVM-rank (Joachims, 2006). W"
W11-2111,2005.eamt-1.15,0,0.151528,"r it is possible to explicitly compute a confidence measure for each translated sentence by using features derived from both the source and target language sides. Albrecht and Hwa (2007a) expanded on this idea and conducted a larger scale study to show the viability of regression as a sentence-level metric of MT quality. They used features derived from several other reference-driven MT metrics. In other work (Albrecht and Hwa, 2007b), they showed that one could substitute translations from other MT systems for human-authored reference translations and derive the regression features from them. Gamon et al. (2005) build a classifier to distinguish machine-generated translations from human 108 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 108–115, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics ones using fluency-based features and show that by combining the scores of this classifier with LM perplexities, they obtain an MT metric that has good correlation with human judgments but not better than the baseline BLEU metric. The fundamental questions that inspired our proposed metrics are as follows: • Can an operational English-profici"
W11-2111,N07-2020,0,0.0667457,"Missing"
W11-2111,2004.tmi-1.8,0,0.0272496,"of such methods is task-based in that the methods determine the quality of a translation in terms of how well it serves the need of an extrinsic task. These tasks can either be downstream NLP Besides extrinsic evaluation, there is another set of methods that attempt to “learn” what makes a good translation and then predict the quality of new translations without comparing to reference translations. Corston-Oliver et al. (2001) proposed the idea of building a decision tree classifier to simply distinguish between machine and human translations using language model (LM) and syntactic features. Kulesza and Shieber (2004) attempt the same task using an support vector machine (SVM) classifier and features derived from reference-based MT metrics such as WER, PER, BLEU and NIST. They also claim that the confidence score for the classifier being used, if available, may be taken as an estimate of translation quality. Quirk (2004) took a different approach and examined whether it is possible to explicitly compute a confidence measure for each translated sentence by using features derived from both the source and target language sides. Albrecht and Hwa (2007a) expanded on this idea and conducted a larger scale study"
W11-2111,P02-1040,0,0.0878901,"se the MTeRater-Plus meta-metric that uses e-rater features plus all of the hybrid features described below. Both metrics were trained on the same data using the same machine learning model, and differ only in their feature sets. 4.1 E-rater Features Each sentence is associated with an e-rater sentencelevel vector and a document-level vector as previously described and each column in these vectors was used a feature. 4.2 Features for Hybrid Models We used existing automatic MT metrics as baselines in our evaluation, and also as features in our hybrid metric. The metrics we used were: 1. BLEU (Papineni et al., 2002): Case-insensitive and case-sensitive BLEU scores were produced using mteval-v13a.pl, which calculates smoothed sentence-level scores. 2. TERp (Snover et al., 2009): Translation Edit Rate plus (TERp) scores were produced using terp v1. The scores were case-insensitive and edit costs from Snover et al. (2009) were used to produce scores tuned for fluency and adequacy. 3. METEOR (Lavie and Denkowski, 2009): Meteor scores were produced using Meteor-next v1.2. All types of matches were allowed (exact, stem, synonym and paraphrase) and scores tuned specifically to rank, HTER and adequacy were produ"
W11-2111,quirk-2004-training,0,0.0527501,"ct the quality of new translations without comparing to reference translations. Corston-Oliver et al. (2001) proposed the idea of building a decision tree classifier to simply distinguish between machine and human translations using language model (LM) and syntactic features. Kulesza and Shieber (2004) attempt the same task using an support vector machine (SVM) classifier and features derived from reference-based MT metrics such as WER, PER, BLEU and NIST. They also claim that the confidence score for the classifier being used, if available, may be taken as an estimate of translation quality. Quirk (2004) took a different approach and examined whether it is possible to explicitly compute a confidence measure for each translated sentence by using features derived from both the source and target language sides. Albrecht and Hwa (2007a) expanded on this idea and conducted a larger scale study to show the viability of regression as a sentence-level metric of MT quality. They used features derived from several other reference-driven MT metrics. In other work (Albrecht and Hwa, 2007b), they showed that one could substitute translations from other MT systems for human-authored reference translations"
W11-2111,W09-0441,1,0.84083,"machine learning model, and differ only in their feature sets. 4.1 E-rater Features Each sentence is associated with an e-rater sentencelevel vector and a document-level vector as previously described and each column in these vectors was used a feature. 4.2 Features for Hybrid Models We used existing automatic MT metrics as baselines in our evaluation, and also as features in our hybrid metric. The metrics we used were: 1. BLEU (Papineni et al., 2002): Case-insensitive and case-sensitive BLEU scores were produced using mteval-v13a.pl, which calculates smoothed sentence-level scores. 2. TERp (Snover et al., 2009): Translation Edit Rate plus (TERp) scores were produced using terp v1. The scores were case-insensitive and edit costs from Snover et al. (2009) were used to produce scores tuned for fluency and adequacy. 3. METEOR (Lavie and Denkowski, 2009): Meteor scores were produced using Meteor-next v1.2. All types of matches were allowed (exact, stem, synonym and paraphrase) and scores tuned specifically to rank, HTER and adequacy were produced using the “-t” flag in the tool. We also implemented features closely related to or inspired by other MT metrics. The set of these auxiliary features is referre"
W11-2111,C08-1109,1,0.871697,"stem has been used operationally for over 10 years in highstakes exams such as the GRE and TOEFL given its speed, reliability and high agreement with human raters. E-rater combines 8 main features using linear regression to produce a numerical score for an essay. These features are grammar, usage, mechanics, style, organization, development, lexical complexity and vocabulary usage. The grammar feature covers errors such as sentence fragments, verb form errors and pronoun errors (Chodorow and Leacock, 2000). The usage feature detects errors related to articles (Han et al., 2006), prepositions (Tetreault and Chodorow, 2008) and collocations (Futagi et al., 2008). The mechanics feature checks for spelling, punctuation and capitalization errors. The style feature checks for passive constructions and word repetition, among others. Organization and development tabulate the presence or absence of discourse elements and the length of each element. Finally, the lexical complexity feature details how complex the writer’s words are based on frequency indices and writing scales, and the vocabulary feature evaluates how appropriate the words are for the given topic). Since many of the features are essay-specific, there is"
W11-2111,vilar-etal-2006-error,0,0.0565763,"Missing"
W11-2111,P09-1048,1,\N,Missing
W12-2005,N03-1003,0,0.0574673,"; Rosti et al., 2007; Feng et al., 2009). However, our combination approach is different in that all the round-trip translations are produced by a single system but via different pivot languages. Finally, the idea of combining multiple surface renderings with the same meaning has also been explored in paraphrase generation. Pang et al. (2003) propose an algorithm to align sets of parallel sentences driven entirely by the syntactic representations of the sentences. The alignment algorithm outputs a merged lattice from which lexical, phrasal, and sentential paraphrases could simply be read off. Barzilay and Lee (2003) cluster topically related sentences into slotted word lattices by using multiple sequence alignment for the purpose of downstream paraphrase generation from comparable corpora. More recently, Zhao et al. (2010) perform round-trip translation of English sentences via different pivot languages and different off-the-shelf SMT systems to generate candidate paraphrases. However, they do not combine the candidate paraphrases in any way. A detailed survey of paraphrase generation techniques can be found in (Androutsopoulos and Malakasiotis, 2010) and (Madnani and Dorr, 2010). 3 Methodology The basic"
W12-2005,P06-1032,0,0.470893,"Missing"
W12-2005,D09-1115,0,0.0341623,"Missing"
W12-2005,W09-2110,0,0.28295,"Missing"
W12-2005,J10-3003,1,0.826535,"ould simply be read off. Barzilay and Lee (2003) cluster topically related sentences into slotted word lattices by using multiple sequence alignment for the purpose of downstream paraphrase generation from comparable corpora. More recently, Zhao et al. (2010) perform round-trip translation of English sentences via different pivot languages and different off-the-shelf SMT systems to generate candidate paraphrases. However, they do not combine the candidate paraphrases in any way. A detailed survey of paraphrase generation techniques can be found in (Androutsopoulos and Malakasiotis, 2010) and (Madnani and Dorr, 2010). 3 Methodology The basic idea underlying our error correction technique is quite simple: if we can automatically generate alternative surface renderings of the meaning expressed in the original sentence and then pick the one that is most fluent, we are likely to have picked a version of the sentence in which the original grammatical errors have been fixed. In this paper, we propose generating such alternative formulations using statistical machine translation. For example, we take the original sentence E and translate it to Chinese using the Google TransOriginal Swedish Italian Russian French"
W12-2005,N03-1024,0,0.0396708,"r detection, our combination approach is directly related to the research on machine translation system combination wherein translation hypotheses produced by different SMT systems are combined to allow the extraction of a better, combined hypothesis (Bangalore et al., 2001; Rosti et al., 2007; Feng et al., 2009). However, our combination approach is different in that all the round-trip translations are produced by a single system but via different pivot languages. Finally, the idea of combining multiple surface renderings with the same meaning has also been explored in paraphrase generation. Pang et al. (2003) propose an algorithm to align sets of parallel sentences driven entirely by the syntactic representations of the sentences. The alignment algorithm outputs a merged lattice from which lexical, phrasal, and sentential paraphrases could simply be read off. Barzilay and Lee (2003) cluster topically related sentences into slotted word lattices by using multiple sequence alignment for the purpose of downstream paraphrase generation from comparable corpora. More recently, Zhao et al. (2010) perform round-trip translation of English sentences via different pivot languages and different off-the-shelf"
W12-2005,P11-1094,0,0.0311841,"Missing"
W12-2005,N07-1029,0,0.0345598,"f different round-trip translations and explore a whole new set of corrections that go beyond the translations themselves. Finally, we do not restrict our analysis to any single type of 45 error. In fact, our test sentences contain several different types of grammatical errors. Outside of the literature on grammatical error detection, our combination approach is directly related to the research on machine translation system combination wherein translation hypotheses produced by different SMT systems are combined to allow the extraction of a better, combined hypothesis (Bangalore et al., 2001; Rosti et al., 2007; Feng et al., 2009). However, our combination approach is different in that all the round-trip translations are produced by a single system but via different pivot languages. Finally, the idea of combining multiple surface renderings with the same meaning has also been explored in paraphrase generation. Pang et al. (2003) propose an algorithm to align sets of parallel sentences driven entirely by the syntactic representations of the sentences. The alignment algorithm outputs a merged lattice from which lexical, phrasal, and sentential paraphrases could simply be read off. Barzilay and Lee (20"
W12-2005,W09-0441,1,0.823581,"ree itself. Therefore, the task is more complex than simply selecting the right round-trip translation. We posit that a better approach will be to combine the evidence of correction produced by each independent translation model and increase the likelihood of producing a final whole-sentence correction. Additionally, by engineering such a combination, we increase the likelihood that the final correction will preserve the meaning of the original sentence. In order to combine the round-trip translations, we developed a heuristic alignment algorithm that uses the TERp machine translation metric (Snover et al., 2009). The TERp metric takes a pair of sentences and computes the least number of edit operations that can be employed to turn one sentence into the other.2 As a by-product of computing the edit sequence, TERp produces an alignment between the two sentences where each alignment link is defined by an edit operation. Figure 2 shows an example of the alignment produced by TERp between the original sentence from Figure 1 and its Russian roundtrip translation. Note that TERp also allows shifting words and phrases in the second sentence in order to obtain a smaller edit cost (as indicated by the asterisk"
W12-2005,C10-1149,0,0.0207563,"combining multiple surface renderings with the same meaning has also been explored in paraphrase generation. Pang et al. (2003) propose an algorithm to align sets of parallel sentences driven entirely by the syntactic representations of the sentences. The alignment algorithm outputs a merged lattice from which lexical, phrasal, and sentential paraphrases could simply be read off. Barzilay and Lee (2003) cluster topically related sentences into slotted word lattices by using multiple sequence alignment for the purpose of downstream paraphrase generation from comparable corpora. More recently, Zhao et al. (2010) perform round-trip translation of English sentences via different pivot languages and different off-the-shelf SMT systems to generate candidate paraphrases. However, they do not combine the candidate paraphrases in any way. A detailed survey of paraphrase generation techniques can be found in (Androutsopoulos and Malakasiotis, 2010) and (Madnani and Dorr, 2010). 3 Methodology The basic idea underlying our error correction technique is quite simple: if we can automatically generate alternative surface renderings of the meaning expressed in the original sentence and then pick the one that is mo"
W13-1739,N13-1055,1,0.88711,"Missing"
W13-1739,W11-2838,0,0.0291019,"age learners. Therefore, we place more importance on the precision of the system than recall. We train our model on features that take the context of a pair of words into account, as well as other discriminative features. We present a number of evaluations on both artificially generated errors and naturally occurring learner errors and show that our classifiers achieve high precision and reasonable recall. 2 Related Work The task of detecting missing hyphens is related to previous work on detecting punctuation errors. One of the classes of errors in the Helping Our Own (HOO) 2011 shared task (Dale and Kilgarriff, 2011) was punctuation. Comma errors are the most frequent kind of punctuation error made by learners. Israel et al. (2012) present a model for detecting these kinds of errors in learner texts. They train CRF models on sentences from unedited essays written by high-level college students and show that they performs well on detecting errors in learner text. As 300 Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 300–305, c Atlanta, Georgia, June 13 2013. 2013 Association for Computational Linguistics far as we are aware, the HOO 2011 system desc"
W13-1739,N12-1029,1,0.854062,"Missing"
W13-1739,W11-2843,0,0.105521,"ation. Comma errors are the most frequent kind of punctuation error made by learners. Israel et al. (2012) present a model for detecting these kinds of errors in learner texts. They train CRF models on sentences from unedited essays written by high-level college students and show that they performs well on detecting errors in learner text. As 300 Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 300–305, c Atlanta, Georgia, June 13 2013. 2013 Association for Computational Linguistics far as we are aware, the HOO 2011 system description of Rozovskaya et al. (2011) is the only work to specifically reference hyphen errors. They use rules derived from frequencies in the training corpus to determine whether a hyphen was required between two words separated by white space. The task of detecting missing hyphens is related to the task of inserting punctuation into the output of unpunctuated text (for example, the output of speech recognition, automatic generation, machine translation, etc.). Systems that are built on the output of speech recognition can obviously take features like prosody into account. In our case, we are dealing only with written text. Grav"
W13-1739,P11-1019,0,0.0559877,"uating on the Brown Corpus with hyphens removed combined news and Wikipedia revision text achieve the highest overall f-score. Figure (1a) shows the Precision Recall curves for the Wikipedia baselines and the three classifiers. The curves mirror the results in the table, showing that the classifier trained on the newswire text, and the classifier trained on the combined data perform best. The Wikipedia counts baseline performs worst. 6 Evaluating on Learner Text We carry out two evaluations of our system on learner text. We first evaluate on the missing hyphen errors contained in the CLC-FCE (Yannakoudakis et al., 2011). This corpus contains 1,244 exam scripts written by learners of English as part of the Cambridge ESOL First Certificate in English. In total, there are 173 instances of missing hyphen errors. The results are given in Table 4, and the precision recall curves are displayed in Figure (1b). The results show that the classifiers consistently achieve high precision on this data set. This is as expected, given the high threshold set. Looking at the curves, it seems that a slightly lower threshold in this case may lead to better results. The curves show that the combined classifier is performing slig"
W14-1801,P10-1126,0,0.0197364,"tem performance for individual features Table 5 reports accuracies of systems built using feature set combinations. The first feature set combination, grammar + colprep, is a set of all features obtained from essay scoring. Here we see that addition of colprep does not improve the performance over that obtained by grammar features alone. Further, when colprep is combined with 8 paraphrasing corpus. In a vast body of related work, automated methods have been explored for the generation of descriptions of images (Kulkarni et al., 2013; Kuznetsova et al., 2012; Li et al., 2011; Yao et al., 2010; Feng and Lapata, 2010a; Feng and Lapata, 2010b; Leong et al., 2010; Mitchell et al., 2012). There is also work in the opposite direction, of finding or generating pictures for a given narration. Joshi et al. (2006) found the best set of images from an image database to match the keywords in a story. Coyne and Sproat (2001) developed a natural language understanding system which converts English text into threedimensional scenes that represent the text. For a high-stakes assessment, it would be highly undesirable to have any noise in the gold-standard reference picture descriptions. Hence we chose to use manual des"
W14-1801,P10-1089,0,0.0235887,"Missing"
W14-1801,N10-1125,0,0.0206003,"tem performance for individual features Table 5 reports accuracies of systems built using feature set combinations. The first feature set combination, grammar + colprep, is a set of all features obtained from essay scoring. Here we see that addition of colprep does not improve the performance over that obtained by grammar features alone. Further, when colprep is combined with 8 paraphrasing corpus. In a vast body of related work, automated methods have been explored for the generation of descriptions of images (Kulkarni et al., 2013; Kuznetsova et al., 2012; Li et al., 2011; Yao et al., 2010; Feng and Lapata, 2010a; Feng and Lapata, 2010b; Leong et al., 2010; Mitchell et al., 2012). There is also work in the opposite direction, of finding or generating pictures for a given narration. Joshi et al. (2006) found the best set of images from an image database to match the keywords in a story. Coyne and Sproat (2001) developed a natural language understanding system which converts English text into threedimensional scenes that represent the text. For a high-stakes assessment, it would be highly undesirable to have any noise in the gold-standard reference picture descriptions. Hence we chose to use manual des"
W14-1801,P11-1020,0,0.044392,"Missing"
W14-1801,A00-2019,1,0.867033,"he reference corpus expanded using WordNet Synonyms. 4. cov-wn-hyper: the reference corpus expanded using WordNet Hypernyms. and between word triples (trigram ABC) as 5. cov-wn-hypo: the reference corpus expanded using WordNet Hyponyms. log2 p(ABC) p(A).p(B).p(C) The higher the value of the PMI, the more common is the collocation for the word pair/triple in well formed texts. On the other hand, negative values of PMI indicate that the given word pair (or triple) is less likely than chance to occur together. We hypothesized that this would be a good indicator of awkward usage, as suggested in (Chodorow and Leacock, 2000). The PMI values for adjacent words obtained over the entire response are then assigned to bins, with 8 bins for word pairs and another 8 for word triples. Each bin represents a range for PMI p taking real values R as follows: 6. cov-all: the reference corpus expanded using all of the above methods. Mean proportions of overlap ranged from 0.65 for lemmas to 0.97 for cov-all. The 6 features listed above, along with the prompt id give a total of 7 features that form our relevance feature set. We use prompt id as a feature because the extent of overlap can depend on the prompt. Some pictures are"
W14-1801,W07-1604,1,0.801436,"Missing"
W14-1801,N10-1019,0,0.0352137,"Missing"
W14-1801,W13-1914,0,0.0856731,"ntence, incorporating both key words, consistent with the picture. Presumably, a test-taker with competent knowledge of the key words will be able to use them in a well-formed grammatical sentence in the context of the picture. Picture description tasks have been employed in a number of areas of study ranging from second language acquisition to Alzheimer’s disease (Ellis, 2000; Forbes-McKay and Venneri, 2005). Pictures and picture-based story narration have also been used to study referring expressions (Lee et al., 2012) and to analyze child narratives in order to predict language impairment (Hassanali et al., 2013). Evanini et al. (2014) employ a series of pictures and elicit (oral) story narration to test English language proficiency. In our task, the picture is used as a constraining factor to limit the type and content of sentences that can be generated using the given key words. In the course of developing our system, we examined existing features that have been developed for essay scoring, such as detectors of errors in grammar, usage and mechanics, as well as collocation features, to see if they can be re-used for scoring short responses. We also developed new features for assessing the quality of"
W14-1801,D11-1010,0,0.0211716,"Missing"
W14-1801,dale-narroway-2012-framework,0,0.0245061,"Missing"
W14-1801,W13-1702,0,0.292545,"f avenues open for future exploration. The automated scoring system might be improved by extending the relevance feature to include overlap with previously collected highscoring responses. The reference corpus could also be expanded and diversified by using a large number of annotators, at least some of whom are speakers of the languages that are most prominently represented in the population of test-takers. Finally, one particular avenue we would like to explore is the use of our features to provide feedback in low stakes practice environments. Work that is closely related to ours is that of King and Dickinson (2013). They parse picture descriptions from interactive learner sentences, classify sentences into syntactic types and extract the logical subject, verb and object in order to recover simple semantic representations of the descriptions. We do not explicitly model the semantic representations of the pictures, but rather our goal in this work is to ascertain if a response is relevant to the picture and to measure other factors that reflect vocabulary proficiency. We employ human annotators and use word similarity measures to obtain alternative forms of description because the proprietary nature of ou"
W14-1801,W12-2006,0,0.0194802,"often said that the best way to see if a person knows the meaning of a word is to have that person use the word in a sentence. Despite this widespread view, most vocabulary testing continues to rely on multiple choice items (e.g. (Lawless et al., 2012; Lawrence et al., 2012)). In fact, few assessments use constructed sentence responses to measure vocabulary knowledge, in part because of the considerable time and cost required to score such responses manually. While much progress has been made in automatically scoring writing quality in essays (Attali and Burstein, 2006; Leacock et al., 2014; Dale et al., 2012), the essay scoring engines do not measure proficiency in the use of specific words, except perhaps for some frequently confused homophones (e.g., its/it’s, there/their/they’re, affect/effect). 1 Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 1–11, c Baltimore, Maryland USA, June 26, 2014. 2014 Association for Computational Linguistics veloped features that measure the overlap between the content of the responses and the textual description. Our automated scoring system is partly based on deterministic scoring criteria and partly statis"
W14-1801,P12-1038,0,0.0262353,"Missing"
W14-1801,E12-1076,0,0.0328495,"Missing"
W14-1801,W10-0721,0,0.0340062,"Missing"
W14-1801,D11-1119,1,0.905135,"Missing"
W14-1801,W12-2501,0,0.156192,"hat provides the context for the sentence construction. The task is to generate a single sentence, incorporating both key words, consistent with the picture. Presumably, a test-taker with competent knowledge of the key words will be able to use them in a well-formed grammatical sentence in the context of the picture. Picture description tasks have been employed in a number of areas of study ranging from second language acquisition to Alzheimer’s disease (Ellis, 2000; Forbes-McKay and Venneri, 2005). Pictures and picture-based story narration have also been used to study referring expressions (Lee et al., 2012) and to analyze child narratives in order to predict language impairment (Hassanali et al., 2013). Evanini et al. (2014) employ a series of pictures and elicit (oral) story narration to test English language proficiency. In our task, the picture is used as a constraining factor to limit the type and content of sentences that can be generated using the given key words. In the course of developing our system, we examined existing features that have been developed for essay scoring, such as detectors of errors in grammar, usage and mechanics, as well as collocation features, to see if they can be"
W14-1801,C10-2074,0,0.0190294,"reports accuracies of systems built using feature set combinations. The first feature set combination, grammar + colprep, is a set of all features obtained from essay scoring. Here we see that addition of colprep does not improve the performance over that obtained by grammar features alone. Further, when colprep is combined with 8 paraphrasing corpus. In a vast body of related work, automated methods have been explored for the generation of descriptions of images (Kulkarni et al., 2013; Kuznetsova et al., 2012; Li et al., 2011; Yao et al., 2010; Feng and Lapata, 2010a; Feng and Lapata, 2010b; Leong et al., 2010; Mitchell et al., 2012). There is also work in the opposite direction, of finding or generating pictures for a given narration. Joshi et al. (2006) found the best set of images from an image database to match the keywords in a story. Coyne and Sproat (2001) developed a natural language understanding system which converts English text into threedimensional scenes that represent the text. For a high-stakes assessment, it would be highly undesirable to have any noise in the gold-standard reference picture descriptions. Hence we chose to use manual description for creating our reference corpus. e"
W14-1801,W11-0326,0,0.0159275,"7.44 65.00 62.50 61.26 Table 4: System performance for individual features Table 5 reports accuracies of systems built using feature set combinations. The first feature set combination, grammar + colprep, is a set of all features obtained from essay scoring. Here we see that addition of colprep does not improve the performance over that obtained by grammar features alone. Further, when colprep is combined with 8 paraphrasing corpus. In a vast body of related work, automated methods have been explored for the generation of descriptions of images (Kulkarni et al., 2013; Kuznetsova et al., 2012; Li et al., 2011; Yao et al., 2010; Feng and Lapata, 2010a; Feng and Lapata, 2010b; Leong et al., 2010; Mitchell et al., 2012). There is also work in the opposite direction, of finding or generating pictures for a given narration. Joshi et al. (2006) found the best set of images from an image database to match the keywords in a story. Coyne and Sproat (2001) developed a natural language understanding system which converts English text into threedimensional scenes that represent the text. For a high-stakes assessment, it would be highly undesirable to have any noise in the gold-standard reference picture descr"
W14-1801,P98-2127,0,0.0911381,"esponse and the lemmatized version of the corresponding reference corpus, as follows: |Response ∩ Corpus| |Response| It is not always necessary for the test-taker to use exactly the same words found in the reference corpus. For example, the annotator might have referred to a person in the picture as a “lady”, while a response may refer to the same person as a “woman” or “girl” or even just “person”. Thus, we needed to go beyond simple lexical match. In order to account for synonyms, we expanded the content words in the reference corpus by adding their synonyms, as provided in Lin’s thesaurus (Lin, 1998) and then compared the expanded reference to each response. Along the same lines, we also used expansions from WordNet synonyms, WordNet hypernyms and WordNet hyponyms. The following is the list of our content 4.1 Features for Grammatical Error Detection R Essay scoring engines such as e-rater (Attali and Burstein, 2006) typically use a number of 1 We do not conduct inter-annotator agreement studies as the goal of the double annotation was to create a diverse description. 4 this intuition in constructing our PMI-based features. We find the PMI of all adjacent word pairs (bigrams), as well as a"
W14-1801,W10-0707,0,\N,Missing
W14-1801,C98-2122,0,\N,Missing
W14-4906,Q13-1009,1,0.856097,"Missing"
W14-4906,W08-1202,0,0.065611,"Missing"
W14-4906,E03-1003,1,0.675842,"Missing"
W14-4906,N04-1024,1,0.644061,"ors developed personal protocols reflecting their intuitions about essay coherence, thus reducing standard front-end resources. The paper presents related work (Section 2), the experimental annotation (Section 3), system evaluations (Section 4), and conclusions (Section 5). 2 Related Work Even after extensive training, subjective tasks may yield low inter-annotator agreement (Burstein & Wolska, 2003; Reidsma & op den Akker, 2008; Burstein et al., 2013). Front-end annotation activities may require significant resources (protocol development and annotator training) (Miltsakaki and Kukich, 2000; Higgins, et al., 2004; Wang et al., 2012; Burstein et al., 2013). Burstein et al (2013) reviewed coherence features as discussed in cognitive psychology (Graesser et al., 2004), reading research (Van den Broek, 2012), and computational linguistics, and concluded that evaluating text coherence is highly personal , relying on a variety of features, including adherence to standard writing conventions (e.g., grammar), and patterns of rhetorical structure and vocabulary usage. They describe an annotation protocol that uses a 3-point coherence quality scale (3 (high), 2 (somewhat,) and 1 (low)) applied by 2 annotators t"
W14-4906,W08-1203,0,0.0665934,"Missing"
W15-0605,A00-2019,1,0.821474,"rd pairs (bigrams), as well as all adjacent word triples (trigrams) in the Google 1T web corpus (Brants and Franz, 2006). The higher the value of the PMI, the more common is the collocation for the word pair/triple in well formed texts. On the other hand, negative values of PMI indicate that the given word pair or triple is less likely than chance to occur together. We hypothesized that this would be a good indicator of awkward usage, as suggested in 3 We do not calculate agreement as producing different descriptions and having variety was the goal of the task of reference corpus creation. 44 Chodorow and Leacock (2000). The PMI values for adjacent words obtained over the entire response are then assigned to bins, with 8 bins for word pairs and another 8 for word triples following the procedure from (Somasundaran and Chodorow, 2014). Each of the 8 bins represents a range of PMI : p &gt; 20, 10 &lt; p ≤ 20, 1 &lt; p ≤ 10, 0 &lt; p ≤ 1, −1 &lt; p ≤ 0, −10 &lt; p ≤ −1, −20 &lt; p ≤ −10, p ≤ −20. We generate two sets of features based on the proportions of bigrams/trigrams falling into each bin, resulting in a total of 16 features. In addition to binning, we also encode as features the maximum, minimum and median PMI value obtained"
W15-0605,W13-1914,0,0.0550708,"long history of use for detecting collocations and measuring their quality (see Manning and Sch¨utze (1999) and Leacock et al. (2014) for reviews). Our application of a large n-gram database and PMI is to encode language proficiency in sentence construction without using a parser. Picture description tasks have been employed in a number of areas of study ranging from second language acquisition to Alzheimer’s disease (Ellis, 2000; Forbes-McKay and Venneri, 2005). Picturebased story narration has also been used to study referring expressions (Lee et al., 2012) and to analyze child narratives (Hassanali et al., 2013). 3 Data The TOEFL Junior Comprehensive assessment is a computer-based test intended for middle school students around the ages of 11 - 15, and is designed to assess a student’s English communication skills. As mentioned above, we focus on the Six-Picture Narration task. Human expert raters listen to the recorded responses, which are about 60 seconds in duration, and assign a score to each on a scale of 1 - 4, with score point 4 indicating an excellent response. In this work, we use the automatic speech recognition (ASR) output transcription of the re43 Total Train Eval 877 674 —Score Distribu"
W15-0605,W13-1702,0,0.122535,"ntent relevance and story development. Somasundaran and Chodorow (2014) construct features for awkward word usage and content relevance for a written vocabulary test which we adapt for our task. Discourse organization features have been employed for essay scoring of written essays in the expository and argumentative genre (Attali and Burstein, 2006). Our discourse features are focused on the structure of spoken narratives. Our relevance measure is intended to capture topicality while providing leeway for creative story telling, which is different from scoring summaries (Loukina et al., 2014). King and Dickinson (2013) use dependency parses of written picture descriptions. Given that our data is automatically recognized speech, parse features are not likely to be reliable. We use measures of n-gram association, such as pointwise mutual information (PMI), that have a long history of use for detecting collocations and measuring their quality (see Manning and Sch¨utze (1999) and Leacock et al. (2014) for reviews). Our application of a large n-gram database and PMI is to encode language proficiency in sentence construction without using a parser. Picture description tasks have been employed in a number of areas"
W15-0605,W12-2501,0,0.0240337,"as pointwise mutual information (PMI), that have a long history of use for detecting collocations and measuring their quality (see Manning and Sch¨utze (1999) and Leacock et al. (2014) for reviews). Our application of a large n-gram database and PMI is to encode language proficiency in sentence construction without using a parser. Picture description tasks have been employed in a number of areas of study ranging from second language acquisition to Alzheimer’s disease (Ellis, 2000; Forbes-McKay and Venneri, 2005). Picturebased story narration has also been used to study referring expressions (Lee et al., 2012) and to analyze child narratives (Hassanali et al., 2013). 3 Data The TOEFL Junior Comprehensive assessment is a computer-based test intended for middle school students around the ages of 11 - 15, and is designed to assess a student’s English communication skills. As mentioned above, we focus on the Six-Picture Narration task. Human expert raters listen to the recorded responses, which are about 60 seconds in duration, and assign a score to each on a scale of 1 - 4, with score point 4 indicating an excellent response. In this work, we use the automatic speech recognition (ASR) output transcrip"
W15-0605,P98-2127,0,0.0219505,"with a reference corpus containing a detailed description of each picture, and also an overall narrative that ties together the events in the pictures. Each reference corpus was created by merging the picture descriptions and narratives that were generated independently by 10 annotators.3 To calculate overlap, stop words were first removed from lemmatized versions of the response and the reference corpus. Because test-takers often use synonyms and other words related to the prompt, we expanded the content words in the reference corpus by adding their synonyms, as provided in Lin’s thesaurus (Lin, 1998) and in WordNet, and also included their WordNet hypernyms and hyponyms. This gave us the following 6 features which measure the overlap, or coverage, between the lemmatized response and the lemmatized (i) reference corpus (lemmas), (ii) reference corpus expanded using Lin’s thesaurus (cov-lin), (iii) reference corpus expanded using WordNet Synonyms (cov-wn-syns), (iv) reference corpus expanded using WordNet Hypernyms (cov-wn-hyper), (v) reference corpus expanded using WordNet Hyponyms (cov-wn-hypo), and (vi) reference corpus expanded using all of the above methods (cov-all). 4.2 Collocation I"
W15-0605,W14-1809,0,0.0287553,"uch as language use, content relevance and story development. Somasundaran and Chodorow (2014) construct features for awkward word usage and content relevance for a written vocabulary test which we adapt for our task. Discourse organization features have been employed for essay scoring of written essays in the expository and argumentative genre (Attali and Burstein, 2006). Our discourse features are focused on the structure of spoken narratives. Our relevance measure is intended to capture topicality while providing leeway for creative story telling, which is different from scoring summaries (Loukina et al., 2014). King and Dickinson (2013) use dependency parses of written picture descriptions. Given that our data is automatically recognized speech, parse features are not likely to be reliable. We use measures of n-gram association, such as pointwise mutual information (PMI), that have a long history of use for detecting collocations and measuring their quality (see Manning and Sch¨utze (1999) and Leacock et al. (2014) for reviews). Our application of a large n-gram database and PMI is to encode language proficiency in sentence construction without using a parser. Picture description tasks have been em"
W15-0605,prasad-etal-2008-penn,0,0.0593867,"rigrams. These encode the best and the worst word collocations in a response as well as the overall general quality of the response. 4.3 Discourse Stories are characterized by events that are related (and ordered) temporally or causally. In order to form a coherent narrative, it is often necessary to use proper transition cues to organize the story. Intuitively, coherent responses are more likely to have these cues than less coherent responses. In order to detect discourse organization cues, we use two lexicons. The first was obtained from the Penn Discourse Treebank (PDTB) annotation manual (Prasad et al., 2008). The second was developed by manually mining websites giving advice on good narrative writing. The two lexicons gave us a total of over 550 cues. From the PDTB and our lexicon, we extracted the number of times each connective was encountered in a particular sense (sense information such as “Temporal” or “Cause” is directly provided in the PDTB manual, and we added similar information to our manually collected lexicon) and used the frequencies to construct a probability distribution over the senses for that cue. Then, for each response, we produced the following features: the number of cues fo"
W15-0605,W14-1801,1,0.854525,"istic and construct-relevant features are combined with the speech features. 2 Related Work Evanini et al. (2013; 2014) use features extracted mainly from speech for scoring the picture narration task. They employ measures capturing fluency, 42 Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, 2015, pages 42–48, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics prosody and pronunciation. Our work explores the other (complementary) dimensions of the test such as language use, content relevance and story development. Somasundaran and Chodorow (2014) construct features for awkward word usage and content relevance for a written vocabulary test which we adapt for our task. Discourse organization features have been employed for essay scoring of written essays in the expository and argumentative genre (Attali and Burstein, 2006). Our discourse features are focused on the structure of spoken narratives. Our relevance measure is intended to capture topicality while providing leeway for creative story telling, which is different from scoring summaries (Loukina et al., 2014). King and Dickinson (2013) use dependency parses of written picture desc"
W15-0605,H05-1044,0,0.011459,"gory in order to balance the trade-off between sparsity and informativeness. The count features are more informative, but they can be sparse (especially for higher counts). 4.5 Sentiment One common technique used in developing a story is to reveal the character’s private states, emotions and feelings. This requires the use of subjectivity and sentiment terms. We use lexicons for annotating sentiment and subjective words in the response. Specifically, we use a sentiment lexicon (ASSESS) developed in previous work in assessments (Beigman Klebanov et al., 2013) and the MPQA subjectivity lexicon (Wilson et al., 2005). ASSESS lexicon assigns a positive/negative/neutral polarity probability profile to 45 its entries, and MPQA lexicon associates a positive, negative or neutral polarity category to its entries. We consider a word from the ASSESS lexicon to be polar if the sum of positive and negative probabilities is greater than 0.65 (we arrived at this number after manual inspection of the lexicon). This gives us the subjectivity feature set comprised of the following features: A binary value indicating whether the response contains any polar words from the ASSESS lexicon (presencePolarProfile), the number"
W15-0605,zhang-etal-2004-interpreting,0,0.058119,"ption to this is the collocation feature set that performs as well as the EW13 baseline in the cross validation experiments. Notably, the combination of all five feature sets proposed in this work (All Feats), performs better than the EW13 baseline, indicating that our relevance and 46 linguistic features are important for scoring for this spoken response item type. Finally the best performance is obtained when we combine our features with the speech-based features. This improvement of All Feats + EW13 over the baseline is statistically significant at p &lt; 0.01, based on 10K bootstrap samples (Zhang et al., 2004). Somewhat surprisingly, the testing on the evaluation dataset showed slightly better performance for most types of features than the cross validation testing. We believe that this might be due to the fact that, for the Eval results, all the training data were available to train the scoring models. We also performed analysis on the Train set to see if the baseline’s performance is impacted when each of our individual feature sets is added to it. As shown in Table 3, each of the feature sets is able to improve the baseline’s performance (of 0.48 QWK). Specifically, Discourse and Subjectivity pr"
W15-0605,C98-2122,0,\N,Missing
W15-0619,N13-1055,1,0.848424,"on P1 may be incorrect. 3. Detailed Feedback 1. The incorrect preposition is highlighted and the feedback message is of the form: The highlighted preposition P1 may be incorrect; the preposition P2 may be more appropriate, where P2 is a human expert’s suggested correction for the error. 4. Detailed Feedback 2. The incorrect preposition is highlighted and the feedback message is of the form: The highlighted preposition P1 may be incorrect; the preposition P2 may be more appropriate, where P2 is the correction assigned the highest probability by an automated preposition error correction system (Cahill et al., 2013). 5. Detailed Feedback 3. The incorrect preposition is highlighted and the feedback message is of the form: The highlighted preposition P1 may be incorrect; the following is a list of prepositions that may be more appropriate, where the list contains the top 5 suggested corrections from the automated error correction system. For all three detailed feedback types, Turkers were told that the corrections were generated by an automated system. Table 1 shows the design of our experimental study wherein all recruited Turkers were divided into five mutually exclusive groups, each corresponding to one"
W15-0619,C10-2103,0,0.580405,"n the quite restricted question of the extent to which error correction influences writing accuracy for L2-English students. This study concluded that overt error correction actually has a small negative influence on learners’ abilities to write accurately. However, the meta-analysis was based on only six research studies, making it somewhat difficult to be confident about the generalizability of the findings.” Biber et al. (2011) also mention that “In actual practice, direct feedback is rarely used as a treatment in empirical research.” The work most directly relevant to our study is that of Nagata and Nakatani (2010), who attempt to measure actual impact of feedback on learning outcomes for English language learners whose native language is Japanese. At the beginning of the study, students wrote English essays on 10 different topics. Errors involving articles and noun number were then flagged either by a human or by two different automatic error detection systems: one with high precision and another with high recall. A control group received no error feedback. Learning was measured in terms of reduction of error rate for the noun phrases in the students’ essays. Results showed that learning was quite simi"
W15-0619,P11-1019,0,0.0336793,"completed Session 1, we were able to use our own qualifications and a range of qualification scores to assign Turkers to groups and control the order in which they completed the sessions. Although the Turkers were assigned randomly to groups, we manually ensured that the distributions of Session 1 scores were similar across groups. 165 amount increased by 50 cents for each new session, adding up to a total of $10 per Turker if they completed all five sessions. Table 2 shows the number of Turkers assigned to each group who participated in each of the five sessions. We used the CLC-FCE corpus (Yannakoudakis et al., 2011), which has been manually annotated for preposition errors by professional English language instructors. We randomly selected 90 sentences with preposition errors and 45 sentences without errors and manually reviewed them to ensure their suitability. Unsuitable sentences were replaced from the pool of automatically extracted sentences until we had a total of 135 suitable sentences. We annotated each sentence containing an error with a correct preposition. The 135 sentences were then randomly divided into 5 HITs (Human Intelligence Tasks, the basic unit of work on MTurk), one for each of the fi"
W98-0303,W97-0713,0,0.0150665,"nd ArgContent performed consistently across items. In fact. over the 15 essay questions, the discourse t~atures output by APA and scores output by ArgContent (based on discourse-chunked text) account for the majority of the most frequently occurring predictive features. These are shown in Table 3. We believe that the discourse related features used by e-rater might be the most useful building blocks for automated generation of diagnostic and instructional sumnaaries about essays. For example, sentences indicated as ""'the beginning of an argument"" could be used to flag main points of an essay (Marcu (1997)). ArgContent's ability to generate ""'scores"" for each argument could provide information about the relevance of individual arguments in an essay, which in turn could be used to generate helpful diagnostic or instructional information. Table 3: Most Frequently Occurring Predictive Features Across 15 Essay Questions 5. R e f e r e n c e s Featu~'e Ar,.,Content EssavContent Total Argument Development Words Auxiliary Modals Arg lnit: Complement Clauses Arg Development: Rhetorical Question Words Arg Development: Evidence Words Subordinate Clauses Relative Clauses Feature , Class ~ i'opical/ Discou"
W99-0411,J93-2001,0,0.0115483,"Missing"
W99-0411,P98-1032,1,0.852104,"Missing"
W99-0411,W98-0303,1,0.720277,"Missing"
W99-0411,C98-1032,1,\N,Missing
Y14-1063,W13-1703,0,0.130257,"(Connors and Lunsford, 1988; Lunsford and Lunsford, 2008), non-native speakers appear to be especially prone to producing them, possibly due to interference from syntactic differences in L1 (Tseng and Liou, 2006; Bennui, 2008; Rahimi, 2009). This may be especially true for L1s where comma splices are frequently found and are not considered mistakes, such as in Chinese (Lin, 2002). Comma splices are one of the errors addressed in the 2014 CoNLL Shared Task on Grammatical Error Correction (Ng et al., 2014). They are annotated in many learner corpora, including the NUS Corpus of Learner English (Dahlmeier et al., 2013) and the EFCambridge Open Language Database (Geertzen et al., 2013). This paper addresses the task of detecting comma splices. We report human agreement in detecting these errors and propose a CRF model to automatically detect them. Our best model, which uses features derived from parse trees produced by the Stanford parser (Klein and Manning, 2003), significantly outperforms both a baseline that does not consider Copyright 2014 by John Lee, Chak Yan Yeung, and Martin Chodorow 28th Pacific Asia Conference on Language, Information and Computation pages 551–560 !551 PACLIC 28 syntactic informati"
Y14-1063,W11-2838,0,0.0247348,"ices are detected. WhiteSmoke3 underlines the problematic comma and suggests that it should be replaced with either a full-stop or a semi-colon. The grammar checker embedded in Microsoft Word, perhaps the most widely used system, also gives feedback about comma splices. To the best of our knowledge, the first three do not explicitly consider parse tree patterns; we will evaluate our approach against the !552 2 3 www.grammarly.com www.whitesmoke.com PACLIC 28 fourth. In addition to these four, a number of writing assistance systems have also been built for the two Helping Our Own shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012) and two CoNLL shared tasks (Ng et al., 2013; Ng et al., 2014). Run-on sentences and comma splices were among the 28 error types introduced in the CoNLL-2014 shared task (Ng et al., 2014). Among teams that tackled individual error types, none addressd run-on sentences and comma splices. Among teams that attempted to correct all error types, many obtained good results for word- and phrase-level errors, but none achieved any recall for run-on errors and comma splices. 3 Approach We cast comma splice detection as a sequence labeling task, using a linear-chain CRF as our model."
Y14-1063,W12-2006,0,0.0177049,"ke3 underlines the problematic comma and suggests that it should be replaced with either a full-stop or a semi-colon. The grammar checker embedded in Microsoft Word, perhaps the most widely used system, also gives feedback about comma splices. To the best of our knowledge, the first three do not explicitly consider parse tree patterns; we will evaluate our approach against the !552 2 3 www.grammarly.com www.whitesmoke.com PACLIC 28 fourth. In addition to these four, a number of writing assistance systems have also been built for the two Helping Our Own shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012) and two CoNLL shared tasks (Ng et al., 2013; Ng et al., 2014). Run-on sentences and comma splices were among the 28 error types introduced in the CoNLL-2014 shared task (Ng et al., 2014). Among teams that tackled individual error types, none addressd run-on sentences and comma splices. Among teams that attempted to correct all error types, many obtained good results for word- and phrase-level errors, but none achieved any recall for run-on errors and comma splices. 3 Approach We cast comma splice detection as a sequence labeling task, using a linear-chain CRF as our model. Each comma in a sen"
Y14-1063,W08-1105,0,0.0248904,"eft words Left POS Left combo there is an NP followed by a VP in the clause to the right. Accurate extraction of parse features depends on the quality of the parse trees, but non-native errors in the sentence often cause the parser to produce unexpected tree patterns (Foster et al., 2008), hence causing noise in the parse tree features. In general, parsers perform better on shorter sentences. To reduce this kind of interference, therefore, we remove those parts of the sentence that cannot contain comma splices. Unlike the task of sentence compression for summarization (Knight and Marcu, 2000; Filippova and Strube, 2008), we do not need to preserve important words or the meaning of the original sentence. Rather, we aim to preserve the phrases in the sentence that can potentially result in comma splices and strip away the rest so that the parser has the best chance to produce the expected parse patterns. Specifically, using the parse tree of the original sentence, we remove (1) introductory phrases at the beginning of a sentence, which include transition phrases such as “for example”, as well as prepositional phrases and adverbials6 ; (2) clauses that are properly connected to the rest of the sentence by a coo"
Y14-1063,P08-2056,0,0.0779017,"Missing"
Y14-1063,W09-2112,0,0.113928,"e a re6 The list of phrases are taken from http://www.msu.edu/user/jdowell/135/transw.html 7 We used a list of 292 verbs that are the hyponyms of the words “express” and “convey” in WordNet 3.0 (Miller, 1995). Example raining, was VBG, VBD raining VBG, was VBD we, stayed PRP, VBD we PRP, stayed VBD it PRP it PRP was raining we stayed VBD VBG PRP VBD 3 3 - 1 2 yes yes no yes no yes yes Table 1: List of features. Example values for each feature are drawn from the comma of the sentence “It was raining, we stayed home”. alistic estimate of system performance on arbitrary learner text. Similar to (Foster and Andersen, 2009), we artificially introduced comma splices into the text by removing conjunctions and relative pronouns to the right of commas. To ensure that the generated sen!554 PACLIC 28 Feature Pattern Example S Pattern S+NP+VP S Pattern S+S Pattern VP+VP , S [It NP VP S was raining], NP [we] VP [stayed home.] S [The pink shirt is $20], S [black skirt is $18], S [dark pant is $15]. S , S VP It can help salesperson VP [to VP [promote up-sales and cross sales] , VP [provide better services]]. VP , VP Table 2: Parse tree patterns distinctive of comma splices, illustrated with examples. tence is a comma spli"
Y14-1063,P03-1054,0,0.0169385,", such as in Chinese (Lin, 2002). Comma splices are one of the errors addressed in the 2014 CoNLL Shared Task on Grammatical Error Correction (Ng et al., 2014). They are annotated in many learner corpora, including the NUS Corpus of Learner English (Dahlmeier et al., 2013) and the EFCambridge Open Language Database (Geertzen et al., 2013). This paper addresses the task of detecting comma splices. We report human agreement in detecting these errors and propose a CRF model to automatically detect them. Our best model, which uses features derived from parse trees produced by the Stanford parser (Klein and Manning, 2003), significantly outperforms both a baseline that does not consider Copyright 2014 by John Lee, Chak Yan Yeung, and Martin Chodorow 28th Pacific Asia Conference on Language, Information and Computation pages 551–560 !551 PACLIC 28 syntactic information and a widely used commercial grammar checker. Recently, there has been much effort in developing writing assistance systems that can automatically correct errors in text written by non-native speakers. Such systems focus mostly on word or phrase-level errors, such as the misuse of articles (Han et al., 2006), prepositions (Tetreault et al., 2010)"
Y14-1063,P08-1021,1,0.817951,"as a sequence labelling problem (Israel et al., 2012), where each space between words was considered by a CRF model to determine whether a comma should be present. Features such as POS, bi-grams, and distances to the nearest conjunctions were effective and these will form the basis of our baseline model. The comma errors addressed in Israel et al. (2012), however, are distinct from ours. Instead of adding in missing commas or deleting unnecessary ones, our focus is on the improper linking of clauses that manifests as wrongly used commas, which cannot be fixed by simply removing them. Work by Lee and Seneff (2008) on correcting the misuse of verb forms is relevant to detecting comma splice errors that involve participles. They found that verb form errors result in predictable irregularities in parse trees which can be used as cues for error detection. We follow their approach of using parse tree patterns, but will incorporate these patterns in a machine learning framework rather than a rule-based system. We are not aware of any previous work on detecting or restoring missing conjunctions, but this task is implicitly or explicitly performed by four existing systems that give feedback about comma splices"
Y14-1063,P12-2049,1,0.857764,"nts’ writing, but it would not be the case for the sentences created with this method. Therefore, we did not include comma splices introduced by fusing sentences together. Out of 13159 instances of commas, this method yielded 2775 comma splices. 4.2 Test Sets Although run-on sentences and comma splices were among the 28 error types introduced in the CoNLL2014 shared task (Ng et al., 2014), the test set used in the task only contained about 26 such errors, and is therefore too small for our purpose. We evaluated our system on two test sets8 : the learner corpus at City University of Hong Kong (Lee and Webster, 2012) (henceforth, the “CityU Set”) and the EFCambridge Open Language Database (Geertzen et al., 2013) (henceforth, the “Cambridge Set”). CityU Set. The learner corpus at City University of Hong Kong consists of academic writing by university students, most of whom are native speakers of Chinese. Three of the error categories in this corpus are concerned with comma splices — “new sentence”, “conjunction missing” and “missing relative pronoun”. We randomly selected 550 sentences that are marked with one of these three categories: 215 with “new sentences”, 215 with “conjunction missing”, and 120 with"
Y14-1063,P02-1047,0,0.0777088,"errors do involve long-distance grammatical constructions, this paper is the first report of a research effort to address the improper linking of clauses, a sentence-level error. Our ultimate goal, after detecting a comma splice, is to automatically correct it. We will, however, not treat the correction task here because it concerns a host of other issues, such as automatic analysis of style, to choose between splitting the comma splice into two sentences (“It was raining. We stayed home.”) and conjoining them (“It was raining, so we stayed home”), as well as inference of discourse relations (Marcu and Echihabi, 2002), to choose an appropriate conjunction (e.g., use of “so” rather than “because” in the above example). The rest of the paper is organized as follows. After reviewing previous research in related areas (section 2), we describe our approach for comma splice detection (section 3). We then describe our datasets, report on human agreement and experimental results (section 4), followed by our conclusions (section 5). 2 Previous work A comma splice may be the result of a misuse of punctuation (comma instead of full-stop), a misuse of verb form (finite instead of participle), or a missing conjunction."
Y14-1063,J93-2004,0,0.0456555,"Missing"
Y14-1063,W13-3601,0,0.0340027,"ts that it should be replaced with either a full-stop or a semi-colon. The grammar checker embedded in Microsoft Word, perhaps the most widely used system, also gives feedback about comma splices. To the best of our knowledge, the first three do not explicitly consider parse tree patterns; we will evaluate our approach against the !552 2 3 www.grammarly.com www.whitesmoke.com PACLIC 28 fourth. In addition to these four, a number of writing assistance systems have also been built for the two Helping Our Own shared tasks (Dale and Kilgarriff, 2011; Dale et al., 2012) and two CoNLL shared tasks (Ng et al., 2013; Ng et al., 2014). Run-on sentences and comma splices were among the 28 error types introduced in the CoNLL-2014 shared task (Ng et al., 2014). Among teams that tackled individual error types, none addressd run-on sentences and comma splices. Among teams that attempted to correct all error types, many obtained good results for word- and phrase-level errors, but none achieved any recall for run-on errors and comma splices. 3 Approach We cast comma splice detection as a sequence labeling task, using a linear-chain CRF as our model. Each comma in a sentence is to be tagged as T[rue] (it is a com"
Y14-1063,P12-2039,0,0.02193,"tly outperforms both a baseline that does not consider Copyright 2014 by John Lee, Chak Yan Yeung, and Martin Chodorow 28th Pacific Asia Conference on Language, Information and Computation pages 551–560 !551 PACLIC 28 syntactic information and a widely used commercial grammar checker. Recently, there has been much effort in developing writing assistance systems that can automatically correct errors in text written by non-native speakers. Such systems focus mostly on word or phrase-level errors, such as the misuse of articles (Han et al., 2006), prepositions (Tetreault et al., 2010) and verbs (Tajiri et al., 2012). Although these errors do involve long-distance grammatical constructions, this paper is the first report of a research effort to address the improper linking of clauses, a sentence-level error. Our ultimate goal, after detecting a comma splice, is to automatically correct it. We will, however, not treat the correction task here because it concerns a host of other issues, such as automatic analysis of style, to choose between splitting the comma splice into two sentences (“It was raining. We stayed home.”) and conjoining them (“It was raining, so we stayed home”), as well as inference of disc"
Y14-1063,P10-2065,1,0.857976,"lein and Manning, 2003), significantly outperforms both a baseline that does not consider Copyright 2014 by John Lee, Chak Yan Yeung, and Martin Chodorow 28th Pacific Asia Conference on Language, Information and Computation pages 551–560 !551 PACLIC 28 syntactic information and a widely used commercial grammar checker. Recently, there has been much effort in developing writing assistance systems that can automatically correct errors in text written by non-native speakers. Such systems focus mostly on word or phrase-level errors, such as the misuse of articles (Han et al., 2006), prepositions (Tetreault et al., 2010) and verbs (Tajiri et al., 2012). Although these errors do involve long-distance grammatical constructions, this paper is the first report of a research effort to address the improper linking of clauses, a sentence-level error. Our ultimate goal, after detecting a comma splice, is to automatically correct it. We will, however, not treat the correction task here because it concerns a host of other issues, such as automatic analysis of style, to choose between splitting the comma splice into two sentences (“It was raining. We stayed home.”) and conjoining them (“It was raining, so we stayed home"
Y14-1063,W14-1701,0,\N,Missing
Y14-1063,N12-1029,1,\N,Missing
