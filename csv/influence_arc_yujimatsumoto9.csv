2003.mtsummit-papers.47,C00-1019,0,0.0225145,"ribe the difficulties of applying EBMT to S2ST in Section 3. Then, we describe our purpose and retrieval method for meaning-equivalent sentences in Section 4 and a modification of the translation of meaning-equivalent sentences in Section 5. We report an experiment comparing our method with two other methods in Section 6. The experiment demonstrates the robustness of our method to the length of the input sentence and the style differences between the input sentences and the example corpus. 2 Related Work The rough translation proposed in this paper is a type of EBMT (Sumita, 2001; Carl, 1999; Brown, 2000). The basic idea of EBMT is that sentences similar to the input sentences are retrieved from an example corpus and their translations become the basis of outputs. Here, let us consider the difference between our method and other EBMT methods by dividing similarity into a content-word part and a functionword part. In the content-word part, our method and other EBMT methods are almost the same. Content words are important information in a similarity measure process, and thesauri are utilized to extend lexical coverage. In the function-word part, our method is characterized by disregarding functi"
2003.mtsummit-papers.47,1999.mtsummit-1.37,0,0.0277591,"ated We describe the difficulties of applying EBMT to S2ST in Section 3. Then, we describe our purpose and retrieval method for meaning-equivalent sentences in Section 4 and a modification of the translation of meaning-equivalent sentences in Section 5. We report an experiment comparing our method with two other methods in Section 6. The experiment demonstrates the robustness of our method to the length of the input sentence and the style differences between the input sentences and the example corpus. 2 Related Work The rough translation proposed in this paper is a type of EBMT (Sumita, 2001; Carl, 1999; Brown, 2000). The basic idea of EBMT is that sentences similar to the input sentences are retrieved from an example corpus and their translations become the basis of outputs. Here, let us consider the difference between our method and other EBMT methods by dividing similarity into a content-word part and a functionword part. In the content-word part, our method and other EBMT methods are almost the same. Content words are important information in a similarity measure process, and thesauri are utilized to extend lexical coverage. In the function-word part, our method is characterized by disre"
2003.mtsummit-papers.47,W02-0718,0,0.028516,"tyle differences between the input sentence and the example corpus. 1 Introduction Speech-to-speech translation (S2ST) technologies consist of speech recognition, machine translation (MT), and speech synthesis (Waibel, 1996; Wahlster, 2000; Yamamoto, 2000). The MT part receives speech texts recognized by a speech recognizer. The nature of speech causes difficulty in translation since the styles of speech are different from Yuji Matsumoto Nara Institute of Science and Technology 8916-5 Takayama, Ikoma Nara 630-0101 matsu@is.aist-nara.ac.jp those of written text and are sometimes ungrammatical (Lazzari, 2002). Therefore, rule-based MT cannot translate speech accurately compared with its performance for written-style text. Example-based MT (EBMT) is one of the corpusbased machine translation methods. It retrieves examples similar to the input sentence and modifies their translations to obtain the output (Nagao, 1981). EBMT is a promising method for S2ST in that it performs robust translation of ungrammatical sentences and requires far less manual work than rule-based MT. However, there are two problems in applying EBMT to S2ST. One is that the translation accuracy drastically drops as input sentenc"
2003.mtsummit-papers.47,W01-1401,1,0.922553,"nslated Translated We describe the difficulties of applying EBMT to S2ST in Section 3. Then, we describe our purpose and retrieval method for meaning-equivalent sentences in Section 4 and a modification of the translation of meaning-equivalent sentences in Section 5. We report an experiment comparing our method with two other methods in Section 6. The experiment demonstrates the robustness of our method to the length of the input sentence and the style differences between the input sentences and the example corpus. 2 Related Work The rough translation proposed in this paper is a type of EBMT (Sumita, 2001; Carl, 1999; Brown, 2000). The basic idea of EBMT is that sentences similar to the input sentences are retrieved from an example corpus and their translations become the basis of outputs. Here, let us consider the difference between our method and other EBMT methods by dividing similarity into a content-word part and a functionword part. In the content-word part, our method and other EBMT methods are almost the same. Content words are important information in a similarity measure process, and thesauri are utilized to extend lexical coverage. In the function-word part, our method is characteri"
2003.mtsummit-papers.47,takezawa-etal-2002-toward,1,0.906734,"nt sentences to an input sentence are defined as follows. A sentence that shares the main meaning with the input sentence despite missing some unimportant information. It does not contain information additional to that in the input sentence. Table 2: Cross Perplexity a sufficiently large volume of examples. These texts do not come from real speech but are directly written by imaging speech. They rarely contain unnecessary words. We call the style used in such a corpus “concise” and the style seen in conversational speech “conversational.” Table 1 shows the average numbers of words in concise (Takezawa et al., 2002) and conversational corpora (Takezawa, 1999). Sentences in conversational style are about 2.5 words longer than those in concise style in both English and Japanese. This is because conversational style sentences contain unnecessary words or subordinate clauses, which have the effects of assisting the listener’s comprehension and avoiding the possibility of giving the listener a curt impression. Table 2 shows cross perplexity between concise and conversational corpora (Takezawa et al., 2002). Perplexity is used as a metric for how well a language model derived from a training set matches a test"
2003.mtsummit-papers.47,A00-1006,1,0.874376,"Missing"
2004.tmi-1.12,W03-0318,1,0.764349,"duction Although machine translation (MT) technology has been undergoing development for several decades, its performance does not yet satisfy users’ needs. Modifying an input sentence into a more translatable one, known as “pre-editing,” is an important means of improving MT performance. (Bernth & Gdaniec 2001) provided a guideline for the manual pre-editing of input sentences. (Mitamura & Nyberg 2001) proposed a controlled language that is advantageous for MT. They also proposed a rewriting tool named KANTOO that supports an author in matching free input sentences to a controlled language. (Doi & Sumita 2003) proposed an automatic pre-editing method that splits long input sentences. All of the previous works of pre-editing deal with partial modiﬁcation of an input sentence. In this paper, we propose a novel pre-editing technique that incorporates similar sentence retrieval in MT to improve the translation of hard-to-translate1 input sentences. The retrieval method has the advantage of relying only on a monolingual corpus, which is easy to prepare on large scale. Figure 1 shows an overview of our proposal. An input sentence 1 A “hard-to-translate” sentence refers to a sentence whose translation qua"
2004.tmi-1.12,P02-1040,0,0.091785,"experiment among three methods and two additional conditions. 3.1 Overview of Automatic Evaluation of Machine Translation In recent years, research on automatic evaluation of MT has become increasingly active. The basic idea of automatic evaluation is that a translation to be tested obtains a higher score as it shares more common parts with several reference translations. There are three basic methods for measuring similarity between a test translation and reference translations: the common N-gram, the word error rate (WER), and the position-independent word error rate (PER). The BLEU method (Papineni et al. 2002), which is one of the major methods, uses the common N-gram method. The similarity score is based on a common N-gram ratio of a test translation to the reference translations. The value of BLEU similarity ranges from 0 to 1. The higher the BLUE score is, the more similar a test sentence is. The method uses a brevity penalty to penalize short-length sentences. The NIST method (NIST 2002), which is also widely used, is a revised version of the BLEU method. The other two methods, WER and PER, are also often used (Tillmann et al. 1997). WER is a length-normalized Levenshtein distance. This metric"
2004.tmi-1.12,W01-1401,1,0.857073,"translation quality of retrieved sentences is better than that of the original sentences. Our approach requires a translation quality measure to determine whether an input is hard-to-translate. Some MT systems can measure their translation quality by themselves. (Ueﬃng et al. 2003) proposed a method to estimate a conﬁdence measure for statistical MT based on word graphs and N-best lists. The low-conﬁdence translations correspond to hardto-translate input sentences. Example-based MT systems can estimate their translation quality from the similarity distance between input and example sentences (Sumita 2001). The parsing result of an input sentence is also useful. The proposed method for measuring the similarity between input and candidate sentences3 is based on research of the automatic evaluation of MT results. We adopt a metric based on the common N-gram after a comparative study of three methods: common Ngram, common word sequence, and common word set. (Section 3) Furthermore, we add two additional conditions to improve retrieval precision. (Section 4) We describe an experiment on applying similar sentence retrieval to a Japanese-to-English MT system in Section 5. 2 Experimental Data We focus"
2004.tmi-1.12,1999.mtsummit-1.34,1,0.879999,"measuring the similarity between input and candidate sentences3 is based on research of the automatic evaluation of MT results. We adopt a metric based on the common N-gram after a comparative study of three methods: common Ngram, common word sequence, and common word set. (Section 3) Furthermore, we add two additional conditions to improve retrieval precision. (Section 4) We describe an experiment on applying similar sentence retrieval to a Japanese-to-English MT system in Section 5. 2 Experimental Data We focused on travel conversation, which is a major target domain in speech translation (Sumita et al. 1999) (Wahlster 2000). We used two types of Japanese corpora: a corpus based on machine-aided dialog (MAD) and the basic travel expression corpus (BTEC). The MAD corpus was used as a collection of inputs and the BTEC as a candidate corpus. The MAD corpus contains 2,135 diﬀerent sentences. These sentences are a collection of transcribed utterances that were spoken in several travel situations. Therefore, the corpus reﬂects the characteristics of spoken language. The sentences are divided into two parts: one containing 437 sentences and the other 1,698 sentences. The former part was used in 2 3 The c"
2004.tmi-1.12,takezawa-etal-2002-toward,1,0.810282,"a corpus to be retrieved. an experiment to ﬁnd a method for similar sentence retrieval (Sections 3 and 4). The latter part was used in an experiment applying similar sentence retrieval to MT (Section 5). The BTEC is a collection of edited colloquial travel expressions often found in phrasebooks. It contains 116,773 diﬀerent sentences. The MAD corpus is derived from transcribed utterances, while the BTEC is not. Although both corpora contain expressions frequently used in travel conversation, they have diﬀerent characteristics in terms of average number of words in a sentence 4 and perplexity (Takezawa et al. 2002). We used the BTEC as the candidate corpus because its sentences are shorter and more grammatical than those in the MAD corpus. Consequently, these sentences are more suitable for application to MT. 3 Method for Measuring Similarity between Two Sentences Our method for measuring similarity is based on research into automatic evaluation of MT results. Similarity measurement in automatic evaluation only depends on reference sentences5 and does not require any other knowledge. In addition, many research attempts have demonstrated that automatic evaluation is strongly correlated to human evaluatio"
2004.tmi-1.12,2003.mtsummit-papers.52,0,0.0324055,"slation by Similar Sentence Retrieval can be classiﬁed as hard-to-translate or not by an MT system. If a given input sentence is hard to translate, the similar sentence retrieval function searches for the most similar sentence from a translatable sentence corpus2 and provides it to the MT system. MT performance can be improved if the translation quality of retrieved sentences is better than that of the original sentences. Our approach requires a translation quality measure to determine whether an input is hard-to-translate. Some MT systems can measure their translation quality by themselves. (Ueﬃng et al. 2003) proposed a method to estimate a conﬁdence measure for statistical MT based on word graphs and N-best lists. The low-conﬁdence translations correspond to hardto-translate input sentences. Example-based MT systems can estimate their translation quality from the similarity distance between input and example sentences (Sumita 2001). The parsing result of an input sentence is also useful. The proposed method for measuring the similarity between input and candidate sentences3 is based on research of the automatic evaluation of MT results. We adopt a metric based on the common N-gram after a compara"
2006.iwslt-evaluation.11,P05-1033,0,0.0533135,"cal machine translation method. In the following sections, we ﬁrst explain our translation model and phrase reordering model. We then report the experiments’ results using our phrase reordering model based on predicate-argument structure. 1. Introduction 2. Baseline Translation Model Recently, phrase-based statistical machine translation model has become the mainstream in the machine translation community. Phrase-based approaches are capable of constructing better context-dependent word selection model than wordbased approaches. Though the unit of translation is still under active development [1], there is no approach more widely used than phrase-based one. Statistical machine translation, however, uses less linguistic knowledge such as syntax and semantics than conventional rule-based machine translation systems. For instance, the chunk-based approach in [2] does not rely on monolingual chunker and the hierarchical phrase-based approach in [1] does not use any kind of syntactic information except for a synchronous context-free grammar. Some SMT systems, however, try to incorporate syntactic knowledge, such as [3], yet it is hard to use it effectively as described in [4]. Another issu"
2006.iwslt-evaluation.11,P03-1039,0,0.0209387,"ranslation Model Recently, phrase-based statistical machine translation model has become the mainstream in the machine translation community. Phrase-based approaches are capable of constructing better context-dependent word selection model than wordbased approaches. Though the unit of translation is still under active development [1], there is no approach more widely used than phrase-based one. Statistical machine translation, however, uses less linguistic knowledge such as syntax and semantics than conventional rule-based machine translation systems. For instance, the chunk-based approach in [2] does not rely on monolingual chunker and the hierarchical phrase-based approach in [1] does not use any kind of syntactic information except for a synchronous context-free grammar. Some SMT systems, however, try to incorporate syntactic knowledge, such as [3], yet it is hard to use it effectively as described in [4]. Another issue in statistical machine translation is reordering. Global reordering is essential to translation of languages with different word orders [5], and some aspects of global reordering in translation between German and English was stated in [6] and [7]. They used some heu"
2006.iwslt-evaluation.11,P01-1067,0,0.0677036,"aches. Though the unit of translation is still under active development [1], there is no approach more widely used than phrase-based one. Statistical machine translation, however, uses less linguistic knowledge such as syntax and semantics than conventional rule-based machine translation systems. For instance, the chunk-based approach in [2] does not rely on monolingual chunker and the hierarchical phrase-based approach in [1] does not use any kind of syntactic information except for a synchronous context-free grammar. Some SMT systems, however, try to incorporate syntactic knowledge, such as [3], yet it is hard to use it effectively as described in [4]. Another issue in statistical machine translation is reordering. Global reordering is essential to translation of languages with different word orders [5], and some aspects of global reordering in translation between German and English was stated in [6] and [7]. They used some heuristics to pre-process German corpus and reported successful results. In this paper, we present a novel phrase reordering model based on a predicate-argument structure analyzer. Given predicate-argument structure information from the analyzer, source sentence"
2006.iwslt-evaluation.11,N03-1017,0,0.0124383,"e development [1], there is no approach more widely used than phrase-based one. Statistical machine translation, however, uses less linguistic knowledge such as syntax and semantics than conventional rule-based machine translation systems. For instance, the chunk-based approach in [2] does not rely on monolingual chunker and the hierarchical phrase-based approach in [1] does not use any kind of syntactic information except for a synchronous context-free grammar. Some SMT systems, however, try to incorporate syntactic knowledge, such as [3], yet it is hard to use it effectively as described in [4]. Another issue in statistical machine translation is reordering. Global reordering is essential to translation of languages with different word orders [5], and some aspects of global reordering in translation between German and English was stated in [6] and [7]. They used some heuristics to pre-process German corpus and reported successful results. In this paper, we present a novel phrase reordering model based on a predicate-argument structure analyzer. Given predicate-argument structure information from the analyzer, source sentence is reordered according to match that of the target languag"
2006.iwslt-evaluation.11,P05-1066,0,0.122301,"the chunk-based approach in [2] does not rely on monolingual chunker and the hierarchical phrase-based approach in [1] does not use any kind of syntactic information except for a synchronous context-free grammar. Some SMT systems, however, try to incorporate syntactic knowledge, such as [3], yet it is hard to use it effectively as described in [4]. Another issue in statistical machine translation is reordering. Global reordering is essential to translation of languages with different word orders [5], and some aspects of global reordering in translation between German and English was stated in [6] and [7]. They used some heuristics to pre-process German corpus and reported successful results. In this paper, we present a novel phrase reordering model based on a predicate-argument structure analyzer. Given predicate-argument structure information from the analyzer, source sentence is reordered according to match that of the target language. The translation model trained on a reWe followed the noisy channel approach to machine translation. In this approach, we search for the target (English) sentence by maximizing the probability of the target sentence eˆ given the source (foreign) senten"
2006.iwslt-evaluation.11,2001.mtsummit-papers.45,0,0.178026,"k-based approach in [2] does not rely on monolingual chunker and the hierarchical phrase-based approach in [1] does not use any kind of syntactic information except for a synchronous context-free grammar. Some SMT systems, however, try to incorporate syntactic knowledge, such as [3], yet it is hard to use it effectively as described in [4]. Another issue in statistical machine translation is reordering. Global reordering is essential to translation of languages with different word orders [5], and some aspects of global reordering in translation between German and English was stated in [6] and [7]. They used some heuristics to pre-process German corpus and reported successful results. In this paper, we present a novel phrase reordering model based on a predicate-argument structure analyzer. Given predicate-argument structure information from the analyzer, source sentence is reordered according to match that of the target language. The translation model trained on a reWe followed the noisy channel approach to machine translation. In this approach, we search for the target (English) sentence by maximizing the probability of the target sentence eˆ given the source (foreign) sentence fˆ. B"
2006.iwslt-evaluation.11,J03-1002,0,0.0120577,"of the source phrase phrase that was translated into the ith target phrase and bi−1 denotes the end position of the source phrase translated into the (i − 1)th target phrase. 77 I P (f 1 |eI1 ) = ΠIi=1 φ(f i |ei )d(ai a predicate-argument structure analyzer [12] on our corpus, which assigns these three cases to the arguments of predicates given a sentence. Figure 1 describes Japanese predicate-argument structure analysis of the following sentence: − bi−1 ) Translation probability is obtained from the relative frequency of the source phrase given the target phrase aligned by the GIZA++ toolkit [9]. count(f , e) φ(f |e) = ∑ f count(f , e) 住所/address を/WO-ACC ここ/here に/NILOC 書い/write て/PARTICLE 下さい/please where count(f , e) gives the source phrase f aligned to the target phrase e in the parallel corpus. The distortion model can be deﬁned as follows with an appropriate value for the parameter α: In this case, “書い/write て/PARTICLE 下さい/please” is identiﬁed as a predicate, “住所/address を/WO-ACC” is assigned WO case, and “ここ/here に/NI-LOC” is assigned NI case, respectively. Our predicate-argument structure analyzer does not only use dependency information and explicit case markers, but also us"
2006.iwslt-evaluation.11,P06-1079,1,0.810285,"f 1 . Each I source phrase f i in f 1 is translated into a a target phrase ei . The target phrases may be reordered. Phrase translation is then modeled by a probability distribution φ(f i |ei ) and reordering of target phrases is modeled by a relative distortion probability distribution d(ai − bi−1 ), where ai denotes the starting position of the source phrase phrase that was translated into the ith target phrase and bi−1 denotes the end position of the source phrase translated into the (i − 1)th target phrase. 77 I P (f 1 |eI1 ) = ΠIi=1 φ(f i |ei )d(ai a predicate-argument structure analyzer [12] on our corpus, which assigns these three cases to the arguments of predicates given a sentence. Figure 1 describes Japanese predicate-argument structure analysis of the following sentence: − bi−1 ) Translation probability is obtained from the relative frequency of the source phrase given the target phrase aligned by the GIZA++ toolkit [9]. count(f , e) φ(f |e) = ∑ f count(f , e) 住所/address を/WO-ACC ここ/here に/NILOC 書い/write て/PARTICLE 下さい/please where count(f , e) gives the source phrase f aligned to the target phrase e in the parallel corpus. The distortion model can be deﬁned as follows with"
2006.iwslt-evaluation.11,P06-1132,0,0.0301701,"Missing"
2006.iwslt-evaluation.11,J05-1004,0,0.0289442,"Missing"
2006.iwslt-evaluation.11,W03-1707,0,0.0504988,"Missing"
2006.iwslt-evaluation.11,W02-2016,1,0.823108,"error rate training (MERT) tool provided by CMU [20] with 500 normal order sentences to tune 4. Experiments and Discussions 4.1. Corpus and Tools We participated in Open Data Track in Japanese-English translation because we have built only Japanese predicateargument structure analyzer and thus source language is limited to Japanese in our phrase reordering model. We used ChaSen [16] for Word segmentation and POS tagging for Japanese. We did not use the original word segmentation information of Japanese because we used another POS tagger, ChaSen, instead. Dependency parsing was done by CaboCha [17]. We used tokenizer.sed from LDC to tokenize English sentences, and MXPOST [18] for POS tagging. Word translation probabilities were calculated by GIZA++ [9]. English words were lowercased for training and testing. We used a back-off word trigram model for the language model. It is trained on the lowercased English side of the parallel corpus by Palmkit [19]. We ﬁrst manually aligned English and Japanese sentences and obtained parallel corpus of 45,909 JapaneseEnglish sentences from 39,953 conversations. We then reordered Japanese sentences by using the predicate-argument structure analyzer. W"
2006.iwslt-evaluation.11,koen-2004-pharaoh,0,\N,Missing
2006.iwslt-evaluation.11,2005.iwslt-1.16,1,\N,Missing
2007.tmi-papers.17,2005.mtsummit-papers.22,1,0.846744,"(h) indicating scopal relations, events (e), and entities (x). Figure 2 gives the MRS for the sentence “Research is fun.” The sentence is a statement, and the message, proposition m rel(e2) indicates this. tanoshii a rel(e2,x6)is an event, and takes kenkyuu s rel(x6)as its subject. noun-relation(x6) nominalizes kenkyuu s rel(x6), which is normally an event, turning it into an entity. MRS provides several features that make it attractive as a transfer language, such as uniform representation of pronouns, specifiers, temporal expressions, and the like over grammars. More details can be found in Flickinger et al. (2005). MRS 3.3 Transfer Rules As illustrated in Oepen et al. (2004), transfer rules take the form of MRS tuples: 136 v L }WD [ LTOP: h1 INDEX: e2 [ e TENSE: PRES MOOD: INDICATIVE PROG: - PERF: - ] RELS: < [ PRED proposition_m_rel LBL: h1 ARG0: e2 MARG: h3 ] [ PRED &quot;_kenkyuu_s_rel&quot; LBL: h4 ARG0: x5 ARG1: u7 ARG2: u6 ] [ PRED &quot;noun-relation&quot; LBL: h8 ARG0: x5 ARG1: h9 ] [ PRED proposition_m_rel LBL: h9 ARG0: x5 MARG: h10 ] [ PRED udef_rel LBL: h11 ARG0: x5 RSTR: h12 BODY: h13 ] [ PRED &quot;_tanoshii_a_rel&quot; LBL: h14 ARG0: e2 ARG1: x5 ] > HCONS: < h3 qeq h14, h10 qeq h4, h12 qeq h8 > ] Figure 2: MRS for v L"
2007.tmi-papers.17,2006.eamt-1.29,0,0.0476002,"Missing"
2007.tmi-papers.17,2005.mtsummit-tutorials.1,0,0.0211357,"osition that translation is a problem of meaning preservation, and that deep NLP is essential in meeting goals of high quality translation. Our ultimate aim is to have a robust, high quality and easily extensible Japanese↔English machine translation system. Current stochastic MT systems are both robust and of high quality, but only for those domains and language pairs where there is a large amount of existing parallel text. Changing the type of the text to be translated causes the quality to drop off dramatically (Paul, 2006). Quality is proportional to the log of the amount of training data (Och, 2005), which makes it hard to quickly extend a system. Rulebased systems can also produce high quality in a limited domain (Oepen et al., 2004). Further, it is relatively easy to tweak rule-based systems by the use of user dictionaries (Sukehiro et al., 2001), although these changes are limited in scope. Our approach to producing a robust, high quality system is to concentrate on translation quality and system extensibility, without worrying so much about coverage. We are able to do this because of the availability of a robust open source statistical machine translation systems (Koehn et al., 2007)"
2007.tmi-papers.17,J03-1002,0,0.00330736,"actic categories • Translations that match indicate existence of compatible rule template 4. Create a transfer rule by combining the rule template and lists of source and target MRS relations 138 RBMT (Jaen) Ranking engine Source text SMT (Moses) Target text Figure 3: The combined Jaen and Moses system 00 [ g“a; g“X] <gloss>farmland</gloss> <gloss>rice field or paddy</gloss> → <gloss>rice field</gloss> <gloss>rice paddy</gloss> Using this algorithm we can extract rules from any list of word pairs and have created rules from the EDR5 Electronic Dictionary, Wikipedia6 article links, and GIZA++ (Och and Ney, 2003) word alignments from the IWSLT 2006 training data. Our primary source of rules, however, is JMDict. The results of open category transfer rule acquisition from JMDict are summarized in Table 1. These two extensions make it possible to produce transfer rules only for those entries which are true translations. 4.2.1 4.3 Enhancing the Bilingual Dictionary The resource bottleneck is a well know problem for machine translation systems. As part of our strategy to overcome it, we are consciously avoiding the creation of specialty lexicons. Instead we are reusing and contributing to an existing dicti"
2007.tmi-papers.17,2004.tmi-1.2,0,0.0535076,"Missing"
2007.tmi-papers.17,2007.tmi-papers.18,0,0.0969807,"anese→English system (hereafter referred to as “Jaen”) is semantic transfer via rewrite rules, as shown in Figure 1. The source text is parsed using an HPSG grammar for the source language, and a semantic analysis in the form of Minimal Recursion Semantics (MRS) is produced. That semantic structure is rewritten using transfer rules into a target-language MRS structure, which is finally used to generate text from a target-language HPSG grammar. Statistical models are used at various stages in the process. There are seperate models for analyses, transfer and generation, combined as described in Oepen et al. (2007). At each stage we prune the search space, only passing n different results (5 by default) to the next stage. Although we mainly discuss Jaen in this paper, we have also built a reverse system, Enja, using the same components. 1 http://logos-os.dfki.de/ 135 3.1 System Components The grammars and processing systems we use are all being developed within the DELPH - IN 2 project (Deep Linguistic Processing with HPSG Initiative) and are available for download. The lexicon is from an unconnected project (JMdict 3 ). 3.1.1 Processing Engines Jaen uses the LKB (Copestake, 2002) for both parsing and g"
2007.tmi-papers.17,2001.mtsummit-papers.59,0,0.0346266,"slation system. Current stochastic MT systems are both robust and of high quality, but only for those domains and language pairs where there is a large amount of existing parallel text. Changing the type of the text to be translated causes the quality to drop off dramatically (Paul, 2006). Quality is proportional to the log of the amount of training data (Och, 2005), which makes it hard to quickly extend a system. Rulebased systems can also produce high quality in a limited domain (Oepen et al., 2004). Further, it is relatively easy to tweak rule-based systems by the use of user dictionaries (Sukehiro et al., 2001), although these changes are limited in scope. Our approach to producing a robust, high quality system is to concentrate on translation quality and system extensibility, without worrying so much about coverage. We are able to do this because of the availability of a robust open source statistical machine translation systems (Koehn et al., 2007). As long as we can produce a system that produces good translations for those sentences it can translate, we can fall back on the SMT system for sentences that it cannot translate. This leaves the problem of how to build a system that is both high quali"
2007.tmi-papers.17,W04-2209,0,\N,Missing
2007.tmi-papers.17,W02-1502,0,\N,Missing
2007.tmi-papers.17,P07-2045,0,\N,Missing
2007.tmi-papers.17,2005.mtsummit-osmtw.3,1,\N,Missing
2007.tmi-papers.17,2006.iwslt-evaluation.1,0,\N,Missing
2015.mtsummit-papers.1,P84-1068,0,0.64869,"Missing"
2015.mtsummit-papers.1,N12-1047,0,0.062571,"t (Koehn et al., 2007) with distortion limit of six as the baseline. We examined each of our SSSS transfer, and pre-ordering modules and their combination over the baseline. For reference, we investigated the performance of phrase-based SMT with a larger distortion limit 20, as well as hierarchical phrase-based SMT. Throughout the experiments, we used KenLM (Heafield et al., 2013) for training language models and SyMGIZA++ (Junczys-Dowmunt and Szał, 2010) for word alignment. We used the grow-diag-final method for obtaining phrase pairs. Weights of the models were tuned with n-best batch MIRA (Cherry and Foster, 2012) regarding BLEU (Papineni et al., 2002) as the objective. For each system, we performed weight tuning three times and selected for the test the setting that achieved the best BLEU on the development data. 4.3. Evaluation Metrics Each system is evaluated using two metrics: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a). Although our primary concern in this experiment is the effect of long distance relationship, in general, n-gram based metrics such as BLEU alone do not fully illustrate it. RIBES is therefore used alongside BLEU. RIBES is an automatic evaluation method based on r"
2015.mtsummit-papers.1,P05-1033,0,0.267749,"uilt a patent-adapted (not claim-adapted) parsing model by applying a self-learning procedure (Huang et al., 2009) to the above automatic parses. (ROOT (S (NP (PRP He)) (VP (VBZ likes) (NP (NNS apples))) (. .))) Figure 7. Parsing result of “He likes apples.” 4. Experiments We evaluated to what extent our SSSS transfer and pre-ordering improved the translation quality. As mentioned in Section 3, these methods are implemented as an add-on to off-the-shelf SMT systems. In particular, we used phrase-based SMT (Koehn et al., 2003) as the base system. We also regard it and its hierarchical version (Chiang, 2005) as baseline SMT systems. 4.1. Data The training data for SMT consists of two subcorpora. The first is the Japanese-English Patent Translation data comprising 3.2 million sentence pairs provided by the organizer of the Patent Machine Translation Task (PatentMT) at the NTCIR-9 Workshop (Goto et al., 2011). We randomly selected 3.0 million sentence pairs. Henceforth, we call this Corpus A. SMT systems trained on the corpus are reasonably good at lexical selection in translating claim sentences, because the vocabulary and phrases are commonly used in entire patent documents, and Corpus A is of a"
2015.mtsummit-papers.1,P05-1066,0,0.0440006,"most the entire gain in the Japanese-to-English setting. Conversely, English sentences are much more difficult to parse than Japanese. As a result, the pre-ordering module can sometimes fail to bring the English word order close to that in Japanese. Nevertheless, as a result of SSSS transfer, which divides an input English sentence into shorter pieces, pre-ordering became more accurate, and the RIBES score was further improved. 5. Related Work The quality of machine translation across distant languages has been improved as a result of the recent introduction of syntactic information into SMT (Collins et al., 2005; Quirk et al., 2005; Katz-Brown and Collins, 2008; Sudo et al., 2013; Hoshino et al., 2013; Cai et al., 2014; Goto Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 11 et al., 2015). One of the promising avenues for further improvement appears to be the incorporation of sublanguage-specific information (Buchmann et al., 1984; Luckhardt, 1991). This is particularly important for translating formalized documents that tend to form sublanguage-specific document structures and sentence structures. In dealing with structures across close language pairs, an ea"
2015.mtsummit-papers.1,N15-1105,0,0.107415,"Missing"
2015.mtsummit-papers.1,2012.amta-caas14.1,0,0.0123933,"tend to form sublanguage-specific document structures and sentence structures. In dealing with structures across close language pairs, an early study of sublanguage introduced the notion of flat trees which represents both source and target sentences using minimal depth structures for facilitating the transfer between the source and target structures (Buchmann et al., 1984). Much of the recent work relating to document and sentence structures between close languages focuses on structures centered on discourse connectives (Miltsakaki et al., 2005; Pitler and Nenkova, 2009; Meyer et al., 2011; Hajlaoui and Popescu-Belis, 2012; Meyer et al., 2012) and on resolving the ambiguity of discourse connectives connecting structural components. Conversely, when dealing with structures across distant language pairs, a more comprehensive approach is more appropriate. A wide range of research has been conducted in this direction. A study by Marcu et al. (2000) proposed a method for improving Japanese-to-English translation by transforming the source structure generated by a rhetorical structure theory (RST) parser, to the corresponding target structure. Some work in this direction has been conducted in translations across dist"
2015.mtsummit-papers.1,P13-2121,0,0.0327298,"entence pairs was randomly divided into development and test data respectively consisting of 1,000 English-Japanese claim sentence pairs. 4.2. Systems In this experiment, we regard the implementation of phrase-based SMT in the Moses toolkit (Koehn et al., 2007) with distortion limit of six as the baseline. We examined each of our SSSS transfer, and pre-ordering modules and their combination over the baseline. For reference, we investigated the performance of phrase-based SMT with a larger distortion limit 20, as well as hierarchical phrase-based SMT. Throughout the experiments, we used KenLM (Heafield et al., 2013) for training language models and SyMGIZA++ (Junczys-Dowmunt and Szał, 2010) for word alignment. We used the grow-diag-final method for obtaining phrase pairs. Weights of the models were tuned with n-best batch MIRA (Cherry and Foster, 2012) regarding BLEU (Papineni et al., 2002) as the objective. For each system, we performed weight tuning three times and selected for the test the setting that achieved the best BLEU on the development data. 4.3. Evaluation Metrics Each system is evaluated using two metrics: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a). Although our primary c"
2015.mtsummit-papers.1,I13-1147,0,0.0147867,"much more difficult to parse than Japanese. As a result, the pre-ordering module can sometimes fail to bring the English word order close to that in Japanese. Nevertheless, as a result of SSSS transfer, which divides an input English sentence into shorter pieces, pre-ordering became more accurate, and the RIBES score was further improved. 5. Related Work The quality of machine translation across distant languages has been improved as a result of the recent introduction of syntactic information into SMT (Collins et al., 2005; Quirk et al., 2005; Katz-Brown and Collins, 2008; Sudo et al., 2013; Hoshino et al., 2013; Cai et al., 2014; Goto Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 11 et al., 2015). One of the promising avenues for further improvement appears to be the incorporation of sublanguage-specific information (Buchmann et al., 1984; Luckhardt, 1991). This is particularly important for translating formalized documents that tend to form sublanguage-specific document structures and sentence structures. In dealing with structures across close language pairs, an early study of sublanguage introduced the notion of flat trees which represents both source a"
2015.mtsummit-papers.1,D10-1092,0,0.147645,"Missing"
2015.mtsummit-papers.1,P13-1048,0,0.0299936,"sive approach is more appropriate. A wide range of research has been conducted in this direction. A study by Marcu et al. (2000) proposed a method for improving Japanese-to-English translation by transforming the source structure generated by a rhetorical structure theory (RST) parser, to the corresponding target structure. Some work in this direction has been conducted in translations across distant languages, in which the source text is parsed using an RST parser, and translation rules are automatically extracted from the source and target pair (Kurohashi and Nagao, 1994; Wu and Fung, 2009; Joty et al., 2013; Tu et al., 2013). There are also approaches of simplifying long sentences by capturing the overall structure of a sentence, or a group of sentences. The skeleton-based approach (Mellebeek et al., 2006; Xiao, 2014) attempts to extract the key elements/structure (or skeleton) from the input sentence using a syntactic parser. The divide-and-translate approach (Shinhori et al., 2003; Sudo et al., 2010; Hung et al., 2012) also makes use of syntactically motivated features, such as phrases and clauses, for extracting subcomponents to be translated by SMT. There are also studies on pattern translat"
2015.mtsummit-papers.1,N03-1017,0,0.0190389,"ble. We first parsed 200,000 patent sentences using the initial parsing model. We then built a patent-adapted (not claim-adapted) parsing model by applying a self-learning procedure (Huang et al., 2009) to the above automatic parses. (ROOT (S (NP (PRP He)) (VP (VBZ likes) (NP (NNS apples))) (. .))) Figure 7. Parsing result of “He likes apples.” 4. Experiments We evaluated to what extent our SSSS transfer and pre-ordering improved the translation quality. As mentioned in Section 3, these methods are implemented as an add-on to off-the-shelf SMT systems. In particular, we used phrase-based SMT (Koehn et al., 2003) as the base system. We also regard it and its hierarchical version (Chiang, 2005) as baseline SMT systems. 4.1. Data The training data for SMT consists of two subcorpora. The first is the Japanese-English Patent Translation data comprising 3.2 million sentence pairs provided by the organizer of the Patent Machine Translation Task (PatentMT) at the NTCIR-9 Workshop (Goto et al., 2011). We randomly selected 3.0 million sentence pairs. Henceforth, we call this Corpus A. SMT systems trained on the corpus are reasonably good at lexical selection in translating claim sentences, because the vocabula"
2015.mtsummit-papers.1,C94-2183,0,0.0489313,"cross distant language pairs, a more comprehensive approach is more appropriate. A wide range of research has been conducted in this direction. A study by Marcu et al. (2000) proposed a method for improving Japanese-to-English translation by transforming the source structure generated by a rhetorical structure theory (RST) parser, to the corresponding target structure. Some work in this direction has been conducted in translations across distant languages, in which the source text is parsed using an RST parser, and translation rules are automatically extracted from the source and target pair (Kurohashi and Nagao, 1994; Wu and Fung, 2009; Joty et al., 2013; Tu et al., 2013). There are also approaches of simplifying long sentences by capturing the overall structure of a sentence, or a group of sentences. The skeleton-based approach (Mellebeek et al., 2006; Xiao, 2014) attempts to extract the key elements/structure (or skeleton) from the input sentence using a syntactic parser. The divide-and-translate approach (Shinhori et al., 2003; Sudo et al., 2010; Hung et al., 2012) also makes use of syntactically motivated features, such as phrases and clauses, for extracting subcomponents to be translated by SMT. Ther"
2015.mtsummit-papers.1,E91-1054,0,0.456166,"tactic parsing (Isozaki et al., 2010b; de Gispert et al., 2015), with growing volumes of parallel patent corpora available, have brought significant improvements in the performance of statistical machine translation (SMT) for translating patent documents across distant language pairs (Goto et al., 2012; Goto et al., 2015). However, among various sentences within a patent document, patent claim sentences still pose difficulties for SMT resulting in low translation quality, despite their utmost legal importance. A patent claim sentence is written in a kind of sublanguage (Buchmann et al., 1984; Luckhardt, 1991) in the sense that it has the following two characteristics: (i) comprising a patent claim by itself with an extreme length and (ii) having a typical sentence structure composed of a fixed set of components irrespective of language, such as those illustrated in Figures 1 and 2. The difficulties in patent claim translation lie in these two characteristics. Regarding the first characteristic, the extreme lengths cause syntactic parsers to fail with consequent low Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 1 reordering accuracy. Regarding the second"
2015.mtsummit-papers.1,2006.eamt-1.24,0,0.551983,"Missing"
2015.mtsummit-papers.1,2012.amta-papers.20,0,0.0173078,"document structures and sentence structures. In dealing with structures across close language pairs, an early study of sublanguage introduced the notion of flat trees which represents both source and target sentences using minimal depth structures for facilitating the transfer between the source and target structures (Buchmann et al., 1984). Much of the recent work relating to document and sentence structures between close languages focuses on structures centered on discourse connectives (Miltsakaki et al., 2005; Pitler and Nenkova, 2009; Meyer et al., 2011; Hajlaoui and Popescu-Belis, 2012; Meyer et al., 2012) and on resolving the ambiguity of discourse connectives connecting structural components. Conversely, when dealing with structures across distant language pairs, a more comprehensive approach is more appropriate. A wide range of research has been conducted in this direction. A study by Marcu et al. (2000) proposed a method for improving Japanese-to-English translation by transforming the source structure generated by a rhetorical structure theory (RST) parser, to the corresponding target structure. Some work in this direction has been conducted in translations across distant languages, in whi"
2015.mtsummit-papers.1,P02-1040,0,0.0931704,"(2) Claim sentences are translated according to the sentence structure, producing structurally natural translation outputs. We manually extracted a set of language independent claim components. Moreover, using these components, we constructed a set of synchronous rules for English and Japanese to transfer the SSSS in the source language to the target language. The results of an experiment demonstrate these two major effects of our SSSS transfer method. Regarding the first effect, when used in conjunction with pre-ordering, our method improves translation quality by five points in BLEU score (Papineni et al., 2002), in both English-to-Japanese and Japanese-to-English translations. Regarding the second effect, gains in RIBES score (Isozaki et al., 2010a) of over 30 points are obtained, indicating that our SSSS transfer is effective in transferring an input sentence structure to the output sentence. Components Preamble Transitional phrase Body Element Element Element Example strings An apparatus, comprising: a pencil; an eraser attached to the pencil; and a light attached to the pencil. Figure 1. Example of an English patent claim (WIPO, 2014) Components Element Body Element Element Transitional phrase Pr"
2015.mtsummit-papers.1,P09-2004,0,0.0322502,"tant for translating formalized documents that tend to form sublanguage-specific document structures and sentence structures. In dealing with structures across close language pairs, an early study of sublanguage introduced the notion of flat trees which represents both source and target sentences using minimal depth structures for facilitating the transfer between the source and target structures (Buchmann et al., 1984). Much of the recent work relating to document and sentence structures between close languages focuses on structures centered on discourse connectives (Miltsakaki et al., 2005; Pitler and Nenkova, 2009; Meyer et al., 2011; Hajlaoui and Popescu-Belis, 2012; Meyer et al., 2012) and on resolving the ambiguity of discourse connectives connecting structural components. Conversely, when dealing with structures across distant language pairs, a more comprehensive approach is more appropriate. A wide range of research has been conducted in this direction. A study by Marcu et al. (2000) proposed a method for improving Japanese-to-English translation by transforming the source structure generated by a rhetorical structure theory (RST) parser, to the corresponding target structure. Some work in this di"
2015.mtsummit-papers.1,P06-1055,0,0.0259631,"“having:”, ”備える”〉 〈“wherein:”, ”ことを特徴とする”〉 〈“wherein:”, ”する”〉 3.2. Pre-ordering Another major issue in patent claim translation is that the extreme lengths cause syntactic parsers to fail with consequent low reordering accuracy. To evaluate the effect of introducing our SSSS transfer on the translation quality, we also implemented a pre-ordering tool using state-of-the-art techniques (Isozaki et al., 2010b; Goto et al., 2012; Goto et al., 2015). Our pre-ordering method is based on syntactic parsing. First, the input sentence is parsed into a binary tree structure by using the Berkeley Parser (Petrov et al., 2006). For example, when “He likes apples.” is inputted into our English-to-Japanese translation system, it is parsed as shown in Figure 7. Second, the nodes in the parse tree are reordered using a classifier. For example, according to the classifier's decision, the two children of the “VP” node, i.e., “VBZ” and “NP”, are swapped, whereas the order of the two children of the “S” node, i.e., “NP” and “VP”, is retained. Once such a decision is made for every node with two children (henceforth, binary mode), the word order of the entire sentence becomes very similar to that in Japanese, i.e., “He (kar"
2015.mtsummit-papers.1,P05-1034,0,0.049209,"n the Japanese-to-English setting. Conversely, English sentences are much more difficult to parse than Japanese. As a result, the pre-ordering module can sometimes fail to bring the English word order close to that in Japanese. Nevertheless, as a result of SSSS transfer, which divides an input English sentence into shorter pieces, pre-ordering became more accurate, and the RIBES score was further improved. 5. Related Work The quality of machine translation across distant languages has been improved as a result of the recent introduction of syntactic information into SMT (Collins et al., 2005; Quirk et al., 2005; Katz-Brown and Collins, 2008; Sudo et al., 2013; Hoshino et al., 2013; Cai et al., 2014; Goto Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 11 et al., 2015). One of the promising avenues for further improvement appears to be the incorporation of sublanguage-specific information (Buchmann et al., 1984; Luckhardt, 1991). This is particularly important for translating formalized documents that tend to form sublanguage-specific document structures and sentence structures. In dealing with structures across close language pairs, an early study of sublang"
2015.mtsummit-papers.1,P13-2066,0,0.0184134,"re appropriate. A wide range of research has been conducted in this direction. A study by Marcu et al. (2000) proposed a method for improving Japanese-to-English translation by transforming the source structure generated by a rhetorical structure theory (RST) parser, to the corresponding target structure. Some work in this direction has been conducted in translations across distant languages, in which the source text is parsed using an RST parser, and translation rules are automatically extracted from the source and target pair (Kurohashi and Nagao, 1994; Wu and Fung, 2009; Joty et al., 2013; Tu et al., 2013). There are also approaches of simplifying long sentences by capturing the overall structure of a sentence, or a group of sentences. The skeleton-based approach (Mellebeek et al., 2006; Xiao, 2014) attempts to extract the key elements/structure (or skeleton) from the input sentence using a syntactic parser. The divide-and-translate approach (Shinhori et al., 2003; Sudo et al., 2010; Hung et al., 2012) also makes use of syntactically motivated features, such as phrases and clauses, for extracting subcomponents to be translated by SMT. There are also studies on pattern translation (Xia et al., 2"
2015.mtsummit-papers.1,2007.mtsummit-papers.63,1,0.700657,"abulary and phrases are commonly used in entire patent documents, and Corpus A is of a substantial size to cover a large portion of them. However, the claim-specific sentence structure would never be taken into account, as Corpus A does not contain any claim sentences. To bring claim-specific characteristics into the SMT training, even for the baseline systems, we also used Corpus B comprising 1.0 million parallel sentences of patent claims. These were automatically extracted from pairs of English and Japanese patent documents published between 1999 and 2012 using a sentence alignment method (Utiyama and Isahara, 2007). The concatenation of Corpora A and B was used to train baseline SMT systems, as well as those for our extensions. 2 Note that Goto et al. (2015) learned the SWAP/STRAIGHT classification problem jointly with the parsing source sentences. 3 https://www.cis.upenn.edu/~treebank/ 4 https://www2.nict.go.jp/out-promotion/techtransfer/EDR/index.html Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 7 Development and test data were constructed separately from the training data in the following manner. First, we randomly extracted English patent documents from p"
2015.mtsummit-papers.1,N09-2004,0,0.0270482,"s, a more comprehensive approach is more appropriate. A wide range of research has been conducted in this direction. A study by Marcu et al. (2000) proposed a method for improving Japanese-to-English translation by transforming the source structure generated by a rhetorical structure theory (RST) parser, to the corresponding target structure. Some work in this direction has been conducted in translations across distant languages, in which the source text is parsed using an RST parser, and translation rules are automatically extracted from the source and target pair (Kurohashi and Nagao, 1994; Wu and Fung, 2009; Joty et al., 2013; Tu et al., 2013). There are also approaches of simplifying long sentences by capturing the overall structure of a sentence, or a group of sentences. The skeleton-based approach (Mellebeek et al., 2006; Xiao, 2014) attempts to extract the key elements/structure (or skeleton) from the input sentence using a syntactic parser. The divide-and-translate approach (Shinhori et al., 2003; Sudo et al., 2010; Hung et al., 2012) also makes use of syntactically motivated features, such as phrases and clauses, for extracting subcomponents to be translated by SMT. There are also studies"
2015.mtsummit-papers.1,C04-1073,0,0.141971,"Missing"
2015.mtsummit-papers.1,P14-2092,0,0.218298,"Missing"
2015.mtsummit-papers.1,P09-2035,0,0.0194868,"rall structure of a sentence, or a group of sentences. The skeleton-based approach (Mellebeek et al., 2006; Xiao, 2014) attempts to extract the key elements/structure (or skeleton) from the input sentence using a syntactic parser. The divide-and-translate approach (Shinhori et al., 2003; Sudo et al., 2010; Hung et al., 2012) also makes use of syntactically motivated features, such as phrases and clauses, for extracting subcomponents to be translated by SMT. There are also studies on pattern translation (Xia et al., 2004; Murakami et al., 2009; Murakami et al., 2013) and sentence segmentation (Xiong et al., 2009; Jin and Liu, 2010) for dealing with long input sentences with complex structures. Our approach is similar to the above models in the sense that it incorporates structural information into SMT, but differs in that it uses sublanguage-specific sentence structures, rather than syntactically motivated structures. This results in significant improvement in translation quality for the claim sublanguage using only a handful of rules. 6. Conclusion In this paper, we described a method for transferring sublanguage-specific sentence structure for English-to-Japanese and Japanese-to-English patent clai"
2016.amta-researchers.11,D13-1176,0,0.0174281,"ing words into smaller units, using an extra dataset of a related language pair and using monolingual data) for improving the performance of NMT models on language pairs with limited data. Our experiments show that, in some cases, our proposed approach to subword-units performs better than BPE (Byte pair encoding) and that auxiliary language-pairs and monolingual data can help improve the performance of languages with limited resources. 1 Introduction In the recent years interest in Deep Neural Networks (DNN) has grown in the ﬁeld of Natural Language Processing (NLP), as new training methods (Blunsom and Kalchbrenner, 2013; Sutskever et al., 2014) have been proposed. The encoder-decoder approach for Neural Machine Translation (NMT) consists in encoding the source sentence into an intermediate vector representation and then generating (decoding) the target sentence from this representation. Cho et al. (2014) is an example of this approach. The NMT approach of jointly training alignment and translation models described by Bahdanau et al. (2014) and its successive variations have shown promising results. Its attention mechanism deals with the problem of having a ﬁxed length vector for sentences of varying length b"
2016.amta-researchers.11,C10-1027,0,0.0296071,"rtiﬁcially increase the number of samples by counterfeiting or using data of a third language. This research explores how much of an improvement using auxiliary parallel sentences from a third language to the target language (A → T) in modeling MT from of a resourcelimited language pair (S → T) brings. We also explore the effect of using an auxiliary language on the decoder side. For the case of phrase-based Statistical Machine Translation, similar ideas have been explored before with varied results. For example, for closely related pairs (Nakov and Ng, 2012) or through lexical triangulation (Crego et al., 2010; Dholakia and Sarkar, 2014). For NMT, a couple of authors have also explored this possibility. Dong et al. (2015) modeled translation to different targets from a common source with shared representation. Firat et al. (2016) also explored the case of a common target language for different source languages. Both papers claimed to get higher translation quality over individually trained models. A comparison to these papers follows in Section 2. In order to assist the learning of the target language pair, MT for the auxiliary pair is trained jointly. We argue that doing so prevents the language p"
2016.amta-researchers.11,2014.amta-researchers.24,0,0.0341,"the number of samples by counterfeiting or using data of a third language. This research explores how much of an improvement using auxiliary parallel sentences from a third language to the target language (A → T) in modeling MT from of a resourcelimited language pair (S → T) brings. We also explore the effect of using an auxiliary language on the decoder side. For the case of phrase-based Statistical Machine Translation, similar ideas have been explored before with varied results. For example, for closely related pairs (Nakov and Ng, 2012) or through lexical triangulation (Crego et al., 2010; Dholakia and Sarkar, 2014). For NMT, a couple of authors have also explored this possibility. Dong et al. (2015) modeled translation to different targets from a common source with shared representation. Firat et al. (2016) also explored the case of a common target language for different source languages. Both papers claimed to get higher translation quality over individually trained models. A comparison to these papers follows in Section 2. In order to assist the learning of the target language pair, MT for the auxiliary pair is trained jointly. We argue that doing so prevents the language pair with the small dataset f"
2016.amta-researchers.11,P15-1166,0,0.0205457,"res how much of an improvement using auxiliary parallel sentences from a third language to the target language (A → T) in modeling MT from of a resourcelimited language pair (S → T) brings. We also explore the effect of using an auxiliary language on the decoder side. For the case of phrase-based Statistical Machine Translation, similar ideas have been explored before with varied results. For example, for closely related pairs (Nakov and Ng, 2012) or through lexical triangulation (Crego et al., 2010; Dholakia and Sarkar, 2014). For NMT, a couple of authors have also explored this possibility. Dong et al. (2015) modeled translation to different targets from a common source with shared representation. Firat et al. (2016) also explored the case of a common target language for different source languages. Both papers claimed to get higher translation quality over individually trained models. A comparison to these papers follows in Section 2. In order to assist the learning of the target language pair, MT for the auxiliary pair is trained jointly. We argue that doing so prevents the language pair with the small dataset from overﬁtting and leads to more robust models. This problem could also be addressed t"
2016.amta-researchers.11,N16-1101,0,0.147434,"e (A → T) in modeling MT from of a resourcelimited language pair (S → T) brings. We also explore the effect of using an auxiliary language on the decoder side. For the case of phrase-based Statistical Machine Translation, similar ideas have been explored before with varied results. For example, for closely related pairs (Nakov and Ng, 2012) or through lexical triangulation (Crego et al., 2010; Dholakia and Sarkar, 2014). For NMT, a couple of authors have also explored this possibility. Dong et al. (2015) modeled translation to different targets from a common source with shared representation. Firat et al. (2016) also explored the case of a common target language for different source languages. Both papers claimed to get higher translation quality over individually trained models. A comparison to these papers follows in Section 2. In order to assist the learning of the target language pair, MT for the auxiliary pair is trained jointly. We argue that doing so prevents the language pair with the small dataset from overﬁtting and leads to more robust models. This problem could also be addressed through Transfer Learning, as explored in Yosinski et al. (2014) and Zoph et al. (2016), but that approach fall"
2016.amta-researchers.11,P15-1001,0,0.0327174,"nslation models described by Bahdanau et al. (2014) and its successive variations have shown promising results. Its attention mechanism deals with the problem of having a ﬁxed length vector for sentences of varying length by encoding the source sentence into a set of vectors, one vector for each of the tokens in the source sentence. NMT doesn’t need complex feature engineering, which is convenient when dealing with resource-limited languages. However, a large parallel corpus is still needed in order to get competitive performance and avoid overﬁtting. As an example, Bahdanau et al. (2014) and Jean et al. (2015) use a dataset of about 12 million parallel sentences. Two main problems arise when using a small dataset for training a MT model. One of those problems is that the vocabulary exposed by a small dataset is inherently small. Also, even if a word shows up in the data it may occur too few times for learning a reliable representation. One strategy for minimizing this problem is subdividing words into subword 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S units, like syllables. Doing so reduces the total vocabulary size and increases the hit-rate of"
2020.coling-main.271,D19-1371,0,0.0127032,"shi et al., 2019). Their method trains bidirectional long short-term memories (BiLSTMs) (Hochreiter and Schmidhuber, 1997) on annotated data, and runs the CKY algorithm to find the globally optimal coordinate structures in a sentence. 4.2 Dataset We evaluate our method on GENIA treebank beta (Tateisi et al., 2005), which is a biomedical-domain corpus that consists of abstracts taken from the MEDLINE database, and contains syntactic annotations, 1 When the similarity takes a negative value, we multiply the square by -1. We used word embeddings pre-trained in bio-domain corpora, namely SciBERT (Beltagy et al., 2019) and Biowordvec (Yijia et al., 2019). For ELMo, we used the model trained on PubMed, available at https://allennlp.org/elmo. 3 When there is a word decomposed into subwords, we create the word vector from the mean of the subword vectors. 4 The span of the second conjunct is determined by the path of the edit graph; once the path reaches the right-most column, we stop the operation and regard the last vertex as the span of the second conjunct. 5 We tune our hyper-parameters, namely, the skip score and normalization value, on the extended Penn Treebank (Ficler and Goldberg, 2016a). 6 Note that t"
2020.coling-main.271,Q17-1010,0,0.00770191,"but not the RARE-mediated signal.” A diagonal edge represents the alignment between two words at the top and right of the edge. In this example, three pairs of words (“the–the,” “retinoid-induced–RARE-mediated,” and “program–signal”) are aligned. The vertical and horizontal edges represent a skipping operation, which indicates that the words are not aligned. To calculate the similarities of the words, we use the square of the cosine similarity of the word embeddings.1 We used three different word embedding methods, namely, BERT (Devlin et al., 2019), ELMo (Peters et al., 2018), and FastText2 (Bojanowski et al., 2017) to investigate the impacts of the embedding methods. For BERT and ELMo, we input a whole sentence containing a conjunction, and use the last layer of the hidden states as the contextualized word embeddings.3 The largest difference between the approach by Shimbo and Hara (2007) and our method is that they use coordination-annotated data to train the feature weights, whereas our method does not. This difference requires some modifications in their algorithm: Because we do not have access to the gold span of the conjuncts, we need to consider all possible candidates of conjuncts within the outer"
2020.coling-main.271,N19-1423,0,0.0435968,"articularly in NER and relation extraction tasks within in scientific domains. In this paper, we propose a simple yet effective method for finding coordination with related compound nouns, such as technical terms. Compared to previous methods (Ficler and Goldberg, 2016b; Teranishi et al., 2017; Teranishi et al., 2019), our approach does not require any training on labeled data, and is applicable under the realistic conditions where annotations of coordinate structures are not readily available. Our method employs recent pre-training language models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) to measure the similarities of words, and identifies coordination boundaries based on the property in which conjuncts share syntactic and semantic similarities. This property has been exploited in traditional alignment-based methods (Kurohashi and Nagao, 1994; Shimbo and Hara, 2007; Hara et al., 2009), and our system extends and simplifies such methods by using neural embedding representations instead of the handcrafted features or heuristic rules used in their approaches. Our experiments show that, even without training, our method achieves good results in identifying nominal coordination bo"
2020.coling-main.271,P16-1079,0,0.0805813,"ges in named entity recognition (NER) tasks, and most of the current NER models (Ma and Hovy, 2016) can identify only non-elliptical conjuncts, e.g., “breast cancer cells,” or incorrectly extract the whole coordinate phrases as single complex entities. Therefore, identifying coordinated noun phrases is crucial to improving the model performance in NLP, particularly in NER and relation extraction tasks within in scientific domains. In this paper, we propose a simple yet effective method for finding coordination with related compound nouns, such as technical terms. Compared to previous methods (Ficler and Goldberg, 2016b; Teranishi et al., 2017; Teranishi et al., 2019), our approach does not require any training on labeled data, and is applicable under the realistic conditions where annotations of coordinate structures are not readily available. Our method employs recent pre-training language models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) to measure the similarities of words, and identifies coordination boundaries based on the property in which conjuncts share syntactic and semantic similarities. This property has been exploited in traditional alignment-based methods (Kurohashi and"
2020.coling-main.271,D16-1003,0,0.102405,"ges in named entity recognition (NER) tasks, and most of the current NER models (Ma and Hovy, 2016) can identify only non-elliptical conjuncts, e.g., “breast cancer cells,” or incorrectly extract the whole coordinate phrases as single complex entities. Therefore, identifying coordinated noun phrases is crucial to improving the model performance in NLP, particularly in NER and relation extraction tasks within in scientific domains. In this paper, we propose a simple yet effective method for finding coordination with related compound nouns, such as technical terms. Compared to previous methods (Ficler and Goldberg, 2016b; Teranishi et al., 2017; Teranishi et al., 2019), our approach does not require any training on labeled data, and is applicable under the realistic conditions where annotations of coordinate structures are not readily available. Our method employs recent pre-training language models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) to measure the similarities of words, and identifies coordination boundaries based on the property in which conjuncts share syntactic and semantic similarities. This property has been exploited in traditional alignment-based methods (Kurohashi and"
2020.coling-main.271,P09-1109,1,0.833289,"et al., 2019), our approach does not require any training on labeled data, and is applicable under the realistic conditions where annotations of coordinate structures are not readily available. Our method employs recent pre-training language models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) to measure the similarities of words, and identifies coordination boundaries based on the property in which conjuncts share syntactic and semantic similarities. This property has been exploited in traditional alignment-based methods (Kurohashi and Nagao, 1994; Shimbo and Hara, 2007; Hara et al., 2009), and our system extends and simplifies such methods by using neural embedding representations instead of the handcrafted features or heuristic rules used in their approaches. Our experiments show that, even without training, our method achieves good results in identifying nominal coordination boundaries in the GENIA corpus (Tateisi et al., 2005). When targeting only the coordination of noun phrases that do not contain clauses or prepositional phrases, our method is even comparable to a supervised baseline model trained on annotated data. This work is licensed under a Creative Commons Attribut"
2020.coling-main.271,J94-4001,0,0.813011,"Goldberg, 2016b; Teranishi et al., 2017; Teranishi et al., 2019), our approach does not require any training on labeled data, and is applicable under the realistic conditions where annotations of coordinate structures are not readily available. Our method employs recent pre-training language models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) to measure the similarities of words, and identifies coordination boundaries based on the property in which conjuncts share syntactic and semantic similarities. This property has been exploited in traditional alignment-based methods (Kurohashi and Nagao, 1994; Shimbo and Hara, 2007; Hara et al., 2009), and our system extends and simplifies such methods by using neural embedding representations instead of the handcrafted features or heuristic rules used in their approaches. Our experiments show that, even without training, our method achieves good results in identifying nominal coordination boundaries in the GENIA corpus (Tateisi et al., 2005). When targeting only the coordination of noun phrases that do not contain clauses or prepositional phrases, our method is even comparable to a supervised baseline model trained on annotated data. This work is"
2020.coling-main.271,P16-1101,0,0.017409,"scientific literature, coordination is a common syntactic structure and is frequently used to describe technical terminologies. These coordinate structures often involve ellipsis, a linguistic phenomenon in which certain redundant words inferable from the context are omitted. For instance, the phrase “prostate cancer and breast cancer cells” conjoins two cell names, “prostate cancer cell” and “breast cancer cell,” with the token “cell” eliminated from the first conjunct. This phenomenon raises significant challenges in named entity recognition (NER) tasks, and most of the current NER models (Ma and Hovy, 2016) can identify only non-elliptical conjuncts, e.g., “breast cancer cells,” or incorrectly extract the whole coordinate phrases as single complex entities. Therefore, identifying coordinated noun phrases is crucial to improving the model performance in NLP, particularly in NER and relation extraction tasks within in scientific domains. In this paper, we propose a simple yet effective method for finding coordination with related compound nouns, such as technical terms. Compared to previous methods (Ficler and Goldberg, 2016b; Teranishi et al., 2017; Teranishi et al., 2019), our approach does not"
2020.coling-main.271,N18-1202,0,0.118081,"the model performance in NLP, particularly in NER and relation extraction tasks within in scientific domains. In this paper, we propose a simple yet effective method for finding coordination with related compound nouns, such as technical terms. Compared to previous methods (Ficler and Goldberg, 2016b; Teranishi et al., 2017; Teranishi et al., 2019), our approach does not require any training on labeled data, and is applicable under the realistic conditions where annotations of coordinate structures are not readily available. Our method employs recent pre-training language models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) to measure the similarities of words, and identifies coordination boundaries based on the property in which conjuncts share syntactic and semantic similarities. This property has been exploited in traditional alignment-based methods (Kurohashi and Nagao, 1994; Shimbo and Hara, 2007; Hara et al., 2009), and our system extends and simplifies such methods by using neural embedding representations instead of the handcrafted features or heuristic rules used in their approaches. Our experiments show that, even without training, our method achieves good results in iden"
2020.coling-main.271,D07-1064,0,0.387012,"et al., 2017; Teranishi et al., 2019), our approach does not require any training on labeled data, and is applicable under the realistic conditions where annotations of coordinate structures are not readily available. Our method employs recent pre-training language models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) to measure the similarities of words, and identifies coordination boundaries based on the property in which conjuncts share syntactic and semantic similarities. This property has been exploited in traditional alignment-based methods (Kurohashi and Nagao, 1994; Shimbo and Hara, 2007; Hara et al., 2009), and our system extends and simplifies such methods by using neural embedding representations instead of the handcrafted features or heuristic rules used in their approaches. Our experiments show that, even without training, our method achieves good results in identifying nominal coordination boundaries in the GENIA corpus (Tateisi et al., 2005). When targeting only the coordination of noun phrases that do not contain clauses or prepositional phrases, our method is even comparable to a supervised baseline model trained on annotated data. This work is licensed under a Creat"
2020.coling-main.271,I05-2038,0,0.297837,"f words, and identifies coordination boundaries based on the property in which conjuncts share syntactic and semantic similarities. This property has been exploited in traditional alignment-based methods (Kurohashi and Nagao, 1994; Shimbo and Hara, 2007; Hara et al., 2009), and our system extends and simplifies such methods by using neural embedding representations instead of the handcrafted features or heuristic rules used in their approaches. Our experiments show that, even without training, our method achieves good results in identifying nominal coordination boundaries in the GENIA corpus (Tateisi et al., 2005). When targeting only the coordination of noun phrases that do not contain clauses or prepositional phrases, our method is even comparable to a supervised baseline model trained on annotated data. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 3043 Proceedings of the 28th International Conference on Computational Linguistics, pages 3043–3049 Barcelona, Spain (Online), December 8-13, 2020 2 Related Studies The goal of our method is to identify the coordination boundaries of noun phrases, includ"
2020.coling-main.271,I17-1027,1,0.837084,"ion (NER) tasks, and most of the current NER models (Ma and Hovy, 2016) can identify only non-elliptical conjuncts, e.g., “breast cancer cells,” or incorrectly extract the whole coordinate phrases as single complex entities. Therefore, identifying coordinated noun phrases is crucial to improving the model performance in NLP, particularly in NER and relation extraction tasks within in scientific domains. In this paper, we propose a simple yet effective method for finding coordination with related compound nouns, such as technical terms. Compared to previous methods (Ficler and Goldberg, 2016b; Teranishi et al., 2017; Teranishi et al., 2019), our approach does not require any training on labeled data, and is applicable under the realistic conditions where annotations of coordinate structures are not readily available. Our method employs recent pre-training language models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) to measure the similarities of words, and identifies coordination boundaries based on the property in which conjuncts share syntactic and semantic similarities. This property has been exploited in traditional alignment-based methods (Kurohashi and Nagao, 1994; Shimbo and H"
2020.coling-main.271,N19-1343,1,0.805816,"t of the current NER models (Ma and Hovy, 2016) can identify only non-elliptical conjuncts, e.g., “breast cancer cells,” or incorrectly extract the whole coordinate phrases as single complex entities. Therefore, identifying coordinated noun phrases is crucial to improving the model performance in NLP, particularly in NER and relation extraction tasks within in scientific domains. In this paper, we propose a simple yet effective method for finding coordination with related compound nouns, such as technical terms. Compared to previous methods (Ficler and Goldberg, 2016b; Teranishi et al., 2017; Teranishi et al., 2019), our approach does not require any training on labeled data, and is applicable under the realistic conditions where annotations of coordinate structures are not readily available. Our method employs recent pre-training language models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) to measure the similarities of words, and identifies coordination boundaries based on the property in which conjuncts share syntactic and semantic similarities. This property has been exploited in traditional alignment-based methods (Kurohashi and Nagao, 1994; Shimbo and Hara, 2007; Hara et al., 2"
2020.emnlp-demos.4,D17-1277,0,0.18255,"ponent in various recent studies. We publicize the source code, demonstration, and the pretrained embeddings for 12 languages at https://wikipedia2vec.github.io. 1 Introduction Entity embeddings, i.e., vector representations of entities in knowledge base (KB), have played a vital role in many recent models in natural language processing (NLP). These embeddings provide rich information (or knowledge) regarding entities available in KB using fixed continuous vectors. They have been shown to be beneficial not only for tasks directly related to entities (e.g., entity linking (Yamada et al., 2016; Ganea and Hofmann, 2017)) but also for general NLP tasks (e.g., text classification (Yamada and Shindo, 2019), question answering (Poerner et al., 2019)). Notably, recent studies have also shown that these embeddings can be used to enhance the performance of state-of-the-art contextualized word embeddings (i.e., BERT (Devlin et al., 2019)) on downstream tasks (Zhang et al., 2019; Peters et al., 2019; Poerner et al., 2019). 23 Proceedings of the 2020 EMNLP (Systems Demonstrations), pages 23–30 c November 16-20, 2020. 2020 Association for Computational Linguistics using dimensionality reduction algorithms. The demonstr"
2020.emnlp-demos.4,Q17-1010,0,0.714014,"ly similar words and entities close to one another in the vector space. In particular, our tool implements the word-based skip-gram model (Mikolov et al., 2013a,b) to learn word embeddings, and its extensions proposed in Yamada et al. (2016) to learn entity embeddings. Wikipedia2Vec enables users to train embeddings by simply running a single command with a Wikipedia dump file as an input. We highly optimized our implementation, which makes our implementation of the skip-gram model faster than the well-established implementaˇ uˇrek and Sojka, 2010) tion available in gensim (Reh˚ and fastText (Bojanowski et al., 2017). Experimental results demonstrated that our tool achieved enhanced quality compared to the existing tools on several standard benchmarks. Notably, our tool achieved a state-of-the-art result on the entity relatedness task based on the KORE dataset. Due to its effectiveness and efficiency, our tool has been successfully used in various downstream NLP tasks, including entity linking (Yamada et al., 2016; Eshel et al., 2017; Chen et al., 2019), named entity recognition (Sato et al., 2017; Lara-Clares and Garcia-Serrano, 2019), question answering (Yamada et al., 2018b; Poerner et al., 2019), know"
2020.emnlp-demos.4,P17-1149,0,0.0306014,"e model. Note that we used the RDF2Vec and Wiki2Vec as baselines in our experiments, and achieved enhanced empirical performance over these tools on the KORE dataset. Additionally, there have been various relational embedding models proposed (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015) that aim to learn the entity representations that are particularly effective for knowledge graph completion tasks. Related Work Many studies have recently proposed methods to learn entity embeddings from a KB (Hu et al., 2015; Li et al., 2016; Tsai and Roth, 2016; Yamada et al., 2016, 2017, 2018a; Cao et al., 2017; Ganea and Hofmann, 2017). These embeddings are typically based on conventional word embedding models (e.g., skip-gram (Mikolov et al., 2013a)) trained with data retrieved from a KB. For example, Ristoski et al. (2018) proposed RDF2Vec, which learns entity embeddings using the skip-gram model with inputs generated by random walks over the large knowledge graphs such as Wikidata and DBpedia. Furthermore, a simple method that has been widely used in various studies (Yaghoobzadeh and Schutze, 2015; Yamada et al., 2017, 2018a; AlBadrashiny et al., 2017; Suzuki et al., 2018) trains entity embeddin"
2020.emnlp-demos.4,C16-1252,0,0.0214659,"d by internal hyperlinks of Wikipedia as additional contexts to train the model. Note that we used the RDF2Vec and Wiki2Vec as baselines in our experiments, and achieved enhanced empirical performance over these tools on the KORE dataset. Additionally, there have been various relational embedding models proposed (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015) that aim to learn the entity representations that are particularly effective for knowledge graph completion tasks. Related Work Many studies have recently proposed methods to learn entity embeddings from a KB (Hu et al., 2015; Li et al., 2016; Tsai and Roth, 2016; Yamada et al., 2016, 2017, 2018a; Cao et al., 2017; Ganea and Hofmann, 2017). These embeddings are typically based on conventional word embedding models (e.g., skip-gram (Mikolov et al., 2013a)) trained with data retrieved from a KB. For example, Ristoski et al. (2018) proposed RDF2Vec, which learns entity embeddings using the skip-gram model with inputs generated by random walks over the large knowledge graphs such as Wikidata and DBpedia. Furthermore, a simple method that has been widely used in various studies (Yaghoobzadeh and Schutze, 2015; Yamada et al., 2017, 2018"
2020.emnlp-demos.4,N19-1423,0,0.0231458,"anguage processing (NLP). These embeddings provide rich information (or knowledge) regarding entities available in KB using fixed continuous vectors. They have been shown to be beneficial not only for tasks directly related to entities (e.g., entity linking (Yamada et al., 2016; Ganea and Hofmann, 2017)) but also for general NLP tasks (e.g., text classification (Yamada and Shindo, 2019), question answering (Poerner et al., 2019)). Notably, recent studies have also shown that these embeddings can be used to enhance the performance of state-of-the-art contextualized word embeddings (i.e., BERT (Devlin et al., 2019)) on downstream tasks (Zhang et al., 2019; Peters et al., 2019; Poerner et al., 2019). 23 Proceedings of the 2020 EMNLP (Systems Demonstrations), pages 23–30 c November 16-20, 2020. 2020 Association for Computational Linguistics using dimensionality reduction algorithms. The demonstration also allows users to explore the embeddings by querying similar words and entities. The source code has been tested on Linux, Windows, and macOS, and released under the Apache License 2.0. We also release the pretrained embeddings for 12 languages (i.e., English, Arabic, Chinese, Dutch, French, German, Italia"
2020.emnlp-demos.4,K17-1008,1,0.782208,", which makes our implementation of the skip-gram model faster than the well-established implementaˇ uˇrek and Sojka, 2010) tion available in gensim (Reh˚ and fastText (Bojanowski et al., 2017). Experimental results demonstrated that our tool achieved enhanced quality compared to the existing tools on several standard benchmarks. Notably, our tool achieved a state-of-the-art result on the entity relatedness task based on the KORE dataset. Due to its effectiveness and efficiency, our tool has been successfully used in various downstream NLP tasks, including entity linking (Yamada et al., 2016; Eshel et al., 2017; Chen et al., 2019), named entity recognition (Sato et al., 2017; Lara-Clares and Garcia-Serrano, 2019), question answering (Yamada et al., 2018b; Poerner et al., 2019), knowledge graph completion (Shah et al., 2019), paraphrase detection (Duong et al., 2019), fake news detection (Singh et al., 2019), and text classification (Yamada and Shindo, 2019). We also introduce a web-based demonstration of our tool that visualizes the embeddings by plotting them onto a two- or three-dimensional space The embeddings of entities in a large knowledge base (e.g., Wikipedia) are highly beneficial for solvi"
2020.emnlp-demos.4,D19-1180,0,0.0378261,"Missing"
2020.emnlp-demos.4,D15-1083,0,0.0606389,"Missing"
2020.emnlp-demos.4,D19-1005,0,0.039818,"Missing"
2020.emnlp-demos.4,K19-1052,1,0.932139,"ol achieved a state-of-the-art result on the entity relatedness task based on the KORE dataset. Due to its effectiveness and efficiency, our tool has been successfully used in various downstream NLP tasks, including entity linking (Yamada et al., 2016; Eshel et al., 2017; Chen et al., 2019), named entity recognition (Sato et al., 2017; Lara-Clares and Garcia-Serrano, 2019), question answering (Yamada et al., 2018b; Poerner et al., 2019), knowledge graph completion (Shah et al., 2019), paraphrase detection (Duong et al., 2019), fake news detection (Singh et al., 2019), and text classification (Yamada and Shindo, 2019). We also introduce a web-based demonstration of our tool that visualizes the embeddings by plotting them onto a two- or three-dimensional space The embeddings of entities in a large knowledge base (e.g., Wikipedia) are highly beneficial for solving various natural language tasks that involve real world knowledge. In this paper, we present Wikipedia2Vec, a Pythonbased open-source tool for learning the embeddings of words and entities from Wikipedia. The proposed tool enables users to learn the embeddings efficiently by issuing a single command with a Wikipedia dump file as an argument. We also"
2020.emnlp-demos.4,K16-1025,1,0.91512,"kipedia2Vec, a Python-based open source tool for learning the embeddings of words and entities easily and efficiently from Wikipedia. Due to its scale, availability in a variety of languages, and constantly evolving nature, Wikipedia is commonly used as a KB to learn entity embeddings. Our proposed tool jointly learns the embeddings of words and entities, and places semantically similar words and entities close to one another in the vector space. In particular, our tool implements the word-based skip-gram model (Mikolov et al., 2013a,b) to learn word embeddings, and its extensions proposed in Yamada et al. (2016) to learn entity embeddings. Wikipedia2Vec enables users to train embeddings by simply running a single command with a Wikipedia dump file as an input. We highly optimized our implementation, which makes our implementation of the skip-gram model faster than the well-established implementaˇ uˇrek and Sojka, 2010) tion available in gensim (Reh˚ and fastText (Bojanowski et al., 2017). Experimental results demonstrated that our tool achieved enhanced quality compared to the existing tools on several standard benchmarks. Notably, our tool achieved a state-of-the-art result on the entity relatedness"
2020.emnlp-demos.4,C18-1016,1,0.8471,"nsim (Reh˚ and fastText (Bojanowski et al., 2017). Experimental results demonstrated that our tool achieved enhanced quality compared to the existing tools on several standard benchmarks. Notably, our tool achieved a state-of-the-art result on the entity relatedness task based on the KORE dataset. Due to its effectiveness and efficiency, our tool has been successfully used in various downstream NLP tasks, including entity linking (Yamada et al., 2016; Eshel et al., 2017; Chen et al., 2019), named entity recognition (Sato et al., 2017; Lara-Clares and Garcia-Serrano, 2019), question answering (Yamada et al., 2018b; Poerner et al., 2019), knowledge graph completion (Shah et al., 2019), paraphrase detection (Duong et al., 2019), fake news detection (Singh et al., 2019), and text classification (Yamada and Shindo, 2019). We also introduce a web-based demonstration of our tool that visualizes the embeddings by plotting them onto a two- or three-dimensional space The embeddings of entities in a large knowledge base (e.g., Wikipedia) are highly beneficial for solving various natural language tasks that involve real world knowledge. In this paper, we present Wikipedia2Vec, a Pythonbased open-source tool for"
2020.emnlp-demos.4,I17-2017,1,0.909757,"n the well-established implementaˇ uˇrek and Sojka, 2010) tion available in gensim (Reh˚ and fastText (Bojanowski et al., 2017). Experimental results demonstrated that our tool achieved enhanced quality compared to the existing tools on several standard benchmarks. Notably, our tool achieved a state-of-the-art result on the entity relatedness task based on the KORE dataset. Due to its effectiveness and efficiency, our tool has been successfully used in various downstream NLP tasks, including entity linking (Yamada et al., 2016; Eshel et al., 2017; Chen et al., 2019), named entity recognition (Sato et al., 2017; Lara-Clares and Garcia-Serrano, 2019), question answering (Yamada et al., 2018b; Poerner et al., 2019), knowledge graph completion (Shah et al., 2019), paraphrase detection (Duong et al., 2019), fake news detection (Singh et al., 2019), and text classification (Yamada and Shindo, 2019). We also introduce a web-based demonstration of our tool that visualizes the embeddings by plotting them onto a two- or three-dimensional space The embeddings of entities in a large knowledge base (e.g., Wikipedia) are highly beneficial for solving various natural language tasks that involve real world knowled"
2020.emnlp-demos.4,P19-1139,0,0.085606,"provide rich information (or knowledge) regarding entities available in KB using fixed continuous vectors. They have been shown to be beneficial not only for tasks directly related to entities (e.g., entity linking (Yamada et al., 2016; Ganea and Hofmann, 2017)) but also for general NLP tasks (e.g., text classification (Yamada and Shindo, 2019), question answering (Poerner et al., 2019)). Notably, recent studies have also shown that these embeddings can be used to enhance the performance of state-of-the-art contextualized word embeddings (i.e., BERT (Devlin et al., 2019)) on downstream tasks (Zhang et al., 2019; Peters et al., 2019; Poerner et al., 2019). 23 Proceedings of the 2020 EMNLP (Systems Demonstrations), pages 23–30 c November 16-20, 2020. 2020 Association for Computational Linguistics using dimensionality reduction algorithms. The demonstration also allows users to explore the embeddings by querying similar words and entities. The source code has been tested on Linux, Windows, and macOS, and released under the Apache License 2.0. We also release the pretrained embeddings for 12 languages (i.e., English, Arabic, Chinese, Dutch, French, German, Italian, Japanese, Polish, Portuguese, Russian,"
2020.emnlp-demos.4,N16-1072,0,0.0174405,"erlinks of Wikipedia as additional contexts to train the model. Note that we used the RDF2Vec and Wiki2Vec as baselines in our experiments, and achieved enhanced empirical performance over these tools on the KORE dataset. Additionally, there have been various relational embedding models proposed (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015) that aim to learn the entity representations that are particularly effective for knowledge graph completion tasks. Related Work Many studies have recently proposed methods to learn entity embeddings from a KB (Hu et al., 2015; Li et al., 2016; Tsai and Roth, 2016; Yamada et al., 2016, 2017, 2018a; Cao et al., 2017; Ganea and Hofmann, 2017). These embeddings are typically based on conventional word embedding models (e.g., skip-gram (Mikolov et al., 2013a)) trained with data retrieved from a KB. For example, Ristoski et al. (2018) proposed RDF2Vec, which learns entity embeddings using the skip-gram model with inputs generated by random walks over the large knowledge graphs such as Wikidata and DBpedia. Furthermore, a simple method that has been widely used in various studies (Yaghoobzadeh and Schutze, 2015; Yamada et al., 2017, 2018a; AlBadrashiny et al"
2020.emnlp-demos.4,D14-1167,0,0.0372418,"ntity(""Python ( programming language)""))[:3] [(<Word python>, 0.7265), (<Entity Ruby (programming language)>, 0.6856), (<Entity Perl>, 0.6794)] Figure 2: An example that uses the Wikipedia2Vec embeddings on a Python interactive shell. neighboring entities connected by internal hyperlinks of Wikipedia as additional contexts to train the model. Note that we used the RDF2Vec and Wiki2Vec as baselines in our experiments, and achieved enhanced empirical performance over these tools on the KORE dataset. Additionally, there have been various relational embedding models proposed (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015) that aim to learn the entity representations that are particularly effective for knowledge graph completion tasks. Related Work Many studies have recently proposed methods to learn entity embeddings from a KB (Hu et al., 2015; Li et al., 2016; Tsai and Roth, 2016; Yamada et al., 2016, 2017, 2018a; Cao et al., 2017; Ganea and Hofmann, 2017). These embeddings are typically based on conventional word embedding models (e.g., skip-gram (Mikolov et al., 2013a)) trained with data retrieved from a KB. For example, Ristoski et al. (2018) proposed RDF2Vec, which learns entity embeddi"
2020.emnlp-main.523,C18-1139,0,0.258466,"E achieves a new state of the art by outperforming K-Adapter by 0.7 F1 points. 6446 Name BERT (Zhang et al., 2019) C-GCN (Zhang et al., 2018b) ERNIE (Zhang et al., 2019) SpanBERT (Joshi et al., 2020) MTB (Baldini Soares et al., 2019) KnowBERT (Peters et al., 2019) KEPLER (Wang et al., 2019b) K-Adapter (Wang et al., 2020) RoBERTa (Wang et al., 2020) LUKE Prec. 67.2 69.9 70.0 70.8 71.6 70.4 68.9 70.2 70.4 Rec. 64.8 63.3 66.1 70.9 71.4 73.0 75.4 72.4 75.1 F1 66.0 66.4 68.0 70.8 71.5 71.5 71.7 72.0 71.3 72.7 Name LSTM-CRF (Lample et al., 2016) ELMo (Peters et al., 2018) BERT (Devlin et al., 2019) Akbik et al. (2018) Baevski et al. (2019) RoBERTa LUKE Table 3: Results of named entity recognition on the CoNLL-2003 dataset. Table 2: Results of relation classification on the TACRED dataset. 4.2 Relation Classification Relation classification determines the correct relation between head and tail entities in a sentence. We conduct experiments using TACRED dataset (Zhang et al., 2017), a large-scale relation classification dataset containing 106,264 sentences with 42 relation types. Following Wang et al. (2020), we report the micro-precision, recall, and F1, and use the micro-F1 as the primary metric. Model We"
2020.emnlp-main.523,D19-6007,0,0.0272885,"Missing"
2020.emnlp-main.523,N18-1202,0,0.23558,"of this line of work, when representing entities in text, are that (1) they need to resolve entities in the text to corresponding KB entries to represent the entities, and (2) they cannot represent entities that do not exist in the KB. Contextualized Word Representations Many recent studies have addressed entity-related tasks based on the contextualized representations of entities in text computed using the word representations of CWRs (Zhang et al., 2019; Baldini Soares et al., 2019; Peters et al., 2019; Joshi et al., 2020; Wang et al., 2019b, 2020). Representative examples of CWRs are ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), which are based on deep bidirectional long short-term memory (LSTM) and the transformer (Vaswani et al., 2017), respectively. BERT is trained using an MLM, a pretraining task that masks random words in the text and trains the model to predict the masked words. Most recent CWRs, such as RoBERTa (Liu et al., 2020), XLNet (Yang et al., 2019), SpanBERT (Joshi et al., 2020), ALBERT (Lan et al., 2020), BART (Lewis et al., 2020), and T5 (Raffel et al., 2020), are based on transformer trained using a task equivalent to or similar to the MLM. Similar to our proposed pre"
2020.emnlp-main.523,D19-1005,0,0.0594225,"Missing"
2020.emnlp-main.523,D18-1309,0,0.060673,"Missing"
2020.emnlp-main.523,D16-1264,0,0.378255,"ed model by conducting extensive experiments on five standard entity-related tasks: entity typing, relation classification, NER, cloze-style QA, and extractive QA. Our model outperforms all baseline models, including RoBERTa, in all experiments, and obtains state-of-the-art results on five tasks: entity typing on the Open Entity dataset (Choi et al., 2018), relation classification on the TACRED dataset (Zhang et al., 2017), NER on the CoNLL-2003 dataset (Tjong Kim Sang and De Meulder, 2003), clozestyle QA on the ReCoRD dataset (Zhang et al., 2018a), and extractive QA on the SQuAD 1.1 dataset (Rajpurkar et al., 2016). We publicize our source code and pretrained representations at https://github.com/studio-ousia/luke. The main contributions of this paper are summarized as follows: • We propose LUKE, a new contextualized representations specifically designed to address entityrelated tasks. LUKE is trained to predict randomly masked words and entities using a large amount of entity-annotated corpus obtained from Wikipedia. • We introduce an entity-aware self-attention mechanism, an effective extension of the original mechanism of transformer. The proposed mechanism considers the type of the tokens (words or"
2020.emnlp-main.523,K16-1025,1,0.760269,"ractive question answering). Our source code and pretrained representations are available at https: //github.com/studio-ousia/luke. 1 Introduction Many natural language tasks involve entities, e.g., relation classification, entity typing, named entity recognition (NER), and question answering (QA). Key to solving such entity-related tasks is a model to learn the effective representations of entities. Conventional entity representations assign each entity a fixed embedding vector that stores information regarding the entity in a knowledge base (KB) (Bordes et al., 2013; Trouillon et al., 2016; Yamada et al., 2016, 2017). Although these models capture the rich information in the KB, they require entity linking to represent entities in a text, and cannot represent entities that do not exist in the KB. By contrast, contextualized word representations (CWRs) based on the transformer (Vaswani et al., 2017), such as BERT (Devlin et al., 2019), and RoBERTa (Liu et al., 2020), provide effective general-purpose word representations trained with unsupervised pretraining tasks based on language modeling. Many recent studies have solved entity-related tasks using the contextualized representations of entities com"
2020.emnlp-main.523,D18-1244,0,0.38931,"the token attended to. We validate the effectiveness of our proposed model by conducting extensive experiments on five standard entity-related tasks: entity typing, relation classification, NER, cloze-style QA, and extractive QA. Our model outperforms all baseline models, including RoBERTa, in all experiments, and obtains state-of-the-art results on five tasks: entity typing on the Open Entity dataset (Choi et al., 2018), relation classification on the TACRED dataset (Zhang et al., 2017), NER on the CoNLL-2003 dataset (Tjong Kim Sang and De Meulder, 2003), clozestyle QA on the ReCoRD dataset (Zhang et al., 2018a), and extractive QA on the SQuAD 1.1 dataset (Rajpurkar et al., 2016). We publicize our source code and pretrained representations at https://github.com/studio-ousia/luke. The main contributions of this paper are summarized as follows: • We propose LUKE, a new contextualized representations specifically designed to address entityrelated tasks. LUKE is trained to predict randomly masked words and entities using a large amount of entity-annotated corpus obtained from Wikipedia. • We introduce an entity-aware self-attention mechanism, an effective extension of the original mechanism of transfor"
2020.emnlp-main.523,D17-1004,0,0.289926,". To this end, we enhance the self-attention mechanism by adopting different query mechanisms based on the attending token and the token attended to. We validate the effectiveness of our proposed model by conducting extensive experiments on five standard entity-related tasks: entity typing, relation classification, NER, cloze-style QA, and extractive QA. Our model outperforms all baseline models, including RoBERTa, in all experiments, and obtains state-of-the-art results on five tasks: entity typing on the Open Entity dataset (Choi et al., 2018), relation classification on the TACRED dataset (Zhang et al., 2017), NER on the CoNLL-2003 dataset (Tjong Kim Sang and De Meulder, 2003), clozestyle QA on the ReCoRD dataset (Zhang et al., 2018a), and extractive QA on the SQuAD 1.1 dataset (Rajpurkar et al., 2016). We publicize our source code and pretrained representations at https://github.com/studio-ousia/luke. The main contributions of this paper are summarized as follows: • We propose LUKE, a new contextualized representations specifically designed to address entityrelated tasks. LUKE is trained to predict randomly masked words and entities using a large amount of entity-annotated corpus obtained from Wi"
2020.emnlp-main.523,P19-1139,0,0.450505,"se models capture the rich information in the KB, they require entity linking to represent entities in a text, and cannot represent entities that do not exist in the KB. By contrast, contextualized word representations (CWRs) based on the transformer (Vaswani et al., 2017), such as BERT (Devlin et al., 2019), and RoBERTa (Liu et al., 2020), provide effective general-purpose word representations trained with unsupervised pretraining tasks based on language modeling. Many recent studies have solved entity-related tasks using the contextualized representations of entities computed based on CWRs (Zhang et al., 2019; Peters et al., 2019; Joshi et al., 2020). However, the architecture of CWRs is not well suited to representing entities for the following two reasons: (1) Because CWRs do not output the span-level representations of entities, they typically need to learn how to compute such representations based on a downstream dataset that is typically small. (2) Many entity-related tasks, e.g., relation classification and QA, involve reasoning about the relationships between entities. Although the transformer can capture the complex relationships between words by relating them to each other multiple times"
2021.acl-long.275,C18-1139,0,0.0137256,"g learning rate ητ = η0 /(1 + γ · τ ), where τ is the index of the current epoch. For ACE2004, ACE2005, and GENIA, the initial learning rates η0 are 0.2, 0.2, and 0.1, and the decay rates γ are 0.01, 0.02, and 0.02 respectively. We set the weight decay rate, the momentum, the batch size, and the number of epochs to be 10−8 , 0.5, 32, and 100 respectively, especially we use batch size 64 on the GENIA dataset. We clip the gradient exceeding 5. Besides, we also conduct experiments to evaluate the performance of our model with contextual word representations. BERT (Devlin et al., 2019) and Flair (Akbik et al., 2018) are the most commonly used contextual word representations in previous work, and have also been proved that they can substantially improve the model performance. In these settings, contextual word representations are concatenated with word and character representations to form the token representations, i.e., xt = [wt , ct , et ], where et is the contextual word representation and it is not fine-tuned in any of our experiments. 1 https://github.com/cambridgeltl/ BioNLP-2016 3551 Methods P ACE2004 R F1 Ju et al. (2018) Wang et al. (2018) Wang and Lu (2018) Luo and Zhao (2020) Lin et al. (2019)"
2021.acl-long.275,N18-1079,0,0.0165581,"ll possible spans and utilize a maximum entropy tagger (Byrne, 2007) and neural networks (Xu et al., 2017; Sohrab and Miwa, 2018; Zheng et al., 2019) for classification. Luan et al. (2019) additionally aims to consider the relationship among entities and proposed a novel method to jointly learn both entities and relations. Hypergraph-based Model Lu and Roth (2015) proposed a hyper-graph structure, in which edges are connected to multiple nodes to represents nested entities. Muis and Lu (2017) and Wang and Lu (2018) resolved spurious structures and ambiguous issue of hyper-graph structure. And Katiyar and Cardie (2018) proposed another kind of hyper-graph structure. Parsing-based Model Finkel and Manning (2009) indicated all these nested entities are located in some non-terminal nodes of the constituency parses of the original sentences, thus they proposed to use a CRF-based constituency parser to obtain them. However, the cubic time complexity limits its applicability. Wang et al. (2018) instead proposed to use a transition-based constituency parser to incrementally build constituency forest, its linear time complexity ensures it can handle longer sentences. 5 Conclusion In this paper, we proposed a simple"
2021.acl-long.275,W16-2922,0,0.0122812,"can be found in Table 1. Dataset Sentences Mentions |Y| m ACE2004 ACE2005 GENIA 6,198 / 742 / 809 7,285 / 968 / 1,058 15,022 / 1,669 / 1,855 22,195 / 2,514 / 3,034 24,700 / 3,218 / 3,029 47,006 / 4,461 / 5,596 29 29 21 6 6 4 Table 1: Sizes of the dataset shown in the train/dev/test split. |Y |is the size of the label set, m is the maximal depth of entity nesting. Hyper-parameters Settings For word embeddings initialization, we utilize 100dimensional pre-trained GloVe (Pennington et al., 2014) for the ACE2004 and the ACE2005 datasets, and use 200-dimensional biomedical domain word embeddings1 (Chiu et al., 2016) for the GENIA dataset. Moreover, we randomly initialize 30dimensional vectors for character embeddings. The hidden state dimension of character-level LSTM dc is 100, i.e., 50 in each direction, thus the dimension of token representation dx is 200. We apply dropout (Srivastava et al., 2014) on token representations before feeding it into the encoder. The hidden state dimension of the three-layered LSTM is 600 for ACE2004 and ACE2005, i.e., 300 in each direction, and 400 for GENIA. Choosing a different dimension is because the maximal depth of entity nesting m is different. We apply layer norma"
2021.acl-long.275,N19-1423,0,0.00801778,"nt descent (SGD), with a decaying learning rate ητ = η0 /(1 + γ · τ ), where τ is the index of the current epoch. For ACE2004, ACE2005, and GENIA, the initial learning rates η0 are 0.2, 0.2, and 0.1, and the decay rates γ are 0.01, 0.02, and 0.02 respectively. We set the weight decay rate, the momentum, the batch size, and the number of epochs to be 10−8 , 0.5, 32, and 100 respectively, especially we use batch size 64 on the GENIA dataset. We clip the gradient exceeding 5. Besides, we also conduct experiments to evaluate the performance of our model with contextual word representations. BERT (Devlin et al., 2019) and Flair (Akbik et al., 2018) are the most commonly used contextual word representations in previous work, and have also been proved that they can substantially improve the model performance. In these settings, contextual word representations are concatenated with word and character representations to form the token representations, i.e., xt = [wt , ct , et ], where et is the contextual word representation and it is not fine-tuned in any of our experiments. 1 https://github.com/cambridgeltl/ BioNLP-2016 3551 Methods P ACE2004 R F1 Ju et al. (2018) Wang et al. (2018) Wang and Lu (2018) Luo an"
2021.acl-long.275,N16-1030,0,0.0169756,"at each level. In addition, we demonstrate that recognizing innermost entities first results in better performance than the conventional outermost entities first scheme. We provide extensive experimental results on ACE2004, ACE2005, and GENIA datasets to show the effectiveness and efficiency of our proposed method. 1 PER ROLE ROLE PER Figure 1: An example of nested NER. Introduction Named entity recognition (NER), as a key technique in natural language processing, aims at detecting entities and assigning semantic category labels to them. Early research (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016) proposed to employ deep learning methods and obtained significant performance improvements. However, most of them assume that the entities are not nested within other entities, so-called flat NER. Inherently, these methods do not work satisfactorily when nested entities exist. Figure 1 displays an example of the nested NER task. Recently, a large number of papers proposed novel methods (Fisher and Vlachos, 2019; Wang et al., 2020) for the nested NER task. Among them, layered methods solve this task through multi-level sequential labeling, in which entities are divided into several levels, whe"
2021.acl-long.275,doddington-etal-2004-automatic,0,0.037815,"ion function. We optimize our model by minimizing the sum of the negative log-likelihoods of all levels. L=− m X log p (y l |Hl ) (10) l=1 On the decoding stage, we iteratively apply the Viterbi algorithm (Forney, 1973) at each level to search the most probable label sequences. ˆ l = arg max p (y 0 |Hl ) y (11) y 0 ∈Y n The pseudocodes of the training and the decoding algorithms with max or logsumexp potential function can be found in Algorithms 1 and 2, respectively. 3 Experiments 3.1 Datasets We conduct experiments on three nested named entity recognition datasets in English, i.e., ACE2004 (Doddington et al., 2004), ACE2005 (Walker et al., 2006) and GENIA (Kim et al., 2003). We divide all these datasets into tran/dev/test split by following Shibuya and Hovy (2020) and Wang et al. (2020). The dataset statistics can be found in Table 1. Dataset Sentences Mentions |Y| m ACE2004 ACE2005 GENIA 6,198 / 742 / 809 7,285 / 968 / 1,058 15,022 / 1,669 / 1,855 22,195 / 2,514 / 3,034 24,700 / 3,218 / 3,029 47,006 / 4,461 / 5,596 29 29 21 6 6 4 Table 1: Sizes of the dataset shown in the train/dev/test split. |Y |is the size of the label set, m is the maximal depth of entity nesting. Hyper-parameters Settings For word"
2021.acl-long.275,D09-1015,0,0.145613,"Missing"
2021.acl-long.275,P19-1585,0,0.0578994,"(NER), as a key technique in natural language processing, aims at detecting entities and assigning semantic category labels to them. Early research (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016) proposed to employ deep learning methods and obtained significant performance improvements. However, most of them assume that the entities are not nested within other entities, so-called flat NER. Inherently, these methods do not work satisfactorily when nested entities exist. Figure 1 displays an example of the nested NER task. Recently, a large number of papers proposed novel methods (Fisher and Vlachos, 2019; Wang et al., 2020) for the nested NER task. Among them, layered methods solve this task through multi-level sequential labeling, in which entities are divided into several levels, where the term level indicates the depth of entity nesting, and sequential labeling is performed repeatedly. As a special case of layered method, Shibuya and Hovy (2020) force the ∗ ROLE This work was done when the first author was at NAIST. next level entities to locate on the second-best path of the current level search space. Hence, their algorithm can repeatedly detect inner entities through applying a conventi"
2021.acl-long.275,N18-1131,0,0.16333,"h contextual word representations. BERT (Devlin et al., 2019) and Flair (Akbik et al., 2018) are the most commonly used contextual word representations in previous work, and have also been proved that they can substantially improve the model performance. In these settings, contextual word representations are concatenated with word and character representations to form the token representations, i.e., xt = [wt , ct , et ], where et is the contextual word representation and it is not fine-tuned in any of our experiments. 1 https://github.com/cambridgeltl/ BioNLP-2016 3551 Methods P ACE2004 R F1 Ju et al. (2018) Wang et al. (2018) Wang and Lu (2018) Luo and Zhao (2020) Lin et al. (2019) Strakov´a et al. (2019) Shibuya and Hovy (2020) Wang et al. (2020) Our Method (naive) Our Method (max) Our Method (logsumexp) 74.9 78.0 71.8 72.4 73.3 75.1 78.92 79.93 80.83 81.12 81.90 81.24 75.33 75.10 78.86 77.71 78.05 78.96 Strakov´a et al. (2019) [B] Shibuya and Hovy (2020) [B] Wang et al. (2020) [B] Our Method (naive)[B] Our Method (max)[B] Our Method (logsumexp)[B] 84.71 85.23 86.08 86.19 86.27 86.42 Strakov´a et al. (2019) [B+F] Shibuya and Hovy (2020) [B+F] Wang et al. (2020) [B+F] Our Method (naive)[B+F] Our"
2021.acl-long.275,P19-1511,0,0.0237053,"Missing"
2021.acl-long.275,D15-1102,0,0.024461,"ese l-gram spans. Region-based Model Lin et al. (2019) proposed an anchor-region network to recognize nested entities through detecting anchor words and entity boundaries first, and then classify each detected span. Exhaustive models simply enumerate all possible spans and utilize a maximum entropy tagger (Byrne, 2007) and neural networks (Xu et al., 2017; Sohrab and Miwa, 2018; Zheng et al., 2019) for classification. Luan et al. (2019) additionally aims to consider the relationship among entities and proposed a novel method to jointly learn both entities and relations. Hypergraph-based Model Lu and Roth (2015) proposed a hyper-graph structure, in which edges are connected to multiple nodes to represents nested entities. Muis and Lu (2017) and Wang and Lu (2018) resolved spurious structures and ambiguous issue of hyper-graph structure. And Katiyar and Cardie (2018) proposed another kind of hyper-graph structure. Parsing-based Model Finkel and Manning (2009) indicated all these nested entities are located in some non-terminal nodes of the constituency parses of the original sentences, thus they proposed to use a CRF-based constituency parser to obtain them. However, the cubic time complexity limits i"
2021.acl-long.275,N19-1308,0,0.0155588,"m. Wang et al. (2020) proposed to learn the l-gram representations at layer l through applying a decoder component to reduce a sentence layer by layer and to directly classify these l-gram spans. Region-based Model Lin et al. (2019) proposed an anchor-region network to recognize nested entities through detecting anchor words and entity boundaries first, and then classify each detected span. Exhaustive models simply enumerate all possible spans and utilize a maximum entropy tagger (Byrne, 2007) and neural networks (Xu et al., 2017; Sohrab and Miwa, 2018; Zheng et al., 2019) for classification. Luan et al. (2019) additionally aims to consider the relationship among entities and proposed a novel method to jointly learn both entities and relations. Hypergraph-based Model Lu and Roth (2015) proposed a hyper-graph structure, in which edges are connected to multiple nodes to represents nested entities. Muis and Lu (2017) and Wang and Lu (2018) resolved spurious structures and ambiguous issue of hyper-graph structure. And Katiyar and Cardie (2018) proposed another kind of hyper-graph structure. Parsing-based Model Finkel and Manning (2009) indicated all these nested entities are located in some non-terminal"
2021.acl-long.275,2020.acl-main.571,0,0.0565698,"2019) and Flair (Akbik et al., 2018) are the most commonly used contextual word representations in previous work, and have also been proved that they can substantially improve the model performance. In these settings, contextual word representations are concatenated with word and character representations to form the token representations, i.e., xt = [wt , ct , et ], where et is the contextual word representation and it is not fine-tuned in any of our experiments. 1 https://github.com/cambridgeltl/ BioNLP-2016 3551 Methods P ACE2004 R F1 Ju et al. (2018) Wang et al. (2018) Wang and Lu (2018) Luo and Zhao (2020) Lin et al. (2019) Strakov´a et al. (2019) Shibuya and Hovy (2020) Wang et al. (2020) Our Method (naive) Our Method (max) Our Method (logsumexp) 74.9 78.0 71.8 72.4 73.3 75.1 78.92 79.93 80.83 81.12 81.90 81.24 75.33 75.10 78.86 77.71 78.05 78.96 Strakov´a et al. (2019) [B] Shibuya and Hovy (2020) [B] Wang et al. (2020) [B] Our Method (naive)[B] Our Method (max)[B] Our Method (logsumexp)[B] 84.71 85.23 86.08 86.19 86.27 86.42 Strakov´a et al. (2019) [B+F] Shibuya and Hovy (2020) [B+F] Wang et al. (2020) [B+F] Our Method (naive)[B+F] Our Method (max)[B+F] Our Method (logsumexp)[B+F] 84.51 85.94"
2021.acl-long.275,P16-1101,0,0.0330506,"ion for recognition at each level. In addition, we demonstrate that recognizing innermost entities first results in better performance than the conventional outermost entities first scheme. We provide extensive experimental results on ACE2004, ACE2005, and GENIA datasets to show the effectiveness and efficiency of our proposed method. 1 PER ROLE ROLE PER Figure 1: An example of nested NER. Introduction Named entity recognition (NER), as a key technique in natural language processing, aims at detecting entities and assigning semantic category labels to them. Early research (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016) proposed to employ deep learning methods and obtained significant performance improvements. However, most of them assume that the entities are not nested within other entities, so-called flat NER. Inherently, these methods do not work satisfactorily when nested entities exist. Figure 1 displays an example of the nested NER task. Recently, a large number of papers proposed novel methods (Fisher and Vlachos, 2019; Wang et al., 2020) for the nested NER task. Among them, layered methods solve this task through multi-level sequential labeling, in which entities are divided in"
2021.acl-long.275,P19-1527,0,0.0242444,"Missing"
2021.acl-long.275,D17-1276,0,0.0193825,"ting anchor words and entity boundaries first, and then classify each detected span. Exhaustive models simply enumerate all possible spans and utilize a maximum entropy tagger (Byrne, 2007) and neural networks (Xu et al., 2017; Sohrab and Miwa, 2018; Zheng et al., 2019) for classification. Luan et al. (2019) additionally aims to consider the relationship among entities and proposed a novel method to jointly learn both entities and relations. Hypergraph-based Model Lu and Roth (2015) proposed a hyper-graph structure, in which edges are connected to multiple nodes to represents nested entities. Muis and Lu (2017) and Wang and Lu (2018) resolved spurious structures and ambiguous issue of hyper-graph structure. And Katiyar and Cardie (2018) proposed another kind of hyper-graph structure. Parsing-based Model Finkel and Manning (2009) indicated all these nested entities are located in some non-terminal nodes of the constituency parses of the original sentences, thus they proposed to use a CRF-based constituency parser to obtain them. However, the cubic time complexity limits its applicability. Wang et al. (2018) instead proposed to use a transition-based constituency parser to incrementally build constitu"
2021.acl-long.275,D14-1162,0,0.0864412,"ll these datasets into tran/dev/test split by following Shibuya and Hovy (2020) and Wang et al. (2020). The dataset statistics can be found in Table 1. Dataset Sentences Mentions |Y| m ACE2004 ACE2005 GENIA 6,198 / 742 / 809 7,285 / 968 / 1,058 15,022 / 1,669 / 1,855 22,195 / 2,514 / 3,034 24,700 / 3,218 / 3,029 47,006 / 4,461 / 5,596 29 29 21 6 6 4 Table 1: Sizes of the dataset shown in the train/dev/test split. |Y |is the size of the label set, m is the maximal depth of entity nesting. Hyper-parameters Settings For word embeddings initialization, we utilize 100dimensional pre-trained GloVe (Pennington et al., 2014) for the ACE2004 and the ACE2005 datasets, and use 200-dimensional biomedical domain word embeddings1 (Chiu et al., 2016) for the GENIA dataset. Moreover, we randomly initialize 30dimensional vectors for character embeddings. The hidden state dimension of character-level LSTM dc is 100, i.e., 50 in each direction, thus the dimension of token representation dx is 200. We apply dropout (Srivastava et al., 2014) on token representations before feeding it into the encoder. The hidden state dimension of the three-layered LSTM is 600 for ACE2004 and ACE2005, i.e., 300 in each direction, and 400 for"
2021.acl-long.275,W95-0107,0,0.136025,"o propose three different selection strategies for fully leveraging information among hidden states. Besides, Shibuya and Hovy (2020) proposed to recognize entities from outermost to inner. We empirically demonstrate that extracting the innermost entities first results in better performance. This may due to the fact that some long entities do not contain any inner entity, so using outermostfirst encoding mixes these entities with other short entities at the same levels, therefore leading encoder representations to be dislocated. In this paper, we convert entities to the IOBES encoding scheme (Ramshaw and Marcus, 1995), and solve nested NER through applying CRF level by level. Our contributions are considered as fourfold, (a) we design a novel nested NER algorithm to explicitly exclude the influence of the best path through using a different potential function at each level, (b) we propose three different selection strategies for fully utilizing information among hidden states, (c) we empirically demonstrate that recognizing entities from innermost to outer results in better performance, (d) and we provide extensive experimental results to demonstrate the effectiveness and efficiency of our proposed method"
2021.acl-long.275,2020.acl-demos.38,0,0.0134811,"al information, then selecting chunks in the original order is sufficient, thus our dynamic selecting mechanism can only slightly improve the model performance. 3.5 nested outermost entities at the same level would dislocate the encoding representation. Furthermore, even if we use the outermost-first encoding scheme, our method is superior to Shibuya and Hovy (2020), which further demonstrates the effectiveness of excluding the influence of the best path. 3.6 Time Complexity and Speed The time complexity of encoder is O (n), and because we employ the same tree reduction acceleration trick4 as Rush (2020), the time complexity of CRF is reduced to O (log n), therefore the overall time complexity is O (n + m · log n). Even our model outperforms slightly worse than Wang et al. (2020), the training and inference speed of our model is much faster than them, as shown in Table 4, since we do not need to stack the decoding component to 16 layers. Especially, when we increase the batch size to 64, the decoding speed is more than two times faster than their model. Method Batch Size Training Decoding Wang et al. (2020) 16 32 64 1,937.16 3,632.64 6,298.85 3,626.53 4,652.05 5,113.85 Our Method 16 32 64 4,1"
2021.acl-long.275,2020.tacl-1.39,0,0.0610903,"not nested within other entities, so-called flat NER. Inherently, these methods do not work satisfactorily when nested entities exist. Figure 1 displays an example of the nested NER task. Recently, a large number of papers proposed novel methods (Fisher and Vlachos, 2019; Wang et al., 2020) for the nested NER task. Among them, layered methods solve this task through multi-level sequential labeling, in which entities are divided into several levels, where the term level indicates the depth of entity nesting, and sequential labeling is performed repeatedly. As a special case of layered method, Shibuya and Hovy (2020) force the ∗ ROLE This work was done when the first author was at NAIST. next level entities to locate on the second-best path of the current level search space. Hence, their algorithm can repeatedly detect inner entities through applying a conventional conditional random field (CRF) (Lafferty et al., 2001) and then exclude the obtained best paths from the search space. To accelerate computation, they also designed an algorithm to efficiently compute the partition function with the best path excluded. Moreover, because they search the outermost entities first, performing the second-best path s"
2021.acl-long.275,D18-1309,0,0.0332534,"Missing"
2021.acl-long.275,D18-1019,0,0.0639049,"ERT (Devlin et al., 2019) and Flair (Akbik et al., 2018) are the most commonly used contextual word representations in previous work, and have also been proved that they can substantially improve the model performance. In these settings, contextual word representations are concatenated with word and character representations to form the token representations, i.e., xt = [wt , ct , et ], where et is the contextual word representation and it is not fine-tuned in any of our experiments. 1 https://github.com/cambridgeltl/ BioNLP-2016 3551 Methods P ACE2004 R F1 Ju et al. (2018) Wang et al. (2018) Wang and Lu (2018) Luo and Zhao (2020) Lin et al. (2019) Strakov´a et al. (2019) Shibuya and Hovy (2020) Wang et al. (2020) Our Method (naive) Our Method (max) Our Method (logsumexp) 74.9 78.0 71.8 72.4 73.3 75.1 78.92 79.93 80.83 81.12 81.90 81.24 75.33 75.10 78.86 77.71 78.05 78.96 Strakov´a et al. (2019) [B] Shibuya and Hovy (2020) [B] Wang et al. (2020) [B] Our Method (naive)[B] Our Method (max)[B] Our Method (logsumexp)[B] 84.71 85.23 86.08 86.19 86.27 86.42 Strakov´a et al. (2019) [B+F] Shibuya and Hovy (2020) [B+F] Wang et al. (2020) [B+F] Our Method (naive)[B+F] Our Method (max)[B+F] Our Method (logsume"
2021.acl-long.275,D18-1124,0,0.0249544,"Missing"
2021.acl-long.275,2020.acl-main.525,0,0.174513,"in natural language processing, aims at detecting entities and assigning semantic category labels to them. Early research (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016) proposed to employ deep learning methods and obtained significant performance improvements. However, most of them assume that the entities are not nested within other entities, so-called flat NER. Inherently, these methods do not work satisfactorily when nested entities exist. Figure 1 displays an example of the nested NER task. Recently, a large number of papers proposed novel methods (Fisher and Vlachos, 2019; Wang et al., 2020) for the nested NER task. Among them, layered methods solve this task through multi-level sequential labeling, in which entities are divided into several levels, where the term level indicates the depth of entity nesting, and sequential labeling is performed repeatedly. As a special case of layered method, Shibuya and Hovy (2020) force the ∗ ROLE This work was done when the first author was at NAIST. next level entities to locate on the second-best path of the current level search space. Hence, their algorithm can repeatedly detect inner entities through applying a conventional conditional ran"
2021.acl-long.275,P17-1114,0,0.0249752,"e other is the tokens in recognized entities, to model the interaction among them. Wang et al. (2020) proposed to learn the l-gram representations at layer l through applying a decoder component to reduce a sentence layer by layer and to directly classify these l-gram spans. Region-based Model Lin et al. (2019) proposed an anchor-region network to recognize nested entities through detecting anchor words and entity boundaries first, and then classify each detected span. Exhaustive models simply enumerate all possible spans and utilize a maximum entropy tagger (Byrne, 2007) and neural networks (Xu et al., 2017; Sohrab and Miwa, 2018; Zheng et al., 2019) for classification. Luan et al. (2019) additionally aims to consider the relationship among entities and proposed a novel method to jointly learn both entities and relations. Hypergraph-based Model Lu and Roth (2015) proposed a hyper-graph structure, in which edges are connected to multiple nodes to represents nested entities. Muis and Lu (2017) and Wang and Lu (2018) resolved spurious structures and ambiguous issue of hyper-graph structure. And Katiyar and Cardie (2018) proposed another kind of hyper-graph structure. Parsing-based Model Finkel and"
2021.acl-long.275,P18-1030,0,0.0116537,"s naive. These observations further demonstrate our dynamic chunk selection strategies are capable of learning more meaningful representations. 4 Related Work Existing NER algorithms commonly employ various neural networks to leverage more morphological and contextual information to improve performance. For example, to handle the out-ofvocabulary issue through introducing morphological features, Huang et al. (2015) proposed to employ manual spelling feature, while Ma and Hovy (2016) and Lample et al. (2016) suggested introducing CNN and LSTM to build word representations from character-level. Zhang et al. (2018) and Chen et al. (2019) introduced global representation to enhance encoder capability of encoding contextual information. Layered Model As a layered model, Ju et al. (2018) dynamically update span-level representations for next layer recognition according to recognized inner entities. Fisher and Vlachos (2019) proposed a merge and label method to enhance this idea further. Recently, Shibuya and Hovy (2020) designed a novel algorithm to efficiently learn and decode the second-best path on the span of detected entities. Luo and Zhao (2020) build two different graphs, one is the original token s"
2021.acl-long.275,D19-1034,0,0.0118739,"ties, to model the interaction among them. Wang et al. (2020) proposed to learn the l-gram representations at layer l through applying a decoder component to reduce a sentence layer by layer and to directly classify these l-gram spans. Region-based Model Lin et al. (2019) proposed an anchor-region network to recognize nested entities through detecting anchor words and entity boundaries first, and then classify each detected span. Exhaustive models simply enumerate all possible spans and utilize a maximum entropy tagger (Byrne, 2007) and neural networks (Xu et al., 2017; Sohrab and Miwa, 2018; Zheng et al., 2019) for classification. Luan et al. (2019) additionally aims to consider the relationship among entities and proposed a novel method to jointly learn both entities and relations. Hypergraph-based Model Lu and Roth (2015) proposed a hyper-graph structure, in which edges are connected to multiple nodes to represents nested entities. Muis and Lu (2017) and Wang and Lu (2018) resolved spurious structures and ambiguous issue of hyper-graph structure. And Katiyar and Cardie (2018) proposed another kind of hyper-graph structure. Parsing-based Model Finkel and Manning (2009) indicated all these nested en"
2021.eacl-demos.4,W04-1213,0,0.154095,"RGANISM SUBSTANCE, SIMPLE CHEMICAL, TISSUE Entity Recognition We use biomedical entity recognition models specialized for predicting entity type and provided by SciSpacy (Neumann et al., 2019) (Table 1). Each of the models is trained on a different annotated corpus, thus, covers a different set of biomedical entities. By using multiple entity systems, we can obtain various specialized entity information: chemicals and diseases with BCD5CDR (Li et al., 2016), cell types, chemicals, proteins, and genes with CRAFT (Bada et al., 2012), cell lines, cell types, DNAs, RNAs, and proteins with JNLPBA (Collier and Kim, 2004), and cancer genetics with BioNLP13CG (Pyysalo et al., 2015). • ReVerb (Fader et al., 2011) tackles the problems of incoherent and uninformative extractions by introducing constraints on binary, verb-based relation phrases. • OLLIE (Mausam et al., 2012) addresses the problems that Open IE systems such as ReVerb only extract relations that are mediated by verbs. Not only by verbs, OLIEE extracts relations mediated also by nouns, adjectives, and more. 3.4 Relation Clustering We build a cluster hierarchy on a subset of the extracted relations (this subset contains all relations in which both arg1"
2021.eacl-demos.4,D11-1142,0,0.0957587,"tion models specialized for predicting entity type and provided by SciSpacy (Neumann et al., 2019) (Table 1). Each of the models is trained on a different annotated corpus, thus, covers a different set of biomedical entities. By using multiple entity systems, we can obtain various specialized entity information: chemicals and diseases with BCD5CDR (Li et al., 2016), cell types, chemicals, proteins, and genes with CRAFT (Bada et al., 2012), cell lines, cell types, DNAs, RNAs, and proteins with JNLPBA (Collier and Kim, 2004), and cancer genetics with BioNLP13CG (Pyysalo et al., 2015). • ReVerb (Fader et al., 2011) tackles the problems of incoherent and uninformative extractions by introducing constraints on binary, verb-based relation phrases. • OLLIE (Mausam et al., 2012) addresses the problems that Open IE systems such as ReVerb only extract relations that are mediated by verbs. Not only by verbs, OLIEE extracts relations mediated also by nouns, adjectives, and more. 3.4 Relation Clustering We build a cluster hierarchy on a subset of the extracted relations (this subset contains all relations in which both arg1 and arg2 are biomedical entities), so users can quickly find their interested relation exp"
2021.eacl-demos.4,P15-1034,0,0.0288929,"onnected phrases, not for identifying clause type like ClauseIE. We utilize FINCH (Sarfraz et al., 2019), hierarchical clustering method, and BERT (Devlin et al., 2019) for this task. First, BERT-Base model is used to encode each relation as a simple sentence “ arg1 rel arg2 ” into a 768-dimensional vector. Then, FINCH is used to build the cluster hierarchy. For each cluster, representative expressions of the cluster are selected from its rels from top informative relations scored by the formula presented in the next subsection. The result cluster hierarchy is illustrated in Fig. 3. • OpenIE (Angeli et al., 2015) extracts relations by breaking a long sentence into short, coherent clauses, and then finds the maximally simple relations. The extracted relations are also tagged with biomedical entities recognized by using entity recognition models presented in the next subsection. 26 Figure 3: Illustration of cluster hierarchy. “DISEASE-0-7”: the type of an entity contained in the arg1 is DISEASE, the id of the level 0 (root) cluster is 0, the id of the level 1 cluster is 7. An expression has the form of ENTITY TYPE (in arg1 , omitted) relation/verb phrase ENTITY TYPE (in arg2 ). Expressions are separated"
2021.eacl-demos.4,D12-1048,0,0.0533372,"ted corpus, thus, covers a different set of biomedical entities. By using multiple entity systems, we can obtain various specialized entity information: chemicals and diseases with BCD5CDR (Li et al., 2016), cell types, chemicals, proteins, and genes with CRAFT (Bada et al., 2012), cell lines, cell types, DNAs, RNAs, and proteins with JNLPBA (Collier and Kim, 2004), and cancer genetics with BioNLP13CG (Pyysalo et al., 2015). • ReVerb (Fader et al., 2011) tackles the problems of incoherent and uninformative extractions by introducing constraints on binary, verb-based relation phrases. • OLLIE (Mausam et al., 2012) addresses the problems that Open IE systems such as ReVerb only extract relations that are mediated by verbs. Not only by verbs, OLIEE extracts relations mediated also by nouns, adjectives, and more. 3.4 Relation Clustering We build a cluster hierarchy on a subset of the extracted relations (this subset contains all relations in which both arg1 and arg2 are biomedical entities), so users can quickly find their interested relation expressions or they can choose some clusters which may contain their interested relation expressions. • ClausIE (Del Corro and Gemulla, 2013) is a clause-based appro"
2021.eacl-demos.4,W19-5034,0,0.0213431,"ISSUE STRUCTURE, ORGANISM SUBDIVISION, PATHOLOGICAL FORMATION, 3.2 Relation Extraction 3.3 With the objective of extracting as many relations as possible, we employ several relation extraction methods. Each method has their own characteristics, thus, may extract different kinds of relations. By combining several methods, we can obtain higher extraction coverage. The methods are briefly described as follows. ORGAN, ORGANISM, ORGANISM SUBSTANCE, SIMPLE CHEMICAL, TISSUE Entity Recognition We use biomedical entity recognition models specialized for predicting entity type and provided by SciSpacy (Neumann et al., 2019) (Table 1). Each of the models is trained on a different annotated corpus, thus, covers a different set of biomedical entities. By using multiple entity systems, we can obtain various specialized entity information: chemicals and diseases with BCD5CDR (Li et al., 2016), cell types, chemicals, proteins, and genes with CRAFT (Bada et al., 2012), cell lines, cell types, DNAs, RNAs, and proteins with JNLPBA (Collier and Kim, 2004), and cancer genetics with BioNLP13CG (Pyysalo et al., 2015). • ReVerb (Fader et al., 2011) tackles the problems of incoherent and uninformative extractions by introducin"
2021.eacl-demos.4,2020.sdp-1.5,0,0.0281385,"D-19 outbreak, it is essential to grasp valuable knowledge from a huge number of COVID-19-related papers for dealing with the pandemic effectively. Sohrab et al. (2020) introduced the BENNERD system that detects named entities in biomedical text and links them to the unified medical language system (UMLS) to facilitate the COVID-19 research. Hope et al. (2020) created a dataset annotated for mechanism relations and trained an information extraction model on this data. Then, they used the model to extract a Knowledge Base (KB) of mechanism and effect relations from papers relating to COVID-19. Zhang et al. (2020) built Covidex, a search infrastructure that provides information access to the COVID-19 Open Research Dataset such as answering questions. Esteva et al. (2020) also presented Co-Search, a retriever-ranker semantic search engine designed to handle complex queries over the COVID-19 literature. Wang et al. (2020) created the EvidenceMiner web-based system. Given a query as a natural language statement, EvidenceMiner automatically retrieves sentence-level textual evidence from the CORD-19 corpus. Clearly, previous works made a great effort to acquire useful knowledge from the COVID-19 literature,"
2021.eacl-demos.4,2020.emnlp-demos.24,0,0.0957812,"Missing"
2021.eacl-main.323,W14-3348,0,0.0239705,"mber of the object pairs was 11,607 and 10,612 in the settings A and B, respectively. We set the batch size to eight and terminated the training when the best validation score (specifically, the CIDEr score) did not exceed for 20 epochs. For the optimizer, we used Adam with the recommended hyperparameters (Kingma and Ba, 2015). Evaluation. In the evaluation, we set the maximum decoding length to 20. Our model decoded captions by using greedy search and unique-object decoding, described in Section 2.4. The evaluation metrics we used were BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Denkowski and Lavie, 2014), CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016). 3.3 Comparison with the State-of-the-Art Results Table 2 lists the results of our model compared with the previous state-of-the-art results. To avoid evaluating cherry-picked scores, we computed the mean and standard deviation of five results obtained with different seeds8 . Our method outperforms the previous approaches in terms of all evaluation metrics. These results confirm the effectiveness of our simple method. 8 In all the experiments, we specified a seed of 0, 1, 2, 3, 4 for each run. 3696 A B gate pseudoL unique image"
2021.eacl-main.323,W04-1013,0,0.0573135,"in the pairs. The number of the object pairs was 11,607 and 10,612 in the settings A and B, respectively. We set the batch size to eight and terminated the training when the best validation score (specifically, the CIDEr score) did not exceed for 20 epochs. For the optimizer, we used Adam with the recommended hyperparameters (Kingma and Ba, 2015). Evaluation. In the evaluation, we set the maximum decoding length to 20. Our model decoded captions by using greedy search and unique-object decoding, described in Section 2.4. The evaluation metrics we used were BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Denkowski and Lavie, 2014), CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016). 3.3 Comparison with the State-of-the-Art Results Table 2 lists the results of our model compared with the previous state-of-the-art results. To avoid evaluating cherry-picked scores, we computed the mean and standard deviation of five results obtained with different seeds8 . Our method outperforms the previous approaches in terms of all evaluation metrics. These results confirm the effectiveness of our simple method. 8 In all the experiments, we specified a seed of 0, 1, 2, 3, 4 for each run."
2021.eacl-main.323,K19-1009,0,0.014115,"nsidering images: is sitting on followed cat in both (c) and (e). 4 Related Work There has been considerable research with different settings and approaches to describe scenes that have no image–sentence pairs. Novel object captioning (Hendricks et al., 2016; Venugopalan et al., 2017; Anderson et al., 2018a; Agrawal et al., 2019) attempted describing unseen objects in captions. 3699 They incorporated an image classifier or object detector trained on objects not included in image– sentence pairs. Lu et al. (2018) tested captioning models on the generation of unseen combinations of objects, and Nikolaus et al. (2019) extended this to the unseen combinations of objects, attributes, and relations. In both settings, only the combinations were unseen, but each word in the combinations appeared in the training data. Semisupervised approaches utilized caption retrieval models to automatically collect the corresponding captions for unannotated images to augment image– sentence pairs (Liu et al., 2018; Kim et al., 2019). The above work was evaluated on the scenes where correct descriptions partially overlapped with those in the training image–sentence pairs. However, there can be scenes with no such overlap due t"
2021.eacl-main.323,P02-1040,0,0.114114,"e same sampling on each object in the pairs. The number of the object pairs was 11,607 and 10,612 in the settings A and B, respectively. We set the batch size to eight and terminated the training when the best validation score (specifically, the CIDEr score) did not exceed for 20 epochs. For the optimizer, we used Adam with the recommended hyperparameters (Kingma and Ba, 2015). Evaluation. In the evaluation, we set the maximum decoding length to 20. Our model decoded captions by using greedy search and unique-object decoding, described in Section 2.4. The evaluation metrics we used were BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Denkowski and Lavie, 2014), CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016). 3.3 Comparison with the State-of-the-Art Results Table 2 lists the results of our model compared with the previous state-of-the-art results. To avoid evaluating cherry-picked scores, we computed the mean and standard deviation of five results obtained with different seeds8 . Our method outperforms the previous approaches in terms of all evaluation metrics. These results confirm the effectiveness of our simple method. 8 In all the experiments, we specified a seed of 0, 1, 2,"
2021.eacl-main.323,P18-1238,0,0.0145311,"ious work; thus, we used the object detector trained on OpenImagesv2 (Krasin et al., 2017) to compare with Feng et al. (2019) and that trained on OpenImages-v4 (Kuznetsova et al., 2020) to compare with Laina et al. (2019). Note that these object detectors were not trained on MS COCO images. Following the previous work, we refrained from using the detected bounding boxes and their features. Training Text. Following the previous work, we used the Shutterstock image description corpus (SS) (Feng et al., 2019) and the training split captions (without images) of Google’s Conceptual Captions (GCC) (Sharma et al., 2018) for comparison with Feng et al. (2019) and Laina et al. (2019), respectively. SS consists of 2.3M image descriptions crawled from Shutterstock, an online stock photography website; GCC consists of 3.3M image descriptions crawled from the web. Note that these sentences are not the descriptions of the images in MS COCO. 3.2 Implementation Details Image Encoder. For a fair comparison with the previous work, we employed different image encoders depending on the compared method: Inception-v4 (Szegedy et al., 2017) in the settings of Feng et al. (2019) and ResNet-101 (He et al., 2016a,b) in the set"
2021.eacl-main.323,Q14-1006,0,0.0530427,"rformance. These results confirm the importance of careful alignment in word-level details.1 1 Introduction Image captioning is a task to describe images in natural languages. This is a fundamental challenge with regard to automatically retrieving and summarizing the visual information in a human-readable form. Recently, considerable progress has been made (Vinyals et al., 2015; Xu et al., 2015; Anderson et al., 2018b) owing to the development of neural networks and a large number of annotated 1 Code will be available at https://github.com/ ukyh/RemovingSpuriousAlignment image–sentence pairs (Young et al., 2014; Lin et al., 2014; Krishna et al., 2017). However, these pairs are limited in their coverage of scenes2 , and scaling them is difficult owing to the cost of manual annotation. Unsupervised image captioning (Feng et al., 2019) aims to describe scenes that have no corresponding image–sentence pairs, without requiring additional annotation of the pairs. The only available resources are images and sentences drawn from different sources and object labels detected from the images. Although it is highly challenging, unsupervised image captioning has the potential to cover a broad range of scenes by"
2021.findings-acl.164,K18-2005,0,0.0214902,"number of epochs is 100, and gradients exceed 5 will be clipped. In addition, since the pre-trained contextualized word embeddings technique is widely accepted as a new fundamental utility of natural language processing, we also conduct experiments with ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). In these settings, tokens are represented as xt = [wt , ct , et ], where et is the contextual word representation. ELMo vectors are obtained by averaging output vectors over all layers of ELMo. For English experiments, we use the original checkpoint, and use the checkpoints provided by Che et al. (2018) for Chinese experiments. BERT representations are the averages all BERT subword embeddings in the last four layers. Following Li et al. (2020b) and Li et al. (2020a), we utilize bert-large-cased and hfl/chinese-bert-wwm checkpoints for English and Chinese experiments respectively. 4.3 Evaluation NER experiments are evaluated by using F1 scores, and POS tagging experiments are evaluated with accuracy scores. All of our experiments were run 4 times with different random seeds, and the averaged scores are reported in the following tables. Our models3 are implemented with deep learning framework"
2021.findings-acl.164,Q16-1026,0,0.580398,"ly than Cui and Zhang (2019). Notably, the model of Jie and Lu (2019) relies on external dependency annotations, whereas our model requires no external knowledge4 . In the case of employing ELMo, our model outperforms Jie and Lu (2019) by 0.11 F1 score. On the CoNLL 2003 English dataset, our model performs worse than these baseline models, but, with ELMo, it outperforms Jie and Lu (2019) and 3 https://github.com/speedcell4/refiner In this paper, we use “external knowledge” to denote any additional resources other than word embeddings and contextual word representations. 1877 4 Model EK P R F1 Chiu and Nichols (2016) Strubell et al. (2017) Li et al. (2017) Ghaddar and Langlais (2018) Fisher and Vlachos (2019) Cui and Zhang (2019) Yan et al. (2019) Jie and Lu (2019) Our Method X X X - 86.04 88.00 88.53 88.71 86.53 86.50 88.50 88.60 86.28 86.84 87.21 87.95 87.59 88.16 88.43 88.52 88.65 Yan et al. (2019) [E] Jie and Lu (2019) [E] Our Method [E] X - 89.59 89.51 90.17 90.48 89.78 89.88 89.99 Devlin et al. (2019) [B] Fisher and Vlachos (2019) [B] Li et al. (2020b) [B] Yu et al. (2020)[B] Our Method [B] - 90.01 92.98 91.1 90.00 88.35 89.95 91.5 91.17 89.16 89.71 91.11 91.3 90.93 our model significantly outperfor"
2021.findings-acl.164,D19-1422,0,0.0595106,"ploited subword-level features. Moreover, introducing long-term dependency features is also found to be beneficial for sequential labeling. Jie and Lu (2019) attempted to explicitly exploit dependency relations with additional annotations, while Zhang et al. (2018) and Chen et al. (2019) endeavored to learn these relations implicitly with more complex encoders. ∗ O This work was done when the first author was at NAIST. However, as Tishby and Zaslavsky (2015) pointed out, features are not created equal, only the target-relevant features are profitable for improving model performance. Recently, Cui and Zhang (2019) proposed a hierarchically-refined label attention network (LAN), which explicitly leverages label embeddings and captures long-term label dependency relations through multiple refinements layers. Individually picking up the most likely label at each time step is undoubtedly critical, however, considering the entire historical progress is also indispensable. We find that the locally normalized attention, which Cui and Zhang (2019) used to leverage information from label embeddings, can eventually hurt performance. Since it only considers the current time step but ignores labels at other time s"
2021.findings-acl.164,N19-1423,0,0.180674,"g rate ητ = η0 /(1 + 0.075 · τ ), where τ is the index of the current epoch, and the initial learning rate η0 for Chinese experiments without contextual word representations is 0.05, and for all the other experiments we use 0.1. The weight decay rate is 10−8 , the momentum is 0.15, the batch size is 10, the number of epochs is 100, and gradients exceed 5 will be clipped. In addition, since the pre-trained contextualized word embeddings technique is widely accepted as a new fundamental utility of natural language processing, we also conduct experiments with ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). In these settings, tokens are represented as xt = [wt , ct , et ], where et is the contextual word representation. ELMo vectors are obtained by averaging output vectors over all layers of ELMo. For English experiments, we use the original checkpoint, and use the checkpoints provided by Che et al. (2018) for Chinese experiments. BERT representations are the averages all BERT subword embeddings in the last four layers. Following Li et al. (2020b) and Li et al. (2020a), we utilize bert-large-cased and hfl/chinese-bert-wwm checkpoints for English and Chinese experiments respectively. 4.3 Evaluat"
2021.findings-acl.164,C18-1161,0,0.0258358,"Missing"
2021.findings-acl.164,D19-1096,0,0.0263898,"Missing"
2021.findings-acl.164,P19-1027,0,0.0111793,", each tag is a part-of-speech category. For instance, NN represents a singular noun and VBN is the past participle of a verb. Introduction Sequential labeling tasks, e.g., named entity recognition (NER) and part-of-speech (POS) tagging, play an important role in natural language processing. Figure 1 shows two examples of sequential labeling tasks. Early studies focused on introducing rich features to improve performance. For example, to handle out-of-vocabulary words by introducing morphological features, Lample et al. (2016) and Ma and Hovy (2016) leveraged character-level features, whereas Heinzerling and Strube (2019) exploited subword-level features. Moreover, introducing long-term dependency features is also found to be beneficial for sequential labeling. Jie and Lu (2019) attempted to explicitly exploit dependency relations with additional annotations, while Zhang et al. (2018) and Chen et al. (2019) endeavored to learn these relations implicitly with more complex encoders. ∗ O This work was done when the first author was at NAIST. However, as Tishby and Zaslavsky (2015) pointed out, features are not created equal, only the target-relevant features are profitable for improving model performance. Recentl"
2021.findings-acl.164,D19-1399,0,0.169071,"named entity recognition (NER) and part-of-speech (POS) tagging, play an important role in natural language processing. Figure 1 shows two examples of sequential labeling tasks. Early studies focused on introducing rich features to improve performance. For example, to handle out-of-vocabulary words by introducing morphological features, Lample et al. (2016) and Ma and Hovy (2016) leveraged character-level features, whereas Heinzerling and Strube (2019) exploited subword-level features. Moreover, introducing long-term dependency features is also found to be beneficial for sequential labeling. Jie and Lu (2019) attempted to explicitly exploit dependency relations with additional annotations, while Zhang et al. (2018) and Chen et al. (2019) endeavored to learn these relations implicitly with more complex encoders. ∗ O This work was done when the first author was at NAIST. However, as Tishby and Zaslavsky (2015) pointed out, features are not created equal, only the target-relevant features are profitable for improving model performance. Recently, Cui and Zhang (2019) proposed a hierarchically-refined label attention network (LAN), which explicitly leverages label embeddings and captures long-term labe"
2021.findings-acl.164,N16-1030,0,0.747727,"ity, while O signifies this word is outside any named entity. In the case of POS tagging, each tag is a part-of-speech category. For instance, NN represents a singular noun and VBN is the past participle of a verb. Introduction Sequential labeling tasks, e.g., named entity recognition (NER) and part-of-speech (POS) tagging, play an important role in natural language processing. Figure 1 shows two examples of sequential labeling tasks. Early studies focused on introducing rich features to improve performance. For example, to handle out-of-vocabulary words by introducing morphological features, Lample et al. (2016) and Ma and Hovy (2016) leveraged character-level features, whereas Heinzerling and Strube (2019) exploited subword-level features. Moreover, introducing long-term dependency features is also found to be beneficial for sequential labeling. Jie and Lu (2019) attempted to explicitly exploit dependency relations with additional annotations, while Zhang et al. (2018) and Chen et al. (2019) endeavored to learn these relations implicitly with more complex encoders. ∗ O This work was done when the first author was at NAIST. However, as Tishby and Zaslavsky (2015) pointed out, features are not created"
2021.findings-acl.164,D19-1099,0,0.0456713,"Missing"
2021.findings-acl.164,D18-1149,0,0.0286425,"he widespread use of contextual word representations, e.g., ELMo (Peters et al., 2018), Flair (Akbik et al., 2018), and BERT (Devlin et al., 2019), greatly improves the performance of NER models and they are accepted as new fundamental techniques of natural language processing. Intuitively speaking, the refinement mechanism provides the models with additional chances to revise previous decisions. In existing work, this method was successfully applied to various tasks, e.g., text classification (Yu et al., 2017), sequential labeling (Cui and Zhang, 2019; Lyu et al., 2019), machine translation (Lee et al., 2018), and question answering (Nema et al., 2019). Our work is not the first attempt of introducing refinement mechanism to sequential labeling tasks. Cui and Zhang (2019) relied on locally normalized attention to softly refine hidden representations layer by layer, while Liu et al. (2019a) chose to discretely filter out target-irrelevant semantic aspects and thus could be considered as a hard refinement mechanism. 6 Conclusion Motivated by the structured attention, we enhanced the previous refinement mechanism by replacing the locally normalized attention with our globally normalized attention. Ex"
2021.findings-acl.164,P16-1101,0,0.630049,"is word is outside any named entity. In the case of POS tagging, each tag is a part-of-speech category. For instance, NN represents a singular noun and VBN is the past participle of a verb. Introduction Sequential labeling tasks, e.g., named entity recognition (NER) and part-of-speech (POS) tagging, play an important role in natural language processing. Figure 1 shows two examples of sequential labeling tasks. Early studies focused on introducing rich features to improve performance. For example, to handle out-of-vocabulary words by introducing morphological features, Lample et al. (2016) and Ma and Hovy (2016) leveraged character-level features, whereas Heinzerling and Strube (2019) exploited subword-level features. Moreover, introducing long-term dependency features is also found to be beneficial for sequential labeling. Jie and Lu (2019) attempted to explicitly exploit dependency relations with additional annotations, while Zhang et al. (2018) and Chen et al. (2019) endeavored to learn these relations implicitly with more complex encoders. ∗ O This work was done when the first author was at NAIST. However, as Tishby and Zaslavsky (2015) pointed out, features are not created equal, only the target"
2021.findings-acl.164,D17-1282,0,0.103833,"l of Jie and Lu (2019) relies on external dependency annotations, whereas our model requires no external knowledge4 . In the case of employing ELMo, our model outperforms Jie and Lu (2019) by 0.11 F1 score. On the CoNLL 2003 English dataset, our model performs worse than these baseline models, but, with ELMo, it outperforms Jie and Lu (2019) and 3 https://github.com/speedcell4/refiner In this paper, we use “external knowledge” to denote any additional resources other than word embeddings and contextual word representations. 1877 4 Model EK P R F1 Chiu and Nichols (2016) Strubell et al. (2017) Li et al. (2017) Ghaddar and Langlais (2018) Fisher and Vlachos (2019) Cui and Zhang (2019) Yan et al. (2019) Jie and Lu (2019) Our Method X X X - 86.04 88.00 88.53 88.71 86.53 86.50 88.50 88.60 86.28 86.84 87.21 87.95 87.59 88.16 88.43 88.52 88.65 Yan et al. (2019) [E] Jie and Lu (2019) [E] Our Method [E] X - 89.59 89.51 90.17 90.48 89.78 89.88 89.99 Devlin et al. (2019) [B] Fisher and Vlachos (2019) [B] Li et al. (2020b) [B] Yu et al. (2020)[B] Our Method [B] - 90.01 92.98 91.1 90.00 88.35 89.95 91.5 91.17 89.16 89.71 91.11 91.3 90.93 our model significantly outperforms them by 1.41 F1 score with BERT. More"
2021.findings-acl.164,2020.acl-main.611,0,0.138687,"We release our CRF implementation with these two tricks as an independent library1 for future study and use. 3.5 Character Embeddings Initialization We describe a trick for Chinese character embeddings initialization. The most striking difference between Chinese and English is that the minimal semantic units, i.e., sememes, of Chinese are characters instead of words or subwords. The character vocabulary size of Chinese, e.g., around 2,000 on the OntoNote 5.0 dataset, is markedly larger than English, e.g., around 100 on the OntoNotes 5.0 English dataset. Existing models (Zhang and Yang, 2018; Li et al., 2020a) generally focused on introducing additional pre-trained character embeddings on the top of lexicon embeddings, and attempted to selectively leverage information from both of them according to the different word segmentation schemes. However, we notice that most of these characters already exist in the word vocabulary as single-character words, thus we employ a randomly initialized orthogonal matrix2 to project the pre-trained word embeddings into the same dimension as the character embeddings, and use these projected embeddings for initialization. 4 Experiments 4.1 Datasets We conduct exper"
2021.findings-acl.164,2020.acl-main.519,0,0.207212,"We release our CRF implementation with these two tricks as an independent library1 for future study and use. 3.5 Character Embeddings Initialization We describe a trick for Chinese character embeddings initialization. The most striking difference between Chinese and English is that the minimal semantic units, i.e., sememes, of Chinese are characters instead of words or subwords. The character vocabulary size of Chinese, e.g., around 2,000 on the OntoNote 5.0 dataset, is markedly larger than English, e.g., around 100 on the OntoNotes 5.0 English dataset. Existing models (Zhang and Yang, 2018; Li et al., 2020a) generally focused on introducing additional pre-trained character embeddings on the top of lexicon embeddings, and attempted to selectively leverage information from both of them according to the different word segmentation schemes. However, we notice that most of these characters already exist in the word vocabulary as single-character words, thus we employ a randomly initialized orthogonal matrix2 to project the pre-trained word embeddings into the same dimension as the character embeddings, and use these projected embeddings for initialization. 4 Experiments 4.1 Datasets We conduct exper"
2021.findings-acl.164,P19-1532,0,0.334171,"word representation already provides rich enough morphological information, thus careful character embeddings initialization can only bring little benefit. On the OntoNotes 5.0 ChiTable 2: Experimental results on the OntoNotes 5.0 English dataset. Checkmark X in the “EK” column indicates that external knowledge is utilized in that model. [E] and [B] stands for ELMo and BERT respectively. Bold and underlined numbers indicate the best and the second-best results respectively. Model EK P R F1 Huang et al. (2015) Lample et al. (2016) Ma and Hovy (2016) Zhang et al. (2018) Chiu and Nichols (2016) Liu et al. (2019a) Yan et al. (2019) Liu et al. (2019b) Our Method X X X - 90.70 90.81 88.83 90.94 91.21 91.57 91.62 91.80 91.33 91.96 90.76 Jie and Lu (2019) [E] Yan et al. (2019)[E] Our Method [E] X - 92.60 93.19 92.40 92.62 92.89 Devlin et al. (2019) [B] Li et al. (2020b) [B] Yu et al. (2020) [B] Our Method [B] - 92.33 93.7 92.66 94.61 93.3 92.98 92.8 93.04 93.5 93.23 Model EK P R F1 Zhang and Yang (2018) Mengge et al. (2019) Gui et al. (2019a) Gui et al. (2019b) Yan et al. (2019) Li et al. (2020a) Our Method Our Method (init) X X X X X X - 76.35 76.78 76.40 76.13 75.28 75.49 71.56 72.54 72.60 73.68 72.39"
2021.findings-acl.164,P19-1233,0,0.180243,"word representation already provides rich enough morphological information, thus careful character embeddings initialization can only bring little benefit. On the OntoNotes 5.0 ChiTable 2: Experimental results on the OntoNotes 5.0 English dataset. Checkmark X in the “EK” column indicates that external knowledge is utilized in that model. [E] and [B] stands for ELMo and BERT respectively. Bold and underlined numbers indicate the best and the second-best results respectively. Model EK P R F1 Huang et al. (2015) Lample et al. (2016) Ma and Hovy (2016) Zhang et al. (2018) Chiu and Nichols (2016) Liu et al. (2019a) Yan et al. (2019) Liu et al. (2019b) Our Method X X X - 90.70 90.81 88.83 90.94 91.21 91.57 91.62 91.80 91.33 91.96 90.76 Jie and Lu (2019) [E] Yan et al. (2019)[E] Our Method [E] X - 92.60 93.19 92.40 92.62 92.89 Devlin et al. (2019) [B] Li et al. (2020b) [B] Yu et al. (2020) [B] Our Method [B] - 92.33 93.7 92.66 94.61 93.3 92.98 92.8 93.04 93.5 93.23 Model EK P R F1 Zhang and Yang (2018) Mengge et al. (2019) Gui et al. (2019a) Gui et al. (2019b) Yan et al. (2019) Li et al. (2020a) Our Method Our Method (init) X X X X X X - 76.35 76.78 76.40 76.13 75.28 75.49 71.56 72.54 72.60 73.68 72.39"
2021.findings-acl.164,L18-1008,0,0.0245265,"(Marcus et al., 1993) and the Universal Dependencies (UD) v2.2 English dataset for POS tagging experiments. The only data pre-processing that we have performed is replacing digital tokens with a special token. And we convert labels to the IOBES labeling scheme (Ramshaw and Marcus, 1995; Ratinov and Roth, 2009) on NER datasets. The dataset statistics are provided in Table 1. 4.2 Hyper-parameter Settings Following Cui and Zhang (2019) and Jie and Lu (2019), 100-dimensional Glove (Pennington et al., 2014) word embeddings are utilized for all the English experiments, and 300-dimensional FastText (Mikolov et al., 2018) word embeddings are employed for Chinese experiments. The dimension of character embeddings is 30, and the hidden states dimension dc of the character bidirectional LSTM is 100, i.e., 50 in each direction. We apply dropout (Srivastava et al., 2014) on token representations with a rate of 0.5. For encoding and refinement layers, the dimension of the hidden state dh of bidirectional LSTMs is 600, i.e., 300 in each direction. We apply dropout (l) on hidden states ht with a rate of 0.5 before feeding into refinement layers. The number of refinement layers L is just 1. We optimize our model by app"
2021.findings-acl.164,D19-1326,0,0.056138,"Missing"
2021.findings-acl.164,D14-1162,0,0.0940471,"of target label types. For NER datasets, we count types with the IOBES labeling scheme. Street Journal (WSJ) dataset (Marcus et al., 1993) and the Universal Dependencies (UD) v2.2 English dataset for POS tagging experiments. The only data pre-processing that we have performed is replacing digital tokens with a special token. And we convert labels to the IOBES labeling scheme (Ramshaw and Marcus, 1995; Ratinov and Roth, 2009) on NER datasets. The dataset statistics are provided in Table 1. 4.2 Hyper-parameter Settings Following Cui and Zhang (2019) and Jie and Lu (2019), 100-dimensional Glove (Pennington et al., 2014) word embeddings are utilized for all the English experiments, and 300-dimensional FastText (Mikolov et al., 2018) word embeddings are employed for Chinese experiments. The dimension of character embeddings is 30, and the hidden states dimension dc of the character bidirectional LSTM is 100, i.e., 50 in each direction. We apply dropout (Srivastava et al., 2014) on token representations with a rate of 0.5. For encoding and refinement layers, the dimension of the hidden state dh of bidirectional LSTMs is 600, i.e., 300 in each direction. We apply dropout (l) on hidden states ht with a rate of 0."
2021.findings-acl.164,N18-1202,0,0.241169,"ent (SGD) with decaying learning rate ητ = η0 /(1 + 0.075 · τ ), where τ is the index of the current epoch, and the initial learning rate η0 for Chinese experiments without contextual word representations is 0.05, and for all the other experiments we use 0.1. The weight decay rate is 10−8 , the momentum is 0.15, the batch size is 10, the number of epochs is 100, and gradients exceed 5 will be clipped. In addition, since the pre-trained contextualized word embeddings technique is widely accepted as a new fundamental utility of natural language processing, we also conduct experiments with ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). In these settings, tokens are represented as xt = [wt , ct , et ], where et is the contextual word representation. ELMo vectors are obtained by averaging output vectors over all layers of ELMo. For English experiments, we use the original checkpoint, and use the checkpoints provided by Che et al. (2018) for Chinese experiments. BERT representations are the averages all BERT subword embeddings in the last four layers. Following Li et al. (2020b) and Li et al. (2020a), we utilize bert-large-cased and hfl/chinese-bert-wwm checkpoints for English and Chinese experi"
2021.findings-acl.164,W13-3516,0,0.0423898,"Missing"
2021.findings-acl.164,W95-0107,0,0.625899,"lish 38,219 / 5,527 / 5,462 12,544 / 2,003 / 2,078 45 50 Table 1: Dataset statistics, where the “Sentences” column displays the number of sentences in train/dev/test split respectively, the |Y |column displays the number of target label types. For NER datasets, we count types with the IOBES labeling scheme. Street Journal (WSJ) dataset (Marcus et al., 1993) and the Universal Dependencies (UD) v2.2 English dataset for POS tagging experiments. The only data pre-processing that we have performed is replacing digital tokens with a special token. And we convert labels to the IOBES labeling scheme (Ramshaw and Marcus, 1995; Ratinov and Roth, 2009) on NER datasets. The dataset statistics are provided in Table 1. 4.2 Hyper-parameter Settings Following Cui and Zhang (2019) and Jie and Lu (2019), 100-dimensional Glove (Pennington et al., 2014) word embeddings are utilized for all the English experiments, and 300-dimensional FastText (Mikolov et al., 2018) word embeddings are employed for Chinese experiments. The dimension of character embeddings is 30, and the hidden states dimension dc of the character bidirectional LSTM is 100, i.e., 50 in each direction. We apply dropout (Srivastava et al., 2014) on token repres"
2021.findings-acl.164,W09-1119,0,0.111709,"2 12,544 / 2,003 / 2,078 45 50 Table 1: Dataset statistics, where the “Sentences” column displays the number of sentences in train/dev/test split respectively, the |Y |column displays the number of target label types. For NER datasets, we count types with the IOBES labeling scheme. Street Journal (WSJ) dataset (Marcus et al., 1993) and the Universal Dependencies (UD) v2.2 English dataset for POS tagging experiments. The only data pre-processing that we have performed is replacing digital tokens with a special token. And we convert labels to the IOBES labeling scheme (Ramshaw and Marcus, 1995; Ratinov and Roth, 2009) on NER datasets. The dataset statistics are provided in Table 1. 4.2 Hyper-parameter Settings Following Cui and Zhang (2019) and Jie and Lu (2019), 100-dimensional Glove (Pennington et al., 2014) word embeddings are utilized for all the English experiments, and 300-dimensional FastText (Mikolov et al., 2018) word embeddings are employed for Chinese experiments. The dimension of character embeddings is 30, and the hidden states dimension dc of the character bidirectional LSTM is 100, i.e., 50 in each direction. We apply dropout (Srivastava et al., 2014) on token representations with a rate of"
2021.findings-acl.164,2020.acl-demos.38,0,0.0284906,"We apply the Viterbi algorithm (Forney, 1973) to efficiently search for the most probable label sequences on the decoding stage. ˆ = arg max p (y 0 |h(L+1) ) y (14) y 0 ∈Y n 3.4 Complexity and Implementation Tricks One concern regarding our proposed method is its computational complexity, as it requires to compute not only the partition function but also the marginal probability. Calculating the partition function, as in Equation 8, is the well-known bottleneck of CRF computation. And this is commonly achieved through reducing potential matrices by applying matrix multiplications. Similar to Rush (2020), we make use of the associative property of matrix multiplication to accelerate computation. The product of multiplying matrices A, B, C, and D is equivalent to the product of AB and CD. Leveraging the power of GPU to compute AB and CD in parallel, and recursively applying this trick, we can reduce the time complexity of obtainP|B| ing the partition function from O ( i=1 |x|i ) to P|B| O ( i=1 log |x|i ), where |x|i is the length of i-th sentence in batch B. Moreover, instead of padding the sequence length |xi |out to the nearest power of two as Rush (2020) does, we pre-compile argument indic"
2021.findings-acl.164,D17-1283,0,0.114274,"019). Notably, the model of Jie and Lu (2019) relies on external dependency annotations, whereas our model requires no external knowledge4 . In the case of employing ELMo, our model outperforms Jie and Lu (2019) by 0.11 F1 score. On the CoNLL 2003 English dataset, our model performs worse than these baseline models, but, with ELMo, it outperforms Jie and Lu (2019) and 3 https://github.com/speedcell4/refiner In this paper, we use “external knowledge” to denote any additional resources other than word embeddings and contextual word representations. 1877 4 Model EK P R F1 Chiu and Nichols (2016) Strubell et al. (2017) Li et al. (2017) Ghaddar and Langlais (2018) Fisher and Vlachos (2019) Cui and Zhang (2019) Yan et al. (2019) Jie and Lu (2019) Our Method X X X - 86.04 88.00 88.53 88.71 86.53 86.50 88.50 88.60 86.28 86.84 87.21 87.95 87.59 88.16 88.43 88.52 88.65 Yan et al. (2019) [E] Jie and Lu (2019) [E] Our Method [E] X - 89.59 89.51 90.17 90.48 89.78 89.88 89.99 Devlin et al. (2019) [B] Fisher and Vlachos (2019) [B] Li et al. (2020b) [B] Yu et al. (2020)[B] Our Method [B] - 90.01 92.98 91.1 90.00 88.35 89.95 91.5 91.17 89.16 89.71 91.11 91.3 90.93 our model significantly outperforms them by 1.41 F1 scor"
2021.findings-acl.164,D18-1279,0,0.0300428,"Missing"
2021.findings-acl.164,N18-1089,0,0.0439197,"Missing"
2021.findings-acl.164,2020.acl-main.577,0,0.129141,"to denote any additional resources other than word embeddings and contextual word representations. 1877 4 Model EK P R F1 Chiu and Nichols (2016) Strubell et al. (2017) Li et al. (2017) Ghaddar and Langlais (2018) Fisher and Vlachos (2019) Cui and Zhang (2019) Yan et al. (2019) Jie and Lu (2019) Our Method X X X - 86.04 88.00 88.53 88.71 86.53 86.50 88.50 88.60 86.28 86.84 87.21 87.95 87.59 88.16 88.43 88.52 88.65 Yan et al. (2019) [E] Jie and Lu (2019) [E] Our Method [E] X - 89.59 89.51 90.17 90.48 89.78 89.88 89.99 Devlin et al. (2019) [B] Fisher and Vlachos (2019) [B] Li et al. (2020b) [B] Yu et al. (2020)[B] Our Method [B] - 90.01 92.98 91.1 90.00 88.35 89.95 91.5 91.17 89.16 89.71 91.11 91.3 90.93 our model significantly outperforms them by 1.41 F1 score with BERT. Moreover, on the OntoNotes 5.0 Chinese dataset, our model constantly outperforms the best previous work (Jie and Lu, 2019) by 0.65 F1 score without utilizing external knowledge. Besides, we can notice initializing character embeddings with our trick remarkably improves model performance by 0.76 F1 score on the OntoNotes 4.0 Chinese dataset, even this improvement reduces to only 0.00 and 0.20 F1 scores on ELMo and BERT experiments."
2021.findings-acl.164,D17-1056,0,0.0258509,"ll possible spans and to utilize a biaffine classifier to assign category labels to them. Besides, the widespread use of contextual word representations, e.g., ELMo (Peters et al., 2018), Flair (Akbik et al., 2018), and BERT (Devlin et al., 2019), greatly improves the performance of NER models and they are accepted as new fundamental techniques of natural language processing. Intuitively speaking, the refinement mechanism provides the models with additional chances to revise previous decisions. In existing work, this method was successfully applied to various tasks, e.g., text classification (Yu et al., 2017), sequential labeling (Cui and Zhang, 2019; Lyu et al., 2019), machine translation (Lee et al., 2018), and question answering (Nema et al., 2019). Our work is not the first attempt of introducing refinement mechanism to sequential labeling tasks. Cui and Zhang (2019) relied on locally normalized attention to softly refine hidden representations layer by layer, while Liu et al. (2019a) chose to discretely filter out target-irrelevant semantic aspects and thus could be considered as a hard refinement mechanism. 6 Conclusion Motivated by the structured attention, we enhanced the previous refineme"
2021.findings-acl.164,P18-1030,0,0.266715,"ge processing. Figure 1 shows two examples of sequential labeling tasks. Early studies focused on introducing rich features to improve performance. For example, to handle out-of-vocabulary words by introducing morphological features, Lample et al. (2016) and Ma and Hovy (2016) leveraged character-level features, whereas Heinzerling and Strube (2019) exploited subword-level features. Moreover, introducing long-term dependency features is also found to be beneficial for sequential labeling. Jie and Lu (2019) attempted to explicitly exploit dependency relations with additional annotations, while Zhang et al. (2018) and Chen et al. (2019) endeavored to learn these relations implicitly with more complex encoders. ∗ O This work was done when the first author was at NAIST. However, as Tishby and Zaslavsky (2015) pointed out, features are not created equal, only the target-relevant features are profitable for improving model performance. Recently, Cui and Zhang (2019) proposed a hierarchically-refined label attention network (LAN), which explicitly leverages label embeddings and captures long-term label dependency relations through multiple refinements layers. Individually picking up the most likely label at"
2021.findings-acl.164,P18-1144,0,0.01654,"to O (maxi log |x|i ). We release our CRF implementation with these two tricks as an independent library1 for future study and use. 3.5 Character Embeddings Initialization We describe a trick for Chinese character embeddings initialization. The most striking difference between Chinese and English is that the minimal semantic units, i.e., sememes, of Chinese are characters instead of words or subwords. The character vocabulary size of Chinese, e.g., around 2,000 on the OntoNote 5.0 dataset, is markedly larger than English, e.g., around 100 on the OntoNotes 5.0 English dataset. Existing models (Zhang and Yang, 2018; Li et al., 2020a) generally focused on introducing additional pre-trained character embeddings on the top of lexicon embeddings, and attempted to selectively leverage information from both of them according to the different word segmentation schemes. However, we notice that most of these characters already exist in the word vocabulary as single-character words, thus we employ a randomly initialized orthogonal matrix2 to project the pre-trained word embeddings into the same dimension as the character embeddings, and use these projected embeddings for initialization. 4 Experiments 4.1 Datasets"
2021.mrl-1.2,E17-1088,0,0.0199535,"s the ISO-639-3 language code for Na. 6 8 They are tuned on a small subset of de-en or fr-en data. Note that Griko is not included in ISO-639-3, and “grk” is an arbitrary (non-assigned) designator used in this paper. 9 18 Na raw cln njɤ˧ |ɑ˩ʁo˧ |ə˧si˧-ɳɯ˧ ʐwɤ˩qʰv˩mv˩-hĩ˧ lɑ˩ ɲi˩ mæ˩! njɤ˧ ɑ˩ʁo˧ ə˧si˧-ɳɯ˧ ʐwɤ˩qʰv˩mv˩-hĩ˧ lɑ˩ ɲi˩ mæ˩ English raw cln It (i.e. this story) is only what (we’ve) heard our great-grandmother tell. it is only what heard our great grandmother tell Table 2: An example Na–English parallel sentence before and after pre-processing (“raw” vs. “cln”) processing code used in Adams et al. (2017) with minor modifications.10 3.1.2 common problems for endangered languages. 3.2 Baselines Shipibo-Konibo We compare our model against various crosslingual models that are trained on a parallel corpus. First, we compare our model against a recently-proposed word-alignment model based on mBERT (Dou and Neubig, 2021).12 It fine-tunes mBERT on parallel corpora using various crosslingual objectives, and achieves state-of-the-art performance on word alignment tasks across many language pairs. We also include Levy et al. (2017), Luong et al. (2015a), and Sabet et al. (2020) as recent word embedding"
2021.mrl-1.2,2020.lrec-1.356,0,0.0222913,"word (Anastasopoulos et al., 2018). Unlike the Na and Shipibo-Konibo data sets, Griko and Italian are very similar in many ways: they both use the Latin script and have similar syntax. Therefore, the main challenge comes from the data paucity and inconsistent orthography in Griko, both of which are 10 We use white space as the word delimiter and keep all tones in Na sentences, as removing tones increases polysemy in the bilingual dictionary we use for evaluation. For Chinese, we perform word segmentation using the Stanford Word Segmenter (Chang et al., 2008) after data cleaning. 11 Recently, Bustamante et al. (2020) attempted to scrape monolingual data from PDF documents, but the size of the resulting data is still too small (22k sentences) to apply the latest pretraining methods. 12 We used the bert-base-multilingual-cased model, following the original paper. 13 We use Fast Align to generate the alignment. 14 Besides, Wada et al. (2019) show that the mapping models perform very poorly on low-resource conditions. Based on these findings, we did not include them as our baselines. 15 Except for the mBERT baseline, which has its pre-defined vocabulary and word embedding dimension, i.e. 768. 19 (Brown et al."
2021.mrl-1.2,N19-1391,0,0.0192352,"l too small (22k sentences) to apply the latest pretraining methods. 12 We used the bert-base-multilingual-cased model, following the original paper. 13 We use Fast Align to generate the alignment. 14 Besides, Wada et al. (2019) show that the mapping models perform very poorly on low-resource conditions. Based on these findings, we did not include them as our baselines. 15 Except for the mBERT baseline, which has its pre-defined vocabulary and word embedding dimension, i.e. 768. 19 (Brown et al., 1993), and still serve as de facto standard models to generate word alignments (Cao et al., 2020; Aldarmaki and Diab, 2019). For all the baselines, we use the authors’ implementations.16 3.3 Experimental Settings and Evaluation In our experiments, we train cross-lingual embeddings for five low-resource language pairs: Griko– Italian, Shipibo-Konibo–Spanish and Na–{French, Chinese, English}. For the Griko–Italian pair, we evaluate models on a cross-lingual word alignment task and report alignment accuracy (1−AER). We use the gold alignments manually annotated over the 330 Griko–Italian sentences. To produce alignments using Giza++ and Fast Align, we train them on the 330 sentences with or without additional 10k sen"
2021.mrl-1.2,W08-0336,0,0.148786,"Missing"
2021.mrl-1.2,C18-1214,0,0.0233141,"monolingual corpus for the language,11 but for cross-lingual resources there are two parallel corpora aligned with Spanish, which are extracted from the Bible and educational books (Galarreta et al., 2017). Similar to Na, Shipibo-Konibo is an SOV language with very rich morphology (Valenzuela, 1997; Vasquez et al., 2018), whereas Spanish is an SVO language. 3.1.3 Griko Griko is a Greek dialect spoken in southern Italy, and “severely endangered” according to UNESCO. There is no large-scale monolingual corpus of Griko, but there are two Griko–Italian parallel corpora (Zanon Boito et al., 2018; Anastasopoulos et al., 2018), with the smaller one including gold word alignment annotations. However, Griko has never had a consistent orthography, and hence its tokenisation and word segmentation differ across these corpora: the smaller data set is based on orthographic conventions from Italian, while the larger one follows the concept of a phonological word (Anastasopoulos et al., 2018). Unlike the Na and Shipibo-Konibo data sets, Griko and Italian are very similar in many ways: they both use the Latin script and have similar syntax. Therefore, the main challenge comes from the data paucity and inconsistent orthograph"
2021.mrl-1.2,2020.emnlp-main.42,0,0.0366598,"Missing"
2021.mrl-1.2,P18-1073,0,0.0194224,"presentations even in extremely lowresource conditions. Furthermore, our model also works well on high-resource conditions, achieving state-of-the-art performance on a German-English word-alignment task.1 1 Introduction Cross-lingual word embedding learning has the goal of learning representations for words of different languages in a common space (Mikolov et al., 2013b; Conneau et al., 2018; Levy et al., 2017). Cross-lingual representations are beneficial for finding correspondences between languages, and are utilised in many downstream tasks such as machine translation (Lample et al., 2018; Artetxe et al., 2018b) and cross-lingual named entity recognition (Xie et al., 2018). ∗ This work was partially done at Nara Institute of Science and Technology. 1 Our code is available at https://github.com/ twadada/multilingual-nlm 16 Proceedings of the 1st Workshop on Multilingual Representation Learning, pages 16–31 November 11, 2021. ©2021 Association for Computational Linguistics 2 Methodology 2.1 ??? (e.g.) ! ?? (????) = ??? (????) + ?(?@??? , ??) ? ?! ℓ Model Architecture (1) s us1 ..., usN = f (r1s ..., rN ), (2) + ??? ??? ?! ! ??? M +1 ∏ (3) − → p(yit |h i , us ), i=1 − → − → → t hi = − g t ( h i−1 , ri"
2021.mrl-1.2,J82-2005,0,0.663283,"Missing"
2021.mrl-1.2,Q19-1038,0,0.0362526,"Missing"
2021.mrl-1.2,Q17-1010,0,0.0443519,"posed model is based an LSTM2 encoderdecoder model with attention (Luong et al., 2015b), trained with translation and reconstruction objectives (Figure 1). Suppose our model encodes a sentence ⟨xs1 ..., xsN ⟩ in the source language s and det ⟩ in the target language codes a sentence ⟨y1t ..., yM t. The encoder employs bi-directional LSTMs f , which are shared among all languages: ris = E s xsi , = 2.2 Shared Subword Embeddings (4) To incorporate orthographic information into word embeddings, we propose a simple yet effective method to combine word and subword embeddings, inspired by FastText (Bojanowski et al., 2017). For each word wiℓ , we calculate its subword˜wℓ as follows: aware word embedding E i (5) (6) (7) ˜ ℓ = E ℓ + F(Zk∈Q(w ) ), E wi wi i 2 We use LSTM rather than Transformer (Vaswani et al., 2017) because LSTM is less sensitive to hyper-parameters and performs better at translation under extremely low-resource conditions (Zhang et al., 2020). 3 In our preliminary experiments, we have found that learning language-specific decoders improves cross-lingual embeddings for distant languages, e.g. SOV and SVO languages. (8) 4 This simplifies the lexical model proposed by Nguyen and Chiang (2018). Whil"
2021.mrl-1.2,W03-0301,0,0.414717,"Missing"
2021.mrl-1.2,P19-1314,0,0.0598251,"Missing"
2021.mrl-1.2,W15-1521,0,0.227358,"rd embeddings of xsi . Given the en→ − coder states us , the decoders − g t and ← g t translate (when s = t) or reconstruct (when s = t) the input sentence left-to-right and right-to-left. We train separate decoders for each language and direction to allow for the differences of word order.3 Similar to ELMo (Peters et al., 2018), the decoding is performed independently in both directions: t p(y1t ..., yM , EOS) = ? ' ?! ( + ?(?) Figure 1: Our proposed model. where xsi rit = E t yit , ?ℓ + ??, ? ) Attention ((???, ? , ? Our proposed model is based an LSTM2 encoderdecoder model with attention (Luong et al., 2015b), trained with translation and reconstruction objectives (Figure 1). Suppose our model encodes a sentence ⟨xs1 ..., xsN ⟩ in the source language s and det ⟩ in the target language codes a sentence ⟨y1t ..., yM t. The encoder employs bi-directional LSTMs f , which are shared among all languages: ris = E s xsi , = 2.2 Shared Subword Embeddings (4) To incorporate orthographic information into word embeddings, we propose a simple yet effective method to combine word and subword embeddings, inspired by FastText (Bojanowski et al., 2017). For each word wiℓ , we calculate its subword˜wℓ as follows:"
2021.mrl-1.2,D15-1166,0,0.0790593,"rd embeddings of xsi . Given the en→ − coder states us , the decoders − g t and ← g t translate (when s = t) or reconstruct (when s = t) the input sentence left-to-right and right-to-left. We train separate decoders for each language and direction to allow for the differences of word order.3 Similar to ELMo (Peters et al., 2018), the decoding is performed independently in both directions: t p(y1t ..., yM , EOS) = ? ' ?! ( + ?(?) Figure 1: Our proposed model. where xsi rit = E t yit , ?ℓ + ??, ? ) Attention ((???, ? , ? Our proposed model is based an LSTM2 encoderdecoder model with attention (Luong et al., 2015b), trained with translation and reconstruction objectives (Figure 1). Suppose our model encodes a sentence ⟨xs1 ..., xsN ⟩ in the source language s and det ⟩ in the target language codes a sentence ⟨y1t ..., yM t. The encoder employs bi-directional LSTMs f , which are shared among all languages: ris = E s xsi , = 2.2 Shared Subword Embeddings (4) To incorporate orthographic information into word embeddings, we propose a simple yet effective method to combine word and subword embeddings, inspired by FastText (Bojanowski et al., 2017). For each word wiℓ , we calculate its subword˜wℓ as follows:"
2021.mrl-1.2,N18-1031,0,0.0197724,"Text (Bojanowski et al., 2017). For each word wiℓ , we calculate its subword˜wℓ as follows: aware word embedding E i (5) (6) (7) ˜ ℓ = E ℓ + F(Zk∈Q(w ) ), E wi wi i 2 We use LSTM rather than Transformer (Vaswani et al., 2017) because LSTM is less sensitive to hyper-parameters and performs better at translation under extremely low-resource conditions (Zhang et al., 2020). 3 In our preliminary experiments, we have found that learning language-specific decoders improves cross-lingual embeddings for distant languages, e.g. SOV and SVO languages. (8) 4 This simplifies the lexical model proposed by Nguyen and Chiang (2018). While they apply separate output layers to the weighted average of word embeddings and hidden states, we have found that sharing the same output layer (E t ) performed the best, suggesting that the optimal model architecture is different between word and sentence translations. 17 where F(·) denotes the subword encoding function; Zk denotes the k-th subword embedding and Q(wi ) denotes the indices of the subwords included in wi . The subword embeddings Z are shared among all languages, capturing orthographic similarities across languages. For the encoding function F(·), we experiment with two"
2021.mrl-1.2,L18-1697,0,0.0167371,"we train them on the same data, and align each word in a sentence to the closest word in its translation using static or contextualised word embeddings.18 To calculate word similarity, we use cross-domain similarity local scaling (Conneau et al., 2018): 1 ∑ CSLS(x, y) = 2 cos(x, y) − cos(x, yt ) K yt ∈NT (x) − 1 K ∑ each source word in a bilingual dictionary, we extract the k nearest words from the whole target vocabulary and see whether they are listed as translations in the dictionary. We set k to 1 or 5, and report P@1 and P@5. For evaluation, we use a Shipibo-Konibo–Spanish dictionary20 (Maguiño-Valencia et al., 2018) and Na–French– Chinese–English dictionaries (Michaud, 2018). Based on extracting words that are present in the parallel corpora, we identified 79, 262, 215 and 87 word pairs for Shipibo-Konibo–Spanish, Na–French, Na–Chinese, and Na–English.21 To perform BLI with GIZA++ and Fast Align, we use their source-to-target probability table. We also try using the result of bidirectional word alignments, aligning each word to the most frequently aligned words to it.22 For the neural baselines and our model, we use static word embeddings and employ CSLS to measure the word embedding similarities. To obt"
2021.mrl-1.2,J03-1002,0,0.0434436,"d BIS2V trains a Continuous Bag-of-Words (CBOW) model that predicts a target word from the rest of the sentence and its parallel sentence. Sabet et al. (2020) and Marie and Fujita (2019) show that these joint learning models perform better than mapping-based methods, which align monolingual word embeddings cross-lingually.14 Regarding the vocabulary size and word embedding dimension, we always use the same values for all the baselines and our model, to ensure fairness.15 In addition to these neural baselines, we also compare our model against statistical word alignment methods, namely GIZA++ (Och and Ney, 2003) and Fast Align (Dyer et al., 2013). These are pre-neural methods based on the IBM models Shipibo-Konibo is an indigenous language spoken by around 35,000 native speakers in the Amazon region of Peru (Vasquez et al., 2018), and is “definitely endangered” according to the UNESCO’s Atlas of the World’s Languages in Danger (Moseley, 2010). There is no large monolingual corpus for the language,11 but for cross-lingual resources there are two parallel corpora aligned with Spanish, which are extracted from the Bible and educational books (Galarreta et al., 2017). Similar to Na, Shipibo-Konibo is an"
2021.mrl-1.2,P19-1312,0,0.0201844,"Sabet et al. (2020) as recent word embedding baselines, which we denote as SENTID, BIVEC and BIS2V, respectively. All of these baselines are very similar in terms of methodology: SENTID trains a Skip-Gram model that predicts a sentence ID (which is assigned to each set of parallel sentences) from the component words; BIVEC trains a Skip-Gram model that predicts the context cross-lingually based on the wordalignment information;13 and BIS2V trains a Continuous Bag-of-Words (CBOW) model that predicts a target word from the rest of the sentence and its parallel sentence. Sabet et al. (2020) and Marie and Fujita (2019) show that these joint learning models perform better than mapping-based methods, which align monolingual word embeddings cross-lingually.14 Regarding the vocabulary size and word embedding dimension, we always use the same values for all the baselines and our model, to ensure fairness.15 In addition to these neural baselines, we also compare our model against statistical word alignment methods, namely GIZA++ (Och and Ney, 2003) and Fast Align (Dyer et al., 2013). These are pre-neural methods based on the IBM models Shipibo-Konibo is an indigenous language spoken by around 35,000 native speake"
2021.mrl-1.2,2020.acl-main.156,0,0.0813123,"Missing"
2021.mrl-1.2,W03-0320,0,0.167078,"Missing"
2021.mrl-1.2,N18-1202,0,0.055646,"r¯is + hi in Eqn. (5) before the linear transformation, with the dropout rate all set to 0.5. We show that this strong regularisation leads to better cross-lingual representations. denotes a one-hot vector. In cross-lingual tasks, we employ ris and usi as the static and contextualised word embeddings of xsi . Given the en→ − coder states us , the decoders − g t and ← g t translate (when s = t) or reconstruct (when s = t) the input sentence left-to-right and right-to-left. We train separate decoders for each language and direction to allow for the differences of word order.3 Similar to ELMo (Peters et al., 2018), the decoding is performed independently in both directions: t p(y1t ..., yM , EOS) = ? ' ?! ( + ?(?) Figure 1: Our proposed model. where xsi rit = E t yit , ?ℓ + ??, ? ) Attention ((???, ? , ? Our proposed model is based an LSTM2 encoderdecoder model with attention (Luong et al., 2015b), trained with translation and reconstruction objectives (Figure 1). Suppose our model encodes a sentence ⟨xs1 ..., xsN ⟩ in the source language s and det ⟩ in the target language codes a sentence ⟨y1t ..., yM t. The encoder employs bi-directional LSTMs f , which are shared among all languages: ris = E s xsi ,"
2021.mrl-1.2,E17-2025,0,0.0608051,"Missing"
2021.mrl-1.2,P19-1300,1,0.812151,"delimiter and keep all tones in Na sentences, as removing tones increases polysemy in the bilingual dictionary we use for evaluation. For Chinese, we perform word segmentation using the Stanford Word Segmenter (Chang et al., 2008) after data cleaning. 11 Recently, Bustamante et al. (2020) attempted to scrape monolingual data from PDF documents, but the size of the resulting data is still too small (22k sentences) to apply the latest pretraining methods. 12 We used the bert-base-multilingual-cased model, following the original paper. 13 We use Fast Align to generate the alignment. 14 Besides, Wada et al. (2019) show that the mapping models perform very poorly on low-resource conditions. Based on these findings, we did not include them as our baselines. 15 Except for the mBERT baseline, which has its pre-defined vocabulary and word embedding dimension, i.e. 768. 19 (Brown et al., 1993), and still serve as de facto standard models to generate word alignments (Cao et al., 2020; Aldarmaki and Diab, 2019). For all the baselines, we use the authors’ implementations.16 3.3 Experimental Settings and Evaluation In our experiments, we train cross-lingual embeddings for five low-resource language pairs: Griko–"
2021.mrl-1.2,2020.sltu-1.13,0,0.0499318,"Missing"
2021.mrl-1.2,D18-1034,0,0.0217819,", our model also works well on high-resource conditions, achieving state-of-the-art performance on a German-English word-alignment task.1 1 Introduction Cross-lingual word embedding learning has the goal of learning representations for words of different languages in a common space (Mikolov et al., 2013b; Conneau et al., 2018; Levy et al., 2017). Cross-lingual representations are beneficial for finding correspondences between languages, and are utilised in many downstream tasks such as machine translation (Lample et al., 2018; Artetxe et al., 2018b) and cross-lingual named entity recognition (Xie et al., 2018). ∗ This work was partially done at Nara Institute of Science and Technology. 1 Our code is available at https://github.com/ twadada/multilingual-nlm 16 Proceedings of the 1st Workshop on Multilingual Representation Learning, pages 16–31 November 11, 2021. ©2021 Association for Computational Linguistics 2 Methodology 2.1 ??? (e.g.) ! ?? (????) = ??? (????) + ?(?@??? , ??) ? ?! ℓ Model Architecture (1) s us1 ..., usN = f (r1s ..., rN ), (2) + ??? ??? ?! ! ??? M +1 ∏ (3) − → p(yit |h i , us ), i=1 − → − → → t hi = − g t ( h i−1 , ri−1 ), t p(BOS, y1t ..., yM )= M ∏ ← − t s p(yM −i |h M −i , u ),"
2021.mrl-1.2,P18-2037,0,0.0387565,"Missing"
2021.mrl-1.2,N15-1104,0,0.0870183,"Missing"
2021.mrl-1.2,W17-2619,0,0.0616142,"Missing"
2021.mrl-1.2,J96-1001,0,0.692304,"and report the best score; we fine-tune the mBERT baseline for 40,000 steps26 with 20 checkpoints, and train SENTID and BIS2V for 1,000 epochs with 100 checkpoints to ensure convergence. For BIVEC, we increase the training corpus size by 20 times by duplicating the sentences and train the model for 50 epochs with 50 checkpoints.27 For our model, on the other hand, we use a simple early-stopping criterion that doesn’t require external data. First, we build a pseudo bilingual dictionary from the training data. To retrieve pseudo bilingual word pairs, we compute the Dice Coeﬀicient (Dice, 1945; Smadja et al., 1996) and extract pairs of words that appear ≥ 3 times in each language and whose Dice Coeﬀicient is ≥ 0.8 across two languages. We perform model selection based on the BLI performance on this pseudo dictionary. 3.5 Word src–tgt nru–en nru–fr nru–zh +SWave +SWcnn bi multi bi mult bi multi 30.2 27.3 31.6 34.2 28.7 36.9 32.0 29.9 36.7 37.8 29.8 40.1 35.6 32.1 38.5 40.5 30.7 40.8 Table 4: Our model performance (P@1) on BLI when the model is trained on two and four languages (“bi” vs. “multi”). All scores are averaged over three runs. our model. Compared to the neural baselines, our model performs bett"
2021.mrl-1.2,2020.acl-main.146,0,0.0389663,"Missing"
2021.mrl-1.2,P17-1179,0,0.0515976,"Missing"
2021.mrl-1.2,2020.emnlp-main.43,0,0.0841257,"Missing"
2021.starsem-1.20,W18-4912,0,0.0122904,"11 classes to achieve the state-ofthe-art performance on implicit discourse relation classiﬁcation task in PDTB framework. Their work was motivated by the method taken by Devlin et al. (2019) to pretrain BERT, which is called “next sentence prediction task” (NSP). In the process of pretraining using NSP, the model is presented with pairs of sentences. The model predicts whether the second sentence is the actual subsequent sentence. NSP enables BERT to represent a pair of sentences by packing them together as a single sequence. Some studies have focused on discourse structure in AMR framework. Donatelli et al. (2018) enhances AMR by annontating tense and aspect phenomena at discourse-level. The work by O’Gorman et al. (2018) targets relations of sentences and provides annotation of coreference in multi-sentence AMR corpus. Yet, neither the structure of the complex sentence constructions nor the coherence relations between subordinate and matrix clauses have been much of a concern in this framework. 3 GHSHQGHQFWUHH FKDUWHUHG DGYFO QVXEM GLVUXSWHG PDUN 6LQFH QVXEMSDVV IOLJKWV FRPSDQ SODQH WKH D FRS ZHUH DLUOLQH 3DWWHUQ0DWFKLQJ FKDUWHUHG DGYFO QVXEM ZHUH PDUN 6LQFH QVXEMSDVV GLVUXSWHG IOLJKWV FRPSDQ S"
2021.starsem-1.20,P14-1134,0,0.0326336,"mong AMR parsers which are aware of syntactic structures, CAMR (Wang et al., 2015) directly transforms the result of dependency parsing into an AMR graph with transition-based algorithm. As Figure 2(a) shows an example parse with CAMR, existing parsers have trouble capturing the relation Introduction Abstract Meaning Representation (AMR) is a sentence-level meaning representation based on predicate argument structure (Banarescu et al., 2013). AMR Parsing is the task of transforming a sentence into an AMR graph with nodes and edges, each representing a concept or relation. While early studies (Flanigan et al., 2014; Wang et al., 2015; Artzi et al., 2015; Pust et al., 2015) used dependency parsers to integrate syntactic features to their models, recent deep neural network-based approaches (Konstas et al., 2017; Peng et al., 2017; Zhang et al., 2019; Cai and Lam, 2020) tend to encode the input sentence as a sequence without considering its syntactic structure. Generally speaking, syntactic and semantic structures share much in common. It is assumed 1Code and resource are available at https://github. com/yama-yuki/skeletal-amr. 212 Proceedings of the 10th Conference on Lexical and Computational Semantics,"
2021.starsem-1.20,D15-1198,0,0.0183329,"tic structures, CAMR (Wang et al., 2015) directly transforms the result of dependency parsing into an AMR graph with transition-based algorithm. As Figure 2(a) shows an example parse with CAMR, existing parsers have trouble capturing the relation Introduction Abstract Meaning Representation (AMR) is a sentence-level meaning representation based on predicate argument structure (Banarescu et al., 2013). AMR Parsing is the task of transforming a sentence into an AMR graph with nodes and edges, each representing a concept or relation. While early studies (Flanigan et al., 2014; Wang et al., 2015; Artzi et al., 2015; Pust et al., 2015) used dependency parsers to integrate syntactic features to their models, recent deep neural network-based approaches (Konstas et al., 2017; Peng et al., 2017; Zhang et al., 2019; Cai and Lam, 2020) tend to encode the input sentence as a sequence without considering its syntactic structure. Generally speaking, syntactic and semantic structures share much in common. It is assumed 1Code and resource are available at https://github. com/yama-yuki/skeletal-amr. 212 Proceedings of the 10th Conference on Lexical and Computational Semantics, pages 212–221 August 5–6, 2021, Bangkok"
2021.starsem-1.20,C18-1048,0,0.0121876,"HUHG DSODQHWRIOWKHH[HFXWLYHVEDFNWRWKH:HVW&RDVW /H[LFDO 6QWDFWLF3URFHVVLQJ Related Works While our focus is on clause-level relation of complex sentence constructions, not much study has been done speciﬁcally on this topic in AMR framework. Rather, the topic is dealt with in the ﬁeld of discourse structures, where coherence relations between any text segments are the main focus. In the studies of discourse parsing, various attempts have been made to capture coherence relations between pairs of sentences or clauses (Pitler et al., 2008; Rutherford et al., 2017; Qin et al., 2017; Bai and Zhao, 2018). These works basically rely on discourse frameworks such as Rhetorical Structure Theory (RST; Thompson and Mann 1987) or Penn Discourse Tree Bank (PDTB; Prasad et al. 2008). Most recently, Shi and Demberg (2019) has presented a ﬁnetuning-based approach using the bidirectional encoder representation from transformers (BERT; Devlin et al. 2019). They designed their model to learn 11 classes to achieve the state-ofthe-art performance on implicit discourse relation classiﬁcation task in PDTB framework. Their work was motivated by the method taken by Devlin et al. (2019) to pretrain BERT, which is"
2021.starsem-1.20,P17-1014,0,0.0328637,"a) shows an example parse with CAMR, existing parsers have trouble capturing the relation Introduction Abstract Meaning Representation (AMR) is a sentence-level meaning representation based on predicate argument structure (Banarescu et al., 2013). AMR Parsing is the task of transforming a sentence into an AMR graph with nodes and edges, each representing a concept or relation. While early studies (Flanigan et al., 2014; Wang et al., 2015; Artzi et al., 2015; Pust et al., 2015) used dependency parsers to integrate syntactic features to their models, recent deep neural network-based approaches (Konstas et al., 2017; Peng et al., 2017; Zhang et al., 2019; Cai and Lam, 2020) tend to encode the input sentence as a sequence without considering its syntactic structure. Generally speaking, syntactic and semantic structures share much in common. It is assumed 1Code and resource are available at https://github. com/yama-yuki/skeletal-amr. 212 Proceedings of the 10th Conference on Lexical and Computational Semantics, pages 212–221 August 5–6, 2021, Bangkok, Thailand (online) ©2021 Association for Computational Linguistics FDXVH EHOLHYH $5* $5* JLUO $5* VHHP $5* ER $5* EHOLHYH VHHP $5*"
2021.starsem-1.20,W13-2322,0,0.048075,"sentences, various types of complex sentence are used in human language. This characteristics makes it challenging for existing AMR parsers to capture its structure correctly. Among AMR parsers which are aware of syntactic structures, CAMR (Wang et al., 2015) directly transforms the result of dependency parsing into an AMR graph with transition-based algorithm. As Figure 2(a) shows an example parse with CAMR, existing parsers have trouble capturing the relation Introduction Abstract Meaning Representation (AMR) is a sentence-level meaning representation based on predicate argument structure (Banarescu et al., 2013). AMR Parsing is the task of transforming a sentence into an AMR graph with nodes and edges, each representing a concept or relation. While early studies (Flanigan et al., 2014; Wang et al., 2015; Artzi et al., 2015; Pust et al., 2015) used dependency parsers to integrate syntactic features to their models, recent deep neural network-based approaches (Konstas et al., 2017; Peng et al., 2017; Zhang et al., 2019; Cai and Lam, 2020) tend to encode the input sentence as a sequence without considering its syntactic structure. Generally speaking, syntactic and semantic structures share much in commo"
2021.starsem-1.20,2020.lrec-1.497,0,0.0616752,"Missing"
2021.starsem-1.20,L18-1266,0,0.0220631,"Missing"
2021.starsem-1.20,C18-1313,0,0.0380173,"Missing"
2021.starsem-1.20,2020.acl-main.119,0,0.0111753,"ouble capturing the relation Introduction Abstract Meaning Representation (AMR) is a sentence-level meaning representation based on predicate argument structure (Banarescu et al., 2013). AMR Parsing is the task of transforming a sentence into an AMR graph with nodes and edges, each representing a concept or relation. While early studies (Flanigan et al., 2014; Wang et al., 2015; Artzi et al., 2015; Pust et al., 2015) used dependency parsers to integrate syntactic features to their models, recent deep neural network-based approaches (Konstas et al., 2017; Peng et al., 2017; Zhang et al., 2019; Cai and Lam, 2020) tend to encode the input sentence as a sequence without considering its syntactic structure. Generally speaking, syntactic and semantic structures share much in common. It is assumed 1Code and resource are available at https://github. com/yama-yuki/skeletal-amr. 212 Proceedings of the 10th Conference on Lexical and Computational Semantics, pages 212–221 August 5–6, 2021, Bangkok, Thailand (online) ©2021 Association for Computational Linguistics FDXVH EHOLHYH $5* $5* JLUO $5* VHHP $5* ER $5* EHOLHYH VHHP $5* $5* $5* ER UHOLDEOH UHOLDEOH GRPDLQ $5* JLUO $5*"
2021.starsem-1.20,J05-1004,0,0.158861,"Wikipedia corpus, establishing a new baseline for future works. The developed complex sentence patterns and the corresponding AMR descriptions will be made public1. 1 6 9 ORQJ DGYFO QVXEM 6 9 PDUN DV 9 $5* DGYPRG FRQGLWLRQ 6 DGYPRG DVORQJDV DV 9 RS $5* 6 Figure 1: Representation of as long as-construction in dependency tree (left) and AMR graph (right). that dependency trees and semantic role labeling structures have a strong correlation in that nsubj and dobj can be used interchangeably for ARG0 and ARG1 role (Xia et al., 2019). Since AMR is annotated based on PropBank frames (Palmer et al., 2005), the same could be said for AMR structures. This holds to be true for a simple sentence, which is basically a matrix clause, comprised of a predicate and its arguments. However, it is not always the case with complex sentence constructions, each of which consists of a matrix clause and one or more subordinate clause(s). Consider Figure 1 which shows both dependency and AMR representation of a complex sentence with a subordinator as long as. While variables S’s and V’s are interchangeable between the representations, predicative relations and subordinator itself are expressed quite diﬀerently."
2021.starsem-1.20,E17-1035,0,0.013225,"rse with CAMR, existing parsers have trouble capturing the relation Introduction Abstract Meaning Representation (AMR) is a sentence-level meaning representation based on predicate argument structure (Banarescu et al., 2013). AMR Parsing is the task of transforming a sentence into an AMR graph with nodes and edges, each representing a concept or relation. While early studies (Flanigan et al., 2014; Wang et al., 2015; Artzi et al., 2015; Pust et al., 2015) used dependency parsers to integrate syntactic features to their models, recent deep neural network-based approaches (Konstas et al., 2017; Peng et al., 2017; Zhang et al., 2019; Cai and Lam, 2020) tend to encode the input sentence as a sequence without considering its syntactic structure. Generally speaking, syntactic and semantic structures share much in common. It is assumed 1Code and resource are available at https://github. com/yama-yuki/skeletal-amr. 212 Proceedings of the 10th Conference on Lexical and Computational Semantics, pages 212–221 August 5–6, 2021, Bangkok, Thailand (online) ©2021 Association for Computational Linguistics FDXVH EHOLHYH $5* $5* JLUO $5* VHHP $5* ER $5* EHOLHYH VHHP $5* $5* $5* ER UH"
2021.starsem-1.20,A00-2015,1,0.472139,"(hereinafter referred to as“ skeletal AMRs ”). Then, we provide a pattern matcher which captures clausal relations between a superordinate and subordinate clauses in a complex sentence. Our pattern matching approach faces the problem of syntactic and semantic ambiguities. When a complex sentence has more than one subordinate clause, we need to determine which pair of clauses are related. Consider the following example where two subordinate clauses appear in a single sentence. 199-192783-6849434_0102.3) While there has been studies regarding the syntactic scope of a subordinate clause such as Utsuro et al. (2000), this problem is beyond the scope of this paper. We rely on the output of the dependency parser, which we employ in our pattern matching system, to decide which pair of clauses are syntactically related. Meanwhile, when a subordinator itself is ambiguous between several senses, we need to select the correct type of coherence relation between the clauses. Sentences in (2) show usages of a subordinator since, which is semantically ambiguous between causal and temporal senses. (2) a. Since there is responsibility, we are not afraid. (AMR: bolt12_6455_6561.15) b. Also since he turned 80, people h"
2021.starsem-1.20,prasad-etal-2008-penn,0,0.0730739,"ructions, not much study has been done speciﬁcally on this topic in AMR framework. Rather, the topic is dealt with in the ﬁeld of discourse structures, where coherence relations between any text segments are the main focus. In the studies of discourse parsing, various attempts have been made to capture coherence relations between pairs of sentences or clauses (Pitler et al., 2008; Rutherford et al., 2017; Qin et al., 2017; Bai and Zhao, 2018). These works basically rely on discourse frameworks such as Rhetorical Structure Theory (RST; Thompson and Mann 1987) or Penn Discourse Tree Bank (PDTB; Prasad et al. 2008). Most recently, Shi and Demberg (2019) has presented a ﬁnetuning-based approach using the bidirectional encoder representation from transformers (BERT; Devlin et al. 2019). They designed their model to learn 11 classes to achieve the state-ofthe-art performance on implicit discourse relation classiﬁcation task in PDTB framework. Their work was motivated by the method taken by Devlin et al. (2019) to pretrain BERT, which is called “next sentence prediction task” (NSP). In the process of pretraining using NSP, the model is presented with pairs of sentences. The model predicts whether the second"
2021.starsem-1.20,N15-1040,0,0.0236498,"and one or more subordinate clause(s). Consider Figure 1 which shows both dependency and AMR representation of a complex sentence with a subordinator as long as. While variables S’s and V’s are interchangeable between the representations, predicative relations and subordinator itself are expressed quite diﬀerently. Compared to uniform structures of simple sentences, various types of complex sentence are used in human language. This characteristics makes it challenging for existing AMR parsers to capture its structure correctly. Among AMR parsers which are aware of syntactic structures, CAMR (Wang et al., 2015) directly transforms the result of dependency parsing into an AMR graph with transition-based algorithm. As Figure 2(a) shows an example parse with CAMR, existing parsers have trouble capturing the relation Introduction Abstract Meaning Representation (AMR) is a sentence-level meaning representation based on predicate argument structure (Banarescu et al., 2013). AMR Parsing is the task of transforming a sentence into an AMR graph with nodes and edges, each representing a concept or relation. While early studies (Flanigan et al., 2014; Wang et al., 2015; Artzi et al., 2015; Pust et al., 2015) u"
2021.starsem-1.20,N19-1075,0,0.0152462,"through training classiﬁcation models on data derived from AMR and Wikipedia corpus, establishing a new baseline for future works. The developed complex sentence patterns and the corresponding AMR descriptions will be made public1. 1 6 9 ORQJ DGYFO QVXEM 6 9 PDUN DV 9 $5* DGYPRG FRQGLWLRQ 6 DGYPRG DVORQJDV DV 9 RS $5* 6 Figure 1: Representation of as long as-construction in dependency tree (left) and AMR graph (right). that dependency trees and semantic role labeling structures have a strong correlation in that nsubj and dobj can be used interchangeably for ARG0 and ARG1 role (Xia et al., 2019). Since AMR is annotated based on PropBank frames (Palmer et al., 2005), the same could be said for AMR structures. This holds to be true for a simple sentence, which is basically a matrix clause, comprised of a predicate and its arguments. However, it is not always the case with complex sentence constructions, each of which consists of a matrix clause and one or more subordinate clause(s). Consider Figure 1 which shows both dependency and AMR representation of a complex sentence with a subordinator as long as. While variables S’s and V’s are interchangeable between the representations, predic"
2021.starsem-1.20,D15-1136,0,0.0153082,"(Wang et al., 2015) directly transforms the result of dependency parsing into an AMR graph with transition-based algorithm. As Figure 2(a) shows an example parse with CAMR, existing parsers have trouble capturing the relation Introduction Abstract Meaning Representation (AMR) is a sentence-level meaning representation based on predicate argument structure (Banarescu et al., 2013). AMR Parsing is the task of transforming a sentence into an AMR graph with nodes and edges, each representing a concept or relation. While early studies (Flanigan et al., 2014; Wang et al., 2015; Artzi et al., 2015; Pust et al., 2015) used dependency parsers to integrate syntactic features to their models, recent deep neural network-based approaches (Konstas et al., 2017; Peng et al., 2017; Zhang et al., 2019; Cai and Lam, 2020) tend to encode the input sentence as a sequence without considering its syntactic structure. Generally speaking, syntactic and semantic structures share much in common. It is assumed 1Code and resource are available at https://github. com/yama-yuki/skeletal-amr. 212 Proceedings of the 10th Conference on Lexical and Computational Semantics, pages 212–221 August 5–6, 2021, Bangkok, Thailand (online)"
2021.starsem-1.20,D19-1392,0,0.0260054,"Missing"
2021.starsem-1.20,2020.acl-demos.14,0,0.0127711,"not ﬁnd a copular clause with complex sentence patterns alone. To avoid redundancy of creating additional patterns substituting V’s with copulas for each entry in dictionary, we make an extra pattern outside that describes a copula-complement structure: 3For “John is tall.”, UD treats “tall” as a head of “is”. 216 ^5(*(;AQVXEM` V 4.1 Experimental Setup F 7$*^5(*(;A1eA-` For creating the dataset, we use the lateset release of AMR corpus (LDC2020T02), which provides 59.2k pairs of sentences and AMR graphs. To extract complex sentence constructions from the corpus, we use the Stanza pipeline (Qi et al., 2020) for lexical and syntactic processing of the sentences and employ our pattern matcher with all patterns in the dictionary. In order to check whether the corresponding AMR graph describes the relation we want for each class, we look for alignments between sentence tokens and AMR graphs45. Finally, we split the sentence to obtain a pair of clauses and a subordinator. While the data derived from AMR corpus can be regarded as “supervised”, the amount is relatively small with the total of 1,933 pairs of subordinate and matrix clauses. As it consumes time and money to create more supervised data, we"
2021.starsem-1.20,P17-1093,0,0.0117107,"WKHFRPSDQFKDUWHUHG DSODQHWRIOWKHH[HFXWLYHVEDFNWRWKH:HVW&RDVW /H[LFDO 6QWDFWLF3URFHVVLQJ Related Works While our focus is on clause-level relation of complex sentence constructions, not much study has been done speciﬁcally on this topic in AMR framework. Rather, the topic is dealt with in the ﬁeld of discourse structures, where coherence relations between any text segments are the main focus. In the studies of discourse parsing, various attempts have been made to capture coherence relations between pairs of sentences or clauses (Pitler et al., 2008; Rutherford et al., 2017; Qin et al., 2017; Bai and Zhao, 2018). These works basically rely on discourse frameworks such as Rhetorical Structure Theory (RST; Thompson and Mann 1987) or Penn Discourse Tree Bank (PDTB; Prasad et al. 2008). Most recently, Shi and Demberg (2019) has presented a ﬁnetuning-based approach using the bidirectional encoder representation from transformers (BERT; Devlin et al. 2019). They designed their model to learn 11 classes to achieve the state-ofthe-art performance on implicit discourse relation classiﬁcation task in PDTB framework. Their work was motivated by the method taken by Devlin et al. (2019) to pr"
2021.starsem-1.20,E17-1027,0,0.0121815,"HIOLJKWVZHUHGLVUXSWHGWKHFRPSDQFKDUWHUHG DSODQHWRIOWKHH[HFXWLYHVEDFNWRWKH:HVW&RDVW /H[LFDO 6QWDFWLF3URFHVVLQJ Related Works While our focus is on clause-level relation of complex sentence constructions, not much study has been done speciﬁcally on this topic in AMR framework. Rather, the topic is dealt with in the ﬁeld of discourse structures, where coherence relations between any text segments are the main focus. In the studies of discourse parsing, various attempts have been made to capture coherence relations between pairs of sentences or clauses (Pitler et al., 2008; Rutherford et al., 2017; Qin et al., 2017; Bai and Zhao, 2018). These works basically rely on discourse frameworks such as Rhetorical Structure Theory (RST; Thompson and Mann 1987) or Penn Discourse Tree Bank (PDTB; Prasad et al. 2008). Most recently, Shi and Demberg (2019) has presented a ﬁnetuning-based approach using the bidirectional encoder representation from transformers (BERT; Devlin et al. 2019). They designed their model to learn 11 classes to achieve the state-ofthe-art performance on implicit discourse relation classiﬁcation task in PDTB framework. Their work was motivated by the method taken by Devlin e"
2021.starsem-1.20,D19-1586,0,0.0126766,"ne speciﬁcally on this topic in AMR framework. Rather, the topic is dealt with in the ﬁeld of discourse structures, where coherence relations between any text segments are the main focus. In the studies of discourse parsing, various attempts have been made to capture coherence relations between pairs of sentences or clauses (Pitler et al., 2008; Rutherford et al., 2017; Qin et al., 2017; Bai and Zhao, 2018). These works basically rely on discourse frameworks such as Rhetorical Structure Theory (RST; Thompson and Mann 1987) or Penn Discourse Tree Bank (PDTB; Prasad et al. 2008). Most recently, Shi and Demberg (2019) has presented a ﬁnetuning-based approach using the bidirectional encoder representation from transformers (BERT; Devlin et al. 2019). They designed their model to learn 11 classes to achieve the state-ofthe-art performance on implicit discourse relation classiﬁcation task in PDTB framework. Their work was motivated by the method taken by Devlin et al. (2019) to pretrain BERT, which is called “next sentence prediction task” (NSP). In the process of pretraining using NSP, the model is presented with pairs of sentences. The model predicts whether the second sentence is the actual subsequent sent"
A00-1032,J97-4004,0,0.0171112,"seline. Example sentence: &quot;Itu &apos;suMr. u L e e u &apos;s u p e n u •&quot; N S P We remove all the spaces in sentences and apply the method for non-segmented languages to the sentences. Example sentence: &quot;It &apos;sMr. L e e &apos;spen.&quot; N O R Sentences are in the original normal format. We apply the method for non-segmented languages to the sentences. Example sentence: &quot;It &apos;SuMr. u L e e &apos;s u p e n . &quot; Because of no segmentation ambiguity, &quot;LXS&quot; performs better than &quot;NSP&quot; and &quot;NOR.&quot; The following are typical example of segmentation errors. The errors originate from conjunctive ambiguity and disjunctive ambiguity(Guo, 1997). conjunctive ambiguity The analyzer recognized &quot;away,.... ahead,&quot; &quot;anymore,&quot; and &apos;~orkforce&quot; as &quot;a way,&quot; &quot;a head,&quot; &quot;any more,&quot; and &apos;~ork force,&quot; respectively. In the results of &quot;NSP,&quot; the number of this type of error is 11,267. disjunctive a m b i g u i t y The analyzer recognized &quot;a tour,&quot; &quot;a ton,&quot; and &quot;Alaskan or&quot; as &quot;at our,&quot; &quot;at on,&quot; and &quot;Alaska nor,&quot; respectively. In the results of &quot;NSP,&quot; the number of this type of error is 233. Since only &quot;NSP&quot; has disjunctive ambiguity, &quot;NOR&quot; performs better than &quot;NSP.&quot; This shows that white spaces between segments help to decrease segmentation ambigui"
A00-1032,Y96-1024,1,0.666079,"are separated by spaces into phrasal segments, Korean is a non-segmented language essentially, since each phrasal segment does not have lexeme boundaries. We call this type of languages incompletely-segmented languages. German is also categorized as this type. The following is the example for Korean. 1. Language type: non-segmented language 2. Delimiters: spaces, tabs, and carriage-returns 3. Punctuation marks: not required In incompletely-segmented languages, such as Korean, we have to consider two types of connection of lexemes, one is &quot;over a delimiter&quot; and the other is &quot;inside a segment&quot; (Hirano and Matsumoto, 1996). If we regard delimiters as lexemes, a trigram model can make it possible to treat both types. The definition gives possible starting positions of MFs in sentences of the language and the same morphological analysis system is usable for any language. We examined an effect of applying the morphofragments to analysis. Conditions of the experiment are almost the same as &quot;NOR.&quot; The difference is that we use the morpho-fragments definition for English. The row labeled &quot;MF&quot; in Table 1 shows the results of the analysis. Using the morpho-fragments decreases the analysis time drastically. The accuracy"
A00-1032,C94-1032,0,0.0323588,"enough to deal with language independent tokenization. Non-segmented languages do not have a delimiter between lexemes (Figure 2). Therefore, a treatment of further segmentation and rounding up has been well considered. In a non-segmented language, the analyzer considers all prefixes from each position in the sentence, checks whether each prefix matches the lexeme in the dictionary, stores these lexemes in a graph structure, and finds the most plausible sequence of lexemes in the graph structure. To find the sequence, Nagata proposed a probabilistic language model for non-segmented languages(Nagata, 1994)(Nagata, 1999). The crucial difference between segmented and non-segmented languages in the process of morphological analysis appears in the way of the dictionary look-up. The standard technique for looking up lexemes in Japanese dictionaries is to use a trie structure(Fredkin, 1960)(Knuth, 1998). A trie structured dictionary gives all possible lexemes that start at a given position in a sentence effectively(Morimoto and Aoe, 1993). We call this method of word looking-up as &quot;common prefix search&quot; (hereafter CPS). Figure 3 shows a part of the trie for Japanese lexeme dictionary. The results of"
A00-1032,P99-1036,0,0.0997173,"l with language independent tokenization. Non-segmented languages do not have a delimiter between lexemes (Figure 2). Therefore, a treatment of further segmentation and rounding up has been well considered. In a non-segmented language, the analyzer considers all prefixes from each position in the sentence, checks whether each prefix matches the lexeme in the dictionary, stores these lexemes in a graph structure, and finds the most plausible sequence of lexemes in the graph structure. To find the sequence, Nagata proposed a probabilistic language model for non-segmented languages(Nagata, 1994)(Nagata, 1999). The crucial difference between segmented and non-segmented languages in the process of morphological analysis appears in the way of the dictionary look-up. The standard technique for looking up lexemes in Japanese dictionaries is to use a trie structure(Fredkin, 1960)(Knuth, 1998). A trie structured dictionary gives all possible lexemes that start at a given position in a sentence effectively(Morimoto and Aoe, 1993). We call this method of word looking-up as &quot;common prefix search&quot; (hereafter CPS). Figure 3 shows a part of the trie for Japanese lexeme dictionary. The results of CPS for &quot; ~ j"
A00-1032,J97-2002,0,0.0221733,"d searches the dictionary for these graphic words. However, in practice, we want a sequence of lexemes (see the line labeled &quot;Lexemes&quot; in Figure 1). We list two major problems of tokenization in segmented languages below (examples in English). We use the term segment to refer to a string separated by white spaces. 1. Segmentation(one segment into several lexemes): Segments with a period at the end (e.g, &quot;Calif.&quot; and &quot;etc.&quot;) suffer from segmentation ambiguity. The period can denote an abbreviation, the end of a sentence, or both. The problem of sentence boundary ambiguity is not easy to solve (Palmer and Hearst, 1997). A segment with an apostrophe also has segmentation ambiguity. For example, &quot;McDonald&apos;s&quot; is ambiguous since this string can be segmented into either &quot;McDonald / Proper noun&quot; + &quot; &apos;s / Possessive ending&quot; or &quot;McDonald&apos;s / Proper noun (company name)&quot;. In addition, &quot;boys&apos; &quot; in a sentence &quot;... the boys&apos; toys ...&quot; is ambiguous. The string can be segmented into either &quot;boys&apos; / Plural possessive&quot; or &quot; b o y s / P l u r a l Noun&quot; ÷ &quot; &apos; / Punctuation (the end of a quotation)&quot; (Manning and Schiitze, 1999). If a hyphenated segment such as &quot;data-base,&quot; &quot;F-16,&quot; or &quot;MS-DOS&quot; exists in the dictionary, it shoul"
A00-1032,C92-4173,0,0.262979,"old&quot; does not exist in the dictionary, hyphens should be treated as independent tokens(Fox, 1992). Other punctuation marks such 233 as &quot;/&quot; or &quot;_&quot; have the same problem in &quot;OS/2&quot; or &quot;max_size&quot; (in programming languages). 2. Round-up(several segments into one lexeme): If a lexeme consisting of a sequence of segments such as a proper noun (e.g., &quot;New York&quot;) or a phrasal verb (e.g., &quot;look at&quot; and &quot;get up&quot;) exists in the dictionary, it should be a lexeme. To handle such lexemes, we need to store multisegment lexemes in the dictionary. Webster and Kit handle idioms and fixed expressions in this way(Webster and Kit, 1992). In Penn Treebank(Santorini, 1990), a proper noun like &quot;New York&quot; is defined as two individual proper nouns &quot;New / NNP&quot; ÷ &quot;York / NNP,&quot; disregarding round-up of several:segments into a lexeme. The definition of lexemes in a dictionary depends on the requirement of application. Therefore, a simple pattern matcher is not enough to deal with language independent tokenization. Non-segmented languages do not have a delimiter between lexemes (Figure 2). Therefore, a treatment of further segmentation and rounding up has been well considered. In a non-segmented language, the analyzer considers all pr"
A00-2015,P96-1025,0,0.382272,"e. 1. For each piece of evidence, calculate the likelihood ratio of the conditional probability of a decision D = xl (given the presence of that piece of evidence) to the conditional probability of the rest of the decisions D =-,xl: P(D=xl I E = I ) l°g2 P(D='~xl [ E = I ) Decision List Learning A decision list (Yarowsky, 1994) is a sorted list of the decision rules each of which decides the value of a decision D given some evidence E. Each decision rule in a decision list is sorted TOur modeling is slightly different from those of other standard approaches to statistical dependency analysis (Collins, 1996; Fujio and Matsumoto, 1998; Haruno et al., 1998) which simply distinguish the two cases: the case where dependency relation holds between the given two vp chunks or clauses, and the case where dependency relation does not hold. In contrast to those standard approaches, we ignore the case where the head vp chunk of Clause1 modifies that of another subordinate clause which precedes Clause2. This is because we assume that this case is more loosely related to the scope embedding preference of subordinate clauses. 114 Then, a decision list is constructed with pieces of evidence sorted in descendin"
A00-2015,C96-1058,0,0.0289129,"Missing"
A00-2015,W98-1511,1,0.84927,", we employ the decision list learning method of Yarowsky (1994), where optimal combination of those features are selected and sorted in the form of decision rules, according to the strength of correlation between those features and the dependency preference of the two subordinate clauses. We evaluate the proposed method through the experiment on learning dependency preference of Japanese subordinate clauses from the EDR bracketed corpus (section 4). We show that the proposed method outperforms other related methods/models. We also evaluate the estimated dependencies of subordinate clauses in Fujio and Matsumoto (1998)'s framework of the statistical dependency analysis of a whole sentence, in which we successfully increase the precisions of both chunk level and sentence level dependencies thanks to the estimated dependencies of subordinate clauses. 2 2.1 Analyzing Dependencies between Japanese Subordinate Clauses based on Scope Embedding Preference Dependency Analysis of A Japanese S e n t e n c e First, we overview dependency analysis of a Japanese sentence. Since words in a Japanese sentence are not segmented by explicit delimiters, input sentences are first word segmented, 111 Phrase Structure Scope of S"
A00-2015,P98-1083,0,0.305366,"e the likelihood ratio of the conditional probability of a decision D = xl (given the presence of that piece of evidence) to the conditional probability of the rest of the decisions D =-,xl: P(D=xl I E = I ) l°g2 P(D='~xl [ E = I ) Decision List Learning A decision list (Yarowsky, 1994) is a sorted list of the decision rules each of which decides the value of a decision D given some evidence E. Each decision rule in a decision list is sorted TOur modeling is slightly different from those of other standard approaches to statistical dependency analysis (Collins, 1996; Fujio and Matsumoto, 1998; Haruno et al., 1998) which simply distinguish the two cases: the case where dependency relation holds between the given two vp chunks or clauses, and the case where dependency relation does not hold. In contrast to those standard approaches, we ignore the case where the head vp chunk of Clause1 modifies that of another subordinate clause which precedes Clause2. This is because we assume that this case is more loosely related to the scope embedding preference of subordinate clauses. 114 Then, a decision list is constructed with pieces of evidence sorted in descending order with respect to their likelihood ratios,"
A00-2015,utsuro-2000-learning,1,0.581614,"Missing"
A00-2015,P94-1013,0,0.703477,"lem of deciding scope embedding preference as a classification problem, in which various types of linguistic information of each subordinate clause are encoded as features and used for deciding which one of given two subordinate clauses has a broader scope than the other. As in the case of Shirai et al. (1995), we formalize the problem of deciding dependency preference of subordinate clauses by utilizing the correlation of scope embedding preference and dependency preference of Japanese subordinate clauses. Then, as a statistical learning method, we employ the decision list learning method of Yarowsky (1994), where optimal combination of those features are selected and sorted in the form of decision rules, according to the strength of correlation between those features and the dependency preference of the two subordinate clauses. We evaluate the proposed method through the experiment on learning dependency preference of Japanese subordinate clauses from the EDR bracketed corpus (section 4). We show that the proposed method outperforms other related methods/models. We also evaluate the estimated dependencies of subordinate clauses in Fujio and Matsumoto (1998)'s framework of the statistical depend"
A97-1053,J96-1002,0,0.0308303,"Missing"
A97-1053,P93-1005,0,0.0119421,"ing probabilistic subcategorization preference from the EDR Japanese bracketed corpus, as well as those on evaluating the performance of subcategorization preference. 1 Introduction In corpus-based NLP, extraction of linguistic knowledge such as lexical/semantic collocation is one of the most important issues and has been intensively studied in recent years. In those research, extracted lexical/semantic collocation is especially useful in terms of ranking parses in syntactic analysis as well as automatic construction of lexicon for NLP. For example, in the context of syntactic disambiguation, Black (1993) and Magerman (1995) proposed statistical parsing models based-on decisiontree learning techniques, which incorporated not only syntactic but also lexical/semantic information in the decision-trees. As lexical/semantic information, Black (1993) used about 50 semantic categories, while Magerman (1995) used lexicai forms of words. Collins (1996) proposed a statistical parser which is based on probabilities of dependencies between head-words in the parse tree. In those works, lexical/semantic collocation are used for ranking parses in syntactic analysis. *The authors would like to thank Dr. Hang"
A97-1053,P96-1025,0,0.158537,"in recent years. In those research, extracted lexical/semantic collocation is especially useful in terms of ranking parses in syntactic analysis as well as automatic construction of lexicon for NLP. For example, in the context of syntactic disambiguation, Black (1993) and Magerman (1995) proposed statistical parsing models based-on decisiontree learning techniques, which incorporated not only syntactic but also lexical/semantic information in the decision-trees. As lexical/semantic information, Black (1993) used about 50 semantic categories, while Magerman (1995) used lexicai forms of words. Collins (1996) proposed a statistical parser which is based on probabilities of dependencies between head-words in the parse tree. In those works, lexical/semantic collocation are used for ranking parses in syntactic analysis. *The authors would like to thank Dr. Hang Li of NEC C&:C Research Laboratories, Dr. Kentaro Inui of Tokyo Institute of Technology, Dr. Koiti Hasida of Electrotechnical Laboratory, Dr. Tak_ashi Miyata of Nara Institute of Science and Technology, and also anonymous reviewers of ANLP97 for valuable comments on this work. ac. jp On the other hand, in the context of automatic lexicon const"
A97-1053,J93-1003,0,0.0229666,", . . . , fn). (~(e) = {(f,,...,f.) (fl ..... f,)--e} (24) I t&apos;) = max ¢((fl .... ,f~) ~ e) (23) (11.....Y~) Now, the problem of learning probabilistic subcategorization preference is stated as: for every verb-noun collocation e in C, estimating the probability distribution P((fl, 6Resnik (1993) applys the idea of the KL distance to measuring the association of a verb v and its object noun class c. Our definition of ekt corresponds to an extension of Resnik&apos;s association score, which considers dependencies of more than one case-markers in a subcategorization frame. 7Another related measure is Dunning (1993)&apos;s likelihood ratio tests for binomial and multinomial distributions, which are claimed to be effective even with very much smaller volumes of text than is necessary for other tests based on assumed normal distributions. F(e) contains a tuple ( f ) consisting of only one subcategorization frame f only if f can not be divided into several independent partial subcategorization frames. Then, we assume that each element of F ( e ) occurs evenly and estimate the initial conditional probability distribution P ( ( f l , . . . , f,)j I e) of generating e from (fl,..., fn)j as an approximation below: P"
A97-1053,C96-1004,0,0.120287,"ot decidable which superordinate class generates each observed leaf class in the verb-noun collocation. So far, there exist several researches which worked on these two issues in learning collocational knowledge of verbs and also evaluated the results in t e r m s of syntactic disambiguation. Resnik (1993) and Li and Abe (1995) studied how to find an optimal abstraction level of an argument noun in a treestructured thesaurus. Although they evaluated the obtained abstraction level of the argument noun by its performance in syntactic disambiguation, their works are limited to only one argument. Li and Abe (1996) also studied a method for learning dependencies between case slots and evaluated the discovered dependencies in the syntactic disambiguation task. They first obtained optimal abstraction levels of the argument nouns by the method in Li and Abe (1995), and then tried to discover dependencies between the classbased case slots. They reported that dependencies 364 were discovered only at the slot-level and not at the class-level. C o m p a r e d with those previous works, this paper proposes to cope with the above two ambiguities in a uniform way. First, we introduce a d a t a structure which rep"
A97-1053,P95-1037,0,0.0571541,"subcategorization preference from the EDR Japanese bracketed corpus, as well as those on evaluating the performance of subcategorization preference. 1 Introduction In corpus-based NLP, extraction of linguistic knowledge such as lexical/semantic collocation is one of the most important issues and has been intensively studied in recent years. In those research, extracted lexical/semantic collocation is especially useful in terms of ranking parses in syntactic analysis as well as automatic construction of lexicon for NLP. For example, in the context of syntactic disambiguation, Black (1993) and Magerman (1995) proposed statistical parsing models based-on decisiontree learning techniques, which incorporated not only syntactic but also lexical/semantic information in the decision-trees. As lexical/semantic information, Black (1993) used about 50 semantic categories, while Magerman (1995) used lexicai forms of words. Collins (1996) proposed a statistical parser which is based on probabilities of dependencies between head-words in the parse tree. In those works, lexical/semantic collocation are used for ranking parses in syntactic analysis. *The authors would like to thank Dr. Hang Li of NEC C&:C Resea"
A97-1053,H93-1054,0,0.461301,"tion 1) is caused by the fact that, only by observing each verb-noun collocation in corpus, it is not decidable which cases are dependent on each other and which cases are optional and independent of other cases. 2) is caused by the fact that, only by observing each verbnoun collocation in corpus, it is not decidable which superordinate class generates each observed leaf class in the verb-noun collocation. So far, there exist several researches which worked on these two issues in learning collocational knowledge of verbs and also evaluated the results in t e r m s of syntactic disambiguation. Resnik (1993) and Li and Abe (1995) studied how to find an optimal abstraction level of an argument noun in a treestructured thesaurus. Although they evaluated the obtained abstraction level of the argument noun by its performance in syntactic disambiguation, their works are limited to only one argument. Li and Abe (1996) also studied a method for learning dependencies between case slots and evaluated the discovered dependencies in the syntactic disambiguation task. They first obtained optimal abstraction levels of the argument nouns by the method in Li and Abe (1995), and then tried to discover dependenci"
asahara-etal-2002-use,ide-etal-2000-xces,0,\N,Missing
C00-1004,J95-4004,0,0.083928,"Missing"
C00-1004,A92-1018,0,0.113139,"Missing"
C00-2135,A00-2018,0,\N,Missing
C00-2135,P97-1003,0,\N,Missing
C00-2135,P91-1022,0,\N,Missing
C00-2135,P93-1004,1,\N,Missing
C00-2135,W98-1511,1,\N,Missing
C00-2135,W96-0107,1,\N,Missing
C02-1053,P01-1041,1,0.897899,"Missing"
C02-1053,W00-1303,1,0.435633,"t summarization. Aone et al. (1998) and Kupiec et al. (1995) employed Bayesian classiﬁers, Mani et al. (1998), Nomoto et al. (1997), Lin (1999), and Okumura et al. (1999) used decision tree learning. However, most machine learning methods overﬁt the training data when many features are given. Therefore, we need to select features carefully. Support Vector Machines (SVMs) (Vapnik, 1995) is robust even when the number of features is large. Therefore, SVMs have shown good performance for text categorization (Joachims, 1998), chunking (Kudo and Matsumoto, 2001), and dependency structure analysis (Kudo and Matsumoto, 2000). In this paper, we present an important sentence extraction technique based on SVMs. We veriﬁed the technique against the Text Summarization Challenge (TSC) (Fukushima and Okumura, 2001) corpus. 2 Important Sentence Extraction based on Support Vector Machines 2.1 Support Vector Machines (SVMs) SVM is a supervised learning algorithm for 2class problems. Training data is given by (x1 , y1 ), · · · , (xu , yu ), xj ∈ Rn , yj ∈ {+1, −1}. Here, xj is a feature vector of the j-th sample; yj is its class label, positive(+1) or negative(−1). SVM separates positive and negative examples by a hyperplan"
C02-1053,N01-1025,1,0.881428,"learning has attracted attention in the ﬁeld of automatic text summarization. Aone et al. (1998) and Kupiec et al. (1995) employed Bayesian classiﬁers, Mani et al. (1998), Nomoto et al. (1997), Lin (1999), and Okumura et al. (1999) used decision tree learning. However, most machine learning methods overﬁt the training data when many features are given. Therefore, we need to select features carefully. Support Vector Machines (SVMs) (Vapnik, 1995) is robust even when the number of features is large. Therefore, SVMs have shown good performance for text categorization (Joachims, 1998), chunking (Kudo and Matsumoto, 2001), and dependency structure analysis (Kudo and Matsumoto, 2000). In this paper, we present an important sentence extraction technique based on SVMs. We veriﬁed the technique against the Text Summarization Challenge (TSC) (Fukushima and Okumura, 2001) corpus. 2 Important Sentence Extraction based on Support Vector Machines 2.1 Support Vector Machines (SVMs) SVM is a supervised learning algorithm for 2class problems. Training data is given by (x1 , y1 ), · · · , (xu , yu ), xj ∈ Rn , yj ∈ {+1, −1}. Here, xj is a feature vector of the j-th sample; yj is its class label, positive(+1) or negative(−1"
C02-1053,C00-2167,0,0.0456631,"Missing"
C02-1053,C96-2166,0,0.0378212,"r Machines Important sentence extraction can be regarded as a two-class problem: important or unimportant. However, the proportion of important sentences in training data will diﬀer from that in the test data. The number of important sentences in a document is determined by a summarization rate that is given at run-time. A simple solution for this problem is to rank sentences in a document. We use g(x) the distance from the hyperplane to x to rank the sentences. 2.3 Features We deﬁne the boolean features discussed below that are associated with sentence Si by taking past studies into account (Zechner, 1996; Nobata et al., 2001; Hirao et al., 2001; Nomoto and Matsumoto, 1997). We use 410 boolean variables for each Si . Where x = (x[1], · · ·, x[410]). A real-valued feature normalized between 0 and 1 is represented by 10 boolean variables. Each variable corresponds to an internal [i/10,(i + 1)/10) where i = 0 to 9. For example, Posd = 0.75 is represented by “0000000100” because 0.75 belongs to [7/10,8/10). Position of sentences We deﬁne three feature functions for the position of Si . First, Lead is a boolean that corresponds to the output of the lead-based method described below1 . Second, Posd"
C02-1053,P98-1009,0,\N,Missing
C02-1053,C98-1009,0,\N,Missing
C02-1101,W99-0606,0,0.334414,"Missing"
C02-1101,J95-4004,0,0.169326,"Missing"
C02-1101,A00-2020,0,0.169635,"Missing"
C02-1101,matsumoto-yamashita-2000-using,1,0.90943,"Missing"
C02-1101,P02-1063,1,0.823235,"the weights. To detect exceptional examples in a corpus annotated with POS tags, we first construct an SVM model for POS tagging using all the elements in a corpus as the training examples. Note that each example corresponds to a word in the corpus. Then SVMs assign weights to the examples, and large weights are assigned to difficult examples. Finally, we extract examples with a large weight greater than or equal to a threshold value θα . In the next subsection, we describe how to construct an SVM model for POS tagging. 2.2 Revision Learning for POS tagging We use a revision learning method (Nakagawa et al., 2002) for POS tagging with SVMs1 . This method creates training examples of SVMs with 1 The well known one-versus-rest method (Allwein et al., 2000) can be also used for POS tagging with SVMs, but it has large computational cost and cannot handle segmentation of words directly that is necessary for Japanese morphological analysis. binary labels for each POS tag class using a stochastic model (e.g. n-gram) as follows: each word in a corpus becomes a positive example of its POS tag class. We then build a simple stochastic POS tagger based on n-gram (POS bigram or trigram) model, and words in the corp"
C02-1101,P98-2164,0,0.0355479,"Missing"
C02-1101,W96-0213,0,0.21106,"Missing"
C02-1101,C98-2159,0,\N,Missing
C02-1101,J01-2002,0,\N,Missing
C04-1066,N03-1002,1,0.895604,"Missing"
C04-1066,A00-1031,0,0.0925994,"Missing"
C04-1066,P03-2039,1,0.880388,"Missing"
C04-1066,shinnou-ikeya-2000-extraction,0,0.0587615,"Missing"
C04-1066,N01-1025,1,0.686062,"Seven character types ar defined: Space, Digit, Lowercase alphabet, Uppercase alphabet, Hiragana, Katakana, Other (Kanji). The character type is directly or indirectly used in most of previous work and appears an important feature to characterize unknown words in Japanese texts. Table 1: Tags for positions in a word Tag S B E I Description one-character word first character in a multi-character word last character in a multi-character word intermediate character in a multi-character word (only for words longer than 2 chars) 2.3 Support Vector Machine-based Chunking We use the chunker YamCha (Kudo and Matsumoto, 2001), which is based on SVMs (Vapnik, 1998). Suppose we have a set of training data for a binary class problem: (x1 , y1 ), . . . , (xN , yN ), where xi ∈ Rn is a feature vector of the i th sample in the training data and yi ∈ {+1, −1} is the label of the sample. The goal is to find a decision function which accurately predicts y for an unseen x. An support vector machine classifier gives a decision function f (x) = sign(g(x)) for an input vector x where g(x) =  αi yi K(x, zi ) + b. zi ∈SV K(x, z) is a kernel function which maps vectors into a higher dimensional space. We use a polynomial kernel"
C04-1066,C96-2202,0,0.414243,"Missing"
C04-1066,P99-1036,0,0.150218,"Missing"
C04-1066,W95-0107,0,0.0365165,"* unknown word tag B I I Figure 1: An example of features for chunking for the extension, the “One vs. Rest method” and the “Pairwise method”. In the “One vs. Rest methods”, we prepare n binary classifiers between one class and the remain classes. Whereas in the “Pairwise method”, we prepare n C2 binary classifiers between all pairs of classes. We use “Pairwise method” since it is efficient to train than the “One vs. Rest method”. Chunking is performed by deterministically annotating a tag on each character. Table 2 shows the unknown word tags for chunking, which are known as the IOB2 model (Ramshaw and Marcus, 1995). Table 2: Tags for unknown word chunking Tag Description B first character in an unknown word character in an unknown word (except B) I character in a known word O We perform chunking either from the beginning or from the end of the sentence. Figure 1 illustrates a snapshot of chunking procedure. Two character contexts on both sides are referred to. Information of two preceding unknown word tags is also used since the chunker has already determined them and they are available. In the example, the chunker uses the features appearing within the solid box to infer the unknown word tag (“I”) at t"
C04-1066,W02-1817,0,0.0343378,"Missing"
C04-1066,W01-0512,0,\N,Missing
C04-1066,C02-1049,0,\N,Missing
C04-1066,C02-2019,0,\N,Missing
C04-1066,P03-1061,0,\N,Missing
C04-1066,maekawa-etal-2000-spontaneous,0,\N,Missing
C04-1130,W02-0811,0,0.198756,"ayesian classifiers, each of which is based on lexical features that represent co-occurring words in varying sized windows of context. The sense that receives majority of the votes was assigned as the final selection. Stevenson and Wilks (2001) presented a classifier combination framework where three different disambiguation modules were combined using a memory-based approach. Hoste et al. (2002) used word experts consisted of four memory-based learners trained on different context. Output of the word experts is based on majority voting or weighted voting. Florian et al.(2002) and Florian and Yarowsky (2002) used six different classifiers as components of their combination. They compared several different strategies of combination, which include combining the posterior distribution, combination based on order statistics and several different voting. Klein et al. (2002) combined a number of different first-order classifiers using majority voting, weighted voting and maximum entropy. In Park (2003), a committee of classifiers was used to learn from the unlabeled examples. The label of an unlabeled example is predicted by weighted majority voting. Frank at al. (2003) presented a locally weighted Naï"
C04-1130,J98-1001,0,0.0167158,"Missing"
C04-1130,P96-1006,0,0.330805,"et is the number of senses. Numbers in each bracket are amounts of samples used for each sense. They were annotated by a Chinese native speaker, and checked by another native speaker. Some samples without inter-agreement between two native speakers had been excluded. Only word co-occurrences in given windows are used as features through all experiments in this paper. 4.2 Experimental Method In order to do a comparative study, we have implemented not only our algorithm, but also four other related algorithms in our experiments. They fall into two classes. NB (Manning and Schutze 1999) and KNN (Ng and Lee 1996) are two components of our approach. Locally weighted NB(LWNB, Frank et al. 2003) and Ensemble NB(ENB Pedersen 2000) are two combinational approaches. Since our aim is to compare not only the performance but also the robustness of these algorithms, we implemented each algorithm in following way. We note our approach TB_KNN when (3.2) is used for final decision, and TB_VOTE when (3.3) is used for final decision. We firstly constructed a sequence of context windows p k =( l k , rk ) k = 1,...,40 in following way: 1. Initiate: l1 = 0, r1 = 1 2. Generate next window: l k +1 = l k , rk +1 = rk + 1"
C04-1130,A00-2009,0,0.160905,"Since different algorithms have different strengths and perform well on different feature space, classifier combination is a reasonable candidate to achieve better performance by taking advantages of different approaches. In the field of ML, ensembles of classifiers have been shown to be successful in last decade (Dietterich 1997). For the specific task of WSD, classifier combination has been received more and more attention in recent years. Kilgarriff and Rosenzweig (2000) presented the first empirical study. They combined the output of the participating SENSEVAL1 systems via simple voting. Pedersen (2000) built an ensemble of Naïve Bayesian classifiers, each of which is based on lexical features that represent co-occurring words in varying sized windows of context. The sense that receives majority of the votes was assigned as the final selection. Stevenson and Wilks (2001) presented a classifier combination framework where three different disambiguation modules were combined using a memory-based approach. Hoste et al. (2002) used word experts consisted of four memory-based learners trained on different context. Output of the word experts is based on majority voting or weighted voting. Florian"
C04-1130,P94-1013,0,0.0247992,". Only information in the context selection is to some extent similar with the window is then used for classifiers and relation between a particle’s position and its disambiguating. What is the best window size for momentum in Heisenberg Uncertainty Principle. WSD has been long for a problem. Weaver (1955) By the Uncertainty Principle, when we hoped we could find a minimum value of the measure the position and the momentum of a window size which can lead to the correct choice particle, we cannot measure them with of sense for the target ambiguous word. zero-variance simultaneously. In Quantum Yarowsky (1994) argued the optimal value is Theory, the wave-function is used to describe the sensitive to the type of ambiguity. Semantic or state of a particle. The method to deal with this topic-based ambiguities warrant a larger window problem in Quantum Theory suggests us an idea (from 20 to 50), while more local syntactic to deal with the similar problem in WSD. ambiguities warrant a smaller window (3 or 4). Firstly, since the existence of the uncertainty Leacock at el (1998) showed the local context is of sense selection at different window sizes, superior to topical context as an indicator of sense s"
C04-1130,W02-1004,0,\N,Missing
C04-1130,S01-1021,0,\N,Missing
C08-1001,W02-2016,1,0.753818,"rn are shared with each other. We call such anchors type-based anchors because bread in (3a) and bread in (3b) do not refer to the same object but are identical just as type. Given a pair of predicates Pred1 and Pred2 , we search a corpus for sentences where Pred1 and Pred2 co-occur, and calculate the frequency counts of their argument ﬁllers appearing in those sentences: 4 Experiments 4.1 Settings For an empirical evaluation, we used a sample of approximately 500M sentences taken from the Web corpus collected by Kawahara and Kurohashi (2006). The sentences were dependencyparsed with CaboCha (Kudo and Matsumoto, 2002), and co-occurrence samples of event mentions were extracted. Event mentions with patterns whose frequency was less than 20 were discarded in order to reduce computational costs. In our experiments, we considered two of Inui et al. (2003)’s four types of causal relations: actioneffect relations (Effect in Inui et al.’s terminology) and action-means relations (Means). An actioneffect relation holds between events x and y if and only if non-volitional event y is likely to happen as either a direct or indirect effect of volitional action x. For example, the action X-ga undou-suru (X exercises) an"
C08-1001,P06-1015,0,0.0337029,"s boosts the accuracy as well as identifying shared arguments. (a) Anchor word Anc is the head of a noun phrase ﬁlling argument Arg1 of Pred1 appearing in a Web page. 3.2 Predicate pair acquisition (b) Anc also ﬁlls argument Arg2 of Pred2 appearing in the same Web page as above. For predicate pair acquisition, we can choose one from a range of state-of-the-art pattern-based methods. Among others, in our experiments, we adopted Abe et al. (2008)’s method because it had an advantage in that it was capable of learning patterns as well as relation instances. Abe et al. (2008)’s method is based on Pantel and Pennacchiotti (2006)’s Espresso algorithm, (c) Anc must not be any of those in the stop list. (d) pmi(Predi , Argi ) ≥ −1.0 for i ∈ {1, 2} For our experiments, we manually created the stop list, which contained 219 words including pronouns, numerals and highly generic nouns such as 3 Figure 1: Two-phased event relation acquisition 4 S1 = {Arg|Pred1 -Arg1 ; Pred2 ; Anc}, “ͱ (thing)”, “ͷ (thing)” and “ͱ (time)”. pmi(Predi , Argi ) in condition (d) is the point-wise mutual information between Predi and Argi . This condition is imposed for pruning wrong anchors misidentiﬁed due to parsing errors. While Pekar car"
C08-1001,I08-1065,1,0.212189,"o obtain and maintain a potentially huge collection of such event relation instances. This paper addresses the issue of how to automatically acquire such instances of relations between events (henceforth, event relation instances) from a large-scale text collection. Motivated by this issue, several research groups have reported their experiments on automatic acquisition of causal, temporal and entailment relations between event expressions (typically verbs or verb phrases) (Lin and Pantel, 2001; Inui et al., 2003; Chklovski and Pantel, 2005; Torisawa, 2006; Pekar, 2006; Zanzotto et al., 2006; Abe et al., 2008, etc.). As we explain below, however, none of these studies fully achieves the goal we pursue in this paper. An important aspect to consider in event relation acquisition is that each event has arguments. For example, the causal relation between wash something and something is clean can be represented naturally as: Addressing the task of acquiring semantic relations between events from a large corpus, we ﬁrst argue the complementarity between the pattern-based relation-oriented approach and the anchor-based argumentoriented approach. We then propose a twophased approach, which ﬁrst uses lexic"
C08-1001,N06-1007,0,0.547598,"hing event. A crucial issue is how to obtain and maintain a potentially huge collection of such event relation instances. This paper addresses the issue of how to automatically acquire such instances of relations between events (henceforth, event relation instances) from a large-scale text collection. Motivated by this issue, several research groups have reported their experiments on automatic acquisition of causal, temporal and entailment relations between event expressions (typically verbs or verb phrases) (Lin and Pantel, 2001; Inui et al., 2003; Chklovski and Pantel, 2005; Torisawa, 2006; Pekar, 2006; Zanzotto et al., 2006; Abe et al., 2008, etc.). As we explain below, however, none of these studies fully achieves the goal we pursue in this paper. An important aspect to consider in event relation acquisition is that each event has arguments. For example, the causal relation between wash something and something is clean can be represented naturally as: Addressing the task of acquiring semantic relations between events from a large corpus, we ﬁrst argue the complementarity between the pattern-based relation-oriented approach and the anchor-based argumentoriented approach. We then propose a"
C08-1001,I05-1069,0,0.103686,"this object is clean as a result of the washing event. A crucial issue is how to obtain and maintain a potentially huge collection of such event relation instances. This paper addresses the issue of how to automatically acquire such instances of relations between events (henceforth, event relation instances) from a large-scale text collection. Motivated by this issue, several research groups have reported their experiments on automatic acquisition of causal, temporal and entailment relations between event expressions (typically verbs or verb phrases) (Lin and Pantel, 2001; Inui et al., 2003; Chklovski and Pantel, 2005; Torisawa, 2006; Pekar, 2006; Zanzotto et al., 2006; Abe et al., 2008, etc.). As we explain below, however, none of these studies fully achieves the goal we pursue in this paper. An important aspect to consider in event relation acquisition is that each event has arguments. For example, the causal relation between wash something and something is clean can be represented naturally as: Addressing the task of acquiring semantic relations between events from a large corpus, we ﬁrst argue the complementarity between the pattern-based relation-oriented approach and the anchor-based argumentoriented"
C08-1001,P06-1079,1,0.89187,"Missing"
C08-1001,N06-1008,0,0.20994,"esult of the washing event. A crucial issue is how to obtain and maintain a potentially huge collection of such event relation instances. This paper addresses the issue of how to automatically acquire such instances of relations between events (henceforth, event relation instances) from a large-scale text collection. Motivated by this issue, several research groups have reported their experiments on automatic acquisition of causal, temporal and entailment relations between event expressions (typically verbs or verb phrases) (Lin and Pantel, 2001; Inui et al., 2003; Chklovski and Pantel, 2005; Torisawa, 2006; Pekar, 2006; Zanzotto et al., 2006; Abe et al., 2008, etc.). As we explain below, however, none of these studies fully achieves the goal we pursue in this paper. An important aspect to consider in event relation acquisition is that each event has arguments. For example, the causal relation between wash something and something is clean can be represented naturally as: Addressing the task of acquiring semantic relations between events from a large corpus, we ﬁrst argue the complementarity between the pattern-based relation-oriented approach and the anchor-based argumentoriented approach. We th"
C08-1001,P06-1107,0,0.0300652,"crucial issue is how to obtain and maintain a potentially huge collection of such event relation instances. This paper addresses the issue of how to automatically acquire such instances of relations between events (henceforth, event relation instances) from a large-scale text collection. Motivated by this issue, several research groups have reported their experiments on automatic acquisition of causal, temporal and entailment relations between event expressions (typically verbs or verb phrases) (Lin and Pantel, 2001; Inui et al., 2003; Chklovski and Pantel, 2005; Torisawa, 2006; Pekar, 2006; Zanzotto et al., 2006; Abe et al., 2008, etc.). As we explain below, however, none of these studies fully achieves the goal we pursue in this paper. An important aspect to consider in event relation acquisition is that each event has arguments. For example, the causal relation between wash something and something is clean can be represented naturally as: Addressing the task of acquiring semantic relations between events from a large corpus, we ﬁrst argue the complementarity between the pattern-based relation-oriented approach and the anchor-based argumentoriented approach. We then propose a twophased approach, whi"
C08-1001,N06-1023,0,0.0463994,"un bread to be an anchor indicating that the object of bake and the subject of burn are shared with each other. We call such anchors type-based anchors because bread in (3a) and bread in (3b) do not refer to the same object but are identical just as type. Given a pair of predicates Pred1 and Pred2 , we search a corpus for sentences where Pred1 and Pred2 co-occur, and calculate the frequency counts of their argument ﬁllers appearing in those sentences: 4 Experiments 4.1 Settings For an empirical evaluation, we used a sample of approximately 500M sentences taken from the Web corpus collected by Kawahara and Kurohashi (2006). The sentences were dependencyparsed with CaboCha (Kudo and Matsumoto, 2002), and co-occurrence samples of event mentions were extracted. Event mentions with patterns whose frequency was less than 20 were discarded in order to reduce computational costs. In our experiments, we considered two of Inui et al. (2003)’s four types of causal relations: actioneffect relations (Effect in Inui et al.’s terminology) and action-means relations (Means). An actioneffect relation holds between events x and y if and only if non-volitional event y is likely to happen as either a direct or indirect effect of"
C08-1001,W04-3206,0,\N,Missing
C08-1046,P06-1105,0,0.034975,"Missing"
C08-1046,C96-1058,0,0.0901231,"(a) “He is a man who doesn’t read books.” 彼は kare-wa (He) 本を hon-wo (books) 読まない。 yomanai. (doesn’t read .) (b) “He doesn’t read books.” Figure 1: Examples of Japanese sentences. Introduction The shared tasks of multi-lingual dependency parsing took place at CoNLL-2006 (Buchholz and Marsi, 2006) and CoNLL-2007 (Nivre et al., 2007). Many language-independent parsing algorithms were proposed there. The algorithms need to adapt to various dependency structure constraints according to target languages: projective vs. non-projective, head-initial vs. head-final, and single-rooted vs. multi-rooted. Eisner (1996) proposed a CKY-like O(n3 ) algorithm. Yamada and Matsumoto (2003) proposed a shift-reducelike O(n2 ) deterministic algorithm. Nivre et al. (2003; 2004) also proposed a shift-reduce-like c 2008.  Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. O(n) deterministic algorithm for projective languages. The model is enhanced for non-projective languages by Nivre and Nilsson (2005). McDonald et al. (2005) proposed a method based on search of maximum spanning trees employing the C"
C08-1046,W03-2604,1,0.757988,"ved the highest accuracy in the experiments with Kyoto Text Corpus Version 3.0 data 1 , since other deterministic methods do not consider relative preference among candidate heads but solely consider whether the focused-on pair of bunsetsu’s is in a dependency relation or not. We propose a model that takes a bunsetsu and two candidate heads into consideration and selects the better candidate head out of those two. This step is repeated in a step ladder tournament to get the best candidate head (hereafter we call this model as a “tournament model”). The tournament model was first introduced by Iida et al. (2003) for coreference resolution. We applied this model to selecting the most plausible candidate head for each bunsetsu except for the sentence final one. Section 2 describes the tournament model comparing with previous research. Section 3 describes 1 Note: Sassano’s SR algorithm is the highest by experiment with the smaller data Kyoto Text Corpus Version 2.0 Relative preference method and SR algorithm are not compared directly with the same data. The most likely candidate head 彼は kare-wa (He) Focused-on dependent 本を hon-wo (books) 読まない yomanai (doesn’t read) 人だ。 hito-da. (man .) Its candidate hea"
C08-1046,C04-1010,0,0.173635,"Missing"
C08-1046,W03-3017,0,0.0888379,"“yomanai” in (b) but not in (a). In dependency parsing of Japanese, deterministic algorithms outperform probabilistic CKY methods. Kudo and Matsumoto (2002) applied the 361 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 361–368 Manchester, August 2008 cascaded chunking algorithm (hereafter “CC algorithm”) to Japanese dependency parsing. Yamada’s method (Yamada and Matsumoto, 2003) employed a similar algorithm. Sassano (2004) proposed a linear-order shift-reduce-like algorithm (hereafter “SR algorithm”), which is similar to Nivre’s algorithm (Nivre, 2003). These deterministic algorithms are biased to select nearer candidate heads since they examine the candidates sequentially, and once they find a plausible one they never consider further candidates. We experimented the CLE algorithm with Japanese dependency parsing, and found that the CLE algorithm is comparable to or in some cases poorer than the deterministic algorithms in our experiments. Actually, the CLE algorithm is not suitable for some of the constraints in Japanese dependency structures: head-final and projective. First, head-final means that dependency relation always goes from left"
C08-1046,C04-1002,0,0.343353,"(a) is similar to sentence (b), the syntactic structures of these two are different, especially because “kare-wa” directly depends on “yomanai” in (b) but not in (a). In dependency parsing of Japanese, deterministic algorithms outperform probabilistic CKY methods. Kudo and Matsumoto (2002) applied the 361 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 361–368 Manchester, August 2008 cascaded chunking algorithm (hereafter “CC algorithm”) to Japanese dependency parsing. Yamada’s method (Yamada and Matsumoto, 2003) employed a similar algorithm. Sassano (2004) proposed a linear-order shift-reduce-like algorithm (hereafter “SR algorithm”), which is similar to Nivre’s algorithm (Nivre, 2003). These deterministic algorithms are biased to select nearer candidate heads since they examine the candidates sequentially, and once they find a plausible one they never consider further candidates. We experimented the CLE algorithm with Japanese dependency parsing, and found that the CLE algorithm is comparable to or in some cases poorer than the deterministic algorithms in our experiments. Actually, the CLE algorithm is not suitable for some of the constraints"
C08-1046,D07-1064,0,0.0462536,"Missing"
C08-1046,W08-2132,1,0.882103,"Missing"
C08-1046,W03-3023,1,0.956027,"(He) 本を hon-wo (books) 読まない。 yomanai. (doesn’t read .) (b) “He doesn’t read books.” Figure 1: Examples of Japanese sentences. Introduction The shared tasks of multi-lingual dependency parsing took place at CoNLL-2006 (Buchholz and Marsi, 2006) and CoNLL-2007 (Nivre et al., 2007). Many language-independent parsing algorithms were proposed there. The algorithms need to adapt to various dependency structure constraints according to target languages: projective vs. non-projective, head-initial vs. head-final, and single-rooted vs. multi-rooted. Eisner (1996) proposed a CKY-like O(n3 ) algorithm. Yamada and Matsumoto (2003) proposed a shift-reducelike O(n2 ) deterministic algorithm. Nivre et al. (2003; 2004) also proposed a shift-reduce-like c 2008.  Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. O(n) deterministic algorithm for projective languages. The model is enhanced for non-projective languages by Nivre and Nilsson (2005). McDonald et al. (2005) proposed a method based on search of maximum spanning trees employing the Chu-Liu-Edmonds algorithm (hereafter “CLE algorithm”) (Chu and Liu,"
C08-1046,W02-2016,1,0.947086,"e dependency structures have the following constraints: head-final, singlehead, single-rooted, connected, acyclic and projective. Figure 1 shows examples of Japanese sentences and their dependency structures. Each box represents a bunsetsu. A dependency relation is represented by an edge from a dependent to its head. Though sentence (a) is similar to sentence (b), the syntactic structures of these two are different, especially because “kare-wa” directly depends on “yomanai” in (b) but not in (a). In dependency parsing of Japanese, deterministic algorithms outperform probabilistic CKY methods. Kudo and Matsumoto (2002) applied the 361 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 361–368 Manchester, August 2008 cascaded chunking algorithm (hereafter “CC algorithm”) to Japanese dependency parsing. Yamada’s method (Yamada and Matsumoto, 2003) employed a similar algorithm. Sassano (2004) proposed a linear-order shift-reduce-like algorithm (hereafter “SR algorithm”), which is similar to Nivre’s algorithm (Nivre, 2003). These deterministic algorithms are biased to select nearer candidate heads since they examine the candidates sequentially, and once they find"
C08-1046,P05-1012,0,0.442665,"vs. non-projective, head-initial vs. head-final, and single-rooted vs. multi-rooted. Eisner (1996) proposed a CKY-like O(n3 ) algorithm. Yamada and Matsumoto (2003) proposed a shift-reducelike O(n2 ) deterministic algorithm. Nivre et al. (2003; 2004) also proposed a shift-reduce-like c 2008.  Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. O(n) deterministic algorithm for projective languages. The model is enhanced for non-projective languages by Nivre and Nilsson (2005). McDonald et al. (2005) proposed a method based on search of maximum spanning trees employing the Chu-Liu-Edmonds algorithm (hereafter “CLE algorithm”) (Chu and Liu, 1965; Edmonds, 1967). Most Japanese dependency parsers are based on bunsetsu units, which are similar concept to English base phrases. The constraints in Japanese dependency structure are stronger than those in other languages. Japanese dependency structures have the following constraints: head-final, singlehead, single-rooted, connected, acyclic and projective. Figure 1 shows examples of Japanese sentences and their dependency structures. Each box repr"
C08-1046,W06-2920,0,\N,Missing
C08-1046,P05-1013,0,\N,Missing
C08-1046,D07-1096,0,\N,Missing
C08-1111,S07-1094,0,0.019138,"limited to Uncertain and Non-Uncertain because the purpose of ITSPOKE is to recognize the user’s problem or discomfort in a tutoring dialog. Our goal, on the other hand, is to classify the user’s emotions into more fine-grained emotion classes. In a more general research context, while quite a few studies have been presented about opinion mining and sentiment analysis (Liu, 2006), research into fine-grained emotion classification has emerged only recently. There are two approaches commonly used in emotion classification: a rulebased approach and a statistical approach. Masum et al. (2007) and Chaumartin (2007) propose a rule-based approach to emotion classification. Chaumartin has developed a linguistic rulebased system, which classifies the emotions engendered by news headlines using the WordNet, SentiWordNet, and WordNet-Affect lexical resources. The system detects the sentiment polarity for each word in a news headline based on linguistic resources, and then attempts emotion classification by using rules based on its knowledge of sentence structures. The recall of this system is low, however, because of the limited coverage of the lexical resources. Regarding the statistical approach, Kozareva e"
C08-1111,P97-1023,0,0.0294959,"emotion classification. Chaumartin has developed a linguistic rulebased system, which classifies the emotions engendered by news headlines using the WordNet, SentiWordNet, and WordNet-Affect lexical resources. The system detects the sentiment polarity for each word in a news headline based on linguistic resources, and then attempts emotion classification by using rules based on its knowledge of sentence structures. The recall of this system is low, however, because of the limited coverage of the lexical resources. Regarding the statistical approach, Kozareva et al. (2007) apply the theory of (Hatzivassiloglou and McKeown, 1997) and (Turney, 2002) to emotion classification and propose a method based on the co-occurrence distribution over content words and six emotion words (e.g. joy, fear). For example, birthday appears more often with joy, while war appears more often with fear. However, the accuracy achieved by their method is not practical in applications assumed in this paper. As we demonstrate in Section 4, our method significantly outperforms Kozareva’s method. 3 Emotion Classification 3.1 The basic idea We consider the task of emotion classification as a classification problem where a given input sentence (a u"
C08-1111,kawahara-kurohashi-2006-case,0,0.0202172,"Missing"
C08-1111,S07-1072,0,0.581091,"tin (2007) propose a rule-based approach to emotion classification. Chaumartin has developed a linguistic rulebased system, which classifies the emotions engendered by news headlines using the WordNet, SentiWordNet, and WordNet-Affect lexical resources. The system detects the sentiment polarity for each word in a news headline based on linguistic resources, and then attempts emotion classification by using rules based on its knowledge of sentence structures. The recall of this system is low, however, because of the limited coverage of the lexical resources. Regarding the statistical approach, Kozareva et al. (2007) apply the theory of (Hatzivassiloglou and McKeown, 1997) and (Turney, 2002) to emotion classification and propose a method based on the co-occurrence distribution over content words and six emotion words (e.g. joy, fear). For example, birthday appears more often with joy, while war appears more often with fear. However, the accuracy achieved by their method is not practical in applications assumed in this paper. As we demonstrate in Section 4, our method significantly outperforms Kozareva’s method. 3 Emotion Classification 3.1 The basic idea We consider the task of emotion classification as a"
C08-1111,W04-3239,1,0.80589,"t3} 4. {event4} 5. {event5} increase emotion similarity &lt;disappointment> 0.75 &lt;unpleasantness> 0.70 &lt;loneliness> 0.70 &lt;loneliness> 0.67 &lt;loneliness> 0.63 Ranking of emotion rank emotion score 2.0 1. &lt;loneliness> 2. &lt;disappointment> 0.75 3.&lt;unpleasantness> 0.70 voting Figure 3: An example of a word-polarity lattice Figure 4: Emotion Classification by kNN (k=5) Various methods have already been proposed for sentiment polarity classification, ranging from the use of co-occurrence with typical positive and negative words (Turney, 2002) to bag of words (Pang et al., 2002) and dependency structure (Kudo and Matsumoto, 2004). Our sentiment polarity classification model is trained with SVMs (Vapnik, 1995), and the features are {1-gram, 2-gram, 3gram} of words and the sentiment polarity of the words themselves. Figure 3 illustrates how the sentence “子供の教育の負担が増える (The cost of educating my child increases)” is encoded to a feature vector. Here we assume the sentiment polarity of the “子供 (child)” and “教育 (education)” are positive, while the “負担 (cost)” is negative. These polarity values are represented in parallel with the corresponding words, as shown in Figure 3. By expanding {1-gram, 2-gram, 3-gram} in this lattice"
C08-1111,P04-1045,0,0.018237,"y have forgotten to lock my house should be You’re worried about that. In this paper, we address the above issue of emotion classification in the context of humancomputer dialog, and demonstrate that massive examples of emotion-provoking events can be extracted from the Web with a reasonable accuracy and those examples can be used to build a semantic content-based model for fine-grained emotion classification. 2 Related Work Recently, several studies have reported about dialog systems that are capable of classifying emotions in a human-computer dialog (Batliner et al., 2004; Ang et al., 2002; Litman and Forbes-Riley, 2004; Rotaru et al., 2005). ITSPOKE is a tutoring dialog system, that can recognize the user’s emotion using acoustic-prosodic features and lexical features. However, the emotion classes are limited to Uncertain and Non-Uncertain because the purpose of ITSPOKE is to recognize the user’s problem or discomfort in a tutoring dialog. Our goal, on the other hand, is to classify the user’s emotions into more fine-grained emotion classes. In a more general research context, while quite a few studies have been presented about opinion mining and sentiment analysis (Liu, 2006), research into fine-grained em"
C08-1111,W02-1011,0,0.019451,"s rank event 1. {event1} 2. {event2} 2. {event3} 4. {event4} 5. {event5} increase emotion similarity &lt;disappointment> 0.75 &lt;unpleasantness> 0.70 &lt;loneliness> 0.70 &lt;loneliness> 0.67 &lt;loneliness> 0.63 Ranking of emotion rank emotion score 2.0 1. &lt;loneliness> 2. &lt;disappointment> 0.75 3.&lt;unpleasantness> 0.70 voting Figure 3: An example of a word-polarity lattice Figure 4: Emotion Classification by kNN (k=5) Various methods have already been proposed for sentiment polarity classification, ranging from the use of co-occurrence with typical positive and negative words (Turney, 2002) to bag of words (Pang et al., 2002) and dependency structure (Kudo and Matsumoto, 2004). Our sentiment polarity classification model is trained with SVMs (Vapnik, 1995), and the features are {1-gram, 2-gram, 3gram} of words and the sentiment polarity of the words themselves. Figure 3 illustrates how the sentence “子供の教育の負担が増える (The cost of educating my child increases)” is encoded to a feature vector. Here we assume the sentiment polarity of the “子供 (child)” and “教育 (education)” are positive, while the “負担 (cost)” is negative. These polarity values are represented in parallel with the corresponding words, as shown in Figure 3. B"
C08-1111,W06-1323,1,0.802163,"nt polarity errors, which are considered fatal errors in real dialog applications. 1 Introduction Previous research into human-computer interaction has mostly focused on task-oriented dialogs, where the goal is considered to be to achieve a c 2008.  Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. given task as precisely and efficiently as possible by exchanging information required for the task through dialog (Allen et al., 1994, etc.). More recent research (Foster, 2007; Tokuhisa and Terashima, 2006, etc.), on the other hand, has been providing evidence for the importance of the affective or emotional aspect in a wider range of dialogic contexts, which has been largely neglected in the context of task-oriented dialogs. A dialog system may be expected to serve, for example, as an active listening 1 partner of an elderly user living alone who sometimes wishes to have a chat. In such a context, the dialog system is expected to understand the user’s emotions and sympathize with the user. For example, given an utterence I traveled far to get to the shop, but it was closed from the user, if th"
C08-1111,P02-1053,0,0.0128052,"veloped a linguistic rulebased system, which classifies the emotions engendered by news headlines using the WordNet, SentiWordNet, and WordNet-Affect lexical resources. The system detects the sentiment polarity for each word in a news headline based on linguistic resources, and then attempts emotion classification by using rules based on its knowledge of sentence structures. The recall of this system is low, however, because of the limited coverage of the lexical resources. Regarding the statistical approach, Kozareva et al. (2007) apply the theory of (Hatzivassiloglou and McKeown, 1997) and (Turney, 2002) to emotion classification and propose a method based on the co-occurrence distribution over content words and six emotion words (e.g. joy, fear). For example, birthday appears more often with joy, while war appears more often with fear. However, the accuracy achieved by their method is not practical in applications assumed in this paper. As we demonstrate in Section 4, our method significantly outperforms Kozareva’s method. 3 Emotion Classification 3.1 The basic idea We consider the task of emotion classification as a classification problem where a given input sentence (a user’s utterance) is"
C08-1113,N06-1019,0,0.0211435,"sks. 10 Although the order in which the candidate tags appear has not been standardized in the PTB corpus, we assume that annotators might order the candidate tags with their conﬁdence. 902 Ex.1 Ex.2 P APA P APA mrg 94.39 73.10 95.08 76.70 random 94.27 71.58 94.98 74.27 ﬁrst 94.26 72.65 94.97 75.28 frequent 94.27 71.68 94.97 74.32 discarded 94.19 71.91 94.98 75.16 Table 5: The average POS tagging performance over 5 trials. Our model is interpreted as one of the CRFs with hidden variables (Quattoni et al., 2004). There are previous work which handles hidden variables in discriminative parsers (Clark and Curran, 2006; Petrov and Klein, 2008). In their methods, the objective functions are also formulated as same as equation (3). For interactive annotation, Culotta et al. (2006) proposed corrective feedback that effectively reduces user operations utilizing partial annotations. Although they assume that the users correct entire label structures so that the CRFs are trained as usual, our proposed method extends their system when the users cannot annotate all of the labels in a sentence. 7 Conclusions and Future Work We are proposing a parameter estimation method for CRFs incorporating partial or ambiguous an"
C08-1113,W04-3230,1,0.42517,"es for non-segmented languages such as Japanese or Chinese. For example, the correct segmentation of the Japanese phrase “ΓইΓই” (incised wound or abrasion) is shown by the lowest boxes segmented by the solid lines in Figure 1. However, there are several overlapping segmentation candidates, which are shown by the other boxes, and possible segmentation by the dashed lines. Thus, the decisions on the word segmentation require considering the context, so simple dictionary lookup approach is not appropriate. Therefore statistical methods have been successfully used for JWS tasks. Previous work (Kudo et al., 2004) showed CRFs outperform generative Markov models and discriminative history-based methods in JWS. In practice, a statistical word segment analyzer tends to perform worse for text from different domains, so that additional annotations for each target domain are required. A major cause of errors is the occurrence of unknown words. For example, if “Γই” (abrasion) is an unknown word, the system may accept the word sequence of “Γ ইΓই” as “Γই” (incised wound), “ Γ” (ﬁle), and “ই” (injury) by mistake. On one hand, lists of new terms in the target domain are often available in the forms of tech"
C08-1113,W03-1025,0,0.0161797,"lowing sentence from the PTB corpus includes an ambiguous annotation for the POS tag of “pending”: That/DT suit/NN is/VBZ pending/VBG|JJ ./. , where words are paired with their part-of-speech tag by a forward slash (“/”).2 Uncertainty concerning the proper POS tag of “pending” is represented by the disjunctive POS tag (“VBG and JJ”) as indicated by a vertical bar. The existence of the ambiguous annotations is due to the task deﬁnition itself, the procedure man1 The boundary policies of some words are different even among linguists. In addition, the boundary agreement is even lower in Chinese (Luo, 2003). 2 These POS tags used here are DT:determiner, NN:common noun, VBZ:present tense 3rd person singular verb, VBG:gerund or present participle verb, JJ:adjective, NNS:plural noun, RBR:comparative adverb, IN:preposition or subordinating conjunction, and RB:adverb. 898 frequency 15 10 7 4 word data more pending than POS tags NN|NNS JJR|RBR JJ|VBG IN|RB Table 1: Words in the PTB with ambiguous POSs. ual for the annotators, or the inadequate knowledge of the annotators. Ideally, the annotations should be disambiguated by a skilled annotator for the training data. However, even the PTB corpus, whose"
C08-1113,J93-2004,0,0.0394081,"nd properly incorporates partial annotations. 5.2 Part-of-speech Tagging Task In this section, we show the results of the POS tagging experiments to assess the proposed method using ambiguous annotations. 9 We selected word occurrences in a batch mode since each training of the CRFs takes too much time for interactive use. 901 ambiguous sentences (training) unique sentences (training) unique sentences (test) Ex.1 Ex.2 118 1,480 2,960 11,840 Table 4: Training and test data for POS tagging. As mentioned in Section 2.2, there are words which have two or more candidate POS tags in the PTB corpus (Marcus et al., 1993). In this experiment, we used 118 sentences in which some words (82 distinct words) are annotated with ambiguous POS tags, and these sentences are called the POS ambiguous sentences. On the other hand, we call sentences in which the POS tags of these terms are uniquely annotated as the POS unique sentences. The goal of this experiment is to effectively improve the tagging performance using both these POS ambiguous sentences and the POS unique sentences as the training data. We assume that the amount of training data is not sufﬁcient to ignore the POS ambiguous sentences, or that the POS ambigu"
C08-1113,C04-1081,0,0.209878,". Then the supervised structured output problem can be deﬁned as learning a map X → Y . In the Japanese word segmentation task, x can represent a given sequence of character boundaries and y is a sequence of the corresponding labels, which specify whether the current position is a word boundary.3 In the POS tagging task, x represents a word sequence and y is a corresponding POS tag sequence. An incomplete annotation, then, is deﬁned as a sequence of subset of the label set instead of a sequence of labels. Let L=(L1 , L2 , · · · , LT ) be a sequence of label subsets for an observed sequence 3 Peng et al. (2004) deﬁned the word segmentation problem as labeling each character as whether or not the previous character boundary of the current character is a word boundary. However, we employ our problem formulation since it is redundant to assign the ﬁrst character of a sentence as the word boundary in their formulation. x, where Lt ∈ 2Y − {∅}. The partial annotation at position s is where Ls is a singleton and the rest Lt=s is Y . For example, if a sentence with 6 character boundaries (7 characters) is partially annotated using the KWIC UI described in Section 2.1, a word annotation where its boundary b"
C08-1113,P92-1017,0,0.172527,"Missing"
C08-1113,N03-1028,0,0.0208949,"lizes the unknown ys out. Then the maximum likelihood estimator for this model can be obtained by maximizing the log likelihood function: LL(θ) = = N   n=1 N  ln P„ (YL(n) |x (n) ) n=1 ln Z„,x(n) ,Y L(n) − ln Z„,x(n) ,Y (3)  . This modeling naturally embraces label ambiguities in the incomplete annotation.4 Unfortunately, equation (3) is not a concave function5 so that there are local maxima in the objective function. Although this non-concavity prevents efﬁcient global maximization of equation (3), it still allows us to incorporate incomplete annotations using gradient ascent iterations (Sha and Pereira, 2003). Gradient ascent methods require the partial derivative of equation (3): ⎛ N ∂ LL(θ) ⎝  = P„ (y|YL(n) , x(n) )Φ(x(n) , y) ∂θ n=1 y∈Y (n) L ⎞  P„ (y|x(n) )Φ(x(n) , y)⎠ , (4) − y∈Y where P„ (y|YL , x) = e„·Φ(x,y) Z„,x,YL (5) is a conditional probability that is normalized over YL . Equations (3) and (4) include the summations of all of the label sequences in Y or YL . It is not practical to enumerate and evaluate all of the label conﬁgurations explicitly, since the number of all of the possible label sequences is exponential on the number of positions t with |Lt |> 1. However, under the Mark"
C08-1113,H05-1010,0,0.0100293,"ll of the labels in a sentence. 7 Conclusions and Future Work We are proposing a parameter estimation method for CRFs incorporating partial or ambiguous annotations of structured data. The empirical results suggest that the proposed method reduces the domain adaptation costs, and improves the prediction performance for the linguistic phenomena that are sometimes difﬁcult for people to label. The proposed method is applicable to other structured output tasks in NLP, such as syntactic parsing, information extraction, and so on. However, there are some NLP tasks, such as the word alignment task (Taskar et al., 2005), in which it is not possible to efﬁciently calculate the sum score of all of the possible label conﬁgurations. Recently, Verbeek and Triggs (2008) independently proposed a parameter estimation method for CRFs using partially labeled images. Although the objective function in their formulation is equivalent to equation (3), they used Loopy Belief Propagation to approximate the sum score for their application (scene segmentation). Their results imply these approximation methods can be used for such applications that cannot use dynamic programming techniques. Acknowledgments We would like to tha"
C12-1066,W06-2920,0,0.0236693,"Missing"
C12-1066,J90-1003,0,0.191249,"r Enju2 . The outputs of Enju are then converted to parse graphs. The edges of the resulting parse graph have one of the following labels: “forward dependency”, “backward dependency”, “semantic subject (forward semantic subject)”, and “semantic predicate (backward semantic subject).” We next collect pairs of (stemmed and lemmatized) words such that (i) each word occurs at least 10 times in BNC, and (ii) the pair is linked by a syntactic/semantic relation at least once in the parse graph collection we converted from BNC, but excluding the pairs for which the pointwise mutual information (pmi) (Church and Hanks, 1990) between the words are less than 1. Finally, for 2 http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html 1089 every word w occurring in this collection, we make a type vector in which each component corresponds to a pair (w ′ , r) of another word w ′ and the relation type r between w and w ′ , and holds the pmi score of word w and the pair (w ′ , r), In preliminary experiments, we also tested type vectors made from the words in a fixed contextual window size (i.e., without using parser outputs), but the results were inferior. 5.1 Experiment 1: word sense disambiguation For the WSD experiment, w"
C12-1066,D11-1096,0,0.0196707,"Our kernels take this into consideration, as reflected in Eq. (5) and the first term of Eq. (6). Both the bag-of-walks kernels and the Desrosiers-Karypis kernels borrow idea from Kashima et al. (2003) who defined a kernel (marginalized graph kernel) between two graphs (not graph nodes) on the basis of the marginal probability of parallel random walks taking place in the two graphs. Elegant product graph formulations of various graph kernels can be found in Vishwanathan et al. (2010). 4.4.2 Tree kernels Tree kernels (Collins and Duffy, 2001; Kashima and Koyanagi, 2002; Moschitti et al., 2008; Croce et al., 2011) efficiently count all common subtrees in two tree-structured data. Although it is tempting to apply these kernels to measure the similarity between the dependency (sub)trees surrounding the target words, it should be noted that our goal is to measure the similarity of particular word pairs in two sentences, and not of the sentences or their entire dependency trees. In particular, tree kernels do not give any special treatment of the target words, and thus they even count subtrees that do not involve a target word at all. Tree kernels are hence not suitable for measuring word similarity. In co"
C12-1066,D10-1113,0,0.228078,"ed with the help of contextual similarity (Jurafsky and Martin, 2008, Section 20.7). Polysemous words have identical surface forms, but the contexts in which they appear are often distinct; and even though synonyms have different surface forms, they are expected to appear in similar context. There has been a volume of work investigating semantic similarity of words in context. However, syntactic or semantic structure in the context has not been fully explored. The methods representing context as a bag of words or a bag of n-grams (Schütze, 1998; Reisinger and Mooney, 2010; Erk and Padó, 2010; Dinu and Lapata, 2010; Giuliano et al., 2009) ignore the underlying syntactic/semantic structure. Recent studies on compositional semantics of words (Mitchell and Lapata, 2008; Erk and Padó, 2008; Thater et al., 2010) take advantage of structure in context, but they only use information of words that directly stand in a predicate-argument relation with the target word. Consider the following sentences (i) and (ii), shown with the word dependency relations. (i) The branch is referred to as the face of the bank. (ii) These plants shielded the face of the bank. In sentence (i), the noun (homonym) bank means “a financ"
C12-1066,D08-1094,0,0.346087,"en distinct; and even though synonyms have different surface forms, they are expected to appear in similar context. There has been a volume of work investigating semantic similarity of words in context. However, syntactic or semantic structure in the context has not been fully explored. The methods representing context as a bag of words or a bag of n-grams (Schütze, 1998; Reisinger and Mooney, 2010; Erk and Padó, 2010; Dinu and Lapata, 2010; Giuliano et al., 2009) ignore the underlying syntactic/semantic structure. Recent studies on compositional semantics of words (Mitchell and Lapata, 2008; Erk and Padó, 2008; Thater et al., 2010) take advantage of structure in context, but they only use information of words that directly stand in a predicate-argument relation with the target word. Consider the following sentences (i) and (ii), shown with the word dependency relations. (i) The branch is referred to as the face of the bank. (ii) These plants shielded the face of the bank. In sentence (i), the noun (homonym) bank means “a financial institution” whereas the one in sentence (ii) means “the slope of land adjoining a river.” If these sentences are treated as bags of words, we see that the words unique t"
C12-1066,P10-2017,0,0.1074,"lems can be alleviated with the help of contextual similarity (Jurafsky and Martin, 2008, Section 20.7). Polysemous words have identical surface forms, but the contexts in which they appear are often distinct; and even though synonyms have different surface forms, they are expected to appear in similar context. There has been a volume of work investigating semantic similarity of words in context. However, syntactic or semantic structure in the context has not been fully explored. The methods representing context as a bag of words or a bag of n-grams (Schütze, 1998; Reisinger and Mooney, 2010; Erk and Padó, 2010; Dinu and Lapata, 2010; Giuliano et al., 2009) ignore the underlying syntactic/semantic structure. Recent studies on compositional semantics of words (Mitchell and Lapata, 2008; Erk and Padó, 2008; Thater et al., 2010) take advantage of structure in context, but they only use information of words that directly stand in a predicate-argument relation with the target word. Consider the following sentences (i) and (ii), shown with the word dependency relations. (i) The branch is referred to as the face of the bank. (ii) These plants shielded the face of the bank. In sentence (i), the noun (homony"
C12-1066,J09-4007,0,0.282884,"textual similarity (Jurafsky and Martin, 2008, Section 20.7). Polysemous words have identical surface forms, but the contexts in which they appear are often distinct; and even though synonyms have different surface forms, they are expected to appear in similar context. There has been a volume of work investigating semantic similarity of words in context. However, syntactic or semantic structure in the context has not been fully explored. The methods representing context as a bag of words or a bag of n-grams (Schütze, 1998; Reisinger and Mooney, 2010; Erk and Padó, 2010; Dinu and Lapata, 2010; Giuliano et al., 2009) ignore the underlying syntactic/semantic structure. Recent studies on compositional semantics of words (Mitchell and Lapata, 2008; Erk and Padó, 2008; Thater et al., 2010) take advantage of structure in context, but they only use information of words that directly stand in a predicate-argument relation with the target word. Consider the following sentences (i) and (ii), shown with the word dependency relations. (i) The branch is referred to as the face of the bank. (ii) These plants shielded the face of the bank. In sentence (i), the noun (homonym) bank means “a financial institution” whereas"
C12-1066,W04-2405,0,0.0356876,"and evaluate the performance. 5.1.2 Experimental procedure WSD with the bag-of-walks kernels consists of four steps: (1) computing the cosine of local context vectors computed from the contextual paragraphs, (2) computing bag-of-walks kernels on parse graphs output by a parser, (3) combining kernels computed in Steps 1 and 2, and (4) sense prediction by support vector machines (SVMs) using the combined kernels. We detail each step below. Step 1. Compute the cosine of local context vectors. We construct a local context vector, for each target instance according to a standard procedure for WSD (Mihalcea, 2004; Navigli, 2009). Specifically, we treat a contextual paragraph simply as a bag-of-words. After removing stop words4 , we make a local context vector in which components are the weighted frequencies (tf-idf) of words in the “bag.” Then we construct a matrix holding cosine between every pairs of local context vectors. Because cosine matrices are positive semi-definite, this matrix can be regarded as a kernel matrix. Step 2. Compute bag-of-walks kernels. We compute a bag-of-walks kernel between instances of a target word as follows. For each target instance, we pick up a sentence containing the"
C12-1066,W04-0807,0,0.0439758,"p/enju/index.html 1089 every word w occurring in this collection, we make a type vector in which each component corresponds to a pair (w ′ , r) of another word w ′ and the relation type r between w and w ′ , and holds the pmi score of word w and the pair (w ′ , r), In preliminary experiments, we also tested type vectors made from the words in a fixed contextual window size (i.e., without using parser outputs), but the results were inferior. 5.1 Experiment 1: word sense disambiguation For the WSD experiment, we apply the bag-of-walks kernels to the Senseval-3 English lexical sample (ELS) task (Mihalcea et al., 2004). 5.1.1 Task and dataset The Senseval-3 ELS dataset is a collection of instances of polysemous target words and the contextual paragraphs in which they appear, consisting mostly of several sentences. Each target instance is annotated with one or more gold standard senses selected from a sense inventory (also distributed with the dataset). The dataset comes with predefined training/test splits, and the task goal is to predict a sense for each of the 3944 test instances of 57 polysemous words: 1807 instances for 20 nouns, 1978 for 32 verbs, and 159 for 5 adjectives. A standard approach for this"
C12-1066,P08-1028,0,0.27206,"n which they appear are often distinct; and even though synonyms have different surface forms, they are expected to appear in similar context. There has been a volume of work investigating semantic similarity of words in context. However, syntactic or semantic structure in the context has not been fully explored. The methods representing context as a bag of words or a bag of n-grams (Schütze, 1998; Reisinger and Mooney, 2010; Erk and Padó, 2010; Dinu and Lapata, 2010; Giuliano et al., 2009) ignore the underlying syntactic/semantic structure. Recent studies on compositional semantics of words (Mitchell and Lapata, 2008; Erk and Padó, 2008; Thater et al., 2010) take advantage of structure in context, but they only use information of words that directly stand in a predicate-argument relation with the target word. Consider the following sentences (i) and (ii), shown with the word dependency relations. (i) The branch is referred to as the face of the bank. (ii) These plants shielded the face of the bank. In sentence (i), the noun (homonym) bank means “a financial institution” whereas the one in sentence (ii) means “the slope of land adjoining a river.” If these sentences are treated as bags of words, we see tha"
C12-1066,J08-2003,0,0.012755,"s 1088 with zero length. Our kernels take this into consideration, as reflected in Eq. (5) and the first term of Eq. (6). Both the bag-of-walks kernels and the Desrosiers-Karypis kernels borrow idea from Kashima et al. (2003) who defined a kernel (marginalized graph kernel) between two graphs (not graph nodes) on the basis of the marginal probability of parallel random walks taking place in the two graphs. Elegant product graph formulations of various graph kernels can be found in Vishwanathan et al. (2010). 4.4.2 Tree kernels Tree kernels (Collins and Duffy, 2001; Kashima and Koyanagi, 2002; Moschitti et al., 2008; Croce et al., 2011) efficiently count all common subtrees in two tree-structured data. Although it is tempting to apply these kernels to measure the similarity between the dependency (sub)trees surrounding the target words, it should be noted that our goal is to measure the similarity of particular word pairs in two sentences, and not of the sentences or their entire dependency trees. In particular, tree kernels do not give any special treatment of the target words, and thus they even count subtrees that do not involve a target word at all. Tree kernels are hence not suitable for measuring w"
C12-1066,N10-1013,0,0.0144361,"hich they appear. These problems can be alleviated with the help of contextual similarity (Jurafsky and Martin, 2008, Section 20.7). Polysemous words have identical surface forms, but the contexts in which they appear are often distinct; and even though synonyms have different surface forms, they are expected to appear in similar context. There has been a volume of work investigating semantic similarity of words in context. However, syntactic or semantic structure in the context has not been fully explored. The methods representing context as a bag of words or a bag of n-grams (Schütze, 1998; Reisinger and Mooney, 2010; Erk and Padó, 2010; Dinu and Lapata, 2010; Giuliano et al., 2009) ignore the underlying syntactic/semantic structure. Recent studies on compositional semantics of words (Mitchell and Lapata, 2008; Erk and Padó, 2008; Thater et al., 2010) take advantage of structure in context, but they only use information of words that directly stand in a predicate-argument relation with the target word. Consider the following sentences (i) and (ii), shown with the word dependency relations. (i) The branch is referred to as the face of the bank. (ii) These plants shielded the face of the bank. In sentence ("
C12-1066,J98-1004,0,0.580902,"documents in which they appear. These problems can be alleviated with the help of contextual similarity (Jurafsky and Martin, 2008, Section 20.7). Polysemous words have identical surface forms, but the contexts in which they appear are often distinct; and even though synonyms have different surface forms, they are expected to appear in similar context. There has been a volume of work investigating semantic similarity of words in context. However, syntactic or semantic structure in the context has not been fully explored. The methods representing context as a bag of words or a bag of n-grams (Schütze, 1998; Reisinger and Mooney, 2010; Erk and Padó, 2010; Dinu and Lapata, 2010; Giuliano et al., 2009) ignore the underlying syntactic/semantic structure. Recent studies on compositional semantics of words (Mitchell and Lapata, 2008; Erk and Padó, 2008; Thater et al., 2010) take advantage of structure in context, but they only use information of words that directly stand in a predicate-argument relation with the target word. Consider the following sentences (i) and (ii), shown with the word dependency relations. (i) The branch is referred to as the face of the bank. (ii) These plants shielded the fac"
C12-1066,P10-1097,0,0.202048,"n though synonyms have different surface forms, they are expected to appear in similar context. There has been a volume of work investigating semantic similarity of words in context. However, syntactic or semantic structure in the context has not been fully explored. The methods representing context as a bag of words or a bag of n-grams (Schütze, 1998; Reisinger and Mooney, 2010; Erk and Padó, 2010; Dinu and Lapata, 2010; Giuliano et al., 2009) ignore the underlying syntactic/semantic structure. Recent studies on compositional semantics of words (Mitchell and Lapata, 2008; Erk and Padó, 2008; Thater et al., 2010) take advantage of structure in context, but they only use information of words that directly stand in a predicate-argument relation with the target word. Consider the following sentences (i) and (ii), shown with the word dependency relations. (i) The branch is referred to as the face of the bank. (ii) These plants shielded the face of the bank. In sentence (i), the noun (homonym) bank means “a financial institution” whereas the one in sentence (ii) means “the slope of land adjoining a river.” If these sentences are treated as bags of words, we see that the words unique to (i) and (ii) are res"
C12-1066,S07-1009,0,\N,Missing
C12-1144,P12-2073,0,0.0231917,"ract confusion pairs that consist of words before and after correction. A number of studies depend on such edit distance between written and corrected words as Levenshtein Distance (LD), Longest Common Subsequence (LCS) string matching, and pronunciation similarities (Kukich, 1992; Brill and Moore, 2000; Islam and Inkpen, 2009; Bao et al., 2011; Toutanova and Moore, 2002). In order to cover more misspelled words, many spelling errors were collected from web search queries and their results (Chen et al., 2007; Gao et al., 2010), click through logs (Sun et al., 2010), and users’ keystroke logs (Baba and Suzuki, 2012). Note that previous studies for spelling correction described above focus on errors made by native speakers rather than second language learners, who show a wider range of misspellings with, for example, split, merge, inflection and derivation errors. 2359 In most grammatical error detection and correction research, spelling error correction is performed before such linguistic analysis as POS tagging and syntactic parsing. Spelling correction as preprocessing generally uses existing spelling checkers such as GNU Aspell2 and Jazzy3 , which depend on edit distance between words before and after"
C12-1144,P11-1091,0,0.0590753,"their), confusion (*form/from), split (*Now a day/Nowadays), merge (*swimingpool/swimming pool), inflection (*please/pleased), and derivation (*badly/bad) errors. Unlike typographical errors, these spelling errors are difficult to detect because the words to be corrected are possible words in English. Previous studies in spelling correction for ESL writing depend mainly on edit distance between the words before and after correction. Some previous works for correcting misspelled words in native speaker misspellings focus on homophone, confusion, split, and merge errors (Golding and Roth, 1999; Bao et al., 2011), but no research has been done on inflection and derivation errors. One of the biggest problems in grammatical error detection and correction studies is that ESL writing contains spelling errors, and they are often obstacles to POS tagging and syntactic parsing. For example, POS tagging fails for the following sentence1 : Input: ... it is *verey/very *convent/convenient for the group. without spelling error correction: ... it/PRP, is/VBZ, verey/PRP, convent/NN ... with spelling error correction: ... it/PRP, is/VBZ, very/RB, convenient/JJ ... Conversely, spelling correction requires POS inform"
C12-1144,P00-1037,0,0.0271202,", and joint analysis (Section 2), and then describe our proposed method in detail (Section 3). The experimental setting and the results are presented in Section 4, and error analysis is given in Section 5. Finally, we conclude in Section 6. 2 Related works In spelling error correction, the main concern is how to extract confusion pairs that consist of words before and after correction. A number of studies depend on such edit distance between written and corrected words as Levenshtein Distance (LD), Longest Common Subsequence (LCS) string matching, and pronunciation similarities (Kukich, 1992; Brill and Moore, 2000; Islam and Inkpen, 2009; Bao et al., 2011; Toutanova and Moore, 2002). In order to cover more misspelled words, many spelling errors were collected from web search queries and their results (Chen et al., 2007; Gao et al., 2010), click through logs (Sun et al., 2010), and users’ keystroke logs (Baba and Suzuki, 2012). Note that previous studies for spelling correction described above focus on errors made by native speakers rather than second language learners, who show a wider range of misspellings with, for example, split, merge, inflection and derivation errors. 2359 In most grammatical erro"
C12-1144,P06-1032,0,0.0327827,"s preprocessing generally uses existing spelling checkers such as GNU Aspell2 and Jazzy3 , which depend on edit distance between words before and after correction. Then, candidate words are often re-ranked or filtered using a language model. In fact, in the Helping Our Own (HOO) 2012 (Dale et al., 2012), which is a shared task on preposition and determiner error correction, highlyranked teams employ the strategy of spelling correction as preprocessing based on edit distance. Some recent studies deal with spelling correction at the same time as whole grammatical error correction. For example, (Brockett et al., 2006) presents a method to correct whole sentences containing various errors, applying a statistical machine translation (SMT) technique where input sentences are translated into correct English. Although this approach can deal with any type of spelling errors, it suffers from a poverty of error-annotated resources and cannot correct misspelled words that have never appeared in a corpus. Similarly, (Park and Levy, 2011) propose a noisy channel model to correct errors, although they depend on a bigram language model and do not use syntactic information. A discriminative approach for whole grammatica"
C12-1144,D07-1019,0,0.0210056,", we conclude in Section 6. 2 Related works In spelling error correction, the main concern is how to extract confusion pairs that consist of words before and after correction. A number of studies depend on such edit distance between written and corrected words as Levenshtein Distance (LD), Longest Common Subsequence (LCS) string matching, and pronunciation similarities (Kukich, 1992; Brill and Moore, 2000; Islam and Inkpen, 2009; Bao et al., 2011; Toutanova and Moore, 2002). In order to cover more misspelled words, many spelling errors were collected from web search queries and their results (Chen et al., 2007; Gao et al., 2010), click through logs (Sun et al., 2010), and users’ keystroke logs (Baba and Suzuki, 2012). Note that previous studies for spelling correction described above focus on errors made by native speakers rather than second language learners, who show a wider range of misspellings with, for example, split, merge, inflection and derivation errors. 2359 In most grammatical error detection and correction research, spelling error correction is performed before such linguistic analysis as POS tagging and syntactic parsing. Spelling correction as preprocessing generally uses existing sp"
C12-1144,P11-1092,0,0.0206235,"novel types of misspelling in ESL writing. Keywords: Part-of-Speech Tagging, Spelling Error Correction. Proceedings of COLING 2012: Technical Papers, pages 2357–2374, COLING 2012, Mumbai, December 2012. 2357 1 Introduction Automated grammatical error detection and correction have been focused on natural language processing (NLP) over the past dozen years or so. Researchers have mainly studied English grammatical error detection and correction of areas such as determiners, prepositions and verbs (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Lee and Seneff, 2008; Gamon, 2010; Dahlmeier and Ng, 2011; Rozovskaya and Roth, 2011; Tajiri et al., 2012). In previous work on grammatical error detection and correction, spelling errors are usually corrected in a preprocessing step in a pipeline. These studies generally deal with typographical errors (e.g. *begginning/beginning). In ESL writing, however, there exist many other types of spelling errors, which often occur in combination with, for example, homophone (*there/their), confusion (*form/from), split (*Now a day/Nowadays), merge (*swimingpool/swimming pool), inflection (*please/pleased), and derivation (*badly/bad) errors. Unlike typograph"
C12-1144,D12-1052,0,0.151857,"g various errors, applying a statistical machine translation (SMT) technique where input sentences are translated into correct English. Although this approach can deal with any type of spelling errors, it suffers from a poverty of error-annotated resources and cannot correct misspelled words that have never appeared in a corpus. Similarly, (Park and Levy, 2011) propose a noisy channel model to correct errors, although they depend on a bigram language model and do not use syntactic information. A discriminative approach for whole grammatical error correction is also proposed in a recent study (Dahlmeier and Ng, 2012) where spelling errors are corrected simultaneously. In terms of spelling error types, however, typographical errors using GNU Aspell are dealt with, but not other misspelling types such as split and merge errors. Our proposed model uses POS features in order to correct spelling. As result, a wider range of spelling errors such as inflection and derivation errors can be corrected. Inflection and derivation errors are usually regarded as grammatical errors, not spelling errors. However, we include inflection and derivation error correction in our task, given the difficulty of determining whethe"
C12-1144,W12-2006,0,0.0136988,"e learners, who show a wider range of misspellings with, for example, split, merge, inflection and derivation errors. 2359 In most grammatical error detection and correction research, spelling error correction is performed before such linguistic analysis as POS tagging and syntactic parsing. Spelling correction as preprocessing generally uses existing spelling checkers such as GNU Aspell2 and Jazzy3 , which depend on edit distance between words before and after correction. Then, candidate words are often re-ranked or filtered using a language model. In fact, in the Helping Our Own (HOO) 2012 (Dale et al., 2012), which is a shared task on preposition and determiner error correction, highlyranked teams employ the strategy of spelling correction as preprocessing based on edit distance. Some recent studies deal with spelling correction at the same time as whole grammatical error correction. For example, (Brockett et al., 2006) presents a method to correct whole sentences containing various errors, applying a statistical machine translation (SMT) technique where input sentences are translated into correct English. Although this approach can deal with any type of spelling errors, it suffers from a poverty"
C12-1144,C08-1022,0,0.0300052,"ne analysis. We also show that the joint model can deal with novel types of misspelling in ESL writing. Keywords: Part-of-Speech Tagging, Spelling Error Correction. Proceedings of COLING 2012: Technical Papers, pages 2357–2374, COLING 2012, Mumbai, December 2012. 2357 1 Introduction Automated grammatical error detection and correction have been focused on natural language processing (NLP) over the past dozen years or so. Researchers have mainly studied English grammatical error detection and correction of areas such as determiners, prepositions and verbs (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Lee and Seneff, 2008; Gamon, 2010; Dahlmeier and Ng, 2011; Rozovskaya and Roth, 2011; Tajiri et al., 2012). In previous work on grammatical error detection and correction, spelling errors are usually corrected in a preprocessing step in a pipeline. These studies generally deal with typographical errors (e.g. *begginning/beginning). In ESL writing, however, there exist many other types of spelling errors, which often occur in combination with, for example, homophone (*there/their), confusion (*form/from), split (*Now a day/Nowadays), merge (*swimingpool/swimming pool), inflection (*please/ple"
C12-1144,C10-1041,0,0.0540719,"ction 6. 2 Related works In spelling error correction, the main concern is how to extract confusion pairs that consist of words before and after correction. A number of studies depend on such edit distance between written and corrected words as Levenshtein Distance (LD), Longest Common Subsequence (LCS) string matching, and pronunciation similarities (Kukich, 1992; Brill and Moore, 2000; Islam and Inkpen, 2009; Bao et al., 2011; Toutanova and Moore, 2002). In order to cover more misspelled words, many spelling errors were collected from web search queries and their results (Chen et al., 2007; Gao et al., 2010), click through logs (Sun et al., 2010), and users’ keystroke logs (Baba and Suzuki, 2012). Note that previous studies for spelling correction described above focus on errors made by native speakers rather than second language learners, who show a wider range of misspellings with, for example, split, merge, inflection and derivation errors. 2359 In most grammatical error detection and correction research, spelling error correction is performed before such linguistic analysis as POS tagging and syntactic parsing. Spelling correction as preprocessing generally uses existing spelling checkers suc"
C12-1144,P08-1043,0,0.0312985,"regarded as grammatical errors, not spelling errors. However, we include inflection and derivation error correction in our task, given the difficulty of determining whether they are grammatical or spelling errors, as will be explained in Section 4.1. Joint learning and joint analysis have received much attention in recent studies for linguistic analysis. For example, the CoNLL-2008 Shared Task (Surdeanu et al., 2008) shows promising results in joint syntactic and semantic dependency parsing. There are also models that deal with joint morphological segmentation and syntactic parsing in Hebrew (Goldberg and Tsarfaty, 2008), joint word segmentation and POS tagging in Chinese (Zhang and Clark, 2010), and joint word segmentation, POS tagging and dependency parsing in Chinese (Hatori et al., 2012). These studies demonstrate that joint models outperform conventional pipelined systems. Our work applies for the first time a joint analysis to spelling correction and POS tagging for ESL writing in which input sentences contains multiple errors, whereas previous joint models deal only with canonical texts. 3 Joint analysis of POS tagging and spelling correction In this section, we describe our proposed joint analysis of"
C12-1144,P12-1110,0,0.0225835,"grammatical or spelling errors, as will be explained in Section 4.1. Joint learning and joint analysis have received much attention in recent studies for linguistic analysis. For example, the CoNLL-2008 Shared Task (Surdeanu et al., 2008) shows promising results in joint syntactic and semantic dependency parsing. There are also models that deal with joint morphological segmentation and syntactic parsing in Hebrew (Goldberg and Tsarfaty, 2008), joint word segmentation and POS tagging in Chinese (Zhang and Clark, 2010), and joint word segmentation, POS tagging and dependency parsing in Chinese (Hatori et al., 2012). These studies demonstrate that joint models outperform conventional pipelined systems. Our work applies for the first time a joint analysis to spelling correction and POS tagging for ESL writing in which input sentences contains multiple errors, whereas previous joint models deal only with canonical texts. 3 Joint analysis of POS tagging and spelling correction In this section, we describe our proposed joint analysis of spelling error correction and POS tagging for ESL writing. Our method is based on Japanese morphological analysis (Kudo et al., 2004), which disambiguates word boundaries and"
C12-1144,D09-1129,0,0.0264771,"ection 2), and then describe our proposed method in detail (Section 3). The experimental setting and the results are presented in Section 4, and error analysis is given in Section 5. Finally, we conclude in Section 6. 2 Related works In spelling error correction, the main concern is how to extract confusion pairs that consist of words before and after correction. A number of studies depend on such edit distance between written and corrected words as Levenshtein Distance (LD), Longest Common Subsequence (LCS) string matching, and pronunciation similarities (Kukich, 1992; Brill and Moore, 2000; Islam and Inkpen, 2009; Bao et al., 2011; Toutanova and Moore, 2002). In order to cover more misspelled words, many spelling errors were collected from web search queries and their results (Chen et al., 2007; Gao et al., 2010), click through logs (Sun et al., 2010), and users’ keystroke logs (Baba and Suzuki, 2012). Note that previous studies for spelling correction described above focus on errors made by native speakers rather than second language learners, who show a wider range of misspellings with, for example, split, merge, inflection and derivation errors. 2359 In most grammatical error detection and correcti"
C12-1144,W04-3230,1,0.574724,"ing and dependency parsing in Chinese (Hatori et al., 2012). These studies demonstrate that joint models outperform conventional pipelined systems. Our work applies for the first time a joint analysis to spelling correction and POS tagging for ESL writing in which input sentences contains multiple errors, whereas previous joint models deal only with canonical texts. 3 Joint analysis of POS tagging and spelling correction In this section, we describe our proposed joint analysis of spelling error correction and POS tagging for ESL writing. Our method is based on Japanese morphological analysis (Kudo et al., 2004), which disambiguates word boundaries and assigns POS tags using re-defined Conditional Random Fields (CRFs) (Lafferty et al., 1999), while the original CRFs deal with sequential labeling for sentences with word boundaries fixed. We use the re-defined CRFs rather than the original CRFs because disambiguating word boundaries is necessary for split and merge error correction. In terms of decoding, our model has a similar approach to the decoder proposed by (Dahlmeier and Ng, 2012), though the decoder by Dahlmeier and Ng uses beam search. In (Kudo et al., 2004),  they define CRFs as the conditio"
C12-1144,P08-1021,0,0.0164348,"that the joint model can deal with novel types of misspelling in ESL writing. Keywords: Part-of-Speech Tagging, Spelling Error Correction. Proceedings of COLING 2012: Technical Papers, pages 2357–2374, COLING 2012, Mumbai, December 2012. 2357 1 Introduction Automated grammatical error detection and correction have been focused on natural language processing (NLP) over the past dozen years or so. Researchers have mainly studied English grammatical error detection and correction of areas such as determiners, prepositions and verbs (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Lee and Seneff, 2008; Gamon, 2010; Dahlmeier and Ng, 2011; Rozovskaya and Roth, 2011; Tajiri et al., 2012). In previous work on grammatical error detection and correction, spelling errors are usually corrected in a preprocessing step in a pipeline. These studies generally deal with typographical errors (e.g. *begginning/beginning). In ESL writing, however, there exist many other types of spelling errors, which often occur in combination with, for example, homophone (*there/their), confusion (*form/from), split (*Now a day/Nowadays), merge (*swimingpool/swimming pool), inflection (*please/pleased), and derivation"
C12-1144,P11-1121,0,0.213437,"Missing"
C12-1144,P11-1094,0,0.0123656,"pelling correction as preprocessing based on edit distance. Some recent studies deal with spelling correction at the same time as whole grammatical error correction. For example, (Brockett et al., 2006) presents a method to correct whole sentences containing various errors, applying a statistical machine translation (SMT) technique where input sentences are translated into correct English. Although this approach can deal with any type of spelling errors, it suffers from a poverty of error-annotated resources and cannot correct misspelled words that have never appeared in a corpus. Similarly, (Park and Levy, 2011) propose a noisy channel model to correct errors, although they depend on a bigram language model and do not use syntactic information. A discriminative approach for whole grammatical error correction is also proposed in a recent study (Dahlmeier and Ng, 2012) where spelling errors are corrected simultaneously. In terms of spelling error types, however, typographical errors using GNU Aspell are dealt with, but not other misspelling types such as split and merge errors. Our proposed model uses POS features in order to correct spelling. As result, a wider range of spelling errors such as inflect"
C12-1144,P11-1093,0,0.0159978,"ing in ESL writing. Keywords: Part-of-Speech Tagging, Spelling Error Correction. Proceedings of COLING 2012: Technical Papers, pages 2357–2374, COLING 2012, Mumbai, December 2012. 2357 1 Introduction Automated grammatical error detection and correction have been focused on natural language processing (NLP) over the past dozen years or so. Researchers have mainly studied English grammatical error detection and correction of areas such as determiners, prepositions and verbs (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Lee and Seneff, 2008; Gamon, 2010; Dahlmeier and Ng, 2011; Rozovskaya and Roth, 2011; Tajiri et al., 2012). In previous work on grammatical error detection and correction, spelling errors are usually corrected in a preprocessing step in a pipeline. These studies generally deal with typographical errors (e.g. *begginning/beginning). In ESL writing, however, there exist many other types of spelling errors, which often occur in combination with, for example, homophone (*there/their), confusion (*form/from), split (*Now a day/Nowadays), merge (*swimingpool/swimming pool), inflection (*please/pleased), and derivation (*badly/bad) errors. Unlike typographical errors, these spelling"
C12-1144,P10-1028,0,0.013743,"ror correction, the main concern is how to extract confusion pairs that consist of words before and after correction. A number of studies depend on such edit distance between written and corrected words as Levenshtein Distance (LD), Longest Common Subsequence (LCS) string matching, and pronunciation similarities (Kukich, 1992; Brill and Moore, 2000; Islam and Inkpen, 2009; Bao et al., 2011; Toutanova and Moore, 2002). In order to cover more misspelled words, many spelling errors were collected from web search queries and their results (Chen et al., 2007; Gao et al., 2010), click through logs (Sun et al., 2010), and users’ keystroke logs (Baba and Suzuki, 2012). Note that previous studies for spelling correction described above focus on errors made by native speakers rather than second language learners, who show a wider range of misspellings with, for example, split, merge, inflection and derivation errors. 2359 In most grammatical error detection and correction research, spelling error correction is performed before such linguistic analysis as POS tagging and syntactic parsing. Spelling correction as preprocessing generally uses existing spelling checkers such as GNU Aspell2 and Jazzy3 , which dep"
C12-1144,P12-2039,1,0.830212,"s: Part-of-Speech Tagging, Spelling Error Correction. Proceedings of COLING 2012: Technical Papers, pages 2357–2374, COLING 2012, Mumbai, December 2012. 2357 1 Introduction Automated grammatical error detection and correction have been focused on natural language processing (NLP) over the past dozen years or so. Researchers have mainly studied English grammatical error detection and correction of areas such as determiners, prepositions and verbs (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Lee and Seneff, 2008; Gamon, 2010; Dahlmeier and Ng, 2011; Rozovskaya and Roth, 2011; Tajiri et al., 2012). In previous work on grammatical error detection and correction, spelling errors are usually corrected in a preprocessing step in a pipeline. These studies generally deal with typographical errors (e.g. *begginning/beginning). In ESL writing, however, there exist many other types of spelling errors, which often occur in combination with, for example, homophone (*there/their), confusion (*form/from), split (*Now a day/Nowadays), merge (*swimingpool/swimming pool), inflection (*please/pleased), and derivation (*badly/bad) errors. Unlike typographical errors, these spelling errors are difficult"
C12-1144,P02-1019,0,0.0253919,"method in detail (Section 3). The experimental setting and the results are presented in Section 4, and error analysis is given in Section 5. Finally, we conclude in Section 6. 2 Related works In spelling error correction, the main concern is how to extract confusion pairs that consist of words before and after correction. A number of studies depend on such edit distance between written and corrected words as Levenshtein Distance (LD), Longest Common Subsequence (LCS) string matching, and pronunciation similarities (Kukich, 1992; Brill and Moore, 2000; Islam and Inkpen, 2009; Bao et al., 2011; Toutanova and Moore, 2002). In order to cover more misspelled words, many spelling errors were collected from web search queries and their results (Chen et al., 2007; Gao et al., 2010), click through logs (Sun et al., 2010), and users’ keystroke logs (Baba and Suzuki, 2012). Note that previous studies for spelling correction described above focus on errors made by native speakers rather than second language learners, who show a wider range of misspellings with, for example, split, merge, inflection and derivation errors. 2359 In most grammatical error detection and correction research, spelling error correction is perf"
C12-1144,P11-1019,0,0.0256083,"Missing"
C12-1144,D10-1082,0,0.030964,"n and derivation error correction in our task, given the difficulty of determining whether they are grammatical or spelling errors, as will be explained in Section 4.1. Joint learning and joint analysis have received much attention in recent studies for linguistic analysis. For example, the CoNLL-2008 Shared Task (Surdeanu et al., 2008) shows promising results in joint syntactic and semantic dependency parsing. There are also models that deal with joint morphological segmentation and syntactic parsing in Hebrew (Goldberg and Tsarfaty, 2008), joint word segmentation and POS tagging in Chinese (Zhang and Clark, 2010), and joint word segmentation, POS tagging and dependency parsing in Chinese (Hatori et al., 2012). These studies demonstrate that joint models outperform conventional pipelined systems. Our work applies for the first time a joint analysis to spelling correction and POS tagging for ESL writing in which input sentences contains multiple errors, whereas previous joint models deal only with canonical texts. 3 Joint analysis of POS tagging and spelling correction In this section, we describe our proposed joint analysis of spelling error correction and POS tagging for ESL writing. Our method is bas"
C12-1144,W08-2121,0,\N,Missing
C12-1144,P03-2026,0,\N,Missing
C12-2084,J96-1002,0,0.0357184,"rtion-limit for Moses, therefore we chose the edit distance to be smaller than the distortion-limit. 866 4.1 Tools and experimental data We used Moses 2010-08-13 5 with default parameters as a decoder and GIZA++ 1.0.5 6 as an alignment tool to implement an error correction system with phrase-based SMT. We applied growdiag-ﬁnal-and (Och and Ney, 2003) heuristics for phrase extraction. The number of extracted phrases are 1,050,070 (245 MB) using all data of Lang-8 Corpus. We used 3-gram as a language model trained on the corrected text of Lang-8 Corpus. Next, we built the maximum entropy model (Berger et al., 1996) as a multi-class classiﬁer baseline for preposition error correction (Sakaguchi et al., 2012). We used the implementation of Maximum Entropy Modeling Toolkit 7 with its default parameters. We incorporated surface, POS, WordNet, parse and language model features described in (Tetreault et al., 2010) and (De Felice and Pulman, 2008). POS and parse features were extracted using the Stanford Parser 2.0.2. This system achieves recall of 18.44, precision of 34.88 and F-measure of 24.12 trained and tested on the CLC FCE dataset (Yannakoudakis et al., 2011), which ranked the 4th out of 13 systems at"
C12-2084,P06-1032,0,0.289136,"in preposition error correction when we trained a phrase-based SMT system on a small learner corpus. However, in this work, we exploit a large scale error-annotated corpus extracted from the web to overcome the data sparseness problem. 3 3.1 Using a large scale learner corpus with phrase-based SMT for grammatical error correction Error correction with phrase-based SMT We use phrase-based statistical machine translation (Koehn et al., 2003) to conduct unrestricted error correction. There are several studies about grammatical error correction using phrase-based statistical machine translation (Brockett et al., 2006; Mizumoto et al., 2011; Ehsan and Faili, 2012). Although Brockett et al. (2006) corrected English learners’ error using phrase-based statistical machine translation, they only targeted mass noun errors. Mizumoto et al. (2011) dealt with un865 restricted types of learners’ errors, but their target is not English but Japanese. Ehsan and Faili (2012) applied an SMT framework to English and Persian grammatical error correction, but used artiﬁcially created learner corpora. The well-known statistical machine translation formulation using a log-linear model (Och and Ney, 2002) is deﬁned by: eˆ = ar"
C12-2084,P11-1092,0,0.0143922,"ount for the most errors, but it should be noted that there are many different types of errors in learner corpus. Thus far, a lot of studies have been made on automated error correction in regard to errors ESL learners make. However, most previous studies of second language learning deal with one or a few restricted types of learners’ errors. For example, there are studies on preposition errors (Rozovskaya and Roth, 2011), verb selection errors (Liu et al., 2011), tense errors (Tajiri et al., 2012), verb form errors (agreement and tense) (Lee and Seneff, 2008), preposition and article errors (Dahlmeier and Ng, 2011) and spelling, article, preposition and word form (agreement and tense) errors (Park and Levy, 2011). Recently, Swanson and Yamangil (2012) presented a detailed analysis on correcting all types of errors in the Cambridge Learner Corpus, but their task is different from the others in that their goal is to detect errors and select error types given both the original and corrected text, which is not often available in practice. Some types of errors like agreement errors can be corrected by simple rules using heuristics, while others like preposition errors are difﬁcult to correct without statisti"
C12-2084,D12-1052,0,0.085074,"rror-tagged corpus annotated by the wisdom of crowds. In addition, they targeted only spelling, article, preposition and word form errors, while we do not restrict error types. Third, Han et al. (2010) developed a preposition correction system using a large scale error-tagged corpus of learner English. They built a maximum entropy-based model for preposition errors trained on learner and native corpora. We also take advantage of a large scale error-tagged corpus of learner English, but use phrase-based SMT to deal with various kinds of errors and to fully exploit the learner corpus. Recently, Dahlmeier and Ng (2012) presented a beam-search decoder for correcting spelling, article, preposition, punctuation and noun number errors. They reported that their discriminative model achieves considerably better results than an SMT baseline trained on a few hundreds of sentences. As we will see later, we observed a similar tendency in preposition error correction when we trained a phrase-based SMT system on a small learner corpus. However, in this work, we exploit a large scale error-annotated corpus extracted from the web to overcome the data sparseness problem. 3 3.1 Using a large scale learner corpus with phras"
C12-2084,W12-2006,0,0.0189159,"r baseline for preposition error correction (Sakaguchi et al., 2012). We used the implementation of Maximum Entropy Modeling Toolkit 7 with its default parameters. We incorporated surface, POS, WordNet, parse and language model features described in (Tetreault et al., 2010) and (De Felice and Pulman, 2008). POS and parse features were extracted using the Stanford Parser 2.0.2. This system achieves recall of 18.44, precision of 34.88 and F-measure of 24.12 trained and tested on the CLC FCE dataset (Yannakoudakis et al., 2011), which ranked the 4th out of 13 systems at the HOO 2012 Shared Task (Dale et al., 2012). We use KJ Corpus as a test data. KJ Corpus consist of 170 essays, containing 2,411 sentences. When we experiment on a system using KJ Corpus, we perform 5-fold cross validation. 4.2 Evaluation metrics For the evaluation metrics, we use automatic evaluation criteria. To be precise, we use recall, precision and F-measure. Recall and precision for each type of errors are calculated from true positive, false positive and false negative based on error tags in KJ Corpus. The word which does not have any tag in KJ Corpus does not affect precision for each type of errors 8 . For example, let us cons"
C12-2084,C08-1022,0,0.0206406,"Missing"
C12-2084,han-etal-2010-using,0,0.0387109,"to emphasize that our work is the ﬁrst attempt to use a real world large learner corpus with phrase-based SMT technique. We will show that phrase-based SMT especially suffers from data sparseness. Second, Park and Levy (2011) attempted to correct various kinds of errors with a noisy channel model using a large scale unannotated corpus of learner English. Ours differs from their work in that we use a large scale error-tagged corpus annotated by the wisdom of crowds. In addition, they targeted only spelling, article, preposition and word form errors, while we do not restrict error types. Third, Han et al. (2010) developed a preposition correction system using a large scale error-tagged corpus of learner English. They built a maximum entropy-based model for preposition errors trained on learner and native corpora. We also take advantage of a large scale error-tagged corpus of learner English, but use phrase-based SMT to deal with various kinds of errors and to fully exploit the learner corpus. Recently, Dahlmeier and Ng (2012) presented a beam-search decoder for correcting spelling, article, preposition, punctuation and noun number errors. They reported that their discriminative model achieves conside"
C12-2084,N03-1017,0,0.023552,"eir discriminative model achieves considerably better results than an SMT baseline trained on a few hundreds of sentences. As we will see later, we observed a similar tendency in preposition error correction when we trained a phrase-based SMT system on a small learner corpus. However, in this work, we exploit a large scale error-annotated corpus extracted from the web to overcome the data sparseness problem. 3 3.1 Using a large scale learner corpus with phrase-based SMT for grammatical error correction Error correction with phrase-based SMT We use phrase-based statistical machine translation (Koehn et al., 2003) to conduct unrestricted error correction. There are several studies about grammatical error correction using phrase-based statistical machine translation (Brockett et al., 2006; Mizumoto et al., 2011; Ehsan and Faili, 2012). Although Brockett et al. (2006) corrected English learners’ error using phrase-based statistical machine translation, they only targeted mass noun errors. Mizumoto et al. (2011) dealt with un865 restricted types of learners’ errors, but their target is not English but Japanese. Ehsan and Faili (2012) applied an SMT framework to English and Persian grammatical error correc"
C12-2084,P08-1021,0,0.0105552,"It is not surprising that frequent types of errors account for the most errors, but it should be noted that there are many different types of errors in learner corpus. Thus far, a lot of studies have been made on automated error correction in regard to errors ESL learners make. However, most previous studies of second language learning deal with one or a few restricted types of learners’ errors. For example, there are studies on preposition errors (Rozovskaya and Roth, 2011), verb selection errors (Liu et al., 2011), tense errors (Tajiri et al., 2012), verb form errors (agreement and tense) (Lee and Seneff, 2008), preposition and article errors (Dahlmeier and Ng, 2011) and spelling, article, preposition and word form (agreement and tense) errors (Park and Levy, 2011). Recently, Swanson and Yamangil (2012) presented a detailed analysis on correcting all types of errors in the Cambridge Learner Corpus, but their task is different from the others in that their goal is to detect errors and select error types given both the original and corrected text, which is not often available in practice. Some types of errors like agreement errors can be corrected by simple rules using heuristics, while others like pr"
C12-2084,I11-1017,1,0.927515,"orrection when we trained a phrase-based SMT system on a small learner corpus. However, in this work, we exploit a large scale error-annotated corpus extracted from the web to overcome the data sparseness problem. 3 3.1 Using a large scale learner corpus with phrase-based SMT for grammatical error correction Error correction with phrase-based SMT We use phrase-based statistical machine translation (Koehn et al., 2003) to conduct unrestricted error correction. There are several studies about grammatical error correction using phrase-based statistical machine translation (Brockett et al., 2006; Mizumoto et al., 2011; Ehsan and Faili, 2012). Although Brockett et al. (2006) corrected English learners’ error using phrase-based statistical machine translation, they only targeted mass noun errors. Mizumoto et al. (2011) dealt with un865 restricted types of learners’ errors, but their target is not English but Japanese. Ehsan and Faili (2012) applied an SMT framework to English and Persian grammatical error correction, but used artiﬁcially created learner corpora. The well-known statistical machine translation formulation using a log-linear model (Och and Ney, 2002) is deﬁned by: eˆ = arg max P(e |f ) = arg ma"
C12-2084,P02-1038,0,0.0170949,"chine translation (Brockett et al., 2006; Mizumoto et al., 2011; Ehsan and Faili, 2012). Although Brockett et al. (2006) corrected English learners’ error using phrase-based statistical machine translation, they only targeted mass noun errors. Mizumoto et al. (2011) dealt with un865 restricted types of learners’ errors, but their target is not English but Japanese. Ehsan and Faili (2012) applied an SMT framework to English and Persian grammatical error correction, but used artiﬁcially created learner corpora. The well-known statistical machine translation formulation using a log-linear model (Och and Ney, 2002) is deﬁned by: eˆ = arg max P(e |f ) = arg max e e M ∑ λm hm (e, f ) (1) m=1 where e represents target sentences (corrected sentences) and f represents source sentences (sentences written by learners). hm (e, f ) is a feature function and λm is a model parameter for each feature function. This formulation ﬁnds a target sentence e that maximizes a weighted linear combination of feature functions for source sentence f . A translation model and a language model can be used as feature functions. The translation model is commonly represented as conditional probability P( f |e) factored into the tra"
C12-2084,J03-1002,0,0.0251172,"the effect of error correction methods, we also experimented on the preposition error correction task using a maximum entropy model as a discriminative baseline and SMT-based models as our proposal for all error correction. 3 http://lang-8.com/ 4 We use 6 as a distortion-limit for Moses, therefore we chose the edit distance to be smaller than the distortion-limit. 866 4.1 Tools and experimental data We used Moses 2010-08-13 5 with default parameters as a decoder and GIZA++ 1.0.5 6 as an alignment tool to implement an error correction system with phrase-based SMT. We applied growdiag-ﬁnal-and (Och and Ney, 2003) heuristics for phrase extraction. The number of extracted phrases are 1,050,070 (245 MB) using all data of Lang-8 Corpus. We used 3-gram as a language model trained on the corrected text of Lang-8 Corpus. Next, we built the maximum entropy model (Berger et al., 1996) as a multi-class classiﬁer baseline for preposition error correction (Sakaguchi et al., 2012). We used the implementation of Maximum Entropy Modeling Toolkit 7 with its default parameters. We incorporated surface, POS, WordNet, parse and language model features described in (Tetreault et al., 2010) and (De Felice and Pulman, 2008"
C12-2084,P11-1094,0,0.0394033,"er corpus. Thus far, a lot of studies have been made on automated error correction in regard to errors ESL learners make. However, most previous studies of second language learning deal with one or a few restricted types of learners’ errors. For example, there are studies on preposition errors (Rozovskaya and Roth, 2011), verb selection errors (Liu et al., 2011), tense errors (Tajiri et al., 2012), verb form errors (agreement and tense) (Lee and Seneff, 2008), preposition and article errors (Dahlmeier and Ng, 2011) and spelling, article, preposition and word form (agreement and tense) errors (Park and Levy, 2011). Recently, Swanson and Yamangil (2012) presented a detailed analysis on correcting all types of errors in the Cambridge Learner Corpus, but their task is different from the others in that their goal is to detect errors and select error types given both the original and corrected text, which is not often available in practice. Some types of errors like agreement errors can be corrected by simple rules using heuristics, while others like preposition errors are difﬁcult to correct without statistical model trained on native corpora and/or learner corpora. It was not until recently that large sca"
C12-2084,P11-1093,0,0.0268429,"s the distribution of errors found in KJ Corpus2 . The most frequent error type is article errors, followed by noun number and preposition errors. It is not surprising that frequent types of errors account for the most errors, but it should be noted that there are many different types of errors in learner corpus. Thus far, a lot of studies have been made on automated error correction in regard to errors ESL learners make. However, most previous studies of second language learning deal with one or a few restricted types of learners’ errors. For example, there are studies on preposition errors (Rozovskaya and Roth, 2011), verb selection errors (Liu et al., 2011), tense errors (Tajiri et al., 2012), verb form errors (agreement and tense) (Lee and Seneff, 2008), preposition and article errors (Dahlmeier and Ng, 2011) and spelling, article, preposition and word form (agreement and tense) errors (Park and Levy, 2011). Recently, Swanson and Yamangil (2012) presented a detailed analysis on correcting all types of errors in the Cambridge Learner Corpus, but their task is different from the others in that their goal is to detect errors and select error types given both the original and corrected text, which is not of"
C12-2084,W12-2033,1,0.810438,"on-limit. 866 4.1 Tools and experimental data We used Moses 2010-08-13 5 with default parameters as a decoder and GIZA++ 1.0.5 6 as an alignment tool to implement an error correction system with phrase-based SMT. We applied growdiag-ﬁnal-and (Och and Ney, 2003) heuristics for phrase extraction. The number of extracted phrases are 1,050,070 (245 MB) using all data of Lang-8 Corpus. We used 3-gram as a language model trained on the corrected text of Lang-8 Corpus. Next, we built the maximum entropy model (Berger et al., 1996) as a multi-class classiﬁer baseline for preposition error correction (Sakaguchi et al., 2012). We used the implementation of Maximum Entropy Modeling Toolkit 7 with its default parameters. We incorporated surface, POS, WordNet, parse and language model features described in (Tetreault et al., 2010) and (De Felice and Pulman, 2008). POS and parse features were extracted using the Stanford Parser 2.0.2. This system achieves recall of 18.44, precision of 34.88 and F-measure of 24.12 trained and tested on the CLC FCE dataset (Yannakoudakis et al., 2011), which ranked the 4th out of 13 systems at the HOO 2012 Shared Task (Dale et al., 2012). We use KJ Corpus as a test data. KJ Corpus consi"
C12-2084,N12-1037,0,0.0216011,"udies have been made on automated error correction in regard to errors ESL learners make. However, most previous studies of second language learning deal with one or a few restricted types of learners’ errors. For example, there are studies on preposition errors (Rozovskaya and Roth, 2011), verb selection errors (Liu et al., 2011), tense errors (Tajiri et al., 2012), verb form errors (agreement and tense) (Lee and Seneff, 2008), preposition and article errors (Dahlmeier and Ng, 2011) and spelling, article, preposition and word form (agreement and tense) errors (Park and Levy, 2011). Recently, Swanson and Yamangil (2012) presented a detailed analysis on correcting all types of errors in the Cambridge Learner Corpus, but their task is different from the others in that their goal is to detect errors and select error types given both the original and corrected text, which is not often available in practice. Some types of errors like agreement errors can be corrected by simple rules using heuristics, while others like preposition errors are difﬁcult to correct without statistical model trained on native corpora and/or learner corpora. It was not until recently that large scale learner corpora became widely availa"
C12-2084,P12-2039,1,0.760617,"rticle errors, followed by noun number and preposition errors. It is not surprising that frequent types of errors account for the most errors, but it should be noted that there are many different types of errors in learner corpus. Thus far, a lot of studies have been made on automated error correction in regard to errors ESL learners make. However, most previous studies of second language learning deal with one or a few restricted types of learners’ errors. For example, there are studies on preposition errors (Rozovskaya and Roth, 2011), verb selection errors (Liu et al., 2011), tense errors (Tajiri et al., 2012), verb form errors (agreement and tense) (Lee and Seneff, 2008), preposition and article errors (Dahlmeier and Ng, 2011) and spelling, article, preposition and word form (agreement and tense) errors (Park and Levy, 2011). Recently, Swanson and Yamangil (2012) presented a detailed analysis on correcting all types of errors in the Cambridge Learner Corpus, but their task is different from the others in that their goal is to detect errors and select error types given both the original and corrected text, which is not often available in practice. Some types of errors like agreement errors can be c"
C12-2084,P10-2065,0,0.0129726,"d SMT. We applied growdiag-ﬁnal-and (Och and Ney, 2003) heuristics for phrase extraction. The number of extracted phrases are 1,050,070 (245 MB) using all data of Lang-8 Corpus. We used 3-gram as a language model trained on the corrected text of Lang-8 Corpus. Next, we built the maximum entropy model (Berger et al., 1996) as a multi-class classiﬁer baseline for preposition error correction (Sakaguchi et al., 2012). We used the implementation of Maximum Entropy Modeling Toolkit 7 with its default parameters. We incorporated surface, POS, WordNet, parse and language model features described in (Tetreault et al., 2010) and (De Felice and Pulman, 2008). POS and parse features were extracted using the Stanford Parser 2.0.2. This system achieves recall of 18.44, precision of 34.88 and F-measure of 24.12 trained and tested on the CLC FCE dataset (Yannakoudakis et al., 2011), which ranked the 4th out of 13 systems at the HOO 2012 Shared Task (Dale et al., 2012). We use KJ Corpus as a test data. KJ Corpus consist of 170 essays, containing 2,411 sentences. When we experiment on a system using KJ Corpus, we perform 5-fold cross validation. 4.2 Evaluation metrics For the evaluation metrics, we use automatic evaluati"
C12-2084,P11-1019,0,0.0348522,"Corpus. Next, we built the maximum entropy model (Berger et al., 1996) as a multi-class classiﬁer baseline for preposition error correction (Sakaguchi et al., 2012). We used the implementation of Maximum Entropy Modeling Toolkit 7 with its default parameters. We incorporated surface, POS, WordNet, parse and language model features described in (Tetreault et al., 2010) and (De Felice and Pulman, 2008). POS and parse features were extracted using the Stanford Parser 2.0.2. This system achieves recall of 18.44, precision of 34.88 and F-measure of 24.12 trained and tested on the CLC FCE dataset (Yannakoudakis et al., 2011), which ranked the 4th out of 13 systems at the HOO 2012 Shared Task (Dale et al., 2012). We use KJ Corpus as a test data. KJ Corpus consist of 170 essays, containing 2,411 sentences. When we experiment on a system using KJ Corpus, we perform 5-fold cross validation. 4.2 Evaluation metrics For the evaluation metrics, we use automatic evaluation criteria. To be precise, we use recall, precision and F-measure. Recall and precision for each type of errors are calculated from true positive, false positive and false negative based on error tags in KJ Corpus. The word which does not have any tag in"
C16-2011,C16-1066,1,0.824744,"ency parsed (Figure 6). We also plan to use this function with a historical Japanese corpus containing translations into contemporary Japanese. 50 Figure 4: Visualization of two word segmentation standards Figure 5: Visualization of Japanese-English parallel corpus 2.4 Visualization of Time ChaKi.NET can store the start time, end time, and duration of words or morphemes for speech transcription corpora. The demo for ‘Corpus of Spontaneous Japanese’ (CSJ) (Maekawa et al., 2000) can be accessed at https://youtu.be/Qod6J14X9mU. 2.5 Combination of Projection and Time The BCCWJ EyeTracking Corpus (Asahara et al., 2016) contains the reading time data of 24 experiment subjects, obtained from BCCWJ samples. We can define two word orders – the reading order of the subject and the word order in the original text. For the former order, we can define the start time, end time, and duration. For the latter order, reading time is aggregated into the following three duration types: first pass duration, regression path duration, and total duration. First pass duration is the time spent in a word region before moving on or looking back. Regression path duration is the time from Figure 6: Visualization of dialect and sta"
C16-2011,maekawa-etal-2000-spontaneous,0,0.150456,"t corpus is Bunsetsu-segmented with Katanaka transcription. The dialect is translated into standard Japanese, which is POS tagged and dependency parsed (Figure 6). We also plan to use this function with a historical Japanese corpus containing translations into contemporary Japanese. 50 Figure 4: Visualization of two word segmentation standards Figure 5: Visualization of Japanese-English parallel corpus 2.4 Visualization of Time ChaKi.NET can store the start time, end time, and duration of words or morphemes for speech transcription corpora. The demo for ‘Corpus of Spontaneous Japanese’ (CSJ) (Maekawa et al., 2000) can be accessed at https://youtu.be/Qod6J14X9mU. 2.5 Combination of Projection and Time The BCCWJ EyeTracking Corpus (Asahara et al., 2016) contains the reading time data of 24 experiment subjects, obtained from BCCWJ samples. We can define two word orders – the reading order of the subject and the word order in the original text. For the former order, we can define the start time, end time, and duration. For the latter order, reading time is aggregated into the following three duration types: first pass duration, regression path duration, and total duration. First pass duration is the time s"
C18-1067,D14-1082,0,0.0500947,"on which the standard transition-based parser fails and attaches “Monday” to “John”, since on the POS level and the usual behavior of “on”, this sequence is misleading as a typical noun phrase. By introducing attention on feature extraction, we expect the model to attend to important tokens, in this case “Monday”, which is not likely to attach to a person and suggests the parser to anticipate the following predicate. Our technique can be applied to any models with feed-forward networks on concatenated feature embeddings, and in this work, we apply it on the standard transition-based parser of Chen and Manning (2014). On the multilingual experiment on Universal Dependencies (UD) 2.0 (Zeman et al., 2017), we find our attention brings performance gain for most languages. To inspect the model’s behavior, we also introduce a controlled experiment with manually created data. For this experiment, we prepare a set of sentences for which the parser must attend to the key points for correct disambiguation, as in Figure 1, and see whether the model behaves as expected. There we give detailed error analysis to suggest what makes it difficult to solve the local ambiguities and how attention achieves it. This type of"
C18-1067,P15-1033,0,0.0607787,"Missing"
C18-1067,K17-3012,0,0.011997,"e. With respect to the macro averaged score, in the Table 1 below, we can see that our model without attention (w/o Att.) is comparable to UDPipe; with attention, it outperforms both. When inspecting in detail, we see that our attention improves the scores on 54 treebanks on the development set and 57 treebanks on the test set. We also see that the treebanks for which our attention degrades the performance are relatively small, e.g., en partut (1,035 sentences) and hu (864 sentences), which indicates our attention may be more data-hungry. 2 There are three systems (Straka and Strakov´a, 2017; Kanerva et al., 2017; Yu et al., 2017) that outperform UDPipe v1.1 but the improvements come not from parsing models but from preprocessing, such as improvements to the POS tagger. 3 https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-1990 787 Development Test Treebank UDPipe w/o Att. w/ Att. UDPipe w/o Att. w/ Att. ar 78.11 78.73 79.84 65.30 64.72 65.43 bg 87.56 86.90 87.35 83.64 83.23 83.44 ca 88.35 87.52 88.28 85.39 84.66 85.43 88.19 86.29 87.06 82.87 81.15 82.27 cs cs cac 86.57 86.13 87.01 82.46 81.48 82.15 78.95 79.96 79.22 71.64 72.08 72.56 cs cltt cu 79.44 79.46 81.69 62.76 63.19 65.40 da 81.13 80.5"
C18-1067,Q16-1023,0,0.0256211,"ng. We focus on a system that builds a parse tree incrementally receiving each word of a sentence, which is crucial for interactive systems to achieve fast response or human-like behavior such as understanding from partial input (Baumann, 2013). The most natural way to achieve incremental parsing is using a transition system (Nivre, 2008), and for such parsers, the main challenge is to choose an appropriate action with only the local context information. While some recent transition-based parsers alleviate this difficulty by exploiting the entire input sentence with recurrent neural networks (Kiperwasser and Goldberg, 2016; Shi et al., 2017), one possible disadvantage is to require that all inputs are visible from the beginning, which should be problem when we try more strict incremental conditions such as simultaneous translation. Therefore there are still demands to explore the effective way to extract better feature representation from incomplete inputs. In this paper, we incorporate a simple attention mechanism (Bahdanau et al., 2015) with an incremental parser and investigate its effectiveness during the feature extraction. Attention mechanism itself has firstly succeeded in machine translation, capturing"
C18-1067,D15-1176,0,0.050827,"Missing"
C18-1067,D15-1166,0,0.0534793,"the beginning, which should be problem when we try more strict incremental conditions such as simultaneous translation. Therefore there are still demands to explore the effective way to extract better feature representation from incomplete inputs. In this paper, we incorporate a simple attention mechanism (Bahdanau et al., 2015) with an incremental parser and investigate its effectiveness during the feature extraction. Attention mechanism itself has firstly succeeded in machine translation, capturing relative importances of tokens on a certain step for a proper output (Bahdanau et al., 2015; Luong et al., 2015). The characteristic to weight on some features automatically and effectively can be applied to various tasks such as seq-to-seq parsing model (Vinyals et al., 2015), text summarization (Rush et al., 2015), dialogue generation (Shang et al., 2015), image captioning (Xu et al., 2015) in which the systems can enjoy performance gain by attending to specific clues depending on a given situation. We can also expect this behavior is helpful to fix the error which transition-based parsers often commits due to local ambiguities. 40nmod 40obl John on Monday introduces advisors. Figure 1: A locally ambi"
C18-1067,W09-3811,0,0.0701513,"Missing"
C18-1067,J08-4003,0,0.0254643,"ess across many languages. We also present an experiment with augmented test dataset and demonstrate it helps to understand the model’s behavior on locally ambiguous points. 1 Introduction This paper explores better feature representations for incremental dependency parsing. We focus on a system that builds a parse tree incrementally receiving each word of a sentence, which is crucial for interactive systems to achieve fast response or human-like behavior such as understanding from partial input (Baumann, 2013). The most natural way to achieve incremental parsing is using a transition system (Nivre, 2008), and for such parsers, the main challenge is to choose an appropriate action with only the local context information. While some recent transition-based parsers alleviate this difficulty by exploiting the entire input sentence with recurrent neural networks (Kiperwasser and Goldberg, 2016; Shi et al., 2017), one possible disadvantage is to require that all inputs are visible from the beginning, which should be problem when we try more strict incremental conditions such as simultaneous translation. Therefore there are still demands to explore the effective way to extract better feature represe"
C18-1067,D15-1044,0,0.0505467,"ure representation from incomplete inputs. In this paper, we incorporate a simple attention mechanism (Bahdanau et al., 2015) with an incremental parser and investigate its effectiveness during the feature extraction. Attention mechanism itself has firstly succeeded in machine translation, capturing relative importances of tokens on a certain step for a proper output (Bahdanau et al., 2015; Luong et al., 2015). The characteristic to weight on some features automatically and effectively can be applied to various tasks such as seq-to-seq parsing model (Vinyals et al., 2015), text summarization (Rush et al., 2015), dialogue generation (Shang et al., 2015), image captioning (Xu et al., 2015) in which the systems can enjoy performance gain by attending to specific clues depending on a given situation. We can also expect this behavior is helpful to fix the error which transition-based parsers often commits due to local ambiguities. 40nmod 40obl John on Monday introduces advisors. Figure 1: A locally ambiguous sentence. “Monday” should be analyzed as oblique of “introduce” while tends to be analyzed as a noun modifier of “John”. This work is licensed under a Creative Commons Attribution 4.0 International L"
C18-1067,P15-1152,0,0.0348949,". In this paper, we incorporate a simple attention mechanism (Bahdanau et al., 2015) with an incremental parser and investigate its effectiveness during the feature extraction. Attention mechanism itself has firstly succeeded in machine translation, capturing relative importances of tokens on a certain step for a proper output (Bahdanau et al., 2015; Luong et al., 2015). The characteristic to weight on some features automatically and effectively can be applied to various tasks such as seq-to-seq parsing model (Vinyals et al., 2015), text summarization (Rush et al., 2015), dialogue generation (Shang et al., 2015), image captioning (Xu et al., 2015) in which the systems can enjoy performance gain by attending to specific clues depending on a given situation. We can also expect this behavior is helpful to fix the error which transition-based parsers often commits due to local ambiguities. 40nmod 40obl John on Monday introduces advisors. Figure 1: A locally ambiguous sentence. “Monday” should be analyzed as oblique of “introduce” while tends to be analyzed as a noun modifier of “John”. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4."
C18-1067,P17-1024,0,0.0184499,"e gain for most languages. To inspect the model’s behavior, we also introduce a controlled experiment with manually created data. For this experiment, we prepare a set of sentences for which the parser must attend to the key points for correct disambiguation, as in Figure 1, and see whether the model behaves as expected. There we give detailed error analysis to suggest what makes it difficult to solve the local ambiguities and how attention achieves it. This type of analysis is common in psycholinguistics (Levy, 2008), and a similar idea has recently begun to be explored in NLP neural models (Shekhar et al., 2017). 2 2.1 Model Base model Our base model is a transition-based neural parser of Chen and Manning (2014).1 For each step, this parser first creates feature vectors of words (xw ), POS tags (xp ), and labels (xl ), each of which is a concatenation of embeddings around a stack and a buffer. These vectors are transformed with corresponding weights, i.e., h = Ww xw + Wp xp + Wl xl + b, followed by nonlinearity. A next softmax layer then provides action probabilities. Although this method is actually old, the approach which creates the feature vector from independent embeddings becomes useful in our"
C18-1067,D17-1002,0,0.0211969,"ilds a parse tree incrementally receiving each word of a sentence, which is crucial for interactive systems to achieve fast response or human-like behavior such as understanding from partial input (Baumann, 2013). The most natural way to achieve incremental parsing is using a transition system (Nivre, 2008), and for such parsers, the main challenge is to choose an appropriate action with only the local context information. While some recent transition-based parsers alleviate this difficulty by exploiting the entire input sentence with recurrent neural networks (Kiperwasser and Goldberg, 2016; Shi et al., 2017), one possible disadvantage is to require that all inputs are visible from the beginning, which should be problem when we try more strict incremental conditions such as simultaneous translation. Therefore there are still demands to explore the effective way to extract better feature representation from incomplete inputs. In this paper, we incorporate a simple attention mechanism (Bahdanau et al., 2015) with an incremental parser and investigate its effectiveness during the feature extraction. Attention mechanism itself has firstly succeeded in machine translation, capturing relative importance"
C18-1067,K17-3009,0,0.0242243,"Missing"
C18-1067,L16-1680,0,0.0159573,"1 For each step, this parser first creates feature vectors of words (xw ), POS tags (xp ), and labels (xl ), each of which is a concatenation of embeddings around a stack and a buffer. These vectors are transformed with corresponding weights, i.e., h = Ww xw + Wp xp + Wl xl + b, followed by nonlinearity. A next softmax layer then provides action probabilities. Although this method is actually old, the approach which creates the feature vector from independent embeddings becomes useful in our second experiment inspecting our attention behaviors (See section 3.3 in detail). In addition, UDPipe (Straka et al., 2016) which is the baseline parser in the latest shared task (Zeman et al., 2017) also adopts this approach and holds good performance compared to others using recent techniques. 2.2 Attention on local features We introduce attention in feature computation from the input embeddings to h. Note that three components Ww xw , Wp wp , and Wl xl are independent; in the following we focus on just one part, abstracted by Wx, and describe how attention is applied for this computation. Our attention calculates the importance of input elements. First, note that x is a concatenation of embeddings of input elem"
C18-1067,K17-3013,0,0.0118507,"macro averaged score, in the Table 1 below, we can see that our model without attention (w/o Att.) is comparable to UDPipe; with attention, it outperforms both. When inspecting in detail, we see that our attention improves the scores on 54 treebanks on the development set and 57 treebanks on the test set. We also see that the treebanks for which our attention degrades the performance are relatively small, e.g., en partut (1,035 sentences) and hu (864 sentences), which indicates our attention may be more data-hungry. 2 There are three systems (Straka and Strakov´a, 2017; Kanerva et al., 2017; Yu et al., 2017) that outperform UDPipe v1.1 but the improvements come not from parsing models but from preprocessing, such as improvements to the POS tagger. 3 https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-1990 787 Development Test Treebank UDPipe w/o Att. w/ Att. UDPipe w/o Att. w/ Att. ar 78.11 78.73 79.84 65.30 64.72 65.43 bg 87.56 86.90 87.35 83.64 83.23 83.44 ca 88.35 87.52 88.28 85.39 84.66 85.43 88.19 86.29 87.06 82.87 81.15 82.27 cs cs cac 86.57 86.13 87.01 82.46 81.48 82.15 78.95 79.96 79.22 71.64 72.08 72.56 cs cltt cu 79.44 79.46 81.69 62.76 63.19 65.40 da 81.13 80.57 82.01 73.38 73.1"
C92-2088,P91-1027,0,0.0420531,"Missing"
C92-2088,P91-1017,0,0.087783,"and word mf~anings (such as English and Japanese), and to c(nnt~are analyzed results from each language, h| many (:asc~, the two languagcs }Lave different types of syntactic ambiguities, anti comparison of syntactic structures of both bmguagcs helps to resolve the ambiguities. Also, a pair of bilingually equivalent snrface words helps to a~&apos;4ociate tile words with conceptual l&apos;~oc. OF COL]NG-92. NANTES.AUG.23-28, 1992 words helps to associate the words with conceptual items, because the intersection of conceptual items that each surface word has could be considered as one conceptual item[ll] [2]. [&quot;or example, in tire case of the translation example given in Example 1, both syntactic and semantic ambiguities are resolved. Example 1 E: J: I hung my coat on the hook. ~:L(I) ;~ (topic) ~2~ (coat) ~ (ca.se-m~trker) ~&apos;5&quot; (hook) lZ (case-marker) zi&apos;$~&apos;f: (hung)o 1. S y n t a c t i c d i s u m b i g u a t i o n The English sentence in Example 1 is syntactically ambiguous because the prepositional phrase &quot;on the hook&quot; can modify both the verb &quot;hung&quot; aad tim noun phrase &quot;my coat&quot; using grammatical knowledge only. On the other band, in the Japanese sentence, the phrase &quot;7)~~&apos;, Is_&quot; can modify"
C92-2088,P90-1034,0,0.0411537,"ion. One ~nch approach is to extract hierarclfical relations or it thesanrtm of conceptual items froln hunLall dictionaries in an automatic way. q)surrnnaru et el. studied to construct a t}LeSaLLrlIsof nominal concepts from noun detinitions[t3], q b m i a r a et al. also extracted snperordinatc-subordmatc relation between verbs from the defining sentences in IPAL[12]. l i e sidcs these rcseasches, there are other several research activitics tbr lexical knowledge acquisition, which syntactically anMyze the sentences m large corpora and attcmpt to extract lcxical knowledge from statistical data [3] [1]. Most of the works undertake shallow analysis of texts and they extract only superticial lexical information. For the development of tile techniques of knowledge acquisition from natural language texts, it is very important to improve the httter approach of cornpiling semantic dictionaries by comimter l)rograuL~. Ilowever, there are at least two basic difficulties in this at)preach 1. Tire i~robh~m (ff s y n t a c t i c a m b i g u i t i e s When analyzing a sentence., syntactic ambiguities often remain. So i~ is not easy to obtain correct parsed results automatically. 2. The, probh~rrr o"
C92-2088,C90-3044,1,0.839076,"COLING-92, NANTES,AUG. 23-28, 1992 cable to sew:ral otller problenrs as well. One of t h e m is to acquire features of nominal concepts. We are at the m o m e n t looking at some specitie nominal expression &quot;A q) B&quot; in Japanese, corresponding literally to &quot;I1 of A&quot; in English. T h a t expression specifies a variety of relationships of noun phrases, which are often stated in different expressions in English. T h e y will help to acquire typical attributes of nominal concepts fl&apos;om bilingual corpora. Our ntethod is also useful to collect parsed traamlation examples tbr example-based translation [9] attd to acquire translation p a t t e r n s between two languages. &apos;Fable 3: Acquired Case Frames for &quot;~-[ &lt; (wr;le)&quot; (7~Lse Frame 15 (on) l~t . :6¢ (sub3) ~ ) J3.. ~ ( [subj,passive]) &quot;~&quot; (with) -e I PRO IIUM REL, QUA, LIN (i,,) Ca.~e Frlmm 2 V- (to) HUM ~;t • fie (subj) HUM l;~ &quot; ~ (obj) t:]: • fit ( LIN [subj,p,,ssive]) ~ (with) -e (i,,) the e x t r a c t e d cm~e slots, ttle systenr ~sks the h u m a n instructor a b o u t the pcx~sibi[ities of tile co-occurrence of the case slots that do not cc.occur in the trans lation examples by composing saml)le phr,&apos;~ses. T h e questions and answers"
C92-2088,H91-1067,0,\N,Missing
C92-2088,P86-1038,0,\N,Missing
C94-2175,J90-2002,0,0.292042,"Missing"
C94-2175,J93-2003,0,0.0512019,"Missing"
C94-2175,J93-1004,0,0.225334,"echniques are apt plied to estimate word correspondences not included in bilingual dictionaries. Estimated word correspondences are useful for improving both sentence alignment and structural matching. Introduction Bilingnal (or parallel) texts are useful as resources of linguistic knowledge as well as in applications such as machine translation. One of the major approaches to analyzing bilingual texts is the statistical approach. The statistical approach involves the following: alignment of bilingual texts at the sentence level nsing statistical techniques (e.g. Brown, Lai and Mercer (1991), Gale and Church (1993), Chen (1993), and Kay and RSscheisen (1993)), statistical machine translation models (e.g. Brown, Cooke, Pietra, Pietra et al. (1990)), finding character-level / word-level / phrase-level correspondences from bilingual texts (e.g. Gale and Church (1991), Church (1993), and Kupiec (1993)), and word sense disambiguation for MT (e.g. Dagan, Itai and Schwall (1991)). In general, the statistical approach does not use existing hand-written bilingual dictionaries, and depends solely upon statistics. For example, sentence alignment of bilingual texts are performed just by measuring sentence lengths i"
C94-2175,C90-3101,0,0.017792,"91; Gale and Church, 1993), or by statistically estimating word level correspondences (Chen, 1993; Kay and RSscheisen, 1993). The statistical approach analyzes unstructured sentences in bilingual texts, and it is claimed that the results are useful enough in real applications such as machine translation and word sense disambiguation. However, structured bilingual sentences are undoubtedly more informative and important for future natural language researches. Structured bilingual or multilingual corpora serve as richer sonrces for extracting linguistic knowledge (Klavans and Tzonkermann, 1990; Sadler and Vendelmans, 1990; Kaji, Kida attd Morimoto, 1992; Utsuro, Matsnmoto and Nagao, 1992; Matsumoto, l.shimoto and Utsuro, 1993; Ut1076 Makoto Nagao ~ tGraduate School of Information Science Abstract 1 Yuji Matsumoto t suro, Matsumoto and Nagao, 1993). Compared with the statistical approach, those works are quite different in that they use word correspondence information available in hand-written bilingual dictionaries and try to extract structured linguistic knowledge such as structured translation patterns and case frames of verbs. For example, in Matsunloto et al. (1993), we proposed a method for finding struct"
C94-2175,C92-2088,1,0.843099,"proach, those works are quite different in that they use word correspondence information available in hand-written bilingual dictionaries and try to extract structured linguistic knowledge such as structured translation patterns and case frames of verbs. For example, in Matsunloto et al. (1993), we proposed a method for finding structural matching of parallel sentences, making use of word level similarities calculated from a bilingual dictionary and a thesaurus. Then, those structurally matched parallel sentences are used as a source for acquiring lexical knowledge snch as verbal case frames (Utsuro et al., 1992; Utsuro et al., 1993). With the aim of acquiring those structnred linguistic knowledge, this paper describes a unilied framework for bilingual text matching by combining existing hand-written bilingual dictionaries and statistical techniques. The process of bilingual text matchin 9 consists of two major steps: sentence alignment and structural matching of bilingual sentences. In those two steps, we use word correspondence information, which is available in hand-written bilingual dictionaries, or not included in bilingual dictionaries but estimated with statistical techniques. The reasons why"
C94-2175,C92-2101,0,0.0750694,"Missing"
C94-2175,C90-3031,0,0.0660578,"Missing"
C94-2175,P93-1003,0,0.0605224,"Missing"
C94-2175,P93-1001,0,\N,Missing
C94-2175,P91-1022,0,\N,Missing
C94-2175,P93-1004,1,\N,Missing
C94-2175,P91-1017,0,\N,Missing
C94-2175,J93-1006,0,\N,Missing
C94-2175,H91-1026,0,\N,Missing
C94-2175,P93-1002,0,\N,Missing
C96-1095,P95-1037,0,0.0264513,"s that give more information than current measures, and evaluate the quality of the test. We also show that statistical significance cannot be calculated in a straightforward way, and suggest a calculation method for the case of Bracket Recall. 1 2 Introduction During the last few years large treebanks have become available to m a n y researchers, which has resulted in researches applying a range of new techniques for parsing systems. Most of the methods that are being suggested include some kind of Machine Learning, such as history based grammars and decision tree models (Black et al., 1993; Magerman, 1995), training or inducing statistical grammars (Black, Garside and Leech, 1993; Pereira and Schabes, 1992; Schabes et al., 1993), or other techniques (Bod, 1993). Consequently, syntactical analysis has become an area with a wide variety of (a) algorithms and methods for learning and parsing, and (b) type of information used for learning and parsing (sometimes referred to as feature set). These methods only could become popular through evaluation methods for parsing systems, such as Bracket Accuracy, Bracket Recall, Sentence Accuracy and Viterbi Score. Some of them were introduced in (Black et al."
C96-1095,P92-1017,0,0.018599,"also show that statistical significance cannot be calculated in a straightforward way, and suggest a calculation method for the case of Bracket Recall. 1 2 Introduction During the last few years large treebanks have become available to m a n y researchers, which has resulted in researches applying a range of new techniques for parsing systems. Most of the methods that are being suggested include some kind of Machine Learning, such as history based grammars and decision tree models (Black et al., 1993; Magerman, 1995), training or inducing statistical grammars (Black, Garside and Leech, 1993; Pereira and Schabes, 1992; Schabes et al., 1993), or other techniques (Bod, 1993). Consequently, syntactical analysis has become an area with a wide variety of (a) algorithms and methods for learning and parsing, and (b) type of information used for learning and parsing (sometimes referred to as feature set). These methods only could become popular through evaluation methods for parsing systems, such as Bracket Accuracy, Bracket Recall, Sentence Accuracy and Viterbi Score. Some of them were introduced in (Black et al., 1991; Harrison et M., 1991). These evaluation metrics have a number of problems, and in this paper w"
C96-1095,E93-1040,0,0.0414746,"Missing"
C96-1095,H91-1060,0,0.0144481,"erman, 1995), training or inducing statistical grammars (Black, Garside and Leech, 1993; Pereira and Schabes, 1992; Schabes et al., 1993), or other techniques (Bod, 1993). Consequently, syntactical analysis has become an area with a wide variety of (a) algorithms and methods for learning and parsing, and (b) type of information used for learning and parsing (sometimes referred to as feature set). These methods only could become popular through evaluation methods for parsing systems, such as Bracket Accuracy, Bracket Recall, Sentence Accuracy and Viterbi Score. Some of them were introduced in (Black et al., 1991; Harrison et M., 1991). These evaluation metrics have a number of problems, and in this paper we argue that they 562 Problems with Evaluation Metrics Until now a number of problems with evaluation have been pointed out. One well known problem is that measures based only on the absence of crossing errors on sentence level, such as Sentence Accuracy and Viterbi Consistency, are not usable for parsing systems that apply a partial bracketing, since a sparse bracketing improves the score. For example (Lin, 1995) discusses some other problems, but suggests an alternative that is difficult to apply."
C96-1095,P93-1005,0,0.0331647,"a number of measures that give more information than current measures, and evaluate the quality of the test. We also show that statistical significance cannot be calculated in a straightforward way, and suggest a calculation method for the case of Bracket Recall. 1 2 Introduction During the last few years large treebanks have become available to m a n y researchers, which has resulted in researches applying a range of new techniques for parsing systems. Most of the methods that are being suggested include some kind of Machine Learning, such as history based grammars and decision tree models (Black et al., 1993; Magerman, 1995), training or inducing statistical grammars (Black, Garside and Leech, 1993; Pereira and Schabes, 1992; Schabes et al., 1993), or other techniques (Bod, 1993). Consequently, syntactical analysis has become an area with a wide variety of (a) algorithms and methods for learning and parsing, and (b) type of information used for learning and parsing (sometimes referred to as feature set). These methods only could become popular through evaluation methods for parsing systems, such as Bracket Accuracy, Bracket Recall, Sentence Accuracy and Viterbi Score. Some of them were introduced"
C96-1095,E93-1006,0,0.0197104,"ightforward way, and suggest a calculation method for the case of Bracket Recall. 1 2 Introduction During the last few years large treebanks have become available to m a n y researchers, which has resulted in researches applying a range of new techniques for parsing systems. Most of the methods that are being suggested include some kind of Machine Learning, such as history based grammars and decision tree models (Black et al., 1993; Magerman, 1995), training or inducing statistical grammars (Black, Garside and Leech, 1993; Pereira and Schabes, 1992; Schabes et al., 1993), or other techniques (Bod, 1993). Consequently, syntactical analysis has become an area with a wide variety of (a) algorithms and methods for learning and parsing, and (b) type of information used for learning and parsing (sometimes referred to as feature set). These methods only could become popular through evaluation methods for parsing systems, such as Bracket Accuracy, Bracket Recall, Sentence Accuracy and Viterbi Score. Some of them were introduced in (Black et al., 1991; Harrison et M., 1991). These evaluation metrics have a number of problems, and in this paper we argue that they 562 Problems with Evaluation Metrics U"
C96-2128,P94-1030,0,0.0590053,"Missing"
C96-2128,E95-1025,0,0.332229,"pa,rsing mid 1)y the SGX trallsl&tor for generation. (~rgtHin&r rules Call 1)e labelled to be compiled only by SAX or only by SGX, st) thai, parsing could for e,xaml)le use some rules with wider coverage l;han others used in genergttion, while sharing I[IOSt of t h e gra, innl&r. Like the earlier BUP parser, the SAX translat;or uses t)art;ial execution to t)roduee efficient code for bol;1;om-ut) (:hart t)arsing. The SGX transla, tor eontl)iles tal)les of chain rules and also uses partial ex(~eution to l)rt)dut:e efliehmt code for Sill) (:ha,rt ge,ncration. compilation The P r o F I T system (Erbaeh, 1995) iv an extension of fq'olog whic, h SUl)ports a typed feature formalisin with multiple inheiitalwe. CLE used a sort hierarchy only for senlantie seleetional restrictions. HPSG uses a sort hierarchy also for syntactic resl;rictions, aim exploits multiple inheritmme for lexicon organization. 2In this pape,r we follow (Shieber et al., 1990) and (Pollard add Sag, 1.994) in equating logical form with semmltic content. A separate logical form is therefore redundant, as the. content fe.ature could /)e used to &lt;:ontrol SHD generation. Itowever, logical form may need to include other inforination, suet"
C96-2128,C94-1039,0,0.0685546,"Missing"
C96-2128,J88-1004,0,\N,Missing
C98-2158,C94-2192,0,0.0295318,"Missing"
C98-2158,J86-3001,0,0.082529,"Missing"
C98-2158,C94-2183,0,0.0143517,"interpersonal relations is predicated mainly on the interests, beliefs, and attitudes of addressee a n d / o r author. To deal with this problem, we must incorporate the notion of intentional structure and focus space structure (Grosz and Sidher, 1986). Since we have focused on te-linkage in this paper, we need not to consider how clauses are combined. However, to detect the discourse structure, we need to extend the method so as to deal with the relations between sentences. We must estimate some kind of reliable scores among possible segments and choose the relation having the maximum score (Kurohashi and Nagao, 1994). These issues remain to be studied in the future. 6 Summary Since the semantic relations exhibited by re-linkage vary so diversely, it has been claimed that the interpreter must infer the intended relationship on the basis of extralinguistic knowledge. The particulars of individual common sense knowledge are crucial to understanding any discourse (Hobbs et al., 1993; Asher and Lascarides, 1995). Nevertheless, one can, through the use of the relevant structures of events, eliminate a very large number of rules for calculating the plausible relations. Although we have concentrated on re-linkage"
C98-2158,J92-4007,0,0.046445,"Missing"
C98-2209,J96-1002,0,0.0117999,"mited to only one argument. Li and Abe (1996) also studied a method for learning dependencies between case slots and reported that dependencies were discovered only at the slotlevel and not at the class-level. Compared with these previous works, this paper proposes to consider the above two issues in a uniform way. First, we introduce a model of generating a collocation of a verb and argum e n t / a d j u n c t nouns (section 2) and then view the model as a probability model (section 3). As a model learning method, we adopt the maximum entropy model learning method (Della Pietra et al., 1997; Berger et al., 1996). Case dependencies and noun class generalization are represented as features in the maximum entropy approach. Features are allowed to have overlap and this is quite advantageous when we consider case dependcncies and noun class generalization in parameter estimation. An optimal model is selected by searching for an optimal set of features, i.e, optimal case dependencies and optimal noun class generMization levels. As the feature selection process, this paper proposes a new feature selection algorithm which starts from the most general model and gradually examines more specific models (section"
C98-2209,P96-1025,0,0.0255014,"ximum entropy modeling method. We also propose a new model selection algorithm which starts from the most general model and gradually examines more specific models. In the experimental evaluation, it is shown that both of the case dependencies and specific sense restriction selected by the proposed method contribute to improving the performance in subcategorization preference resolution. 1 Introduction In empirical approaches to parsing, lexical/semantic collocation extracted from corpus has been proved to be quite useful for ranking parses in syntactic analysis. For example, Magerman (1995), Collins (1996), and Charniak (1997) proposed statistical parsing models which incorporated lexical/semantic information. In their models, syntactic and lexical/semantic features are dependent on each other and are combined together. This paper also proposes a method of utilizing lexical/semantic features for the purpose of applying them to ranking parses in syntactic analysis. However, unlike the models of Magerman (1995), Collins (1996), and Charniak (1997), we assume that syntactic and lexical/semantic features are independent. Then, we focus on extracting lcxical/semantic collocational knowledge of verbs"
C98-2209,C96-1004,0,0.0176758,", 1998. An extended version of this paper is available from the above URL. 1314 dependent of other cases. When considering 2), we have to decide which superordinate class generates each observed leaf class in the verb-noun collocation. So far, there exist several works which worked on these two issues in learning collocational knowledge of verbs and also evaluated the results in terms of syntactic disambiguation. Resnik (1993) and Li and Abe (1995) studied how to find an optimal abstraction level of an argument noun in a tree-structured thesaurus. Their works are limited to only one argument. Li and Abe (1996) also studied a method for learning dependencies between case slots and reported that dependencies were discovered only at the slotlevel and not at the class-level. Compared with these previous works, this paper proposes to consider the above two issues in a uniform way. First, we introduce a model of generating a collocation of a verb and argum e n t / a d j u n c t nouns (section 2) and then view the model as a probability model (section 3). As a model learning method, we adopt the maximum entropy model learning method (Della Pietra et al., 1997; Berger et al., 1996). Case dependencies and n"
C98-2209,P95-1037,0,0.013454,"employing the maximum entropy modeling method. We also propose a new model selection algorithm which starts from the most general model and gradually examines more specific models. In the experimental evaluation, it is shown that both of the case dependencies and specific sense restriction selected by the proposed method contribute to improving the performance in subcategorization preference resolution. 1 Introduction In empirical approaches to parsing, lexical/semantic collocation extracted from corpus has been proved to be quite useful for ranking parses in syntactic analysis. For example, Magerman (1995), Collins (1996), and Charniak (1997) proposed statistical parsing models which incorporated lexical/semantic information. In their models, syntactic and lexical/semantic features are dependent on each other and are combined together. This paper also proposes a method of utilizing lexical/semantic features for the purpose of applying them to ranking parses in syntactic analysis. However, unlike the models of Magerman (1995), Collins (1996), and Charniak (1997), we assume that syntactic and lexical/semantic features are independent. Then, we focus on extracting lcxical/semantic collocational kn"
C98-2209,H93-1054,0,0.0333564,"re optional and in* This research was partially supported by the Ministry of Education, Science, Sports and Culture, Japan, Grantin-Aid for Encouragement of Young Scientists, 09780338, 1998. An extended version of this paper is available from the above URL. 1314 dependent of other cases. When considering 2), we have to decide which superordinate class generates each observed leaf class in the verb-noun collocation. So far, there exist several works which worked on these two issues in learning collocational knowledge of verbs and also evaluated the results in terms of syntactic disambiguation. Resnik (1993) and Li and Abe (1995) studied how to find an optimal abstraction level of an argument noun in a tree-structured thesaurus. Their works are limited to only one argument. Li and Abe (1996) also studied a method for learning dependencies between case slots and reported that dependencies were discovered only at the slotlevel and not at the class-level. Compared with these previous works, this paper proposes to consider the above two issues in a uniform way. First, we introduce a model of generating a collocation of a verb and argum e n t / a d j u n c t nouns (section 2) and then view the model a"
C98-2209,A97-1053,1,0.575665,"Missing"
C98-2209,J98-2002,0,\N,Missing
C98-2222,J90-1004,0,0.024858,"Missing"
C98-2222,C96-2128,1,0.729158,"ated. 2.1 Semantic heads llead-driven generation algorithms are based on the idea that most grammar rules have a semantic head (laughter whose logical form is identical to the logical form of the mother. Tile bottom-up generation (BUG) algorithm of van Noord (1990) requires every rule to have such a head (except lexical entries). The semantic head-driven (SHD) algorithm of Shieber et al. (1.990) relaxes this, dividing rules into chain rules with such a head (processed bottom-up), and nonchain rules (processed top-down). The chart-based semantic head-driven (CSItD) algorithm 1 of Haruno et al. (1996) increases efficiency by using a chart to eliminate recomputation of partial results. llead-driven bottom-up generation is efficient as it is geared both to the input logical form (headdriven) and to lexical information (bottom-up). It is good for HPSG, which is highly lexiealist and ha.s 1For simplicity we illustrate the approach with BUG. A P r o F I T / I I P S ( ] framework using the CStlD algorithm is described by Wilcock and M a t s u m o t o (1996). := synsem!loc!cat!head!HF & hd_dtr!synsem!loc!cat!head!HF. &apos;SemP&apos; := synsem!loc!cont!Cont & hd_dtr!synsem!loc!cont!Cont. &apos;SemP&apos;(adjunct) :="
C98-2222,E95-1025,0,\N,Missing
cheng-etal-2014-parsing,D07-1013,0,\N,Missing
cheng-etal-2014-parsing,J09-4006,0,\N,Missing
cheng-etal-2014-parsing,W06-2933,0,\N,Missing
cheng-etal-2014-parsing,E09-1100,0,\N,Missing
cheng-etal-2014-parsing,J92-4003,0,\N,Missing
cheng-etal-2014-parsing,P08-1068,0,\N,Missing
cheng-etal-2014-parsing,D12-1132,0,\N,Missing
cheng-etal-2014-parsing,O03-4002,0,\N,Missing
cheng-etal-2014-parsing,P11-1141,0,\N,Missing
cheng-etal-2014-parsing,P13-1013,0,\N,Missing
cheng-etal-2014-parsing,W06-0127,0,\N,Missing
cheng-etal-2014-parsing,I08-4008,1,\N,Missing
cheng-etal-2014-parsing,D07-1101,0,\N,Missing
cheng-etal-2014-parsing,P07-1106,0,\N,Missing
D07-1068,E06-1002,0,0.0626311,"Missing"
D07-1068,N04-4028,0,0.0190297,"Networks. Since Bayesian Networks are directed graphical models, PRMs cannot model directly the cases where instantiated graph contains cycles. Taskar et al. proposed Relational Markov Networks (RMNs) 656 (2002). RMNs are the special case of Conditional Markov Networks (or Conditional Random Fields) in which graph structure and parameter tying are determined by SQL-like form. As for the marginal probability to use as a confidence measure shown in Figure 4, Peng et al. (2004) has applied linear-chain CRFs to Chinese word segmentation. It is calculated by constrained forwardbackward algorithm (Culotta and McCallum, 2004), and confident segments are added to the dictionary in order to improve segmentation accuracy. 6 Conclusion In this paper, we proposed a method for categorizing NEs in Wikipedia. We defined three types of cliques that are constitute dependent anchor texts in construct CRFs graph structure, and introduced potential functions for them to reflect classification. The experimental results show that the effectiveness of capturing dependencies, and proposed CRFs model can achieve significant improvements compare to baseline methods with SVMs. The results also show that the dependency information fro"
D07-1068,C04-1081,0,0.0107572,"ectly estimate most likely assignments. Getoor et al. proposed Probabilistic Relational Models (PRMs) (2001) which are built upon Bayesian Networks. Since Bayesian Networks are directed graphical models, PRMs cannot model directly the cases where instantiated graph contains cycles. Taskar et al. proposed Relational Markov Networks (RMNs) 656 (2002). RMNs are the special case of Conditional Markov Networks (or Conditional Random Fields) in which graph structure and parameter tying are determined by SQL-like form. As for the marginal probability to use as a confidence measure shown in Figure 4, Peng et al. (2004) has applied linear-chain CRFs to Chinese word segmentation. It is calculated by constrained forwardbackward algorithm (Culotta and McCallum, 2004), and confident segments are added to the dictionary in order to improve segmentation accuracy. 6 Conclusion In this paper, we proposed a method for categorizing NEs in Wikipedia. We defined three types of cliques that are constitute dependent anchor texts in construct CRFs graph structure, and introduced potential functions for them to reflect classification. The experimental results show that the effectiveness of capturing dependencies, and propos"
D07-1068,N06-1025,0,0.0262168,"Missing"
D07-1068,sekine-etal-2002-extended,0,0.16391,"v ,v )∈E (d) ∪E (d) ∪E (d) k i j S C R +   λk fk (yi , x(d) ) − logZ(x(d) )] vi ∈V (d) k  −  λ2   λ2 k k − 2 2σ 2 2σ  k (6) k where the last two terms are due to the Gaussian prior (Chen and Rosenfeld, 1999) used to reduce overfitting. Quasi-Newton methods, such as LBFGS (Liu and Nocedal, 1989) can be used for maximizing the function. 652 Dataset Our dataset is a random selection of 2300 articles from the Japanese version of Wikipedia as of October 2005. All anchor texts appearing under HTML <LI&gt; tags are hand-annotated with NE class label. We use the Extended Named Entity Hierarchy (Sekine et al., 2002) as the NE class labeling guideline, but reduce the number of classes to 13 from the original 200+ by ignoring fine-grained categories and nearby categories in order to avoid data sparseness. We eliminate examples that consist of less than two nodes in the SCR model. There are 16136 anchor texts with 14285 NEs. The number of Sibling, Cousin and Relative edges in the dataset are |ES |= 4925, |EC |= 13134 and |ER |= 746 respectively. 4.2 Experimental settings The aims of experiments are the two-fold. Firstly, we investigate the effect of each cliques. The several graphs are composed with the thr"
D07-1068,N06-2018,0,0.0356922,"Missing"
D07-1114,W03-2607,0,0.0319522,"the generality of the model. Aspect-of relations can be regarded as a sub-type of bridging reference (Clark, 1977), which is a common linguistic phenomenon where the referent of a definite noun phrase refers to a discourse-new entity implicitly related to some previously mentioned entity. For example, we can see a relation of bridging reference between “the door” and “the room” in “She entered the room. The door closed automatically.” A common approach is to use cooccurrence statistics between the referring expression (e.g. “the door” in the above example) and the related entity (“the room”) (Bunescu, 2003; Poesio et al., 2004). Our approach newly incorporates automatically induced syntactic patterns as contextual clues into such a co-occurrence model, producing significant improvements of accuracy. 3.2 Our approach Now we describe our approach to aspect-evaluation and aspect-of relation extraction. The key idea is to combine the following two kinds of information using a machine learning technique for both tasks. Contextual clues: Syntactic patterns such as hAspecti-ga hAspecti-NOM VP-te, hEvaluationi VP-CONJ hEvaluationi which matches such a sentence as hsekkyakui-ga kunrens-aretei-te hkimoch"
D07-1114,W06-1651,0,0.469248,"the task of extracting hSubject,Aspect,Evaluationi. However, none of those papers reports on such an extensive corpus study as what we report in this paper. In addition, in this paper, we consider not only aspect-evaluation relations but also hierarchical chains of subject-aspect and aspect-aspect relations, which has never been addressed in previous work. Open-domain opinion extraction is another trend of research on opinion extraction, which aims to extract a wider range of opinions from such texts as newspaper articles (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Wiebe et al., 2005; Choi et al., 2006). To the best of our knowledge, one of the most extensive corpus studies in this field has been conducted in the MPQA project (Wiebe et al., 2005); while their concerns include the types of opinions we consider, they annotate newspaper articles, which presumably exhibit considerably different characteristics from customer-generated texts. Though we do not discuss the problem of determining semantic orientation, we assume availability of state-of-the-art methods that perform this task (Suzuki et al., 2006; Takamura et al., 2006, etc.). The problem of determining semantic orientation will be sol"
D07-1114,W03-2604,1,0.664242,"entification: Given a target evaluation (or aspect), select the most likely candidate aspect c∗ within the target sentence with the intra-sentential model described in 3.2.1. If the score of c∗ is positive, return c∗ ; otherwise, go to the inter-sentential relation extraction phase. 2) Inter-sentential relation identification: Search for the most likely candidate aspect in the sentences preceding the target evaluation (or aspect). This task can be regarded as a zero-anaphora resolution problem. For this purpose, we employ the supervised learning model for zero-anaphora resolution proposed by (Iida et al., 2003). 3.5 Opinion-hood determination Evaluation phrases do not always extract correct opinion units in a given domain. Consider an example from the digital camera domain, “The weather was good. so I went to the park to take some pictures”. “good” expresses the evaluation for “the weather”, but “the weather” is not an aspect of digital cameras. Therefore, hthe weather, goodi is not an opinion in the digital camera domain. We can consider a binary classification task of judging whether the obtained opinion unit is a real opinion or not in 1071 a given domain. In this paper, we conduct a preliminary"
D07-1114,C04-1071,0,0.019269,"Missing"
D07-1114,C04-1200,0,0.0348149,"aper (Kobayashi et al., 2005) addresses the task of extracting hSubject,Aspect,Evaluationi. However, none of those papers reports on such an extensive corpus study as what we report in this paper. In addition, in this paper, we consider not only aspect-evaluation relations but also hierarchical chains of subject-aspect and aspect-aspect relations, which has never been addressed in previous work. Open-domain opinion extraction is another trend of research on opinion extraction, which aims to extract a wider range of opinions from such texts as newspaper articles (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Wiebe et al., 2005; Choi et al., 2006). To the best of our knowledge, one of the most extensive corpus studies in this field has been conducted in the MPQA project (Wiebe et al., 2005); while their concerns include the types of opinions we consider, they annotate newspaper articles, which presumably exhibit considerably different characteristics from customer-generated texts. Though we do not discuss the problem of determining semantic orientation, we assume availability of state-of-the-art methods that perform this task (Suzuki et al., 2006; Takamura et al., 2006, etc.). The problem of dete"
D07-1114,W06-0301,0,0.15085,"ually devised rules. An exception is the model reported by Kanayama et al.(2004), which uses a component of an existing MT system to identify the “aspect” argument of a given “evaluation” predicate. However, the MT component they use is not publicly available, and even if it were, it would be difficult to apply it to tasks in hand due of the opaqueness of its mechanism. Our approach aims to develop a more generally applicable model of aspect-evaluation extraction. In open-domain opinion extraction, some approaches use syntactic features obtained from parsed input sentences (Choi et al., 2006; Kim and Hovy, 2006), as is commonly done in semantic role labeling. Choi et al. (2006) address the task of extracting opinion entities and their relations, and incorporate syntactic features to their relation extraction 1069 model. Kim and Hovy (2006) proposed a method for extracting opinion holders, topics and opinion words, in which they use semantic role labeling as an intermediate step to label opinion holders and topics. However, these approaches do not address the task of extracting aspect-of relations and make use of syntactic features only for labeling opinion holders and topics. In contrast, as we descr"
D07-1114,I05-2030,1,0.510845,"heavily domain-dependent. In fact, according to our investigation on our opinion-annotated corpus, the number 1068 # !"" $# !""      There are several researches on customer opinion extraction. Hu and Liu (2004) considered the task of extracting hAspect, Sentence, Semantic-orientationi triples in our terminology, where Sentence is the one that includes the Aspect, and Semantic-orientation is either positive or negative. The notion of Evaluation in our term has also been introduced by previous work (Popescu and Etzioni, 2005; Tateishi et al., 2004; Suzuki et al., 2006; Kobayashi et al., 2005, etc.). For example, our previous paper (Kobayashi et al., 2005) addresses the task of extracting hSubject,Aspect,Evaluationi. However, none of those papers reports on such an extensive corpus study as what we report in this paper. In addition, in this paper, we consider not only aspect-evaluation relations but also hierarchical chains of subject-aspect and aspect-aspect relations, which has never been addressed in previous work. Open-domain opinion extraction is another trend of research on opinion extraction, which aims to extract a wider range of opinions from such texts as newspaper artic"
D07-1114,W04-3239,1,0.748648,"aluation t, we first represent all the sentences in the annotated corpus that has both an aspect and its evaluation, as shown in Figure 3. A sentence is analyzed by a dependency parser, then the dependency tree is converted so as to represent the relation between content words clearly and to attach other information (such as POS labels and other morphological features of content words and the functional words attached to the content words) as shown in the lower part of Figure 3. Among various classifier induction algorithms for tree-structured data, in our experiments, we have so far examined Kudo and Matsumoto (2004)’s algorithm, packaged as a free software named BACT. 1070 Given a set of training examples represented as ordered trees labeled either positive or negative class, this algorithm learns a list of weighted decision stumps as a discrimination function with the Boosting algorithm. Each decision stump is associated with tuple hs, l, wi, where s is a subtree appearing in the training set, l a label, and w a weight of this pattern. The strength of this algorithm is that it automatically acquires structured features and allows us to analyze the utility of features. Given a c-t pair in an annotated se"
D07-1114,W02-1011,0,0.0157809,"luation and Aspect-of Relations in Opinion Mining Nozomi Kobayashi ∗ Kentaro Inui, and Yuji Matsumoto Graduate School of Information Science, Nara Institute of Science and Technology 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan {nozomi-k,inui,matsu}@is.naist.jp Abstract reviews) can be classified into two approaches: Document classification and information extraction. The former is the task of classifying documents or passages according to their semantic orientation such as positive vs. negative. This direction has been forming the mainstream of research on opinion-sensitive text processing (Pang et al., 2002; Turney, 2002, etc.). The latter, on the other hand, focuses on the task of extracting opinions consisting of information about, for example, hwho feels how about which aspect of what producti from unstructured text data. In this paper, we refer to this information extractionoriented task as opinion extraction. In contrast to sentiment classification, opinion extraction aims at producing richer information and requires an indepth analysis of opinions, which has only recently been attempted by a growing but still relatively small research community (Yi et al., 2003; Hu and Liu, 2004; Popescu a"
D07-1114,P04-1019,0,0.0075112,"of the model. Aspect-of relations can be regarded as a sub-type of bridging reference (Clark, 1977), which is a common linguistic phenomenon where the referent of a definite noun phrase refers to a discourse-new entity implicitly related to some previously mentioned entity. For example, we can see a relation of bridging reference between “the door” and “the room” in “She entered the room. The door closed automatically.” A common approach is to use cooccurrence statistics between the referring expression (e.g. “the door” in the above example) and the related entity (“the room”) (Bunescu, 2003; Poesio et al., 2004). Our approach newly incorporates automatically induced syntactic patterns as contextual clues into such a co-occurrence model, producing significant improvements of accuracy. 3.2 Our approach Now we describe our approach to aspect-evaluation and aspect-of relation extraction. The key idea is to combine the following two kinds of information using a machine learning technique for both tasks. Contextual clues: Syntactic patterns such as hAspecti-ga hAspecti-NOM VP-te, hEvaluationi VP-CONJ hEvaluationi which matches such a sentence as hsekkyakui-ga kunrens-aretei-te hkimochiyoii hservicei-NOM be"
D07-1114,H05-1043,0,0.907688,"al., 2002; Turney, 2002, etc.). The latter, on the other hand, focuses on the task of extracting opinions consisting of information about, for example, hwho feels how about which aspect of what producti from unstructured text data. In this paper, we refer to this information extractionoriented task as opinion extraction. In contrast to sentiment classification, opinion extraction aims at producing richer information and requires an indepth analysis of opinions, which has only recently been attempted by a growing but still relatively small research community (Yi et al., 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005, etc.). The technology of opinion extraction allows users to retrieve and analyze people’s opinions scattered over Web documents. We define an opinion unit as a quadruple consisting of the opinion holder, the subject being evaluated, the part or the attribute in which the subject is evaluated, and the value of the evaluation that expresses a positive or negative assessment. We use this definition as the basis for our opinion extraction task. We focus on two important subtasks of opinion extraction: (a) extracting aspect-evaluation relations, and (b) extracting aspect-of relations, and we appr"
D07-1114,E06-1026,0,0.0335255,"Missing"
D07-1114,P02-1053,0,0.00485714,"of Relations in Opinion Mining Nozomi Kobayashi ∗ Kentaro Inui, and Yuji Matsumoto Graduate School of Information Science, Nara Institute of Science and Technology 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan {nozomi-k,inui,matsu}@is.naist.jp Abstract reviews) can be classified into two approaches: Document classification and information extraction. The former is the task of classifying documents or passages according to their semantic orientation such as positive vs. negative. This direction has been forming the mainstream of research on opinion-sensitive text processing (Pang et al., 2002; Turney, 2002, etc.). The latter, on the other hand, focuses on the task of extracting opinions consisting of information about, for example, hwho feels how about which aspect of what producti from unstructured text data. In this paper, we refer to this information extractionoriented task as opinion extraction. In contrast to sentiment classification, opinion extraction aims at producing richer information and requires an indepth analysis of opinions, which has only recently been attempted by a growing but still relatively small research community (Yi et al., 2003; Hu and Liu, 2004; Popescu and Etzioni, 20"
D07-1114,W03-1017,0,0.0181728,".). For example, our previous paper (Kobayashi et al., 2005) addresses the task of extracting hSubject,Aspect,Evaluationi. However, none of those papers reports on such an extensive corpus study as what we report in this paper. In addition, in this paper, we consider not only aspect-evaluation relations but also hierarchical chains of subject-aspect and aspect-aspect relations, which has never been addressed in previous work. Open-domain opinion extraction is another trend of research on opinion extraction, which aims to extract a wider range of opinions from such texts as newspaper articles (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Wiebe et al., 2005; Choi et al., 2006). To the best of our knowledge, one of the most extensive corpus studies in this field has been conducted in the MPQA project (Wiebe et al., 2005); while their concerns include the types of opinions we consider, they annotate newspaper articles, which presumably exhibit considerably different characteristics from customer-generated texts. Though we do not discuss the problem of determining semantic orientation, we assume availability of state-of-the-art methods that perform this task (Suzuki et al., 2006; Takamura et al., 2006, etc.)."
D07-1114,H05-2017,0,\N,Missing
D08-1106,J04-3004,0,0.0966542,". It incorporates heuristics not present in Simplified Espresso to reduce semantic drift, but these heuristics have limited effect as we demonstrate in Section 3.3. In Section 4, we propose two graph-based algorithms to reduce semantic drift. These algorithms are used in link analysis community to reduce the effect of topic drift. In Section 5 we apply them to the task of word sense disambiguation on Senseval-3 Lexical Sample Task and verify that they indeed reduce semantic drift. Finally, we conclude our work in Section 6. sented an unsupervised WSD system which rivals supervised techniques. Abney (2004) presented a thorough discussion on the Yarowsky algorithm. He extended the original Yarowsky algorithm to a new family of bootstrapping algorithms that are mathematically well understood. Li and Li (2004) proposed a method called Bilingual Bootstrapping. It makes use of a translation dictionary and a comparable corpus to help disambiguate word senses in the source language, by exploiting the asymmetric many-to-many sense mapping relationship between words in two languages. Curran et al. (2007) presented an algorithm called Mutual Exclusion Bootstrapping, which minimizes semantic drift using m"
D08-1106,W06-1669,0,0.0306795,"Missing"
D08-1106,P01-1005,0,0.00906266,"izes semantic drift using mutual exclusion between semantic classes of learned instances. They prepared a list of so-called stop classes similar to a stop word list used in information retrieval to help bound the semantic classes. Stop classes are sets of terms known to cause semantic drift in particular semantic classes. However, stop classes vary from task to task and domain to domain, and human intervention is essential to create an effective list of stop classes. A major drawback of bootstrapping is the lack of principled method for selecting optimal parameter values (Ng and Cardie, 2003; Banko and Brill, 2001). Also, there is an issue of generic patterns which deteriorates the quality of acquired instances. Previously proposed bootstrapping algorithms differ in how they deal with the problem of semantic drift. We will take recently proposed Espresso algorithm as the example to explain common configuration for bootstrapping in detail. 2.2 The Espresso Algorithm 2 Related Work 2.1 Overview of Bootstrapping Bootstrapping (or self-training) is a general framework for reducing the requirement of manual annotation. Hearst (1992) described a bootstrapping procedure for extracting words in hyponym (is-a) r"
D08-1106,W99-0613,0,0.282231,"s have been actively studied. Hearst (1992) first presented a bootstrapping method which requires only a small amount of instances Masashi Shimbo NAIST, Japan shimbo@is.naist.jp Yuji Matsumoto NAIST, Japan matsu@is.naist.jp (seed instances) to start with, but can easily multiply the number of tagged instances with minimal human annotation cost, by iteratively applying the following phases: pattern induction, pattern ranking/selection, and instance extraction. Bootstrapping has been widely adopted in NLP applications such as word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999) and relation extraction (Riloff and Jones, 1999; Pantel and Pennacchiotti, 2006). However, it is known that bootstrapping often acquires instances not related to seed instances. For example, consider the task of collecting the names of common tourist sites from web corpora. Given words like “Geneva” and “Bali” as seed instances, bootstrapping would eventually learn generic patterns such as “pictures” and “photos,” which also co-occur with many other unrelated instances. The subsequent iterations would likely acquire frequent words that co-occur with these generic patterns, such as “Britney Sp"
D08-1106,C92-2082,0,0.0511404,"s and are easy to calibrate. 1 Introduction In recent years machine learning techniques become widely used in natural language processing (NLP). These techniques offer various ways to exploit large corpora and are known to perform well in many tasks. However, these techniques often require tagged corpora, which are not readily available to many languages. So far, reducing the cost of human annotation is one of the important problems for building NLP systems. To mitigate the problem of hand-tagging resources, semi(or minimally)-supervised and unsupervised techniques have been actively studied. Hearst (1992) first presented a bootstrapping method which requires only a small amount of instances Masashi Shimbo NAIST, Japan shimbo@is.naist.jp Yuji Matsumoto NAIST, Japan matsu@is.naist.jp (seed instances) to start with, but can easily multiply the number of tagged instances with minimal human annotation cost, by iteratively applying the following phases: pattern induction, pattern ranking/selection, and instance extraction. Bootstrapping has been widely adopted in NLP applications such as word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999) and relation extr"
D08-1106,I08-1047,1,0.901317,"which also co-occur with many other unrelated instances. The subsequent iterations would likely acquire frequent words that co-occur with these generic patterns, such as “Britney Spears.” This phenomenon is called semantic drift (Curran et al., 2007). A straightforward approach to avoid semantic drift is to terminate iterations before hitting generic patterns, but the optimal number of iterations is task dependent and is hard to come by. The recently proposed Espresso (Pantel and Pennacchiotti, 2006) algorithm incorporates sophisticated scoring functions to cope with generic patterns, but as Komachi and Suzuki (2008) pointed out, Espresso still shows semantic drift unless iterations are terminated appropriately. Another deficiency in bootstrapping is its sensitivity to many parameters such as the number of 1011 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1011–1020, c Honolulu, October 2008. 2008 Association for Computational Linguistics seed instances, the stopping criterion of iteration, the number of instances and patterns selected on each iteration, and so forth. These parameters also need to be calibrated for each task. In this paper, we present a grap"
D08-1106,J04-1001,0,0.0130383,"ased algorithms to reduce semantic drift. These algorithms are used in link analysis community to reduce the effect of topic drift. In Section 5 we apply them to the task of word sense disambiguation on Senseval-3 Lexical Sample Task and verify that they indeed reduce semantic drift. Finally, we conclude our work in Section 6. sented an unsupervised WSD system which rivals supervised techniques. Abney (2004) presented a thorough discussion on the Yarowsky algorithm. He extended the original Yarowsky algorithm to a new family of bootstrapping algorithms that are mathematically well understood. Li and Li (2004) proposed a method called Bilingual Bootstrapping. It makes use of a translation dictionary and a comparable corpus to help disambiguate word senses in the source language, by exploiting the asymmetric many-to-many sense mapping relationship between words in two languages. Curran et al. (2007) presented an algorithm called Mutual Exclusion Bootstrapping, which minimizes semantic drift using mutual exclusion between semantic classes of learned instances. They prepared a list of so-called stop classes similar to a stop word list used in information retrieval to help bound the semantic classes. S"
D08-1106,N03-1023,0,0.0565315,"trapping, which minimizes semantic drift using mutual exclusion between semantic classes of learned instances. They prepared a list of so-called stop classes similar to a stop word list used in information retrieval to help bound the semantic classes. Stop classes are sets of terms known to cause semantic drift in particular semantic classes. However, stop classes vary from task to task and domain to domain, and human intervention is essential to create an effective list of stop classes. A major drawback of bootstrapping is the lack of principled method for selecting optimal parameter values (Ng and Cardie, 2003; Banko and Brill, 2001). Also, there is an issue of generic patterns which deteriorates the quality of acquired instances. Previously proposed bootstrapping algorithms differ in how they deal with the problem of semantic drift. We will take recently proposed Espresso algorithm as the example to explain common configuration for bootstrapping in detail. 2.2 The Espresso Algorithm 2 Related Work 2.1 Overview of Bootstrapping Bootstrapping (or self-training) is a general framework for reducing the requirement of manual annotation. Hearst (1992) described a bootstrapping procedure for extracting w"
D08-1106,P06-1015,0,0.581935,"method which requires only a small amount of instances Masashi Shimbo NAIST, Japan shimbo@is.naist.jp Yuji Matsumoto NAIST, Japan matsu@is.naist.jp (seed instances) to start with, but can easily multiply the number of tagged instances with minimal human annotation cost, by iteratively applying the following phases: pattern induction, pattern ranking/selection, and instance extraction. Bootstrapping has been widely adopted in NLP applications such as word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999) and relation extraction (Riloff and Jones, 1999; Pantel and Pennacchiotti, 2006). However, it is known that bootstrapping often acquires instances not related to seed instances. For example, consider the task of collecting the names of common tourist sites from web corpora. Given words like “Geneva” and “Bali” as seed instances, bootstrapping would eventually learn generic patterns such as “pictures” and “photos,” which also co-occur with many other unrelated instances. The subsequent iterations would likely acquire frequent words that co-occur with these generic patterns, such as “Britney Spears.” This phenomenon is called semantic drift (Curran et al., 2007). A straight"
D08-1106,J98-1004,0,0.273044,"Missing"
D08-1106,P95-1026,0,0.752126,"ally)-supervised and unsupervised techniques have been actively studied. Hearst (1992) first presented a bootstrapping method which requires only a small amount of instances Masashi Shimbo NAIST, Japan shimbo@is.naist.jp Yuji Matsumoto NAIST, Japan matsu@is.naist.jp (seed instances) to start with, but can easily multiply the number of tagged instances with minimal human annotation cost, by iteratively applying the following phases: pattern induction, pattern ranking/selection, and instance extraction. Bootstrapping has been widely adopted in NLP applications such as word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999) and relation extraction (Riloff and Jones, 1999; Pantel and Pennacchiotti, 2006). However, it is known that bootstrapping often acquires instances not related to seed instances. For example, consider the task of collecting the names of common tourist sites from web corpora. Given words like “Geneva” and “Bali” as seed instances, bootstrapping would eventually learn generic patterns such as “pictures” and “photos,” which also co-occur with many other unrelated instances. The subsequent iterations would likely acquire frequent words that co-o"
D11-1058,P02-1034,0,0.0433932,"ral studies are in this direction. In the method proposed in Finkel et al. (2006), a cascades of sequence predictions is viewed as a Bayesian network, and sample sequences are drawn at each stage according to the output distribution. The samples are then used to estimate the entire distribution of the cascade. In the method proposed in Bunescu (2008), an upper labeler uses the probabilities marginalized on the parts of the output sequences on lower stages as weights for the features. The weighted features are integrated in the model of the labeler on the upper stage. A k-best approach (e.g., (Collins and Duffy, 2002)) and the methods mentioned above are effective to improve the forward information propagation. However, they can never contribute on backward information propagation. To improve the both directions of information 629 propagation, Some studies propose the joint learning of multiple sequence labelers. Sutton et al. (2007) proposes the joint learning method in case where multiple labels are assigned to each time slice of the input sequences. It enables simultaneous learning and estimation of multiple sequence labelings on the same input sequences, where time slices of the outputs of all the out"
D11-1058,W06-1673,0,0.0345994,"the more difficult it is to enumerate and maintain the k-best results. It is particularly prominent in sequence labeling. The essence of this orientation is that the labeler on an upper stage utilizes the information of all the possible output candidates on lower stages. However, the size of the output space can become quite large in sequence labeling. It effectively forbids explicit enumeration of all possible outputs, so it is required to represent all the labeling possibilities compactly or employ some approximation schemes. Several studies are in this direction. In the method proposed in Finkel et al. (2006), a cascades of sequence predictions is viewed as a Bayesian network, and sample sequences are drawn at each stage according to the output distribution. The samples are then used to estimate the entire distribution of the cascade. In the method proposed in Bunescu (2008), an upper labeler uses the probabilities marginalized on the parts of the output sequences on lower stages as weights for the features. The weighted features are integrated in the model of the labeler on the upper stage. A k-best approach (e.g., (Collins and Duffy, 2002)) and the methods mentioned above are effective to improv"
D11-1058,D09-1005,0,0.183338,"e H⟨1,y1 ⟩ ≡ e1 ∈y1 ∂ h¯ · h⟨1,e1 ⟩ . In other ⟨1,e1 ⟩ words, ∂θ∂L2 becomes the covariance between the ⟨1,k1 ⟩ k1 -th input feature for L1 and the hypothetical feadef ture h′⟨1,e1 ⟩ ≡ ∂L2 ¯ ⟨1,e ⟩ ∂h 1 · h⟨1,e1 ⟩ . The final problem is to derive an efficient way to compute the first term of (18). The second term of (18) can be calculated by the ordinary F-B because it consists of the marginals of arc features. There are two derivations of the algorithm for calculating the first term. We describe briefly the both derivations. One is a variant of the F-B on the expectation semi-ring proposed in Li and Eisner (2009). First, the F-B is generalized to the expectation semi-ring with respect to the hypothetical feature h′⟨1,e1 ⟩ , and by summing up the marginals of the feature vectors f⟨1,e1 ,x⟩ on all the arcs under the distribution of the semi-ring, then we obtain the expectation of the feature vector f⟨1,e1 ,x⟩ on the semi-ring potential. This expectation is equal to the first term of (18). 1 Another derivation is to apply the automatic differentiation (AD)(Wengert, 1964; [Corliss et] al., 2002) on the F-B calculating EP1 F⟨1,y1 ,x⟩ ¯ . It [ ] ¯ ∂ EP1′ F⟨1,y1 ,x⟩ ¯ is exploits the fact that ∂λ λ=0 equal t"
D11-1058,J93-2004,0,0.036539,"f words into syntactic categories. This task is performed by annotating a chunking label on each word (Ramshaw and Marcus, 1995). The types of chunking label consist of “Begin-Category”, which represents the beginning of a chunk, “Inside-Category”, which represents the inside of a chunk, and “Other.” Usually, POS labeling runs first before base-phrase chunking is performed. Therefore, this task is a typical interesting case where a sequence labeling depends on the output from other sequence labelers. The data used for our experiment consist of English sentences from the Penn Treebank project (Marcus et al., 1993) consisting of 10948 sentences and 259104 words. We divided them into two groups, training data consisting of 8936 sentences and 211727 words and test data consisting of 2012 1 For the detailed description, see Li and Eisner (2009) and its references. 2 For example, Berz (1992) gives a detailed description of the reason why the dual number is used for this purpose. def 634 Algorithm 2 Forward-backward Algorithm for Calculating Feature Covariances ( ) def def Input: f⟨1,x⟩ , ϕe1 ≡ exp θ 1 · f⟨1,e1 ,x⟩ , h′e1 ≡ ∂ h¯∂L2 · h⟨1,e1 ⟩ ] ⟨1,e(1 ⟩ [ ) ′ ∀k ∈ K Output: qk1 = CovP(y1 |x) H⟨1,y , F 1 1 ⟨1"
D11-1058,W95-0107,0,0.0310886,"sformation is achieved in an automatic manner, by replacing all appearances of λ in the F-B with a dual number λ + ε. The dual number is a variant of the complex number, with a kind of the imaginary unit ε with the property ε2 = 0. Like the usual complex We examined effectiveness of the method proposed in this paper on a real task. The task is to annotate the POS tags and to perform base-phrase chunking on English sentences. Base-phrase chunking is a task to classify continuous subsequences of words into syntactic categories. This task is performed by annotating a chunking label on each word (Ramshaw and Marcus, 1995). The types of chunking label consist of “Begin-Category”, which represents the beginning of a chunk, “Inside-Category”, which represents the inside of a chunk, and “Other.” Usually, POS labeling runs first before base-phrase chunking is performed. Therefore, this task is a typical interesting case where a sequence labeling depends on the output from other sequence labelers. The data used for our experiment consist of English sentences from the Penn Treebank project (Marcus et al., 1993) consisting of 10948 sentences and 259104 words. We divided them into two groups, training data consisting o"
D11-1058,D08-1070,0,\N,Missing
D11-1137,D07-1101,0,0.0228192,"ocal features. Our framework is for a dependency parser and the decoding in the reranking stage is done with an exact 1-best dynamic programming algorithm. Sangati et al. (2009) proposed a k-best generative reranking algorithm for dependency parsing. In this paper, we use a similar generative model, but combined with a variational model learned on the fly. Moreover, our framework is applicable to forests, not k-best lists. Koo and Collins (2010) presented third-order dependency parsing algorithm. Their model 1 is defined by an enclosing grandsibling for each sibling or grandchild part used in Carreras (2007). Our grandsibling model is similar to the model 1, but ours is defined by a generative model. The decoding in the reranking stage is also similar to the parsing algorithm of their model 1. In order to capture grandsibling factors, our decoding calculates inside probablities for not the current head node but each pair of the node and its outgoing edges. Titov and Henderson (2006) reported that the MBR approach could be applied to a projective dependency parser. In the field of SMT, for an approximation of MAP decoding, Li et al. (2009) proposed variational decoding and Kumar et al. (2009) pres"
D11-1137,P05-1022,0,0.903388,"baseline parser on the fly. The final prediction in the reranking stage is performed using linear interpolation of these models and discriminative model. In order to efficiently train the model from and decode on a hypergraph data structure representing a forest, we apply extended inside/outside and Viterbi algorithms. Experimental results show that our proposed forest reranking algorithm achieves significant improvement when compared with conventional approaches. 1 Introduction Recently, much of research on statistical parsing has been focused on k-best (or forest) reranking (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008). Typically, reranking methods first generate a list of top-k candidates (or a forest) from a baseline system, then rerank the candidates with arbitrary features that are intractable within the baseline system. In the reranking framework, the baseline system is usually modeled with a generative model, and a discriminative model is used for reranking. Sangati et al. (2009) reversed the usual order of the two models for dependency parsing by employing a generative model to rescore the k-best candidates provided by a discriminative model. They use a variant of Eisner’s generative mo"
D11-1137,J07-2003,0,0.019011,"of each model in Table 3. Our reranking models are generative versions of Koo and Collins (2010)’s third-order factorization model. Non-locality of weight function makes it difficult to perform the search of Eq.8 with an usual exact Viterbi 1-best algorithm. One solution to resolve the intractability is an approximate k-best Viterbi search. For a constituent parser, Huang (2008) applied cube pruning techniques to forest reranking with non-local features. Cube pruning is originally proposed for the decoding of statistical machine translation (SMT) with an integrated n-gram lan1482 guage model (Chiang, 2007). It is an approximate k-best Viterbi search algorithm using beam search and lazy computation (Huang and Chiang, 2005). In the case of a dependency parser, Koo and Collins (2010) proposed dynamic-programmingbased third-order parsing algorithm, which enumerates all grandparents with an additional loop. Our hypergraph based search algorithm for Eq.8 share the same spirit to their third-order parsing algorithm since the grandsibling model is similar to their model 1 in that it is factored in grandsibling structure. Algorithm 1 shows the search algorithm. This is almost the same bottom-up 1-best V"
D11-1137,C96-1058,0,0.523394,", 2008). Typically, reranking methods first generate a list of top-k candidates (or a forest) from a baseline system, then rerank the candidates with arbitrary features that are intractable within the baseline system. In the reranking framework, the baseline system is usually modeled with a generative model, and a discriminative model is used for reranking. Sangati et al. (2009) reversed the usual order of the two models for dependency parsing by employing a generative model to rescore the k-best candidates provided by a discriminative model. They use a variant of Eisner’s generative model C (Eisner, 1996b; 1479 Eisner, 1996a) for reranking and extend it to capture higher-order information than Eisner’s second-order generative model. Their reranking model showed large improvements in dependency parsing accuracy. They reported that the discriminative model is very effective at filtering out bad candidates, while the generative model is able to further refine the selection among the few best candidates. In this paper, we propose a forest generative reranking algorithm, opposed to Sangati et al. (2009)’s approach which reranks only k-best candidates. Forests usually encode better candidates more"
D11-1137,N10-1115,0,0.0368683,"ecoding. The search algorithm in the reranking stage can be performed using dynamic programming algorithm. Our variational reranking is aimed at selecting a candidate from a forest, which is correct both in local and global. Our experimental results show more significant improvements than conventional approaches, such as k-best and forest generative reranking. In the future, we plan to investigate more appropriate generative models for reranking. PPAttachment is one of the most difficult problems for a natural language parser. We plan to examine to model such a complex structure (granduncle) (Goldberg and Elhadad, 2010) or higher-order structure than third-order for reranking which is computationally expensive for a baseline parser. As we mentioned in Section 5.4, we also plan to incorporate semi-supervised learning into our framework, which may potentially improve our reranking performance. Acknowledgments We would like to thank Graham Neubig and Masashi Shimbo for their helpful comments and to the anonymous reviewers for their effort of reviewing our paper and giving valuable comments. This work was supported in part by Grant-in-Aid for Japan Society for the Promotion of Science (JSPS) Research Fellowship"
D11-1137,W05-1506,0,0.441204,"er factorization model. Non-locality of weight function makes it difficult to perform the search of Eq.8 with an usual exact Viterbi 1-best algorithm. One solution to resolve the intractability is an approximate k-best Viterbi search. For a constituent parser, Huang (2008) applied cube pruning techniques to forest reranking with non-local features. Cube pruning is originally proposed for the decoding of statistical machine translation (SMT) with an integrated n-gram lan1482 guage model (Chiang, 2007). It is an approximate k-best Viterbi search algorithm using beam search and lazy computation (Huang and Chiang, 2005). In the case of a dependency parser, Koo and Collins (2010) proposed dynamic-programmingbased third-order parsing algorithm, which enumerates all grandparents with an additional loop. Our hypergraph based search algorithm for Eq.8 share the same spirit to their third-order parsing algorithm since the grandsibling model is similar to their model 1 in that it is factored in grandsibling structure. Algorithm 1 shows the search algorithm. This is almost the same bottom-up 1-best Viterbi algorithm except an additional loop in line 4. Line 4 references outgoing edge e′ of node h from a set of outgo"
D11-1137,P10-1110,0,0.158821,"Missing"
D11-1137,P08-1067,0,0.247023,"The final prediction in the reranking stage is performed using linear interpolation of these models and discriminative model. In order to efficiently train the model from and decode on a hypergraph data structure representing a forest, we apply extended inside/outside and Viterbi algorithms. Experimental results show that our proposed forest reranking algorithm achieves significant improvement when compared with conventional approaches. 1 Introduction Recently, much of research on statistical parsing has been focused on k-best (or forest) reranking (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008). Typically, reranking methods first generate a list of top-k candidates (or a forest) from a baseline system, then rerank the candidates with arbitrary features that are intractable within the baseline system. In the reranking framework, the baseline system is usually modeled with a generative model, and a discriminative model is used for reranking. Sangati et al. (2009) reversed the usual order of the two models for dependency parsing by employing a generative model to rescore the k-best candidates provided by a discriminative model. They use a variant of Eisner’s generative model C (Eisner,"
D11-1137,P10-1001,0,0.173547,"2008). Moreover, our reranking uses not only a generative model obtained from training data, but also a sentence specific generative model learned from a forest. In the reranking stage, we use linearly combined model of these models. We call this variational reranking model. The model proposed in this paper is factored in the third-order structure, therefore, its non-locality makes it difficult to perform the reranking with an usual 1-best Viterbi search. To solve this problem, we also propose a new search algorithm, which is inspired by the third-order dynamic programming parsing algorithm (Koo and Collins, 2010). This algorithm enables us an exact 1-best reranking without any approximation. We summarize our contributions in this paper as follows. • To extend k-best to forest generative reranking. • We introduce variational reranking which is a combination approach of generative reranking and variational decoding (Li et al., 2009). • To obtain 1-best tree in the reranking stage, we Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1479–1488, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics .top.0,8 propose an exact"
D11-1137,P08-1068,0,0.0309923,"hows the examples whose accuracy scores improved by the grandsibling model. For example, the dependency relationship from Verb to Noun phrase was corrected by our proposed model. On the other hand, many errors remain still in 1486 Table 10 shows the comparison of the performance of variational reranking (16-best forests) with that of other systems. Our method outperforms supervised parsers with second-order features, and achieves comparable results compared to a parser with thirdorder features (Koo and Collins, 2010). We can not directly compare our method with semi-supervised parsers such as Koo et al. (2008)’s semi-sup and Suzuki et al. (2009), because ours does not use additional unlabeled data for training. The model trained from unlabeled data can be easily incorporated into our reranking framework. We plan to investigate semi-supervised learning in future work. Table 9: Examples of outputs for input sentence No.148 and No.283 in section 23 from baseline and variational reranking parsers. The underlined portions show the effect of the grandsibling model. sent (No.148) A quick turnaround is crucial to Quantum because its cash requirements remain heavy . correct 3 3 4 0 4 5 6 4 11 11 12 8 12 4 b"
D11-1137,P09-1019,0,0.105002,"locally and globally appropriate candidate from a forest. Table 7 shows the parsing time (on 2.66GHz Quad-Core Xeon) of the baseline k-best, generative reranking and variational reranking parsers (java implemented). The variational reranking parser contains the following procedures. 1. 2. 3. 4. k-best forest creation (baseline) Estimation of variational model Forest pruning Search with the third-order model Our reranking parser incurred little overhead to the Table 5: The comparison of the decoding frameworks: MBR decoding seeks a candidate which has the highest accuracy scores over a forest (Kumar et al., 2009). Variational decoding is performed based on Eq.8. XX XXX Eval Unlabeled XXX Decoding XX baseline 91.9 MBR (8-best forest) 91.99 Variational (8-best forest) 92.17 Table 7: The parsing time (CPU second per sentence) and accuracy score of the baseline k-best, generative reranking and variational reranking parsers k baseline generative variational 2 0.09 (91.9) +0.03 (92.67) +0.05 (92.76) 4 0.1 (91.9) +0.05 (92.68) +0.09 (92.81) 8 0.13 (91.9) +0.06 (92.72) +0.11 (92.87) 16 0.18 (91.9) +0.07 (92.75) +0.12 (92.89) 32 0.29 (91.9) +0.07 (92.73) +0.13 (92.89) 64 0.54 (91.9) +0.08 (92.72) +0.15 (92.87)"
D11-1137,P09-1067,0,0.366974,"hird-order structure, therefore, its non-locality makes it difficult to perform the reranking with an usual 1-best Viterbi search. To solve this problem, we also propose a new search algorithm, which is inspired by the third-order dynamic programming parsing algorithm (Koo and Collins, 2010). This algorithm enables us an exact 1-best reranking without any approximation. We summarize our contributions in this paper as follows. • To extend k-best to forest generative reranking. • We introduce variational reranking which is a combination approach of generative reranking and variational decoding (Li et al., 2009). • To obtain 1-best tree in the reranking stage, we Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1479–1488, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics .top.0,8 propose an exact 1-best search algorithm with the third-order model. In experiments on English Penn Treebank data, we show that our proposed methods bring significant improvement to dependency parsing. Moreover, our variational reranking framework achieves consistent improvement, compared to conventional approaches, such as simple k-best a"
D11-1137,P05-1010,0,0.0396665,"n rule to dependency parsing. For dependency parsing, we can choose to model q ∗ as the tri-sibling and grandsibling generative models in section 3. 2 In case of dependency parsing, Titov and Henderson (2006) proposed that a loss function is simply defined using a dependency attachment score. 3 In SMT, a marginalization of all derivations which yield a paticular translation needs to be carried out for each translation. This makes the MAP decoding NP-hard in SMT. This variational approximate framework can be applied to other tasks collapsing spurious ambiguity, such as latent-variable parsing (Matsuzaki et al., 2005). Algorithm 2 DP-ML Estimation(HG(x)) 100 1: run inside and outside algorithm on HG(x) 2: for v ∈ V do 3: for e ∈ IE(v) do 4: ctsib = pe · α(v)/β(top) 5: for u ∈ tails(e) do 6: ctsib = ctsib · β(u) 7: for e′ ∈ IE(u) do 8: cgsib = pe · pe′ · α(v)/β(top) 9: for u′ ∈ tails(e)  u do 10: cgsib = cgsib · β(u′ ) 11: for u′′ ∈ tails(e′ ) do 12: cgsib = cgsib · β(u′′ ) 13: for u′′ ∈ tails(e′ ) do 14: c2 (u′′ |C(u′′ ))+ = cgsib 15: c2 (C(u′′ ))+ = cgsib 16: for u ∈ tails(e) do 17: c1 (u|C(u))+ = ctsib 18: c1 (C(u))+ = ctsib 19: MLE estimate q1∗ , q2∗ using formula Eq.14 = Unlabeled Accuracy 98 k=100 97"
D11-1137,E06-1011,0,0.138728,"b .sib .v g.. .h .sib .v Figure 2: The left side denotes tri-sibling structure and the right side denotes grandsibling structure. Table 3: A summarization of the model factorization and order first-order second-order (sibling) third-order (tri-sibling) third-order (grandsibling) McDonald et al. (2005) Eisner (1996a) McDonald et al. (2005) tri-sibling model Model 2 (Koo and Collins, 2010) grandsibling model (Sangati et al., 2009) Model 1 (Koo and Collins, 2010) 3.2 Exact Search Algorithm Our baseline discriminative model uses first- and second-order features provided in (McDonald et al., 2005; McDonald and Pereira, 2006). Therefore, both our tri-sibling model and baseline discriminative model integrate local features that are factored in one hyperedge. On the other hand, the grandsibling model has non-local features because the grandparent is not factored in one hyperedge. We summarize the order of each model in Table 3. Our reranking models are generative versions of Koo and Collins (2010)’s third-order factorization model. Non-locality of weight function makes it difficult to perform the search of Eq.8 with an usual exact Viterbi 1-best algorithm. One solution to resolve the intractability is an approximate"
D11-1137,P05-1012,0,0.710609,"g 2-nd term wt(h),t(sib),d 3-rd term wt(v),t(h),t(sib),d wt(h),wt(sib),t(tsib),d t(h),t(sib),d t(v),t(h),t(sib),d wt(h),wt(sib),t(g),d t(h),t(sib),d t(v),t(h),t(sib),d t(h),wt(sib),t(tsib),d wt(h),t(sib),t(tsib),d t(h),t(sib),t(tsib),d — — — — — — t(h),wt(sib),t(g),d wt(h),t(sib),t(g),d t(h),t(sib),t(g),d — — .. h .tsib .sib .v g.. .h .sib .v Figure 2: The left side denotes tri-sibling structure and the right side denotes grandsibling structure. Table 3: A summarization of the model factorization and order first-order second-order (sibling) third-order (tri-sibling) third-order (grandsibling) McDonald et al. (2005) Eisner (1996a) McDonald et al. (2005) tri-sibling model Model 2 (Koo and Collins, 2010) grandsibling model (Sangati et al., 2009) Model 1 (Koo and Collins, 2010) 3.2 Exact Search Algorithm Our baseline discriminative model uses first- and second-order features provided in (McDonald et al., 2005; McDonald and Pereira, 2006). Therefore, both our tri-sibling model and baseline discriminative model integrate local features that are factored in one hyperedge. On the other hand, the grandsibling model has non-local features because the grandparent is not factored in one hyperedge. We summarize the"
D11-1137,D08-1022,0,0.0136474,"ctsib as the posterior weight for computing expected count c1 of events in the tri-sibling model q1∗ . Lines 16-18 compute c1 for all events occuring in a hyperedge e. The expected count c2 needed for the estimation of grandsibling model q2∗ is extracted in lines 7-15. c2 for a grandsibling model must be extracted over two hyperedges e and e′ because it needs grandparent information. Lines 8-12 show the algorithm to compute the posterior weight cgsib of e and e′ , which 1484 is similar to that to compute the posterior weight of rules of tree substitution grammars used in treebased MT systems (Mi and Huang, 2008). Lines 13-15 compute expected counts c2 of events occuring over two hyperedges e and e′ . Finally, line 19 estimates q1∗ and q2∗ using the form in Eq.14. Li et al. (2009) assumes n-gram locality of the forest to efficiently train the model, namely, the baseline n-gram model has larger n than that of variational n-gram model. In our case, grandsibling locality is not embedded in the forest generated from the baseline parser. Therefore, we need to reference incoming hyperedges of tail nodes in line 7. y ∗ of Eq.12 may be locally appropriate but globally inadequate because q ∗ only approximates"
D11-1137,P03-1021,0,0.0907227,"e show the reductions list for each term of two models in Table 2. The usage of reductions list is identical to Eisner (1996a) and readers may refer to it for further details. The final prediction is performed using a loglinear interpolated model. It interpolates the baseline discriminative model and two (tri-sibling and grandsibling) generative models. yˆ = argmax 2 ∑ log qn (top(y))θn y∈G(x) n=1 + log p(y|x)θbase (8) (5) where θ are parameters to adjust the weight of each = q1 (dist(v, h), wrd(v), tag(v)|h, sib, tsib, dir) term in prediction. These parameters are tuned using MERT algorithm (Och, 2003) on development data = q1 (tag(v)|h, sib, tsib, dir) using a criterion of accuracy maximization. The rea×q1 (wrd(v)|tag(v), h, sib, tsib, dir) son why we chose MERT is that it effectively tunes ×q1 (dist(v, h)|wrd(v), tag(v), h, sib, tsib, dir) dense parameters with a line search algorithm. 1481 q1 (v|h, sib, tsib, dir) Table 2: Reduction lists for tri-sibling and grandsibling models: wt(), w() and t() mean word and POS-tag, word, POS-tag for a node. d indicates the direction. The first reduction on the list keeps all or most of the original condition; later reductions throw away more and more"
D11-1137,W09-3839,0,0.304606,"achieves significant improvement when compared with conventional approaches. 1 Introduction Recently, much of research on statistical parsing has been focused on k-best (or forest) reranking (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008). Typically, reranking methods first generate a list of top-k candidates (or a forest) from a baseline system, then rerank the candidates with arbitrary features that are intractable within the baseline system. In the reranking framework, the baseline system is usually modeled with a generative model, and a discriminative model is used for reranking. Sangati et al. (2009) reversed the usual order of the two models for dependency parsing by employing a generative model to rescore the k-best candidates provided by a discriminative model. They use a variant of Eisner’s generative model C (Eisner, 1996b; 1479 Eisner, 1996a) for reranking and extend it to capture higher-order information than Eisner’s second-order generative model. Their reranking model showed large improvements in dependency parsing accuracy. They reported that the discriminative model is very effective at filtering out bad candidates, while the generative model is able to further refine the selec"
D11-1137,D09-1058,0,0.0295312,"scores improved by the grandsibling model. For example, the dependency relationship from Verb to Noun phrase was corrected by our proposed model. On the other hand, many errors remain still in 1486 Table 10 shows the comparison of the performance of variational reranking (16-best forests) with that of other systems. Our method outperforms supervised parsers with second-order features, and achieves comparable results compared to a parser with thirdorder features (Koo and Collins, 2010). We can not directly compare our method with semi-supervised parsers such as Koo et al. (2008)’s semi-sup and Suzuki et al. (2009), because ours does not use additional unlabeled data for training. The model trained from unlabeled data can be easily incorporated into our reranking framework. We plan to investigate semi-supervised learning in future work. Table 9: Examples of outputs for input sentence No.148 and No.283 in section 23 from baseline and variational reranking parsers. The underlined portions show the effect of the grandsibling model. sent (No.148) A quick turnaround is crucial to Quantum because its cash requirements remain heavy . correct 3 3 4 0 4 5 6 4 11 11 12 8 12 4 baseline 3 3 4 0 4 5 6 4 11 11 8 8 12"
D11-1137,C10-1123,0,0.0187849,"using G(x), the conditional probability p(y|x) is typically derived as follows: p(y|x) = eγ·s(x,y) eγ·s(x,y) =∑ γ·s(x,y) Z(x) y∈G(x) e (2) where s(x, y) is the score function shown in Eq.1 and γ is a scaling factor to adjust the sharpness of the distribution and Z(x) is a normarization factor. 2.1 Hypergraph Representation We propose to encode many hypotheses in a compact representation called dependency forest. While there may be exponentially many dependency trees, the forest represents them in polynomial space. A dependency forest (or tree) can be defined as a hypergraph data strucure HG (Tu et al., 2010). Figure 1 shows an example of a hypergraph for a dependency tree. A shaded hyperedge e is defined as the following form: e : ⟨(I1,2 , girl3,5 , with5,8 ), saw1,8 ⟩. 1480 . .. .e . .saw1,8 . :V ... . . g. irl3,5 . :N .I1,2.:N . . . . . . . . . . . .a3,4.:D . . . . .with5,8 . :P . . . 6,8 :N . . .telescope . . .a .:D . . . . . 6,7 Figure 1: An example of dependency tree for a sentence “I saw a girl with a telescope”. The node saw1,8 is a head node of e. The nodes, I1,2 , girl3,5 and with5,8 , are tail nodes of e. The hyperedge e is an incoming edge for saw1,8 and outgoing edge for each of I1,2"
D11-1137,W03-3023,1,0.78739,"tion of variational decoding and generative reranking. We call this framework variational reranking. Table 4: The statistics of forests and 20-best lists on development data: this shows the average number of hyperedges and nodes per sentence and oracle scores. forest 20-best pruning threshold ρ = 10−3 — ave. num of hyperedges 180.67 255.04 ave. num of nodes 135.74 491.42 oracle scores 98.76 96.78 5 Experiments Experiments are performed on English Penn Treebank data. We split WSJ part of the Treebank into sections 02-21 for training, sections 22 for development, sections 23 for testing. We use Yamada and Matsumoto (2003)’s head rules to convert phrase structure to dependency structure. We obtain k-best lists and forests generated from the baseline discriminative model which has the same feature set as provided in (McDonald et al., 2005), using the secondorder Eisner algorithms. We use MIRA for training as it is one of the learning algorithms that achieves the best performance in dependency parsing. We set the scaling factor γ = 1.0. We also train a generative reranking model from the training data. To reduce the data sparseness problem, we use the back-off strategy proposed in (Eisner, 1996a). Parameters θ ar"
D13-1014,D10-1115,0,0.519479,"s addressed the question of how to construct individual word representations, for example distributional models (Mitchell and Lapata, 2010) and neural models (Collobert and Weston, 2008). These word representations are used in various natural language processing (NLP) tasks such as part-of-speech tagging, chunking, named entity recognition, and semantic role labeling (Turian et al., 2010; Collobert et al., 2011). Recently, modeling of semantic compositionality (Frege, 1892) in vector space has emerged as another important line of research (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Socher et al., 2012; Grefenstette and Sadrzadeh, 2011; Van de Cruys et al., 2013). The goal is to formulate how individual word representations ought to be combined to achieve phrasal or sentential semantics. The main questions for semantic compositionality that we are concerned with are: (1) how can polysemy be handled by a single vector representation per word type, learned by either a distributional or neural model, and (2) how does composition resolve 130 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 130–140, c Seattle, Washington, USA, 18-"
D13-1014,D12-1050,0,0.209522,"subject and object vector, the outer product of the verb vector with itself, and then the element-wise product of both results. 3. Erk and Pad´o’s (2008) model, which adapts the word vectors based on context and is the most similar in terms of motivation to ours. 4. Van de Cruy et al. (2013) multi-way interaction model based on matrix factorization. This achieves the best result for this task to date. A detailed explanation of these models will be provided in Section 7. For the underlying word representations, we experiment with sparse 2000-dim SDS and dense 50-dim NLM. These are provided by Blacoe and Lapata (2012)5 and trained on the British National Corpus (BNC). We are interested in knowing how sensitive each model is to the underlying word representation. In general, this is a challenging task: the upper-bound of ρ = 0.62 is the inter-annotator agreement. 5 http://www.cs.ox.ac.uk/ people/edward.grefenstette/ http://homepages.inf.ed.ac.uk/ s1066731/index.php?page=resources 135 5.3 Implementation details In terms of implementation detail, our model and our re-implementation of Erk and Pado’s model make use of the ukWaC corpus (Baroni et al., 2009).6 This corpus is a two billion word corpus automatical"
D13-1014,W13-3206,0,0.0922558,"ators that combine two word vector representations, u, v ∈ Rn and their learning parameters. Our model only needs two hyper-parameters: the number of prototype words m and dimensional reduction k in SVD verbs, which is the dataset we used here. The previous state-of-the-art result for this task comes from the model of Van de Cruys et al. (2013). They model compositionality as a multi-way interaction between latent factors, which are automatically constructed from corpus data via matrix factorization. Comprehensive evaluation of various existing models are reported in (Blacoe and Lapata, 2012; Dinu et al., 2013). Blacoe and Lapata (2012) highlight the importance of jointly examining word representations and compositionality operators. However, two out of three composition methods they evaluate are parameter-free, so that they can side-step the issue of parameter estimation. Dinu et al. (2013) describe the relation between word vector and compositionality in more detail with free parameters. Table 6 summarizes some ways to compose the meaning of two word vectors (u, v), following (Dinu et al., 2013). These range from simple operators (e.g. Add and Multiply) to expressive models with many free paramete"
D13-1014,D08-1094,0,0.263669,"Missing"
D13-1014,W09-0208,0,0.0282538,"Missing"
D13-1014,D11-1129,0,0.235862,"generating the overall semantics. This readily addresses some major challenges with current vector space models, notably the polysemy issue and the use of one representation per word type. We implement cocompositionality using prototype projections on predicates/arguments and show that this is effective in adapting their word representations. We further cast the model as a neural network and propose an unsupervised algorithm to jointly train word representations with co-compositionality. The model achieves the best result to date (ρ = 0.47) on the semantic similarity task of transitive verbs (Grefenstette and Sadrzadeh, 2011). Ok Figure 1: Here, we capture the semantics of run in run company by projecting the original word representation of run to the prototype space of company (and vice versa). Introduction Vector space models of words have been very successful in capturing the semantic and syntactic characteristics of individual lexical items (Turney and Pantel, 2010). Much research has addressed the question of how to construct individual word representations, for example distributional models (Mitchell and Lapata, 2010) and neural models (Collobert and Weston, 2008). These word representations are used in vari"
D13-1014,W10-2805,0,0.0596095,"he first approaches is the vector addition/multiplication idea of Mitchell and Lapata (2008). The appeal of this kind of simple approach is its intuitive geometric interpretation and its robustness to various datasets. However, it may not be sufficiently expressive to represent the various factors involved in compositional semantics, such as syntax and context. To this end, Baroni and Zamparelli (2010) present a compositional model for adjectives and nouns. In their model, an adjective is a matrix operator that modifies the noun vector into an adjective-noun vector. Zanzotto et al. (2010) and Guevara (2010) also proposed linear transformation models for composition and address the issue of estimating large matrices with least squares or regression techniques. Socher et al. (2012) extend this linear transformation approach with the more powerful model of Matrix-Vector Recursive Neural Networks (MV-RNN). Each node in a parse tree is assigned both a vector and a matrix. The vector captures the actual meaning of the word itself, while the matrix is modeled as a operator that modify the meaning of neighboring words and phrases. This model captures semantic change phenomenon like not bad is similar to"
D13-1014,P12-1092,0,0.0574691,"are usually called word embeddings, and it has been shown that such vectors can capture interesting linear relationships, such as king − man + woman ≈ queen (Mikolov et al., 2013). In this work, we adopt the model by Collobert and Weston (2008). The idea is to construct a neural network based on word sequences, where one outputs high scores for n-grams that occur in a large unlabeled corpus and low scores for nonsense n-grams where one word is replaced by a random word. This word representation with NLM has been used to good effect, for example in (Turian et al., 2010; Collobert et al., 2011; Huang et al., 2012) where induced word representations are used with sophisticated features to improve performance in various NLP tasks. Specifically, we first represent the word sequence as a vector x = [d(w1 ); d(w2 ); . . . ; d(wm )], where wi is ith word in the sequence, m is the window size, d(w) is the vector representation of word w (an n-dimensional column vector) and [d(w1 ); d(w2 ); . . . ; d(wm )] is the concatenation of word vectors as an input of neural network. Second, we compute the score of the sequence, score(x) = sT (tanh(Wx + b)) (2) where W ∈ Rh×(mn) and s ∈ Rh are the first and second layer"
D13-1014,N13-1090,0,0.0190747,"e target word t, the total count of all word tokens, the frequency of the target word t, and the frequency of the context word ci , respectively. 2.2 Neural Language Model (NLM) word embeddings Another popular way to learn word representations is based on the Neural Language Model (NLM) (Bengio et al., 2003). In comparison with SDS, NLM tend to be low-dimensional (e.g. 50 dimensions) but employ dense features. These dense feature vectors are usually called word embeddings, and it has been shown that such vectors can capture interesting linear relationships, such as king − man + woman ≈ queen (Mikolov et al., 2013). In this work, we adopt the model by Collobert and Weston (2008). The idea is to construct a neural network based on word sequences, where one outputs high scores for n-grams that occur in a large unlabeled corpus and low scores for nonsense n-grams where one word is replaced by a random word. This word representation with NLM has been used to good effect, for example in (Turian et al., 2010; Collobert et al., 2011; Huang et al., 2012) where induced word representations are used with sophisticated features to improve performance in various NLP tasks. Specifically, we first represent the word"
D13-1014,P08-1028,0,0.854122,"ical items (Turney and Pantel, 2010). Much research has addressed the question of how to construct individual word representations, for example distributional models (Mitchell and Lapata, 2010) and neural models (Collobert and Weston, 2008). These word representations are used in various natural language processing (NLP) tasks such as part-of-speech tagging, chunking, named entity recognition, and semantic role labeling (Turian et al., 2010; Collobert et al., 2011). Recently, modeling of semantic compositionality (Frege, 1892) in vector space has emerged as another important line of research (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Socher et al., 2012; Grefenstette and Sadrzadeh, 2011; Van de Cruys et al., 2013). The goal is to formulate how individual word representations ought to be combined to achieve phrasal or sentential semantics. The main questions for semantic compositionality that we are concerned with are: (1) how can polysemy be handled by a single vector representation per word type, learned by either a distributional or neural model, and (2) how does composition resolve 130 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Pr"
D13-1014,nivre-etal-2006-maltparser,0,0.065447,"Missing"
D13-1014,N10-1013,0,0.0253016,"el (Section 4.2). 4.1 Compositional Neural Language Model (C-NLM) Compositional Neural Language Model (C-NLM) is a combination of a word representation learning method and compositional rule. In contrast to other compositional models based on machine learning, our model has no complex parameters for modeling composition. Composition is modeled using straightforward vector addition/multiplications; instead, what is learned is the word representation. Figure 3 shows the C-NLM. The learning algorithm is unsupervised, and works by artificially 3 There are works on multiple representations, e.g., (Reisinger and Mooney, 2010); we focus on single representation here. 134 verb v Pverb obj o Figure 4: Co-Compositional Neural Language Model (CoC-NLM) is C-NLM with prototype projection. generating negative examples in a fashion analogous to the NLM learning algorithm of (Collobert and Weston, 2008) and contrastive estimation (Smith and Eisner, 2005). First, given some initial word representations and raw sentences, we compute the compositional vector with function f (in this section, we will assume that we will be using the addition operator). Second, in order to obtain the score of compositional vector, we compute the"
D13-1014,P05-1044,0,0.0282066,"ition is modeled using straightforward vector addition/multiplications; instead, what is learned is the word representation. Figure 3 shows the C-NLM. The learning algorithm is unsupervised, and works by artificially 3 There are works on multiple representations, e.g., (Reisinger and Mooney, 2010); we focus on single representation here. 134 verb v Pverb obj o Figure 4: Co-Compositional Neural Language Model (CoC-NLM) is C-NLM with prototype projection. generating negative examples in a fashion analogous to the NLM learning algorithm of (Collobert and Weston, 2008) and contrastive estimation (Smith and Eisner, 2005). First, given some initial word representations and raw sentences, we compute the compositional vector with function f (in this section, we will assume that we will be using the addition operator). Second, in order to obtain the score of compositional vector, we compute the dot product with vector s ∈ Rn (n is the dimension of the word vector space): verb vector v = d(wv ) and object vector o = d(wo ). score(v, o) = sT f (v, o) = sT (v + o) (8) We also create a corrupted pair by substituting a random verb wverb ′ . The cost function J = max(0, 1 − score(v, o) + score(vc , o)), where vc is the"
D13-1014,D12-1110,0,0.386869,"ow to construct individual word representations, for example distributional models (Mitchell and Lapata, 2010) and neural models (Collobert and Weston, 2008). These word representations are used in various natural language processing (NLP) tasks such as part-of-speech tagging, chunking, named entity recognition, and semantic role labeling (Turian et al., 2010; Collobert et al., 2011). Recently, modeling of semantic compositionality (Frege, 1892) in vector space has emerged as another important line of research (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Socher et al., 2012; Grefenstette and Sadrzadeh, 2011; Van de Cruys et al., 2013). The goal is to formulate how individual word representations ought to be combined to achieve phrasal or sentential semantics. The main questions for semantic compositionality that we are concerned with are: (1) how can polysemy be handled by a single vector representation per word type, learned by either a distributional or neural model, and (2) how does composition resolve 130 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 130–140, c Seattle, Washington, USA, 18-21 October 2013. 2013"
D13-1014,P10-1097,0,0.164923,"Missing"
D13-1014,I11-1127,0,0.061731,"Missing"
D13-1014,P10-1040,0,0.16006,"pace of company (and vice versa). Introduction Vector space models of words have been very successful in capturing the semantic and syntactic characteristics of individual lexical items (Turney and Pantel, 2010). Much research has addressed the question of how to construct individual word representations, for example distributional models (Mitchell and Lapata, 2010) and neural models (Collobert and Weston, 2008). These word representations are used in various natural language processing (NLP) tasks such as part-of-speech tagging, chunking, named entity recognition, and semantic role labeling (Turian et al., 2010; Collobert et al., 2011). Recently, modeling of semantic compositionality (Frege, 1892) in vector space has emerged as another important line of research (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Socher et al., 2012; Grefenstette and Sadrzadeh, 2011; Van de Cruys et al., 2013). The goal is to formulate how individual word representations ought to be combined to achieve phrasal or sentential semantics. The main questions for semantic compositionality that we are concerned with are: (1) how can polysemy be handled by a single vector representation per w"
D13-1014,N13-1134,0,0.625416,"Missing"
D13-1014,C10-1142,0,0.0217523,"ded survey papers. One of the first approaches is the vector addition/multiplication idea of Mitchell and Lapata (2008). The appeal of this kind of simple approach is its intuitive geometric interpretation and its robustness to various datasets. However, it may not be sufficiently expressive to represent the various factors involved in compositional semantics, such as syntax and context. To this end, Baroni and Zamparelli (2010) present a compositional model for adjectives and nouns. In their model, an adjective is a matrix operator that modifies the noun vector into an adjective-noun vector. Zanzotto et al. (2010) and Guevara (2010) also proposed linear transformation models for composition and address the issue of estimating large matrices with least squares or regression techniques. Socher et al. (2012) extend this linear transformation approach with the more powerful model of Matrix-Vector Recursive Neural Networks (MV-RNN). Each node in a parse tree is assigned both a vector and a matrix. The vector captures the actual meaning of the word itself, while the matrix is modeled as a operator that modify the meaning of neighboring words and phrases. This model captures semantic change phenomenon like no"
D13-1014,2014.lilt-9.5,0,\N,Missing
D13-1014,L08-1000,0,\N,Missing
D16-1109,D15-1085,0,0.0260782,"ence from the stack. Using this approach, they achieved high performance in terms of both dependency parsing and disfluency detection on the Switchboard corpus. However, the authors assume that the input texts to parse are transcribed by human annotators, which, in practice, is unrealistic. In real-world applications, in addition to disfluencies, the input texts contain ASR errors; these issues might degrade the parsing performance. For example, proper nouns that are not contained in the ASR system vocabulary may break up into smaller pieces, yielding a difficult problem for the parsing unit (Cheng et al., 2015): Joint dependency parsing with disfluency detection is an important task in speech language processing. Recent methods show high performance for this task, although most authors make the unrealistic assumption that input texts are transcribed by human annotators. In real-world applications, the input text is typically the output of an automatic speech recognition (ASR) system, which implies that the text contains not only disfluency noises but also recognition errors from the ASR system. In this work, we propose a parsing method that handles both disfluency and ASR errors using an incremental"
D16-1109,E14-4009,0,0.0228192,"for transferring the gold dependency annotation to the ASR output texts to construct training data for our parser. We conducted an experiment on the Switchboard corpus and show that our method outperforms conventional methods in terms of dependency parsing and disfluency detection. 1 Introduction Spontaneous speech is different from written text in many ways, one of which is that it contains disfluencies, that is, parts of the utterance that are corrected by the speaker during the utterance. NLP system performance is reported to deteriorate when there are disfluencies, for example, with SMT (Cho et al., 2014). Therefore, it is desirable to preprocess the speech before passing it to other NLP tasks. REF: what can we get at Litanfeeth HYP: what can we get it leaks on feet In this work, we propose a method for joint dependency parsing and disfluency detection that can robustly parse ASR output texts. Our parser handles both disfluencies and ASR errors using an incremental shift-reduce algorithm, with novel features that consider recognition errors of the ASR system. Furthermore, to evaluate dependency parsing per1036 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processi"
D16-1109,de-marneffe-etal-2006-generating,0,0.178034,"Missing"
D16-1109,N15-1029,0,0.350391,"Missing"
D16-1109,Q14-1011,0,0.421667,"tArcError, and RightArcError, to handle disfluencies and ASR errors. Edit action removes a disfluent token when it is the first element of the stack. This is different from Honnibal (2014)’s Edit action: theirs accumulates consecutive disfluent tokens on the top of the stack and removes them all at once, whereas our method removes this kind of token one-by-one. Use of this Edit action guarantees that the length of the action sequence is always 2n − 1. This property is advantageous because the parser can use the standard beam search and does not require normalization, such as those adopted in (Honnibal and Johnson, 2014) and (Zhu et al., 2013). LeftArcError and RightArcError act in the same way as LeftArc and RightArc, except that these act only on ASR error tokens, whereas the original LeftArc and RightArc are reserved for non ASR error tokens. Using two different kinds of Arc actions for the two types of tokens (ASR error or not) allows for the weights not to be shared between them, and is expected to yield improved performance. In the experiment below, we train all of the models using structured perceptron with max violation (Huang et al., 2012). The feature set is mainly based on (Honnibal and Johnson, 20"
D16-1109,D14-1009,0,0.0135883,"d Disfluency Detection for Automatic Speech Recognition Texts Masashi Yoshikawa and Hiroyuki Shindo and Yuji Matsumoto Graduate School of Information and Science Nara Institute of Science and Technology 8916-5, Takayama, Ikoma, Nara, 630-0192, Japan { masashi.yoshikawa.yh8, shindo, matsu }@is.naist.jp Abstract There are a number of studies that address the problem of detecting disfluencies. Some of these studies include dependency parsing (Honnibal and Johnson, 2014; Wu et al., 2015; Rasooli and Tetreault, 2014), whereas others are dedicated systems (Qian and Liu, 2013; Ferguson et al., 2015; Hough and Purver, 2014; Hough and Schlangen, 2015; Liu et al., 2003). Among these studies, Honnibal (2014) and Wu (2015) address this problem by adding a new action to transition-based dependency parsing that removes the disfluent parts of the input sentence from the stack. Using this approach, they achieved high performance in terms of both dependency parsing and disfluency detection on the Switchboard corpus. However, the authors assume that the input texts to parse are transcribed by human annotators, which, in practice, is unrealistic. In real-world applications, in addition to disfluencies, the input texts con"
D16-1109,N12-1015,0,0.060834,"Missing"
D16-1109,N13-1102,0,0.118923,"oint Transition-based Dependency Parsing and Disfluency Detection for Automatic Speech Recognition Texts Masashi Yoshikawa and Hiroyuki Shindo and Yuji Matsumoto Graduate School of Information and Science Nara Institute of Science and Technology 8916-5, Takayama, Ikoma, Nara, 630-0192, Japan { masashi.yoshikawa.yh8, shindo, matsu }@is.naist.jp Abstract There are a number of studies that address the problem of detecting disfluencies. Some of these studies include dependency parsing (Honnibal and Johnson, 2014; Wu et al., 2015; Rasooli and Tetreault, 2014), whereas others are dedicated systems (Qian and Liu, 2013; Ferguson et al., 2015; Hough and Purver, 2014; Hough and Schlangen, 2015; Liu et al., 2003). Among these studies, Honnibal (2014) and Wu (2015) address this problem by adding a new action to transition-based dependency parsing that removes the disfluent parts of the input sentence from the stack. Using this approach, they achieved high performance in terms of both dependency parsing and disfluency detection on the Switchboard corpus. However, the authors assume that the input texts to parse are transcribed by human annotators, which, in practice, is unrealistic. In real-world applications, i"
D16-1109,E14-4010,0,0.144007,"Missing"
D16-1109,N03-1033,0,0.0728066,"Missing"
D16-1109,P15-1048,0,0.481042,"Missing"
D16-1109,P11-2033,0,0.0477469,"gned tokens do not match exactly on the character level, the mismatch is regarded as an instance of a substitution type of ASR error. Therefore, we encode this fact in the label of the arc from the token to its head. In Figure 1(a), the words “made” and “slipped” in the ASR hypothesis do not match the gold transcription tokens, “may” and “flip”, respectively. Therefore, we automatically re-label the arc from each token to its head as “error”. 3 Transition-based Dependency Parsing To parse texts that contain disfluencies and ASR errors, we extend the ArcEager shift-reduce dependency parser of (Zhang and Nivre, 2011). Our proposed parser adopts the same Shift, Reduce, LeftArc, and RightArc actions as ArcEager. To this parser we add three new actions, i.e., Edit, LeftArcError, and RightArcError, to handle disfluencies and ASR errors. Edit action removes a disfluent token when it is the first element of the stack. This is different from Honnibal (2014)’s Edit action: theirs accumulates consecutive disfluent tokens on the top of the stack and removes them all at once, whereas our method removes this kind of token one-by-one. Use of this Edit action guarantees that the length of the action sequence is always"
D16-1109,P13-1043,0,0.028161,"handle disfluencies and ASR errors. Edit action removes a disfluent token when it is the first element of the stack. This is different from Honnibal (2014)’s Edit action: theirs accumulates consecutive disfluent tokens on the top of the stack and removes them all at once, whereas our method removes this kind of token one-by-one. Use of this Edit action guarantees that the length of the action sequence is always 2n − 1. This property is advantageous because the parser can use the standard beam search and does not require normalization, such as those adopted in (Honnibal and Johnson, 2014) and (Zhu et al., 2013). LeftArcError and RightArcError act in the same way as LeftArc and RightArc, except that these act only on ASR error tokens, whereas the original LeftArc and RightArc are reserved for non ASR error tokens. Using two different kinds of Arc actions for the two types of tokens (ASR error or not) allows for the weights not to be shared between them, and is expected to yield improved performance. In the experiment below, we train all of the models using structured perceptron with max violation (Huang et al., 2012). The feature set is mainly based on (Honnibal and Johnson, 2014), such as the disflu"
D18-1191,W04-2412,0,0.0923687,"sentations far away from each other (Wen et al., 2016; Luo et al., 2017). 6 Related Work 6.1 Semantic Role Labeling Tasks Automatic SRL has been widely studied (Gildea and Jurafsky, 2002). There have been two main styles of SRL. Figure 4 illustrates an example of span-based and dependency-based SRL. In dependency-based SRL (at the upper part of Figure 4), the correct A2 argument for the predicate “hit” is the word “with”. On one hand, in span-based SRL (at the lower part of Figure 4), the correct A2 argument is the span “with the bat”. For span-based SRL, the CoNLL-2004 and 2005 shared tasks (Carreras and Marquez, 2004; Carreras and M`arquez, 2005) provided the task settings and datasets. In the task settings, various SRL models, from traditional pipeline models to recent neural ones, have been proposed and competed with each other (Pradhan et al., 2005; He et al., 2017; Tan et al., 2018). For dependency-based SRL, the CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009) provided the task settings and datasets. As in span-based SRL, recent neural models achieved high-performance in dependency-based SRL (Marcheggiani et al., 2017; Marcheggiani and Titov, 2017; He et al., 2018b; Cai e"
D18-1191,P18-1192,0,0.353053,"reras and Marquez, 2004; Carreras and M`arquez, 2005) provided the task settings and datasets. In the task settings, various SRL models, from traditional pipeline models to recent neural ones, have been proposed and competed with each other (Pradhan et al., 2005; He et al., 2017; Tan et al., 2018). For dependency-based SRL, the CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009) provided the task settings and datasets. As in span-based SRL, recent neural models achieved high-performance in dependency-based SRL (Marcheggiani et al., 2017; Marcheggiani and Titov, 2017; He et al., 2018b; Cai et al., 2018). This paper focuses on span-based SRL. 6.2 BIO-based SRL Models • FrameNet-style SRL (Baker et al., 1998) • PropBank-style SRL (Palmer et al., 2005) In this paper, we have tackled PropBank-style SRL.11 In PropBank-style SRL, there have been two main task settings. • Span-based SRL: CoNLL-2004 and 2005 shared tasks (Carreras and Marquez, 2004; Carreras and M`arquez, 2005) • Dependency-based SRL: CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009) 11 A1 A2 A1 1.0 A1 Detailed descriptions on FrameNet-style and PropBankstyle SRL can be found in Baker"
D18-1191,W05-0620,0,0.873706,"Missing"
D18-1191,kingsbury-palmer-2002-treebank,0,0.280375,"ocuses on span-based SRL. 6.2 BIO-based SRL Models • FrameNet-style SRL (Baker et al., 1998) • PropBank-style SRL (Palmer et al., 2005) In this paper, we have tackled PropBank-style SRL.11 In PropBank-style SRL, there have been two main task settings. • Span-based SRL: CoNLL-2004 and 2005 shared tasks (Carreras and Marquez, 2004; Carreras and M`arquez, 2005) • Dependency-based SRL: CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009) 11 A1 A2 A1 1.0 A1 Detailed descriptions on FrameNet-style and PropBankstyle SRL can be found in Baker et al. (1998); Das et al. (2014); Kingsbury and Palmer (2002); Palmer et al. (2005). Span-based SRL can be solved as BIO sequential tagging (Hacioglu et al., 2004; Pradhan et al., 2005; M`arquez et al., 2005). Neural models State-of-the-art SRL models use neural networks based on the BIO tagging approach. The pioneering neural SRL model was proposed by Collobert et al. (2011). They use convolutional neural networks (CNNs) and CRFs. Instead of CNNs, Zhou and Xu (2015) and He et al. (2017) used stacked BiLSTMs and achieved strong performance without syntactic inputs. Tan et al. (2018) replaced stacked BiLSTMs with self-attention architectures. Strubell et"
D18-1191,D15-1112,0,0.160051,"Missing"
D18-1191,J02-3001,0,0.363318,"model. On Label Embeddings We analyze the label embeddings in the labeling function (Eq. 8). Figure 3 shows the distribution of the learned label embeddings. The adjunct labels are close to each other, which are likely to be less discriminative. Also, the core label A2 is close to the adjunct label DIR, which are often confused by the model. To enhance the discriminative power, it is promising to apply techniques that keep label representations far away from each other (Wen et al., 2016; Luo et al., 2017). 6 Related Work 6.1 Semantic Role Labeling Tasks Automatic SRL has been widely studied (Gildea and Jurafsky, 2002). There have been two main styles of SRL. Figure 4 illustrates an example of span-based and dependency-based SRL. In dependency-based SRL (at the upper part of Figure 4), the correct A2 argument for the predicate “hit” is the word “with”. On one hand, in span-based SRL (at the lower part of Figure 4), the correct A2 argument is the span “with the bat”. For span-based SRL, the CoNLL-2004 and 2005 shared tasks (Carreras and Marquez, 2004; Carreras and M`arquez, 2005) provided the task settings and datasets. In the task settings, various SRL models, from traditional pipeline models to recent neur"
D18-1191,P18-1249,0,0.0437631,"d to the performance improvement (Collobert et al., 2011; Zhou and Xu, 2015; He et al., 2017). Recently, Peters et al. (2018) integrated contextualized word representation, ELMo, into the model of He et al. (2017) and improved the performance by 3.2 F1 score. Strubell and McCallum (2018) also integrated ELMo into the model of Strubell et al. (2018) and reported the performance improvement. 6.3 Span-based SRL Models 6.4 Span-based Models in Other NLP Tasks In syntactic parsing, Wang and Chang (2016) proposed an LSTM-based sentence segment embedding method named LSTM-Minus. Stern et al. (2017); Kitaev and Klein (2018) incorporated the LSTM Minus into their parsing model and achieved the best results in constituency parsing. In coreference resolution, Lee et al. (2017, 2018) presented an end-to-end coreference resolution model, which considers all spans in a document as potential mentions and learn distributions over possible antecedents for each. Our model can be regarded as an extension of their model. 7 Conclusion and Future Work Another line of approaches to SRL is labeled span modeling (Xue and Palmer, 2004; Koomen et al., 2005; Toutanova et al., 2005). Typical models Typically, in this approach, model"
D18-1191,W05-0625,0,0.0637979,"entence segment embedding method named LSTM-Minus. Stern et al. (2017); Kitaev and Klein (2018) incorporated the LSTM Minus into their parsing model and achieved the best results in constituency parsing. In coreference resolution, Lee et al. (2017, 2018) presented an end-to-end coreference resolution model, which considers all spans in a document as potential mentions and learn distributions over possible antecedents for each. Our model can be regarded as an extension of their model. 7 Conclusion and Future Work Another line of approaches to SRL is labeled span modeling (Xue and Palmer, 2004; Koomen et al., 2005; Toutanova et al., 2005). Typical models Typically, in this approach, models firstly identify candidate argument spans (argument identification) and then classify each span into one of the semantic role labels (argument classification). For inference, several effective methods have been proposed, such as structural constraint inference by using integer linear programming (Punyakanok et al., 2008) or dynamic programming (T¨ackstr¨om et al., 2015; FitzGerald et al., 2015). Recent span-based model A very recent work, He et al. (2018a), proposed a span-based SRL model similar to our model. They a"
D18-1191,D17-1018,0,0.134271,"ectly predicting the spans. Another approach is based on labeled span prediction (T¨ackstr¨om et al., 2015; FitzGerald et al., 2015). This approach scores each span with its label. One advantage of this approach is to allow us to design and use span-level features, that are difficult to use in BIO tagging approaches. However, the performance has lagged behind that of the state-of-the-art BIO-based neural models. To fill this gap, this paper presents a simple and accurate span-based model. Inspired by recent span-based models in syntactic parsing and coreference resolution (Stern et al., 2017; Lee et al., 2017), our model directly scores all possible labeled spans based on span representations induced from neural networks. At decoding time, we greedily select higher scoring labeled spans. The model parameters are learned by optimizing loglikelihood of correct labeled spans. We evaluate the performance of our span-based model on the CoNLL-2005 and 2012 datasets (Carreras and M`arquez, 2005; Pradhan et al., 2012). Experimental results show that the spanbased model outperforms the BiLSTM-CRF model. In addition, by using contextualized word representations, ELMo (Peters et al., 2018), our ensemble model"
D18-1191,N18-2108,0,0.0400001,"Missing"
D18-1191,K17-1041,0,0.172056,"span-based SRL, the CoNLL-2004 and 2005 shared tasks (Carreras and Marquez, 2004; Carreras and M`arquez, 2005) provided the task settings and datasets. In the task settings, various SRL models, from traditional pipeline models to recent neural ones, have been proposed and competed with each other (Pradhan et al., 2005; He et al., 2017; Tan et al., 2018). For dependency-based SRL, the CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009) provided the task settings and datasets. As in span-based SRL, recent neural models achieved high-performance in dependency-based SRL (Marcheggiani et al., 2017; Marcheggiani and Titov, 2017; He et al., 2018b; Cai et al., 2018). This paper focuses on span-based SRL. 6.2 BIO-based SRL Models • FrameNet-style SRL (Baker et al., 1998) • PropBank-style SRL (Palmer et al., 2005) In this paper, we have tackled PropBank-style SRL.11 In PropBank-style SRL, there have been two main task settings. • Span-based SRL: CoNLL-2004 and 2005 shared tasks (Carreras and Marquez, 2004; Carreras and M`arquez, 2005) • Dependency-based SRL: CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009) 11 A1 A2 A1 1.0 A1 Detailed descriptions on FrameNet-sty"
D18-1191,W04-2416,0,0.0441822,"le SRL (Palmer et al., 2005) In this paper, we have tackled PropBank-style SRL.11 In PropBank-style SRL, there have been two main task settings. • Span-based SRL: CoNLL-2004 and 2005 shared tasks (Carreras and Marquez, 2004; Carreras and M`arquez, 2005) • Dependency-based SRL: CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009) 11 A1 A2 A1 1.0 A1 Detailed descriptions on FrameNet-style and PropBankstyle SRL can be found in Baker et al. (1998); Das et al. (2014); Kingsbury and Palmer (2002); Palmer et al. (2005). Span-based SRL can be solved as BIO sequential tagging (Hacioglu et al., 2004; Pradhan et al., 2005; M`arquez et al., 2005). Neural models State-of-the-art SRL models use neural networks based on the BIO tagging approach. The pioneering neural SRL model was proposed by Collobert et al. (2011). They use convolutional neural networks (CNNs) and CRFs. Instead of CNNs, Zhou and Xu (2015) and He et al. (2017) used stacked BiLSTMs and achieved strong performance without syntactic inputs. Tan et al. (2018) replaced stacked BiLSTMs with self-attention architectures. Strubell et al. (2018) improved the self-attention SRL model by incorporating syntactic information. 1637 Word r"
D18-1191,D17-1159,0,0.072181,"004 and 2005 shared tasks (Carreras and Marquez, 2004; Carreras and M`arquez, 2005) provided the task settings and datasets. In the task settings, various SRL models, from traditional pipeline models to recent neural ones, have been proposed and competed with each other (Pradhan et al., 2005; He et al., 2017; Tan et al., 2018). For dependency-based SRL, the CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009) provided the task settings and datasets. As in span-based SRL, recent neural models achieved high-performance in dependency-based SRL (Marcheggiani et al., 2017; Marcheggiani and Titov, 2017; He et al., 2018b; Cai et al., 2018). This paper focuses on span-based SRL. 6.2 BIO-based SRL Models • FrameNet-style SRL (Baker et al., 1998) • PropBank-style SRL (Palmer et al., 2005) In this paper, we have tackled PropBank-style SRL.11 In PropBank-style SRL, there have been two main task settings. • Span-based SRL: CoNLL-2004 and 2005 shared tasks (Carreras and Marquez, 2004; Carreras and M`arquez, 2005) • Dependency-based SRL: CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009) 11 A1 A2 A1 1.0 A1 Detailed descriptions on FrameNet-style and PropBankstyle SRL can b"
D18-1191,W05-0628,0,0.118562,"Missing"
D18-1191,J05-1004,0,0.643444,"odels to recent neural ones, have been proposed and competed with each other (Pradhan et al., 2005; He et al., 2017; Tan et al., 2018). For dependency-based SRL, the CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009) provided the task settings and datasets. As in span-based SRL, recent neural models achieved high-performance in dependency-based SRL (Marcheggiani et al., 2017; Marcheggiani and Titov, 2017; He et al., 2018b; Cai et al., 2018). This paper focuses on span-based SRL. 6.2 BIO-based SRL Models • FrameNet-style SRL (Baker et al., 1998) • PropBank-style SRL (Palmer et al., 2005) In this paper, we have tackled PropBank-style SRL.11 In PropBank-style SRL, there have been two main task settings. • Span-based SRL: CoNLL-2004 and 2005 shared tasks (Carreras and Marquez, 2004; Carreras and M`arquez, 2005) • Dependency-based SRL: CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009) 11 A1 A2 A1 1.0 A1 Detailed descriptions on FrameNet-style and PropBankstyle SRL can be found in Baker et al. (1998); Das et al. (2014); Kingsbury and Palmer (2002); Palmer et al. (2005). Span-based SRL can be solved as BIO sequential tagging (Hacioglu et al., 2004; Pradh"
D18-1191,D14-1162,0,0.0829841,"d in our BiLSTM-span model. 4.3 Model Setup As the base function fbase , we use 4 BiLSTM layers with 300 dimensional hidden units. To optimize the model parameters, we use Adam (Kingma and Ba, 2014). Other hyperparameters are described in Appendix C in detail. As the objective function, we use the crossentropy Lθ in Eq. 3 with L2 weight decay, Lθ = ∑ (X,Y )∈D ℓθ (X, Y ) + λ ||θ||2 , 2 (11) where the hyperparameter λ is the coefficient governing the L2 weight decay. 6 http://ronan.collobert.com/senna/ http://allennlp.org/elmo 8 In our preliminary experiments, we also used the GloVe embeddings (Pennington et al., 2014), but the performance was worse than SENNA. 1634 7 4.4 Results We report averaged scores across five different runs of the model training. Tables 1 and 2 show the experimental results on the CoNLL-2005 and 2012 datasets. Overall, our span-based ensemble model using ELMo achieved the best F1 scores, 87.4 F1 and 87.0 F1 on the CoNLL-2005 and CoNLL-2012 datasets, respectively. In comparison with the CRF-based single model, our span-based single model consistently yielded better F1 scores regardless of the word embeddings, S ENNA and ELM O. Although the performance difference was small between the"
D18-1191,D18-1548,0,0.236057,"Missing"
D18-1191,N18-1202,0,0.368834,"n (Stern et al., 2017; Lee et al., 2017), our model directly scores all possible labeled spans based on span representations induced from neural networks. At decoding time, we greedily select higher scoring labeled spans. The model parameters are learned by optimizing loglikelihood of correct labeled spans. We evaluate the performance of our span-based model on the CoNLL-2005 and 2012 datasets (Carreras and M`arquez, 2005; Pradhan et al., 2012). Experimental results show that the spanbased model outperforms the BiLSTM-CRF model. In addition, by using contextualized word representations, ELMo (Peters et al., 2018), our ensemble model achieves the state-of-the-art results, 87.4 F1 and 87.0 F1 on the CoNLL-2005 and 2012 datasets, respectively. Empirical analysis on these results shows that the label prediction ability of our span-based model is better than that of the CRF-based model. Another finding is that ELMo improves the model performance for span boundary identification. In summary, our main contributions include: • A simple span-based model that achieves the state-of-the-art results. • Quantitative and qualitative analysis on strengths and weaknesses of the span-based model. • Empirical analysis o"
D18-1191,W08-2121,0,0.214884,"Missing"
D18-1191,W05-0634,0,0.0598564,"ates an example of span-based and dependency-based SRL. In dependency-based SRL (at the upper part of Figure 4), the correct A2 argument for the predicate “hit” is the word “with”. On one hand, in span-based SRL (at the lower part of Figure 4), the correct A2 argument is the span “with the bat”. For span-based SRL, the CoNLL-2004 and 2005 shared tasks (Carreras and Marquez, 2004; Carreras and M`arquez, 2005) provided the task settings and datasets. In the task settings, various SRL models, from traditional pipeline models to recent neural ones, have been proposed and competed with each other (Pradhan et al., 2005; He et al., 2017; Tan et al., 2018). For dependency-based SRL, the CoNLL-2008 and 2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009) provided the task settings and datasets. As in span-based SRL, recent neural models achieved high-performance in dependency-based SRL (Marcheggiani et al., 2017; Marcheggiani and Titov, 2017; He et al., 2018b; Cai et al., 2018). This paper focuses on span-based SRL. 6.2 BIO-based SRL Models • FrameNet-style SRL (Baker et al., 1998) • PropBank-style SRL (Palmer et al., 2005) In this paper, we have tackled PropBank-style SRL.11 In PropBank-style SRL, t"
D18-1191,W12-4501,0,0.061457,"ral models. To fill this gap, this paper presents a simple and accurate span-based model. Inspired by recent span-based models in syntactic parsing and coreference resolution (Stern et al., 2017; Lee et al., 2017), our model directly scores all possible labeled spans based on span representations induced from neural networks. At decoding time, we greedily select higher scoring labeled spans. The model parameters are learned by optimizing loglikelihood of correct labeled spans. We evaluate the performance of our span-based model on the CoNLL-2005 and 2012 datasets (Carreras and M`arquez, 2005; Pradhan et al., 2012). Experimental results show that the spanbased model outperforms the BiLSTM-CRF model. In addition, by using contextualized word representations, ELMo (Peters et al., 2018), our ensemble model achieves the state-of-the-art results, 87.4 F1 and 87.0 F1 on the CoNLL-2005 and 2012 datasets, respectively. Empirical analysis on these results shows that the label prediction ability of our span-based model is better than that of the CRF-based model. Another finding is that ELMo improves the model performance for span boundary identification. In summary, our main contributions include: • A simple span"
D18-1191,J08-2005,0,0.288347,"over possible antecedents for each. Our model can be regarded as an extension of their model. 7 Conclusion and Future Work Another line of approaches to SRL is labeled span modeling (Xue and Palmer, 2004; Koomen et al., 2005; Toutanova et al., 2005). Typical models Typically, in this approach, models firstly identify candidate argument spans (argument identification) and then classify each span into one of the semantic role labels (argument classification). For inference, several effective methods have been proposed, such as structural constraint inference by using integer linear programming (Punyakanok et al., 2008) or dynamic programming (T¨ackstr¨om et al., 2015; FitzGerald et al., 2015). Recent span-based model A very recent work, He et al. (2018a), proposed a span-based SRL model similar to our model. They also used BiLSTMs to induce span representations in an endto-end fashion. A main difference is that while they model P(r|i, j), we model P(i, j|r). In other words, while their model seeks to select an appropriate label for each span (label selection), our model seeks to select appropriate spans for each label (span selection). This point distinguishes between their model and ours. FrameNet span-bas"
D18-1191,Q15-1003,0,0.0745551,"Missing"
D18-1191,I17-1027,1,0.839811,"concatenated and used as the feature for a span s = (i, j). The resulting vector hs is a 2dhidden dimensional vector. The middle part of Figure 1 shows an example of this process. For the span (3, 5), the span feature function fspan receives the 3rd and 5th features (h3 and h5 ). Then, these two vectors are added, and the 5th vector is subtracted from the 3rd vector. The resulting vectors are concatenated and given to the labeling function flabel . Our design of the span features is inspired by the span (or segment) features used in syntactic parsing (Wang and Chang, 2016; Stern et al., 2017; Teranishi et al., 2017). While these neural span features cannot be used in BIO-based SRL models, they can easily be incorporated into spanbased models. flabel (hs , r) = W[r] · hs , row vector. As the result of the inner product of W[r] and hs , we obtain the score for a span (i, j) with a label r. The upper part of Figure 1 shows an example of this process. The span representation hs for the span s = (3, 5) is created from addition and subtraction of h3 and h5 . Then, we calculate the inner product of hs and W[r]. The score for the label A0 is 2.1, and the score for the label A1 is 3.7. In the same manner, by calc"
D18-1191,P05-1073,0,0.0993099,"ding method named LSTM-Minus. Stern et al. (2017); Kitaev and Klein (2018) incorporated the LSTM Minus into their parsing model and achieved the best results in constituency parsing. In coreference resolution, Lee et al. (2017, 2018) presented an end-to-end coreference resolution model, which considers all spans in a document as potential mentions and learn distributions over possible antecedents for each. Our model can be regarded as an extension of their model. 7 Conclusion and Future Work Another line of approaches to SRL is labeled span modeling (Xue and Palmer, 2004; Koomen et al., 2005; Toutanova et al., 2005). Typical models Typically, in this approach, models firstly identify candidate argument spans (argument identification) and then classify each span into one of the semantic role labels (argument classification). For inference, several effective methods have been proposed, such as structural constraint inference by using integer linear programming (Punyakanok et al., 2008) or dynamic programming (T¨ackstr¨om et al., 2015; FitzGerald et al., 2015). Recent span-based model A very recent work, He et al. (2018a), proposed a span-based SRL model similar to our model. They also used BiLSTMs to induc"
D18-1191,P16-1218,0,0.2806,"es of the i-th and j-th hidden states are concatenated and used as the feature for a span s = (i, j). The resulting vector hs is a 2dhidden dimensional vector. The middle part of Figure 1 shows an example of this process. For the span (3, 5), the span feature function fspan receives the 3rd and 5th features (h3 and h5 ). Then, these two vectors are added, and the 5th vector is subtracted from the 3rd vector. The resulting vectors are concatenated and given to the labeling function flabel . Our design of the span features is inspired by the span (or segment) features used in syntactic parsing (Wang and Chang, 2016; Stern et al., 2017; Teranishi et al., 2017). While these neural span features cannot be used in BIO-based SRL models, they can easily be incorporated into spanbased models. flabel (hs , r) = W[r] · hs , row vector. As the result of the inner product of W[r] and hs , we obtain the score for a span (i, j) with a label r. The upper part of Figure 1 shows an example of this process. The span representation hs for the span s = (3, 5) is created from addition and subtraction of h3 and h5 . Then, we calculate the inner product of hs and W[r]. The score for the label A0 is 2.1, and the score for the"
D18-1191,W04-3212,0,0.141723,"oposed an LSTM-based sentence segment embedding method named LSTM-Minus. Stern et al. (2017); Kitaev and Klein (2018) incorporated the LSTM Minus into their parsing model and achieved the best results in constituency parsing. In coreference resolution, Lee et al. (2017, 2018) presented an end-to-end coreference resolution model, which considers all spans in a document as potential mentions and learn distributions over possible antecedents for each. Our model can be regarded as an extension of their model. 7 Conclusion and Future Work Another line of approaches to SRL is labeled span modeling (Xue and Palmer, 2004; Koomen et al., 2005; Toutanova et al., 2005). Typical models Typically, in this approach, models firstly identify candidate argument spans (argument identification) and then classify each span into one of the semantic role labels (argument classification). For inference, several effective methods have been proposed, such as structural constraint inference by using integer linear programming (Punyakanok et al., 2008) or dynamic programming (T¨ackstr¨om et al., 2015; FitzGerald et al., 2015). Recent span-based model A very recent work, He et al. (2018a), proposed a span-based SRL model similar"
D18-1191,P15-1109,0,0.531738,"ults, 87.4 F1 and 87.0 F1 on the CoNLL-2005 and 2012 datasets, respectively. 1 Introduction Semantic Role Labeling (SRL) is a shallow semantic parsing task whose goal is to recognize the predicate-argument structure of each predicate. Given a sentence and a target predicate, SRL systems have to predict semantic arguments of the predicate. Each argument is a span, a unit that consists of one or more words. A key to the argument span prediction is how to represent and model spans. One popular approach to it is based on BIO tagging schemes. State-of-the-art neural SRL models adopt this approach (Zhou and Xu, 2015; He et al., 2017; Tan et al., 2018). Using features induced by neural networks, they predict a BIO tag for each word. Words at the beginning and inside of argument spans have the “B” and “I” tags, and words outside argument spans have the tag “O.” While yielding high accuracies, this approach reconstructs argument spans from the predicted BIO tags instead of directly predicting the spans. Another approach is based on labeled span prediction (T¨ackstr¨om et al., 2015; FitzGerald et al., 2015). This approach scores each span with its label. One advantage of this approach is to allow us to desig"
D18-1191,P17-1076,0,0.294602,"tags instead of directly predicting the spans. Another approach is based on labeled span prediction (T¨ackstr¨om et al., 2015; FitzGerald et al., 2015). This approach scores each span with its label. One advantage of this approach is to allow us to design and use span-level features, that are difficult to use in BIO tagging approaches. However, the performance has lagged behind that of the state-of-the-art BIO-based neural models. To fill this gap, this paper presents a simple and accurate span-based model. Inspired by recent span-based models in syntactic parsing and coreference resolution (Stern et al., 2017; Lee et al., 2017), our model directly scores all possible labeled spans based on span representations induced from neural networks. At decoding time, we greedily select higher scoring labeled spans. The model parameters are learned by optimizing loglikelihood of correct labeled spans. We evaluate the performance of our span-based model on the CoNLL-2005 and 2012 datasets (Carreras and M`arquez, 2005; Pradhan et al., 2012). Experimental results show that the spanbased model outperforms the BiLSTM-CRF model. In addition, by using contextualized word representations, ELMo (Peters et al., 2018),"
D18-1191,W18-2904,0,0.0163241,"Tan et al. (2018) replaced stacked BiLSTMs with self-attention architectures. Strubell et al. (2018) improved the self-attention SRL model by incorporating syntactic information. 1637 Word representations Typical word representations, such as SENNA (Collobert et al., 2011) and GloVe (Pennington et al., 2014), have been used and contributed to the performance improvement (Collobert et al., 2011; Zhou and Xu, 2015; He et al., 2017). Recently, Peters et al. (2018) integrated contextualized word representation, ELMo, into the model of He et al. (2017) and improved the performance by 3.2 F1 score. Strubell and McCallum (2018) also integrated ELMo into the model of Strubell et al. (2018) and reported the performance improvement. 6.3 Span-based SRL Models 6.4 Span-based Models in Other NLP Tasks In syntactic parsing, Wang and Chang (2016) proposed an LSTM-based sentence segment embedding method named LSTM-Minus. Stern et al. (2017); Kitaev and Klein (2018) incorporated the LSTM Minus into their parsing model and achieved the best results in constituency parsing. In coreference resolution, Lee et al. (2017, 2018) presented an end-to-end coreference resolution model, which considers all spans in a document as potentia"
D18-1191,P98-1013,0,\N,Missing
D18-1191,C98-1013,0,\N,Missing
D18-1191,P17-1044,0,\N,Missing
E03-1029,2002.tmi-papers.9,1,0.900479,"rce sentence. For example, the Japanese sentence &quot;Kono toraberaazu chekku wo genkin ni shite kudasai&quot; can be translated into English any of the following sentences. • I&apos;d like to cash these traveler&apos;s checks. • Could you change these traveler&apos;s checks into cash? • Please cash these traveler&apos;s checks. These translations are all correct. Actually, the corpus of Takezawa et al. (2002) contains ten different translations of this source sentence. When we construct MT knowledge from corpora that contain such variety, redundant rules are acquired. For instance, a pattern-based MT system described in Imamura (2002) acquires different transfer rules from each multiple translations, although only one rule is necessary for translating a sentence. Redundant rules increase ambiguity or decrease translation speed (Meyers et al., 2000). 156 3 Appropriate Translation for MTs 3.1 Controlled Translation Controlled language (Mitamura et al., 1991; Mitamura and Nyberg, 1995) is proposed for monolingual processing in order to reduce variety. This method allows monolingual texts within a restricted vocabulary and a restricted grammar. Texts written by the controlled language method have fewer semantic and syntactic a"
E03-1029,J00-2004,0,0.0227829,"k up words in the dictionary by target words. Tt denotes the number of target words found in the definition parts of the dictionary. 3. If there is an entry that includes both the source and target word, the word pair is regarded as the word link L denotes the number of word links. 4.1 Literalness Measure A literal translation means that source words are translated one by one to target words. Therefore, a bilingual sentence that has many word correspondences is literal. The word correspondences can be acquired by referring to translation dictionaries or using statistical word aligners (e.g., (Melamed, 2000)). However, not all source words always have an exact corresponding target word. For example, in 158 4. Calculate the literalness with the following equation, which we call the Translation Correspondence Rate (TCR) in this paper. TCR = 2L Ts + Tt (1) The TCR denotes the portion of the directly translated words among the words that should be translated. This definition is bi-directional, Ts,Tt TCR Word Links and Words in the Dictionary Target 1 (English) Source (Japanese: 4111M va Target 2 (English) wo is Cihfferet from what dei muse 0 ordere Figure 2: Example of Measuring Literalness Using Tra"
E03-1029,W01-1406,0,0.0360482,"Missing"
E03-1029,C00-1078,0,0.0154717,"ould you change these traveler&apos;s checks into cash? • Please cash these traveler&apos;s checks. These translations are all correct. Actually, the corpus of Takezawa et al. (2002) contains ten different translations of this source sentence. When we construct MT knowledge from corpora that contain such variety, redundant rules are acquired. For instance, a pattern-based MT system described in Imamura (2002) acquires different transfer rules from each multiple translations, although only one rule is necessary for translating a sentence. Redundant rules increase ambiguity or decrease translation speed (Meyers et al., 2000). 156 3 Appropriate Translation for MTs 3.1 Controlled Translation Controlled language (Mitamura et al., 1991; Mitamura and Nyberg, 1995) is proposed for monolingual processing in order to reduce variety. This method allows monolingual texts within a restricted vocabulary and a restricted grammar. Texts written by the controlled language method have fewer semantic and syntactic ambiguities when they are read by a human or analyzed by a computer. A similar idea can be applied to bilingual corpora. Namely, the expressions in bilingual corpora should be restricted, and &quot;translations that are appr"
E03-1029,1995.tmi-1.12,0,0.0357637,"lly, the corpus of Takezawa et al. (2002) contains ten different translations of this source sentence. When we construct MT knowledge from corpora that contain such variety, redundant rules are acquired. For instance, a pattern-based MT system described in Imamura (2002) acquires different transfer rules from each multiple translations, although only one rule is necessary for translating a sentence. Redundant rules increase ambiguity or decrease translation speed (Meyers et al., 2000). 156 3 Appropriate Translation for MTs 3.1 Controlled Translation Controlled language (Mitamura et al., 1991; Mitamura and Nyberg, 1995) is proposed for monolingual processing in order to reduce variety. This method allows monolingual texts within a restricted vocabulary and a restricted grammar. Texts written by the controlled language method have fewer semantic and syntactic ambiguities when they are read by a human or analyzed by a computer. A similar idea can be applied to bilingual corpora. Namely, the expressions in bilingual corpora should be restricted, and &quot;translations that are appropriate for the MT&quot; should be used in knowledge construction. This approach assumes that context/situation-dependent translations should"
E03-1029,1991.mtsummit-papers.9,0,0.022503,"are all correct. Actually, the corpus of Takezawa et al. (2002) contains ten different translations of this source sentence. When we construct MT knowledge from corpora that contain such variety, redundant rules are acquired. For instance, a pattern-based MT system described in Imamura (2002) acquires different transfer rules from each multiple translations, although only one rule is necessary for translating a sentence. Redundant rules increase ambiguity or decrease translation speed (Meyers et al., 2000). 156 3 Appropriate Translation for MTs 3.1 Controlled Translation Controlled language (Mitamura et al., 1991; Mitamura and Nyberg, 1995) is proposed for monolingual processing in order to reduce variety. This method allows monolingual texts within a restricted vocabulary and a restricted grammar. Texts written by the controlled language method have fewer semantic and syntactic ambiguities when they are read by a human or analyzed by a computer. A similar idea can be applied to bilingual corpora. Namely, the expressions in bilingual corpora should be restricted, and &quot;translations that are appropriate for the MT&quot; should be used in knowledge construction. This approach assumes that context/situation-de"
E03-1029,J93-2003,0,0.00415245,"rce sentence agrees substantially with that of a target sentence. This measure ensures that the cost of word order adjustment is small. • Word Translation Stability: A source word is better translated into the same target word through the corpus. For example, the Japanese adjectival verb `hitstiyoo-da&apos; can be translated into the English adjective &apos;necessary,&apos; the verb &apos;need,&apos; or the verb &apos;require.&apos; It is better for an MT system to always translate this word into &apos;necessary,&apos; if possible. Effective measures of controlled translation depend on MT methods. For example, word-level statistical MT (Brown et al., 1993) translates a source sentence with a combination of word transfer and word order adjustment. Thus, wordorder agreement is an important measure. On the other hand, this is not important for transfer-based MTs because the word order can be significantly changed through syntactic transfer. A transferbased MT method using the phrase structure is studied here. 3.2 Base MT System We use Hierarchical Phrase Alignment-based Translator (HPAT) (Imamura, 2002) as the target transfer-based MT system. HPAT is an new version of Transfer Driven Machine Translator (TDMT) (Furuse and Iida, 1994). Transfer rule"
E03-1029,P02-1040,0,0.0821353,"Missing"
E03-1029,C94-1015,0,0.114985,"l statistical MT (Brown et al., 1993) translates a source sentence with a combination of word transfer and word order adjustment. Thus, wordorder agreement is an important measure. On the other hand, this is not important for transfer-based MTs because the word order can be significantly changed through syntactic transfer. A transferbased MT method using the phrase structure is studied here. 3.2 Base MT System We use Hierarchical Phrase Alignment-based Translator (HPAT) (Imamura, 2002) as the target transfer-based MT system. HPAT is an new version of Transfer Driven Machine Translator (TDMT) (Furuse and Iida, 1994). Transfer rules of HPAT are automatically acquired from a parallel corpus, but those of TDMT were constructed manually. The procedure of HPAT is briefly described as follows (Figure 1). First, phrasal correspondences are hierarchically extracted from a parallel corpus using Hierarchical Phrase Alignment (Imamura, 2001). Next, the hierarchical correspondences are transferred into patterns, and transfer rules are generated. At the time of translation, the input sentence is parsed by using source patterns in the transfer rules. The MT result is generated by mapping the source patterns to the tar"
E03-1029,takezawa-etal-2002-toward,1,0.802206,"r situation. 2.2 Multiple Translations Generally speaking, a single source expression can be translated into multiple target expressions. Therefore, a corpus contains multiple translations even though they are translated from the same source sentence. For example, the Japanese sentence &quot;Kono toraberaazu chekku wo genkin ni shite kudasai&quot; can be translated into English any of the following sentences. • I&apos;d like to cash these traveler&apos;s checks. • Could you change these traveler&apos;s checks into cash? • Please cash these traveler&apos;s checks. These translations are all correct. Actually, the corpus of Takezawa et al. (2002) contains ten different translations of this source sentence. When we construct MT knowledge from corpora that contain such variety, redundant rules are acquired. For instance, a pattern-based MT system described in Imamura (2002) acquires different transfer rules from each multiple translations, although only one rule is necessary for translating a sentence. Redundant rules increase ambiguity or decrease translation speed (Meyers et al., 2000). 156 3 Appropriate Translation for MTs 3.1 Controlled Translation Controlled language (Mitamura et al., 1991; Mitamura and Nyberg, 1995) is proposed fo"
E03-1029,2001.mtsummit-ebmt.4,0,\N,Missing
E14-4030,P13-2107,0,0.075521,"Missing"
E14-4030,J99-2004,0,0.856615,"Missing"
E14-4030,P06-1037,0,0.456152,"ing Dependency Parsers with Supertags Hiroki Ouchi Kevin Duh Yuji Matsumoto Computational Linguistics Laboratory Nara Institute of Science and Technology {ouchi.hiroki.nt6, kevinduh, matsu}@is.naist.jp Abstract mation that imposes complex constraints in a local context (Bangalore and Joshi, 1999). While supertags have been used in frameworks based on lexicalized grammars, e.g. Lexicalized TreeAdjoining Grammar (LTAG), Head-driven Phrase Structure Grammar (HPSG) and Combinatory Categorial Grammar (CCG), they have scarcely been utilized for dependency parsing so far. Previous work by Foth et al (2006) demonstrate that supertags improve German dependency parsing under a Weighted Constraint Dependency Grammar (WCDG). Recent work by Ambati et al (2013) show that supertags based on CCG lexicon improves transition-based dependency parsing for Hindi. In particular, they argue that supertags can improve long distance dependencies (e.g. coordination, relative clause) in a morphologicallyrich free-word-order language. Zhang et. al. (2010) define supertags that incorporate that longdistance dependency information for the purpose of HPSG parsing. All these works suggest the promising synergy between"
E14-4030,N10-1115,0,0.0438009,"left or right dependents. For instance, the word ’Monday’ has a left dependent ’Black’, so we encode it as ’PRD/L+L’, where the part before ’+’ specifies the head information (’PRD/L’) and the part afterwards (’L’) specifies the position of the dependent (’L’ for left, ’R’ for right). When a word has its dependents in both left and right directions, such as the word ’was’ in Figure 1, we combine them using ’ ’, as in: ’ROOT+L R’. On our Penn Treebank data, Model 1 has 79 supertags. 3 Supertags as Features in a Transition-based Dependency Parser In this work, we adopt the Easy-First parser of (Goldberg and Elhadad, 2010), a highly-accurate transition-based dependency parser. We describe how we incorporate supertag features in the EasyFirst framework, though it can be done similarly 155 Model Model1 Model2 for other transition-based frameworks like left-toright arc-eager and arc-standard models (Nivre et al., 2006; Yamada and Matsumoto, 2003). In the Easy-First algorithm, a dependency tree is constructed by two kinds of actions: ATTACH L EFT(i) and ATTACH R IGHT(i) to a list of partial tree structures p1 ,...,pk initialized with the n words of the sentence w1 ,...,wn . ATTACH L EFT(i) attaches (pi , p+1 ) and"
E14-4030,P10-1110,0,0.0230229,"baseline features based on POS/word in Goldberg and Elhadad (2010). # tags 79 312 Dev 87.81 87.22 Test 88.12 87.13 Table 3: Supertag accuracy evaluated on development and test set. Dev = development set, PTB 22; Test = test set, PTB 23 4 Experiments To evaluate the effectiveness of supertags as features, we perform experiments on the Penn Treebank (PTB), converted into dependency format with Penn2Malt1 . Adopting standard approach, we split PTB sections 2-21 for training, section 22 for development and 23 for testing. We assigned POS-tags to the training data by ten-fold jackknifing following Huang and Sagae (2010). Development and test sets are automatically tagged by the tagger trained on the training set. For a partial tree structure p, features are defined based on information in its head: we use wp to refer to the surface word form of the head word of p, tp to refer to the head word’s POS tag, and sp to refer to the head word’s supertag. Further, we not only use a supertag as is, but split each supertag into subparts. For instance, the supertag ’ROOT+SUB/L PRD/R’ is split into ’ROOT’, ’SUB/L’ and ’PRD/R’, a supertag representing the supertag head information shp , supertag left dependent informatio"
E14-4030,J93-2004,0,0.0488395,"Missing"
E14-4030,W03-3023,1,0.690838,"ft and right directions, such as the word ’was’ in Figure 1, we combine them using ’ ’, as in: ’ROOT+L R’. On our Penn Treebank data, Model 1 has 79 supertags. 3 Supertags as Features in a Transition-based Dependency Parser In this work, we adopt the Easy-First parser of (Goldberg and Elhadad, 2010), a highly-accurate transition-based dependency parser. We describe how we incorporate supertag features in the EasyFirst framework, though it can be done similarly 155 Model Model1 Model2 for other transition-based frameworks like left-toright arc-eager and arc-standard models (Nivre et al., 2006; Yamada and Matsumoto, 2003). In the Easy-First algorithm, a dependency tree is constructed by two kinds of actions: ATTACH L EFT(i) and ATTACH R IGHT(i) to a list of partial tree structures p1 ,...,pk initialized with the n words of the sentence w1 ,...,wn . ATTACH L EFT(i) attaches (pi , p+1 ) and removes pi+1 from the partial tree list. ATTACH R IGHT(i) attaches (pi+1 , pi ) and removes pi from the partial tree list. Features are extracted from the attachment point as well as two neighboring structures: pi−2 , pi−1 , pi , pi+1 , pi+2 , pi+3 . Table 2 summarizes the supertag features we extract from this neighborhood;"
E14-4030,P11-2033,0,0.0591563,"Missing"
E14-4030,N10-1090,0,0.0416732,"Missing"
E14-4030,W06-2933,0,\N,Missing
E14-4037,J93-2003,0,0.09062,"on It is generally acknowledged that absolute equivalence between two languages is impossible, since concept lexicalization varies across languages. Major translation theories thus argue that texts should be translated ‘sense-for-sense’ instead of ‘word-for-word’ (Nida, 1964). This suggests that unalignable words may be an issue for the parallel text used to train current statistical machine translation (SMT) systems. Although existing automatic word alignment methods have some mechanism to handle the lack of exact word-for-word alignment (e.g. null probabilities, fertility in the IBM models (Brown et al., 1993)), they may be too coarse-grained to model the ’sense-for-sense’ translations created by professional human translators. For example, the Chinese term ‘tai-yang’ literally means ‘sun’, yet the concept it represents is equivalent to the English term ‘the sun’. Since the concept of a definite article is not incorporated in the morphology of ‘tai yang’, the added ‘the’ is not aligned to any Chinese word. Yet in another context like ’the man’, ‘the’ can be the translation 2 Analysis of Unalignable Words Our manually-aligned data, which we call ORACLE data, is a Chinese-to-English corpus released b"
E14-4037,D10-1062,0,0.0159191,"tatistical significant improvements in BLEU and METEOR. 6 Comparing to the results of PBMT, this suggests our method may be most effective in improving systems where rule extraction is sen5 Conclusion We analyzed in-depth the phenomenon of unalignable words in parallel text, and show that what is unalignable depends on the word’s concept and context. We argue that this is not a trivial problem, but with an unalignable word classifier and a simple modified MT training pipeline, we can achieve small but significant gains in end-to-end translation. In related work, the issue of dropped pronouns (Chung and Gildea, 2010) and function words (Setiawan et al., 2010; Nakazawa and Kurohashi, 2012) have been found important in word alignment, and (Fossum et al., 2008) showed that syntax features are helpful for fixing alignments. An interesting avenue of future work is to integrate these ideas with ours, in particular by exploiting syntax and viewing unalignable words as aligned at a structure above the lexical level. 5 We use the standard MT08 test sets; the training data includes LDC2004T08, 2005E47, 2005T06, 2007T23, 2008T06, 2008T08, 2008T18, 2009T02, 2009T06, 2009T15, and 2010T03 (34M English words and 1.1M se"
E14-4037,W08-0306,0,0.0229129,"ving systems where rule extraction is sen5 Conclusion We analyzed in-depth the phenomenon of unalignable words in parallel text, and show that what is unalignable depends on the word’s concept and context. We argue that this is not a trivial problem, but with an unalignable word classifier and a simple modified MT training pipeline, we can achieve small but significant gains in end-to-end translation. In related work, the issue of dropped pronouns (Chung and Gildea, 2010) and function words (Setiawan et al., 2010; Nakazawa and Kurohashi, 2012) have been found important in word alignment, and (Fossum et al., 2008) showed that syntax features are helpful for fixing alignments. An interesting avenue of future work is to integrate these ideas with ours, in particular by exploiting syntax and viewing unalignable words as aligned at a structure above the lexical level. 5 We use the standard MT08 test sets; the training data includes LDC2004T08, 2005E47, 2005T06, 2007T23, 2008T06, 2008T08, 2008T18, 2009T02, 2009T06, 2009T15, and 2010T03 (34M English words and 1.1M sentences). Since we do not have access to all OpenMT data, e.g. FBIS, our results may not be directly comparable to other systems in the evaluati"
E14-4037,P07-2045,0,0.00470613,"nd F1 of the resulting alignments, and quality of the final MT outputs. Baseline is the standard MT training pipeline without removal of unaligned words. Our Proposed approach performs better in alignment, phrasebased (PBMT) and hierarchical (Hiero) systems. The results, evaluated by BLEU, METEOR and TER, support our hypothesis that removing gold unalignable words helps improve word alignment and the resulting SMT. 3 We can suppress the NULL probabilities of the model. All experiments are done using standard settings for Moses PBMT and Hiero with 4-gram LM and mslrbidirectional-fe reordering (Koehn et al., 2007). The classifier is trained using LIBSVM (Chang and Lin, 2011). 4 2 We define the list as the top 100 word types with the highest count of unalignable words per language according to the hand annotated data. 192 Align acc. ORACLE P .711 Baseline R .488 F1.579 ORACLE P .802 Proposed R .509 (gold) F1.623 REAL Baseline REAL Proposed (predict) B T M B T M B T M B T M PBMT C-E E-C 11.4 17.4 70.9 69.0 21.8 23.9 11.8+ 18.3+ 71.4− 65.7+ 22.1+ 24.1+ 18.2 18.5 63.4 67.2 22.9 24.6 18.6 18.5 63.8− 66.5+ 23.2+ 24.5 Hiero C-E E-C 10.3 15.8 75.9 72.3 21.08 23.7 11.0+ 17.2+ 74.7+ 68.7+ 22.0+ 24.0+ 17.0 17.2 6"
E14-4037,li-etal-2010-enriching,0,0.0150687,"be too coarse-grained to model the ’sense-for-sense’ translations created by professional human translators. For example, the Chinese term ‘tai-yang’ literally means ‘sun’, yet the concept it represents is equivalent to the English term ‘the sun’. Since the concept of a definite article is not incorporated in the morphology of ‘tai yang’, the added ‘the’ is not aligned to any Chinese word. Yet in another context like ’the man’, ‘the’ can be the translation 2 Analysis of Unalignable Words Our manually-aligned data, which we call ORACLE data, is a Chinese-to-English corpus released by the LDC (Li et al., 2010)1 . It consists of ∼13000 Chinese sentences from news and blog domains and their English translation . English words are manually aligned with the Chinese characters. Characters without an exact counterpart are annotated with categories that state the functions of the words. These characters are either aligned to ‘NULL’, or attached to their dependency heads, if any, and aligned together to form a multi-word alignment. For example, ‘the’ is annotated as [DET], for ‘determiner’, and aligned to ‘tai-yang’ together with ‘sun’. In this work, any English word or Chinese character without an exact c"
E14-4037,C12-1120,0,0.0184062,"to the results of PBMT, this suggests our method may be most effective in improving systems where rule extraction is sen5 Conclusion We analyzed in-depth the phenomenon of unalignable words in parallel text, and show that what is unalignable depends on the word’s concept and context. We argue that this is not a trivial problem, but with an unalignable word classifier and a simple modified MT training pipeline, we can achieve small but significant gains in end-to-end translation. In related work, the issue of dropped pronouns (Chung and Gildea, 2010) and function words (Setiawan et al., 2010; Nakazawa and Kurohashi, 2012) have been found important in word alignment, and (Fossum et al., 2008) showed that syntax features are helpful for fixing alignments. An interesting avenue of future work is to integrate these ideas with ours, in particular by exploiting syntax and viewing unalignable words as aligned at a structure above the lexical level. 5 We use the standard MT08 test sets; the training data includes LDC2004T08, 2005E47, 2005T06, 2007T23, 2008T06, 2008T08, 2008T18, 2009T02, 2009T06, 2009T15, and 2010T03 (34M English words and 1.1M sentences). Since we do not have access to all OpenMT data, e.g. FBIS, our"
E14-4037,D10-1052,0,0.0158377,"and METEOR. 6 Comparing to the results of PBMT, this suggests our method may be most effective in improving systems where rule extraction is sen5 Conclusion We analyzed in-depth the phenomenon of unalignable words in parallel text, and show that what is unalignable depends on the word’s concept and context. We argue that this is not a trivial problem, but with an unalignable word classifier and a simple modified MT training pipeline, we can achieve small but significant gains in end-to-end translation. In related work, the issue of dropped pronouns (Chung and Gildea, 2010) and function words (Setiawan et al., 2010; Nakazawa and Kurohashi, 2012) have been found important in word alignment, and (Fossum et al., 2008) showed that syntax features are helpful for fixing alignments. An interesting avenue of future work is to integrate these ideas with ours, in particular by exploiting syntax and viewing unalignable words as aligned at a structure above the lexical level. 5 We use the standard MT08 test sets; the training data includes LDC2004T08, 2005E47, 2005T06, 2007T23, 2008T06, 2008T08, 2008T18, 2009T02, 2009T06, 2009T15, and 2010T03 (34M English words and 1.1M sentences). Since we do not have access to a"
E14-4037,N03-1033,0,0.0234339,"Missing"
E17-2001,P14-1130,0,0.10112,"Missing"
E17-2001,H05-1066,0,0.202967,"Missing"
E17-2001,P16-1231,0,0.0244463,"Missing"
E17-2001,P13-2017,0,0.0755537,"Missing"
E17-2001,W08-1301,0,0.141413,"Missing"
E17-2001,de-marneffe-etal-2014-universal,0,0.0612274,"Missing"
E17-2001,P06-1033,0,0.414782,"words, e.g., the tree in the lower part of Figure 1 (Schwartz et al., 2012; Ivanova et al., 2013). To overcome this issue, in this paper, we show the effectiveness of a back-and-forth conversion approach where we train a model and parse sentences in an anontation format with higher parsability, and then reconvert the parser output into the UD scheme. Figure 1 shows an example of our conversion. We use the function head trees (below) as an intermediate representation. This is not the first attempt to improve dependency parsing accuracy with tree conversions. The positive result is reported in Nilsson et al. (2006) using the Prague Dependency Treebank. For the conversion of content and function head in UD, however, the effect is still inconclusive. Using English UD data, Silveira and Manning (2015) report the negative result, which they argue is due to error propagation at backward conversions, in particular in copula constructions that often incur drastic changes of the structure. Rosa (2015) report the advantage of funcion head in the adposition construction, but the data is HamleDT (Zeman et al., 2012) rather than UD and the conversion target is conversely too restrictive. Our main contribution is to"
E17-2001,P15-1033,0,0.0394915,"Missing"
E17-2001,W15-2131,0,0.584881,"n. We use the function head trees (below) as an intermediate representation. This is not the first attempt to improve dependency parsing accuracy with tree conversions. The positive result is reported in Nilsson et al. (2006) using the Prague Dependency Treebank. For the conversion of content and function head in UD, however, the effect is still inconclusive. Using English UD data, Silveira and Manning (2015) report the negative result, which they argue is due to error propagation at backward conversions, in particular in copula constructions that often incur drastic changes of the structure. Rosa (2015) report the advantage of funcion head in the adposition construction, but the data is HamleDT (Zeman et al., 2012) rather than UD and the conversion target is conversely too restrictive. Our main contribution is to show that the backand-forth conversion can bring consistent accuracy improvements across languages in UD, by As shown in Figure 1 there are several variations in annotations of dependencies. A famous example is a head choice in a prepositional phrase (e.g, to a bar), which diverges in the two trees. Though various annotation schemes have been proposed so far (Hajic et al., 2001; Joh"
E17-2001,C12-1147,0,0.559929,"Missing"
E17-2001,W15-2134,0,0.659865,"rth conversion approach where we train a model and parse sentences in an anontation format with higher parsability, and then reconvert the parser output into the UD scheme. Figure 1 shows an example of our conversion. We use the function head trees (below) as an intermediate representation. This is not the first attempt to improve dependency parsing accuracy with tree conversions. The positive result is reported in Nilsson et al. (2006) using the Prague Dependency Treebank. For the conversion of content and function head in UD, however, the effect is still inconclusive. Using English UD data, Silveira and Manning (2015) report the negative result, which they argue is due to error propagation at backward conversions, in particular in copula constructions that often incur drastic changes of the structure. Rosa (2015) report the advantage of funcion head in the adposition construction, but the data is HamleDT (Zeman et al., 2012) rather than UD and the conversion target is conversely too restrictive. Our main contribution is to show that the backand-forth conversion can bring consistent accuracy improvements across languages in UD, by As shown in Figure 1 there are several variations in annotations of dependenc"
E17-2001,P13-3005,0,0.136931,"parsability, and reconvert the parser outputs to follow the UD scheme as a postprocess. We show that this technique consistently improves LAS across languages even with a state-of-the-art parser, in particular on core dependency arcs such as nominal modifier. We also provide an in-depth analysis to understand why our method increases parsability. 1 1 nsubj mark det ccomp case nmod Figure 1: Dependency trees with content head (above) and function head (below). difficult than the conventional style centering on function words, e.g., the tree in the lower part of Figure 1 (Schwartz et al., 2012; Ivanova et al., 2013). To overcome this issue, in this paper, we show the effectiveness of a back-and-forth conversion approach where we train a model and parse sentences in an anontation format with higher parsability, and then reconvert the parser output into the UD scheme. Figure 1 shows an example of our conversion. We use the function head trees (below) as an intermediate representation. This is not the first attempt to improve dependency parsing accuracy with tree conversions. The positive result is reported in Nilsson et al. (2006) using the Prague Dependency Treebank. For the conversion of content and func"
I05-1050,J90-1003,0,0.293839,"h these two sub-tasks simultaneously, using supervised machine learning. The third problem we tackle (§3.3) is estimation of the part of speech of MWEs. This problem is also solved using supervised machine learning. In this research we limit ourselves to MWEs containing only two words (i.e. bigrams). In the future, we plan to generalize the method so that it works with MWEs of arbitrary length. 2 Related Work Collocation extraction has been covered extensively in the literature. One of the earliest attempts to automatically extract collocations from a corpus was undertaken by Church and Hanks [1]. The statistical measure they used to identify collocations was based on mutual information. Smadja [2] developed a tool called Xtract for the extraction of collocations. His deﬁnition of a collocation diﬀered slightly from that of Church and Hanks because he claimed expressions such as doctors and nurses are not real collocations, just words related by virtue of their shared domain or semantics. Thanopoulos, Fakotakis and Kokkinakis in [3] reviewed the statistical measures most frequently used for collocation extraction, and evaluated them by comparing their performance with that of two new"
I05-1050,J93-1007,0,0.0755391,".3) is estimation of the part of speech of MWEs. This problem is also solved using supervised machine learning. In this research we limit ourselves to MWEs containing only two words (i.e. bigrams). In the future, we plan to generalize the method so that it works with MWEs of arbitrary length. 2 Related Work Collocation extraction has been covered extensively in the literature. One of the earliest attempts to automatically extract collocations from a corpus was undertaken by Church and Hanks [1]. The statistical measure they used to identify collocations was based on mutual information. Smadja [2] developed a tool called Xtract for the extraction of collocations. His deﬁnition of a collocation diﬀered slightly from that of Church and Hanks because he claimed expressions such as doctors and nurses are not real collocations, just words related by virtue of their shared domain or semantics. Thanopoulos, Fakotakis and Kokkinakis in [3] reviewed the statistical measures most frequently used for collocation extraction, and evaluated them by comparing their performance with that of two new measures of their own. Their ﬁrst novel measure, Mutual Dependency (MD) is pointwise mutual information"
I05-1050,thanopoulos-etal-2002-comparative,0,0.0603543,"covered extensively in the literature. One of the earliest attempts to automatically extract collocations from a corpus was undertaken by Church and Hanks [1]. The statistical measure they used to identify collocations was based on mutual information. Smadja [2] developed a tool called Xtract for the extraction of collocations. His deﬁnition of a collocation diﬀered slightly from that of Church and Hanks because he claimed expressions such as doctors and nurses are not real collocations, just words related by virtue of their shared domain or semantics. Thanopoulos, Fakotakis and Kokkinakis in [3] reviewed the statistical measures most frequently used for collocation extraction, and evaluated them by comparing their performance with that of two new measures of their own. Their ﬁrst novel measure, Mutual Dependency (MD) is pointwise mutual information minus self-information. The second measure attempts to introduce a slight frequency bias by combining the t-score with mutual dependency. Although frequency alone is not suﬃcient evidence of collocational status, they argue that candidate collocations that have a high frequency are more likely to be valid than those that are very rare. Whi"
I05-1050,W02-2001,0,0.016545,"eceived attention over a number of years, MWEs have only relatively recently emerged as a research topic within natural language processing. In consequence, there are relatively few articles speciﬁcally about MWEs. Sag et al. in [4] gave a linguistic categorisation of the diﬀerent types of MWE, and described ways of representing them eﬃciently within a computational framework2. Although MWEs as a whole have yet to receive widespread investigation, attention has been paid to speciﬁc types of MWE. For example, verb-particle constructions have been the subject of several studies (see for example [5] and [6]). 2 Head-driven Phrase Structure Grammar (HPSG). Automatic Extraction of Fixed Multiword Expressions 3 569 Method In order to extract information about ﬁxed MWEs from a corpus, we use a three stage process. In the ﬁrst stage we identify a list of candidate MWEs based on the statistical behaviour of the tokens in the corpus. Two words whose probability of appearing together is greater than that which would be expected based on their individual frequencies, are considered to constitute a potential MWE and are extracted for later processing. In other words, stage one is collocation extra"
I05-1050,W03-1808,0,0.0167121,"attention over a number of years, MWEs have only relatively recently emerged as a research topic within natural language processing. In consequence, there are relatively few articles speciﬁcally about MWEs. Sag et al. in [4] gave a linguistic categorisation of the diﬀerent types of MWE, and described ways of representing them eﬃciently within a computational framework2. Although MWEs as a whole have yet to receive widespread investigation, attention has been paid to speciﬁc types of MWE. For example, verb-particle constructions have been the subject of several studies (see for example [5] and [6]). 2 Head-driven Phrase Structure Grammar (HPSG). Automatic Extraction of Fixed Multiword Expressions 3 569 Method In order to extract information about ﬁxed MWEs from a corpus, we use a three stage process. In the ﬁrst stage we identify a list of candidate MWEs based on the statistical behaviour of the tokens in the corpus. Two words whose probability of appearing together is greater than that which would be expected based on their individual frequencies, are considered to constitute a potential MWE and are extracted for later processing. In other words, stage one is collocation extraction. I"
I05-1050,J93-1003,0,0.045184,"ting in a classiﬁer capable of distinguishing between, on the one hand, true MWEs, and on the other, non-MWEs and pseudo-MWEs. In the ﬁnal stage we use supervised machine learning to train a classiﬁer to perform MWE part of speech assignment. By examining the context surrounding an MWE, it is possible for the classiﬁer to determine the most likely part of speech for the MWE in that context. 3.1 Collocation Extraction Collocation extraction was performed using one of the statistical measures discussed in [3]. The measures we experimented with were: frequency, χ-square [7], log-likelihood ratio [8], t-score [7], mutual information (MI) [1], mutual dependency (MD – mutual information minus self-information) [3], and log-frequency biased mutual dependency (LFMD – a combination of the t-score and mutual dependency) [3]. The equations of these measures are shown in Fig. 2. We compared the resulting ranked lists of bigrams with a list of target MWEs extracted from the British National Corpus (BNC)3 . The target list was produced by starting with a list of all MWEs tagged as such in the BNC, and removing MWEs with a frequency of less than ﬁve, and MWEs with a part of speech of noun, or adject"
I05-1050,W03-3023,1,0.717499,"sed on bigrams. We hope to generalise our approach, so that MWEs of length greater than two can be extracted and assigned a part of speech. We plan to evaluate the performance of the BNC models described above on another corpus to determine their ﬂexibility. Speciﬁcally, we plan to use a corpus of North American English such as the Penn Treebank, in the hope of demonstrating the models’ ability to handle American as well as British English. We also plan to check the eﬀect on parsing accuracy of using the extracted multiword expressions in the input to a parser such as Collins’ [9] or Yamada’s [10]. 6 Conclusion In this research we aimed to identify a method for the automatic extraction and part of speech estimation of ﬁxed MWEs. Knowledge about ﬁxed MWEs has Automatic Extraction of Fixed Multiword Expressions 575 the potential to improve the accuracy of numerous natural language processing applications. Generating such a list therefore represents an important natural language processing task. Our method uses a collocation measure to produce a list of candidate bigrams. These candidates are then used to select training data for a classiﬁer. The trained classiﬁer was successfully able to"
I05-1059,C94-1048,0,0.120434,"Missing"
I05-1059,2001.mtsummit-papers.10,0,0.178013,"Missing"
I05-1059,Y04-1018,0,0.0617146,"Missing"
I05-1079,P98-1013,0,0.0126582,"phrasing has recently been attracting increasing attention due to its potential in a broad range of natural language processing tasks. For example, a system that is capable of simplifying a given text, or showing the user several alternative expressions conveying the same content, would be useful for assisting a reader. There are several classes of paraphrase that exhibit a degree of regularity. For example, paraphrasing associated with verb alternation, lexical derivation, compound word decomposition, and paraphrasing of light-verb constructions (LVC(s)) all fall into such classes. Examples1 (1) and (2) appear to exhibit the same transformation pattern, in which a compound noun is transformed into a verb phrase. Likewise, paraphrases involving an LVC as in (3) and (4) (from [4]) have considerable similarities. (1) s. t. (2) s. t. (3) s. t. (4) s. t. 1 His machine operation is very good. He operates the machine very well. My son’s bat control is unskillful yet. My son controls his bat poorly yet. Steven made an attempt to stop playing. Steven attempted to stop playing. It had a noticeable effect on the trade. It noticeably affected the trade. For each example, “s” and “t” denote an or"
I05-1079,W04-2412,0,0.0472235,"has recently been attracting increasing attention due to its potential in a broad range of natural language processing tasks. For example, a system that is capable of simplifying a given text, or showing the user several alternative expressions conveying the same content, would be useful for assisting a reader. There are several classes of paraphrase that exhibit a degree of regularity. For example, paraphrasing associated with verb alternation, lexical derivation, compound word decomposition, and paraphrasing of light-verb constructions (LVC(s)) all fall into such classes. Examples1 (1) and (2) appear to exhibit the same transformation pattern, in which a compound noun is transformed into a verb phrase. Likewise, paraphrases involving an LVC as in (3) and (4) (from [4]) have considerable similarities. (1) s. t. (2) s. t. (3) s. t. (4) s. t. 1 His machine operation is very good. He operates the machine very well. My son’s bat control is unskillful yet. My son controls his bat poorly yet. Steven made an attempt to stop playing. Steven attempted to stop playing. It had a noticeable effect on the trade. It noticeably affected the trade. For each example, “s” and “t” denote an original s"
I05-1079,J02-3001,0,0.122308,"Missing"
I05-1079,J94-3013,0,0.0841281,"19] have shown that the theory of the LCS provides a systematic explanation of semantic decomposition as well as syntax determines. In particular, Kageyama [9] has shown that even a simple typology of LCS can explain a wide variety of linguistic phenomena including word association within compounds, transitivity alternation, and lexical derivation. Second, large-scale LCS dictionaries have been developed through practical use on machine translation and compound noun analysis [3,19]. The LCS dictionary for English [3] (4,163-verbs with 468 LCS types) was tailored based on a verb classification [12] with an expansion for the semantic role delivered to arguments. For Japanese, Takeuchi et al. [19] developed a 1,210-verbs LCS dictionary (with 12 LCS types) called the T-LCS dictionary, following Kageyama’s analysis [9]. In this paper, we make use of the current version of the T-LCS dictionary, because it provides a set of concrete rules for LCS assignment, which ensures the reliability of the dictionary. Examples of LCS in the T-LCS dictionary are shown in Table 1. An LCS consists of a combination of semantic predicates (“CONTROL,” “BE AT,” etc.) and their argument slots (x, y, and z). Each"
I05-1079,J87-3006,0,0.040094,"Missing"
I05-1079,J05-1004,0,0.0407037,"Missing"
I05-1079,W02-1409,0,0.0544453,"ularities underlying paraphrases can be explained by means of lexical semantics and how, and (ii) how lexical semantics theories can be enhanced with feedback from practical use, namely, paraphrase generation. We make an attempt to exploit the LCS among several lexical semantics frameworks, and propose a paraphrase generation model which utilizes LCS combining with syntactic transformation. 2 Lexical Conceptual Structure 2.1 Basic Framework Among several frameworks of lexical semantics, we focus on the Lexical Conceptual Structure (LCS) [8] due to the following reasons. First, several studies [9,3,19] have shown that the theory of the LCS provides a systematic explanation of semantic decomposition as well as syntax determines. In particular, Kageyama [9] has shown that even a simple typology of LCS can explain a wide variety of linguistic phenomena including word association within compounds, transitivity alternation, and lexical derivation. Second, large-scale LCS dictionaries have been developed through practical use on machine translation and compound noun analysis [3,19]. The LCS dictionary for English [3] (4,163-verbs with 468 LCS types) was tailored based on a verb classification [12"
I05-1079,C98-1013,0,\N,Missing
I05-1079,W05-0620,0,\N,Missing
I05-2030,J95-2003,0,0.00340212,"004), is cost effective, while achieving a better performance than the best-performing rule-based systems for the test sets of MUC-6 and MUC-7 2 . As suggested by Figure 1, anaphora resolution can be decomposed into two subtasks: anaphoricity determination and antecedent identification. Anaphoricity determination is the task of judging whether a given NP is anaphoric or nonanaphoric. Recent research advances have provided several important findings as follows: • Learning-based methods for antecedent identification can also benefit from the use of linguistic clues inspired by Centering Theory (Grosz et al., 1995). • One useful clue for anaphoricity determination is the availability of a plausible candidate for the antecedent. If an appropriate candidate for the antecedent is found in the preceding discourse context, the NP is likely to be anaphoric. For these reasons, an anaphora resolution model performs best if it carries out the following pro2 The 7th Message Understanding Conference (1998): www.itl.nist.gov/iaui/894.02/related projects/muc/ 175 target value Select the best candidate attribute candidates Taro-wa shisetsu-wo（φ-ga）shirabe-te Onaka-ga hetta-node anaphor hungry Tarō-NOM attendance-ACC"
I05-2030,W03-2604,1,0.908937,"indefinite. While the figure shows Japanese examples, the similarity between anaphora resolution and opinion extraction is language independent. This analogy naturally leads us to think of applying existing techniques for anaphora resolution to our opinion extraction task since anaphora resolution has been studied for a considerably longer period in a wider range of disciplines as we briefly review below. 3.2 Existing techniques for anaphora resolution Corpus-based empirical approaches to anaphora resolution have been reasonably successful. This approach, as exemplified by (Soon et al., 2001; Iida et al., 2003; Ng, 2004), is cost effective, while achieving a better performance than the best-performing rule-based systems for the test sets of MUC-6 and MUC-7 2 . As suggested by Figure 1, anaphora resolution can be decomposed into two subtasks: anaphoricity determination and antecedent identification. Anaphoricity determination is the task of judging whether a given NP is anaphoric or nonanaphoric. Recent research advances have provided several important findings as follows: • Learning-based methods for antecedent identification can also benefit from the use of linguistic clues inspired by Centering T"
I05-2030,C04-1071,0,0.0255642,"nd recommendations. Previous approaches to the task of mining a large-scale document collection for opinions can be classified into two groups: the document classification approach and the information extraction approach. In the document classification approach, researchers have been exploring techniques for classifying documents according to semantic/sentiment orientation such as positive vs. negative (e.g. (Dave et al., 2003; Pang and Lee, 2004; Turney, 2002)). The information extraction approach, on the other hand, focuses on the task of extracting elements which constitute opinions (e.g. (Kanayama and Nasukawa, 2004; Hu and Liu, 2004; Tateishi et al., 2001)). The aim of this paper is to extract opinions that represent an evaluation of a products together with the evidence. To achieve this, we consider our task from the information extraction view173 point. We term the above task opinion extraction in this paper. While they can be linguistically realized in many ways, opinions on a product are in fact often expressed in the form of an attribute-value pair. An attribute represents one aspect of a subject and the value is a specific language expression that qualifies or quantifies the aspect. Given this obs"
I05-2030,P04-1020,0,0.0110102,"he figure shows Japanese examples, the similarity between anaphora resolution and opinion extraction is language independent. This analogy naturally leads us to think of applying existing techniques for anaphora resolution to our opinion extraction task since anaphora resolution has been studied for a considerably longer period in a wider range of disciplines as we briefly review below. 3.2 Existing techniques for anaphora resolution Corpus-based empirical approaches to anaphora resolution have been reasonably successful. This approach, as exemplified by (Soon et al., 2001; Iida et al., 2003; Ng, 2004), is cost effective, while achieving a better performance than the best-performing rule-based systems for the test sets of MUC-6 and MUC-7 2 . As suggested by Figure 1, anaphora resolution can be decomposed into two subtasks: anaphoricity determination and antecedent identification. Anaphoricity determination is the task of judging whether a given NP is anaphoric or nonanaphoric. Recent research advances have provided several important findings as follows: • Learning-based methods for antecedent identification can also benefit from the use of linguistic clues inspired by Centering Theory (Gros"
I05-2030,P04-1035,0,0.0333573,"ead of communication on the Web has attracted increasing interest in technologies for automatically mining large numbers of message boards and blog pages for opinions and recommendations. Previous approaches to the task of mining a large-scale document collection for opinions can be classified into two groups: the document classification approach and the information extraction approach. In the document classification approach, researchers have been exploring techniques for classifying documents according to semantic/sentiment orientation such as positive vs. negative (e.g. (Dave et al., 2003; Pang and Lee, 2004; Turney, 2002)). The information extraction approach, on the other hand, focuses on the task of extracting elements which constitute opinions (e.g. (Kanayama and Nasukawa, 2004; Hu and Liu, 2004; Tateishi et al., 2001)). The aim of this paper is to extract opinions that represent an evaluation of a products together with the evidence. To achieve this, we consider our task from the information extraction view173 point. We term the above task opinion extraction in this paper. While they can be linguistically realized in many ways, opinions on a product are in fact often expressed in the form of"
I05-2030,J01-4004,0,0.372641,"licit referent) or indefinite. While the figure shows Japanese examples, the similarity between anaphora resolution and opinion extraction is language independent. This analogy naturally leads us to think of applying existing techniques for anaphora resolution to our opinion extraction task since anaphora resolution has been studied for a considerably longer period in a wider range of disciplines as we briefly review below. 3.2 Existing techniques for anaphora resolution Corpus-based empirical approaches to anaphora resolution have been reasonably successful. This approach, as exemplified by (Soon et al., 2001; Iida et al., 2003; Ng, 2004), is cost effective, while achieving a better performance than the best-performing rule-based systems for the test sets of MUC-6 and MUC-7 2 . As suggested by Figure 1, anaphora resolution can be decomposed into two subtasks: anaphoricity determination and antecedent identification. Anaphoricity determination is the task of judging whether a given NP is anaphoric or nonanaphoric. Recent research advances have provided several important findings as follows: • Learning-based methods for antecedent identification can also benefit from the use of linguistic clues insp"
I05-2030,P02-1053,0,0.0547136,"on the Web has attracted increasing interest in technologies for automatically mining large numbers of message boards and blog pages for opinions and recommendations. Previous approaches to the task of mining a large-scale document collection for opinions can be classified into two groups: the document classification approach and the information extraction approach. In the document classification approach, researchers have been exploring techniques for classifying documents according to semantic/sentiment orientation such as positive vs. negative (e.g. (Dave et al., 2003; Pang and Lee, 2004; Turney, 2002)). The information extraction approach, on the other hand, focuses on the task of extracting elements which constitute opinions (e.g. (Kanayama and Nasukawa, 2004; Hu and Liu, 2004; Tateishi et al., 2001)). The aim of this paper is to extract opinions that represent an evaluation of a products together with the evidence. To achieve this, we consider our task from the information extraction view173 point. We term the above task opinion extraction in this paper. While they can be linguistically realized in many ways, opinions on a product are in fact often expressed in the form of an attribute-v"
I05-3003,P01-1017,0,0.0346125,"Missing"
I05-3003,P04-1015,0,0.082201,"Missing"
I05-3003,C04-1040,0,0.050351,"Missing"
I05-3003,J91-4001,0,0.122637,"Missing"
I05-3003,C04-1010,0,0.192855,"Missing"
I05-3018,W04-1109,1,0.93577,"ods for Optimum Chinese Word Segmentation Masayuki Asahara Kenta Fukuoka Ai Azuma Chooi-Ling Goh Yotaro Watanabe Yuji Matsumoto Nara Institute of Science and Technology, Japan E-mail: cje@is.naist.jp Abstract Takashi Tsuzuki Matsushita Electric Industrial Co., Ltd. word classes and character classes in order to overcome the data sparse problem. The word classes are used as the hidden states in MEMM and CRF-based word segmenters. The character classes are used as the features in character-based tagging, character-based chunking and word segmentation. Model b is our previous method proposed in (Goh et al., 2004b): First, a MaxEnt classifier is used to perform character-based tagging to identify OOV words in the test data. In-vocabulary (IV) word list together with the extracted OOV word candidates is used in Maximum Matching algorithm. Overlapping ambiguity is denoted by the different outputs from Forward and Backward Maximum Matching algorithm. Finally, character-based tagging by MaxEnt classifier resolves the ambiguity. Section 2 describes Models a and c. Section 3 describes Model b. Section 4 discusses the differences among the three models. This article presents our recent work for participation"
I05-3018,Y04-1014,1,0.836321,"ods for Optimum Chinese Word Segmentation Masayuki Asahara Kenta Fukuoka Ai Azuma Chooi-Ling Goh Yotaro Watanabe Yuji Matsumoto Nara Institute of Science and Technology, Japan E-mail: cje@is.naist.jp Abstract Takashi Tsuzuki Matsushita Electric Industrial Co., Ltd. word classes and character classes in order to overcome the data sparse problem. The word classes are used as the hidden states in MEMM and CRF-based word segmenters. The character classes are used as the features in character-based tagging, character-based chunking and word segmentation. Model b is our previous method proposed in (Goh et al., 2004b): First, a MaxEnt classifier is used to perform character-based tagging to identify OOV words in the test data. In-vocabulary (IV) word list together with the extracted OOV word candidates is used in Maximum Matching algorithm. Overlapping ambiguity is denoted by the different outputs from Forward and Backward Maximum Matching algorithm. Finally, character-based tagging by MaxEnt classifier resolves the ambiguity. Section 2 describes Models a and c. Section 3 describes Model b. Section 4 discusses the differences among the three models. This article presents our recent work for participation"
I05-3018,N01-1025,1,0.86496,"Missing"
I05-3018,W04-3230,1,0.871441,"fferty et al., 2001). The chunker annotates BIO position tags: “B” stands for ’the first character of an OOV word’; “I” stands for ’other characters in an OOV word’; “O” stands for ’a character outside an OOV word’. The features used in the two chunkers are the characters, the character classes and the information of other characters in five-character window size. The word sequence output by the MEMM-based word segmenter is converted into character sequence with BIES position tags and the word classes. The position tags 135 Final word segmentation is carried out by a CRF-based word segmenter (Kudo and Matsumoto, 2004) (Peng and McCallum, 2004). The word trellis is composed by the similar method with MEMM-based word segmenter. Though state transition probabilities are estimated in the case of MaxEnt framework, the probabilities are normalized in the whole sentence in CRFbased method. CRF-based word segmenter is robust to length-bias problem (Kudo and Matsumoto, 2004) by the global normalization. We will discuss the lengthbias problem in section 4. 2.5 Note on MSR data Unfortunately, we could not complete Models a and c for the MSR data due to time constraints. Therefore, we submitted the following 2 fragmen"
I05-3018,C04-1067,0,0.0620608,"cause the precision of 136 OOV word extraction becomes higher. Other types of OOV word extraction methods should be introduced. For example, (Uchimoto et al., 2001) embeded OOV models in MEMM-based word segmenter (with POS tagging). Less than six-character substrings are extracted as the OOV word candidates in the word trellis. (Peng and McCallum, 2004) proposed OOV word extraction methods based on CRF-based word segmenter. Their CRF-based word segmenter can compute a confidence in each segment. The high confident segments that are not in the IV word list are regarded as OOV word candidates. (Nakagawa, 2004) proposed integration of word and OOV word position tag in a trellis. These three OOV extraction method are different from our methods – character-based tagging. Future work will include implementation of these different sorts of OOV word extraction modules. Length bias problem means the tendency that the locally normalized Markov Model family prefers longer words. Since choosing the longer words reduces the number of words in a sentence, the state-transitions are reduced. The less the state-transitions, the larger the likelihood of the whole sentence. Actually, the lengthbias reflects the rea"
I05-3018,C04-1081,0,0.515471,"hunker annotates BIO position tags: “B” stands for ’the first character of an OOV word’; “I” stands for ’other characters in an OOV word’; “O” stands for ’a character outside an OOV word’. The features used in the two chunkers are the characters, the character classes and the information of other characters in five-character window size. The word sequence output by the MEMM-based word segmenter is converted into character sequence with BIES position tags and the word classes. The position tags 135 Final word segmentation is carried out by a CRF-based word segmenter (Kudo and Matsumoto, 2004) (Peng and McCallum, 2004). The word trellis is composed by the similar method with MEMM-based word segmenter. Though state transition probabilities are estimated in the case of MaxEnt framework, the probabilities are normalized in the whole sentence in CRFbased method. CRF-based word segmenter is robust to length-bias problem (Kudo and Matsumoto, 2004) by the global normalization. We will discuss the lengthbias problem in section 4. 2.5 Note on MSR data Unfortunately, we could not complete Models a and c for the MSR data due to time constraints. Therefore, we submitted the following 2 fragmented models: Model a for MS"
I05-3018,W01-0512,0,0.018108,"but the precision is low. We tried to refine OOV extraction by voting and merging (Model a and c). However, the ROOV of Models a and c are not as good as that of Model b. Figure 1 shows type-precision and type-recall of each OOV extraction modules. While voting helps to make the precision higher, voting deteriorates the recall. Defining some hand written rules to prune false OOV words will help to improve the IV word segmentation (Goh et al., 2004b), because the precision of 136 OOV word extraction becomes higher. Other types of OOV word extraction methods should be introduced. For example, (Uchimoto et al., 2001) embeded OOV models in MEMM-based word segmenter (with POS tagging). Less than six-character substrings are extracted as the OOV word candidates in the word trellis. (Peng and McCallum, 2004) proposed OOV word extraction methods based on CRF-based word segmenter. Their CRF-based word segmenter can compute a confidence in each segment. The high confident segments that are not in the IV word list are regarded as OOV word candidates. (Nakagawa, 2004) proposed integration of word and OOV word position tag in a trellis. These three OOV extraction method are different from our methods – character-ba"
I05-3018,W02-1815,0,0.150494,"mplete Models a and c for the MSR data due to time constraints. Therefore, we submitted the following 2 fragmented models: Model a for MSR data is MEMM-based word segmenter with OOV word list by voting; Model c for MSR data is CRF-based word segmenter with no OOV word candidate. 3 Model b Model b uses a different approach. First, we extract the OOV words using a MaxEnt classifier with only the character as the features. We did not use the character classes as the features. Each character is assigned with BIES position tags. Word segmentation by characterbased tagging is firstly introduced by (Xue and Converse, 2002). In encoding, we extract characters within five-character window size for each character position in the training data as the features for the classifier. In decoding, the BIES position tag is deterministically annotated character by character in the test data. The words that appear only in the test data are treated as OOV word candidates. We can obtain quite high unknown word recall with this model but the precision is a bit low. However, the following segmentation model will try to eliminate some false unknown words. In the next step, we append OOV word candidates into the IV word list extr"
I08-1018,P05-3013,0,\N,Missing
I08-1018,W04-3247,0,\N,Missing
I08-1062,E06-1002,0,0.11292,"Missing"
I08-1065,I05-1069,0,0.113061,"al (and entailment) relation holds between the verb phrases wash something and something is clean, which reflects the commonsense notion that if someone has washed something, this object is clean as a result of the washing event. A crucial issue is how to obtain and maintain a potentially huge collection of such event relations instances. Motivated by this background, several research groups have reported their experiments on automatic 497 acquisition of causal, temporal and entailment relations between event mentions (typically verbs or verb phrases) (Lin and Pantel, 2001; Inui et al., 2003; Chklovski and Pantel, 2005; Torisawa, 2006; Pekar, 2006; Zanzotto et al., 2006, etc.). The common idea behind them is to use a small number of manually selected generic lexico-syntactic cooccurrence patterns (LSPs or simply patterns). to Verb-X and then Verb-Y, for example, is used to obtain temporal relations such as marry and divorce (Chklovski and Pantel, 2005). The use of such generic patterns, however, tends to be high recall but low precision, which requires an additional component for pruning extracted relations. This issue has been addressed in basically two approaches, either by devising heuristic statistical"
I08-1065,P06-1079,1,0.905366,"Missing"
I08-1065,N06-1023,0,0.0642798,"Missing"
I08-1065,W02-2016,1,0.673894,"Missing"
I08-1065,P06-1015,0,0.508012,"ever, tends to be high recall but low precision, which requires an additional component for pruning extracted relations. This issue has been addressed in basically two approaches, either by devising heuristic statistical scores (Chklovski and Pantel, 2005; Torisawa, 2006; Zanzotto et al., 2006) or training classifiers for disambiguation with heavy supervision (Inui et al., 2003). This paper explores a third way for enhancing present LSP-based methods for event relation acquisition. The basic idea is inspired by the following recent findings in relation extraction (Ravichandran and Hovy, 2002; Pantel and Pennacchiotti, 2006, etc.), which aims at extracting semantic relations between entities (as opposed to events) from texts. (a) The use of generic patterns tends to be high recall but low precision, which requires an additional component for pruning. (b) On the other hand, there are specific patterns that are highly reliable but they are much less frequent than generic patterns and each makes only a small contribution to recall. (c) Combining a few generic patters with a much larger collection of reliable specific patterns boosts both precision and recall. Such specific patterns can be acquired from a very large"
I08-1065,N06-1007,0,0.131442,"he verb phrases wash something and something is clean, which reflects the commonsense notion that if someone has washed something, this object is clean as a result of the washing event. A crucial issue is how to obtain and maintain a potentially huge collection of such event relations instances. Motivated by this background, several research groups have reported their experiments on automatic 497 acquisition of causal, temporal and entailment relations between event mentions (typically verbs or verb phrases) (Lin and Pantel, 2001; Inui et al., 2003; Chklovski and Pantel, 2005; Torisawa, 2006; Pekar, 2006; Zanzotto et al., 2006, etc.). The common idea behind them is to use a small number of manually selected generic lexico-syntactic cooccurrence patterns (LSPs or simply patterns). to Verb-X and then Verb-Y, for example, is used to obtain temporal relations such as marry and divorce (Chklovski and Pantel, 2005). The use of such generic patterns, however, tends to be high recall but low precision, which requires an additional component for pruning extracted relations. This issue has been addressed in basically two approaches, either by devising heuristic statistical scores (Chklovski and Pantel,"
I08-1065,P02-1006,0,0.164278,"of such generic patterns, however, tends to be high recall but low precision, which requires an additional component for pruning extracted relations. This issue has been addressed in basically two approaches, either by devising heuristic statistical scores (Chklovski and Pantel, 2005; Torisawa, 2006; Zanzotto et al., 2006) or training classifiers for disambiguation with heavy supervision (Inui et al., 2003). This paper explores a third way for enhancing present LSP-based methods for event relation acquisition. The basic idea is inspired by the following recent findings in relation extraction (Ravichandran and Hovy, 2002; Pantel and Pennacchiotti, 2006, etc.), which aims at extracting semantic relations between entities (as opposed to events) from texts. (a) The use of generic patterns tends to be high recall but low precision, which requires an additional component for pruning. (b) On the other hand, there are specific patterns that are highly reliable but they are much less frequent than generic patterns and each makes only a small contribution to recall. (c) Combining a few generic patters with a much larger collection of reliable specific patterns boosts both precision and recall. Such specific patterns c"
I08-1065,N06-1008,0,0.149248,"holds between the verb phrases wash something and something is clean, which reflects the commonsense notion that if someone has washed something, this object is clean as a result of the washing event. A crucial issue is how to obtain and maintain a potentially huge collection of such event relations instances. Motivated by this background, several research groups have reported their experiments on automatic 497 acquisition of causal, temporal and entailment relations between event mentions (typically verbs or verb phrases) (Lin and Pantel, 2001; Inui et al., 2003; Chklovski and Pantel, 2005; Torisawa, 2006; Pekar, 2006; Zanzotto et al., 2006, etc.). The common idea behind them is to use a small number of manually selected generic lexico-syntactic cooccurrence patterns (LSPs or simply patterns). to Verb-X and then Verb-Y, for example, is used to obtain temporal relations such as marry and divorce (Chklovski and Pantel, 2005). The use of such generic patterns, however, tends to be high recall but low precision, which requires an additional component for pruning extracted relations. This issue has been addressed in basically two approaches, either by devising heuristic statistical scores (Chklovsk"
I08-1065,P06-1107,0,0.505661,"es wash something and something is clean, which reflects the commonsense notion that if someone has washed something, this object is clean as a result of the washing event. A crucial issue is how to obtain and maintain a potentially huge collection of such event relations instances. Motivated by this background, several research groups have reported their experiments on automatic 497 acquisition of causal, temporal and entailment relations between event mentions (typically verbs or verb phrases) (Lin and Pantel, 2001; Inui et al., 2003; Chklovski and Pantel, 2005; Torisawa, 2006; Pekar, 2006; Zanzotto et al., 2006, etc.). The common idea behind them is to use a small number of manually selected generic lexico-syntactic cooccurrence patterns (LSPs or simply patterns). to Verb-X and then Verb-Y, for example, is used to obtain temporal relations such as marry and divorce (Chklovski and Pantel, 2005). The use of such generic patterns, however, tends to be high recall but low precision, which requires an additional component for pruning extracted relations. This issue has been addressed in basically two approaches, either by devising heuristic statistical scores (Chklovski and Pantel, 2005; Torisawa, 2006;"
I08-4005,W04-3205,0,0.106072,"Missing"
I08-4005,P04-1074,0,0.251641,"ation extraction are still limited. Temporal relation extraction includes the following issues: identifying events, anchoring events on the timeline, ordering events, and reasoning with contextually underspecified temporal expressions. To extract temporal relations, several knowledge resources are necessary, such as tense and aspect of verbs, temporal adverbs, and world knowledge (Mani, et al., 2006). In English, TimeBank (Pustejovsky, et al., 2006), a temporal information annotated corpus, is available to machine learning approaches for automatically extracting temporal relation. In Chinese, Li (2004) proposed a machine learning based method for temporal relation identification, but they considered the relation between adjacent verbs in a small scale corpus. There is no publicly available Chinese resource for temporal information processing. We proposed (Cheng, 2007) a dependency structure based method to annotate temporal relations manually on a limited set of event pairs and extend the relations using inference rules. In our previous research, the dependency structure helps to detect subordinate and coordinate structures in sentences. Our proposed criteria can reduce the manual effort fo"
I08-4005,S07-1014,0,\N,Missing
I08-4008,O03-4001,0,0.0541009,"Missing"
I08-4008,O97-4003,0,0.0604099,"Missing"
I08-4008,C00-1026,0,0.0442978,"Missing"
I08-4008,W02-1811,0,\N,Missing
I08-4008,Y06-1044,1,\N,Missing
I11-1017,P06-1132,0,0.118427,"learned from a monolingual corpus of the language to be learned. Once we obtain a manuallycorrected corpus of language learners, it is possible to translate erroneous sentences into correct sentences using SMT. The use of SMT for spelling and grammar correction has the following three advantages. (1) It does not require expert knowledge. (2) It is straightforward to apply SMT tools to this task. (3) Error correction using SMT can benefit from the improvement of SMT method. Related work on error correction using phrasebased SMT includes research on English and Japanese (Brockett et al., 2006; Suzuki and Toutanova, 2006). Brockett et al. (2006) proposed to correct mass noun errors using SMT and used 45,000 sentences as training sets randomly extracted from automatically created 346,000 sentences. Our work differs from them in that we (1) do not restrict ourselves to a specific error type such as mass noun; and (2) exploit a large-scale real world data set. Suzuki and Toutanova (2006) proposed a machine learning-based method to preError Correction Using SMT eˆ = arg max P(e |f ) = arg max P(e)P( f |e) 銭湯に行った。 いつ行ったかがある方がいい (1) where e represents target sentences and f represents source sentences. P(e) is the p"
I11-1017,P01-1008,0,0.0146404,"Missing"
I11-1017,C10-2157,0,0.049317,"apanese language learners around the world has increased more than 30-fold in the past three decades. The Japan Foundation reports that more than 3.65 million people in 133 countries and regions are studying Japanese in 2009 1 . However, there are only 50,000 Japanese language teachers overseas, and thus it is in high demand to find good instructors for writers of Japanese as a Second Language (JSL). Recently, natural language processing research has begun to pay attention to second language learning (Rozovskaya and Roth, 2011; Park and Levy, 2011; Liu et al., 2011; Oyama and Matsumoto, 2010; Xue and Hwa, 2010). However, most previous research for second language learning deals with restricted types of learners’ errors. For example, research for JSL learners’ 1 http://www.jpf.go.jp/e/japanese/ survey/result/index.html 147 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 147–155, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP feedbacks from other users of the native language. However, they are not able to write about arbitrary topics. Third, Lang-8 is a “Multi-lingual language learning and language exchange Social Networking Service” 4 , which has"
I11-1017,P06-1032,0,0.745415,"at the character-wise model outperforms the word-wise model. 1 Yuji Matsumoto NAIST, Japan 何で日本語はこんなに難しい な の？ (Why does Japanese are so difficult?) which has a grammatical error of inserting ‘な’ due to literal translation from Chinese. Park and Levy (2011) proposed an EM-based unsupervised approach to perform whole sentence grammar correction, but the types of errors must be predetermined to learn the parameters for their noisy channel model. It requires expert knowledge of L2 teaching, which is often hard to obtain. One promising approach for correcting unrestricted errors of JSL learners is Brockett et al. (2006)’s automated error correction method using statistical machine translation (SMT). The advantage of their method is that it does not require expert knowledge. Instead, it learns a correction model from sentence-aligned corrected learners’ corpora. However, it is not easy to acquire largescale learners’ corpora. In fact, Brockett et al. (2006) used regular expressions to automatically create erroneous corpora from native corpora. To solve the knowledge acquisition bottleneck, we propose a method of mining revision logs to create a large-scale learners’ corpus. The corpus is compiled from error r"
I11-1017,J93-2003,0,0.0463045,"is that annotators may not correct all the errors in a sentence. Table 5 shows an example of JSL learner’s sentence for confusing case markers of “が” (NOM) and “は” (TOP). In this example, “は” and “が” should be corrected to “が” and “は”, respectively. However, the annotator left the second case markers “は” unchanged. Because the number of these cases seems low, we regard it as safe to ignore this issue for creating the corpus. 3 In this study, we attempt to solve the problem of JSL learners’ error correction using the SMT technique. The well-known SMT formulation using the noisy channel model (Brown et al., 1993) is: e e (I went to a public bath. It is better to say when you went.) from sentence-aligned parallel corpus while LM is learned from target language corpus. To adapt SMT to error correction, f can be regarded as the sentences written by Japanese learners, whereas e represents the manually-corrected Japanese sentences. TM can be learned from the sentence-aligned learners’ corpus. LM can be learned from a monolingual corpus of the language to be learned. Once we obtain a manuallycorrected corpus of language learners, it is possible to translate erroneous sentences into correct sentences using S"
I11-1017,P03-1021,0,0.0408272,"the effect of corpus size; (3) the difference of L1 model. We used Moses 2.1 14 as a decoder and GIZA++ 1.0.5 15 as an alignment tool. We used Japanese morphological analyzer MeCab 0.97 with UniDic 1.3.12 16 for word segmentation. We created a word-wise model as baseline. Hereafter, we refer to this as W and also constructed model with entries from UniDic for better alignment, denoted as W+Dic. We used word trigram as LM for W and W+Dic. We built two character-wise models: character 3-gram and 5-gram represented as C3 and C5, respectively. We also conducted minimum error rate training (MERT) (Och, 2003) in all experiments 17 . でもじょずじゃりません The correct counterpart would be: でもじょう ずじゃあ りません (But I am not good at it.) The corrected sentence has “う” and “あ” inserted 12 . These sentences written by a learner and corrected by a native speaker are tokenized as follows by MeCab 13 , which is one of the most popular Japanese Morphological Analyzer: でも じ ょずじゃりません ( but (fragment) (garbled word) ) でも じょうず じゃ あり ません ( but good at be 4.1 Experimental Data not ) All the data was created from 849,894 Japanese sentences extracted from revision logs of Lang8 crawled in December 2010. To see the difference of"
I11-1017,P02-1040,0,0.0983684,"granularity of tokenization Table 6 illustrates the performance with different 18 The W+Dic 0.9083 0.9210 0.9146 0.8101 methods (Training Corpus: L1 = ALL; Test Corpus: L1 = English; TM: 0.3M sentences; LM: 1M sentence). The character-wise models outperform the word-wise model in both recall and precision. C5 achieved the best precision, F and BLEU, while C3 obtained the best recall. As evaluation metrics, we use automatic evaluation criteria. To be precise, we used recall (R) and precision (P) based on longest common subsequence (LCS) (Mori et al., 1999; Aho, 1990) and character-based BLEU (Papineni et al., 2002). Park and Levy (2011) adopted character-based BLEU for automatic assessment of ESL errors. We followed their use of BLEU in the error correction task of JSL learners. Since we perform minimum error rate training using BLEU we can directly compare each model’s performance. Recall and precision based on LCS are defined as follows: Recall = W 0.9043 0.9175 0.9109 0.8072 19 Note that LM was trained from the whole training corpus. We did not change L1 for LM. pronunciation of “わ” is the same as “は”. 153 Table 7: Comparison of the performance (recall, precision, F, BLEU) of error correction trained"
I11-1017,P11-1094,0,0.0783935,"ructors. We also demonstrate that the extracted learners’ corpus of Japanese as a second language can be used as training data for learners’ error correction using an SMT approach. We evaluate different granularities of tokenization to alleviate the problem of word segmentation errors caused by erroneous input from language learners. Experimental results show that the character-wise model outperforms the word-wise model. 1 Yuji Matsumoto NAIST, Japan 何で日本語はこんなに難しい な の？ (Why does Japanese are so difficult?) which has a grammatical error of inserting ‘な’ due to literal translation from Chinese. Park and Levy (2011) proposed an EM-based unsupervised approach to perform whole sentence grammar correction, but the types of errors must be predetermined to learn the parameters for their noisy channel model. It requires expert knowledge of L2 teaching, which is often hard to obtain. One promising approach for correcting unrestricted errors of JSL learners is Brockett et al. (2006)’s automated error correction method using statistical machine translation (SMT). The advantage of their method is that it does not require expert knowledge. Instead, it learns a correction model from sentence-aligned corrected learne"
I11-1017,P11-1093,0,\N,Missing
I11-1023,P08-1090,0,0.0179781,"ffect of these features explicitly in their systems. Therefore we also investigate these in this paper. Suppose we want to identify the argument of 自首 した (surrendered) in Example (1). The argument is an antecedent of zero-pronoun φ of the predicate. police arrested I had surrendered that heard 私は (φ が) 自首した と 聞いた． I heard that φ had surrendered. 2.3 Similarity between an Argument Position and a co-Reference Chain In the study of implicit arguments1 for English nominal predicates, Gerber and Chai (2010) used similarity features between an argument position and a co-reference chain, inspired by Chambers and Jurafsky (2008), who proposed unsupervised learning of narrative event chains using pointwise mutual information (PMI) between syntactic positions. This method stands on the assumption 1 wa-particle hanako wo-particle (1) 警察 は 花子 を 逮捕した． Police arrested Hanako. With Salient Reference List for “自首する (surrendered)”, the rank of “警察 (police)” is higher than that of “花子 (Hanako)” and it is noisy information for analysis. We also cannot distinguish them with argument frequency information, because frequencies of both “花子 (Hanako)” and “ 警察 (police)” are 1. Though it is reasonable to use the similarity between an"
I11-1023,kawahara-kurohashi-2006-case,0,0.0274238,"Missing"
I11-1023,N09-1018,0,0.0516117,"Missing"
I11-1023,P10-1160,0,0.160346,"t shows if each candidate is ever used as an argument of predicates or not. However, they did not investigate the effect of these features explicitly in their systems. Therefore we also investigate these in this paper. Suppose we want to identify the argument of 自首 した (surrendered) in Example (1). The argument is an antecedent of zero-pronoun φ of the predicate. police arrested I had surrendered that heard 私は (φ が) 自首した と 聞いた． I heard that φ had surrendered. 2.3 Similarity between an Argument Position and a co-Reference Chain In the study of implicit arguments1 for English nominal predicates, Gerber and Chai (2010) used similarity features between an argument position and a co-reference chain, inspired by Chambers and Jurafsky (2008), who proposed unsupervised learning of narrative event chains using pointwise mutual information (PMI) between syntactic positions. This method stands on the assumption 1 wa-particle hanako wo-particle (1) 警察 は 花子 を 逮捕した． Police arrested Hanako. With Salient Reference List for “自首する (surrendered)”, the rank of “警察 (police)” is higher than that of “花子 (Hanako)” and it is noisy information for analysis. We also cannot distinguish them with argument frequency information, beca"
I11-1023,2002.tmi-papers.15,0,0.151366,"resolver. They compute PMI as follows. Suppose the resulting data has N co-referential pairs of argument positions and M of these pairs comprising Ea = hPa , Aa i, Eb = hPb , Ab i, and Ec = hPc , Ac i. Pa , Pb , and Pc are predicates, and Aa , Ab , and Ac are labels such as ARG0 or ARG1 . G(Ea , Eb ) pmi(Ea , Eb ) = log G(Ea , ∗)G(Eb , ∗) M G(Ea , Eb ) = N With this similarity between argument positions, they defined scores between an argument position and a co-reference chain. Related Work Capturing Discourse Context 2.1 Salient Reference List Iida et al. (2003) used Salient Reference List (Nariyama, 2002) based on Centering Theory (Grosz et al., 1995), which explains the structure of discourse and the transition of topics in order to capture discourse context. The list has the following four ordered slots. TOPIC (marked by wa-particle) &gt; SUBJECT (ga) &gt; INDIRECT OBJECT (ni) &gt; DIRECT OBJECT (wo), We check whether each candidate corresponds to any slots from the beginning of a document. If the candidate corresponds to a slot, we (over)write the slot with the candidate. We repeat this until we reach the predicate to analyze. We use the ranks of candidates in the list as a feature. 3 Predicate Argu"
I11-1023,J95-2003,0,0.426459,"pose the resulting data has N co-referential pairs of argument positions and M of these pairs comprising Ea = hPa , Aa i, Eb = hPb , Ab i, and Ec = hPc , Ac i. Pa , Pb , and Pc are predicates, and Aa , Ab , and Ac are labels such as ARG0 or ARG1 . G(Ea , Eb ) pmi(Ea , Eb ) = log G(Ea , ∗)G(Eb , ∗) M G(Ea , Eb ) = N With this similarity between argument positions, they defined scores between an argument position and a co-reference chain. Related Work Capturing Discourse Context 2.1 Salient Reference List Iida et al. (2003) used Salient Reference List (Nariyama, 2002) based on Centering Theory (Grosz et al., 1995), which explains the structure of discourse and the transition of topics in order to capture discourse context. The list has the following four ordered slots. TOPIC (marked by wa-particle) &gt; SUBJECT (ga) &gt; INDIRECT OBJECT (ni) &gt; DIRECT OBJECT (wo), We check whether each candidate corresponds to any slots from the beginning of a document. If the candidate corresponds to a slot, we (over)write the slot with the candidate. We repeat this until we reach the predicate to analyze. We use the ranks of candidates in the list as a feature. 3 Predicate Argument Structure Analysis Exploiting Argument Pos"
I11-1023,N06-1007,0,0.0198555,"s of each argument position (Sorted by frequency) chain, the similarity measure described in Section 2.3 has two problems. One is the strong dependency on the accuracy of co-reference resolver system. In fact, the accuracy of Japanese co-reference resolvers is not accurate enough to create co-reference chains in good quality.2 The other problem is the problem that it needs a lot of documents, because the method does not use any non co-referring nouns. To avoid using an unreliable co-reference resolver, we can suppose the same noun lemmas without pronouns in the same document are coreferences. Pekar (2006) called the noun lemmas anchors and they supposed the similarity measure between syntactic positions. For example, there are two anchors: “Mary” and “house” in the sentences “Mary bought a house. The house belongs to Mary.” They extract two groups: { buy(obj:X), belong(subj:X) } and {buy(subj:X), belong(to:X). } Nevertheless, this method also requires many documents because noun lemmas without anchors are not used for the calculation. In this paper, we propose a more robust similarity measure between argument positions which does not depend on unreliable co-reference annotations by the resolve"
I11-1023,W03-2604,1,0.920871,"references between arguments using a coreference resolver. They compute PMI as follows. Suppose the resulting data has N co-referential pairs of argument positions and M of these pairs comprising Ea = hPa , Aa i, Eb = hPb , Ab i, and Ec = hPc , Ac i. Pa , Pb , and Pc are predicates, and Aa , Ab , and Ac are labels such as ARG0 or ARG1 . G(Ea , Eb ) pmi(Ea , Eb ) = log G(Ea , ∗)G(Eb , ∗) M G(Ea , Eb ) = N With this similarity between argument positions, they defined scores between an argument position and a co-reference chain. Related Work Capturing Discourse Context 2.1 Salient Reference List Iida et al. (2003) used Salient Reference List (Nariyama, 2002) based on Centering Theory (Grosz et al., 1995), which explains the structure of discourse and the transition of topics in order to capture discourse context. The list has the following four ordered slots. TOPIC (marked by wa-particle) &gt; SUBJECT (ga) &gt; INDIRECT OBJECT (ni) &gt; DIRECT OBJECT (wo), We check whether each candidate corresponds to any slots from the beginning of a document. If the candidate corresponds to a slot, we (over)write the slot with the candidate. We repeat this until we reach the predicate to analyze. We use the ranks of candidat"
I11-1023,D08-1055,0,0.145069,"tion’ step in this model selects the most likely argument from all noun phrases preceding the predicate. 4.5 Targets for Comparison of Predicate Argument Analysis Model We evaluate our selection-and-classification approach by comparing our baseline model with two previous approaches TA and IM. 5 Discussion Table 7 presents the result of the experiments. According to the bottom row in Table 7, we achieved the state-of-the-art of Japanese predicate argument structure analysis by combining all discourse context features (+A+B+C+D+E). We investigate our result from five different standpoints. TA: Taira et al. (2008) used decision lists where features were sorted by their weights learned from Support Vector Machine. They simultaneously solved the argument of event nouns in the same lists. IM: Imamura et al. (2009) used discriminative models based on maximum entropy. They added the special noun phrase NULL, which expresses that the predicate does not have any argument. 5.1 Effect of the Selection-and-Classification Approach Because previous work use different features and machine learning methods and experiment on different setting from ours, we also compare with a baseline model BL in order to analyze the"
I11-1023,W07-1522,1,0.315603,"Kawahara and Kurohashi (2006) collected from the web. They are part-of-speech tagged with JUMAN7 and dependency structure parsed with KNP8 . We extracted 1,101,472,855 pairs of a predicate and an argument.9 Table 5: Discourse context features used in the experiment we generate two training examples: One is an example of (b) with the label INTER, “花子”, and the most likely argument selected by (a) at ‘Classification’ step. The other one is an example of (c) with the label HAVE-ARG and “花子”. 4 wo with the case maker が, を and に. 4.2 Training and Evaluation Dataset We used NAIST Text Corpus 1.4β (Iida et al., 2007) for training and evaluation. It is based on Kyoto Text Corpus 3.010 and annotated with predicate-argument structure, event noun structure, and co-reference of nouns about 40,000 sentences of Japanese newspaper text. We excluded 11 articles due to annotation error. We conducted five-fold cross-validation. In the experiments, base phrases and dependency relations are acquired from the Kyoto Text Corpus 3.0 in the same way of related work. Evaluation Setting of Predicate Argument Structure Analysis Exploiting Argument Position and Type We evaluate our proposed selection-andclassification approac"
I11-1023,P09-2022,0,0.659184,"hether each candidate corresponds to any slots from the beginning of a document. If the candidate corresponds to a slot, we (over)write the slot with the candidate. We repeat this until we reach the predicate to analyze. We use the ranks of candidates in the list as a feature. 3 Predicate Argument Structure Analysis Exploiting Argument Position and Type 2.2 Argument Frequency 3.1 Similarity between Argument Positions using Distribution Similarity Iida et al. (2003) used a feature (CHAIN LENGTH) that stands for how often each candidate is used as an argument of predicates in preceding context. Imamura et al. (2009) used a similar binary feature (USED) that shows if each candidate is ever used as an argument of predicates or not. However, they did not investigate the effect of these features explicitly in their systems. Therefore we also investigate these in this paper. Suppose we want to identify the argument of 自首 した (surrendered) in Example (1). The argument is an antecedent of zero-pronoun φ of the predicate. police arrested I had surrendered that heard 私は (φ が) 自首した と 聞いた． I heard that φ had surrendered. 2.3 Similarity between an Argument Position and a co-Reference Chain In the study of implicit ar"
I11-1033,P93-1024,0,0.0472564,"e for classification of unmarked characters. Therefore, we carry out clustering on Kanji characters and add character class n-gramin feature sets. For example, if “深” and “寒” (cold) belong to the same class X, and “寒” appears in training corpus as in a phrase “寒ければ” (if it is cold), then features corresponding to a phrase “X けれは” (if it is X) will be learned from “寒ければ.” As a result, we will be able to exploit “深” as evidence of detecting “は” in “深けれは” as unmarked character. Clustering was performed on Kanji characters with the subsequent and the previous two characters individually based on (Pereira et al. 1993). A Kanji character that appears left of the target character is replaced with the class of the formerclusters and that appears right is replaced with the class of the latter-clusters.  3/C 3/0  First, we built a naive generative model as baseline for labeling voiced consonant mark. This method labels voiced consonant marks that maximize the likelihood of a sentence by using a character 3-gram model. One deficiency of the baseline method is that it requires a fully annotated corpus with the marks. Second, for the dictionary-based approach, we created a dictionary and corpus from the same tra"
I11-1033,neubig-mori-2010-word,0,0.0317903,"ed (+1) or not (-1). Since proposed method does not require a corpus annotated with word boundaries or part-of-speech tags for learning, we take advantage of a large modern a Japanese corpus, Taiyo-Corpus,4 which is based on Japanese magazines from the Meiji Era. This corpus is not annotated with neither word boundaries nor partof-speech tags but is manually annotated with unmarked characters. We employed pointwise prediction which makes a single independent decision at each point: ambiguous Hiragana character or Kunoji-ten5 .6 Therefore, our method can learn from partially annotated corpora (Neubig and Mori, 2010) including raw corpora of modern Japanese literary text, and thus it is easy to obtain training data. Neubig et al. (2011) extend the word segmentation method proposed by Sassano (2002) to Japanese morphological analysis using pointwise prediction. In our method, we adopt the binary features from (Sassano, 2002) to this task. Unlike Sassano and Neubig et al. who use an SVM, we use an online Passive-Aggressive algorithm for 3.1.2 Character type n-grams These features are similar to previously mentioned character n-grams with only the modification of replacing the character itself with the chara"
I11-1033,P11-2093,0,0.02105,"or learning, we take advantage of a large modern a Japanese corpus, Taiyo-Corpus,4 which is based on Japanese magazines from the Meiji Era. This corpus is not annotated with neither word boundaries nor partof-speech tags but is manually annotated with unmarked characters. We employed pointwise prediction which makes a single independent decision at each point: ambiguous Hiragana character or Kunoji-ten5 .6 Therefore, our method can learn from partially annotated corpora (Neubig and Mori, 2010) including raw corpora of modern Japanese literary text, and thus it is easy to obtain training data. Neubig et al. (2011) extend the word segmentation method proposed by Sassano (2002) to Japanese morphological analysis using pointwise prediction. In our method, we adopt the binary features from (Sassano, 2002) to this task. Unlike Sassano and Neubig et al. who use an SVM, we use an online Passive-Aggressive algorithm for 3.1.2 Character type n-grams These features are similar to previously mentioned character n-grams with only the modification of replacing the character itself with the character type. We deal with eleven character types, Hiragana/H, Katakana/K, Kanji/C, Odoriji/O, Latin/L, Digit/D, dash/d, stop"
I11-1033,P02-1064,0,0.0250315,"aiyo-Corpus,4 which is based on Japanese magazines from the Meiji Era. This corpus is not annotated with neither word boundaries nor partof-speech tags but is manually annotated with unmarked characters. We employed pointwise prediction which makes a single independent decision at each point: ambiguous Hiragana character or Kunoji-ten5 .6 Therefore, our method can learn from partially annotated corpora (Neubig and Mori, 2010) including raw corpora of modern Japanese literary text, and thus it is easy to obtain training data. Neubig et al. (2011) extend the word segmentation method proposed by Sassano (2002) to Japanese morphological analysis using pointwise prediction. In our method, we adopt the binary features from (Sassano, 2002) to this task. Unlike Sassano and Neubig et al. who use an SVM, we use an online Passive-Aggressive algorithm for 3.1.2 Character type n-grams These features are similar to previously mentioned character n-grams with only the modification of replacing the character itself with the character type. We deal with eleven character types, Hiragana/H, Katakana/K, Kanji/C, Odoriji/O, Latin/L, Digit/D, dash/d, stop and comma/S, BOS (⟨s⟩)/B, EOS (⟨/s⟩)/E and others/o as the cha"
I11-1033,P98-2152,0,0.0633088,"rked characters is much simpler than error correction of all the Hiragana. Our method differs from Shinnou’s method in that we focus on automatic labeling of voiced consonant marks and emIf we assume an unmarked character as substitution error of one voiced consonant to one voiceless consonant, the task of detecting an unmarked character can be considered as a kind of error correction. In English, we can perform error correction for the one character’s error by word-based approach. However, in Japanese, we cannot simply apply word-based approach because sentences are not segmented into words. Nagata (1998) proposed a statistical method using dynamic programming for selecting the most likely word sequences from candidate word lattice estimated from observed characters in Japanese sentence. In this method, the product of the transition probability of words is used as a word segmentation model. However, most of the historical materials that we deal with are raw text, and there exist little, if any, annotated texts with words 294 ploy a discriminative character n-gram model using a classification-based method. Although Shinnou’s generative model is not capable of using overlapping features, our cla"
I11-1033,C10-1140,0,0.0526366,"Missing"
I11-1126,I05-1005,0,0.0251042,"ata, it extracts more numbers of ga-case than the others. However, ga-case is often the most important for PA relation extraction and sometimes called indispensable case. Our method can extract such important information better than previous work. Although our model did not exploit large-scale corpora, our results are competitive to the results of Imamura et al. (2009). By global optimization in a sentence, our Global model overcame the lack of semantic features and successfully identiﬁed “����” as wo-case of “ ����”. This PA relation is in a relative clause and often hard to identify. Though Abekawa and Okumura (2005) resolved Japanese PA relations in relative clauses by exploiting large-scale corpora, our Markov Logic approach handles this problem by global optimization. Moreover, in global model, � � delete(1), delete(2), delete(7) are also output and “��” and “��” did not become argument candidates. As a result, “��������” was correctly selected as a ga-case of “��”. Error Analysis 6 Conclusion (this) (reason) (Gray Wolf) (revival in the US) �� �� � ���� ���� 1 2 3 4 (plan) (FWS) (in Canada) (capture) ��� �������� ���� ���� 7 6 8 5 (transport by air) (wild) (twelve wolves) ��� ���� �� � 9 10 11 (Form th"
I11-1126,Y01-1001,0,0.0283936,"arguments for the predicate, that is, a nominative case role (ga) is “� (He)” and a dative case role (ni) is “��� (library)”. 1 Introduction Predicate-argument (PA) relation extraction is one of the challenging problems in Natural Language Processing. The analysis extracts semantic information such as “who did what to whom”, which is often useful to various applications like information extraction, document summarization, and machine translation. Predicate-argument relation extraction is often called semantic role labeling. In English, it has been researched on large corpora such as FrameNet (Fillmore et al., 2001) and PropBank (Palmer et al., 2005). CoNLL Shared Task 2008 (Surdeanu et al., 2008) is a representative work of semantic role labeling based on these corpora. Japanese PA relation extraction is a kind of semantic role labeling but an argument is often called case. A typical example of Japanese PA reIn Japanese, Taira et al. (2008) and Imamura et al. (2009) tackled PA relation extraction on NAIST Text Corpus (Iida et al., 2007). They created three separated models corresponding to each of the case; ga (Nominative), wo (Accusative), and ni (Dative). Even though some English semantic role labeler"
I11-1126,P06-1079,1,0.773228,"not an argument. If a is an argument of p with the role r then neither p nor a is deleted. ga wo ni Dep. 13,086 5,192 3,645 Zero-Intra 4,556 376 231 Total 17,642 5,568 3,876 Table 5: Statistics in Evaluation Data isArg delete role Local R 71.4 90.4 72.5 F 75.1 88.4 78.8 P 94.6 94.3 85.5 Global R 84.2 97.9 77.7 F 89.1 96.1 81.4 Table 6: Local vs Global lations (Dep.) are much more common than intra-sentential zero-anaphoric PA relations (ZeroIntra). However, in Japanese, we often ﬁnd zero-anaphoric PA relations called case ellipsis. More detailed descriptions of PA relation types are shown in (Iida et al., 2006). Note that we target only PA relations which occur in a sentence (intra-sentential PA relations). The joint approach using Markov Logic is computationally hard even if it targets only intra-sentential PA relations. Therefore, extraction of inter-sentential PA relations which are crossing sentence boundaries is intractable. Moreover, our approach ﬁnds the most optimized PA assignments in a whole sentence. To keep consistency in a sentence, we delete the sentences which have inter-sentential PA relations. For extracting features, we exploit the annotation of Kyoto Text Corpus as the POS and the"
I11-1126,W07-1522,1,0.932493,"ine translation. Predicate-argument relation extraction is often called semantic role labeling. In English, it has been researched on large corpora such as FrameNet (Fillmore et al., 2001) and PropBank (Palmer et al., 2005). CoNLL Shared Task 2008 (Surdeanu et al., 2008) is a representative work of semantic role labeling based on these corpora. Japanese PA relation extraction is a kind of semantic role labeling but an argument is often called case. A typical example of Japanese PA reIn Japanese, Taira et al. (2008) and Imamura et al. (2009) tackled PA relation extraction on NAIST Text Corpus (Iida et al., 2007). They created three separated models corresponding to each of the case; ga (Nominative), wo (Accusative), and ni (Dative). Even though some English semantic role labeler apply global models, most of them solve problems on a per-predicate basis (Toutanova et al., 2008; Watanabe et al., 2010). In this work, we propose an approach to Japanese PA relation extraction on a per-sentence basis and utilize important dependencies between one PA relation and another in the same sentence. In order to use such dependencies as global constraints, we apply a Markov Logic approach to Japanese PA relation ext"
I11-1126,P09-2022,0,0.132113,"applications like information extraction, document summarization, and machine translation. Predicate-argument relation extraction is often called semantic role labeling. In English, it has been researched on large corpora such as FrameNet (Fillmore et al., 2001) and PropBank (Palmer et al., 2005). CoNLL Shared Task 2008 (Surdeanu et al., 2008) is a representative work of semantic role labeling based on these corpora. Japanese PA relation extraction is a kind of semantic role labeling but an argument is often called case. A typical example of Japanese PA reIn Japanese, Taira et al. (2008) and Imamura et al. (2009) tackled PA relation extraction on NAIST Text Corpus (Iida et al., 2007). They created three separated models corresponding to each of the case; ga (Nominative), wo (Accusative), and ni (Dative). Even though some English semantic role labeler apply global models, most of them solve problems on a per-predicate basis (Toutanova et al., 2008; Watanabe et al., 2010). In this work, we propose an approach to Japanese PA relation extraction on a per-sentence basis and utilize important dependencies between one PA relation and another in the same sentence. In order to use such dependencies as global c"
I11-1126,kawahara-etal-2002-construction,0,0.0129164,"selectional preference features obtained from large-scale unlabelled data. In qualitative analysis, we ﬁnd that our global model resolves some diﬃcult cases such as PA relations in relative clauses. The remainder of this paper is organized as follows: Section 2 describes related work; Section 3 introduces Markov Logic; Section 4 explains our proposed Markov Logic Network; Section 5 presents and discusses the experimental setting and the results; and in Section 6 we conclude and present ideas for future research. 2 Related Work In Japanese, PA annotated corpora such as Kyoto Text Corpus (KTC) (Kawahara et al., 2002) and NAIST Text Corpus (NTC) (Iida et al., 2007) have been developed and utilized. 1 CoNLL Shared Task 2009 (Hajiˇc et al., 2009) included Japanese PA relation extraction on the data from KTC. The data we used in this work is from NTC. NTC is based on the same text as KTC, which contains 38,384 sentences from 2,929 news articles. 2 The annotation in NTC has the three case roles: “ga (Nominative)”, “wo (Accusative)”, and “ni (Dative)”. The predicate-argument annotation in NTC is based on deep cases and is more diﬃcult to ana1 KTC is annotated with surface cases and NTC is annotated with deep ca"
I11-1126,N09-1018,0,0.396125,"ough some English semantic role labeler apply global models, most of them solve problems on a per-predicate basis (Toutanova et al., 2008; Watanabe et al., 2010). In this work, we propose an approach to Japanese PA relation extraction on a per-sentence basis and utilize important dependencies between one PA relation and another in the same sentence. In order to use such dependencies as global constraints, we apply a Markov Logic approach to Japanese PA relation extraction. In recent years, in English semantic role labeling, a Markov Logic model has achieved one of the state-of-theart results (Meza-Ruiz and Riedel, 2009a). With global constraints between multiple PA relations, a Markov Logic model can avoid inconsistencies between several PA relations and improve performance of extraction. In addition, we introduce new global constraints to eﬀectively delete inappropriate argument candidates which are unrelated to PA relations. We consider that extraction of PA relations and dele1125 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1125–1133, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP Figure 2: Diﬀerence in Methods tion of the other phrases are two si"
I11-1126,W09-1213,0,0.196183,"ough some English semantic role labeler apply global models, most of them solve problems on a per-predicate basis (Toutanova et al., 2008; Watanabe et al., 2010). In this work, we propose an approach to Japanese PA relation extraction on a per-sentence basis and utilize important dependencies between one PA relation and another in the same sentence. In order to use such dependencies as global constraints, we apply a Markov Logic approach to Japanese PA relation extraction. In recent years, in English semantic role labeling, a Markov Logic model has achieved one of the state-of-theart results (Meza-Ruiz and Riedel, 2009a). With global constraints between multiple PA relations, a Markov Logic model can avoid inconsistencies between several PA relations and improve performance of extraction. In addition, we introduce new global constraints to eﬀectively delete inappropriate argument candidates which are unrelated to PA relations. We consider that extraction of PA relations and dele1125 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1125–1133, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP Figure 2: Diﬀerence in Methods tion of the other phrases are two si"
I11-1126,J05-1004,0,0.0129451,"a nominative case role (ga) is “� (He)” and a dative case role (ni) is “��� (library)”. 1 Introduction Predicate-argument (PA) relation extraction is one of the challenging problems in Natural Language Processing. The analysis extracts semantic information such as “who did what to whom”, which is often useful to various applications like information extraction, document summarization, and machine translation. Predicate-argument relation extraction is often called semantic role labeling. In English, it has been researched on large corpora such as FrameNet (Fillmore et al., 2001) and PropBank (Palmer et al., 2005). CoNLL Shared Task 2008 (Surdeanu et al., 2008) is a representative work of semantic role labeling based on these corpora. Japanese PA relation extraction is a kind of semantic role labeling but an argument is often called case. A typical example of Japanese PA reIn Japanese, Taira et al. (2008) and Imamura et al. (2009) tackled PA relation extraction on NAIST Text Corpus (Iida et al., 2007). They created three separated models corresponding to each of the case; ga (Nominative), wo (Accusative), and ni (Dative). Even though some English semantic role labeler apply global models, most of them"
I11-1126,D08-1068,0,0.0477078,"arkov Logic (Richardson and Domingos, 2006), a combination of ﬁrstorder logic and Markov Networks. It can be understood as a formalism that extends ﬁrst-order logic to allow formulae that can be violated with some penalty. From an alternative point of view, it is an expressive template language that uses ﬁrst order logic formulae to instantiate Markov Networks of repetitive structure. In the ﬁeld of NLP, the Markov Logic approach has been applied to various tasks such as entity resolution (Singla and Domingos, 2006)�information extraction (Poon and Domingos, 2007), and coreference resolution (Poon and Domingos, 2008), among others. From a wide range of SRL languages we chose Markov Logic because it supports discriminative training (as opposed to generative SRL languages such as PRM (Koller, 1999)). Moreover, several Markov Logic software libraries exist and are freely available (as opposed to other discriminative frameworks such as Relational Markov Networks (Taskar et al., 2002)). A Markov Logic Network (MLN) M is a set of pairs (φ, w) where φ is a ﬁrst order formula and w is a real number (the formula’s weight). It deﬁnes a probability distribution over sets of ground atoms, or so-called possible worlds"
I11-1126,W08-2121,0,0.093796,"Missing"
I11-1126,D08-1055,0,0.402372,"often useful to various applications like information extraction, document summarization, and machine translation. Predicate-argument relation extraction is often called semantic role labeling. In English, it has been researched on large corpora such as FrameNet (Fillmore et al., 2001) and PropBank (Palmer et al., 2005). CoNLL Shared Task 2008 (Surdeanu et al., 2008) is a representative work of semantic role labeling based on these corpora. Japanese PA relation extraction is a kind of semantic role labeling but an argument is often called case. A typical example of Japanese PA reIn Japanese, Taira et al. (2008) and Imamura et al. (2009) tackled PA relation extraction on NAIST Text Corpus (Iida et al., 2007). They created three separated models corresponding to each of the case; ga (Nominative), wo (Accusative), and ni (Dative). Even though some English semantic role labeler apply global models, most of them solve problems on a per-predicate basis (Toutanova et al., 2008; Watanabe et al., 2010). In this work, we propose an approach to Japanese PA relation extraction on a per-sentence basis and utilize important dependencies between one PA relation and another in the same sentence. In order to use suc"
I11-1126,J08-2002,0,0.166651,", 2008) is a representative work of semantic role labeling based on these corpora. Japanese PA relation extraction is a kind of semantic role labeling but an argument is often called case. A typical example of Japanese PA reIn Japanese, Taira et al. (2008) and Imamura et al. (2009) tackled PA relation extraction on NAIST Text Corpus (Iida et al., 2007). They created three separated models corresponding to each of the case; ga (Nominative), wo (Accusative), and ni (Dative). Even though some English semantic role labeler apply global models, most of them solve problems on a per-predicate basis (Toutanova et al., 2008; Watanabe et al., 2010). In this work, we propose an approach to Japanese PA relation extraction on a per-sentence basis and utilize important dependencies between one PA relation and another in the same sentence. In order to use such dependencies as global constraints, we apply a Markov Logic approach to Japanese PA relation extraction. In recent years, in English semantic role labeling, a Markov Logic model has achieved one of the state-of-theart results (Meza-Ruiz and Riedel, 2009a). With global constraints between multiple PA relations, a Markov Logic model can avoid inconsistencies betwe"
I11-1126,P10-2018,1,0.920047,"ive work of semantic role labeling based on these corpora. Japanese PA relation extraction is a kind of semantic role labeling but an argument is often called case. A typical example of Japanese PA reIn Japanese, Taira et al. (2008) and Imamura et al. (2009) tackled PA relation extraction on NAIST Text Corpus (Iida et al., 2007). They created three separated models corresponding to each of the case; ga (Nominative), wo (Accusative), and ni (Dative). Even though some English semantic role labeler apply global models, most of them solve problems on a per-predicate basis (Toutanova et al., 2008; Watanabe et al., 2010). In this work, we propose an approach to Japanese PA relation extraction on a per-sentence basis and utilize important dependencies between one PA relation and another in the same sentence. In order to use such dependencies as global constraints, we apply a Markov Logic approach to Japanese PA relation extraction. In recent years, in English semantic role labeling, a Markov Logic model has achieved one of the state-of-theart results (Meza-Ruiz and Riedel, 2009a). With global constraints between multiple PA relations, a Markov Logic model can avoid inconsistencies between several PA relations"
I11-1126,W09-1201,0,\N,Missing
I13-1094,W95-0107,0,0.16069,"Missing"
I13-1094,W09-1206,0,0.0338762,"Missing"
I13-1094,D11-1001,0,0.0130879,"oundaries and to extract relevant information for training classifiers. However, there have been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. Sun et. al (2009) and Hacioglu et. al (2004) addressed the SRL problem on the basis of shallow syntactic information at the level of phrase chunks. In their approach, SRL is formulated as a sequence labelIn recent years, SRL has become an important component in many kinds of deep natural language processing applications, such as question answering (Narayanan and Harabagiu, 2004), event extraction (Riedel and McCallum, 2011), document categorization (Persson et al., 2009). SRL aims at identifying the semantic relations between predicates in a sentence and their associated arguments, with these relations drawn from a pre-specific list of possible semantic roles for 781 International Joint Conference on Natural Language Processing, pages 781–787, Nagoya, Japan, 14-18 October 2013. ing problem, performing IOB2 decisions on the syntactic chunks of a sentence. However, this method ignores the full syntactic parsing information entirely, and we believe that even the accuracy of full syntactic parsing is not ideal, it i"
I13-1094,P06-2013,0,0.0284832,"ts would be improved. Under this hypothesis, we simply extracted the following features from every parse tree in the Nbest list which are generated using second-order MST parser. These features are also included in the ”standard” feature set when N = 1. 4 Results and Discussion 4.1 Experimental Setting We used the Chinese dataset provided by CoNLL2009 shared task for experiments. For comparison, two kinds of dependency parsing results are provided, the first is from MALT parser, the second is from second-order MST parser. As for chunking information, we used the chunk definition presented in (Chen et al., 2006) to extract chunks from Chinese Tree Bank as training corpus. The line CH in Figure 1 shows the definition of chunks. In this example, ” 金融工作”(finance work) is a noun phrase and is composed by two nouns. 1 783 http://crfpp.sourceforge.net/ WORD 去年 西藏 金融 工作 取得 显著 成绩 POS NN NR NN NN VV JJ NN CH [NP] [NP] [VP] [ADJP] [NP] TAG B-NP B-NP B-NP I-NP B-VP B-ADJP B-NP SRL TMP NONE NONE A0 取得.01 NONE A1 [ NP ] Figure 1: Chunking information for a predicate-argument structure. Feature Name Chunk features Path features Description chunk tag of headword with IB representation (e.g. B − N P ) chunk tag of t"
I13-1094,W04-2416,0,0.0849248,"Missing"
I13-1094,N04-1032,0,0.0384608,"ious dependencybased SRL research (Johansson and Nugues, 2008; Luo et al., 2012). We do not explain ”standard” features, however, we give a detailed description of the features used in this work. 3.3 Error Analysis for Dependency-based SRL 4.2.1 Base Phrase Chunking Related Features In Figure 1, obviously, words in chunks do not have equal importance for SRL. Headwords represent the main meaning of the chunks. The base phrase chunking related features shown in Table 2 are only applied to these headwords. For other words in chunks, only lemma and POS information is used. The rules described in Sun and Jurafsky (2004) are used to extract headwords. Verb class in Table 2 is represented similarly as V erb.C1C2, which means this verb has two senses. For its first sense, it has one core argument and for its second sense, it has two core arguments. These verb classes are extracted from Chinese PropBank (Xue, 2008). Using the gold parse of dependency relations between a predicate and its arguments and according to these relations, we classified SRL errors into following three types. • C: children of a predicate should be arguments but they are tagged incorrectly. • G: grand children of a predicate should be argu"
I13-1094,D09-1153,0,0.0377288,"Missing"
I13-1094,D08-1008,0,0.18199,"hod ignores the full syntactic parsing information entirely, and we believe that even the accuracy of full syntactic parsing is not ideal, it is still helpful for SRL. Moreover, their method is inapplicable to dependency based SRL since a chunk usually consists of successive words. A substantial amount of research has focused on dependency-based SRL (Meza-Ruiz and Riedel, 2009; Luo et al., 2012) since the CoNLL-2009 shared task and rich linguistic features (Zhao et al., 2009) are applied. For dependency related features, most studies focused on extracting them from the best dependency result. Johansson and Nugues (2008) tried to use N-best dependency parsing results. In their work, they applied 16-best dependency trees to generate predicate-argument structures and applied both syntactic trees and predicate-argument structures to a linear model. This model reranks the predicate-argument structures and the top 16 dependency trees at the same time. Though their work suggests that N-best dependency parsing can enhance the SRL, little is known about how the N-best dependency parsing related features perform on SRL. 3.1 Predicate Sense Disambiguation and SRL with a Local Model Since the predicate cannot be an argu"
I13-1094,J08-2002,0,0.0842463,"Missing"
I13-1094,P05-1012,0,0.148457,"Missing"
I13-1094,J08-2004,0,0.0611353,"Missing"
I13-1094,N09-1018,0,0.0167972,"ist of possible semantic roles for 781 International Joint Conference on Natural Language Processing, pages 781–787, Nagoya, Japan, 14-18 October 2013. ing problem, performing IOB2 decisions on the syntactic chunks of a sentence. However, this method ignores the full syntactic parsing information entirely, and we believe that even the accuracy of full syntactic parsing is not ideal, it is still helpful for SRL. Moreover, their method is inapplicable to dependency based SRL since a chunk usually consists of successive words. A substantial amount of research has focused on dependency-based SRL (Meza-Ruiz and Riedel, 2009; Luo et al., 2012) since the CoNLL-2009 shared task and rich linguistic features (Zhao et al., 2009) are applied. For dependency related features, most studies focused on extracting them from the best dependency result. Johansson and Nugues (2008) tried to use N-best dependency parsing results. In their work, they applied 16-best dependency trees to generate predicate-argument structures and applied both syntactic trees and predicate-argument structures to a linear model. This model reranks the predicate-argument structures and the top 16 dependency trees at the same time. Though their work s"
I13-1094,W09-1208,0,0.0254255,"81–787, Nagoya, Japan, 14-18 October 2013. ing problem, performing IOB2 decisions on the syntactic chunks of a sentence. However, this method ignores the full syntactic parsing information entirely, and we believe that even the accuracy of full syntactic parsing is not ideal, it is still helpful for SRL. Moreover, their method is inapplicable to dependency based SRL since a chunk usually consists of successive words. A substantial amount of research has focused on dependency-based SRL (Meza-Ruiz and Riedel, 2009; Luo et al., 2012) since the CoNLL-2009 shared task and rich linguistic features (Zhao et al., 2009) are applied. For dependency related features, most studies focused on extracting them from the best dependency result. Johansson and Nugues (2008) tried to use N-best dependency parsing results. In their work, they applied 16-best dependency trees to generate predicate-argument structures and applied both syntactic trees and predicate-argument structures to a linear model. This model reranks the predicate-argument structures and the top 16 dependency trees at the same time. Though their work suggests that N-best dependency parsing can enhance the SRL, little is known about how the N-best depe"
I13-1094,D07-1100,0,0.0665788,"Missing"
I13-1094,C04-1100,0,0.00959114,"onstituent parse of sentences to define argument boundaries and to extract relevant information for training classifiers. However, there have been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. Sun et. al (2009) and Hacioglu et. al (2004) addressed the SRL problem on the basis of shallow syntactic information at the level of phrase chunks. In their approach, SRL is formulated as a sequence labelIn recent years, SRL has become an important component in many kinds of deep natural language processing applications, such as question answering (Narayanan and Harabagiu, 2004), event extraction (Riedel and McCallum, 2011), document categorization (Persson et al., 2009). SRL aims at identifying the semantic relations between predicates in a sentence and their associated arguments, with these relations drawn from a pre-specific list of possible semantic roles for 781 International Joint Conference on Natural Language Processing, pages 781–787, Nagoya, Japan, 14-18 October 2013. ing problem, performing IOB2 decisions on the syntactic chunks of a sentence. However, this method ignores the full syntactic parsing information entirely, and we believe that even the accurac"
I13-1094,W08-2121,0,\N,Missing
I13-1094,P10-2018,1,\N,Missing
I17-1003,D15-1166,0,0.0713244,"network which encodes a variable-length input sequence into a vector and decodes it into a variable-length output. Since the model uses the information of the source representation and the previously generated words to produce the next-word token, this distributed representation allows the Seq2Seq model to generate appropriate mapping between the input and the output (Li et al., 2016). For specific tasks, Neural Machine Translation (NMT) model, which is based on the Seq2Seq learning, has achieved excellent translation performance in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Firat et al., 2016). In particular, the NMT model which is built upon an encoder-decoder framework with attention mechanism (Bahdanau et al., 2015) can also pay attention and its decoder knows which part of the input is relevant for the word that is currently being translated. Therefore, it has shown competitive results and outperformed conventional statistical methods (Bentivogli et al., 2016). Despite of these advantages, NMT model still has a couple particular issues to be solved such as dealing with fixed vocabulary, not applicable to small-data, additional phrases, wrong lexical choice"
I17-1003,P17-2021,0,0.0288665,"t al., 2015; Socher et al., 2014; Levy and Goldberg, 2014; Komnios, 2016; Ono and Hatano, 2014). It has been shown that the combination of words and their dependency information can boost performance. Besides, in the work of Vinyals et al. (Vinyals et al., 2014), they also represent output as a linearized tree structure, but their work showed that generic sequence-to-sequence approaches can achieve excellent results on syntactic constituency parsing. At a glance, our proposed method is a little similar to the works of Dyer et al., Aharoni et al., Eriguchi et al., Wu et al. (Dyer et al., 2016; Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Wu et al., 2017) in use of parse tree and generation. However, Dyer et al. and Aharoni et al.’s works concern predicting constituent trees. Eriguchi et al.’s model employs syntactic dependency parsing but their model is hybridized the decoder of NMT and the Recurrent Neural Network Grammars, and the target sentences are parsed in transition-based parsing. Wu et al.’s model also employs dependency parsing but their model separately predicts the target translation sequence and parsing action sequence which maps to translation. On the other hand, our proposed model’s deco"
I17-1003,D16-1096,0,0.0516265,"Seq2Seq model and the proposed approach. cial end token, it is incapable in capturing longdistance dependencies in history, so ineffective for long sentences translation (Zhang et al., 2016; Toral and S´anchez-Cartagena, 2017). Even with an attention mechanism, the Seq2Seq model just pays attention to the current alignment information between the inputs and the output at the current position but ignores past alignments information. Therefore, it cannot keep track of the attention history when it updates information at each current time step, leading to the over-production (Tu et al., 2016a,c; Mi et al., 2016; Tu et al., 2016b). In order to address the above two issues, it is worth considering that using syntactic dependency information and representing the output as a tree structure would be effective. This approach allows the next tokens to be output based on not only the previous tokens but also the syntactic dependencies so far, thereby conditioning them on more abundant information so it has the ability to make smarter predictions. Basically, in this paper, we train the model with an encoder-decoder neural network and using dependencies in which the input of the source language is in sequence"
I17-1003,W16-2007,0,0.0311432,"ences with up to 50 and 80 tokens, respectively. Furthermore, the proposed method also solved the two existing problems, ineffective translation of long sentences and over-translation in Neural Machine Translation. 1 Introduction Our task is to construct a model which learns input in sequence form and decodes output as a linearized dependency tree. In this work, we propose an approach in which dependency labels are incorporated into the model to represent more grammatical information in the output sequence. As we know, the Sequence to Sequence (Seq2Seq) Learning model (Sutskever et al., 2014; Aharoni et al., 2016) is extremely effective on a va• Translation of long sentences • Over-translation Since the decoder of the Seq2Seq model produces the target language word by word simply based on the previous target words and the sourceside representation vector until it reaches the spe∗ This author’s present affiliation is CyberAgent, Inc., Tokyo, Japan, yoshimoto akifumi xa@cyberagent.co.jp 21 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 21–29, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP tively. Also, we compare and analyze the results of att"
I17-1003,P02-1040,0,0.0981443,"es that of the attention-based Seq2Seq model. The BLEU score falls from 19.31 to 16.97 with a 2.34 points difference for the attention-based Seq2Seq model while the point difference is 1.51 in the Seq2Dep model. From the experiments, we confirm that by using the syntactic dependency information, the Seq2Dep model can learn well and reduce the drop in BLEU score compared to the baseline model even if the sentence is very long. Besides, we can see the BLEU score is low for short sentences which have a length of 10 words or less. This is because of the brevity penalty on short sentences in BLEU (Papineni et al., 2002). With regards to the BLEU score without post-processing, we see that the score of the Seq2Dep-80 model is higher than that of the Seq2Dep-50 model. The reason could be: The longer the sentences are, the more syntactic deAlso, in terms of the over-translation problem, Figure 6 shows that the repetition rates of the two models decrease gradually with respect to the length of the sentences and the Seq2Dep model has a lower repetition rate. When we checked the translation results, we saw that Node-closing token “}” was almost generated after each subtree. Moreover, we saw that there were some ver"
I17-1003,D16-1025,0,0.033175,"specific tasks, Neural Machine Translation (NMT) model, which is based on the Seq2Seq learning, has achieved excellent translation performance in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Firat et al., 2016). In particular, the NMT model which is built upon an encoder-decoder framework with attention mechanism (Bahdanau et al., 2015) can also pay attention and its decoder knows which part of the input is relevant for the word that is currently being translated. Therefore, it has shown competitive results and outperformed conventional statistical methods (Bentivogli et al., 2016). Despite of these advantages, NMT model still has a couple particular issues to be solved such as dealing with fixed vocabulary, not applicable to small-data, additional phrases, wrong lexical choice errors, long sentence translation, over and under translation, etc. In this paper, we touch upon the following two major problems: Sequence to Sequence Neural Machine Translation has achieved significant performance in recent years. Yet, there are some existing issues that Neural Machine Translation still does not solve completely. Two of them are translation of long sentences and “over-translati"
I17-1003,Q14-1017,0,0.0213235,"ed dependency-based tree structure. That is, instead of predicting only words at each time step, the model trains the network to predict both words and their grammatical dependencies as a dependency tree at each time step. Therefore, it is hoped that the accuracy of output will be improved. The major contributions of this work are as follows: 2 Related Work In fact, the effectiveness of using dependency information of words has been reported in some previous NLP tasks, for example, in dependencybased word embeddings, relation classification and sentence classification tasks (Liu et al., 2015; Socher et al., 2014; Levy and Goldberg, 2014; Komnios, 2016; Ono and Hatano, 2014). It has been shown that the combination of words and their dependency information can boost performance. Besides, in the work of Vinyals et al. (Vinyals et al., 2014), they also represent output as a linearized tree structure, but their work showed that generic sequence-to-sequence approaches can achieve excellent results on syntactic constituency parsing. At a glance, our proposed method is a little similar to the works of Dyer et al., Aharoni et al., Eriguchi et al., Wu et al. (Dyer et al., 2016; Aharoni and Goldberg, 2017; Erig"
I17-1003,D14-1082,0,0.015024,"he next tokens. This means that the model is capable of modeling grammatical dependencies in the output symbols. Also, in Seq2Dep model, we define the Nonterminal “{DEPENDENCY LABEL”, and Node-closing “}” tokens. Nonterminal indicates subtree (Dong and Lapata, 2016), which means open subtree to visit its children nodes. Node-closing indicates end-ofand the next token lyj , which may be a word or dependency label, will be generated as follows:  lyj = f sj , lyj−1 , Cj , (3) In this paper, dependencies are defined as the dependency labels which are achieved from the Stanford Dependency Parser (Chen and Manning, 2014). The decoder will decode the next output based on relations between governors and dependents in a linearized tree structure. In regards to the order of generating the dependency labels and the words, the decoder will produce these symbols in a manner called Depth-first pre-order traversal. In this section, we will describe the model stepby-step as follows: 3.1 Transformation and Tree Traversal Processing Data Since there is no parallel corpus in which the source-side is represented in sequence and targetside is represented in linearized dependency tree, we have to prepare data for training by"
I17-1003,P16-1004,0,0.0275309,"reate output with a linearized tree structure to train the model. That is, for each rooted subtree, governors and dependency labels of the sentence are predicted first, and their information will be used to predict the next dependent words. In other words, the model can capture the dependency information between label-word and wordword pairs to predict the next tokens. This means that the model is capable of modeling grammatical dependencies in the output symbols. Also, in Seq2Dep model, we define the Nonterminal “{DEPENDENCY LABEL”, and Node-closing “}” tokens. Nonterminal indicates subtree (Dong and Lapata, 2016), which means open subtree to visit its children nodes. Node-closing indicates end-ofand the next token lyj , which may be a word or dependency label, will be generated as follows:  lyj = f sj , lyj−1 , Cj , (3) In this paper, dependencies are defined as the dependency labels which are achieved from the Stanford Dependency Parser (Chen and Manning, 2014). The decoder will decode the next output based on relations between governors and dependents in a linearized tree structure. In regards to the order of generating the dependency labels and the words, the decoder will produce these symbols in"
I17-1003,E17-1100,0,0.040705,"Missing"
I17-1003,N16-1024,0,0.0197855,"cation tasks (Liu et al., 2015; Socher et al., 2014; Levy and Goldberg, 2014; Komnios, 2016; Ono and Hatano, 2014). It has been shown that the combination of words and their dependency information can boost performance. Besides, in the work of Vinyals et al. (Vinyals et al., 2014), they also represent output as a linearized tree structure, but their work showed that generic sequence-to-sequence approaches can achieve excellent results on syntactic constituency parsing. At a glance, our proposed method is a little similar to the works of Dyer et al., Aharoni et al., Eriguchi et al., Wu et al. (Dyer et al., 2016; Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Wu et al., 2017) in use of parse tree and generation. However, Dyer et al. and Aharoni et al.’s works concern predicting constituent trees. Eriguchi et al.’s model employs syntactic dependency parsing but their model is hybridized the decoder of NMT and the Recurrent Neural Network Grammars, and the target sentences are parsed in transition-based parsing. Wu et al.’s model also employs dependency parsing but their model separately predicts the target translation sequence and parsing action sequence which maps to translation. On the other han"
I17-1003,P17-2012,0,0.0175823,"2014; Levy and Goldberg, 2014; Komnios, 2016; Ono and Hatano, 2014). It has been shown that the combination of words and their dependency information can boost performance. Besides, in the work of Vinyals et al. (Vinyals et al., 2014), they also represent output as a linearized tree structure, but their work showed that generic sequence-to-sequence approaches can achieve excellent results on syntactic constituency parsing. At a glance, our proposed method is a little similar to the works of Dyer et al., Aharoni et al., Eriguchi et al., Wu et al. (Dyer et al., 2016; Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Wu et al., 2017) in use of parse tree and generation. However, Dyer et al. and Aharoni et al.’s works concern predicting constituent trees. Eriguchi et al.’s model employs syntactic dependency parsing but their model is hybridized the decoder of NMT and the Recurrent Neural Network Grammars, and the target sentences are parsed in transition-based parsing. Wu et al.’s model also employs dependency parsing but their model separately predicts the target translation sequence and parsing action sequence which maps to translation. On the other hand, our proposed model’s decoder directly predicts t"
I17-1003,N16-1101,0,0.0245308,"s a variable-length input sequence into a vector and decodes it into a variable-length output. Since the model uses the information of the source representation and the previously generated words to produce the next-word token, this distributed representation allows the Seq2Seq model to generate appropriate mapping between the input and the output (Li et al., 2016). For specific tasks, Neural Machine Translation (NMT) model, which is based on the Seq2Seq learning, has achieved excellent translation performance in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Firat et al., 2016). In particular, the NMT model which is built upon an encoder-decoder framework with attention mechanism (Bahdanau et al., 2015) can also pay attention and its decoder knows which part of the input is relevant for the word that is currently being translated. Therefore, it has shown competitive results and outperformed conventional statistical methods (Bentivogli et al., 2016). Despite of these advantages, NMT model still has a couple particular issues to be solved such as dealing with fixed vocabulary, not applicable to small-data, additional phrases, wrong lexical choice errors, long sentence"
I17-1003,P16-5005,0,0.135144,"of attention-based Seq2Seq model and the proposed approach. cial end token, it is incapable in capturing longdistance dependencies in history, so ineffective for long sentences translation (Zhang et al., 2016; Toral and S´anchez-Cartagena, 2017). Even with an attention mechanism, the Seq2Seq model just pays attention to the current alignment information between the inputs and the output at the current position but ignores past alignments information. Therefore, it cannot keep track of the attention history when it updates information at each current time step, leading to the over-production (Tu et al., 2016a,c; Mi et al., 2016; Tu et al., 2016b). In order to address the above two issues, it is worth considering that using syntactic dependency information and representing the output as a tree structure would be effective. This approach allows the next tokens to be output based on not only the previous tokens but also the syntactic dependencies so far, thereby conditioning them on more abundant information so it has the ability to make smarter predictions. Basically, in this paper, we train the model with an encoder-decoder neural network and using dependencies in which the input of the source lan"
I17-1003,N16-1175,0,0.019259,"nstead of predicting only words at each time step, the model trains the network to predict both words and their grammatical dependencies as a dependency tree at each time step. Therefore, it is hoped that the accuracy of output will be improved. The major contributions of this work are as follows: 2 Related Work In fact, the effectiveness of using dependency information of words has been reported in some previous NLP tasks, for example, in dependencybased word embeddings, relation classification and sentence classification tasks (Liu et al., 2015; Socher et al., 2014; Levy and Goldberg, 2014; Komnios, 2016; Ono and Hatano, 2014). It has been shown that the combination of words and their dependency information can boost performance. Besides, in the work of Vinyals et al. (Vinyals et al., 2014), they also represent output as a linearized tree structure, but their work showed that generic sequence-to-sequence approaches can achieve excellent results on syntactic constituency parsing. At a glance, our proposed method is a little similar to the works of Dyer et al., Aharoni et al., Eriguchi et al., Wu et al. (Dyer et al., 2016; Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Wu et al., 2017) in u"
I17-1003,P16-1008,0,0.0265709,"of attention-based Seq2Seq model and the proposed approach. cial end token, it is incapable in capturing longdistance dependencies in history, so ineffective for long sentences translation (Zhang et al., 2016; Toral and S´anchez-Cartagena, 2017). Even with an attention mechanism, the Seq2Seq model just pays attention to the current alignment information between the inputs and the output at the current position but ignores past alignments information. Therefore, it cannot keep track of the attention history when it updates information at each current time step, leading to the over-production (Tu et al., 2016a,c; Mi et al., 2016; Tu et al., 2016b). In order to address the above two issues, it is worth considering that using syntactic dependency information and representing the output as a tree structure would be effective. This approach allows the next tokens to be output based on not only the previous tokens but also the syntactic dependencies so far, thereby conditioning them on more abundant information so it has the ability to make smarter predictions. Basically, in this paper, we train the model with an encoder-decoder neural network and using dependencies in which the input of the source lan"
I17-1003,E17-1117,0,0.040828,"Missing"
I17-1003,P14-2050,0,0.0430942,"ree structure. That is, instead of predicting only words at each time step, the model trains the network to predict both words and their grammatical dependencies as a dependency tree at each time step. Therefore, it is hoped that the accuracy of output will be improved. The major contributions of this work are as follows: 2 Related Work In fact, the effectiveness of using dependency information of words has been reported in some previous NLP tasks, for example, in dependencybased word embeddings, relation classification and sentence classification tasks (Liu et al., 2015; Socher et al., 2014; Levy and Goldberg, 2014; Komnios, 2016; Ono and Hatano, 2014). It has been shown that the combination of words and their dependency information can boost performance. Besides, in the work of Vinyals et al. (Vinyals et al., 2014), they also represent output as a linearized tree structure, but their work showed that generic sequence-to-sequence approaches can achieve excellent results on syntactic constituency parsing. At a glance, our proposed method is a little similar to the works of Dyer et al., Aharoni et al., Eriguchi et al., Wu et al. (Dyer et al., 2016; Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Wu et"
I17-1003,P17-1065,0,0.0314179,", 2014; Komnios, 2016; Ono and Hatano, 2014). It has been shown that the combination of words and their dependency information can boost performance. Besides, in the work of Vinyals et al. (Vinyals et al., 2014), they also represent output as a linearized tree structure, but their work showed that generic sequence-to-sequence approaches can achieve excellent results on syntactic constituency parsing. At a glance, our proposed method is a little similar to the works of Dyer et al., Aharoni et al., Eriguchi et al., Wu et al. (Dyer et al., 2016; Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Wu et al., 2017) in use of parse tree and generation. However, Dyer et al. and Aharoni et al.’s works concern predicting constituent trees. Eriguchi et al.’s model employs syntactic dependency parsing but their model is hybridized the decoder of NMT and the Recurrent Neural Network Grammars, and the target sentences are parsed in transition-based parsing. Wu et al.’s model also employs dependency parsing but their model separately predicts the target translation sequence and parsing action sequence which maps to translation. On the other hand, our proposed model’s decoder directly predicts the linearized depe"
I17-1003,P15-2047,0,0.0227832,"ated in a linearized dependency-based tree structure. That is, instead of predicting only words at each time step, the model trains the network to predict both words and their grammatical dependencies as a dependency tree at each time step. Therefore, it is hoped that the accuracy of output will be improved. The major contributions of this work are as follows: 2 Related Work In fact, the effectiveness of using dependency information of words has been reported in some previous NLP tasks, for example, in dependencybased word embeddings, relation classification and sentence classification tasks (Liu et al., 2015; Socher et al., 2014; Levy and Goldberg, 2014; Komnios, 2016; Ono and Hatano, 2014). It has been shown that the combination of words and their dependency information can boost performance. Besides, in the work of Vinyals et al. (Vinyals et al., 2014), they also represent output as a linearized tree structure, but their work showed that generic sequence-to-sequence approaches can achieve excellent results on syntactic constituency parsing. At a glance, our proposed method is a little similar to the works of Dyer et al., Aharoni et al., Eriguchi et al., Wu et al. (Dyer et al., 2016; Aharoni and"
I17-1027,J94-4001,0,0.843456,"Missing"
I17-1027,P06-1055,0,0.0336394,"the similarity score. Hanamoto (2012) used dual decomposition to combine an HPSG parser with the model of Hara et al. (2009). The method of use of the replaceability property has recently been adopted by Ficler and Goldberg (2016). They incorporated the replaceability property between conjuncts into the feature representations, as well as the similarity property. They made use of these properties to assign scores to candidate pairs of conjuncts. Their method consists of three components: a binary classifier to detect the presence of coordination, the parser extended from the Berkeley Parser (Petrov et al., 2006) to generate candidate pairs, and a discriminative neural network to identify conjuncts. As similarity features, they compute the Euclidean distance between the two representations of con6 Conclusions We propose a neural network model to disambiguate coordinate structure boundaries. Our method relies on two properties: (i) conjuncts tend to have a similar structure in syntax or semantics and (ii) conjuncts can be replaced with each other, maintaining sentence consistency. On the basis of these observations, we compute two feature vectors from a sequence of vectors produced by bidirectional RNN"
I17-1027,P16-1079,0,0.387324,"NE) = w · hk + b s = [Score(N ONE); Score(1, k + 1); . . . ; Replaceability feature vector We define a feature vector based on the conjunct replaceability as follows. frepl (h1:N , i, j, k) =  |hi−1 hi − hi−1 hk+1 |;  |hj hj+1 − hk−1 hj+1 | To cope with the absence of coordination against a coordinator, we also calculate the score for a candidate of N ONE. The score N ONE is simply computed as the product of a weight vector and the sentence-level representation of the coordinator from the RNN layer. 4 Experiments We evaluate our proposed model using the coordination annotated Penn Treebank (Ficler, 2016) and the Genia treebank beta (Kim et al., 2003). We present the number of occurrences of coordinator words and the number of sentences with coordination in Table 13 . 3 We consider “and,” “or,” “but,” “nor,” and “and/or” in the PTB and “and,” “or,” and “but” in the Genia as coordinator words following Ficler and Goldberg (2016) and Hara et al. (2009). (7) 268 Penn Treebank Training Development Testing Genia # Coordinators 27903 (24450) 22670 (17893) 953 (848) 1282 (1099) 3598 (3598) # Sentences 21314 (19095) 17282 (13932) 742 (673) 985 (873) 2508 (2508) Table 1: The number of coordinators in t"
I17-1027,D07-1064,0,0.886581,"s, or external resources such as thesauri. To overcome these problems, Ficler and Goldberg (2016) proposed a neural network model with the replaceability feature as well as the similarity feature. Their model produces candidate pairs of conjuncts using probabilities assigned by the Berkeley Parser. All candidate pairs are scored on the basis of the similarity, replaceability and parser-derived features, and then the best scored pair is picked. Their approach outperforms existing constituent parsers for the Penn Treebank and similarity-based coordination disambiguation methods such as those by Shimbo and Hara (2007) and Hara et al. (2009) for the Genia treebank. Although Ficler and Goldberg’s (2016) method improves performance significantly, it heavily depends on the syntactic parser. They use the outputs from the parser not only for candidates generation and the feature for scoring, but also for computation of the similarities. The problems of propagated errors from the parser and dependencies on external resources still remain in their work. In this work, we propose a neural network model for coordination disambiguation that does not require any external syntactic parser. Our model exploits both the si"
I17-1027,D16-1003,0,0.494479,"tudies of coordination disambiguation rely only on the similarities between conjuncts, despite the fact that similarities are not always helpful (Shimbo and Hara, 2007; Hara et al., 2009; Hanamoto, 2012). For example, the sentence “[at least two commercial versions have been put on the U.S. market], and [an estimated 500 have been sold].” does not have sim264 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 264–272, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP (whose conjuncts make sense individually) and outperforms the methods by Ficler and Goldberg (2016) and Hara et al. (2009) in Section 4. The contributions of our work include the following: (i) Our model can capture dissimilar conjuncts as well as similar ones using the similarity and replaceability features. (ii) Our model performs better than others without any thesauri, feature engineering, or syntactic parsers to extract conjunct features. 2 Figure 1: The coordination identification task and our subtask. Coordinate Structure Analysis 2.1 Task Description Coordination is a frequently occurring syntactic structure along with several phrases, known as conjuncts. The task of coordination di"
I17-1027,E12-1044,0,0.267796,"g coordination still remains one of the difficult problems that state-of-the-art parsers cannot cope with. Given a coordinator word, how can we find conjuncts? Coordinate structures are characterized by two properties: (1) similar structures often appear in conjuncts, and (2) one conjunct can be replaced with another conjunct without losing sentence consistency in syntax or semantics. However, many previous studies of coordination disambiguation rely only on the similarities between conjuncts, despite the fact that similarities are not always helpful (Shimbo and Hara, 2007; Hara et al., 2009; Hanamoto, 2012). For example, the sentence “[at least two commercial versions have been put on the U.S. market], and [an estimated 500 have been sold].” does not have sim264 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 264–272, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP (whose conjuncts make sense individually) and outperforms the methods by Ficler and Goldberg (2016) and Hara et al. (2009) in Section 4. The contributions of our work include the following: (i) Our model can capture dissimilar conjuncts as well as similar ones using the simil"
I17-1027,P09-1109,1,0.154005,"ch as thesauri. To overcome these problems, Ficler and Goldberg (2016) proposed a neural network model with the replaceability feature as well as the similarity feature. Their model produces candidate pairs of conjuncts using probabilities assigned by the Berkeley Parser. All candidate pairs are scored on the basis of the similarity, replaceability and parser-derived features, and then the best scored pair is picked. Their approach outperforms existing constituent parsers for the Penn Treebank and similarity-based coordination disambiguation methods such as those by Shimbo and Hara (2007) and Hara et al. (2009) for the Genia treebank. Although Ficler and Goldberg’s (2016) method improves performance significantly, it heavily depends on the syntactic parser. They use the outputs from the parser not only for candidates generation and the feature for scoring, but also for computation of the similarities. The problems of propagated errors from the parser and dependencies on external resources still remain in their work. In this work, we propose a neural network model for coordination disambiguation that does not require any external syntactic parser. Our model exploits both the similarity and replaceabi"
I17-1027,P15-1150,0,0.0289335,"jpost = g(hk+1:j ) (k + 1 ≤ j ≤ N ) 267 (4) Then vipre and vjpost are fed into the following two feature extraction functions. Similarity feature vector In order to capture the similarity between the preconjunct and the post-conjunct, the feature vector is computed as follows:   fsim (vipre , vjpost ) = |vipre − vjpost |; vipre vjpost (5) where |vipre − vjpost |is the absolute value of element-wise subtraction, and vipre vjpost is element-wise multiplication. These subtraction and multiplication operations are intended to model the semantic distance and relatedness (Ji and Eisenstein, 2013; Tai et al., 2015; Hashimoto et al., 2016). yˆ = arg max pˆθ (y|x)  y (9) (6) 3.5 Learning The loss function is the negative log-likelihood of the true pair of conjuncts y (k) : J(θ) = − D X log pˆθ (y (d) |x(d) ) + d=1 λ kθk2 2 (10) where D is the number of occurrences of coordinator words in a training dataset, θ is a set of model parameters, and the hyperparameter λ adjusts the regularization strength. The model parameters are optimized by minimizing the loss using the stochastic gradient descent (SGD). This layer computes the scores of pairs of conjuncts based on the similarity feature vectors and the rep"
I17-1027,N03-1033,0,0.0866573,"he inner, outer, and exact metrics, we simply divide the preconjuncts into subconjuncts using the character “,” as the divider. Evaluation Using the Penn Treebank Experimental Setup We use the coordination annotated Penn Treebank and divide it into wsj 2-21 as the training set, wsj 22 as the development set, and wsj 23 as the testing set. We use pretrained 200-dimensional word embeddings from the New York Times section in English Gigaword (fifth edition) (Parker et al., 2011) using Word2Vec4 with its default parameter. For the POS tags, we use 10-way jackknifing using the Stanford POS Tagger (Toutanova et al., 2003) and initialize the 50-dimensional embeddings with the uniform distribution within [−1, 1]. We use three-layer bidirectional LSTMs as an RNN layer. The dimensionality of the LSTM hidden vectors in each direction is selected from {400, 600}. Our MLP consists of one hidden layer with ReLU activation, and an output layer. The number of the hidden layer units is selected from {1200, 2400}. The model parameters are optimized by the minibatched SGD with a batch size of 20. The learning rate is automatically tuned by Adam (Kingma and Ba, 2014). When training, we apply dropout (Srivastava et al., 2014"
I17-1027,D13-1090,0,0.0275831,"i:k−1 ) (1 ≤ i ≤ k − 1) vjpost = g(hk+1:j ) (k + 1 ≤ j ≤ N ) 267 (4) Then vipre and vjpost are fed into the following two feature extraction functions. Similarity feature vector In order to capture the similarity between the preconjunct and the post-conjunct, the feature vector is computed as follows:   fsim (vipre , vjpost ) = |vipre − vjpost |; vipre vjpost (5) where |vipre − vjpost |is the absolute value of element-wise subtraction, and vipre vjpost is element-wise multiplication. These subtraction and multiplication operations are intended to model the semantic distance and relatedness (Ji and Eisenstein, 2013; Tai et al., 2015; Hashimoto et al., 2016). yˆ = arg max pˆθ (y|x)  y (9) (6) 3.5 Learning The loss function is the negative log-likelihood of the true pair of conjuncts y (k) : J(θ) = − D X log pˆθ (y (d) |x(d) ) + d=1 λ kθk2 2 (10) where D is the number of occurrences of coordinator words in a training dataset, θ is a set of model parameters, and the hyperparameter λ adjusts the regularization strength. The model parameters are optimized by minimizing the loss using the stochastic gradient descent (SGD). This layer computes the scores of pairs of conjuncts based on the similarity feature v"
I17-1027,C08-1054,0,0.295405,"Missing"
I17-1027,W15-2208,1,0.882138,"Missing"
I17-2017,P16-1101,0,0.325294,"nd syntactic chunking are segment-level sequence modeling tasks, which require to recognize a segment from a sequence of words. A segment means a sequence of words that may compose an expression as shown in Figure 1. Current high performance NER systems use the word-level linear chain Conditional Random Fields (CRF) (Lafferty et al., 2001) with neural networks. Especially, it has been shown that the combination of LSTMs (Hochreiter and Schmidhuber, 1997; Gers et al., 2000), convolutional neural networks (CNNs) (LeCun et al., 1989), and word-level CRF achieves the state-of-the-art performance (Ma and Hovy, 2016). Figure 1 shows an overview of the word-level CRF for NER. However, the word-level neural CRF has two main limitations: (1) it captures only first-order word label dependencies thus it cannot capture 97 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 97–102, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP 2 Word-level Neural CRF score for jumping from tag yi−1 to yi , and Y indicates all possible paths. At test time, the predicted sequence is obtained by finding the highest score in a all possible paths using Viterbi algorithm as fol"
I17-2017,D13-1032,0,0.0788849,"Missing"
I17-2017,Q16-1026,0,0.072638,"Missing"
I17-2017,D14-1162,0,0.077618,"v 99.71 99.96 99.98 Test Prec. Recall F1 BLSTM-CNN 90.85 91.92 91.38 BLSTM-CNN-CRF 94.67 94.43 94.55 Our method 94.55 95.12 94.84 Table 3: Result of CoNLL 2000 Chunking. Test 99.27 99.71 99.83 Table 1: Threshold T and Oracle score on NER. Test Prec. Recall F1 BLSTM-CNN 89.04 90.40 89.72 BLSTM-CNN-CRF3 90.82 91.11 90.96 Our method 91.07 91.50 91.28 + Binary Dict 91.05 91.69 91.37 + WikiEmb Dict 91.29 91.58 91.44 + Binary + WikiEmb 91.47 91.62 91.55 Ma and Hovy (2016) 91.35 91.06 91.21 Table 2: Result of CoNLL 2003 English NER. level CNN, and 100 dimentional pre-trained word embedding of GloVe (Pennington et al., 2014). At input layer and output layer, we apply dropout (Srivastava et al., 2014) with rate at 0.5. In our model, we set 400 filters with window size 3 in CNN for segment vector. To optimize our model, we use AdaDelta (Zeiler, 2012) with batch size 10 and gradient clipping 5. We use early stopping (Caruana et al., 2001) based on performance on development sets. improves the F1 score from 91.28 to 91.44. Eventually, we achieve the F1 score 91.55 with two dictionary features. The results of CoNLL 2000 Chunking is shown in Table 3. Similar to NER task, by adding a CRF layer to BLSTM-CNN, it improves"
I17-2017,P16-2038,0,0.023615,"is the probability of a possible NE type. Finally, we apply a linear chain CRF to find the highest score path in the segment lattice as we describe in Section 2. 4.1 Datasets We evaluate our method on two segment-level sequence tagging tasks: NER and text chunking3 . For NER, we use CoNLL 2003 English NER shared task (Tjong Kim Sang and De Meulder, 2003). Following previous work (Ma and Hovy, 2016), we use BIOES tagging scheme in the wordlevel tagging model. For text chunking, we use the CoNLL 2000 English text chunking shared task (Tjong Kim Sang and Buchholz, 2000). Following previous work (Søgaard and Goldberg, 2016), the section 19 of WSJ corpus is used as the development set. We use BIOES tagging scheme in the word-level tagging model and measure performance using F1 score in all experiments. 3.3 Dictionary Features for NER In this subsection, we describe the use of two additional dictionary features for NER. Since an entry of named entity dictionary and the segment in our model are in one-to-one correspondence, it is easy to directly incorporate the dictionary features into our model. We use following two dictionary features on NER task. 4.2 Model Settings To generate a segment lattice, we train word-l"
I17-2017,W00-0726,0,0.623325,"Missing"
I17-2017,N16-1030,0,0.244544,"Missing"
I17-2017,W03-0419,0,0.160611,"Missing"
I17-2017,K16-1025,1,0.901799,"Missing"
I17-2017,P16-1134,0,0.0802545,"Missing"
I17-2027,P10-2012,0,0.0214242,"se units. In turn, in the RST Signaling Corpus, each DR is further labeled with one or more types of signaling strategy. These signals not only include explicit discourse markers but also other features typically used in automatic implicit relation identification and psycholinguistic research, such as reference, lexical, semantic, syntactic, graphical and genre features (Das and Taboada, 2017). For example, the temporal relation in Example (A) is annotated with three signal labels in the RST Signaling Corpus:1 these models can also be used to reflect difficulties in human language processing (Keller, 2010; Demberg et al., 2013). However, we are not aware of any prior work that implements a discourse processor with such a strong assumption to incrementality. Although expectation for upcoming DRs is demonstrated in various lexico-syntactic constructions in the first clause/sentence (Cristea and Webber, 1997), existing methods of discourse parsing rely on a pipeline, in which the raw text is first segmented into discourse units, mostly clauses or sentences, and the relation is predicted based on two complete discourse units. In this respect, even shiftreduce discourse parsers (Marcu, 1999; Reitte"
I17-2027,J13-4008,0,0.0184045,"urn, in the RST Signaling Corpus, each DR is further labeled with one or more types of signaling strategy. These signals not only include explicit discourse markers but also other features typically used in automatic implicit relation identification and psycholinguistic research, such as reference, lexical, semantic, syntactic, graphical and genre features (Das and Taboada, 2017). For example, the temporal relation in Example (A) is annotated with three signal labels in the RST Signaling Corpus:1 these models can also be used to reflect difficulties in human language processing (Keller, 2010; Demberg et al., 2013). However, we are not aware of any prior work that implements a discourse processor with such a strong assumption to incrementality. Although expectation for upcoming DRs is demonstrated in various lexico-syntactic constructions in the first clause/sentence (Cristea and Webber, 1997), existing methods of discourse parsing rely on a pipeline, in which the raw text is first segmented into discourse units, mostly clauses or sentences, and the relation is predicted based on two complete discourse units. In this respect, even shiftreduce discourse parsers (Marcu, 1999; Reitter, 2003; Sagae, 2009; J"
I17-2027,D14-1036,0,0.0142309,"hough intuitively for parsing every word is relevant to the syntactical structure, it may not be the case for more global linguistic structures such as DRs, which may only be triggered by some informative cue words, and it is yet unclear at which point the human processor recogRelated Work This work is related to incremental approaches of natural language processing (NLP) and psycholinguistic studies on human discourse processing. In NLP, incremental approaches are used in tasks such as syntactic parsing (Stolcke, 1995; Collins and Roark, 2004; K¨ohn and Menzel, 2014), semantic role labeling (Konstas et al., 2014) and other joint tasks (Hatori et al., 2012; Li and Ji, 2014; Zhou et al., 2016). These incremental systems are advantageous since they are capable of synchronous analysis by accepting sentence prefixes as inputs. On top of generating more natural and timely response in dialogue systems and improving language modeling in speech recognition, 157 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 157–162, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP Data The RST Signaling Corpus consists of annotation of discourse signals over the RST D"
I17-2027,P04-1015,0,0.0196853,"e that matches the sentence prefix read so far (Tanenhaus et al., 1995). Though intuitively for parsing every word is relevant to the syntactical structure, it may not be the case for more global linguistic structures such as DRs, which may only be triggered by some informative cue words, and it is yet unclear at which point the human processor recogRelated Work This work is related to incremental approaches of natural language processing (NLP) and psycholinguistic studies on human discourse processing. In NLP, incremental approaches are used in tasks such as syntactic parsing (Stolcke, 1995; Collins and Roark, 2004; K¨ohn and Menzel, 2014), semantic role labeling (Konstas et al., 2014) and other joint tasks (Hatori et al., 2012; Li and Ji, 2014; Zhou et al., 2016). These incremental systems are advantageous since they are capable of synchronous analysis by accepting sentence prefixes as inputs. On top of generating more natural and timely response in dialogue systems and improving language modeling in speech recognition, 157 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 157–162, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP Data The RST Sig"
I17-2027,P14-1038,0,0.0310695,"tical structure, it may not be the case for more global linguistic structures such as DRs, which may only be triggered by some informative cue words, and it is yet unclear at which point the human processor recogRelated Work This work is related to incremental approaches of natural language processing (NLP) and psycholinguistic studies on human discourse processing. In NLP, incremental approaches are used in tasks such as syntactic parsing (Stolcke, 1995; Collins and Roark, 2004; K¨ohn and Menzel, 2014), semantic role labeling (Konstas et al., 2014) and other joint tasks (Hatori et al., 2012; Li and Ji, 2014; Zhou et al., 2016). These incremental systems are advantageous since they are capable of synchronous analysis by accepting sentence prefixes as inputs. On top of generating more natural and timely response in dialogue systems and improving language modeling in speech recognition, 157 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 157–162, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP Data The RST Signaling Corpus consists of annotation of discourse signals over the RST Discourse Treebank (Carlson et al., 2002), which is a discour"
I17-2027,D16-1065,0,0.0148472,"it may not be the case for more global linguistic structures such as DRs, which may only be triggered by some informative cue words, and it is yet unclear at which point the human processor recogRelated Work This work is related to incremental approaches of natural language processing (NLP) and psycholinguistic studies on human discourse processing. In NLP, incremental approaches are used in tasks such as syntactic parsing (Stolcke, 1995; Collins and Roark, 2004; K¨ohn and Menzel, 2014), semantic role labeling (Konstas et al., 2014) and other joint tasks (Hatori et al., 2012; Li and Ji, 2014; Zhou et al., 2016). These incremental systems are advantageous since they are capable of synchronous analysis by accepting sentence prefixes as inputs. On top of generating more natural and timely response in dialogue systems and improving language modeling in speech recognition, 157 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 157–162, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP Data The RST Signaling Corpus consists of annotation of discourse signals over the RST Discourse Treebank (Carlson et al., 2002), which is a discourse annotated resourc"
I17-2027,P99-1047,0,0.197673,"ssing (Keller, 2010; Demberg et al., 2013). However, we are not aware of any prior work that implements a discourse processor with such a strong assumption to incrementality. Although expectation for upcoming DRs is demonstrated in various lexico-syntactic constructions in the first clause/sentence (Cristea and Webber, 1997), existing methods of discourse parsing rely on a pipeline, in which the raw text is first segmented into discourse units, mostly clauses or sentences, and the relation is predicted based on two complete discourse units. In this respect, even shiftreduce discourse parsers (Marcu, 1999; Reitter, 2003; Sagae, 2009; Ji and Eisenstein, 2014) are incremental only at discourse unit level. In psycholinguistics, expectation in language processing is a well studied topic (e.g. Altmann (1998)). Experimental studies suggest that humans use available pragmatic cues to generate expectations and anticipate the upcoming discourse structure (Rohde, 2008), but there are diverging findings about the time-course for humans to recognize and integrate DRs. For example, Millis and Just (1994) state that integration of a causal relation takes place at the end of the second clause. In contrast, o"
I17-2027,J93-2004,0,0.0581015,"we want a human subject to identify the cues within the component clauses/sentences that trigger the recognition of a given DR, such as the underlined tokens in Example (1). Although the exact annotated resource is not yet available, we obtained such annotation by converting the annotation in the RST Signaling Corpus (Das et al., 2015). 1 The list of DR signals and the relation between the RST Treebank and the RST Signaling Corpus can be found in the appendix. Details can be found in the related literature. 2 provided by the Penn Treebank, which annotates on the same text as the RST Treebank (Marcus et al., 1993) 3 List of excluded signals are shown in the appendix. 158 category relation sense count expansion elaboration joint background evaluation manner-mean summary topic-comment topic-change 7, 070 1, 031 787 505 197 170 44 21 comparison contrast comparison 934 243 contingency enablement cause explanation condition 512 499 325 263 temporal temporal 429 attribution Total attribution discourse informativeness of prefixes in different sizes. The informativeness of each prefix is calculated from the cues covered by the prefix. For each DR spanning two consecutive clauses/sentences, the prefix size rang"
I17-2027,W09-3813,0,0.0276704,"et al., 2013). However, we are not aware of any prior work that implements a discourse processor with such a strong assumption to incrementality. Although expectation for upcoming DRs is demonstrated in various lexico-syntactic constructions in the first clause/sentence (Cristea and Webber, 1997), existing methods of discourse parsing rely on a pipeline, in which the raw text is first segmented into discourse units, mostly clauses or sentences, and the relation is predicted based on two complete discourse units. In this respect, even shiftreduce discourse parsers (Marcu, 1999; Reitter, 2003; Sagae, 2009; Ji and Eisenstein, 2014) are incremental only at discourse unit level. In psycholinguistics, expectation in language processing is a well studied topic (e.g. Altmann (1998)). Experimental studies suggest that humans use available pragmatic cues to generate expectations and anticipate the upcoming discourse structure (Rohde, 2008), but there are diverging findings about the time-course for humans to recognize and integrate DRs. For example, Millis and Just (1994) state that integration of a causal relation takes place at the end of the second clause. In contrast, other experiments report that"
I17-2027,J95-2002,0,0.512148,"l syntactic tree that matches the sentence prefix read so far (Tanenhaus et al., 1995). Though intuitively for parsing every word is relevant to the syntactical structure, it may not be the case for more global linguistic structures such as DRs, which may only be triggered by some informative cue words, and it is yet unclear at which point the human processor recogRelated Work This work is related to incremental approaches of natural language processing (NLP) and psycholinguistic studies on human discourse processing. In NLP, incremental approaches are used in tasks such as syntactic parsing (Stolcke, 1995; Collins and Roark, 2004; K¨ohn and Menzel, 2014), semantic role labeling (Konstas et al., 2014) and other joint tasks (Hatori et al., 2012; Li and Ji, 2014; Zhou et al., 2016). These incremental systems are advantageous since they are capable of synchronous analysis by accepting sentence prefixes as inputs. On top of generating more natural and timely response in dialogue systems and improving language modeling in speech recognition, 157 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 157–162, c Taipei, Taiwan, November 27 – December 1, 2017 20"
I17-2044,D16-1161,0,0.0422605,"ta. When we sample dialect pattern ms from MatchedList, we use two types of p(ms |mt ). The first type is fixed probability. We set p(ms |mt ) = 1/len(MatchedList) for all matched patterns. The second type is generative probability, which is calculated from the training data (see the previous subsection). The comparison of these two types of probabilities is discussed in the experimental section. 4.2 Settings For the baseline model other than encoderdecoder models, we used Moses. Moses is a tool of training statistical machine translation and a strong baseline for the text-normalization task (Junczys-Dowmunt and Grundkiewicz, 2016). For such a task, we can ignore the word reordering; therefore, we set the distortion limit to 0. We used MERT on the development set for tuning. We confirmed that using both manually annotated and augmented data for building LM greatly degraded its final BLUE score in our preliminary experiments and used only manually annotated data as the training data of LM. We used beam search for the encoder-decoder model (EncDec) and set the beam size to 10. When in the n beam search step, we used length normalized score S(t, s), where 3.2 Generating Augmented Data using Character-level Conversion For o"
I17-2044,D15-1166,0,0.0797738,"ence Laboratories 3 Nara Institute of Science and Technology {saito.itsumi, suzuki.jun, nishida.kyosuke}@lab.ntt.co.jp, {masumura.ryo, kobashikawa.satoshi, tomita.junji}@lab.ntt.co.jp matsu@is.naist.jp, k.sadamitsu.ic@future.co.jp Abstract using statistical machine translation (SMT) techniques (Junczys-Dowmunt and Grundkiewicz, 2016; Yuan and Briscoe, 2016), in particular, utilizing the Moses toolkit (Koehn et al., 2007). In recent years, encoder-decoder models with an attention mechanism (Bahdanau et al., 2014) have made great progress regarding many NLP tasks, including machine translation (Luong et al., 2015; Sennrich et al., 2016), text summarization (Rush et al., 2015) and text normalization (Xie et al., 2016; Yuan and Briscoe, 2016; Ikeda et al., 2017). We can also simply apply an encoder-decoder model to text normalization tasks. However, it is well-known that encoderdecoder models often fail to perform better than conventional methods when the availability of training data is insufficient. Unfortunately, the amount of training data for text normalization tasks is generally relatively small to sufficiently train encoder-decoder models. Therefore, data utilization and augmentation are importan"
I17-2044,P02-1040,0,0.0995881,"p(t|s, θ))/|t|. We maximize S(t, s) to find normalized sentence. We set the embedding size of the character and hidden layer to 300 and 256, respectively. We used ”mrphaug (mr)” as the augmented data generated from morphological-level conversion and ”mosesaug (mo)” as augmented data generated from character-level conversion (Moses). The “mr:R” and “mr:W” represent the difference in generative probability p(ms |mt ), which is used when generating augmented data; “mr:R” indicates fixed generative probability and “mr:W” indicates weighted generative probability. For the evaluation, we used BLEU (Papineni et al., 2002), which is widely used for machine translation. 4.3 Results Table 3 lists the normalization results. Notransformation indicates the result of evaluating input sentences without transformation. Moses achieved a reasonable BLEU score with a small amount of human-annotated data. However, the improvement of adding augmented data was limited. On the other hand, the encoder-decoder model showed a very low BLEU score with a small amount of human-annotated data. With this amount of data, the encoder-decoder model gen5 Discussion Oracle Analysis To investigate the further improvement on normalization a"
I17-2044,D15-1044,0,0.0499875,"ito.itsumi, suzuki.jun, nishida.kyosuke}@lab.ntt.co.jp, {masumura.ryo, kobashikawa.satoshi, tomita.junji}@lab.ntt.co.jp matsu@is.naist.jp, k.sadamitsu.ic@future.co.jp Abstract using statistical machine translation (SMT) techniques (Junczys-Dowmunt and Grundkiewicz, 2016; Yuan and Briscoe, 2016), in particular, utilizing the Moses toolkit (Koehn et al., 2007). In recent years, encoder-decoder models with an attention mechanism (Bahdanau et al., 2014) have made great progress regarding many NLP tasks, including machine translation (Luong et al., 2015; Sennrich et al., 2016), text summarization (Rush et al., 2015) and text normalization (Xie et al., 2016; Yuan and Briscoe, 2016; Ikeda et al., 2017). We can also simply apply an encoder-decoder model to text normalization tasks. However, it is well-known that encoderdecoder models often fail to perform better than conventional methods when the availability of training data is insufficient. Unfortunately, the amount of training data for text normalization tasks is generally relatively small to sufficiently train encoder-decoder models. Therefore, data utilization and augmentation are important to take full advantage of encoder-decoder models. Xie et al. ("
I17-2044,P13-2001,0,0.0292238,"n encoder-decoder model and achieve higher BLEU score than that of baselines. We also investigated the oracle performance and revealed that there is sufficient room for improving an encoder-decoder model. 1 Introduction Text normalization is an important fundamental technology in actual natural language processing (NLP) systems to appropriately handle texts such as those for social media. This is because social media texts contain non-standard texts, such as typos, dialects, chat abbreviations1 , and emoticons; thus, current NLP systems often fail to correctly analyze such texts (Huang, 2015; Sajjad et al., 2013; Han et al., 2013). Normalization can help correctly analyze and understand these texts. One of the most promising conventional approaches for tackling text normalizing tasks is ∗ Present affiliation: Future Architect,Inc. short forms of words or phrases such as “4u” to represent “for you” 1 257 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 257–262, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP region. We propose two-level data-augmentation methods that do not use prior knowledge. The contributions of this study are summarized as"
I17-2044,P16-1009,0,0.0383275,"Nara Institute of Science and Technology {saito.itsumi, suzuki.jun, nishida.kyosuke}@lab.ntt.co.jp, {masumura.ryo, kobashikawa.satoshi, tomita.junji}@lab.ntt.co.jp matsu@is.naist.jp, k.sadamitsu.ic@future.co.jp Abstract using statistical machine translation (SMT) techniques (Junczys-Dowmunt and Grundkiewicz, 2016; Yuan and Briscoe, 2016), in particular, utilizing the Moses toolkit (Koehn et al., 2007). In recent years, encoder-decoder models with an attention mechanism (Bahdanau et al., 2014) have made great progress regarding many NLP tasks, including machine translation (Luong et al., 2015; Sennrich et al., 2016), text summarization (Rush et al., 2015) and text normalization (Xie et al., 2016; Yuan and Briscoe, 2016; Ikeda et al., 2017). We can also simply apply an encoder-decoder model to text normalization tasks. However, it is well-known that encoderdecoder models often fail to perform better than conventional methods when the availability of training data is insufficient. Unfortunately, the amount of training data for text normalization tasks is generally relatively small to sufficiently train encoder-decoder models. Therefore, data utilization and augmentation are important to take full advantage"
I17-2044,N16-1042,0,0.025244,"rmalization with Data Augmentation at Character- and Morphological Levels Itsumi Saito1 Jun Suzuki2 Kyosuke Nishida1 Kugatsu Sadamitsu1∗ Satoshi Kobashikawa1 Ryo Masumura1 Yuji Matsumoto3 Junji Tomita1 1 NTT Media Intelligence Laboratories, 2 NTT Communication Science Laboratories 3 Nara Institute of Science and Technology {saito.itsumi, suzuki.jun, nishida.kyosuke}@lab.ntt.co.jp, {masumura.ryo, kobashikawa.satoshi, tomita.junji}@lab.ntt.co.jp matsu@is.naist.jp, k.sadamitsu.ic@future.co.jp Abstract using statistical machine translation (SMT) techniques (Junczys-Dowmunt and Grundkiewicz, 2016; Yuan and Briscoe, 2016), in particular, utilizing the Moses toolkit (Koehn et al., 2007). In recent years, encoder-decoder models with an attention mechanism (Bahdanau et al., 2014) have made great progress regarding many NLP tasks, including machine translation (Luong et al., 2015; Sennrich et al., 2016), text summarization (Rush et al., 2015) and text normalization (Xie et al., 2016; Yuan and Briscoe, 2016; Ikeda et al., 2017). We can also simply apply an encoder-decoder model to text normalization tasks. However, it is well-known that encoderdecoder models often fail to perform better than conventional methods wh"
K16-1030,C12-1163,0,0.0187967,"semantic lexicon from a training corpus. These works focus on the pragmatic use of language, where the informativeness and lexicon of an utterance largely depends on the context (e.g. ‘Red’ is not valid to be used to refer to a blue ball). In this work, we apply RSA to predict the usage of DCs, which is more universal across different contexts (i.e. A DC can be used or dropped given various discourse senses and contexts). Our model is built upon the speaker’s model of RSA to predict speaker’s choice of explicit or implicit DCs. 2.2 and lexicons or psycholinguistic experiments. More recently, Asr and Demberg (2012) presents an analysis of the PDTB, showing that ‘causal’ and ‘continuous’ senses are more often implicit, or marked by less specific DCs. Indeed these senses are presupposed by listeners according to linguistics theories (Segal et al., 1991; Murray, 1997; Levinson, 2000; Sanders, 2005; Kuperberg et al., 2011). On the other hand, Asr and Demberg (2015) finds that DCs are more often dropped for the discourse relation Chosen Alternative (the relation typically signalled by the DC ‘instead’), if the context contains negation words, which are identified cues for this relation. Similarly, contextual"
K16-1030,W13-2610,0,0.2146,"are reported in attempts to train implicit DC classifiers based on explicit DC instances (Sporleder and Lascarides, 2008; Webber, 2009). Asr and Demberg (2012; 2015) attribute the corpus statistics to the UID hypothesis, which explains that expected, predictable relations are more likely to be conveyed implicitly, and thus more ambiguously, to maintain steady information flow. However, there are explicit ‘causal’ and ‘continuous’ relations and some Chosen Alternative are marked even argument 1 is negated. Although markedness measures are proposed to rate the implicitness of a relation sense (Asr and Demberg, 2013; Jin and de Marneffe, 2015), these measures only quantify the general markedness of the sense in the data, but not the speaker’s choice for each particular instance. In contrast, this work specifically measures the predictability of a given relation; generalizes the approach to all discourse senses instead of particular senses or cues; and combines the markedness preference with other language production factors, in order to model each instance of relation. Patterson and Kehler (2013) is the only study we are aware of that predicts the choice of explicit or implicit DCs of each instance of re"
K16-1030,W98-1414,0,0.112878,"s. The predictor measures how easily a candidate utterance can be predicted and the speaker adjusts information density based on the expected predictability. UID is applied to explain a variety of speaker’s options, such as phonetic (Aylett and Turk, 2004), morphological (Frank and Jaeger, 2008) and syntactic (Jaeger, 2010) reductions, and also referring expressions (Tily and Piantadosi, 2009). 2.3 Explicit vs. Implicit DCs The choice of discourse marking strategies has been studied in earlier works as a subtask for natural language generation (Scott and de Souza, 1990; Moser and Moore, 1995; Grote and Stede, 1998; Soria and Ferrari, 1998; Allbritton and Moore, 1999). In the absence of large-scale resources, investigations are based on manually derived rules 3 This is opposite to ‘informativeness’ in RSA, which is defined by negative surprisal (Equation 4). 304 bedded or shared, the previous discourse relation, argument lengths, and content word ratios. The classifier is trained and tested on a subset of relations from the PDTB, after screening away infrequent senses and DCs. An overall high classification accuracy is achieved. Relation-level and discourse-level features are found to be more useful tha"
K16-1030,W15-0205,0,0.0375864,"Missing"
K16-1030,W15-0117,0,0.188885,"A DC can be used or dropped given various discourse senses and contexts). Our model is built upon the speaker’s model of RSA to predict speaker’s choice of explicit or implicit DCs. 2.2 and lexicons or psycholinguistic experiments. More recently, Asr and Demberg (2012) presents an analysis of the PDTB, showing that ‘causal’ and ‘continuous’ senses are more often implicit, or marked by less specific DCs. Indeed these senses are presupposed by listeners according to linguistics theories (Segal et al., 1991; Murray, 1997; Levinson, 2000; Sanders, 2005; Kuperberg et al., 2011). On the other hand, Asr and Demberg (2015) finds that DCs are more often dropped for the discourse relation Chosen Alternative (the relation typically signalled by the DC ‘instead’), if the context contains negation words, which are identified cues for this relation. Similarly, contextual difference in explicit and implicit discourse relations are reported in attempts to train implicit DC classifiers based on explicit DC instances (Sporleder and Lascarides, 2008; Webber, 2009). Asr and Demberg (2012; 2015) attribute the corpus statistics to the UID hypothesis, which explains that expected, predictable relations are more likely to be c"
K16-1030,W15-2505,0,0.0324473,"Missing"
K16-1030,D15-1132,0,0.0241696,"Missing"
K16-1030,P02-1026,0,0.0853376,"tic or textual factors. A classifier is trained to predict whether a candidate DC (i.e. the DC that actually occurs in the text as an explicit DC, or annotated as an implicit DC) is actually present, given the sense of the discourse relation and the arguments. Relatively shallow linguistic features are used, such as whether the relations are emUniform Information Density The UID principle views language communication as a form of information transmission through a noisy channel and a constant rate of information flow is optimal according to Shannon’s Information Theory (Levy and Jaeger, 2006; Genzel and Charniak, 2002; Shannon, 1948). It states that speakers structure utterances by optimizing information density, which is the quantity of information (measured by surprisal3 ) transmitted per unit of utterance, such as word. Information density rises when the utterance is ‘surprising’ and drops when an utterance is highly predictable. To smooth the peaks and troughs, speakers adjust the ambiguity of an utterance by including or reducing linguistic markers. Following the UID principle, linguistic choices made by speakers are predicted more accurately by incorporating an information density predictor on top of"
K16-1030,D09-1036,0,0.0303188,"features in the arguments that signal a particular sense makes the sense more predictable, and thus promote the reduction of a DC. For example, the DC ‘instead’ is less used to present the Chosen Alternative sense if the first argument is negated (Asr and Demberg, 2015). Generalizing this idea to capture various cues in the arguments for various senses, we approximate I(s; arg, C) by the confidence of an automatic discourse parser in predicting the discourse sense. An implicit relation parser uses various features in the arguments to identify the implicit relation sense (Pitler et al., 2009; Lin et al., 2009; Park and Cardi, 2012; Rutherford and Xue, 2014). If the arguments contain much informative features, the parser will predict the sense more confidently. We propose two methods, for comparison, to measure the confidence of the parser prediction. A confident prediction means the parser will assign a high probability to the one output sense. Therefore, we use the negative surprisal of the estimated probability Pp of the parser output sense soutput (Equation 14) to approximate I(s; arg, C). (13) and that s/he will use an implicit DC otherwise. 3.2 Informativeness of arguments Informativeness of"
K16-1030,prasad-etal-2008-penn,0,0.0877943,"Missing"
K16-1030,P14-5010,0,0.00470031,"ge word length of all DCs. A lexicon of possible DC per each discourse sense is derived from the whole corpus. For multi-word DCs, a white space is simply counted The range of values of the cost function depends on the cost definition. We thus adjust the values with a constant weight wc that is tuned on the dev set in the experiments: D(exp) = wc · cost(exp) 8 The implicit DC classifier is trained by Na¨ıve Bayes based on features including syntactic features, polarity, immediately preceding DC, and Brown cluster pairs. Syntactic features are based on automatic parsing using Stanford CoreNLP (Manning et al., 2014). The parser is trained on the same sections of the PDTB as the training set used in our experiment. 9 http://www.cs.brandeis.edu/˜clp/ conll15st/results.html 10 We use the parser’s probability estimates as is; conceivably it may be improved by an additional probabilistic calibration step (Nguyen and O’Connor, 2015). 4 (16) Experiment We apply the model to simulate speaker’s choice of explicit or implicit DC for discourse relations in the PDTB corpus. The aim of the experiment is to answer two questions: (1) Does the model explain the factors affecting speaker’s choice of DC markedness? If the"
K16-1030,J14-4007,0,0.0519489,"it wasn’t enough. (WSJ0097) 3. Before (Explicit; Temporal-AsynchronousPrecedence) becoming a consultant in 1974, Mr. Achenbaum was a senior executive at J. Walter Thompson Co..(WSJ0295) Explicit DCs are labelled with relation senses (Example 1). If an explicit DC is absent between two sentences within the same paragraph and an implicit relation can be inferred, a candidate DC and the relation sense are annotated (Example 2). Our model is based on the assumption that W = {explicit, implicit} for all relations, yet it is notable that intra-sentential implicit DCs are not annotated in the PDTB (Prasad et al., 2014). We thus exclude intra-sentential samples, such that W = {explicit, implicit} is always true and free of grammatical constraints. Also, as a result of the annotation procedure, implicit DCs always occur in between 2 arguments in their original order, i.e. Arg1-DC-Arg2. To preserve the original order of the discourse arguments, which is also part of the communicative structure intended by the speaker but out of the scope of this model, we only use samples in the Arg1-DC-Arg2 order. For example, Example (3) is excluded from our training data. Finally, annotations of other forms of discourse rel"
K16-1030,W13-3303,0,0.0584378,"Missing"
K16-1030,E14-1068,0,0.0127307,"particular sense makes the sense more predictable, and thus promote the reduction of a DC. For example, the DC ‘instead’ is less used to present the Chosen Alternative sense if the first argument is negated (Asr and Demberg, 2015). Generalizing this idea to capture various cues in the arguments for various senses, we approximate I(s; arg, C) by the confidence of an automatic discourse parser in predicting the discourse sense. An implicit relation parser uses various features in the arguments to identify the implicit relation sense (Pitler et al., 2009; Lin et al., 2009; Park and Cardi, 2012; Rutherford and Xue, 2014). If the arguments contain much informative features, the parser will predict the sense more confidently. We propose two methods, for comparison, to measure the confidence of the parser prediction. A confident prediction means the parser will assign a high probability to the one output sense. Therefore, we use the negative surprisal of the estimated probability Pp of the parser output sense soutput (Equation 14) to approximate I(s; arg, C). (13) and that s/he will use an implicit DC otherwise. 3.2 Informativeness of arguments Informativeness of DCs This section explains how we estimate the inf"
K16-1030,P95-1018,0,0.407035,"top of other constraints. The predictor measures how easily a candidate utterance can be predicted and the speaker adjusts information density based on the expected predictability. UID is applied to explain a variety of speaker’s options, such as phonetic (Aylett and Turk, 2004), morphological (Frank and Jaeger, 2008) and syntactic (Jaeger, 2010) reductions, and also referring expressions (Tily and Piantadosi, 2009). 2.3 Explicit vs. Implicit DCs The choice of discourse marking strategies has been studied in earlier works as a subtask for natural language generation (Scott and de Souza, 1990; Moser and Moore, 1995; Grote and Stede, 1998; Soria and Ferrari, 1998; Allbritton and Moore, 1999). In the absence of large-scale resources, investigations are based on manually derived rules 3 This is opposite to ‘informativeness’ in RSA, which is defined by negative surprisal (Equation 4). 304 bedded or shared, the previous discourse relation, argument lengths, and content word ratios. The classifier is trained and tested on a subset of relations from the PDTB, after screening away infrequent senses and DCs. An overall high classification accuracy is achieved. Relation-level and discourse-level features are foun"
K16-1030,D15-1182,0,0.0605133,"Missing"
K16-1030,P15-1158,0,0.0258334,"Missing"
K16-1030,W12-1614,0,0.0166023,"rguments that signal a particular sense makes the sense more predictable, and thus promote the reduction of a DC. For example, the DC ‘instead’ is less used to present the Chosen Alternative sense if the first argument is negated (Asr and Demberg, 2015). Generalizing this idea to capture various cues in the arguments for various senses, we approximate I(s; arg, C) by the confidence of an automatic discourse parser in predicting the discourse sense. An implicit relation parser uses various features in the arguments to identify the implicit relation sense (Pitler et al., 2009; Lin et al., 2009; Park and Cardi, 2012; Rutherford and Xue, 2014). If the arguments contain much informative features, the parser will predict the sense more confidently. We propose two methods, for comparison, to measure the confidence of the parser prediction. A confident prediction means the parser will assign a high probability to the one output sense. Therefore, we use the negative surprisal of the estimated probability Pp of the parser output sense soutput (Equation 14) to approximate I(s; arg, C). (13) and that s/he will use an implicit DC otherwise. 3.2 Informativeness of arguments Informativeness of DCs This section expla"
K16-1030,D13-1094,0,0.0797956,"even argument 1 is negated. Although markedness measures are proposed to rate the implicitness of a relation sense (Asr and Demberg, 2013; Jin and de Marneffe, 2015), these measures only quantify the general markedness of the sense in the data, but not the speaker’s choice for each particular instance. In contrast, this work specifically measures the predictability of a given relation; generalizes the approach to all discourse senses instead of particular senses or cues; and combines the markedness preference with other language production factors, in order to model each instance of relation. Patterson and Kehler (2013) is the only study we are aware of that predicts the choice of explicit or implicit DCs of each instance of relation. They argue that while the decision is related to the ease to infer the relation, it may also depend on other stylistic or textual factors. A classifier is trained to predict whether a candidate DC (i.e. the DC that actually occurs in the text as an explicit DC, or annotated as an implicit DC) is actually present, given the sense of the discourse relation and the arguments. Relatively shallow linguistic features are used, such as whether the relations are emUniform Information D"
K16-1030,W98-0306,0,0.17836,"es how easily a candidate utterance can be predicted and the speaker adjusts information density based on the expected predictability. UID is applied to explain a variety of speaker’s options, such as phonetic (Aylett and Turk, 2004), morphological (Frank and Jaeger, 2008) and syntactic (Jaeger, 2010) reductions, and also referring expressions (Tily and Piantadosi, 2009). 2.3 Explicit vs. Implicit DCs The choice of discourse marking strategies has been studied in earlier works as a subtask for natural language generation (Scott and de Souza, 1990; Moser and Moore, 1995; Grote and Stede, 1998; Soria and Ferrari, 1998; Allbritton and Moore, 1999). In the absence of large-scale resources, investigations are based on manually derived rules 3 This is opposite to ‘informativeness’ in RSA, which is defined by negative surprisal (Equation 4). 304 bedded or shared, the previous discourse relation, argument lengths, and content word ratios. The classifier is trained and tested on a subset of relations from the PDTB, after screening away infrequent senses and DCs. An overall high classification accuracy is achieved. Relation-level and discourse-level features are found to be more useful than argument-level features"
K16-1030,C08-2022,0,0.0318175,"s stateof-the-art approaches, while giving an explanatory account of the speaker’s choice. 1 1. It was a great movie, but I did not like it. 2. It was a great movie, therefore I liked it. 3. It was a great movie. I liked it. The word ‘but’ indicates a Concession relation in Example (1), and ‘therefore’ indicates a Result relation in Example (2). We call ‘but’ and ‘therefore’ explicit discourse connectives (DCs). In Example (3), DCs are absent but a Result relation can be inferred. We say the DC is implicit in this case. Explicit DCs are highly informative cues to identify discourse relations (Pitler et al., 2008) while implicit DCs are more ambiguous. For example, ‘I liked it’ can also be read as a Justification for the first sentence in Example (3). Marking a discourse relation or not is subject to ambiguity and redundancy. On one hand, using an explicit DC avoids ambiguity. For example, if the DC ‘but’ is omitted in Example (1), readers may have problems in inferring the Concession sense. On the other hand, if the intended discourse sense is highly predictable, it is verbose or redundant to insert an explicit DC in the utterance, such as the DC ‘therefore’ in Example (2). Introduction Speakers or au"
K16-1030,P09-1077,0,0.0298011,"referred. Presence of features in the arguments that signal a particular sense makes the sense more predictable, and thus promote the reduction of a DC. For example, the DC ‘instead’ is less used to present the Chosen Alternative sense if the first argument is negated (Asr and Demberg, 2015). Generalizing this idea to capture various cues in the arguments for various senses, we approximate I(s; arg, C) by the confidence of an automatic discourse parser in predicting the discourse sense. An implicit relation parser uses various features in the arguments to identify the implicit relation sense (Pitler et al., 2009; Lin et al., 2009; Park and Cardi, 2012; Rutherford and Xue, 2014). If the arguments contain much informative features, the parser will predict the sense more confidently. We propose two methods, for comparison, to measure the confidence of the parser prediction. A confident prediction means the parser will assign a high probability to the one output sense. Therefore, we use the negative surprisal of the estimated probability Pp of the parser output sense soutput (Equation 14) to approximate I(s; arg, C). (13) and that s/he will use an implicit DC otherwise. 3.2 Informativeness of arguments I"
K16-1030,tonelli-etal-2010-annotation,0,0.028255,"p forward to formalize the idea of the UID theory, that redundant explicit markers are avoided if the discourse relation is clear enough from the context. As future work, we plan to improve the markedness model by making fuller use of the training data, such as learning a more expressive formulation of the context governing the choice of explicit or implicit DCs. We also plan to evaluate the effectiveness of the model in applications, such as natural language generation or machine translation tasks. On the other hand, as discourse presentation differs across genres (Webber, 2009) and mediums (Tonelli et al., 2010), the model can be applied to predict the explicitation of discourse relations from, for example, news articles to spoken dialogues. Another direction is to apply the RSA framework in the opposite direction - to build a listener’s model that simulates a listener’s recognition of a discourse sense given an utterance, as proposed in Yung et al.(2016). Acknowledgments We thank the anonymous reviewers for their valuable feedback on the previous versions of this paper. References 14 When extracting the argument informativeness features from the training set, using the automatic discourse parser, we"
K16-1030,K15-2002,0,0.0283782,"Missing"
K16-1030,P09-1076,0,0.0982275,"steners according to linguistics theories (Segal et al., 1991; Murray, 1997; Levinson, 2000; Sanders, 2005; Kuperberg et al., 2011). On the other hand, Asr and Demberg (2015) finds that DCs are more often dropped for the discourse relation Chosen Alternative (the relation typically signalled by the DC ‘instead’), if the context contains negation words, which are identified cues for this relation. Similarly, contextual difference in explicit and implicit discourse relations are reported in attempts to train implicit DC classifiers based on explicit DC instances (Sporleder and Lascarides, 2008; Webber, 2009). Asr and Demberg (2012; 2015) attribute the corpus statistics to the UID hypothesis, which explains that expected, predictable relations are more likely to be conveyed implicitly, and thus more ambiguously, to maintain steady information flow. However, there are explicit ‘causal’ and ‘continuous’ relations and some Chosen Alternative are marked even argument 1 is negated. Although markedness measures are proposed to rate the implicitness of a relation sense (Asr and Demberg, 2013; Jin and de Marneffe, 2015), these measures only quantify the general markedness of the sense in the data, but not"
K16-1030,W15-2519,1,0.866134,"Missing"
K16-1030,P16-2086,1,0.876576,"Missing"
K17-1042,E17-2026,0,0.115622,"ward directional LSTM that reads the sequence from beginning to end with the output vector of the backward directional LSTM that reads the sequence in the reverse direction. cas:n cas:na OUT(cas) OUT(cas) 3.3 LSTM rt Our baseline model does not share any information between morphosyntactic prediction tasks, as it is trained separately. However, it is beneficial to utilize information from other morphosyntactic categories when predicting a label for one category. In order to do this, we adopt a multi-task learning approach (Collobert et al., 2011; Yang et al., 2016; Søgaard and Goldberg, 2016; Bingel and Søgaard, 2017; Mart´ınez Alonso and Plank, 2017). Specifically, we use parameter sharing in the hidden layers of our bi-LSTM model so that we can generate a unified model that can carry information beneficial to each task. LSTM LSTM LSTM wt rt+1 ct Hb (“love”) wt+1 ct+1 fy (“in”) ct Concat LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM Character Lookup Table <w&gt; H b Joint Prediction Model gen:m pos:noun cas:n </w&gt; ••• Hb (“love”) ••• OUT(pos) ••• OUT(cas) ••• OUT(gen) Figure 1: Top: Our baseline model for the category “cas”. We have one model for each category, resulting in 14 models in total. Bottom: How to crea"
K17-1042,N04-4038,0,0.376178,"POS tagging, Mueller et al. (2013) presented an approximated higher-order CRF for morphosyntactic tagging across six languages, assuming gold clitic segmentation. Pasha et al. (2014) used an analyze-anddisambiguate approach, in which they ranked the possible analyses provided by a morphological analyzer for each space-delimited word. The stateof-the-art tagger (Shahrour et al., 2015) extended their model by adjusting the outputs of Pasha et al.’s tagger by utilizing case-state classifiers that incorporate additional syntactic information provided by a dependency parser and hand-written rules. Diab et al. (2004) proposed a segmentationbased approach, in which they tag each cliticsegmented token using SVMs. Mohamed and K¨ubler (2010) proposed a word-based approach which takes space-delimited words as inputs and uses memory-based learning. Their experiment showed that the word-based approach performed better than the segmentation-based approach, avoiding segmentation error propagation. Zhang et al. (2015) proposed joint modeling of segmentation, POS tagging, and dependency parsing using a randomized greedy algorithm. The aforementioned studies were focused on tagging 428 Compared to their approaches, o"
K17-1042,E17-1005,0,0.089899,"sequence from beginning to end with the output vector of the backward directional LSTM that reads the sequence in the reverse direction. cas:n cas:na OUT(cas) OUT(cas) 3.3 LSTM rt Our baseline model does not share any information between morphosyntactic prediction tasks, as it is trained separately. However, it is beneficial to utilize information from other morphosyntactic categories when predicting a label for one category. In order to do this, we adopt a multi-task learning approach (Collobert et al., 2011; Yang et al., 2016; Søgaard and Goldberg, 2016; Bingel and Søgaard, 2017; Mart´ınez Alonso and Plank, 2017). Specifically, we use parameter sharing in the hidden layers of our bi-LSTM model so that we can generate a unified model that can carry information beneficial to each task. LSTM LSTM LSTM wt rt+1 ct Hb (“love”) wt+1 ct+1 fy (“in”) ct Concat LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM Character Lookup Table <w&gt; H b Joint Prediction Model gen:m pos:noun cas:n </w&gt; ••• Hb (“love”) ••• OUT(pos) ••• OUT(cas) ••• OUT(gen) Figure 1: Top: Our baseline model for the category “cas”. We have one model for each category, resulting in 14 models in total. Bottom: How to create character-level embeddings. <w&gt;"
K17-1042,P05-1071,0,0.425079,"logy 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan {inoue.go.ib4, shindo, matsu}@is.naist.jp Abstract Part-of-speech (POS) tagging is a fundamental task in natural language processing. The granularity of the POS tag set that reflects languagespecific information varies from language to language. In morphologically simple languages such as English, the size of the tag set is typically less than a hundred. On the other hand, in morphologically rich languages such as Arabic, the number of theoretically possible tags can be up to 333,000, of which only 2,200 tags might appear in an actual corpus (Habash and Rambow, 2005). One reason for this is that in the tagging scheme for such languages, a complete POS tag is formed by combining tags from multiple tag sets defined for each morphosyntactic category. For example, a complete POS tag for the word Hb (“love”)2 can be defined as the combination of a noun from the coarse POS category, a nominative (n) from the case category, “not applicable” (na) from the mood category, and so on. The enormous number of resulting tags causes fine-grained POS tagging for Arabic to be more challenging. In order to perform this task, it is beneficial to utilize information from othe"
K17-1042,N10-1105,0,0.0500973,"Missing"
K17-1042,L16-1681,0,0.0365912,"Missing"
K17-1042,D13-1032,0,0.102368,"Missing"
K17-1042,C16-2047,0,0.171353,"Missing"
K17-1042,P16-2038,0,0.246909,"the output vector of the forward directional LSTM that reads the sequence from beginning to end with the output vector of the backward directional LSTM that reads the sequence in the reverse direction. cas:n cas:na OUT(cas) OUT(cas) 3.3 LSTM rt Our baseline model does not share any information between morphosyntactic prediction tasks, as it is trained separately. However, it is beneficial to utilize information from other morphosyntactic categories when predicting a label for one category. In order to do this, we adopt a multi-task learning approach (Collobert et al., 2011; Yang et al., 2016; Søgaard and Goldberg, 2016; Bingel and Søgaard, 2017; Mart´ınez Alonso and Plank, 2017). Specifically, we use parameter sharing in the hidden layers of our bi-LSTM model so that we can generate a unified model that can carry information beneficial to each task. LSTM LSTM LSTM wt rt+1 ct Hb (“love”) wt+1 ct+1 fy (“in”) ct Concat LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM Character Lookup Table <w&gt; H b Joint Prediction Model gen:m pos:noun cas:n </w&gt; ••• Hb (“love”) ••• OUT(pos) ••• OUT(cas) ••• OUT(gen) Figure 1: Top: Our baseline model for the category “cas”. We have one model for each category, resulting in 14 models in"
K17-1042,N15-1005,0,0.0263409,"eir model by adjusting the outputs of Pasha et al.’s tagger by utilizing case-state classifiers that incorporate additional syntactic information provided by a dependency parser and hand-written rules. Diab et al. (2004) proposed a segmentationbased approach, in which they tag each cliticsegmented token using SVMs. Mohamed and K¨ubler (2010) proposed a word-based approach which takes space-delimited words as inputs and uses memory-based learning. Their experiment showed that the word-based approach performed better than the segmentation-based approach, avoiding segmentation error propagation. Zhang et al. (2015) proposed joint modeling of segmentation, POS tagging, and dependency parsing using a randomized greedy algorithm. The aforementioned studies were focused on tagging 428 Compared to their approaches, our model is simple but powerful: It does not assume gold clitic segmentation, since segmentation is also modeled as part of the morphosyntactic categories, nor does it require the additional pipeline process of syntactic parsing. Nonetheless, it is more accurate than the current state-of-the-art. Another related line of work tackles sequential labeling problems using multi-task learning with deep"
K17-1042,pasha-etal-2014-madamira,0,0.366943,"Missing"
K17-1042,P16-2067,0,0.0337,"ory “cas”. We have one model for each category, resulting in 14 models in total. Bottom: How to create character-level embeddings. <w&gt; and </w&gt; indicates the beginning and the end of a word. cas:na ••• LSTM rt Independent Prediction Model gen:na ••• OUT(pos) ••• OUT(cas) ••• OUT(gen) LSTM LSTM LSTM !t ct Hb (“love”) 3.2 pos:prep rt+1 !t+1 ct+1 fy (“in”) Figure 2: Multi-task bi-directional LSTM model for fine-grained Arabic POS tagging. For our baseline method, we use a model that independently predicts each morphosyntactic category using bi-LSTMs. Our baseline is similar to the basic model in Plank et al. (2016). The top part of Figure 1 illustrates an overview of our baseline model. Given a sequence of n words x1:n , we encode each word xt into a vector represenFigure 2 shows an overview of our joint model. The output vectors of the bi-LSTMs are fed into multiple output layers, each performing a corresponding morphosyntactic prediction task. Our model trains to minimize the cross-entropy loss 423 L(ˆ y f ine , y f ine ) = ••• 1 X L(ˆ ym , ym ) |M |m∈M (cas) ; ... ; dt (gen) ; ... ; dt cas:na ••• OUT(pos) ••• OUT(cas) ••• OUT(gen) LSTM LSTM z ,t rt gen:na ••• LSTM ct rt+1 dt Hb (“love”) One of our co"
K17-1042,D15-1152,0,0.222371,"ing for Arabic to be more challenging. In order to perform this task, it is beneficial to utilize information from other morphosyntactic categories when predicting a label for one category. For example, if a word is a noun, it should take one of three tags from the case category: nominative (n), accusative (a), or genitive (g), while it should take “not applicable” (na) from the mood category since mood is not defined for nominals. However, most of the previous approaches in Arabic did not utilize this information, applying one model for each task (Habash and Rambow, 2005; Pasha et al., 2014; Shahrour et al., 2015). To make use of this information, we propose an approach that jointly models multiple morphosyntactic prediction tasks using a multi-task learning scheme. Specifically, we adopt parameter sharing in our bi-directional LSTM model in the hope that the shared parameters will store information beneficial to multiple tasks. To further boost the performance, we propose a method of incorporating tag dictionary information into our neural models by combining word representations with representations of the sets of possible tags. Our experiments showed that the joint model 1 Our code is available at h"
K17-3007,K17-3001,0,0.0583893,"Missing"
K17-3007,Q16-1023,0,0.0762394,"arc according to si,j , in which the k-th element corresponds to the score of k-th label: (label−head) hi (label−dep) hj (label) hi,j Biaffine Attention model (label) si,j = MLP(label−head) (ri ), = MLP(label−dep) (rj ), (label−head) = hi = (label−dep) ⊕ hj , T(label−head) (label) (label−dep) hi U hj T(label) +hi,j W (label) + u(label) , Our baseline model is the biaffine attention model (Dozat and Manning, 2017), which is an extension to the recently proposed dependency parsing method calculating the score of each arc independently from the representations of two tokens obtained by Bi-LSTMs (Kiperwasser and Goldberg, 2016). For labeled dependency parsing, this model 4 Domain Adaptation Techniques with Adversarial Training 1 We were not aware of the jack-knifed training data provided by the organizer at submission time. Here we describe our network architectures for domain adaptation. We present two different netwhere U(label) is a third-order tensor, W (label) is a weight matrix, and u(label) is a bias vector. 72 Shared LSTMs Biaffine Biaffine Biaffine Predict Biaffine Biaffine Domain 1 Domain 1 MLP Domain 2 Bi−LSTMs Domain 2 Input embedding x1 x2 Gradient Reversal Layer x3 Figure 1: Overview of the biaffine mo"
K17-3007,L16-1680,0,0.0872786,"Missing"
L16-1261,W08-1301,0,0.258594,"Missing"
L16-1261,de-marneffe-etal-2014-universal,0,0.14179,"Missing"
L16-1261,den-etal-2008-proper,0,0.0441224,"Missing"
L16-1261,N06-1023,0,0.0301,"ersal part-of-speech (POS) tags (UPOS) (Petrov et al., 2012). In our research, we attempt to port the UD annotation scheme to the Japanese language. The traditional annotation schemes for the Japanese language have been uniquely developed and are markedly different from other schemes, such as Penn Treebank-style annotation. Japanese syntactic parsing trees are usually represented as unlabeled dependency structures between bunsetsu chunks (base phrase units), as found in the Kyoto University Text Corpus (Kurohashi and Nagao, 2003) and the outputs of syntactic parsers (Kudo and Matsumoto, 2002; Kawahara and Kurohashi, 2006). Therefore, we must devise a method to construct word-based dependency structures that match the characteristics of the Japanese language (Uchimoto and Den, 2008; Mori et al., 2014; Tanaka and Nagata, 2015) and are able to derive the syntactic information required to assign relation types to dependencies. We describe the conversion from the Japanese POS tagset to the UPOS tagset, the adaptation of the UD annotation for Japanese syntax, and the attempt to build a UD corpus by converting the existing resources. We also address the remaining issues that may emerge when applying the UD scheme to"
L16-1261,W02-2016,1,0.600528,"al., 2014) and Google universal part-of-speech (POS) tags (UPOS) (Petrov et al., 2012). In our research, we attempt to port the UD annotation scheme to the Japanese language. The traditional annotation schemes for the Japanese language have been uniquely developed and are markedly different from other schemes, such as Penn Treebank-style annotation. Japanese syntactic parsing trees are usually represented as unlabeled dependency structures between bunsetsu chunks (base phrase units), as found in the Kyoto University Text Corpus (Kurohashi and Nagao, 2003) and the outputs of syntactic parsers (Kudo and Matsumoto, 2002; Kawahara and Kurohashi, 2006). Therefore, we must devise a method to construct word-based dependency structures that match the characteristics of the Japanese language (Uchimoto and Den, 2008; Mori et al., 2014; Tanaka and Nagata, 2015) and are able to derive the syntactic information required to assign relation types to dependencies. We describe the conversion from the Japanese POS tagset to the UPOS tagset, the adaptation of the UD annotation for Japanese syntax, and the attempt to build a UD corpus by converting the existing resources. We also address the remaining issues that may emerge"
L16-1261,W04-3230,1,0.799406,"Missing"
L16-1261,maekawa-etal-2000-spontaneous,0,0.265949,"Missing"
L16-1261,mori-etal-2014-japanese,1,0.826748,"apanese language have been uniquely developed and are markedly different from other schemes, such as Penn Treebank-style annotation. Japanese syntactic parsing trees are usually represented as unlabeled dependency structures between bunsetsu chunks (base phrase units), as found in the Kyoto University Text Corpus (Kurohashi and Nagao, 2003) and the outputs of syntactic parsers (Kudo and Matsumoto, 2002; Kawahara and Kurohashi, 2006). Therefore, we must devise a method to construct word-based dependency structures that match the characteristics of the Japanese language (Uchimoto and Den, 2008; Mori et al., 2014; Tanaka and Nagata, 2015) and are able to derive the syntactic information required to assign relation types to dependencies. We describe the conversion from the Japanese POS tagset to the UPOS tagset, the adaptation of the UD annotation for Japanese syntax, and the attempt to build a UD corpus by converting the existing resources. We also address the remaining issues that may emerge when applying the UD scheme to other languages. Word unit The definition of a word unit is indispensable in UD annotation, which is not a trivial question for Japanese, since a sentence is not segmented into word"
L16-1261,P11-2093,1,0.783083,"e adaptation of the UD annotation for Japanese syntax, and the attempt to build a UD corpus by converting the existing resources. We also address the remaining issues that may emerge when applying the UD scheme to other languages. Word unit The definition of a word unit is indispensable in UD annotation, which is not a trivial question for Japanese, since a sentence is not segmented into words or morphemes by white space in its orthography. Thus, we have several word unit standards that can be found in corpus annotation schemata or in the outputs of morphological analyzers (Kudo et al., 2004; Neubig et al., 2011). NINJAL1 proposed several word unit standards for Japanese corpus linguistics, such as the minimum word unit (Maekawa et al., 2000). Since 2002, the Institute has maintained a morphological information annotated lexicon, UniDic (Den et al., 2008), and has proposed three types of word unit standards: Short Unit Word (SUW): SUW is a minimal language unit that has a morphological function. SUW almost always corresponds to an entry in traditional Japanese dictionaries. Middle Unit Word (MUW): MUW is based on the rightbranching compound word construction and on phonological constructions, such as"
L16-1261,petrov-etal-2012-universal,0,0.0821258,". Keywords: typed dependencies, Short Unit Word, multiword expression, UniDic 1. Introduction 2. The Universal Dependencies (UD) project has been developing cross-linguistically consistent treebank annotation for various languages in recent years. The goal of the project is to facilitate multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective (Nivre, 2015). The annotation scheme is based on (universal) Stanford dependencies (de Marneffe and Manning, 2008; de Marneffe et al., 2014) and Google universal part-of-speech (POS) tags (UPOS) (Petrov et al., 2012). In our research, we attempt to port the UD annotation scheme to the Japanese language. The traditional annotation schemes for the Japanese language have been uniquely developed and are markedly different from other schemes, such as Penn Treebank-style annotation. Japanese syntactic parsing trees are usually represented as unlabeled dependency structures between bunsetsu chunks (base phrase units), as found in the Kyoto University Text Corpus (Kurohashi and Nagao, 2003) and the outputs of syntactic parsers (Kudo and Matsumoto, 2002; Kawahara and Kurohashi, 2006). Therefore, we must devise a m"
L16-1261,W13-4913,1,0.86233,"(10) 太郎 . NOUN . Taro . cop は . ADP . -TOPIC . 学生 . . NOUN . student . だ . AUX . COPULA . ‘Taro is a student.’ 5. Corpus It is reasonable to obtain Japanese UD corpora by converting existent linguistic resources; however, a direct conversion from the major Japanese corpora such as the Kyoto University Text Corpus (Kurohashi and Nagao, 2003) is not simple since they lack syntactic information (unlabeled) and the structure is not suitable to recover constituents (bunsetsu chunk-based dependency trees). Therefore, we first constructed conversion rules for use with Japanese constituent treebank (Tanaka and Nagata, 2013) 1655 for the Mainichi Shimbun Newspaper. The treebank was initially built by converting the Kyoto University Text Corpus and was manually annotated. The treebank has clause level annotations with syntactic function labels, e.g., syntactic role and clause type, and coordination construction, which are required for UD annotation. The treebank is composed of complete binary trees, and can be easily converted to dependency tree by adapting the head percolation rules and dependency type rules for each partial tree. The UD corpus is composed of 10,000 sentences, and it contains 267,631 tokens. The"
L16-1261,P15-2039,1,0.532985,"ve been uniquely developed and are markedly different from other schemes, such as Penn Treebank-style annotation. Japanese syntactic parsing trees are usually represented as unlabeled dependency structures between bunsetsu chunks (base phrase units), as found in the Kyoto University Text Corpus (Kurohashi and Nagao, 2003) and the outputs of syntactic parsers (Kudo and Matsumoto, 2002; Kawahara and Kurohashi, 2006). Therefore, we must devise a method to construct word-based dependency structures that match the characteristics of the Japanese language (Uchimoto and Den, 2008; Mori et al., 2014; Tanaka and Nagata, 2015) and are able to derive the syntactic information required to assign relation types to dependencies. We describe the conversion from the Japanese POS tagset to the UPOS tagset, the adaptation of the UD annotation for Japanese syntax, and the attempt to build a UD corpus by converting the existing resources. We also address the remaining issues that may emerge when applying the UD scheme to other languages. Word unit The definition of a word unit is indispensable in UD annotation, which is not a trivial question for Japanese, since a sentence is not segmented into words or morphemes by white sp"
L16-1261,uchimoto-den-2008-word,0,0.0247262,"tation schemes for the Japanese language have been uniquely developed and are markedly different from other schemes, such as Penn Treebank-style annotation. Japanese syntactic parsing trees are usually represented as unlabeled dependency structures between bunsetsu chunks (base phrase units), as found in the Kyoto University Text Corpus (Kurohashi and Nagao, 2003) and the outputs of syntactic parsers (Kudo and Matsumoto, 2002; Kawahara and Kurohashi, 2006). Therefore, we must devise a method to construct word-based dependency structures that match the characteristics of the Japanese language (Uchimoto and Den, 2008; Mori et al., 2014; Tanaka and Nagata, 2015) and are able to derive the syntactic information required to assign relation types to dependencies. We describe the conversion from the Japanese POS tagset to the UPOS tagset, the adaptation of the UD annotation for Japanese syntax, and the attempt to build a UD corpus by converting the existing resources. We also address the remaining issues that may emerge when applying the UD scheme to other languages. Word unit The definition of a word unit is indispensable in UD annotation, which is not a trivial question for Japanese, since a sentence is not"
L16-1263,P14-1070,0,0.394582,"single token improves accuracy for various NLP tasks, such as dependency parsing (Nivre and Nilsson, 2004), and constituency parsing (Arun and Keller, 2005). For syntactic parsing that takes in MWEs, it is preferable that the information of an MWE (e.g., part of speech and span of tokens) be integrated into a corpus, such as a phrase or dependency treebank, because an MWE should be a syntactic unit. Actually, an MWE is grouped under “subtree” in French Treebank (Abeill´e et al., 2003), which is often used in research focusing on both MWE recognition and syntactic parsing (Green et al., 2011; Candito and Constant, 2014). However, MWEs are not annotated in Penn Treebank, the standard corpus of English syntactic parsing. In dependency structure that takes MWEs into consideration (i.e., MWE-aware dependency), each MWE becomes a single node. Therefore, to convert word-based dependency to MWE-aware dependency directly, one could combine nodes in an MWE into a single node. Nevertheless, this method often leads to the following problem: A node derived from an MWE could have multiple heads and the whole dependency structure including MWE might be cyclic (Figure 1). This is mainly because Penn Treebank style annotati"
L16-1263,W08-1301,0,0.20658,"Missing"
L16-1263,N09-1037,0,0.0258983,"structure tree. For example, “even though” in Figure 3a is annotated as an MWE in Shigeto et al. (2013). We convert it as in Figure 3b. If we can convert the span of an MWE into a single subtree without influencing the structures of other subtrees, we call this instance “Simple”. Otherwise the instance is “Complex”. When grouping MWE, we focus on the LCA-tree, which is the subtree rooted in the Least Common Ancestor (LCA) of the components of the MWE. In Figure 3a, the tree rooted in the LCA of “even” and “though” (LCA here being SBAR) is the LCA-tree. The method we described above relates to Finkel and Manning (2009). For joint parsing and named entity recognition, they classified named entities which do not correspond to a phrase in the constituency tree to the following two categories. A named entity belonging to the first category is contiguous multiple children of some nonterminal. This category corresponds to the above “Simple” case. On the other hand, A span of each named entity belonging to the second category crosses brackets in the parse tree. It corresponds to the above “Complex” case. 3.1. Simple Case In the “Simple” case, we insert a new internal node under the LCA (Figure 3a → Figure 3b). Thi"
L16-1263,D11-1067,0,0.200124,"bining an MWE into a single token improves accuracy for various NLP tasks, such as dependency parsing (Nivre and Nilsson, 2004), and constituency parsing (Arun and Keller, 2005). For syntactic parsing that takes in MWEs, it is preferable that the information of an MWE (e.g., part of speech and span of tokens) be integrated into a corpus, such as a phrase or dependency treebank, because an MWE should be a syntactic unit. Actually, an MWE is grouped under “subtree” in French Treebank (Abeill´e et al., 2003), which is often used in research focusing on both MWE recognition and syntactic parsing (Green et al., 2011; Candito and Constant, 2014). However, MWEs are not annotated in Penn Treebank, the standard corpus of English syntactic parsing. In dependency structure that takes MWEs into consideration (i.e., MWE-aware dependency), each MWE becomes a single node. Therefore, to convert word-based dependency to MWE-aware dependency directly, one could combine nodes in an MWE into a single node. Nevertheless, this method often leads to the following problem: A node derived from an MWE could have multiple heads and the whole dependency structure including MWE might be cyclic (Figure 1). This is mainly because"
L16-1263,H05-1066,0,0.11424,"Missing"
L16-1263,P13-2017,0,0.158907,"Missing"
L16-1263,W13-1021,1,\N,Missing
L18-1147,abdelali-etal-2014-amara,0,0.0352532,"Missing"
L18-1147,C10-2010,0,0.0673981,"Missing"
L18-1147,2012.eamt-1.60,0,0.0339438,"future work, we will explore sentence alignment methods to improve the quality of our corpus. We also plan to explore MT techniques for under-resourced language pairs such as pivoting, and domain adaptation from better resourced domains. 6. 4. Related Work Much work has been done on building multilingual parallel corpora which include the language pair of Arabic and Japanese. Table 8 summarizes the statistics of publicly available parallel corpora of this language pair. Lison and Tiedemann (2016) presents the largest corpus, in which they collected movie and TV subtitles from OpenSubtitles.11 Cettolo and Girardi (2012) constructed a parallel corpus that consists of transcribed and translated TED talks. Abdelali et al. (2014) developed the AMARA corpus 11 Acknowledgements This work has been partially funded by the Tobitate! (Leap for Tomorrow) Young Ambassador Program. Part of the work was done during the first author’s visit to New York University Abu Dhabi. The creation of translated articles has been carried out at Tokyo University of Foreign Studies. We are grateful to the contributors of the TUFS Media Project for providing translated articles. We thank the anonymous reviewers, Nasser Zalmout, Alexander"
L18-1147,W08-0509,0,0.0219614,"as approximately 3.4 million. This is much closer to the number of Japanese tokens, 3.7 million. Documents 4,253 1,854 811 608 330 280 244 194 48 15 10 5 8,652 Percentage 49.16 21.43 9.37 7.03 3.81 3.24 2.82 2.24 0.55 0.17 0.12 0.06 100.00 Table 4: Category distribution of our entire corpus. 3. Machine Translation Baselines In this section, we present the baseline results of phrasebased MT from Arabic to Japanese. 3.1. Experimental Settings Phrase-based MT Settings We use the Moses toolkit (Koehn et al., 2007) to build a standard phrasebased MT system. Word alignment was extracted by MGIZA++ (Gao and Vogel, 2008) with a maximum phrase size of 8. We use the grow-diag-final-and and msdbidirectional-fe options for symmetrization and reordering. We train a 5-gram language model on the target side of the training set using KenLM (Heafield, 2011). We use MERT (Och, 2003) for decoding weight optimization. Data and Preprocessing We use the manually aligned data described in Section 2.2.4. for tuning and testing, and the automatically aligned data using Gargantua described in Section 2.2.5. for training. We tokenize Arabic data using the MADAMIRA toolkit (Pasha et al., 2014) with six tokenization schemes (D0,"
L18-1147,W11-2123,0,0.0365625,"00 Table 4: Category distribution of our entire corpus. 3. Machine Translation Baselines In this section, we present the baseline results of phrasebased MT from Arabic to Japanese. 3.1. Experimental Settings Phrase-based MT Settings We use the Moses toolkit (Koehn et al., 2007) to build a standard phrasebased MT system. Word alignment was extracted by MGIZA++ (Gao and Vogel, 2008) with a maximum phrase size of 8. We use the grow-diag-final-and and msdbidirectional-fe options for symmetrization and reordering. We train a 5-gram language model on the target side of the training set using KenLM (Heafield, 2011). We use MERT (Och, 2003) for decoding weight optimization. Data and Preprocessing We use the manually aligned data described in Section 2.2.4. for tuning and testing, and the automatically aligned data using Gargantua described in Section 2.2.5. for training. We tokenize Arabic data using the MADAMIRA toolkit (Pasha et al., 2014) with six tokenization schemes (D0, D1, D2, D3, D3*, and ATB) following Zalmout and Habash (2017). Examples of the six tokenization schemes are shown in Table 1. We normalize Japanese texts using the NFKC normalization and tokenize them using the MeCab morphological a"
L18-1147,D10-1092,0,0.0227262,"e 1. We normalize Japanese texts using the NFKC normalization and tokenize them using the MeCab morphological analyzer (0.996) (Kudo, 2005) with IPAdic. We eliminate long sentences with more than 100 words using the script clean-corpus-n.perl before training translation models. Table 6 shows statistics of training data after cleaning. Evaluation Before evaluating, we de-tokenize the predicted output by deleting spaces between Japanese characters, and then re-tokenize them using MeCab with IPAdic. We calculate automatic evaluation scores for two metrics: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010). We use the multi-bleu.perl script in the Moses toolkit to compute BLEU scores. We calculate RIBES scores using the RIBES.py (1.03.1.).10 10 http://www.kecl.ntt.co.jp/icl/lirg/ ribes/ 921 dev-tune dev-test blind-test train Total Documents 100 400 400 7,752 8,652 Sentences 621 2,393 2,236 59,238 64,488 Tokens (ar) 23,312 92,760 85,940 2,175,438 2,377,460 Tokens (ja) 36,595 147,536 144,358 3,403,244 3,731,733 Table 5: The basic statistics of our parallel corpus. Sentences in the training set are aligned using Gargantua. Tokenization D0 D1 D2 ATB D3 D3* Sentences 54,223 54,123 54,029 53,933 53,1"
L18-1147,P07-2045,0,0.00901545,"e to Japanese tokens. We ran MADAMIRA on our corpus to obtain the number of D3-tokenized tokens, which was approximately 3.4 million. This is much closer to the number of Japanese tokens, 3.7 million. Documents 4,253 1,854 811 608 330 280 244 194 48 15 10 5 8,652 Percentage 49.16 21.43 9.37 7.03 3.81 3.24 2.82 2.24 0.55 0.17 0.12 0.06 100.00 Table 4: Category distribution of our entire corpus. 3. Machine Translation Baselines In this section, we present the baseline results of phrasebased MT from Arabic to Japanese. 3.1. Experimental Settings Phrase-based MT Settings We use the Moses toolkit (Koehn et al., 2007) to build a standard phrasebased MT system. Word alignment was extracted by MGIZA++ (Gao and Vogel, 2008) with a maximum phrase size of 8. We use the grow-diag-final-and and msdbidirectional-fe options for symmetrization and reordering. We train a 5-gram language model on the target side of the training set using KenLM (Heafield, 2011). We use MERT (Och, 2003) for decoding weight optimization. Data and Preprocessing We use the manually aligned data described in Section 2.2.4. for tuning and testing, and the automatically aligned data using Gargantua described in Section 2.2.5. for training. We"
L18-1147,L16-1147,0,0.0198438,"3.7 million Japanese tokens. We also reported the first results of Arabic–Japanese phrase-based MT trained on our corpus. As future work, we will explore sentence alignment methods to improve the quality of our corpus. We also plan to explore MT techniques for under-resourced language pairs such as pivoting, and domain adaptation from better resourced domains. 6. 4. Related Work Much work has been done on building multilingual parallel corpora which include the language pair of Arabic and Japanese. Table 8 summarizes the statistics of publicly available parallel corpora of this language pair. Lison and Tiedemann (2016) presents the largest corpus, in which they collected movie and TV subtitles from OpenSubtitles.11 Cettolo and Girardi (2012) constructed a parallel corpus that consists of transcribed and translated TED talks. Abdelali et al. (2014) developed the AMARA corpus 11 Acknowledgements This work has been partially funded by the Tobitate! (Leap for Tomorrow) Young Ambassador Program. Part of the work was done during the first author’s visit to New York University Abu Dhabi. The creation of translated articles has been carried out at Tokyo University of Foreign Studies. We are grateful to the contribu"
L18-1147,P03-1021,0,0.0430365,"ion of our entire corpus. 3. Machine Translation Baselines In this section, we present the baseline results of phrasebased MT from Arabic to Japanese. 3.1. Experimental Settings Phrase-based MT Settings We use the Moses toolkit (Koehn et al., 2007) to build a standard phrasebased MT system. Word alignment was extracted by MGIZA++ (Gao and Vogel, 2008) with a maximum phrase size of 8. We use the grow-diag-final-and and msdbidirectional-fe options for symmetrization and reordering. We train a 5-gram language model on the target side of the training set using KenLM (Heafield, 2011). We use MERT (Och, 2003) for decoding weight optimization. Data and Preprocessing We use the manually aligned data described in Section 2.2.4. for tuning and testing, and the automatically aligned data using Gargantua described in Section 2.2.5. for training. We tokenize Arabic data using the MADAMIRA toolkit (Pasha et al., 2014) with six tokenization schemes (D0, D1, D2, D3, D3*, and ATB) following Zalmout and Habash (2017). Examples of the six tokenization schemes are shown in Table 1. We normalize Japanese texts using the NFKC normalization and tokenize them using the MeCab morphological analyzer (0.996) (Kudo, 20"
L18-1147,P02-1040,0,0.10982,"nization schemes are shown in Table 1. We normalize Japanese texts using the NFKC normalization and tokenize them using the MeCab morphological analyzer (0.996) (Kudo, 2005) with IPAdic. We eliminate long sentences with more than 100 words using the script clean-corpus-n.perl before training translation models. Table 6 shows statistics of training data after cleaning. Evaluation Before evaluating, we de-tokenize the predicted output by deleting spaces between Japanese characters, and then re-tokenize them using MeCab with IPAdic. We calculate automatic evaluation scores for two metrics: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010). We use the multi-bleu.perl script in the Moses toolkit to compute BLEU scores. We calculate RIBES scores using the RIBES.py (1.03.1.).10 10 http://www.kecl.ntt.co.jp/icl/lirg/ ribes/ 921 dev-tune dev-test blind-test train Total Documents 100 400 400 7,752 8,652 Sentences 621 2,393 2,236 59,238 64,488 Tokens (ar) 23,312 92,760 85,940 2,175,438 2,377,460 Tokens (ja) 36,595 147,536 144,358 3,403,244 3,731,733 Table 5: The basic statistics of our parallel corpus. Sentences in the training set are aligned using Gargantua. Tokenization D0 D1 D2 ATB D3 D3* Sentences"
L18-1147,pasha-etal-2014-madamira,1,0.773483,"Missing"
L18-1147,L16-1144,0,0.0631373,"Missing"
L18-1147,C14-2019,0,0.0454772,"Missing"
L18-1147,tiedemann-2012-parallel,0,0.0405343,"t is understandable since Japanese also lacks the definite article. Tokenization D0 D1 D2 ATB D3 D3* dev-test BLEU RIBES 10.78 56.61 10.70 56.90 11.13 56.94 11.29 57.54 10.53 56.80 11.48 57.86 blind-test BLEU RIBES 8.76 55.71 8.83 55.63 9.34 56.08 9.24 56.41 8.56 55.77 9.38 56.63 Table 7: BLEU and RIBES scores of Arabic–Japanese PBMT systems with different tokenization schemes in the source side. that includes subtitles of educational video lectures on Massive Online Open Courses (MOOCs). Christodouloupoulos and Steedman (2015) presents a collection of Bible translations across 100 languages. Tiedemann (2012) provides a collection of Quran translations (Tanzil), localization files of technical manuals (GNOME, Ubuntu, and KDE4), as well as the collections of translations in the news domain (Global Voices, Tatoeba, News-Commentary 11). Prokopidis et al. (2016) constructed parallel corpora from Global Voices similar to Tiedemann (2012). Compared to the domains such as subtitles, religious texts, and technical manuals, the amount of data in the news domain is very limited. Our corpus aims to supplement the lack of parallel data in this domain by constructing a parallel corpus with over 64,000 sentence"
L18-1147,vondricka-2014-aligning,0,0.0393118,"Missing"
L18-1175,chaimongkol-etal-2014-corpus,0,0.0152538,"eneral-purpose linguistic annotation tools such as BRAT (Stenetorp et al., 2012) and WebAnno (Yimam et al., 2013) only support text documents. Some commercial software packages provide annotation functions for PDF, however, they lack a function of relation annotation suitable for dependency relation and coreference chain. Since PDF has become widespread standard for many publications, a linguistic annotation tool for PDF is strongly desired for knowledge extraction from PDF documents. For example, previous work has developed an annotated corpus for coreference resolution on scientific papers (Panot et al., 2014; Schafer et al., 2012; Steven et al., 2008). In their work, PDF articles are converted to plain-text format using OCR software, then import them to a text annotation tool. As pointed out in the literature, OCR errors are present in the data and they need to clean up the text by viewing the associated PDF file. This motivates us to develop a new annotation tool that can directly annotate on PDF. There are two types of annotation processes for creating an annotated text from a PDF file as shown in Figure 1. One is to convert the PDF into plain text or HTML format, then annotate it using a text"
L18-1175,C12-2103,0,0.170153,"istic annotation tools such as BRAT (Stenetorp et al., 2012) and WebAnno (Yimam et al., 2013) only support text documents. Some commercial software packages provide annotation functions for PDF, however, they lack a function of relation annotation suitable for dependency relation and coreference chain. Since PDF has become widespread standard for many publications, a linguistic annotation tool for PDF is strongly desired for knowledge extraction from PDF documents. For example, previous work has developed an annotated corpus for coreference resolution on scientific papers (Panot et al., 2014; Schafer et al., 2012; Steven et al., 2008). In their work, PDF articles are converted to plain-text format using OCR software, then import them to a text annotation tool. As pointed out in the literature, OCR errors are present in the data and they need to clean up the text by viewing the associated PDF file. This motivates us to develop a new annotation tool that can directly annotate on PDF. There are two types of annotation processes for creating an annotated text from a PDF file as shown in Figure 1. One is to convert the PDF into plain text or HTML format, then annotate it using a text annotation tool, as in"
L18-1175,E12-2021,0,0.35351,"ation conflicts. PDFAnno is freely available under open-source license at https://github.com/paperai/pdfanno. Keywords: text annotation, annotation tool, pdf 1. Introduction Gold standard annotations for texts are a prerequisite for training and evaluation of statistical models in Natural Language Processing (NLP). Since human annotation is known as one of the most costly and time-consuming tasks in NLP, an easy-to-use and easy-to-manage annotation tool is highly required for cost effective development of gold standard data. Currently, general-purpose linguistic annotation tools such as BRAT (Stenetorp et al., 2012) and WebAnno (Yimam et al., 2013) only support text documents. Some commercial software packages provide annotation functions for PDF, however, they lack a function of relation annotation suitable for dependency relation and coreference chain. Since PDF has become widespread standard for many publications, a linguistic annotation tool for PDF is strongly desired for knowledge extraction from PDF documents. For example, previous work has developed an annotated corpus for coreference resolution on scientific papers (Panot et al., 2014; Schafer et al., 2012; Steven et al., 2008). In their work, P"
L18-1175,bird-etal-2008-acl,0,0.265542,"such as BRAT (Stenetorp et al., 2012) and WebAnno (Yimam et al., 2013) only support text documents. Some commercial software packages provide annotation functions for PDF, however, they lack a function of relation annotation suitable for dependency relation and coreference chain. Since PDF has become widespread standard for many publications, a linguistic annotation tool for PDF is strongly desired for knowledge extraction from PDF documents. For example, previous work has developed an annotated corpus for coreference resolution on scientific papers (Panot et al., 2014; Schafer et al., 2012; Steven et al., 2008). In their work, PDF articles are converted to plain-text format using OCR software, then import them to a text annotation tool. As pointed out in the literature, OCR errors are present in the data and they need to clean up the text by viewing the associated PDF file. This motivates us to develop a new annotation tool that can directly annotate on PDF. There are two types of annotation processes for creating an annotated text from a PDF file as shown in Figure 1. One is to convert the PDF into plain text or HTML format, then annotate it using a text annotation tool, as in the previous work. An"
L18-1175,P13-4001,0,0.108004,"Missing"
L18-1287,W16-5406,1,0.730532,"D Japanese-GSD, UD JapanesePUD, and UD Japanese-Modern (Omura et al., 2017). Table 1 presents the current status of UD Japanese resources. Below, we describe these resources briefly. UD Japanese-BCCWJ is UD data based on the ‘Balanced Corpus of Contemporary Written Japanese’ (hereafter BCCWJ) (Maekawa et al., 2014). The BCCWJ defines 1 million word-scale core data samples in which the morphological information is manually annotated with three layers of word delimitations: Short Unit Word (SUW), Long Unit Word (LUW), and bunsetsu. The BCCWJ has several syntactic annotations. The BCCWJ-DepPara (Asahara and Matsumoto, 2016) is a bunsetsu-based syntactic dependency and coordinate structure annotation. The BCCWJ-PAS (Ueda et al., 2015) is a predicate-argument relation annotation with the NAIST Text Corpus annotation schema (Iida et al., 2007). We maintain conversion rules based on these annotations. UD Japanese-KTC (Tanaka et al., 2016) is based on the NTT Japanese Phrase Structure Treebank (Tanaka and Nagata, 2013) which contains the same original text as the Kyoto Text Corpus (KTC) (Kurohashi and Nagao, 2003). KTC is a bunsetsu, namely base phrase, based dependency treebank with its own word delimitation schema"
L18-1287,W07-1522,1,0.718422,"ced Corpus of Contemporary Written Japanese’ (hereafter BCCWJ) (Maekawa et al., 2014). The BCCWJ defines 1 million word-scale core data samples in which the morphological information is manually annotated with three layers of word delimitations: Short Unit Word (SUW), Long Unit Word (LUW), and bunsetsu. The BCCWJ has several syntactic annotations. The BCCWJ-DepPara (Asahara and Matsumoto, 2016) is a bunsetsu-based syntactic dependency and coordinate structure annotation. The BCCWJ-PAS (Ueda et al., 2015) is a predicate-argument relation annotation with the NAIST Text Corpus annotation schema (Iida et al., 2007). We maintain conversion rules based on these annotations. UD Japanese-KTC (Tanaka et al., 2016) is based on the NTT Japanese Phrase Structure Treebank (Tanaka and Nagata, 2013) which contains the same original text as the Kyoto Text Corpus (KTC) (Kurohashi and Nagao, 2003). KTC is a bunsetsu, namely base phrase, based dependency treebank with its own word delimitation schema and POS tagset. The NTT Japanese Phrase Structure Treebank is a phrase structure-based treebank. The word delimitation and POS are adapted to the UniDic SUW standard. The data is still in version 1.0 schema as of February"
L18-1287,C00-1060,1,0.238943,"version 1.0 schema as of February 2018. We are now modifying UD Japanese KTC from version 1.0 schema to version 2.0. UD Japanese-GSD (formerly known as UD Japanese) consists of sentences from Wikipedia. The version 2.0 of this annotated corpus was provided for the CoNLL 2017 Shared Task (Zeman et al., 2017). In the release of version 2.0, the sentences have been automatically split into words by IBM’s word segmenter. The segmentation errors were removed by adding lexicons specific to the data. In addition, the dependencies are automatically resolved using the bunsetsu-level dependency parser (Kanayama et al., 2000) with the attachment rules for functional words defined in UD Japanese (Tanaka et al., 2016). Complex sentences with parenthesis were removed to avoid parsing errors. In the version 2.1 released in November 2017, manual annotations were merged with the semi-automatic annotations to reduce remaining errors. UD Japanese-PUD was created in the same manner as UD Japanese-GSD, with the goal of maintaining consistency with UD Japanese-GSD. Since it is a parallel corpus with other languages, no sentences were removed from the corpus, including the ones containing parenthesis. UD Japanese-Modern (Omur"
L18-1287,P13-2017,0,0.1339,"Missing"
L18-1287,W13-4913,1,0.80844,"information is manually annotated with three layers of word delimitations: Short Unit Word (SUW), Long Unit Word (LUW), and bunsetsu. The BCCWJ has several syntactic annotations. The BCCWJ-DepPara (Asahara and Matsumoto, 2016) is a bunsetsu-based syntactic dependency and coordinate structure annotation. The BCCWJ-PAS (Ueda et al., 2015) is a predicate-argument relation annotation with the NAIST Text Corpus annotation schema (Iida et al., 2007). We maintain conversion rules based on these annotations. UD Japanese-KTC (Tanaka et al., 2016) is based on the NTT Japanese Phrase Structure Treebank (Tanaka and Nagata, 2013) which contains the same original text as the Kyoto Text Corpus (KTC) (Kurohashi and Nagao, 2003). KTC is a bunsetsu, namely base phrase, based dependency treebank with its own word delimitation schema and POS tagset. The NTT Japanese Phrase Structure Treebank is a phrase structure-based treebank. The word delimitation and POS are adapted to the UniDic SUW standard. The data is still in version 1.0 schema as of February 2018. We are now modifying UD Japanese KTC from version 1.0 schema to version 2.0. UD Japanese-GSD (formerly known as UD Japanese) consists of sentences from Wikipedia. The ver"
L18-1287,uchimoto-den-2008-word,0,0.0154352,"d on UniDic word boundary definition. The definition contains three layers: SUW, LUW, and bunsetsu. SUW can be produced by the morphological analyser MeCab.1 with UniDic2 LUW and bunsetsu can be produced by the pre-trained chunker Comainu.3 NINJAL4 defined five sorts of word unit definitions by operationalism. The most fine-grained unit is NINJAL Minimum Unit Word. SUW (Short Unit Word: 短単位) is constructively defined by the NINJAL Minimum Unit Word (最小単位). MUW (Middle Unit Word: 中単位) is a basic unit where a sound may change at the beginning or the ending of a word and/or an accent may change (Uchimoto and Den, 2008). The Middle Unit Word defines voiced compound (“rendaku”) (van de Weijer et al., 2005). 1825 1 taku910.github.io/mecab/ unidic.ninjal.ac.jp/ 3 osdn.net/projects/comainu/ 4 National Institute for Japanese Language and Linguistics. 2 Short Unit Word (SUW) advcl root iobj name punct iobj name 中国 . PROPN . China . . ・ . PUNCT . . . 北京 . PROPN . Beijing . . punct case 大 . NOUN . univ. . . obj aux に . ADP . -IOBJ . . 留学 . VERB . . . compound し . AUX . . study . abroad . 、 . PUNCT . . . 帰国 . VERB . return . to Japan . case 後 . NOUN . after . . case に . ADP . -IOBJ . . 双子 . NOUN . twins . . を. ADP ."
L18-1287,K17-3001,1,0.858266,"Missing"
L18-1355,W04-3230,1,0.623242,"post-processed output. For many companies tokenization is a fundamental and important technology for text processing. However, when increasing number of companies are demanding Japanese text processing recently, we are lacking freely available and useful resources for tokenization. In order to improve this situation, we develop a new Japanese tokenizer and dictionary for business use. We make them available to the public as an open source software (OSS). 2. Previous Work 2.1. Japanese Tokenizers When conducting Japanese tokenization for business applications, in the majority of cases MeCab1 (Kudo et al., 2004) or Kuromoji2 (the re-implementation of MeCab) are used. MeCab can process text at excellent speed, however, its functions are limited to segmentation, POS tagging, and lemmatization; Users need to pre-/post-process the text by themselves. It is common to conduct text formatting, sentence segmentation, and character normalization as pre-processing. Typical post-processing includes simple chunking (e.g., for numeric expressions) and filtering by POS tags. Each user performs these processes on their own, therefore we tend to reinvent the wheel, or conduct such processes in inefficient ways. Ther"
L18-1355,I08-7018,0,0.00826257,"bularies, and IPADIC license issues have been solved subsequently. UniDic5 (Den et al., 2007; Kouno and Ogiso, 2015) (National Institute for Japanese Language and Linguistics, 2017) is a project to develop a Japanese electronic dictionary with uniformity and identity. The outcome is used 3 https://lucene.apache.org/ https://ja.osdn.net/projects/naist-jdic/ 5 http://pj.ninjal.ac.jp/corpus center/unidic/ 4 1 http://taku910.github.io/mecab/ 2 https://www.atilika.com/ja/kuromoji/ 2246 for building Corpus of Spontaneous Japanese (CSJ)6 and Balanced Corpus of Contemporary Written Japanese (BCCWJ)7 (Maekawa, 2008). The project also offers a dictionary for conducting tokenization using MeCab. To emphasize the reproducibility of annotation, it adopts shorter token units in order for the annotators to process text unambiguously. To ensure the reproducibility the segmentation rules are defined as operating procedures, therefore it may get annotated in unintuitive fashions. It is shown effective for search purposes (Takahashi and Sassano, 2016), however, it is not suitable for syntactic or semantic analysis (National Institute for Japanese Language and Linguistics, 2017). The number of language resources de"
L18-1356,P09-1113,0,0.0692397,"Missing"
L18-1396,J09-2001,0,0.0583446,"Missing"
L18-1396,P14-1070,0,0.127933,"ompound nouns and compound function words. An accurate recognition of VMWEs is challenging because VMWEs could be discontinuous (e.g., take .. off). We show the main categories of VMWEs in Table 1. While dependency parsing and MWE recognition could be solved independently, dependency structures in that each MWE is a syntactic unit are preferable to word-based dependency structures for downstream NLP tasks, such as semantic parsing. Because MWE recognition could help syntactic parsing (Nivre and Nilsson, 2004; Eryi˘git et al., 2011), several works tackle MWE-aware dependency parsing in French (Candito and Constant, 2014; Nasr et al., 2015). They use French Treebank (Abeill´e et al., 2003) because of its explicit MWE annotations. Regarding English MWEs, Schneider et al. (2014) constructs an MWE-annotated corpus based on English Web Treebank (Bies et al., 2012). However, the number of VMWE occurrences (1,444) and types (1,155) in their corpus is relatively small-scale. In this work, we conduct full-scale VMWE annotations on the Wall Street Journal (WSJ) portion of English Ontonotes (Pradhan et al., 2007), which results in 7,833 VMWE occurrences and 1,608 types. Concretely, we construct a VMWE dictionary based"
L18-1396,W11-3806,0,0.0772059,"Missing"
L18-1396,L16-1263,1,0.854487,"s of “take the reins” and “take over”. Also, we resolve pseudo overlaps originating from false annotations. As a result, we reduce the number of overlaps to 11 instances, which correspond to essential overlaps, such as “look back” and “look .. on .. as” in the following sentence: “He may be able to look back on this election as the Finally, we check inclusions and overlaps between annotations by us and those by (Komai et al., 2015), which results in 159 inclusions and 40 overlaps. Regarding inclusions, 2497 5 6 http://dictionary.cambridge.org http://idioms.thefreedictionary.com literal usage. Kato et al. (2016) and Kato et al. (2017) integrates annotations of these functional MWEs and named entities (NEs) 8 into phrase structures by establishing MWEs as subtrees. They exploit this dataset for experiments on English MWE-aware dependency parsing. (a) A positive instance (non-literal usage) 4. (b) A negative instance (literal usage) Figure 3: Positive and negative instances of a VMWE (get up). In this work, we conduct large-scale annotations of English VMWEs in the Wall Street Journal portion of Ontonotes. Based on a VMWE dictionary extracted from English Wiktionary, we collect possible VMWE occurrence"
L18-1396,P17-2068,1,0.803739,"nd “take over”. Also, we resolve pseudo overlaps originating from false annotations. As a result, we reduce the number of overlaps to 11 instances, which correspond to essential overlaps, such as “look back” and “look .. on .. as” in the following sentence: “He may be able to look back on this election as the Finally, we check inclusions and overlaps between annotations by us and those by (Komai et al., 2015), which results in 159 inclusions and 40 overlaps. Regarding inclusions, 2497 5 6 http://dictionary.cambridge.org http://idioms.thefreedictionary.com literal usage. Kato et al. (2016) and Kato et al. (2017) integrates annotations of these functional MWEs and named entities (NEs) 8 into phrase structures by establishing MWEs as subtrees. They exploit this dataset for experiments on English MWE-aware dependency parsing. (a) A positive instance (non-literal usage) 4. (b) A negative instance (literal usage) Figure 3: Positive and negative instances of a VMWE (get up). In this work, we conduct large-scale annotations of English VMWEs in the Wall Street Journal portion of Ontonotes. Based on a VMWE dictionary extracted from English Wiktionary, we collect possible VMWE occurrences in Ontonotes, and fil"
L18-1396,Y15-2015,1,0.931392,", a function-head scheme is preferable to a content-head scheme. 1,235 270 80 ≥5 31 Total 7,833 23 1,608 Table 2: Corpus statistics. We show VMWE instances and types by the number of constituent word tokens. # of gaps VMWE instances 0 6,855 1 968 2 10 Table 3: VMWE instances by the number of gaps. VPCs, we regard a candidate as a positive VMWE occurrence iff the dependency label is “prt”. For prepositional verbs, if the dependency label is “prep”, and there is no gap between the verb and the particle, we regard this candidate as a positive VMWE occurrence. This is subject to rules proposed by Komai et al. (2015). Otherwise, we conduct crowdsourced annotations. 2.2. Large-scale Annotations of VMWEs by Crowdsourcing Figure 2: A screenshot of a web interface for VMWE annotations on CrowdFlower. oneself). We exclude candidates that do not include any verbs by using gold part-of-speech information. Also, we filter out candidates that have other verbs or punctuation marks within the gaps. Because most of the VMWEs are syntactically regular, we filter a VMWE whose components form a subtree in a Stanford basic dependency tree (Marneffe and Manning, 2008), which is converted from a phrase structure tree given"
L18-1396,P13-2017,0,0.0472047,"Missing"
L18-1396,P15-1108,0,0.0536278,"Missing"
L18-1396,W17-1704,0,0.070234,"s a dataset for French MWE-aware dependency parsing (Candito and Constant, 2014) because of its explicit MWE annotations. It consists of phrase structure trees, augmented with morphological information and functional annotations of verbal dependents. Second, Vincze (2012) provides an English-Hungarian parallel corpus annotated for LVCs, which belong to VMWEs. Their corpus contains 703 LVCs in Hungarian and 727 in English based on 14,261 sentence alignment units, taken from economiclegal texts and literature. Recently, PARSEME organized a shared task on automatic identification of verbal MWEs (Savary et al., 2017). They provide annotation guidelines and annotated corpora of 5.5 million tokens and 60,000 VMWE annotations for 18 languages. Note that their corpora do not support English in edition 1.0. Regarding English MWEs, Shigeto et al. (2013) first constructs an MWE dictionary by extracting functional MWEs 7 from the English-language Wiktionary, and classifies their occurrences in Ontonotes into either MWE or Conclusion 2. We get VMWE occurrences in Ontonotes for only 1,608 out of 8,369 types in our VMWE dictionary. Therefore, we plan to explore VMWE occurrences on a larger corpus, such as the Annota"
L18-1396,schneider-etal-2014-comprehensive,0,0.173587,"Missing"
L18-1396,vincze-2012-light,0,0.254981,"er of discontinuous instances. Our corpus annotations are represented as token indices of components of VMWEs. By using them, we can classify potential VMWEs in our corpus as positive and negative instances (Figure 3). 3. Related Work We introduce several MWE-annotated corpora. First, French Treebank (Abeill´e et al., 2003) is often used as a dataset for French MWE-aware dependency parsing (Candito and Constant, 2014) because of its explicit MWE annotations. It consists of phrase structure trees, augmented with morphological information and functional annotations of verbal dependents. Second, Vincze (2012) provides an English-Hungarian parallel corpus annotated for LVCs, which belong to VMWEs. Their corpus contains 703 LVCs in Hungarian and 727 in English based on 14,261 sentence alignment units, taken from economiclegal texts and literature. Recently, PARSEME organized a shared task on automatic identification of verbal MWEs (Savary et al., 2017). They provide annotation guidelines and annotated corpora of 5.5 million tokens and 60,000 VMWE annotations for 18 languages. Note that their corpora do not support English in edition 1.0. Regarding English MWEs, Shigeto et al. (2013) first constructs"
L18-1418,L16-1343,0,0.028752,"ing features for both daily and business tasks. Text based methods like emails or text messengers are still very convenient and indispensable to us because of its unique advantages: they do not require intermediate responses and can be used for the sake of recordkeeping. However, it is already proved that in any online communication methods, users experience more difficulties in interpreting and conveying emotions than face-toface communication due to the limitation in communication modality. Furthermore, text-based methods are where difficulties are encountered the most (Kruger et al., 2005; Arimoto and Okanoya, 2016). Therefore, we targeted the effort to build an emotion analysis system focusing on text data. The starting point is to develop an emotional corpus that has conversational texts and is as close to real-life communication as possible. Existing emotional text corpora are often collected from micro-blog platforms using multiclass scheme - one emotion per example (Liew et al., 2016). Most of them are automatically annotated by extracting hashtags rather than by human judgements (Dini and Bittar, 2016; Li et al., 2016). While the text data from micro-blog platforms like Twitters are very convenient"
L18-1418,L16-1624,0,0.119854,"ore, text-based methods are where difficulties are encountered the most (Kruger et al., 2005; Arimoto and Okanoya, 2016). Therefore, we targeted the effort to build an emotion analysis system focusing on text data. The starting point is to develop an emotional corpus that has conversational texts and is as close to real-life communication as possible. Existing emotional text corpora are often collected from micro-blog platforms using multiclass scheme - one emotion per example (Liew et al., 2016). Most of them are automatically annotated by extracting hashtags rather than by human judgements (Dini and Bittar, 2016; Li et al., 2016). While the text data from micro-blog platforms like Twitters are very convenient and easy to collect, the fact that they are limited in the number of characters (140 for a tweet) differs themselves from daily conversation text and therefore, have limited use in a real-life settings. On the other hand, multiclass scheme has its own limitation. One input is only associated to one emotion. However, it is observed in some research (Liew et al., 2016) that multilabel scheme with no limitation in the number of emotions per example is a better and more natural way of annotating emo"
L18-1418,P13-1095,0,0.0256487,"n, exclamation, command, or statement), the general context of the conversation, each and every utterances in the conversation - especially when what is said in the previous utterance can have an impact on the emotions of the later one (Collier, 2014). Maybe, because of this complicated nature of the problem, there is a lack of emotional conversation corpus. Another problem with the existing corpora is the annotating scheme: many works limit the emotion labels to a small number (Mohammad, 2012b; Wang et al., 2015) or only allow annotators to label one emotion per utterance (Yang et al., 2007; Hasegawa et al., 2013). As pointed out in many psychology research (Plutchik, 2001; Russell, 2003), emotions are not mutually exclusive. In fact, in many cases, people may experience a mixture of various emotions at the same time (Choe et al., 2013). Therefore, the corpus for any emotion analysis task should be multilabel. Limiting the number of emotion labels may narrow down the problem but can cause troubles for the annotators to provide correct judgement when the emotions in an example are sophisticated or expressed implicitly. In our work, we employ Plutchik’s theory of emotions and extend the set of labels to"
L18-1418,L16-1291,0,0.0309117,"are where difficulties are encountered the most (Kruger et al., 2005; Arimoto and Okanoya, 2016). Therefore, we targeted the effort to build an emotion analysis system focusing on text data. The starting point is to develop an emotional corpus that has conversational texts and is as close to real-life communication as possible. Existing emotional text corpora are often collected from micro-blog platforms using multiclass scheme - one emotion per example (Liew et al., 2016). Most of them are automatically annotated by extracting hashtags rather than by human judgements (Dini and Bittar, 2016; Li et al., 2016). While the text data from micro-blog platforms like Twitters are very convenient and easy to collect, the fact that they are limited in the number of characters (140 for a tweet) differs themselves from daily conversation text and therefore, have limited use in a real-life settings. On the other hand, multiclass scheme has its own limitation. One input is only associated to one emotion. However, it is observed in some research (Liew et al., 2016) that multilabel scheme with no limitation in the number of emotions per example is a better and more natural way of annotating emotion labels. We de"
L18-1418,L16-1183,0,0.047959,"Missing"
L18-1418,N12-1071,0,0.121056,"example is a better and more natural way of annotating emotion labels. We describe our efforts to construct and annotate partly the Emotional Movie Transcript Corpus (EMTC). Most of the corpus are unsupervised data. We annotated by ourselves 10,000 utterances and use them for training. Finally, the testing data, which include 1000 utterances, are annotated by 5 independent annotators. To our understanding, EMTC is the only emotional corpus that is annotated using multilabel scheme and has conversational text instead of short text like tweets or news headlines (Strapparava and Mihalcea, 2007; Mohammad, 2012b). Moreover, EMTC provide the annotators with movie clips instead of just text to help them give better annotation. Our contributions are summarized as follow: • We explain the multilabel annotating scheme following Plutchik’s theory of emotions (Plutchik, 2001). We then later conclude that our annotating scheme provide much better inter-annotators agreement score than other corpora. • We present and describe the characteristics of the conversational corpus and the statistics of the annotated data. • We conduct supervised machine learning experiments to evaluate the emotion classification usi"
L18-1418,S12-1033,0,0.245187,"example is a better and more natural way of annotating emotion labels. We describe our efforts to construct and annotate partly the Emotional Movie Transcript Corpus (EMTC). Most of the corpus are unsupervised data. We annotated by ourselves 10,000 utterances and use them for training. Finally, the testing data, which include 1000 utterances, are annotated by 5 independent annotators. To our understanding, EMTC is the only emotional corpus that is annotated using multilabel scheme and has conversational text instead of short text like tweets or news headlines (Strapparava and Mihalcea, 2007; Mohammad, 2012b). Moreover, EMTC provide the annotators with movie clips instead of just text to help them give better annotation. Our contributions are summarized as follow: • We explain the multilabel annotating scheme following Plutchik’s theory of emotions (Plutchik, 2001). We then later conclude that our annotating scheme provide much better inter-annotators agreement score than other corpora. • We present and describe the characteristics of the conversational corpus and the statistics of the annotated data. • We conduct supervised machine learning experiments to evaluate the emotion classification usi"
L18-1418,S07-1013,0,0.618873,"on in the number of emotions per example is a better and more natural way of annotating emotion labels. We describe our efforts to construct and annotate partly the Emotional Movie Transcript Corpus (EMTC). Most of the corpus are unsupervised data. We annotated by ourselves 10,000 utterances and use them for training. Finally, the testing data, which include 1000 utterances, are annotated by 5 independent annotators. To our understanding, EMTC is the only emotional corpus that is annotated using multilabel scheme and has conversational text instead of short text like tweets or news headlines (Strapparava and Mihalcea, 2007; Mohammad, 2012b). Moreover, EMTC provide the annotators with movie clips instead of just text to help them give better annotation. Our contributions are summarized as follow: • We explain the multilabel annotating scheme following Plutchik’s theory of emotions (Plutchik, 2001). We then later conclude that our annotating scheme provide much better inter-annotators agreement score than other corpora. • We present and describe the characteristics of the conversational corpus and the statistics of the annotated data. • We conduct supervised machine learning experiments to evaluate the emotion cl"
L18-1418,strapparava-valitutti-2004-wordnet,0,0.411759,"Missing"
L18-1418,P15-2125,0,0.0298509,"ucture and syntactic variables such as negations, embedded sentences, and type of sentence (question, exclamation, command, or statement), the general context of the conversation, each and every utterances in the conversation - especially when what is said in the previous utterance can have an impact on the emotions of the later one (Collier, 2014). Maybe, because of this complicated nature of the problem, there is a lack of emotional conversation corpus. Another problem with the existing corpora is the annotating scheme: many works limit the emotion labels to a small number (Mohammad, 2012b; Wang et al., 2015) or only allow annotators to label one emotion per utterance (Yang et al., 2007; Hasegawa et al., 2013). As pointed out in many psychology research (Plutchik, 2001; Russell, 2003), emotions are not mutually exclusive. In fact, in many cases, people may experience a mixture of various emotions at the same time (Choe et al., 2013). Therefore, the corpus for any emotion analysis task should be multilabel. Limiting the number of emotion labels may narrow down the problem but can cause troubles for the annotators to provide correct judgement when the emotions in an example are sophisticated or expr"
matsumoto-etal-2006-annotated,J93-2004,0,\N,Missing
matsumoto-etal-2006-annotated,W02-2016,1,\N,Missing
matsumoto-etal-2006-annotated,P96-1025,0,\N,Missing
matsumoto-yamashita-2000-using,W99-0606,0,\N,Missing
matsumoto-yamashita-2000-using,J93-2004,0,\N,Missing
matsumoto-yamashita-2000-using,P97-1030,1,\N,Missing
matsumoto-yamashita-2000-using,A00-1032,1,\N,Missing
matsumoto-yamashita-2000-using,W98-1511,1,\N,Missing
matsuyoshi-etal-2010-annotating,W09-3027,1,\N,Missing
matsuyoshi-etal-2010-annotating,W09-3417,0,\N,Missing
matsuyoshi-etal-2010-annotating,P07-1125,0,\N,Missing
matsuyoshi-etal-2010-annotating,W09-1201,0,\N,Missing
matsuyoshi-etal-2010-annotating,W04-3103,0,\N,Missing
N01-1025,A00-2007,0,\N,Missing
N01-1025,C00-2102,0,\N,Missing
N01-1025,W00-1303,1,\N,Missing
N01-1025,E99-1023,0,\N,Missing
N01-1025,W00-0730,1,\N,Missing
N01-1025,W00-0733,0,\N,Missing
N01-1025,P00-1042,0,\N,Missing
N01-1025,W00-0726,0,\N,Missing
N01-1025,J95-4004,0,\N,Missing
N01-1025,W00-0734,0,\N,Missing
N01-1025,C00-2124,0,\N,Missing
N03-1002,C02-1054,0,\N,Missing
N03-1002,P00-1042,0,\N,Missing
N03-1002,N01-1025,1,\N,Missing
N03-1002,W02-1036,0,\N,Missing
N16-1133,P06-1032,0,0.225498,"ults of the SMT and reorder the results. Our experiments show that our reranking system using parts of speech and syntactic features improves performance and achieves state-of-theart quality, with an F0.5 score of 40.0. Figure 1: Flow of reranking. 1 Introduction Research on assisting second language learners has received considerable attention, especially regarding grammatical error correction of essays written by English as a Second Language (ESL) learners. To address all types of errors, grammatical error correction methods that use statistical machine translation (SMT) have been proposed (Brockett et al., 2006; Mizumoto et al., 2012; Buys and van der Merwe, 2013; Yuan and Felice, 2013; Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014). SMTbased error correction systems have achieved rankings first and third in the CoNLL2014 Shared Task (Ng et al., 2014). SMT systems generate many candidates of translation. SMT systems generate scored candidates and select a sentence having the highest score as the translation result. However, the 1-best result of SMT system is not always the best result because the scoring is conducted only with local features. In other words, N-best (N &gt; 1) results may"
N16-1133,W13-3606,0,0.0317994,"Missing"
N16-1133,N12-1067,0,0.0689645,"training data of reranking, Lang-8 Learner Corpora was split into 10 parts and each part was corrected by a grammatical error correction system trained on the other nine parts. We selected 10 as N for N-best reranking. PukWaC corpus (Baroni et al., 2009) was used for constructing our “Web dependency N-gram” feature. We use Stanford Parser 3.2.05 as a dependency parser. CoNLL-2013 test set were split into 700 sentences for parameter tuning of SMT and 681 sentences for tuning parameter beta. CoNLL-2014 test set, 1,312 sentences were used for evaluation. We used M2 Scorer as an evaluation tool (Dahlmeier and Ng, 2012). This scorer calculates precision, recall, and F0.5 scores. We used F0.5 as a tuning metric. In addition, we used GLEU (Napoles et al., 2015) as evaluation metrics. 5.2 Experimental Results and Discussion Table 3 shows the experimental results. We used the 1-best result of the SMT correction system and reranking by probability of the large N-gram language model (Felice et al., 2014) as baseline systems. In addition, we compared the systems that are ranked first (CAMB) and second (CUUI) (Felice et al., 2014; Rozovskaya et al., 2014) in CoNLL2014 5 http://nlp.stanford.edu/software/ lex-parser.s"
N16-1133,W13-1703,0,0.11149,"nglish essays that were written by ESL learners and cleaned noise with the method proposed in (Mizumoto et al., 2011). From the results, we obtained 1,069,127 sentence pairs. We used a 5-gram language model built on the “Associated Press Worldstream English 2 http://www2.nict.go.jp/univ-com/multi_ trans/cicada/ 3 https://kheafield.com/code/kenlm/ 4 http://cs.jhu.edu/ ozaidan/zmert/ ˜ TP FN FP GLEU 598 834 772 623 1847 1797 1793 1881 764 1280 1172 868 65.7 64.7 64.5 64.8 606 669 657 679 1834 1837 1813 1827 781 842 778 851 65.7 65.8 66.1 65.8 Service” from English Gigaword corpus and NUCLE 3.2 (Dahlmeier et al., 2013). We used these two language models as separate feature functions in the SMT system. For training data of reranking, Lang-8 Learner Corpora was split into 10 parts and each part was corrected by a grammatical error correction system trained on the other nine parts. We selected 10 as N for N-best reranking. PukWaC corpus (Baroni et al., 2009) was used for constructing our “Web dependency N-gram” feature. We use Stanford Parser 3.2.05 as a dependency parser. CoNLL-2013 test set were split into 700 sentences for parameter tuning of SMT and 681 sentences for tuning parameter beta. CoNLL-2014 test"
N16-1133,W14-1702,0,0.589527,"of speech and syntactic features improves performance and achieves state-of-theart quality, with an F0.5 score of 40.0. Figure 1: Flow of reranking. 1 Introduction Research on assisting second language learners has received considerable attention, especially regarding grammatical error correction of essays written by English as a Second Language (ESL) learners. To address all types of errors, grammatical error correction methods that use statistical machine translation (SMT) have been proposed (Brockett et al., 2006; Mizumoto et al., 2012; Buys and van der Merwe, 2013; Yuan and Felice, 2013; Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014). SMTbased error correction systems have achieved rankings first and third in the CoNLL2014 Shared Task (Ng et al., 2014). SMT systems generate many candidates of translation. SMT systems generate scored candidates and select a sentence having the highest score as the translation result. However, the 1-best result of SMT system is not always the best result because the scoring is conducted only with local features. In other words, N-best (N &gt; 1) results may be better than the 1-best result. Reranking approaches have been devised to solve the scoring pro"
N16-1133,W09-0408,0,0.0585046,"2007) to SMT tasks. The approach of Carter and Monz (2011) was similar to that of Li and Khudanpur (2008), but they used additional syntactic features (e.g. part of speech (POS), parse tree) for reranking of common SMT tasks. The reranking approach has been used in grammatical error correction based on phrase-based SMT (Felice et al., 2014). However, their method uses only language model scores. In the reranking step, the system can consider not only surface but also syntactic features such as those in the approach of Carter and Monz (2011). We use syntactic features in our reranking system. Heafield et al. (2009) proposed a system combination method for machine translation that is similar to that of reranking. System combination is a method that merges the outputs of multiple systems to produce an output that is better than each individual system. Susanto et al. (2014) applied this system combination to grammatical error correction. They combined pipeline systems based on classification approaches and SMT systems. Classifier-based systems use syntactic features as POS and dependency for error correction. However, syntactic information 1134 Table 1: Oracle score of grammatical error correction N-best 1"
N16-1133,W14-1703,0,0.237914,"tic features improves performance and achieves state-of-theart quality, with an F0.5 score of 40.0. Figure 1: Flow of reranking. 1 Introduction Research on assisting second language learners has received considerable attention, especially regarding grammatical error correction of essays written by English as a Second Language (ESL) learners. To address all types of errors, grammatical error correction methods that use statistical machine translation (SMT) have been proposed (Brockett et al., 2006; Mizumoto et al., 2012; Buys and van der Merwe, 2013; Yuan and Felice, 2013; Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014). SMTbased error correction systems have achieved rankings first and third in the CoNLL2014 Shared Task (Ng et al., 2014). SMT systems generate many candidates of translation. SMT systems generate scored candidates and select a sentence having the highest score as the translation result. However, the 1-best result of SMT system is not always the best result because the scoring is conducted only with local features. In other words, N-best (N &gt; 1) results may be better than the 1-best result. Reranking approaches have been devised to solve the scoring problem. Reranking is a method that re-score"
N16-1133,2008.amta-papers.12,0,0.0447161,"(B in Figure 1). In this study, we apply a discriminative reranking method to the task of grammatical error correction. Syntactic information is not considered in the phrase-based SMT. We show that using syntactic features in the reranking system can improve error correction performance. Although reranking using only surface features (Shen et al., 2004) is not effective for grammatical error correction, reranking using syntactic features improves the F0.5 score. 2 Related Work for Reranking Reranking approaches have been proposed for common SMT tasks (Shen et al., 2004; Carter and Monz, 2011; Li and Khudanpur, 2008; Och et al., 2004). Shen et al. (2004) first used a perceptron-like algorithm for reranking of common SMT tasks. However, they used only a few features. Li and Khudanpur (2008) proposed a reranking approach that uses a large-scale discriminative Ngram language model for common SMT tasks. They extended the reranking method for automatic speech recognition (Roark et al., 2007) to SMT tasks. The approach of Carter and Monz (2011) was similar to that of Li and Khudanpur (2008), but they used additional syntactic features (e.g. part of speech (POS), parse tree) for reranking of common SMT tasks. T"
N16-1133,I11-1017,1,0.891639,"ection to observe the effect of discriminative reranking and our syntactic features. 5.1 Experimental Settings We used phrase-based SMT which many previous studies used for grammatical error correction for a baseline system. We used cicada 0.3.52 for the machine translation tool and KenLM3 as the language modeling tool. We used ZMERT4 as the parameter tuning tool and implemented the averaged perceptron for reranking. The translation model was trained on the Lang-8 Learner Corpora v2.0. We extracted English essays that were written by ESL learners and cleaned noise with the method proposed in (Mizumoto et al., 2011). From the results, we obtained 1,069,127 sentence pairs. We used a 5-gram language model built on the “Associated Press Worldstream English 2 http://www2.nict.go.jp/univ-com/multi_ trans/cicada/ 3 https://kheafield.com/code/kenlm/ 4 http://cs.jhu.edu/ ozaidan/zmert/ ˜ TP FN FP GLEU 598 834 772 623 1847 1797 1793 1881 764 1280 1172 868 65.7 64.7 64.5 64.8 606 669 657 679 1834 1837 1813 1827 781 842 778 851 65.7 65.8 66.1 65.8 Service” from English Gigaword corpus and NUCLE 3.2 (Dahlmeier et al., 2013). We used these two language models as separate feature functions in the SMT system. For train"
N16-1133,C12-2084,1,0.889465,"rder the results. Our experiments show that our reranking system using parts of speech and syntactic features improves performance and achieves state-of-theart quality, with an F0.5 score of 40.0. Figure 1: Flow of reranking. 1 Introduction Research on assisting second language learners has received considerable attention, especially regarding grammatical error correction of essays written by English as a Second Language (ESL) learners. To address all types of errors, grammatical error correction methods that use statistical machine translation (SMT) have been proposed (Brockett et al., 2006; Mizumoto et al., 2012; Buys and van der Merwe, 2013; Yuan and Felice, 2013; Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014). SMTbased error correction systems have achieved rankings first and third in the CoNLL2014 Shared Task (Ng et al., 2014). SMT systems generate many candidates of translation. SMT systems generate scored candidates and select a sentence having the highest score as the translation result. However, the 1-best result of SMT system is not always the best result because the scoring is conducted only with local features. In other words, N-best (N &gt; 1) results may be better than the 1-be"
N16-1133,P15-2097,0,0.0428539,"trained on the other nine parts. We selected 10 as N for N-best reranking. PukWaC corpus (Baroni et al., 2009) was used for constructing our “Web dependency N-gram” feature. We use Stanford Parser 3.2.05 as a dependency parser. CoNLL-2013 test set were split into 700 sentences for parameter tuning of SMT and 681 sentences for tuning parameter beta. CoNLL-2014 test set, 1,312 sentences were used for evaluation. We used M2 Scorer as an evaluation tool (Dahlmeier and Ng, 2012). This scorer calculates precision, recall, and F0.5 scores. We used F0.5 as a tuning metric. In addition, we used GLEU (Napoles et al., 2015) as evaluation metrics. 5.2 Experimental Results and Discussion Table 3 shows the experimental results. We used the 1-best result of the SMT correction system and reranking by probability of the large N-gram language model (Felice et al., 2014) as baseline systems. In addition, we compared the systems that are ranked first (CAMB) and second (CUUI) (Felice et al., 2014; Rozovskaya et al., 2014) in CoNLL2014 5 http://nlp.stanford.edu/software/ lex-parser.shtml 1136 Shared Task. The discriminative reranking system with our features achieved the best F0.5 score. The difference between the results"
N16-1133,N04-1023,0,0.133785,"sing SMT for a learner sentence (A in Figure 1). A reranking sys1133 Proceedings of NAACL-HLT 2016, pages 1133–1138, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics tem then re-scores the N-best results and reorders them (B in Figure 1). In this study, we apply a discriminative reranking method to the task of grammatical error correction. Syntactic information is not considered in the phrase-based SMT. We show that using syntactic features in the reranking system can improve error correction performance. Although reranking using only surface features (Shen et al., 2004) is not effective for grammatical error correction, reranking using syntactic features improves the F0.5 score. 2 Related Work for Reranking Reranking approaches have been proposed for common SMT tasks (Shen et al., 2004; Carter and Monz, 2011; Li and Khudanpur, 2008; Och et al., 2004). Shen et al. (2004) first used a perceptron-like algorithm for reranking of common SMT tasks. However, they used only a few features. Li and Khudanpur (2008) proposed a reranking approach that uses a large-scale discriminative Ngram language model for common SMT tasks. They extended the reranking method for auto"
N16-1133,D14-1102,0,0.538079,"d in grammatical error correction based on phrase-based SMT (Felice et al., 2014). However, their method uses only language model scores. In the reranking step, the system can consider not only surface but also syntactic features such as those in the approach of Carter and Monz (2011). We use syntactic features in our reranking system. Heafield et al. (2009) proposed a system combination method for machine translation that is similar to that of reranking. System combination is a method that merges the outputs of multiple systems to produce an output that is better than each individual system. Susanto et al. (2014) applied this system combination to grammatical error correction. They combined pipeline systems based on classification approaches and SMT systems. Classifier-based systems use syntactic features as POS and dependency for error correction. However, syntactic information 1134 Table 1: Oracle score of grammatical error correction N-best 1 10 50 100 Precision 43.9 79.1 89.5 92.3 Recall 24.5 36.7 43.1 45.3 F0.5 37.9 64.3 73.6 76.4 is not considered in combining systems. 3 Why is Reranking Necessary? Grammatical error correction using SMT has the same problem as that of common SMT task: the 1best"
N16-1133,N06-1012,0,0.0433576,"∑ N ∑ wt T t=1 i=1 (1) To select the best correction from N-best candidates, we use the following formula: S(z) = β ϕ0 (z) + w · ϕ (z) (2) where ϕ0 (z) is the score calculated by the SMT system for each translation hypothesis. This score is weighted by β . Using ϕ0 (z) as a feature in the perceptron algorithm is possible, but this may lead to 1135 4: 5: 6: 7: 8: 9: 10: 11: w←0 for t = 1 to T do for i = 1 to N do yi ← ORACLE(xi ) zi ← argmaxx∈GEN(xi ) ϕ (z) · w if zi = yi then w ← w + ϕ (yi ) − ϕ (zi ) end if end for end for return w Figure 2: Perceptron algorithm for ranking. under-training (Sutton et al., 2006). We select the value for β with the highest F0.5 score by changing β from 0 to 100 in 0.1 increments on the development data. 4.2 Features of Discriminative Reranking for Grammatical Error Correction In this study, we use the features used in Carter and Monz (2011) as well as our new features of POS and dependency. We use the features extracted from the following sequences: POS tag , shallow parse tag, and shallow parse tag plus POS tag sequences (Carter and Monz, 2011). From these sequences, features are extracted based on the following three definitions: 1. (ti−2ti−1ti ), (ti−1ti ), (ti wi"
N16-1133,W13-3607,0,0.0644238,"king system using parts of speech and syntactic features improves performance and achieves state-of-theart quality, with an F0.5 score of 40.0. Figure 1: Flow of reranking. 1 Introduction Research on assisting second language learners has received considerable attention, especially regarding grammatical error correction of essays written by English as a Second Language (ESL) learners. To address all types of errors, grammatical error correction methods that use statistical machine translation (SMT) have been proposed (Brockett et al., 2006; Mizumoto et al., 2012; Buys and van der Merwe, 2013; Yuan and Felice, 2013; Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014). SMTbased error correction systems have achieved rankings first and third in the CoNLL2014 Shared Task (Ng et al., 2014). SMT systems generate many candidates of translation. SMT systems generate scored candidates and select a sentence having the highest score as the translation result. However, the 1-best result of SMT system is not always the best result because the scoring is conducted only with local features. In other words, N-best (N &gt; 1) results may be better than the 1-best result. Reranking approaches have been devised to"
N16-1133,W14-1701,0,\N,Missing
N16-1133,W14-1704,0,\N,Missing
N16-1133,W12-2025,0,\N,Missing
N16-1133,N04-1021,0,\N,Missing
N19-1286,S10-1057,0,0.0237209,"ll relational information. Our proposed model achieves new state-of-the-art results on SemEval-2010 Task 8, compared with other complex models. 2 Related Work RC plays a significant role in many NLP applications. Recent work usually present the task from a supervised perspective. 2793 Proceedings of NAACL-HLT 2019, pages 2793–2798 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Traditional supervised approaches can be divided into feature-based methods and kernel methods. Feature-based methods focus on extracting and combining relevant features. Rink and Harabagiu (2010) leveraged useful features to achieve the best performance on SemEval-2010 Task 8. Meanwhile, kernel methods measure the structural similarity between two data samples, based on carefully designed kernels. Wang (2008) combined convolutional kernel and syntactic features to gain benefits for relation extraction. Nowadays, deep neural networks are widely utilized in RC. Zeng et al. (2014) exploited a CNN to extract lexical and sentence features. Qin et al. (2016) used ETF to specify target entities in input sentences and fed them to a CNN. Vu et al. (2016) combined CNN and RNN to improve perform"
N19-1286,N16-1065,0,0.0341265,"Missing"
N19-1286,P16-1123,0,0.0491196,"Missing"
N19-1286,D14-1181,0,0.0107207,"Missing"
N19-1286,I08-2119,0,0.0389526,"y present the task from a supervised perspective. 2793 Proceedings of NAACL-HLT 2019, pages 2793–2798 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Traditional supervised approaches can be divided into feature-based methods and kernel methods. Feature-based methods focus on extracting and combining relevant features. Rink and Harabagiu (2010) leveraged useful features to achieve the best performance on SemEval-2010 Task 8. Meanwhile, kernel methods measure the structural similarity between two data samples, based on carefully designed kernels. Wang (2008) combined convolutional kernel and syntactic features to gain benefits for relation extraction. Nowadays, deep neural networks are widely utilized in RC. Zeng et al. (2014) exploited a CNN to extract lexical and sentence features. Qin et al. (2016) used ETF to specify target entities in input sentences and fed them to a CNN. Vu et al. (2016) combined CNN and RNN to improve performance. Some recent work leveraged SDP for RC. Yang et al. (2016) proposed a position encoding CNN based on dependency parse trees, while Wen (2017) presented a model that learns representations from SDP, using both CNN"
N19-1286,N16-1175,0,0.0580871,"Missing"
N19-1286,D18-1250,0,0.084662,"man, 2015). Traditional approaches (Kambhatla, 2004; Zhang et al., 2006) usually rely heavily on hand-crafted features and lexical resources, or elaborately designed kernels, which are time-consuming and challenging to adapt to novel domains. Recently, neural network (NN) models have dominated the work on RC since they can effectively learn meaningful hidden features without human intervention. However, most previous NN models only exploit one of the following structures to represent relation instances: raw word sequences (Zhou et al., 2016; Wang et al., 2016) and dependency trees (Wen, 2017; Le et al., 2018). While raw sequences can provide all the information of relation instances, they also add noise to the models from redundant information. While dependency tree structures help the models focus on the concise information captured by the shortest dependency path (SDP) between two entities, they lose some supplementary context in the raw sequence. It is clear that the raw sequence and SDP highly complement each other. We, therefore, combine them to be more effective in determining the relation without losing any information. While CNNs are able to learn short patterns (local features) (LeCun et"
N19-1286,D15-1062,0,0.0832671,"Missing"
N19-1286,D16-1007,0,0.238505,"plement each other. We, therefore, combine them to be more effective in determining the relation without losing any information. While CNNs are able to learn short patterns (local features) (LeCun et al., 1995), RNNs have been effective in learning word sequence information (long-distance features) (Chung et al., 2014). In this paper, we present a new model combining both CNNs and RNNs, exploiting the information from both the raw sequence and the SDP. Our contributions are summarized as follows: (a) We combine Entity Tag Feature (ETF) (Qin et al., 2016) and Tree-based Position Feature (TPF) (Yang et al., 2016) to improve the semantic information between the marked entities in the raw input sentences. (b) We propose Segment-Level Attention-based Convolutional Neural Networks (SACNNs) which automatically pay special attention to the important text segments from the raw sentence for RC. (c) We build Dependency-based Recurrent Neural Networks (DepRNNs) on the SDP to gain longdistance features. Then, we combine the SACNN and the DepRNN to preserve the full relational information. Our proposed model achieves new state-of-the-art results on SemEval-2010 Task 8, compared with other complex models. 2 Relate"
N19-1286,C14-1220,0,0.0708909,"on for Computational Linguistics Traditional supervised approaches can be divided into feature-based methods and kernel methods. Feature-based methods focus on extracting and combining relevant features. Rink and Harabagiu (2010) leveraged useful features to achieve the best performance on SemEval-2010 Task 8. Meanwhile, kernel methods measure the structural similarity between two data samples, based on carefully designed kernels. Wang (2008) combined convolutional kernel and syntactic features to gain benefits for relation extraction. Nowadays, deep neural networks are widely utilized in RC. Zeng et al. (2014) exploited a CNN to extract lexical and sentence features. Qin et al. (2016) used ETF to specify target entities in input sentences and fed them to a CNN. Vu et al. (2016) combined CNN and RNN to improve performance. Some recent work leveraged SDP for RC. Yang et al. (2016) proposed a position encoding CNN based on dependency parse trees, while Wen (2017) presented a model that learns representations from SDP, using both CNN and RNN. 3 Our Method Given a sentence S with an annotated pair of entities (e1, e2), we aim to identify the semantic relation between them. Since the set of target relati"
N19-1286,P06-1104,0,0.0744021,"path of the related entities. Experiments on the SemEval-2010 Task 8 dataset show that our model is comparable to the stateof-the-art without using any external lexical features. 1 Introduction Relation classification (RC) is a fundamental task in Natural Language Processing (NLP) that aims to identify semantic relations between pairs of marked entities in given sentences (instances). It has attracted much research effort as it plays a vital role in many NLP applications such as Information Extraction and Question Answering (Nguyen and Grishman, 2015). Traditional approaches (Kambhatla, 2004; Zhang et al., 2006) usually rely heavily on hand-crafted features and lexical resources, or elaborately designed kernels, which are time-consuming and challenging to adapt to novel domains. Recently, neural network (NN) models have dominated the work on RC since they can effectively learn meaningful hidden features without human intervention. However, most previous NN models only exploit one of the following structures to represent relation instances: raw word sequences (Zhou et al., 2016; Wang et al., 2016) and dependency trees (Wen, 2017; Le et al., 2018). While raw sequences can provide all the information of"
N19-1286,P16-2034,0,0.144372,"such as Information Extraction and Question Answering (Nguyen and Grishman, 2015). Traditional approaches (Kambhatla, 2004; Zhang et al., 2006) usually rely heavily on hand-crafted features and lexical resources, or elaborately designed kernels, which are time-consuming and challenging to adapt to novel domains. Recently, neural network (NN) models have dominated the work on RC since they can effectively learn meaningful hidden features without human intervention. However, most previous NN models only exploit one of the following structures to represent relation instances: raw word sequences (Zhou et al., 2016; Wang et al., 2016) and dependency trees (Wen, 2017; Le et al., 2018). While raw sequences can provide all the information of relation instances, they also add noise to the models from redundant information. While dependency tree structures help the models focus on the concise information captured by the shortest dependency path (SDP) between two entities, they lose some supplementary context in the raw sequence. It is clear that the raw sequence and SDP highly complement each other. We, therefore, combine them to be more effective in determining the relation without losing any information. W"
N19-1343,P16-1079,0,0.547122,"000, which is in the same weight class but is much slower and has less memory, and the T-1600, which also uses a 286 microprocessor, but which weighs almost twice as much and is three times the size,” we cannot find correct conjuncts for each coordinator at a glance. The presence of coordination makes a sentence more ambiguous and longer, resulting in errors in syntactic parsing. To identify the conjuncts of a given coordinator, previous studies have explored two properties of coordinate structures: (1) similarity – conjuncts tend to be similar; (2) replaceability – conjuncts can be replaced. Ficler and Goldberg (2016b) combine the syntactic parser and neural networks to compute the similarity and replaceability features of conjuncts. Teranishi et al. (2017) also exploit the two properties without deploying any syntactic parser, and achieve state-of-the-art results. Although both approaches outperform the similarity-based approaches (Shimbo and Hara, 2007; Hara et al., 2009), they cannot handle more than two conjuncts in a coordination, and multiple coordinations in a sentence at one time. Hence, their systems may produce coordinations that conflict with each other. In contrast, Hara et al. (2009) define p"
N19-1343,D16-1003,0,0.62932,"000, which is in the same weight class but is much slower and has less memory, and the T-1600, which also uses a 286 microprocessor, but which weighs almost twice as much and is three times the size,” we cannot find correct conjuncts for each coordinator at a glance. The presence of coordination makes a sentence more ambiguous and longer, resulting in errors in syntactic parsing. To identify the conjuncts of a given coordinator, previous studies have explored two properties of coordinate structures: (1) similarity – conjuncts tend to be similar; (2) replaceability – conjuncts can be replaced. Ficler and Goldberg (2016b) combine the syntactic parser and neural networks to compute the similarity and replaceability features of conjuncts. Teranishi et al. (2017) also exploit the two properties without deploying any syntactic parser, and achieve state-of-the-art results. Although both approaches outperform the similarity-based approaches (Shimbo and Hara, 2007; Hara et al., 2009), they cannot handle more than two conjuncts in a coordination, and multiple coordinations in a sentence at one time. Hence, their systems may produce coordinations that conflict with each other. In contrast, Hara et al. (2009) define p"
N19-1343,E12-1044,0,0.0208099,"computes scores based on the syntactic and morphological features assigned to edges and nodes in a sequence alignment. While their method focused on non-nested coordinations, Hara et al. (2009) extended their work to accommodate nested coordinations using CFG rules. A consistent global structure of coordinations is produced using discriminative functions based on the similarity of conjuncts with dynamic programming. Our concept of the CKY parsing is borrowed from their work; however, a key difference of our approach lies in how it computes the score of conjuncts and trains the score function. Hanamoto et al. (2012) used dual decomposition to combine HPSG parsing with the discriminative model developed by Hara et al. (2009). 5.2 Non Similarity-based Approaches Kawahara and Kurohashi (2008) focused on resolving the ambiguities of coordinate structures without the use of any similarities. Their method relied on the dependency relations surrounding the conjuncts and the generative probabilities of phrases. Yoshimoto et al. (2015) extended the Eis3401 ner algorithm by adding new rules to accommodate coordinations during dependency parsing. 5.3 Coordination Boundary Identification using Neural Networks Ficler"
N19-1343,P09-1109,1,0.934359,"ic parsing. To identify the conjuncts of a given coordinator, previous studies have explored two properties of coordinate structures: (1) similarity – conjuncts tend to be similar; (2) replaceability – conjuncts can be replaced. Ficler and Goldberg (2016b) combine the syntactic parser and neural networks to compute the similarity and replaceability features of conjuncts. Teranishi et al. (2017) also exploit the two properties without deploying any syntactic parser, and achieve state-of-the-art results. Although both approaches outperform the similarity-based approaches (Shimbo and Hara, 2007; Hara et al., 2009), they cannot handle more than two conjuncts in a coordination, and multiple coordinations in a sentence at one time. Hence, their systems may produce coordinations that conflict with each other. In contrast, Hara et al. (2009) define production rules for coordination in order to output consistent coordinate structures. Here, we propose a new framework for coordination boundary identification. We generalize a scoring function that takes a pair of spans with a coordinator and returns a higher score when the two spans appear to be coordinated. Using this function in the CKY parsing with producti"
N19-1343,P82-1020,0,0.8199,"Missing"
N19-1343,P07-1086,0,0.0441636,"hological information, and not contextual word senses, are clues for shorter and similar coordinations such as NP coordinations. For the feature extraction function of the outer-boundary scoring model, the concat function that performs the same function as the inner-boundary scoring model does not achieve competitive advantage. The feature function described as Eq. 12 is designed to capRelated Work Similarity-based Approaches For the coordination identification task in Japanese, Kurohashi and Nagao (1994) used a chart to find the highest similarity pair of conjuncts using dynamic programming. Hogan (2007) developed a generative parsing model for coordinated noun phrases, incorporating symmetry in conjunct structures and head words. Shimbo and Hara (2007) proposed a discriminative model that computes scores based on the syntactic and morphological features assigned to edges and nodes in a sequence alignment. While their method focused on non-nested coordinations, Hara et al. (2009) extended their work to accommodate nested coordinations using CFG rules. A consistent global structure of coordinations is produced using discriminative functions based on the similarity of conjuncts with dynamic pro"
N19-1343,C08-1054,0,0.0310004,"ons, Hara et al. (2009) extended their work to accommodate nested coordinations using CFG rules. A consistent global structure of coordinations is produced using discriminative functions based on the similarity of conjuncts with dynamic programming. Our concept of the CKY parsing is borrowed from their work; however, a key difference of our approach lies in how it computes the score of conjuncts and trains the score function. Hanamoto et al. (2012) used dual decomposition to combine HPSG parsing with the discriminative model developed by Hara et al. (2009). 5.2 Non Similarity-based Approaches Kawahara and Kurohashi (2008) focused on resolving the ambiguities of coordinate structures without the use of any similarities. Their method relied on the dependency relations surrounding the conjuncts and the generative probabilities of phrases. Yoshimoto et al. (2015) extended the Eis3401 ner algorithm by adding new rules to accommodate coordinations during dependency parsing. 5.3 Coordination Boundary Identification using Neural Networks Ficler and Goldberg (2016b) used neural networks for the coordination boundary identification task. They incorporated the replaceability property between conjuncts, in addition to the"
N19-1343,J94-4001,0,0.477605,"he other hand, the use of contextual embedding, ELMo, does not improve performance. We deduce that POS tags and morphological information, and not contextual word senses, are clues for shorter and similar coordinations such as NP coordinations. For the feature extraction function of the outer-boundary scoring model, the concat function that performs the same function as the inner-boundary scoring model does not achieve competitive advantage. The feature function described as Eq. 12 is designed to capRelated Work Similarity-based Approaches For the coordination identification task in Japanese, Kurohashi and Nagao (1994) used a chart to find the highest similarity pair of conjuncts using dynamic programming. Hogan (2007) developed a generative parsing model for coordinated noun phrases, incorporating symmetry in conjunct structures and head words. Shimbo and Hara (2007) proposed a discriminative model that computes scores based on the syntactic and morphological features assigned to edges and nodes in a sequence alignment. While their method focused on non-nested coordinations, Hara et al. (2009) extended their work to accommodate nested coordinations using CFG rules. A consistent global structure of coordina"
N19-1343,P16-1101,0,0.0285531,"nk beta (Kim et al., 2003) (GENIA). Unlike the evaluation by Teranishi et al. (2017) and Ficler and Goldberg (2016b), we strip the PTB of all quotation marks (“) and (”) to normalize irregular coordinations such as h. . . “Daybreak,” “Daywatch,” “Newsday,” and “Newsnight,” . . . i. We follow the standard train/development/test split on the PTB. For the GENIA, we do not apply the preprocessing described above. We evaluate the model through a five-fold cross-validation, as in Hara et al. (2009). 4.1.2 Model We use pretrained word vectors, POS tags, and character vectors produced by the CharCNN (Ma and Hovy, 2016), regarded as the default. We also investigate the performance of the model, using three different word representations for the encoder: (1) pretrained word embeddings; GloVe (Pennington et al., 2014) for the PTB, BioASQ (Tsatsaronis et al., 2012) for the GENIA, (2) contextualized sentence embeddings; ELMo, (3) randomly initialized word vectors. For the PTB, POS tags are obtained using the Stanford POS Tagger (Toutanova et al., 2003) with 10-way jackknifing. For the GENIA, we use the gold POS tags, as in Hara et al. (2009). To optimize the model parameters, we use Adam (Kingma and Ba, 2015). O"
N19-1343,D14-1162,0,0.087859,"oordinations such as h. . . “Daybreak,” “Daywatch,” “Newsday,” and “Newsnight,” . . . i. We follow the standard train/development/test split on the PTB. For the GENIA, we do not apply the preprocessing described above. We evaluate the model through a five-fold cross-validation, as in Hara et al. (2009). 4.1.2 Model We use pretrained word vectors, POS tags, and character vectors produced by the CharCNN (Ma and Hovy, 2016), regarded as the default. We also investigate the performance of the model, using three different word representations for the encoder: (1) pretrained word embeddings; GloVe (Pennington et al., 2014) for the PTB, BioASQ (Tsatsaronis et al., 2012) for the GENIA, (2) contextualized sentence embeddings; ELMo, (3) randomly initialized word vectors. For the PTB, POS tags are obtained using the Stanford POS Tagger (Toutanova et al., 2003) with 10-way jackknifing. For the GENIA, we use the gold POS tags, as in Hara et al. (2009). To optimize the model parameters, we use Adam (Kingma and Ba, 2015). Other hyperparameters are described in Appendix A. 4.1.3 Baseline Model We adopt our implementation of Teranishi et al. (2017) as the baseline. The original model of Teranishi et al. (2017) predicts th"
N19-1343,N18-1202,0,0.0237265,"es the vector to a MLP. ff eature (bl , er , wt , h1:N ) =   hbl − ht+1 ; her − ht−1 Coordinator Classifier We use a linear transformation of the sentencelevel representation of a coordinator key for fckey . fckey (wt ) = Wckey ht + bckey (10) (12) fouter (bl , er , wt ) = out w2out ReLU(W1out r) + bout 1 ) + b2 l (13) r r = ff eature (b , e , wt , h1:N ) (9) The dimensionality of each resulting vector ht is 2dhidden . For the BiLSTMs inputs, we use finput to map words and POS tags onto their representations. We can use different word representations including a pretrained word model, ELMo (Peters et al., 2018), BERT (Devlin et al., 2018) or character-level LSTMs/convolutional neural networks (CharCNNs). We demonstrate the differences between the different choices in Section 4. The entire network consisting of finput and BiLSTMs is referred to as the encoder; it is shared by the three neural networks in the higher layer. in Outer-Boundary Scoring Model Encoder To get sentence-level representations for a sequence of words and POS tags, we use bidirectional long short-term memories (BiLSTMs) (Hochreiter and Schmidhuber, 1997). h1:N = BiLSTMs(finput (w1:N , p1:N )) hidden d , win ∈ where W1in ∈ Rd ×4d"
N19-1343,C18-1011,0,0.0248453,"function in the CKY parsing with production rules for coordination, our system produces globally consistent coordinations in a given sentence. To obtain such a function, we decompose the task into three independent subtasks – finding a coordinator, identifying the inner boundaries of a pair of conjuncts and delineating its outer boundaries. We use three different neural networks for the tasks, and the networks are trained on the basis of their local decisions. Our method is inspired by recent successes with locally-trained models for structured inference problems such as constituency parsing (Teng and Zhang, 2018) and dependency parsing (Dozat and Manning, 2017) without globally-optimized training. Experimental results reveal that our model outperforms existing systems and our strong baseline, an extension of Teranishi et al. (2017), and ensures that the global structure of the coordinations is consistent. In summary, our contributions include the following: • We propose a simple framework that trains a generalized scoring function of a pair of conjuncts and uses it for inference. • We decompose the task and use three local models that interoperate for the CKY parsing. 3394 Proceedings of NAACL-HLT 201"
N19-1343,I17-1027,1,0.524882,"ighs almost twice as much and is three times the size,” we cannot find correct conjuncts for each coordinator at a glance. The presence of coordination makes a sentence more ambiguous and longer, resulting in errors in syntactic parsing. To identify the conjuncts of a given coordinator, previous studies have explored two properties of coordinate structures: (1) similarity – conjuncts tend to be similar; (2) replaceability – conjuncts can be replaced. Ficler and Goldberg (2016b) combine the syntactic parser and neural networks to compute the similarity and replaceability features of conjuncts. Teranishi et al. (2017) also exploit the two properties without deploying any syntactic parser, and achieve state-of-the-art results. Although both approaches outperform the similarity-based approaches (Shimbo and Hara, 2007; Hara et al., 2009), they cannot handle more than two conjuncts in a coordination, and multiple coordinations in a sentence at one time. Hence, their systems may produce coordinations that conflict with each other. In contrast, Hara et al. (2009) define production rules for coordination in order to output consistent coordinate structures. Here, we propose a new framework for coordination boundar"
N19-1343,N03-1033,0,0.0469421,"el through a five-fold cross-validation, as in Hara et al. (2009). 4.1.2 Model We use pretrained word vectors, POS tags, and character vectors produced by the CharCNN (Ma and Hovy, 2016), regarded as the default. We also investigate the performance of the model, using three different word representations for the encoder: (1) pretrained word embeddings; GloVe (Pennington et al., 2014) for the PTB, BioASQ (Tsatsaronis et al., 2012) for the GENIA, (2) contextualized sentence embeddings; ELMo, (3) randomly initialized word vectors. For the PTB, POS tags are obtained using the Stanford POS Tagger (Toutanova et al., 2003) with 10-way jackknifing. For the GENIA, we use the gold POS tags, as in Hara et al. (2009). To optimize the model parameters, we use Adam (Kingma and Ba, 2015). Other hyperparameters are described in Appendix A. 4.1.3 Baseline Model We adopt our implementation of Teranishi et al. (2017) as the baseline. The original model of Teranishi et al. (2017) predicts the beginning and the end of a coordinate structure, and then splits it into conjuncts by commas. Their model decides the boundary of a coordinate structure individually, which may cause conflicts with that of other coordinate structure(s)"
N19-1343,W15-2208,1,0.840907,". Our concept of the CKY parsing is borrowed from their work; however, a key difference of our approach lies in how it computes the score of conjuncts and trains the score function. Hanamoto et al. (2012) used dual decomposition to combine HPSG parsing with the discriminative model developed by Hara et al. (2009). 5.2 Non Similarity-based Approaches Kawahara and Kurohashi (2008) focused on resolving the ambiguities of coordinate structures without the use of any similarities. Their method relied on the dependency relations surrounding the conjuncts and the generative probabilities of phrases. Yoshimoto et al. (2015) extended the Eis3401 ner algorithm by adding new rules to accommodate coordinations during dependency parsing. 5.3 Coordination Boundary Identification using Neural Networks Ficler and Goldberg (2016b) used neural networks for the coordination boundary identification task. They incorporated the replaceability property between conjuncts, in addition to the similarity property, in the computation of a score for a pair of conjuncts. They first used a binary classifier for coordinating words; then, they extracted probable candidate pairs of conjuncts using the Berkeley Parser (Petrov et al., 2006"
N19-1343,P06-1055,0,0.0499212,"imoto et al. (2015) extended the Eis3401 ner algorithm by adding new rules to accommodate coordinations during dependency parsing. 5.3 Coordination Boundary Identification using Neural Networks Ficler and Goldberg (2016b) used neural networks for the coordination boundary identification task. They incorporated the replaceability property between conjuncts, in addition to the similarity property, in the computation of a score for a pair of conjuncts. They first used a binary classifier for coordinating words; then, they extracted probable candidate pairs of conjuncts using the Berkeley Parser (Petrov et al., 2006); afterward, they assigned scores to the pairs using neural networks. However, the shortcoming of their work is that it is highly dependent on the external parser. The work of Teranishi et al. (2017) developed an end-to-end model, as opposed to the pipeline approach of Ficler and Goldberg (2016b). They also used similarity and replaceability feature representations without information from a syntactic parser. While Ficler and Goldberg (2016b) cut off improbable pairs of conjuncts ahead of training, Teranishi et al. (2017) calculated scores for all possible pairs of the beginning and the end of"
N19-1343,D07-1064,0,0.489229,"ng in errors in syntactic parsing. To identify the conjuncts of a given coordinator, previous studies have explored two properties of coordinate structures: (1) similarity – conjuncts tend to be similar; (2) replaceability – conjuncts can be replaced. Ficler and Goldberg (2016b) combine the syntactic parser and neural networks to compute the similarity and replaceability features of conjuncts. Teranishi et al. (2017) also exploit the two properties without deploying any syntactic parser, and achieve state-of-the-art results. Although both approaches outperform the similarity-based approaches (Shimbo and Hara, 2007; Hara et al., 2009), they cannot handle more than two conjuncts in a coordination, and multiple coordinations in a sentence at one time. Hence, their systems may produce coordinations that conflict with each other. In contrast, Hara et al. (2009) define production rules for coordination in order to output consistent coordinate structures. Here, we propose a new framework for coordination boundary identification. We generalize a scoring function that takes a pair of spans with a coordinator and returns a higher score when the two spans appear to be coordinated. Using this function in the CKY p"
O00-3003,J93-1003,0,0.0190652,"Missing"
O00-3003,1998.amta-tutorials.5,0,0.0870078,"Missing"
O00-3003,W00-0803,1,0.858545,"Missing"
O00-3003,J96-3004,0,0.0381457,"Missing"
O00-3003,J90-1003,0,\N,Missing
O05-4005,O97-1011,0,0.831118,"Missing"
O05-4005,C02-1049,0,0.178643,"Missing"
O05-4005,Y03-1009,0,0.0209175,"Missing"
O05-4005,N01-1025,1,0.865225,"Missing"
O05-4005,W03-1701,0,0.23614,"Missing"
O05-4005,C02-1055,0,0.264809,"Missing"
O05-4005,W03-1705,0,0.175471,"Missing"
O05-4005,E99-1023,0,0.099543,"Missing"
O05-4005,W03-1719,0,0.0912641,"Missing"
O05-4005,P00-1042,0,0.0566609,"Missing"
O05-4005,W02-1815,0,0.0917483,"Missing"
O05-4005,W03-1728,0,0.201181,"Missing"
O05-4005,W02-1817,0,0.218727,"Missing"
O05-4005,W03-1730,0,0.033193,"Missing"
O08-4003,O08-4003,1,0.0530913,"Missing"
O08-4003,W04-3205,0,0.164092,"Missing"
O08-4003,P04-1074,0,0.230621,"Missing"
O08-4003,P06-1095,0,0.39501,"Missing"
O08-4003,H05-1066,0,0.0712675,"Missing"
O08-4003,S07-1014,0,0.120093,"Missing"
O08-4003,I05-1061,0,0.292541,"Missing"
ogiso-etal-2012-unidic,W04-3230,1,\N,Missing
ogiso-etal-2012-unidic,den-etal-2008-proper,1,\N,Missing
ogiso-etal-2012-unidic,maekawa-etal-2010-design,1,\N,Missing
P02-1059,C96-2166,0,0.310707,"ked list of sentences. A summary with compression rate γ is obtained by selecting top γ percent of the list. When coupled with FindDiversity, on the other hand, each ProbDT is set to work on each cluster discovered by the diversity component, producing multiple lists of sentences, each corresponding to one of the clusters identified. A summary is formed by collecting top ranking sentences from each list. Evaluation was done by 10-fold cross validation. For the purpose of comparison, we also ran the diversity based model as given in Nomoto and Matsumoto (2001c) and a tfidf based ranking model (Zechner, 1996) (call it Z model), which simply ranks sentences according to the tfidf score and selects those which rank highest. Recall that the diversity based model (DBS) (Nomoto and Matsumoto, 2001c) consists in Find-Diversity and the ranking model by Zechner (1996), which they call Reduce-Redundancy. 7 Results and Discussion Tables 4-8 show performance of each ProbDT and its combination with the diversity (clustering) component. It also shows performance of Z model and DBS. In the tables, the slashed ‘V’ after the name of a classifier indicates that the relevant classifier is diversity-enabled, meaning"
P02-1063,W01-0502,0,0.0380416,"Missing"
P02-1063,W00-0730,1,0.800798,"is to decide which learning model to use. Various learning models have been studied such as Hidden Markov models (HMMs) (Rabiner and Juang, 1993), decision trees (Breiman et al., 1984) and maximum entropy models (Berger et al., 1996). Recently, Support Vector Machines (SVMs) (Vapnik, 1998; Cortes and Vapnik, 1995) are getting to be used, which are supervised machine learning algorithm for binary classification. SVMs have good generalization performance and can handle a large number of features, and are applied to some tasks ∗ Presently with Oki Electric Industry successfully (Joachims, 1998; Kudoh and Matsumoto, 2000). However, their computational cost is large and is a weakness of SVMs. In general, a trade-off between capacity and computational cost of learning models exists. For example, SVMs have relatively high generalization capacity, but have high computational cost. On the other hand, HMMs have lower computational cost, but have lower capacity and difficulty in handling data with a large number of features. Learning models with higher capacity may not be of practical use because of their prohibitive computational cost. This problem becomes more serious when a large amount of data is used. To solve t"
P02-1063,W99-0608,0,0.0349903,"Missing"
P02-1063,J96-1002,0,0.00947952,"tof-speech tagging and Japanese morphological analysis, and show that the method performs well. 1 Introduction Recently, corpus-based approaches have been widely studied in many natural language processing tasks, such as part-of-speech (POS) tagging, syntactic analysis, text categorization and word sense disambiguation. In corpus-based natural language processing, one important issue is to decide which learning model to use. Various learning models have been studied such as Hidden Markov models (HMMs) (Rabiner and Juang, 1993), decision trees (Breiman et al., 1984) and maximum entropy models (Berger et al., 1996). Recently, Support Vector Machines (SVMs) (Vapnik, 1998; Cortes and Vapnik, 1995) are getting to be used, which are supervised machine learning algorithm for binary classification. SVMs have good generalization performance and can handle a large number of features, and are applied to some tasks ∗ Presently with Oki Electric Industry successfully (Joachims, 1998; Kudoh and Matsumoto, 2000). However, their computational cost is large and is a weakness of SVMs. In general, a trade-off between capacity and computational cost of learning models exists. For example, SVMs have relatively high genera"
P02-1063,A00-1031,0,0.00946168,"eriments of English Part-of-Speech Tagging Experiments of English POS tagging with revision learning (RL) are performed on the Penn Treebank WSJ corpus. The corpus is randomly separated into training data of 41,342 sentences and test data of 11,771 sentences. The dictionary for HMMs is constructed from all the words in the training data. T3 of ICOPOST release 0.9.0 (Schr¨ oder, 2001) is used as the stochastic model for ranking stage. This is equivalent to POS-based second order HMMs. SVMs with second order polynomial kernel are used as the binary classifier. The results are compared with TnT (Brants, 2000) based on second order HMMs, and with POS tagger using SVMs with one-versus-rest (1v-r) (Nakagawa et al., 2001). The accuracies of those systems for known words, unknown words and all the words are shown in Table 1. The accuracies for both known words and unknown words are improved through revision learning. However, revision learning could not surpass the one-versus-rest. The main difference in the accuracies stems from those for unknown words. The reason for that seems to be that the dictionary of HMMs for POS tagging is obtained from the training data, as a result, virtually no unknown word"
P02-1063,P98-1029,0,0.0411245,"Missing"
P02-1063,J95-4004,0,0.263954,"Missing"
P02-1063,J01-2002,0,\N,Missing
P02-1063,C98-1029,0,\N,Missing
P03-1004,C02-1054,0,0.799352,"lassifier into a simple and fast linear classifier. Experimental results on English BaseNP Chunking, Japanese Word Segmentation and Japanese Dependency Parsing show that our new classifiers are about 30 to 300 times faster than the standard kernel-based classifiers. 1 Introduction Kernel methods (e.g., Support Vector Machines (Vapnik, 1995)) attract a great deal of attention recently. In the field of Natural Language Processing, many successes have been reported. Examples include Part-of-Speech tagging (Nakagawa et al., 2002) Text Chunking (Kudo and Matsumoto, 2001), Named Entity Recognition (Isozaki and Kazawa, 2002), and Japanese Dependency Parsing (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2002). It is known in NLP that combination of features contributes to a significant improvement in accuracy. For instance, in the task of dependency parsing, it would be hard to confirm a correct dependency relation with only a single set of features from either a head or its modifier. Rather, dependency relations should be determined by at least information from both of two phrases. In previous research, feature combination has been selected manually, and the performance significantly depended on these selections"
P03-1004,W00-1303,1,0.585036,"ntal results on English BaseNP Chunking, Japanese Word Segmentation and Japanese Dependency Parsing show that our new classifiers are about 30 to 300 times faster than the standard kernel-based classifiers. 1 Introduction Kernel methods (e.g., Support Vector Machines (Vapnik, 1995)) attract a great deal of attention recently. In the field of Natural Language Processing, many successes have been reported. Examples include Part-of-Speech tagging (Nakagawa et al., 2002) Text Chunking (Kudo and Matsumoto, 2001), Named Entity Recognition (Isozaki and Kazawa, 2002), and Japanese Dependency Parsing (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2002). It is known in NLP that combination of features contributes to a significant improvement in accuracy. For instance, in the task of dependency parsing, it would be hard to confirm a correct dependency relation with only a single set of features from either a head or its modifier. Rather, dependency relations should be determined by at least information from both of two phrases. In previous research, feature combination has been selected manually, and the performance significantly depended on these selections. This is not the case with kernel-based methodology. For i"
P03-1004,N01-1025,1,0.565063,"a Basket Mining algorithm to convert a kernel-based classifier into a simple and fast linear classifier. Experimental results on English BaseNP Chunking, Japanese Word Segmentation and Japanese Dependency Parsing show that our new classifiers are about 30 to 300 times faster than the standard kernel-based classifiers. 1 Introduction Kernel methods (e.g., Support Vector Machines (Vapnik, 1995)) attract a great deal of attention recently. In the field of Natural Language Processing, many successes have been reported. Examples include Part-of-Speech tagging (Nakagawa et al., 2002) Text Chunking (Kudo and Matsumoto, 2001), Named Entity Recognition (Isozaki and Kazawa, 2002), and Japanese Dependency Parsing (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2002). It is known in NLP that combination of features contributes to a significant improvement in accuracy. For instance, in the task of dependency parsing, it would be hard to confirm a correct dependency relation with only a single set of features from either a head or its modifier. Rather, dependency relations should be determined by at least information from both of two phrases. In previous research, feature combination has been selected manually, and the p"
P03-1004,W02-2016,1,0.485815,"seNP Chunking, Japanese Word Segmentation and Japanese Dependency Parsing show that our new classifiers are about 30 to 300 times faster than the standard kernel-based classifiers. 1 Introduction Kernel methods (e.g., Support Vector Machines (Vapnik, 1995)) attract a great deal of attention recently. In the field of Natural Language Processing, many successes have been reported. Examples include Part-of-Speech tagging (Nakagawa et al., 2002) Text Chunking (Kudo and Matsumoto, 2001), Named Entity Recognition (Isozaki and Kazawa, 2002), and Japanese Dependency Parsing (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2002). It is known in NLP that combination of features contributes to a significant improvement in accuracy. For instance, in the task of dependency parsing, it would be hard to confirm a correct dependency relation with only a single set of features from either a head or its modifier. Rather, dependency relations should be determined by at least information from both of two phrases. In previous research, feature combination has been selected manually, and the performance significantly depended on these selections. This is not the case with kernel-based methodology. For instance, if we use a polyno"
P03-1004,W95-0107,0,0.00718148,"verlapping phrases. BaseNP chunking deals with a part of this task and recognizes the chunks that form noun phrases. Here is an example sentence: [He] reckons [the current account deficit] will narrow to [only $ 1.8 billion] . A BaseNP chunk is represented as sequence of words between square brackets. BaseNP chunking task is usually formulated as a simple tagging task, where we represent chunks with three types of tags: B: beginning of a chunk. I: non-initial word. O: outside of the chunk. In our experiments, we used the same settings as (Kudo and Matsumoto, 2002). We use a standard data set (Ramshaw and Marcus, 1995) consisting of sections 15-19 of the WSJ corpus as training and section 20 as testing. 5.2 Japanese Word Segmentation (JWS) 5.4 Results Tables 2, 3 and 4 show the execution time, accuracy4 , and |Ω |(size of extracted subsets), by changing σ from 0.01 to 0.0005. The PKI leads to about 2 to 12 times improvements over the PKB. In JDP, the improvement is significant. This is because B, the average of h(i) over Let s = c1 c2 · · · cm be a sequence of Japanese all items i ∈ F , is relatively small in JDP. The imcharacters, t = t1 t2 · · · tm be a sequence of Japanese provement significantly depends"
P03-1004,P02-1063,1,\N,Missing
P03-1057,2001.mtsummit-papers.3,1,0.436254,"02)). We employ the second approach in this paper. The cutoff by frequency (Menezes and Richardson, 2001) and the hypothesis test (Imamura, 2002) have been applied to clean the rules. The cutoff by frequency can slightly improve MT quality, but the improvement is still insufficient from the viewpoint of the large number of redundant rules. The hypothesis test requires very large corpora in order to obtain a sufficient number of rules that are statistically confident. Another current topic of machine translation is automatic evaluation of MT quality (Papineni et al., 2002; Yasuda et al., 2001; Akiba et al., 2001). These methods aim to replace subjective evaluation in order to speed up the development cycle of MT systems. However, they can be utilized not only as developers’ aids but also for automatic tuning of MT systems (Su et al., 1992). We propose feedback cleaning that utilizes an automatic evaluation for removing incorrect/redundant translation rules as a tuning method Feedback Cleaning Evaluation Corpus Training Corpus Automatic Acquisition Translation Rules MT Engine MT Results Rule Selection/Deletion Automatic Evaluation Figure 1: Structure of Feedback Cleaning (Figure 1). Our method evaluate"
P03-1057,C94-1015,0,0.0544641,"en the system translates an input sentence, the sentence is first parsed by using source patterns of the transfer rules. Next, a tree structure of the target language is generated by mapping the source patterns to the corresponding target patterns. When non-terminal symbols remain in the target tree, target words are inserted by referring to a translation dictionary. Ambiguities, which occur during parsing or mapping, are resolved by selecting the rules that minimize the semantic distance between the input words and source examples (real examples in the training corpus) of the transfer rules (Furuse and Iida, 1994). For instance, when the input phrase “leave at 11 a.m.” is translated into Japanese, Rule 2 in Figure 2 is selected because the semantic distance from the source example (arrive, p.m.) is the shortest to the head words of the input phrase (leave, a.m.). 2.2 Problems of Automatic Acquisition HPAT automatically acquires its transfer rules from parallel corpora by using Hierarchical Phrase Alignment (Imamura, 2001). However, the rule set contains many incorrect/redundant rules. The reasons for this problem are roughly classified as follows. • Errors in automatic rule acquisition • Translation va"
P03-1057,2002.tmi-papers.9,1,0.934691,"corpora. Such rules conflict with other existing rules and cause implausible Yuji Matsumoto Nara Institute of Science and Technology Ikoma-shi, Nara, Japan matsu@is.aist-nara.ac.jp MT results or increase ambiguity. If incorrect rules could be avoided, MT quality would necessarily improve. There are two approaches to overcoming incorrect/redundant rules: • Selecting appropriate rules in a disambiguation process during the translation (on-line processing, (Meyers et al., 2000)). • Cleaning incorrect/redundant rules after automatic acquisition (off-line processing, (Menezes and Richardson, 2001; Imamura, 2002)). We employ the second approach in this paper. The cutoff by frequency (Menezes and Richardson, 2001) and the hypothesis test (Imamura, 2002) have been applied to clean the rules. The cutoff by frequency can slightly improve MT quality, but the improvement is still insufficient from the viewpoint of the large number of redundant rules. The hypothesis test requires very large corpora in order to obtain a sufficient number of rules that are statistically confident. Another current topic of machine translation is automatic evaluation of MT quality (Papineni et al., 2002; Yasuda et al., 2001; Aki"
P03-1057,W01-1406,0,0.0360282,"or translation variety in the corpora. Such rules conflict with other existing rules and cause implausible Yuji Matsumoto Nara Institute of Science and Technology Ikoma-shi, Nara, Japan matsu@is.aist-nara.ac.jp MT results or increase ambiguity. If incorrect rules could be avoided, MT quality would necessarily improve. There are two approaches to overcoming incorrect/redundant rules: • Selecting appropriate rules in a disambiguation process during the translation (on-line processing, (Meyers et al., 2000)). • Cleaning incorrect/redundant rules after automatic acquisition (off-line processing, (Menezes and Richardson, 2001; Imamura, 2002)). We employ the second approach in this paper. The cutoff by frequency (Menezes and Richardson, 2001) and the hypothesis test (Imamura, 2002) have been applied to clean the rules. The cutoff by frequency can slightly improve MT quality, but the improvement is still insufficient from the viewpoint of the large number of redundant rules. The hypothesis test requires very large corpora in order to obtain a sufficient number of rules that are statistically confident. Another current topic of machine translation is automatic evaluation of MT quality (Papineni et al., 2002; Yasuda e"
P03-1057,C00-1078,0,0.0461382,"e for transferbased MT acquired from corpora contains many incorrect/redundant rules due to acquisition errors or translation variety in the corpora. Such rules conflict with other existing rules and cause implausible Yuji Matsumoto Nara Institute of Science and Technology Ikoma-shi, Nara, Japan matsu@is.aist-nara.ac.jp MT results or increase ambiguity. If incorrect rules could be avoided, MT quality would necessarily improve. There are two approaches to overcoming incorrect/redundant rules: • Selecting appropriate rules in a disambiguation process during the translation (on-line processing, (Meyers et al., 2000)). • Cleaning incorrect/redundant rules after automatic acquisition (off-line processing, (Menezes and Richardson, 2001; Imamura, 2002)). We employ the second approach in this paper. The cutoff by frequency (Menezes and Richardson, 2001) and the hypothesis test (Imamura, 2002) have been applied to clean the rules. The cutoff by frequency can slightly improve MT quality, but the improvement is still insufficient from the viewpoint of the large number of redundant rules. The hypothesis test requires very large corpora in order to obtain a sufficient number of rules that are statistically confide"
P03-1057,P02-1040,0,0.124322,", (Menezes and Richardson, 2001; Imamura, 2002)). We employ the second approach in this paper. The cutoff by frequency (Menezes and Richardson, 2001) and the hypothesis test (Imamura, 2002) have been applied to clean the rules. The cutoff by frequency can slightly improve MT quality, but the improvement is still insufficient from the viewpoint of the large number of redundant rules. The hypothesis test requires very large corpora in order to obtain a sufficient number of rules that are statistically confident. Another current topic of machine translation is automatic evaluation of MT quality (Papineni et al., 2002; Yasuda et al., 2001; Akiba et al., 2001). These methods aim to replace subjective evaluation in order to speed up the development cycle of MT systems. However, they can be utilized not only as developers’ aids but also for automatic tuning of MT systems (Su et al., 1992). We propose feedback cleaning that utilizes an automatic evaluation for removing incorrect/redundant translation rules as a tuning method Feedback Cleaning Evaluation Corpus Training Corpus Automatic Acquisition Translation Rules MT Engine MT Results Rule Selection/Deletion Automatic Evaluation Figure 1: Structure of Feedbac"
P03-1057,C92-2067,0,0.186427,"ality, but the improvement is still insufficient from the viewpoint of the large number of redundant rules. The hypothesis test requires very large corpora in order to obtain a sufficient number of rules that are statistically confident. Another current topic of machine translation is automatic evaluation of MT quality (Papineni et al., 2002; Yasuda et al., 2001; Akiba et al., 2001). These methods aim to replace subjective evaluation in order to speed up the development cycle of MT systems. However, they can be utilized not only as developers’ aids but also for automatic tuning of MT systems (Su et al., 1992). We propose feedback cleaning that utilizes an automatic evaluation for removing incorrect/redundant translation rules as a tuning method Feedback Cleaning Evaluation Corpus Training Corpus Automatic Acquisition Translation Rules MT Engine MT Results Rule Selection/Deletion Automatic Evaluation Figure 1: Structure of Feedback Cleaning (Figure 1). Our method evaluates the contribution of each rule to the MT results and removes inappropriate rules as a way to increase the evaluation scores. Since the automatic evaluation correlates with a subjective evaluation, MT quality will improve after cle"
P03-1057,takezawa-etal-2002-toward,1,0.054083,"rule set is a subset of the base rule set. 4. Apply the feedback cleaning algorithm to each of the N pairs and record the rule contributions even if the rules are removed. The purpose of this step is to obtain the rule contributions. 5. For each rule in the base rule set, sum up the rule contributions obtained from the rule subsets. If the sum is negative, remove the rule from the base rule set. The major difference of this method from crossvalidation is Step 5. In the case of cross-cleaning, Bilingual Corpora The corpus used in the following experiments is the Basic Travel Expression Corpus (Takezawa et al., 2002). This is a collection of Japanese sentences and their English translations based on expressions that are usually found in phrasebooks for foreign tourists. We divided it into sub-corpora for training, evaluation, and test as shown in Table 1. The number of rules acquired from the training corpus (the base rule set size) was 105,588. Evaluation Methods of MT Quality We used the following two methods to evaluate MT quality. 1. Test Corpus BLEU Score The BLUE score was calculated with the test corpus. The number of references was one for each sentence, in the same way used for the feedback clean"
P03-1057,2001.mtsummit-papers.67,0,0.0955976,"on, 2001; Imamura, 2002)). We employ the second approach in this paper. The cutoff by frequency (Menezes and Richardson, 2001) and the hypothesis test (Imamura, 2002) have been applied to clean the rules. The cutoff by frequency can slightly improve MT quality, but the improvement is still insufficient from the viewpoint of the large number of redundant rules. The hypothesis test requires very large corpora in order to obtain a sufficient number of rules that are statistically confident. Another current topic of machine translation is automatic evaluation of MT quality (Papineni et al., 2002; Yasuda et al., 2001; Akiba et al., 2001). These methods aim to replace subjective evaluation in order to speed up the development cycle of MT systems. However, they can be utilized not only as developers’ aids but also for automatic tuning of MT systems (Su et al., 1992). We propose feedback cleaning that utilizes an automatic evaluation for removing incorrect/redundant translation rules as a tuning method Feedback Cleaning Evaluation Corpus Training Corpus Automatic Acquisition Translation Rules MT Engine MT Results Rule Selection/Deletion Automatic Evaluation Figure 1: Structure of Feedback Cleaning (Figure 1)"
P03-1057,E03-1010,0,0.0111989,"output scores are applicable to feedback cleaning. The characteristics common to these methods, including BLEU, is that the similarity to references are measured for each sentence, and the evaluation score of an MT system is calculated by aggregating the similarities. Therefore, MT results of the evaluation corpus are necessary to evaluate the system, and reducing the number of sentence translations is an important technique for all of these methods. The effects of feedback cleaning depend on the characteristics of objective measures. DP-based measures and BLEU have different characteristics (Yasuda et al., 2003). The exploration of several measures for feedback cleaning remains an interesting future work. 7.2 Domain Adaptation When applying corpus-based machine translation to a different domain, bilingual corpora of the new domain are necessary. However, the sizes of the new corpora are generally smaller than that of the original corpus because the collection of bilingual sentences requires a high cost. The feedback cleaning proposed in this paper can be interpreted as adapting the translation rules so that the MT results become similar to the evaluation corpus. Therefore, if we regard the bilingual"
P03-1057,2001.mtsummit-ebmt.4,0,\N,Missing
P03-2039,N01-1025,1,0.561182,"nization names in the corpus. For general unknown word, all words that occurred only once in the corpus were deleted from the dictionary, and were treated as unknown words. 12,730 unknown words were created under this condition. 4 Results Character types can also be used as features for chunking. However, the only information at our disposal is the possibility for a character to be a family name. The set of characters used for transliteration may also be useful for retrieving transliterated names. 2.3 Chunking with Support Vector Machine We use a Support Vector Machines-based chunker, YamCha (Kudo and Matsumoto, 2001), to extract We now present the results of our experiments in recall, precision and F-measure, as usual in such experiments. 4.1 Person Name Extraction Table 2 shows the results of person name extraction. The accuracy for retrieving person names was quite satisfiable. We could also extract names overlapping with the next known word. For example, for /v /f /v /v the sequence “ /Ng /Ag        Position i-2 i-1 i i+1 i+2 Char.     POS(best) n-S Ag-S Ng-S n-B n-E Family Name Y N N N Y Chunk B I I O O Figure 1: An illustration of chunking process ‘President Jiang Zemin’  /u   /n”"
P03-2039,W02-1817,0,0.171219,"Missing"
P03-2039,C02-1049,0,\N,Missing
P06-1079,W04-3239,1,0.887184,"Missing"
P06-1079,J94-4002,0,0.200142,"logy 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan {ryu-i,inui,matsu}@is.naist.jp Abstract can not be interpreted only by shallow syntactic parsing, a model specialized for zero-anaphora resolution needs to be devised on the top of shallow syntactic and semantic processing. Recent work on zero-anaphora resolution can be located in two different research contexts. First, zero-anaphora resolution is studied in the context of anaphora resolution (AR), in which zeroanaphora is regarded as a subclass of anaphora. In AR, the research trend has been shifting from rulebased approaches (Baldwin, 1995; Lappin and Leass, 1994; Mitkov, 1997, etc.) to empirical, or corpus-based, approaches (McCarthy and Lehnert, 1995; Ng and Cardie, 2002a; Soon et al., 2001; Strube and M¨uller, 2003; Yang et al., 2003) because the latter are shown to be a cost-efficient solution achieving a performance that is comparable to best performing rule-based systems (see the Coreference task in MUC1 and the Entity Detection and Tracking task in the ACE program2 ). The same trend is observed also in Japanese zeroanaphora resolution, where the findings made in rule-based or theory-oriented work (Kameyama, 1986; Nakaiwa and Shirai, 1996; Okumu"
P06-1079,W97-1303,0,0.0566686,"oma, Nara, 630-0192, Japan {ryu-i,inui,matsu}@is.naist.jp Abstract can not be interpreted only by shallow syntactic parsing, a model specialized for zero-anaphora resolution needs to be devised on the top of shallow syntactic and semantic processing. Recent work on zero-anaphora resolution can be located in two different research contexts. First, zero-anaphora resolution is studied in the context of anaphora resolution (AR), in which zeroanaphora is regarded as a subclass of anaphora. In AR, the research trend has been shifting from rulebased approaches (Baldwin, 1995; Lappin and Leass, 1994; Mitkov, 1997, etc.) to empirical, or corpus-based, approaches (McCarthy and Lehnert, 1995; Ng and Cardie, 2002a; Soon et al., 2001; Strube and M¨uller, 2003; Yang et al., 2003) because the latter are shown to be a cost-efficient solution achieving a performance that is comparable to best performing rule-based systems (see the Coreference task in MUC1 and the Entity Detection and Tracking task in the ACE program2 ). The same trend is observed also in Japanese zeroanaphora resolution, where the findings made in rule-based or theory-oriented work (Kameyama, 1986; Nakaiwa and Shirai, 1996; Okumura and Tamura,"
P06-1079,C96-2137,0,0.0779105,"n, 1995; Lappin and Leass, 1994; Mitkov, 1997, etc.) to empirical, or corpus-based, approaches (McCarthy and Lehnert, 1995; Ng and Cardie, 2002a; Soon et al., 2001; Strube and M¨uller, 2003; Yang et al., 2003) because the latter are shown to be a cost-efficient solution achieving a performance that is comparable to best performing rule-based systems (see the Coreference task in MUC1 and the Entity Detection and Tracking task in the ACE program2 ). The same trend is observed also in Japanese zeroanaphora resolution, where the findings made in rule-based or theory-oriented work (Kameyama, 1986; Nakaiwa and Shirai, 1996; Okumura and Tamura, 1996, etc.) have been successfully incorporated in machine learning-based frameworks (Seki et al., 2002; Iida et al., 2003). Second, the task of zero-anaphora resolution has some overlap with Propbank3 -style semantic role labeling (SRL), which has been intensively studied, for example, in the context of the CoNLL SRL task4 . In this task, given a sentence “To attract younger listeners, Radio Free Europe intersperses the latest in Western rock groups”, an SRL We approach the zero-anaphora resolution problem by decomposing it into intra-sentential and inter-sentential zero"
P06-1079,P04-1020,0,0.119987,"e are two alternative ways for anaphoricity determination: the single-step model and the two-step model. The single-step model (Soon et al., 2001; Ng and Cardie, 2002a) determines the anaphoricity of a given anaphor indirectly as a by-product of the search for its antecedent. If an appropriate candidate antecedent is found, the anaphor is classified as anaphoric; otherwise, it is classified as non-anaphoric. One disadvantage of this model is that it cannot employ the preferencebased model because the preference-based model is not capable of identifying non-anaphoric cases. The two-step model (Ng, 2004; Poesio et al., 2004; Iida et al., 2005), on the other hand, carries out anaphoricity determination in a separate step from antecedent identification. Poesio et al. (2004) and Iida et al. (2005) claim that the latter subtask should be done before the former. For example, given a target anaphor (TA), Iida et al.’s selection-then-classification model: 1. selects the most likely candidate antecedent (CA) of TA using the tournament model, 2. classifies TA paired with CA as either anaphoric or non-anaphoric using an anaphoricity determination model. If the CA-TA pair is classified as anaphoric, CA"
P06-1079,P02-1014,0,0.493031,"nly by shallow syntactic parsing, a model specialized for zero-anaphora resolution needs to be devised on the top of shallow syntactic and semantic processing. Recent work on zero-anaphora resolution can be located in two different research contexts. First, zero-anaphora resolution is studied in the context of anaphora resolution (AR), in which zeroanaphora is regarded as a subclass of anaphora. In AR, the research trend has been shifting from rulebased approaches (Baldwin, 1995; Lappin and Leass, 1994; Mitkov, 1997, etc.) to empirical, or corpus-based, approaches (McCarthy and Lehnert, 1995; Ng and Cardie, 2002a; Soon et al., 2001; Strube and M¨uller, 2003; Yang et al., 2003) because the latter are shown to be a cost-efficient solution achieving a performance that is comparable to best performing rule-based systems (see the Coreference task in MUC1 and the Entity Detection and Tracking task in the ACE program2 ). The same trend is observed also in Japanese zeroanaphora resolution, where the findings made in rule-based or theory-oriented work (Kameyama, 1986; Nakaiwa and Shirai, 1996; Okumura and Tamura, 1996, etc.) have been successfully incorporated in machine learning-based frameworks (Seki et al."
P06-1079,C96-2147,0,0.0563355,"1994; Mitkov, 1997, etc.) to empirical, or corpus-based, approaches (McCarthy and Lehnert, 1995; Ng and Cardie, 2002a; Soon et al., 2001; Strube and M¨uller, 2003; Yang et al., 2003) because the latter are shown to be a cost-efficient solution achieving a performance that is comparable to best performing rule-based systems (see the Coreference task in MUC1 and the Entity Detection and Tracking task in the ACE program2 ). The same trend is observed also in Japanese zeroanaphora resolution, where the findings made in rule-based or theory-oriented work (Kameyama, 1986; Nakaiwa and Shirai, 1996; Okumura and Tamura, 1996, etc.) have been successfully incorporated in machine learning-based frameworks (Seki et al., 2002; Iida et al., 2003). Second, the task of zero-anaphora resolution has some overlap with Propbank3 -style semantic role labeling (SRL), which has been intensively studied, for example, in the context of the CoNLL SRL task4 . In this task, given a sentence “To attract younger listeners, Radio Free Europe intersperses the latest in Western rock groups”, an SRL We approach the zero-anaphora resolution problem by decomposing it into intra-sentential and inter-sentential zeroanaphora resolution. For t"
P06-1079,W04-0707,0,0.0690539,"unity has recently made two important findings: • A model that identifies the antecedent of an anaphor by a series of comparisons between candidate antecedents has a remarkable advantage over a model that estimates the absolute likelihood of each candidate independently of other candidates (Iida et al., 2003; Yang et al., 2003). • An AR model that carries out antecedent identification before anaphoricity determination, the decision whether a given NP is anaphoric or not (i.e. discourse-new), significantly outperforms a model that executes those subtasks in the reverse order or simultaneously (Poesio et al., 2004; Iida et al., 2005). 2 Zero-anaphora resolution In this paper, we consider only zero-pronouns that function as an obligatory argument of a predicate for two reasons: • Providing a clear definition of zero-pronouns appearing in adjunctive argument positions involves awkward problems, which we believe should be postponed until obligatory zero-anaphora is well studied. • Resolving obligatory zero-anaphora tends to be more important than adjunctive zeropronouns in actual applications. A zero-pronoun may have its antecedent in the discourse; in this case, we say the zero-pronoun is anaphoric. On t"
P06-1079,C02-1078,0,0.701082,"ardie, 2002a; Soon et al., 2001; Strube and M¨uller, 2003; Yang et al., 2003) because the latter are shown to be a cost-efficient solution achieving a performance that is comparable to best performing rule-based systems (see the Coreference task in MUC1 and the Entity Detection and Tracking task in the ACE program2 ). The same trend is observed also in Japanese zeroanaphora resolution, where the findings made in rule-based or theory-oriented work (Kameyama, 1986; Nakaiwa and Shirai, 1996; Okumura and Tamura, 1996, etc.) have been successfully incorporated in machine learning-based frameworks (Seki et al., 2002; Iida et al., 2003). Second, the task of zero-anaphora resolution has some overlap with Propbank3 -style semantic role labeling (SRL), which has been intensively studied, for example, in the context of the CoNLL SRL task4 . In this task, given a sentence “To attract younger listeners, Radio Free Europe intersperses the latest in Western rock groups”, an SRL We approach the zero-anaphora resolution problem by decomposing it into intra-sentential and inter-sentential zeroanaphora resolution. For the former problem, syntactic patterns of the appearance of zero-pronouns and their antecedents are"
P06-1079,J01-4004,0,0.746746,"ic parsing, a model specialized for zero-anaphora resolution needs to be devised on the top of shallow syntactic and semantic processing. Recent work on zero-anaphora resolution can be located in two different research contexts. First, zero-anaphora resolution is studied in the context of anaphora resolution (AR), in which zeroanaphora is regarded as a subclass of anaphora. In AR, the research trend has been shifting from rulebased approaches (Baldwin, 1995; Lappin and Leass, 1994; Mitkov, 1997, etc.) to empirical, or corpus-based, approaches (McCarthy and Lehnert, 1995; Ng and Cardie, 2002a; Soon et al., 2001; Strube and M¨uller, 2003; Yang et al., 2003) because the latter are shown to be a cost-efficient solution achieving a performance that is comparable to best performing rule-based systems (see the Coreference task in MUC1 and the Entity Detection and Tracking task in the ACE program2 ). The same trend is observed also in Japanese zeroanaphora resolution, where the findings made in rule-based or theory-oriented work (Kameyama, 1986; Nakaiwa and Shirai, 1996; Okumura and Tamura, 1996, etc.) have been successfully incorporated in machine learning-based frameworks (Seki et al., 2002; Iida et al.,"
P06-1079,P03-1022,0,0.0257275,"Missing"
P06-1079,P03-1005,0,0.0206942,"earning algorithm As noted in Section 1, the use of zero-pronouns in Japanese is relatively less constrained by syntax compared, for example, with English. This forces the above way of encoding path information to produce an explosive number of different paths, which inevitably leads to serious data sparseness. This issue can be addressed in several ways. The SRL community has devised a range of variants of the standard path representation to reduce the complexity (Carreras and Marquez, 2005). Applying Kernel methods such as Tree kernels (Collins and Duffy, 2001) and Hierarchical DAG kernels (Suzuki et al., 2003) is another strong option. The Boosting-based algorithm pro• A path is represented by a subtree consisting of backbone nodes: φ (zero-pronoun), Ant (antecedent), Node (the lowest common ancestor), LeftNode (left-branch node) and RightNode. • Each backbone node has daughter nodes, each corresponding to a function word associated with it. • Content words are deleted. This way of encoding syntactic patterns is used in intra-sentential anaphoricity determination. In antecedent identification, on the other hand, the tournament model allows us to incorporate three paths, a path for each pair of a ze"
P06-1079,W05-0620,0,0.0407031,"s anaphoric as its antecedent, ‘shusho (prime minister)’, appears in the same sentence. In sentence (2), on the other hand, φj is considered non-anaphoric if its referent (i.e. the first person) does not appear in the discourse. To our best knowledge, however, existing SRL models do not exploit these advantages. In SRL, on the other hand, it is common to use syntactic features derived from the parse tree of a given input sentence for argument identification. A typical syntactic feature is the path on a parse tree from a target predicate to a noun phrase in question (Gildea and Jurafsky, 2002; Carreras and Marquez, 2005). However, existing AR models deal with intra- and inter-sentential anaphoric relations in a uniform manner; that is, they do not use as rich syntactic features as state-of-the-art SRL models do, even in finding intra-sentential anaphoric relations. We believe that the AR and SRL communities can learn more from each other. Given this background, in this paper, we show that combining the aforementioned techniques derived from each research trend makes significant impact on zero-anaphora resolution, taking Japanese as a target language. More specifically, we demonstrate the following: • Incorpor"
P06-1079,J02-3001,0,0.0236012,"ence (1), zero-pronoun φi is anaphoric as its antecedent, ‘shusho (prime minister)’, appears in the same sentence. In sentence (2), on the other hand, φj is considered non-anaphoric if its referent (i.e. the first person) does not appear in the discourse. To our best knowledge, however, existing SRL models do not exploit these advantages. In SRL, on the other hand, it is common to use syntactic features derived from the parse tree of a given input sentence for argument identification. A typical syntactic feature is the path on a parse tree from a target predicate to a noun phrase in question (Gildea and Jurafsky, 2002; Carreras and Marquez, 2005). However, existing AR models deal with intra- and inter-sentential anaphoric relations in a uniform manner; that is, they do not use as rich syntactic features as state-of-the-art SRL models do, even in finding intra-sentential anaphoric relations. We believe that the AR and SRL communities can learn more from each other. Given this background, in this paper, we show that combining the aforementioned techniques derived from each research trend makes significant impact on zero-anaphora resolution, taking Japanese as a target language. More specifically, we demonstr"
P06-1079,J95-2003,0,0.0943109,"Missing"
P06-1079,W03-2604,1,0.954084,"et al., 2001; Strube and M¨uller, 2003; Yang et al., 2003) because the latter are shown to be a cost-efficient solution achieving a performance that is comparable to best performing rule-based systems (see the Coreference task in MUC1 and the Entity Detection and Tracking task in the ACE program2 ). The same trend is observed also in Japanese zeroanaphora resolution, where the findings made in rule-based or theory-oriented work (Kameyama, 1986; Nakaiwa and Shirai, 1996; Okumura and Tamura, 1996, etc.) have been successfully incorporated in machine learning-based frameworks (Seki et al., 2002; Iida et al., 2003). Second, the task of zero-anaphora resolution has some overlap with Propbank3 -style semantic role labeling (SRL), which has been intensively studied, for example, in the context of the CoNLL SRL task4 . In this task, given a sentence “To attract younger listeners, Radio Free Europe intersperses the latest in Western rock groups”, an SRL We approach the zero-anaphora resolution problem by decomposing it into intra-sentential and inter-sentential zeroanaphora resolution. For the former problem, syntactic patterns of the appearance of zero-pronouns and their antecedents are useful clues. Taking"
P06-1079,P86-1031,0,0.0835096,"proaches (Baldwin, 1995; Lappin and Leass, 1994; Mitkov, 1997, etc.) to empirical, or corpus-based, approaches (McCarthy and Lehnert, 1995; Ng and Cardie, 2002a; Soon et al., 2001; Strube and M¨uller, 2003; Yang et al., 2003) because the latter are shown to be a cost-efficient solution achieving a performance that is comparable to best performing rule-based systems (see the Coreference task in MUC1 and the Entity Detection and Tracking task in the ACE program2 ). The same trend is observed also in Japanese zeroanaphora resolution, where the findings made in rule-based or theory-oriented work (Kameyama, 1986; Nakaiwa and Shirai, 1996; Okumura and Tamura, 1996, etc.) have been successfully incorporated in machine learning-based frameworks (Seki et al., 2002; Iida et al., 2003). Second, the task of zero-anaphora resolution has some overlap with Propbank3 -style semantic role labeling (SRL), which has been intensively studied, for example, in the context of the CoNLL SRL task4 . In this task, given a sentence “To attract younger listeners, Radio Free Europe intersperses the latest in Western rock groups”, an SRL We approach the zero-anaphora resolution problem by decomposing it into intra-sentential"
P06-1079,P03-1023,0,\N,Missing
P06-1089,J96-2001,0,0.04224,"s for unknown words in the test data (i.e., N is equal to the number of the open class tags). In the training phase, we need to estimate two types of parameters; local model (parameters), which is necessary to calculate p0 (t|w), and global model (parameters), i.e., λi,j . The local model parameters are estimated using all the training data (Figure 2, *2). Local 3 A major method for generating such pseudo unknown words is to collect the words that appear only once in a corpus (Nagata, 1999). These words are called hapax legomena and known to have similar characteristics to real unknown words (Baayen and Sproat, 1996). These words are interpreted as being collected by the leave-one-out technique (which is a special case of cross-validation) as follows: One word is picked from the corpus and the rest of the corpus is considered as training data. The picked word is regarded as an unknown word if it does not exist in the training data. This procedure is iterated for all the words in the corpus. However, this approach is not applicable to our experiments because those words that appear only once in the corpus do not have global information and are useless for learning the global model, so we use the two-fold c"
P06-1089,J96-1002,0,0.0363666,"Missing"
P06-1089,C02-1025,0,0.0501505,"n words. With this method, all the occurrences of the unknown words in a document1 are taken into consideration at once, rather than that each occurrence of the words is processed separately. Thus, the method models the whole document and finds a set of parts-of-speech by maximizing its conditional joint probability given the document, rather than independently maximizing the probability of each part-of-speech given each sentence. Global information is known to be useful in other NLP tasks, especially in the named entity recognition task, and several studies successfully used global features (Chieu and Ng, 2002; Finkel et al., 2005). One potential advantage of our method is its In this paper, we present a method for guessing POS tags of unknown words using local and global information. Although many existing methods use only local information (i.e. limited window size or intra-sentential features), global information (extra-sentential features) provides valuable clues for predicting POS tags of unknown words. We propose a probabilistic model for POS guessing of unknown words using global information as well as local information, and estimate its parameters using Gibbs sampling. We also attempt to ap"
P06-1089,P05-1045,0,0.591283,"ethod, all the occurrences of the unknown words in a document1 are taken into consideration at once, rather than that each occurrence of the words is processed separately. Thus, the method models the whole document and finds a set of parts-of-speech by maximizing its conditional joint probability given the document, rather than independently maximizing the probability of each part-of-speech given each sentence. Global information is known to be useful in other NLP tasks, especially in the named entity recognition task, and several studies successfully used global features (Chieu and Ng, 2002; Finkel et al., 2005). One potential advantage of our method is its In this paper, we present a method for guessing POS tags of unknown words using local and global information. Although many existing methods use only local information (i.e. limited window size or intra-sentential features), global information (extra-sentential features) provides valuable clues for predicting POS tags of unknown words. We propose a probabilistic model for POS guessing of unknown words using global information as well as local information, and estimate its parameters using Gibbs sampling. We also attempt to apply the model to semis"
P06-1089,J97-3003,0,0.0664668,") tagging is a fundamental language analysis task. In POS tagging, we frequently encounter words that do not exist in training data. Such words are called unknown words. They are usually handled by an exceptional process in POS tagging, because the tagging system does not have information about the words. Guessing the POS tags of such unknown words is a difficult task. But it is an important issue both for conducting POS tagging accurately and for creating word dictionaries automatically or semiautomatically. There have been many studies on POS guessing of unknown words (Mori and Nagao, 1996; Mikheev, 1997; Chen et al., 1997; Nagata, 1999; Orphanos and Christodoulakis, 1999). In most of these previous works, POS tags of unknown words were predicted using only local information, such as lexical forms and POS tags of surrounding words or word-internal features (e.g. suffixes and character types) of the unknown words. However, this approach has limitations in available information. For example, common nouns and proper nouns are sometimes difficult to distinguish with only the information of a single occurrence because their syntactic functions are almost identical. In English, proper nouns are cap"
P06-1089,C96-2202,0,0.123427,"on Part-of-speech (POS) tagging is a fundamental language analysis task. In POS tagging, we frequently encounter words that do not exist in training data. Such words are called unknown words. They are usually handled by an exceptional process in POS tagging, because the tagging system does not have information about the words. Guessing the POS tags of such unknown words is a difficult task. But it is an important issue both for conducting POS tagging accurately and for creating word dictionaries automatically or semiautomatically. There have been many studies on POS guessing of unknown words (Mori and Nagao, 1996; Mikheev, 1997; Chen et al., 1997; Nagata, 1999; Orphanos and Christodoulakis, 1999). In most of these previous works, POS tags of unknown words were predicted using only local information, such as lexical forms and POS tags of surrounding words or word-internal features (e.g. suffixes and character types) of the unknown words. However, this approach has limitations in available information. For example, common nouns and proper nouns are sometimes difficult to distinguish with only the information of a single occurrence because their syntactic functions are almost identical. In English, prope"
P06-1089,P99-1036,0,0.812353,"e analysis task. In POS tagging, we frequently encounter words that do not exist in training data. Such words are called unknown words. They are usually handled by an exceptional process in POS tagging, because the tagging system does not have information about the words. Guessing the POS tags of such unknown words is a difficult task. But it is an important issue both for conducting POS tagging accurately and for creating word dictionaries automatically or semiautomatically. There have been many studies on POS guessing of unknown words (Mori and Nagao, 1996; Mikheev, 1997; Chen et al., 1997; Nagata, 1999; Orphanos and Christodoulakis, 1999). In most of these previous works, POS tags of unknown words were predicted using only local information, such as lexical forms and POS tags of surrounding words or word-internal features (e.g. suffixes and character types) of the unknown words. However, this approach has limitations in available information. For example, common nouns and proper nouns are sometimes difficult to distinguish with only the information of a single occurrence because their syntactic functions are almost identical. In English, proper nouns are capitalized and there is generally l"
P06-1089,E99-1018,0,0.0306085,"k. In POS tagging, we frequently encounter words that do not exist in training data. Such words are called unknown words. They are usually handled by an exceptional process in POS tagging, because the tagging system does not have information about the words. Guessing the POS tags of such unknown words is a difficult task. But it is an important issue both for conducting POS tagging accurately and for creating word dictionaries automatically or semiautomatically. There have been many studies on POS guessing of unknown words (Mori and Nagao, 1996; Mikheev, 1997; Chen et al., 1997; Nagata, 1999; Orphanos and Christodoulakis, 1999). In most of these previous works, POS tags of unknown words were predicted using only local information, such as lexical forms and POS tags of surrounding words or word-internal features (e.g. suffixes and character types) of the unknown words. However, this approach has limitations in available information. For example, common nouns and proper nouns are sometimes difficult to distinguish with only the information of a single occurrence because their syntactic functions are almost identical. In English, proper nouns are capitalized and there is generally little ambiguity between common nouns"
P06-1089,W96-0213,0,0.584546,"Missing"
P06-1089,P05-1017,0,0.162443,"al. In English, proper nouns are capitalized and there is generally little ambiguity between common nouns and proper nouns. In Chinese and Japanese, no such convention exists and the problem of the ambiguity is serious. However, if an unknown word with the same lex1 In this paper, we use the word document to denote the whole data consisting of multiple sentences (training corpus or test corpus). 705 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 705–712, c Sydney, July 2006. 2006 Association for Computational Linguistics Takamura et al. (2005) applied this model to an NLP task, semantic orientation extraction, and we apply it to POS guessing of unknown words here. ability to incorporate unlabeled data. Global features can be increased by simply adding unlabeled data into the test data. Models in which the whole document is taken into consideration need a lot of computation compared to models with only local features. They also cannot process input data one-by-one. Instead, the entire document has to be read before processing. We adopt Gibbs sampling in order to compute the models efficiently, and these models are suitable for offli"
P06-1089,W01-0512,0,0.549224,"1 if w and t pora, the accuracies were improved using global satisfy certain conditions, and otherwise 0; for exinformation (statistically significant at p < 0.05), ample: n compared to the accuracies obtained using only lo1 (ω−1 =“President” and τ−1 =“NNP” and t = 5), g123 (w, t)= cal information. The increases of the accuracies on 0 (otherwise). the English corpora (the GEN and SUS corpora) The features we use are shown in Table 2, which were small. Table 4 shows the increased/decreased are based on the features used by Ratnaparkhi number of correctly tagged words using global in(1996) and Uchimoto et al. (2001). formation in the PFR, RWC and SUS corpora. The parameters αh in Equation (20) are estiIn the PFR (Chinese) and RWC (Japanese) cormated using all the words in the training data pora, many proper nouns were correctly tagged uswhose POS tags are the open class tags. ing global information. In Chinese and Japanese, proper nouns are not capitalized, therefore proper 3.3 Experimental Results nouns are difficult to distinguish from common nouns with only local information. One reason The results are shown in Table 3. In the table, lothat only the small increases were obtained with cal, local+global"
P06-1089,P95-1026,0,0.0405958,"l studies concerning the use of global information have been conducted, especially in named entity recognition, which is a similar task to POS guessing of unknown words. Chieu and Ng (2002) conducted named entity recognition using global features as well as local features. In their ME 711 Acknowledgements method, semantic orientations (positive or negative) of words are regarded as states of spins, in order to model the property that the semantic orientation of a word tends to have the same orientation as words in its gloss. The mean field approximation was used for inference in their method. Yarowsky (1995) studied a method for word sense disambiguation using unlabeled data. Although no probabilistic models were considered explicitly in the method, they used the property of label consistency named “one sense per discourse” for unsupervised learning together with local information named “one sense per collocation”. There exist other approaches using global information which do not necessarily aim to use label consistency. Rosenfeld et al. (2001) proposed whole-sentence exponential language models. The method calculates the probability of a sentence s as follows: ( ) P (s)= This work was supported"
P09-1046,D08-1073,0,0.602023,"Missing"
P09-1046,S07-1052,1,0.565953,"cal formulae for the Tasks A, B and C. We say that a formula is local if it only considers the hidden temporal relation of a single event-event, event-time or event-DCT pair. The formulae in the second class are global: they in4 Proposed Markov Logic Network volve two or more temporal relations at the same As stated before, our aim is to jointly tackle time, and consider Tasks A, B and C simultaneTasks A, B and C of the TempEval challenge. In ously. this section we introduce the Markov Logic NetThe local formulae are based on features emwork we designed for this goal. ployed in previous work (Cheng et al., 2007; We have three hidden predicates, corresponding Bethard and Martin, 2007) and are listed in Table 1. to Tasks A, B, and C: relE2T(e, t, r) represents the What follows is a simple example in order to illustemporal relation of class r between an event e trate how we implement each feature as a formula feature: here for every event e the decision “e happens be- (or set of formulae). fore DCT” becomes more likely with a higher weight for this feature. 7 http://alchemy.cs.washington.edu/ 8 http://code.google.com/p/thebeast/ Consider the tense-feature for Task C. For this feature we first introduce"
P09-1046,P06-1095,0,0.721207,"ions —and to the best results for the task when compared to those of other machine learning based systems. 1 Introduction Temporal relation identification (or temporal ordering) involves the prediction of temporal order between events and/or time expressions mentioned in text, as well as the relation between events in a document and the time at which the document was created. With the introduction of the TimeBank corpus (Pustejovsky et al., 2003), a set of documents annotated with temporal information, it became possible to apply machine learning to temporal ordering (Boguraev and Ando, 2005; Mani et al., 2006). These tasks have been regarded as essential for complete document understanding and are useful for a wide range of NLP applications such as question answering and machine translation. Most of these approaches follow a simple schema: they learn classifiers that predict the temporal order of a given event pair based on a set of the pair’s of features. This approach is local in the sense that only a single temporal relation is considered at a time. Learning to predict temporal relations in this isolated manner has at least two advantages over any approach that considers several temporal relatio"
P09-1046,D08-1068,0,0.140991,"Missing"
P09-1046,S07-1108,0,0.198726,"Missing"
P09-1046,S07-1014,0,0.757283,": we only need to define features (in terms of formulae) and provide input data in the correct format. 2 In particular, we do not need to manually construct ILPs for each document we encounter. Moreover, we can exploit and compare advanced methods of global inference and learning, as long as they are implemented in our Markov Logic interpreter of choice. Hence, in our future work we can focus entirely on temporal relations, as opposed to inference or learning techniques for machine learning. We evaluate our approach using the data of the “TempEval” challenge held at the SemEval 2007 Workshop (Verhagen et al., 2007). This challenge involved three tasks corresponding to three types of temporal relations: between events and time expressions in a sentence (Task A), between events of a document and the document creation time (Task B), and between events in two consecutive sentences (Task C). Our findings show that by incorporating global constraints that hold between temporal relations predicted in Tasks A, B and C, the accuracy for all three tasks can be improved significantly. In comparison to other participants of the “TempEval” challenge our approach is very competitive: for two out of the three tasks we"
P09-1046,S07-1025,0,\N,Missing
P09-1073,D08-1069,0,0.336351,"didate in sentence Si . In this situation, for example, candidate c12 is labeled as retained when creating training instances for sentence S1 , but labeled as discarded from S2 onwards, because of the appearance of its zeropronoun. Another candidate c13 which is never referred to in the text is labeled as discarded for all training instances. Second, we need to capture the ‘relative’ salience of candidates appearing in the current discourse for each cache update, as also exploited in the tournament-based or ranking-based approaches to anaphora resolution (Iida et al., 2003; Yang et al., 2003; Denis and Baldridge, 2008). To solve it, we use a ranker trained on the instances created as described above. In order to train the ranker, we adopt the Ranking SVM algorithm (Joachims, 2002), which learns a weight vector to rank candidates for a given partial ranking of each discourse entity. Each training instance is created from the set of retained candidates, Ri , paired with the set of discarded candidates, Di , in each sentence. To Figure 2: Pseudo-code for creating training instances training instances S1 c11 c12 c13 c14 S 2 c21 c22 c23 φ φ i j S3 c31 c32 c33 φk φ retained c11 c12 discarded c13 c14 retained c11"
P09-1073,J86-3001,0,0.408154,"entity. Each training instance is created from the set of retained candidates, Ri , paired with the set of discarded candidates, Di , in each sentence. To Figure 2: Pseudo-code for creating training instances training instances S1 c11 c12 c13 c14 S 2 c21 c22 c23 φ φ i j S3 c31 c32 c33 φk φ retained c11 c12 discarded c13 c14 retained c11 c22 discarded c12 c13 c14 c21 c23 l Figure 3: Creating training instnaces define the partial ranking of candidates, we simply rank candidates in Ri as first place and candidates in Di as second place. 4.2 Static cache model Other research on discourse such as Grosz and Sidner (1986) has studied global focus, which generally refers to the entity or set of entities that are salient throughout the entire discourse. Since global focus may not be captured by Centeringbased models, we also propose another cache model which directly captures the global salience of a text. To train the model, all the candidates in a text which have an inter-sentential anaphoric relation with zero-pronouns are used as positive instances and the others used as negative ones. Unlike the 650 Table 1: Feature set used in the cache models Feature Description POS Part-of-speech of C followed by IPADIC4"
P09-1073,J95-2003,0,0.832228,"machine learning-based methods using considerable amounts of annotated data provided by, for example, the Message Understanding Conference and Automatic Context Extraction programs (Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; McCallum and Wellner, 2003, etc.). These methods reach a level comparable to or better than the state-of-the-art rule-based systems (e.g. Baldwin (1995)) by recasting the task of anaphora resolution into classification or clustering problems. However, such approaches tend to disregard theoretical findings from discourse theories, such as Centering Theory (Grosz et al., 1995). Therefore, one of the challenging issues in this area is to incorporate such findings from linguistic theories into machine learning-based approaches. A typical machine learning-based approach to zero-anaphora resolution searches for an antecedent in the set of candidates appearing in all the preceding contexts. However, computational time makes this approach largely infeasible for long texts. An alternative approach is to heuristically limit the search space (e.g. the system deals with candidates only occurring in the N previous sentences). Various research such as Yang et al. (2008) has ad"
P09-1073,P97-1014,0,0.20415,"to be excluded from target candidate antecedents. On the other hand, rule-based methods derived from theoretical background such as Centering Theory (Grosz et al., 1995) only deal with the salient discourse entities at each point of the discourse status. By incrementally updating the discourse status, the set of candidates in question is automatically limited. Although these methods have a theoretical advantage, they have a serious drawback in that Centering Theory only retains information about the previous sentence. A few methods have attempted to overcome this fault (Suri and McCoy, 1994; Hahn and Strube, 1997), but they are overly dependent upon the restrictions fundamental to the notion of centering. We hope that by relaxing such restrictions it will be possible for an anaphora resolution system to achieve a good balance between accuracy and computational cost. From this background, we focus on the issue of reducing candidate antecedents (discourse entities) for a given anaphor. Inspired by Walker’s argument (Walker, 1996), we propose a machine learning-based caching mechanism that captures the most salient candidates at each point of the discourse for efficient anaphora resolution. More specifica"
P09-1073,W03-2604,1,0.850397,"in Figure 3, where cij is the j-th candidate in sentence Si . In this situation, for example, candidate c12 is labeled as retained when creating training instances for sentence S1 , but labeled as discarded from S2 onwards, because of the appearance of its zeropronoun. Another candidate c13 which is never referred to in the text is labeled as discarded for all training instances. Second, we need to capture the ‘relative’ salience of candidates appearing in the current discourse for each cache update, as also exploited in the tournament-based or ranking-based approaches to anaphora resolution (Iida et al., 2003; Yang et al., 2003; Denis and Baldridge, 2008). To solve it, we use a ranker trained on the instances created as described above. In order to train the ranker, we adopt the Ranking SVM algorithm (Joachims, 2002), which learns a weight vector to rank candidates for a given partial ranking of each discourse entity. Each training instance is created from the set of retained candidates, Ri , paired with the set of discarded candidates, Di , in each sentence. To Figure 2: Pseudo-code for creating training instances training instances S1 c11 c12 c13 c14 S 2 c21 c22 c23 φ φ i j S3 c31 c32 c33 φk φ r"
P09-1073,W07-1522,1,0.905738,"irical evaluation with Japanese newspaper articles shows that the number of candidate antecedents for each zero-pronoun can be dramatically reduced while preserving the accuracy of resolving it. 1 Introduction There have been recently increasing concerns with the need for anaphora resolution to make NLP applications such as IE and MT more reliable. In particular, for languages such as Japanese, anaphora resolution is crucial for resolving a phrase in a text to its referent since phrases, especially nominative arguments of predicates, are frequently omitted by anaphoric functions in discourse (Iida et al., 2007b). Many researchers have recently explored machine learning-based methods using considerable amounts of annotated data provided by, for example, the Message Understanding Conference and Automatic Context Extraction programs (Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; McCallum and Wellner, 2003, etc.). These methods reach a level comparable to or better than the state-of-the-art rule-based systems (e.g. Baldwin (1995)) by recasting the task of anaphora resolution into classification or clustering problems. However, such approaches tend to disregard theoretical findings from dis"
P09-1073,W03-1024,0,0.0170854,"nd to the most salient discourse entities within the local transition and within the global focus of a text. Hahn and Strube (1997) estimate hierarchical discourse segments of a text by taking into account a series of Cp and then the resolution model searches for an antecedent in the estimated segment. Although these methods remedy the drawback of Centering, they still overly depend on the notion of Centering such as Cp. On the other hand, the existing machine learning-based methods (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2005; Iida et al., 2007a, etc.) have been developed with less attention given to such a problem. These methods exhaustively search for an antecedent within the list of all candidate antecedents until the beginning of the text. Otherwise, the process to search for antecedents is heuristically carried out in a limited search space (e.g. the previous N sentences of an anaphor) (Yang et al., 2008). In this paper, we consider only zero-pronouns that function as an obligatory argument of a predicate. A zero-pronoun may or may not have its antecedent in the discourse; in the case it do"
P09-1073,P86-1031,0,0.568605,"ion and then Section 3 gives an overview of previous work. Next, in Section 4 we propose a machine learning-based cache model. Section 5 presents the antecedent identification and anaphoricity determination models used in the experiments. To evaluate the model, we conduct several empirical evaluations and report their results in Section 6. Finally, we conclude and discuss the future direction of this research in Section 7. 2 Zero-anaphora resolution 3 Previous work Early methods for zero-anaphora resolution were developed with rule-based approaches in mind. Theory-oriented rule-based methods (Kameyama, 1986; Walker et al., 1994), for example, focus on the Centering Theory (Grosz et al., 1995) and are designed to collect the salient candidate antecedents in the forward-looking center (Cf ) list, and then choose the most salient candidate, Cp, as an antecedent of a zero-pronoun according to heuristic rules (e.g. topic > subject > indirect object > direct object > others1 ). Although these methods have a theoretical advantage, they have a serious drawback in that the original Centering Theory is restricted to keeping information about the previous sentence only. In order to loosen this restriction,"
P09-1073,W04-3239,1,0.836643,"r or not the model using a small number of discourse entities in the cache achieves performance comparable to the original one in a practical setting. For intra-sentential zero-anaphora resolution, we adopt the model proposed by Iida et al. (2007a), which exploits syntactic patterns as features that appear in the dependency path of a zero-pronoun and its candidate antecedent. Note that for simplicity we use bag-of-functional words and their part-of-speech intervening between a zero-pronoun and its candidate antecedent as features instead of learning syntactic patterns with the Bact algorithm (Kudo and Matsumoto, 2004). We illustrated the recall-precision curve of each model by altering the threshold parameter of intrasentential anaphoricity determination, which is shown in Figure 5. The results show that all models achieved almost the same performance when decreasing the cache size. It indicates that it is enough to cache a small number of the most salient References candidates in the current zero-anaphora resolution C. Aone and S. W. Bennett. 1995. Evaluating automated and manual acquisition of anaphora resolution strategies. model, while coverage decreases when the cache In Proceedings of 33th Annual Mee"
P09-1073,2002.tmi-papers.15,0,0.0271594,"antecedents when appearing zero-pronouns refer to an antecedent in a preceding sentence, i.e. we evaluate the cases of inter-sentential anaphora resolution. 6.2 Evaluation of the caching mechanism In this experiment, we directly compare the proAs a baseline, we adopt the following two cache posed static and dynamic cache models with the models. One is the Centering-derived model which heuristic methods presented in Section 2. Note that only stores the preceding ‘wa’ (topic)-marked or 652 ‘ga’ (subject)-marked candidate antecedents in the cache. It is an approximation of the model proposed by Nariyama (2002) for extending the local focus transition defined by Centering Theory. We henceforth call this model the centering-based cache model. The other baseline model stores candidates appearing in the N previous sentences of a zero-pronoun to simulate a heuristic approach used in works like Soon et al. (2001). We call this model the sentence-based cache model. By comparing these baselines with our cache models, we can see whether our models contribute to more efficiently storing salient candidates or not. The above dynamic cache model retains the salient candidates independently of the results of ant"
P09-1073,P02-1014,0,0.667329,"e need for anaphora resolution to make NLP applications such as IE and MT more reliable. In particular, for languages such as Japanese, anaphora resolution is crucial for resolving a phrase in a text to its referent since phrases, especially nominative arguments of predicates, are frequently omitted by anaphoric functions in discourse (Iida et al., 2007b). Many researchers have recently explored machine learning-based methods using considerable amounts of annotated data provided by, for example, the Message Understanding Conference and Automatic Context Extraction programs (Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; McCallum and Wellner, 2003, etc.). These methods reach a level comparable to or better than the state-of-the-art rule-based systems (e.g. Baldwin (1995)) by recasting the task of anaphora resolution into classification or clustering problems. However, such approaches tend to disregard theoretical findings from discourse theories, such as Centering Theory (Grosz et al., 1995). Therefore, one of the challenging issues in this area is to incorporate such findings from linguistic theories into machine learning-based approaches. A typical machine learning-based approach to zero"
P09-1073,D08-1068,0,0.0470475,"nt candidates. Our empirical evaluation on Japanese zero-anaphora resolution shows that our learningbased cache model drastically reduces the search space while preserving accuracy. The procedure for zero-anaphora resolution adopted in our model assumes that resolution is carried out linearly, i.e. an antecedent is independently selected without taking into account any other zero-pronouns. However, trends in anaphora resolution have shifted from such linear approaches to more sophisticated ones which globally optimize the interpretation of all the referring expressions in a text. For example, Poon and Domingos (2008) has empirically reported that such global approaches achieve performance better than the ones based on incrementally processing a text. Because their work basically builds on inductive logic programing, we can naturally extend this to incorporate our caching mechanism into the global optimization by expressing cache constraints as predicate logic, which is one of our next challenges in this research area. 6.4 Overall zero-anaphora resolution We finally investigate the effects of introducing the proposed model on overall zero-anaphora resolution including intra-sentential cases. The resolution"
P09-1073,C02-1078,0,0.0197281,"f Cp, that correspond to the most salient discourse entities within the local transition and within the global focus of a text. Hahn and Strube (1997) estimate hierarchical discourse segments of a text by taking into account a series of Cp and then the resolution model searches for an antecedent in the estimated segment. Although these methods remedy the drawback of Centering, they still overly depend on the notion of Centering such as Cp. On the other hand, the existing machine learning-based methods (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001; Ng and Cardie, 2002; Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2005; Iida et al., 2007a, etc.) have been developed with less attention given to such a problem. These methods exhaustively search for an antecedent within the list of all candidate antecedents until the beginning of the text. Otherwise, the process to search for antecedents is heuristically carried out in a limited search space (e.g. the previous N sentences of an anaphor) (Yang et al., 2008). In this paper, we consider only zero-pronouns that function as an obligatory argument of a predicate. A zero-pronoun may or may not have its antecedent in the dis"
P09-1073,J01-4004,0,0.924987,"ng concerns with the need for anaphora resolution to make NLP applications such as IE and MT more reliable. In particular, for languages such as Japanese, anaphora resolution is crucial for resolving a phrase in a text to its referent since phrases, especially nominative arguments of predicates, are frequently omitted by anaphoric functions in discourse (Iida et al., 2007b). Many researchers have recently explored machine learning-based methods using considerable amounts of annotated data provided by, for example, the Message Understanding Conference and Automatic Context Extraction programs (Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; McCallum and Wellner, 2003, etc.). These methods reach a level comparable to or better than the state-of-the-art rule-based systems (e.g. Baldwin (1995)) by recasting the task of anaphora resolution into classification or clustering problems. However, such approaches tend to disregard theoretical findings from discourse theories, such as Centering Theory (Grosz et al., 1995). Therefore, one of the challenging issues in this area is to incorporate such findings from linguistic theories into machine learning-based approaches. A typical machine learning-b"
P09-1073,J94-2006,0,0.330217,"ts anaphor, causing it to be excluded from target candidate antecedents. On the other hand, rule-based methods derived from theoretical background such as Centering Theory (Grosz et al., 1995) only deal with the salient discourse entities at each point of the discourse status. By incrementally updating the discourse status, the set of candidates in question is automatically limited. Although these methods have a theoretical advantage, they have a serious drawback in that Centering Theory only retains information about the previous sentence. A few methods have attempted to overcome this fault (Suri and McCoy, 1994; Hahn and Strube, 1997), but they are overly dependent upon the restrictions fundamental to the notion of centering. We hope that by relaxing such restrictions it will be possible for an anaphora resolution system to achieve a good balance between accuracy and computational cost. From this background, we focus on the issue of reducing candidate antecedents (discourse entities) for a given anaphor. Inspired by Walker’s argument (Walker, 1996), we propose a machine learning-based caching mechanism that captures the most salient candidates at each point of the discourse for efficient anaphora re"
P09-1073,J94-2003,0,0.121523,"tion 3 gives an overview of previous work. Next, in Section 4 we propose a machine learning-based cache model. Section 5 presents the antecedent identification and anaphoricity determination models used in the experiments. To evaluate the model, we conduct several empirical evaluations and report their results in Section 6. Finally, we conclude and discuss the future direction of this research in Section 7. 2 Zero-anaphora resolution 3 Previous work Early methods for zero-anaphora resolution were developed with rule-based approaches in mind. Theory-oriented rule-based methods (Kameyama, 1986; Walker et al., 1994), for example, focus on the Centering Theory (Grosz et al., 1995) and are designed to collect the salient candidate antecedents in the forward-looking center (Cf ) list, and then choose the most salient candidate, Cp, as an antecedent of a zero-pronoun according to heuristic rules (e.g. topic > subject > indirect object > direct object > others1 ). Although these methods have a theoretical advantage, they have a serious drawback in that the original Centering Theory is restricted to keeping information about the previous sentence only. In order to loosen this restriction, the Centering-based m"
P09-1073,J96-2005,0,0.412033,"rious drawback in that Centering Theory only retains information about the previous sentence. A few methods have attempted to overcome this fault (Suri and McCoy, 1994; Hahn and Strube, 1997), but they are overly dependent upon the restrictions fundamental to the notion of centering. We hope that by relaxing such restrictions it will be possible for an anaphora resolution system to achieve a good balance between accuracy and computational cost. From this background, we focus on the issue of reducing candidate antecedents (discourse entities) for a given anaphor. Inspired by Walker’s argument (Walker, 1996), we propose a machine learning-based caching mechanism that captures the most salient candidates at each point of the discourse for efficient anaphora resolution. More specifically, we choose salient candidates for each sentence from the set of candidates appearing in that sentence and the candidates which are already 647 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 647–655, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP in the cache. Searching only through the set of salient candidates, the computational cost of zeroanaphora resolution is"
P09-1073,P03-1023,0,0.080586,"cij is the j-th candidate in sentence Si . In this situation, for example, candidate c12 is labeled as retained when creating training instances for sentence S1 , but labeled as discarded from S2 onwards, because of the appearance of its zeropronoun. Another candidate c13 which is never referred to in the text is labeled as discarded for all training instances. Second, we need to capture the ‘relative’ salience of candidates appearing in the current discourse for each cache update, as also exploited in the tournament-based or ranking-based approaches to anaphora resolution (Iida et al., 2003; Yang et al., 2003; Denis and Baldridge, 2008). To solve it, we use a ranker trained on the instances created as described above. In order to train the ranker, we adopt the Ranking SVM algorithm (Joachims, 2002), which learns a weight vector to rank candidates for a given partial ranking of each discourse entity. Each training instance is created from the set of retained candidates, Ri , paired with the set of discarded candidates, Di , in each sentence. To Figure 2: Pseudo-code for creating training instances training instances S1 c11 c12 c13 c14 S 2 c21 c22 c23 φ φ i j S3 c31 c32 c33 φk φ retained c11 c12 dis"
P09-1073,P08-1096,0,0.399047,"esolution to make NLP applications such as IE and MT more reliable. In particular, for languages such as Japanese, anaphora resolution is crucial for resolving a phrase in a text to its referent since phrases, especially nominative arguments of predicates, are frequently omitted by anaphoric functions in discourse (Iida et al., 2007b). Many researchers have recently explored machine learning-based methods using considerable amounts of annotated data provided by, for example, the Message Understanding Conference and Automatic Context Extraction programs (Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2008; McCallum and Wellner, 2003, etc.). These methods reach a level comparable to or better than the state-of-the-art rule-based systems (e.g. Baldwin (1995)) by recasting the task of anaphora resolution into classification or clustering problems. However, such approaches tend to disregard theoretical findings from discourse theories, such as Centering Theory (Grosz et al., 1995). Therefore, one of the challenging issues in this area is to incorporate such findings from linguistic theories into machine learning-based approaches. A typical machine learning-based approach to zero-anaphora resolutio"
P09-1073,P95-1017,0,\N,Missing
P09-1109,P92-1003,0,0.143223,"metry in conjunct structures and dependencies between coordinated head words. The model was then used to rerank the nbest outputs of the Bikel parser (2005). Recently, Buyko et al. (2007; 2008) and Shimbo and Hara (2007) applied discriminative learning methods to coordinate structure analysis. Buyko et al. used a linear-chain CRF, whereas Shimbo and Hara proposed an approach based on perceptron learning of edit distance between conjuncts. Shimbo and Hara’s approach has its root in Kurohashi and Nagao’s (1994) rule-based method for Japanese coordinations. Other studies on coordination include (Agarwal and Boggess, 1992; Chantree et al., 2005; Goldberg, 1999; Okumura and Muraki, 1994). technologies. One such potential application is extracting the outcome of clinical tests as illustrated above. 2. As the system is designed independently from parsers, it can be combined with any types of parsers (e.g., phrase structure or dependency parsers), if necessary. 3. Because coordination bracketing is sometimes inconsistent with phrase structure bracketing, processing coordinations apart from phrase structures might be beneficial. Consider, for example, John likes, and Bill adores, Sue. (Carston and Blakemore, 2005)"
P09-1109,C08-1012,0,0.332435,"Missing"
P09-1109,J94-4001,0,0.789655,"Missing"
P09-1109,H05-1105,0,0.0997754,"apping (non-nested coordinations), or (ii) one coordination must be embedded within the scope of a conjunct of the other coordination (nested coordinations). Below, we call a parse tree built from the grammar a coordination tree. Related work Resnik (1999) disambiguated coordinations of the form [n1 and n2 n3 ], where ni are all nouns. This type of phrase has two possible readings: [(n1 ) and (n2 n3 )] and [((n1 ) and (n2 )) n3 ]. He demonstrated the effectiveness of semantic similarity calculated from a large text collection, and agreement of numbers between n1 and n2 and between n1 and n3 . Nakov and Hearst (2005) collected web-based 968 COORD COORD CJT N CC SEP W 3.1.2 Production rules Table 1: Non-terminals Complete coordination. Partially-built coordination. Conjunct. Non-coordination. Coordinate conjunction like “and,” “or,” and “but”. Connector of conjuncts other than CC: e.g., punctuations like “,” and “;”. Any word. Table 2 lists the production rules. Rules are shown with explicit subscripts indicating the span of their production. The subscript to a terminal word (shown in a box) specifies its position within a sentence (word index). Non-terminals have two subscript indices denoting the span o"
P09-1109,A94-1007,0,0.101946,"head words. The model was then used to rerank the nbest outputs of the Bikel parser (2005). Recently, Buyko et al. (2007; 2008) and Shimbo and Hara (2007) applied discriminative learning methods to coordinate structure analysis. Buyko et al. used a linear-chain CRF, whereas Shimbo and Hara proposed an approach based on perceptron learning of edit distance between conjuncts. Shimbo and Hara’s approach has its root in Kurohashi and Nagao’s (1994) rule-based method for Japanese coordinations. Other studies on coordination include (Agarwal and Boggess, 1992; Chantree et al., 2005; Goldberg, 1999; Okumura and Muraki, 1994). technologies. One such potential application is extracting the outcome of clinical tests as illustrated above. 2. As the system is designed independently from parsers, it can be combined with any types of parsers (e.g., phrase structure or dependency parsers), if necessary. 3. Because coordination bracketing is sometimes inconsistent with phrase structure bracketing, processing coordinations apart from phrase structures might be beneficial. Consider, for example, John likes, and Bill adores, Sue. (Carston and Blakemore, 2005) This kind of structure might be treated by assuming the presence o"
P09-1109,P05-1022,0,0.0624264,". We use global alignment (a complete path) in these smaller graphs, as opposed to chainable local alignment (partial paths) in a global edit graph used by Shimbo and Hara. Since nested coordinations cannot be encoded as chainable partial paths (Shimbo and Hara, 2007), their method cannot cope with nested coordinations such as those illustrated in Figure 2(b). 5.2 Evaluation method We tested the proposed method in two tasks: (i) identify the scope of coordinations regardless of phrase types, and (ii) detect noun phrase (NP) coordinations and identify their scopes. 4.3 Integration with parsers Charniak and Johnson (2005) reported an improved parsing accuracy by reranking n-best parse trees, using features based on similarity of coordinated phrases, among others. It should be interesting to investigate whether alignment-based features like ours can be built into their reranker, or more generally, whether the coordination scopes output by our method help improving parsing accuracy. While the goal of task (i) is to determine the scopes of 3598 coordinations, task (ii) demands both to judge whether each of the coordinations constructs an NP, and if it does, to determine its scope. 1 A C++ implementation of our me"
P09-1109,J07-4004,0,0.0131548,"nto account, but it would lead to a combinatorial 4 Discussion 4.1 Computational complexity Given an input sentence of N words, finding its maximum scoring coordination tree by a bottomup chart parsing algorithm incurs a time complexity of O(N 3 ). While the right-hand side of rules (i)–(iv) involves more than three variables and thus appears to increase complexity, this is not the case since 971 The combinatory categorial grammar (CCG) (Steedman, 2000) provides an account for various coordination constructs in an elegant manner, and incorporating alignment-based features into the CCG parser (Clark and Curran, 2007) is also a viable possibility. some of the variables ( j and k in rules (i) and (iii), and j, k, and m in rules (ii) and (iv)) are constrained by the location of conjunct connectors (CC and SEP), whose number in a sentence is negligible compared to the sentence length N. As a result, these rules can be processed in O(N 2 ) time. Hence the run-time complexity is dominated by rule (vi), which has three variables and leads to O(N 3 ). Each iteration of the perceptron algorithm for a sentence of length N also incurs O(N 3 ) for the same reason. Our method also requires pre-processing in the beginn"
P09-1109,D07-1064,1,0.683276,"ing 967 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 967–975, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP statistics with search engines and applied them to a task similar to Resnik’s. Hogan (2007) improved the parsing accuracy of sentences in which coordinated noun phrases are known to exist. She presented a generative model incorporating symmetry in conjunct structures and dependencies between coordinated head words. The model was then used to rerank the nbest outputs of the Bikel parser (2005). Recently, Buyko et al. (2007; 2008) and Shimbo and Hara (2007) applied discriminative learning methods to coordinate structure analysis. Buyko et al. used a linear-chain CRF, whereas Shimbo and Hara proposed an approach based on perceptron learning of edit distance between conjuncts. Shimbo and Hara’s approach has its root in Kurohashi and Nagao’s (1994) rule-based method for Japanese coordinations. Other studies on coordination include (Agarwal and Boggess, 1992; Chantree et al., 2005; Goldberg, 1999; Okumura and Muraki, 1994). technologies. One such potential application is extracting the outcome of clinical tests as illustrated above. 2. As the system"
P09-1109,W02-1001,0,0.0171016,"position of b upward to the COORD node. The same applies to computing the similarity of b and c; the end position of c is needed at the left COORD . The bracketed index of COORD exactly maintains this information, i.e., the end of the first conjunct below the COORD . See production rules (iii) and (iv) in Table 2. 3.3 Perceptron learning of feature weights As we saw above, our model is a linear model with the global weight vector w acting as the coefficient vector, and hence various existing techniques can be exploited to optimize w. In this paper, we use the averaged perceptron learning (Collins, 2002; Freund and Schapire, 1999) to optimize w on a training corpus, so that the system assigns the highest score to the correct coordination tree among all possible trees for each training sentence. 3.2.4 Coordination with three or more conjuncts For a coordination with three or more conjuncts, we define its score as the sum of the similarity scores of all pairwise consecutive conjuncts; i.e., for a coordination “a, b, c, and d” with four conjuncts, the score is the sum of the similarity scores for conjunct pairs (a, b), (b, c), and (c, d). Ideally, we should take all combinations of conjuncts in"
P09-1109,P99-1081,0,0.0195702,"een coordinated head words. The model was then used to rerank the nbest outputs of the Bikel parser (2005). Recently, Buyko et al. (2007; 2008) and Shimbo and Hara (2007) applied discriminative learning methods to coordinate structure analysis. Buyko et al. used a linear-chain CRF, whereas Shimbo and Hara proposed an approach based on perceptron learning of edit distance between conjuncts. Shimbo and Hara’s approach has its root in Kurohashi and Nagao’s (1994) rule-based method for Japanese coordinations. Other studies on coordination include (Agarwal and Boggess, 1992; Chantree et al., 2005; Goldberg, 1999; Okumura and Muraki, 1994). technologies. One such potential application is extracting the outcome of clinical tests as illustrated above. 2. As the system is designed independently from parsers, it can be combined with any types of parsers (e.g., phrase structure or dependency parsers), if necessary. 3. Because coordination bracketing is sometimes inconsistent with phrase structure bracketing, processing coordinations apart from phrase structures might be beneficial. Consider, for example, John likes, and Bill adores, Sue. (Carston and Blakemore, 2005) This kind of structure might be treated"
P09-1109,P07-1086,0,\N,Missing
P09-2002,P09-2002,1,0.0523391,"ghest score to paths denoting the correct co(c) Path 2 (d) Path 3 (no coordination) ordinate structure. Shimbo and Hara defined this measure as a linear function of many features asFigure 1: Alignment graph for “a policeman and sociated to arcs, and used perceptron training to warehouse guard” ((a)), and example paths repreoptimize the weight coefficients for these features senting different coordinate structure ((b)–(d)). from corpora. 2 2.2 Features Alignment-based coordinate structure analysis For the description of features used in our adaptation of the Shimbo-Hara model to Japanese, see (Okuma et al., 2009). In this model, all features are defined as indicator functions asking whether one or more attributes (e.g., surface form, part-ofspeech) take specific values at the neighbor of an arc. One example of a feature assigned to a diagonal arc at row i and column j of the alignment graph is ⎧ ⎨1 if POS[i] = Noun, POS[ j] = Adjective, f= and the label of the arc is Inside, ⎩ 0 otherwise. We first describe Shimbo and Hara’s method upon which our improvements are made. 2.1 Triangular alignment graph The basis of their method is a triangular alignment graph, illustrated in Figure 1(a). Kurohashi and Na"
P09-2002,D07-1064,1,0.797164,"anese, on the other hand, detecting coordination is non-trivial. Many of the coordination markers in Japanese are ambiguous and do not always indicate the presence of coordinations. Compare sentences (1) and (2) below: (1) rondon to pari ni itta (London) (and) (Paris) (to) (went) (I went to London and Paris) Introduction (2) kanojo (her) Coordination remains one of the challenging problems in natural language processing. One key characteristic of coordination explored in the past is the structural and semantic symmetry of conjuncts (Chantree et al., 2005; Hogan, 2007; Resnik, 1999). Recently, Shimbo and Hara (2007) proposed to use a large number of features to model this symmetry, and optimize the feature weights with perceptron training. These features are assigned to the arcs of the alignment graph (or edit graph) originally developed for biological sequence alignment. Coordinate structure analysis involves two related but different tasks: to pari ni itta (with) (Paris) (to) (went) (I went to Paris with her) These sentences differ only in the first word. Both contain a particle to, which is one of the most frequent coordination markers in Japanese—but only the first sentence contains a coordinate stru"
P09-2002,J94-4001,0,\N,Missing
P09-2002,P07-1086,0,\N,Missing
P10-2018,D09-1003,0,0.0547945,"Missing"
P10-2018,D09-1002,0,0.0294119,"Missing"
P10-2018,W08-2123,0,0.0388026,"Missing"
P10-2018,D07-1033,0,0.0290824,"pair of p and A. We deﬁne the score function for ∑ predicate-argument structures as s(p, A) = Fk ∈F Fk (x, p, A). F is a set of all the factors, Fk (x, p, A) corresponds to a particular factor in Figure 1, and gives a score to a predicate or argument label assignments. Since we use linear models, Fk (x, p, A) = w · Φk (x, p, A). 2.2 Inference The crucial point of the model is how to deal with the global factor FG , because enumerating possible assignments is too costly. A number of methods have been proposed for the use of global features for linear models such as (Daum´e III and Marcu, 2005; Kazama and Torisawa, 2007). In this work, we use the approach proposed in (Kazama and Torisawa, 2007). Although the approach is proposed for sequence labeling tasks, it 99 can be easily extended to our structured model. That is, for each possible predicate sense p of the predicate, we provide N-best argument role assignments using three local factors FP , FA and FP A , and then add scores of the global factor FG , ﬁnally select the argmax from them. In this case, the argmax is selected from |Pl |N candidates. lL+G is the loss function for the case of using both local and global features, corresponding to the constraint"
P10-2018,N09-1018,0,0.113401,"anguages without applying any feature engineering procedure. Table 3 shows the performances of predicate sense disambiguation and argument role labeling separately. In terms of sense disambiguation results, incorporating FP A and FG worked well. Although incorporating either of FP A and FG provided improvements of +0.13 and +0.18 on average, adding both factors provided improvements of +0.50. We compared the predicate sense disTable 3: Predicate sense disambiguation and argument role labeling results (average). used for FP A are inspired by formulae used in the MLN-based SRL systems, such as (Meza-Ruiz and Riedel, 2009b). We used the same feature templates for all languages. 3.2 Results Table 2 shows the results of the experiments, and also shows the results of the top 3 systems in the CoNLL-2009 Shared Task participants of the SRLonly system. By incorporating FP A , we achieved performance improvement for all languages. This results suggest that it is effective to capture local interdependencies between a predicate sense and one of its argument roles. Comparing the results with FP +FA and FP +FA +FG , incorporating FG also contributed performance improvements for all languages, especially the substantial F"
P10-2018,W09-1213,0,0.0544183,"anguages without applying any feature engineering procedure. Table 3 shows the performances of predicate sense disambiguation and argument role labeling separately. In terms of sense disambiguation results, incorporating FP A and FG worked well. Although incorporating either of FP A and FG provided improvements of +0.13 and +0.18 on average, adding both factors provided improvements of +0.50. We compared the predicate sense disTable 3: Predicate sense disambiguation and argument role labeling results (average). used for FP A are inspired by formulae used in the MLN-based SRL systems, such as (Meza-Ruiz and Riedel, 2009b). We used the same feature templates for all languages. 3.2 Results Table 2 shows the results of the experiments, and also shows the results of the top 3 systems in the CoNLL-2009 Shared Task participants of the SRLonly system. By incorporating FP A , we achieved performance improvement for all languages. This results suggest that it is effective to capture local interdependencies between a predicate sense and one of its argument roles. Comparing the results with FP +FA and FP +FA +FG , incorporating FG also contributed performance improvements for all languages, especially the substantial F"
P10-2018,W08-2125,0,0.0200125,"lysis. They used argument sequences tied with a predicate sense (e.g. AGENT-buy.01/ActivePATIENT) as a feature for the re-ranker of the system where predicate sense and argument role candidates are generated by their pipelined architecture. They reported that incorporating this type of features provides substantial gain of the system performance. The other factor is inter-dependencies between a predicate sense and argument roles, which relate to selectional preference, and motivated us to jointly identify a predicate sense and its argument roles. This type of dependencies has been explored by Riedel and Meza-Ruiz (2008; 2009b; 2009a), all of which use Markov Logic Networks (MLN). The work uses the global formulae that have atoms in terms of both a predicate sense and each of its argument roles, and the system identiﬁes predicate senses and argument roles simultaneously. Ideally, we want to capture both types of dependencies simultaneously. The former approaches can not explicitly include features that capture inter-dependencies between a predicate sense and its argument roles. Though these are implicitly incorporated by re-ranking where the most plausible assignment is selected from a small subset of predic"
P10-2018,W08-2121,0,0.130008,"Missing"
P10-2018,J08-2002,0,0.161717,"Missing"
P10-2018,W09-1206,0,0.0674744,"Missing"
P11-2006,I08-1065,1,0.855014,"AT i Scale p so that the components sum to one. p ← S ELECT KB EST(p, k) i ← Ap Scale i so that the components sum to one. i ← S ELECT KB EST(i, m) return i and p function S ELECT KB EST(v, k) Retain only the k largest components of v, resetting the remaining components to 0. 20: return v 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: clustering to find competing semantic classes (negative categories). 2.3 Espresso Espresso (Pantel and Pennacchiotti, 2006) is one of the state-of-the-art bootstrapping algorithms used in many natural language tasks (Komachi and Suzuki, 2008; Abe et al., 2008; Ittoo and Bouma, 2010; Yoshida et al., 2010). Espresso takes advantage of pointwise mutual information (pmi) (Manning and Sch¨utze, 1999) between instances and patterns to evaluate their reliability. Let n be the number of all instances in the corpus, and p the number of all possible patterns. We denote all pmi values as an n × p instance-pattern matrix A, with the (i, j) element of A holding the value of pmi between the ith instance and the jth pattern. Let AT denote the matrix transpose of A. Algorithm 1 shows the pseudocode of Espresso. The input vector i0 (called seed vector) is an ndime"
P11-2006,J04-3004,0,0.0254462,"s and creating stop lists are two effective ways to reduce semantic drift, but these methods generally need human supervision. In this paper, we propose a graphbased approach to helping editors choose effective seeds and stop list instances, applicable to Pantel and Pennacchiotti’s Espresso bootstrapping algorithm. The idea is to select seeds and create a stop list using the rankings of instances and patterns computed by Kleinberg’s HITS algorithm. Experimental results on a variation of the lexical sample task show the effectiveness of our method. 1 Introduction Bootstrapping (Yarowsky, 1995; Abney, 2004) is a technique frequently used in natural language processing to expand limited resources with minimal supervision. Given a small amount of sample data (seeds) representing a particular semantic class of interest, bootstrapping first trains a classifier (which often is a weighted list of surface patterns characterizing the seeds) using the seeds, and then apply it on the remaining data to select instances most likely to be of the same class as the seeds. These selected instances are added to the seed set, and the process is iterated until sufficient labeled data are acquired. Many bootstrappi"
P11-2006,P94-1020,0,0.0126978,"Missing"
P11-2006,W03-0407,0,0.0770922,"acterizing the seeds) using the seeds, and then apply it on the remaining data to select instances most likely to be of the same class as the seeds. These selected instances are added to the seed set, and the process is iterated until sufficient labeled data are acquired. Many bootstrapping algorithms have been proposed for a variety of tasks: word sense disambiguation (Yarowsky, 1995; Abney, 2004), information extraction (Hearst, 1992; Riloff and Jones, 1999; Thelen and Riloff, 2002; Pantel and Pennacchiotti, 2006), named entity recognition (Collins and Singer, 1999), part-of-speech tagging (Clark et al., 2003), 30 and statistical parsing (Steedman et al., 2003; McClosky et al., 2006). Bootstrapping algorithms, however, are known to suffer from the problem called semantic drift: as the iteration proceeds, the algorithms tend to select instances increasingly irrelevant to the seed instances (Curran et al., 2007). For example, suppose we want to collect the names of common tourist sites from a web corpus. Given seed instances {New York City, Maldives Islands}, bootstrapping might learn, at one point of the iteration, patterns like “pictures of X” and “photos of X,” which also co-occur with many irrele"
P11-2006,W99-0613,0,0.0704383,"h often is a weighted list of surface patterns characterizing the seeds) using the seeds, and then apply it on the remaining data to select instances most likely to be of the same class as the seeds. These selected instances are added to the seed set, and the process is iterated until sufficient labeled data are acquired. Many bootstrapping algorithms have been proposed for a variety of tasks: word sense disambiguation (Yarowsky, 1995; Abney, 2004), information extraction (Hearst, 1992; Riloff and Jones, 1999; Thelen and Riloff, 2002; Pantel and Pennacchiotti, 2006), named entity recognition (Collins and Singer, 1999), part-of-speech tagging (Clark et al., 2003), 30 and statistical parsing (Steedman et al., 2003; McClosky et al., 2006). Bootstrapping algorithms, however, are known to suffer from the problem called semantic drift: as the iteration proceeds, the algorithms tend to select instances increasingly irrelevant to the seed instances (Curran et al., 2007). For example, suppose we want to collect the names of common tourist sites from a web corpus. Given seed instances {New York City, Maldives Islands}, bootstrapping might learn, at one point of the iteration, patterns like “pictures of X” and “photo"
P11-2006,C92-2082,0,0.0559469,"of sample data (seeds) representing a particular semantic class of interest, bootstrapping first trains a classifier (which often is a weighted list of surface patterns characterizing the seeds) using the seeds, and then apply it on the remaining data to select instances most likely to be of the same class as the seeds. These selected instances are added to the seed set, and the process is iterated until sufficient labeled data are acquired. Many bootstrapping algorithms have been proposed for a variety of tasks: word sense disambiguation (Yarowsky, 1995; Abney, 2004), information extraction (Hearst, 1992; Riloff and Jones, 1999; Thelen and Riloff, 2002; Pantel and Pennacchiotti, 2006), named entity recognition (Collins and Singer, 1999), part-of-speech tagging (Clark et al., 2003), 30 and statistical parsing (Steedman et al., 2003; McClosky et al., 2006). Bootstrapping algorithms, however, are known to suffer from the problem called semantic drift: as the iteration proceeds, the algorithms tend to select instances increasingly irrelevant to the seed instances (Curran et al., 2007). For example, suppose we want to collect the names of common tourist sites from a web corpus. Given seed instance"
P11-2006,P10-1135,0,0.0168595,"at the components sum to one. p ← S ELECT KB EST(p, k) i ← Ap Scale i so that the components sum to one. i ← S ELECT KB EST(i, m) return i and p function S ELECT KB EST(v, k) Retain only the k largest components of v, resetting the remaining components to 0. 20: return v 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: clustering to find competing semantic classes (negative categories). 2.3 Espresso Espresso (Pantel and Pennacchiotti, 2006) is one of the state-of-the-art bootstrapping algorithms used in many natural language tasks (Komachi and Suzuki, 2008; Abe et al., 2008; Ittoo and Bouma, 2010; Yoshida et al., 2010). Espresso takes advantage of pointwise mutual information (pmi) (Manning and Sch¨utze, 1999) between instances and patterns to evaluate their reliability. Let n be the number of all instances in the corpus, and p the number of all possible patterns. We denote all pmi values as an n × p instance-pattern matrix A, with the (i, j) element of A holding the value of pmi between the ith instance and the jth pattern. Let AT denote the matrix transpose of A. Algorithm 1 shows the pseudocode of Espresso. The input vector i0 (called seed vector) is an ndimensional binary vector w"
P11-2006,I08-1047,1,0.820481,"r t = 1, 2, ..., τ do p ← AT i Scale p so that the components sum to one. p ← S ELECT KB EST(p, k) i ← Ap Scale i so that the components sum to one. i ← S ELECT KB EST(i, m) return i and p function S ELECT KB EST(v, k) Retain only the k largest components of v, resetting the remaining components to 0. 20: return v 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: clustering to find competing semantic classes (negative categories). 2.3 Espresso Espresso (Pantel and Pennacchiotti, 2006) is one of the state-of-the-art bootstrapping algorithms used in many natural language tasks (Komachi and Suzuki, 2008; Abe et al., 2008; Ittoo and Bouma, 2010; Yoshida et al., 2010). Espresso takes advantage of pointwise mutual information (pmi) (Manning and Sch¨utze, 1999) between instances and patterns to evaluate their reliability. Let n be the number of all instances in the corpus, and p the number of all possible patterns. We denote all pmi values as an n × p instance-pattern matrix A, with the (i, j) element of A holding the value of pmi between the ith instance and the jth pattern. Let AT denote the matrix transpose of A. Algorithm 1 shows the pseudocode of Espresso. The input vector i0 (called seed v"
P11-2006,D08-1106,1,0.958751,"be extracted (Curran et al., 2007; McIntosh and Curran, 2009). Drift can also be reduced with carefully selected seeds. However, both of these approaches require expert knowledge. In this paper, we propose a graph-based approach to seed selection and stop list creation for the stateof-the-art bootstrapping algorithm Espresso (Pantel and Pennacchiotti, 2006). An advantage of this approach is that it requires zero or minimal supervision. The idea is to use the hubness score of instances and patterns computed from the pointwise mutual information matrix with the HITS algorithm (Kleinberg, 1999). Komachi et al. (2008) pointed out that semantic drift in Espresso has the same root as topic drift (Bharat and Henzinger, 1998) observed with HITS, noting the algorithmic similarity between them. While Komachi et al. proposed to use algorithms different from Espresso to Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 30–36, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics avoid semantic drift, in this paper we take advantage of this similarity to make better use of Espresso. We demonstrate the effectiveness of our approa"
P11-2006,H93-1051,0,0.201279,"d we can expect the most frequent sense in the corpus to form the highest HITS ranking instances. This allows us to completely automate our experiments, without the need to manually check the HITS ranking in Step 2 of Section 3.2. That is, for the most frequent sense (majority sense), we take Step 3a and use the highest ranked instances as seeds; for the rest of the senses (minority senses), we take Step 3b and use them as the stop list. 4.1 Datasets We used the seven most frequent polysemous nouns (arm, bank, degree, difference, paper, party and shelter) in the S ENSEVAL-3 dataset, and line (Leacock et al., 1993) and interest (Bruce and Wiebe, Task Method MAP AUC R-Precision P@30 P@50 P@100 arm Random HITS Random HITS Random HITS Random HITS Random HITS Random HITS Random HITS Random HITS Random HITS 84.3 ±4.1 85.9 74.8 ±6.5 84.8 69.4 ±3.0 62.4 48.3 ±3.8 50.2 75.2 ±4.1 75.2 79.1 ±5.0 85.2 74.9 ±2.3 77.0 44.5 ±15.1 72.2 64.9 ±8.3 75.3 59.6 ±8.1 59.7 61.6 ±9.6 77.6 54.3 ±4.2 49.3 54.5 ±5.0 60.1 56.4 ±7.1 61.0 57.0 ±9.7 68.2 51.5 ±3.3 54.6 36.3 ±16.9 68.6 64.9 ±12.0 83.0 80.9 ±2.2 79.3 72.6 ±4.5 78.0 66.7 ±2.3 63.2 47.0 ±4.4 51.1 71.6 ±3.3 75.2 76.6 ±3.1 78.5 73.2 ±1.3 72.0 40.1 ±14.6 68.5 63.7 ±10.2 80."
P11-2006,N06-1020,0,0.0271799,"g data to select instances most likely to be of the same class as the seeds. These selected instances are added to the seed set, and the process is iterated until sufficient labeled data are acquired. Many bootstrapping algorithms have been proposed for a variety of tasks: word sense disambiguation (Yarowsky, 1995; Abney, 2004), information extraction (Hearst, 1992; Riloff and Jones, 1999; Thelen and Riloff, 2002; Pantel and Pennacchiotti, 2006), named entity recognition (Collins and Singer, 1999), part-of-speech tagging (Clark et al., 2003), 30 and statistical parsing (Steedman et al., 2003; McClosky et al., 2006). Bootstrapping algorithms, however, are known to suffer from the problem called semantic drift: as the iteration proceeds, the algorithms tend to select instances increasingly irrelevant to the seed instances (Curran et al., 2007). For example, suppose we want to collect the names of common tourist sites from a web corpus. Given seed instances {New York City, Maldives Islands}, bootstrapping might learn, at one point of the iteration, patterns like “pictures of X” and “photos of X,” which also co-occur with many irrelevant instances. In this case, a later iteration would likely acquire freque"
P11-2006,P09-1045,0,0.0938664,"2007). For example, suppose we want to collect the names of common tourist sites from a web corpus. Given seed instances {New York City, Maldives Islands}, bootstrapping might learn, at one point of the iteration, patterns like “pictures of X” and “photos of X,” which also co-occur with many irrelevant instances. In this case, a later iteration would likely acquire frequent words co-occurring with these generic patterns, such as Michael Jackson. Previous work has tried to reduce the effect of semantic drift by making the stop list of instances that must not be extracted (Curran et al., 2007; McIntosh and Curran, 2009). Drift can also be reduced with carefully selected seeds. However, both of these approaches require expert knowledge. In this paper, we propose a graph-based approach to seed selection and stop list creation for the stateof-the-art bootstrapping algorithm Espresso (Pantel and Pennacchiotti, 2006). An advantage of this approach is that it requires zero or minimal supervision. The idea is to use the hubness score of instances and patterns computed from the pointwise mutual information matrix with the HITS algorithm (Kleinberg, 1999). Komachi et al. (2008) pointed out that semantic drift in Espr"
P11-2006,D10-1035,0,0.0171502,"e stop list of the other sessions. Curran et al. (2007) pursued a similar idea in their Mutual Exclusion Bootstrapping, which uses multiple semantic classes in addition to hand-crafted stop lists. While multi-class bootstrapping is a clever way to reduce human supervision in stop list construction, it is not generally applicable to bootstrapping for a single class. To apply the idea of multi-class bootstrapping to singleclass bootstrapping, one has to first find appropriate competing semantic classes and good seeds for them, which is in itself a difficult problem. Along this line of research, McIntosh (2010) recently used 31 Algorithm 1 Espresso algorithm Input: Seed vector i0 Instance-pattern co-occurrence matrix A Instance cutoff parameter k Pattern cutoff parameter m Number of iterations τ Output: Instance score vector i Pattern score vector p function E SPRESSO(i0 , A, k, m, τ) i ← i0 for t = 1, 2, ..., τ do p ← AT i Scale p so that the components sum to one. p ← S ELECT KB EST(p, k) i ← Ap Scale i so that the components sum to one. i ← S ELECT KB EST(i, m) return i and p function S ELECT KB EST(v, k) Retain only the k largest components of v, resetting the remaining components to 0. 20: retu"
P11-2006,P06-1015,0,0.808699,"of interest, bootstrapping first trains a classifier (which often is a weighted list of surface patterns characterizing the seeds) using the seeds, and then apply it on the remaining data to select instances most likely to be of the same class as the seeds. These selected instances are added to the seed set, and the process is iterated until sufficient labeled data are acquired. Many bootstrapping algorithms have been proposed for a variety of tasks: word sense disambiguation (Yarowsky, 1995; Abney, 2004), information extraction (Hearst, 1992; Riloff and Jones, 1999; Thelen and Riloff, 2002; Pantel and Pennacchiotti, 2006), named entity recognition (Collins and Singer, 1999), part-of-speech tagging (Clark et al., 2003), 30 and statistical parsing (Steedman et al., 2003; McClosky et al., 2006). Bootstrapping algorithms, however, are known to suffer from the problem called semantic drift: as the iteration proceeds, the algorithms tend to select instances increasingly irrelevant to the seed instances (Curran et al., 2007). For example, suppose we want to collect the names of common tourist sites from a web corpus. Given seed instances {New York City, Maldives Islands}, bootstrapping might learn, at one point of th"
P11-2006,N03-1031,0,0.0317941,"pply it on the remaining data to select instances most likely to be of the same class as the seeds. These selected instances are added to the seed set, and the process is iterated until sufficient labeled data are acquired. Many bootstrapping algorithms have been proposed for a variety of tasks: word sense disambiguation (Yarowsky, 1995; Abney, 2004), information extraction (Hearst, 1992; Riloff and Jones, 1999; Thelen and Riloff, 2002; Pantel and Pennacchiotti, 2006), named entity recognition (Collins and Singer, 1999), part-of-speech tagging (Clark et al., 2003), 30 and statistical parsing (Steedman et al., 2003; McClosky et al., 2006). Bootstrapping algorithms, however, are known to suffer from the problem called semantic drift: as the iteration proceeds, the algorithms tend to select instances increasingly irrelevant to the seed instances (Curran et al., 2007). For example, suppose we want to collect the names of common tourist sites from a web corpus. Given seed instances {New York City, Maldives Islands}, bootstrapping might learn, at one point of the iteration, patterns like “pictures of X” and “photos of X,” which also co-occur with many irrelevant instances. In this case, a later iteration wou"
P11-2006,W02-1028,0,0.0429194,"particular semantic class of interest, bootstrapping first trains a classifier (which often is a weighted list of surface patterns characterizing the seeds) using the seeds, and then apply it on the remaining data to select instances most likely to be of the same class as the seeds. These selected instances are added to the seed set, and the process is iterated until sufficient labeled data are acquired. Many bootstrapping algorithms have been proposed for a variety of tasks: word sense disambiguation (Yarowsky, 1995; Abney, 2004), information extraction (Hearst, 1992; Riloff and Jones, 1999; Thelen and Riloff, 2002; Pantel and Pennacchiotti, 2006), named entity recognition (Collins and Singer, 1999), part-of-speech tagging (Clark et al., 2003), 30 and statistical parsing (Steedman et al., 2003; McClosky et al., 2006). Bootstrapping algorithms, however, are known to suffer from the problem called semantic drift: as the iteration proceeds, the algorithms tend to select instances increasingly irrelevant to the seed instances (Curran et al., 2007). For example, suppose we want to collect the names of common tourist sites from a web corpus. Given seed instances {New York City, Maldives Islands}, bootstrappin"
P11-2006,C02-1154,0,0.0262596,"as et al., 2009). Vyas et al. (2009) studied the impact of the composition of the seed sets on the expansion performance, confirming that seed set composition has a significant impact on the quality of expansions. They also found that the seeds chosen by non-expert editors are often worse than randomly chosen ones. A similar observation was made by McIntosh and Curran (2009), who reported that randomly chosen seeds from the gold-standard set often outperformed seeds chosen by domain experts. These results suggest that even for humans, selecting good seeds is a non-trivial task. 2.2 Stop Lists Yangarber et al. (2002) proposed to run multiple bootstrapping sessions in parallel, with each session trying to extract one of several mutually exclusive semantic classes. Thus, the instances harvested in one bootstrapping session can be used as the stop list of the other sessions. Curran et al. (2007) pursued a similar idea in their Mutual Exclusion Bootstrapping, which uses multiple semantic classes in addition to hand-crafted stop lists. While multi-class bootstrapping is a clever way to reduce human supervision in stop list construction, it is not generally applicable to bootstrapping for a single class. To app"
P11-2006,P95-1026,0,0.271556,"ecting good seeds and creating stop lists are two effective ways to reduce semantic drift, but these methods generally need human supervision. In this paper, we propose a graphbased approach to helping editors choose effective seeds and stop list instances, applicable to Pantel and Pennacchiotti’s Espresso bootstrapping algorithm. The idea is to select seeds and create a stop list using the rankings of instances and patterns computed by Kleinberg’s HITS algorithm. Experimental results on a variation of the lexical sample task show the effectiveness of our method. 1 Introduction Bootstrapping (Yarowsky, 1995; Abney, 2004) is a technique frequently used in natural language processing to expand limited resources with minimal supervision. Given a small amount of sample data (seeds) representing a particular semantic class of interest, bootstrapping first trains a classifier (which often is a weighted list of surface patterns characterizing the seeds) using the seeds, and then apply it on the remaining data to select instances most likely to be of the same class as the seeds. These selected instances are added to the seed set, and the process is iterated until sufficient labeled data are acquired. Ma"
P12-1069,W06-2922,0,0.42502,"Missing"
P12-1069,P04-1015,0,0.203063,"wn (beam 8, pred 5) and shift-reduce (beam 8) and MST(2nd) parsers in Table 1. 0.4 0.2 0 0 10 20 30 40 50 length of input sentence 60 70 Figure 5: Scatter plot of parsing time against sentence length, comparing with top-down, 2nd-MST and shiftreduce parsers (beam size: 8, pred size: 5) we used the information of words and fine-grained POS-tags for features. We also implemented and experimented Huang and Sagae (2010)’s arc-standard shift-reduce parser. For the 2nd-order Eisner-Satta algorithm, we used MSTParser (McDonald, 2012). We used an early update version of averaged perceptron algorithm (Collins and Roark, 2004) for training of shift-reduce and top-down parsers. A set of feature templates in (Huang and Sagae, 2010) were used for the stack-based model, and a set of feature templates in (McDonald and Pereira, 2006) were used for the 2nd-order prediction model. The weighted prediction and stack-based models of topdown parser were jointly trained. 8.1 Results for English Data During training, we fixed the prediction size and beam size to 5 and 16, respectively, judged by pre663 top-down (beam:8, pred:5) shift-reduce (beam:8) 2nd-MST oracle (sh+mst) oracle (top+sh) oracle (top+mst) oracle (top+sh+mst) acc"
P12-1069,P81-1022,0,0.771692,", 2004; Zhang and Clark, 2008), are widely used for dependency analysis because of the efficiency and comparatively good performance. However, these parsers have one major problem that they can handle only local information. Isozaki et al. (2004) pointed out that the drawbacks of shift-reduce parser could be resolved by incorporating top-down information such as root finding. This work presents an O(n2 ) top-down headdriven transition-based parsing algorithm which can parse complex structures that are not trivial for shiftreduce parsers. The deductive system is very similar to Earley parsing (Earley, 1970). The Earley prediction is tied to a particular grammar rule, but the proposed algorithm is data-driven, following the current trends of dependency parsing (Nivre, 2006; McDonald and Pereira, 2006; Koo et al., 2010). To do the prediction without any grammar rules, we introduce a weighted prediction that is to predict lower nodes from higher nodes with a statistical model. Definition 2.1 (Dependency Graph) Given an input sentence W = n0 . . . nn where n0 is a special root node $, a directed graph is defined as GW = (VW , AW ) where VW = {0, 1, . . . , n} is a set of (indices of) nodes and AW ⊆"
P12-1069,P99-1059,0,0.203438,"pushes them into hypo in line 9 of Algorithm 1. 6 Time Complexity Our proposed top-down algorithm has three kinds of actions which are scan, comp and predict. Each scan and comp actions occurs n times when parsing a sentence with the length n. Predict action also occurs n times in which a child node is selected from 662 n ∑ i= n(n + 1) . (11) 2 n2 As for prediction is the most dominant factor, the time complexity of the algorithm is O(n2 ) and that of the algorithm with beam search is O(n2 ∗ b). 7 Related Work Alshawi (1996) proposed head automaton which recognizes an input sentence top-down. Eisner and Satta (1999) showed that there is a cubic-time parsing algorithm on the formalism of the head automaton grammars, which are equivalently converted into split-head bilexical context-free grammars (SBCFGs) (McAllester, 1999; Johnson, 2007). Although our proposed algorithm does not employ the formalism of SBCFGs, it creates left children before right children, implying that it does not have spurious ambiguities as well as parsing algorithms on the SBCFGs. Head-corner parsing algorithm (Kay, 1989) creates dependency tree top-down, and in this our algorithm has similar spirit to it. Yamada and Matsumoto (2003)"
P12-1069,N10-1115,0,0.0762616,"Missing"
P12-1069,D11-1137,1,0.712619,"Missing"
P12-1069,P10-1110,0,0.439528,"rsing with Top-down Prediction Katsuhiko Hayashi† , Taro Watanabe‡ , Masayuki Asahara§ , Yuji Matsumoto† † Nara Institute of Science and Technology Ikoma, Nara, 630-0192, Japan ‡ National Institute of Information and Communications Technology Sorakugun, Kyoto, 619-0289, Japan § National Institute for Japanese Language and Linguistics Tachikawa, Tokyo, 190-8561, Japan katsuhiko-h@is.naist.jp, taro.watanabe@nict.go.jp masayu-a@ninjal.ac.jp, matsu@is.naist.jp Abstract To improve parsing flexibility in deterministic parsing, our top-down parser uses beam search algorithm with dynamic programming (Huang and Sagae, 2010). The complexity becomes O(n2 ∗ b) where b is the beam size. To reduce prediction errors, we propose a lookahead technique based on a FIRST function, inspired by the LL(1) parser (Aho and Ullman, 1972). Experimental results show that the proposed top-down parser achieves competitive results with other data-driven parsing algorithms. This paper presents a novel top-down headdriven parsing algorithm for data-driven projective dependency analysis. This algorithm handles global structures, such as clause and coordination, better than shift-reduce or other bottom-up algorithms. Experiments on the E"
P12-1069,C04-1040,0,0.0334442,"ttom-up algorithms. Experiments on the English Penn Treebank data and the Chinese CoNLL-06 data show that the proposed algorithm achieves comparable results with other data-driven dependency parsing algorithms. 2 Definition of Dependency Graph 1 Introduction A dependency graph is defined as follows. Transition-based parsing algorithms, such as shiftreduce algorithms (Nivre, 2004; Zhang and Clark, 2008), are widely used for dependency analysis because of the efficiency and comparatively good performance. However, these parsers have one major problem that they can handle only local information. Isozaki et al. (2004) pointed out that the drawbacks of shift-reduce parser could be resolved by incorporating top-down information such as root finding. This work presents an O(n2 ) top-down headdriven transition-based parsing algorithm which can parse complex structures that are not trivial for shiftreduce parsers. The deductive system is very similar to Earley parsing (Earley, 1970). The Earley prediction is tied to a particular grammar rule, but the proposed algorithm is data-driven, following the current trends of dependency parsing (Nivre, 2006; McDonald and Pereira, 2006; Koo et al., 2010). To do the predic"
P12-1069,W07-2416,0,0.0235581,"e results are almost 664 the same as those of the English results. 8.3 Analysis of Results Table 4 shows two interesting results, on which topdown parser is superior to either shift-reduce parser or 2nd-MST parser. The sentence No.717 contains an adverbial clause structure between the subject and the main verb. Top-down parser is able to handle the long-distance dependency while shift-reudce parser cannot correctly analyze it. The effectiveness on the clause structures implies that our head-driven parser may handle non-projective structures well, which are introduced by Johansonn’s head rule (Johansson and Nugues, 2007). The sentence No.127 contains a coordination structure, which it is difficult for bottom-up parsers to handle, but, top-down parser handles it well because its top-down prediction globally captures the coordination. 9 Conclusion This paper presents a novel head-driven parsing algorithm and empirically shows that it is as practical as other dependency parsing algorithms. Our head-driven parser has potential for handling nonprojective structures better than other non-projective dependency algorithms (McDonald et al., 2005; Attardi, 2006; Nivre, 2008b; Koo et al., 2010). We are in the process of"
P12-1069,P07-1022,0,0.0198248,"length n. Predict action also occurs n times in which a child node is selected from 662 n ∑ i= n(n + 1) . (11) 2 n2 As for prediction is the most dominant factor, the time complexity of the algorithm is O(n2 ) and that of the algorithm with beam search is O(n2 ∗ b). 7 Related Work Alshawi (1996) proposed head automaton which recognizes an input sentence top-down. Eisner and Satta (1999) showed that there is a cubic-time parsing algorithm on the formalism of the head automaton grammars, which are equivalently converted into split-head bilexical context-free grammars (SBCFGs) (McAllester, 1999; Johnson, 2007). Although our proposed algorithm does not employ the formalism of SBCFGs, it creates left children before right children, implying that it does not have spurious ambiguities as well as parsing algorithms on the SBCFGs. Head-corner parsing algorithm (Kay, 1989) creates dependency tree top-down, and in this our algorithm has similar spirit to it. Yamada and Matsumoto (2003) applied a shiftreduce algorithm to dependency analysis, which is known as arc-standard transition-based algorithm (Nivre, 2004). Nivre (2003) proposed another transition-based algorithm, known as arc-eager algorithm. The arc"
P12-1069,W89-0206,0,0.583899,". 7 Related Work Alshawi (1996) proposed head automaton which recognizes an input sentence top-down. Eisner and Satta (1999) showed that there is a cubic-time parsing algorithm on the formalism of the head automaton grammars, which are equivalently converted into split-head bilexical context-free grammars (SBCFGs) (McAllester, 1999; Johnson, 2007). Although our proposed algorithm does not employ the formalism of SBCFGs, it creates left children before right children, implying that it does not have spurious ambiguities as well as parsing algorithms on the SBCFGs. Head-corner parsing algorithm (Kay, 1989) creates dependency tree top-down, and in this our algorithm has similar spirit to it. Yamada and Matsumoto (2003) applied a shiftreduce algorithm to dependency analysis, which is known as arc-standard transition-based algorithm (Nivre, 2004). Nivre (2003) proposed another transition-based algorithm, known as arc-eager algorithm. The arc-eager algorithm processes rightdependent top-down, but this does not involve the prediction of lower nodes from higher nodes. Therefore, the arc-eager algorithm is a totally bottom-up algorithm. Zhang and Clark (2008) proposed a combination approach of the tra"
P12-1069,P10-2035,0,0.537808,"Missing"
P12-1069,P10-1001,0,0.138001,"Missing"
P12-1069,D10-1125,0,0.0267333,"information. Isozaki et al. (2004) pointed out that the drawbacks of shift-reduce parser could be resolved by incorporating top-down information such as root finding. This work presents an O(n2 ) top-down headdriven transition-based parsing algorithm which can parse complex structures that are not trivial for shiftreduce parsers. The deductive system is very similar to Earley parsing (Earley, 1970). The Earley prediction is tied to a particular grammar rule, but the proposed algorithm is data-driven, following the current trends of dependency parsing (Nivre, 2006; McDonald and Pereira, 2006; Koo et al., 2010). To do the prediction without any grammar rules, we introduce a weighted prediction that is to predict lower nodes from higher nodes with a statistical model. Definition 2.1 (Dependency Graph) Given an input sentence W = n0 . . . nn where n0 is a special root node $, a directed graph is defined as GW = (VW , AW ) where VW = {0, 1, . . . , n} is a set of (indices of) nodes and AW ⊆ VW × VW is a set of directed arcs. The set of arcs is a set of pairs (x, y) where x is a head and y is a dependent of x. x →∗ l denotes a path from x to l. A directed graph GW = (VW , AW ) is well-formed if and only"
P12-1069,E06-1011,0,0.214123,"t they can handle only local information. Isozaki et al. (2004) pointed out that the drawbacks of shift-reduce parser could be resolved by incorporating top-down information such as root finding. This work presents an O(n2 ) top-down headdriven transition-based parsing algorithm which can parse complex structures that are not trivial for shiftreduce parsers. The deductive system is very similar to Earley parsing (Earley, 1970). The Earley prediction is tied to a particular grammar rule, but the proposed algorithm is data-driven, following the current trends of dependency parsing (Nivre, 2006; McDonald and Pereira, 2006; Koo et al., 2010). To do the prediction without any grammar rules, we introduce a weighted prediction that is to predict lower nodes from higher nodes with a statistical model. Definition 2.1 (Dependency Graph) Given an input sentence W = n0 . . . nn where n0 is a special root node $, a directed graph is defined as GW = (VW , AW ) where VW = {0, 1, . . . , n} is a set of (indices of) nodes and AW ⊆ VW × VW is a set of directed arcs. The set of arcs is a set of pairs (x, y) where x is a head and y is a dependent of x. x →∗ l denotes a path from x to l. A directed graph GW = (VW , AW ) is well"
P12-1069,H05-1066,0,0.130502,"Missing"
P12-1069,J03-1006,0,0.035985,"-order one for weighted prediction. 661 Pred takes either predx or predy . Beam search is performed based on the following linear order for the two states p and p′ at the same step, which have (cfw , cin ) and (c′ fw , c′ in ) respectively: p ≻ p′ iff cfw < c′ fw or cfw = c′ fw ∧ cin < c′ . (9) a node sequence in the input queue. Thus, the algorithm takes the following times for prediction: n + (n − 1) + · · · + 1 = i in We prioritize the forward cost over the inside cost since forward cost pertains to longer action sequence and is better suited to evaluate hypothesis states than inside cost (Nederhof, 2003). 5.4 FIRST Function for Lookahead Top-down backtrack parser usually reduces backtracking by precomputing the set FIRST(·) (Aho and Ullman, 1972). We define the set FIRST(·) for our top-down dependency parser: FIRST(t’) = {ld.t|ld ∈ lmdescendant(Tree, t’) Tree ∈ Corpus} (10) where t’ is a POS-tag, Tree is a correct dependency tree which exists in Corpus, a function lmdescendant(Tree, t’) returns the set of the leftmost descendant node ld of each nodes in Tree whose POS-tag is t’, and ld.t denotes a POS-tag of ld. Though our parser does not backtrack, it looks ahead when selecting possible chil"
P12-1069,W03-3017,0,0.098252,"rted into split-head bilexical context-free grammars (SBCFGs) (McAllester, 1999; Johnson, 2007). Although our proposed algorithm does not employ the formalism of SBCFGs, it creates left children before right children, implying that it does not have spurious ambiguities as well as parsing algorithms on the SBCFGs. Head-corner parsing algorithm (Kay, 1989) creates dependency tree top-down, and in this our algorithm has similar spirit to it. Yamada and Matsumoto (2003) applied a shiftreduce algorithm to dependency analysis, which is known as arc-standard transition-based algorithm (Nivre, 2004). Nivre (2003) proposed another transition-based algorithm, known as arc-eager algorithm. The arc-eager algorithm processes rightdependent top-down, but this does not involve the prediction of lower nodes from higher nodes. Therefore, the arc-eager algorithm is a totally bottom-up algorithm. Zhang and Clark (2008) proposed a combination approach of the transition-based algorithm with graph-based algorithm (McDonald and Pereira, 2006), which is the same as our combination model of stack-based and prediction models. 8 Experiments Experiments were performed on the English Penn Treebank data and the Chinese CoN"
P12-1069,W04-0308,0,0.433655,"hms. This paper presents a novel top-down headdriven parsing algorithm for data-driven projective dependency analysis. This algorithm handles global structures, such as clause and coordination, better than shift-reduce or other bottom-up algorithms. Experiments on the English Penn Treebank data and the Chinese CoNLL-06 data show that the proposed algorithm achieves comparable results with other data-driven dependency parsing algorithms. 2 Definition of Dependency Graph 1 Introduction A dependency graph is defined as follows. Transition-based parsing algorithms, such as shiftreduce algorithms (Nivre, 2004; Zhang and Clark, 2008), are widely used for dependency analysis because of the efficiency and comparatively good performance. However, these parsers have one major problem that they can handle only local information. Isozaki et al. (2004) pointed out that the drawbacks of shift-reduce parser could be resolved by incorporating top-down information such as root finding. This work presents an O(n2 ) top-down headdriven transition-based parsing algorithm which can parse complex structures that are not trivial for shiftreduce parsers. The deductive system is very similar to Earley parsing (Earley"
P12-1069,J08-4003,0,0.154551,"”. We follow a convention and write the stack with its topmost element to the right, and the queue with its first element to the left. In this example, we set the window size d to 1, and write the descendants of trees on stack elements s0 and s1 within depth 1. parser constructs left and right children of a head node in a left-to-right direction by scanning the head node prior to its right children. Figure 2 shows an example for parsing a sentence “I saw a girl”. 4 Correctness To prove the correctness of the system in Figure 1 for the projective dependency graph, we use the proof strategy of (Nivre, 2008a). The correct deductive system is both sound and complete. Theorem 4.1 The deductive system in Figure 1 is correct for the class of dependency forest. Proof 4.1 To show soundness, we show that Gp0 = (VW , ∅), which is a directed graph defined by the axiom, is well-formed and projective, and that every transition preserves this property. • ROOT: The node 0 is a root in Gp0 , and the node 0 is on the top of stack of p0 . The two pred actions put a word onto the top of stack, and predict an arc from root or its descendant to the child. The comp actions add the predicted arcs which include no ar"
P12-1069,J95-2002,0,0.086641,"e cost of a top tree s0 in stack S. We define these costs using a combination of stack-based model and weighted prediction model. The forward and inside costs of the combination model are as follows: { fw fw c = cfw s + cp (5) in in in c = cs + cp in where cfw s and cs are a forward cost and an inside in cost for stack-based model, and cfw p and cp are a forward cost and an inside cost for weighted prediction model. We add the following tuple of costs to a state: in fw in (cfw s , cs , cp , cp ). For each action, we define how to efficiently calculate the forward and inside costs3 , following Stolcke (1995) and Huang and Sagae (2010)’s works. In either case of predx or predy , fw (cfw s , , cp , ) fw (cfw s + λ, 0, cp + cp (s0 .h, nk ), 0) where λ= m−1 ∑ θs · fs,predx (i, h, j, S) if predx θs · fs,predy (i, h, j, S) if predy (6) In the case of scan, cp (h, ly , ly+1 ) in fw in (cfw s , cs , cp , cp ) in fw in (cfw s + ξ, cs + ξ, cp , cp ) y=1 +cp (h, −, r1 ) + { cp (h, ry , ry+1 ). where y=1 This is different from McDonald and Pereira (2006) in that the cost factors for left children are calculated from left to right, while those in McDonald and Pereira (2006)’s definition are calculated from ri"
P12-1069,W03-3023,1,0.78688,"wn. Eisner and Satta (1999) showed that there is a cubic-time parsing algorithm on the formalism of the head automaton grammars, which are equivalently converted into split-head bilexical context-free grammars (SBCFGs) (McAllester, 1999; Johnson, 2007). Although our proposed algorithm does not employ the formalism of SBCFGs, it creates left children before right children, implying that it does not have spurious ambiguities as well as parsing algorithms on the SBCFGs. Head-corner parsing algorithm (Kay, 1989) creates dependency tree top-down, and in this our algorithm has similar spirit to it. Yamada and Matsumoto (2003) applied a shiftreduce algorithm to dependency analysis, which is known as arc-standard transition-based algorithm (Nivre, 2004). Nivre (2003) proposed another transition-based algorithm, known as arc-eager algorithm. The arc-eager algorithm processes rightdependent top-down, but this does not involve the prediction of lower nodes from higher nodes. Therefore, the arc-eager algorithm is a totally bottom-up algorithm. Zhang and Clark (2008) proposed a combination approach of the transition-based algorithm with graph-based algorithm (McDonald and Pereira, 2006), which is the same as our combinat"
P12-1069,D08-1059,0,0.367636,"er presents a novel top-down headdriven parsing algorithm for data-driven projective dependency analysis. This algorithm handles global structures, such as clause and coordination, better than shift-reduce or other bottom-up algorithms. Experiments on the English Penn Treebank data and the Chinese CoNLL-06 data show that the proposed algorithm achieves comparable results with other data-driven dependency parsing algorithms. 2 Definition of Dependency Graph 1 Introduction A dependency graph is defined as follows. Transition-based parsing algorithms, such as shiftreduce algorithms (Nivre, 2004; Zhang and Clark, 2008), are widely used for dependency analysis because of the efficiency and comparatively good performance. However, these parsers have one major problem that they can handle only local information. Isozaki et al. (2004) pointed out that the drawbacks of shift-reduce parser could be resolved by incorporating top-down information such as root finding. This work presents an O(n2 ) top-down headdriven transition-based parsing algorithm which can parse complex structures that are not trivial for shiftreduce parsers. The deductive system is very similar to Earley parsing (Earley, 1970). The Earley pred"
P12-1069,P11-2033,0,0.155303,"Missing"
P12-2039,P06-1032,0,0.461086,"*go to KAIYUKAN 1 with my friends. Introduction Because of the growing number of learners of English, there is an increasing demand to help learners of English. It is highly effective for learners to receive feedback on their essays from a human tutor (Nagata and Nakatani, 2010). However, manual feedback needs a lot of work and time, and it also requires much grammatical knowledge. Thus, a variety of automatic methods for helping English learning and education have been proposed. The mainstream of English error detection and correction has focused on article errors (Knight and Chander, 1994; Brockett et al., 2006) and preposition errors (Chodorow et al., 2007; Rozovskaya and In this example, go in the second sentence should be written as went. It is difficult to correct this type of error because there are two choices for correction, namely went and will go. In this case, we can exploit global context to determine which correction is appropriate: the first sentence describes a past event, and the second sentence refers the first sentence. Thus, the verb should be changed to past tense. This deduction is easy for humans, but is difficult for machines. One way to incorporate such global context into tens"
P12-2039,W07-1604,0,0.0262177,"n Because of the growing number of learners of English, there is an increasing demand to help learners of English. It is highly effective for learners to receive feedback on their essays from a human tutor (Nagata and Nakatani, 2010). However, manual feedback needs a lot of work and time, and it also requires much grammatical knowledge. Thus, a variety of automatic methods for helping English learning and education have been proposed. The mainstream of English error detection and correction has focused on article errors (Knight and Chander, 1994; Brockett et al., 2006) and preposition errors (Chodorow et al., 2007; Rozovskaya and In this example, go in the second sentence should be written as went. It is difficult to correct this type of error because there are two choices for correction, namely went and will go. In this case, we can exploit global context to determine which correction is appropriate: the first sentence describes a past event, and the second sentence refers the first sentence. Thus, the verb should be changed to past tense. This deduction is easy for humans, but is difficult for machines. One way to incorporate such global context into tense/aspect error correction is to use a machine"
P12-2039,P08-1021,0,0.0537025,"Missing"
P12-2039,I11-1017,1,0.695233,"(Lafferty, 2001), which provides state-of-the-art performance in sequence labeling while allowing flexible feature design for combining local and global feature sets. 3.1 Local Features Table 1 shows the local features used to train the error correction model. 2 Konan-JIEM Learner Corpus Second Edition (http:// gsk.or.jp/catalog/GSK2011-B/catalog.html) contains 170 essays, and Cambridge English First Certificate in English (http://www.cambridgeesol.org/exams/ fce/index.html) contains 1244 essays. 3 http://lang-8.com/ 4 As of January, 2012. More details about the Lang-8 corpus can be found in (Mizumoto et al., 2011). 5 Note that not all the 750,000 verb phrases were corrected due to the misuse of tense/aspect. 199 name t-learn bare L R nsubj dobj aux pobj p-tmod norm-p-tmod advmod conj main-clause sub-clause description tense/aspect written by the learner (surface tense/aspect) the verb lemma the word to the left the word to the right nominal subject direct object auxiliary verb object of a preposition temporal adverb normalized temporal adverb other adverb subordinating conjunction true if the target VP is in main clause true if the target VP is in subordinate clause We use dependency relations such as"
P12-2039,C10-2103,0,0.0218078,"n order to take global information into account, we regard the task as sequence labeling: each verb phrase in a document is labeled with tense/aspect depending on surrounding labels. Our experiments show that the global context makes a moderate contribution to tense/aspect error correction. 1 (1) I had a good time this Summer Vacation. First, I *go to KAIYUKAN 1 with my friends. Introduction Because of the growing number of learners of English, there is an increasing demand to help learners of English. It is highly effective for learners to receive feedback on their essays from a human tutor (Nagata and Nakatani, 2010). However, manual feedback needs a lot of work and time, and it also requires much grammatical knowledge. Thus, a variety of automatic methods for helping English learning and education have been proposed. The mainstream of English error detection and correction has focused on article errors (Knight and Chander, 1994; Brockett et al., 2006) and preposition errors (Chodorow et al., 2007; Rozovskaya and In this example, go in the second sentence should be written as went. It is difficult to correct this type of error because there are two choices for correction, namely went and will go. In this"
P12-2039,P11-1093,0,0.165612,"Missing"
P13-2124,P05-1074,0,0.0745339,"Missing"
P13-2124,C12-1038,0,0.0502083,"Missing"
P13-2124,P07-1033,0,0.164947,"Missing"
P13-2124,D10-1104,0,0.0295498,"ith a variety of resources used for creating candidate sets. • WordNet Candidates are retrieved from the synsets and verbs sharing the same hypernyms in the WordNet 3.0. • LearnerSmall Candidates are retrieved from following learner corpora: NUS corpus of learner English (NUCLE), Konan-JIEM (KJ), and NICT Japanese learner English (JLE) corpus. • Roundtrip Candidates are collected by performing “round-trip” translation, which is similar to (Bannard and Callison-Burch, 2005) 10 . • WordNet+Roundtrip A combination of the thesaurus-based and the translation table-based candidate sets, similar to (Liu et al., 2010) and (Liu et al., 2011). • Lang-8 The proposed candidate sets obtained from a large scale learner corpus. • Lang-8+DA Lang-8 candidate sets with domain adaptaExperiments Performance of verb suggestion is evaluated on two error-tagged learner corpora: CLC-FCE and KJ. In the experiments, we assume that the target verb and its context for suggestion are already given. For the experiment on the CLC-FCE dataset, the targets are all words tagged with “RV” (re10 Our roundtrip translation lexicons are built using a subset of the WIT3 corpus (Cettolo et al., 2012), which is available 9 https://github.c"
P13-2124,I11-1017,1,0.872798,"th the CLC-FCE and the KJ corpora used for testing. The proposed verb suggestion system follows the standard approach in related tasks (Rozovskaya and Roth, 2011; Wu et al., 2010), where the candidate selection is formalized as a multi-class classification problem with predefined candidate sets. 2.1 Suggestion Model Candidate Sets For reflecting tendency of learner errors to the candidate sets, we use a large scale corpus obtained from learners’ writing on an SNS (Social Networking Service), Lang-83 . An advantage of using the learner corpus from such website is the size of annotated portion (Mizumoto et al., 2011). This SNS has over 1 million manually annotated English sentences written by ESL learners. We have collected the learner writings on the site, and released the dataset for research purpose4 . First, we performed POS tagging for the dataset using the treebank POS tagger in the NLTK toolkit 2.10. Second, we extracted the correction pairs which have “VB*” tag. The set of correction pairs given an incorrect verb is considered as a candidate set for the verb. We then performed the following preprocessing for the dataset because we focus on lexical selection of verbs: • Lemmatize verbs to reduce da"
P13-2124,P11-1093,0,0.0215626,"nk task, the candidate sets and domain adaptation can be applied to this task to take the original word into account. The model is trained on a huge native corpus, namely the ukWaC corpus, because the data-size of learner corpora is limited compared to native corpora. It is then adapted to the target domain, i.e., learner writing. In our experiment, the Lang8 corpus is used as the target domain corpus, since we assume that it shares the same characteristics with the CLC-FCE and the KJ corpora used for testing. The proposed verb suggestion system follows the standard approach in related tasks (Rozovskaya and Roth, 2011; Wu et al., 2010), where the candidate selection is formalized as a multi-class classification problem with predefined candidate sets. 2.1 Suggestion Model Candidate Sets For reflecting tendency of learner errors to the candidate sets, we use a large scale corpus obtained from learners’ writing on an SNS (Social Networking Service), Lang-83 . An advantage of using the learner corpus from such website is the size of annotated portion (Mizumoto et al., 2011). This SNS has over 1 million manually annotated English sentences written by ESL learners. We have collected the learner writings on the s"
P13-2124,C10-1133,0,0.0640981,"Missing"
P13-2124,P10-2021,0,0.117977,"and domain adaptation can be applied to this task to take the original word into account. The model is trained on a huge native corpus, namely the ukWaC corpus, because the data-size of learner corpora is limited compared to native corpora. It is then adapted to the target domain, i.e., learner writing. In our experiment, the Lang8 corpus is used as the target domain corpus, since we assume that it shares the same characteristics with the CLC-FCE and the KJ corpora used for testing. The proposed verb suggestion system follows the standard approach in related tasks (Rozovskaya and Roth, 2011; Wu et al., 2010), where the candidate selection is formalized as a multi-class classification problem with predefined candidate sets. 2.1 Suggestion Model Candidate Sets For reflecting tendency of learner errors to the candidate sets, we use a large scale corpus obtained from learners’ writing on an SNS (Social Networking Service), Lang-83 . An advantage of using the learner corpus from such website is the size of annotated portion (Mizumoto et al., 2011). This SNS has over 1 million manually annotated English sentences written by ESL learners. We have collected the learner writings on the site, and released"
P13-2124,J92-4003,0,\N,Missing
P13-2124,P12-2076,0,\N,Missing
P13-2124,2012.eamt-1.60,0,\N,Missing
P13-3008,W09-2107,0,0.182522,"rain the size of the confusion set, lit. to put in tea lit. to see a dream 52 Proceedings of the ACL Student Research Workshop, pages 52–58, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics similarity measures are used. To rank the best candidates, the strength of association in the learner’s construction and in each of the generated alternative construction are measured. For example, Futagi et al. (2008) generated synonyms for each candidate string using WordNet and Roget’s Thesaurus and used the rank ratio measure to score them by their semantic similarity. Liu et al. (2009) also used WordNet to generate synonyms, but used Pointwise Mutual Information as association measure to rank the candidates. Chang et al. (2008) used bilingual dictionaries to derive collocation candidates and used the loglikelihood measure to rank them. One drawback of these approaches is that they rely on resources of limited coverage, such as dictionaries, thesaurus or manually constructed databases to generate the candidates. Other studies have tried to offer better coverage by automatically deriving paraphrases from parallel corpora (Dahlmeier and Ng, 2011), but similar to Chang et al. ("
P13-3008,I11-1017,1,0.841303,"m is that most research does not actually take the learners&apos; tendency of collocation errors into account; instead, their systems are trained only on well-formed text corpora. Our work follows the general approach, that is, uses similarity measures for generating the confusion set and association measures for ranking the best candidates. However, instead of using only wellformed text for generating the confusion set, we use a large learner corpus created by crawling the revision log of a language learning social networking service (SNS), Lang-83. Another work that also uses data from Lang-8 is Mizumoto et al. (2011), which uses Lang-8 in creating a largescale Japanese learner’s corpus. The biggest benefit of using such kind of data is that we can obtain in large scale pairs of learners’ sentences and their corrections assigned by native speakers. 3 three steps: 1) for each extracted tuple in the second learner’s composition, we created a set of candidates by substituting words generated using word similarity algorithms; 2) then, we measured the strength of association in the writer’s phrase and in each generated candidate phrase using association measures to compute collocation scores; 3) the highest ran"
P13-3008,D11-1010,0,0.174758,"e them by their semantic similarity. Liu et al. (2009) also used WordNet to generate synonyms, but used Pointwise Mutual Information as association measure to rank the candidates. Chang et al. (2008) used bilingual dictionaries to derive collocation candidates and used the loglikelihood measure to rank them. One drawback of these approaches is that they rely on resources of limited coverage, such as dictionaries, thesaurus or manually constructed databases to generate the candidates. Other studies have tried to offer better coverage by automatically deriving paraphrases from parallel corpora (Dahlmeier and Ng, 2011), but similar to Chang et al. (2008), it is essential to identify the learner’s first language and to have bilingual dictionaries and parallel corpora for every first language (L1) in order to extend the resulting system. Another problem is that most research does not actually take the learners&apos; tendency of collocation errors into account; instead, their systems are trained only on well-formed text corpora. Our work follows the general approach, that is, uses similarity measures for generating the confusion set and association measures for ranking the best candidates. However, instead of using"
P13-3008,J93-1003,0,0.362212,"Missing"
P13-3008,D10-1094,0,0.0212236,"xt, our approach makes use of a large Japanese language learner corpus for generating collocation candidates, in order to build a system that is more sensitive to constructions that are difficult for learners. Our results show that we get better results compared to other methods that use only wellformed text. 1 Introduction Automated grammatical error correction is emerging as an interesting topic of natural language processing (NLP). However, previous research in second language learning are focused on restricted types of learners’ errors, such as article and preposition errors (Gamon, 2010; Rozovskaya and Roth, 2010; Tetreault et al., 2010). For example, research for Japanese language mainly focuses on Japanese case particles (Suzuki and Toutanova, 2006; Oyama and Matsumoto, 2010). It is only recently that NLP research has addressed issues of collocation errors. Collocations are conventional word combinations in a language. In Japanese, ocha wo ireru “お茶を入れる 1 [to make tea]” and yume wo miru ” 夢を見る2 [to have a dream]” are examples of collocations. Even though their accurate use is crucial to make communication precise and to sound like a native speaker, learning them 1 2 2 Related Work Collocation correc"
P13-3008,N10-1019,0,0.0198933,"ell-formed text, our approach makes use of a large Japanese language learner corpus for generating collocation candidates, in order to build a system that is more sensitive to constructions that are difficult for learners. Our results show that we get better results compared to other methods that use only wellformed text. 1 Introduction Automated grammatical error correction is emerging as an interesting topic of natural language processing (NLP). However, previous research in second language learning are focused on restricted types of learners’ errors, such as article and preposition errors (Gamon, 2010; Rozovskaya and Roth, 2010; Tetreault et al., 2010). For example, research for Japanese language mainly focuses on Japanese case particles (Suzuki and Toutanova, 2006; Oyama and Matsumoto, 2010). It is only recently that NLP research has addressed issues of collocation errors. Collocations are conventional word combinations in a language. In Japanese, ocha wo ireru “お茶を入れる 1 [to make tea]” and yume wo miru ” 夢を見る2 [to have a dream]” are examples of collocations. Even though their accurate use is crucial to make communication precise and to sound like a native speaker, learning them 1 2 2 Rela"
P13-3008,J96-1001,0,0.36235,"Missing"
P13-3008,P06-1132,0,0.0260684,"at is more sensitive to constructions that are difficult for learners. Our results show that we get better results compared to other methods that use only wellformed text. 1 Introduction Automated grammatical error correction is emerging as an interesting topic of natural language processing (NLP). However, previous research in second language learning are focused on restricted types of learners’ errors, such as article and preposition errors (Gamon, 2010; Rozovskaya and Roth, 2010; Tetreault et al., 2010). For example, research for Japanese language mainly focuses on Japanese case particles (Suzuki and Toutanova, 2006; Oyama and Matsumoto, 2010). It is only recently that NLP research has addressed issues of collocation errors. Collocations are conventional word combinations in a language. In Japanese, ocha wo ireru “お茶を入れる 1 [to make tea]” and yume wo miru ” 夢を見る2 [to have a dream]” are examples of collocations. Even though their accurate use is crucial to make communication precise and to sound like a native speaker, learning them 1 2 2 Related Work Collocation correction currently follows a similar approach used in article and preposition correction. The general strategy compares the learner&apos;s word choic"
P13-3008,I08-7018,0,0.540774,"ctors that are based on syntactic dependencies. We are interested in computing similarity of nouns and verbs and hence the context of a particular noun is a vector of verbs that are in an object relation with that noun. The context of a particular verb is a vector Table 3 Context of a particular noun represented as a co-occurrence vector of nouns that are in an object relation with that verb. Table 2 and Table 3 show examples of part of co-occurrence vectors for the noun “日記 [diary]” and the verb “ 食べる [eat]”, respectively. The numbers indicate the co-occurrence frequency in the BCCWJ corpus (Maekawa, 2008). We computed the similarity between co-occurrence vectors using different metrics: Cosine Similarity, Dice coefficient (Curran, 2004), KullbackLeibler divergence or KL divergence or relative entropy (Kullback and Leibler, 1951) and the Jenson-Shannon divergence (Lee, 1999). Confusion Set derived from learner corpus: In order to build a module that can “guess” common construction errors, we created a confusion set using Lang-8 corpus. Instead of generating words that have similar meaning to the learner’s written construction, we extracted all the possible noun and verb corrections for each of"
P13-3008,P10-2021,0,0.0184211,"n Set example for the words suru (する) and biru (ビル) 日記を diary 書く 読む つける ご飯を ラーメンを カレーを write read put on rice ramen noodle soup curry 15 11 8 164 53 39 食べる eat Table 2 Context of a particular noun represented as a co-occurrence vector rity: 1) thesaurus-based word similarity, 2) distributional similarity and 3) confusion set derived from learner corpus. The first two measures generate the collocation candidates by finding words that are analogous to the writer’s choice, a common approach used in the related work on collocation error correction (Liu et al., 2009; Östling and O. Knutsson, 2009; Wu et al., 2010) and the third measure generates the candidates based on the corrections given by native speakers in the learner corpus. Thesaurus-based word similarity: The intuition of this measure is to check if the given words have similar glosses (definitions). Two words are considered similar if they are near each other in the thesaurus hierarchy (have a path within a pre-defined threshold length). Distributional Similarity: Thesaurus-based methods produce weak recall since many words, phrases and semantic connections are not covered by hand-built thesauri, especially for verbs and adjectives. As an alt"
P13-3008,P99-1004,0,0.0433323,"xt of a particular noun represented as a co-occurrence vector of nouns that are in an object relation with that verb. Table 2 and Table 3 show examples of part of co-occurrence vectors for the noun “日記 [diary]” and the verb “ 食べる [eat]”, respectively. The numbers indicate the co-occurrence frequency in the BCCWJ corpus (Maekawa, 2008). We computed the similarity between co-occurrence vectors using different metrics: Cosine Similarity, Dice coefficient (Curran, 2004), KullbackLeibler divergence or KL divergence or relative entropy (Kullback and Leibler, 1951) and the Jenson-Shannon divergence (Lee, 1999). Confusion Set derived from learner corpus: In order to build a module that can “guess” common construction errors, we created a confusion set using Lang-8 corpus. Instead of generating words that have similar meaning to the learner’s written construction, we extracted all the possible noun and verb corrections for each of the nouns and verbs found in the data. Table 1 shows some examples extracted. For instance, the confusion set of the verb suru “する [to do]” is composed of verbs such as ukeru “受ける [to accept]”, which does not necessarily have similar meaning with suru. The confusion set mea"
P13-3008,J90-1003,0,\N,Missing
P13-3008,P10-2065,0,\N,Missing
P15-1093,W02-1001,0,0.0740395,"Missing"
P15-1093,P10-1160,0,0.100864,"Missing"
P15-1093,I11-1023,1,0.841898,"Missing"
P15-1093,P08-1067,0,0.0786277,"Missing"
P15-1093,W07-1522,1,0.769102,"Missing"
P15-1093,P09-2022,0,0.517951,"Missing"
P15-1093,P13-1116,0,0.0315645,"Missing"
P15-1093,E06-1011,0,0.0563756,"Missing"
P15-1093,I11-1085,0,0.409695,"Missing"
P15-1093,D08-1055,0,0.319631,"Missing"
P15-1093,D14-1041,0,0.0607383,"Missing"
P15-1093,D14-1109,0,0.0485623,"Missing"
P15-2043,I05-3017,0,0.276786,"Missing"
P15-2043,D12-1132,0,0.199707,"Missing"
P15-2043,W03-1719,0,0.0991892,"Missing"
P15-2043,D11-1090,0,0.358815,"Missing"
P15-2043,N09-1007,0,0.392543,"Missing"
P15-2043,P12-1027,0,0.0930338,"is.naist.jp Abstract words (IVs). By running a synthetic word parser on each of the words in a CWS training set, we can generate a fine-grained segmentation standard that contains more IVs. Since the current conditional random field (CRF) word segmenters (Tseng et al., 2005; Sun and Xu, 2011) perform well on IVs, this transforming process can conceivably improve the handling of pseudo-OOV words, as long as we can recover the original word segmentation standard from the fine-grained sub-word segmentation. In recent years, some related works about improving OOV problem in CWS have been ongoing. Sun et al. (2012) presented a joint model for Chinese word segmentation and OOVs detection. Their models achieved fast training speed, high accuracies and increase on OOV recall. Sun (2011) proposed a similar sub-word structure which is generated by merging the segmentations provided by different segmenters (a word-based segmenter, a character-based segmenter and a local character classifier). However, her models does not predict the sub-words of all the synthetic words, but those words with different segmented results of the three segmenters. Her work maximizes the agreement of different models to improve CWS"
P15-2043,I05-3027,0,0.0428039,"a graph-based parser (McDonald, 2006) on the data released by Cheng et al. (2014) and include the dictionary (NAIST Chinese Dictionary1 ) features and Brown clustering features extracted from a large unlabeled corpus (Chinese Gigaword Second Edition2 ) as described in Cheng et al. (2014). For native Chinese speakers, single character and two character words are usually treated as the 2.2 CRF-based Word Segmenter Xue et al. (2003) proposed a method which treated Chinese word segmentation as a character-based sequential labeling problem and exploited several discriminative learning algorithms. Tseng et al. (2005) adopted the CRFs as the learning method and obtained the best results in the second international Chinese word segmentation bakeoff2005. Moreover, Sun and Xu (2011) attempted to extract information from large unlabeled data to enhance the Chinese word segmentation results. In this work, we train a traditional CRF-based supervised model on the fine-grained training data, include the dictionary (NAIST Chinese Dictionary) features and access variety features extracted from a large unlabeled corpus (Chinese Gigaword Second Edition) as described in Sun and Xu (2011). 1 http://cl.naist.jp/index.php"
P15-2043,O03-4002,0,0.361957,"Missing"
P15-2043,P07-1106,0,0.241264,"Missing"
P15-2043,D13-1031,0,0.600326,"Missing"
P15-2043,J09-4006,0,\N,Missing
P15-2043,P11-1139,0,\N,Missing
P15-2043,W03-1726,0,\N,Missing
P15-2140,P13-2131,0,0.0581399,"Missing"
P15-2140,P14-1134,0,0.297952,"of two nouns (e.g., assigning a relation 1 851 http://amr.isi.edu/ Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 851–856, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Train 3504 Dev 463 Test 398 person! &quot;! retire-01! plant! Table 1: Statistics of the extracted NP data work-01! a two-step approach: first identifying distinct concepts (nodes) in the AMR graph, then defining the dependency relations between those concepts (Flanigan et al., 2014). In the concept identification step, unlike POS tagging, one word is sometimes assigned with more than one concept, and the number of possible concepts is far more than the number of possible parts-of-speech. As the concept identification accuracy remains low, such a pipeline method suffers from error propagation, thus resulting in a suboptimal AMR parsing performance. To solve this problem, we extend a transitionbased dependency parsing algorithm, and propose a novel algorithm which jointly identifies the concepts and the relations in AMR trees. Compared to the baseline, our method improves"
P15-2140,N03-1013,0,0.0103458,"T((run-01)) and S HIFT((sleep-01)) are different actions, they share the features “S”, “S”◦“DICTPRED ” because they share the generation rule DICTPRED . Action a S HIFT(c) φaction ((σ, [wi |β], R), a) {“S”, “S” ◦ rule(wi , c), “S” ◦ rule(wi , c) ◦ c} L EFT-R EDUCE(r, n) {“L-R”, “L-R” ◦ r, “L-R” ◦ r ◦ n} R IGHT-R EDUCE(r, n) {“R-R”, “R-R”◦r, “R-R”◦r ◦n} E MPTY-R EDUCE {“E-R”} Table 5: Feature sets for the action word. Finally, we add the rules DICTPRED and DICTNOUN . These two rules need conversion from nouns and adjectives to their verb and noun forms, For this conversion, we use CatVar v2.1 (Habash and Dorr, 2003), which lists categorial variations of words (such as verb run for noun runner). We also use definitions of the predicates from PropBank (Palmer et al., 2005), which AMR tries to reuse as much as possible, and impose constraints that the defined predicates can only have semantic relations consistent with the definition. During the training, we use the max-violation perceptron (Huang et al., 2012) with beam size 8 and average the parameters. During the testing, we also perform beam search with beam size 8. Table 6 shows the overall performance on NP semantic structure analysis. We evaluate the"
P15-2140,I11-1136,0,0.0124877,") retire-01 ARG0-of ARG2 work-01 person 7 L EFT-R EDUCE(ARG0-of, nroot ) ARG0-of work-01 ARG2 Figure 3: Derivation of an AMR tree for a retired plant worker (σ0 and σ1 denote the top and the second top of the stack, respectively.) Action S HIFT(c(wi )) L EFT-R EDUCE(r, n) R IGHT-R EDUCE(r, n) E MPTY-R EDUCE Current state (σ, [wi |β], R) ([σ|ci |cj ], β, R) ([σ|ci |cj ], β, R) ([σ|φ], β, R) Next state ([σ|c(wi )], β, R) r ([σ|cj ], β, R ∪ {nroot (ci ) ← − n(cj )}) r ([σ|ci ], β, R ∪ {n(ci ) − → nroot (cj )}) (σ, β, R) Table 2: Definitions of the actions relations. Our algorithm is similar to (Hatori et al., 2011), in which they perform POS tagging and dependency parsing jointly by assigning a POS tag to a word when performing S HIFT, but differs in that, unlike POS tagging, one word is sometimes assigned with more than one concept. In our algorithm, the input words are stored in the buffer and the identified concepts are stored in the stack. S HIFT identifies a concept subtree associated with the top word in the buffer. R EDUCE identifies the dependency relation between the top two concept subtrees in the stack. Figure 3 illustrates the process of deriving an AMR tree for a retired plant worker, and F"
P15-2140,N12-1015,0,0.0367744,"Missing"
P15-2140,P03-1054,0,0.00434346,"m spanning tree (MST) algorithm used for dependency parsing (McDonald et al., 2005). They report that using gold concepts yields much better performance, implying that joint identification of concepts and relations can be helpful. Abstract Meaning Representation 2.1 Extraction of NPs We extract substructures (subtrees) corresponding to NPs from the AMR Bank (LDC2014T12). In the AMR Bank, there is no alignment between the words and the concepts (nodes) in the AMR graphs. We obtain this alignment by using the rule-based alignment tool by Flanigan et al. (2014). Then, we use the Stanford Parser (Klein and Manning, 2003) to obtain constituency trees, and extract NPs that contain more than one noun and are not included by another NP. We exclude NPs that contain named entities, because they would require various kinds of manually crafted rules for each type of named entity. We also exclude NPs that contain possessive pronouns or conjunctions, which prove problematic for the alignment tool. Table 1 shows the statistics of the extracted NP data. 3 Proposed Method In this paper, we propose a novel approach for mapping the word sequence in an NP to an AMR tree, where the concepts (nodes) corresponding to the words"
P15-2140,J02-3004,0,0.0634192,"e, given a sequence of words in an NP. The previous method for AMR parsing takes a Introduction Semantic structure analysis of noun phrases (NPs) is an important research topic, which is beneficial for various NLP tasks, such as machine translation and question answering (Nakov and Hearst, 2013; Nakov, 2013). Among the previous works on NP analysis are internal NP structure analysis (Vadas and Curran, 2007; Vadas and Curran, 2008), noun-noun relation analysis of noun compounds (Girju et al., 2005; Tratz and Hovy, 2010; Kim and Baldwin, 2013), and predicate-argument analysis of noun compounds (Lapata, 2002). The goal of internal NP structure analysis is to assign bracket information inside an NP (e.g., (lung cancer) deaths indicates that the phrase lung cancer modifies the head deaths). In noun-noun relation analysis, the goal is to assign one of the predefined semantic relations to a noun compound consisting of two nouns (e.g., assigning a relation 1 851 http://amr.isi.edu/ Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 851–856, c Beijing, China, July 26-31, 2"
P15-2140,H05-1066,0,0.16785,"Missing"
P15-2140,W04-0308,0,0.026101,"P. We exclude NPs that contain named entities, because they would require various kinds of manually crafted rules for each type of named entity. We also exclude NPs that contain possessive pronouns or conjunctions, which prove problematic for the alignment tool. Table 1 shows the statistics of the extracted NP data. 3 Proposed Method In this paper, we propose a novel approach for mapping the word sequence in an NP to an AMR tree, where the concepts (nodes) corresponding to the words and the dependency relations between those concepts must be identified. We extend the arc-standard algorithm by Nivre (2004) for AMR parsing, and propose a transition-based algorithm which jointly identifies concepts and dependency ARG0-of! retire-01! plant! ARG2! 2.2 Previous Method for AMR Analysis a We adopt the method proposed by Flanigan et al. (2014) as our baseline, which is a two-step pipeline method of concept identification step and retired! plant! person! ARG0-of! work-01! worker! Figure 4: a retired plant worker in AMR 852 Previous action 0 (initial state) 1 S HIFT(EMPTY(a)) 2 E MPTY-R EDUCE σ1 ∅ 3 S HIFT(DICTPRED (retired)) 4 S HIFT(LEMMA(plant)) σ0 retire-01 retire-01 plant β [a retired plant worker]"
P15-2140,J05-1004,0,0.0126885,"a S HIFT(c) φaction ((σ, [wi |β], R), a) {“S”, “S” ◦ rule(wi , c), “S” ◦ rule(wi , c) ◦ c} L EFT-R EDUCE(r, n) {“L-R”, “L-R” ◦ r, “L-R” ◦ r ◦ n} R IGHT-R EDUCE(r, n) {“R-R”, “R-R”◦r, “R-R”◦r ◦n} E MPTY-R EDUCE {“E-R”} Table 5: Feature sets for the action word. Finally, we add the rules DICTPRED and DICTNOUN . These two rules need conversion from nouns and adjectives to their verb and noun forms, For this conversion, we use CatVar v2.1 (Habash and Dorr, 2003), which lists categorial variations of words (such as verb run for noun runner). We also use definitions of the predicates from PropBank (Palmer et al., 2005), which AMR tries to reuse as much as possible, and impose constraints that the defined predicates can only have semantic relations consistent with the definition. During the training, we use the max-violation perceptron (Huang et al., 2012) with beam size 8 and average the parameters. During the testing, we also perform beam search with beam size 8. Table 6 shows the overall performance on NP semantic structure analysis. We evaluate the performance using the Smatch score (Cai and Knight, 4 Experiments We conduct an experiment using our NP data set (Table 1). We use the implementation 2 of (Fl"
P15-2140,P10-1070,0,0.0245489,"ures corresponding to NPs are trees. Thus, we define our task as predicting the AMR tree structure, given a sequence of words in an NP. The previous method for AMR parsing takes a Introduction Semantic structure analysis of noun phrases (NPs) is an important research topic, which is beneficial for various NLP tasks, such as machine translation and question answering (Nakov and Hearst, 2013; Nakov, 2013). Among the previous works on NP analysis are internal NP structure analysis (Vadas and Curran, 2007; Vadas and Curran, 2008), noun-noun relation analysis of noun compounds (Girju et al., 2005; Tratz and Hovy, 2010; Kim and Baldwin, 2013), and predicate-argument analysis of noun compounds (Lapata, 2002). The goal of internal NP structure analysis is to assign bracket information inside an NP (e.g., (lung cancer) deaths indicates that the phrase lung cancer modifies the head deaths). In noun-noun relation analysis, the goal is to assign one of the predefined semantic relations to a noun compound consisting of two nouns (e.g., assigning a relation 1 851 http://amr.isi.edu/ Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on"
P15-2140,P07-1031,0,0.0378677,", since we found out that NPs mostly form trees rather than graphs in the AMR Bank, we can assume that AMR substructures corresponding to NPs are trees. Thus, we define our task as predicting the AMR tree structure, given a sequence of words in an NP. The previous method for AMR parsing takes a Introduction Semantic structure analysis of noun phrases (NPs) is an important research topic, which is beneficial for various NLP tasks, such as machine translation and question answering (Nakov and Hearst, 2013; Nakov, 2013). Among the previous works on NP analysis are internal NP structure analysis (Vadas and Curran, 2007; Vadas and Curran, 2008), noun-noun relation analysis of noun compounds (Girju et al., 2005; Tratz and Hovy, 2010; Kim and Baldwin, 2013), and predicate-argument analysis of noun compounds (Lapata, 2002). The goal of internal NP structure analysis is to assign bracket information inside an NP (e.g., (lung cancer) deaths indicates that the phrase lung cancer modifies the head deaths). In noun-noun relation analysis, the goal is to assign one of the predefined semantic relations to a noun compound consisting of two nouns (e.g., assigning a relation 1 851 http://amr.isi.edu/ Proceedings of the 5"
P15-2140,P08-1039,0,0.0235787,"t NPs mostly form trees rather than graphs in the AMR Bank, we can assume that AMR substructures corresponding to NPs are trees. Thus, we define our task as predicting the AMR tree structure, given a sequence of words in an NP. The previous method for AMR parsing takes a Introduction Semantic structure analysis of noun phrases (NPs) is an important research topic, which is beneficial for various NLP tasks, such as machine translation and question answering (Nakov and Hearst, 2013; Nakov, 2013). Among the previous works on NP analysis are internal NP structure analysis (Vadas and Curran, 2007; Vadas and Curran, 2008), noun-noun relation analysis of noun compounds (Girju et al., 2005; Tratz and Hovy, 2010; Kim and Baldwin, 2013), and predicate-argument analysis of noun compounds (Lapata, 2002). The goal of internal NP structure analysis is to assign bracket information inside an NP (e.g., (lung cancer) deaths indicates that the phrase lung cancer modifies the head deaths). In noun-noun relation analysis, the goal is to assign one of the predefined semantic relations to a noun compound consisting of two nouns (e.g., assigning a relation 1 851 http://amr.isi.edu/ Proceedings of the 53rd Annual Meeting of the"
P15-2140,2003.mtsummit-systems.9,0,\N,Missing
P16-2086,P14-5010,0,0.00758309,"onsiders if the speaker would prefer a particular DC, explicit or implicit, when expressing the intended sense. In this experiment, we integrate the output of an automatic discourse parser with the probability prediction by the pragmatic listener L1 . We employ the winning parser of the CONLL shared task (Wang and Lan, 2015). The parser is also trained on Sections 2-22 of PDTB, and thus does not overlap with our test set. The sense classification of the parser is based on a pool of lexicosyntactic features drawn from gold standard arguments, DCs and automatic parsed trees produced by CoreNLP (Manning et al., 2014). For each test sample, the parser outputs a probability estimate for each sense. We use these estimates to replace the salience measure (PL (s)) (in Eq. 8) and deduce PL0 1 (s|d, C), where C is the previous relation form. Non-Explicit .2616 .2616 .2507 .2692 .2616 .2616 .2698* .2671 .2616 .2616 .2616 .2616 PS (d|s, C)Pparser (s) PL0 1 (s|d, C) = P 1 PS1 (d|s0 , C)Pparser (s0 ) (9) s0 ∈S Table 2: Accuracy of prediction by L0 , L1 and L2 . Improvements above the baseline are bolded. * means significant at p &lt; 0.02 by McNemar Test. Table 3 compares the performance of the original parser output a"
P16-2086,P15-1158,0,0.104697,"Missing"
P16-2086,prasad-etal-2008-penn,0,0.14344,"Missing"
P16-2086,K15-2002,0,0.118809,"helps automatic discourse sense classification. A full discourse parser typically consists of a pipeline of classifiers: explicit and implicit DCs are first classified and then processed separately by 2 classifiers (Xue et al., 2015). On the contrary, the pragmatic listener of the RSA model considers if the speaker would prefer a particular DC, explicit or implicit, when expressing the intended sense. In this experiment, we integrate the output of an automatic discourse parser with the probability prediction by the pragmatic listener L1 . We employ the winning parser of the CONLL shared task (Wang and Lan, 2015). The parser is also trained on Sections 2-22 of PDTB, and thus does not overlap with our test set. The sense classification of the parser is based on a pool of lexicosyntactic features drawn from gold standard arguments, DCs and automatic parsed trees produced by CoreNLP (Manning et al., 2014). For each test sample, the parser outputs a probability estimate for each sense. We use these estimates to replace the salience measure (PL (s)) (in Eq. 8) and deduce PL0 1 (s|d, C), where C is the previous relation form. Non-Explicit .2616 .2616 .2507 .2692 .2616 .2616 .2698* .2671 .2616 .2616 .2616 .2"
P16-2086,K15-2001,0,\N,Missing
P17-1026,J07-4004,0,0.740074,"th β = 0.00001, and utilize a tag dictionary, which maps frequent words to the possible 7 We use the same tag dictionary provided with their biLSTM model. 8 http://nlp.stanford.edu/projects/ glove/ 9 https://github.com/mynlp/jigg 10 http://www.cl.ecei.tohoku.ac.jp/ ˜m-suzuki/jawiki_vector/ 6 We annotate POS tags on this data using Stanford POS tagger (Toutanova et al., 2003). 282 Method CCGbank L EWIS RULE w/o dep L EWIS RULE H EAD F IRST w/o dep H EAD F IRST Tri-training L EWIS RULE H EAD F IRST Labeled Unlabeled 85.8 86.0 85.6 86.6 91.7 92.5 91.6 92.8 86.9 87.6 93.0 93.3 Method CCGbank C&C (Clark and Curran, 2007) w/ LSTMs (Vaswani et al., 2016) EasySRL (Lewis et al., 2016) EasySRL reimpl H EAD F IRST w/o NF (Ours) Tri-training EasySRL (Lewis et al., 2016) neuralccg (Lee et al., 2016) H EAD F IRST w/o NF (Ours) Table 1: Parsing results (F1) on English development set. “w/o dep” means that the model discards dependency components at prediction. Method CCGbank L EWIS RULE w/o dep L EWIS RULE H EAD F IRST w/o dep H EAD F IRST Tri-training L EWIS RULE H EAD F IRST Unlabeled 85.5 88.3 87.2 86.8 87.7 91.7 92.3 93.4 88.0 88.7 88.8 92.9 93.7 94.0 Table 3: Parsing results (F1) on English test set (Section 23)."
P17-1026,P15-1033,0,0.0516457,"Missing"
P17-1026,C10-1053,0,0.176984,"Missing"
P17-1026,D14-1107,0,0.141357,"> &lt; &lt; NP (b) a house NP Paris NP Paris NP in (NPNP)/NP NPNP NP NPNP NP France NP > &lt; > &lt; Figure 1: CCG trees that are equally likely under Eq. 1. Our model resolves this ambiguity by modeling the head of every word (dependencies). Introduction Supertagging in lexicalized grammar parsing is known as almost parsing (Bangalore and Joshi, 1999), in that each supertag is syntactically informative and most ambiguities are resolved once a correct supertag is assigned to every word. Recently this property is effectively exploited in A* Combinatory Categorial Grammar (CCG; Steedman (2000)) parsing (Lewis and Steedman, 2014; Lewis et al., 2016), in which the probability of a CCG tree y on a sentence x of length N is the product of the probabilities of supertags (categories) ci (locally factored model): ∏ P (y|x) = Ptag (ci |x). (1) parses are derived from the same supertags. Lewis et al.’s approach to this problem is resorting to some deterministic rule. For example, Lewis et al. (2016) employ the attach low heuristics, which is motivated by the right-branching tendency of English, and always prioritizes (b) for this type of ambiguity. Though for English it empirically works well, an obvious limitation is that i"
P17-1026,J07-3004,0,0.189471,"eigo ACC wo speak hanasi want tai . . NP NPNP NP NPNP (S NP)NP S S S S NP S NP &lt;B2 (S NP)NP , it was n’t Black (d) H EAD F IRST I Boku S n’t rp S &lt; was > S &lt; it rp S NP , &lt; &lt; Boku &lt; S &lt; wa eigo wo hanasi tai . (e) H EAD F INAL (b) Japanese sentence “I want to speak English.” Figure 4: Examples of applying conversion rules in Section 4 to English and Japanese sentences. One issue when applying this method for obtaining the training data is that due to the mismatch between the rule set of our CCG parser, for which we follow Lewis and Steedman (2014), and the grammar in English CCGbank (Hockenmaier and Steedman, 2007) we cannot extract dependencies from some of annotated CCG trees.4 For this reason, we instead obtain the training data for this method from the original dependency annotations on CCGbank. Fortunately the dependency annotations of CCGbank matches L EWIS RULE above in most cases and thus they can be a good approximation to it. Though this conversion may look odd at first sight it also has some advantages over L EWIS RULE. First, since the model with L EWIS RULE is trained on the CCGbank dependencies, at inference, occasionally the two components Pdep and Ptag cause some conflicts on their predi"
P17-1026,N03-1016,0,0.0967973,"Missing"
P17-1026,W02-2016,1,0.808638,"oes not suffer from such conflicts. Second, by fixing the direction of arcs, the prediction of heads becomes easier, meaning that the dependency predictions become more reliable. Later we show that this is in fact the case for existing dependency parsers (see Section 5), and in practice, we find that this simple conversion rule leads to the higher parsing scores than L EWIS RULE on English (Section 6). H EAD F INAL Among SOV languages, Japanese is known as a strictly head final language, meaning that the head of every word always follows it. Japanese dependency parsing (Uchimoto et al., 1999; Kudo and Matsumoto, 2002) has exploited this property explicitly by only allowing left-toright dependency arcs. Inspired by this tradition, we try a simple H EAD F INAL rule in Japanese CCG parsing, in which we always select the right argument as the head. For example we obtain the head final dependency tree in Figure 4e from the Japanese CCG tree in Figure 4b. 5 Tri-training We extend the existing tri-training method to our models and apply it to our English parsers. Tri-training is one of the semi-supervised methods, in which the outputs of two parsers on unlabeled data are intersected to create (silver) new trainin"
P17-1026,D16-1262,0,0.131273,"Missing"
P17-1026,P14-1130,0,0.0887189,"Missing"
P17-1026,N03-1033,0,0.230732,"Missing"
P17-1026,P10-1040,0,0.0237718,"Missing"
P17-1026,E99-1026,0,0.215812,"F IRST, in contract, does not suffer from such conflicts. Second, by fixing the direction of arcs, the prediction of heads becomes easier, meaning that the dependency predictions become more reliable. Later we show that this is in fact the case for existing dependency parsers (see Section 5), and in practice, we find that this simple conversion rule leads to the higher parsing scores than L EWIS RULE on English (Section 6). H EAD F INAL Among SOV languages, Japanese is known as a strictly head final language, meaning that the head of every word always follows it. Japanese dependency parsing (Uchimoto et al., 1999; Kudo and Matsumoto, 2002) has exploited this property explicitly by only allowing left-toright dependency arcs. Inspired by this tradition, we try a simple H EAD F INAL rule in Japanese CCG parsing, in which we always select the right argument as the head. For example we obtain the head final dependency tree in Figure 4e from the Japanese CCG tree in Figure 4b. 5 Tri-training We extend the existing tri-training method to our models and apply it to our English parsers. Tri-training is one of the semi-supervised methods, in which the outputs of two parsers on unlabeled data are intersected to"
P17-1026,P13-1103,0,0.0165033,"pped to “UNK”. Other model configurations are: 4-layer biLSTMs with left and right 300-dimensional LSTMs, 1-layer 100-dimensional MLPs with ELU non-linearity (Clevert et al., 2015) for all dep dep tag tag M LPchild , M LPhead , M LPchild and M LPhead , and the Adam optimizer with β1 = 0.9, β2 = 0.9, L2 norm (1e−6 ), and learning rate decay with the ratio 0.75 for every 2,500 iteration starting from 2e−3 , which is shown to be effective for training the biaffine parser (Dozat and Manning, 2016). 6.2 Japanese Experimental Settings We follow the default train/dev/test splits of Japanese CCGbank (Uematsu et al., 2013). For the baselines, we use an existing shift-reduce CCG parser implemented in an NLP tool Jigg9 (Noji and Miyao, 2016), and our implementation of the supertag-factored model using bi-LSTMs. For Japanese, we use as word representation the concatenation of word vectors initialized to Japanese Wikipedia Entity Vector10 , and 100dimensional vectors computed from randomly initialized 50-dimensional character embeddings through convolution (dos Santos and Zadrozny, 2014). We do not use affix vectors as affixes are less informative in Japanese. All characters appearing less than two times are mapped"
P17-1026,N16-1027,0,0.0953946,"Missing"
P17-1026,P15-1032,0,0.0253462,"ht dependency arcs. Inspired by this tradition, we try a simple H EAD F INAL rule in Japanese CCG parsing, in which we always select the right argument as the head. For example we obtain the head final dependency tree in Figure 4e from the Japanese CCG tree in Figure 4b. 5 Tri-training We extend the existing tri-training method to our models and apply it to our English parsers. Tri-training is one of the semi-supervised methods, in which the outputs of two parsers on unlabeled data are intersected to create (silver) new training data. This method is successfully applied to dependency parsing (Weiss et al., 2015) and CCG supertagging (Lewis et al., 2016). We simply combine the two previous approaches. Lewis et al. (2016) obtain their silver data annotated with the high quality supertags. Since they make this data publicly available 5 , we obtain our silver data by assigning dependency H EAD F IRST We apply the similar idea as H EAD F INAL into English. Since English has the opposite, SVO word order, we define the simple “head first” rule, in which the left argument always becomes the head (Figure 4d). 4 For example, the combinatory rules in Lewis and Steedman (2014) do not contain Nconj → N N in CCGba"
P17-1026,P14-1021,0,0.0348486,"of the Japanese parsing experiment in Table 5. The simple application of Lewis 12 This experiment is performed on a laptop with 4-thread 2.0 GHz CPU. 284 in that we do not use the one-best dependency structure alone, but rather we search for a CCG tree that is optimal in terms of dependencies and CCG supertags. Zhang et al. (2010) use the syntactic dependencies in a different way, and show that dependency-based features are useful for predicting HPSG supertags. In the CCG parsing literature, some work optimizes a dependency model, instead of supertags or a derivation (Clark and Curran, 2007; Xu et al., 2014). This approach is reasonable given that the objective matches the evaluation metric. Instead of modeling dependencies alone, our method finds a CCG derivation that has a higher dependency score. Lewis et al. (2015) present a joint model of CCG parsing and semantic role labeling (SRL), which is closely related to our approach. They map each CCG semantic dependency to an SRL relation, for which they give the A* upper bound by the score from a predicate to the most probable argument. Our approach is similar; the largest difference is that we instead model syntactic dependencies from each token t"
P17-1026,N10-1090,0,0.0250112,"f the output tree. As in our method, for every rule (schema) application they define which child becomes the head and impose a soft constraint that these dependencies agree with the output of the dependency parser. Our method is different 6.4 Japanese Parsing Result We show the results of the Japanese parsing experiment in Table 5. The simple application of Lewis 12 This experiment is performed on a laptop with 4-thread 2.0 GHz CPU. 284 in that we do not use the one-best dependency structure alone, but rather we search for a CCG tree that is optimal in terms of dependencies and CCG supertags. Zhang et al. (2010) use the syntactic dependencies in a different way, and show that dependency-based features are useful for predicting HPSG supertags. In the CCG parsing literature, some work optimizes a dependency model, instead of supertags or a derivation (Clark and Curran, 2007; Xu et al., 2014). This approach is reasonable given that the objective matches the evaluation metric. Instead of modeling dependencies alone, our method finds a CCG derivation that has a higher dependency score. Lewis et al. (2015) present a joint model of CCG parsing and semantic role labeling (SRL), which is closely related to ou"
P17-1026,J99-2004,0,\N,Missing
P17-1026,P07-1079,0,\N,Missing
P17-1026,Q16-1023,0,\N,Missing
P17-1026,N16-1026,0,\N,Missing
P17-1026,P16-4018,1,\N,Missing
P17-1146,W05-0620,0,0.14695,"Missing"
P17-1146,P09-2022,0,0.676518,"Linguistics https://doi.org/10.18653/v1/P17-1146 Figure 2: Overview of neural models: (i) single-sequence and (ii) multi-sequence models. In this paper, we first introduce a basic model that uses RNNs. This model independently estimates the arguments of each predicate without considering multi-predicate interactions (Sec. 3). Then, extending this model, we propose a neural model that uses Grid-RNNs (Sec. 4). Performing experiments on the NAIST Text Corpus (Iida et al., 2007), we demonstrate that even without syntactic information, our neural models outperform previous syntax-dependent models (Imamura et al., 2009; Ouchi et al., 2015). In particular, the neural model using Grid-RNNs achieved the best result. This suggests that the proposed grid-type neural architecture effectively captures multi-predicate interactions and contributes to performance improvements. 1 2 Japanese Predicate Argument Structure Analysis 2.1 Task Description In Japanese PAS analysis, arguments are identified that each fulfills one of the three major case roles, nominative (NOM), accusative (ACC) and dative (DAT) cases, for each predicate. Arguments can be divided into the following three categories according to the positions re"
P17-1146,P15-1093,1,0.276039,"mantic analysis task, in which systems are required to identify the semantic units of a sentence, such as who did what to whom. In prodrop languages such as Japanese, Chinese and Italian, arguments are often omitted in text, and such argument omission is regarded as one of the most problematic issues facing PAS analysis (Iida and Poesio, 2011; Sasano and Kurohashi, 2011; Hangyo et al., 2013). In response to the argument omission problem, in Japanese PAS analysis, a joint model of the interactions between multiple predicates has been gaining popularity and achieved the state-ofthe-art results (Ouchi et al., 2015; Shibata et al., 2016). This approach is based on the linguistic intuition that the predicates in a sentence are semantically related to each other, and capturing this relation can be useful for PAS analysis. In the example sentence in Figure 1, the word “男 i (mani )” is the accusative argument of the predicate “逮捕し た (arrested)” and is shared by the other predicate “逃走した (escaped)” as its nominative argument. Considering the semantic relation between “逮捕 した (arrested)” and “逃走した (escaped)”, we intuitively know that the person arrested by someone is likely to be the escaper. That is, informat"
P17-1146,D13-1095,0,0.403526,"rguments. “NOM” and “ACC” denote the nominative and accusative arguments, respectively. “ϕi ” is a zero pronoun, referring to the antecedent “男 i (mani )”. Introduction Predicate argument structure (PAS) analysis is a basic semantic analysis task, in which systems are required to identify the semantic units of a sentence, such as who did what to whom. In prodrop languages such as Japanese, Chinese and Italian, arguments are often omitted in text, and such argument omission is regarded as one of the most problematic issues facing PAS analysis (Iida and Poesio, 2011; Sasano and Kurohashi, 2011; Hangyo et al., 2013). In response to the argument omission problem, in Japanese PAS analysis, a joint model of the interactions between multiple predicates has been gaining popularity and achieved the state-ofthe-art results (Ouchi et al., 2015; Shibata et al., 2016). This approach is based on the linguistic intuition that the predicates in a sentence are semantically related to each other, and capturing this relation can be useful for PAS analysis. In the example sentence in Figure 1, the word “男 i (mani )” is the accusative argument of the predicate “逮捕し た (arrested)” and is shared by the other predicate “逃走した"
P17-1146,I11-1023,1,0.962427,"rticular, the neural model using Grid-RNNs achieved the best result. This suggests that the proposed grid-type neural architecture effectively captures multi-predicate interactions and contributes to performance improvements. 1 2 Japanese Predicate Argument Structure Analysis 2.1 Task Description In Japanese PAS analysis, arguments are identified that each fulfills one of the three major case roles, nominative (NOM), accusative (ACC) and dative (DAT) cases, for each predicate. Arguments can be divided into the following three categories according to the positions relative to their predicates (Hayashibe et al., 2011; Ouchi et al., 2015): Dep: Arguments that have direct syntactic dependency on the predicate. Zero: Arguments referred to by zero pronouns within the same sentence that have no direct syntactic dependency on the predicate. Inter-Zero: Arguments referred to by zero pronouns outside of the same sentence. 1 Our source code is publicly available https://github.com/hiroki13/neural-pasa-system at For example, in Figure 1, the nominative argument “警察 (police)” for the predicate “逮捕した (arrested)” is regarded as a Dep argument, because the argument has a direct syntactic dependency on the predicate. By"
P17-1146,I11-1085,0,0.566161,"he lower edges denote case arguments. “NOM” and “ACC” denote the nominative and accusative arguments, respectively. “ϕi ” is a zero pronoun, referring to the antecedent “男 i (mani )”. Introduction Predicate argument structure (PAS) analysis is a basic semantic analysis task, in which systems are required to identify the semantic units of a sentence, such as who did what to whom. In prodrop languages such as Japanese, Chinese and Italian, arguments are often omitted in text, and such argument omission is regarded as one of the most problematic issues facing PAS analysis (Iida and Poesio, 2011; Sasano and Kurohashi, 2011; Hangyo et al., 2013). In response to the argument omission problem, in Japanese PAS analysis, a joint model of the interactions between multiple predicates has been gaining popularity and achieved the state-ofthe-art results (Ouchi et al., 2015; Shibata et al., 2016). This approach is based on the linguistic intuition that the predicates in a sentence are semantically related to each other, and capturing this relation can be useful for PAS analysis. In the example sentence in Figure 1, the word “男 i (mani )” is the accusative argument of the predicate “逮捕し た (arrested)” and is shared by the"
P17-1146,P16-1117,0,0.604448,", in which systems are required to identify the semantic units of a sentence, such as who did what to whom. In prodrop languages such as Japanese, Chinese and Italian, arguments are often omitted in text, and such argument omission is regarded as one of the most problematic issues facing PAS analysis (Iida and Poesio, 2011; Sasano and Kurohashi, 2011; Hangyo et al., 2013). In response to the argument omission problem, in Japanese PAS analysis, a joint model of the interactions between multiple predicates has been gaining popularity and achieved the state-ofthe-art results (Ouchi et al., 2015; Shibata et al., 2016). This approach is based on the linguistic intuition that the predicates in a sentence are semantically related to each other, and capturing this relation can be useful for PAS analysis. In the example sentence in Figure 1, the word “男 i (mani )” is the accusative argument of the predicate “逮捕し た (arrested)” and is shared by the other predicate “逃走した (escaped)” as its nominative argument. Considering the semantic relation between “逮捕 した (arrested)” and “逃走した (escaped)”, we intuitively know that the person arrested by someone is likely to be the escaper. That is, information about one predicate"
P17-1146,D08-1055,0,0.573656,"quence model, we use the softmax function to calculate the probability of the case labels of each word wt for each predicate pm : (L) ym,t = softmax(Wy hm,t ) (L) where hm,t is a hidden state vector calculated in the last grid layer. 5 Related Work 5.1 Japanese PAS Analysis Approaches Existing approaches to Japanese PAS analysis are divided into two categories: (i) the pointwise approach and (ii) the joint approach. The pointwise approach involves estimating the score of each argument candidate for one predicate, and then selecting the argument candidate with the maximum score as an argument (Taira et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011; Iida et al., 2016). The joint approach involves scoring all the predicateargument combinations in one sentence, and then selecting the combination with the highest score (Yoshikawa et al., 2011; Sasano and Kurohashi, 1595 2011; Ouchi et al., 2015; Shibata et al., 2016). Compared with the pointwise approach, the joint approach achieves better results. 5.2 Multi-Predicate Interactions Ouchi et al. (2015) reported that it is beneficial to Japanese PAS analysis to capture the interactions between all predicates in a sentence. This is based on the lin"
P17-1146,W07-1522,1,0.842901,"sociation for Computational Linguistics, pages 1591–1600 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1146 Figure 2: Overview of neural models: (i) single-sequence and (ii) multi-sequence models. In this paper, we first introduce a basic model that uses RNNs. This model independently estimates the arguments of each predicate without considering multi-predicate interactions (Sec. 3). Then, extending this model, we propose a neural model that uses Grid-RNNs (Sec. 4). Performing experiments on the NAIST Text Corpus (Iida et al., 2007), we demonstrate that even without syntactic information, our neural models outperform previous syntax-dependent models (Imamura et al., 2009; Ouchi et al., 2015). In particular, the neural model using Grid-RNNs achieved the best result. This suggests that the proposed grid-type neural architecture effectively captures multi-predicate interactions and contributes to performance improvements. 1 2 Japanese Predicate Argument Structure Analysis 2.1 Task Description In Japanese PAS analysis, arguments are identified that each fulfills one of the three major case roles, nominative (NOM), accusative"
P17-1146,1983.tc-1.13,0,0.102912,"Missing"
P17-1146,P11-1081,0,0.209368,"ndency relations, and the lower edges denote case arguments. “NOM” and “ACC” denote the nominative and accusative arguments, respectively. “ϕi ” is a zero pronoun, referring to the antecedent “男 i (mani )”. Introduction Predicate argument structure (PAS) analysis is a basic semantic analysis task, in which systems are required to identify the semantic units of a sentence, such as who did what to whom. In prodrop languages such as Japanese, Chinese and Italian, arguments are often omitted in text, and such argument omission is regarded as one of the most problematic issues facing PAS analysis (Iida and Poesio, 2011; Sasano and Kurohashi, 2011; Hangyo et al., 2013). In response to the argument omission problem, in Japanese PAS analysis, a joint model of the interactions between multiple predicates has been gaining popularity and achieved the state-ofthe-art results (Ouchi et al., 2015; Shibata et al., 2016). This approach is based on the linguistic intuition that the predicates in a sentence are semantically related to each other, and capturing this relation can be useful for PAS analysis. In the example sentence in Figure 1, the word “男 i (mani )” is the accusative argument of the predicate “逮捕し た (arre"
P17-1146,D14-1041,0,0.0358225,"wa et al., 2011; Sasano and Kurohashi, 1595 2011; Ouchi et al., 2015; Shibata et al., 2016). Compared with the pointwise approach, the joint approach achieves better results. 5.2 Multi-Predicate Interactions Ouchi et al. (2015) reported that it is beneficial to Japanese PAS analysis to capture the interactions between all predicates in a sentence. This is based on the linguistic intuition that the predicates in a sentence are semantically related to each other, and that the information regarding this semantic relation can be useful for PAS analysis. Similarly, in semantic role labeling (SRL), Yang and Zong (2014) reported that their reranking model, which captures the multi-predicate interactions, is effective for the English constituentbased SRL task (Carreras and M`arquez, 2005). Taking this a step further, we propose a neural architecture that effectively models the multipredicate interactions. 5.3 Neural Approaches Japanese PAS In recent years, several attempts have been made to apply neural networks to Japanese PAS analysis (Shibata et al., 2016; Iida et al., 2016)4 . In Shibata et al. (2016), a feed-forward neural network is used for the score calculation part of the joint model proposed by Ouch"
P17-1146,D15-1260,0,0.527956,"Missing"
P17-1146,I11-1126,1,0.931219,"Missing"
P17-1146,D16-1132,0,0.564313,"lity of the case labels of each word wt for each predicate pm : (L) ym,t = softmax(Wy hm,t ) (L) where hm,t is a hidden state vector calculated in the last grid layer. 5 Related Work 5.1 Japanese PAS Analysis Approaches Existing approaches to Japanese PAS analysis are divided into two categories: (i) the pointwise approach and (ii) the joint approach. The pointwise approach involves estimating the score of each argument candidate for one predicate, and then selecting the argument candidate with the maximum score as an argument (Taira et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011; Iida et al., 2016). The joint approach involves scoring all the predicateargument combinations in one sentence, and then selecting the combination with the highest score (Yoshikawa et al., 2011; Sasano and Kurohashi, 1595 2011; Ouchi et al., 2015; Shibata et al., 2016). Compared with the pointwise approach, the joint approach achieves better results. 5.2 Multi-Predicate Interactions Ouchi et al. (2015) reported that it is beneficial to Japanese PAS analysis to capture the interactions between all predicates in a sentence. This is based on the linguistic intuition that the predicates in a sentence are semantical"
P17-1146,P15-1109,0,0.39401,"ds of information: (i) the context of the entire sentence, and (ii) multi-predicate interactions. For the former, we introduce single-sequence model that induces context-sensitive representations from a sequence of argument candidates of a predicate. For the latter, we introduce multisequence model that induces predicate-sensitive representations from multiple sequences of argument candidates of all predicates in a sentence (shown in Figure 2). 3 Single-Sequence Model The single-sequence model exploits stacked bidirectional RNNs (Bi-RNN) (Schuster and Paliwal, 1997; Graves et al., 2005, 2013; Zhou and Xu, 2015). Figure 3 shows the overall architecture, which consists of the following three components: Input Layer: Map each word to a feature vector representation. Figure 4: Example of feature extraction. The underlined word is the target predicate. From the sentence “彼女はパンを食べた。(She ate bread.)”, three types of features are extracted for the target predicate “食べた (ate)”. Figure 5: Example of the process of creating a feature vector. The extracted features are mapped to each vector, and all the vectors are concatenated into one feature vector. In the following subsections, we describe each of these thr"
P17-2068,N09-1037,0,0.252376,"e consistent with MWEs, by extending Kato et al. (2016)’s corpus 2 . As is the case with their corpus, each MME is a syntactic unit in an MWE-aware dependency structure from our corpus (Figure 1b). Moreover, our corpus includes not only functional MWEs but also NEs. Because NEs are highly productive and occur more frequently than functional MWEs, they are difficult to cover in a dictionary. To solve complex Natural Language Processing (NLP) tasks that require deep syntactic analysis, various levels of annotation such as parse trees and named entities (NEs) must be consistent with one another (Finkel and Manning, 2009). Otherwise, it is usually impossible to combine these pieces of information effectively. However, the standard syntactic corpus of English, Penn Treebank, is not concerned with consistency between syntactic trees and spans of multiword expressions (MWEs). In Penn Treebank, that is, an MWE-span does not always correspond to a span dominated by a single non-terminal node. Therefore, word-based dependency structures converted from Penn Treebank are generally inconsistent with MWE-spans (Figure 1a). To mitigate this inconsistency, Kato et al. (2016) estabConsistency between NE-spans and phrase st"
P17-2068,P13-2017,0,0.0725619,"Missing"
P17-2068,P15-1108,0,0.16997,"20-way jackknifing for the training split. The test split was automatically tagged by the sequential labeler trained on the training split. 12 When calculating UAS/LAS, we removed punctuation. 13 FUM only focuses on MWE-spans, whereas FTM focuses on both MWE-spans and MWE POS tags. 430 respect to functional MWEs. investigate a CFG-based model and a model based on tree-substitution grammars. Second, Candito and Constant (2014) compares several architectures for graph-based dependency parsing and MWE recognition, in which MWE recognition is conducted before, during, and after parsing. Finally, Nasr et al. (2015) explores a joint model of MWE recognition and dependency parsing. They focus on complex function words. In terms of data representation, they adopt one similar to ours, insofar as the components of an MWE are linked by dependency edges whose labels are MWEspecific. By adding MWE-specific features to the joint model, however, we observe at least a 2.52 / 3.00 point improvement in terms of UAS / LAS regarding the first tokens of MWEs, and a 2.90 / 2.99 point improvement regarding FUM / FTM. As a result, we obtain a 1.35 / 1.28 point improvement with joint(+pred span) compared with the pipeline"
P17-2068,J13-1009,0,0.0598946,"Missing"
P17-2068,L16-1263,1,0.904354,"ts over a pipeline model. 1 (a) a word-based dependency structure (b) an MWE-aware dependency structure Figure 1: A word-based and an MWE-aware dependency structure. In the former, a span of an MWE (“a number of”) does not correspond to any subtree. The MWE is represented as a single node in the latter structure. lishes each span of functional MWEs 1 as a subtree of a phrase structure in the Wall Street Journal portion of Ontonotes (Pradhan et al., 2007). Introduction To pursue this direction further, we construct a corpus such that dependency structures are consistent with MWEs, by extending Kato et al. (2016)’s corpus 2 . As is the case with their corpus, each MME is a syntactic unit in an MWE-aware dependency structure from our corpus (Figure 1b). Moreover, our corpus includes not only functional MWEs but also NEs. Because NEs are highly productive and occur more frequently than functional MWEs, they are difficult to cover in a dictionary. To solve complex Natural Language Processing (NLP) tasks that require deep syntactic analysis, various levels of annotation such as parse trees and named entities (NEs) must be consistent with one another (Finkel and Manning, 2009). Otherwise, it is usually imp"
P17-2068,schneider-etal-2014-comprehensive,0,0.0376648,"hing. 6 Acknowledgments Related Work This work was partially supported by JST CREST Grant Number JPMJCR1513 and JSPS KAKENHI Grant Number 15K16053. We are grateful to members of the Computational Linguistics Laboratory at NAIST, and to the anonymous reviewers for their valuable feedback. Regarding the preparation of a title list from English-language Wikipedia articles, we are particularly grateful for the assistance given by Motoki Sato. Whereas French Treebank is available for French MWEs (Abeill´e et al., 2003), there have been only limited corpora for English MWE-aware dependency parsing. Schneider et al. (2014) constructs an MWE-annotated corpus on English Web Treebank (Bies et al., 2012). However, this corpus is relatively small as training data for a parser, and its MWE annotations are not consistent with syntactic trees. By contrast, our corpus covers the whole of the WSJ portion of Ontonotes and ensures consistency between MWE annotations and parse trees. References Anne Abeill´e, Lionel Cl´ement, and Franc¸ois Toussenel. 2003. Building a Treebank for French, Springer Netherlands, Dordrecht, pages 165–187. Korkontzelos and Manandhar (2010) reports an improvement in base-phrase chunking by pregro"
P17-2068,N10-1089,0,0.0347379,"r English MWE-aware dependency parsing. Schneider et al. (2014) constructs an MWE-annotated corpus on English Web Treebank (Bies et al., 2012). However, this corpus is relatively small as training data for a parser, and its MWE annotations are not consistent with syntactic trees. By contrast, our corpus covers the whole of the WSJ portion of Ontonotes and ensures consistency between MWE annotations and parse trees. References Anne Abeill´e, Lionel Cl´ement, and Franc¸ois Toussenel. 2003. Building a Treebank for French, Springer Netherlands, Dordrecht, pages 165–187. Korkontzelos and Manandhar (2010) reports an improvement in base-phrase chunking by pregrouping MWEs as words-with-spaces. They focus on compound nouns, adjective-noun constructions, and named entities. However, they use gold MWE-spans, and this is not a realistic setting. By contrast, we use predicted MWE-spans. Ann Bies, Justin Mott, Colin Warner, and Seth Kulick. 2012. English web treebank. Technical Report LDC2012T13, Linguistic Data Consortium, Philadelphia, Pennsylvania, USA. . Marie Candito and Matthieu Constant. 2014. Strategies for contiguous multiword expression analysis and dependency parsing. In Proceedings of the"
P17-2068,N03-1033,0,0.102424,"red span) Dependency Parsing (First tokens of MWEs) Functional MWEs NEs UAS LAS UAS 78.89 64.01 85.58 71.28 65.05 85.07 79.93 73.70 85.79 81.31 74.74 85.89 MWE Recognition LAS 82.41 81.49 82.82 83.23 Functional MWEs FUM FTM 96.76 96.42 91.01 89.93 97.94 97.25 97.59 96.91 NEs FUM 89.81 88.47 90.16 91.32 Table 4: Breakdown of experimental results by type of MWE. Note that UAS / LAS are calculated regarding first tokens of MWEs. For NEs, the FTM is the same as the FUM because each NE always takes NNP as an MWE-level POS tag, and is not repeated. the POS tags predicted by the Stanford POS tagger (Toutanova et al., 2003) 10 . For the pipeline model and joint(+pred span), we used MWEspans and MWE POS tags predicted by CRF 11 . For dependency parsing, we used Redshift (Honnibal et al., 2013) for all models, with a beam size of 16 for decoding. For training, we removed non-projective dependency trees. For testing, we parsed all sentences. To evaluate parsing, we used unlabeled and labeled attachment scores (UAS/LAS) 12 . For the pipeline model, we converted each concatenated token corresponding to an MWE into a head-initial structure and compared this with the gold tree. For the joint model, we directly compared"
P17-2068,W08-1300,0,0.0796598,"Missing"
P17-2068,P11-2033,0,0.254436,"ONEY, QUANTITY, ORDINAL, and CARDINAL. Note that we only focus on multiword NEs. 5 https://catalog.ldc.upenn.edu/LDC2017T01 6 https://en.wiktionary.org 7 We do not require manual annotations for Case (A). 8 NEs have NNP as an MWE-level POS tag. Table 2: Histogram tabling the consistency between MWE-spans and phrase structures. 3 Although Kato et al. (2016) conducts experiments regarding MWE-aware dependency parsing, they use gold MWE-spans. This is not a realistic scenario. By contrast, our parsing models do not use gold MWE-spans. 428 baseline features and rich non-local features proposed by Zhang and Nivre (2011). 3.2 Joint Model In the proposed joint model, MWE-spans and MWE POS tags are encoded as dependency labels, and conventional word-based dependency parsing is performed by an arc-eager transitionbased parser. We use the same parsing features used in the pipeline model. We convert MWEs in MWE-aware dependency structures (Figure 1b) to head-initial structures (Figure 3) that encode MWE-spans and MWE POS tags. Note that this representation is similar to Universal Dependency (McDonald et al., 2013). When parsing, we use constraints based on a history of transitions and the dictionary of functional"
P18-2015,H05-1050,0,0.0383621,"apping process. Another approach, called “distant supervision” (DS) (Mintz et al., 2009), does not require any labels on the text. The assumption of DS is that if two entities participate in a known Freebase relation, any sentence that contains those two entities might express that relation. However, this technique often introduces noise to the generated training data. As a result, DS is still limited by the 2.1 Related Work Automatic Seed Selection for Bootstrapping RE As manually selecting the seeds requires tremendous effort, some research proposed methods to select the seed automatically. Eisner and Karakos (2005) used a “strapping” approach to evaluate many candidate seeds automatically for a word sense disambiguation task. Kozareva and Hovy (2010) proposed a method for measuring seed quality using a regression model and applied it to the extraction of unary semantic relations, such as“people” and “city”. Kiso et al. (2011) suggested a HITS-based approach to ranking the seeds, based on Komachi et al. (2008)’s analysis of the Espresso algorithm (Pantel and Pennacchiotti, 1 We release our annotated dataset https://github.com/pvthuy/part-whole-relations. 89 Proceedings of the 56th Annual Meeting of the A"
P18-2015,D11-1142,0,0.0486481,"k in noise reduction may be much larger than in seed selection. Problem Formulation Let R∗ be the set of target relations. The goal is to find instances, or pairs of entities, upon which the relation holds. For each target relation r ∈ R∗ , we assume there is a set Dr of triples representing the relation r. The triples in Dr have the form (e1 , p, e2 ), where e1 and e2 denote entities, and p denotes the pattern that connects the two entities. A pair of entities (e1 , e2 ) is called an instance. This terminology is similar to the one used in open information extraction systems, such as Reverb (Fader et al., 2011). For example, in triple (Barack Obama, was born in, Honolulu), (Barack Obama, Honolulu) is the instance, and “was born in” is the pattern. The two tasks we address are defined as follows: 4 Approaches to Automatic Seed Selection and Noise Reduction In this section, we propose several methods that can be applied for both automatic seed selection and noise reduction tasks, inspired by ranking relation instances and patterns computed by the HITS algorithm, and picking cluster centroids using the K-means, latent semantic analysis (LSA), or non-negative matrix factorization (NMF) method. 4.1 K-mea"
P18-2015,P09-1113,0,0.0866586,"1 Introduction 2 Bootstrapping for relation extraction (RE) (Brin, 1998; Riloff et al., 1999; Agichtein and Gravano, 2000) is a class of minimally supervised methods frequently used in machine learning: initialized by a small set of example instances called seeds, to represent a particular semantic relation, the bootstrapping system operates iteratively to acquire new instances of a target relation. Selecting “good” seeds is one of the most important steps to reduce semantic drift, which is a typical phenomenon of the bootstrapping process. Another approach, called “distant supervision” (DS) (Mintz et al., 2009), does not require any labels on the text. The assumption of DS is that if two entities participate in a known Freebase relation, any sentence that contains those two entities might express that relation. However, this technique often introduces noise to the generated training data. As a result, DS is still limited by the 2.1 Related Work Automatic Seed Selection for Bootstrapping RE As manually selecting the seeds requires tremendous effort, some research proposed methods to select the seed automatically. Eisner and Karakos (2005) used a “strapping” approach to evaluate many candidate seeds a"
P18-2015,P11-2006,1,0.834263,"ntroduces noise to the generated training data. As a result, DS is still limited by the 2.1 Related Work Automatic Seed Selection for Bootstrapping RE As manually selecting the seeds requires tremendous effort, some research proposed methods to select the seed automatically. Eisner and Karakos (2005) used a “strapping” approach to evaluate many candidate seeds automatically for a word sense disambiguation task. Kozareva and Hovy (2010) proposed a method for measuring seed quality using a regression model and applied it to the extraction of unary semantic relations, such as“people” and “city”. Kiso et al. (2011) suggested a HITS-based approach to ranking the seeds, based on Komachi et al. (2008)’s analysis of the Espresso algorithm (Pantel and Pennacchiotti, 1 We release our annotated dataset https://github.com/pvthuy/part-whole-relations. 89 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 89–95 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics at With these inputs, the task is to choose good seeds from the instances appearing in Dr for each r ∈ R∗ , such that they work effectively in bootstrapping"
P18-2015,W12-2402,0,0.0385929,"Missing"
P18-2015,D08-1106,1,0.753551,"y the 2.1 Related Work Automatic Seed Selection for Bootstrapping RE As manually selecting the seeds requires tremendous effort, some research proposed methods to select the seed automatically. Eisner and Karakos (2005) used a “strapping” approach to evaluate many candidate seeds automatically for a word sense disambiguation task. Kozareva and Hovy (2010) proposed a method for measuring seed quality using a regression model and applied it to the extraction of unary semantic relations, such as“people” and “city”. Kiso et al. (2011) suggested a HITS-based approach to ranking the seeds, based on Komachi et al. (2008)’s analysis of the Espresso algorithm (Pantel and Pennacchiotti, 1 We release our annotated dataset https://github.com/pvthuy/part-whole-relations. 89 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 89–95 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics at With these inputs, the task is to choose good seeds from the instances appearing in Dr for each r ∈ R∗ , such that they work effectively in bootstrapping RE. 2006). Movshovitz-Attias and Cohen (2012) generated a ranking based on pointwise"
P18-2015,P06-1015,0,0.0882361,"of part-whole relations as a reliable resource for selecting seeds. Our dataset was collected from Wikipedia and ClueWeb, and annotated by two annotators. One of its special characteristics is that the part-whole relation is a collection of relations, not a single relation (Iris, 1989; Winston et al., 1987). Table 1 gives the frequencies of each subtype of part-whole relations. There are 5,727 instances of 8 subtypes that were annotated with the same labels by both annotators. We use Espresso+Word2vec (Phi and Matsumoto, 2016), which is an improved version for the original Espresso algorithm (Pantel and Pennacchiotti, 2006). Espresso+Word2vec outperformed the Espresso system for harvesting part-whole relations by utilizing the Similarity Ranker, which uses the embedded vector difference between instance pairs of relations. The performance is measured with Precision@N (Manning et al., 2008), N = 50. In total, 5,000 instances are checked by 5.2 Performance on Automatic Seed Selection Task The performances of the seed selection methods are presented in Table 2. For the HITS-based and HITS+K-means-based methods, we display the P@50 with three types of graph representation as shown in Section 4.2. We use random seed"
P18-2015,N10-1087,0,0.0226272,"mption of DS is that if two entities participate in a known Freebase relation, any sentence that contains those two entities might express that relation. However, this technique often introduces noise to the generated training data. As a result, DS is still limited by the 2.1 Related Work Automatic Seed Selection for Bootstrapping RE As manually selecting the seeds requires tremendous effort, some research proposed methods to select the seed automatically. Eisner and Karakos (2005) used a “strapping” approach to evaluate many candidate seeds automatically for a word sense disambiguation task. Kozareva and Hovy (2010) proposed a method for measuring seed quality using a regression model and applied it to the extraction of unary semantic relations, such as“people” and “city”. Kiso et al. (2011) suggested a HITS-based approach to ranking the seeds, based on Komachi et al. (2008)’s analysis of the Espresso algorithm (Pantel and Pennacchiotti, 1 We release our annotated dataset https://github.com/pvthuy/part-whole-relations. 89 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 89–95 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Comp"
P18-2015,Y16-2015,1,0.859342,"m each column of W. 5 5.1 Experiments Datasets and Settings We provide an annotated dataset of part-whole relations as a reliable resource for selecting seeds. Our dataset was collected from Wikipedia and ClueWeb, and annotated by two annotators. One of its special characteristics is that the part-whole relation is a collection of relations, not a single relation (Iris, 1989; Winston et al., 1987). Table 1 gives the frequencies of each subtype of part-whole relations. There are 5,727 instances of 8 subtypes that were annotated with the same labels by both annotators. We use Espresso+Word2vec (Phi and Matsumoto, 2016), which is an improved version for the original Espresso algorithm (Pantel and Pennacchiotti, 2006). Espresso+Word2vec outperformed the Espresso system for harvesting part-whole relations by utilizing the Similarity Ranker, which uses the embedded vector difference between instance pairs of relations. The performance is measured with Precision@N (Manning et al., 2008), N = 50. In total, 5,000 instances are checked by 5.2 Performance on Automatic Seed Selection Task The performances of the seed selection methods are presented in Table 2. For the HITS-based and HITS+K-means-based methods, we dis"
P18-2015,W17-2323,0,0.0227472,"orate the quality of the triple classifier trained subsequently. Noise Reduction for Distantly Supervised RE The DS assumption is too strong and leads to wrongly labeled data that affects performance. Many studies focused on methods of noise reduction in DS. Intxaurrondo et al. (2013) filtered out noisy mentions from the distantly supervised dataset using their frequencies, PMI, or the similarity between the centroids of all relation mentions and each individual mention. Xiang et al. (2016) introduced ranking-based methods according to different strategies to select effective training groups. Li et al. (2017) proposed three novel heuristics that use lexical and syntactic information to remove noise in the biomedical domain. The data generated by the noise reduction process can be used by supervised learning algorithms to train models. 3 Formulation as Ranking Tasks: As we can see from the task definitions above, both seed selection and noise reduction are the task of selecting triples from a given collection. Indeed, the two tasks essentially have a similar goal in terms of the ranking-based perspective. We thus formulate them as the task of ranking instances (in seed selection) or triples (in noi"
P18-2015,P16-1200,0,0.015507,"n method. For the noise reduction task, we use the training and testing set developed by (Riedel et al., 2010), which contains 53 relation classes. This dataset was generated by aligning Freebase relations with the New York Times corpus. After removing noisy triples from the dataset using the proposed methods, we use the filtered data to train two kinds of convolutional neural networks (CNN) (the CNN model in (Zeng et al., 2014) and the PCNN model in (Zeng et al., 2015)) with at-least-one multiinstance learning (ONE) used in (Zeng et al., 2015), and the sentence-level attention (ATT) used in (Lin et al., 2016). Finally, we report the area under the precision-recall (AUCPR) of each noise reduction method. A ≈ WH The non-negativity constraint is the main difference between NMF and LSA. Similarly to the LSA-based method, we set the NMF parameter K to k, the desired number of instances to select. We then select the k instances that have the highest values from each column of W. 5 5.1 Experiments Datasets and Settings We provide an annotated dataset of part-whole relations as a reliable resource for selecting seeds. Our dataset was collected from Wikipedia and ClueWeb, and annotated by two annotators. O"
P18-2015,D15-1203,0,0.0237102,"ey express partwhole relations. We vary the number k of seeds between 5 and 50 with a step of 5 to report the average P@50 of each seed selection method. For the noise reduction task, we use the training and testing set developed by (Riedel et al., 2010), which contains 53 relation classes. This dataset was generated by aligning Freebase relations with the New York Times corpus. After removing noisy triples from the dataset using the proposed methods, we use the filtered data to train two kinds of convolutional neural networks (CNN) (the CNN model in (Zeng et al., 2014) and the PCNN model in (Zeng et al., 2015)) with at-least-one multiinstance learning (ONE) used in (Zeng et al., 2015), and the sentence-level attention (ATT) used in (Lin et al., 2016). Finally, we report the area under the precision-recall (AUCPR) of each noise reduction method. A ≈ WH The non-negativity constraint is the main difference between NMF and LSA. Similarly to the LSA-based method, we set the NMF parameter K to k, the desired number of instances to select. We then select the k instances that have the highest values from each column of W. 5 5.1 Experiments Datasets and Settings We provide an annotated dataset of part-whole"
P18-2015,C14-1220,0,0.0310187,"ruct A: annotators to ascertain whether they express partwhole relations. We vary the number k of seeds between 5 and 50 with a step of 5 to report the average P@50 of each seed selection method. For the noise reduction task, we use the training and testing set developed by (Riedel et al., 2010), which contains 53 relation classes. This dataset was generated by aligning Freebase relations with the New York Times corpus. After removing noisy triples from the dataset using the proposed methods, we use the filtered data to train two kinds of convolutional neural networks (CNN) (the CNN model in (Zeng et al., 2014) and the PCNN model in (Zeng et al., 2015)) with at-least-one multiinstance learning (ONE) used in (Zeng et al., 2015), and the sentence-level attention (ATT) used in (Lin et al., 2016). Finally, we report the area under the precision-recall (AUCPR) of each noise reduction method. A ≈ WH The non-negativity constraint is the main difference between NMF and LSA. Similarly to the LSA-based method, we set the NMF parameter K to k, the desired number of instances to select. We then select the k instances that have the highest values from each column of W. 5 5.1 Experiments Datasets and Settings We"
P18-4010,W16-4901,1,0.882811,"Missing"
P18-4010,Y13-2007,0,0.0184071,"gence Project (AIP) {liu.jun.lc3, shindo, matsu}@is.naist.jp Abstract ない (have to)”, “ことができる (be able to)”. Due to various meanings and usages of Japanese functional expressions, it is fairly difficult for JSL learners to learn them. In recent years, certain online Japanese learning systems are developed to support JSL learners, such as Reading Tutor2, Asunaro3, Rikai4, and WWWJDIC5. Some of these systems are particularly designed to enable JSL learners to read and write Japanese texts by offering the word information with their corresponding difficulty information or translation information (Ohno et al., 2013; Toyoda 2016). However, learners’ native language background has not been taken into account in these systems. Moreover, these systems provide learners with limited information about the various types of Japanese functional expressions, which learners actually intend to learn as a part of the procedure for learning Japanese. Therefore, developing a learning system that can assist JSL learners to learn Japanese functional expressions is crucial in Japanese education. In this paper, we present Jastudy, a computerassisted learning system, aiming at helping Chinese-speaking JSL learners with thei"
P19-1158,P17-2096,0,0.0603862,"thod ensures that the tokenization is consistent between training and evaluation, particularly for a sentence containing a low frequency phrase. 3.5 Embedding for Unfixed Vocabulary Since our model does not limit the vocabulary, there are many ways to tokenize a single sentence. To use token-level representations, we typically employ a lookup embedding mechanism, which requires a fixed vocabulary. In our model, however, the vocabulary changes as the language model is updated. We, therefore, introduce word embeddings with continuous cache inspired by (Grave et al., 2016; Kawakami et al., 2017; Cai et al., 2017). This method enables the proposed model to assign token-level representations to recently sampled tokens. Although embeddings of older tokens are discarded from the cache memory, we assume that meaningful tokens to solve the task appear frequently, and they remain in the cache during training if the size of the cache is large enough. By updating representations in the cache, the model can use token-level information adequately. In our embedding mechanism with a cache component, the model has a list Q that stores |Q |elements of recent tokenization history. The model also keeps a lookup table"
P19-1158,P06-1085,0,0.23266,"at our new tokenization strategy is effective on some classification tasks. 8 F1-score 80.31 80.41 78.95 81.71 80.46 https://www.rondhuit.com/download. html#ldcc 6 Related Work Our work is related to word segmentation for a neural network encoder. To tokenize a sentence into subwords without dictionary-based segmentation, BPE is commonly used in neural machine translation (NMT) (Sennrich et al., 2016). BPE forces a merger of tokens without any exceptions, and tokenization does not become natural. The problem associate with BPE has been addressed using a language model to tokenize a sentence. (Goldwater et al., 2006, 2009) proposed unsupervised word segmentation by sampling tokenization and updating a language model with Gibbs sampling. The language model for unsupervised word segmentation is smoothed with base probabilities of words to give a probability for all possible words in a text. (Mochihashi et al., 2009) extended this to the use of blocked Gibbs sampling, which samples tokenization by a sentence. The authors introduced a nested Bayesian language model that calculates a probability of a word by hierarchical language models. Recently, (Kudo and Richardson, 2018) proposed a subword generator for N"
P19-1158,P15-1162,0,0.0539002,"Missing"
P19-1158,E17-2068,0,0.0402014,"ow that our method achieves better performance than previous methods. 1 Introduction Tokenization is a fundamental problem in text classification such as sentiment analysis (Tang et al., 2014; Kim, 2014; dos Santos and Gatti, 2014), topic detection (Lai et al., 2015; Zhang et al., 2015), and spam detection (Liu and Jia, 2012; Liu et al., 2016). In text classification with neural networks, sentence representation is calculated based on tokens that compose the sentence. Specifically, a sentence is first tokenized into meaningful units such as characters, words, and subwords (Zhang et al., 2015; Joulin et al., 2017). Then, the token embeddings are looked up and fed into a neural network encoder such as a feed-forward neural network (Iyyer et al., 2015), a convolutional neural network (CNN) (Kim, 2014; Kalchbrenner et al., 2014), or a long short-term memory (LSTM) network (Wang et al., 2016a,b). For English and other languages that use the Latin alphabet, the whitespace is a good indicator of word segmentation. However, tokenization is a non-trivial problem in unsegmented languages such as Chinese and Japanese since they have no explicit word boundaries. For these languages, tokenizers based on supervised"
P19-1158,P14-1062,0,0.0175508,"ntos and Gatti, 2014), topic detection (Lai et al., 2015; Zhang et al., 2015), and spam detection (Liu and Jia, 2012; Liu et al., 2016). In text classification with neural networks, sentence representation is calculated based on tokens that compose the sentence. Specifically, a sentence is first tokenized into meaningful units such as characters, words, and subwords (Zhang et al., 2015; Joulin et al., 2017). Then, the token embeddings are looked up and fed into a neural network encoder such as a feed-forward neural network (Iyyer et al., 2015), a convolutional neural network (CNN) (Kim, 2014; Kalchbrenner et al., 2014), or a long short-term memory (LSTM) network (Wang et al., 2016a,b). For English and other languages that use the Latin alphabet, the whitespace is a good indicator of word segmentation. However, tokenization is a non-trivial problem in unsegmented languages such as Chinese and Japanese since they have no explicit word boundaries. For these languages, tokenizers based on supervised machine learning with a dictionary (Zhang et al., 2003; Kudo, 2006) have been used to segment a sentence into units (Lai et al., 2015). In addition, we use a neural network-based word segmenter to tokenize a raw cor"
P19-1158,P17-1137,0,0.235461,"phase. This updating method ensures that the tokenization is consistent between training and evaluation, particularly for a sentence containing a low frequency phrase. 3.5 Embedding for Unfixed Vocabulary Since our model does not limit the vocabulary, there are many ways to tokenize a single sentence. To use token-level representations, we typically employ a lookup embedding mechanism, which requires a fixed vocabulary. In our model, however, the vocabulary changes as the language model is updated. We, therefore, introduce word embeddings with continuous cache inspired by (Grave et al., 2016; Kawakami et al., 2017; Cai et al., 2017). This method enables the proposed model to assign token-level representations to recently sampled tokens. Although embeddings of older tokens are discarded from the cache memory, we assume that meaningful tokens to solve the task appear frequently, and they remain in the cache during training if the size of the cache is large enough. By updating representations in the cache, the model can use token-level information adequately. In our embedding mechanism with a cache component, the model has a list Q that stores |Q |elements of recent tokenization history. The model also ke"
P19-1158,D14-1181,0,0.00650062,"r model incorporates a language model for unsupervised tokenization into a text classifier and then trains both models simultaneously. To make the model robust against infrequent tokens, we sampled segmentation for each sentence stochastically during training, which resulted in improved performance of text classification. We conducted experiments on sentiment analysis as a text classification task and show that our method achieves better performance than previous methods. 1 Introduction Tokenization is a fundamental problem in text classification such as sentiment analysis (Tang et al., 2014; Kim, 2014; dos Santos and Gatti, 2014), topic detection (Lai et al., 2015; Zhang et al., 2015), and spam detection (Liu and Jia, 2012; Liu et al., 2016). In text classification with neural networks, sentence representation is calculated based on tokens that compose the sentence. Specifically, a sentence is first tokenized into meaningful units such as characters, words, and subwords (Zhang et al., 2015; Joulin et al., 2017). Then, the token embeddings are looked up and fed into a neural network encoder such as a feed-forward neural network (Iyyer et al., 2015), a convolutional neural network (CNN) (Kim"
P19-1158,P18-1007,0,0.401623,"re frequently when sampling. We use a dictionary-based morphological analyzer or unsupervised word segmentation to tokenize a corpus initially, and the language model is initialized with the tokenized corpus. 3.3 Nested Unigram Language Model pbase (t : c1 ...cM ) = puni (c1 ) is given as Sampling Tokenization With the nested unigram language model introduced above, the tokenization of a sentence is sampled from the distribution P (t|s) where t is possible tokenization for the sentence. A probability of tokenization is obtained Q by a nested language model (4) as p(t|s) = t∈t p(t). Following (Kudo, 2018) and (Mochihashi et al., 2009), we employ a dynamic programming (DP) technique called forward filtering backward sampling (FFBS) (Scott, 2002) to sample tokens stochastically. With FFBS, we can sample tokens in a sentence from a distribution considering all possible tokenizations within the limit of the maximum token length l. In the forward calculation of FFBS, a DP Table D is calculated as follows: pbi (cm |cm−1 ) min(i−j,l) m=2 (5) To deal with a token that includes an unknown character, both puni (cm ) and pbi (cm |cm−1 ) are also calculated by a smoothed language model. A smoothed charact"
P19-1158,D18-2012,0,0.029369,"anguage model to tokenize a sentence. (Goldwater et al., 2006, 2009) proposed unsupervised word segmentation by sampling tokenization and updating a language model with Gibbs sampling. The language model for unsupervised word segmentation is smoothed with base probabilities of words to give a probability for all possible words in a text. (Mochihashi et al., 2009) extended this to the use of blocked Gibbs sampling, which samples tokenization by a sentence. The authors introduced a nested Bayesian language model that calculates a probability of a word by hierarchical language models. Recently, (Kudo and Richardson, 2018) proposed a subword generator for NMT, which tokenizes a sentence stochastically with a subwordlevel language model while (Kudo, 2018) reports improvement in performance of NMT by the idea 1627 of sampling tokenization. Considering multiple subwords makes an NMT model robust against noise and segmentation errors. This differs from BPE in that it does not merge tokens uniquely by its frequency and differs from unsupervised word segmentation with a language model in that it limits subword vocabulary. Our work is similar to this line of research, but we focus on NLP tasks that do not require deco"
P19-1158,P09-1012,0,0.904122,"n sampling. We use a dictionary-based morphological analyzer or unsupervised word segmentation to tokenize a corpus initially, and the language model is initialized with the tokenized corpus. 3.3 Nested Unigram Language Model pbase (t : c1 ...cM ) = puni (c1 ) is given as Sampling Tokenization With the nested unigram language model introduced above, the tokenization of a sentence is sampled from the distribution P (t|s) where t is possible tokenization for the sentence. A probability of tokenization is obtained Q by a nested language model (4) as p(t|s) = t∈t p(t). Following (Kudo, 2018) and (Mochihashi et al., 2009), we employ a dynamic programming (DP) technique called forward filtering backward sampling (FFBS) (Scott, 2002) to sample tokens stochastically. With FFBS, we can sample tokens in a sentence from a distribution considering all possible tokenizations within the limit of the maximum token length l. In the forward calculation of FFBS, a DP Table D is calculated as follows: pbi (cm |cm−1 ) min(i−j,l) m=2 (5) To deal with a token that includes an unknown character, both puni (cm ) and pbi (cm |cm−1 ) are also calculated by a smoothed language model. A smoothed character unigram probability puni (c"
P19-1158,C14-1008,0,0.0476734,"rates a language model for unsupervised tokenization into a text classifier and then trains both models simultaneously. To make the model robust against infrequent tokens, we sampled segmentation for each sentence stochastically during training, which resulted in improved performance of text classification. We conducted experiments on sentiment analysis as a text classification task and show that our method achieves better performance than previous methods. 1 Introduction Tokenization is a fundamental problem in text classification such as sentiment analysis (Tang et al., 2014; Kim, 2014; dos Santos and Gatti, 2014), topic detection (Lai et al., 2015; Zhang et al., 2015), and spam detection (Liu and Jia, 2012; Liu et al., 2016). In text classification with neural networks, sentence representation is calculated based on tokens that compose the sentence. Specifically, a sentence is first tokenized into meaningful units such as characters, words, and subwords (Zhang et al., 2015; Joulin et al., 2017). Then, the token embeddings are looked up and fed into a neural network encoder such as a feed-forward neural network (Iyyer et al., 2015), a convolutional neural network (CNN) (Kim, 2014; Kalchbrenner et al.,"
P19-1158,P16-1162,0,0.458161,"trivial problem in unsegmented languages such as Chinese and Japanese since they have no explicit word boundaries. For these languages, tokenizers based on supervised machine learning with a dictionary (Zhang et al., 2003; Kudo, 2006) have been used to segment a sentence into units (Lai et al., 2015). In addition, we use a neural network-based word segmenter to tokenize a raw corpus in Chinese text classification (Zhou et al., 2016; Zhang and Yang, 2018). In machine translation, subword tokenization with byte pair encoding (BPE) addresses the problem of unknown words and improves performance (Sennrich et al., 2016). However, segmentation is potentially ambiguous, and it is unclear whether preset tokenization offers the best performance for target tasks. To address this problem, in this paper, we propose a new tokenization strategy that segments a sentence stochastically and trains a classification model with various segmentations. During training, our model first segments sentences into tokens stochastically with the language model and then feeds the tokenized sentences into a neural text classifier. The text classifier is trained to decrease the cross-entropy loss for true labels, and the language mode"
P19-1158,P14-1146,0,0.0408455,"these problems. Our model incorporates a language model for unsupervised tokenization into a text classifier and then trains both models simultaneously. To make the model robust against infrequent tokens, we sampled segmentation for each sentence stochastically during training, which resulted in improved performance of text classification. We conducted experiments on sentiment analysis as a text classification task and show that our method achieves better performance than previous methods. 1 Introduction Tokenization is a fundamental problem in text classification such as sentiment analysis (Tang et al., 2014; Kim, 2014; dos Santos and Gatti, 2014), topic detection (Lai et al., 2015; Zhang et al., 2015), and spam detection (Liu and Jia, 2012; Liu et al., 2016). In text classification with neural networks, sentence representation is calculated based on tokens that compose the sentence. Specifically, a sentence is first tokenized into meaningful units such as characters, words, and subwords (Zhang et al., 2015; Joulin et al., 2017). Then, the token embeddings are looked up and fed into a neural network encoder such as a feed-forward neural network (Iyyer et al., 2015), a convolutional neural network"
P19-1158,P16-2037,0,0.0131871,"2015), and spam detection (Liu and Jia, 2012; Liu et al., 2016). In text classification with neural networks, sentence representation is calculated based on tokens that compose the sentence. Specifically, a sentence is first tokenized into meaningful units such as characters, words, and subwords (Zhang et al., 2015; Joulin et al., 2017). Then, the token embeddings are looked up and fed into a neural network encoder such as a feed-forward neural network (Iyyer et al., 2015), a convolutional neural network (CNN) (Kim, 2014; Kalchbrenner et al., 2014), or a long short-term memory (LSTM) network (Wang et al., 2016a,b). For English and other languages that use the Latin alphabet, the whitespace is a good indicator of word segmentation. However, tokenization is a non-trivial problem in unsegmented languages such as Chinese and Japanese since they have no explicit word boundaries. For these languages, tokenizers based on supervised machine learning with a dictionary (Zhang et al., 2003; Kudo, 2006) have been used to segment a sentence into units (Lai et al., 2015). In addition, we use a neural network-based word segmenter to tokenize a raw corpus in Chinese text classification (Zhou et al., 2016; Zhang an"
P19-1158,D16-1058,0,0.0976317,"Missing"
P19-1158,W03-1730,0,0.0873188,"ed up and fed into a neural network encoder such as a feed-forward neural network (Iyyer et al., 2015), a convolutional neural network (CNN) (Kim, 2014; Kalchbrenner et al., 2014), or a long short-term memory (LSTM) network (Wang et al., 2016a,b). For English and other languages that use the Latin alphabet, the whitespace is a good indicator of word segmentation. However, tokenization is a non-trivial problem in unsegmented languages such as Chinese and Japanese since they have no explicit word boundaries. For these languages, tokenizers based on supervised machine learning with a dictionary (Zhang et al., 2003; Kudo, 2006) have been used to segment a sentence into units (Lai et al., 2015). In addition, we use a neural network-based word segmenter to tokenize a raw corpus in Chinese text classification (Zhou et al., 2016; Zhang and Yang, 2018). In machine translation, subword tokenization with byte pair encoding (BPE) addresses the problem of unknown words and improves performance (Sennrich et al., 2016). However, segmentation is potentially ambiguous, and it is unclear whether preset tokenization offers the best performance for target tasks. To address this problem, in this paper, we propose a new"
P19-1158,P18-1144,0,0.213352,"l., 2016a,b). For English and other languages that use the Latin alphabet, the whitespace is a good indicator of word segmentation. However, tokenization is a non-trivial problem in unsegmented languages such as Chinese and Japanese since they have no explicit word boundaries. For these languages, tokenizers based on supervised machine learning with a dictionary (Zhang et al., 2003; Kudo, 2006) have been used to segment a sentence into units (Lai et al., 2015). In addition, we use a neural network-based word segmenter to tokenize a raw corpus in Chinese text classification (Zhou et al., 2016; Zhang and Yang, 2018). In machine translation, subword tokenization with byte pair encoding (BPE) addresses the problem of unknown words and improves performance (Sennrich et al., 2016). However, segmentation is potentially ambiguous, and it is unclear whether preset tokenization offers the best performance for target tasks. To address this problem, in this paper, we propose a new tokenization strategy that segments a sentence stochastically and trains a classification model with various segmentations. During training, our model first segments sentences into tokens stochastically with the language model and then f"
P19-1300,D18-1024,0,0.208267,"Missing"
P19-1300,P18-1073,0,0.320103,"ou et al., 2013) and bilingual named entity recognition (Rudramurthy et al., 2016). It also enables the transfer of knowledge from one language into another (Xiao and Guo, 2014; Adams et al., 2017). A number of supervised and unsupervised methods have been proposed that obtain crosslingual word embeddings. Both supervised and unsupervised methods aim to find such a linear transformation that maps word embeddings in a source language into a target language space. Supervised methods employ bilingual dictionaries to learn the mapping (Mikolov et al., 2013b; Xing et al., 2015; Smith et al., 2017; Artetxe et al., 2018a), while unsupervised ones utilise the similarities or distance of word embeddings spaces across different languages (Conneau et al., 2018; Zhang et al., 2017a; Xu et al., 2018; Artetxe et al., 2018b). Since the common objective of most of the supervised and unsupervised methods is to find an orthogonal linear mapping between languages, they heavily rely on the assumption that monolingual word embeddings are approximately isomorphic. However, Søgaard et al. (2018) have found that this assumption does not hold true in general, and demonstrated that it requires three specific conditions for the"
P19-1300,2005.mtsummit-papers.11,0,0.0708223,"ween multiple minor languages, or between minor and major languages. For the different-domain condition, we added {Tamil, Turkish}-Japanese pairs and North Saami-{Finnish, English} pairs to the seven pairs described above. North Saami is one of the minor languages spoken in northern Finland, Sweden and Norway, and it is so close to Finnish that transfer learning between them is very effective in dependency parsing (Lim et al., 2018). Note that the basic word order of Tamil, Turkish and Japanese is subject-object-verb (SOV), while the one of the other languages is SVO. We used Europarl corpus (Koehn, 2005) for English, Wikipedia for Japanese, SIKOR North Saami corpus4 for North Saami, and news data for the other languages5 . We extracted 1M sentences 3 downloaded from http://www.statmt.org and http://wortschatz.uni-leipzig.de/en/ download 4 https://dataverse.no/dataset.xhtml? persistentId=doi:10.18710/8AK7KZ 5 The vocabulary sizes of Europarl and News Crawl corpora in English are significantly different (79,258 v.s. 265,368 words), indicating the major differences between these domains 3116 from these corpora except for North Saami, for which we used the whole corpus which contains 0.75M senten"
P19-1300,L18-1352,0,0.127454,"e languages, and either 50k or 1M sentences for the target language (i.e., English). This condition simulates two realistic scenarios; the case when analysing inter-dependencies between multiple minor languages, or between minor and major languages. For the different-domain condition, we added {Tamil, Turkish}-Japanese pairs and North Saami-{Finnish, English} pairs to the seven pairs described above. North Saami is one of the minor languages spoken in northern Finland, Sweden and Norway, and it is so close to Finnish that transfer learning between them is very effective in dependency parsing (Lim et al., 2018). Note that the basic word order of Tamil, Turkish and Japanese is subject-object-verb (SOV), while the one of the other languages is SVO. We used Europarl corpus (Koehn, 2005) for English, Wikipedia for Japanese, SIKOR North Saami corpus4 for North Saami, and news data for the other languages5 . We extracted 1M sentences 3 downloaded from http://www.statmt.org and http://wortschatz.uni-leipzig.de/en/ download 4 https://dataverse.no/dataset.xhtml? persistentId=doi:10.18710/8AK7KZ 5 The vocabulary sizes of Europarl and News Crawl corpora in English are significantly different (79,258 v.s. 265,3"
P19-1300,N18-1202,0,0.0218386,"s to align are distant or monolingual corpora are not comparable across language. Recently, Chen and Cardie (2018) and Alaux et al. (2018) have proposed un3114 supervised multilingual word embedding methods. Their methods map word embeddings of more than two languages into a common space by capturing the inter-dependencies among multiple languages. 3 Our Model 3.1 Overview We propose a new unsupervised multilingual word embeddings method called Multilingual Neural Language Model. Fig.1 briefly illustrates our proposed model. The model consists of bidirectional language models similar to ELMo (Peters et al., 2018), and most of the parameters are shared among multiple languages. In what follows, we summaries which parameters are shared across languages or specific to each language: • Shared Parameters − → ← − – f and f : LSTM networks which perform as forward and backward language models, independently. – E BOSfwd and E BOSbkw : The embeddings of initial inputs to the forward and backward language models, respectively. – W EOS : The linear mapping for &lt;EOS&gt;, which is used to calculate the probability of the end of a sentence at every time-step. • Specific Parameters to Language ` – E ` : Word embeddings"
P19-1300,P18-1072,0,0.0653253,"Missing"
P19-1300,W14-1613,0,0.0244823,"languages. Our code is publicly available1 . 1 Introduction Learning cross-lingual or multilingual word embedding has been recognised as a very important research topic in natural language processing 1 https://github.com/twadada/multilingual-nlm (NLP). Its objective is to map monolingual word embeddings of different languages into a common space, and this research has been applied to many multilingual tasks such as machine translation (Zou et al., 2013) and bilingual named entity recognition (Rudramurthy et al., 2016). It also enables the transfer of knowledge from one language into another (Xiao and Guo, 2014; Adams et al., 2017). A number of supervised and unsupervised methods have been proposed that obtain crosslingual word embeddings. Both supervised and unsupervised methods aim to find such a linear transformation that maps word embeddings in a source language into a target language space. Supervised methods employ bilingual dictionaries to learn the mapping (Mikolov et al., 2013b; Xing et al., 2015; Smith et al., 2017; Artetxe et al., 2018a), while unsupervised ones utilise the similarities or distance of word embeddings spaces across different languages (Conneau et al., 2018; Zhang et al., 2"
P19-1300,N15-1104,0,0.367982,"Missing"
P19-1300,D18-1268,0,0.179307,"s et al., 2017). A number of supervised and unsupervised methods have been proposed that obtain crosslingual word embeddings. Both supervised and unsupervised methods aim to find such a linear transformation that maps word embeddings in a source language into a target language space. Supervised methods employ bilingual dictionaries to learn the mapping (Mikolov et al., 2013b; Xing et al., 2015; Smith et al., 2017; Artetxe et al., 2018a), while unsupervised ones utilise the similarities or distance of word embeddings spaces across different languages (Conneau et al., 2018; Zhang et al., 2017a; Xu et al., 2018; Artetxe et al., 2018b). Since the common objective of most of the supervised and unsupervised methods is to find an orthogonal linear mapping between languages, they heavily rely on the assumption that monolingual word embeddings are approximately isomorphic. However, Søgaard et al. (2018) have found that this assumption does not hold true in general, and demonstrated that it requires three specific conditions for the unsupervised method of Conneau et al. (2018) to perform well. The conditions are: Languages to align are linguistically similar; Monolingual word embeddings are trained by the"
P19-1300,P17-1179,0,0.196181,"o and Guo, 2014; Adams et al., 2017). A number of supervised and unsupervised methods have been proposed that obtain crosslingual word embeddings. Both supervised and unsupervised methods aim to find such a linear transformation that maps word embeddings in a source language into a target language space. Supervised methods employ bilingual dictionaries to learn the mapping (Mikolov et al., 2013b; Xing et al., 2015; Smith et al., 2017; Artetxe et al., 2018a), while unsupervised ones utilise the similarities or distance of word embeddings spaces across different languages (Conneau et al., 2018; Zhang et al., 2017a; Xu et al., 2018; Artetxe et al., 2018b). Since the common objective of most of the supervised and unsupervised methods is to find an orthogonal linear mapping between languages, they heavily rely on the assumption that monolingual word embeddings are approximately isomorphic. However, Søgaard et al. (2018) have found that this assumption does not hold true in general, and demonstrated that it requires three specific conditions for the unsupervised method of Conneau et al. (2018) to perform well. The conditions are: Languages to align are linguistically similar; Monolingual word embeddings a"
P19-1300,D17-1207,0,0.12748,"o and Guo, 2014; Adams et al., 2017). A number of supervised and unsupervised methods have been proposed that obtain crosslingual word embeddings. Both supervised and unsupervised methods aim to find such a linear transformation that maps word embeddings in a source language into a target language space. Supervised methods employ bilingual dictionaries to learn the mapping (Mikolov et al., 2013b; Xing et al., 2015; Smith et al., 2017; Artetxe et al., 2018a), while unsupervised ones utilise the similarities or distance of word embeddings spaces across different languages (Conneau et al., 2018; Zhang et al., 2017a; Xu et al., 2018; Artetxe et al., 2018b). Since the common objective of most of the supervised and unsupervised methods is to find an orthogonal linear mapping between languages, they heavily rely on the assumption that monolingual word embeddings are approximately isomorphic. However, Søgaard et al. (2018) have found that this assumption does not hold true in general, and demonstrated that it requires three specific conditions for the unsupervised method of Conneau et al. (2018) to perform well. The conditions are: Languages to align are linguistically similar; Monolingual word embeddings a"
P19-1300,D13-1141,0,0.0489722,"ed and even supervised methods trained with 500 bilingual pairs of words. Our model also outperforms unsupervised methods given different-domain corpora across languages. Our code is publicly available1 . 1 Introduction Learning cross-lingual or multilingual word embedding has been recognised as a very important research topic in natural language processing 1 https://github.com/twadada/multilingual-nlm (NLP). Its objective is to map monolingual word embeddings of different languages into a common space, and this research has been applied to many multilingual tasks such as machine translation (Zou et al., 2013) and bilingual named entity recognition (Rudramurthy et al., 2016). It also enables the transfer of knowledge from one language into another (Xiao and Guo, 2014; Adams et al., 2017). A number of supervised and unsupervised methods have been proposed that obtain crosslingual word embeddings. Both supervised and unsupervised methods aim to find such a linear transformation that maps word embeddings in a source language into a target language space. Supervised methods employ bilingual dictionaries to learn the mapping (Mikolov et al., 2013b; Xing et al., 2015; Smith et al., 2017; Artetxe et al.,"
P93-1004,J90-2002,0,0.0344871,"Missing"
P93-1004,P91-1023,0,0.0926974,"Missing"
P93-1004,P87-1033,0,0.0198529,"Missing"
P93-1004,J93-1004,0,\N,Missing
P93-1004,C92-2088,1,\N,Missing
P93-1004,C90-3031,0,\N,Missing
P93-1004,C92-2101,0,\N,Missing
P93-1004,P91-1022,0,\N,Missing
P93-1004,P91-1017,0,\N,Missing
P93-1004,H91-1026,0,\N,Missing
P97-1030,H92-1022,0,0.0776694,"Missing"
P97-1030,1993.eamt-1.1,0,0.0271559,"oo general to selectively detect exceptions. 6 Related Work Although statistical natural language processing has mainly focused on Maximum Likelihood Estimators, (Pereira et al., 1995) proposed a mixture approach to predict next words by using the Context Tree Weighting (CTW) method .(Willems et al., 1995). The C T W method computes probability by mixing subtrees in a single context tree in Bayesian fashion. Although the method is very efficient, it cannot be used to construct hierarchical tag context trees. Various kinds of re-sampling techniques have been studied in statistics (Efron, 1979; Efron and Tibshirani, 1993) and machine learning (Breiman, 1996; Hull et al., 1996; Freund and Schapire, 1996a). In particular, the mistake-driven mixture algorithm 235 97 i i i mixtureof biKjrarns .emixtureof contexttrees -+--- .-"" f................ 95 ................... •2.j , "" . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . I g94 F_ 93' 92 91 I I 3 Numberof Mixture 4 90 Figure 4: Context Tree Mixture v.s. Bi-gram Mixture mistake-driven mixture method is extremely effective for 1. incorporating exceptional connections and 2. avoiding data over-fitting"
P97-1030,C94-1032,0,0.225404,"Missing"
P97-1030,W95-0108,0,0.0851287,"Missing"
P97-1030,P94-1025,0,0.0860472,"h and Telephone Corporation. 2In P(w,]t,) = P(,,,)P(,,lu,,) P(t,) ' P ( w i ) cannot be considered to be identical for ~ll segmentations. 1 Introduction 230 tag models is the set of collocational sequences of words that cannot be captured by just their tags. Because the maximal likelihood estimator (MLE) emphasizes the most frequent connections, an exceptional connection is placed in the same class as a frequent connection. To tackle this problem, we introduce a new tag model based on the mistake-driven mixture of hierarchical tag context trees. Compared to Schiitze and Singer's context tree (Schiitze and Singer, 1994), the hierarchical tag context tree is extended in that the context is represented by a hierarchical tag set (i.e.,NTT < proper noun < noun). This is extremely useful in capturing exceptional connections that can be detected only at the word level. To make the best use of the hierarchical context tree, the mistake-driven mixture method imitates the process in which linguists incorporate exceptional connections into hand-crafted rules: They first construct coarse rules which seems to cover broad range of data. They then try to analyze data by using the rules and extract exceptions that the rule"
P97-1030,A88-1019,0,\N,Missing
P97-1045,J88-2003,0,\N,Missing
P97-1045,E91-1039,0,\N,Missing
P97-1045,J88-2006,0,\N,Missing
P97-1045,J88-2005,0,\N,Missing
P97-1045,P92-1033,0,\N,Missing
P98-2163,C94-2192,0,0.0295444,"Missing"
P98-2163,J86-3001,0,0.0299416,"owledge is crucial to our recognition of the coherence relations. In (Hovy and Maier, 1993), they classified the Concession relation as interpersonal (i.e., a u t h o r - a n d / o r addresseerelated) rather than ideational (i.e., semantic), since they defined it as &quot;one of the text segments raises expectations which are contradicted/violated by the other.&quot; The use of interpersonal relations is predicated mainly on the interests, beliefs, and attitudes of addressee a n d / o r author. To deal with this problem, we must incorporate the notion of intentional structure and focus space structure (Grosz and Sidner, 1986). Since we have focused on te-linkage in this paper, we need not to consider how clauses are combined. However, to detect the discourse structure, we need to extend the method so as to deal with the relations between sentences. We must estimate some kind of reliable scores among possible segments and choose the relation having the m a x i m u m score (Kurohashi and Nagao, 1994). These issues remain to be studied in the future. 6 Summary Since the semantic relations exhibited by re-linkage vary so diversely, it has been claimed t h a t the interpreter must infer the intended relationship on the"
P98-2163,C94-2183,0,0.0136588,"ersonal relations is predicated mainly on the interests, beliefs, and attitudes of addressee a n d / o r author. To deal with this problem, we must incorporate the notion of intentional structure and focus space structure (Grosz and Sidner, 1986). Since we have focused on te-linkage in this paper, we need not to consider how clauses are combined. However, to detect the discourse structure, we need to extend the method so as to deal with the relations between sentences. We must estimate some kind of reliable scores among possible segments and choose the relation having the m a x i m u m score (Kurohashi and Nagao, 1994). These issues remain to be studied in the future. 6 Summary Since the semantic relations exhibited by re-linkage vary so diversely, it has been claimed t h a t the interpreter must infer the intended relationship on the basis of extralinguistic knowledge. T h e particulars of individual common sense knowledge are crucial to understanding any discourse (Hobbs et al., 1993; Asher and Lascarides, 1995). Nevertheless, one can, through the use of the relevant structures of events, eliminate a very large number of rules for calculating the plausible relations. Although we have concentrated on re-li"
P98-2163,J92-4007,0,0.0426958,"Missing"
P98-2214,J96-1002,0,0.0117505,"mited to only one argument. Li and Abe (1996) also studied a method for learning dependencies between case slots and reported that dependencies were discovered only at the slotlevel and not at the class-level. Compared with these previous works, this paper proposes to consider the above two issues in a uniform way. First, we introduce a model of generating a collocation of a verb and argum e n t / a d j u n c t nouns (section 2) and then view the model as a probability model (section 3). As a model learning method, we adopt the maximum entropy model learning method (Della Pietra et al., 1997; Berger et al., 1996). Case dependencies and noun class generalization are represented as features in the maximum entropy approach. Features are allowed to have overlap and this is quite advantageous when we consider case dependencies and noun class generalization in parameter estimation. An optimal model is selected by searching for an optimal set of features, i.e, optimal case dependencies and optimal noun class generalization levels. As the feature selection process, this paper proposes a new feature selection algorithm which starts from the most general model and gradually examines more specific models (sectio"
P98-2214,P96-1025,0,0.0273121,"ximum entropy modeling method. We also propose a new model selection algorithm which starts from the most general model and gradually examines more specific models. In the experimental evaluation, it is shown that both of the case dependencies and specific sense restriction selected by the proposed method contribute to improving the performance in subcategorization preference resolution. 1 Introduction In empirical approaches to parsing, lexical/semantic collocation extracted from corpus has been proved to be quite useful for ranking parses in syntactic analysis. For example, Magerman (1995), Collins (1996), and Charniak (1997) proposed statistical parsing models which incorporated lexical/semantic information. In their models, syntactic and lexical/semantic features are dependent on each other and are combined together. This paper also proposes a method of utilizing lexical/semantic features for the purpose of applying them to ranking parses in syntactic analysis. However, unlike the models of Magerman (1995), Collins (1996), and Charniak (1997), we assume that syntactic and lexical/semantic features are independent. Then, we focus on extracting lexical/semantic collocational knowledge of verbs"
P98-2214,C96-1004,0,0.018434,", 1998. An extended version of this paper is available from the above URL. 1314 dependent of other cases. When considering 2), we have to decide which superordinate class generates each observed leaf class in the verb-noun collocation. So far, there exist several works which worked on these two issues in learning collocational knowledge of verbs and also evaluated the results in terms of syntactic disambiguation. Resnik (1993) and Li and Abe (1995) studied how to find an optimal abstraction level of an argument noun in a tree-structured thesaurus. Their works are limited to only one argument. Li and Abe (1996) also studied a method for learning dependencies between case slots and reported that dependencies were discovered only at the slotlevel and not at the class-level. Compared with these previous works, this paper proposes to consider the above two issues in a uniform way. First, we introduce a model of generating a collocation of a verb and argum e n t / a d j u n c t nouns (section 2) and then view the model as a probability model (section 3). As a model learning method, we adopt the maximum entropy model learning method (Della Pietra et al., 1997; Berger et al., 1996). Case dependencies and n"
P98-2214,P95-1037,0,0.0117733,"employing the maximum entropy modeling method. We also propose a new model selection algorithm which starts from the most general model and gradually examines more specific models. In the experimental evaluation, it is shown that both of the case dependencies and specific sense restriction selected by the proposed method contribute to improving the performance in subcategorization preference resolution. 1 Introduction In empirical approaches to parsing, lexical/semantic collocation extracted from corpus has been proved to be quite useful for ranking parses in syntactic analysis. For example, Magerman (1995), Collins (1996), and Charniak (1997) proposed statistical parsing models which incorporated lexical/semantic information. In their models, syntactic and lexical/semantic features are dependent on each other and are combined together. This paper also proposes a method of utilizing lexical/semantic features for the purpose of applying them to ranking parses in syntactic analysis. However, unlike the models of Magerman (1995), Collins (1996), and Charniak (1997), we assume that syntactic and lexical/semantic features are independent. Then, we focus on extracting lexical/semantic collocational kn"
P98-2214,H93-1054,0,0.0333755,"re optional and in* This research was partially supported by the Ministry of Education, Science, Sports and Culture, Japan, Grantin-Aid for Encouragement of Young Scientists, 09780338, 1998. An extended version of this paper is available from the above URL. 1314 dependent of other cases. When considering 2), we have to decide which superordinate class generates each observed leaf class in the verb-noun collocation. So far, there exist several works which worked on these two issues in learning collocational knowledge of verbs and also evaluated the results in terms of syntactic disambiguation. Resnik (1993) and Li and Abe (1995) studied how to find an optimal abstraction level of an argument noun in a tree-structured thesaurus. Their works are limited to only one argument. Li and Abe (1996) also studied a method for learning dependencies between case slots and reported that dependencies were discovered only at the slotlevel and not at the class-level. Compared with these previous works, this paper proposes to consider the above two issues in a uniform way. First, we introduce a model of generating a collocation of a verb and argum e n t / a d j u n c t nouns (section 2) and then view the model a"
P98-2214,A97-1053,1,0.561477,"Missing"
P98-2214,J98-2002,0,\N,Missing
pereira-etal-2014-collocation,J96-1001,0,\N,Missing
pereira-etal-2014-collocation,W12-3311,0,\N,Missing
pereira-etal-2014-collocation,J90-1003,0,\N,Missing
pereira-etal-2014-collocation,W02-2016,1,\N,Missing
pereira-etal-2014-collocation,P07-2045,0,\N,Missing
pereira-etal-2014-collocation,N03-1017,0,\N,Missing
Q13-1012,W06-2922,0,0.0263674,"Treebank data (CTB5): evaluations are performed without punctuations. 7 Related Works 7.1 How to Handle Spurious Ambiguity The graph-based approach employs Eisner and Satta (1999)’s algorithm where spurious ambiguities are eliminated by the notion of split head automaton grammars (Alshawi, 1996). However, the arc-standard transition-based parser has the spurious ambiguity problem. Cohen et al. (2012) proposed a method to eliminate the spurious ambiguity of shift-reduce transition systems. Their method covers existing systems such as the arcstandard and non-projective transition-based parsers (Attardi, 2006). Our system copes only with the projective case, but is simpler than theirs and we show its efficacy empirically through some experiments. The arc-eager shift-reduce parser also has a spurious ambiguity problem. Goldberg and Nivre (2012) addressed this problem by not only training with a canonical transition sequence but also with alternate optimal transitions that are calculated dynamically for a current state. 7.2 Methods to Improve Dependency Parsing Higher-order features like third-order dependency relations are essential to improve dependency parsing accuracy (Koo and Collins, 2010; Rush"
Q13-1012,E12-1009,0,0.231597,", s′d |s′d−1 |. . . |s′1 |s′0 ) : π ′ ℓ : (j, k, sd |sd−1 |. . . |s1 |s0 ) : π ′ s0 .h.w = w0 ∧ p ∈ π ℓ + 1 : (i, k, s′d |s′d−1 |. . . |s′1 |s′0 ↶ s0 ) : π ′ }| { z }| { : (i, j, s′d |s′d−1 |. . . |s′1 |s′0 ) : π ′ ℓ : (j, k, sd |sd−1 |. . . |s1 |s0 ) : π p∈π ℓ + 1 : (i, k, s′d |s′d−1 |. . . |s′1 |s′0 ↷ s0 ) : π ′ Figure 1: The arc-standard transition-based dependency parsing system with dynamic programming: anything”. a↷ b denotes that a tree b is attached to a tree a. rated in standard graph-based models. • In contrast to joint transition-based/graphbased approaches (Zhang and Clark, 2008; Bohnet and Kuhn, 2012) which require a large beam size and make dynamic programming impractical, our two-stage approach can integrate both models with little loss of efficiency. In addition, the elimination of spurious ambiguity from the arc-standard shift-reduce parser improves the efficiency and accuracy of our approach. 2 Arc-Standard Shift-Reduce Parsing We use a beam search shift-reduce parser with dynamic programming as our baseline system. Figure 1 shows it as a deductive system (Shieber et al., 1995). A state is defined as the following: ℓ : (i, j, sd |sd−1 |. . . |s1 |s0 ) : π where ℓ is the step size, [i,"
Q13-1012,P05-1022,0,0.326962,"has a spurious ambiguity problem. Goldberg and Nivre (2012) addressed this problem by not only training with a canonical transition sequence but also with alternate optimal transitions that are calculated dynamically for a current state. 7.2 Methods to Improve Dependency Parsing Higher-order features like third-order dependency relations are essential to improve dependency parsing accuracy (Koo and Collins, 2010; Rush and Petrov, 2012; Zhang and McDonald, 2012). A reranking approach is one effective solution to introduce rich features to a parser model in the context of constituency parsing (Charniak and Johnson, 2005; Huang, 2008). Hall (2007) applied a k-best maximum spanning tree algorithm to non-projective dependency analysis, and showed that k-best discriminative reranking improves parsing accuracy in several languages. Sangati et al. (2009) proposed a k-best dependency reranking algorithm using a third-order generative model, and Hayashi et al. (2011) extended it to a 148 forest algorithm. Though forest reranking requires some approximations such as cube-pruning to integrate non-local features, it can explore larger search space than k-best reranking. The stacking approach (Nivre and McDonald, 2008;"
Q13-1012,P04-1015,0,0.0437676,") 0.017 92.6 (93.6) 0.018 92.7 (93.3) 0.017 92.9 (93.5) 0.018 32 92.6 (93.6) 0.03 92.6 (93.6) 0.03 92.7 (93.3) 0.03 92.9 (93.5) 0.03 64 92.6 (93.6) 0.06 92.6 (93.6) 0.07 92.8 (93.3) 0.06 92.9 (93.5) 0.06 128 92.6 (93.6) 0.13 92.6 (93.6) 0.13 92.8 (93.3) 0.13 92.9 (93.5) 0.13 Table 2: Unlabeled accuracy scores (UAS) and parsing times (+forest dumping times, second per sentence) for parsing development (WSJ22) and test (WSJ23) data with spurious shift-reduce and proposed shift-reduce parser (non-sp.) using several beam sizes. We used an early update version of the averaged perceptron algorithm (Collins and Roark, 2004; Huang et al., 2012) to train two shift-reduce dependency parsers with beam size of 12. Table 2 shows experimental results of parsing the development and test datasets with each of the spurious and non-spurious shift-reduce parsers using several beam sizes. Parsing accuracies were evaluated by unlabeled accuracy scores (UAS) with and without punctuations. The parsing times were measured on an Intel Core i7 2.8GHz. The average cpu time (per sentence) includes that of dumping packed forests. This result indicates that the non-spurious parser achieves better accuracies than the spurious 142 beam"
Q13-1012,P99-1059,0,0.0899964,"nd nonspurious parsers to 12, and the number of perceptron training iterations to 25 for the parsers and to 8 for both rerankers. Table 14 shows the results for the test sets. As we expected, reranking on non-spurious forests outperforms that on spurious forests. system sr (12) w/ non-local (12, k=3) non-sp sr (12) w/ non-local (12, k=3) UAS 85.3 85.8 85.3 85.9 root 78.6 79.4 78.4 79.6 comp. 33.4 34.2 33.7 34.3 Table 14: Results on Chinese Treebank data (CTB5): evaluations are performed without punctuations. 7 Related Works 7.1 How to Handle Spurious Ambiguity The graph-based approach employs Eisner and Satta (1999)’s algorithm where spurious ambiguities are eliminated by the notion of split head automaton grammars (Alshawi, 1996). However, the arc-standard transition-based parser has the spurious ambiguity problem. Cohen et al. (2012) proposed a method to eliminate the spurious ambiguity of shift-reduce transition systems. Their method covers existing systems such as the arcstandard and non-projective transition-based parsers (Attardi, 2006). Our system copes only with the projective case, but is simpler than theirs and we show its efficacy empirically through some experiments. The arc-eager shift-reduc"
Q13-1012,1997.iwpt-1.10,0,0.24187,"encoded into the vertices. In the example, information about the topmost stack element is attached to the corresponding vertex marked with a non-terminal symbol X. Weights are omitted in the example. In practice, we attach each reduction weight to the corresponding hyperedge, and add the shift weight to the reduction weight when a shifted word is reduced. 3 Arc-Standard Shift-Reduce Parsing without Spurious Ambiguity One solution to remove spurious ambiguity in the arc-standard transition system is to give priority to the construction of left arcs over that of right arcs (or vice versa) like Eisner (1997). For example, an Earley dependency parser (Hayashi et al., 2012) attaches all left dependents to a word before right dependents. The parser uses a scan action to stop the construction of left arcs. We apply this idea to the arc-standard transition system and show the resulting transition system in Figure 3. We introduce the ∗ symbol to indicate that 141 the root node of the topmost element on the stack has not been scanned yet. The shift and reduce↷ actions can be used only when the root of the topmost element on the stack has already been scanned, and all left arcs are always attached to the"
Q13-1012,C12-1059,0,0.0183457,"inated by the notion of split head automaton grammars (Alshawi, 1996). However, the arc-standard transition-based parser has the spurious ambiguity problem. Cohen et al. (2012) proposed a method to eliminate the spurious ambiguity of shift-reduce transition systems. Their method covers existing systems such as the arcstandard and non-projective transition-based parsers (Attardi, 2006). Our system copes only with the projective case, but is simpler than theirs and we show its efficacy empirically through some experiments. The arc-eager shift-reduce parser also has a spurious ambiguity problem. Goldberg and Nivre (2012) addressed this problem by not only training with a canonical transition sequence but also with alternate optimal transitions that are calculated dynamically for a current state. 7.2 Methods to Improve Dependency Parsing Higher-order features like third-order dependency relations are essential to improve dependency parsing accuracy (Koo and Collins, 2010; Rush and Petrov, 2012; Zhang and McDonald, 2012). A reranking approach is one effective solution to introduce rich features to a parser model in the context of constituency parsing (Charniak and Johnson, 2005; Huang, 2008). Hall (2007) applie"
Q13-1012,P07-1050,0,0.0400013,"g and Nivre (2012) addressed this problem by not only training with a canonical transition sequence but also with alternate optimal transitions that are calculated dynamically for a current state. 7.2 Methods to Improve Dependency Parsing Higher-order features like third-order dependency relations are essential to improve dependency parsing accuracy (Koo and Collins, 2010; Rush and Petrov, 2012; Zhang and McDonald, 2012). A reranking approach is one effective solution to introduce rich features to a parser model in the context of constituency parsing (Charniak and Johnson, 2005; Huang, 2008). Hall (2007) applied a k-best maximum spanning tree algorithm to non-projective dependency analysis, and showed that k-best discriminative reranking improves parsing accuracy in several languages. Sangati et al. (2009) proposed a k-best dependency reranking algorithm using a third-order generative model, and Hayashi et al. (2011) extended it to a 148 forest algorithm. Though forest reranking requires some approximations such as cube-pruning to integrate non-local features, it can explore larger search space than k-best reranking. The stacking approach (Nivre and McDonald, 2008; Martins et al., 2008) uses"
Q13-1012,D11-1137,1,0.944552,"ined with each of its word and POS tag. When using non-local features, we removed the local grand-child features from the model. 5.3 Oracle for Discriminative Training A discriminative reranking model is trained on packed forests by using their oracle trees as the correct parse. More accurate oracles are essential to train a discriminative reranking model well. While large size forests have much more accurate oracles than small size forests, large forests have too many hyperedges to train a discriminative model on them, as shown in Figure 4. The usual forest reranking algorithms (Huang, 2008; Hayashi et al., 2011) 4 If each item is augmented with richer information, even features based on the entire subtree can be defined. 144 6.1 Experimental Setting Following (Huang, 2008), the training set (WSJ0221) is split into 20 folds, and each fold is parsed by each of the spurious and non-spurious shift-reduce parsers using beam size 12 with the model trained on sentences from the remaining 19 folds, dumping the outputs as packed forests. The reranker is modeled by either equation (1) or (4). By our preliminary experiments using development data (WSJ22), we modeled the reranker with equation (1) when training,"
Q13-1012,P12-1069,1,0.803509,"bout the topmost stack element is attached to the corresponding vertex marked with a non-terminal symbol X. Weights are omitted in the example. In practice, we attach each reduction weight to the corresponding hyperedge, and add the shift weight to the reduction weight when a shifted word is reduced. 3 Arc-Standard Shift-Reduce Parsing without Spurious Ambiguity One solution to remove spurious ambiguity in the arc-standard transition system is to give priority to the construction of left arcs over that of right arcs (or vice versa) like Eisner (1997). For example, an Earley dependency parser (Hayashi et al., 2012) attaches all left dependents to a word before right dependents. The parser uses a scan action to stop the construction of left arcs. We apply this idea to the arc-standard transition system and show the resulting transition system in Figure 3. We introduce the ∗ symbol to indicate that 141 the root node of the topmost element on the stack has not been scanned yet. The shift and reduce↷ actions can be used only when the root of the topmost element on the stack has already been scanned, and all left arcs are always attached to the head before the head is scanned. The arc-standard shift-reduce p"
Q13-1012,P07-1019,0,0.0218484,"is provided to the second as guide features. In particular, they used a transition-based parser for the first stage and a graph-based parser for the second stage. The main drawback of this approach is that the efficiency of the transition-based parser is sacrificed because the second-stage employs full parsing. This paper proposes an efficient stacked parsing method through discriminative reranking with higher-order graph-based features, which works on the forests output by the first-stage dynamic programming shift-reduce parser and integrates nonlocal features efficiently with cube-pruning (Huang and Chiang, 2007). The advantages of our method are as follows: • Unlike the conventional stacking approach, the first-stage shift-reduce parser prunes the search space of the second-stage graph-based parser. • In addition to guide features, the second-stage graph-based parser can employ the scores of the first-stage parser which cannot be incorpo139 Transactions of the Association for Computational Linguistics, 1 (2013) 139–150. Action Editor: Joakim Nivre. c Submitted 12/2012; Revised 3/2013; Published 5/2013. 2013 Association for Computational Linguistics. axiom(c0 ) : goal(c2n ) : shift : 0 : (0, 1, w0 ) :"
Q13-1012,P10-1110,0,0.657622,"length n. Recently, some pruning techniques have been proposed to improve the efficiency of third-order models (Rush and Petrov, 2012; Zhang and McDonald, 2012). The transition-based approach usually employs the shift-reduce parsing algorithm with linear-time complexity (Nivre, 2008). It greedily chooses the transition with the highest score and the resulting transition sequence is not always globally optimal. The beam search algorithm improves parsing flexibility in deterministic parsing (Zhang and Clark, 2008; Zhang and Nivre, 2011), and dynamic programming makes beam search more efficient (Huang and Sagae, 2010). There is also an alternative approach that integrates graph-based and transition-based models (Sagae and Lavie, 2006; Zhang and Clark, 2008; Nivre and McDonald, 2008; Martins et al., 2008). Martins et al. (2008) formulated their approach as stacking of parsers where the output of the first-stage parser is provided to the second as guide features. In particular, they used a transition-based parser for the first stage and a graph-based parser for the second stage. The main drawback of this approach is that the efficiency of the transition-based parser is sacrificed because the second-stage emp"
Q13-1012,N12-1015,0,0.024897,"Missing"
Q13-1012,P08-1067,0,0.505409,"est Viterbi algorithm, it can calculate the non-local scores efficiently. The baseline score can be taken into the reranker as a linear interpolation: oracle unlabeled accuracy 0.995 ""kbest"" ""forest"" ""non-sp-kbest"" ""non-sp-forest"" 0.99 beam 64 0.985 0.98 beam 16 beam 64 0.975 0.97 0.965 yˆ = argmax β · sctr (x, y) + α · fg (x, y) 0.96 0 500 1000 1500 2000 ave. # of hyperedges 2500 3000 Figure 4: Each plot shows oracle unlabeled accuracies of spurious k-best lists, spurious forests, and non-spurious forests. The oracle accuracies are evaluated using UAS with punctuations. est oracle algorithm (Huang, 2008). Both forests produce much better results than the k-best lists, and non-spurious forests have almost the same oracle accuracies as spurious forests. However, as shown in Table 3, spurious forests encode a number of non-unique dependency trees while all dependency trees in non-spurious forests are distinct from each other. 5 (4) y∈H Forest Reranking 5.1 Discriminative Reranking Model We define a reranking model based on the graphbased features as the following: yˆ = argmax α · fg (x, y) (1) y∈H where α is a weight vector, fg is a feature vector (g indicates “graph-based”), x is the input sent"
Q13-1012,W01-1812,0,0.0493449,"distinct from each other. 5 (4) y∈H Forest Reranking 5.1 Discriminative Reranking Model We define a reranking model based on the graphbased features as the following: yˆ = argmax α · fg (x, y) (1) y∈H where α is a weight vector, fg is a feature vector (g indicates “graph-based”), x is the input sentence, y is a dependency tree and H is a dependency forest. This model assumes a hyperedge factorization which induces a decomposition of the feature vector as the following: ∑ α · fg (x, y) = α · fg,e (e). (2) e∈y The search problem can be solved by simply using the (generalized) Viterbi algorithm (Klein and Manning, 2001). When using non-local features, the hyperedge factorization is redefined to the following: ∑ α · fg (x, y) = α · fg,e (e) + α · fg,e,N (e) (3) e∈y 143 where sctr is the score from the baseline parser (tr indicates “transition-based”), and β is a scaling factor. 5.2 Features for Discriminative Model 5.2.1 Local Features While the inference algorithm is a simple Viterbi algorithm, the discriminative model can use all trisibling features and some grand-sibling features2 (Koo and Collins, 2010) as a local scoring factor in addition to the first- and sibling second-order graphbased features. This"
Q13-1012,P10-1001,0,0.569407,",e (e). (2) e∈y The search problem can be solved by simply using the (generalized) Viterbi algorithm (Klein and Manning, 2001). When using non-local features, the hyperedge factorization is redefined to the following: ∑ α · fg (x, y) = α · fg,e (e) + α · fg,e,N (e) (3) e∈y 143 where sctr is the score from the baseline parser (tr indicates “transition-based”), and β is a scaling factor. 5.2 Features for Discriminative Model 5.2.1 Local Features While the inference algorithm is a simple Viterbi algorithm, the discriminative model can use all trisibling features and some grand-sibling features2 (Koo and Collins, 2010) as a local scoring factor in addition to the first- and sibling second-order graphbased features. This is because the first stage shiftreduce parser uses features described in Section 2 and this information can be encoded into vertices of a hypergraph. The reranking model also uses guide features extracted from the 1-best tree predicted by the first stage shift-reduce parser. We define the guide features as first-order relations like those used in Nivre and McDonald (2008) though our parser handles only unlabeled and projective dependency structures. We summarize the features for discriminati"
Q13-1012,P11-1068,0,0.0346683,"Missing"
Q13-1012,D08-1017,0,0.297077,"usually employs the shift-reduce parsing algorithm with linear-time complexity (Nivre, 2008). It greedily chooses the transition with the highest score and the resulting transition sequence is not always globally optimal. The beam search algorithm improves parsing flexibility in deterministic parsing (Zhang and Clark, 2008; Zhang and Nivre, 2011), and dynamic programming makes beam search more efficient (Huang and Sagae, 2010). There is also an alternative approach that integrates graph-based and transition-based models (Sagae and Lavie, 2006; Zhang and Clark, 2008; Nivre and McDonald, 2008; Martins et al., 2008). Martins et al. (2008) formulated their approach as stacking of parsers where the output of the first-stage parser is provided to the second as guide features. In particular, they used a transition-based parser for the first stage and a graph-based parser for the second stage. The main drawback of this approach is that the efficiency of the transition-based parser is sacrificed because the second-stage employs full parsing. This paper proposes an efficient stacked parsing method through discriminative reranking with higher-order graph-based features, which works on the forests output by the f"
Q13-1012,P05-1012,0,0.139596,"features in addition to third-order graph-based features. To improve efficiency and accuracy, this paper also proposes a novel shift-reduce parser that eliminates the spurious ambiguity of arcstandard transition systems. Testing on the English Penn Treebank data, forest reranking gave a state-of-the-art unlabeled dependency accuracy of 93.12. 1 Introduction There are two main approaches of data-driven dependency parsing – one is graph-based and the other is transition-based. In the graph-based approach, global optimization algorithms find the highest-scoring tree with locally factored models (McDonald et al., 2005). While third-order graph-based models achieve stateof-the-art accuracy, it has O(n4 ) time complexity for a sentence of length n. Recently, some pruning techniques have been proposed to improve the efficiency of third-order models (Rush and Petrov, 2012; Zhang and McDonald, 2012). The transition-based approach usually employs the shift-reduce parsing algorithm with linear-time complexity (Nivre, 2008). It greedily chooses the transition with the highest score and the resulting transition sequence is not always globally optimal. The beam search algorithm improves parsing flexibility in determi"
Q13-1012,P08-1108,0,0.414044,"transition-based approach usually employs the shift-reduce parsing algorithm with linear-time complexity (Nivre, 2008). It greedily chooses the transition with the highest score and the resulting transition sequence is not always globally optimal. The beam search algorithm improves parsing flexibility in deterministic parsing (Zhang and Clark, 2008; Zhang and Nivre, 2011), and dynamic programming makes beam search more efficient (Huang and Sagae, 2010). There is also an alternative approach that integrates graph-based and transition-based models (Sagae and Lavie, 2006; Zhang and Clark, 2008; Nivre and McDonald, 2008; Martins et al., 2008). Martins et al. (2008) formulated their approach as stacking of parsers where the output of the first-stage parser is provided to the second as guide features. In particular, they used a transition-based parser for the first stage and a graph-based parser for the second stage. The main drawback of this approach is that the efficiency of the transition-based parser is sacrificed because the second-stage employs full parsing. This paper proposes an efficient stacked parsing method through discriminative reranking with higher-order graph-based features, which works on the"
Q13-1012,J08-4003,0,0.0575782,"parsing – one is graph-based and the other is transition-based. In the graph-based approach, global optimization algorithms find the highest-scoring tree with locally factored models (McDonald et al., 2005). While third-order graph-based models achieve stateof-the-art accuracy, it has O(n4 ) time complexity for a sentence of length n. Recently, some pruning techniques have been proposed to improve the efficiency of third-order models (Rush and Petrov, 2012; Zhang and McDonald, 2012). The transition-based approach usually employs the shift-reduce parsing algorithm with linear-time complexity (Nivre, 2008). It greedily chooses the transition with the highest score and the resulting transition sequence is not always globally optimal. The beam search algorithm improves parsing flexibility in deterministic parsing (Zhang and Clark, 2008; Zhang and Nivre, 2011), and dynamic programming makes beam search more efficient (Huang and Sagae, 2010). There is also an alternative approach that integrates graph-based and transition-based models (Sagae and Lavie, 2006; Zhang and Clark, 2008; Nivre and McDonald, 2008; Martins et al., 2008). Martins et al. (2008) formulated their approach as stacking of parsers"
Q13-1012,P03-1021,0,0.00908604,"tron algorithm with 5 iterations. When training nonlocal reranking models, we set k-best size of cubepruning to 5. For dumping packed forests for test data, spurious and non-spurious shift-reduce parsers are trained by the averaged perceptron algorithm. In all experiments on English data, we fixed beam size to 12 for training both parsers. 6.2 Test with Gold POS tags We show the comparison of dumped spurious and non-spurious packed forests for training data in Table 4. Both oracle accuracies are 100.0 due to the 5 The scaling factor β was tuned by minimum error rate training (MERT) algorithm (Och, 2003) using development data. The MERT algorithm is suited to tune low-dimensional parameters. The β was set to about 1.2 in case of local reranking, and to about 1.5 in case of non-local reranking. system sr (12) (8) (12) (32) (64) (12, k=3) (64, k=3) non-sp sr (12) (8) (12) (32) (64) (12, k=3) (64, k=3) w/ rerank. – w/ local w/ local w/ local w/ local w/ non-local w/ non-local – w/ local w/ local w/ local w/ local w/ non-local w/ non-local sec. (per sent.) 0.011 0.009 + 0.0056 0.011 + 0.0079 0.03 + 0.019 0.06 + 0.039 0.011 + 0.0085 0.06 + 0.046 0.012 0.01 + 0.005 0.012 + 0.0074 0.031 + 0.0184 0.0"
Q13-1012,N12-1054,0,0.293279,"nk data, forest reranking gave a state-of-the-art unlabeled dependency accuracy of 93.12. 1 Introduction There are two main approaches of data-driven dependency parsing – one is graph-based and the other is transition-based. In the graph-based approach, global optimization algorithms find the highest-scoring tree with locally factored models (McDonald et al., 2005). While third-order graph-based models achieve stateof-the-art accuracy, it has O(n4 ) time complexity for a sentence of length n. Recently, some pruning techniques have been proposed to improve the efficiency of third-order models (Rush and Petrov, 2012; Zhang and McDonald, 2012). The transition-based approach usually employs the shift-reduce parsing algorithm with linear-time complexity (Nivre, 2008). It greedily chooses the transition with the highest score and the resulting transition sequence is not always globally optimal. The beam search algorithm improves parsing flexibility in deterministic parsing (Zhang and Clark, 2008; Zhang and Nivre, 2011), and dynamic programming makes beam search more efficient (Huang and Sagae, 2010). There is also an alternative approach that integrates graph-based and transition-based models (Sagae and Lavi"
Q13-1012,N06-2033,0,0.212195,"d Petrov, 2012; Zhang and McDonald, 2012). The transition-based approach usually employs the shift-reduce parsing algorithm with linear-time complexity (Nivre, 2008). It greedily chooses the transition with the highest score and the resulting transition sequence is not always globally optimal. The beam search algorithm improves parsing flexibility in deterministic parsing (Zhang and Clark, 2008; Zhang and Nivre, 2011), and dynamic programming makes beam search more efficient (Huang and Sagae, 2010). There is also an alternative approach that integrates graph-based and transition-based models (Sagae and Lavie, 2006; Zhang and Clark, 2008; Nivre and McDonald, 2008; Martins et al., 2008). Martins et al. (2008) formulated their approach as stacking of parsers where the output of the first-stage parser is provided to the second as guide features. In particular, they used a transition-based parser for the first stage and a graph-based parser for the second stage. The main drawback of this approach is that the efficiency of the transition-based parser is sacrificed because the second-stage employs full parsing. This paper proposes an efficient stacked parsing method through discriminative reranking with highe"
Q13-1012,W09-3839,0,0.24162,"7.2 Methods to Improve Dependency Parsing Higher-order features like third-order dependency relations are essential to improve dependency parsing accuracy (Koo and Collins, 2010; Rush and Petrov, 2012; Zhang and McDonald, 2012). A reranking approach is one effective solution to introduce rich features to a parser model in the context of constituency parsing (Charniak and Johnson, 2005; Huang, 2008). Hall (2007) applied a k-best maximum spanning tree algorithm to non-projective dependency analysis, and showed that k-best discriminative reranking improves parsing accuracy in several languages. Sangati et al. (2009) proposed a k-best dependency reranking algorithm using a third-order generative model, and Hayashi et al. (2011) extended it to a 148 forest algorithm. Though forest reranking requires some approximations such as cube-pruning to integrate non-local features, it can explore larger search space than k-best reranking. The stacking approach (Nivre and McDonald, 2008; Martins et al., 2008) uses the output of one dependency parser to provide guide features for another. Stacking improves the parsing accuracy of second stage parsers on various language datasets. The joint graph-based and transition-b"
Q13-1012,N06-1012,0,0.0229365,"s, and each fold is parsed by each of the spurious and non-spurious shift-reduce parsers using beam size 12 with the model trained on sentences from the remaining 19 folds, dumping the outputs as packed forests. The reranker is modeled by either equation (1) or (4). By our preliminary experiments using development data (WSJ22), we modeled the reranker with equation (1) when training, and with equation (4) when testing5 (i.e., the scores of the first-stage parser are not considered during training of the reranking model). This prevents the discriminative reranking features from under-training (Sutton et al., 2006; Hollingshead and Roark, 2008). A discriminative reranking model is trained on the packed forests by using the averaged perceptron algorithm with 5 iterations. When training nonlocal reranking models, we set k-best size of cubepruning to 5. For dumping packed forests for test data, spurious and non-spurious shift-reduce parsers are trained by the averaged perceptron algorithm. In all experiments on English data, we fixed beam size to 12 for training both parsers. 6.2 Test with Gold POS tags We show the comparison of dumped spurious and non-spurious packed forests for training data in Table 4."
Q13-1012,W03-3023,1,0.654983,"ate and all the hyperedges going out from a vertex corresponding to the consequent state can be attached to the vertex corresponding to the antecedent state. The scan weight of the removed unary hyperedge is added to each weight of the hyperedges attached to the antecedent. 4 Experiments (Spurious Ambiguity vs. Non-Spurious Ambiguity) We conducted experiments on the English Penn Treebank (PTB) data to compare spurious and nonspurious shift-reduce parsers. We split the WSJ part of PTB into sections 02-21 for training, section 22 for development, and section 23 for test. We used the head rules (Yamada and Matsumoto, 2003) to convert phrase structure to dependency structure. axiom(c0 ) : goal(c3n ) : shift : scan : 0 : (0, 1, w0 ) : ∅ 3n : (0, n, s0 ) : ∅ state p z }| { ℓ : ( , j, sd |sd−1 |. . . |s1 |s0 ) : j&lt;n ℓ + 1 : (j, j + 1, sd−1 |sd−2 |. . . |s0 |w∗j ) : (p) ℓ : (i, j, sd |sd−1 |. . . |s1 |s∗0 ) : π ℓ + 1 : (i, j, sd |sd−1 |. . . |s1 |s0 ) : π reduce↶ : z reduce↷ : z state p state q state p state q }| { z }| { : (i, j, s′d |s′d−1 |. . . |s′0 |s′0 ) : π ′ ℓ : (j, k, sd |sd−1 |. . . |s1 |s∗0 ) : π ′ s0 .h.w = w0 ∧ p ∈ π ℓ + 1 : (i, k, s′d |s′d−1 |. . . |s′1 |s′0 ↶ s∗0 ) : π ′ }| { z }| { : (i, j, s′d |s′d"
Q13-1012,D08-1059,0,0.514104,"der graph-based models achieve stateof-the-art accuracy, it has O(n4 ) time complexity for a sentence of length n. Recently, some pruning techniques have been proposed to improve the efficiency of third-order models (Rush and Petrov, 2012; Zhang and McDonald, 2012). The transition-based approach usually employs the shift-reduce parsing algorithm with linear-time complexity (Nivre, 2008). It greedily chooses the transition with the highest score and the resulting transition sequence is not always globally optimal. The beam search algorithm improves parsing flexibility in deterministic parsing (Zhang and Clark, 2008; Zhang and Nivre, 2011), and dynamic programming makes beam search more efficient (Huang and Sagae, 2010). There is also an alternative approach that integrates graph-based and transition-based models (Sagae and Lavie, 2006; Zhang and Clark, 2008; Nivre and McDonald, 2008; Martins et al., 2008). Martins et al. (2008) formulated their approach as stacking of parsers where the output of the first-stage parser is provided to the second as guide features. In particular, they used a transition-based parser for the first stage and a graph-based parser for the second stage. The main drawback of this"
Q13-1012,D12-1030,0,0.575379,"ng gave a state-of-the-art unlabeled dependency accuracy of 93.12. 1 Introduction There are two main approaches of data-driven dependency parsing – one is graph-based and the other is transition-based. In the graph-based approach, global optimization algorithms find the highest-scoring tree with locally factored models (McDonald et al., 2005). While third-order graph-based models achieve stateof-the-art accuracy, it has O(n4 ) time complexity for a sentence of length n. Recently, some pruning techniques have been proposed to improve the efficiency of third-order models (Rush and Petrov, 2012; Zhang and McDonald, 2012). The transition-based approach usually employs the shift-reduce parsing algorithm with linear-time complexity (Nivre, 2008). It greedily chooses the transition with the highest score and the resulting transition sequence is not always globally optimal. The beam search algorithm improves parsing flexibility in deterministic parsing (Zhang and Clark, 2008; Zhang and Nivre, 2011), and dynamic programming makes beam search more efficient (Huang and Sagae, 2010). There is also an alternative approach that integrates graph-based and transition-based models (Sagae and Lavie, 2006; Zhang and Clark, 2"
Q13-1012,P11-2033,0,0.0439476,"achieve stateof-the-art accuracy, it has O(n4 ) time complexity for a sentence of length n. Recently, some pruning techniques have been proposed to improve the efficiency of third-order models (Rush and Petrov, 2012; Zhang and McDonald, 2012). The transition-based approach usually employs the shift-reduce parsing algorithm with linear-time complexity (Nivre, 2008). It greedily chooses the transition with the highest score and the resulting transition sequence is not always globally optimal. The beam search algorithm improves parsing flexibility in deterministic parsing (Zhang and Clark, 2008; Zhang and Nivre, 2011), and dynamic programming makes beam search more efficient (Huang and Sagae, 2010). There is also an alternative approach that integrates graph-based and transition-based models (Sagae and Lavie, 2006; Zhang and Clark, 2008; Nivre and McDonald, 2008; Martins et al., 2008). Martins et al. (2008) formulated their approach as stacking of parsers where the output of the first-stage parser is provided to the second as guide features. In particular, they used a transition-based parser for the first stage and a graph-based parser for the second stage. The main drawback of this approach is that the ef"
S07-1052,J93-2004,0,\N,Missing
S07-1052,E06-1011,0,\N,Missing
S07-1052,S07-1014,0,\N,Missing
S07-1052,P06-1095,0,\N,Missing
S07-1052,P02-1034,0,\N,Missing
S07-1052,W04-3205,0,\N,Missing
shimohata-etal-2004-building,W02-1611,1,\N,Missing
shimohata-etal-2004-building,W01-1401,1,\N,Missing
shimohata-etal-2004-building,P03-1057,1,\N,Missing
W00-0803,1998.amta-tutorials.5,0,0.198319,"Missing"
W00-0803,Y95-1006,0,0.0236303,"Missing"
W00-0803,X96-1035,0,0.0271732,"ing single character indexing, n-gram character indexing and (segmented) word indexing in Chinese information retrieval is reported in Nie et al. (1996, 1998, 1999) and Kwok (1997). For the case of monolingual information retrieval (MLIR) task, in comparison to the single character based indexing approach, n-gram based and word based approaches obtained better retrieval at the cost of the extra time and space complexity. Similar comparison and conclusion for Japanese and Korean MLIR are made in Fujii et al. (1993) and Lee et al. (1999), respectively. Cross language information retrieval (CUR, Oard and Dorr, 1996) refers to the retrieval when the query and the document collection are in different languages. Unlike MLIR, in cross language information retrieval, a great deal of efforts is allocated in maintaining the multilingual dictionary and thesaurus, and translating the queries and documents, and so on. There are other approaches to CLIR where techniques like latent semantic indexing (LSI) are used to automatically establish associations between queries and documents independent of language differences (Rchder et al., 1998). Due to the special nature (ideographic, undefimited, etc.) of the CJK langu"
W00-0803,J96-3004,0,\N,Missing
W00-1303,A00-2018,0,0.00428156,"on..... ..-&quot; sistency, since there are a number of features that affect the accuracy of the final results, and these features usually relate to one another. On the other hand, as large-scale tagged corpora have become available these days, a number of statistical parsing techniques which estimate the dependency probabilities using such tagged corpora have been developed(Collins, 1996; Fujio and Matsumoto, 1998). These approaches have overcome the systems based on the rule-based approaches. Decision Trees(Haruno et al., 1998) and Maximum Entropy models(Ratnaparkhi, 1997; Uchimoto et al., 1999; Charniak, 2000) have been applied to dependency or syntactic structure analysis. However, these models require an appropriate feature selection in order to achieve a high performance. In addition, acquisition of an efficient combination of features is difficult in these models. In recent years, new statistical learning techniques such as Support Vector Machines (SVMs) (Cortes and Vapnik, 1995; Vapnik, 1998) and Boosting(Freund and Schapire, 1996) are proposed. These techniques take a strategy that maximize the margin between critical examples and the separating hyperplane. In particular, compared with other"
W00-1303,P96-1025,0,0.0470783,"optimal combination of dependencies to form the entire sentence. In previous approaches, these probabilites of dependencies axe given by manually constructed rules. However, rule-based approaches have problems in coverage and con..... ..-&quot; sistency, since there are a number of features that affect the accuracy of the final results, and these features usually relate to one another. On the other hand, as large-scale tagged corpora have become available these days, a number of statistical parsing techniques which estimate the dependency probabilities using such tagged corpora have been developed(Collins, 1996; Fujio and Matsumoto, 1998). These approaches have overcome the systems based on the rule-based approaches. Decision Trees(Haruno et al., 1998) and Maximum Entropy models(Ratnaparkhi, 1997; Uchimoto et al., 1999; Charniak, 2000) have been applied to dependency or syntactic structure analysis. However, these models require an appropriate feature selection in order to achieve a high performance. In addition, acquisition of an efficient combination of features is difficult in these models. In recent years, new statistical learning techniques such as Support Vector Machines (SVMs) (Cortes and Vap"
W00-1303,W98-1511,1,0.253756,"tion of dependencies to form the entire sentence. In previous approaches, these probabilites of dependencies axe given by manually constructed rules. However, rule-based approaches have problems in coverage and con..... ..-&quot; sistency, since there are a number of features that affect the accuracy of the final results, and these features usually relate to one another. On the other hand, as large-scale tagged corpora have become available these days, a number of statistical parsing techniques which estimate the dependency probabilities using such tagged corpora have been developed(Collins, 1996; Fujio and Matsumoto, 1998). These approaches have overcome the systems based on the rule-based approaches. Decision Trees(Haruno et al., 1998) and Maximum Entropy models(Ratnaparkhi, 1997; Uchimoto et al., 1999; Charniak, 2000) have been applied to dependency or syntactic structure analysis. However, these models require an appropriate feature selection in order to achieve a high performance. In addition, acquisition of an efficient combination of features is difficult in these models. In recent years, new statistical learning techniques such as Support Vector Machines (SVMs) (Cortes and Vapnik, 1995; Vapnik, 1998) and"
W00-1303,P98-1083,0,0.180904,"manually constructed rules. However, rule-based approaches have problems in coverage and con..... ..-&quot; sistency, since there are a number of features that affect the accuracy of the final results, and these features usually relate to one another. On the other hand, as large-scale tagged corpora have become available these days, a number of statistical parsing techniques which estimate the dependency probabilities using such tagged corpora have been developed(Collins, 1996; Fujio and Matsumoto, 1998). These approaches have overcome the systems based on the rule-based approaches. Decision Trees(Haruno et al., 1998) and Maximum Entropy models(Ratnaparkhi, 1997; Uchimoto et al., 1999; Charniak, 2000) have been applied to dependency or syntactic structure analysis. However, these models require an appropriate feature selection in order to achieve a high performance. In addition, acquisition of an efficient combination of features is difficult in these models. In recent years, new statistical learning techniques such as Support Vector Machines (SVMs) (Cortes and Vapnik, 1995; Vapnik, 1998) and Boosting(Freund and Schapire, 1996) are proposed. These techniques take a strategy that maximize the margin between"
W00-1303,C00-1051,0,0.0170471,"Missing"
W00-1303,W97-0301,0,0.0122974,"approaches have problems in coverage and con..... ..-&quot; sistency, since there are a number of features that affect the accuracy of the final results, and these features usually relate to one another. On the other hand, as large-scale tagged corpora have become available these days, a number of statistical parsing techniques which estimate the dependency probabilities using such tagged corpora have been developed(Collins, 1996; Fujio and Matsumoto, 1998). These approaches have overcome the systems based on the rule-based approaches. Decision Trees(Haruno et al., 1998) and Maximum Entropy models(Ratnaparkhi, 1997; Uchimoto et al., 1999; Charniak, 2000) have been applied to dependency or syntactic structure analysis. However, these models require an appropriate feature selection in order to achieve a high performance. In addition, acquisition of an efficient combination of features is difficult in these models. In recent years, new statistical learning techniques such as Support Vector Machines (SVMs) (Cortes and Vapnik, 1995; Vapnik, 1998) and Boosting(Freund and Schapire, 1996) are proposed. These techniques take a strategy that maximize the margin between critical examples and the separating hyperpl"
W00-1303,E99-1026,0,0.479777,"blems in coverage and con..... ..-&quot; sistency, since there are a number of features that affect the accuracy of the final results, and these features usually relate to one another. On the other hand, as large-scale tagged corpora have become available these days, a number of statistical parsing techniques which estimate the dependency probabilities using such tagged corpora have been developed(Collins, 1996; Fujio and Matsumoto, 1998). These approaches have overcome the systems based on the rule-based approaches. Decision Trees(Haruno et al., 1998) and Maximum Entropy models(Ratnaparkhi, 1997; Uchimoto et al., 1999; Charniak, 2000) have been applied to dependency or syntactic structure analysis. However, these models require an appropriate feature selection in order to achieve a high performance. In addition, acquisition of an efficient combination of features is difficult in these models. In recent years, new statistical learning techniques such as Support Vector Machines (SVMs) (Cortes and Vapnik, 1995; Vapnik, 1998) and Boosting(Freund and Schapire, 1996) are proposed. These techniques take a strategy that maximize the margin between critical examples and the separating hyperplane. In particular, com"
W00-1303,C00-2109,0,\N,Missing
W00-1303,C98-1080,0,\N,Missing
W01-1412,J96-1001,0,\N,Missing
W01-1412,C00-2135,1,\N,Missing
W01-1412,C00-2131,0,\N,Missing
W01-1412,P93-1003,0,\N,Missing
W01-1412,J00-2004,0,\N,Missing
W01-1412,C96-1089,0,\N,Missing
W01-1412,W96-0107,1,\N,Missing
W02-2016,W98-1511,1,0.662143,"current segment modifies the segment on its immediate right hand side. Experiments using the Kyoto University Corpus show that the method outperforms previous systems as well as improves the parsing and training efficiency. 1 Introduction Dependency analysis has been recognized as a basic process in Japanese sentence analysis, and a number of studies have been proposed. Japanese dependency structure is usually defined in terms of the relationship between phrasal units called bunsetsu segments (hereafter “segments”). Most of the previous statistical approaches for Japanese dependency analysis (Fujio and Matsumoto, 1998; Haruno et al., 1999; Uchimoto et al., 1999; Kanayama et al., 2000; Uchimoto et al., 2000; Kudo and Matsumoto, 2000) are based on a probabilistic model consisting of the following two steps. First, they estimate modification probabilities, in other words, how probable one segment tends to modify another. Second the optimal combination of dependencies is searched from the all candidates dependencies. Such a probabilistic model is not always efficient since it needs to calculate the probabilities for all possible dependencies and cre˙ − 1)/2 (where n is the number of segments ates n(n in a sent"
W02-2016,C00-1060,0,0.055113,"Experiments using the Kyoto University Corpus show that the method outperforms previous systems as well as improves the parsing and training efficiency. 1 Introduction Dependency analysis has been recognized as a basic process in Japanese sentence analysis, and a number of studies have been proposed. Japanese dependency structure is usually defined in terms of the relationship between phrasal units called bunsetsu segments (hereafter “segments”). Most of the previous statistical approaches for Japanese dependency analysis (Fujio and Matsumoto, 1998; Haruno et al., 1999; Uchimoto et al., 1999; Kanayama et al., 2000; Uchimoto et al., 2000; Kudo and Matsumoto, 2000) are based on a probabilistic model consisting of the following two steps. First, they estimate modification probabilities, in other words, how probable one segment tends to modify another. Second the optimal combination of dependencies is searched from the all candidates dependencies. Such a probabilistic model is not always efficient since it needs to calculate the probabilities for all possible dependencies and cre˙ − 1)/2 (where n is the number of segments ates n(n in a sentence) training examples per sentence. In addition, the probabilisti"
W02-2016,W00-1303,1,0.730375,"show that the method outperforms previous systems as well as improves the parsing and training efficiency. 1 Introduction Dependency analysis has been recognized as a basic process in Japanese sentence analysis, and a number of studies have been proposed. Japanese dependency structure is usually defined in terms of the relationship between phrasal units called bunsetsu segments (hereafter “segments”). Most of the previous statistical approaches for Japanese dependency analysis (Fujio and Matsumoto, 1998; Haruno et al., 1999; Uchimoto et al., 1999; Kanayama et al., 2000; Uchimoto et al., 2000; Kudo and Matsumoto, 2000) are based on a probabilistic model consisting of the following two steps. First, they estimate modification probabilities, in other words, how probable one segment tends to modify another. Second the optimal combination of dependencies is searched from the all candidates dependencies. Such a probabilistic model is not always efficient since it needs to calculate the probabilities for all possible dependencies and cre˙ − 1)/2 (where n is the number of segments ates n(n in a sentence) training examples per sentence. In addition, the probabilistic model assumes that each pairs of dependency stru"
W02-2016,E99-1026,0,0.809439,"diate right hand side. Experiments using the Kyoto University Corpus show that the method outperforms previous systems as well as improves the parsing and training efficiency. 1 Introduction Dependency analysis has been recognized as a basic process in Japanese sentence analysis, and a number of studies have been proposed. Japanese dependency structure is usually defined in terms of the relationship between phrasal units called bunsetsu segments (hereafter “segments”). Most of the previous statistical approaches for Japanese dependency analysis (Fujio and Matsumoto, 1998; Haruno et al., 1999; Uchimoto et al., 1999; Kanayama et al., 2000; Uchimoto et al., 2000; Kudo and Matsumoto, 2000) are based on a probabilistic model consisting of the following two steps. First, they estimate modification probabilities, in other words, how probable one segment tends to modify another. Second the optimal combination of dependencies is searched from the all candidates dependencies. Such a probabilistic model is not always efficient since it needs to calculate the probabilities for all possible dependencies and cre˙ − 1)/2 (where n is the number of segments ates n(n in a sentence) training examples per sentence. In add"
W02-2016,2000.iwpt-1.43,0,0.447508,"yoto University Corpus show that the method outperforms previous systems as well as improves the parsing and training efficiency. 1 Introduction Dependency analysis has been recognized as a basic process in Japanese sentence analysis, and a number of studies have been proposed. Japanese dependency structure is usually defined in terms of the relationship between phrasal units called bunsetsu segments (hereafter “segments”). Most of the previous statistical approaches for Japanese dependency analysis (Fujio and Matsumoto, 1998; Haruno et al., 1999; Uchimoto et al., 1999; Kanayama et al., 2000; Uchimoto et al., 2000; Kudo and Matsumoto, 2000) are based on a probabilistic model consisting of the following two steps. First, they estimate modification probabilities, in other words, how probable one segment tends to modify another. Second the optimal combination of dependencies is searched from the all candidates dependencies. Such a probabilistic model is not always efficient since it needs to calculate the probabilities for all possible dependencies and cre˙ − 1)/2 (where n is the number of segments ates n(n in a sentence) training examples per sentence. In addition, the probabilistic model assumes that ea"
W02-2028,J90-1003,0,\N,Missing
W02-2028,J92-4003,0,\N,Missing
W03-0311,P01-1004,0,0.013974,"ty and tense. Consequently, our method gains robustness to length and the style differences between inputs and the example corpus. 5.2 Translation Memory Translation memory (TM) is aimed at retrieving informative translation example from example corpus. TM and our method share the retrieval strategy of rough and wide coverage. However, recall is more highly weighted than precision in TM, while recall and precision should be equally considered in our method. To carry out wide coverage retrieval, TM relaxed various conditions on inputs: Preserving only mono-gram and bi-gram on words/characters (Baldwin, 2001; Sato, 1992), removing functional words (Kumano et al., 2002; Wakita et al., 2000), and removing content words (Sumita and Tsutsumi, 1988). In our method, information on functional words is removed and that on modality and tense is introduced instead. Information on word order is also removed while instead we preserve information on whether each word is located in the focus area. 6 Conclusions In this paper, we introduced the idea of meaningequivalent sentences for robust example-based S2ST. Meaning-equivalent sentences have the same main meaning as the input despite lacking some unimportant"
W03-0311,C00-1019,0,0.0760963,"Missing"
W03-0311,1999.mtsummit-1.37,0,0.0756867,"Missing"
W03-0311,W02-0718,0,0.0275687,"lacking some unimportant information. The translations of meaning-equivalent sentences correspond to “rough translations.” The retrieval is based on content words, modality, and tense. 1 Introduction Speech-to-speech translation (S2ST) technologies consist of speech recognition, machine translation (MT), and speech synthesis (Waibel, 1996; Wahlster, 2000; Yamamoto, 2000). The MT part receives speech texts recognized by a speech recognizer. The nature of speech causes difficulty in translation since the styles of speech are different from those of written text and are sometimes ungrammatical (Lazzari, 2002). Therefore, rule-based MT cannot translate speech accurately compared with its performance for written-style text . Example-based MT (EBMT) is one of the corpusbased machine translation methods. It retrieves examples similar to inputs and adjusts their translations to obtain the output (Nagao, 1981). EBMT is a promising method for S2ST in that it performs robust translation of ungramYuji Matsumoto Nara Institute of Science and Technology matsu@is.aist-nara.ac.jp matical sentences and requires far less manual work than rule-based MT. However, there are two problems in applying EBMT to S2ST. On"
W03-0311,C92-4203,0,0.00964272,"onsequently, our method gains robustness to length and the style differences between inputs and the example corpus. 5.2 Translation Memory Translation memory (TM) is aimed at retrieving informative translation example from example corpus. TM and our method share the retrieval strategy of rough and wide coverage. However, recall is more highly weighted than precision in TM, while recall and precision should be equally considered in our method. To carry out wide coverage retrieval, TM relaxed various conditions on inputs: Preserving only mono-gram and bi-gram on words/characters (Baldwin, 2001; Sato, 1992), removing functional words (Kumano et al., 2002; Wakita et al., 2000), and removing content words (Sumita and Tsutsumi, 1988). In our method, information on functional words is removed and that on modality and tense is introduced instead. Information on word order is also removed while instead we preserve information on whether each word is located in the focus area. 6 Conclusions In this paper, we introduced the idea of meaningequivalent sentences for robust example-based S2ST. Meaning-equivalent sentences have the same main meaning as the input despite lacking some unimportant information."
W03-0311,1988.tmi-1.13,1,0.610238,"pus. 5.2 Translation Memory Translation memory (TM) is aimed at retrieving informative translation example from example corpus. TM and our method share the retrieval strategy of rough and wide coverage. However, recall is more highly weighted than precision in TM, while recall and precision should be equally considered in our method. To carry out wide coverage retrieval, TM relaxed various conditions on inputs: Preserving only mono-gram and bi-gram on words/characters (Baldwin, 2001; Sato, 1992), removing functional words (Kumano et al., 2002; Wakita et al., 2000), and removing content words (Sumita and Tsutsumi, 1988). In our method, information on functional words is removed and that on modality and tense is introduced instead. Information on word order is also removed while instead we preserve information on whether each word is located in the focus area. 6 Conclusions In this paper, we introduced the idea of meaningequivalent sentences for robust example-based S2ST. Meaning-equivalent sentences have the same main meaning as the input despite lacking some unimportant information. Translation of meaning-equivalent sentences corresponds to rough translations, which aim not at exact translation with narrow"
W03-0311,W01-1401,1,0.874799,"ted Translated Concise Conversational 100 80 Table 1: Number of Words by Sentences 60 40 20 0 Test 2-5 6-10 11-15 16- Sentence Length (Words) Figure 1: Distribution of Untranslated Inputs by Length 2 Difficulty in Example-based S2ST 2.1 Translation Degradation by Input Length A major problem with machine translation, regardless of the translation method, is that performance drops rapidly as input sentences become longer. For EBMT, the longer input sentences become, the fewer similar example sentences exist in the example corpus. Figure 1 shows translation difficulty in long sentences in EBMT (Sumita, 2001). The EBMT system is given 591 test sentences and returns translation result as translated/untranslated. Untranslated means that there exists no similar example sentences for the input. Although the EBMT is equipped with a large example corpus (about 170K sentences), it often failed to translate long inputs. 2.2 Language English Japanese 5.4 6.2 7.9 8.9 Style Differences between Concise and Conversational The performance of example-based S2ST greatly depends on the example corpus. It is advantageous for an example corpus to have a large volume and the same style as the input sentences. A corpu"
W03-0311,takezawa-etal-2002-toward,1,0.921832,"texts dictated from conversational speech is favorable for S2ST. Unfortunately, it is very difficult to prepare such an example corpus since this task requires laborious work such as speech recording and speech transcription. Therefore, we cannot avoid using a written-style corpus, such as phrasebooks, to prepare a sufficiently large volume of examples. Contained texts are almost grammatical and rarely contain unnecessary words. We call the style used in such a corpus “concise” and the style seen in conversational speech “conversational.” Table 1 shows the average numbers of words in concise (Takezawa et al., 2002) and conversational corpora (Takezawa, 1999). Sentences in conversational style are about 2.5 words longer than those in concise style in both Concise Conversational Language Model Concise Conversational 16.4 58.3 72.3 16.3 Table 2: Cross Perplexity English and Japanese. This is because conversational style sentences contain unnecessary words or subordinate clauses, which have the effects of assisting the listener’s comprehension and avoiding the possibility of giving the listener a curt impression. Table 2 shows cross perplexity between concise and conversational corpora (Takezawa et al., 200"
W03-0311,P99-1049,0,0.0174701,"a (my baggage was stolen.) question past negation Meaning-equivalent Sentence baggu wo nusuma re ta (My bag was stolen). Sentence3 hoteru wo yoyaku shi mashi ta ka? (Did you reserve this hotel?) hoteru wo yoyaku shi tei masen (I do not reserve this hotel.) Modality & Tense4 C1 C2 C3 C4 C5 C6 Figure 3: Sentences and their Modality and Tense sample sentences and their modality and tense. Clues are underlined. A speech act is a concept similar to modality in which speakers’ intentions are represented. The two studies introduced information of the speech act in their S2ST systems (Wahlster, 2000; Tanaka and Yokoo, 1999). The two studies and our method differ in the effect of speech act information. Their effect of speech act information is so small that it is limited to generating the translation text. Translation texts are refined by selecting proper expressions according to the detected speakers’ intention. 3.3 Retrieved sentences are ranked by the conditions described below. Conditions are described in order of priority. If there is more than one sentence having the highest score under these conditions, the most similar sentence is selected randomly. C1: C2: C3: C4: C5: C6: Sentences that satisfy the cond"
W03-0311,J00-4006,0,\N,Missing
W03-0314,A00-1031,0,0.0201531,"with high association scores. Sequential pattern mining takes care of translation candidate generation as well as efficient counting of the generated candidates. This characteristic is well-suited for our purpose in generating overlapped translation candidates of which frequencies are efficiently counted. 3 3.1 Experimental Results type coverage Data We use the English-Japanese parallel corpora that are automatically aligned from comparable corpora of the news wires (Utiyama and Isahara, 2002). There are 150,000 parallel sentences which satisfy their proposed sentence similarity. We use TnT (Brants, 2000) for English POS tagging and ChaSen (Matsumoto et al., 2000) for Japanese morphological analysis, and label each token to either content or functional depending on its partof-speech. Table 2: Statistics of 150,000 parallel sentences Japanese English content (token) 2,039,656 2,257,806 content (type) 47,316 57,666 functional (token) 2,660,855 1,704,189 functional (type) 1,811 386 3.2 Evaluation Criteria We evaluate our sequence-to-sequence correspondence by accuracy and coverage, which we believe, similar criteria to (Moore, 2001) and (Melamed, 2001) 2 . Let Cseq be the set of correct bilingual"
W03-0314,J93-1003,0,0.0336086,"ning from bilingual sequence database can better be seen in a contingency table shown in Table 1. Frequencies of a bilingual pattern Ei Jj , an English pattern Ei , and a Japanese pattern Jj correspond to a, a + b, and a + c respectively. Since we know the total number of bilingual sequences N = a + b + c + d, values of b, c and d can be calculated immediately. Table 1: Contingency Table Jj ¬ Jj Ei a b a+b ¬ Ei c d a+c N The contingency table is used for calculating a similarity (or association) score between Ei and Jj . For this present work, we use Dunning’s log-likelihood ratio statistics (Dunning, 1993) defined as follows: sim = a log a + b log b + c log c + d log d −(a + b) log (a + b) − (a + c) log (a + c) −(b + d) log (b + d) − (c + d) log (c + d) +(a + b + c + d) log (a + b + c + d) For each bilingual pattern Ei Jj , we compute its similarity score and qualify it as a bilingual sequence-to-sequence correspondence if no equally strong or stronger association for monolingual constituent is found. This step is conservative and the same as step 5 in Moore (2001) or step 6(b) in Kitamura and Matsumoto (1996). Our implementation uses a digital trie structure called Double Array for efficient s"
W03-0314,W96-0107,1,0.84247,") score between Ei and Jj . For this present work, we use Dunning’s log-likelihood ratio statistics (Dunning, 1993) defined as follows: sim = a log a + b log b + c log c + d log d −(a + b) log (a + b) − (a + c) log (a + c) −(b + d) log (b + d) − (c + d) log (c + d) +(a + b + c + d) log (a + b + c + d) For each bilingual pattern Ei Jj , we compute its similarity score and qualify it as a bilingual sequence-to-sequence correspondence if no equally strong or stronger association for monolingual constituent is found. This step is conservative and the same as step 5 in Moore (2001) or step 6(b) in Kitamura and Matsumoto (1996). Our implementation uses a digital trie structure called Double Array for efficient storage and retrieval of sequential patterns (Aoe, 1989). For non-segmented language, a word unit depends on results of morphological analysis. In case of Japanese morphological analysis, ChaSen (Matsumoto et al., 2000) tends to over-segment words, while JUMAN (Kurohashi et al., 1994) tends to under-segment words. It is difficult to define units of correspondences only consulting the Japanese half of parallel corpora. A parallel sentencepair may resolve some Japanese word segmentation ambiguity, however, we ha"
W03-0314,P93-1003,0,0.0332135,"nce, our method tends to suffer from indirect association when the association score is low, as pointed out by Melamed (2001). Although our method relies on an empirical observation that “direct associations are usually stronger than indirect association”, it seems effective enough for multi-word translation. balanced by a As far as we know, our method is the first attempt to make an exhaustive enumeration of rigid and gapped translation candidates of both languages possible, yet avoiding combinatorial explosion. Previous approaches effectively narrow down its search space by some heuristics. Kupiec (1993) focuses on noun-phrase translations only, Smadja et al. (1996) limits to find French translation of English collocation identified by his Xtract system, and Kitamura and Matsumoto (1996) can exhaustively enumerate only rigid word sequences. Many of works mentioned in the last paragraph as well as ours extract non-probabilistic translation lexicons. However, there are research works which go beyond word-level translations in statistical machine translation. One notable work is that of Marcu and Wong (2002), which is based on a joint probability model for statistical machine translation where w"
W03-0314,W02-1018,0,0.0302786,"rial explosion. Previous approaches effectively narrow down its search space by some heuristics. Kupiec (1993) focuses on noun-phrase translations only, Smadja et al. (1996) limits to find French translation of English collocation identified by his Xtract system, and Kitamura and Matsumoto (1996) can exhaustively enumerate only rigid word sequences. Many of works mentioned in the last paragraph as well as ours extract non-probabilistic translation lexicons. However, there are research works which go beyond word-level translations in statistical machine translation. One notable work is that of Marcu and Wong (2002), which is based on a joint probability model for statistical machine translation where word equivalents and phrase (rigid sequence) equivalents are automatically learned form bilingual corpora. Our method does not iterate an extraction process as shown in Figure 1. This could be a cause of poor performance in single-word translation pairs, since there is no mechanism for imposing mutually exclusion constrains. An interesting question then is what kind of iteration should be performed to improve performance. Probabilistic translation lexicon acquisition often uses EM training on Viterbi alignm"
W03-0314,W01-1411,0,0.313293,"r addresses the problem of identifying “multiword” (sequence-to-sequence) translation correspondences from parallel corpora. It is well-known that translation does not always proceed by word-for-word. This highlights the need for finding multi-word translation correspondences. Previous works that focus on multi-word translation correspondences from parallel corpora include noun phrase correspondences (Kupiec, 1993), fixed/flexible collocations (Smadja et al., 1996), n-gram word sequences of arbitrary length (Kitamura and Matsumoto, 1996), non-compositional compounds (Melamed, 2001), captoids (Moore, 2001), and named entities 1 . In all of these approaches, a common problem seems to be an identification of meaningful multi-word translation units. There are a number of factors which make handling of multi-word units more complicated than it appears. First, it is a many-to-many mapping which potentially leads to a combinatorial explosion. Second, multiword translation units are not necessarily contiguous, so an algorithm should not be hampered by the word adjacency constraint. Third, word segmentation itself is ambiguous for non-segmented languages such as Chinese or Japanese. We need to resolve"
W03-0314,J96-1001,0,0.229282,"ern and Japanese pattern as reported in Tables 7 and 8. It reveals that the word adjacency constraint in rigid sequences are too stringent. By relaxing the constraint, 436 (546 - 110) correct 2-2 translation pairs are encountered, though 200 (229 - 29) wrong 2-2 pairs are introduced at the same time. At this particular instance of minsup = 10 and maxpat = 3, considering gapped sequence of length 3 seems to introduce more noise. Admittedly, we still require further analysis as to searching a break-even point of rigid/gapped sequences. Our preliminary finding supports the work on collocation by Smadja et al. (1996) in that gapped sequences are also an important class of multi-word translations. 4 Related Work Moore (2001) presents insightful work which is closest to ours. His method first computes an initial association score, hypothesizes an occurrence of compounds, fuses it to a single token, recomputes association scores as if all translations are one-to-one mapping, and returns the highest association pairs. As for captoids, he also computes association of an inferred compound and its constituent words. He also uses language-specific features (e.g. capital letters, punctuation symbols) to identify l"
W03-0314,W01-1412,1,0.844348,"Missing"
W03-1107,W00-1016,0,\N,Missing
W03-1107,W02-1904,0,\N,Missing
W03-1107,P01-1026,0,\N,Missing
W03-1309,W99-0613,0,0.0238378,"oints of f-score for G#protein X, which are comparable to approaches reported in the literature. An increase of training data from 590 abstracts to 1600 abstracts helps the overall performance improve, given the corpus error is minimized. Our internal experiments with GENIA 3.0 (the version was corrected to GENIA 3.01) reveal that the corpus error is critical in our method. Even corpus errors have been successfully removed, it would not be practical to increase the size of labor-intensive annotated corpus. Use of unlabeled data in conjunction with a small but quality set of labeled data. e.g. Collins and Singer (1999), would have to be explored. References 4 The Gene Ontology Consortium. 2000. Gene ontology: tool for the unification of biology. Nature Genetics, 25:25–29. Related Work Tanabe and Wilbur (2002) use a hybrid approach of transformation-based learning (Brill Tagger) with rule-based post processing. An obvious drawback in their approach as with other rule-based approaches including Fukuda et al. (1998) is that the approaches cannot handle many correlated features. As pointed out in their paper, errors in the early stage of rule application are often propagated to the later stage, damaging the ove"
W03-1309,W02-0301,0,0.681982,"points for protein molecule names, and 75 points for protein names including molecules, families and domains. 1 Introduction This paper describes a protein name tagging method which is a fundamental precursor to information extraction of protein-protein interactions (PPIs) from MEDLINE abstracts. Previous work in bio-entity (including protein) recognition can be categorized into three approaches: (a) exact and approximate string matching (Hanisch et al., 2003), (b) handcrafted rule-based approaches (Fukuda et al., 1998) (Olsson et al., 2002), and (c) machine learning (Collier et al., 2000), (Kazama et al., 2002). Previous approaches in (b) and (c) ignore the fact that bio-entities have boundary ambiguities. Unlike general English, a space character is not a sufficient token delimiter. Moreover, name descriptions in biomedical resources are mostly compounds. A conventional English preprocessing undergoes a pipeline of simple tokenization and partof-speech tagging. The tokenization is based on a graphic word1 for the subsequent part-of-speech tagging to work. The conventional paradigm does not properly handle peculiarities of biomedical English. To remedy the problem, we propose morphological analysis"
W03-1309,N01-1025,1,0.876081,"and thus offers a simple way to incorporate biomedical resources in language processing. When a sentence is morphologically analyzed, miscellaneous information field is attached, which can be used for the feature extraction component. 2.2 BaseNP Recognition BaseNP recognition is applied to obtain approximate boundaries of BaseNPs in a sentence. The CoNLL-1999 shared task dataset is used for training with YamCha, the general purpose SVM-based chunker4 . There are four kinds of chunk tags in the CoNLL-1999 dataset, namely IOB1, IOB2, IOE1, and IOE2 (Tjong Kim Sang and Veenstra, 1999). We follow Kudo and Matsumoto (2001) to train four BaseNP recognizers, one for each chunk tag. The word-based output from the morphological analysis is cascaded to each BaseNP recognizer to mark BaseNP boundaries. We collect outputs from the 4 http://cl.aist-nara.ac.jp/˜taku−ku/software/yamcha/). four recognizers, and interpret the tag as outside of a BaseNP if all recognizers estimate the “O(utside)” tag, otherwise inside of a BaseNP. The intention is to distinguish words that are definitely not a constituent of a BaseNP (outside) from words that may be a constituent of a BaseNP (inside). In this way, we obtain approximate boun"
W03-1309,E99-1023,0,0.0276771,"exeme with a white space character, and thus offers a simple way to incorporate biomedical resources in language processing. When a sentence is morphologically analyzed, miscellaneous information field is attached, which can be used for the feature extraction component. 2.2 BaseNP Recognition BaseNP recognition is applied to obtain approximate boundaries of BaseNPs in a sentence. The CoNLL-1999 shared task dataset is used for training with YamCha, the general purpose SVM-based chunker4 . There are four kinds of chunk tags in the CoNLL-1999 dataset, namely IOB1, IOB2, IOE1, and IOE2 (Tjong Kim Sang and Veenstra, 1999). We follow Kudo and Matsumoto (2001) to train four BaseNP recognizers, one for each chunk tag. The word-based output from the morphological analysis is cascaded to each BaseNP recognizer to mark BaseNP boundaries. We collect outputs from the 4 http://cl.aist-nara.ac.jp/˜taku−ku/software/yamcha/). four recognizers, and interpret the tag as outside of a BaseNP if all recognizers estimate the “O(utside)” tag, otherwise inside of a BaseNP. The intention is to distinguish words that are definitely not a constituent of a BaseNP (outside) from words that may be a constituent of a BaseNP (inside). In"
W03-1309,A00-1032,1,0.818687,"cps start and ↓ a cps end. M is a mark and D is a delimiter. The cps starts and cps ends can be determined by marks M and delimiters D. → is a token found in the dictionary by common prefix search. A bold → is the optimal path in the trellis. w is a word. m is a morpheme. Morphological Analysis Our morphological analysis gives (a) sophisticated tokenization, (b) part-of-speech tagging and (c) annotation of value-added information such as the stemmed form of a word, accession numbers to biomedical resources. Our morphological analyzer for biomedical English, cocab2 , is inspired by the work of Yamashita and Matsumoto (2000). 2.1.1 Preliminaries We first define terms used in this paper with an illustration in Figure 2. A lexeme is an entry in a dictionary. A common prefix search (cps) is a standard technique for looking up lexemes in morphological analysis of nonsegmented languages. A dictionary is often a trie data structure so that all possible lexemes that match with the prefix starting at a given position in the sentence are retrieved efficiently. A common prefix search start position (cps start) is a position in a sentence at which a dictionary lookup can start. A common prefix search end position (cps end)"
W03-1309,C02-1110,0,\N,Missing
W03-1309,C00-1030,0,\N,Missing
W03-1720,N01-1025,1,0.898246,"m using the training material (See (Manning and Sch¨utze., 1999)). The learning process is based on the Baum-Welch algorithm and is the same as the well-known use of HMM for part-of-speech tagging problem, except that the number of states are arbitrarily determined and the initial probabilities are randomly assigned in our model. 1.2 Correction by Support Vector Machine-based Chunker While the HMM-based word segmenter achieves good accuracy for known words, it cannot identify compound words and out-of-vocabulary words. Therefore, we introduce a Support Vector Machine(below SVM)-based chunker (Kudo and Matsumoto, 2001) to cover the errors made by the segmenter. The SVM-based chunker re-assigns new word boundaries to the output of the segmenter. An SVM (Vapnik, 1998) is a binary classifier. Suppose we have a set of training data for a binary class problem: (x1 , y1 ), . . . , (xN , yN ), where xi ∈ Rn is a feature vector of the i th sample in the training data and yi ∈ {+1, −1} is the label of the sample. The goal is to find a decision function which accurately predicts y for an unseen x. An SVM classifier gives a decision function f (x) for an input vector x where X f (x) = sign( αi yi K(x, zi ) + b). zi ∈S"
W03-1720,W95-0107,0,0.0701678,"Missing"
W03-3023,A00-2018,0,0.198645,"ng process even in deterministic manner. The best result is (e) and comparable to the result of (b). The accuracy of (e) and (b) are superior to (c) and (d) even though feature set (e) and (b) include all of the lexical information appeared in the training examples and are much more higher dimensional feature space than both feature set (c) and (d). The results suggest that the lexical information is more effective features for dependency analysis. 5.2 Comparison with Related Work We compare our parser with other statistical parsers: maximum entropy inspired parser (MEIP) proposed by Charniak [3], and probabilistic generative model proposed by Collins [7]. These parsers produce phrase structure of sentences, thus the output in phrase structure for section 23 are converted into dependency trees using the same head rules of our parser 1 . The input sentences for the Collins parser are assigned part of speech by using the Nakagawa’s tagger for comparison. We didn’t use the Nakagawa’s tagger for input sentences of MEIP because MEIP can assigned the part of speech to each word of input sentences by itself. Our parse is trained with the best setting: Kernel function is (x0 · x00 + 1)2 , Con"
W03-3023,C96-1058,0,0.79429,"od knowledge of the target domain but lack deep linguistic knowledge. Besides, since annotating simpler structure is useful for reaching a consensus among annotators, it is expected that construction of training data will become more noise-free. In this paper, we focus on dependency structure analysis hoping for the possibility of preparing sufficient training data with less noise. While statistical parsers of phrase structure trees have been intensively studied as mentioned above, little has paid attention to statistical dependency analysis of English sentences. Eisner’s probabilistic models [5] propose methods of dependency parsing, but the accuracies are not satisfactorily high. We propose a new method for statistical dependency parsing of English said VBD S VP Inc. expects NNP VBZ SBAR -NONES VP Rolls-Royce Motor Cars it remain NNP NNP NNPS PRP VB S VP sales to steady at NNS TO JJ IN VP cars PP its U.S. PRP$ NNP NP NP Rolls-Royce Motor NNP NNP NP Cars Inc. said it NNPS NNP VBD PRP NP expects VBZ its ADJP U.S. sales PRP$ NNP to NNS TO remain steady VB JJ NNS about IN QP at about 1,200 cars IN IN CD NNS (a) 1,200 CD (b) Figure 1: Phrase structure tree and dependency tree: (a) is the"
W03-3023,P96-1025,0,0.0107593,"Missing"
W03-3023,P97-1003,0,0.0363188,"(e) and comparable to the result of (b). The accuracy of (e) and (b) are superior to (c) and (d) even though feature set (e) and (b) include all of the lexical information appeared in the training examples and are much more higher dimensional feature space than both feature set (c) and (d). The results suggest that the lexical information is more effective features for dependency analysis. 5.2 Comparison with Related Work We compare our parser with other statistical parsers: maximum entropy inspired parser (MEIP) proposed by Charniak [3], and probabilistic generative model proposed by Collins [7]. These parsers produce phrase structure of sentences, thus the output in phrase structure for section 23 are converted into dependency trees using the same head rules of our parser 1 . The input sentences for the Collins parser are assigned part of speech by using the Nakagawa’s tagger for comparison. We didn’t use the Nakagawa’s tagger for input sentences of MEIP because MEIP can assigned the part of speech to each word of input sentences by itself. Our parse is trained with the best setting: Kernel function is (x0 · x00 + 1)2 , Context length = (2,4), feature set is (e) in table 4. Table 5"
W03-3023,J93-2004,0,0.0457311,"Missing"
W03-3023,P02-1063,1,0.262451,"Missing"
W04-0402,P01-1008,0,0.0672653,"as those indicating that “to make an attempt” can be paraphrased into “to attempt,” and that “potential” can be paraphrased into “possibility.” Several attempts have been made to develop such resources manually (Sato, 1999; Dras, 1999; Inui and Nogami, 2001); those work have, however, tended to restrict their scope to specific classes of paraphrases, and cannot be used to construct a sufficiently comprehensive resource for practical applications. There is another trend in the research in this field, namely, the automatic acquisition of paraphrase patterns from parallel or comparable corpora (Barzilay and McKeown, 2001; Lin and Pantel, 2001; Pang et al., 2003; Shinyama and Sekine, 2003, etc.). This type of approach may be able to reduce the cost of resource development. There are problems that must be overcome, however, before they can work practically. First, automatically acquired patterns tend to be complex. For example, from the paraphrase of (4s) into (4t), we can naively obtain the pattern: “X is purchased by Y ⇒ Y buys X.” (4) s. This car was purchased by him. t. He bought this car. This could also, however, be regarded as a combination of a simpler pattern of lexical paraphrasing (“purchase ⇒ buy”)"
W04-0402,E99-1042,0,0.0232588,"ing LCS dictionary. Experimental results show that our LCS-based paraphrasing model characterizes some of the semantic features of those verbs required for generating paraphrases, such as the direction of an action and the relationship between arguments and surface cases. 1 Introduction Automatic paraphrase generation technology offers the potential to bridge gaps between the authors and readers of documents. For example, a system that is capable of simplifying a given text, or showing the user several alternative expressions conveying the same content, would be useful for assisting a reader (Carroll et al., 1999; Inui et al., 2003). In Japanese, like other languages, there are several classes of paraphrasing that exhibit a degree of regularity that allows them to be explained by a handful of sophisticated general rules and lexical semantic knowledge. For example, paraphrases associated with voice alteration, verb/case alteration, compounds, and lexical derivations all fall into such classes. In this paper, we focus our discussion on another useful class of paraphrases, namely, the paraphrasing of light-verb constructions (LVCs), and propose a computational model for generating paraphrases of this cla"
W04-0402,W01-0814,1,0.592452,"iments (Section 5). Finally, we conclude this paper with a brief of description of work to be done in the future (Section 6). 2 Motivation, target, and related work 2.1 Motivation One of the critical issues that we face in paraphrase generation is how to develop and maintain knowledge resources that covers a sufficiently wide range of paraphrasing patterns such as those indicating that “to make an attempt” can be paraphrased into “to attempt,” and that “potential” can be paraphrased into “possibility.” Several attempts have been made to develop such resources manually (Sato, 1999; Dras, 1999; Inui and Nogami, 2001); those work have, however, tended to restrict their scope to specific classes of paraphrases, and cannot be used to construct a sufficiently comprehensive resource for practical applications. There is another trend in the research in this field, namely, the automatic acquisition of paraphrase patterns from parallel or comparable corpora (Barzilay and McKeown, 2001; Lin and Pantel, 2001; Pang et al., 2003; Shinyama and Sekine, 2003, etc.). This type of approach may be able to reduce the cost of resource development. There are problems that must be overcome, however, before they can work practi"
W04-0402,W03-1602,1,0.844244,"perimental results show that our LCS-based paraphrasing model characterizes some of the semantic features of those verbs required for generating paraphrases, such as the direction of an action and the relationship between arguments and surface cases. 1 Introduction Automatic paraphrase generation technology offers the potential to bridge gaps between the authors and readers of documents. For example, a system that is capable of simplifying a given text, or showing the user several alternative expressions conveying the same content, would be useful for assisting a reader (Carroll et al., 1999; Inui et al., 2003). In Japanese, like other languages, there are several classes of paraphrasing that exhibit a degree of regularity that allows them to be explained by a handful of sophisticated general rules and lexical semantic knowledge. For example, paraphrases associated with voice alteration, verb/case alteration, compounds, and lexical derivations all fall into such classes. In this paper, we focus our discussion on another useful class of paraphrases, namely, the paraphrasing of light-verb constructions (LVCs), and propose a computational model for generating paraphrases of this class. Sentence (1s) is"
W04-0402,W02-2016,1,0.676943,"alized verbs. We retrieved 1,210 nominalized verbs from the TLCS dictionary. Light-verbs: Since a verb takes different meanings when it is a part of LVCs with different case particles, we collected pairs c, v of case particle c and verb v in the following way: Step 1. We collected 876,101 types of triplets n, c, v of nominalized verb n, case particle c, and base form of verb v from the parsed5 sentences of newspaper articles6 . 4 A sahen-noun is a verbal noun in Japanese, which acts as a verb in the form of “sahen-noun + suru”. 5 We used the statistical Japanese dependency parser CaboCha (Kudo and Matsumoto, 2002) for parsing. http://chasen.naist.jp/˜taku/software/cabocha/ 6 Excerpts from 9 years of the Mainichi Shinbun and 10 years of the Nihon Keizai Shinbun, giving a total of 25,061,504 sentences, were used. Table 2: Extensions of LCS Ext.1 Verb hankou-suru (resist) Ext.2 ukeru (receive) Ext.3 motomeru (ask) Ext.4 kandou-suru (be impressed) Verb phrase and its LCS representation [[Ken]y BE AGAINST [parents]z] Ken-ga oya-ni hankou-suru. Ken-NOM parents-DAT resist- PRES (Ken resists his parents.) [BECOME [[salesclerk]z BE WITH [[complaint]y MOVE FROM [customer]x TO [salesclerk]z]]] Ten’in-ga kyaku-kar"
W04-0402,J87-3006,0,0.0613912,"Missing"
W04-0402,N03-1024,0,0.0405069,"be paraphrased into “to attempt,” and that “potential” can be paraphrased into “possibility.” Several attempts have been made to develop such resources manually (Sato, 1999; Dras, 1999; Inui and Nogami, 2001); those work have, however, tended to restrict their scope to specific classes of paraphrases, and cannot be used to construct a sufficiently comprehensive resource for practical applications. There is another trend in the research in this field, namely, the automatic acquisition of paraphrase patterns from parallel or comparable corpora (Barzilay and McKeown, 2001; Lin and Pantel, 2001; Pang et al., 2003; Shinyama and Sekine, 2003, etc.). This type of approach may be able to reduce the cost of resource development. There are problems that must be overcome, however, before they can work practically. First, automatically acquired patterns tend to be complex. For example, from the paraphrase of (4s) into (4t), we can naively obtain the pattern: “X is purchased by Y ⇒ Y buys X.” (4) s. This car was purchased by him. t. He bought this car. This could also, however, be regarded as a combination of a simpler pattern of lexical paraphrasing (“purchase ⇒ buy”) and a voice activization (“X (b) Noun + C"
W04-0402,W03-1609,0,0.0123349,"o “to attempt,” and that “potential” can be paraphrased into “possibility.” Several attempts have been made to develop such resources manually (Sato, 1999; Dras, 1999; Inui and Nogami, 2001); those work have, however, tended to restrict their scope to specific classes of paraphrases, and cannot be used to construct a sufficiently comprehensive resource for practical applications. There is another trend in the research in this field, namely, the automatic acquisition of paraphrase patterns from parallel or comparable corpora (Barzilay and McKeown, 2001; Lin and Pantel, 2001; Pang et al., 2003; Shinyama and Sekine, 2003, etc.). This type of approach may be able to reduce the cost of resource development. There are problems that must be overcome, however, before they can work practically. First, automatically acquired patterns tend to be complex. For example, from the paraphrase of (4s) into (4t), we can naively obtain the pattern: “X is purchased by Y ⇒ Y buys X.” (4) s. This car was purchased by him. t. He bought this car. This could also, however, be regarded as a combination of a simpler pattern of lexical paraphrasing (“purchase ⇒ buy”) and a voice activization (“X (b) Noun + Case Particle (c) Noun + Cas"
W04-3230,C00-1004,1,0.500068,"was to predict label ∗ At present, NTT Communication Science Laboratories, 2-4, Hikaridai, Seika-cho, Soraku, Kyoto, 619-0237 Japan taku@cslab.kecl.ntt.co.jp sequence (e.g. part-of-speech). However, word boundaries are not clear in non-segmented languages. One has to identify word segmentation as well as to predict part-of-speech in morphological analysis of non-segmented languages. In this paper, we show how CRFs can be applied to situations where word boundary ambiguity exists. CRFs offer a solution to the problems in Japanese morphological analysis with hidden Markov models (HMMs) (e.g., (Asahara and Matsumoto, 2000)) or with maximum entropy Markov models (MEMMs) (e.g., (Uchimoto et al., 2001)). First, as HMMs are generative, it is hard to employ overlapping features stemmed from hierarchical tagsets and nonindependent features of the inputs such as surrounding words, word suffixes and character types. These features have usually been ignored in HMMs, despite their effectiveness in unknown word guessing. Second, as mentioned in the literature, MEMMs could evade neither from label bias (Lafferty et al., 2001) nor from length bias (a bias occurring because of word boundary ambiguity). Easy sequences with lo"
W04-3230,N04-1039,0,0.0307754,"Missing"
W04-3230,W03-0430,0,0.339972,"y solve the long-standing problems but also improve the performance over HMMs and MEMMs. 1 Introduction Conditional random fields (CRFs) (Lafferty et al., 2001) applied to sequential labeling problems are conditional models, trained to discriminate the correct sequence from all other candidate sequences without making independence assumption for features. They are considered to be the state-of-the-art framework to date. Empirical successes with CRFs have been reported recently in part-of-speech tagging (Lafferty et al., 2001), shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003), Chinese word segmentation (Peng et al., 2004), and Information Extraction (Pinto et al., 2003; Peng and McCallum, 2004). Previous applications with CRFs assumed that observation sequence (e.g. word) boundaries are fixed, and the main focus was to predict label ∗ At present, NTT Communication Science Laboratories, 2-4, Hikaridai, Seika-cho, Soraku, Kyoto, 619-0237 Japan taku@cslab.kecl.ntt.co.jp sequence (e.g. part-of-speech). However, word boundaries are not clear in non-segmented languages. One has to identify word segmentation as well as to predict part-of-speech in morphological analysis"
W04-3230,C04-1081,0,0.518633,"the performance over HMMs and MEMMs. 1 Introduction Conditional random fields (CRFs) (Lafferty et al., 2001) applied to sequential labeling problems are conditional models, trained to discriminate the correct sequence from all other candidate sequences without making independence assumption for features. They are considered to be the state-of-the-art framework to date. Empirical successes with CRFs have been reported recently in part-of-speech tagging (Lafferty et al., 2001), shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003), Chinese word segmentation (Peng et al., 2004), and Information Extraction (Pinto et al., 2003; Peng and McCallum, 2004). Previous applications with CRFs assumed that observation sequence (e.g. word) boundaries are fixed, and the main focus was to predict label ∗ At present, NTT Communication Science Laboratories, 2-4, Hikaridai, Seika-cho, Soraku, Kyoto, 619-0237 Japan taku@cslab.kecl.ntt.co.jp sequence (e.g. part-of-speech). However, word boundaries are not clear in non-segmented languages. One has to identify word segmentation as well as to predict part-of-speech in morphological analysis of non-segmented languages. In this paper, we s"
W04-3230,N03-1028,0,0.0585094,"n this task. Our results confirm that CRFs not only solve the long-standing problems but also improve the performance over HMMs and MEMMs. 1 Introduction Conditional random fields (CRFs) (Lafferty et al., 2001) applied to sequential labeling problems are conditional models, trained to discriminate the correct sequence from all other candidate sequences without making independence assumption for features. They are considered to be the state-of-the-art framework to date. Empirical successes with CRFs have been reported recently in part-of-speech tagging (Lafferty et al., 2001), shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003), Chinese word segmentation (Peng et al., 2004), and Information Extraction (Pinto et al., 2003; Peng and McCallum, 2004). Previous applications with CRFs assumed that observation sequence (e.g. word) boundaries are fixed, and the main focus was to predict label ∗ At present, NTT Communication Science Laboratories, 2-4, Hikaridai, Seika-cho, Soraku, Kyoto, 619-0237 Japan taku@cslab.kecl.ntt.co.jp sequence (e.g. part-of-speech). However, word boundaries are not clear in non-segmented languages. One has to identify word segmentation as well as to"
W04-3230,W01-0512,0,0.0447531,"aridai, Seika-cho, Soraku, Kyoto, 619-0237 Japan taku@cslab.kecl.ntt.co.jp sequence (e.g. part-of-speech). However, word boundaries are not clear in non-segmented languages. One has to identify word segmentation as well as to predict part-of-speech in morphological analysis of non-segmented languages. In this paper, we show how CRFs can be applied to situations where word boundary ambiguity exists. CRFs offer a solution to the problems in Japanese morphological analysis with hidden Markov models (HMMs) (e.g., (Asahara and Matsumoto, 2000)) or with maximum entropy Markov models (MEMMs) (e.g., (Uchimoto et al., 2001)). First, as HMMs are generative, it is hard to employ overlapping features stemmed from hierarchical tagsets and nonindependent features of the inputs such as surrounding words, word suffixes and character types. These features have usually been ignored in HMMs, despite their effectiveness in unknown word guessing. Second, as mentioned in the literature, MEMMs could evade neither from label bias (Lafferty et al., 2001) nor from length bias (a bias occurring because of word boundary ambiguity). Easy sequences with low entropy are likely to be selected during decoding in MEMMs. The consequence"
W04-3230,C02-2019,0,0.0193982,"ignore the influence of the length bias either. By the length bias, we mean that short paths, consisting of a small number of tokens, are preferred to long path. Even if the transition probability of each token is small, the total probability of the path will be amplified when the path is short 2:(b)). Length bias occurs in Japanese morphological analysis because the number of output tokens y varies by use of prior lexicons. Uchimoto et al. attempted a variant of MEMMs for Japanese morphological analysis with a number of features including suffixes and character types (Uchimoto et al., 2001; Uchimoto et al., 2002; Uchimoto et al., 2003). Although the performance of unknown words were improved, that of known words degraded due to the label and length bias. Wrong segmentation had been reported in sentences which are analyzed correctly by naive rule-based or HMMs-based analyzers. 3 Conditional Random Fields Conditional random fields (CRFs) (Lafferty et al., 2001) overcome the problems described in Section 2.2. CRFs are discriminative models and can thus capture many correlated features of the inputs. This allows flexible feature designs for hierarchical tagsets. CRFs have a single exponential model for t"
W04-3230,P03-1061,0,\N,Missing
W04-3230,N04-1042,0,\N,Missing
W04-3239,P02-1034,0,0.648649,"set of all subtrees (bag-of-subtrees) for the feature set without any constraints. iii) Even though the size of the candidate feature set becomes quite large, it automatically selects a compact and relevant feature set based on Boosting. This paper is organized as follows. First, we describe the details of our Boosting algorithm in which the subtree-based decision stumps are applied as weak learners. Second, we show an implementation issue related to constructing an efficient learning algorithm. We also discuss the relation between our algorithm and SVMs (Boser et al., 1992) with tree kernel (Collins and Duffy, 2002; Kashima and Koyanagi, 2002). Two experiments on the opinion and modality classification tasks are employed to confirm that subtree features are important. 2 Classifier for Trees We first assume that a text to be classified is represented as a labeled ordered tree. The focused problem can be formalized as a general problem, called the tree classification problem. The tree classification problem is to induce a mapping f (x) : X → {±1}, from given training examples T = {hxi , yi i}L i=1 , where xi ∈ X is a labeled ordered tree and yi ∈ {±1} is a class label associated with each training data (w"
W04-3239,P02-1053,0,0.00567612,"s because each word occurring in the text is highly relevant to the predefined “topics” to be identified. ∗ At present, NTT Communication Science Laboratories, 2-4, Hikaridai, Seika-cho, Soraku, Kyoto, 619-0237 Japan taku@cslab.kecl.ntt.co.jp Given that a number of successes have been reported in the field of traditional text classification, the focus of recent research has expanded from simple topic identification to more challenging tasks such as opinion/modality identification. Example includes categorization of customer E-mails and reviews by types of claims, modalities or subjectivities (Turney, 2002; Wiebe, 2000). For the latter, the traditional bag-of-words representation is not sufficient, and a richer, structural representation is required. A straightforward way to extend the traditional bag-of-words representation is to heuristically add new types of features to the original bag-of-words features, such as fixed-length n-grams (e.g., word bi-gram or tri-gram) or fixedlength syntactic relations (e.g., modifier-head relations). These ad-hoc solutions might give us reasonable performance, however, they are highly taskdependent and require careful design to create the “optimal” feature se"
W06-2927,I05-3003,1,0.876312,"Missing"
W06-2927,1995.iwpt-1.17,0,0.063867,"g of a preposition which includes few children is usually a verb; on the other hand, the real POS tag of a preposition is usually a preposition. If our parser considers the preposition which leads a short phrase, the parser will estimate the relation of the preposition as a verb. At the same time, if the boundary of prepositional phrase is analyzed incorrectly, other succeeding words will be wrongly analyzed, too. Error analysis of Japanese data (Kawata and Bartels, 2000) shows that CNJ (Conjunction) is a difficult POS tag. The parser does not have any module to detect coordinate structures. (Kurohashi, 1995) proposed a method in which coordinate structure with punctuation is detected by a coeffi195 Conclusion This paper reported on multi-lingual dependency parsing on combining SVMs and MaxEnt. The system uses SVMs for word dependency attachment analysis and MaxEnt for the label tagging when the new dependency attachment is generated. We discussed some preprocessing methods that are useful in our preceding work for Chinese dependency analysis, but these methods, except one, cannot be used in multi-lingual dependency parsing. Only using the SVM-based tagger to extract the neighbor relation could im"
W06-2927,W04-0308,0,0.293436,"we not only adopted the Nivre algorithm with SVMs, but also tried some preprocessing methods. We investigated several preprocessing methods on a Chinese Treebank. In this shared task (Buchholz et. al, 2006), we also investigate which preprocessing method is effective on other languages. We found that only the method that uses a tagger to extract the word dependency attachment between two neighboring words works effectively in most of the languages. Abstract In this paper, we present a framework for multi-lingual dependency parsing. Our bottom-up deterministic parser adopts Nivre’s algorithm (Nivre, 2004) with a preprocessor. Support Vector Machines (SVMs) are utilized to determine the word dependency attachments. Then, a maximum entropy method (MaxEnt) is used for determining the label of the dependency relation. To improve the performance of the parser, we construct a tagger based on SVMs to find neighboring attachment as a preprocessor. Experimental evaluation shows that the proposed extension improves the parsing accuracy of our base parser in 9 languages. (Hajič et al., 2004; Simov et al., 2005; Simov and Osenova, 2003; Chen et al., 2003; Böhmová et al., 2003; Kromann, 2003; van der Beek"
W06-2927,W03-2403,0,\N,Missing
W06-2927,W06-2920,0,\N,Missing
W06-2927,dzeroski-etal-2006-towards,0,\N,Missing
W06-2927,W03-2405,0,\N,Missing
W07-1522,P06-1079,1,0.472904,"rpus version 4.0 (Kawahara et al., 2002) and the GDATagged Corpus (Hasida, 2005). However, there is still much room for reﬁning their speciﬁcations. For this reason, we discuss issues in annotating these two types of relations, and propose a new speciﬁcation for each. In accordance with the speciﬁcation, we built a large-scaled annotated corpus, and examined its reliability. As a result of our current work, we have released an annotated corpus named the NAIST Text Corpus1 , which is used as the evaluation data set in the coreference and zero-anaphora resolution tasks in Iida et al. (2005) and Iida et al. (2006). 1 Introduction Coreference resolution and predicate-argument structure analysis has recently been a growing ﬁeld of research due to the demands from NLP application such as information extraction and machine translation. With the research focus placed on these tasks, the speciﬁcation of annotating corpora and the 1 The NAIST Text Corpus is downloadable from http://cl.naist.jp/nldata/corpus/, and it has already been downloaded by 102 unique users. data sets used in supervised techniques (Soon et al., 2001; Ng and Cardie, 2002, etc.) have also grown in sophistication. For English, several anno"
W07-1522,kawahara-etal-2002-construction,0,0.390972,"Japanese Text Corpus with Predicate-Argument and Coreference Relations Ryu Iida, Mamoru Komachi, Kentaro Inui and Yuji Matsumoto Graduate School of Information Science, Nara Institute of Science and Technology 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan {ryu-i,mamoru-k,inui,matsu}@is.naist.jp Abstract In this paper, we discuss how to annotate coreference and predicate-argument relations in Japanese written text. There have been research activities for building Japanese text corpora annotated with coreference and predicate-argument relations as are done in the Kyoto Text Corpus version 4.0 (Kawahara et al., 2002) and the GDATagged Corpus (Hasida, 2005). However, there is still much room for reﬁning their speciﬁcations. For this reason, we discuss issues in annotating these two types of relations, and propose a new speciﬁcation for each. In accordance with the speciﬁcation, we built a large-scaled annotated corpus, and examined its reliability. As a result of our current work, we have released an annotated corpus named the NAIST Text Corpus1 , which is used as the evaluation data set in the coreference and zero-anaphora resolution tasks in Iida et al. (2005) and Iida et al. (2006). 1 Introduction Coref"
W07-1522,P02-1014,0,0.136204,"ference and zero-anaphora resolution tasks in Iida et al. (2005) and Iida et al. (2006). 1 Introduction Coreference resolution and predicate-argument structure analysis has recently been a growing ﬁeld of research due to the demands from NLP application such as information extraction and machine translation. With the research focus placed on these tasks, the speciﬁcation of annotating corpora and the 1 The NAIST Text Corpus is downloadable from http://cl.naist.jp/nldata/corpus/, and it has already been downloaded by 102 unique users. data sets used in supervised techniques (Soon et al., 2001; Ng and Cardie, 2002, etc.) have also grown in sophistication. For English, several annotation schemes have already been proposed for both coreference relation and argument structure, and annotated corpora have been developed accordingly (Hirschman, 1997; Poesio et al., 2004; Doddington et al., 2004). For instance, in the Coreference task on Message Understanding Conference (MUC) and the Entity Detection and Tracking (EDT) task in the Automatic Content Extraction (ACE) program, which is the successor of MUC, the details of speciﬁcation of annotating coreference relation have been discussed for several years. On t"
W07-1522,J05-1004,0,0.129176,"notated corpora have been developed accordingly (Hirschman, 1997; Poesio et al., 2004; Doddington et al., 2004). For instance, in the Coreference task on Message Understanding Conference (MUC) and the Entity Detection and Tracking (EDT) task in the Automatic Content Extraction (ACE) program, which is the successor of MUC, the details of speciﬁcation of annotating coreference relation have been discussed for several years. On the other hand, the speciﬁcation of predicate-argument structure analysis has mainly been discussed in the context of the CoNLL shared task2 on the basis of the PropBank (Palmer et al., 2005). In parallel with these efforts, there have also been research activities for building Japanese text corpora annotated with coreference and predicate-argument relations such as the Kyoto Text Corpus version 4.0 (Kawahara et al., 2002) and the GDA3 -Tagged Corpus (Hasida, 2005). However, as we discuss in this paper, there is still much room for arguing and reﬁning the speciﬁcation of such sorts of semantic annotation. In fact, for neither of the above two corpora, the adequacy and reliability of the annotation scheme has been deeply examined. In this paper, we discuss how to annotate coreferen"
W07-1522,P04-1019,0,0.0132674,"uch as information extraction and machine translation. With the research focus placed on these tasks, the speciﬁcation of annotating corpora and the 1 The NAIST Text Corpus is downloadable from http://cl.naist.jp/nldata/corpus/, and it has already been downloaded by 102 unique users. data sets used in supervised techniques (Soon et al., 2001; Ng and Cardie, 2002, etc.) have also grown in sophistication. For English, several annotation schemes have already been proposed for both coreference relation and argument structure, and annotated corpora have been developed accordingly (Hirschman, 1997; Poesio et al., 2004; Doddington et al., 2004). For instance, in the Coreference task on Message Understanding Conference (MUC) and the Entity Detection and Tracking (EDT) task in the Automatic Content Extraction (ACE) program, which is the successor of MUC, the details of speciﬁcation of annotating coreference relation have been discussed for several years. On the other hand, the speciﬁcation of predicate-argument structure analysis has mainly been discussed in the context of the CoNLL shared task2 on the basis of the PropBank (Palmer et al., 2005). In parallel with these efforts, there have also been research a"
W07-1522,J01-4004,0,0.294742,"ata set in the coreference and zero-anaphora resolution tasks in Iida et al. (2005) and Iida et al. (2006). 1 Introduction Coreference resolution and predicate-argument structure analysis has recently been a growing ﬁeld of research due to the demands from NLP application such as information extraction and machine translation. With the research focus placed on these tasks, the speciﬁcation of annotating corpora and the 1 The NAIST Text Corpus is downloadable from http://cl.naist.jp/nldata/corpus/, and it has already been downloaded by 102 unique users. data sets used in supervised techniques (Soon et al., 2001; Ng and Cardie, 2002, etc.) have also grown in sophistication. For English, several annotation schemes have already been proposed for both coreference relation and argument structure, and annotated corpora have been developed accordingly (Hirschman, 1997; Poesio et al., 2004; Doddington et al., 2004). For instance, in the Coreference task on Message Understanding Conference (MUC) and the Entity Detection and Tracking (EDT) task in the Automatic Content Extraction (ACE) program, which is the successor of MUC, the details of speciﬁcation of annotating coreference relation have been discussed fo"
W07-1522,P06-2105,0,0.00720716,"ikely to be more cost-efﬁcient than semantic roles 134 because they are often explicitly marked by case markers. This fact also allows us to avoid the difﬁculties in deﬁning a label set. • In Japanese, the mapping from syntactic cases to semantic roles tends to be reasonably straightforward if a semantically rich lexicon of verbs like the VerbNet (Kipper et al., 2000) is available. • Furthermore, we have not yet found many NLP applications for which the utility of semantic roles is actually demonstrated. One may think of using semantic roles in textual inference as exempliﬁed by, for example, Tatu and Moldovan (2006). However, similar sort of inference may well be realized with syntactic cases as demonstrated in the information extraction and question answering literature. Taking these respects into account, we choose to label predicate-argument relations in terms of syntactic cases, which follows the annotation scheme adopted in the Kyoto Corpus. 3.2 Syntactic case alternation Once the level of syntactic cases is chosen for our annotation, another issue immediately arises, alteration of syntactic cases by syntactic transformations such as passivization and causativization. For example, sentence (5) is an"
W07-1522,W99-0213,0,0.0702454,"Missing"
W07-1522,M95-1005,0,0.107471,"Missing"
W07-1522,W04-2705,0,\N,Missing
W07-1522,M98-1029,0,\N,Missing
W08-2132,W06-2920,0,0.119472,"Missing"
W08-2132,J02-3001,0,0.0502326,"nd that for noun predicates. Semantic frames are useful information for semantic role classiﬁcation. Generally, obligatory arguments not included in semantic frames do not appear in actual texts. For this reason, we use PropBank/NomBank semantic frames for semantic role pruning. Suppose semantic roles in the semantic frame are Fi = {A0, A1, A2, A3}. Since obligatory arguments are {A0...AA}, the remaining arguments {A4, A5, AA} are removed from label candidates. For verb predicates, the features used in our system are based on (Hacioglu, 2004). We also employed some other features proposed in (Gildea and Jurafsky, 2002; Pradhan et al., 2004b). For noun predicates, the features are primarily based on (Pradhan et al., 2004a). The features that we deﬁned for semantic role labeling are as follows: Word features: SPLIT LEMMA and PPOS of the predicate, dependent and dependent’s head, and its conjunctions. Dependency label: The dependency label between the argument candidate and the its head. Family: The position of the argument candidate with respect to the predicate position over the dependency tree (e.g., child, sibling). Position: The position of the head of the dependency relation with respect to the predicat"
W08-2132,C04-1186,0,0.0181747,"on, we created two models: argument classiﬁer for verb predicates and that for noun predicates. Semantic frames are useful information for semantic role classiﬁcation. Generally, obligatory arguments not included in semantic frames do not appear in actual texts. For this reason, we use PropBank/NomBank semantic frames for semantic role pruning. Suppose semantic roles in the semantic frame are Fi = {A0, A1, A2, A3}. Since obligatory arguments are {A0...AA}, the remaining arguments {A4, A5, AA} are removed from label candidates. For verb predicates, the features used in our system are based on (Hacioglu, 2004). We also employed some other features proposed in (Gildea and Jurafsky, 2002; Pradhan et al., 2004b). For noun predicates, the features are primarily based on (Pradhan et al., 2004a). The features that we deﬁned for semantic role labeling are as follows: Word features: SPLIT LEMMA and PPOS of the predicate, dependent and dependent’s head, and its conjunctions. Dependency label: The dependency label between the argument candidate and the its head. Family: The position of the argument candidate with respect to the predicate position over the dependency tree (e.g., child, sibling). Position: The"
W08-2132,W03-2604,1,0.810969,"uration, which achieved 80.89 F1 for the joint task, and 74.53 semantic dependencies F1. The result shows that the conﬁguration of pipeline is a crucial issue in the task. Figure 1: Overview of the System This paper presents the description of our system in CoNLL-2008 shared task. We split the shared task into ﬁve sub-problems – syntactic dependency parsing, syntactic dependency label classiﬁcation, predicate identiﬁcation, word sense disambiguation, and semantic role labeling. The overview of our system is illustrated in Figure 1. Our dependency parsing module is based on a tournament model (Iida et al., 2003), in which a dependency attachment is estimated in step-ladder tournament matches. The relative preference of the attachment is modeled by one-on-one match in the tournament. Iwatate et al. (Iwatate et al., 2008) initially proposed the method for Japanese dependency parsing, and we applied it to other languages by relaxing some constraints (Section 2.1). Dependency label classiﬁcation is performed by a linearchain sequential labeling on the dependency siblings like McDonald’s schemata (McDonald et al., 2006). We use an online passive-aggressive algorithm (Crammer et al., 2006) for linear-chain"
W08-2132,C08-1046,1,0.910766,"paper presents the description of our system in CoNLL-2008 shared task. We split the shared task into ﬁve sub-problems – syntactic dependency parsing, syntactic dependency label classiﬁcation, predicate identiﬁcation, word sense disambiguation, and semantic role labeling. The overview of our system is illustrated in Figure 1. Our dependency parsing module is based on a tournament model (Iida et al., 2003), in which a dependency attachment is estimated in step-ladder tournament matches. The relative preference of the attachment is modeled by one-on-one match in the tournament. Iwatate et al. (Iwatate et al., 2008) initially proposed the method for Japanese dependency parsing, and we applied it to other languages by relaxing some constraints (Section 2.1). Dependency label classiﬁcation is performed by a linearchain sequential labeling on the dependency siblings like McDonald’s schemata (McDonald et al., 2006). We use an online passive-aggressive algorithm (Crammer et al., 2006) for linear-chain sequential labeling (Section 2.2). We also use the other linear-chain sequential labeling method to annotate whether each word is a predicate or not (Section 2.3). If an identiﬁed predicate has more than one sen"
W08-2132,N04-4036,0,0.0182715,". Semantic frames are useful information for semantic role classiﬁcation. Generally, obligatory arguments not included in semantic frames do not appear in actual texts. For this reason, we use PropBank/NomBank semantic frames for semantic role pruning. Suppose semantic roles in the semantic frame are Fi = {A0, A1, A2, A3}. Since obligatory arguments are {A0...AA}, the remaining arguments {A4, A5, AA} are removed from label candidates. For verb predicates, the features used in our system are based on (Hacioglu, 2004). We also employed some other features proposed in (Gildea and Jurafsky, 2002; Pradhan et al., 2004b). For noun predicates, the features are primarily based on (Pradhan et al., 2004a). The features that we deﬁned for semantic role labeling are as follows: Word features: SPLIT LEMMA and PPOS of the predicate, dependent and dependent’s head, and its conjunctions. Dependency label: The dependency label between the argument candidate and the its head. Family: The position of the argument candidate with respect to the predicate position over the dependency tree (e.g., child, sibling). Position: The position of the head of the dependency relation with respect to the predicate position in the sent"
W08-2132,N04-1030,0,0.032753,". Semantic frames are useful information for semantic role classiﬁcation. Generally, obligatory arguments not included in semantic frames do not appear in actual texts. For this reason, we use PropBank/NomBank semantic frames for semantic role pruning. Suppose semantic roles in the semantic frame are Fi = {A0, A1, A2, A3}. Since obligatory arguments are {A0...AA}, the remaining arguments {A4, A5, AA} are removed from label candidates. For verb predicates, the features used in our system are based on (Hacioglu, 2004). We also employed some other features proposed in (Gildea and Jurafsky, 2002; Pradhan et al., 2004b). For noun predicates, the features are primarily based on (Pradhan et al., 2004a). The features that we deﬁned for semantic role labeling are as follows: Word features: SPLIT LEMMA and PPOS of the predicate, dependent and dependent’s head, and its conjunctions. Dependency label: The dependency label between the argument candidate and the its head. Family: The position of the argument candidate with respect to the predicate position over the dependency tree (e.g., child, sibling). Position: The position of the head of the dependency relation with respect to the predicate position in the sent"
W08-2132,W04-3212,0,0.0263834,"e reason of the result of semantic role labeling could be usages of PropBank/NomBank frames. We did not achieve the maximum use of the resources, hence the design of features and the choice of learning algorithm may not be optimal. Figure 4: Overview of the Modiﬁed System The other reason is the design of the pipeline. We changed the design of the pipeline after the test run. The overview of the modiﬁed system is illustrated in Figure 4. After the syntactic dependency parsing, we limited the predicate candidates as verbs and nouns by PPOSS, and ﬁltered the argument candidates by Xue’s method (Xue and Palmer, 2004). Next, the candidate pair of predicate-argument was classiﬁed by an online passive-aggressive algorithm as shown in Section 2.5. Finally, the word sense of the predicate is determined by the module in Section 2.4. The new result is scores with ∗ in Table 1. The result means that the ﬁrst design was not the best for the task. Acknowledgements We would like to thank the CoNLL-2008 shared task organizers and the data providers (Surdeanu et al., 2008). 231 Problem Complete Problem Semantic Dependency Semantic Role Labeling Predicate Identiﬁcation & Word Sense Disambiguation Syntactic Dependency ("
W08-2132,W06-2932,0,0.168294,"is illustrated in Figure 1. Our dependency parsing module is based on a tournament model (Iida et al., 2003), in which a dependency attachment is estimated in step-ladder tournament matches. The relative preference of the attachment is modeled by one-on-one match in the tournament. Iwatate et al. (Iwatate et al., 2008) initially proposed the method for Japanese dependency parsing, and we applied it to other languages by relaxing some constraints (Section 2.1). Dependency label classiﬁcation is performed by a linearchain sequential labeling on the dependency siblings like McDonald’s schemata (McDonald et al., 2006). We use an online passive-aggressive algorithm (Crammer et al., 2006) for linear-chain sequential labeling (Section 2.2). We also use the other linear-chain sequential labeling method to annotate whether each word is a predicate or not (Section 2.3). If an identiﬁed predicate has more than one sense, a nearest neighbour classiﬁer disambiguates the word sense candidates (Section 2.4). We use an online passive-aggressive algorithm again for the semantic role labeling (Section 2.5). The machine learning algorithms used in separated modules are diverse due to role sharing.1 c 2008.  Licensed und"
W08-2132,W08-2121,0,\N,Missing
W08-2132,D07-1096,0,\N,Missing
W09-1218,burchardt-etal-2006-salsa,0,0.0332761,"Missing"
W09-1218,D07-1101,0,0.173593,"ctic-semantic structure among all possible structures in that syntactic dependencies and semantic dependencies are correlated. However, solving this problem is too difﬁcult because the search space of the problem is extremely large. Therefore we focus on improving performance for each subproblem: dependency parsing and semantic role labeling. In the past few years, research investigating higher-order dependency parsing algorithms has found its superiority to ﬁrst-order parsing algorithms. To reap the beneﬁts of these advances, we 114 use a higher-order projective dependency parsing algorithm (Carreras, 2007) which is an extension of the span-based parsing algorithm (Eisner, 1996), for syntactic dependency parsing. In terms of semantic role labeling, we would like to capture global information about predicateargument structures in order to accurately predict the correct predicate-argument structure. Previous research dealt with such information using re-ranking (Toutanova et al., 2005; Johansson and Nugues, 2008). We explore a different approach to deal with such information using global features. Use of global features for structured prediction problem has been explored by several NLP application"
W09-1218,C96-1058,0,0.0582791,"Missing"
W09-1218,P05-1045,0,0.00368784,"pan-based parsing algorithm (Eisner, 1996), for syntactic dependency parsing. In terms of semantic role labeling, we would like to capture global information about predicateargument structures in order to accurately predict the correct predicate-argument structure. Previous research dealt with such information using re-ranking (Toutanova et al., 2005; Johansson and Nugues, 2008). We explore a different approach to deal with such information using global features. Use of global features for structured prediction problem has been explored by several NLP applications such as sequential labeling (Finkel et al., 2005; Krishnan and Manning, 2006; Kazama and Torisawa, 2007) and dependency parsing (Nakagawa, 2007) with a great deal of success. We attempt to use global features for argument classiﬁcation in which the most plausible semantic role assignment is selected using both local and global information. We present an approximate max-margin learning algorithm for argument classiﬁers with global features. 2 Dependency Parsing As in previous work, we use a linear model for dependency parsing. The score function used in our dependency parser is deﬁned as follows. s(y) = ∑ F (h, m, x) (1) (h,m)∈y where h and"
W09-1218,W08-2122,0,0.0258264,"Although our system is not a joint approach but a pipeline approach, the system is comparable to the top system for some of the 7 languages. A further research direction we are investigating is the application of various types of global features. We believe that there is still room for improvements since we used only two types of global features for the argument classiﬁer. Another research direction is investigating joint approaches. To the best of our knowledge, three types of joint approaches have been proposed: N-best based approach (Johansson and Nugues, 2008), synchronous joint approach (Henderson et al., 2008), and a joint approach where parsing and SRL are performed simultaneously (Llu´ıs and M`arquez, 2008). We attempted to perform Nbest based joint approach, however, the expensive computational cost of the 2nd-order projective parser discouraged it. We would like to investigate syntactic-semantic joint approaches with reasonable time complexities. Acknowledgments We would like to thank Richard Johansson for his advice on parser implementation, and the CoNLL2009 organizers (Hajiˇc et al., 2009; Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al."
W09-1218,W08-2123,0,0.114866,"Missing"
W09-1218,kawahara-etal-2002-construction,0,0.0181481,"Missing"
W09-1218,D07-1033,0,0.0793895,"yntactic dependency parsing. In terms of semantic role labeling, we would like to capture global information about predicateargument structures in order to accurately predict the correct predicate-argument structure. Previous research dealt with such information using re-ranking (Toutanova et al., 2005; Johansson and Nugues, 2008). We explore a different approach to deal with such information using global features. Use of global features for structured prediction problem has been explored by several NLP applications such as sequential labeling (Finkel et al., 2005; Krishnan and Manning, 2006; Kazama and Torisawa, 2007) and dependency parsing (Nakagawa, 2007) with a great deal of success. We attempt to use global features for argument classiﬁcation in which the most plausible semantic role assignment is selected using both local and global information. We present an approximate max-margin learning algorithm for argument classiﬁers with global features. 2 Dependency Parsing As in previous work, we use a linear model for dependency parsing. The score function used in our dependency parser is deﬁned as follows. s(y) = ∑ F (h, m, x) (1) (h,m)∈y where h and m denote the head and the dependent of the dependency ed"
W09-1218,P06-1141,0,0.0183095,"orithm (Eisner, 1996), for syntactic dependency parsing. In terms of semantic role labeling, we would like to capture global information about predicateargument structures in order to accurately predict the correct predicate-argument structure. Previous research dealt with such information using re-ranking (Toutanova et al., 2005; Johansson and Nugues, 2008). We explore a different approach to deal with such information using global features. Use of global features for structured prediction problem has been explored by several NLP applications such as sequential labeling (Finkel et al., 2005; Krishnan and Manning, 2006; Kazama and Torisawa, 2007) and dependency parsing (Nakagawa, 2007) with a great deal of success. We attempt to use global features for argument classiﬁcation in which the most plausible semantic role assignment is selected using both local and global information. We present an approximate max-margin learning algorithm for argument classiﬁers with global features. 2 Dependency Parsing As in previous work, we use a linear model for dependency parsing. The score function used in our dependency parser is deﬁned as follows. s(y) = ∑ F (h, m, x) (1) (h,m)∈y where h and m denote the head and the de"
W09-1218,W08-2124,0,0.0331244,"Missing"
W09-1218,D07-1100,0,0.0178301,"ole labeling, we would like to capture global information about predicateargument structures in order to accurately predict the correct predicate-argument structure. Previous research dealt with such information using re-ranking (Toutanova et al., 2005; Johansson and Nugues, 2008). We explore a different approach to deal with such information using global features. Use of global features for structured prediction problem has been explored by several NLP applications such as sequential labeling (Finkel et al., 2005; Krishnan and Manning, 2006; Kazama and Torisawa, 2007) and dependency parsing (Nakagawa, 2007) with a great deal of success. We attempt to use global features for argument classiﬁcation in which the most plausible semantic role assignment is selected using both local and global information. We present an approximate max-margin learning algorithm for argument classiﬁers with global features. 2 Dependency Parsing As in previous work, we use a linear model for dependency parsing. The score function used in our dependency parser is deﬁned as follows. s(y) = ∑ F (h, m, x) (1) (h,m)∈y where h and m denote the head and the dependent of the dependency edge in y, and F (h, m, x) is a Factor tha"
W09-1218,P05-1013,0,0.0400704,"Missing"
W09-1218,W08-2121,0,0.05671,"Missing"
W09-1218,taule-etal-2008-ancora,0,0.079745,"Missing"
W09-1218,P05-1073,0,0.0845518,"Missing"
W09-1218,W08-2132,1,0.745212,"relaxation as in (Kazama and Torisawa, 2007). At ﬁrst we generate n-best assignments using only the local factor, and then add the global factor score for each n-best assignment, ﬁnally select the best scoring assignment from them. In order to generate n-best assignments, we used a beam-search algorithm. 3.2.1 Learning the Model As in dependency parser and predicate classiﬁer, we train the model using the PA algorithm with parameter averaging. The learning algorithm is shown 116 3.2.2 Features for Argument Classiﬁcation The local features used in our system are the same as our previous work (Watanabe et al., 2008) except for language dependent features. The global features that used in our system are based on (Johansson and Nugues, 2008) that used for re-ranking. Local Features Word features: Predicted lemma and predicted POS of the predicate, predicate’s head, argument candidate, argument candidate’s head, leftmost/rightmost dependent and leftmost/rightmost sibling. Dependency label: The dependency label of predicate, argument candidate and argument candidate’s dependent. Family: The position of the argument candicate with respect to the predicate position in the dependency tree (e.g. child, sibling)."
W09-1218,W09-1201,0,\N,Missing
W09-3027,I05-5002,0,0.0382202,"ated as a gold standard and the other’s as the output of the system, as shown in Talbe 2. Corpus Construction Procedure We automatically gather sentences on related topics by following the procedure below: 1. Retrieve documents related to a set number of topics using a search engine 2. Extract real sentences that include major subtopic words which are detected based on TF or DF in the document set 3. Reduce noise in data by using heuristics to eliminate advertisements and comment spam 4. Reduce the search space for identifying sentence pairs and prepare pairs, which look feasible to annotate. Dolan and Brockett (2005) proposed a method to narrow the range of sentence pair candidates and collect candidates of sentence-level paraphrases which correspond [EQUIVALENCE] in [AGREEMENT] class in our task. It worked well 152 scheme complete with the necessary semantic relations to support the development of statement maps that show [AGREEMENT], [CONFLICT], and [EVIDENCE] between statements for assisting users in analyzing credibility of information in Web. We discussed the revelations made from annotating our corpus, and discussed future directions for refining our specifications of the corpus. We are planning to"
W09-3027,W00-1009,0,0.0898713,"diagnosed with autism. !Vaccinations are given around the same time children can be first diagnosed. [CONFLICT]! !The plural of anecdote is not data. Figure 1: An example S TATEMENT M AP for the query “Do vaccines cause autism?” b. Vaccines can trigger autism in a vulnerable subset of children. While it is difficult to assign any relation to this pair in an RTE framework, in order to construct statement maps we need to recognize a contradiction between (1a) and (1b). There is another task of recognizing relations between sentences, CST (Cross-Document Structure Theory) which was developed by Radev (2000). CST is an expanded rhetorical structure analysis based on RST (Mann and Thompson, 1988), and attempts to describe relations between two or more sentences from both single and multiple document sets. The CSTBank corpus (Radev et al., 2003) was constructed to annotate crossdocument relations. CSTBank is divided into clusters in which topically-related articles are gathered. There are 18 kinds of relations in this corpus, including [EQUIVALENCE], [ELABORATION], and [REFINEMENT]. 2.2 Facts and Opinions RTE is used to recognize logical and factual relations between sentences in a pair, and CST is"
W09-3027,sumida-etal-2008-boosting,0,0.0540518,"ed autism in children. A: I don’t think vaccines cause autism. B: I believe vaccines are the cause of my son’s autism. (E) [EQUIVALENCE]! There is no link between the MMR vaccine and autism.! for collecting valid sentence pairs from a large cluster which was constituted by topic-related sentences. The method also seem to work well for [CONFLICT] relations, because lexical similarity based on bag-of-words (BOW) can narrow the range of candidates with this relation as well. We calculate the lexical similarity between the two sentences based on BOW. We also used hyponym and synonym dictionaries (Sumida et al., 2008) and a database of relations between predicate argument structures (Matsuyoshi et al., 2008) as resources. According to our preliminary experiments, unigrams of KANJI and KATAKANA expressions, single and compound nouns, verbs and adjectives worked well as features, and we calculate the similarity using cosine distance. We did not use HIRAGANA expressions because they are also used in function words. The weight of the evidence indicates that vaccines are not associated with autism.! (D) Statement (2)! Vaccines are not associated with autism.! Figure 2: Extracting statements from sentences and a"
W09-3027,radev-etal-2004-cst,0,\N,Missing
W10-3904,bond-etal-2008-boot,0,0.0143804,"in Figure 3, our system correctly identiﬁed negation and analyzed the description “Xylitol alone can not completely” as playing a role of requirement. MENT ] (6) a. キシリトールは虫歯予防に効果がある (Xylitol is effective at preventing cavities.) b. キシリトールだけでは完全な予防は出 来ません (Xylitol alone can not completely prevent cavities.) Our system correctly identiﬁes [AGREE relations in other examples about reduced water from Table 1 by structurally aligning phrases like “promoting good health” and “supports the health” to “good for the health.” These examples show how resources like (Matsuyoshi et al., 2010) and WordNet (Bond et al., 2008) have contributed to the relation classiﬁcation improvement of structural alignment over them baseline in Table 3. Focusing on similarity of syntactic and semantic structures gives our alignment method greater ﬂexibility. However, there are still various examples which the system cannot recognized correctly. In examples on cavity prevention, the phrase “effective at preventing cavities” could not be aligned with “can prevent cavities” or “good for cavity prevention,” nor can “cavity prevention” and “cavity-causing bacteria control.” The above examples illustrate the importance of the role play"
W10-3904,D09-1122,0,0.0154188,"rocess is shown in Figure 1. It consists of the following two phases: 1. lexical alignment 2. structural alignment We developed a heuristic-based algorithm to align chunk based on lexical similarity information. We incorporate the following information into an alignment conﬁdence score that has a range of 0.0-1.0 and align chunk whose scores cross an empirically-determined threshold. • surface level similarity: identical content words or cosine similarity of chunk contents • semantic similarity of predicate-argument structures predicates we check for matches in predicate entailment databases (Hashimoto et al., 2009; Matsuyoshi et al., 2008) considering the default case frames reported by ChaPAS arguments we check for synonym or hypernym matches in the Japanese WordNet (2008) or the Japanese hypernym collection of Sumida et al. (2008) 25 T :! &gt;?@???????????????????AB?C????DEF)! (field) (in)!(agricultural chemicals) (ACC)! (use)! I! H :! &gt;?'???????????????????AB?C????GHF)! (field) (on)!(agricultural chemicals) (ACC)! (spray)! Figure 2: Determining the compatibility of semantic structures We compare the predicate-argument structure of the query to that of the text and determine if the argument structures a"
W10-3904,W02-2016,1,0.759598,"Missing"
W10-3904,N06-1006,0,0.0256939,"Missing"
W10-3904,W05-1201,0,0.0270925,"ral years, several corpora annotated with thousands of (T,H) pairs have been constructed for this task. In these corpora, each pair was tagged indicating its related task (e.g. Information Extraction, Question Answering, Information Retrieval or Summarization). The RTE Challenge has successfully employed a variety of techniques in order to recognize instances of textual entailment, including methods based on: measuring the degree of lexical overlap between bag of words (Glickman et al., 2005; Jijkoun and de Rijke, 2005), the alignment of graphs created from syntactic or semantic dependencies (Marsi and Krahmer, 2005; MacCartney et al., 2006), statistical classiﬁers which leverage a wide range of features (Hickl et al., 2005), or reference rule generation (Szpektor et al., 2007). These approaches have shown great promise in RTE for entailment pairs in the corpus, but more robust models of recognizing logical relations are still desirable. The deﬁnition of contradiction in RTE is that T contradicts H if it is very unlikely that both T and H can be true at the same time. However, in real documents on the Web, there are many pairs of examples which are contradictory in part, or where one statement conﬁnes th"
W10-3904,matsuyoshi-etal-2010-annotating,1,0.82442,"example (6), recognized as [C ONFINE in Figure 3, our system correctly identiﬁed negation and analyzed the description “Xylitol alone can not completely” as playing a role of requirement. MENT ] (6) a. キシリトールは虫歯予防に効果がある (Xylitol is effective at preventing cavities.) b. キシリトールだけでは完全な予防は出 来ません (Xylitol alone can not completely prevent cavities.) Our system correctly identiﬁes [AGREE relations in other examples about reduced water from Table 1 by structurally aligning phrases like “promoting good health” and “supports the health” to “good for the health.” These examples show how resources like (Matsuyoshi et al., 2010) and WordNet (Bond et al., 2008) have contributed to the relation classiﬁcation improvement of structural alignment over them baseline in Table 3. Focusing on similarity of syntactic and semantic structures gives our alignment method greater ﬂexibility. However, there are still various examples which the system cannot recognized correctly. In examples on cavity prevention, the phrase “effective at preventing cavities” could not be aligned with “can prevent cavities” or “good for cavity prevention,” nor can “cavity prevention” and “cavity-causing bacteria control.” The above examples illustrate"
W10-3904,I08-1019,0,0.0193577,"re a lot of opinions on the Web, and it is important to survey opinions in addition to facts to give Internet users a comprehensive view of the discussions on topics of interest. 2.3 Cross-document Summarization Based on CST Relations between Sentences Zhang and Radev (2004) attempted to classify CST relations between sentence pairs extracted from topically related documents. However, they used a vector space model and tried multi-class classiﬁcation. The results were not satisfactory. This observation may indicate that the recognition methods for each relation should be developed separately. Miyabe et al. (2008) attempted to recognize relations that were deﬁned in a Japanese cross-document relation corpus (Etoh and Okumura, 2005). However, their target relations were limited to [E QUIVALENCE ] and [T RANSITION ]; other relations were not targeted. Recognizing [E VIDENCE ] is indispensable for organizing information on the Internet. We need to develop satisfactory methods of [E VIDENCE ] recognition. 2.4 Opinion Mining and Sentiment Analysis Subjective statements, such as opinions, have recently been the focus of much NLP research including review analysis, opinion extraction, opinion question answeri"
W10-3904,W09-3027,1,0.844622,"ons, however the assumptions made by current approaches are often incompatible with this goal. For example, the existing semantic relations considered in Recognizing Textual Entailment (RTE) (Dagan et al., 2005) are often too narrow in scope to be directly applicable to text on the Internet, and theories like Cross-document Structure Theory (CST) (Radev, 2000) are only applicable to facts or second-hand reporting of opinions rather than relations between both. As part of the S TATEMENT M AP project we proposed the development of a system to support information credibility analysis on the Web (Murakami et al., 2009b) by automatically summarizing facts and opinions on topics of interest to users and showing them the evidence and conﬂicts for each viewpoint. To facilitate the detection of semantic relations in Internet data, we deﬁned a sentence-like unit of information called the statement that encompasses both facts and opinions, started compiling a corpus of statements annotated with semantic relations (Murakami et al., 2009a), and begin constructing a system to automatically identify semantic relations between statements. In this paper, we describe the construction and evaluation of a prototype semant"
W10-3904,W00-1009,0,0.283136,"ation, opinion mining, and question answering are illustrative of the great interest in making relevant information easier to ﬁnd. Providing Internet users with thorough information requires recognizing semantic relations between both facts and opinions, however the assumptions made by current approaches are often incompatible with this goal. For example, the existing semantic relations considered in Recognizing Textual Entailment (RTE) (Dagan et al., 2005) are often too narrow in scope to be directly applicable to text on the Internet, and theories like Cross-document Structure Theory (CST) (Radev, 2000) are only applicable to facts or second-hand reporting of opinions rather than relations between both. As part of the S TATEMENT M AP project we proposed the development of a system to support information credibility analysis on the Web (Murakami et al., 2009b) by automatically summarizing facts and opinions on topics of interest to users and showing them the evidence and conﬂicts for each viewpoint. To facilitate the detection of semantic relations in Internet data, we deﬁned a sentence-like unit of information called the statement that encompasses both facts and opinions, started compiling a"
W10-3904,sumida-etal-2008-boosting,0,0.0171268,"e the following information into an alignment conﬁdence score that has a range of 0.0-1.0 and align chunk whose scores cross an empirically-determined threshold. • surface level similarity: identical content words or cosine similarity of chunk contents • semantic similarity of predicate-argument structures predicates we check for matches in predicate entailment databases (Hashimoto et al., 2009; Matsuyoshi et al., 2008) considering the default case frames reported by ChaPAS arguments we check for synonym or hypernym matches in the Japanese WordNet (2008) or the Japanese hypernym collection of Sumida et al. (2008) 25 T :! &gt;?@???????????????????AB?C????DEF)! (field) (in)!(agricultural chemicals) (ACC)! (use)! I! H :! &gt;?'???????????????????AB?C????GHF)! (field) (on)!(agricultural chemicals) (ACC)! (spray)! Figure 2: Determining the compatibility of semantic structures We compare the predicate-argument structure of the query to that of the text and determine if the argument structures are compatible. This process is illustrated in Figure 2 where the T(ext) “Agricultural chemicals are used in the ﬁeld.” is aligned with the H(ypothesis) “Over the ﬁeld, agricultural chemicals are sprayed.” Although the verbs"
W10-3904,P07-1058,0,0.0314166,"task (e.g. Information Extraction, Question Answering, Information Retrieval or Summarization). The RTE Challenge has successfully employed a variety of techniques in order to recognize instances of textual entailment, including methods based on: measuring the degree of lexical overlap between bag of words (Glickman et al., 2005; Jijkoun and de Rijke, 2005), the alignment of graphs created from syntactic or semantic dependencies (Marsi and Krahmer, 2005; MacCartney et al., 2006), statistical classiﬁers which leverage a wide range of features (Hickl et al., 2005), or reference rule generation (Szpektor et al., 2007). These approaches have shown great promise in RTE for entailment pairs in the corpus, but more robust models of recognizing logical relations are still desirable. The deﬁnition of contradiction in RTE is that T contradicts H if it is very unlikely that both T and H can be true at the same time. However, in real documents on the Web, there are many pairs of examples which are contradictory in part, or where one statement conﬁnes the applicability of another, as shown in the examples in Table 1. 2.2 Cross-document Structure Theory Cross-document Structure Theory (CST), developed by Radev (2000)"
W10-3904,P10-2018,1,0.826265,"Missing"
W11-0123,W07-1401,0,0.0528608,"ching with a particular query on the Internet, we want information that tells us what other people think about the query: e.g. do they believe it is true or not; what are the necessary conditions for it to apply. For example, consider the hypothetical search results for the query given in (1). You get opinion (2a), which supports the query, and opinion (2b) which opposes it. (1) Xylitol is effective at preventing tooth decay. (2) a. Xylitol can prevent tooth decay. b. Xylitol is not effective at all at preventing tooth decay. A major task in the Recognizing Textual Entailment (RTE) Challenge (Giampiccolo et al. (2007)) is classifying the semantic relation between a Text and a Hypothesis into E NTAILMENT, C ONTRADICTION, or U NKNOWN. Murakami et al. (2009) report on the S TATEMENT M AP project, the goal of which is to help Internet users evaluate the credibility of information sources by analyzing supporting evidence from a variety of viewpoints on their topics of interest and presenting them to users together with the supporting evidence in a way that makes it clear how they are related. A variety of techniques have been successfully employed in the RTE Challenge in order to recognize instances of textual"
W11-0123,W02-2016,1,0.706871,"rsative conjunction identification, and (VI) semantic template application. Figure 1 shows the work flow of the system. This system takes as input corresponding to S0 and S1 , and return a semantic relation. 5.1 I. Linguistic Analysis In linguistic analysis, we conduct word segmentation, POS tagging, dependency parsing, and extended modality analysis. This linguistic analysis acts as the basis for alignment and semantic feature extraction. For syntactic analysis, we identify words and POS tags with the Japanese morphological analyser Mecab2 , and we use the Japanese dependency parser CaboCha (Kudo and Matsumoto (2002)) to produce dependency trees. We also conduct extended modality analysis using the resources provided by Matsuyoshi et al. (2010). 5.2 II. Structural Alignment To identify the consequence of S0 in S1 , we use Structural Alignment (Mizuno et al. (2010)). In Structural Alignment, dependency parent-child links are aligned across sentences using a variety of resources to ensure semantic relatedness. 5.3 III. Premise and Consequence identification In this step, we identify the Premise and the Consequence in S1 . When a sentence pair satisfies all items is satisfying, we can identify a focused chun"
W11-0123,matsuyoshi-etal-2010-annotating,1,0.797535,"takes as input corresponding to S0 and S1 , and return a semantic relation. 5.1 I. Linguistic Analysis In linguistic analysis, we conduct word segmentation, POS tagging, dependency parsing, and extended modality analysis. This linguistic analysis acts as the basis for alignment and semantic feature extraction. For syntactic analysis, we identify words and POS tags with the Japanese morphological analyser Mecab2 , and we use the Japanese dependency parser CaboCha (Kudo and Matsumoto (2002)) to produce dependency trees. We also conduct extended modality analysis using the resources provided by Matsuyoshi et al. (2010). 5.2 II. Structural Alignment To identify the consequence of S0 in S1 , we use Structural Alignment (Mizuno et al. (2010)). In Structural Alignment, dependency parent-child links are aligned across sentences using a variety of resources to ensure semantic relatedness. 5.3 III. Premise and Consequence identification In this step, we identify the Premise and the Consequence in S1 . When a sentence pair satisfies all items is satisfying, we can identify a focused chunk as the Consequence in S1 : 1. A chunk’s modality in S0 is assertion, this chunk is the Consequence in S0 2. A chunk in S1 align"
W11-0123,W00-1009,0,0.0560636,"stantiating the semantic templates using rules and a list of lexico-semantic patterns. Finally, we conduct empirical evaluation of recognition of the C ONFINEMENT relation between queries and sentences in Japanese-language Web texts. 2 Related Work In RTE research, only three types of relations are defined: E NTAILMENT, C ONTRADICTION, and U NKNOWN. RTE is an important task and has been the target of much research (Szpektor et al. (2007); Sammons et al. (2009)). However, none of the previous research has introduced relations corresponding to C ONFINEMENT. Cross-document Structure Theory (CST, Radev (2000)) is another approach to recognizing semantic relations between sentences. CST is an extended rhetorical structure analysis based on Rhetorical Structure Theory (RST). It attempts to describe the semantic relations between two or more sentences from different source documents that are related to the same topic. It defines 18 kinds of semantic relations between sentences. Etoh and Okumura (2005) constructed a Japanese Cross-document Relation Corpus and defined 14 kinds of semantic relations. It is difficult to consider C ONFINEMENT relations in the CST categorical semantic relations because it"
W11-0123,sumida-etal-2008-boosting,0,0.0290746,"that “Xylitol” and “eating Xylitol” and “using Xylitol” are equivalent. 5.4.3 Consequence Feature Extraction This feature is used to indicate the semantic relationship between Consequences of the sentences pair. Sentences with Consequences that share a certain amount of similarity in polarity and syntax are judged to have E NTAILMENT, otherwise they are in C ONTRADICTION. In order to be judged as E NTAILMENT, the following conditions must all be true: 1. The modality of the Consequences must be identical. 2. The polarity of the Consequences must be identical as indicated by the resources in (Sumida et al. (2008)) 3. The Premises of both sentences must align with each other 221 4. ⋆ The sentences must not contain expressions that limit range or degree such as “ほとんど (almost)” or “程度 (degree)” When all item are satisfied, the Consequence is set to “C”, otherwise it is set to “notC.” We identify whether the consequence has expressions which limit the degree or not. The Consequence is set to “Cr ” or “notCr ” when the following all conditions is satisfied: 1. Any of the children of the Consequence align with a chunk in S0 ’s dependency tree. 2. ⋆ There are expressions limiting the degree of the Consequenc"
W11-0123,P07-1058,0,0.0982338,"h a series of semantic templates that take logical and semantic features as input. We implement a system that detects C ONFINEMENT relations between sentence pairs in Japanese by instantiating the semantic templates using rules and a list of lexico-semantic patterns. Finally, we conduct empirical evaluation of recognition of the C ONFINEMENT relation between queries and sentences in Japanese-language Web texts. 2 Related Work In RTE research, only three types of relations are defined: E NTAILMENT, C ONTRADICTION, and U NKNOWN. RTE is an important task and has been the target of much research (Szpektor et al. (2007); Sammons et al. (2009)). However, none of the previous research has introduced relations corresponding to C ONFINEMENT. Cross-document Structure Theory (CST, Radev (2000)) is another approach to recognizing semantic relations between sentences. CST is an extended rhetorical structure analysis based on Rhetorical Structure Theory (RST). It attempts to describe the semantic relations between two or more sentences from different source documents that are related to the same topic. It defines 18 kinds of semantic relations between sentences. Etoh and Okumura (2005) constructed a Japanese Cross-do"
W11-0318,N07-1026,0,0.019423,"nstruction, despite their lower computational complexity. 1 Introduction Semi-supervised classification try to take advantage of a large amount of unlabeled data in addition to a small amount of labeled data, in order to achieve good classification accuracy while reducing the cost of manually annotating data. In particular, graph-based techniques for semi-supervised classification (Zhou et al., 2004; Zhu et al., 2003; Callut et al., 2008; Wang et al., 2008) are recognized as a promising approach. Some of these techniques have been successfully applied for NLP tasks: word sense disambiguation (Alexandrescu and Kirchhoff, 2007; Niu et al., 2005), sentiment analysis (Goldberg and Zhu, 2006), and statistical machine translation (Alexandrescu and Kirchhoff, 2009), to name but a few. However, the focus of these studies is how to assign accurate labels to vertices in a given graph. By contrast, there has not been much work on how such a graph should be built, and graph construction remains “more of an art than a science” (Zhu, 2005). Yet, it is an essential step for graph-based semisupervised classification and (unsupervised) clustering, and the input graph affects the quality of final classification/clustering results."
W11-0318,P94-1020,0,0.04979,"ion is violated; hence, smaller the φ-edge ratio, the better. The plot for b-matching graph is missing for the 20 newsgroups dataset, because its construction did not finish after one week for this dataset. dataset examples features labels interest line Reuters 20 newsgroups 2,368 4,146 4,028 19,928 3,689 8,009 17,143 62,061 6 6 4 20 Table 2: Datasets used in experiments. portion of the Wall Street Journal Corpus. Each instance of the polysemous word “interest” has been tagged with one of the six senses in Longman Dictionary of Contemporary English. The details of the dataset are described in Bruce and Wiebe (1994). The “line” data is originally used in numerous comparative studies of word sense disambiguation. Each instance of the word “line” has been tagged with one of the six senses on the WordNet thesaurus. Further details can be found in the Leacock et al. (1993). Following Niu et al. (2005), we used the following context features in the word sense disambiguation tasks: part-of-speech of neighboring words, single words in the surrounding context, and local collocation. Details of these context features can be found in Lee and Ng (2002). The Reuters dataset is extracted from RCV1v2/LYRL2004, a text"
W11-0318,D09-1052,0,0.0165412,"d sense disambiguation. Each instance of the word “line” has been tagged with one of the six senses on the WordNet thesaurus. Further details can be found in the Leacock et al. (1993). Following Niu et al. (2005), we used the following context features in the word sense disambiguation tasks: part-of-speech of neighboring words, single words in the surrounding context, and local collocation. Details of these context features can be found in Lee and Ng (2002). The Reuters dataset is extracted from RCV1v2/LYRL2004, a text categorization test collection (Lewis et al., 2004). In the same manner as Crammer et al. (2009), we produced the classification dataset by selecting approximately 4,000 documents from 4 general topics (corporate, economic, government and markets) at random. The features described in Lewis et al. (2004) are used with this dataset. 159 The 20 newsgroups dataset is a popular dataset frequently used for document classification and clustering. The dataset consists of approximately 20,000 messages on newsgroups and is originally distributed by Lang (1995). Each message is assigned one of the 20 possible labels indicating which newsgroup it has been posted to, and represented as binary bag-of-"
W11-0318,W06-3808,0,0.0228219,"Semi-supervised classification try to take advantage of a large amount of unlabeled data in addition to a small amount of labeled data, in order to achieve good classification accuracy while reducing the cost of manually annotating data. In particular, graph-based techniques for semi-supervised classification (Zhou et al., 2004; Zhu et al., 2003; Callut et al., 2008; Wang et al., 2008) are recognized as a promising approach. Some of these techniques have been successfully applied for NLP tasks: word sense disambiguation (Alexandrescu and Kirchhoff, 2007; Niu et al., 2005), sentiment analysis (Goldberg and Zhu, 2006), and statistical machine translation (Alexandrescu and Kirchhoff, 2009), to name but a few. However, the focus of these studies is how to assign accurate labels to vertices in a given graph. By contrast, there has not been much work on how such a graph should be built, and graph construction remains “more of an art than a science” (Zhu, 2005). Yet, it is an essential step for graph-based semisupervised classification and (unsupervised) clustering, and the input graph affects the quality of final classification/clustering results. Both for semi-supervised classification and for clustering, the"
W11-0318,H93-1051,0,0.0486517,"rix, i.e., Pij = Pji for all i and j, while Pˆ may 1 See also the experimental results of Section 6.3.2 in which the full similarity matrix W 0 is used as the baseline. 156 3.2 Effect of Hubs on Classification In this section, we demonstrate that hubs in k-NN graphs are indeed harmful to semi-supervised classification as we claimed earlier. To this end, we eliminate such high degree vertices from the graph, and compare the classification accuracy of other vertices before and after the elimination. For this preliminary experiment, we used the “line” dataset of a word sense disambiguation task (Leacock et al., 1993). For details of the dataset and the task, see Section 6. In this experiment, we randomly selected 10 percent of examples as labeled examples. The remaining 90 percent makes the set of unlabeled examples, and the goal is to predict the label (word sense) of these unlabeled examples. We first built a k-NN graph (with k = 3) from the dataset, and ran Gaussian Random Fields (GRF) (Zhu et al., 2003), one of the most widelyused graph-based semi-supervised classification algorithms. Then we removed vertices with degree greater than or equal to 30 from the k-NN graph, and ran GRF again on this “hub-r"
W11-0318,W02-1006,0,0.00958422,"orary English. The details of the dataset are described in Bruce and Wiebe (1994). The “line” data is originally used in numerous comparative studies of word sense disambiguation. Each instance of the word “line” has been tagged with one of the six senses on the WordNet thesaurus. Further details can be found in the Leacock et al. (1993). Following Niu et al. (2005), we used the following context features in the word sense disambiguation tasks: part-of-speech of neighboring words, single words in the surrounding context, and local collocation. Details of these context features can be found in Lee and Ng (2002). The Reuters dataset is extracted from RCV1v2/LYRL2004, a text categorization test collection (Lewis et al., 2004). In the same manner as Crammer et al. (2009), we produced the classification dataset by selecting approximately 4,000 documents from 4 general topics (corporate, economic, government and markets) at random. The features described in Lewis et al. (2004) are used with this dataset. 159 The 20 newsgroups dataset is a popular dataset frequently used for document classification and clustering. The dataset consists of approximately 20,000 messages on newsgroups and is originally distri"
W11-0318,P05-1049,0,0.0709981,"mputational complexity. 1 Introduction Semi-supervised classification try to take advantage of a large amount of unlabeled data in addition to a small amount of labeled data, in order to achieve good classification accuracy while reducing the cost of manually annotating data. In particular, graph-based techniques for semi-supervised classification (Zhou et al., 2004; Zhu et al., 2003; Callut et al., 2008; Wang et al., 2008) are recognized as a promising approach. Some of these techniques have been successfully applied for NLP tasks: word sense disambiguation (Alexandrescu and Kirchhoff, 2007; Niu et al., 2005), sentiment analysis (Goldberg and Zhu, 2006), and statistical machine translation (Alexandrescu and Kirchhoff, 2009), to name but a few. However, the focus of these studies is how to assign accurate labels to vertices in a given graph. By contrast, there has not been much work on how such a graph should be built, and graph construction remains “more of an art than a science” (Zhu, 2005). Yet, it is an essential step for graph-based semisupervised classification and (unsupervised) clustering, and the input graph affects the quality of final classification/clustering results. Both for semi-supe"
W11-0318,N09-1014,0,\N,Missing
W11-1913,P09-1068,0,0.335718,"stract system based on the cluster-ranking model proposed by Rahman and Ng (2009). We then experimented with adding novel semantic features derived from co-referring predicate-argument chains. These narrative schema were developed by Chambers and Jurafsky (2009). They are described in more detail in a later section. In this paper we describe the system with which we participated in the CoNLL-2011 Shared Task on modelling coreference. Our system is based on a cluster-ranking model proposed by Rahman and Ng (2009), with novel semantic features based on recent research on narrative event schema (Chambers and Jurafsky, 2009). We demonstrate some improvements over the baseline when using schema information, although the effect varied between the metrics used. We also explore the impact of various features on our system’s performance. 1 Yuji Matsumoto Nara Institute of Science and Technology Nara Prefecture, Japan matsu@is.naist.jp 2 Introduction Coreference resolution is a problem for automated document understanding. We say two segments of a natural-language document corefer when they refer to the same real-world entity. The segments of a document which refer to an entity are called mentions. In coreference resol"
W11-1913,chambers-jurafsky-2010-database,0,0.0443508,"tted two results to the CoNLL-2011 Shared Task. In the “closed” track we submitted the results of our baseline system without the schema features, trained on all documents in both the training and development portions of the OntoNotes corpus. We also submitted a result in the “open” track: a version of our system with the schema features added. Due to issues with the implementation of this second version, however, we were only able to submit results from a model trained on just the WSJ portion of the training dataset. For the schema features, we used a database of narrative schema released by Chambers and Jurafsky (2010) – specifically the list of schemas of size 12. 4 The official system scores for our system are listed in Table 3. We can attribute some of the low performance of our system to features which are too noisy, and to having not enough features compared to the large size of the dataset. It is likely that these two factors adversely impact the ability of the SVM to learn effectively. In fact, the features which we introduced partially to provide more features to learn with, the NE features, had the worst impact on performance according to later analysis. Because of a problem with our implementation"
W11-1913,H05-1013,0,0.0607459,"Missing"
W11-1913,N10-1061,0,0.0231043,"rube, 2006b). Surprisingly, Rahman and Ng (2011) demonstrated that a system using almost exclusively lexical features could outperform 86 Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 86–92, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics systems which used more traditional sets of features. Although string features have a large effect on performance, it is recognized that the use of semantic information is important for further improvement (Ng, 2010; Ponzetto and Strube, 2006a; Ponzetto and Strube, 2006b; Haghighi and Klein, 2010). The use of predicate-argument structure has been explored by Ponzetto and Strube (2006b; 2006a). 3 Narrative Schema for Coreference Narrative schema are extracted from large-scale corpora using coreference information to identify predicates whose arguments often corefer. Similarity measures are used to build up schema consisting of one or more event chains – chains of typicallycoreferring predicate arguments (Chambers and Jurafsky, 2009). Each chain corresponds to a role in the schema. A role defines a class of participants in the schema. Conceptually, if a schema is present in a document, t"
W11-1913,P10-1142,0,0.126334,"th several layers of syntactic and semantic information, making it a rich resource for investigating coreference resolution (Pradhan et al., 2007). We participated in both the “open” and “closed” tracks. The “closed” track requires systems to only use the provided data, while the “open” track allows use of external data. We created a baseline Related Work Supervised machine-learning approaches to coreference resolution have been researched for almost two decades. Recently, the state of the art seems to be moving away from the early mention-pair classification model toward entity-based models. Ng (2010) provides an excellent overview of the history and recent developments within the field. Both entity-mention and mention-pair models are formulated as binary classification problems; however, ranking may be a more natural approach to coreference resolution (Ng, 2010; Rahman and Ng, 2009). Rahman and Ng (2009) in particular propose the cluster-ranking model which we used in our baseline. In another approach, Daum´e and Marcu (2005) apply their Learning as Search Optimization framework to coreference resolution, and show good results. Feature selection is important for good performance in corefe"
W11-1913,N06-1025,0,0.398465,"lems; however, ranking may be a more natural approach to coreference resolution (Ng, 2010; Rahman and Ng, 2009). Rahman and Ng (2009) in particular propose the cluster-ranking model which we used in our baseline. In another approach, Daum´e and Marcu (2005) apply their Learning as Search Optimization framework to coreference resolution, and show good results. Feature selection is important for good performance in coreference resolution. Ng (2010) discusses commonly used features, and analyses of the contribution of various features can be found in (Daum´e and Marcu, 2005; Rahman and Ng, 2011; Ponzetto and Strube, 2006b). Surprisingly, Rahman and Ng (2011) demonstrated that a system using almost exclusively lexical features could outperform 86 Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 86–92, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics systems which used more traditional sets of features. Although string features have a large effect on performance, it is recognized that the use of semantic information is important for further improvement (Ng, 2010; Ponzetto and Strube, 2006a; Ponzetto and Strube, 2006b; Haghighi a"
W11-1913,E06-2015,0,0.133754,"lems; however, ranking may be a more natural approach to coreference resolution (Ng, 2010; Rahman and Ng, 2009). Rahman and Ng (2009) in particular propose the cluster-ranking model which we used in our baseline. In another approach, Daum´e and Marcu (2005) apply their Learning as Search Optimization framework to coreference resolution, and show good results. Feature selection is important for good performance in coreference resolution. Ng (2010) discusses commonly used features, and analyses of the contribution of various features can be found in (Daum´e and Marcu, 2005; Rahman and Ng, 2011; Ponzetto and Strube, 2006b). Surprisingly, Rahman and Ng (2011) demonstrated that a system using almost exclusively lexical features could outperform 86 Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 86–92, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics systems which used more traditional sets of features. Although string features have a large effect on performance, it is recognized that the use of semantic information is important for further improvement (Ng, 2010; Ponzetto and Strube, 2006a; Ponzetto and Strube, 2006b; Haghighi a"
W11-1913,W11-1901,0,0.0436156,"t varied between the metrics used. We also explore the impact of various features on our system’s performance. 1 Yuji Matsumoto Nara Institute of Science and Technology Nara Prefecture, Japan matsu@is.naist.jp 2 Introduction Coreference resolution is a problem for automated document understanding. We say two segments of a natural-language document corefer when they refer to the same real-world entity. The segments of a document which refer to an entity are called mentions. In coreference resolution tasks, mentions are usually restricted to noun phrases. The goal of the CoNLL-2011 Shared Task (Pradhan et al., 2011) is to model unrestricted coreference using the OntoNotes corpus. The OntoNotes corpus is annotated with several layers of syntactic and semantic information, making it a rich resource for investigating coreference resolution (Pradhan et al., 2007). We participated in both the “open” and “closed” tracks. The “closed” track requires systems to only use the provided data, while the “open” track allows use of external data. We created a baseline Related Work Supervised machine-learning approaches to coreference resolution have been researched for almost two decades. Recently, the state of the art"
W11-1913,D09-1101,0,0.644344,"p Mamoru Komachi Nara Institute of Science and Technology Nara Prefecture, Japan komachi@is.naist.jp Abstract system based on the cluster-ranking model proposed by Rahman and Ng (2009). We then experimented with adding novel semantic features derived from co-referring predicate-argument chains. These narrative schema were developed by Chambers and Jurafsky (2009). They are described in more detail in a later section. In this paper we describe the system with which we participated in the CoNLL-2011 Shared Task on modelling coreference. Our system is based on a cluster-ranking model proposed by Rahman and Ng (2009), with novel semantic features based on recent research on narrative event schema (Chambers and Jurafsky, 2009). We demonstrate some improvements over the baseline when using schema information, although the effect varied between the metrics used. We also explore the impact of various features on our system’s performance. 1 Yuji Matsumoto Nara Institute of Science and Technology Nara Prefecture, Japan matsu@is.naist.jp 2 Introduction Coreference resolution is a problem for automated document understanding. We say two segments of a natural-language document corefer when they refer to the same r"
W11-1913,D08-1067,0,\N,Missing
W11-3504,W09-3536,0,0.0627478,"Missing"
W11-3504,P97-1017,0,\N,Missing
W11-3506,P00-1031,0,0.04063,"editor by converting erroneous text written in Roman characters into correct text written in kana while leaving foreign words unchanged. Our method consists of three steps: iden2 Related Work Our interest is mainly focused on how to deal with erroneous inputs. Error detection and correction on sentences written in kana with kana character N-gram was proposed in (Shinnou, 1999). Our approach is similar to this, but our target is sentences in Roman characters and has the additional difficulty of language identification. Errortolerant Chinese input methods were introduced in (Zheng et al., 2011; Chen and Lee, 2000). Though Roman-to-kana conversion is similar to pinyin-toChinese conversion, our target differs from them because our motivation is to help Japanese language teachers. Japanese commercial IMs such as Microsoft Office IME1 , ATOK2 , and Google IME3 have a module of spelling correction, but their target is native Japanese speakers. (Ehara and Tanaka-Ishii, 2008) presented a high accuracy language detection system for text input. We perform 1 http://www.microsoft.com/japan/ office/2010/ime/default.mspx 2 http://www.atok.com/ 3 http://www.google.com/intl/ja/ime/ 38 Proceedings of the Workshop on A"
W11-3506,I08-1058,0,0.0138414,"proposed in (Shinnou, 1999). Our approach is similar to this, but our target is sentences in Roman characters and has the additional difficulty of language identification. Errortolerant Chinese input methods were introduced in (Zheng et al., 2011; Chen and Lee, 2000). Though Roman-to-kana conversion is similar to pinyin-toChinese conversion, our target differs from them because our motivation is to help Japanese language teachers. Japanese commercial IMs such as Microsoft Office IME1 , ATOK2 , and Google IME3 have a module of spelling correction, but their target is native Japanese speakers. (Ehara and Tanaka-Ishii, 2008) presented a high accuracy language detection system for text input. We perform 1 http://www.microsoft.com/japan/ office/2010/ime/default.mspx 2 http://www.atok.com/ 3 http://www.google.com/intl/ja/ime/ 38 Proceedings of the Workshop on Advances in Text Input Methods (WTIM 2011), pages 38–42, Chiang Mai, Thailand, November 13, 2011. 5.1 Language Identification error correction in addition to language identification. Correcting Japanese learners’ error is also proposed in (Mizumoto et al., 2011). They try to correct sentences written in kana and kanji mixed, whereas we aim at texts in Roman cha"
W11-3506,I11-1017,1,0.785266,"gle IME3 have a module of spelling correction, but their target is native Japanese speakers. (Ehara and Tanaka-Ishii, 2008) presented a high accuracy language detection system for text input. We perform 1 http://www.microsoft.com/japan/ office/2010/ime/default.mspx 2 http://www.atok.com/ 3 http://www.google.com/intl/ja/ime/ 38 Proceedings of the Workshop on Advances in Text Input Methods (WTIM 2011), pages 38–42, Chiang Mai, Thailand, November 13, 2011. 5.1 Language Identification error correction in addition to language identification. Correcting Japanese learners’ error is also proposed in (Mizumoto et al., 2011). They try to correct sentences written in kana and kanji mixed, whereas we aim at texts in Roman characters. 3 Language identification is done by exact matching input sequences in English with a romanized5 Japanese dictionary. Learners sometimes directly write words in their native language without adapting to Japanese romaji style. Since we are not focusing on implementing full transliteration (Knight and Graehl, 1998), we would like to convert only Japanese words into kana. To achieve this, we use an English word dictionary because most foreign words found in learners’ sentences are English"
W11-3506,C10-1096,0,0.0204255,"mber of candidates using approximate word matching with cosine distance before calculating edit distance (Kukich, 1992). Cosine distance is calculated using character n-gram features. We set n = 1 because it covers most candidates in dictionary and reduces the number of candidates appropriately. For example, when we retrieved the approximate words for packu in our dictionary with cosine distance, the number of candidates is reduced to 163, and examples of retrieved words are kau, pakku, chikau, pachikuri, etc. Approximate word matching with cosine similarity can be performed very efficiently (Okazaki and Tsujii, 2010)9 to get candidates from a large scale word dictionary. kana よろしく おねがい します。 Muscle musical を みたい。 あなた は えいご が わかります か。 Table 2: Examples of successfully corrected word misspelled shuutmatsu do-yoobi packu kana しゅう t まつ どよおび ぱcく correct shuumatsu doyoubi pakku kana しゅうまつ どようび ぱっく some pairs have several possibilities. One of them is a pair of n and following characters. For example, we can read Japanese word kinyuu as “きん ゆう/kin-yuu: finance” and “きにゅう/kinyuu: entry.” The reason why it occurs is that n can be a syllable alone. Solving this kind of ambiguity is out of scope of this paper; and we"
W11-3506,H01-1044,0,0.0967435,"Missing"
W11-3506,J98-4003,0,\N,Missing
W12-2033,P98-1013,0,0.0553111,"s Run0. nize “keep” takes an object and a complement; in Example (5) “love” is the object and “secret” is the complement of “keep” while the former is leftextraposed. A rule-based approach may be better suited for these cases than a machine learning approach. Third, most deletion errors involve discrimination between transitive and intransitive. For instance, “NONE” in Example (6) must be changed to “for”, because “wait” is intransitive. I’ll wait NONEf or your next letter. (6) To deal with these errors, we may use rich knowledge about verbs such as VerbNet (Kipper et al., 2000) and FrameNet (Baker et al., 1998) in order to judge whether a verb is transitive or intransitive. 5.2 Error Analysis of Determiner Correction We conducted additional experiments for determiner errors and report the results here because the submitted system contained a bug. In the submitted system, while the test data were parsed by the “mixed” model, the training data and the test data were parsed by the default grammar provided with Berkeley Parser. Moreover, though there were about 5.5 million sentences in the BNC corpus, only about 286 2.7 million of them had been extracted. Though these errors seem to have improved the pe"
W12-2033,J96-1002,0,0.104285,"Missing"
W12-2033,W11-2841,0,0.041053,"Missing"
W12-2033,W10-4236,0,0.0462786,"Missing"
W12-2033,P07-1033,0,0.126783,"Missing"
W12-2033,C08-1022,0,0.374974,"Missing"
W12-2033,W07-0712,0,0.0636995,"Missing"
W12-2033,N10-1019,0,0.120128,"Missing"
W12-2033,J06-4003,0,0.0368252,"of i (3 to 5) window size phrase including the preposition prep, the value of log100 (fi + 1) The proportion pprep,i (i is 3 to 5). f pprep,i = ∑ prep,i , given the set of target prepositions T . fk,i k∈T Semantic WORDNET CATEGORY WordNet lexicographer classes which are about 40 broad semantic categories for all words used as surface features. As De Felice and Pulman (2008) did not perform word sense disambiguation, neither did we. Table 1: Baseline features for English preposition error correction. original CLC-FCE plain sentences. We performed sentence splitting using the implementation of Kiss and Strunk (2006) in NLTK 2.0.1rc2. We conducted dependency parsing by Stanford parser 1.6.9.8 We used the features described in (Tetreault et al., 2010) as shown in Table 1 with Maximum Entropy (ME) modeling (Berger et al., 1996) as a multi-class classifier. We used the implementation of Maximum Entropy Modeling Toolkit9 with its default parameters. For web n-gram calculation, we used Google N-gram with a search system for giga-scale n-gram corpus, called SSGNC 0.4.6.10 4 System Architecture for Determiner Error Correction We focused on article error correction in the determiner error correction subtask, beca"
W12-2033,P11-1093,0,0.105332,"Missing"
W12-2033,P10-2065,0,0.183624,"pprep,i = ∑ prep,i , given the set of target prepositions T . fk,i k∈T Semantic WORDNET CATEGORY WordNet lexicographer classes which are about 40 broad semantic categories for all words used as surface features. As De Felice and Pulman (2008) did not perform word sense disambiguation, neither did we. Table 1: Baseline features for English preposition error correction. original CLC-FCE plain sentences. We performed sentence splitting using the implementation of Kiss and Strunk (2006) in NLTK 2.0.1rc2. We conducted dependency parsing by Stanford parser 1.6.9.8 We used the features described in (Tetreault et al., 2010) as shown in Table 1 with Maximum Entropy (ME) modeling (Berger et al., 1996) as a multi-class classifier. We used the implementation of Maximum Entropy Modeling Toolkit9 with its default parameters. For web n-gram calculation, we used Google N-gram with a search system for giga-scale n-gram corpus, called SSGNC 0.4.6.10 4 System Architecture for Determiner Error Correction We focused on article error correction in the determiner error correction subtask, because the errors related to articles significantly outnumber the errors unrelated to them. Though more than twenty types of determiners ar"
W12-2033,P07-1031,0,0.01999,"augmenting the training data for the parser model with sentences lacking articles, the span of NPs that lack an article might have better chance of being correctly recognized. In addition, dependency information was extracted from the parse using the Stanford parser 1.6.9. For each NP in the parse, we extracted a feature vector representation. We used the feature templates shown in Table 2, which are inspired by (De Felice, 2008) and adapted to the CFG representation. For the parser models, we trained the “normal” model on the WSJ part of Penn Treebank sections 02-21 with the NP annotation by Vadas and Curran (2007). The “mixed” model was trained on the concatenation of the WSJ part and its modified version. For the classification model, we used the written part of the British National Corpus (BNC) in addition to the CLC FCE Dataset, because the amount of indomain data was limited. In examples taken from the CLC FCE Dataset, the true labels after the correction were used. In examples taken from the BNC, the article of each NP was used as the label. We trained a linear classifier using opal12 with the PA-I algorithm. We also used the feature augmentation 12 Run 0 1 2 3 4 5 6 7 Table 3: Distinct configurat"
W12-2033,C98-1013,0,\N,Missing
W13-1021,W11-0809,0,0.320053,"tasks. Nivre and Nilsson (2004) investigated the effect of recognizing MWEs in syntactic dependency parsing of Swedish. Korkontzelos and Manandhar (2010) showed performance improvement of base phrase chunking by annotating compound and proper nouns. Finlayson and Kulkarni (2011) reported the effect of recognizing MWEs on word sense disambiguation. Most of the previous approaches to MWE recognition are based on frequency or collocation measures of words in large scale corpora. On the other hand, some previous approaches tried to recognize new MWEs using an MWE lexicon and MWEannotated corpora. Constant and Sigogne (2011) presented MWE recognition using a Conditional Random Fields (CRFs)-based tagger with the BIO schema. Green et al. (2011) proposed an MWE recognition method using Tree Substitution Grammars. Constant et al. (2012) compared two phrase structure analysis methods, one that uses MWE recognition as preprocessing and the other that uses a reranking method. Although MWEs show a variety of flexibilities in their appearance, most of the linguistic analyses consider the fixed type of MWEs. For example, the experiments by Nivre and Nilsson (2004) focus on fixed expressions that fall into the following ca"
W13-1021,P12-1022,0,0.195777,"ting compound and proper nouns. Finlayson and Kulkarni (2011) reported the effect of recognizing MWEs on word sense disambiguation. Most of the previous approaches to MWE recognition are based on frequency or collocation measures of words in large scale corpora. On the other hand, some previous approaches tried to recognize new MWEs using an MWE lexicon and MWEannotated corpora. Constant and Sigogne (2011) presented MWE recognition using a Conditional Random Fields (CRFs)-based tagger with the BIO schema. Green et al. (2011) proposed an MWE recognition method using Tree Substitution Grammars. Constant et al. (2012) compared two phrase structure analysis methods, one that uses MWE recognition as preprocessing and the other that uses a reranking method. Although MWEs show a variety of flexibilities in their appearance, most of the linguistic analyses consider the fixed type of MWEs. For example, the experiments by Nivre and Nilsson (2004) focus on fixed expressions that fall into the following categories: 1. Multiword names 2. Numerical expressions 3. Compound function words (a) Adverbs (b) Prepositions (c) Subordinating conjunctions (d) Determiners (e) Pronouns Multiword names and numerical expressions b"
W13-1021,W11-0805,0,0.0429047,"s is annotated with MWEs, its coverage is far from complete. Considering this situation, we started construction of an English MWE dictionary (with functional expressions first) and classified their occurrences in PTB into MWE or literal usage, obtaining MWEannotated version of PTB. The effect of MWE dictionaries have been reported for various NLP tasks. Nivre and Nilsson (2004) investigated the effect of recognizing MWEs in syntactic dependency parsing of Swedish. Korkontzelos and Manandhar (2010) showed performance improvement of base phrase chunking by annotating compound and proper nouns. Finlayson and Kulkarni (2011) reported the effect of recognizing MWEs on word sense disambiguation. Most of the previous approaches to MWE recognition are based on frequency or collocation measures of words in large scale corpora. On the other hand, some previous approaches tried to recognize new MWEs using an MWE lexicon and MWEannotated corpora. Constant and Sigogne (2011) presented MWE recognition using a Conditional Random Fields (CRFs)-based tagger with the BIO schema. Green et al. (2011) proposed an MWE recognition method using Tree Substitution Grammars. Constant et al. (2012) compared two phrase structure analysis"
W13-1021,D11-1067,0,0.145781,"Missing"
W13-1021,W07-1103,0,0.0715402,"Missing"
W13-1021,N10-1089,0,0.0694217,"144, c Atlanta, Georgia, 13-14 June 2013. 2013 Association for Computational Linguistics Swedish. While the British National Corpus is annotated with MWEs, its coverage is far from complete. Considering this situation, we started construction of an English MWE dictionary (with functional expressions first) and classified their occurrences in PTB into MWE or literal usage, obtaining MWEannotated version of PTB. The effect of MWE dictionaries have been reported for various NLP tasks. Nivre and Nilsson (2004) investigated the effect of recognizing MWEs in syntactic dependency parsing of Swedish. Korkontzelos and Manandhar (2010) showed performance improvement of base phrase chunking by annotating compound and proper nouns. Finlayson and Kulkarni (2011) reported the effect of recognizing MWEs on word sense disambiguation. Most of the previous approaches to MWE recognition are based on frequency or collocation measures of words in large scale corpora. On the other hand, some previous approaches tried to recognize new MWEs using an MWE lexicon and MWEannotated corpora. Constant and Sigogne (2011) presented MWE recognition using a Conditional Random Fields (CRFs)-based tagger with the BIO schema. Green et al. (2011) prop"
W13-1021,W04-3230,1,0.542697,"(d) Determiners (e) Pronouns Multiword names and numerical expressions behave as noun phrases and have limited syntactic functionalities. On the other hand, compound func140 tion words have a variety of functionalities that may affect language analyses such as POS tagging and parsing. In this work, we extract compound functional expressions from the English part of Wiktionary, and classify their occurrences in PTB into either literal or MWE usages. We then build a POS tagger that takes MWEs into account. In implementing this, we use CRFs that can handle a sequence of tokens as a single item (Kudo et al., 2004). We evaluate the performance of the tagger and compare it with the method that uses the BIO schema for identifying MWE usages (Constant and Sigogne, 2011). 3 MWEs Extraction from Wiktionary To construct an English MWE dictionary, we extract entries from the English part of Wiktionary (as of July 14, 2012) that include white spaces. We extract only fixed expressions that are categorized either as adverbs, conjunctions, determiners, prepositions, prepositional phrases or pronouns. We exclude compound nouns and phrasal verbs since the former are easily recognized by an existing method such as ch"
W13-1021,P11-1017,0,0.0299234,": Those having a wide range of syntactic variabil1. Construction of an English MWE dictionary (mainly consisting of functional expressions) through extraction from Wiktionary1 . 2. Annotation of MWEs in the Penn Treebank (PTB). 3. Implementation of an MWE-aware POS tagger and evaluation of its performance. 2 Related work While there is a variety of MWE researches only a few of them focus on MWE lexicon construction. Though some examples, such as French adverb dictionaries (Laporte and Voyatzi, 2008; Laporte et al., 2008), a Dutch MWE dictionary (Gr´egoire, 2007) and a Japanese MWE dictionary (Shudo et al., 2011) have been constructed, there is no freely available English MWE dictionary with a broad coverage. Moreover, MWE-annotated corpora are only available for a few languages, including French and 1 https://en.wiktionary.org 139 Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 139–144, c Atlanta, Georgia, 13-14 June 2013. 2013 Association for Computational Linguistics Swedish. While the British National Corpus is annotated with MWEs, its coverage is far from complete. Considering this situation, we started construction of an English MWE dictionary (with functional expressi"
W13-1717,C12-1025,0,0.167283,"entification (NLI) system in the NLI 2013 Shared Task. We apply feature selection using a measure based on frequency for the closed track and try Capping and Sampling data methods for the open tracks. Our system ranked ninth in the closed track, third in open track 1 and fourth in open track 2. 1 Introduction There have been many studies using English as a second language (ESL) learner corpora. For example, automatic grammatical error detection and correction is one of the most active research areas in this field. More recently, attention has been paid to native language identification (NLI) (Brooke and Hirst, 2012; Bykh and Meurers, 2012; Brooke and Hirst, 2011; Wong and Dras, 2011; Wong et al., 2011). Native language identification is the task of identifying the ESL learner’s L1 given a learner’s essay. The NLI Shared Task 2013 (Tetreault et al., 2013) is the first shared task on NLI using the common dataset “TOEFL-11” (Blanchard et al., 2013; Tetreault et al., 2012). TOEFL-11 consists of essays written by learners of 11 native languages (Arabic, Chinese, French, German, Hindi, Italian, Japanese, Koran, Spanish, Telugu, Turkish), and it contains 1,100 essays for each native language. In addition, the"
W13-1717,C12-1027,0,0.0528286,"m in the NLI 2013 Shared Task. We apply feature selection using a measure based on frequency for the closed track and try Capping and Sampling data methods for the open tracks. Our system ranked ninth in the closed track, third in open track 1 and fourth in open track 2. 1 Introduction There have been many studies using English as a second language (ESL) learner corpora. For example, automatic grammatical error detection and correction is one of the most active research areas in this field. More recently, attention has been paid to native language identification (NLI) (Brooke and Hirst, 2012; Bykh and Meurers, 2012; Brooke and Hirst, 2011; Wong and Dras, 2011; Wong et al., 2011). Native language identification is the task of identifying the ESL learner’s L1 given a learner’s essay. The NLI Shared Task 2013 (Tetreault et al., 2013) is the first shared task on NLI using the common dataset “TOEFL-11” (Blanchard et al., 2013; Tetreault et al., 2012). TOEFL-11 consists of essays written by learners of 11 native languages (Arabic, Chinese, French, German, Hindi, Italian, Japanese, Koran, Spanish, Telugu, Turkish), and it contains 1,100 essays for each native language. In addition, the essay topics are balance"
W13-1717,P09-2012,0,0.0311602,"=3 Tree substitution grammer RB difficult NN (i, nsubj) (think, i) (nsubj, i, think) (PRP UNK-INITCKNOWNLC) (VB think) (NP RB DT ADJP NN) (JJ UNK-LC) Table 1: All features for native language identification. using the Stanford Parser 2.0.2 3 . We use tree substitution grammars as features. TSGs are generalized context-free grammars (CFGs) that allow nonterminals to re-write to tree fragments. The fragments reflect both syntactic and surface structures of a given sentence more efficiently than using several CFG rules. In practice, efficient Bayesian approaches have been proposed in prior work (Post and Gildea, 2009). In terms of the application of TSG to NLI task, (Swanson and Charniak, 2012) have shown a promising result. Post (2011) also uses TSG to judge grammaticality of a sentence written by language learners. With these previous findings in mind, we also extract TSG rules. We use the training settings and public software from Post (2011)4 , obtaining 21,020 unique TSG fragments from the training dataset of the TOEFL-11 corpus. 3 Closed Track In this section, we describe our system for the closed track. We use the tools and features described in Section 2. In our system, feature selection is perform"
W13-1717,P11-2038,0,0.0225251,"P NN) (JJ UNK-LC) Table 1: All features for native language identification. using the Stanford Parser 2.0.2 3 . We use tree substitution grammars as features. TSGs are generalized context-free grammars (CFGs) that allow nonterminals to re-write to tree fragments. The fragments reflect both syntactic and surface structures of a given sentence more efficiently than using several CFG rules. In practice, efficient Bayesian approaches have been proposed in prior work (Post and Gildea, 2009). In terms of the application of TSG to NLI task, (Swanson and Charniak, 2012) have shown a promising result. Post (2011) also uses TSG to judge grammaticality of a sentence written by language learners. With these previous findings in mind, we also extract TSG rules. We use the training settings and public software from Post (2011)4 , obtaining 21,020 unique TSG fragments from the training dataset of the TOEFL-11 corpus. 3 Closed Track In this section, we describe our system for the closed track. We use the tools and features described in Section 2. In our system, feature selection is performed using a measure based on frequency. Although Tsur 3 4 http://nlp.stanford.edu/software/lex-parser.shtml https://github"
W13-1717,P12-2038,0,0.0139454,", i, think) (PRP UNK-INITCKNOWNLC) (VB think) (NP RB DT ADJP NN) (JJ UNK-LC) Table 1: All features for native language identification. using the Stanford Parser 2.0.2 3 . We use tree substitution grammars as features. TSGs are generalized context-free grammars (CFGs) that allow nonterminals to re-write to tree fragments. The fragments reflect both syntactic and surface structures of a given sentence more efficiently than using several CFG rules. In practice, efficient Bayesian approaches have been proposed in prior work (Post and Gildea, 2009). In terms of the application of TSG to NLI task, (Swanson and Charniak, 2012) have shown a promising result. Post (2011) also uses TSG to judge grammaticality of a sentence written by language learners. With these previous findings in mind, we also extract TSG rules. We use the training settings and public software from Post (2011)4 , obtaining 21,020 unique TSG fragments from the training dataset of the TOEFL-11 corpus. 3 Closed Track In this section, we describe our system for the closed track. We use the tools and features described in Section 2. In our system, feature selection is performed using a measure based on frequency. Although Tsur 3 4 http://nlp.stanford.e"
W13-1717,C12-1158,0,0.302929,"econd language (ESL) learner corpora. For example, automatic grammatical error detection and correction is one of the most active research areas in this field. More recently, attention has been paid to native language identification (NLI) (Brooke and Hirst, 2012; Bykh and Meurers, 2012; Brooke and Hirst, 2011; Wong and Dras, 2011; Wong et al., 2011). Native language identification is the task of identifying the ESL learner’s L1 given a learner’s essay. The NLI Shared Task 2013 (Tetreault et al., 2013) is the first shared task on NLI using the common dataset “TOEFL-11” (Blanchard et al., 2013; Tetreault et al., 2012). TOEFL-11 consists of essays written by learners of 11 native languages (Arabic, Chinese, French, German, Hindi, Italian, Japanese, Koran, Spanish, Telugu, Turkish), and it contains 1,100 essays for each native language. In addition, the essay topics are balanced, and the number of topics is 8. In the closed track, we tackle feature selection for increasing accuracy. We use a feature selection method based on the frequency of each feature (e.g., document frequency, TF-IDF). In the open tracks, to address the problem of imbalanced data, we tried two approaches: Capping and Sampling data in ord"
W13-1717,W13-1706,0,0.195726,"k, third in open track 1 and fourth in open track 2. 1 Introduction There have been many studies using English as a second language (ESL) learner corpora. For example, automatic grammatical error detection and correction is one of the most active research areas in this field. More recently, attention has been paid to native language identification (NLI) (Brooke and Hirst, 2012; Bykh and Meurers, 2012; Brooke and Hirst, 2011; Wong and Dras, 2011; Wong et al., 2011). Native language identification is the task of identifying the ESL learner’s L1 given a learner’s essay. The NLI Shared Task 2013 (Tetreault et al., 2013) is the first shared task on NLI using the common dataset “TOEFL-11” (Blanchard et al., 2013; Tetreault et al., 2012). TOEFL-11 consists of essays written by learners of 11 native languages (Arabic, Chinese, French, German, Hindi, Italian, Japanese, Koran, Spanish, Telugu, Turkish), and it contains 1,100 essays for each native language. In addition, the essay topics are balanced, and the number of topics is 8. In the closed track, we tackle feature selection for increasing accuracy. We use a feature selection method based on the frequency of each feature (e.g., document frequency, TF-IDF). In"
W13-1717,W07-0602,0,0.392158,"Missing"
W13-1717,D11-1148,0,0.0243131,"selection using a measure based on frequency for the closed track and try Capping and Sampling data methods for the open tracks. Our system ranked ninth in the closed track, third in open track 1 and fourth in open track 2. 1 Introduction There have been many studies using English as a second language (ESL) learner corpora. For example, automatic grammatical error detection and correction is one of the most active research areas in this field. More recently, attention has been paid to native language identification (NLI) (Brooke and Hirst, 2012; Bykh and Meurers, 2012; Brooke and Hirst, 2011; Wong and Dras, 2011; Wong et al., 2011). Native language identification is the task of identifying the ESL learner’s L1 given a learner’s essay. The NLI Shared Task 2013 (Tetreault et al., 2013) is the first shared task on NLI using the common dataset “TOEFL-11” (Blanchard et al., 2013; Tetreault et al., 2012). TOEFL-11 consists of essays written by learners of 11 native languages (Arabic, Chinese, French, German, Hindi, Italian, Japanese, Koran, Spanish, Telugu, Turkish), and it contains 1,100 essays for each native language. In addition, the essay topics are balanced, and the number of topics is 8. In the clos"
W13-1717,U11-1015,0,0.0122657,"sure based on frequency for the closed track and try Capping and Sampling data methods for the open tracks. Our system ranked ninth in the closed track, third in open track 1 and fourth in open track 2. 1 Introduction There have been many studies using English as a second language (ESL) learner corpora. For example, automatic grammatical error detection and correction is one of the most active research areas in this field. More recently, attention has been paid to native language identification (NLI) (Brooke and Hirst, 2012; Bykh and Meurers, 2012; Brooke and Hirst, 2011; Wong and Dras, 2011; Wong et al., 2011). Native language identification is the task of identifying the ESL learner’s L1 given a learner’s essay. The NLI Shared Task 2013 (Tetreault et al., 2013) is the first shared task on NLI using the common dataset “TOEFL-11” (Blanchard et al., 2013; Tetreault et al., 2012). TOEFL-11 consists of essays written by learners of 11 native languages (Arabic, Chinese, French, German, Hindi, Italian, Japanese, Koran, Spanish, Telugu, Turkish), and it contains 1,100 essays for each native language. In addition, the essay topics are balanced, and the number of topics is 8. In the closed track, we tackle"
W13-2263,N10-1015,0,0.0436995,"Missing"
W13-2263,de-marneffe-etal-2006-generating,0,0.00926866,"Missing"
W13-2263,P07-1003,0,0.7654,"an alignment vector such that aj = i indicates the j-th target word aligns to the i-th source word and aj = 0 means the j-th target word is null-aligned. j is the index of the last non null-aligned target word before the index j. In both models, pt (fj |eaj ) is the lexical translation probability and can be defined as conditional probability distributions. As for the distortion probability pd (aj |aj ), pd (aj = 0|aj = i0 ) = p0 where p0 is NULL probability in both models. pd (aj = i|aj = i0 ) is uniform in the Model 1 and proportional to the relative count c(i − i0 ) in the HMM for i 6= 0. DeNero and Klein (2007) proposed a syntax-sensitive distortion model for the HMM alignment, in which the distortion probability depends on the path from the i-th word to the i0 -th word on the source-side phrase-structure tree, instead of the linear distance between the two words. These models can be trained efficiently using the EM algorithm. In practice, models in two directions (source to target and target to source) are trained and then symmetrized by taking their intersection, union or using other heuristics. Liang et al. (2006) proposed a joint objective of alignment models in both directions and the probabili"
W13-2263,P08-1066,0,0.0343647,"ich improves translation rule extraction for tree-to-string transducers. Both models as508 (a) Precision/Recall Curve with Soft-Union. (b) Precision/Recall Curve with Soft-Union + Competitive Thresholding. (c) Precision/Recall Curve with the Best Strategy. (d) Alignment Error Rate with Soft-Union. (e) Precision/Recall Curve with Soft-Union + Competi- (f) Alignment Error Rate with with the Best Strategy. tive Thresholding. Figure 2: Precision/Recall Curve and Alignment Error Rate with Different Models and Strategies. 509 directly incorporate dependency structures, such as string-to-dependency (Shen et al., 2008) and dependency-to-string (Xie et al., 2011) models, would be especially interesting. Last but not least, though the dependency structures don’t pose a hard restriction on the alignment in our model, it is highly likely that parse errors have negative effects on the alignment accuracy. One way to estimate the effect of parse errors on the accuracy is to parse the input sentences with inferior models, for example trained on a limited amount of training data. Moreover, preserving some ambiguities using k-best trees or shared forests might help mitigate the effect of 1best parse errors. sume a ch"
W13-2263,N06-2015,0,0.141282,"Missing"
W13-2263,C96-2141,0,0.837224,"s a tree structure of the hidden variables, which fits well with the notion of word-toword dependency, and it can be trained from unlabeled data via the EM algorithm with the same order of time complexity as HMMs. Introduction Automatic word alignment is the first step in the pipeline of statistical machine translation. Translation models are usually extracted from wordaligned bilingual corpora, and lexical translation probabilities based on word alignment models are also used for translation. The most widely used models are the IBM Model 4 (Brown et al., 1993) and Hidden Markov Models (HMM) (Vogel et al., 1996). These models assume that alignments are largely monotonic, possibly with a few jumps. While such assumption might be adequate for alignment between similar languages, it does not necessarily hold between a pair of distant languages like English and Japanese. Recently, several models have focused on incorporating syntactic structures into word alignment. As an extension to the HMM alignment, Lopez and Resnik (2005) present a distortion model conditioned on the source-side dependency In this paper, we propose a novel word alignment model based on the HMT model and show that it naturally enable"
W13-2263,P07-2045,0,0.00516379,"ave a considerable amount of posterior probability. If that is true, too many links will be above the threshold when it is set low, and too few links can exceed the threshold when it is set high. More sophisticated distortion model may help mitigate such sensitivity to the posterior threshold. Table 2: Alignment error rates (AER) based on each model’s peak performance. with window size w = 4 for the distortion model. The entire training procedure takes around 4 hours on a 3.3 GHz Xeon CPU. We train the IBM Model 4 using GIZA++ (Och and Ney, 2003) with the training script of the Moses toolkit (Koehn et al., 2007). The HMM and S-HMM alignment models are initialized with jointly trained IBM Model 1 parameters (5 iterations) and trained independently for 5 iterations using the Berkeley Aligner. We find that though initialization with jointly trained IBM Model 1 parameters is effective, joint training of HMM alignment models harms the performance on this dataset (results not shown). 5.3 Result We use posterior thresholding for the HMT and HMM alignment models, and the grow-diag-finaland heuristic for the IBM Model 4. Table 2 and Figure 2 show the result. As the Soft-Union criterion performed best, we don’"
W13-2263,1983.tc-1.13,0,0.677602,"Missing"
W13-2263,P09-2037,0,0.211469,"bined weights c and a threshold τ , it choose a link (j, i) only if its weight cji ≥ τ and it is connected to the link with the maximum weight both in row j and column i. 3 Hidden Markov Tree Model The Hidden Markov Tree (HMT) model was first introduced by Crouse et al. (1998). Though it has been applied successfully to various applications such as image segmentation (Choi and Baraniuk, 2001), denoising (Portilla et al., 2003) and biology (Durand et al., 2005), it is largely unnoticed in the field of natural language processing. To the best of our knowledge, the only exception is ˇ Zabokrtsk` y and Popel (2009) who used a variant of the Viterbi algorithm for HMTs in the transfer phase of a deep-syntax based machine translation system. An HMT model consists of an observed random tree X = {x1 , ..., xN } and a hidden random tree S = {s1 , ..., sN }, which is isomorphic to the observed tree. The parameters of the model are • P (s1 = j), the initial hidden state prior • P (st = j|sρ(t) = i), transition probabilities • P (xt = h|st = j), emission probabilities, a = {(i, j) : p(aj = i|f , e) > τ } where ρ() is a function that maps the index of a hidden node to the index of its parent node. These parameter"
W13-2263,W02-2016,1,0.6706,"Berkeley Aligner’s syntactic distortion model, and convert them to dependency trees for our dependency-based distortion model5 . As the Berkeley Parser couldn’t parse 7 (out of about 330K) sentences in the training data, we removed those lines from both sides of the data. All the sentences in the other sets were parsed successfully. For the Japanese side of the data, we first concatenate the function words in the tokenized sentences using a script6 published by the author of the dataset. Then we re-segment and POStag them using MeCab7 version 0.996 and parse them using CaboCha8 version 0.66 (Kudo and Matsumoto, 2002), both with UniDic. Finally, we modify the CoNLL-format output of CaboCha where some kind of symbols such as punctuation marks and parentheses have dependent words. We chose this procedure for a reasonable compromise between the dataset’s default tokenization and the dependency parser we use. As we cannot use the default gold alignment due to the difference in preprocessing, we use a script9 published by the author of the dataset to modify the gold alignment so that it better matches the new tokenization. just like the IBM Models and HMM alignments, where c(f, e) and c(e) are the count of the"
W13-2263,N06-1014,0,0.420834,"odel 1 and proportional to the relative count c(i − i0 ) in the HMM for i 6= 0. DeNero and Klein (2007) proposed a syntax-sensitive distortion model for the HMM alignment, in which the distortion probability depends on the path from the i-th word to the i0 -th word on the source-side phrase-structure tree, instead of the linear distance between the two words. These models can be trained efficiently using the EM algorithm. In practice, models in two directions (source to target and target to source) are trained and then symmetrized by taking their intersection, union or using other heuristics. Liang et al. (2006) proposed a joint objective of alignment models in both directions and the probability of agreement between them, and an EM-like algorithm for training. They also proposed posterior thresholding for decoding and symmetrization, which take where pf (aj = i|f , e) is the alignment probability under the source-to-target model and pr (ai = j|f , e) is the one under the target-to-source model. They also propose a posterior decoding heuristic called competitive thresholding. Given a j × i matrix of combined weights c and a threshold τ , it choose a link (j, i) only if its weight cji ≥ τ and it is co"
W13-2263,W05-0812,0,0.151445,"slation probabilities based on word alignment models are also used for translation. The most widely used models are the IBM Model 4 (Brown et al., 1993) and Hidden Markov Models (HMM) (Vogel et al., 1996). These models assume that alignments are largely monotonic, possibly with a few jumps. While such assumption might be adequate for alignment between similar languages, it does not necessarily hold between a pair of distant languages like English and Japanese. Recently, several models have focused on incorporating syntactic structures into word alignment. As an extension to the HMM alignment, Lopez and Resnik (2005) present a distortion model conditioned on the source-side dependency In this paper, we propose a novel word alignment model based on the HMT model and show that it naturally enables unsupervised training based on both source and target dependency trees in a tractable manner. We also compare our HMT word alignment model with the IBM Model 4 and the HMM alignment models in terms of the standard alignment error rates on a publicly available English-Japanese dataset. 2 IBM Model 1 and HMM Alignment We briefly review the IBM Model 1 (Brown et al., 1993) and the Hidden Markov Model (HMM) word align"
W13-2263,I11-1089,0,0.038217,"Missing"
W13-2263,J03-1002,0,0.190331,"model, the initial hidden state prior described in Section 3 can be defined by assuming an artificial ROOT node for both dependency trees, forcing the target ROOT node to be aligned only to the source ROOT c(d(i0 , i)) 0 00 i00 6=0 c(d(i , i )) = (1 − p0 ) · P if the j-th word is aligned. Note that we must artificially normalize pd (aj = i|aρ(j) = i0 ), because unlike in the case of the linear distance, multiple words can have the same distance from the j-th word on a dependency tree. 1 This dependence on aj can be implemented as a firstorder HMT, analogously to the case of the HMM alignment (Och and Ney, 2003). 505 a0 ↓ −ROOT−f0 a1 ↓ Thatf1 a2 ↓ wasf2 a3 ↓ Mountf3 a4 ↓ Kuramaf4 a5 ↓ .f5 (a) Target sentence with its dependency/alignment tree. Target words {f0 , ..., f5 } are emitted from alignment variables {a0 , ..., a5 }. Ideally, a0 = 0, a1 = 1, a2 = 7, a3 = 5, a4 = 4 and a5 = 9. −ROOT −e0 そのe1 that 山e2 mountain がe3 鞍馬e4 Kurama 山 e5 mountain でe6 あっe7 be たe8 。 e9 . (b) Source sentence with its dependency tree. None of the target words are aligned to e2 , e3 , e6 and e8 . Figure 1: An example of sentence pair under the Hidden Markov Tree word alignment model. If we ignore the source words to which"
W13-2263,P06-1055,0,0.0519743,"Y { βj,j 0 (i)}pt (fj |ei )p(aj = i) i = j 0 ∈c(j) 506 βj (i)pd (aj = i|aρ(j) = i0 )ξρ(j) (i0 ) , p(aj = i)βρ(j),j (i0 ) which is used for the estimation of distortion probabilities, can be extracted during the downward recursion. In the M-step, the lexical translation model can be updated with The tuning set of the KFTT has manual alignments. As the KFTT doesn’t distinguish between sure and possible alignments, F-measure equals 1 − AER on this dataset. 5.1 c(f, e) pt (f |e) = , c(e) We tokenize the English side of the data using the Stanford Tokenizer3 and parse it with the Berkeley Parser4 (Petrov et al., 2006). We use the phrasestructure trees for the Berkeley Aligner’s syntactic distortion model, and convert them to dependency trees for our dependency-based distortion model5 . As the Berkeley Parser couldn’t parse 7 (out of about 330K) sentences in the training data, we removed those lines from both sides of the data. All the sentences in the other sets were parsed successfully. For the Japanese side of the data, we first concatenate the function words in the tokenized sentences using a script6 published by the author of the dataset. Then we re-segment and POStag them using MeCab7 version 0.996 an"
W13-2263,P10-1017,0,0.0190787,"ests might help mitigate the effect of 1best parse errors. sume a chain structure for hidden variables (alignment) as opposed to a tree structure as in our model, and condition distortions on the syntactic structure only in one direction. Nakazawa and Kurohashi (2011) propose a dependency-based phrase-to-phrase alignment model with a sophisticated generative story, which leads to an increase in computational complexity and requires parallel sampling for training. Several supervised, discriminative models use syntax structures to generate features and to guide the search (Burkett et al., 2010; Riesa and Marcu, 2010; Riesa et al., 2011). Such efforts are orthogonal to ours in the sense that discriminative alignment models generally use statistics obtained by unsupervised, generative models as features and can benefit from their improvement. It would be interesting to incorporate statistics of the HMT word alignment model into such discriminative models. ˇ Zabokrtsk` y and Popel (2009) use HMT models for the transfer phase in a tree-based MT system. While our model assumes that the tree structure of alignment variables is isomorphic to target side’s dependency tree, they assume that the deep-syntactic tre"
W13-2263,D11-1046,0,0.0358705,"Missing"
W13-2263,J93-2003,0,\N,Missing
W13-2263,D11-1020,0,\N,Missing
W13-3523,P08-1088,0,0.104927,"Science and Technology 8916-5 Takayama, Ikoma, Nara 630-0192, Japan {xiaodong-l,kevinduh,matsu}@is.naist.jp Abstract One approach for building a bilingual dictionary resource uses parallel sentence-aligned corpora. This is often done in the context of Statistical MT, using word alignment algorithms such as the IBM models (Brown et al., 1993; Och and Ney, 2003). Unfortunately, parallel corpora may be scarce for certain language-pairs or domains of interest (e.g., medical and microblog). Thus, the use of comparable corpora for bilingual dictionary extraction has become an active research topic (Haghighi et al., 2008; Vuli´c et al., 2011). Here, a comparable corpus is defined as collections of document pairs written in different languages but talking about the same topic (Koehn, 2010), such as interconnected Wikipedia articles. The challenge with bilingual dictionary extraction from comparable corpus is that existing word alignment methods developed for parallel corpus cannot be directly applied. We believe there are several desiderata for bilingual dictionary extraction algorithms: We propose a flexible and effective framework for extracting a bilingual dictionary from comparable corpora. Our approach is"
W13-3523,W11-0801,0,0.0288397,"translated as “panfry“). Table 3 shows the distribution of error types by a manual classification. Incorrect Alignment errors are most frequent, implying the topic models are doing a reasonable job of generating the topicaligned corpus. The amount of Incorrect Topic is not trivial, though, so we would still imagine more advanced topic models to help. Segmentation errors are in general hard to solve, even with a better word segmenter, since in general one-to-one crosslingual word correspondence is not consistent–we believe the solution is a system that naturally handles multi-word expressions (Baldwin, 2011). 5 10 15 20 Topic Count Figure 5: Power-law distribution of number of word types with X number of topics. topic model of (Mimno et al., 2009) assumes one topic distribution per document pair. For lowlevels of comparability, a small number of topics may not sufficiently model the differences in topical content. This suggests the use of hierarchical topic models (Haffari and Teh, 2009) or other variants in future work. 3. What are the statistical characteristics of topic-aligned corpora? First, we show the word-topic distribution from multilingual topic modeling in the K = 400 scenario (first s"
W13-3523,W02-0902,0,0.16114,"ceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 212–221, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics with seminal works of (Rapp, 1995; Fung and Lo, 1998). The main idea is to assume that translation pairs have similar contexts, i.e. the distributional hypothesis, so extraction consists of 3 steps: (1) identify context windows around words, (2) translate context words using a seed bilingual dictionary, and (3) extract pairs that have high resulting similarity. Methods differ in how the seed dictionary is acquired (Koehn and Knight, 2002; D´ejean et al., 2002) and how similarity is defined (Fung and Cheung, 2004; Tamura et al., 2012). Projection-based approaches have also been proposed, though they can be shown to be related to the aforementioned distributional approaches (Gaussier et al., 2004); for example, Haghighi (2008) uses CCA to map vectors in different languages into the same latent space. Laroche (2010) presents a good summary. Vuli´c et al. (2011) pioneered a new approach to bilingual dictionary extraction based on topic modeling approach which requires no seed dictionary. While our approach is motivated by (Vuli´c"
W13-3523,J93-2003,0,0.0904013,"Missing"
W13-3523,J10-4005,0,0.0165542,"parallel sentence-aligned corpora. This is often done in the context of Statistical MT, using word alignment algorithms such as the IBM models (Brown et al., 1993; Och and Ney, 2003). Unfortunately, parallel corpora may be scarce for certain language-pairs or domains of interest (e.g., medical and microblog). Thus, the use of comparable corpora for bilingual dictionary extraction has become an active research topic (Haghighi et al., 2008; Vuli´c et al., 2011). Here, a comparable corpus is defined as collections of document pairs written in different languages but talking about the same topic (Koehn, 2010), such as interconnected Wikipedia articles. The challenge with bilingual dictionary extraction from comparable corpus is that existing word alignment methods developed for parallel corpus cannot be directly applied. We believe there are several desiderata for bilingual dictionary extraction algorithms: We propose a flexible and effective framework for extracting a bilingual dictionary from comparable corpora. Our approach is based on a novel combination of topic modeling and word alignment techniques. Intuitively, our approach works by converting a comparable document-aligned corpus into a pa"
W13-3523,C10-1070,0,0.118909,"Missing"
W13-3523,P11-2071,0,0.0551353,"Missing"
W13-3523,P09-1030,0,0.0301144,"Missing"
W13-3523,C02-1166,0,0.107487,"Missing"
W13-3523,P11-2032,0,0.0177225,"k ). While this kind of topic-dependent translation has been proposed for the parallel corpus (Zhao and Xing, 2007), we are the first to enable it for comparable corpora. Our large-scale experiments demonstrated that the proposed framework outperforms existing baselines under both automatic metrics and manual evaluation. We further show that our topicdependent translation models can capture some of the polysemy phenomenon important in dictionary construction. Future work includes: 1. Exploring other topic models (Haffari and Teh, 2009) and word alignment techniques (DeNero and Macherey, 2011; Mermer and Saraclar, 2011; Moore, 2004) in our framework. 2. Extract lexicon from massive multilingual collections. Mausum (2009) and Shezaf (2010) show that language pivots significantly improve the precision of distribution-based approaches. Since multilingual topic models can easily be trained on more than 3 languages, we expect it will give a big boost to our approach. Acknowledgments We thank Mamoru Komachi, Shuhei Kondo and the anonymous reviewers for valuable discussions and comments. Part of this research was executed under the Commissioned Research of National Institute of Information and Communications Techn"
W13-3523,D09-1092,0,0.679415,"uli´c et al. (2011) pioneered a new approach to bilingual dictionary extraction based on topic modeling approach which requires no seed dictionary. While our approach is motivated by (Vuli´c et al., 2011), we exploit the topic model in a very different way (explained in Section 4.2). They do not use word alignments like we do and thus cannot model polysemy. Further, their approach requires training topic models with a large number of topics, which may limit the scalability of the approach. Recently, there has been much interest in multilingual topic models (MLTM) (Jagarlamudi and Daume, 2010; Mimno et al., 2009; Ni et al., 2009; Boyd-Graber and Blei, 2009). Many of these models give p(t|e) and p(t|f ), but stop short of extracting a bilingual lexicon. Although topic models can group related e and f in the same topic cluster, the extraction of a high-precision dictionary requires additional effort. One of our contributions here is an effective way to do this extraction using word alignment methods. Figure 1: Proposed Framework allel topic-aligned corpus, then apply word alignment methods to model co-occurence within topics. By employing topic models, we avoid the need for seed lexicon and operate pur"
W13-3523,P11-1043,0,0.0170978,"ls of the form p(we |wf , tk ). While this kind of topic-dependent translation has been proposed for the parallel corpus (Zhao and Xing, 2007), we are the first to enable it for comparable corpora. Our large-scale experiments demonstrated that the proposed framework outperforms existing baselines under both automatic metrics and manual evaluation. We further show that our topicdependent translation models can capture some of the polysemy phenomenon important in dictionary construction. Future work includes: 1. Exploring other topic models (Haffari and Teh, 2009) and word alignment techniques (DeNero and Macherey, 2011; Mermer and Saraclar, 2011; Moore, 2004) in our framework. 2. Extract lexicon from massive multilingual collections. Mausum (2009) and Shezaf (2010) show that language pivots significantly improve the precision of distribution-based approaches. Since multilingual topic models can easily be trained on more than 3 languages, we expect it will give a big boost to our approach. Acknowledgments We thank Mamoru Komachi, Shuhei Kondo and the anonymous reviewers for valuable discussions and comments. Part of this research was executed under the Commissioned Research of National Institute of Informati"
W13-3523,P04-1066,0,0.0348541,"ic-dependent translation has been proposed for the parallel corpus (Zhao and Xing, 2007), we are the first to enable it for comparable corpora. Our large-scale experiments demonstrated that the proposed framework outperforms existing baselines under both automatic metrics and manual evaluation. We further show that our topicdependent translation models can capture some of the polysemy phenomenon important in dictionary construction. Future work includes: 1. Exploring other topic models (Haffari and Teh, 2009) and word alignment techniques (DeNero and Macherey, 2011; Mermer and Saraclar, 2011; Moore, 2004) in our framework. 2. Extract lexicon from massive multilingual collections. Mausum (2009) and Shezaf (2010) show that language pivots significantly improve the precision of distribution-based approaches. Since multilingual topic models can easily be trained on more than 3 languages, we expect it will give a big boost to our approach. Acknowledgments We thank Mamoru Komachi, Shuhei Kondo and the anonymous reviewers for valuable discussions and comments. Part of this research was executed under the Commissioned Research of National Institute of Information and Communications Technology (NICT),"
W13-3523,P11-2093,0,0.0235328,"Missing"
W13-3523,W04-3208,0,0.0316964,"ning, pages 212–221, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics with seminal works of (Rapp, 1995; Fung and Lo, 1998). The main idea is to assume that translation pairs have similar contexts, i.e. the distributional hypothesis, so extraction consists of 3 steps: (1) identify context windows around words, (2) translate context words using a seed bilingual dictionary, and (3) extract pairs that have high resulting similarity. Methods differ in how the seed dictionary is acquired (Koehn and Knight, 2002; D´ejean et al., 2002) and how similarity is defined (Fung and Cheung, 2004; Tamura et al., 2012). Projection-based approaches have also been proposed, though they can be shown to be related to the aforementioned distributional approaches (Gaussier et al., 2004); for example, Haghighi (2008) uses CCA to map vectors in different languages into the same latent space. Laroche (2010) presents a good summary. Vuli´c et al. (2011) pioneered a new approach to bilingual dictionary extraction based on topic modeling approach which requires no seed dictionary. While our approach is motivated by (Vuli´c et al., 2011), we exploit the topic model in a very different way (explaine"
W13-3523,P04-1067,0,0.342167,"that translation pairs have similar contexts, i.e. the distributional hypothesis, so extraction consists of 3 steps: (1) identify context windows around words, (2) translate context words using a seed bilingual dictionary, and (3) extract pairs that have high resulting similarity. Methods differ in how the seed dictionary is acquired (Koehn and Knight, 2002; D´ejean et al., 2002) and how similarity is defined (Fung and Cheung, 2004; Tamura et al., 2012). Projection-based approaches have also been proposed, though they can be shown to be related to the aforementioned distributional approaches (Gaussier et al., 2004); for example, Haghighi (2008) uses CCA to map vectors in different languages into the same latent space. Laroche (2010) presents a good summary. Vuli´c et al. (2011) pioneered a new approach to bilingual dictionary extraction based on topic modeling approach which requires no seed dictionary. While our approach is motivated by (Vuli´c et al., 2011), we exploit the topic model in a very different way (explained in Section 4.2). They do not use word alignments like we do and thus cannot model polysemy. Further, their approach requires training topic models with a large number of topics, which m"
W13-3523,J03-1002,0,0.0122219,"e known. For each document pair di = [dei , dfi ] consisting of English document dei and Foreign document dfi (where i ∈ {1, . . . , D}, D is number of document pairs), we know that dei and dfi talk about the same topics. While the monolingual topic model lets each document have its own so-called documentspecific distribution over topics, the multilingual topic model assumes that documents in each tuple share the same topic prior (thus the comparable corpora assumption) and each topic consists of several language-specific word distributions. The generative story is shown in Algorithm 1. 1993; Och and Ney, 2003), which proposes the following probabilistic model for alignment: p(e, a, |f ) ≈ f p(wie |wa(i) ) i=1 f p(wie |wa(i) ) (1) Here, captures the translation probability of the English word at position i from the foreign word at position j = a(i), where the actual alignment a is a hidden variable, and training can be done via EM. Although this model does not incorporate much linguistic knowledge, it enables us to find correspondence between distinct objects from paired sets. In machine translation, the distinct objects are words from different languages while the paired sets are sentence-aligned c"
W13-3523,P95-1050,0,0.492305,"s also rely on bilingual dictionaries as integral components. 3. Scalability: The approach should run efficiently an massively large-scale datasets. Our framework addresses the above desired points by exploiting a novel combination of topic models and word alignment, as shown in Figure 1. Intuitively, our approach works by first converting a comparable document-aligned corpus into a par212 Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 212–221, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics with seminal works of (Rapp, 1995; Fung and Lo, 1998). The main idea is to assume that translation pairs have similar contexts, i.e. the distributional hypothesis, so extraction consists of 3 steps: (1) identify context windows around words, (2) translate context words using a seed bilingual dictionary, and (3) extract pairs that have high resulting similarity. Methods differ in how the seed dictionary is acquired (Koehn and Knight, 2002; D´ejean et al., 2002) and how similarity is defined (Fung and Cheung, 2004; Tamura et al., 2012). Projection-based approaches have also been proposed, though they can be shown to be related"
W13-3523,N09-1020,0,0.0892861,"general hard to solve, even with a better word segmenter, since in general one-to-one crosslingual word correspondence is not consistent–we believe the solution is a system that naturally handles multi-word expressions (Baldwin, 2011). 5 10 15 20 Topic Count Figure 5: Power-law distribution of number of word types with X number of topics. topic model of (Mimno et al., 2009) assumes one topic distribution per document pair. For lowlevels of comparability, a small number of topics may not sufficiently model the differences in topical content. This suggests the use of hierarchical topic models (Haffari and Teh, 2009) or other variants in future work. 3. What are the statistical characteristics of topic-aligned corpora? First, we show the word-topic distribution from multilingual topic modeling in the K = 400 scenario (first step of Proposed, Cue, and JS). For each word type w, we count the number of topics it may appear in, i.e. nonzero probabilities according to p(w|t). Fig. 5 shows the number of word types that have x number of topics. This powerlaw is expected since we are modeling all words.9 Next we compute the statistics after constructing the topic-aligned corpora (Step 3 of Fig. 2). For each part"
W13-3523,P10-1011,0,0.163841,"Missing"
W13-3523,D12-1003,0,0.177235,"Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics with seminal works of (Rapp, 1995; Fung and Lo, 1998). The main idea is to assume that translation pairs have similar contexts, i.e. the distributional hypothesis, so extraction consists of 3 steps: (1) identify context windows around words, (2) translate context words using a seed bilingual dictionary, and (3) extract pairs that have high resulting similarity. Methods differ in how the seed dictionary is acquired (Koehn and Knight, 2002; D´ejean et al., 2002) and how similarity is defined (Fung and Cheung, 2004; Tamura et al., 2012). Projection-based approaches have also been proposed, though they can be shown to be related to the aforementioned distributional approaches (Gaussier et al., 2004); for example, Haghighi (2008) uses CCA to map vectors in different languages into the same latent space. Laroche (2010) presents a good summary. Vuli´c et al. (2011) pioneered a new approach to bilingual dictionary extraction based on topic modeling approach which requires no seed dictionary. While our approach is motivated by (Vuli´c et al., 2011), we exploit the topic model in a very different way (explained in Section 4.2). The"
W13-3523,P11-2084,0,0.222493,"Missing"
W13-3523,H01-1033,0,\N,Missing
W13-3604,N03-1017,0,0.0099261,"rs. To focus on preposition and determiner errors, we retain proposed edits that include 48 prepositions and 25 determiners listed in Table 3. best thresholds for singular and plural forms, respectively. For proper and uncountable nouns, we do not change number because of the nature of those nouns. In order to determine whether to change number or not, we create a list which consists of words frequently used as singular forms in the native corpus. 3.3 Prepositions and Determiners For preposition and determiner errors, we construct a system using a phrase-based statistical machine translation (Koehn et al., 2003) framework. The SMT-based approach functions well in corrections of conventional usage of determiners and prepositions such as “the young” and “take care of ”. The characteristic of the SMT-based approach is its ability to capture tendencies in learners’ errors. This approach translates erroneous phrases that learners often make to correct phrases. Hence, it can handle errors in conventional expressions without over-generalization. The phrase-based SMT framework which we used is based on the log-linear model (Och and Ney, 2002), where the decision rule is expressed as follow: argmax P (e|f ) ="
W13-3604,P08-1021,0,0.201996,"nd Technology (NAIST) error correction system in the CoNLL 2013 Shared Task. We constructed three systems: a system based on the Treelet Language Model for verb form and subjectverb agreement errors; a classifier trained on both learner and native corpora for noun number errors; a statistical machine translation (SMT)-based model for preposition and determiner errors. As for subject-verb agreement errors, we show that the Treelet Language Model-based approach can correct errors in which the target verb is distant from its subject. Our system ranked fourth on the official run. 1 2 Related Work Lee and Seneff (2008) tried correcting English verb errors including SVA and Vform. They proposed correction candidates with template matching on parse trees and filtered candidates by utilizing n-gram counts. Our system suggests candidates based on the Part-Of-Speech (POS) tag of a target word and filters them by using a syntactic language model. For the noun number errors, we improved the system proposed by Izumi et al. (2003). In Izumi et al. (2003), a noun number error detection method is a part of an automatic error detection system for transcribed spoken English by Japanese learners. They used a maximum entr"
W13-3604,C12-2084,1,0.823085,"Missing"
W13-3604,P02-1038,0,0.126271,"ct a system using a phrase-based statistical machine translation (Koehn et al., 2003) framework. The SMT-based approach functions well in corrections of conventional usage of determiners and prepositions such as “the young” and “take care of ”. The characteristic of the SMT-based approach is its ability to capture tendencies in learners’ errors. This approach translates erroneous phrases that learners often make to correct phrases. Hence, it can handle errors in conventional expressions without over-generalization. The phrase-based SMT framework which we used is based on the log-linear model (Och and Ney, 2002), where the decision rule is expressed as follow: argmax P (e|f ) = argmax e e M ∑ 4 Experiments 4.1 Experimental setting 4.1.1 Subject-Verb Agreement and Verb Form We describe here the training data and tools used to train our model. Our model was trained with the Berkeley LM9 version 1.1.3. We constructed the training data by concatenating the WSJ sections of the Penn Treebank and the AFP sections of the English Gigaword Corpus version 5.10 Our training data consists of about 27 million sentences. Although human-annotated parses for the WSJ are available, there is no gold standard for the AF"
W13-3604,P12-1101,0,0.0611644,"nd evaluated its performance for 18 error types, including preposition and determiner errors in the Konan-JIEM Learner Corpus. On preposition error correction, they showed that their SMT system outperformed a system using a maximum entropy model. The main difference with this work is that our new corpus collection here is about three times larger. They want *go to Nara this summer. Verbs can be a complement of another verb and preposition. The “go” in the above sentence is incorrect. It should be in the infinitive form, “to go”. 3.1.2 Treelet Language Model We used the Treelet Language Model (Pauls and Klein, 2012) for SVA and Vform error correction. Our model assigns probability to a production rule of the form r = P → C1 · · · Cd in a constituent tree T , conditioned on a context h consisting of previously generated treelets,3 where P is the parent symbol of a rule r and C1d = C1 · · · Cd are its children. 3 System Architecture 3.1 Subject-Verb Agreement and Verb Form For SVA and Vform errors, we used the Treelet Language Model (Pauls and Klein, 2012) to capture syntactic information and lexical information simultaneously. We will first show examples of SVA and Vform errors and then describe our model"
W13-3604,N04-1030,0,0.0509398,"nary classifier indicates “singular” or “plural” for all nouns except proper and uncountable nouns. First, if a noun is found in the training corpus, we extract an instance with features created by the feature template in Table 2.8 Second, we train a classifier with extracted instances and labels from the training corpus. We use unigram, bigram, and trigram features around the target word and the path features between the target word and all the other nodes in the NPs that dominate the target word as the rightmost constituent. The path feature is commonly used in semantic role labeling tasks (Pradhan et al., 2004). For the path features, we do not use the right subtree of the NP as the path features because we assume that right subtrees do not affect the number of the target word. We limit the maximum depth of the subtree containing the NP to be 3 because nodes over this limit may be noisy. To encode the relationship between the target word and another node in the NP, we append a symbol which reflects the direction of tree traversal to the label: ‘p’ for going up (parent) and ‘c’ for going down (child). For example, we show extracted features in Table 2 for the phrase “some interesting and recent topic"
W13-3604,P05-1034,0,0.079463,"Missing"
W13-3604,P06-1032,0,0.309534,"NP, pc JJ, pc recent, pp NP, ppc CC, ppc and, ppc NP, ppcc DT, ppcc some, ppcc JJ, ppcc interesting Table 2: Features used for the detection of noun number errors and example features for the phrase “some interesting and recent topics about politics and economics”. model and the language model. The translation model suggests translation hypotheses and the language model filters out ill-formed hypotheses. For an error correction system based on SMT, the translation model is constructed from pairs of original sentences and corrected sentences, and the language model is built on a native corpus (Brockett et al., 2006). Brockett et al. (2006) trained the translation model on a corpus where the errors are restricted to mass noun errors. In our case, we trained our model on a corpus with no restriction on error types. Consequently, the system corrects all types of errors. To focus on preposition and determiner errors, we retain proposed edits that include 48 prepositions and 25 determiners listed in Table 3. best thresholds for singular and plural forms, respectively. For proper and uncountable nouns, we do not change number because of the nature of those nouns. In order to determine whether to change number"
W13-3604,N12-1067,0,0.0683393,"inal revised Precision Recall F-score Precision Recall F-score ALL 0.2707 0.1832 0.2185 0.3392 0.2405 0.2814 submitted system Verb Nn Prep 0.1378 0.4452 0.2649 0.2520 0.1641 0.1286 0.1782 0.2399 0.1732 0.1814 0.5578 0.3245 0.2867 0.1708 0.1494 0.2222 0.2616 0.2046 ArtOrDet 0.3118 0.2029 0.2458 0.4027 0.2497 0.3082 additional experiments Verb Nn 0.2154 0.3687 0.0569 0.2020 0.0900 0.2610 0.3846 0.4747 0.0880 0.2137 0.1433 0.2947 Table 4: Results of the submitted system for each type of error and results of additional experiments with the SMT-based system. The score is evaluated on the m2scorer (Dahlmeier and Ng, 2012). ALL is the official result of formal run, and each of the others shows the result of the corresponding error type. Since our system did not distinguish SVA and Vform, we report the combined result for them in the column Verb. gold: The rising life expectancy is like a two side sword to the modern world. of” are features with strong weights for the plural class as expected. However, n-gram features sometimes work to the contrary of our expectations. Since the subject of “are” is “expectancies”, the sentence looks correct at first. However, this example includes not only an SVA error but also"
W13-3604,W13-1703,0,0.43257,"e trained a classifier on the mixed corpus of the leaner corpus and the native corpus. We employ a treepath feature in our system. Our SMT system for correcting preposition and Introduction Grammatical error correction is the task of automatically detecting and correcting grammatical errors in text, especially text written by second language learners. Its purpose is to assist learners in writing and helps them learn languages. Last year, HOO 2012 (Dale et al., 2012) was held as a shared task on grammatical error correction, focusing on prepositions and determiners. The CoNLL-2013 shared task (Dahlmeier et al., 2013) includes these areas and also noun number, verb form, and subject-verb agreement errors. We divide the above 5 error types into three groups: (1) subject-verb agreement (SVA) and verb form (Vform) errors, (2) noun number (Nn) errors, and (3) preposition (Prep) and determiner (ArtOrDet) errors. For the subject-verb agreement and verb form errors, we used a syntactic language model, the Treelet Language Model, because syntactic information is important for verb error correction. For the noun number errors, we used a binary classifier trained on both learner and native 26 Proceedings of the Seve"
W13-3604,W12-2006,0,0.146672,"rained a classifier on only a learner corpus. The main difference between theirs and ours is a domain of the training corpus and features we used. We trained a classifier on the mixed corpus of the leaner corpus and the native corpus. We employ a treepath feature in our system. Our SMT system for correcting preposition and Introduction Grammatical error correction is the task of automatically detecting and correcting grammatical errors in text, especially text written by second language learners. Its purpose is to assist learners in writing and helps them learn languages. Last year, HOO 2012 (Dale et al., 2012) was held as a shared task on grammatical error correction, focusing on prepositions and determiners. The CoNLL-2013 shared task (Dahlmeier et al., 2013) includes these areas and also noun number, verb form, and subject-verb agreement errors. We divide the above 5 error types into three groups: (1) subject-verb agreement (SVA) and verb form (Vform) errors, (2) noun number (Nn) errors, and (3) preposition (Prep) and determiner (ArtOrDet) errors. For the subject-verb agreement and verb form errors, we used a syntactic language model, the Treelet Language Model, because syntactic information is i"
W13-3604,N10-1140,0,\N,Missing
W13-3604,J03-4003,0,\N,Missing
W13-3604,P03-2026,0,\N,Missing
W13-4409,P00-1032,0,0.304001,"Missing"
W13-4409,O03-4002,0,0.0907584,"uency of a correct phrase is higher than the corresponding error template. Wu et al. (2010) proposed a system which implemented a translate model and a template module. Then the system merged the output of the two single models and reached a balanced performance on precision and recall. 3 Figure 1: System structure. 3.1 Language Model Based Method To generate the correction candidates, firstly we segment the sentence into words and then find all possible corrections based on the confusion set and a Chinese dictionary. In this study, we use the character based Chinese word segmentation model2 (Xue, 2003), which outperforms the word based word segmentation model in out-of-vocabulary recall. The model is trained on the Academia Sinica corpus, released under the Chinese word segmentation bake-off 20053 and the feature templates are the same in Sun (2011). For example, given the following Chinese sentence (here, the Chinese character in red indicates an error character): ”我 看 過 許 多 勇 敢 的 人 ， 不 怕措折 地 奮 鬥。”. Firstly, we segment the sentence into words separated by a slash as follows. ”我/看過/許多/勇敢/的/人/，/不怕/措折/的/ 奮鬥/。” . Secondly, we build a lattice, as shown in Figure 2, based on the following rules:"
W13-4409,O09-2007,0,0.0386347,"Missing"
W13-4409,W10-4107,0,0.157669,"detect Chinese spelling errors. They used CKIP word segmentation toolkit to generate correction candidates (CKIP, 1999). By incorporating a dictionary and confusion sets, the system can detect whether a segmented word contains error or not. Hung et al. (2008) proposed a system which was based on manually edited error templates (short phrases with one error). For the cost of editing error templates manually, Cheng et al. (2008) proposed an automatic error template generation system. The basic assumption is that the frequency of a correct phrase is higher than the corresponding error template. Wu et al. (2010) proposed a system which implemented a translate model and a template module. Then the system merged the output of the two single models and reached a balanced performance on precision and recall. 3 Figure 1: System structure. 3.1 Language Model Based Method To generate the correction candidates, firstly we segment the sentence into words and then find all possible corrections based on the confusion set and a Chinese dictionary. In this study, we use the character based Chinese word segmentation model2 (Xue, 2003), which outperforms the word based word segmentation model in out-of-vocabulary r"
W13-4409,J93-2003,0,\N,Missing
W13-4409,P11-1139,0,\N,Missing
W13-4604,P11-2093,1,0.605576,"Missing"
W13-4604,2012.eamt-1.60,0,0.0168747,"in the previous section. For training, 4,000 sentences is not enough to build an accurate MT system, so we add several additional corpora for each language pair. For Japanese-English parallel training data, we add the Eijiro dictionary1 and its accompanying sample sentences, the BTEC corpus(Takezawa et al., 2002), and Wikipedia data from the Kyoto Free Translation Task (Neubig, 2011), for a total of 1.33M parallel sentences and 1.97M dictionary entries. For Japanese-Chinese parallel training data, we add a dictionary extracted from Wikipedia’s language links2 , the BTEC corpus, and TED talks (Cettolo et al., 2012) for a total of 519k sentences and 184k dictionary entries. In addition, we add monolingual from English GigaWord with 22.5M sentences and Chinese Wikipedia with 841k sentences. We compare three different statistical translation methodologies: phrase-based MT (PBMT, (Koehn et al., 2003)), hierarchical phrase-based MT (Hiero, (Chiang, 2007)), and forest-to-string MT (F2S, (Mi et al., 2008)). The reason why we test these three methodologies is because the former two methodologies do not rely on syntactic analysis, and thus may be more robust to conversational input that is ill-formed and/or info"
W13-4604,J07-2003,0,0.120881,"Free Translation Task (Neubig, 2011), for a total of 1.33M parallel sentences and 1.97M dictionary entries. For Japanese-Chinese parallel training data, we add a dictionary extracted from Wikipedia’s language links2 , the BTEC corpus, and TED talks (Cettolo et al., 2012) for a total of 519k sentences and 184k dictionary entries. In addition, we add monolingual from English GigaWord with 22.5M sentences and Chinese Wikipedia with 841k sentences. We compare three different statistical translation methodologies: phrase-based MT (PBMT, (Koehn et al., 2003)), hierarchical phrase-based MT (Hiero, (Chiang, 2007)), and forest-to-string MT (F2S, (Mi et al., 2008)). The reason why we test these three methodologies is because the former two methodologies do not rely on syntactic analysis, and thus may be more robust to conversational input that is ill-formed and/or informal. On the other hand, using syntactic information has been shown to improve translation, particularly between language pairs with different syntactic structures such as those we are handling in our experiments. Thus it will be interesting to see which methodology can produce better results, and also if any difference in the effectivenes"
W13-4604,C04-1114,0,0.0639568,"Missing"
W13-4604,P13-4016,1,0.882079,"Missing"
W13-4604,I11-1087,1,0.830624,"Missing"
W13-4604,J03-1002,0,0.0085147,"Missing"
W13-4604,D10-1092,0,0.0662551,"Missing"
W13-4604,P02-1040,0,0.0879458,"Missing"
W13-4604,N03-1017,0,0.0066382,"us(Takezawa et al., 2002), and Wikipedia data from the Kyoto Free Translation Task (Neubig, 2011), for a total of 1.33M parallel sentences and 1.97M dictionary entries. For Japanese-Chinese parallel training data, we add a dictionary extracted from Wikipedia’s language links2 , the BTEC corpus, and TED talks (Cettolo et al., 2012) for a total of 519k sentences and 184k dictionary entries. In addition, we add monolingual from English GigaWord with 22.5M sentences and Chinese Wikipedia with 841k sentences. We compare three different statistical translation methodologies: phrase-based MT (PBMT, (Koehn et al., 2003)), hierarchical phrase-based MT (Hiero, (Chiang, 2007)), and forest-to-string MT (F2S, (Mi et al., 2008)). The reason why we test these three methodologies is because the former two methodologies do not rely on syntactic analysis, and thus may be more robust to conversational input that is ill-formed and/or informal. On the other hand, using syntactic information has been shown to improve translation, particularly between language pairs with different syntactic structures such as those we are handling in our experiments. Thus it will be interesting to see which methodology can produce better r"
W13-4604,P07-2045,0,0.0126736,"Missing"
W13-4604,P10-1017,0,0.13341,"Missing"
W13-4604,takezawa-etal-2002-toward,0,0.0605347,"edical communication as well. As a result, it is likely that adapting to medical terminology of the domain is somewhat less important than adapting to the conversational speaking style of the speech. 4 4.1 Experimental Setup For the tuning and test data for our translation system, we use the data described in the previous section. For training, 4,000 sentences is not enough to build an accurate MT system, so we add several additional corpora for each language pair. For Japanese-English parallel training data, we add the Eijiro dictionary1 and its accompanying sample sentences, the BTEC corpus(Takezawa et al., 2002), and Wikipedia data from the Kyoto Free Translation Task (Neubig, 2011), for a total of 1.33M parallel sentences and 1.97M dictionary entries. For Japanese-Chinese parallel training data, we add a dictionary extracted from Wikipedia’s language links2 , the BTEC corpus, and TED talks (Cettolo et al., 2012) for a total of 519k sentences and 184k dictionary entries. In addition, we add monolingual from English GigaWord with 22.5M sentences and Chinese Wikipedia with 841k sentences. We compare three different statistical translation methodologies: phrase-based MT (PBMT, (Koehn et al., 2003)), hie"
W13-4604,I05-3027,0,0.0659246,"Missing"
W13-4604,A00-2028,0,0.0297292,"challenging for a number of reasons. The first reason is that communication of incomplete or incorrect information could lead to a mistaken diagnosis with severe consequences, and thus extremely high levels of accuracy and reliability are 22 International Joint Conference on Natural Language Processing Workshop on Natural Language Processing for Medical and Healthcare Fields, pages 22–29, Nagoya, Japan, 14-18 October 2013. man interpreters, either based on a manual request of one of the users, or through automatic detection of when the dialogue is going poorly, such as the method described by Walker et al. (2000). Even with this fall-back to human interpreters, it is still desirable that the automatic translation system is effective as possible. In order to ensure this, we must be certain that the ASR, MT, and TTS models are all tuned to work as well as possible in medical situations. Some potential problems that we have identified so far based on our analysis of data are as follows: Figure 1: An overview of the use scenario for the medical translation system. Specialized Vocabulary: Perhaps the most obvious problem is that the ASR, MT, and TTS systems must all be able to handle the specialized vocabu"
W13-4604,C04-1168,0,0.0303816,"r. However, as the cost of hiring and maintaining medical interpreters is quite high, we would also like to reduce our reliance on human effort as much as possible. Thus, each device will use automatic translation by default, but also have functionality to connect to huTranslation/Synthesis of Erroneous Input: As we can expect ASR not to be perfect, it will be necessary to be able to translate input that contains errors. This problem can potentially be ameliorated by passing multiple speech recognition hypotheses to translation (Ney, 1999), and jointly optimizing the parameters of ASR and MT (Zhang et al., 2004; Ohgushi et al., 2013). In addition, it will also be necessary to resolve difficulties in TTS due to grammatical errors, lack of punctuation, and unknown words (Parlikar et al., 2010). While all of these problems need to be solved to provide high-reliability speech translation systems, in this paper as a first step we focus mainly on the MT system, and relegate the last problem of integration with ASR to future work. 23 3 Medical Translation Corpus Construction and Analysis In this section, we describe our collection of a tri-lingual (Japanese, English, Chinese) corpus to serve as an initial"
W13-4604,W12-4213,0,\N,Missing
W13-4604,P08-1023,0,\N,Missing
W14-0819,C10-3010,0,0.0310546,"gual data is largely and freely available for many languages. Our approach requires only a bilingual dictionary and non-parallel monolingual corpora in both languages. 109 Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 109–113, c Gothenburg, Sweden, 26-27 April 2014. 2014 Association for Computational Linguistics Salehi and Cook (2013) predict the degree of compositionality using the string distance between the automatic translation into multiple languages of an expression and the individual translation of its components. They use an online database called Panlex (Baldwin et al., 2010), that can translate words and expressions from English into many languages. Tsvetkov and Wintner (2013) is probably the closest work to ours. They trained a Bayesian Network for identfying MWE’s and one of the features used is a binary feature that assumes value is 1 if the literal translation of the MWE candidate occurs more than 5 times in a large English corpus. 3 Identifying Collocations In this research, we predict whether the expression(s) resulted from the translation of the components of a Japanese collocation candidate is/are also commonly used in English. For instance, if we transla"
W14-0819,N03-1017,0,0.0222043,"our proposed method, for each candidate in the test set, we find all the possible literally translated expressions (as described in Section 3). In the phrase-table generated after the training step, we look for all the entries that contain the original candidate string and check if at least one of the possible literal translations appear as their corresponding translation. For the entries found, we compute the average of the sum of the candidate’s direct and inverse phrase translation probability scores. The direct phrase translation probability and the inverse phrase translation probability (Koehn et al., 2003) are respectively defined as: count(f , e) Φ(e|f ) = P f count(f , e) (1) count(f , e) Φ(f |e) = P e count(f , e) (2) Where f and e indicate a foreign phrase and a source phrase, independently. The candidates are ranked according to the average score as described previously. 5 Evaluation In our evaluation, we average the precision considering all true collocations and idioms as threshold points, obtaining the mean average precision (MAP). Differently from the traditional approach used to evaluate an association measure, using MAP we do not need to set a hard threshold. 112 Table 2 presents the"
W14-0819,P07-2045,0,0.0045739,"ee combinations). After that, we obtained a new Kappa coefficient of 0.5427, which is also considered as showing moderate agreement (Fleiss, 1971). 4.3 Baseline We compare our proposed method with two baselines: an association measure based system and a Phrase-Based Statistical Machine Translation (SMT) based system. Monolingual Association Measure: The system ranks the candidates in the test set according to their Dice score calculated using the Hiragana Times Japanese data. Phrase-Based SMT system: a standard nonfactored phrase-based SMT system was built using the open source Moses toolkit (Koehn et al., 2007) with parameters set similar to those of Neubig (2011), who provides a baseline system previously applied to a Japanese-English corpus built from Wikipedia articles. For training, we used Hiragana Times bilingual corpus. The Japanese sentences were word-segmented and the English sentences were tokenized and lowercased. All sentences with size greater than 60 tokens were previously eliminated. The whole English corpus was used as training data for a 5-gram language model built with the SRILM toolkit (Stolcke, 2002). Similar to what we did for our proposed method, for each candidate in the test"
W14-0819,W02-2016,1,0.833863,"is more prone to appear in an English corpus, since it corresponds to the translation of the expression as well. In our work, we focus on noun-verb expressions in Japanese. Our proposed method consists of three steps: 1) Candidate Extraction: We focus on nounverb constructions in Japanese. We work with three construction types: object-verb, subject-verb and dative-verb constructions, represented respectively as “noun wo verb (noun-を-verb)”, “noun ga verb (noun-が-verb)” and “noun ni verb (noun-にverb)”, respectively. The candidates are extracted from a Japanese corpus using a dependency parser (Kudo and Matsumoto, 2002) and ranked by frequency. 2) Translation of the component words: for each noun-verb candidate, we automatically obtain all the possible English literal translations of the noun and the verb using a Japanese/English dictionary. Using that information, all the possible verb-noun combinations in English are then generated. For instance, for the candidate 本を 1 In Japanese, を is a case marker that indicates the objectverb dependency relation. 110 買う hon-wo-kau ”to buy a book” (buy-を-book), we take the noun 本 hon and the verb 買う kau and check their translation given in the dictionary. 本 has translat"
W14-0819,W97-0311,0,0.150133,"asures are applied to rank the candidates based on association scores and consequently remove noise. One drawback of such method is that association measures might not be able to perform a clear-cut distinction between collocation and noncollocations, since they only assign scores based on statistical evidence, such as co-occurrence frequency in the corpus. Our cross-lingual association measure ameliorates this problem by exploiting both corpora in two languages, one of which may be large. A few studies have attempted to identify noncompositional MWE’s using parallel corpora and dictionaries. Melamed (1997) investigates how non-compositional compounds can be detected from parallel corpus by identifying translation divergences in the component words. Pichotta and DeNero (2013) analyses the frequency statistics of an expression and its component words, using many bilingual corpora to identifying phrasal verbs in English. The disadvantage of such approach is that large-scale parallel corpora is available for only a few language pairs. On the other hand, monolingual data is largely and freely available for many languages. Our approach requires only a bilingual dictionary and non-parallel monolingual"
W14-0819,D13-1060,0,0.0192611,"ght not be able to perform a clear-cut distinction between collocation and noncollocations, since they only assign scores based on statistical evidence, such as co-occurrence frequency in the corpus. Our cross-lingual association measure ameliorates this problem by exploiting both corpora in two languages, one of which may be large. A few studies have attempted to identify noncompositional MWE’s using parallel corpora and dictionaries. Melamed (1997) investigates how non-compositional compounds can be detected from parallel corpus by identifying translation divergences in the component words. Pichotta and DeNero (2013) analyses the frequency statistics of an expression and its component words, using many bilingual corpora to identifying phrasal verbs in English. The disadvantage of such approach is that large-scale parallel corpora is available for only a few language pairs. On the other hand, monolingual data is largely and freely available for many languages. Our approach requires only a bilingual dictionary and non-parallel monolingual corpora in both languages. 109 Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 109–113, c Gothenburg, Sweden, 26-27 April 2014. 2014 Associatio"
W14-0819,W12-3311,0,0.346846,"meaning of the expression cannot be predicted from the meaning of the parts, i.e. they are characterized by limited compositionality (Manning and Sch¨utze, 1999). Given an expression, we predict whether the expression(s) resulted from the word by word translation is also commonly used in another language. If not, that might be evidence that the original expression is a collocation (or an idiom). This can be captured by the ratio of association scores, assigned Related Work Most previous works on MWEs and, more specifically, collocation identification (Evert, 2008; Seretan, 2011; Pecina, 2010; Ramisch, 2012) employ a standard methodology consisting of two steps: 1) candidate extraction, where candidates are extracted based on n-grams or morphosyntactic patterns and 2) candidate filtering, where association measures are applied to rank the candidates based on association scores and consequently remove noise. One drawback of such method is that association measures might not be able to perform a clear-cut distinction between collocation and noncollocations, since they only assign scores based on statistical evidence, such as co-occurrence frequency in the corpus. Our cross-lingual association measu"
W14-0819,S13-1039,0,0.0197324,"cs of an expression and its component words, using many bilingual corpora to identifying phrasal verbs in English. The disadvantage of such approach is that large-scale parallel corpora is available for only a few language pairs. On the other hand, monolingual data is largely and freely available for many languages. Our approach requires only a bilingual dictionary and non-parallel monolingual corpora in both languages. 109 Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 109–113, c Gothenburg, Sweden, 26-27 April 2014. 2014 Association for Computational Linguistics Salehi and Cook (2013) predict the degree of compositionality using the string distance between the automatic translation into multiple languages of an expression and the individual translation of its components. They use an online database called Panlex (Baldwin et al., 2010), that can translate words and expressions from English into many languages. Tsvetkov and Wintner (2013) is probably the closest work to ours. They trained a Bayesian Network for identfying MWE’s and one of the features used is a binary feature that assumes value is 1 if the literal translation of the MWE candidate occurs more than 5 times in"
W14-0819,D11-1077,0,\N,Missing
W15-2208,C10-1094,0,0.0606877,"Missing"
W15-2208,P99-1059,0,0.230422,"Missing"
W15-2208,E12-1044,0,0.415385,"0), the accuracy in parsing the construction known as right node raising, which includes a coordinate structure such as “president and chief executive of the company,” is less than fifty percent. Apart from dependency parsers, there are methods specialized for coordination structure analysis, which attempt to identify coordinate structures from the similarity of conjuncts (Kurohashi and Nagao, 1994; Hara and Shimbo, 2007; Hara et al., 2009). Our final goal is to improve the accuracy of parsing sentences containing coordination, by taking advantage of the above methodologies. In the same vein, Hanamoto et al. (2012) used dual decomposition to combine an HPSG parser with Hara et al.’s (2009) model. punct Florida NNP conj , , cc conj Illinois NNP and CC Pennsylvania NNP Although not explicitly stated in the Stanford manual, in the converted PTB corpus, the first conjunct 1 The Stanford dependencies manual (de Marneffe and Manning, 2008) merely states that the first conjunct is normally the head, but we take this rule as definitive in this paper. The error analysis of our method based on this assumption is given in Section 5. 66 Proceedings of the 14th International Conference on Parsing Technologies, pages"
W15-2208,D07-1064,1,0.873405,"with labels punct and cc, respectively. Even for state-of-the-art dependency parsers, the recovery of dependencies involving coordinate structures remains difficult. According to Nivre et al. (2010), the accuracy in parsing the construction known as right node raising, which includes a coordinate structure such as “president and chief executive of the company,” is less than fifty percent. Apart from dependency parsers, there are methods specialized for coordination structure analysis, which attempt to identify coordinate structures from the similarity of conjuncts (Kurohashi and Nagao, 1994; Hara and Shimbo, 2007; Hara et al., 2009). Our final goal is to improve the accuracy of parsing sentences containing coordination, by taking advantage of the above methodologies. In the same vein, Hanamoto et al. (2012) used dual decomposition to combine an HPSG parser with Hara et al.’s (2009) model. punct Florida NNP conj , , cc conj Illinois NNP and CC Pennsylvania NNP Although not explicitly stated in the Stanford manual, in the converted PTB corpus, the first conjunct 1 The Stanford dependencies manual (de Marneffe and Manning, 2008) merely states that the first conjunct is normally the head, but we take this"
W15-2208,P09-1109,1,0.894132,"cc, respectively. Even for state-of-the-art dependency parsers, the recovery of dependencies involving coordinate structures remains difficult. According to Nivre et al. (2010), the accuracy in parsing the construction known as right node raising, which includes a coordinate structure such as “president and chief executive of the company,” is less than fifty percent. Apart from dependency parsers, there are methods specialized for coordination structure analysis, which attempt to identify coordinate structures from the similarity of conjuncts (Kurohashi and Nagao, 1994; Hara and Shimbo, 2007; Hara et al., 2009). Our final goal is to improve the accuracy of parsing sentences containing coordination, by taking advantage of the above methodologies. In the same vein, Hanamoto et al. (2012) used dual decomposition to combine an HPSG parser with Hara et al.’s (2009) model. punct Florida NNP conj , , cc conj Illinois NNP and CC Pennsylvania NNP Although not explicitly stated in the Stanford manual, in the converted PTB corpus, the first conjunct 1 The Stanford dependencies manual (de Marneffe and Manning, 2008) merely states that the first conjunct is normally the head, but we take this rule as definitive"
W15-2208,W05-1507,0,0.0817364,"Missing"
W15-2208,P07-1022,0,0.0283994,"n, which group both sides of the head of conjuncts, as in: In these rules, i, j, and k indicate word indices. Symbol ` is also used for an index later. Let w(i) denote the ith word in a sentence. Each word w(i) is initially associated with C OMPLETE L EFT and C OMPLETE R IGHT . A root node i POBJ NUM PREP to split into three sectors and to sell its subsidiary In this section, we briefly review the grammar rules used in the Eisner-Satta algorithm (1999), which our proposed rules are designed to augment. For details about the algorithm, see the original paper, as well as (Koo and Collins, 2010; Johnson, 2007). The Eisner-Satta algorithm builds a parse tree in a bottom-up manner using the following four rules, each of which yields a span of different type shown next to the respective rule. (1) conj PM q q q q q CM q −→ −→ q q q q q q q q O(p) O(p) As shown next to the rules, the time needed to apply them is O(p), where p ( n) is the number of coordinate conjunctions and punctuations in a sentence; i.e., the values q can potentially take on. Parsing rules for coordination We augment Eisner and Satta’s split-head dependency parsing rules with a set of additional rules that are tailored for similarit"
W15-2208,P10-1001,0,0.0161611,"fically for coordination, which group both sides of the head of conjuncts, as in: In these rules, i, j, and k indicate word indices. Symbol ` is also used for an index later. Let w(i) denote the ith word in a sentence. Each word w(i) is initially associated with C OMPLETE L EFT and C OMPLETE R IGHT . A root node i POBJ NUM PREP to split into three sectors and to sell its subsidiary In this section, we briefly review the grammar rules used in the Eisner-Satta algorithm (1999), which our proposed rules are designed to augment. For details about the algorithm, see the original paper, as well as (Koo and Collins, 2010; Johnson, 2007). The Eisner-Satta algorithm builds a parse tree in a bottom-up manner using the following four rules, each of which yields a span of different type shown next to the respective rule. (1) conj PM q q q q q CM q −→ −→ q q q q q q q q O(p) O(p) As shown next to the rules, the time needed to apply them is O(p), where p ( n) is the number of coordinate conjunctions and punctuations in a sentence; i.e., the values q can potentially take on. Parsing rules for coordination We augment Eisner and Satta’s split-head dependency parsing rules with a set of additional rules that are tailor"
W15-2208,J94-4001,0,0.859228,"ents of the first conjunct, with labels punct and cc, respectively. Even for state-of-the-art dependency parsers, the recovery of dependencies involving coordinate structures remains difficult. According to Nivre et al. (2010), the accuracy in parsing the construction known as right node raising, which includes a coordinate structure such as “president and chief executive of the company,” is less than fifty percent. Apart from dependency parsers, there are methods specialized for coordination structure analysis, which attempt to identify coordinate structures from the similarity of conjuncts (Kurohashi and Nagao, 1994; Hara and Shimbo, 2007; Hara et al., 2009). Our final goal is to improve the accuracy of parsing sentences containing coordination, by taking advantage of the above methodologies. In the same vein, Hanamoto et al. (2012) used dual decomposition to combine an HPSG parser with Hara et al.’s (2009) model. punct Florida NNP conj , , cc conj Illinois NNP and CC Pennsylvania NNP Although not explicitly stated in the Stanford manual, in the converted PTB corpus, the first conjunct 1 The Stanford dependencies manual (de Marneffe and Manning, 2008) merely states that the first conjunct is normally the"
W15-2208,J93-2004,0,0.0495728,"Missing"
W15-2213,P14-2129,0,0.0123552,"r which directly decides for each cell in the chart how many constituents should be created. Their parser uses beam search with a FOM and a beam for each chart cell. Like these approaches, our method uses a classifier to avoid doing work in certain chart cells. While not completely orthogonal, we believe our independence constraints are complementary. A single decision by our classifier closes a large swath of cells based on the global structure, while their methods make local decision using local information. The high accuracy of their classifiers shows the necessity of improving our model. (Yarmohammadi et al., 2014) proposes a concept of ‘hedge’ parsing, where only spans below a certain length are allowed, and show how this reduces the computation done by the CKY algorithm. Their system does not create spans of length larger than the threshold and thus doesn’t follow the original treebank annotation, while our approach is able to return the original gold parse tree, provided that the classifier does not output a false positive. Their approach of segmenting a sentence before parsing is essentially the same as ours, but they segment based on a maximum span length and their classifier is based on a finite-s"
W15-2213,P11-1045,0,0.1061,"which can be used to identify boundaries between independent words in a sentence using only surface features, and show that it can be used to speed up a CKY parser. We investigate the trade-off between speed and accuracy, and indicate directions for improvement. 1 NP 0 VP DT VB This 1 is 2 . NP . 5 DT NN an 3 example 4 Figure 1: In this tree ‘This’ and ‘is’ are independent, while ‘is’ and ‘an’ are not. Other techniques can be used to prune cells in the chart. Roark et al. (2012) use a finitestate model to label words that do/don’t begin/end spans, and skip cells that don’t satisfy the labels. Bodenstab et al. (2011) directly apply a classifier to each cell to decide how many spans to keep. Both approaches reduce the work done by the parser while preserving accuracy. We propose a novel type of top-down constraint for a CFG parser that we call independence constraints, described in Section 2. In Section 3 we show how the CKY algorithm can be easily modified to accommodate these constraints, and in Section 4 we describe a classifier which can provide the constraints to a parser. We integrate the constraints into the Stanford Parser CKY implementation and show the results in Section 5. Introduction  The CKY"
W15-2213,P05-1022,0,0.0590587,"and the size of the grammar |G|. Methods for improving parsing accuracy typically increase the size of the grammar (Klein and Manning, 2003; Petrov and Klein, 2007) or even the exponent of n (Eisner and Satta, 1999). More powerful “deep” grammar formalisms multiply the computational complexity even more (Bangalore and Joshi, 1999). A common technique for speeding up such parsers is coarse-to-fine parsing, where input is first parsed using a much simpler (and thus smaller) grammar, and the content of the chart is then used to constrain the search over the final grammar (Torisawa et al., 2000; Charniak and Johnson, 2005; Petrov and Klein, 2007). Even with a much smaller grammar, the CKY algorithm may be expensive—Roark et al. (2012) report that the initial CKY step in the Berkeley Parser takes half of the total parse time. 2 Independence Constraints We propose a concept we call independence. Given a sentence s = w1 w2 . . . wn and a contextfree derivation (parse tree) t of s, words wi and wi+1 are independent if every node in t that dominates both wi and wi+1 also dominates w1 and wn . Furthermore, if wi and wi+1 are independent, then ∀j, k s.t. j ≤ i and k > i, wj and wk are 97 Proceedings of the 14th Inter"
W15-2213,C10-1140,0,0.0369961,"Missing"
W15-2213,P99-1059,0,0.106763,"n Section 4 we describe a classifier which can provide the constraints to a parser. We integrate the constraints into the Stanford Parser CKY implementation and show the results in Section 5. Introduction  The CKY algorithm is an O |G|n3 dynamic programming algorithm for finding all of the possible derivations of a sentence in a context-free language. Its complexity depends on both the sentence length n and the size of the grammar |G|. Methods for improving parsing accuracy typically increase the size of the grammar (Klein and Manning, 2003; Petrov and Klein, 2007) or even the exponent of n (Eisner and Satta, 1999). More powerful “deep” grammar formalisms multiply the computational complexity even more (Bangalore and Joshi, 1999). A common technique for speeding up such parsers is coarse-to-fine parsing, where input is first parsed using a much simpler (and thus smaller) grammar, and the content of the chart is then used to constrain the search over the final grammar (Torisawa et al., 2000; Charniak and Johnson, 2005; Petrov and Klein, 2007). Even with a much smaller grammar, the CKY algorithm may be expensive—Roark et al. (2012) report that the initial CKY step in the Berkeley Parser takes half of the"
W15-2213,I11-1143,0,0.0668509,"Missing"
W15-2213,N07-1051,0,\N,Missing
W15-2213,J99-2004,0,\N,Missing
W15-2213,C02-1054,0,\N,Missing
W15-2213,P03-1054,0,\N,Missing
W15-2213,P06-1022,0,\N,Missing
W15-2213,P03-1004,1,\N,Missing
W15-2213,J12-4002,0,\N,Missing
W15-2519,W11-2022,0,0.11599,"oceedings of the Second Workshop on Discourse in Machine Translation (DiscoMT), pages 142–152, c Lisbon, Portugal, 17 September 2015. 2015 Association for Computational Linguistics. 3.1 2014). On the other hand, it is also reported that certain English explicit DCs are not translated explicitly in French or German (Meyer and Webber, 2013). We hypothesize that explicitation is more common in Chinese-to-English translation. To incorporate DC translation in SMT, explicit DCs are annotated in French-English parallel corpus and classifiers are trained to disambiguate DC senses before SMT training (Meyer et al., 2011; Meyer and Popescu-Belis, 2012). Also, translation model based on Rhetorical Structure Theory (Mann and Thompson, 1986) styled discourse parse has been used in Chinese-English SMT (Tu et al., 2013). These works focus on explicit discourse relations. Chinese sentences can be ‘discourse-like’, consisting of a sequence of discourse units. Syntactic parsing of Chinese complex sentences (CCS) (Zhou, 2004) covers certain intersentential discourse relations, including both explicit and implicit relations. Tu et al. (2014) presents a CCS-tree-to-string translation model in which translation rules and"
W15-2519,P07-2045,0,0.00633869,"translate the implicit DCs. We insert the most frequent fine sense of the annotated coarse sense to the source text4 . Referring to the same example, ‘而且’ (‘and’) is inserted at position [1] because it is the most frequent fine sense under the coarse sense Expansion. MT Settings We train baseline MT systems with 2.5 million sentences of bitexts through the LDC6 , including newswire, broadcast news and law genres. To see if there is any bias of DC translation to certain framework, we build 3 types of SMT systems with default settings: a phrase-based model and a hierarchical model using MOSES (Koehn et al., 2007), and a tree-to-string model using TRAVATAR (Neubig, 2013). All models use a 5gram language model trained on the English Gigaword (Parker et al., 2011) and are tuned by MERT (Och, 2003). We use GIZA++ (Och and Ney, 2003) for automatic word alignment and the Stanford Parser (Levy and Manning, 2003) to parse the source text for tree-to-string MT training. Tuning and testing with the newswire portions of OpenMT08 and OpenMT06 respectively, the phrase-based, Hiero and tree-to-string systems yield BLEU scores of 26.7, 26.1 and 20.4 respectively, evaluating against 4 reference translations. We use t"
W15-2519,2012.amta-papers.20,0,0.0375184,"Missing"
W15-2519,P13-4016,0,0.0135362,"nse of the annotated coarse sense to the source text4 . Referring to the same example, ‘而且’ (‘and’) is inserted at position [1] because it is the most frequent fine sense under the coarse sense Expansion. MT Settings We train baseline MT systems with 2.5 million sentences of bitexts through the LDC6 , including newswire, broadcast news and law genres. To see if there is any bias of DC translation to certain framework, we build 3 types of SMT systems with default settings: a phrase-based model and a hierarchical model using MOSES (Koehn et al., 2007), and a tree-to-string model using TRAVATAR (Neubig, 2013). All models use a 5gram language model trained on the English Gigaword (Parker et al., 2011) and are tuned by MERT (Och, 2003). We use GIZA++ (Och and Ney, 2003) for automatic word alignment and the Stanford Parser (Levy and Manning, 2003) to parse the source text for tree-to-string MT training. Tuning and testing with the newswire portions of OpenMT08 and OpenMT06 respectively, the phrase-based, Hiero and tree-to-string systems yield BLEU scores of 26.7, 26.1 and 20.4 respectively, evaluating against 4 reference translations. We use these SMT models to translate the source text in which impl"
W15-2519,P03-1056,0,0.0196722,"line MT systems with 2.5 million sentences of bitexts through the LDC6 , including newswire, broadcast news and law genres. To see if there is any bias of DC translation to certain framework, we build 3 types of SMT systems with default settings: a phrase-based model and a hierarchical model using MOSES (Koehn et al., 2007), and a tree-to-string model using TRAVATAR (Neubig, 2013). All models use a 5gram language model trained on the English Gigaword (Parker et al., 2011) and are tuned by MERT (Och, 2003). We use GIZA++ (Och and Ney, 2003) for automatic word alignment and the Stanford Parser (Levy and Manning, 2003) to parse the source text for tree-to-string MT training. Tuning and testing with the newswire portions of OpenMT08 and OpenMT06 respectively, the phrase-based, Hiero and tree-to-string systems yield BLEU scores of 26.7, 26.1 and 20.4 respectively, evaluating against 4 reference translations. We use these SMT models to translate the source text in which implicit DCs are explicitated by the methods described in Section 4.1. 1178 sentences and 1175 sentences of the manually annotated parallel corpus are used as the tuning and test sets respectively. The systems are tuned with the tuning set prep"
W15-2519,J03-1002,0,0.00481282,"nt fine sense under the coarse sense Expansion. MT Settings We train baseline MT systems with 2.5 million sentences of bitexts through the LDC6 , including newswire, broadcast news and law genres. To see if there is any bias of DC translation to certain framework, we build 3 types of SMT systems with default settings: a phrase-based model and a hierarchical model using MOSES (Koehn et al., 2007), and a tree-to-string model using TRAVATAR (Neubig, 2013). All models use a 5gram language model trained on the English Gigaword (Parker et al., 2011) and are tuned by MERT (Och, 2003). We use GIZA++ (Och and Ney, 2003) for automatic word alignment and the Stanford Parser (Levy and Manning, 2003) to parse the source text for tree-to-string MT training. Tuning and testing with the newswire portions of OpenMT08 and OpenMT06 respectively, the phrase-based, Hiero and tree-to-string systems yield BLEU scores of 26.7, 26.1 and 20.4 respectively, evaluating against 4 reference translations. We use these SMT models to translate the source text in which implicit DCs are explicitated by the methods described in Section 4.1. 1178 sentences and 1175 sentences of the manually annotated parallel corpus are used as the tun"
W15-2519,P14-2047,0,0.06343,"native lexicalization’ of a discourse relation that cannot be isolated from context as an explicit DC, e.g. ‘it was followed by’ for a Temporal relation. Prepositions that mark discourse To investigate how DCs are translated from Chinese to English, we manually align DCs in the source to their translations on a parallel corpus. The DCs are further annotated with their nature and senses. This section describes the strategy and findings of our annotation. 1 Our annotation is independent of existing monolingual discourse annotation on the Chinese Treebank such as the CDTB(Zhou and Xue, 2015) and Li et al. (2014b) 143 relations are also labeled ‘AltLex’, such as ‘through’ for a Contingency relation. This label is defined on English side only. Each pair of aligned DCs are thus tagged with 8 labels. Some annotation examples are shown below.   Example 1 • Coarse sense: We first group the DCs under the 4 top-level discourse senses defined in PDTB, namely Expansion, Contingency, Comparison and Temporal. 中国必须对国有企业进行改革, [1]加强本身的竞争 力。 China must implement reforms on state-owned enterprises so as to [1] improve its own competitiveness. . [1]nature: actual DC: fine sense: coarse sense: • Fine sense: The sens"
W15-2519,P03-1021,0,0.00958229,"cause it is the most frequent fine sense under the coarse sense Expansion. MT Settings We train baseline MT systems with 2.5 million sentences of bitexts through the LDC6 , including newswire, broadcast news and law genres. To see if there is any bias of DC translation to certain framework, we build 3 types of SMT systems with default settings: a phrase-based model and a hierarchical model using MOSES (Koehn et al., 2007), and a tree-to-string model using TRAVATAR (Neubig, 2013). All models use a 5gram language model trained on the English Gigaword (Parker et al., 2011) and are tuned by MERT (Och, 2003). We use GIZA++ (Och and Ney, 2003) for automatic word alignment and the Stanford Parser (Levy and Manning, 2003) to parse the source text for tree-to-string MT training. Tuning and testing with the newswire portions of OpenMT08 and OpenMT06 respectively, the phrase-based, Hiero and tree-to-string systems yield BLEU scores of 26.7, 26.1 and 20.4 respectively, evaluating against 4 reference translations. We use these SMT models to translate the source text in which implicit DCs are explicitated by the methods described in Section 4.1. 1178 sentences and 1175 sentences of the manually annotated"
W15-2519,C14-1055,0,0.428382,"native lexicalization’ of a discourse relation that cannot be isolated from context as an explicit DC, e.g. ‘it was followed by’ for a Temporal relation. Prepositions that mark discourse To investigate how DCs are translated from Chinese to English, we manually align DCs in the source to their translations on a parallel corpus. The DCs are further annotated with their nature and senses. This section describes the strategy and findings of our annotation. 1 Our annotation is independent of existing monolingual discourse annotation on the Chinese Treebank such as the CDTB(Zhou and Xue, 2015) and Li et al. (2014b) 143 relations are also labeled ‘AltLex’, such as ‘through’ for a Contingency relation. This label is defined on English side only. Each pair of aligned DCs are thus tagged with 8 labels. Some annotation examples are shown below.   Example 1 • Coarse sense: We first group the DCs under the 4 top-level discourse senses defined in PDTB, namely Expansion, Contingency, Comparison and Temporal. 中国必须对国有企业进行改革, [1]加强本身的竞争 力。 China must implement reforms on state-owned enterprises so as to [1] improve its own competitiveness. . [1]nature: actual DC: fine sense: coarse sense: • Fine sense: The sens"
W15-2519,D14-1224,0,0.0530394,"native lexicalization’ of a discourse relation that cannot be isolated from context as an explicit DC, e.g. ‘it was followed by’ for a Temporal relation. Prepositions that mark discourse To investigate how DCs are translated from Chinese to English, we manually align DCs in the source to their translations on a parallel corpus. The DCs are further annotated with their nature and senses. This section describes the strategy and findings of our annotation. 1 Our annotation is independent of existing monolingual discourse annotation on the Chinese Treebank such as the CDTB(Zhou and Xue, 2015) and Li et al. (2014b) 143 relations are also labeled ‘AltLex’, such as ‘through’ for a Contingency relation. This label is defined on English side only. Each pair of aligned DCs are thus tagged with 8 labels. Some annotation examples are shown below.   Example 1 • Coarse sense: We first group the DCs under the 4 top-level discourse senses defined in PDTB, namely Expansion, Contingency, Comparison and Temporal. 中国必须对国有企业进行改革, [1]加强本身的竞争 力。 China must implement reforms on state-owned enterprises so as to [1] improve its own competitiveness. . [1]nature: actual DC: fine sense: coarse sense: • Fine sense: The sens"
W15-2519,W12-1614,0,0.0922438,"Missing"
W15-2519,D09-1036,0,0.162578,"Missing"
W15-2519,D13-1094,0,0.195558,"Missing"
W15-2519,P09-1077,0,0.18546,"Missing"
W15-2519,P02-1047,0,0.119414,"Missing"
W15-2519,W12-0117,0,0.20856,"ond Workshop on Discourse in Machine Translation (DiscoMT), pages 142–152, c Lisbon, Portugal, 17 September 2015. 2015 Association for Computational Linguistics. 3.1 2014). On the other hand, it is also reported that certain English explicit DCs are not translated explicitly in French or German (Meyer and Webber, 2013). We hypothesize that explicitation is more common in Chinese-to-English translation. To incorporate DC translation in SMT, explicit DCs are annotated in French-English parallel corpus and classifiers are trained to disambiguate DC senses before SMT training (Meyer et al., 2011; Meyer and Popescu-Belis, 2012). Also, translation model based on Rhetorical Structure Theory (Mann and Thompson, 1986) styled discourse parse has been used in Chinese-English SMT (Tu et al., 2013). These works focus on explicit discourse relations. Chinese sentences can be ‘discourse-like’, consisting of a sequence of discourse units. Syntactic parsing of Chinese complex sentences (CCS) (Zhou, 2004) covers certain intersentential discourse relations, including both explicit and implicit relations. Tu et al. (2014) presents a CCS-tree-to-string translation model in which translation rules and language model are conditioned"
W15-2519,prasad-etal-2008-penn,0,0.0309732,"Missing"
W15-2519,W13-3303,0,0.143615,"course connectives’ (DCs) or implicitly inferred. The markedness of discourse relations varies across languages. For 2 Related Work In translation studies, explicitation of implicit DCs is observed in translations between European languages (Becher, 2011; Zuffery and Cartoni, 142 Proceedings of the Second Workshop on Discourse in Machine Translation (DiscoMT), pages 142–152, c Lisbon, Portugal, 17 September 2015. 2015 Association for Computational Linguistics. 3.1 2014). On the other hand, it is also reported that certain English explicit DCs are not translated explicitly in French or German (Meyer and Webber, 2013). We hypothesize that explicitation is more common in Chinese-to-English translation. To incorporate DC translation in SMT, explicit DCs are annotated in French-English parallel corpus and classifiers are trained to disambiguate DC senses before SMT training (Meyer et al., 2011; Meyer and Popescu-Belis, 2012). Also, translation model based on Rhetorical Structure Theory (Mann and Thompson, 1986) styled discourse parse has been used in Chinese-English SMT (Tu et al., 2013). These works focus on explicit discourse relations. Chinese sentences can be ‘discourse-like’, consisting of a sequence of"
W15-2519,J14-4007,0,0.0436237,"label is defined on English side only. Each pair of aligned DCs are thus tagged with 8 labels. Some annotation examples are shown below.   Example 1 • Coarse sense: We first group the DCs under the 4 top-level discourse senses defined in PDTB, namely Expansion, Contingency, Comparison and Temporal. 中国必须对国有企业进行改革, [1]加强本身的竞争 力。 China must implement reforms on state-owned enterprises so as to [1] improve its own competitiveness. . [1]nature: actual DC: fine sense: coarse sense: • Fine sense: The sense hierarchy of PDTB is always modified in comparable discourse corpora of different languages (Prasad et al., 2014). Instead of defining a list of senses that cover discourse relations of both languages, we group interchangeable explicit DCs under the same category, and the category serves as the ‘fine sense’ label. For example, ‘besides’ ,‘moreover’ and ‘in addition’ are all annotated with the fine sense ‘in addition’. Similar to DC identification, the list of fine senses is built in the course of annotation. In total, there are 74 Chinese and 75 English fine senses (See Table 2). Chinese implicit nil 来 Contingency English explicit so as to in order to Contingency   Example 2   [1] 在投资项目上比上年减少四百四十四件,但"
W15-2519,E14-1068,0,0.0350057,"the English translation. To correctly model the translation of implicit relations, do we need a discourse parser that classifies an implicit source DC to its fine sense or coarse sense? Or will SMT robustly handle implicit-toexplicit DC translation without any discourse preprocessing? We seek to answer these questions in the next section. 4 Total 227(74) 156(61) Total −(54) −(39) With an automatic discoure parser, a discoursetree-to-string translation model can be built. Nonetheless, state-of-the-art accuracy of implicit discourse sense classification is still low for downstream application (Rutherford and Xue, 2014). In this work, we design oracle experiments to evaluate the MT of implicit DCs assuming that the gold discourse sense is given. 4.1 Table 2: Number of unique DCs and DC fine senses (in brackets)3 Explicitating implicit DCs for MT based on manual annotation Method In our annotation scheme, implicit DCs senses are defined by DCs that are identified during explicit DC annotation. In other words, the implicit DCs are represented by explicit DC that acturally occur in Chinese discourse. We hypothize that explicitating implicit DCs in the source based on manual annotation will improve implicit-to-e"
W15-2519,N15-1081,0,0.0114875,"ly In addition, we compare the contexts in which added DC to the reference translation (Li et al., a source implicit DC is translated into an explicit 2014a). Therefore, to improve implicit-to-explicit DC or by other means (by implicit DC or alternaDC translation, an additional task should be detive lexicalization). If the contexts are similar, it fined to identify whether a source implicit DC is suggests that the translation strategy could be an kept implicit, explicitly translated to an ambigous option independent of the context. DC such as ‘and’, or explicitly translated to other Following Rutherford and Xue (2015), we deunambiguous DCs. fine the context of a discourse relation as the uniGenerally, it is pragmatically correct to use gram distribution of words in the 2 arguments con‘and’ to translate an implicit discourse relation, nected by the relation. The context of a particular or to keep the relation implicit as in the source. discourse usage is thus the sum of the unigram disNonetheless, repetatively using this stragegy will tributions of all discourse relations associated with result in excessively long sentences, as in the exthat usage. We also use the Jensen-Shannon Diverample below. In this ca"
W15-2519,P13-2066,0,0.13941,"other hand, it is also reported that certain English explicit DCs are not translated explicitly in French or German (Meyer and Webber, 2013). We hypothesize that explicitation is more common in Chinese-to-English translation. To incorporate DC translation in SMT, explicit DCs are annotated in French-English parallel corpus and classifiers are trained to disambiguate DC senses before SMT training (Meyer et al., 2011; Meyer and Popescu-Belis, 2012). Also, translation model based on Rhetorical Structure Theory (Mann and Thompson, 1986) styled discourse parse has been used in Chinese-English SMT (Tu et al., 2013). These works focus on explicit discourse relations. Chinese sentences can be ‘discourse-like’, consisting of a sequence of discourse units. Syntactic parsing of Chinese complex sentences (CCS) (Zhou, 2004) covers certain intersentential discourse relations, including both explicit and implicit relations. Tu et al. (2014) presents a CCS-tree-to-string translation model in which translation rules and language model are conditioned by automatic CCS parse. Improved BLEU scores are reported, but it is not clear how much the translation of implicit DCs has been improved. The parallel corpus comes f"
W15-2519,P14-1080,0,0.11848,"s and classifiers are trained to disambiguate DC senses before SMT training (Meyer et al., 2011; Meyer and Popescu-Belis, 2012). Also, translation model based on Rhetorical Structure Theory (Mann and Thompson, 1986) styled discourse parse has been used in Chinese-English SMT (Tu et al., 2013). These works focus on explicit discourse relations. Chinese sentences can be ‘discourse-like’, consisting of a sequence of discourse units. Syntactic parsing of Chinese complex sentences (CCS) (Zhou, 2004) covers certain intersentential discourse relations, including both explicit and implicit relations. Tu et al. (2014) presents a CCS-tree-to-string translation model in which translation rules and language model are conditioned by automatic CCS parse. Improved BLEU scores are reported, but it is not clear how much the translation of implicit DCs has been improved. The parallel corpus comes from 325 newswire articles (2353 sentences) of the the Chinese Treebank and their English translation (Palmer et al., 2005; Bies et al., 2007)1 . The annotation was carried out by 1 professional Chinese-English translator. We use translation spotting technique (Meyer et al., 2011) to align the DCs crosslingually, consideri"
W15-2519,W15-3101,1,0.506415,"ditioned by automatic CCS parse. Improved BLEU scores are reported, but it is not clear how much the translation of implicit DCs has been improved. The parallel corpus comes from 325 newswire articles (2353 sentences) of the the Chinese Treebank and their English translation (Palmer et al., 2005; Bies et al., 2007)1 . The annotation was carried out by 1 professional Chinese-English translator. We use translation spotting technique (Meyer et al., 2011) to align the DCs crosslingually, considering both explicit and implicit DCs. Annotation is carried out on the raw texts. Readers are refered to Yung et al. (2015) for details concerning the Chinese side annotation, such as definition of discourse units and annotation policy for parallel connectives. The labels used in the crosslingual annotation are defined as follows: • Explicit DC: An explicit DC is a lexical expression that connects two discourse units with a relation. We do not define a close set of explicit DCs to be annotated. The list is constructed in the course of annotation. We also do not limit the syntactic categories of the DCs. In total, 227 Chinese and 152 English DCs are identified. (See Table 2) • Implicit DC: An implicit DC is an impl"
W15-3101,D14-1224,0,0.414648,"Missing"
W15-3101,P11-1100,0,0.0557413,"Missing"
W15-3101,W01-1605,0,0.139506,"a discourse argument. Above the clause level, Chinese sentences (marked by ‘。’) are also units of discourse (Chu, 1998). When presented with texts where periods and commas are removed, native Chinese speakers disagree with where to restore them (Bittner, 2013). The actual sentence segmentation of the text thus represents the spans of discourse arguments intended by the writer and should be taken into account. Secondly, it is well known that syntactical structure is presented by word order in Chinese - so is Related Work Major discourse annotated resources in English include the RST Treebank (Carlson et al., 2001) and the Penn Discourse Treebank (PDTB) (Prasad et al., 2008). The RST Treebank represents discourse relations in a tree structure, where a satellite text span is related to a nucleus text span. 1 Proceedings of the Eighth SIGHAN Workshop on Chinese Language Processing (SIGHAN-8), pages 1–6, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing discourse. While the Arg1 can occur before or after Arg2 in English, arguments predominantly occur in fixed order in Chinese, depending on the logical relation. For example"
W15-3101,P09-1077,0,0.0852398,"rguments of each relation. We evaluate the accuracy of each component and the overall accuracy of the final output, classifying up to the 4 main senses. The pipeline consists of 5 classifiers, as shown in Figure 1, each of which is trained with the relevant samples, e.g. only arguments annotated with explicit DCs are used to train the explicit DC classifier. 289 and 36 articles are used as training and testing data respectively. Features include lexical and syntactical features (bag of words, bag of POS, word pairs and production rules) that have been used in classifying implicit English DCs (Pitler et al., 2009; Lin et al., 2010), and probability distribution of senses for explicit DC classification. The extraction of features is based on automatic parsing by the Stanford Parser (Levy and Manning, 2003). We also use the surrounding discourse relations as features, hypothesizing that certain relation sequences are more likely than others. The classifiers are trained by SVM with a linear kernel using the LIBSVM package(Chang and Lin, 2011). • Cause-result v.s. result-cause order: 因为...所以... (yinwei...suoyi..., because... therefore...) and 之 所 以...是 因 为... (zhisuoyi...shiyinwei..., the reason why...is"
W15-3101,prasad-etal-2008-penn,0,0.103922,"es (marked by ‘。’) are also units of discourse (Chu, 1998). When presented with texts where periods and commas are removed, native Chinese speakers disagree with where to restore them (Bittner, 2013). The actual sentence segmentation of the text thus represents the spans of discourse arguments intended by the writer and should be taken into account. Secondly, it is well known that syntactical structure is presented by word order in Chinese - so is Related Work Major discourse annotated resources in English include the RST Treebank (Carlson et al., 2001) and the Penn Discourse Treebank (PDTB) (Prasad et al., 2008). The RST Treebank represents discourse relations in a tree structure, where a satellite text span is related to a nucleus text span. 1 Proceedings of the Eighth SIGHAN Workshop on Chinese Language Processing (SIGHAN-8), pages 1–6, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing discourse. While the Arg1 can occur before or after Arg2 in English, arguments predominantly occur in fixed order in Chinese, depending on the logical relation. For example, the same concession relation can be expressed by both const"
W15-3101,N13-1100,0,0.0413613,"Missing"
W15-3101,D14-1196,0,0.0528809,"Missing"
W15-3101,C12-2138,0,0.0449565,"Missing"
W15-3101,zhou-etal-2014-cuhk,0,0.0179967,"aul, 1996; Chu and Ji, 1999). The clauses are semantically arranged in a topic-comment sequence following the writer’s conceptual mind (Tai, 1985; Bittner, 2013). When the arguments are not arranged in the standard order, the sense of the DC is altered. For example, when ‘虽 然’ (suiran, although’ is used in construction (2), it represents an ‘expansion’ relation (Huang et al., 2014). Therefore, discourse relations should be defined given the order of the arguments. Lastly, parallel DCs are frequent in Chinese discourse, yet usually either one DC of the pair occurs to signify the same relation (Zhou et al., 2014). For example, (3) and (4) are grammatical alternatives to (1). Arguments Each clause separated by punctuations except quotation marks is treated as a candidate argument. Clauses that do not function as discourse units are classified into 3 types - attribution, optional punctuation and non-discourse adverbial. The main difference of our annotation scheme is that the the order of the arguments for each DC is defined by default. Since the arguments of a particular discourse relation occur in fixed order and are always adjacent, each argument is related to the immediately preceding argument by a"
W15-3101,W12-1636,0,\N,Missing
W15-3101,P03-1056,0,\N,Missing
W15-3101,I11-1170,0,\N,Missing
W15-3101,C14-1060,0,\N,Missing
W15-4404,D11-1010,0,0.167268,"ordancers seem to be an alternative to search engines, but they retrieve too much information because they usually allow only single-word queries. Too much information might distract and confuse the user (Chen et al., 2014). Thus, a computer program that automatically identifies potential collocation errors and suggests corrections would be a more appropriate resource for second language learners. A few researchers have proposed useful English corpus-based tools for correcting collocation errors (Futagi et al., 2008; Liu et al., 2009; Park et al., 2008; Chang et al., 2008; Wible et al., 2003; Dahlmeier and Ng, 2011). In a user study, Park et al. (2008) observed positive reactions from users when using their system. In another study, Liou et al. (2006) showed that the miscollocation aid proposed by Chang et al. (2008) can help learners improve their knowledge in collocations. One limitation is that these proposed tools rely on resources of limited coverage, such as dictionaries, thesauri, or manually constructed databases to generate the candidates. Another drawback is that most of these systems rely solely on well-formed English resources (except Wible et al., 2003) and do not actually take into account"
W15-4404,I08-7018,0,0.00896209,"ences geared towards foreign language learners. We used the Japanese-English sentences available in the website. 2) Hiragana Times (HT) Corpus3 , a Japanese-English bilingual corpus of magazine articles of Hiragana Times, a bilingual magazine written in Japanese and English to introduce Japan to non-Japanese, covering a wide range of topics (culture, society, history, politics, etc.). 3) Kyoto Wikipedia (KW) Corpus4 , a corpus created by manually translating Japanese Wikipedia articles (related to Kyoto) into English. Monolingual resource: the Balanced Corpus of Contemporary Written Japanese (Maekawa, 2008) was used for the noun-verb expressions where no bilingual examples were available. Tatoeba HT KW BCCWJ # jp sentences 203,191 117,492 329,169 871,184 4.2 A collocation test was designed to examine whether or not the tool could help JSL learners find proper Japanese collocations. This included 12 Japanese sentences from the Lang-8 learner corpus and from another small annotated Japanese learner corpus, NAIST Goyo Corpus (Oyama, Komachi and Matsumoto, 2013). The sentences and their corrections were further validated by a professional Japanese teacher. Each sentence contained one noun-verb collo"
W15-4404,Y13-1014,1,0.899357,"Missing"
W15-4404,P13-3008,1,0.926222,"nce Nara Institute of Science and Technology {lis-k, matsu}@is.naist.jp Abstract In this paper, we describe Collocation Assistant, a web-based and corpus-based collocational aid, aiming at helping JSL learners expand their collocational knowledge. Focusing on noun-verb constructions, Collocation Assistant flags possible collocation errors and suggests a ranked list of more conventional expressions. Each suggestion is supported with evidence from authentic texts, showing several usage examples of the expression in context to help learners choose the best candidate. Based on our previous study (Pereira et al., 2013), the system generates corrections to the learners’ collocation error tendencies by using noun and verb corrections extracted from a large annotated Japanese learner corpus. For ranking the collocation correction candidates, it uses the Weighted Dice coefficient (Kitamura and Matsumoto, 1997). We add to our previous work by implementing an interface that allows end-users to identify and correct their own collocation errors. In addition, we conducted a preliminary evaluation with JSL learners to gather their feedback on using the tool. We present Collocation Assistant, a prototype of a collocat"
W15-4404,I08-2082,0,0.0130445,"ge processing (Leacock et al., 2014). 2 The need for collocational aids Existing linguistic tools are often of limited utility in assisting second language learners with collocations. Most spell checkers and grammar checkers can help correct errors made by native speakers, but offer no assistance for non-native errors. Futagi et al. (2008) note that common aids for second language learners namely, dictionaries and thesauri are often of limited value when the learner does not know the appropriate collocation and must sort through a list of synonyms to find one that is contextually appropriate. Yi et al. (2008) observe that language learners often use search engines to check if a phrase is commonly used by observing the number of results returned. However, search engines are not designed to offer alternative phrases that are more commonly used than the 20 Proceedings of The 2nd Workshop on Natural Language Processing Techniques for Educational Applications, pages 20–25, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing learner’s phrase (Park et al., 2008). Concordancers seem to be an alternative to search engines, but"
W15-4412,I11-1130,0,0.0742013,"Missing"
W15-4412,P11-1093,0,0.0199895,"on assistance of second language learning also has received much attention, especially on grammatical error correction of essays written by learners of English as a second language (ESL) . In the past, three competitions for grammatical error correction have been held: Helping Our Own (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL Shared Task (Ng et al., 2013; Ng et al., 2014). 2 Related work Research on grammatical error correction has recently become very popular. Grammatical error correction methods are roughly divided into two types; (1) targeting few restricted types of errors (Rozovskaya and Roth, 2011; Rozovskaya and Roth, 2013; Tajiri et al., 2012) and (2) targeting 1 http://lang-8.com 2 http://www.gingersoftware.com 82 Proceedings of The 2nd Workshop on Natural Language Processing Techniques for Educational Applications, pages 82–86, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing any types of errors (Mizumoto et al., 2012). In the first type of error correction, classifiers like Support Vector Machines have mainly been used. In the second type, statistical machine translation methods have been used. The"
W15-4412,N10-1029,0,0.167213,"rrors. ESL learners make various kinds of grammatical errors (Mizumoto et al., 2012). For dealing with any types of errors, grammatical error correction methods using phrase-based statistical machine translation (SMT) are proposed (Brockett et al., 2006; Mizumoto et al., 2012). Phrase-based SMT carries out translation with phrases which are a sequence of words as translation units. However, since phrases are extracted in an unsupervised manner, an MWE like “a lot of” may not be treated as one phrase. In machine translation fields, phrase-based SMT considering MWEs achieved higher performance (Carpuat and Diab, 2010; Ren et al., 2009). In this paper, we propose a grammatical error correction method considering MWEs. To be precise, we apply machine translation methods considering MWEs (Carpuat and Diab, 2010) to grammatical error correction. They turn MWEs into single units in the source side sentences (English). Unlike typical machine translation that translates between two languages, in the grammatical error correction task, source side sentences contain errors. Thus, we propose two methods; one is that MWEs are treated as one word in both source and target side sentences, the other is that MWEs are tre"
W15-4412,D13-1074,0,0.0130195,"guage learning also has received much attention, especially on grammatical error correction of essays written by learners of English as a second language (ESL) . In the past, three competitions for grammatical error correction have been held: Helping Our Own (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL Shared Task (Ng et al., 2013; Ng et al., 2014). 2 Related work Research on grammatical error correction has recently become very popular. Grammatical error correction methods are roughly divided into two types; (1) targeting few restricted types of errors (Rozovskaya and Roth, 2011; Rozovskaya and Roth, 2013; Tajiri et al., 2012) and (2) targeting 1 http://lang-8.com 2 http://www.gingersoftware.com 82 Proceedings of The 2nd Workshop on Natural Language Processing Techniques for Educational Applications, pages 82–86, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing any types of errors (Mizumoto et al., 2012). In the first type of error correction, classifiers like Support Vector Machines have mainly been used. In the second type, statistical machine translation methods have been used. The only features for grammatic"
W15-4412,W12-2006,0,0.0563147,"on grammatical error correction. 1 Introduction Publicly usable services on the Web for assisting second language learning are growing recently. For example, there are language learning social networking services such as Lang-81 and English grammar checkers such as Ginger2 . Research on assistance of second language learning also has received much attention, especially on grammatical error correction of essays written by learners of English as a second language (ESL) . In the past, three competitions for grammatical error correction have been held: Helping Our Own (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL Shared Task (Ng et al., 2013; Ng et al., 2014). 2 Related work Research on grammatical error correction has recently become very popular. Grammatical error correction methods are roughly divided into two types; (1) targeting few restricted types of errors (Rozovskaya and Roth, 2011; Rozovskaya and Roth, 2013; Tajiri et al., 2012) and (2) targeting 1 http://lang-8.com 2 http://www.gingersoftware.com 82 Proceedings of The 2nd Workshop on Natural Language Processing Techniques for Educational Applications, pages 82–86, c Beijing, China, July 31, 2015. 2015 Association for Computational"
W15-4412,I11-1017,1,0.889886,"d expgram 0.2.07 . We used ZMERT8 as the parameter tuning tool. For automatic identifying MWEs, we use AMALGr 1.09 (Schneider et al., 2014). The MWE identification tool is re-trained using the MWE data set tagged by (Shigeto et al., 2013) on the Penn Treebank sections of OntoNotes Release 4.0. This is because their annotation was more convenient for our purpose. The translation model was trained on the Lang-8 Learner Corpora v2.0. We extracted English essays which were written by ESL learners whose native language is Japanese from the corpora and cleaned the noise with the method proposed in (Mizumoto et al., 2011). As the results, we got 629,787 sentence pairs. We used a 5-gram 4.2 Error correction methods considering multi-word expressions We propose two methods for grammatical error correction considering MWEs. Previous research of machine translation using MWEs (Carpuat and Diab, 2010) handled MWEs in source side sentences by simply turning MWEs into single units (by conjoining the constituent words with underscores). We essentially apply their method to grammatical error correction; however, in our case identifying MWEs might fail because source side sentences contain grammatical errors. Therefore,"
W15-4412,P12-2039,1,0.841338,"eived much attention, especially on grammatical error correction of essays written by learners of English as a second language (ESL) . In the past, three competitions for grammatical error correction have been held: Helping Our Own (Dale and Kilgarriff, 2011; Dale et al., 2012) and CoNLL Shared Task (Ng et al., 2013; Ng et al., 2014). 2 Related work Research on grammatical error correction has recently become very popular. Grammatical error correction methods are roughly divided into two types; (1) targeting few restricted types of errors (Rozovskaya and Roth, 2011; Rozovskaya and Roth, 2013; Tajiri et al., 2012) and (2) targeting 1 http://lang-8.com 2 http://www.gingersoftware.com 82 Proceedings of The 2nd Workshop on Natural Language Processing Techniques for Educational Applications, pages 82–86, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing any types of errors (Mizumoto et al., 2012). In the first type of error correction, classifiers like Support Vector Machines have mainly been used. In the second type, statistical machine translation methods have been used. The only features for grammatical error correction th"
W15-4412,C12-2084,1,0.911516,"onsidering Multi-word Expressions Tomoya Mizumoto Masato Mita Yuji Matsumoto Nara Institute of Science and Technology 8916-5 Takayama, Ikoma, Nara 630-0192, Japan {tomoya-m, mita.masato.mz2, matsu}@is.naist.jp Abstract Most previous research on ESL learners’ grammatical error correction is targeted on one or few restricted types of learners’ errors. ESL learners make various kinds of grammatical errors (Mizumoto et al., 2012). For dealing with any types of errors, grammatical error correction methods using phrase-based statistical machine translation (SMT) are proposed (Brockett et al., 2006; Mizumoto et al., 2012). Phrase-based SMT carries out translation with phrases which are a sequence of words as translation units. However, since phrases are extracted in an unsupervised manner, an MWE like “a lot of” may not be treated as one phrase. In machine translation fields, phrase-based SMT considering MWEs achieved higher performance (Carpuat and Diab, 2010; Ren et al., 2009). In this paper, we propose a grammatical error correction method considering MWEs. To be precise, we apply machine translation methods considering MWEs (Carpuat and Diab, 2010) to grammatical error correction. They turn MWEs into singl"
W15-4412,P11-1121,0,0.0166266,"ow an example in the following: 6 http://www2.nict.go.jp/univ-com/ multi_trans/cicada/ 7 http://www2.nict.go.jp/univ-com/ multi_trans/expgram/ 8 http://cs.jhu.edu/ ozaidan/zmert/ ˜ 9 https://github.com/nschneid/ pysupersensetagger Source: I have a lot of pen. Target: I have a lot of pens. 84 Table 3: Examples of system outputs Learner Baseline with MWE Last month, she gave me a lot of rice and onion. Last month, she gave me a lot of rice and onion. Last month, she gave me a lot of rice and onions. language model built on corrected sentences of the learner corpora. Konan-JIEM Learner Corpus10 (Nagata et al., 2011) are used for evaluation and development data. We use 2,411 sentences for evaluation, and 300 sentences for development. because the system was able to consider the MWE “a lot of”. The second reason is that the probabilities of translation model and language model are improved by handling MWEs as single units. Let us consider the two sentences, “There are a lot of pens” and “There is a pen.” as examples of language model. Without considering MWEs, the word 3-grams, “There are a” and “There is a”, have high probability. With considering MWEs, however, the former trigram becomes to “There are a"
W15-4412,C12-1127,0,0.0341124,"Missing"
W15-4412,W09-2907,0,0.0575668,"Missing"
W15-4412,W14-1701,0,\N,Missing
W15-4412,P06-1032,0,\N,Missing
W15-4412,Q14-1016,0,\N,Missing
W15-4412,W13-1021,1,\N,Missing
W15-4412,W11-2838,0,\N,Missing
W16-3813,P14-1070,0,0.0630653,"Missing"
W16-3813,P16-1016,0,0.0195966,"Missing"
W16-3813,W08-1301,0,0.142876,"Missing"
W16-3813,L16-1263,1,0.877702,"Missing"
W16-3813,Y15-2015,1,0.927435,"ithin them. MWEs have specific grammatical functionalities and can be regarded as an important part of an extended lexicon. The objective of our work is to construct a wide coverage English syntactically flexible MWE lexicon, to describe their structures in dependency structures with possible modifiers within them, and to annotate their occurrences in the Wall Street Journal portion of OntoNotes corpus (Pradhan et al., 2007). There have been some attempts for constructing English MWE lexicon. An English fixed MWE lexicon and a list of phrasal verbs are presented in (Shigeto et al., 2015) and (Komai et al., 2015). They This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/ 102 Proceedings of the Workshop on Grammar and Lexicon: Interactions and Interfaces, pages 102–109, Osaka, Japan, December 11 2016. Licence details: also annotated the occurrences of those expressions in Penn Treebank. While most of English dictionaries for human use include a large list of multi-word expressions and idioms, to the best of our knowledge, there has been no comprehensive lexicon of English flexible MWEs constructed that is usable for NLP tasks."
W16-3813,L16-1368,0,0.0519487,"Missing"
W16-3813,schneider-etal-2014-comprehensive,0,0.0559871,"Missing"
W16-3813,silveira-etal-2014-gold,0,0.0671405,"Missing"
W16-3918,D15-1166,0,0.452325,"In this work, we adopt the character-level encoder-decoder model as a method of Japanese text normalization. Since the encoder-decoder model was proposed in the field of machine translation, it has This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 1 http://taku910.github.io/mecab/ 129 Proceedings of the 2nd Workshop on Noisy User-generated Text, pages 129–137, Osaka, Japan, December 11 2016. License details: http:// achieved good results in morphological inflection generation and error correction (Bahdanau et al., 2015; Luong et al., 2015; Sutskever et al., 2015; Cho et al., 2014; Manaal et al., 2016; Barret et al., 2016). As these models are capable of capturing sequential patterns, it is possible to apply the encoder-decoder model to Japanese text normalization. As mentioned previously, Japanese has no explicit boundaries between words. Thus, Japanese text normalization is naturally posed to a character-level sequence to sequence learning. Although the encoder-decoder model has been shown its effectiveness in large datasets, it is much less effective for small datasets such as low-resource languages (Barret et al., 2016). Un"
W16-3918,D14-1011,0,0.241885,"4 describes how to construct a synthesized corpus. Section 5 discusses experiments that we have performed and our corresponding analyses of the experimental results. Section 6 concludes the paper with a brief summary and a mention of future work. 2 Related Work In this section, we mainly describe text normalization for Japanese. Furthermore, we explain some related works with regard to the encoder-decoder model in the field of machine translation. 2.1 Text Normalization Several studies for joint modeling of morphological analysis and text normalization have been conducted (Saito et al., 2014; Kaji and Kitsuregawa, 2014; Sasano et al., 2013). Saito et al. (2014) extracted nonstandard tokens from Twitter and blog texts, and manually annotated their standard forms. They automatically generated derivational patterns based on the character-level alignment between non-standard tokens and their standard forms, then included these patterns to a morphological lattice. Kaji et al. (2014) similarly proposed a joint modeling for morphological analysis and text normalization via their construction of a normalization dictionary using handcrafted rules that were similar to those used in (Sasano et al., 2013) to derive non"
W16-3918,C14-1167,0,0.0686058,"s research. Section 4 describes how to construct a synthesized corpus. Section 5 discusses experiments that we have performed and our corresponding analyses of the experimental results. Section 6 concludes the paper with a brief summary and a mention of future work. 2 Related Work In this section, we mainly describe text normalization for Japanese. Furthermore, we explain some related works with regard to the encoder-decoder model in the field of machine translation. 2.1 Text Normalization Several studies for joint modeling of morphological analysis and text normalization have been conducted (Saito et al., 2014; Kaji and Kitsuregawa, 2014; Sasano et al., 2013). Saito et al. (2014) extracted nonstandard tokens from Twitter and blog texts, and manually annotated their standard forms. They automatically generated derivational patterns based on the character-level alignment between non-standard tokens and their standard forms, then included these patterns to a morphological lattice. Kaji et al. (2014) similarly proposed a joint modeling for morphological analysis and text normalization via their construction of a normalization dictionary using handcrafted rules that were similar to those used in (Sasano"
W16-3918,I13-1019,0,0.48894,"a synthesized corpus. Section 5 discusses experiments that we have performed and our corresponding analyses of the experimental results. Section 6 concludes the paper with a brief summary and a mention of future work. 2 Related Work In this section, we mainly describe text normalization for Japanese. Furthermore, we explain some related works with regard to the encoder-decoder model in the field of machine translation. 2.1 Text Normalization Several studies for joint modeling of morphological analysis and text normalization have been conducted (Saito et al., 2014; Kaji and Kitsuregawa, 2014; Sasano et al., 2013). Saito et al. (2014) extracted nonstandard tokens from Twitter and blog texts, and manually annotated their standard forms. They automatically generated derivational patterns based on the character-level alignment between non-standard tokens and their standard forms, then included these patterns to a morphological lattice. Kaji et al. (2014) similarly proposed a joint modeling for morphological analysis and text normalization via their construction of a normalization dictionary using handcrafted rules that were similar to those used in (Sasano et al., 2013) to derive non-standard forms from t"
W16-3918,N16-1077,0,0.0623367,"del as a method of Japanese text normalization. Since the encoder-decoder model was proposed in the field of machine translation, it has This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 1 http://taku910.github.io/mecab/ 129 Proceedings of the 2nd Workshop on Noisy User-generated Text, pages 129–137, Osaka, Japan, December 11 2016. License details: http:// achieved good results in morphological inflection generation and error correction (Bahdanau et al., 2015; Luong et al., 2015; Sutskever et al., 2015; Cho et al., 2014; Manaal et al., 2016; Barret et al., 2016). As these models are capable of capturing sequential patterns, it is possible to apply the encoder-decoder model to Japanese text normalization. As mentioned previously, Japanese has no explicit boundaries between words. Thus, Japanese text normalization is naturally posed to a character-level sequence to sequence learning. Although the encoder-decoder model has been shown its effectiveness in large datasets, it is much less effective for small datasets such as low-resource languages (Barret et al., 2016). Unfortunately, contrary to machine translation, Japanese non-stan"
W16-3918,D16-1163,0,0.14613,"panese text normalization. Since the encoder-decoder model was proposed in the field of machine translation, it has This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 1 http://taku910.github.io/mecab/ 129 Proceedings of the 2nd Workshop on Noisy User-generated Text, pages 129–137, Osaka, Japan, December 11 2016. License details: http:// achieved good results in morphological inflection generation and error correction (Bahdanau et al., 2015; Luong et al., 2015; Sutskever et al., 2015; Cho et al., 2014; Manaal et al., 2016; Barret et al., 2016). As these models are capable of capturing sequential patterns, it is possible to apply the encoder-decoder model to Japanese text normalization. As mentioned previously, Japanese has no explicit boundaries between words. Thus, Japanese text normalization is naturally posed to a character-level sequence to sequence learning. Although the encoder-decoder model has been shown its effectiveness in large datasets, it is much less effective for small datasets such as low-resource languages (Barret et al., 2016). Unfortunately, contrary to machine translation, Japanese non-standard canonical pairs a"
W16-3918,P14-2111,0,0.0506947,"Missing"
W16-3918,P11-1038,0,0.195629,"Missing"
W16-3918,W04-3230,1,0.699446,"Missing"
W16-3918,P11-2093,0,0.0530837,"Missing"
W16-3918,C12-1093,0,0.0197183,"s issue, we propose a method of data augmentation to increase data size by converting existing resources into synthesized non-standard forms using handcrafted rules. We conducted an experiment to demonstrate that the synthesized corpus contributes to stably train an encoder-decoder model and improve the performance of Japanese text normalization. 1 Introduction With the rapid spread of the social media, many texts have been uploaded to the internet. As such, social media texts are considered important language resources owing to an increasing demand for information extraction and text mining (Lau et al., 2012; Aramaki et al., 2011). However, these texts include lexical variants such as insertions, phonetic substitutions and internet slangs. These non-standard forms adversely affect language analysis tools that are trained on a clean corpus. Since Japanese has no explicit boundaries between words, word segmentation and part-of-speech (POS) tagging are extremely important in Japanese language processing. For example, the output obtained using the Japanese morphological analyzer: MeCab 1 for a non-standard sentence: “このあぷりすげえええ！” is as follows: Input:このあぷりすげえええ！ (kono apuri sugeee; This app is greeea"
W16-3918,D11-1145,0,0.0158666,"e a method of data augmentation to increase data size by converting existing resources into synthesized non-standard forms using handcrafted rules. We conducted an experiment to demonstrate that the synthesized corpus contributes to stably train an encoder-decoder model and improve the performance of Japanese text normalization. 1 Introduction With the rapid spread of the social media, many texts have been uploaded to the internet. As such, social media texts are considered important language resources owing to an increasing demand for information extraction and text mining (Lau et al., 2012; Aramaki et al., 2011). However, these texts include lexical variants such as insertions, phonetic substitutions and internet slangs. These non-standard forms adversely affect language analysis tools that are trained on a clean corpus. Since Japanese has no explicit boundaries between words, word segmentation and part-of-speech (POS) tagging are extremely important in Japanese language processing. For example, the output obtained using the Japanese morphological analyzer: MeCab 1 for a non-standard sentence: “このあぷりすげえええ！” is as follows: Input:このあぷりすげえええ！ (kono apuri sugeee; This app is greeeat!) Output:この (This, Pr"
W16-3918,P16-1154,0,0.0838798,"Missing"
W16-4606,N12-1047,0,0.0247922,"in the previous subsection. Out of the 1,000 sentences in the test set, we extract the sentences that show any matching with the n-grams and use these sentences for our evaluation. In our experiments, the number of sentences actually used for evaluation is 300. Baseline SMT The baseline system for our experiment is Moses phrase-based SMT (Koehn et al., 2007) with the default distortion limit of six. We use KenLM (Heafield et al., 2013) for training language models and SyMGIZA++ (Junczys-Dowmunt and Szal, 2010) for word alignment. The weights of the models are tuned with the n-best batch MIRA (Cherry and Foster, 2012). As variants of the baseline, we also evaluate the translation output of the Moses phrase-based SMT with a distortion limit of 20, as well as that of the Moses hierarchical phrase-based (Chiang, 2005) SMT with the default maximum chart span of ten. Conventional syntactic pre-ordering Syntactic pre-ordering is implemented on the Berkeley Parser. The input sentences are parsed using the Berkeley Parser, and the binary nodes are swapped by the classifier (Goto et al., 2015). As a variant of conventional reordering, we also use a reordering model based on the top-down bracketing transducer gramma"
W16-4606,P05-1033,0,0.657321,"bles high quality images to be formed.]] Figure 1: Example of sublanguage-specific bilingual sentences requiring global reordering. A, B, C are the sentence segments constituting global sentence structures. was combined with syntactic pre-ordering. A statistically significant improvement was observed against the syntactic pre-ordering alone, and a substantial gain of more than 25 points in RIBES score against the baseline was observed for both Japanese-to-English and English-to-Japanese translations, and the BLEU scores remained comparable. 2 Related Work The hierarchical phrase-based method (Chiang, 2005) is one of the early attempts at reordering for SMT. In this method, reordering rules are automatically extracted from non-annotated text corpora during the training phase, and the reordering rules are applied in decoding. As the method does not require syntactic parsing and learns from raw text corpora, it is highly portable. However, this method does not specifically capture global sentence structures. The tree-to-string and string-to-tree SMTs are the methods which employ syntactic parsing, whenever it is available, either for the source or for the target language to improve the translation"
W16-4606,N15-1105,0,0.0599862,"Missing"
W16-4606,2015.mtsummit-papers.1,1,0.799688,"method is the skeleton-based statistical machine translation (SMT) which uses a syntactic parser to extract the global sentence structure, or the skeleton, from syntactic trees and uses conventional SMT to train global reordering (Mellebeek et al., 2006; Xiao et al., 2014). However, the performance of this method is limited by syntactic parsing, therefore the global reordering has low accuracy where the accuracy of syntactic parsing is low. Another approach involves manually preparing synchronous context-free grammar rules for capturing the global sentence structure of the target sublanguage (Fuji et al., 2015). However, this method requires manual preparation of rules. Both methods are unsuitable for formal documents such as patent abstracts, because they fail to adapt to sentences with various expressions, for which manual preparation of rules is complex. This paper describes a novel global reordering method for capturing sublanguage-specific global sentence structure to supplement the performance of conventional syntactic reordering. The method learns a global pre-ordering model from non-annotated corpora without using syntactic parsing and uses this model to perform global pre-ordering on newly"
W16-4606,D08-1089,0,0.0313098,"hen reorder these detected segments globally. In step (ii), we experiment with a detection method based on heuristics, as well as a method based on machine learning. Steps (i) and (ii) are described in the following subsections. 3.1 Extraction of Sentence Pairs Containing Global Reordering We extract sentences containing global reordering from the training corpus and store them in the global reordering corpus; they can subsequently be used for training and prediction. We consider that a sentence pair contains global reordering if the segments in the target sentence appear in swap orientation (Galley and Manning, 2008) to the source segments, when the sentences are divided into two or three segments each. Figure 2 shows an example of a sentence pair involving global reordering with the sentence divided into three segments. We take the following steps: 1. We divide each source and target sentence into two or three segments. The candidate segments start at all possible word positions in the sentence. Here, a sentence pair consisting of K segments is represented as (ϕ1 , ϕ2 · · · ϕK ), where ϕk consists of the k th phrase of the source sentence and αk th phrase of the target sentence. These segments meet the s"
W16-4606,P13-2121,0,0.0130852,"sentence pairs for training, 1,000 for development and 1,000 for testing. This training data for the translation experiment are also used for training global reordering as described in the previous subsection. Out of the 1,000 sentences in the test set, we extract the sentences that show any matching with the n-grams and use these sentences for our evaluation. In our experiments, the number of sentences actually used for evaluation is 300. Baseline SMT The baseline system for our experiment is Moses phrase-based SMT (Koehn et al., 2007) with the default distortion limit of six. We use KenLM (Heafield et al., 2013) for training language models and SyMGIZA++ (Junczys-Dowmunt and Szal, 2010) for word alignment. The weights of the models are tuned with the n-best batch MIRA (Cherry and Foster, 2012). As variants of the baseline, we also evaluate the translation output of the Moses phrase-based SMT with a distortion limit of 20, as well as that of the Moses hierarchical phrase-based (Chiang, 2005) SMT with the default maximum chart span of ten. Conventional syntactic pre-ordering Syntactic pre-ordering is implemented on the Berkeley Parser. The input sentences are parsed using the Berkeley Parser, and the b"
W16-4606,P15-2023,0,0.0136127,"tactic parser to extract the global sentence structure, or the skeleton, from syntactic trees, and uses conventional SMT to train global reordering. Another related approach is the reordering method based on predicate-argument structure (Komachi et al., 2006). However, the performance of sentence structure extraction tends to be low when the accuracy of the syntactic parsing is low. The syntactic pre-ordering is the state-of-the-art method which has substantially improved reordering accuracy, and hence the translation quality (Isozaki et al., 2010b; Goto et al., 2015; de Gispert et al., 2015; Hoshino et al., 2015). However, the adaptation of this method to a new domain requires manually parsed corpora for the target domains. In addition, the method does not have a specific function for capturing global sentence structure. Thus, we apply here our proposed global reordering model as a preprocessor to this syntactic reordering method to ensure the capturing of global sentence structures. 3 Global Pre-ordering Method We propose a novel global reordering method for capturing sublanguage-specific global sentence structure. On the basis of the finding that sublanguage-specific global structures can be detecte"
W16-4606,W15-3058,0,0.0128803,"g global reordering. We evaluate both the heuristic and the machine learning-based methods for comparison. Evaluation metrics We use the RIBES (Isozaki et al., 2010a) and the BLEU (Papineni et al., 2002) scores as evaluation metrics. We use both metrics because n-gram-based metrics such as BLEU alone cannot fully illustrate the effects of global reordering. RIBES is an evaluation metric based on rank correlation which measures long-range relationships and is reported to show much higher correlation with human evaluation than BLEU for evaluating document translations between distant languages (Isozaki and Kouchi, 2015). 5 Results The evaluation results based on the present translation experiment are shown in Tables 1 and 2 for Japanese-to-English and English-to-Japanese translations respectively, listing the RIBES and BLEU scores computed for each of the four reordering configurations. The numbers in the brackets refer to the improvement over the baseline phrase-based SMT with a distortion limit of six. A substantial gain of more than 25 points in the RIBES scores compared to the baseline is observed for both Japanese-to-English and English-to-Japanese translations, when global pre-ordering is used in con89"
W16-4606,D10-1092,0,0.0355675,"Missing"
W16-4606,W10-1736,0,0.120586,"tructure (Mellebeek et al., 2006; Xiao et al., 2014). It uses a syntactic parser to extract the global sentence structure, or the skeleton, from syntactic trees, and uses conventional SMT to train global reordering. Another related approach is the reordering method based on predicate-argument structure (Komachi et al., 2006). However, the performance of sentence structure extraction tends to be low when the accuracy of the syntactic parsing is low. The syntactic pre-ordering is the state-of-the-art method which has substantially improved reordering accuracy, and hence the translation quality (Isozaki et al., 2010b; Goto et al., 2015; de Gispert et al., 2015; Hoshino et al., 2015). However, the adaptation of this method to a new domain requires manually parsed corpora for the target domains. In addition, the method does not have a specific function for capturing global sentence structure. Thus, we apply here our proposed global reordering model as a preprocessor to this syntactic reordering method to ensure the capturing of global sentence structures. 3 Global Pre-ordering Method We propose a novel global reordering method for capturing sublanguage-specific global sentence structure. On the basis of th"
W16-4606,P07-2045,0,0.0042033,"ponding original Japanese abstracts, from which we randomly select 1,000,000 sentence pairs for training, 1,000 for development and 1,000 for testing. This training data for the translation experiment are also used for training global reordering as described in the previous subsection. Out of the 1,000 sentences in the test set, we extract the sentences that show any matching with the n-grams and use these sentences for our evaluation. In our experiments, the number of sentences actually used for evaluation is 300. Baseline SMT The baseline system for our experiment is Moses phrase-based SMT (Koehn et al., 2007) with the default distortion limit of six. We use KenLM (Heafield et al., 2013) for training language models and SyMGIZA++ (Junczys-Dowmunt and Szal, 2010) for word alignment. The weights of the models are tuned with the n-best batch MIRA (Cherry and Foster, 2012). As variants of the baseline, we also evaluate the translation output of the Moses phrase-based SMT with a distortion limit of 20, as well as that of the Moses hierarchical phrase-based (Chiang, 2005) SMT with the default maximum chart span of ten. Conventional syntactic pre-ordering Syntactic pre-ordering is implemented on the Berke"
W16-4606,W04-3250,0,0.0354322,"ed to the baseline is observed for both Japanese-to-English and English-to-Japanese translations, when global pre-ordering is used in con89 Table 1: Evaluation of Japanese-to-English translation where glob-pre denotes global pre-ordering and pre denotes conventional syntactic pre-ordering, dl denotes distortion limit, HPB denotes hierarchical phrase-based SMT and TDBTG denotes reordering based on top-down bracketing transduction grammar. The bold numbers indicate a statistically insignificant difference from the best system performance according to the bootstrap resampling method at p = 0.05 (Koehn, 2004). Reordering config Settings Results glob-pre pre SMT glob-pre pre RIBES BLEU PB dl=6 44.9 17.9 T1 PB dl=20 53.7 (+8.8) 21.3 (+3.4) HPB 54.9 (+10.0) 23.1 (+5.2) √ PB dl=6 heuristic 61.7 (+16.8) 19.6 (+1.7) T2 PB dl=6 SVM 61.0 (+16.1) 19.3 (+1.4) √ PB dl=6 TDBTG 64.6 (+19.7) 22.3 (+4.4) T3 PB dl=6 syntactic 64.9 (+20.0) 25.5 (+7.6) √ √ PB dl=6 heuristic syntactic 71.3 (+26.4) 25.3 (+7.4) T4 PB dl=6 SVM syntactic 72.1 (+27.2) 25.6 (+7.7) T1 T2 T3 T4 Table 2: Evaluation of English-to-Japanese translation Reordering config Settings Results glob-pre pre SMT glob-pre pre RIBES BLEU PB dl=6 43.2 27.9"
W16-4606,2006.iwslt-evaluation.11,1,0.718282,"ge to improve the translation of the language pair (Yamada and Knight, 2001; Ambati and Chen, 2007). However, these methods too are not specifically designed for capturing global sentence structures. The skeleton-based SMT is a method particularly focusing on the reordering of global sentence structure (Mellebeek et al., 2006; Xiao et al., 2014). It uses a syntactic parser to extract the global sentence structure, or the skeleton, from syntactic trees, and uses conventional SMT to train global reordering. Another related approach is the reordering method based on predicate-argument structure (Komachi et al., 2006). However, the performance of sentence structure extraction tends to be low when the accuracy of the syntactic parsing is low. The syntactic pre-ordering is the state-of-the-art method which has substantially improved reordering accuracy, and hence the translation quality (Isozaki et al., 2010b; Goto et al., 2015; de Gispert et al., 2015; Hoshino et al., 2015). However, the adaptation of this method to a new domain requires manually parsed corpora for the target domains. In addition, the method does not have a specific function for capturing global sentence structure. Thus, we apply here our p"
W16-4606,E91-1054,0,0.534414,"and works in conjunction with conventional syntactic reordering. Experimental results on the patent abstract sublanguage show substantial gains of more than 25 points in the RIBES metric and comparable BLEU scores both for Japanese-to-English and English-to-Japanese translations. 1 Introduction Formal documents such as legal and technical documents often form sublanguages. Previous studies have highlighted that capturing the sentence structure specific to the sublanguage is extremely necessary for obtaining high-quality translations especially between distant languages (Buchmann et al., 1984; Luckhardt, 1991; Marcu et al., 2000). Figure 1 illustrates two pairs of bilingual sentences specific to the sublanguage of patent abstracts. In both sentence pairs, the global sentence structure ABC in the source sentences must be reordered to CBA in the target sentences to produce a structurally appropriate translation. Each of the components ABC must then be syntactically reordered to complete the reordering. Various attempts have been made along this line of research. One such method is the skeleton-based statistical machine translation (SMT) which uses a syntactic parser to extract the global sentence st"
W16-4606,A00-2002,0,0.0838799,"unction with conventional syntactic reordering. Experimental results on the patent abstract sublanguage show substantial gains of more than 25 points in the RIBES metric and comparable BLEU scores both for Japanese-to-English and English-to-Japanese translations. 1 Introduction Formal documents such as legal and technical documents often form sublanguages. Previous studies have highlighted that capturing the sentence structure specific to the sublanguage is extremely necessary for obtaining high-quality translations especially between distant languages (Buchmann et al., 1984; Luckhardt, 1991; Marcu et al., 2000). Figure 1 illustrates two pairs of bilingual sentences specific to the sublanguage of patent abstracts. In both sentence pairs, the global sentence structure ABC in the source sentences must be reordered to CBA in the target sentences to produce a structurally appropriate translation. Each of the components ABC must then be syntactically reordered to complete the reordering. Various attempts have been made along this line of research. One such method is the skeleton-based statistical machine translation (SMT) which uses a syntactic parser to extract the global sentence structure, or the skele"
W16-4606,2006.eamt-1.24,0,0.0750942,"Missing"
W16-4606,P15-1021,0,0.0168744,"ts of the baseline, we also evaluate the translation output of the Moses phrase-based SMT with a distortion limit of 20, as well as that of the Moses hierarchical phrase-based (Chiang, 2005) SMT with the default maximum chart span of ten. Conventional syntactic pre-ordering Syntactic pre-ordering is implemented on the Berkeley Parser. The input sentences are parsed using the Berkeley Parser, and the binary nodes are swapped by the classifier (Goto et al., 2015). As a variant of conventional reordering, we also use a reordering model based on the top-down bracketing transducer grammar (TDBTG) (Nakagawa, 2015). We use the output of mkcls and SyMGIZA++ obtained during the preparation of the baseline SMT for training TDBTG-based reordering. Global pre-ordering Global pre-ordering consists of the detection of segment boundaries and the reordering of the detected segments. Out of the 1,000,000 phrase-aligned sentence pairs in the training set for SMT, we use the first 100,000 sentence pairs for extracting the sentence pairs containing global reordering. We only use a portion of the SMT training data due to the slow execution speed of the current implementation of the software program for extracting sen"
W16-4606,P02-1040,0,0.101581,"egment boundaries and the reordering of the detected segments. Out of the 1,000,000 phrase-aligned sentence pairs in the training set for SMT, we use the first 100,000 sentence pairs for extracting the sentence pairs containing global reordering. We only use a portion of the SMT training data due to the slow execution speed of the current implementation of the software program for extracting sentence pairs containing global reordering. We evaluate both the heuristic and the machine learning-based methods for comparison. Evaluation metrics We use the RIBES (Isozaki et al., 2010a) and the BLEU (Papineni et al., 2002) scores as evaluation metrics. We use both metrics because n-gram-based metrics such as BLEU alone cannot fully illustrate the effects of global reordering. RIBES is an evaluation metric based on rank correlation which measures long-range relationships and is reported to show much higher correlation with human evaluation than BLEU for evaluating document translations between distant languages (Isozaki and Kouchi, 2015). 5 Results The evaluation results based on the present translation experiment are shown in Tables 1 and 2 for Japanese-to-English and English-to-Japanese translations respective"
W16-4606,2007.mtsummit-papers.63,1,0.694872,"am for Japanese input. Figure 5 shows the same for English input. The accuracy is the average accuracy of a ten-fold cross-validation for the global reordering corpus. From the calibration shown in the tables, we select the settings producing the highest prediction accuracy, namely, a value of f ive for the n of n-grams and a size of 100k for the global reordering corpus, for both Japanese and English inputs. 4.3 Translation Experiment Setup Data As our experimental data, we use the Patent Abstracts of Japan (PAJ), the English translations of Japanese patent abstracts. We automatically align (Utiyama and Isahara, 2007) PAJ with the corresponding original Japanese abstracts, from which we randomly select 1,000,000 sentence pairs for training, 1,000 for development and 1,000 for testing. This training data for the translation experiment are also used for training global reordering as described in the previous subsection. Out of the 1,000 sentences in the test set, we extract the sentences that show any matching with the n-grams and use these sentences for our evaluation. In our experiments, the number of sentences actually used for evaluation is 300. Baseline SMT The baseline system for our experiment is Mose"
W16-4606,P14-2092,0,0.076127,"In both sentence pairs, the global sentence structure ABC in the source sentences must be reordered to CBA in the target sentences to produce a structurally appropriate translation. Each of the components ABC must then be syntactically reordered to complete the reordering. Various attempts have been made along this line of research. One such method is the skeleton-based statistical machine translation (SMT) which uses a syntactic parser to extract the global sentence structure, or the skeleton, from syntactic trees and uses conventional SMT to train global reordering (Mellebeek et al., 2006; Xiao et al., 2014). However, the performance of this method is limited by syntactic parsing, therefore the global reordering has low accuracy where the accuracy of syntactic parsing is low. Another approach involves manually preparing synchronous context-free grammar rules for capturing the global sentence structure of the target sublanguage (Fuji et al., 2015). However, this method requires manual preparation of rules. Both methods are unsuitable for formal documents such as patent abstracts, because they fail to adapt to sentences with various expressions, for which manual preparation of rules is complex. Thi"
W16-4606,P01-1067,0,0.249784,"tempts at reordering for SMT. In this method, reordering rules are automatically extracted from non-annotated text corpora during the training phase, and the reordering rules are applied in decoding. As the method does not require syntactic parsing and learns from raw text corpora, it is highly portable. However, this method does not specifically capture global sentence structures. The tree-to-string and string-to-tree SMTs are the methods which employ syntactic parsing, whenever it is available, either for the source or for the target language to improve the translation of the language pair (Yamada and Knight, 2001; Ambati and Chen, 2007). However, these methods too are not specifically designed for capturing global sentence structures. The skeleton-based SMT is a method particularly focusing on the reordering of global sentence structure (Mellebeek et al., 2006; Xiao et al., 2014). It uses a syntactic parser to extract the global sentence structure, or the skeleton, from syntactic trees, and uses conventional SMT to train global reordering. Another related approach is the reordering method based on predicate-argument structure (Komachi et al., 2006). However, the performance of sentence structure extra"
W16-4901,P15-3006,0,0.0313456,"やいなや、や否や Table 1: Examples of Japanese functional expressions and surface form variations 2.2 Creating Simple Japanese Replacement List Text simplification, defined narrowly, is the process of reducing the linguistic complexity of a text, while retaining the original information contents and meaning (Siddharthan, 2014). Watananabe and Kawamura (2013) introduced a Japanese simplification system with the use of a “Simple Japanese Replacement List”. Kaneniwa and Kawamura (2013) used the same list to rewrite difficult vocabulary automatically for Japanese learners who have non-kanji backgrounds. Kajiwara and Yamamoto (2015), constructed an evaluation dataset for Japanese lexical simplification. They extracted 2330 sentences from a newswire corpus and simplified only one difficult word using several Japanese lexical paraphrasing databases. Kodaira et al. (2016) built a controlled and balanced dataset for Japanese lexical simplification. They extracted 2010 sentences with only one difficult word in each sentence from a balanced corpus and collected simplification candidates using crowdsourcing techniques. Different from the previous research mentioned above, we used a large scale Japanese balanced corpus to extrac"
W16-4901,P16-3001,0,0.0169959,"taining the original information contents and meaning (Siddharthan, 2014). Watananabe and Kawamura (2013) introduced a Japanese simplification system with the use of a “Simple Japanese Replacement List”. Kaneniwa and Kawamura (2013) used the same list to rewrite difficult vocabulary automatically for Japanese learners who have non-kanji backgrounds. Kajiwara and Yamamoto (2015), constructed an evaluation dataset for Japanese lexical simplification. They extracted 2330 sentences from a newswire corpus and simplified only one difficult word using several Japanese lexical paraphrasing databases. Kodaira et al. (2016) built a controlled and balanced dataset for Japanese lexical simplification. They extracted 2010 sentences with only one difficult word in each sentence from a balanced corpus and collected simplification candidates using crowdsourcing techniques. Different from the previous research mentioned above, we used a large scale Japanese balanced corpus to extract simplification candidates for difficult words and through manual selection we constructed a “Simple Japanese Replacement List” for Chinese-speaking Japanese language learners. For the levels of word difficulty, we consider levels 3-5 of th"
W16-4901,I08-7018,0,0.0332784,"取 り 消 す (cancel)”. Kanji characters in these Japanese words are also included in Chinese dictionary and have the same or similar meaning with Chinese words. Therefore, we consider these words as easy words for Chinese-speaking learners, although some of these words are difficult words in the vocabulary list of the new JLPT. We obtain a list of similar words associated with each difficult word which we are going to use for replacement, using Word2vec (https://code.google.com/p/word2vec/). As the training data for the Word2vec model, we use the Balanced Corpus of Contemporary Written Japanese (Maekawa, 2008), which consists of about 5,800,000 sentences in various domains. Based on the list of similar words, we choose easy words which are included in the vocabulary list of new JLPT as simplified words. If there is no appropriate easy word in the vocabulary list of the new JLPT, we use Japanese-Chinese homographs whose meaning is the same, or similar to Chinese words based on Japanese dictionaries (Bunrui goihyo zouhokaiteiban, 2004; Kadokawa ruigo shin jiten, 2002; Kojien 5th Editon, 1998). Japanese-Chinese homographs for simplification are specific for Chinese-natives. Therefore, our “Simple Japa"
W16-4901,Y13-2007,0,0.352036,"ion lexicon consisting of 292 headwords and 13,958 different surface forms. It is crucial to develop a Japanese learning assistant system which supports Japanese language learners to learn such a large number of complicated functional expressions. Recently, with the help of natural language processing technology, many Japanese learning assistant systems have been constructed. For example, Pereira and Matsumoto (2015) presents a Collocation Assistant for Japanese language learners, which flags possible collocation errors and suggests corrections with example sentences. Han and Song (2011), and Ohno et al. (2013) attempt to develop Japanese learning systems for learning and using Japanese sentence patterns with the use of illustrative examples extracted from the Web. As mentioned above, some studies have paid attention to assist learners to learn Japanese functional expressions. However, none of the existing studies has aimed at simplifying difficult example sentences for the need of Chinese-speaking learners of Japanese language. In this paper, we describe our proposed method in Section 2. Section 3 explains the result and evaluation of a small-scale experiment for examining the effectiveness of the"
W16-4912,P14-2075,0,0.0808001,"ed in English. Since there are much larger data and many tools available for English compared with other languages. In this paper, we tackle the sentence simplification problem in Japanese. We propose a method for estimating complexity of Japanese words and for obtaining semantically similar words for the replacement of complex words. 2 Related Work Dominant approaches of previous work in simplification are hybrid approaches, which combine deep semantic and monolingual machine translation (Narayan and Gardent, 2014), word alignment approach (Coster and Kauchak, 2011; Paetzold and Specia 2013; Horn, et al., 2014) or language modeling approach (Kauchak, 2013). The main limitation of these methods is that they depend on parallel corpus between simple and complex sentences such as English Wikipedia, Simple English Wikipedia or Newsela corpus (Xu et al., 2015). Another approach is to implement the simplification task of complex words by replacing them with the extract synonym words obtained from databases such as thesaurus (Devlin and Tait, 1998; De Belder and Moens, 2010), from dictionary definition or from WordNet (Kajiwara et al.,2013). Thesauri can provide good synonyms while their coverage is limited"
W16-4912,P13-1151,0,0.212587,"many tools available for English compared with other languages. In this paper, we tackle the sentence simplification problem in Japanese. We propose a method for estimating complexity of Japanese words and for obtaining semantically similar words for the replacement of complex words. 2 Related Work Dominant approaches of previous work in simplification are hybrid approaches, which combine deep semantic and monolingual machine translation (Narayan and Gardent, 2014), word alignment approach (Coster and Kauchak, 2011; Paetzold and Specia 2013; Horn, et al., 2014) or language modeling approach (Kauchak, 2013). The main limitation of these methods is that they depend on parallel corpus between simple and complex sentences such as English Wikipedia, Simple English Wikipedia or Newsela corpus (Xu et al., 2015). Another approach is to implement the simplification task of complex words by replacing them with the extract synonym words obtained from databases such as thesaurus (Devlin and Tait, 1998; De Belder and Moens, 2010), from dictionary definition or from WordNet (Kajiwara et al.,2013). Thesauri can provide good synonyms while their coverage is limited. Recent work (Glavasˇand Stajner, 2015; Paetz"
W16-4912,P15-2011,0,0.125461,"Missing"
W16-4912,W16-4912,1,0.0512975,"2013). The main limitation of these methods is that they depend on parallel corpus between simple and complex sentences such as English Wikipedia, Simple English Wikipedia or Newsela corpus (Xu et al., 2015). Another approach is to implement the simplification task of complex words by replacing them with the extract synonym words obtained from databases such as thesaurus (Devlin and Tait, 1998; De Belder and Moens, 2010), from dictionary definition or from WordNet (Kajiwara et al.,2013). Thesauri can provide good synonyms while their coverage is limited. Recent work (Glavasˇand Stajner, 2015; Paetzold and Specia, 2016) approached the word embedding model (Mikolov et al., 2013) to estimate word similarity and aims to mitigate the limitation of thesauri and parallel corpora. The next approach is to identify complex words and choose simpler words for replacing them, while keeping the same meaning of the original complex words. Identifying complex words is This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 92 Proceedings of the 3rd Workshop on Natural Language Processing Techniques for Educational Applications, pages"
W16-4912,P14-2050,0,0.11882,"Missing"
W16-4912,P14-1041,0,0.0811007,"processing such as words similarity or paraphrasing words. Most of studies in simplification are conducted in English. Since there are much larger data and many tools available for English compared with other languages. In this paper, we tackle the sentence simplification problem in Japanese. We propose a method for estimating complexity of Japanese words and for obtaining semantically similar words for the replacement of complex words. 2 Related Work Dominant approaches of previous work in simplification are hybrid approaches, which combine deep semantic and monolingual machine translation (Narayan and Gardent, 2014), word alignment approach (Coster and Kauchak, 2011; Paetzold and Specia 2013; Horn, et al., 2014) or language modeling approach (Kauchak, 2013). The main limitation of these methods is that they depend on parallel corpus between simple and complex sentences such as English Wikipedia, Simple English Wikipedia or Newsela corpus (Xu et al., 2015). Another approach is to implement the simplification task of complex words by replacing them with the extract synonym words obtained from databases such as thesaurus (Devlin and Tait, 1998; De Belder and Moens, 2010), from dictionary definition or from"
W16-4912,O13-1007,0,0.268468,"Missing"
W16-4912,P15-3006,0,0.2452,"Missing"
W16-4912,P16-3001,0,0.0448518,"ommon work before doing the simplification task (Carroll et al., 1998; Baustista et al., 2009; Paetzold and Specia, 2016). Estimation of word complexity is mostly based on their frequencies (Devlin and Tait, 1998; De Belder and Moens, 2010; Kauchak et al., 2014, Kajiwara et al., 2015), their length (Bautista et al.,2009), judgment by user study (Paetzold and Specia, 2016), technical words in specific domains (Kauchak et al., 2014), basic vocabularies for children (Kajiwara et. al, 2013), Japanese Language Proficiency Test levels, or Easy Japanese corpus (Moku et al.,2012). The recent work of (Kodaira et al., 2016) used crowdsourcing to for collecting simplification candidates of words. Our approach uses more than one resource. We first use a thesaurus since thesauri produce the best candidates of synonyms. For addressing the limitation of a thesaurus, we utilize dependency-based word embedding (Levy and Goldberg, 2014) since it is shown that dependency-based embedding highlights less topical and more functional similarity than the skip-gram models. For identification of complexity of words, we use existing approaches like frequency, words used by children, technical words, and Japanese Language Profici"
W16-4912,Q15-1021,0,0.0904706,"ds and for obtaining semantically similar words for the replacement of complex words. 2 Related Work Dominant approaches of previous work in simplification are hybrid approaches, which combine deep semantic and monolingual machine translation (Narayan and Gardent, 2014), word alignment approach (Coster and Kauchak, 2011; Paetzold and Specia 2013; Horn, et al., 2014) or language modeling approach (Kauchak, 2013). The main limitation of these methods is that they depend on parallel corpus between simple and complex sentences such as English Wikipedia, Simple English Wikipedia or Newsela corpus (Xu et al., 2015). Another approach is to implement the simplification task of complex words by replacing them with the extract synonym words obtained from databases such as thesaurus (Devlin and Tait, 1998; De Belder and Moens, 2010), from dictionary definition or from WordNet (Kajiwara et al.,2013). Thesauri can provide good synonyms while their coverage is limited. Recent work (Glavasˇand Stajner, 2015; Paetzold and Specia, 2016) approached the word embedding model (Mikolov et al., 2013) to estimate word similarity and aims to mitigate the limitation of thesauri and parallel corpora. The next approach is to"
W16-5406,uchimoto-etal-2006-dependency,0,0.0412276,"ndency treebanks have been word-based. However, treebanking based on bunsetsu (base phrase unit) has been adopted by the Japanese NLP community, due to the nature of the Japanese bunsetsu dependency structure, such as strictly being head-final and projective on the bunsetsu units. Several annotation schemas for the bunsetsu-based treebanks are accessible in selected Japanese cor KC pora. First is the Kyoto Text Corpus Schema(hereafter  )(Kurohashi and Nagao, 1998), which is used for newspaper articles. Second is the Corpus of Spontaneous Japanese (Maekawa, 2003) Schema  (hereafter CSJ )(Uchimoto et al., 2006). We propose a novel annotation schema for the Japanese bunsetsu dependency structure, in which we also annotate coordinate and apposition structure scopes as segments. In this standard, we define the detailed inter-clause attachment guideline based on (Minami, 1974) and also introduce some labels to resolve errors or discrepancies in the upper process of bunsetsu and sentence boundary annotation. We applied the annotation schema for the core data of ‘Balanced Corpus of Contemporary Written Japanese’ (Maekawa et al., 2014) which comprised data from newspaper(PN), books(PB),magazines(PM), white"
W17-6313,P16-1231,0,0.0156123,"on development sets (with gold tags) are: Arabic: 80.83 (Dyer et al.) vs. 82.01 (ours); English: 86.71 (Dyer et al.) vs. 85.28 (ours); and German: 82.87 (Dyers et al.) vs. 84.45 (ours). Both employ greedy search. Note that our system does not use the buffer LSTM. 7 For learning, we find the following heuristics inspired by max-violation (Huang et al., 2012) works well. Our training is basically local with cross-entropy while for each sentence we calculate the max violation point by beam search and use only the prefix until that point. Although this is simpler than global structured learning (Andor et al., 2016), it provides some improvements with much faster training time. 5 For small treebanks without the development set, we randomly divide the training data at 1:9 ratio for development and training. 6 This baseline is competitive to or stronger than the original implementation of Dyer et al. (2015), which also 93 Language grc la grc proiel la ittb eu en lines la proiel nl lassysmall pt de gl treegal nl got hu cu ur sv lines et da sl cs cltt cs cac hi sl sst tr cs el ar kk fr ca ga es ro es ancora ko ru ru syntagrus no nynorsk hr fr sequoia uk no bokmaal fa fi ftb lv fi id it pt br ug sk fr partut"
W17-6313,W06-2922,0,0.0505355,"as neural network architectures (Chen and Manning, 2014; Dyer et al., 2015), and joint modeling with other phenomena (Hatori et al., 2011; Honnibal and Johnson, 2014), with a minimal effort. Such extensions with swap include a recent nonprojective neural parser (Straka et al., 2015) and joint system with POS tagging (Bohnet and Nivre, 2012). Other approaches often employ additional data structures with non-trivial transitions (Covington, 2001; Choi and McCallum, 2013; Pitler and McDonald, 2015), which interfere with the transparency to the standard systems, or cannot handle all crossing arcs (Attardi, 2006). Despite the popularity of the swap system, to our knowlege there is little work focusing on the swap mechanism, or the transition system itself, apart from the original proposal (Nivre, 2009; Nivre et al., 2009). In other words, we are still unsure whether the current swap mechanism is the best strategy for handling crossing arcs with word reordering. In this work, we present a dependency parser with a new transition system that employs swapbased reordering but in a different manner from the existing one (Nivre, 2009) built on the arcstandard system. As we discuss (Section 2.2), in Nivre’s t"
W17-6313,J17-2002,0,0.0113877,"idirectional LSTMs. Our token representation is: exp(gzTt pt + qzt ) , T z 0 ∈A(ct ) exp(gz 0 pt + qz 0 ) p(zt |pt ) = P where gz is the parameters and qz is the bias term for action z. The set A(ct ) returns the set of valid transitions on ct . After each transition we update x = ReLU(V[w; t; tx; f ; wch ] + ex ), where w, t, tx, and f are word, POS, XPOS, and FEAT embeddings, respectively. wch is the output of linear mapping from the concatenation of the last hidden states of the forward and backward character-level LSTMs. 4 Please refer Ballesteros et al. (2015), Ballesteros et al. (2016), Ballesteros et al. (2017) for the latest version which is sophisicated in some architectures (e.g. character information, dynamic oracle). 92 Composition This is the only modification needed to obtain the stack representation in our stay-eager transition system. The subtree representation in Dyer et al. (2015) is fully compositional in that h in Eq. 1 encodes the entire subtree with the recursive network. This is possible essentially because of the bottom-up nature of the arc-standard system. Unfortunatelly the same encoding is not straightforward in our system due to its arc-eager property, in which the right arcs ar"
W17-6313,D15-1041,0,0.0189357,"ter embeddings, which we obtain from character-level bidirectional LSTMs. Our token representation is: exp(gzTt pt + qzt ) , T z 0 ∈A(ct ) exp(gz 0 pt + qz 0 ) p(zt |pt ) = P where gz is the parameters and qz is the bias term for action z. The set A(ct ) returns the set of valid transitions on ct . After each transition we update x = ReLU(V[w; t; tx; f ; wch ] + ex ), where w, t, tx, and f are word, POS, XPOS, and FEAT embeddings, respectively. wch is the output of linear mapping from the concatenation of the last hidden states of the forward and backward character-level LSTMs. 4 Please refer Ballesteros et al. (2015), Ballesteros et al. (2016), Ballesteros et al. (2017) for the latest version which is sophisicated in some architectures (e.g. character information, dynamic oracle). 92 Composition This is the only modification needed to obtain the stack representation in our stay-eager transition system. The subtree representation in Dyer et al. (2015) is fully compositional in that h in Eq. 1 encodes the entire subtree with the recursive network. This is possible essentially because of the bottom-up nature of the arc-standard system. Unfortunatelly the same encoding is not straightforward in our system due"
W17-6313,D16-1211,0,0.0118821,"tain from character-level bidirectional LSTMs. Our token representation is: exp(gzTt pt + qzt ) , T z 0 ∈A(ct ) exp(gz 0 pt + qz 0 ) p(zt |pt ) = P where gz is the parameters and qz is the bias term for action z. The set A(ct ) returns the set of valid transitions on ct . After each transition we update x = ReLU(V[w; t; tx; f ; wch ] + ex ), where w, t, tx, and f are word, POS, XPOS, and FEAT embeddings, respectively. wch is the output of linear mapping from the concatenation of the last hidden states of the forward and backward character-level LSTMs. 4 Please refer Ballesteros et al. (2015), Ballesteros et al. (2016), Ballesteros et al. (2017) for the latest version which is sophisicated in some architectures (e.g. character information, dynamic oracle). 92 Composition This is the only modification needed to obtain the stack representation in our stay-eager transition system. The subtree representation in Dyer et al. (2015) is fully compositional in that h in Eq. 1 encodes the entire subtree with the recursive network. This is possible essentially because of the bottom-up nature of the arc-standard system. Unfortunatelly the same encoding is not straightforward in our system due to its arc-eager property,"
W17-6313,W15-2210,0,0.0269754,"Missing"
W17-6313,D12-1133,0,0.066103,"Missing"
W17-6313,D14-1082,0,0.118416,"Missing"
W17-6313,P13-1104,0,0.0415261,"Missing"
W17-6313,H05-1066,0,0.115488,"on-projectivity. 1 Introduction A dependency tree as illustrated in Figure 1 is called a non-projective tree, which contains discontinuous subtrees and is informally remarked with crossing arcs (arcs from idea4 to talking8 and from who5 to to9 ). Comparing to the class of projective trees, which has a weak equivalence to the context-free grammars (Gaifman, 1965), parsing non-projective trees is generally involved. This is particularly the case for transition-based dependency parsing; contrary to the graph-based approaches, in which a simple spanning-tree algorithm is capable of handling them (McDonald et al., 2005), due to the incremental nature, transition-based parsers need some extra mechanisms to find crossing arcs. There are several attempts to handle crossing arcs in transition-based parsers. Among them online reordering with swap (Nivre, 2009) has a number of appealing properties, of which the most notable is that it inherits the standard architecture of the transition systems using a stack and buffer while covering all types of crossing arcs. This simplicity allows us to incorporate the ideas developed for the standard projective parsers, such 88 Proceedings of the 15th International Conference"
W17-6313,P15-1033,0,0.148174,"stack are locally likely to be connected, choosing correct SW rather than LA and RA is quite difficult. To see an example, let us consider the configuration in Figure 2 (c = (σ|who|talking, to|β, A)), which occurs when parsing the sentence in Figure 1. The correct action here is SW to create a obl crossing arc talking8 −→ who5 . However, at this point LA is a more likely transition since talking8 obl −→ who5 is a typical arc in a relative clause. The problem is that since LA reduces talking, we will case miss the arc who5 −−→ to9 if we choose LA rather than SW. ing model with the stack LSTMs (Dyer et al., 2015) (Section 4). We extensively examine the utility of new transition system (Section 5) with the recently released Universal Dependencies (UD) 2.0 dataset, which contains more than 60 treebanks with varying degree of non-projectivity, and find that our system is superior to the ordinary swap system particularly for languages with a larger amount of non-projectivity. 2 Background We first introduce some notations and the concept of transition systems (Section 2.1), and then describe the existing swap-based transition system of Nivre (2009) (Section 2.2). 2.1 Transition System We focus in this pap"
W17-6313,W03-3017,0,0.434183,"with word reordering. In this work, we present a dependency parser with a new transition system that employs swapbased reordering but in a different manner from the existing one (Nivre, 2009) built on the arcstandard system. As we discuss (Section 2.2), in Nivre’s transition system, choosing a correct swap transition is sometimes hard due to the parser’s preference to local attachments. The proposed system (Section 3) alleviates this difficulty by allowing a swap transition for a token that is already linked on the stack. As we will see, it can be seen as an extension to the arc-eager system (Nivre, 2003) while we divide each attachment transition into two more primitive operations as in the divided formulation of G´omez-Rodr´ıguez and Nivre (2013). The divided system is more flexible, and by operating swap on this we can deal with the issue of reordering at an appropriate step. On this transition system we implement a parsWe present a new transition system with word reordering for unrestricted nonprojective dependency parsing. Our system is based on decomposed arc-eager rather than arc-standard, which allows more flexible ambiguity resolution between a local projective and non-local crossing"
W17-6313,W04-0308,0,0.0797573,"which is sophisicated in some architectures (e.g. character information, dynamic oracle). 92 Composition This is the only modification needed to obtain the stack representation in our stay-eager transition system. The subtree representation in Dyer et al. (2015) is fully compositional in that h in Eq. 1 encodes the entire subtree with the recursive network. This is possible essentially because of the bottom-up nature of the arc-standard system. Unfortunatelly the same encoding is not straightforward in our system due to its arc-eager property, in which the right arcs are constructed top-down (Nivre, 2004). In this work, we give up the full compositionality of the original model, and simply mimic Eq. 1 with the following equation: c0 = tanh(U[h; d; l; c] + eh ). pipe 1.1, the baseline system in the shared task, although the results may not be directly comparable as they tune several settings including the oracle and learning rate etc. for each language. Settings Our network sizes are: 100 dimensional word embeddings and LSTMs, 50 dimentional POS, XPOS, and FEATS embeddings, and 20 dimensional action and label embeddings, and 32 dimensional character embeddings and biLSTMs. We do not use any pre"
W17-6313,C12-1059,0,0.115434,"→ − j∈ /A means a path from i to j does not exist, which is needed to guarantee acyclicity. ure 2 shows how our system can swap after resolving local projective attachments. Buﬀer Stack idea, talking to, . LA idea, who, talking who to, . SW 3.2 idea, talking SL idea, who talking, to, . . idea, who talking, to, . . SW idea Static and Non-static Oracles An oracle for a transition system is a function from a configuration to the action that leads to a given dependency tree. Before discussing in details, we first note that our system suffers from the spurious ambiguity as in the arc-eager system (Goldberg and Nivre, 2012), which means an oracle for some configurations is not unique. Table 2 shows a specific oracle, which checks for each action in descending order whether the current configuration satisfies the condition, and select the first found one. 2 3 who, to, . talking, who, to, . . Figure 2: A configuration difficult for ASS (above), which fails when LA is selected (SW is correct). Our system avoides this difficulty by first attaching who to talking (SL) and then SW (below). Dotted arcs are correct arcs yet unattached. 2 The priority of attaching transitions over SW is also helpful to avoid unnecessary"
W17-6313,P09-1040,0,0.145262,"extensions with swap include a recent nonprojective neural parser (Straka et al., 2015) and joint system with POS tagging (Bohnet and Nivre, 2012). Other approaches often employ additional data structures with non-trivial transitions (Covington, 2001; Choi and McCallum, 2013; Pitler and McDonald, 2015), which interfere with the transparency to the standard systems, or cannot handle all crossing arcs (Attardi, 2006). Despite the popularity of the swap system, to our knowlege there is little work focusing on the swap mechanism, or the transition system itself, apart from the original proposal (Nivre, 2009; Nivre et al., 2009). In other words, we are still unsure whether the current swap mechanism is the best strategy for handling crossing arcs with word reordering. In this work, we present a dependency parser with a new transition system that employs swapbased reordering but in a different manner from the existing one (Nivre, 2009) built on the arcstandard system. As we discuss (Section 2.2), in Nivre’s transition system, choosing a correct swap transition is sometimes hard due to the parser’s preference to local attachments. The proposed system (Section 3) alleviates this difficulty by allowi"
W17-6313,I11-1136,0,0.072981,"Missing"
W17-6313,N12-1015,0,0.071511,"Missing"
W17-6313,N15-1068,0,0.0238414,"Missing"
W17-6313,J08-4003,0,\N,Missing
W18-4922,de-marneffe-etal-2014-universal,0,0.0290054,"Missing"
W18-4922,J93-2004,0,0.0615426,"Missing"
W18-4922,L16-1262,0,0.0285721,"Missing"
W18-4922,schneider-etal-2014-comprehensive,0,0.06236,"Missing"
W18-6009,L18-1287,1,0.271859,"fundamental issues due to hypotactic attributes in terms of syntax in coordinate structures. This paper points out the issues in the treatment of coordinate structures with evidence of linguistic plausibility and the trainability of parsers, reports on the current status of the corpora in those languages, and proposes alternative representations. Section 2 describes the linguistic features of head-final languages, and Section 3 points out the problems in the left-headed coordinate structures in head-final languages. Section 4 summarizes the current status of UD Japanese (Tanaka et al., 2016; Asahara et al., 2018) and UD Korean (Chun et al., 2018) corpora released as version 2.2. Section 5 shows the experimental results on multiple corpora in Japanese and Korean to attest the difficulty in training with left-headed coordination. Section 6 proposes a revision to the UD guidelines more suited to head-final languages. This paper discusses the representation of coordinate structures in the Universal Dependencies framework for two head-final languages, Japanese and Korean. UD applies a strict principle that makes the head of coordination the left-most conjunct. However, the guideline may produce syntactic t"
W18-6009,W17-6508,0,0.118231,"Missing"
W18-6009,W11-3801,1,0.783345,"the complexities outlined in the previous section, the UD Japanese and UD Korean teams had to work within the bounds of the principles laid out by the Universal Dependencies version 2. Therefore, in the official version 2.2 release used for the CoNLL 2018 shared task (Zeman et al., 2018), UD Japanese and UD Korean adopted two separate strategies in order to ensure compliance, 79 the verb ‘run’ with the (nsubj) label. 4.2 UD Korean Unlike the Japanese UD, the Korean UD effort has made a conscious decision to use right-headedness for conjunction following the coordination guidelines proposed by Choi and Palmer (2011). Thus, the coordinate structures in all three of the Korean UD corpora (Chun et al., 2018) were developed with the rightmost conjunct as the head of the phrase, with each conjunct pointing to its right sibling as its head. For the latest available UD_Korean-GSD, however, the dependencies were converted to leftheaded structures post-development in an effort to fully comply with the UD guidelines despite the problems left-headed structures pose for the language as described in Section 3. The other two Korean UD corpora, namely the Kaist and the Korean Penn Treebank, reflect right-headed coordin"
W18-6009,L18-1347,1,0.849927,"attributes in terms of syntax in coordinate structures. This paper points out the issues in the treatment of coordinate structures with evidence of linguistic plausibility and the trainability of parsers, reports on the current status of the corpora in those languages, and proposes alternative representations. Section 2 describes the linguistic features of head-final languages, and Section 3 points out the problems in the left-headed coordinate structures in head-final languages. Section 4 summarizes the current status of UD Japanese (Tanaka et al., 2016; Asahara et al., 2018) and UD Korean (Chun et al., 2018) corpora released as version 2.2. Section 5 shows the experimental results on multiple corpora in Japanese and Korean to attest the difficulty in training with left-headed coordination. Section 6 proposes a revision to the UD guidelines more suited to head-final languages. This paper discusses the representation of coordinate structures in the Universal Dependencies framework for two head-final languages, Japanese and Korean. UD applies a strict principle that makes the head of coordination the left-most conjunct. However, the guideline may produce syntactic trees which are difficult to accept"
W18-6009,de-marneffe-etal-2014-universal,0,0.118166,"Missing"
W18-6009,W15-2113,0,0.550814,"Missing"
W18-6009,W14-4202,1,0.883365,"Missing"
W18-6009,C18-1324,0,0.0259098,"on scheme the conjunctive particle かわいい ⽝ と 猫 が ⾛る “와” (wa) is kept suffixized in the left nominal conkawaii inu -to neko -ga hashiru junct eojeol, thus the conjunction relation cc is not ADJ NOUN CCONJ NOUN ADP VERB overtly marked. ‘cute’ ‘dog’ ‘and’ ‘cat’ -NOM ‘run’ A common problem with adjectival modification in UD shown in Figures 4 and 5 is that there Figure 4: Left-headed representation of a nominal cois no way to distinguish between modification ordination in Japanese “⽝と猫” (‘dog and cat’), in a of the full coordination vs. of the first conjunct sentence “かわいい⽝と猫が⾛る” (‘A cute dog and (Przepiórkowski and Patejuk, 2018) . For examcat run’). root ple, there is no way to specify the scope of the adjective ‘cute’: the two readings (1) only a dog is nsubj cute and (2) both animals are cute. acl conj 3.2 Verbal coordination 예쁜 개와 고양이가 달린다 yeyppun kay+wa koyangi+ka tali+nta ADJ NOUN NOUN VERB ‘cute’ ‘dog+and’ ‘cat-NOM’ ‘run’ Further critical issues are attested in the verbal coordinate structures. Figure 6 shows the left-headed verbal coordination “⾷べて⾛る” (‘eat and run’) in a noun phrase “⾷べて⾛る⼈” (‘a person who eats and runs’), where verb “⾷べ” (‘eat’) is the child of “⼈” (‘person’). Despite this dependency relatio"
W18-6009,L16-1376,0,0.0419402,"Missing"
W18-6009,L16-1680,0,0.017083,"on strictly reduces the dependency distance in head-final languages. These results support the advantages of the rightheaded strategy in Japanese coordinate structures. nsubj 오바마 대통령이 말한다 obama taythonglyeng+i malha+nta PROPN NOUN VERB ‘Obama’ ‘president-NOM’ ‘say’ (a) Korean right-headed flat structure. root nsubj flat 오바마 대통령이 말한다 obama taythonglyeng+i malha+nta PROPN NOUN VERB ‘Obama’ ‘president-NOM’ ‘say’ (b) (a) converted to left-headed structure as reflected in the UD_Korean-GSD. Figure 12: The use of flat in Korean UD v2.2. guages? To answer this question, we trained and tested UDPipe (Straka et al., 2016) on multiple versions of UD Japanese and Korean corpora. 5.1 Japanese As described in Section 4.1, the current UD Japanese-GSD corpus does not use conj tags. The corpus was converted into another version with coordinations without changing the dependency structures (right-headed coordination), that is, some of nmod and advcl labels are converted into conj label when the original manual annotation used conj regarding them as nominal or verbal coordinations. Also CCONJ tag and cc label are assigned to the coordinative case markers. The corpus was further converted into left-headed coordination,"
W18-6009,L16-1261,1,0.697171,"structure poses some fundamental issues due to hypotactic attributes in terms of syntax in coordinate structures. This paper points out the issues in the treatment of coordinate structures with evidence of linguistic plausibility and the trainability of parsers, reports on the current status of the corpora in those languages, and proposes alternative representations. Section 2 describes the linguistic features of head-final languages, and Section 3 points out the problems in the left-headed coordinate structures in head-final languages. Section 4 summarizes the current status of UD Japanese (Tanaka et al., 2016; Asahara et al., 2018) and UD Korean (Chun et al., 2018) corpora released as version 2.2. Section 5 shows the experimental results on multiple corpora in Japanese and Korean to attest the difficulty in training with left-headed coordination. Section 6 proposes a revision to the UD guidelines more suited to head-final languages. This paper discusses the representation of coordinate structures in the Universal Dependencies framework for two head-final languages, Japanese and Korean. UD applies a strict principle that makes the head of coordination the left-most conjunct. However, the guideline"
W19-2609,H89-1033,0,0.0738321,"on and solving. Future work will focus on designing learning agents to solve the games, as well as improving text generation capabilities. We hope that the proposed approach will lead to developing useful systems for action graph extraction as well as other language understanding tasks. Discussion Our approach faces tough challenges. However, we are encouraged by the significant recent advances towards these challenges in related areas, and plan to leverage this progress for our framework. Programming semantics and rewards for instruction-following agents is known to be notoriously difficult (Winograd, 1972) as language and environments grow increasingly complex. Research on learned instruction-conditional reward models (Bahdanau et al., 2018) is a promising approach towards reducing the amount of “environment engineering” required. 66 References Zhengdong Lu, Haotian Cui, Xianggen Liu, Yukun Yan, and Daqi Zheng. 2018. Object-oriented neural programming (oonp) for document understanding. In ACL. The materials science procedural text corpus. https://github.com/olivettigroup/ annotated-materials-syntheses. Accessed: 5/4/2019. Li Lucy and Jon Gauthier. 2017. Are distributional representations ready"
W19-2609,K17-1034,0,0.0305074,"nguage. The ability to automatically parse these texts into a structured form could allow for datadriven synthesis planning, a key enabler in the design and discovery of novel materials (Kim et al., 2018; Mysore et al., 2017). A particularly useful parsing is action graph extraction, which maps a passage describing a procedure to a symbolic action-graph representation of the core entities, operations and their accompanying arguments, as they unfold throughout the text (Fig. 1). Procedural text understanding is a highly challenging task for today’s learning algorithms (Lucy and Gauthier, 2017; Levy et al., 2017). Synthesis procedures are especially challenging, as they are written in difficult and highly technical language assuming prior knowledge. Some texts are long, ∗ Work was begun while author was an intern at RIKEN and continued at the Hebrew University. 62 Proceedings of the Workshop on Extracting Structured Knowledge from Scientific Publications, pages 62–71 c Minneapolis, USA, June 6, 2019. 2019 Association for Computational Linguistics Figure 1: Sample surface text (left) and possible corresponding action-graph (right) for typical partial material synthesis procedure. Operation numbers in p"
W96-0107,C88-1016,0,0.260008,"Missing"
W96-0107,J93-1006,0,0.148265,"Missing"
W96-0107,C94-1009,0,0.0419482,"Missing"
W96-0107,P93-1003,0,0.42506,"Missing"
W96-0107,J96-1001,0,\N,Missing
W96-0109,P94-1002,0,0.0665048,"Missing"
W96-0109,J94-4002,0,0.0303168,"ents conducted on the 19Mb of Japanese newspaper articles. The paper also brings concepts from the rhetorical structure theory (RST) to the statistical analysis of a text structure. Finally, it is shown that information on text structure is more effective for large documents than for small documents. 1. INTRODUCTION Topic identification concerns a problem of predicting terms in text which indicate its subject or theme. In the past, the problem has been addressed mostly by computational linguists in relation to issues like coreference (Hobbs, 1978), anaphora resolution (Grosz and Sidner, 1986; Lappin and Leass, 1994), or discourse center (Joshi and Weinstein, 1981; Walker et al., 1994). In information retrieval, predicting important terms in document is crucial for an effective retrieval of relevant documents(Salton et al., 1993), though they do not necessarily correspond to the subject or the theme. Predicting important terms involves numerical weighting of terms in document. Terms with top weights are judged important and representative of document. A spin-off of information retrieval, known as text categorization, shares a similar research interest. Text categorization concerns associating documents wi"
W96-0109,J94-2003,0,\N,Missing
W96-0109,J86-3001,0,\N,Missing
W97-0113,J97-1002,0,0.059,"Missing"
W97-0113,J96-2004,0,0.072383,"9 6-9 # Articles 352 131 147 the author himself (Watanabe, 1996) or by someone else (McKeown and Radev, 1995) (as in the T I P S T E R Ziff-Davis corpus). Or one takes a roundabout way to identify extracts in a text through a human-supplied abstract (Kupiec et al.: 1995). In the paper, we will propose •a method for identifying summary extracts in a way that allows objective justification. We will do this by examining how humans perform on summary extraction and evaluating the reliability of their performance, using the kappa statistic, a metric standardly used in the behavioral sciences (Jean Carletta, 1996; Sidney Siegel and N. John Castellan Jr., 1988). Based on summary extracts supplied by hum~us, we construct a collection of texts annotated with information on sentence importance. They will be used as training and test data for a decision tree approach to abstracting, which we adopt in the paper (Qulnlan, 1993). In a decision tree approach, the task of extracting summary sentences is treated as a two-way classification task, where a sentence is assigned to either ""yes"" or ""no"" category, depending on its likelihood of being a summary sentence. The merit of a decision tree method is that it pr"
W97-0113,P93-1020,0,0.208101,"material consists of three extraction problems, each with a text from a different category. Though 85 of 112 subjects were assigned to one test, due to the lack of enough subjects, we had to ask the remaining 27 subjects to work on 114 five tests. On the average, each test had about 7 subjects assigned to it. 2.2. Measurement of B.eliability T h e K a p p a S t a t i s t i c Following Jean Carletta (1996), we use the kappa statistic (Sidney Siegel and N. John Castellan Jr., 1988) to measure degree of agreement among subjects. The reason for choosing the kappa over other measures of agreement (Passonneau and Litman, 1993) derives from our interest in discovering a relationship between the reliability or quality of data (as quantified by some metric) and the performance of automatic abstracting. As aptly pointed out in Jean Carletta (1996), agreement measures proposed so far in the computational q linguistics literature has failed to ask an important question of whether results obtained using agreement data are in any way different from random data. It has been left unclear just how high level of agreement among subjects needs to be achieved before reliably using data. It could be the case that data with high a"
W97-0113,C96-2164,0,0.031104,"hat are indicative of its content. In particular, the paper focuses on creating abstracts of Japanese newspaper texts. An approach to abstracting by extraction typically makes use of a text corpus with labelled extracts, indicating which sentence is a summary extract. However, as far as we know, no question has ever been raised on the empirical validity of the extracts used. Usually, extracts are manually supplied by 113 Table 1: Statistics on Corpus Text Type Column Editorial News Report Length in char. about 640 900-1100 800-1000 # Par. 4-5 6-9 6-9 # Articles 352 131 147 the author himself (Watanabe, 1996) or by someone else (McKeown and Radev, 1995) (as in the T I P S T E R Ziff-Davis corpus). Or one takes a roundabout way to identify extracts in a text through a human-supplied abstract (Kupiec et al.: 1995). In the paper, we will propose •a method for identifying summary extracts in a way that allows objective justification. We will do this by examining how humans perform on summary extraction and evaluating the reliability of their performance, using the kappa statistic, a metric standardly used in the behavioral sciences (Jean Carletta, 1996; Sidney Siegel and N. John Castellan Jr., 1988)."
W97-0113,C96-2166,0,0.0723343,"Missing"
W97-1003,H92-1023,0,0.013389,", 1993, Martin, Liermann, and Ney, 1995, Ueberla, 1995). This measure cannot be applied in our case since we look at structure and ignore other words, and consequently algorithms using that measure cannot be applied to the problem we deal with. The mentioned studies use word-clusters for interpolated n-gram language models. Another application of hard clustering methods (in particular bottom-up variants) is that they can also produce a binary tree, which can be used for decision-tree based systems such as the S P A T T E R parser (Magerman, 1995) or the ATR Decision-Tree Part-OfSpeech Tagger (Black et al., 1992, Ushioda, 1996). Table 4: Distribution of dependencies of the words Nippon and Rep., as proper nouns. dep. word Nippon proper n. Rep. proper n. 4 headword tag proper noun proper noun proper noun proper noun Comparison with Based Clustering - relation NP-SBJ NP NP-SBJ NP freq. 3 6 23 45 Co-Occurrence Clustering of words based on syntactic behavior has to our knowledge not been carried out before, but clustering has been applied with the goal of obtaining classes based on co-occurrences. Such clusters were used in particular for interpolated n-gram language models. By looking at co-occurrences"
W97-1003,J92-4003,0,0.0674354,"Treebank) rather than noun phrases or prepositional phrases, usually forming a subject raising construction. This is only a tendency, since all of them can be used in a different way as well, but the tendency is strong enough to make their usage quite similar. Co-occurrence based clustering ignores the structure in which the word occurs, and would therefore not be the right method to find related similarities. As mentioned, co-occurrence based clustering methods often also aim at producing semantically meaningful clusters. Various methods are based on Mutual Information between classes, see (Brown et al., 1992, McMahon and Smith, 1996, Kneser and Ney, 1993, Jardino and Adda, 1993, Martin, Liermann, and Ney, 1995, Ueberla, 1995). This measure cannot be applied in our case since we look at structure and ignore other words, and consequently algorithms using that measure cannot be applied to the problem we deal with. The mentioned studies use word-clusters for interpolated n-gram language models. Another application of hard clustering methods (in particular bottom-up variants) is that they can also produce a binary tree, which can be used for decision-tree based systems such as the S P A T T E R parser"
W97-1003,P96-1025,0,0.0202813,"ail a binary word tree for prepositions t h a t was created by syntactic-behavior based clustering, to show w h a t sort of properties are revealed by the clustering and w h a t one can learn from this a b o u t language. We also discuss various ways in which this kind of clustering can be used in NLP applications. Hogenhout ~ Matsumoto Smith noun Headwords and Dependencies The d a t a we extract are based on the concept of headwords. Such headwords are chosen for every constituent in the parse tree by means of a simple set of rules. These have been used in various studies in this field, see (Collins, 1996, Magerman, 1995, Jelinek et el., 1994). Every headword is p r o p a g a t e d up through the tree such t h a t every parent receives a headword from the head-child. Figure 1 gives an example of a parse tree with headwords. Following the techniques suggested by (Collins, 1996), a parse tree can subsequently be described as a set of dependencies. Every word except the headword of the sentence depends on the lowest headword it is covered by. The syntactic relation is then given by the triple of nonterminals: the modifying nonterminal, the modified nonterminal, and the nonterminal t h a t covers"
W97-1003,P93-1022,0,0.0615138,"ic behavior and discuss the appropriateness of such a subdivision. Applications of this work are also discussed. 1 NP ~$mith John proper proper noun works verb fast adverb Figure 1: Sentence with Parse Tree and Headwords Introduction 2 The construction of classes of words, or calculation of distances between words, has frequently drawn the interest of researchers in n a t u r a l language processing. Many of these studies aimed at finding classes based on co-occurrences, often combined with the aim of establishing semantic similarity between words (McMahon and Smith, 1996, Brown et el., 1992, Dagan, Markus, and Markovitch, 1993, Dagan, Pereira, and Lee, 1994, Pereira and Tishby, 1992, Grefenstette, 1992). We suggest a m e t h o d for clustering words purely on the basis of syntactic behavior. We show how the necessary d a t a for such clustering can easily be drawn from a publicly available treebank, and how distinct types of behavior can be discovered. Although a p a r t of speech tag set can be thought of as a classification based on syntactic behavior, we can construct an a r b i t r a r y number of clusters, or a b i n a r y tree of words t h a t share their p a r t of speech. We discuss in detail a binary word"
W97-1003,P94-1038,0,0.164993,"teness of such a subdivision. Applications of this work are also discussed. 1 NP ~$mith John proper proper noun works verb fast adverb Figure 1: Sentence with Parse Tree and Headwords Introduction 2 The construction of classes of words, or calculation of distances between words, has frequently drawn the interest of researchers in n a t u r a l language processing. Many of these studies aimed at finding classes based on co-occurrences, often combined with the aim of establishing semantic similarity between words (McMahon and Smith, 1996, Brown et el., 1992, Dagan, Markus, and Markovitch, 1993, Dagan, Pereira, and Lee, 1994, Pereira and Tishby, 1992, Grefenstette, 1992). We suggest a m e t h o d for clustering words purely on the basis of syntactic behavior. We show how the necessary d a t a for such clustering can easily be drawn from a publicly available treebank, and how distinct types of behavior can be discovered. Although a p a r t of speech tag set can be thought of as a classification based on syntactic behavior, we can construct an a r b i t r a r y number of clusters, or a b i n a r y tree of words t h a t share their p a r t of speech. We discuss in detail a binary word tree for prepositions t h a t w"
W97-1003,H94-1052,0,0.454825,"Missing"
W97-1003,P95-1037,0,0.0780755,"Missing"
W97-1003,J96-2003,0,0.0489225,"m a t i c a l l y subdivided by their syntactic behavior and discuss the appropriateness of such a subdivision. Applications of this work are also discussed. 1 NP ~$mith John proper proper noun works verb fast adverb Figure 1: Sentence with Parse Tree and Headwords Introduction 2 The construction of classes of words, or calculation of distances between words, has frequently drawn the interest of researchers in n a t u r a l language processing. Many of these studies aimed at finding classes based on co-occurrences, often combined with the aim of establishing semantic similarity between words (McMahon and Smith, 1996, Brown et el., 1992, Dagan, Markus, and Markovitch, 1993, Dagan, Pereira, and Lee, 1994, Pereira and Tishby, 1992, Grefenstette, 1992). We suggest a m e t h o d for clustering words purely on the basis of syntactic behavior. We show how the necessary d a t a for such clustering can easily be drawn from a publicly available treebank, and how distinct types of behavior can be discovered. Although a p a r t of speech tag set can be thought of as a classification based on syntactic behavior, we can construct an a r b i t r a r y number of clusters, or a b i n a r y tree of words t h a t share the"
W97-1003,P93-1024,0,0.315907,"rpolated n-gram language models. By looking at co-occurrences it is possible to find groups of words such as [director, chief, professor, commissioner, commander, superintendent]. The Hogenhout ~ Matsumoto 18 Word Clustering from Syntactic Behavior tion, such as In this case a decision tree contains binary questions to decide the properties of a word. We present a hard clustering algorithm, in the sense that every word belongs to exactly one cluster (or is one leaf in the binary word-tree of a particular part of speech). Besides hard algorithms there have also been studies to soft clustering (Pereira, Tishby, and Lee, 1993, Dagan, Pereira, and Lee, 1994) where the distribution of every word is smoothed with the nearest k words rather than placed in a class which supposedly has a uniform behavior. In fact, in (Dagan, Markus, and Markovitch, 1993) it was argued that reduction to a relatively small number of predetermined word classes or clusters may lead to substantial loss of information. On the other hand, when using soft clusteringit is not possible to give a yes/no answer about class membership, and binary word trees cannot be constructed. 5 Similarity Measure and p(R, thlWdtd) ---- A(R, thlWdtd) + (1 - A)(R,"
W97-1003,W96-0103,0,0.108302,"mann, and Ney, 1995, Ueberla, 1995). This measure cannot be applied in our case since we look at structure and ignore other words, and consequently algorithms using that measure cannot be applied to the problem we deal with. The mentioned studies use word-clusters for interpolated n-gram language models. Another application of hard clustering methods (in particular bottom-up variants) is that they can also produce a binary tree, which can be used for decision-tree based systems such as the S P A T T E R parser (Magerman, 1995) or the ATR Decision-Tree Part-OfSpeech Tagger (Black et al., 1992, Ushioda, 1996). Table 4: Distribution of dependencies of the words Nippon and Rep., as proper nouns. dep. word Nippon proper n. Rep. proper n. 4 headword tag proper noun proper noun proper noun proper noun Comparison with Based Clustering - relation NP-SBJ NP NP-SBJ NP freq. 3 6 23 45 Co-Occurrence Clustering of words based on syntactic behavior has to our knowledge not been carried out before, but clustering has been applied with the goal of obtaining classes based on co-occurrences. Such clusters were used in particular for interpolated n-gram language models. By looking at co-occurrences it is possible t"
W98-1125,J97-1002,0,0.0883319,"Missing"
W98-1125,P96-1025,0,0.0264401,"Missing"
W98-1125,P94-1002,0,0.0971736,"Missing"
W98-1125,C94-2183,0,0.0209648,"n d A n a l y s e s We list major results of the experiments in Table 3, The results show t h a t clues are not of much help to improve performance. Indeed we get the best result of 0.642 when N = 0, i.e., the model does not use clues at all. We even find that an overall performance tends to decline as models use more Of the words in the corpus as clues. It is somewhat tempting to take the results as indicating that clues have bad effects on the performance (more discussion on this later). This, however, appears to run counter to what we expect from results reported in prior work on discourse(Kurohashi and Nagao, 1994; Litman and Passonneau, 1995; Grosz and Sidner, 1986; Marcu, 1997), where the notion of clues or cue phrases forms an i m p o r t a n t part of identifying a structure of discourse7 Table 4 shows how the confidence value (CF) affects the performance of discourse models. The CF 7 One problem with earlier work is that evaluations are done on very small data; 9 sections from a scientific writing (approx. 300 sentences) (Kurohashi and Nagao, 1994): 15 narrathes (I113 clauses) (Lhman and Passonneau. 1995): 3 texts (Marcu, 1997). It is not clear how reliable estimates of performance obtained there"
W98-1125,P95-1015,0,0.018465,"major results of the experiments in Table 3, The results show t h a t clues are not of much help to improve performance. Indeed we get the best result of 0.642 when N = 0, i.e., the model does not use clues at all. We even find that an overall performance tends to decline as models use more Of the words in the corpus as clues. It is somewhat tempting to take the results as indicating that clues have bad effects on the performance (more discussion on this later). This, however, appears to run counter to what we expect from results reported in prior work on discourse(Kurohashi and Nagao, 1994; Litman and Passonneau, 1995; Grosz and Sidner, 1986; Marcu, 1997), where the notion of clues or cue phrases forms an i m p o r t a n t part of identifying a structure of discourse7 Table 4 shows how the confidence value (CF) affects the performance of discourse models. The CF 7 One problem with earlier work is that evaluations are done on very small data; 9 sections from a scientific writing (approx. 300 sentences) (Kurohashi and Nagao, 1994): 15 narrathes (I113 clauses) (Lhman and Passonneau. 1995): 3 texts (Marcu, 1997). It is not clear how reliable estimates of performance obtained there would be. &quot;In this procedure,"
W98-1125,P95-1037,0,0.117849,"Missing"
W98-1125,P97-1013,0,0.0302281,"ults show t h a t clues are not of much help to improve performance. Indeed we get the best result of 0.642 when N = 0, i.e., the model does not use clues at all. We even find that an overall performance tends to decline as models use more Of the words in the corpus as clues. It is somewhat tempting to take the results as indicating that clues have bad effects on the performance (more discussion on this later). This, however, appears to run counter to what we expect from results reported in prior work on discourse(Kurohashi and Nagao, 1994; Litman and Passonneau, 1995; Grosz and Sidner, 1986; Marcu, 1997), where the notion of clues or cue phrases forms an i m p o r t a n t part of identifying a structure of discourse7 Table 4 shows how the confidence value (CF) affects the performance of discourse models. The CF 7 One problem with earlier work is that evaluations are done on very small data; 9 sections from a scientific writing (approx. 300 sentences) (Kurohashi and Nagao, 1994): 15 narrathes (I113 clauses) (Lhman and Passonneau. 1995): 3 texts (Marcu, 1997). It is not clear how reliable estimates of performance obtained there would be. &quot;In this procedure, the available data is divided into N"
W98-1125,J93-2004,0,0.0225877,"Missing"
W98-1304,P97-1003,0,0.0414749,"Missing"
W98-1304,C94-1024,0,0.0499579,"Missing"
W98-1304,P95-1037,0,0.11438,"Missing"
W98-1304,C96-2145,0,0.0250268,"e from a set of traversal strings. Later we will describe a robust, heuristic algorithm for reconstructing the tree. The basic concept of parsing with traver°~l strings is that after seeing a word, one considers a number of possible tree contexts in which that word normally occurs. The most likely ones are selected both by considering the likelihood of a context occun'ing with a word and the likelihood of a context following another context. As for the last relation, it is here that the redundance becomes me~nlngful; since neighboring traversal strings are often partially or completely equal. Oflazer (1996) used a similar structure called &quot;vertex lists&quot; which he defined as the path from a leaf to the root of the tree but, different from our definition, including the tag and the word. Oflazer uses vertex lists for error-tolerant tree-matching. In some cases trees can be said to match approximately, and using vertex lists to quantify the amount of difference between trees, Oflazer shows how trees similar to a given tree can be retrieved from a database (treebank). As will be clear from what follows, we use traversal strings in a completely different way, and to the best of our knowledge these stri"
W98-1304,W97-0301,0,0.0446773,"Missing"
W98-1304,J93-2004,0,\N,Missing
W98-1511,W95-0103,0,0.0457871,"Missing"
W98-1511,E91-1004,0,0.0325226,"io NAIST 8916-5 Takayama, Ikorna Nara, 630-0101 JAPAN masaka-h@is.aist-nara.ac.jp Matsumoto NAIST 8916-5 Takayama, Ikoma Nara, 630-0101 JAPAN matsu@is.aist-nara.ac.jp YUJI MASAKAZU Abstract on collocation counts as the Oilly source of grammatical iilfonna.tiOit. He uses co-occurr<&apos;Hce pa.th&apos;nts of tlw POS tags of head-words. The method, how<&apos;H&apos;f, is not statistical, iu that it only accumulates correct pattems for direct. use. Ma.genna.n [4] proposes a statistical parser based Oil a decision tree model, in which the probabilitiC&apos;:; arc conditioned on the derivation history of the parse tr<.&apos;cs [4, 10]. He compares the decision tiT(&apos; model with the n-gra.m model, a.ud cla.ims that the a.mouut of parauwtcrs in t.h<&apos; resulting model remaiHs relatively constant, depending mostly on the numher of traiuing exa.mph&apos;s. Cha.ntiak [5] proposes a lH&apos;W model alld compared it with Colli11s&apos; 1 aud IVJa.germaiL&apos;s models and shows what aspects of these systems affect. their relative perfonnance. In general, statistical models suffc&apos;r from the probkut of data. spa.rs<&apos;IH.&apos;ss. Instead of usiilg a complex sta.t.istiralinodd combi11nl with vnrions smoothillg techniq11es [1 1 2, I, 0], Ve stick to a. statisti"
W99-0620,P96-1042,0,0.0737396,"es, SEQUENCE and ELABORATION, (LOGICAL relationships being subsumed under the SEQUENCE class) and assumed that a lexical cue marks a major class to which it belongs. The modification successfully raised the ~ score to 0.70. Collapsing LOGICAL and SEQUENCE classes may be justified by noting that both types of relationships have to do with relating two semantically independent sentences, a property not shared by relations of the elaboration type. 3 3.1 Learning with Active Data Selection Committee-based Sampling In the committee-based sampling method (CBS, henceforth) (Dagan and Engelson, 1995; Engelson and Dagan, 1996), a training example is selected from a corpus according to its usefulness; a preferred example is one whose addition to the training corpus improves the current estimate of a model parameter which is relevant to classification and also affects a large proportion of examples. CBS tries to identify such an example by randomly generating multiple models (committee members) based on posterior disis given as the posterior multinomial distribution P ( a l = a l , . . . , a n = an J S), where ai is a model parameter and ai represents one of the possible values. P ( a l = a l , . . . , a n = an I S)"
W99-0620,P97-1013,0,0.0228387,"ssifiers. 1 Introduction The success of corpus-based approaches to discourse ultimately depends on whether one is able to acquire a large volume of data annotated for discourse-level information. However, to acquire merely a few hundred texts annotated for discourse information is often impossible due to the enormity of the haman labor required. This paper presents a novel method for reducing the amount of data for training a decision tree classifier, while not compromising the accuracy. While there has been some work exploring the use of machine leaning techniques for discourse and dialogue (Marcu, 1997; Samuel et al., 1998), to our knowledge, no computational research on discourse or dialogue so far has addressed the problem of reducing or minimizing the amount of data for training a learning algorithm. * The work reported here was conducted while the first author was with Advanced Research Lab., Hitachi Ltd, 2520 Hatoyama Saitama 350-0395 Japan. 158 A particular method proposed here is built on the committee-based sampling, initially proposed for probabilistic classifiers by Dagan and Engelson (1995), where an example is selected from the corpus according to its utility in improving statis"
W99-0620,P98-2188,0,0.169387,"ntroduction The success of corpus-based approaches to discourse ultimately depends on whether one is able to acquire a large volume of data annotated for discourse-level information. However, to acquire merely a few hundred texts annotated for discourse information is often impossible due to the enormity of the haman labor required. This paper presents a novel method for reducing the amount of data for training a decision tree classifier, while not compromising the accuracy. While there has been some work exploring the use of machine leaning techniques for discourse and dialogue (Marcu, 1997; Samuel et al., 1998), to our knowledge, no computational research on discourse or dialogue so far has addressed the problem of reducing or minimizing the amount of data for training a learning algorithm. * The work reported here was conducted while the first author was with Advanced Research Lab., Hitachi Ltd, 2520 Hatoyama Saitama 350-0395 Japan. 158 A particular method proposed here is built on the committee-based sampling, initially proposed for probabilistic classifiers by Dagan and Engelson (1995), where an example is selected from the corpus according to its utility in improving statistics. We extend the me"
Y01-1028,Y00-1038,0,0.0272991,"aw-to ]B [gita-o hiki-nagara .. . guitar-ACC play-while `(Lit.) When Mai sings, while someone playing the guitar, ... &apos; (3) a. [ Mai-ga Mai-NOM sing-then utaw-to I B gita-o hiki-nagara 1,4 .. . b.*[ [ Mai-ga Mai-NOM sing-then guitar-ACC play-while Conjunctive to &apos;then&apos; leads to a subordinate clause in level B while conjunctive nagara &apos;while&apos; leads to a subordinate clause in level A. Level A is the innermost layer of the sentential hierarchy and it cannot contain a level B subordinate clause. Thus, Minami&apos;s description gives a rough sketch of conditions of combinatorial nature of clause level. Yoshimoto et al. (2000) proposed a formal treatment of Minami&apos;s hierarchy under the framework of HPSG and Discourse Representation Theory. However, their analysis focused only on the interpretation of temporal relation between the events described in subordinate and matrix clauses, and it simply reformulates Minami&apos;s hierarchy as &apos;hierarchy feature&apos; which can take value A, B, C, and D. Minami&apos;s (and Yoshimoto&apos;s) work seems to capture a surface ranking for such combination with using somewhat tentative levels, and there is no answer to the question: Which linguistic information is the origin of HCF (and his level of"
Y04-1014,Y03-1009,0,\N,Missing
Y04-1014,C04-1067,0,\N,Missing
Y04-1014,W03-1730,0,\N,Missing
Y04-1014,W03-1705,0,\N,Missing
Y04-1014,W03-1719,0,\N,Missing
Y04-1014,W03-1726,0,\N,Missing
Y04-1014,C04-1081,0,\N,Missing
Y04-1014,W03-1720,1,\N,Missing
Y04-1014,W02-1815,0,\N,Missing
Y04-1014,W04-1109,1,\N,Missing
Y06-1044,W03-1719,0,0.0227289,"ictionary. To build a practical system, this number is too small. Therefore, we tried to enlarge the system dictionary using unknown word extraction methods. We intend to extract a large amount of unknown words from a huge raw text corpus. Based on our methods, we have successfully increased the system dictionary to 120,769 entries. Although this number is good enough for a practical system but we still hope to add more in the future. 2 New Segmentation Unit In Chinese language processing community, no single segmentation standard is agreeable across different instituitions. In SIGHAN bakeoff [1], we could see that different institutions have provided different segmentation standards. Most of the disagreements in the standards come from the segmentation of morphologically derived words [2] and named entities. For example, some would say that “孩子们/NN” (children) as one word and some would prefer to it as two words, “孩子/NN” and “们 /M”. For named entities such as Chinese person names, whether a string of a surname and a given name should be one word or two words, is also under argument. It would be nice if we can build a system that suits everyone’s needs but it sounds almost impossible."
Y06-1044,O03-4001,0,0.164599,"unknown words from a huge raw text corpus. Based on our methods, we have successfully increased the system dictionary to 120,769 entries. Although this number is good enough for a practical system but we still hope to add more in the future. 2 New Segmentation Unit In Chinese language processing community, no single segmentation standard is agreeable across different instituitions. In SIGHAN bakeoff [1], we could see that different institutions have provided different segmentation standards. Most of the disagreements in the standards come from the segmentation of morphologically derived words [2] and named entities. For example, some would say that “孩子们/NN” (children) as one word and some would prefer to it as two words, “孩子/NN” and “们 /M”. For named entities such as Chinese person names, whether a string of a surname and a given name should be one word or two words, is also under argument. It would be nice if we can build a system that suits everyone’s needs but it sounds almost impossible. Wu [2] tried to define tree structures to 1 http://www.cis.upenn.edu/~chinese/ 332 morphologically derived words but that will need a lot of human efforts as they are all based on rules defined. G"
Y06-1044,P04-1059,0,0.0146308,"entities. For example, some would say that “孩子们/NN” (children) as one word and some would prefer to it as two words, “孩子/NN” and “们 /M”. For named entities such as Chinese person names, whether a string of a surname and a given name should be one word or two words, is also under argument. It would be nice if we can build a system that suits everyone’s needs but it sounds almost impossible. Wu [2] tried to define tree structures to 1 http://www.cis.upenn.edu/~chinese/ 332 morphologically derived words but that will need a lot of human efforts as they are all based on rules defined. Gao et al. [3] have tried to modify their current system to adapt for all segmentation standards in SIGHAN bakeoff using the transformation-based learning methods [4]. Since we would like to build a system for CTB, we try to define our segmentation units as close as to the CTB standard, or at least to be able to modify back to the CTB standard easily. There are a few changes that we have made on the CTB corpus to suit our purpose and to ease our processing. We refer to this new segmentation units as minimal segmentation units. The changes are made on proper names, foreign words and numeral type words only."
Y06-1044,Y04-1014,1,0.880443,"Missing"
Y06-1044,P03-2039,1,0.887726,"Missing"
Y06-1044,W02-1817,0,0.0322692,"Missing"
Y06-1044,N01-1025,1,0.748391,"5 81.1 84.6 89.8 84.8 87.2 91.4 88.8 90.1 POS Tagging Rec. Prec. F 82.1 75.8 78.8 83.3 78.7 80.9 84.7 82.2 83.5 90.1 90.7 90.4 91.1 91.8 91.5 80.2 73.6 76.7 81.4 76.8 79.1 83.0 80.6 81.8 4.2 CTB Unit Analysis -YamCha The second layer takes the output from the first layer and joins the words by chunking. In order to obtain the original segmentation and POS tags, our task is to join up family names and given names, numbers, numeral type time nouns, and foreign words. The only difference with the original POS tags is that we cannot get back the original POS tags for foreign words. We used YamCha [10] for chunking as it is proved to be efficient for this task. This system is based on Support Vector Machines. The feature sets used are two words and POS tags at both left and right sides of the current word, plus the previous two output labels. The output labels are NR-PER-B, NR-PER-I, CD-B, CD-I, OD-B, OD-I, NT-B, NT-I, FW-B, FW-I and O. Bottom part of Table 3 shows the results of the second layer analysis. While the results of first layer are based on minimal units, the results of second layer are based on CTB units. Since the results are based on different segmentation units, we cannot do"
Y06-1044,W03-1730,0,0.0608622,"Missing"
Y06-1044,W03-1726,0,\N,Missing
Y09-2021,D09-1003,0,0.0219219,"r several tasks, such as improving parsing. Since latent variables group the kind of arguments expected for a sentence, it is possible to infer the meaning of unknown words, as in the well-known example about tezguino (Lin, 1998b) where it is possible to know what is tezguino from the sentences: A bottle of tezguino is on the table; Everybody likes tezguino; Tezguino makes you drunk; and We make tezguino out of corn. Another application is Semantic Role Labeling, since grouping verb arguments and measuring their plausibility increases performance, as shown by Merlo and Van Der Plas (2009) and Deschacht and Moens (2009). Some other applications are metaphora recognition, since we are able to know common usages of arguments, and an uncommon usage would suggest its presence, or a coherence mistake (v. gr. to drink the moon in a glass). Malapropism detection can use the measure of the plausibility of an argument to determine misuses of words (Bolshakov, 2005) as in hysteric center, instead of historic center; density has brought me to you; It looks like a tattoo subject; and Why you say that with ironing?. 2 Models for Plausible Argument Estimation We explore the models proposed in Calvo et al. (2009a, 2009b);"
Y09-2021,P98-2127,0,0.233288,"roposal model overcomes this and results are shown in Section 3.4. Afterwards, we explore how our model works with ngrams instead of dependency triple relationships in experiments from Section 3.5. Finally, we draw our conclusions in Section 4 mentioning future work and possible applications. 1.1 Possible applications Correct Plausible Verb Argument identification can be used for several tasks, such as improving parsing. Since latent variables group the kind of arguments expected for a sentence, it is possible to infer the meaning of unknown words, as in the well-known example about tezguino (Lin, 1998b) where it is possible to know what is tezguino from the sentences: A bottle of tezguino is on the table; Everybody likes tezguino; Tezguino makes you drunk; and We make tezguino out of corn. Another application is Semantic Role Labeling, since grouping verb arguments and measuring their plausibility increases performance, as shown by Merlo and Van Der Plas (2009) and Deschacht and Moens (2009). Some other applications are metaphora recognition, since we are able to know common usages of arguments, and an uncommon usage would suggest its presence, or a coherence mistake (v. gr. to drink the m"
Y09-2021,P09-1033,0,0.0259356,"Missing"
Y09-2021,P09-1070,0,0.0155601,"detailed categories and examples that fit these categories. For this purpose, recent works take advantage ∗ We thank the support of the Japanese Government and the Mexican Government (SNI, SIP-IPN, COFAA-IPN, and PIFI-IPN). Second author is a JSPS fellow. We also thank our anonymous reviewers for their useful comments and discussion. Copyright 2009 by Hiram Calvo, Kentaro Inui, and Yuji Matsumoto 23rd Pacific Asia Conference on Language, Information and Computation, pages 622–629 622 of existing manually crafted resources such as WordNet, Wikipedia, FrameNet, VerbNet or PropBank. For example, Reisinger and Paşca (2009) annotate existing WordNet concepts with attributes, and extend is-a relations, based on Latent Dirichlet Allocation on Web documents and Wikipedia. Yamada et al. (2009) explore extracting hyponym relations from Wikipedia using pattern-based discovery and distributional similarity clustering. Nevertheless, the approach of using handcrafted resources prevents from obtaining information for languages where those resources do not exist. Calvo et al. (2009a, 2009b) propose a non language-dependent model based on K-Nearest Neighbors for calculating the plausibility of candidate arguments given one"
Y09-2021,W03-1011,0,0.0171257,"-algorithm of this model. for each triple &lt;v,a1,a2&gt; with observed count c, for each argument a1,a2 Find its k most similar words a1s1…a1sk, a2s1…a2sk with similarities s1s1, ..., s1sk and s2s1,...,s2sk. Add votes for each new triple &lt;v,a1si,a2sj&gt; += c·s1si·s2sj Figure 1: Pseudo-algorithm for the K-nearest neighbors DLM algorithm As votes are accumulative, triples that have words with many similar words will get more votes. Common similarity measures range from Euclidean distance, cosine and Jaccard’s coefficient (Lee, 1999), to measures such as Hindle’s measure and Lin’s measure (Lin, 1998a). Weeds and Weir (2003) show that the distributional measure with best performance is the Lin similarity, so this measure is used for smoothing the co-occurrence space, following the procedure as described by Lin (1998a). 2.2 PLSI – Probabilistic Latent Semantic Indexing The probabilistic Latent Semantic Indexing Model (PLSI) was introduced in Hofmann (1999), arose from Latent Semantic Indexing (Deerwester et al., 1990). The model attempts to associate an unobserved class variable z∈Z={z1, ..., zk}, (in our case a generalization of correlation of the co-occurrence of v,a1 and a2), and two sets of observables: argume"
Y09-2021,D09-1097,0,0.0408554,"and examples that fit these categories. For this purpose, recent works take advantage ∗ We thank the support of the Japanese Government and the Mexican Government (SNI, SIP-IPN, COFAA-IPN, and PIFI-IPN). Second author is a JSPS fellow. We also thank our anonymous reviewers for their useful comments and discussion. Copyright 2009 by Hiram Calvo, Kentaro Inui, and Yuji Matsumoto 23rd Pacific Asia Conference on Language, Information and Computation, pages 622–629 622 of existing manually crafted resources such as WordNet, Wikipedia, FrameNet, VerbNet or PropBank. For example, Reisinger and Paşca (2009) annotate existing WordNet concepts with attributes, and extend is-a relations, based on Latent Dirichlet Allocation on Web documents and Wikipedia. Yamada et al. (2009) explore extracting hyponym relations from Wikipedia using pattern-based discovery and distributional similarity clustering. Nevertheless, the approach of using handcrafted resources prevents from obtaining information for languages where those resources do not exist. Calvo et al. (2009a, 2009b) propose a non language-dependent model based on K-Nearest Neighbors for calculating the plausibility of candidate arguments given one"
Y09-2021,P09-1048,0,\N,Missing
Y09-2021,C98-2122,0,\N,Missing
Y09-2021,P99-1004,0,\N,Missing
Y09-2039,W05-0707,0,0.0710566,"Missing"
Y09-2039,W95-0101,0,0.22242,"Missing"
Y09-2039,P08-2009,0,0.0292629,"hologically rich language. The use of affixes improved tagging accuracy but may not be the only factor contributing to a successful POS tagging. As explained in Section 5.1, the tag set used also affected the result of tagging. In a language where inflectional complexity is an issue in tagging, a more refined set of tag that captures this complex characteristic may be considered. Although coarse-grained set of tags may have less margin of errors and may avoid the confusions on how a word should be tagged (Mistica and Baldwin, 2009), fine-grained information can improve a tagger’s performance (Dredze and Wallenberg, 2008). Tagging words to its semantic details can be useful in future syntactic-semantic analysis for the language. The tag set in this research defines specific tags according to the features or classification of its higher level category. For example, NNC and NNP are tags for common noun and proper noun, respectively, while VBTS and VBOF are specific tags for time perfective and object focus verbs, respectively. Having two separate tags for verb features (Section 2.1) had resulted to errors in tagging (Section 5.1). Although the specific tags were defined according to some features of the word cat"
Y09-2039,W04-3230,1,0.870761,"Missing"
Y09-2039,N09-2065,0,0.016413,"w that morphological information can help improve tagging, especially for Tagalog, a morphologically rich language. The use of affixes improved tagging accuracy but may not be the only factor contributing to a successful POS tagging. As explained in Section 5.1, the tag set used also affected the result of tagging. In a language where inflectional complexity is an issue in tagging, a more refined set of tag that captures this complex characteristic may be considered. Although coarse-grained set of tags may have less margin of errors and may avoid the confusions on how a word should be tagged (Mistica and Baldwin, 2009), fine-grained information can improve a tagger’s performance (Dredze and Wallenberg, 2008). Tagging words to its semantic details can be useful in future syntactic-semantic analysis for the language. The tag set in this research defines specific tags according to the features or classification of its higher level category. For example, NNC and NNP are tags for common noun and proper noun, respectively, while VBTS and VBOF are specific tags for time perfective and object focus verbs, respectively. Having two separate tags for verb features (Section 2.1) had resulted to errors in tagging (Secti"
Y09-2039,W96-0213,0,0.231509,"Missing"
Y09-2039,C04-1081,0,\N,Missing
Y11-1036,C08-1046,1,0.836562,"its dependents. When two words are connected by a dependency relation, one takes the role of the head and the other is the dependent (Covington, 2001). The straightforwardness of dependency analysis has been used in other NLP tasks such as word alignment (Ma et al., 2008) and semantic role labeling (Hacioglu, 2004). Dependency parsers developed were either graph-based (McDonald and Pereira, 2006) or transitionbased parsers (Nivre and Scholz, 2004; Yamada and Matsumoto, 2003). Syntactic analysis for different languages also use dependency parsers such as in Japanese (Kudo and Matsumoto, 2001; Iwatate et al., 2008), English (Nivre and Scholz, 2004), Chinese (Chen et al., 2009; Yu et al., 2008) to mention some. The task of dependency-based parsing is to construct structure for an input sentence and identify the syntactic head of the word in the sentence(Nivre et al., 2007). Figure 2 shows a sentence with dependency arcs (below) and its equivalent dependency tree structure(top). Each word in the sentence has one syntactic head except for the root or the head of the sentence. Dependency trees can be projective or non-projective. It is projective if no arcs cross over when drawn above the sentence, while it"
Y11-1036,C04-1186,0,0.0410211,"ntence. Sections 5 describes the experiment set-up and results. Finally, we discuss some issues found in the parsing results in section 6. Figure 2: An example of a dependency structure. (Source: McDonald and Pereira, 2006) 344 2 Dependency Parsing Dependency analysis creates a link from a word to its dependents. When two words are connected by a dependency relation, one takes the role of the head and the other is the dependent (Covington, 2001). The straightforwardness of dependency analysis has been used in other NLP tasks such as word alignment (Ma et al., 2008) and semantic role labeling (Hacioglu, 2004). Dependency parsers developed were either graph-based (McDonald and Pereira, 2006) or transitionbased parsers (Nivre and Scholz, 2004; Yamada and Matsumoto, 2003). Syntactic analysis for different languages also use dependency parsers such as in Japanese (Kudo and Matsumoto, 2001; Iwatate et al., 2008), English (Nivre and Scholz, 2004), Chinese (Chen et al., 2009; Yu et al., 2008) to mention some. The task of dependency-based parsing is to construct structure for an input sentence and identify the syntactic head of the word in the sentence(Nivre et al., 2007). Figure 2 shows a sentence with d"
Y11-1036,W02-2016,1,0.780327,"Missing"
Y11-1036,W08-0409,0,0.0258849,"s the how heads are assigned to words in a sentence. Sections 5 describes the experiment set-up and results. Finally, we discuss some issues found in the parsing results in section 6. Figure 2: An example of a dependency structure. (Source: McDonald and Pereira, 2006) 344 2 Dependency Parsing Dependency analysis creates a link from a word to its dependents. When two words are connected by a dependency relation, one takes the role of the head and the other is the dependent (Covington, 2001). The straightforwardness of dependency analysis has been used in other NLP tasks such as word alignment (Ma et al., 2008) and semantic role labeling (Hacioglu, 2004). Dependency parsers developed were either graph-based (McDonald and Pereira, 2006) or transitionbased parsers (Nivre and Scholz, 2004; Yamada and Matsumoto, 2003). Syntactic analysis for different languages also use dependency parsers such as in Japanese (Kudo and Matsumoto, 2001; Iwatate et al., 2008), English (Nivre and Scholz, 2004), Chinese (Chen et al., 2009; Yu et al., 2008) to mention some. The task of dependency-based parsing is to construct structure for an input sentence and identify the syntactic head of the word in the sentence(Nivre et"
Y11-1036,H05-1066,0,0.053066,"entence and identify the syntactic head of the word in the sentence(Nivre et al., 2007). Figure 2 shows a sentence with dependency arcs (below) and its equivalent dependency tree structure(top). Each word in the sentence has one syntactic head except for the root or the head of the sentence. Dependency trees can be projective or non-projective. It is projective if no arcs cross over when drawn above the sentence, while it is non-projective when there are edges crossing each other. Non-projective dependency is a result of parsing languages with more flexible word order such as German and Czech(McDonald and Pereira, 2005). In this work, we applied the graph-based maximum spanning tree(MST) parser of McDonald and Pereira (2006) on Tagalog language. In MST, a sentence as x = x1 · · · xn is represented as a set of vertices V = v1 · · · vn in a weighted directed graph G. A dependency tree y of a sentence is a set of edges (i, j), where (i, j) ∈ E in the graph G, E ⊆ [1 : n] × [1 : n] and edge (i, j) exists if there is a dependency from word xi to word xj in the sentence. The maximum spanning P tree of graph G is a rooted tree y ⊆ E that maximizes the score (i,j)∈y s(i, j), where s(i, j) is the score of a dependenc"
Y11-1036,E06-1011,0,0.418585,"ll as complete scores. We also look into the number of sentence heads correctly predicted. The free word order nature of Tagalog may put a verb at the initial position or after a nominal or adverbial element in the sentence. We briefly introduce dependency parsing in section 2, section 3 briefly explains sentence structures in Tagalog and section 4 discusses the how heads are assigned to words in a sentence. Sections 5 describes the experiment set-up and results. Finally, we discuss some issues found in the parsing results in section 6. Figure 2: An example of a dependency structure. (Source: McDonald and Pereira, 2006) 344 2 Dependency Parsing Dependency analysis creates a link from a word to its dependents. When two words are connected by a dependency relation, one takes the role of the head and the other is the dependent (Covington, 2001). The straightforwardness of dependency analysis has been used in other NLP tasks such as word alignment (Ma et al., 2008) and semantic role labeling (Hacioglu, 2004). Dependency parsers developed were either graph-based (McDonald and Pereira, 2006) or transitionbased parsers (Nivre and Scholz, 2004; Yamada and Matsumoto, 2003). Syntactic analysis for different languages"
Y11-1036,P08-1108,0,0.033645,"the sentence instead of the verb. The nominative argument marked by determiner ang becomes a sibling of the verb. Sentences with non-verbal predicates follow the basic head-modifier rule in case it is adjectival or PP predicates, making the head of the modified phrase the head of the sentence. 5 5.1 Experiment Data The sentences used in the experiment were extracted from POS annotated Tagalog corpora consisting of novels and short articles. We only chose sentences of lengths L, for 4 ≥ L ≤ 20. Longer sentences have tendencies of lower accuracies while shorter sentences tend to perform better (McDonald and Nivre, 2008). A total of 2,741 sentences were used for the experiments. The total number of sentences were then divided into 5 parts for a 5-fold cross validation. Table 1 gives the data distribution in the 5-fold cross validation while Table 2 shows the distribution of the Test and Training data by sentence lengths. Compared to other treebanks, Tagalog data set is relatively small; however, this is the start in building Tagalog Dependency Treebank for future work. Table 1: 5-fold Cross Validation Data Distribution Data Test Train 1 548 2,192 2 548 2,192 3 548 2,192 4 546 2,194 5 550 2,190 Number of Sente"
Y11-1036,C04-1010,0,0.0800797,"section 6. Figure 2: An example of a dependency structure. (Source: McDonald and Pereira, 2006) 344 2 Dependency Parsing Dependency analysis creates a link from a word to its dependents. When two words are connected by a dependency relation, one takes the role of the head and the other is the dependent (Covington, 2001). The straightforwardness of dependency analysis has been used in other NLP tasks such as word alignment (Ma et al., 2008) and semantic role labeling (Hacioglu, 2004). Dependency parsers developed were either graph-based (McDonald and Pereira, 2006) or transitionbased parsers (Nivre and Scholz, 2004; Yamada and Matsumoto, 2003). Syntactic analysis for different languages also use dependency parsers such as in Japanese (Kudo and Matsumoto, 2001; Iwatate et al., 2008), English (Nivre and Scholz, 2004), Chinese (Chen et al., 2009; Yu et al., 2008) to mention some. The task of dependency-based parsing is to construct structure for an input sentence and identify the syntactic head of the word in the sentence(Nivre et al., 2007). Figure 2 shows a sentence with dependency arcs (below) and its equivalent dependency tree structure(top). Each word in the sentence has one syntactic head except for"
Y11-1036,W10-1401,0,0.0371807,"Missing"
Y11-1036,W03-3023,1,0.612744,"example of a dependency structure. (Source: McDonald and Pereira, 2006) 344 2 Dependency Parsing Dependency analysis creates a link from a word to its dependents. When two words are connected by a dependency relation, one takes the role of the head and the other is the dependent (Covington, 2001). The straightforwardness of dependency analysis has been used in other NLP tasks such as word alignment (Ma et al., 2008) and semantic role labeling (Hacioglu, 2004). Dependency parsers developed were either graph-based (McDonald and Pereira, 2006) or transitionbased parsers (Nivre and Scholz, 2004; Yamada and Matsumoto, 2003). Syntactic analysis for different languages also use dependency parsers such as in Japanese (Kudo and Matsumoto, 2001; Iwatate et al., 2008), English (Nivre and Scholz, 2004), Chinese (Chen et al., 2009; Yu et al., 2008) to mention some. The task of dependency-based parsing is to construct structure for an input sentence and identify the syntactic head of the word in the sentence(Nivre et al., 2007). Figure 2 shows a sentence with dependency arcs (below) and its equivalent dependency tree structure(top). Each word in the sentence has one syntactic head except for the root or the head of the s"
Y11-1036,C08-1132,0,0.017362,"role of the head and the other is the dependent (Covington, 2001). The straightforwardness of dependency analysis has been used in other NLP tasks such as word alignment (Ma et al., 2008) and semantic role labeling (Hacioglu, 2004). Dependency parsers developed were either graph-based (McDonald and Pereira, 2006) or transitionbased parsers (Nivre and Scholz, 2004; Yamada and Matsumoto, 2003). Syntactic analysis for different languages also use dependency parsers such as in Japanese (Kudo and Matsumoto, 2001; Iwatate et al., 2008), English (Nivre and Scholz, 2004), Chinese (Chen et al., 2009; Yu et al., 2008) to mention some. The task of dependency-based parsing is to construct structure for an input sentence and identify the syntactic head of the word in the sentence(Nivre et al., 2007). Figure 2 shows a sentence with dependency arcs (below) and its equivalent dependency tree structure(top). Each word in the sentence has one syntactic head except for the root or the head of the sentence. Dependency trees can be projective or non-projective. It is projective if no arcs cross over when drawn above the sentence, while it is non-projective when there are edges crossing each other. Non-projective depe"
Y11-1036,D07-1096,0,\N,Missing
Y13-1014,P06-1032,0,0.0692453,"d Yuji Matsumoto 27th Pacific Asia Conference on Language, Information, and Computation pages 163－172 PACLIC-27 2012)4 (540 files). Our work aims to add error type annotation on learner corpora. Unlike previous research which depend on entirely manual annotation, we focus on semi-automatic annotation method to reduce human cost and to improve consistency in annotation. in Section 5. 2 Previous Work Automatic Error Detection Systems: In the writing of learners of English, automatic grammatical error detection is used for spelling error (Mays et al., 1991), countable or uncountable noun errors (Brockett et al., 2006; Nagata et al., 2006), prepositional errors (De Felice and Pulman, 2008; Tetreault and Chodorow, 2008; Gamon et al., 2008) and article errors (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008). Sun et al. (2007) focus on discriminating between erroneous and correct sentences without considering error types. As for texts by Japanese learners, most of the research focuses on correcting errors with particles (postpositions) (Imaeda et al., 2003; Suzuki and Toutanova, 2006; Nampo et al., 2007; Oyama and Matsumoto, 2010; Ohki et al., 2011; Imamura et al., 2012). Besides, Mizumoto e"
Y13-1014,C08-1022,0,0.102905,"Missing"
Y13-1014,I08-1059,0,0.0188571,"iles). Our work aims to add error type annotation on learner corpora. Unlike previous research which depend on entirely manual annotation, we focus on semi-automatic annotation method to reduce human cost and to improve consistency in annotation. in Section 5. 2 Previous Work Automatic Error Detection Systems: In the writing of learners of English, automatic grammatical error detection is used for spelling error (Mays et al., 1991), countable or uncountable noun errors (Brockett et al., 2006; Nagata et al., 2006), prepositional errors (De Felice and Pulman, 2008; Tetreault and Chodorow, 2008; Gamon et al., 2008) and article errors (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008). Sun et al. (2007) focus on discriminating between erroneous and correct sentences without considering error types. As for texts by Japanese learners, most of the research focuses on correcting errors with particles (postpositions) (Imaeda et al., 2003; Suzuki and Toutanova, 2006; Nampo et al., 2007; Oyama and Matsumoto, 2010; Ohki et al., 2011; Imamura et al., 2012). Besides, Mizumoto et al. (2011) consider error correction in the language learners’ writing handling any error types. As for automatic error t"
Y13-1014,P12-2076,0,0.19364,"untable noun errors (Brockett et al., 2006; Nagata et al., 2006), prepositional errors (De Felice and Pulman, 2008; Tetreault and Chodorow, 2008; Gamon et al., 2008) and article errors (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008). Sun et al. (2007) focus on discriminating between erroneous and correct sentences without considering error types. As for texts by Japanese learners, most of the research focuses on correcting errors with particles (postpositions) (Imaeda et al., 2003; Suzuki and Toutanova, 2006; Nampo et al., 2007; Oyama and Matsumoto, 2010; Ohki et al., 2011; Imamura et al., 2012). Besides, Mizumoto et al. (2011) consider error correction in the language learners’ writing handling any error types. As for automatic error type classification, Swanson and Yamangil (2012) deal with 15 error type classification in English learners’ essays in the Cambridge Learner Corpus（CLC 1 ). However, they did not report an inter-corpus evaluation. Japanese Language Learners’ Corpora: Japanese language learner corpora include Taiyaku DB, which is a multilingual database of Japanese learners’ essays compiled by the National Institute of Japanese Language 2 consisting of 1,565 essays writt"
Y13-1014,I11-1017,1,0.945977,"al., 2006; Nagata et al., 2006), prepositional errors (De Felice and Pulman, 2008; Tetreault and Chodorow, 2008; Gamon et al., 2008) and article errors (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008). Sun et al. (2007) focus on discriminating between erroneous and correct sentences without considering error types. As for texts by Japanese learners, most of the research focuses on correcting errors with particles (postpositions) (Imaeda et al., 2003; Suzuki and Toutanova, 2006; Nampo et al., 2007; Oyama and Matsumoto, 2010; Ohki et al., 2011; Imamura et al., 2012). Besides, Mizumoto et al. (2011) consider error correction in the language learners’ writing handling any error types. As for automatic error type classification, Swanson and Yamangil (2012) deal with 15 error type classification in English learners’ essays in the Cambridge Learner Corpus（CLC 1 ). However, they did not report an inter-corpus evaluation. Japanese Language Learners’ Corpora: Japanese language learner corpora include Taiyaku DB, which is a multilingual database of Japanese learners’ essays compiled by the National Institute of Japanese Language 2 consisting of 1,565 essays written by learners from 15 different"
Y13-1014,P07-1011,0,0.0120641,"on entirely manual annotation, we focus on semi-automatic annotation method to reduce human cost and to improve consistency in annotation. in Section 5. 2 Previous Work Automatic Error Detection Systems: In the writing of learners of English, automatic grammatical error detection is used for spelling error (Mays et al., 1991), countable or uncountable noun errors (Brockett et al., 2006; Nagata et al., 2006), prepositional errors (De Felice and Pulman, 2008; Tetreault and Chodorow, 2008; Gamon et al., 2008) and article errors (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008). Sun et al. (2007) focus on discriminating between erroneous and correct sentences without considering error types. As for texts by Japanese learners, most of the research focuses on correcting errors with particles (postpositions) (Imaeda et al., 2003; Suzuki and Toutanova, 2006; Nampo et al., 2007; Oyama and Matsumoto, 2010; Ohki et al., 2011; Imamura et al., 2012). Besides, Mizumoto et al. (2011) consider error correction in the language learners’ writing handling any error types. As for automatic error type classification, Swanson and Yamangil (2012) deal with 15 error type classification in English learner"
Y13-1014,P06-1132,0,0.229623,"grammatical error detection is used for spelling error (Mays et al., 1991), countable or uncountable noun errors (Brockett et al., 2006; Nagata et al., 2006), prepositional errors (De Felice and Pulman, 2008; Tetreault and Chodorow, 2008; Gamon et al., 2008) and article errors (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008). Sun et al. (2007) focus on discriminating between erroneous and correct sentences without considering error types. As for texts by Japanese learners, most of the research focuses on correcting errors with particles (postpositions) (Imaeda et al., 2003; Suzuki and Toutanova, 2006; Nampo et al., 2007; Oyama and Matsumoto, 2010; Ohki et al., 2011; Imamura et al., 2012). Besides, Mizumoto et al. (2011) consider error correction in the language learners’ writing handling any error types. As for automatic error type classification, Swanson and Yamangil (2012) deal with 15 error type classification in English learners’ essays in the Cambridge Learner Corpus（CLC 1 ). However, they did not report an inter-corpus evaluation. Japanese Language Learners’ Corpora: Japanese language learner corpora include Taiyaku DB, which is a multilingual database of Japanese learners’ essays c"
Y13-1014,N12-1037,0,0.0291488,"(Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008). Sun et al. (2007) focus on discriminating between erroneous and correct sentences without considering error types. As for texts by Japanese learners, most of the research focuses on correcting errors with particles (postpositions) (Imaeda et al., 2003; Suzuki and Toutanova, 2006; Nampo et al., 2007; Oyama and Matsumoto, 2010; Ohki et al., 2011; Imamura et al., 2012). Besides, Mizumoto et al. (2011) consider error correction in the language learners’ writing handling any error types. As for automatic error type classification, Swanson and Yamangil (2012) deal with 15 error type classification in English learners’ essays in the Cambridge Learner Corpus（CLC 1 ). However, they did not report an inter-corpus evaluation. Japanese Language Learners’ Corpora: Japanese language learner corpora include Taiyaku DB, which is a multilingual database of Japanese learners’ essays compiled by the National Institute of Japanese Language 2 consisting of 1,565 essays written by learners from 15 different countries. The KY corpus (Kamata and Yamauchi, 1999) has spoken data of Japanese language learners at different proficiency levels. There are several Japanese"
Y13-1014,C08-1109,0,0.0266031,"63－172 PACLIC-27 2012)4 (540 files). Our work aims to add error type annotation on learner corpora. Unlike previous research which depend on entirely manual annotation, we focus on semi-automatic annotation method to reduce human cost and to improve consistency in annotation. in Section 5. 2 Previous Work Automatic Error Detection Systems: In the writing of learners of English, automatic grammatical error detection is used for spelling error (Mays et al., 1991), countable or uncountable noun errors (Brockett et al., 2006; Nagata et al., 2006), prepositional errors (De Felice and Pulman, 2008; Tetreault and Chodorow, 2008; Gamon et al., 2008) and article errors (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008). Sun et al. (2007) focus on discriminating between erroneous and correct sentences without considering error types. As for texts by Japanese learners, most of the research focuses on correcting errors with particles (postpositions) (Imaeda et al., 2003; Suzuki and Toutanova, 2006; Nampo et al., 2007; Oyama and Matsumoto, 2010; Ohki et al., 2011; Imamura et al., 2012). Besides, Mizumoto et al. (2011) consider error correction in the language learners’ writing handling any error types. As"
Y14-1056,guthrie-etal-2006-closer,0,0.396919,"Missing"
Y14-1056,H93-1016,0,0.837085,"Missing"
Y14-1056,J07-2003,0,\N,Missing
Y15-1051,H05-1025,0,0.619167,"ternative to normal n-gram method. In this paper, we use directional information to make HWS models more syntactically appropriate so that higher performance can be achieved. For evaluation, we perform intrinsic and extrinsic experiments, both verify the effectiveness of our improved model. 1 Introduction Probabilistic Language Modeling is a fundamental research direction of Natural Language Processing. It is widely used in many applications such as machine translation (Brown et al., 1990), spelling correction (Mays et al., 1991), speech recognition (Rabiner and Juang, 1993), word prediction (Bickel et al., 2005) and so on. Most research about Probabilistic Language Modeling, such as back-off (Katz,1987), Kneser-Ney (Kneser and Ney, 1995), and modified Kneser-Ney (Chen and Goodman, 1999), only focus on smoothing methods because they all take n-gram approach (Shannon, 1948) as a default setting for extracting word sequences from a sentence. Yet even with 30 years worth of newswire text, more than one third of all trigrams are still unseen (Allison et al., 2005), which cannot be distinguished accurately even using a high-performance smoothing method such as modified Kneser-Ney (abbreviated as MKN). It i"
Y15-1051,J90-2002,0,0.488037,"hich uses word frequency information to convert raw sentences into special n-gram sequences, can be viewed as an effective alternative to normal n-gram method. In this paper, we use directional information to make HWS models more syntactically appropriate so that higher performance can be achieved. For evaluation, we perform intrinsic and extrinsic experiments, both verify the effectiveness of our improved model. 1 Introduction Probabilistic Language Modeling is a fundamental research direction of Natural Language Processing. It is widely used in many applications such as machine translation (Brown et al., 1990), spelling correction (Mays et al., 1991), speech recognition (Rabiner and Juang, 1993), word prediction (Bickel et al., 2005) and so on. Most research about Probabilistic Language Modeling, such as back-off (Katz,1987), Kneser-Ney (Kneser and Ney, 1995), and modified Kneser-Ney (Chen and Goodman, 1999), only focus on smoothing methods because they all take n-gram approach (Shannon, 1948) as a default setting for extracting word sequences from a sentence. Yet even with 30 years worth of newswire text, more than one third of all trigrams are still unseen (Allison et al., 2005), which cannot be"
Y15-1051,P97-1064,0,0.831912,"ords tend to be lowfrequency because the connection of ‘as...as’ is still interrupted. On the contrary, the HWS model extracts sequences in a discontinuous way, even ‘soon’ is replaced by another word, the expression ‘as...as’ won’t be affected. This is how the HWS models relieve the data sparseness problem. It unsupervisedly construct a hierarchical structure to adjust the word sequence so that irrelevant words can be filtered out from contexts and long distance information can be used for predicting the next word. On this point, it has something in common with structured language model 450 (Chelba, 1997), which firstly introduced parsing into language modeling. The significant difference is, structured language model is based on CFG parsing structures, while HWS model is based on patternoriented structures. The experimental results reported by Wu and Matsumoto (2014) indicated that HWS model keeps better balance between coverage and usage than normal n-gram and skip-gram models (Guthrie, 2006), which means that more valid sequence patterns can be extracted in this approach. However, the discontinuity of HWS models also brings a disadvantage. In normal n-gram models, since the generation of wo"
Y15-1051,P02-1040,0,0.0916168,"Missing"
Y15-1051,P14-1108,0,0.446538,"Missing"
Y15-1051,2006.amta-papers.25,0,0.0400952,"the 139761 English sentences to train language models. With these models, 50-best translation candidates can be reranked. According to these reranking results, the performance of machine translation system can be evaluated, which also means, the language models can be evaluated indirectly. We use following two measures for evaluating reranking results. BLEU (Papineni et al., 2002): BLEU score measures how many words overlap in a given candidate translation when compared to a reference translation, which provides some insight into how good the fluency of the output from an engine will be. TER (Snover et al., 2006): TER score measures the number of edits required to change a system output into one of the references, which gives an indication as to how much post-editing will be required on the translated output of an engine. As shown in Table 2, since the results performed by our implementation (3-gram+MKN) is almost the same as that performed by existing language model toolkits IRSTLM8 and SRILM9 , we believe that our implementation is correct. Based on the results, considering both BLEU and TER score, DHWS3-gram model using GLM smoothing outperforms other models. 5 Conclusion We proposed an improved hi"
Y15-1051,Y14-1056,1,0.645009,"s because they all take n-gram approach (Shannon, 1948) as a default setting for extracting word sequences from a sentence. Yet even with 30 years worth of newswire text, more than one third of all trigrams are still unseen (Allison et al., 2005), which cannot be distinguished accurately even using a high-performance smoothing method such as modified Kneser-Ney (abbreviated as MKN). It is better to make these unseen sequences actually be observed rather than to leave them to smoothing method directly. For the purpose of extracting more valid word sequences and relieving data sparsity problem, Wu and Matsumoto (2014) proposed a heuristic approach to convert a sentence into a hierarchical word sequence (abbreviated as HWS) structure, by which special ngrams can be achieved. In this paper, we improve HWS models by adding directional information for achieving higher performance. This paper is organized as follows. In Section 2, we give a complete review of the HWS language model. We present our improved HWS model in Section 3. In Section 4, we show the effectiveness of our model by several experiments. Finally, we summarize our findings in Section 5. 2 Review of HWS Language Model The HWS language model is d"
Y15-1051,guthrie-etal-2006-closer,0,\N,Missing
Y15-2015,D09-1049,0,0.0297569,"der and Smith, 2015). In MWE identification tasks, previous work integrated MWE recognition into POS tagging (Constant and Sigogne, 2011). An MWE identification method using Conditional Random Fields was also presented together with the data set (Shigeto et al., 2013). A joint model of MWE identification and constituency parsing was proposed (Constant et al., 2012). They allocated IOB2 tags to MWEs and used MWEs as special features when reranking the parse tree. However, it is difficult for these methods to detect discontinuous MWEs. In contrast, as for methods that can handle separable MWEs, Boukobza and Rappoport (2009) tackled MWE detection on specific MWE types with a binary classification method. In a framework of a sequential labeling method for MWE detection, a new IOB tag scheme, which is augmented to capture discontinuous MWEs and distinguish strong MWEs from week MWEs, was presented (Schneider et al., 2014a). Here strong MWEs indicate the expresJJ 17 ad hoc PRP 14 anything else 126 Other 6 no way sion which has strong idiomaticity, and week one indicate the expression which is to more likely to be a compositional phrase or collocation. Additionally, words between components of MWEs are called gaps, a"
Y15-2015,W11-0809,0,0.0137604,"ork MWEs can be roughly divided into two categories, separable and non-separable (or fixed) MWEs. Previous work annotated fixed MWEs on Penn Treebank, where they used syntactic trees of Penn Treebank and an MWE dictionary that is extracted from Wiktionary (Shigeto et al., 2013). In Schneider et al. (2014b), they annotated all types of MWEs on English Web Treebank completely by hand. Afterward, they added to supersenses, which mean coarsegrained semantic classes of lexical units (Schneider and Smith, 2015). In MWE identification tasks, previous work integrated MWE recognition into POS tagging (Constant and Sigogne, 2011). An MWE identification method using Conditional Random Fields was also presented together with the data set (Shigeto et al., 2013). A joint model of MWE identification and constituency parsing was proposed (Constant et al., 2012). They allocated IOB2 tags to MWEs and used MWEs as special features when reranking the parse tree. However, it is difficult for these methods to detect discontinuous MWEs. In contrast, as for methods that can handle separable MWEs, Boukobza and Rappoport (2009) tackled MWE detection on specific MWE types with a binary classification method. In a framework of a sequen"
Y15-2015,P12-1022,0,0.0128479,"cted from Wiktionary (Shigeto et al., 2013). In Schneider et al. (2014b), they annotated all types of MWEs on English Web Treebank completely by hand. Afterward, they added to supersenses, which mean coarsegrained semantic classes of lexical units (Schneider and Smith, 2015). In MWE identification tasks, previous work integrated MWE recognition into POS tagging (Constant and Sigogne, 2011). An MWE identification method using Conditional Random Fields was also presented together with the data set (Shigeto et al., 2013). A joint model of MWE identification and constituency parsing was proposed (Constant et al., 2012). They allocated IOB2 tags to MWEs and used MWEs as special features when reranking the parse tree. However, it is difficult for these methods to detect discontinuous MWEs. In contrast, as for methods that can handle separable MWEs, Boukobza and Rappoport (2009) tackled MWE detection on specific MWE types with a binary classification method. In a framework of a sequential labeling method for MWE detection, a new IOB tag scheme, which is augmented to capture discontinuous MWEs and distinguish strong MWEs from week MWEs, was presented (Schneider et al., 2014a). Here strong MWEs indicate the expr"
Y15-2015,W08-1301,0,0.0708201,"Missing"
Y15-2015,I13-1168,0,0.0200682,"dentification performance than rule-based and sequence-labeling methods. 1 Introduction Multiword Expressions (MWEs) are roughly defined as those that have “idiosyncratic interpretations that cross word boundaries (or spaces)” (Sag et al., 2001). Vocabulary sizes of single words and MWEs have roughly the same size, thus MWE identification is a crucial issue for deep analysis of natural language text. Indeed, it has been shown in the literature that MWE identification helps various NLP applications, such as information retrieval, machine translation, and syntactic parsing (Newman et al., 2012; Ghoneim and Diab, 2013; Nivre and Nilsson, 2004). Since huge cost is necessary for annotation, there are few corpora that are sufficiently annotated for English MWEs. Schneider et al. (2014b) constructed an MWE-annotated corpus on English Web Treebank, and proposed a sequen(a) We bring our computers up. (b) She goes over the question. (c) Someone goes over there. Figure 1: a positive instance (a) of a separable expression “bring up”, a positive instance (b) and a negative instance (c) of an inseparable expression “go over”. tial labeling method for MWE identification. However, they tried to manually cover the types"
Y15-2015,C12-1127,0,0.0134824,"achieves better MWE identification performance than rule-based and sequence-labeling methods. 1 Introduction Multiword Expressions (MWEs) are roughly defined as those that have “idiosyncratic interpretations that cross word boundaries (or spaces)” (Sag et al., 2001). Vocabulary sizes of single words and MWEs have roughly the same size, thus MWE identification is a crucial issue for deep analysis of natural language text. Indeed, it has been shown in the literature that MWE identification helps various NLP applications, such as information retrieval, machine translation, and syntactic parsing (Newman et al., 2012; Ghoneim and Diab, 2013; Nivre and Nilsson, 2004). Since huge cost is necessary for annotation, there are few corpora that are sufficiently annotated for English MWEs. Schneider et al. (2014b) constructed an MWE-annotated corpus on English Web Treebank, and proposed a sequen(a) We bring our computers up. (b) She goes over the question. (c) Someone goes over there. Figure 1: a positive instance (a) of a separable expression “bring up”, a positive instance (b) and a negative instance (c) of an inseparable expression “go over”. tial labeling method for MWE identification. However, they tried to"
Y15-2015,N15-1177,0,0.0127,"fficient for accurate MWE identification. We also investigate effective features for MWE identification. 2 Related Work MWEs can be roughly divided into two categories, separable and non-separable (or fixed) MWEs. Previous work annotated fixed MWEs on Penn Treebank, where they used syntactic trees of Penn Treebank and an MWE dictionary that is extracted from Wiktionary (Shigeto et al., 2013). In Schneider et al. (2014b), they annotated all types of MWEs on English Web Treebank completely by hand. Afterward, they added to supersenses, which mean coarsegrained semantic classes of lexical units (Schneider and Smith, 2015). In MWE identification tasks, previous work integrated MWE recognition into POS tagging (Constant and Sigogne, 2011). An MWE identification method using Conditional Random Fields was also presented together with the data set (Shigeto et al., 2013). A joint model of MWE identification and constituency parsing was proposed (Constant et al., 2012). They allocated IOB2 tags to MWEs and used MWEs as special features when reranking the parse tree. However, it is difficult for these methods to detect discontinuous MWEs. In contrast, as for methods that can handle separable MWEs, Boukobza and Rappopo"
Y15-2015,Q14-1016,0,0.119319,"tic interpretations that cross word boundaries (or spaces)” (Sag et al., 2001). Vocabulary sizes of single words and MWEs have roughly the same size, thus MWE identification is a crucial issue for deep analysis of natural language text. Indeed, it has been shown in the literature that MWE identification helps various NLP applications, such as information retrieval, machine translation, and syntactic parsing (Newman et al., 2012; Ghoneim and Diab, 2013; Nivre and Nilsson, 2004). Since huge cost is necessary for annotation, there are few corpora that are sufficiently annotated for English MWEs. Schneider et al. (2014b) constructed an MWE-annotated corpus on English Web Treebank, and proposed a sequen(a) We bring our computers up. (b) She goes over the question. (c) Someone goes over there. Figure 1: a positive instance (a) of a separable expression “bring up”, a positive instance (b) and a negative instance (c) of an inseparable expression “go over”. tial labeling method for MWE identification. However, they tried to manually cover the types of comprehensive MWEs, and the number of instances for each MWE was very limited. In this paper, we propose an efficient annotation method for separable MWEs appearin"
Y15-2015,schneider-etal-2014-comprehensive,0,0.513354,"tic interpretations that cross word boundaries (or spaces)” (Sag et al., 2001). Vocabulary sizes of single words and MWEs have roughly the same size, thus MWE identification is a crucial issue for deep analysis of natural language text. Indeed, it has been shown in the literature that MWE identification helps various NLP applications, such as information retrieval, machine translation, and syntactic parsing (Newman et al., 2012; Ghoneim and Diab, 2013; Nivre and Nilsson, 2004). Since huge cost is necessary for annotation, there are few corpora that are sufficiently annotated for English MWEs. Schneider et al. (2014b) constructed an MWE-annotated corpus on English Web Treebank, and proposed a sequen(a) We bring our computers up. (b) She goes over the question. (c) Someone goes over there. Figure 1: a positive instance (a) of a separable expression “bring up”, a positive instance (b) and a negative instance (c) of an inseparable expression “go over”. tial labeling method for MWE identification. However, they tried to manually cover the types of comprehensive MWEs, and the number of instances for each MWE was very limited. In this paper, we propose an efficient annotation method for separable MWEs appearin"
Y16-2004,W05-0909,0,0.184492,"Missing"
Y16-2004,H05-1025,0,0.0463682,". For evaluation, we compare our rearranged word sequences to conventional n-gram word sequences. Both intrinsic and extrinsic experiments verify that our framework can achieve better performance, proving that our method can be considered as a better alternative for ngram language models. 1 Introduction Probabilistic Language Modeling is a fundamental research direction of Natural Language Processing. It is widely used in various application such as machine translation (Brown et al., 1990), spelling correction (Mays et al., 1990), speech recognition (Rabiner and Juang, 1993), word prediction (Bickel et al., 2005) and so on. Most research about Probabilistic Language Modeling, such as Katz back-off (Katz, 1987), KneserNey (Kneser and Ney, 1995), and modiﬁed KneserNey (Chen and Goodman, 1999), only focus on smoothing methods because they all take the n-gram approach (Shannon, 1948) as a default setting for modeling word sequences in a sentence. Yet even with 30 years worth of newswire text, more than one third of all trigrams are still unseen (Allison et al., 2005), which cannot be distinguished accurately even using a high-performance smoothing method such as modiﬁed Kneser-Ney (abbreviated as MKN). An"
Y16-2004,N03-2002,0,0.0867063,"ncy because the connection of ‘as...as’ is still interrupted. On the contrary, the HWS model extracts sequences in a discontinuous way, even ‘soon’ is replaced by another word, the expression ‘as...as’ won’t be affected. This is how the HWS models relieve the data sparseness problem. The HWS model is essentially an n-gram language model based on a different assumption that a word depends upon its nearby high-frequency words instead of its preceding words. Different from other special n-gram language models, such as class-based language model (Brown et al., 1992), factored language model(FLM) (Bilmes and Kirchhoff, 2003), HWS language model doesn’t use any speciﬁc linguistic knowledge or any abstracted categories. Also, differs from dependency tree language models (Shen et al., 2008) (Chen et al., 2012), HWS language model constructs a tree structure in an unsupervised fashion. In HWS structure, word sequences are adjusted so that irrelevant words can be ﬁltered out from contexts and long distance information can be used PACLIC 30 Proceedings Figure 2: An Example of Generative Hierarchical Word Sequence Structure for predicting the next word, which make it more effective and ﬂexible in relieving the data spar"
Y16-2004,J90-2002,0,0.73567,"dopted to rearrange word sequences in a totally unsupervised fashion, which greatly increases the expandability of HWS models. For evaluation, we compare our rearranged word sequences to conventional n-gram word sequences. Both intrinsic and extrinsic experiments verify that our framework can achieve better performance, proving that our method can be considered as a better alternative for ngram language models. 1 Introduction Probabilistic Language Modeling is a fundamental research direction of Natural Language Processing. It is widely used in various application such as machine translation (Brown et al., 1990), spelling correction (Mays et al., 1990), speech recognition (Rabiner and Juang, 1993), word prediction (Bickel et al., 2005) and so on. Most research about Probabilistic Language Modeling, such as Katz back-off (Katz, 1987), KneserNey (Kneser and Ney, 1995), and modiﬁed KneserNey (Chen and Goodman, 1999), only focus on smoothing methods because they all take the n-gram approach (Shannon, 1948) as a default setting for modeling word sequences in a sentence. Yet even with 30 years worth of newswire text, more than one third of all trigrams are still unseen (Allison et al., 2005), which cannot"
Y16-2004,J92-4003,0,0.632591,"of ‘soon’ and its nearby words tend to be lowfrequency because the connection of ‘as...as’ is still interrupted. On the contrary, the HWS model extracts sequences in a discontinuous way, even ‘soon’ is replaced by another word, the expression ‘as...as’ won’t be affected. This is how the HWS models relieve the data sparseness problem. The HWS model is essentially an n-gram language model based on a different assumption that a word depends upon its nearby high-frequency words instead of its preceding words. Different from other special n-gram language models, such as class-based language model (Brown et al., 1992), factored language model(FLM) (Bilmes and Kirchhoff, 2003), HWS language model doesn’t use any speciﬁc linguistic knowledge or any abstracted categories. Also, differs from dependency tree language models (Shen et al., 2008) (Chen et al., 2012), HWS language model constructs a tree structure in an unsupervised fashion. In HWS structure, word sequences are adjusted so that irrelevant words can be ﬁltered out from contexts and long distance information can be used PACLIC 30 Proceedings Figure 2: An Example of Generative Hierarchical Word Sequence Structure for predicting the next word, which ma"
Y16-2004,P97-1064,0,0.145621,"ries. Also, differs from dependency tree language models (Shen et al., 2008) (Chen et al., 2012), HWS language model constructs a tree structure in an unsupervised fashion. In HWS structure, word sequences are adjusted so that irrelevant words can be ﬁltered out from contexts and long distance information can be used PACLIC 30 Proceedings Figure 2: An Example of Generative Hierarchical Word Sequence Structure for predicting the next word, which make it more effective and ﬂexible in relieving the data sparseness problem. On this point, it has something in common with structured language model (Chelba, 1997), which ﬁrstly introduced parsing into language modeling. The signiﬁcant difference is, structured language model is based on CFG parsing structures, while HWS model is based on patternoriented structures. 3 Generalized Hierarchical Word Sequence Structure Suppose we are given a sentence s = w1 , w2 , ..., wn   and a permutation function f : s → s , where s =    w1 , w2 , ..., wn is a permutation of s. For each word index i(1 ≤ i ≤ n, wi ∈ s), there is a corresponding    reordered index j(1 ≤ j ≤ n, wj ∈ s , wj = wi ). Then we create an n × n matrix A. For each row j, we ﬁll cell Aj,i"
Y16-2004,P12-1023,0,0.0232078,"...as’ won’t be affected. This is how the HWS models relieve the data sparseness problem. The HWS model is essentially an n-gram language model based on a different assumption that a word depends upon its nearby high-frequency words instead of its preceding words. Different from other special n-gram language models, such as class-based language model (Brown et al., 1992), factored language model(FLM) (Bilmes and Kirchhoff, 2003), HWS language model doesn’t use any speciﬁc linguistic knowledge or any abstracted categories. Also, differs from dependency tree language models (Shen et al., 2008) (Chen et al., 2012), HWS language model constructs a tree structure in an unsupervised fashion. In HWS structure, word sequences are adjusted so that irrelevant words can be ﬁltered out from contexts and long distance information can be used PACLIC 30 Proceedings Figure 2: An Example of Generative Hierarchical Word Sequence Structure for predicting the next word, which make it more effective and ﬂexible in relieving the data sparseness problem. On this point, it has something in common with structured language model (Chelba, 1997), which ﬁrstly introduced parsing into language modeling. The signiﬁcant difference"
Y16-2004,H93-1016,0,0.46691,"setting for modeling word sequences in a sentence. Yet even with 30 years worth of newswire text, more than one third of all trigrams are still unseen (Allison et al., 2005), which cannot be distinguished accurately even using a high-performance smoothing method such as modiﬁed Kneser-Ney (abbreviated as MKN). An alternative solution is to factor the language model probabilities such that the number of unseen sequences are reduced. It is necessary to extract them in another way, instead of only using the information of left-to-right continuous word order. In (Guthrie et al., 2006), skip-gram (Huang et al., 1993)1 is proposed to overcome the data sparseness problem. For each n-gram word sequence, the skip-gram model enumerates all possible word combinations to increase valid sequences. This has truly helped to decrease the unseen sequences, but we should not neglect the fact that it also brings a greatly increase of processing time and redundant contexts. In (Wu and Matsumoto, 2014), a heuristic approach is proposed to convert any raw sentence into a hierarchical word sequence (abbreviated as 1 The k-skip-n-grams for a sentence w1 , ...wm is deﬁned as the set {wi1 , wi2 , ...win |Σn j=1 ij − ij−1 &lt; k}"
Y16-2004,P02-1040,0,0.0967413,"set English as the target language. As for statistical machine translation toolkit, we use Moses system15 to train the translation model and output 50-best translation candidates for each French sentence of the test data. Then we use 139,761 English sentences to train language models. With these models, 50-best translation candidates are reranked. According to these reranking results, the performance of machine translation system is evaluated, which also means, the language models are evaluated indirectly. In this paper, we use the following measures for evaluating reranking results16 . BLEU (Papineni et al., 2002): BLEU score measures how many words overlap in a given candidate translation when compared to a reference translation, which provides some insight into how good the ﬂuency of the output from an engine will be. METEOR (Banerjee and Lavie, 2005): METEOR score computes a one-to-one alignment between matching words in a candidate translation and a reference. TER (Snover et al., 2006): TER score measures the number of edits required to change a system output into one of the references, which gives an indication as to how much post-editing will be required 15 http://www.statmt.org/moses/ We use ope"
Y16-2004,P08-1066,0,0.0312779,", the expression ‘as...as’ won’t be affected. This is how the HWS models relieve the data sparseness problem. The HWS model is essentially an n-gram language model based on a different assumption that a word depends upon its nearby high-frequency words instead of its preceding words. Different from other special n-gram language models, such as class-based language model (Brown et al., 1992), factored language model(FLM) (Bilmes and Kirchhoff, 2003), HWS language model doesn’t use any speciﬁc linguistic knowledge or any abstracted categories. Also, differs from dependency tree language models (Shen et al., 2008) (Chen et al., 2012), HWS language model constructs a tree structure in an unsupervised fashion. In HWS structure, word sequences are adjusted so that irrelevant words can be ﬁltered out from contexts and long distance information can be used PACLIC 30 Proceedings Figure 2: An Example of Generative Hierarchical Word Sequence Structure for predicting the next word, which make it more effective and ﬂexible in relieving the data sparseness problem. On this point, it has something in common with structured language model (Chelba, 1997), which ﬁrstly introduced parsing into language modeling. The s"
Y16-2004,2006.amta-papers.25,0,0.0450181,"Missing"
Y16-2004,Y14-1056,1,0.769298,"bilities such that the number of unseen sequences are reduced. It is necessary to extract them in another way, instead of only using the information of left-to-right continuous word order. In (Guthrie et al., 2006), skip-gram (Huang et al., 1993)1 is proposed to overcome the data sparseness problem. For each n-gram word sequence, the skip-gram model enumerates all possible word combinations to increase valid sequences. This has truly helped to decrease the unseen sequences, but we should not neglect the fact that it also brings a greatly increase of processing time and redundant contexts. In (Wu and Matsumoto, 2014), a heuristic approach is proposed to convert any raw sentence into a hierarchical word sequence (abbreviated as 1 The k-skip-n-grams for a sentence w1 , ...wm is deﬁned as the set {wi1 , wi2 , ...win |Σn j=1 ij − ij−1 &lt; k}. 30th Pacific Asia Conference on Language, Information and Computation (PACLIC 30) Seoul, Republic of Korea, October 28-30, 2016 69 HWS) structure, by which much more valid word sequences can be modeled while remaining the model size as small as that of n-gram. In (Wu and Matsumoto, 2015) (Wu et al., 2015), instead of only using the information of word frequency, the inform"
Y16-2004,Y15-1051,1,0.780001,"that it also brings a greatly increase of processing time and redundant contexts. In (Wu and Matsumoto, 2014), a heuristic approach is proposed to convert any raw sentence into a hierarchical word sequence (abbreviated as 1 The k-skip-n-grams for a sentence w1 , ...wm is deﬁned as the set {wi1 , wi2 , ...win |Σn j=1 ij − ij−1 &lt; k}. 30th Pacific Asia Conference on Language, Information and Computation (PACLIC 30) Seoul, Republic of Korea, October 28-30, 2016 69 HWS) structure, by which much more valid word sequences can be modeled while remaining the model size as small as that of n-gram. In (Wu and Matsumoto, 2015) (Wu et al., 2015), instead of only using the information of word frequency, the information of direction and word association are also used to construct higher quality HWS structures. However, they are all speciﬁc methods based on certain heuristic assumptions. For the purpose of further improvements, it is also necessary to generalize those models into one uniﬁed structure. This paper is organized as follows. In Section 2, we review the HWS language model. Then we present a generalized hierarchical word sequence structure (GHWSS) in Section 3. In Section 4, we present two strategies for rear"
Y16-2006,W08-1209,0,0.0821262,"Missing"
Y16-2006,W11-0609,0,0.0603692,"Missing"
Y16-2006,P13-1095,0,0.0279677,"complicating mixture of emotions but also the intensity of each of them. In this paper, we propose a three steps method for the detection of emotions in conversation: 1)Building Emotion Lexicon from Wordnet (Miller, 1995). 2) Using simple Neural Network to adapt the lexicon to the training data. 3) Using Deep Network with features extracted from adapted lexicon and classify the multi-label corpus. Related Work Most of the work in the ﬁeld tried to deﬁne a small set of emotions (D’Mello et al., 2006; Yang et al., 2007) which involved only 3 and 4 emotional states respectively. Another work by Hasegawa et al. (2013) performed a multi-class classiﬁcation on dialog data from Twitter in Japanese. They automatically labeled the obtained dialogs by using emotional expression clues, which is similar to our collocation list explained in sub-section 3.3. We propose a more comprehensive approach by exploiting Plutchik’s notion which covers the full spectrum of human emotions to work on challenging multi-label conversation corpus. Having the same notion, Buitinck et al. (2015) proposed a simple Bag of Words approach and tuned RAKEL for multi-label classiﬁcation for movie reviews. We go further and work on conversa"
Y16-2006,P15-1101,0,0.106256,"ther. Generally, the expression of Emotion in general depends on the words being used. However, it also quite depends on the grammar structure and syntactic variables such as: negations, embedded sentence, and the type of sentence question, exclamation, command or statement (Collier, 2014). Therefore, similar to the detection of emotions of sentences in a paragraph, the context information of the whole conversation and what is said in the previous utterance should be taken into consideration. The extraction of context features will be further explained in sub-section 4.3.1 Unlike other works (Li et al., 2015; Wang et al., 2015) where small sets of basic emotions are used, we annotated the dataset using the notion of Plutchik’s basic emotion and dyads (1980). This eases the annotators’ task since it offers annotators with wider range of emotion labels (8 basic and 23 combinations) to choose from. Previous research often relied on a list of 6 basic emotions (Ekman et al., 1987) with some variants. However, this notion fails to show conﬂict side of some emotions. For example, people should not 30th Pacific Asia Conference on Language, Information and Computation (PACLIC 30) Seoul, Republic of Korea,"
Y16-2006,N12-1071,0,0.0321204,"Missing"
Y16-2006,S07-1013,0,0.0601109,"eate another emotional state dyads (Plutchik, 1980) The survey by Dave and Diwanji (2015) predicted the need for Emotion Detection in streaming data and the study of emotion ﬂow during chatting. In this paper, we tackle the simpliﬁed version of this task by detecting the emotions in conversation. The corpus we used is made of conversations among movie characters, who take turns in the conversation. Those turns are called utterances, which are then manually annotated in a multi-label manner. Emotion detection in conversation is essentially different from identifying emotions in news headlines (Strapparava and Mihalcea, 2007) or Tweets (Bollen et al., 2011) where each instance is independent of each other. Generally, the expression of Emotion in general depends on the words being used. However, it also quite depends on the grammar structure and syntactic variables such as: negations, embedded sentence, and the type of sentence question, exclamation, command or statement (Collier, 2014). Therefore, similar to the detection of emotions of sentences in a paragraph, the context information of the whole conversation and what is said in the previous utterance should be taken into consideration. The extraction of context"
Y16-2006,P15-2125,0,0.020663,"the expression of Emotion in general depends on the words being used. However, it also quite depends on the grammar structure and syntactic variables such as: negations, embedded sentence, and the type of sentence question, exclamation, command or statement (Collier, 2014). Therefore, similar to the detection of emotions of sentences in a paragraph, the context information of the whole conversation and what is said in the previous utterance should be taken into consideration. The extraction of context features will be further explained in sub-section 4.3.1 Unlike other works (Li et al., 2015; Wang et al., 2015) where small sets of basic emotions are used, we annotated the dataset using the notion of Plutchik’s basic emotion and dyads (1980). This eases the annotators’ task since it offers annotators with wider range of emotion labels (8 basic and 23 combinations) to choose from. Previous research often relied on a list of 6 basic emotions (Ekman et al., 1987) with some variants. However, this notion fails to show conﬂict side of some emotions. For example, people should not 30th Pacific Asia Conference on Language, Information and Computation (PACLIC 30) Seoul, Republic of Korea, October 28-30, 2016"
Y16-2006,P15-2129,0,0.0194134,"ve. Section 5 evaluates the lexicon, the effectiveness of the adapted lexicon and the proposed method in general. Section 6 gives the conclusion and discusses future work. 2 Figure 1: Plutchik’s basic emotion and dyads - image taken from http://twinklet8.blogspot.jp feel happiness and sadness from the same incident altogether. Furthermore, Ekman’s basic emotions are the result of observation made on human facial expressions so applying such notion in text classiﬁcation task seems irrelevant. Newer works relies on dimensional representation using valence-arousal space (Calvo and Mac Kim, 2013; Yu et al., 2015) Plutchik (1980) suggested 4 axes of bipolar basic emotions: Joy - Sadness, Fear - Anger, Trust Disgust, Surprise - Anticipatation. These primary emotions may blend to form the full spectrum of human emotional experience. The new complex emotions formed by them are called dyads (Figure 1). Plutchik’s notion reasonably explains the connection between emotions. Some emotions will not occur at the same time since they are on the opposite side of the axis. Complex emotions can also be viewed as combinations of primary ones. The idea enables us to approach emotion detection in a more comprehensive"
Y16-2015,H05-1091,0,0.0924444,"from the text in relation extraction. Zhou et al. (2005) explore various features in relation extraction using Support Vector Machine (SVM). They report that chunking information contributes to most of the performance improvement from the syntactic aspect. In kernel methods, a kernel is used to calculate the similarity between two objects. Kernel-based relation extraction methods were ﬁrst attempted by Zelenco et al. (2003). They devise contiguous subtree kernels and sparse subtree kernels for recursively measuring the similarity of two parse trees to apply them to binary relation extraction. Bunescu and Mooney (2005) present a different kernel based on the shortest path between two relation entities in the dependency graph. Zhao and Grishman (2005) deﬁne a feature-based composite kernel to integrate diverse features for relation extraction. Girju et al. (2006) present a domain independent approach for the automatic extraction of part-whole relation. Their method discovers the lexico-syntactic patterns and the semantic classiﬁcation rules needed for the disambiguation of these patterns. Annotated data is lacking and expensive to create in large quantities, therefore making semisupervised or unsupervised te"
Y16-2015,D11-1142,0,0.176234,"orithm ranks instances ﬁrst by instance reliability, and removes unrelated pairs, e.g. • ESP*: The original Espresso algorithm with the careful seed selection step in Section 4.1. 178 PACLIC 30 Proceedings • ESP* W2V: Our proposed method for integrating word embedding approach in the Espresso algorithm, with the careful seed selection step. Seed (iron, hemoglobin) (the committee, the president) (caffeine, coffee) (paper, trees) (Shanghai, China) (references, request) (treatment, surgery) (students, class) 5.1 Data We use ReVerb Extractions 1.1 dataset as a knowledge-base for our task. ReVerb (Fader et al., 2011) is a program that automatically identiﬁes and extracts binary relationships from English sentences. It contains a set of (x, r, y) extraction triples of binary relations (part-whole and other relations), for example, (bananas, be source of, potassium). A collection of 15 million high-precision ReVerb extractions is available for academic use.3 The following statistics are the number of distinct tuples, argument strings, and relation strings in the data set: 14,728,268 triples, 2,263,915 instances, and 664,746 patterns. Table 2: Sample seeds used for part-whole relation The parameters for the"
Y16-2015,J06-1005,0,0.0435948,"t. In kernel methods, a kernel is used to calculate the similarity between two objects. Kernel-based relation extraction methods were ﬁrst attempted by Zelenco et al. (2003). They devise contiguous subtree kernels and sparse subtree kernels for recursively measuring the similarity of two parse trees to apply them to binary relation extraction. Bunescu and Mooney (2005) present a different kernel based on the shortest path between two relation entities in the dependency graph. Zhao and Grishman (2005) deﬁne a feature-based composite kernel to integrate diverse features for relation extraction. Girju et al. (2006) present a domain independent approach for the automatic extraction of part-whole relation. Their method discovers the lexico-syntactic patterns and the semantic classiﬁcation rules needed for the disambiguation of these patterns. Annotated data is lacking and expensive to create in large quantities, therefore making semisupervised or unsupervised techniques is desirable. Early semi-supervised learning and bootstrapping methods are DIPRE (Brin, 1999) and Snowball (Agichtein and Gravano, 2000). They rely on a few learning collections for making the use of bootstrapping for gathering syntactic p"
Y16-2015,P05-1053,0,0.160777,"Missing"
Y16-2015,C92-2082,0,0.305144,"the ﬁrst study to integrate word embedding approach in a bootstrapping algorithm for part-whole relation extraction task. 2 Related Work In this section, we provide an overview of previous studies related to relation extraction problem. Approaches for relation extraction are divided into three classes: rule-based methods, supervised methods, and semi-supervised and unsupervised methods. The ﬁrst approach is usually used in domainspeciﬁc tasks. Systems which use this one rely on some linguistic rules to capture patterns in text. Patterns are manually deﬁned for a particular semantic relation. Hearst (1992) describes the usage of lexico-syntactic patterns for extracting “is-a” relations, for example, “such as”, “including”, “especially”, etc. However, the author notes that this method does not work well for some other kinds of relations, for example, meronymy (part-whole relation). Supervised approaches for relation extraction are divided into feature-based methods and kernel methods. In feature-based methods, syntactic and semantic features can be extracted from the text given a set of positive and negative relation examples. Kambhatla (2004) employs Maximum Entropy model to combine diverse lex"
Y16-2015,P06-1015,0,0.827268,"t role in many domains and applications. Extracting part-whole relations in the text is also a crucial step towards applications in several ﬁelds, such as Information Extraction, Web/Text Mining and Ontology Building. Such systems often need to recognize part-whole relations for better understanding semantic relationships between concepts. Therefore, in our research, we aim at extraction of part-whole relation. We are interested in relations between entities in the newswire domain. Among approaches to addressing the part-whole relation extraction problem, the Espresso bootstrapping algorithm (Pantel and Pennacchiotti, 2006) has proved to be effective by signiﬁcantly improving recall while keeping high precision. Espresso is a well-known bootstrapping algorithm that uses a set of seed instances to induce extraction patterns for the target relation and then acquire new instances in an iterative bootstrapping manner. Nevertheless, it has a bias, called semantic drift, to select unrelated instances if a polysemous instance has been extracted as the iteration proceeds. Recently, Mikolov et al. (2013) have introduced the skip-gram text modeling architecture. It has been shown efﬁciently to learn meaningful distributed"
Y16-2015,N04-1041,0,0.0677111,"measured using the following formula:1 Our Approach One problem of supervised approach is that it requires large amounts of annotated data. Therefore, we choose a bootstrapping method. In this approach, we only need a few high-precision examples as the input. In this section, we ﬁrst describe how we apply the Espresso bootstrapping algorithm for part-whole relation. We focus on seed selection since it is pmi(i, p) = log |x, p, y| |x, ∗, y||∗, p, ∗| where |x, p, y |is the frequency of pattern p linked with the instance (x, y). Then, pmi(i, p) is multiplied with the discounting factor used in (Pantel and Ravichandran, 2004) to mitigate a bias towards infrequent events. 1 175 In that formula, the asterisk (*) represents a wildcard. 3. Instance Extraction: Retrieve from the corpus the set of instances I that match any of the patterns in P, then create an instance ranker, and select the top-m instances by the instance reliability score. Calculating the reliability of an instance is similar to calculating the reliability of a pattern. The reliability of an instance i, rι(i) , is deﬁned as:  rι(i) = pmi(i, p) p∈P ( maxpmi |P | ∗ rπ (p)) (2) A reliable instance should be highly associated with as many reliable patter"
Y16-2015,N07-4013,0,0.0417867,"g methods are DIPRE (Brin, 1999) and Snowball (Agichtein and Gravano, 2000). They rely on a few learning collections for making the use of bootstrapping for gathering syntactic patterns that express relations between the two entities in a large web-based text corpus. Ittoo and Bouma (2013) use a minimally-supervised approach to extract partwhole relations from text iteratively. Wikipedia is 174 PACLIC 30 Proceedings the knowledge base, from which they ﬁrst select a seed set of reliable patterns. Other works include Espresso bootstrapping algorithm (Pantel and Pennacchiotti, 2006), TextRunner (Yates et al., 2007). 3 Part-Whole Relation Extraction Task The part-whole relation is a relationship between the parts of things and the wholes which comprise them. We are interested in relations between two entities in the English newswire domain. If the entities X and Y are related in such a manner that X is one of the constituents of Y, then there is a part-whole relation between X and Y. In the context of knowledge representation and ontologies, the study of part-whole relations has three axioms (Rector et al., 2005): • Transitive - “parts of parts are parts of the whole” - If A is part of B and B is part of"
Y16-2015,P05-1052,0,0.0630629,"SVM). They report that chunking information contributes to most of the performance improvement from the syntactic aspect. In kernel methods, a kernel is used to calculate the similarity between two objects. Kernel-based relation extraction methods were ﬁrst attempted by Zelenco et al. (2003). They devise contiguous subtree kernels and sparse subtree kernels for recursively measuring the similarity of two parse trees to apply them to binary relation extraction. Bunescu and Mooney (2005) present a different kernel based on the shortest path between two relation entities in the dependency graph. Zhao and Grishman (2005) deﬁne a feature-based composite kernel to integrate diverse features for relation extraction. Girju et al. (2006) present a domain independent approach for the automatic extraction of part-whole relation. Their method discovers the lexico-syntactic patterns and the semantic classiﬁcation rules needed for the disambiguation of these patterns. Annotated data is lacking and expensive to create in large quantities, therefore making semisupervised or unsupervised techniques is desirable. Early semi-supervised learning and bootstrapping methods are DIPRE (Brin, 1999) and Snowball (Agichtein and Gra"
Y17-1040,C12-1065,0,0.0172993,"se functional expressions by utilizing Japanese– Chinese homographs with identical or similar meanings, as a critical feature. Section 4 describes the several experiments conducted to examine the effectiveness of our method. Finally, in Section 5, we conclude and describe future work. 2 Previous Research Text difficulty or text readability evaluation is one of the challenges in natural language processing (NLP) owing to the linguistic complexity generated from both vocabulary and grammar. Researchers have been actively exploring methods to evaluate text difficulty (Gonzalez-Dios et al., 2014; Hancke, Vajjala, and Meurers, 2012; Vajjala and Meurers, 2012; Xia, Kochmar and Briscoe, 2016). For English texts, there are numerous popular formulas such as Flesch Reading Ease (Flesch 1948) and Flesch-Kincaid Grade Level, all of which are used for several applications such as compilation of reading materials for language learners. Collins–Thompson and Callan (2004) proposed a language modeling method to estimate the readability of English and French texts. For Japanese texts, Tateishi, Ono, and Yamada (1988a; 1988b) introduced a formula based on six surface characteristics: average number of characters per sentence, average"
Y17-1040,Y13-2007,0,0.0607615,"Missing"
Y17-1040,sato-etal-2008-automatic,0,0.0256495,"n and Callan (2004) proposed a language modeling method to estimate the readability of English and French texts. For Japanese texts, Tateishi, Ono, and Yamada (1988a; 1988b) introduced a formula based on six surface characteristics: average number of characters per sentence, average number of Roman letters and symbols, average number of hiragana characters, average number of kanji characters, average number of katakana characters, and ratio of touten (comma) to kuten (period). Formulabased approaches have also been used or teaching Japanese to young native speakers (Shibasaki and Sawai, 2007; Sato, Matsuyoshi, and Kondoh, 2008; Shibasaki and Tamaoka, 2010). To evaluate text difficulty level for foreign language learners of Japanese, Wang and Andersen (2016) introduced an approach for evaluating Japanese text difficulty 297 that focuses on grammar and utilizes grammar templates. In recent years, a few Japanese text difficulty evaluation systems have been developed to support Japanese language learners (Hasebe and Lee, 2015; Lee and Hasebe, 2016). For example, JReadability5 can analyze input text and estimate its readability to categorize it as belonging to one of six difficulty levels, on the basis of five character"
Y17-1040,W16-0502,0,0.0336071,"Missing"
Y17-1040,C88-2135,0,0.646428,"Missing"
Y17-1040,N04-1025,0,0.155086,"Missing"
Y17-1040,W12-2019,0,0.0255698,"ng Japanese– Chinese homographs with identical or similar meanings, as a critical feature. Section 4 describes the several experiments conducted to examine the effectiveness of our method. Finally, in Section 5, we conclude and describe future work. 2 Previous Research Text difficulty or text readability evaluation is one of the challenges in natural language processing (NLP) owing to the linguistic complexity generated from both vocabulary and grammar. Researchers have been actively exploring methods to evaluate text difficulty (Gonzalez-Dios et al., 2014; Hancke, Vajjala, and Meurers, 2012; Vajjala and Meurers, 2012; Xia, Kochmar and Briscoe, 2016). For English texts, there are numerous popular formulas such as Flesch Reading Ease (Flesch 1948) and Flesch-Kincaid Grade Level, all of which are used for several applications such as compilation of reading materials for language learners. Collins–Thompson and Callan (2004) proposed a language modeling method to estimate the readability of English and French texts. For Japanese texts, Tateishi, Ono, and Yamada (1988a; 1988b) introduced a formula based on six surface characteristics: average number of characters per sentence, average number of Roman letters an"
Y17-1040,C16-1159,0,0.0119338,"Ono, and Yamada (1988a; 1988b) introduced a formula based on six surface characteristics: average number of characters per sentence, average number of Roman letters and symbols, average number of hiragana characters, average number of kanji characters, average number of katakana characters, and ratio of touten (comma) to kuten (period). Formulabased approaches have also been used or teaching Japanese to young native speakers (Shibasaki and Sawai, 2007; Sato, Matsuyoshi, and Kondoh, 2008; Shibasaki and Tamaoka, 2010). To evaluate text difficulty level for foreign language learners of Japanese, Wang and Andersen (2016) introduced an approach for evaluating Japanese text difficulty 297 that focuses on grammar and utilizes grammar templates. In recent years, a few Japanese text difficulty evaluation systems have been developed to support Japanese language learners (Hasebe and Lee, 2015; Lee and Hasebe, 2016). For example, JReadability5 can analyze input text and estimate its readability to categorize it as belonging to one of six difficulty levels, on the basis of five characteristics: average length of sentence; percentage of kango (words of Chinese origin), percentage of wago (words of Japanese origin), per"
Y18-1046,P06-1032,0,0.0944715,"ee et al., 2015; Lee et al., 2016; Rao et al., 2017; Rao et al., 2018). On Japanese GEC, much work has been done on particle error correction for JSL learners (Oyama, 2010; Ohki et al., 2011; Mizumoto et al., 2011; Imamura et al., 2014). Collecting large-scale annotated error data written by second language learners is not so easy. To cope with grammatical error data scarcity, several studies proposed effective approaches for generating artificial error data (Irmawati et al., 2017; Rei et al., 2017). Several approaches of using Statistical machine translation (SMT) for GEC have been proposed (Brockett et al., 2006; Mizumoto et al., 2011; Mizumoto et al., 2015). Recently, neural networks have shown success in many NLP tasks, such as machine translation (MT) (Eriguchi et al., 2016; Gehring et al., 2017), named en395 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 tity recognition (NER) (Kuru et al., 2016; Misawa et al., 2017) and etc. For GEC, several studies have applied neural machine translation (NMT) approach (Chollampatt et al., 2016; Yuan and Briscoe, 2016). NMT is applied in the GEC task as it may be possibl"
Y18-1046,W13-4414,0,0.0190012,"ating artificial error data. Section 4 describes how BiLSTM-CRF model is used to detect Japanese functional expressions and Section 5 explains the method for generating artificial error data. In Section 6, we conduct the experiments using neural machine translation and analyze the results. Section 7 concludes with a summation of this work and describes our future work. 2 Related Work Spelling correction is an automatic algorithm for detecting and correcting human spelling errors in every written language, which has been an active research in Natural Language Processing (NLP) (Sun et al. 2010; Chen et al. 2013; Liu et al. 2013; Liu et al. 2015;). Grammatical error correction (GEC) is a task of detecting and correcting grammatical errors in text written by native language writers or nonnative foreign language writers. over the past few decades, GEC in English has been widely researched, such as Helping Our Own (Dale and Kigarriff, 2011; Dale et al., 2012), CoNLL Shared Task (Ng et al., 2013; Ng et al., 2014). Many shared tasks on GEC for Chinese Second Language Learners have also been held, such as the NLP-TEA Shared Task (Yu et al., 2014; Lee et al., 2015; Lee et al., 2016; Rao et al., 2017; Rao et"
Y18-1046,D14-1179,0,0.0421249,"Missing"
Y18-1046,W11-2838,0,0.061622,"Missing"
Y18-1046,W12-2006,0,0.0154424,"ribes our future work. 2 Related Work Spelling correction is an automatic algorithm for detecting and correcting human spelling errors in every written language, which has been an active research in Natural Language Processing (NLP) (Sun et al. 2010; Chen et al. 2013; Liu et al. 2013; Liu et al. 2015;). Grammatical error correction (GEC) is a task of detecting and correcting grammatical errors in text written by native language writers or nonnative foreign language writers. over the past few decades, GEC in English has been widely researched, such as Helping Our Own (Dale and Kigarriff, 2011; Dale et al., 2012), CoNLL Shared Task (Ng et al., 2013; Ng et al., 2014). Many shared tasks on GEC for Chinese Second Language Learners have also been held, such as the NLP-TEA Shared Task (Yu et al., 2014; Lee et al., 2015; Lee et al., 2016; Rao et al., 2017; Rao et al., 2018). On Japanese GEC, much work has been done on particle error correction for JSL learners (Oyama, 2010; Ohki et al., 2011; Mizumoto et al., 2011; Imamura et al., 2014). Collecting large-scale annotated error data written by second language learners is not so easy. To cope with grammatical error data scarcity, several studies proposed effec"
Y18-1046,Y18-1000,0,0.22362,"Missing"
Y18-1046,C16-1087,0,0.0546775,"Missing"
Y18-1046,W15-4401,0,0.0170559,"Language Processing (NLP) (Sun et al. 2010; Chen et al. 2013; Liu et al. 2013; Liu et al. 2015;). Grammatical error correction (GEC) is a task of detecting and correcting grammatical errors in text written by native language writers or nonnative foreign language writers. over the past few decades, GEC in English has been widely researched, such as Helping Our Own (Dale and Kigarriff, 2011; Dale et al., 2012), CoNLL Shared Task (Ng et al., 2013; Ng et al., 2014). Many shared tasks on GEC for Chinese Second Language Learners have also been held, such as the NLP-TEA Shared Task (Yu et al., 2014; Lee et al., 2015; Lee et al., 2016; Rao et al., 2017; Rao et al., 2018). On Japanese GEC, much work has been done on particle error correction for JSL learners (Oyama, 2010; Ohki et al., 2011; Mizumoto et al., 2011; Imamura et al., 2014). Collecting large-scale annotated error data written by second language learners is not so easy. To cope with grammatical error data scarcity, several studies proposed effective approaches for generating artificial error data (Irmawati et al., 2017; Rei et al., 2017). Several approaches of using Statistical machine translation (SMT) for GEC have been proposed (Brockett et al."
Y18-1046,W13-4409,1,0.770669,"rror data. Section 4 describes how BiLSTM-CRF model is used to detect Japanese functional expressions and Section 5 explains the method for generating artificial error data. In Section 6, we conduct the experiments using neural machine translation and analyze the results. Section 7 concludes with a summation of this work and describes our future work. 2 Related Work Spelling correction is an automatic algorithm for detecting and correcting human spelling errors in every written language, which has been an active research in Natural Language Processing (NLP) (Sun et al. 2010; Chen et al. 2013; Liu et al. 2013; Liu et al. 2015;). Grammatical error correction (GEC) is a task of detecting and correcting grammatical errors in text written by native language writers or nonnative foreign language writers. over the past few decades, GEC in English has been widely researched, such as Helping Our Own (Dale and Kigarriff, 2011; Dale et al., 2012), CoNLL Shared Task (Ng et al., 2013; Ng et al., 2014). Many shared tasks on GEC for Chinese Second Language Learners have also been held, such as the NLP-TEA Shared Task (Yu et al., 2014; Lee et al., 2015; Lee et al., 2016; Rao et al., 2017; Rao et al., 2018). On J"
Y18-1046,P15-1002,0,0.0347606,"sks, such as machine translation (MT) (Eriguchi et al., 2016; Gehring et al., 2017), named en395 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 tity recognition (NER) (Kuru et al., 2016; Misawa et al., 2017) and etc. For GEC, several studies have applied neural machine translation (NMT) approach (Chollampatt et al., 2016; Yuan and Briscoe, 2016). NMT is applied in the GEC task as it may be possible to correct erroneous phrases and sentences that have not been seen in the training data more effectively (Luong et al., 2015). NMT-based systems thus may help ameliorate the shortage of large error-annotated learner corpora for GEC. As previous research mentioned above, few studies have aimed at spelling and grammatical error corrections on Japanese functional expressions. Therefore, our paper is an attempt to do this work using neural machine translation. 3 Language resources We use the following corpora for training the BiLSTM-CRF model to detect Japanese functional expressions. We use Lang-8 Learner, Tatoeba, HiraganaTimes corpora for generating artificial error data, because these three corpora are particularly"
Y18-1046,I11-1017,1,0.84508,"text written by native language writers or nonnative foreign language writers. over the past few decades, GEC in English has been widely researched, such as Helping Our Own (Dale and Kigarriff, 2011; Dale et al., 2012), CoNLL Shared Task (Ng et al., 2013; Ng et al., 2014). Many shared tasks on GEC for Chinese Second Language Learners have also been held, such as the NLP-TEA Shared Task (Yu et al., 2014; Lee et al., 2015; Lee et al., 2016; Rao et al., 2017; Rao et al., 2018). On Japanese GEC, much work has been done on particle error correction for JSL learners (Oyama, 2010; Ohki et al., 2011; Mizumoto et al., 2011; Imamura et al., 2014). Collecting large-scale annotated error data written by second language learners is not so easy. To cope with grammatical error data scarcity, several studies proposed effective approaches for generating artificial error data (Irmawati et al., 2017; Rei et al., 2017). Several approaches of using Statistical machine translation (SMT) for GEC have been proposed (Brockett et al., 2006; Mizumoto et al., 2011; Mizumoto et al., 2015). Recently, neural networks have shown success in many NLP tasks, such as machine translation (MT) (Eriguchi et al., 2016; Gehring et al., 2017),"
Y18-1046,W15-4412,1,0.849798,"2017; Rao et al., 2018). On Japanese GEC, much work has been done on particle error correction for JSL learners (Oyama, 2010; Ohki et al., 2011; Mizumoto et al., 2011; Imamura et al., 2014). Collecting large-scale annotated error data written by second language learners is not so easy. To cope with grammatical error data scarcity, several studies proposed effective approaches for generating artificial error data (Irmawati et al., 2017; Rei et al., 2017). Several approaches of using Statistical machine translation (SMT) for GEC have been proposed (Brockett et al., 2006; Mizumoto et al., 2011; Mizumoto et al., 2015). Recently, neural networks have shown success in many NLP tasks, such as machine translation (MT) (Eriguchi et al., 2016; Gehring et al., 2017), named en395 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 tity recognition (NER) (Kuru et al., 2016; Misawa et al., 2017) and etc. For GEC, several studies have applied neural machine translation (NMT) approach (Chollampatt et al., 2016; Yuan and Briscoe, 2016). NMT is applied in the GEC task as it may be possible to correct erroneous phrases and sentences th"
Y18-1046,W14-1701,0,0.0155506,"n is an automatic algorithm for detecting and correcting human spelling errors in every written language, which has been an active research in Natural Language Processing (NLP) (Sun et al. 2010; Chen et al. 2013; Liu et al. 2013; Liu et al. 2015;). Grammatical error correction (GEC) is a task of detecting and correcting grammatical errors in text written by native language writers or nonnative foreign language writers. over the past few decades, GEC in English has been widely researched, such as Helping Our Own (Dale and Kigarriff, 2011; Dale et al., 2012), CoNLL Shared Task (Ng et al., 2013; Ng et al., 2014). Many shared tasks on GEC for Chinese Second Language Learners have also been held, such as the NLP-TEA Shared Task (Yu et al., 2014; Lee et al., 2015; Lee et al., 2016; Rao et al., 2017; Rao et al., 2018). On Japanese GEC, much work has been done on particle error correction for JSL learners (Oyama, 2010; Ohki et al., 2011; Mizumoto et al., 2011; Imamura et al., 2014). Collecting large-scale annotated error data written by second language learners is not so easy. To cope with grammatical error data scarcity, several studies proposed effective approaches for generating artificial error data ("
Y18-1046,W13-3601,0,0.0259977,"pelling correction is an automatic algorithm for detecting and correcting human spelling errors in every written language, which has been an active research in Natural Language Processing (NLP) (Sun et al. 2010; Chen et al. 2013; Liu et al. 2013; Liu et al. 2015;). Grammatical error correction (GEC) is a task of detecting and correcting grammatical errors in text written by native language writers or nonnative foreign language writers. over the past few decades, GEC in English has been widely researched, such as Helping Our Own (Dale and Kigarriff, 2011; Dale et al., 2012), CoNLL Shared Task (Ng et al., 2013; Ng et al., 2014). Many shared tasks on GEC for Chinese Second Language Learners have also been held, such as the NLP-TEA Shared Task (Yu et al., 2014; Lee et al., 2015; Lee et al., 2016; Rao et al., 2017; Rao et al., 2018). On Japanese GEC, much work has been done on particle error correction for JSL learners (Oyama, 2010; Ohki et al., 2011; Mizumoto et al., 2011; Imamura et al., 2014). Collecting large-scale annotated error data written by second language learners is not so easy. To cope with grammatical error data scarcity, several studies proposed effective approaches for generating artif"
Y18-1046,W06-2404,0,0.119078,"Missing"
Y18-1046,W18-3706,0,0.0128624,". 2013; Liu et al. 2013; Liu et al. 2015;). Grammatical error correction (GEC) is a task of detecting and correcting grammatical errors in text written by native language writers or nonnative foreign language writers. over the past few decades, GEC in English has been widely researched, such as Helping Our Own (Dale and Kigarriff, 2011; Dale et al., 2012), CoNLL Shared Task (Ng et al., 2013; Ng et al., 2014). Many shared tasks on GEC for Chinese Second Language Learners have also been held, such as the NLP-TEA Shared Task (Yu et al., 2014; Lee et al., 2015; Lee et al., 2016; Rao et al., 2017; Rao et al., 2018). On Japanese GEC, much work has been done on particle error correction for JSL learners (Oyama, 2010; Ohki et al., 2011; Mizumoto et al., 2011; Imamura et al., 2014). Collecting large-scale annotated error data written by second language learners is not so easy. To cope with grammatical error data scarcity, several studies proposed effective approaches for generating artificial error data (Irmawati et al., 2017; Rei et al., 2017). Several approaches of using Statistical machine translation (SMT) for GEC have been proposed (Brockett et al., 2006; Mizumoto et al., 2011; Mizumoto et al., 2015)."
Y18-1046,W17-5032,0,0.0152929,"GEC for Chinese Second Language Learners have also been held, such as the NLP-TEA Shared Task (Yu et al., 2014; Lee et al., 2015; Lee et al., 2016; Rao et al., 2017; Rao et al., 2018). On Japanese GEC, much work has been done on particle error correction for JSL learners (Oyama, 2010; Ohki et al., 2011; Mizumoto et al., 2011; Imamura et al., 2014). Collecting large-scale annotated error data written by second language learners is not so easy. To cope with grammatical error data scarcity, several studies proposed effective approaches for generating artificial error data (Irmawati et al., 2017; Rei et al., 2017). Several approaches of using Statistical machine translation (SMT) for GEC have been proposed (Brockett et al., 2006; Mizumoto et al., 2011; Mizumoto et al., 2015). Recently, neural networks have shown success in many NLP tasks, such as machine translation (MT) (Eriguchi et al., 2016; Gehring et al., 2017), named en395 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 tity recognition (NER) (Kuru et al., 2016; Misawa et al., 2017) and etc. For GEC, several studies have applied neural machine translation ("
Y18-1046,N16-1042,0,0.0345489,"machine translation (SMT) for GEC have been proposed (Brockett et al., 2006; Mizumoto et al., 2011; Mizumoto et al., 2015). Recently, neural networks have shown success in many NLP tasks, such as machine translation (MT) (Eriguchi et al., 2016; Gehring et al., 2017), named en395 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 tity recognition (NER) (Kuru et al., 2016; Misawa et al., 2017) and etc. For GEC, several studies have applied neural machine translation (NMT) approach (Chollampatt et al., 2016; Yuan and Briscoe, 2016). NMT is applied in the GEC task as it may be possible to correct erroneous phrases and sentences that have not been seen in the training data more effectively (Luong et al., 2015). NMT-based systems thus may help ameliorate the shortage of large error-annotated learner corpora for GEC. As previous research mentioned above, few studies have aimed at spelling and grammatical error corrections on Japanese functional expressions. Therefore, our paper is an attempt to do this work using neural machine translation. 3 Language resources We use the following corpora for training the BiLSTM-CRF model"
Y18-1046,P10-1028,0,0.0827212,"F model and generating artificial error data. Section 4 describes how BiLSTM-CRF model is used to detect Japanese functional expressions and Section 5 explains the method for generating artificial error data. In Section 6, we conduct the experiments using neural machine translation and analyze the results. Section 7 concludes with a summation of this work and describes our future work. 2 Related Work Spelling correction is an automatic algorithm for detecting and correcting human spelling errors in every written language, which has been an active research in Natural Language Processing (NLP) (Sun et al. 2010; Chen et al. 2013; Liu et al. 2013; Liu et al. 2015;). Grammatical error correction (GEC) is a task of detecting and correcting grammatical errors in text written by native language writers or nonnative foreign language writers. over the past few decades, GEC in English has been widely researched, such as Helping Our Own (Dale and Kigarriff, 2011; Dale et al., 2012), CoNLL Shared Task (Ng et al., 2013; Ng et al., 2014). Many shared tasks on GEC for Chinese Second Language Learners have also been held, such as the NLP-TEA Shared Task (Yu et al., 2014; Lee et al., 2015; Lee et al., 2016; Rao et"
Y18-1052,K17-3002,0,0.0129418,"n of scores, it uses Long Short-Term Memories (LSTMs), Multi-Layer Perceptrons (MLPs) and biaffine classifiers. In the following sections, first, we explain the biaffine transformation which is the essential part of a biaffine classifier while skipping explanation about LSTM and MLP for the sake of simplicity. Then, we describe the overview of the model. It is worth noting that the structure of the model is different from that in (Dozat and Manning, 2016) in that it utilizes character level information. This is because we used an updated version of the model that was made for the shared task (Dozat et al., 2017). 2.1 Biaffine Transformation For the dependency parsing score functions, we use the biaffine transformation shown below to model binary relations. Here, ⊕ stands for concatenation of vectors. The first term on the right side represents relatedness score, and the second term the score of vi and vj appearing independently. b is bias term. g(vi , vj ) = viT Avj + (vi ⊕ vj )T b + b 2.2 (1) to encode each words’ character-level information into a vector. It then sums this vector with a separate token-level word embeddings. 2. It then concatenates the vectors obtained above with POS embeddings and"
Y18-1052,D17-1206,0,0.0197049,"yright 2018 by the authors PACLIC 32 U[1] , U[2] , . . . , U[L] in the scoring functions for labels, we have to assume that they form a commuting family as with the discussion in 3.1.2. 4 Related work Dependency Parsing In recent years, various graph-based parsers with attention mechanisms have been proposed. Kiperwasser and Goldberg (2016) incorporated the attention mechanism used in machine translation (Bahdanau et al., 2014) into their graph-based parser. Their model receives vectors which are made by concatenating LSTM outputs corresponding to each word and its head candidates. Similarly, Hashimoto et al. (2017) proposed a graphbased parser where they substitute the MLP-based classifier in (Kiperwasser and Goldberg, 2016) with the bilinear one in their multi-task neural model, although they still use the MLP-based one in prediction of labels. Accordingly, Dozat and Manning (2016) modified the model by Kiperwasser and Goldberg (2016) using a biaffine classifier instead of an MLP-based one which enables the model to express not only the probability of a word receiving a particular word as dependent but also the prior probability of a word being a head. Likewise, in transition-based parsing literature,"
Y18-1052,P17-2088,1,0.878686,"Missing"
Y18-1052,N18-1047,1,0.834335,"nism based on a biaffne classifier which calculates the probability distribution of the next word which comes into the stack at each time step, with LSTM outputs corresponding to each word in the input sentence. The models proposed in this paper can be incorporated into these models. Parameter Reduction in Neural Networks Recently, numerous methods toward parameter reduction of neural networks have been proposed. As a similar approach to proposed methods, there is a method where a projection matrix is decomposed into smaller matrices by lower-rank approximation (Lu et al., 2016). In addition, Ishihara et al. (2018) introduced eigenvector decomposition into neural tensor networks (Socher et al., 2013) and analyzed the effects of parameter reduction. Although the paper (Ishihara et al., 2018) is similar to the present paper in that their methods address parameter reduction in the bilinear term, our work is different in that we apply it to deep biaffine parser. There are some methods to reduce parameters in a projection matrix by sharing them. Cheng et al. (2015) used circulant matrices in the fully connected layers. Our models are different from theirs in that we use circulant matrices in the bilinear ter"
Y18-1052,Q16-1023,0,0.0299274,"= OT vi , vj0 = OT vj , assuming that the unitary matrix O is learned implicitly. Note that to simultaneously diagonalize the normal matrices whose real parts are the weight matrices 450 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 U[1] , U[2] , . . . , U[L] in the scoring functions for labels, we have to assume that they form a commuting family as with the discussion in 3.1.2. 4 Related work Dependency Parsing In recent years, various graph-based parsers with attention mechanisms have been proposed. Kiperwasser and Goldberg (2016) incorporated the attention mechanism used in machine translation (Bahdanau et al., 2014) into their graph-based parser. Their model receives vectors which are made by concatenating LSTM outputs corresponding to each word and its head candidates. Similarly, Hashimoto et al. (2017) proposed a graphbased parser where they substitute the MLP-based classifier in (Kiperwasser and Goldberg, 2016) with the bilinear one in their multi-task neural model, although they still use the MLP-based one in prediction of labels. Accordingly, Dozat and Manning (2016) modified the model by Kiperwasser and Goldber"
Y18-1052,P18-1130,0,0.111786,"erformance (Kiperwasser and Goldberg, 2016; Hashimoto et al., 2017; Dozat and Manning, 2016). Biaffine transformation is a method to incorporate an attention mechanism into binary relations proposed by Dozat and Manning (2016) (following them, we call this method biaffine classifier). They achieved the state-of-the-art performance among graph-based dependency parsers for the English Penn Treebank. In addition, the state-ofthe-art transition-based parser on English Penn Treebank uses a biaffine classifier to evaluate the probability distribution of a word coming into the stack at a step point (Ma et al., 2018). While biaffine transformation has rich expressiveness in modeling binary relations, its number of parameters in the weight matrix (bilinear term) is O(n2 ) (where n is the number of dimensions). This redundant parameters can result in high degree of freedom of the model, thus causing overfitting especially when a large number of training samples are not available (Nickel et al., 2015). In this paper, we attempt to reduce the redundancy by introducing the assumption of either symmetry or circularity in the weight matrix at a biaffine classifier. With either assumption, we can vectorize the ma"
Y18-1052,D15-1044,0,0.123664,"Missing"
Y95-1022,W93-0305,0,0.0251394,"Missing"
Y95-1022,A88-1019,0,0.188461,"Missing"
Y95-1022,A92-1018,0,0.0528356,"Missing"
Y96-1040,P96-1025,0,0.0215608,"ng discarded. An alternative would be to retain most of the original distribution, only introducing a small amount of noise. For example, a new rule receives a probability of 0.8 times the old value, and 0.2 times a random number between 0 and 1. This will reduce the amount of information that is lost during a separation. One question that remains is the future of statistical grammar induction. While this technique strongly speeds up the process, inducting a large scale grammar is still far from possible. Also, broad coverage parsing is being done with more success elsewhere, see for example (Coffins, 1996; Magerman, 1995). We therefore see this experiment as an experiment in automatically discovering grammatical structures. We also feel grammar induction can play a role in discovering groups of brackets that have similar behavior. Another application can be evaluating a fine grained tag set, since the success of the grammar strongly depends on the word tags. Although this proposal is limited to simple parts of speech, we feel that for context free grammars to be more mature, they should use headwords, either of categories as in (Hogenhout and Matsumoto, 1996), or of words on the lexical level"
Y96-1040,W89-0209,0,0.0295133,"the same way. Various sizes were tried for the training set, but this had no significant affect on the performance. In our experiments we used the same number of nonterminals, the same corpus and about the same amount of training data, but we created the grammar in much less time. 2.2 Enforcing Structure in the Grammar We mention some experiments that aimed at reducing the computational complexity of grammar induction. They are aimed at either giving structure to 384 the grammar before parsing, reducing the number of rules in the grammar, or reducing the number of rules involved in training. Fujisaki et al. (1989) describe an experiment where training was only used to find the parameters of a predefined subset of the rules. They trained with an ambiguous corpus of slightly more than 4200 sentences, on average about 11 words long. The resulting grammar was tested on 84 sentences, but comparison with other experiments is rather difficult. Sharman, Jelinek, and Mercer (1990) describe an experiment where the grammar was in ID/LP format (Immediate Dominance and Linear Precedence), and received initial probabilities from the counts in a treebank. In this way, the grammar already had a strong shape before tra"
Y96-1040,P95-1037,0,0.0476652,"n alternative would be to retain most of the original distribution, only introducing a small amount of noise. For example, a new rule receives a probability of 0.8 times the old value, and 0.2 times a random number between 0 and 1. This will reduce the amount of information that is lost during a separation. One question that remains is the future of statistical grammar induction. While this technique strongly speeds up the process, inducting a large scale grammar is still far from possible. Also, broad coverage parsing is being done with more success elsewhere, see for example (Coffins, 1996; Magerman, 1995). We therefore see this experiment as an experiment in automatically discovering grammatical structures. We also feel grammar induction can play a role in discovering groups of brackets that have similar behavior. Another application can be evaluating a fine grained tag set, since the success of the grammar strongly depends on the word tags. Although this proposal is limited to simple parts of speech, we feel that for context free grammars to be more mature, they should use headwords, either of categories as in (Hogenhout and Matsumoto, 1996), or of words on the lexical level as in (Collins, 1"
Y96-1040,E93-1040,0,0.0174321,"and therefore leads to an upward jump in entropy. But after this severe loss the entropy values become lower than they would have become with less nonterminals almost every time. For example, the 7-nonterminal grammar did not come under an entropy of 3, also not after a running much more iterations than have been indicated here, whereas the final 15 nonterminal grammar came at less then 2.5. This shows that our method gradually improves the grammar, even though a large part of the grammar is discarded before every separation. Table 1 compares our results with those of the grammar inducted in (Schabes, Roth, and Osborne, 1993), with a grammar that gives a right-branching structure to everything except the final punctuation and a grammar directly abstracted from the corpus. (These figures have been taken over from (Schabes, Roth, and Osborne, 1993)). Figure 3 shows the accuracies for the test set differentiated by length. Please note that this is cumulative; for example 20 on the x-axis means &quot;shorter than 20 words.&quot; This shows that our inducted grammar scores equivalent to the grammar inducted in (Schabes, Roth, and Osborne, 1993). &apos;training data&apos; &apos;test data&apos; 4.5 3.5 3- 2.5 2 0 20 40 60 80 100 120 140 160 180 Figu"
Y96-1040,H90-1054,0,0.0607005,"Missing"
