2013.iwslt-evaluation.8,P07-2045,0,0.0254044,"ibe a specialized normalization scheme for evaluating Arabic output, which was adopted for the IWSLT’2013 evaluation campaign. 1. Introduction We describe the Arabic-English and English-Arabic statistical machine translation (SMT) systems developed by the Qatar Computing Research Institute (QCRI) for the 2013 open evaluation campaign on spoken language translation organized in conjunction with the International Workshop on Spoken Language Translation (IWSLT). Below we give an overview of the settings we experimented with: • Decoders: We used a phrase-based SMT (PBSMT), as implemented in Moses [1], and two hierarchical decoders: Jane [2] and cdec [3]. See Section 6 for details. • Decoder settings: There are a variety of settings available for the above decoders. We explored a number of them, most notably, operation sequence model, minimum Bayes risk decoding, monotone-at-punctuation, dropping out-of-vocabulary words, etc. We selected to retain those settings that improved the overall translation quality as measured on the dev-test set. See Section 4 for further details. • Arabic segmentation: To reduce data sparseness, Arabic words are typically segmented into multiple tokens, e.g., by"
2013.iwslt-evaluation.8,W10-1738,0,0.130794,"r evaluating Arabic output, which was adopted for the IWSLT’2013 evaluation campaign. 1. Introduction We describe the Arabic-English and English-Arabic statistical machine translation (SMT) systems developed by the Qatar Computing Research Institute (QCRI) for the 2013 open evaluation campaign on spoken language translation organized in conjunction with the International Workshop on Spoken Language Translation (IWSLT). Below we give an overview of the settings we experimented with: • Decoders: We used a phrase-based SMT (PBSMT), as implemented in Moses [1], and two hierarchical decoders: Jane [2] and cdec [3]. See Section 6 for details. • Decoder settings: There are a variety of settings available for the above decoders. We explored a number of them, most notably, operation sequence model, minimum Bayes risk decoding, monotone-at-punctuation, dropping out-of-vocabulary words, etc. We selected to retain those settings that improved the overall translation quality as measured on the dev-test set. See Section 4 for further details. • Arabic segmentation: To reduce data sparseness, Arabic words are typically segmented into multiple tokens, e.g., by segmenting out conjunctions, pronouns, a"
2013.iwslt-evaluation.8,P10-4002,0,0.0806549,"Arabic output, which was adopted for the IWSLT’2013 evaluation campaign. 1. Introduction We describe the Arabic-English and English-Arabic statistical machine translation (SMT) systems developed by the Qatar Computing Research Institute (QCRI) for the 2013 open evaluation campaign on spoken language translation organized in conjunction with the International Workshop on Spoken Language Translation (IWSLT). Below we give an overview of the settings we experimented with: • Decoders: We used a phrase-based SMT (PBSMT), as implemented in Moses [1], and two hierarchical decoders: Jane [2] and cdec [3]. See Section 6 for details. • Decoder settings: There are a variety of settings available for the above decoders. We explored a number of them, most notably, operation sequence model, minimum Bayes risk decoding, monotone-at-punctuation, dropping out-of-vocabulary words, etc. We selected to retain those settings that improved the overall translation quality as measured on the dev-test set. See Section 4 for further details. • Arabic segmentation: To reduce data sparseness, Arabic words are typically segmented into multiple tokens, e.g., by segmenting out conjunctions, pronouns, articles, etc."
2013.iwslt-evaluation.8,N06-2013,0,0.10965,"coders. We explored a number of them, most notably, operation sequence model, minimum Bayes risk decoding, monotone-at-punctuation, dropping out-of-vocabulary words, etc. We selected to retain those settings that improved the overall translation quality as measured on the dev-test set. See Section 4 for further details. • Arabic segmentation: To reduce data sparseness, Arabic words are typically segmented into multiple tokens, e.g., by segmenting out conjunctions, pronouns, articles, etc. We experimented with standard segmentation schemes such as D0, D1, D2, D3, S2 and ATB, as defined in MADA [4, 5]. See Section 5 for details. • Domain adaptation: We experimented with three domain adaptation methods to make better use of the huge UN data, which is out-of-domain: (i) Modified Moore-Lewis filtering, (ii) phrase table merging, and (iii) phrase table backoff. See Section 7 for details. For our final submission, we synthesized a translation by combining the output of our best individual system with the output of other systems that are both relatively strong and can contribute to having more diversity, e.g., using a different decoder or a different segmentation scheme. We achieved the most not"
2013.iwslt-evaluation.8,P08-2039,0,0.073174,"coders. We explored a number of them, most notably, operation sequence model, minimum Bayes risk decoding, monotone-at-punctuation, dropping out-of-vocabulary words, etc. We selected to retain those settings that improved the overall translation quality as measured on the dev-test set. See Section 4 for further details. • Arabic segmentation: To reduce data sparseness, Arabic words are typically segmented into multiple tokens, e.g., by segmenting out conjunctions, pronouns, articles, etc. We experimented with standard segmentation schemes such as D0, D1, D2, D3, S2 and ATB, as defined in MADA [4, 5]. See Section 5 for details. • Domain adaptation: We experimented with three domain adaptation methods to make better use of the huge UN data, which is out-of-domain: (i) Modified Moore-Lewis filtering, (ii) phrase table merging, and (iii) phrase table backoff. See Section 7 for details. For our final submission, we synthesized a translation by combining the output of our best individual system with the output of other systems that are both relatively strong and can contribute to having more diversity, e.g., using a different decoder or a different segmentation scheme. We achieved the most not"
2013.iwslt-evaluation.8,P12-1016,0,0.268259,"lish BLEU 1-TER System English IWSLT mono 109 English-French SETimes UN (Es-En + En-Fr) UN (Ar-En) News Crawl 2007-2009 News Crawl 2009-2012 Common Crawl Wiki Headlines Europarl v.7 News Commentary v.8 Gigaword v.5 2.7M 575M 4.2M 597M 115M 643M 745M 185M 1.1M 54M 5.3M 4,032M Arabic IWSLT mono UN News Commentary Arabic v.8 Gigaword Arabic v.5 2.7M 134M 4.8M 1,373M Table 1: Admissible training data for language modeling. Here English is tokenized, and Arabic is ATB-segmented. Preprocessing. We segmented the Arabic side of the bitext following the ATB scheme and using the Stanford word segmenter [6]. For the English side, we used the standard tokenizer of Moses, and we further applied truecasing/lowercasing when English was the target/source language. Training. We built separate directed word alignments for English-to-Arabic and for Arabic-to-English using IBM model 4 [7], and we symmetrized them using the grow-diagfinal-and heuristics [8]. We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing, thus obtaining a phrase table where each phrase pair has the standard five translation model features. We"
2013.iwslt-evaluation.8,J93-2003,0,0.0246271,"abic IWSLT mono UN News Commentary Arabic v.8 Gigaword Arabic v.5 2.7M 134M 4.8M 1,373M Table 1: Admissible training data for language modeling. Here English is tokenized, and Arabic is ATB-segmented. Preprocessing. We segmented the Arabic side of the bitext following the ATB scheme and using the Stanford word segmenter [6]. For the English side, we used the standard tokenizer of Moses, and we further applied truecasing/lowercasing when English was the target/source language. Training. We built separate directed word alignments for English-to-Arabic and for Arabic-to-English using IBM model 4 [7], and we symmetrized them using the grow-diagfinal-and heuristics [8]. We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing, thus obtaining a phrase table where each phrase pair has the standard five translation model features. We also built a lexicalized reordering model [9]: msd-bidirectional-fe. For language modeling, we used KenLM [10] to build a 5-gram Kneser-Ney smoothed model, trained on the target side of the training bi-text. Finally, we built a large joint log-linear model, which used standard"
2013.iwslt-evaluation.8,N03-1017,0,0.0173015,"M 134M 4.8M 1,373M Table 1: Admissible training data for language modeling. Here English is tokenized, and Arabic is ATB-segmented. Preprocessing. We segmented the Arabic side of the bitext following the ATB scheme and using the Stanford word segmenter [6]. For the English side, we used the standard tokenizer of Moses, and we further applied truecasing/lowercasing when English was the target/source language. Training. We built separate directed word alignments for English-to-Arabic and for Arabic-to-English using IBM model 4 [7], and we symmetrized them using the grow-diagfinal-and heuristics [8]. We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing, thus obtaining a phrase table where each phrase pair has the standard five translation model features. We also built a lexicalized reordering model [9]: msd-bidirectional-fe. For language modeling, we used KenLM [10] to build a 5-gram Kneser-Ney smoothed model, trained on the target side of the training bi-text. Finally, we built a large joint log-linear model, which used standard PBSMT feature functions: language model probability, word penalty, th"
2013.iwslt-evaluation.8,2005.iwslt-1.8,0,0.0225691,"rd tokenizer of Moses, and we further applied truecasing/lowercasing when English was the target/source language. Training. We built separate directed word alignments for English-to-Arabic and for Arabic-to-English using IBM model 4 [7], and we symmetrized them using the grow-diagfinal-and heuristics [8]. We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing, thus obtaining a phrase table where each phrase pair has the standard five translation model features. We also built a lexicalized reordering model [9]: msd-bidirectional-fe. For language modeling, we used KenLM [10] to build a 5-gram Kneser-Ney smoothed model, trained on the target side of the training bi-text. Finally, we built a large joint log-linear model, which used standard PBSMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. Tuning. We tuned the weights in the log-linear model by optimizing BLEU [11] on the tuning dataset, using PRO [12]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000-best lists on each iteration. De"
2013.iwslt-evaluation.8,W11-2123,0,0.408919,"sing when English was the target/source language. Training. We built separate directed word alignments for English-to-Arabic and for Arabic-to-English using IBM model 4 [7], and we symmetrized them using the grow-diagfinal-and heuristics [8]. We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing, thus obtaining a phrase table where each phrase pair has the standard five translation model features. We also built a lexicalized reordering model [9]: msd-bidirectional-fe. For language modeling, we used KenLM [10] to build a 5-gram Kneser-Ney smoothed model, trained on the target side of the training bi-text. Finally, we built a large joint log-linear model, which used standard PBSMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. Tuning. We tuned the weights in the log-linear model by optimizing BLEU [11] on the tuning dataset, using PRO [12]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000-best lists on each iteration. Decoding. On tuning and testing, we used monotone-atpunctuation. On"
2013.iwslt-evaluation.8,P02-1040,0,0.0879883,", thus obtaining a phrase table where each phrase pair has the standard five translation model features. We also built a lexicalized reordering model [9]: msd-bidirectional-fe. For language modeling, we used KenLM [10] to build a 5-gram Kneser-Ney smoothed model, trained on the target side of the training bi-text. Finally, we built a large joint log-linear model, which used standard PBSMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. Tuning. We tuned the weights in the log-linear model by optimizing BLEU [11] on the tuning dataset, using PRO [12]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000-best lists on each iteration. Decoding. On tuning and testing, we used monotone-atpunctuation. On testing, we further used cube pruning. Table 2 shows the results3 for the baseline English-toArabic and Arabic-to-English SMT systems, compared to the baseline results reported on the WIT3 webpage. 3 For tst2010, we report MultEval BLEU and TER0.8: on tokenized and recased output for English, and on QCRI-normalized output for Arabic. For tst2011, tst2012, and tst2013, the organizers"
2013.iwslt-evaluation.8,D11-1125,0,0.032033,"each phrase pair has the standard five translation model features. We also built a lexicalized reordering model [9]: msd-bidirectional-fe. For language modeling, we used KenLM [10] to build a 5-gram Kneser-Ney smoothed model, trained on the target side of the training bi-text. Finally, we built a large joint log-linear model, which used standard PBSMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. Tuning. We tuned the weights in the log-linear model by optimizing BLEU [11] on the tuning dataset, using PRO [12]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000-best lists on each iteration. Decoding. On tuning and testing, we used monotone-atpunctuation. On testing, we further used cube pruning. Table 2 shows the results3 for the baseline English-toArabic and Arabic-to-English SMT systems, compared to the baseline results reported on the WIT3 webpage. 3 For tst2010, we report MultEval BLEU and TER0.8: on tokenized and recased output for English, and on QCRI-normalized output for Arabic. For tst2011, tst2012, and tst2013, the organizers used slightly different scorers. IWSL"
2013.iwslt-evaluation.8,C12-1121,1,0.915839,"ltEval BLEU and TER0.8: on tokenized and recased output for English, and on QCRI-normalized output for Arabic. For tst2011, tst2012, and tst2013, the organizers used slightly different scorers. IWSLT baseline Our baseline 23.6 24.7 English-Arabic BLEU 1-TER 43.0 45.6 11.9 12.6 28.6 29.1 Table 2: Our vs. IWSLT baseline results for English-toArabic and Arabic-to-English SMT, evaluated on tst2010. 4. System Settings Below we discuss the decoder settings and extensions we experimented with, focusing on Arabic-to-English. Table 3 shows the impact of each feature when added to the baseline. Tuning. [13] have shown that PRO tends to generate too short translations.4 They have suggested that the root of the problem was that PRO optimizes sentence-level BLEU+1, which smooths the precision component of BLEU, but leaves the brevity penalty intact, which destroys the balance between them. They have proposed a number of fixes, the simplest and most efficient among them being to smooth the brevity penalty as well.5 In our experiments, this yielded +0.2 BLEU for Arabic-to-English on tst2010. Operation sequence model. The operation sequence model (OSM) is an n-gram-based model, which represents the al"
2013.iwslt-evaluation.8,P13-2003,1,0.839319,"Missing"
2013.iwslt-evaluation.8,P13-2071,0,0.0200742,"of operations, e.g., generate a sequence of source and target words or perform reordering. The model memorizes Markov chains over such sequences, thus fusing lexical generation and reordering into a single generative model. OSM offers two advantages. First, it considers bilingual contextual information that goes beyond phrase boundaries. Second, it provides a better reordering mechanism that has richer conditioning than a lexicalized reordering model: the probability of an operation is conditioned on the n previous translation and reordering decisions. We used the Moses implementation of OSM [15], which has yielded improvements at WMT’13 [16]. In our experiments, it yielded +0.6 BLEU for Arabic-to-English on tst2010. Minimum Bayes risk decoding. We also experimented with minimum Bayes risk decoding (MBR)[17], which, instead of outputting the translation with the highest probability, prefers the one that is most similar to best n translations. In our case, using MBR did not improve over the baseline. Translation options per input phrase. By default, Moses uses up to 20 translation options per input phrase, but [16] have shown better results with 100. In our experiments, this yielded +0"
2013.iwslt-evaluation.8,W13-2212,0,0.0142268,"urce and target words or perform reordering. The model memorizes Markov chains over such sequences, thus fusing lexical generation and reordering into a single generative model. OSM offers two advantages. First, it considers bilingual contextual information that goes beyond phrase boundaries. Second, it provides a better reordering mechanism that has richer conditioning than a lexicalized reordering model: the probability of an operation is conditioned on the n previous translation and reordering decisions. We used the Moses implementation of OSM [15], which has yielded improvements at WMT’13 [16]. In our experiments, it yielded +0.6 BLEU for Arabic-to-English on tst2010. Minimum Bayes risk decoding. We also experimented with minimum Bayes risk decoding (MBR)[17], which, instead of outputting the translation with the highest probability, prefers the one that is most similar to best n translations. In our case, using MBR did not improve over the baseline. Translation options per input phrase. By default, Moses uses up to 20 translation options per input phrase, but [16] have shown better results with 100. In our experiments, this yielded +0.1 BLEU for Arabic-to-English on tst2010. Trans"
2013.iwslt-evaluation.8,N04-1022,0,0.108744,"e model. OSM offers two advantages. First, it considers bilingual contextual information that goes beyond phrase boundaries. Second, it provides a better reordering mechanism that has richer conditioning than a lexicalized reordering model: the probability of an operation is conditioned on the n previous translation and reordering decisions. We used the Moses implementation of OSM [15], which has yielded improvements at WMT’13 [16]. In our experiments, it yielded +0.6 BLEU for Arabic-to-English on tst2010. Minimum Bayes risk decoding. We also experimented with minimum Bayes risk decoding (MBR)[17], which, instead of outputting the translation with the highest probability, prefers the one that is most similar to best n translations. In our case, using MBR did not improve over the baseline. Translation options per input phrase. By default, Moses uses up to 20 translation options per input phrase, but [16] have shown better results with 100. In our experiments, this yielded +0.1 BLEU for Arabic-to-English on tst2010. Transliterating OOVs. Out-of-vocabulary (OOV) words are problematic for languages with different scripts. Thus, we tried transliteration as post-processing: we extracted 1-1"
2013.iwslt-evaluation.8,P11-1044,1,0.871958,"o best n translations. In our case, using MBR did not improve over the baseline. Translation options per input phrase. By default, Moses uses up to 20 translation options per input phrase, but [16] have shown better results with 100. In our experiments, this yielded +0.1 BLEU for Arabic-to-English on tst2010. Transliterating OOVs. Out-of-vocabulary (OOV) words are problematic for languages with different scripts. Thus, we tried transliteration as post-processing: we extracted 1-1 word alignments from a subset of the UN bitext, and we used them to train a character-level transliteration system [18, 19] using Moses. As Table 3 shows this did not help, probably due to the small number of OOVs in tst2010. 4 See [14] for a discussion about more potential issues with PRO. --proargs=’--smooth-brevity-penalty’ 5 Available in Moses: System Baseline (B) OSM MBR Ttable 100 PRO-fix [13] TRANSLIT Drop UNK Arabic-English (tst2010) BLEU 1-TER 24.7 25.3 24.7 24.8 24.9 24.7 24.8 Arabic-English (tst2010) BLEU 1-TER System 45.6 46.1 45.7 45.6 44.7 45.6 45.7 SEG-D0 SEG-D1 SEG-D2 SEG-D3 SEG-S2 SEG-ATB 22.4 23.6 24.1 24.4 24.5 24.7 43.0 44.2 45.2 45.5 45.7 45.6 Table 5: Using different Arabic segmentation schem"
2013.iwslt-evaluation.8,P12-1049,1,0.860515,"o best n translations. In our case, using MBR did not improve over the baseline. Translation options per input phrase. By default, Moses uses up to 20 translation options per input phrase, but [16] have shown better results with 100. In our experiments, this yielded +0.1 BLEU for Arabic-to-English on tst2010. Transliterating OOVs. Out-of-vocabulary (OOV) words are problematic for languages with different scripts. Thus, we tried transliteration as post-processing: we extracted 1-1 word alignments from a subset of the UN bitext, and we used them to train a character-level transliteration system [18, 19] using Moses. As Table 3 shows this did not help, probably due to the small number of OOVs in tst2010. 4 See [14] for a discussion about more potential issues with PRO. --proargs=’--smooth-brevity-penalty’ 5 Available in Moses: System Baseline (B) OSM MBR Ttable 100 PRO-fix [13] TRANSLIT Drop UNK Arabic-English (tst2010) BLEU 1-TER 24.7 25.3 24.7 24.8 24.9 24.7 24.8 Arabic-English (tst2010) BLEU 1-TER System 45.6 46.1 45.7 45.6 44.7 45.6 45.7 SEG-D0 SEG-D1 SEG-D2 SEG-D3 SEG-S2 SEG-ATB 22.4 23.6 24.1 24.4 24.5 24.7 43.0 44.2 45.2 45.5 45.7 45.6 Table 5: Using different Arabic segmentation schem"
2013.iwslt-evaluation.8,P03-1021,0,0.0120716,"hierarchical cdec decoder [3]. We used its default features: forward and backward translation features, singleton features, a glue-rule probability, and a pass-through feature (to handle OOVs). We tuned the parameters using MIRA with IBM BLEU as the objective function and a k-best forest size of 250. Jane. We also used another hierarchical phrase-based decoder: Jane 2.2 [2]. We used the standard features: phrase translation probabilities and lexical smoothing in both directions, word and phrase penalties, a distance-based distortion model, and a 5-gram LM. We optimized the weights using MERT [21] on 100-best candidates with BLEU as objective. 5. Arabic Segmentation 7. Adaptation In Arabic, various clitics such as pronouns, conjunctions and articles appear concatenated to content words such as nouns and verbs. This can cause data sparseness issues, and thus clitics are typically segmented in a preprocessing step. There are various standard segmentation schemes defined in MADA [4, 5] such as D0, D1, D2, D3 and S2, for which we used the MADA+TOKAN toolkit [20], as well as ATB, which we performed using the Stanford segmenter [6]. Table 5 shows the results when training on the TED bitext o"
2013.iwslt-evaluation.8,P10-2041,0,0.0888063,"Missing"
2013.iwslt-evaluation.8,D11-1033,0,0.090854,"Missing"
2013.iwslt-evaluation.8,W08-0320,1,0.891463,"Missing"
2013.iwslt-evaluation.8,D09-1141,1,0.919233,"Missing"
2013.iwslt-evaluation.8,W09-0408,0,0.0608322,"Missing"
2013.iwslt-evaluation.8,W12-5611,0,0.0448251,"could build a strong LM through interpolation, similarly to our Arabic-to-English LM, that also used the Gigaword Arabic, UN, and News Commentary data (see Table 1). Desegmentation. Unlike the Arabic-to-English direction, where the segmentation was on the input side and thus the output was unaffected, here the segmentation had to be undone. For example, if we use an ATB-segmented target side, we end up with an ATB-segmented translation output, which we have to desegment in order to obtain proper Arabic. Desegmentation is not a trivial task since it involves some morphological adjustments, see [27] for a broader discussion. For desegmentation, we used the best approach described in [27]; in fact, we used their implementation. Normalization. Translating into Arabic is tricky because the Arabic spelling is often inconsistent in terms of punctuation (using both Arabic UTF8 and English punctuation symbols), digits (appearing as both Arabic and Indian characters), diacritics (can be used or omitted, and can often be wrong), spelling (there are many errors in the spelling of some Arabic characters, esp. Alef and Ta Marbuta; also, Waa appears sometimes separated). These problems are especially"
2013.iwslt-papers.2,2010.iwslt-evaluation.1,0,0.145712,"sed for this task, and also observe an absolute improvement of 1.6 BLEU when it is used in combination with TED data. Finally, we analyze some of the specific challenges when translating the educational content. 1. Introduction Lecture Translation has become an active field of research in the wider area of Speech Translation [1, 2]. This is demonstrated by large scale projects like the EU-funded translectures [3] and by evaluation campaigns like the one organized as part of the International Workshop on Spoken Language Translation (IWSLT), which introduced the challenge to translate TED talks [4] for the 2010 competition. However, the main limitation for the success of these projects continues to be the access to high quality training data. With the emergence of Massive Online Open Courses (MOOCs), thousands of video lectures have already been generated. Sites like Khan Academy1 , Coursera2 , Udacity3 , etc., continuously increase their repertoire of lectures, which range from basic math and science topics, to more advanced topics like machine learning, also covering history, economy, psychology, medicine, and more. Online education has bridged the geographical and financial gap, enab"
2013.iwslt-papers.2,federico-etal-2012-iwslt,0,0.0409166,"les are not available. It also can support volunteer translators, by providing an initial translation, which then can be post-edited [5]. Thus, SMT has the potential to increase the penetration of educational content, allowing it to reach a wider audience. To achieve this, an SMT system requires a large quantity of high-quality in-domain training data. Unfortunately, large data for machine translation has traditionally been constrained to domains such as legal documents, parliamentary proceedings and news. So far, the only openly accessible corpus for the lecture domain has been the TED talks [6]. In this paper, we introduce a new parallel corpus of subtitles of educational videos: the AMARA corpus for online educational content. We crawl a collection of multilingual community-generated subtitles6 . Furthermore, we explore the steps necessary to build corpora suitable for Machine Translation by processing the Arabic-English part of the multilingual collection. This yields a parallel corpus of about 2.6M Arabic and 3.9M English words. We explore different approaches to align the subtitles, and verify the quality of the generated parallel corpus by building translation models, and extri"
2013.iwslt-papers.2,2012.eamt-1.60,0,0.110653,"Loop (CHIL) [7], which consists of recordings and transcriptions of technical seminars and meetings in English. The content of the corpus includes a variety of topics: from audio and visual technologies to biology and finance. It is available through ELRA7 to its members. More recently, the IWSLT10 [4] evaluation campaign has turned its attention to the lecture and seminar domain by focusing on TED talks. To support this task, a collection of lecture translations has been automatically crawled from the TED website in a variety of languages and made publicly available through the WIT3 project [8]. In this paper, we used such data as a point of comparison. We crawl parallel subtitles of educational videos and use several measures to show the quality of the crawled corpus in comparison with the closely related IWSLT data set. In the past, multilingual corpora creation from usercontributed movie subtitles has been addressed by [9]. Recently, a large collection of parallel movie subtitles from the Opensrt8 community along with tools for alignment of these has been made available through the Opus project [10]. Combination of corpora to improve the translation model has been explored with r"
2013.iwslt-papers.2,tiedemann-2008-synchronizing,0,0.219263,"s attention to the lecture and seminar domain by focusing on TED talks. To support this task, a collection of lecture translations has been automatically crawled from the TED website in a variety of languages and made publicly available through the WIT3 project [8]. In this paper, we used such data as a point of comparison. We crawl parallel subtitles of educational videos and use several measures to show the quality of the crawled corpus in comparison with the closely related IWSLT data set. In the past, multilingual corpora creation from usercontributed movie subtitles has been addressed by [9]. Recently, a large collection of parallel movie subtitles from the Opensrt8 community along with tools for alignment of these has been made available through the Opus project [10]. Combination of corpora to improve the translation model has been explored with relative success in the past. For the NewsCommentary and OpenSrt corpora, [11] explore different ways to mix the phrase-table to adapt the Europarl corpus. For the Arabic-English IWSLT data, [12] achieve a relative improvement of 0.7 BLEU by mixing phrases from UN and IWSLT data using instance weighting with weights coming from the langu"
2013.iwslt-papers.2,P07-2045,0,0.0105846,"tional-fe. For language modeling, we trained a separate 5-gram Kneser-Ney smoothed LM model on each available corpus (target side of a training bi-text or monolingual dataset) using KenLM [21]; we then interpolated these mod10 We els minimizing the perplexity on the target side of the tuning dataset (IWSLT dev-2010). Finally, we built a large joint log-linear model, which used standard SMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. We used the phrase-based SMT model as implemented in the Moses toolkit [17] for translation, and reported evaluation results over two datasets. We reported BLEU calculated with respect of the original reference using NIST v13a, after detokenization and recasing of the system’s output. Tuning: We tuned the weights in the log-linear model by optimizing BLEU [22] on the tuning dataset, using PRO [23] with the fixed BLEU prosposed by [24]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000best lists for each iteration. Decoding: On tuning and testing, we used monotone-atpunctuation decoding (this had no impact on the translation length). On test"
2013.iwslt-papers.2,P12-1016,0,0.140562,"Missing"
2013.iwslt-papers.2,J93-2003,0,0.041392,"t on the training data 4.2. Experimental Setup Preprocessing: We tokenized the English side of all bi-texts as well as the monolingual data (GigaWord) for language modeling using the standard tokenizer of the Moses toolkit [17]. We further truecased this data by changing the casing of each sentence-initial word to its most frequent casing in the training corpus. For the Arabic side, we segmented the corpus following the ATB segmentation scheme with the Stanford word segmenter [18]. Training: We built separate directed word alignments for English→Arabic and for Arabic→English using IBM model 4 [19], and symmetrized them using grow-diag-final-and heuristic [20]. We extracted phrase pairs of maximum length seven. We scored these phrase pairs using maximum likelihood with Kneser-Ney smoothing,as implemented in the moses toolkit, thus obtaining a phrase table where each phrase-pair has the standard five translation model features. We also built a lexicalized reordering model : msdbidirectional-fe. For language modeling, we trained a separate 5-gram Kneser-Ney smoothed LM model on each available corpus (target side of a training bi-text or monolingual dataset) using KenLM [21]; we then inter"
2013.iwslt-papers.2,tiedemann-2012-parallel,0,0.10406,"ite in a variety of languages and made publicly available through the WIT3 project [8]. In this paper, we used such data as a point of comparison. We crawl parallel subtitles of educational videos and use several measures to show the quality of the crawled corpus in comparison with the closely related IWSLT data set. In the past, multilingual corpora creation from usercontributed movie subtitles has been addressed by [9]. Recently, a large collection of parallel movie subtitles from the Opensrt8 community along with tools for alignment of these has been made available through the Opus project [10]. Combination of corpora to improve the translation model has been explored with relative success in the past. For the NewsCommentary and OpenSrt corpora, [11] explore different ways to mix the phrase-table to adapt the Europarl corpus. For the Arabic-English IWSLT data, [12] achieve a relative improvement of 0.7 BLEU by mixing phrases from UN and IWSLT data using instance weighting with weights coming from the language model perplexity. In this paper, we present the experimental results from data gathered from publicly available crowd-generated data, that has proved to be useful for the lectu"
2013.iwslt-papers.2,N03-1017,0,0.0523188,"e tokenized the English side of all bi-texts as well as the monolingual data (GigaWord) for language modeling using the standard tokenizer of the Moses toolkit [17]. We further truecased this data by changing the casing of each sentence-initial word to its most frequent casing in the training corpus. For the Arabic side, we segmented the corpus following the ATB segmentation scheme with the Stanford word segmenter [18]. Training: We built separate directed word alignments for English→Arabic and for Arabic→English using IBM model 4 [19], and symmetrized them using grow-diag-final-and heuristic [20]. We extracted phrase pairs of maximum length seven. We scored these phrase pairs using maximum likelihood with Kneser-Ney smoothing,as implemented in the moses toolkit, thus obtaining a phrase table where each phrase-pair has the standard five translation model features. We also built a lexicalized reordering model : msdbidirectional-fe. For language modeling, we trained a separate 5-gram Kneser-Ney smoothed LM model on each available corpus (target side of a training bi-text or monolingual dataset) using KenLM [21]; we then interpolated these mod10 We els minimizing the perplexity on the tar"
2013.iwslt-papers.2,W12-3154,0,0.0338013,"allel subtitles of educational videos and use several measures to show the quality of the crawled corpus in comparison with the closely related IWSLT data set. In the past, multilingual corpora creation from usercontributed movie subtitles has been addressed by [9]. Recently, a large collection of parallel movie subtitles from the Opensrt8 community along with tools for alignment of these has been made available through the Opus project [10]. Combination of corpora to improve the translation model has been explored with relative success in the past. For the NewsCommentary and OpenSrt corpora, [11] explore different ways to mix the phrase-table to adapt the Europarl corpus. For the Arabic-English IWSLT data, [12] achieve a relative improvement of 0.7 BLEU by mixing phrases from UN and IWSLT data using instance weighting with weights coming from the language model perplexity. In this paper, we present the experimental results from data gathered from publicly available crowd-generated data, that has proved to be useful for the lecture domain, but that poses specific challenges, as it has a special focus on online education. 3. The AMARA Corpus Amara is a web-based platform for editing and"
2013.iwslt-papers.2,W11-2123,0,0.0748631,"ng IBM model 4 [19], and symmetrized them using grow-diag-final-and heuristic [20]. We extracted phrase pairs of maximum length seven. We scored these phrase pairs using maximum likelihood with Kneser-Ney smoothing,as implemented in the moses toolkit, thus obtaining a phrase table where each phrase-pair has the standard five translation model features. We also built a lexicalized reordering model : msdbidirectional-fe. For language modeling, we trained a separate 5-gram Kneser-Ney smoothed LM model on each available corpus (target side of a training bi-text or monolingual dataset) using KenLM [21]; we then interpolated these mod10 We els minimizing the perplexity on the target side of the tuning dataset (IWSLT dev-2010). Finally, we built a large joint log-linear model, which used standard SMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. We used the phrase-based SMT model as implemented in the Moses toolkit [17] for translation, and reported evaluation results over two datasets. We reported BLEU calculated with respect of the original reference using NIST v13a, after detokenization and recasing o"
2013.iwslt-papers.2,2012.iwslt-papers.7,0,0.0268125,"n with the closely related IWSLT data set. In the past, multilingual corpora creation from usercontributed movie subtitles has been addressed by [9]. Recently, a large collection of parallel movie subtitles from the Opensrt8 community along with tools for alignment of these has been made available through the Opus project [10]. Combination of corpora to improve the translation model has been explored with relative success in the past. For the NewsCommentary and OpenSrt corpora, [11] explore different ways to mix the phrase-table to adapt the Europarl corpus. For the Arabic-English IWSLT data, [12] achieve a relative improvement of 0.7 BLEU by mixing phrases from UN and IWSLT data using instance weighting with weights coming from the language model perplexity. In this paper, we present the experimental results from data gathered from publicly available crowd-generated data, that has proved to be useful for the lecture domain, but that poses specific challenges, as it has a special focus on online education. 3. The AMARA Corpus Amara is a web-based platform for editing and managing subtitles of online videos. It provides an easy-to-use interface, which allows users to collaboratively sub"
2013.iwslt-papers.2,P02-1040,0,0.091011,"uning dataset (IWSLT dev-2010). Finally, we built a large joint log-linear model, which used standard SMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. We used the phrase-based SMT model as implemented in the Moses toolkit [17] for translation, and reported evaluation results over two datasets. We reported BLEU calculated with respect of the original reference using NIST v13a, after detokenization and recasing of the system’s output. Tuning: We tuned the weights in the log-linear model by optimizing BLEU [22] on the tuning dataset, using PRO [23] with the fixed BLEU prosposed by [24]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000best lists for each iteration. Decoding: On tuning and testing, we used monotone-atpunctuation decoding (this had no impact on the translation length). On testing, we further used cube pruning. did not use the second test set for the experiments in this paper. For the baseline system, we trained the phrase and the reordering models on the IWSLT training dataset. The language model was trained on the English side of the IWSLT training data. We"
2013.iwslt-papers.2,J93-1004,0,0.461534,"Missing"
2013.iwslt-papers.2,D11-1125,0,0.042418,"y, we built a large joint log-linear model, which used standard SMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. We used the phrase-based SMT model as implemented in the Moses toolkit [17] for translation, and reported evaluation results over two datasets. We reported BLEU calculated with respect of the original reference using NIST v13a, after detokenization and recasing of the system’s output. Tuning: We tuned the weights in the log-linear model by optimizing BLEU [22] on the tuning dataset, using PRO [23] with the fixed BLEU prosposed by [24]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000best lists for each iteration. Decoding: On tuning and testing, we used monotone-atpunctuation decoding (this had no impact on the translation length). On testing, we further used cube pruning. did not use the second test set for the experiments in this paper. For the baseline system, we trained the phrase and the reordering models on the IWSLT training dataset. The language model was trained on the English side of the IWSLT training data. We tuned the weights on IWSLT-dev2010. B"
2013.iwslt-papers.2,C12-1121,1,0.888738,"odel, which used standard SMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. We used the phrase-based SMT model as implemented in the Moses toolkit [17] for translation, and reported evaluation results over two datasets. We reported BLEU calculated with respect of the original reference using NIST v13a, after detokenization and recasing of the system’s output. Tuning: We tuned the weights in the log-linear model by optimizing BLEU [22] on the tuning dataset, using PRO [23] with the fixed BLEU prosposed by [24]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000best lists for each iteration. Decoding: On tuning and testing, we used monotone-atpunctuation decoding (this had no impact on the translation length). On testing, we further used cube pruning. did not use the second test set for the experiments in this paper. For the baseline system, we trained the phrase and the reordering models on the IWSLT training dataset. The language model was trained on the English side of the IWSLT training data. We tuned the weights on IWSLT-dev2010. Below, we present the experimental resu"
2013.iwslt-papers.2,D09-1141,0,0.300772,"ARA only (T M1 ): Instead of using the IWSLT training data, we built the translation and reordering models using only the AMARA corpus. Concatenation (T M2 ): In this setting, we concatenated AMARA with IWSLT for training of the translation and reordering models. This generally improves word alignment, reduces OOV rate and improves translation quality if two corpora are from similar domain. However, if the added corpus is noisy or of out-of-domain, (e.g. UN data), we can observe a degradation in performance. Phrase table combination (T M3 ): We applied phrase table combination as described in [25]. We built two phrase tables and reordering models separately on the IWSLT and AMARA data. Then, we merged them by adding three additional indicator features to each entry to inform the decoder if the phrase was found in the first, second or both tables. This can be seen as a form of log-linear interpolation. SYS TM IW10 OOV AM13 OOV SYS LM IW10 AM13 B1 TM1 TM2 TM3 IWSLT AMARA IW+AM PT(IW,AM) 22.97 22.40 23.41 23.57 1.9 2.4 1.2 1.2 23.26 23.66 27.63 27.65 3.9 1.7 1.8 1.8 B1 LM1 LM2 LM3 LM4 IWSLT AMARA IWSLT+AMARA INTERPOL GW 22.97 22.83 23.69 23.59 24.24 23.26 24.05 25.90 25.62 24.79 Table 4:"
2015.mtsummit-papers.10,abdelali-etal-2014-amara,1,0.856126,"a standard task of translating German-to-English and Arabic-to-English IWSLT TED talks, we observed statistically significant improvements of up to +0.9 BLEU points. 1 Introduction Parallel data required to train Statistical Machine Translation (SMT) systems is often inadequate, and is typically collected opportunistically from wherever it is available. The conventional wisdom is that more data improves the translation quality. Additional data however, may not be best suited for tasks such as translating TED talks (Cettolo et al., 2014) or patents (Fujii et al., 2010) or educational content (Abdelali et al., 2014), and often come with the challenges of dealing with word-sense ambiguities and stylistic variance of other domains. When additional data, later referred as out-domain, is much larger than in-domain, the resultant distribution can get biased towards out-domain, yielding a sub-optimal system. Domain adaptation aims to preserve the identity of the in-domain data while using the best of the out-domain data. This is done by selecting a subset from the out-domain data, which is closer to the in-domain (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution i"
2015.mtsummit-papers.10,D11-1033,0,0.702839,"ni et al., 2002) improvements of up to +0.6 for DE-EN and +0.9 for AR-EN. • Log-linear variant performed better in the case of NNJM giving an average improvements of +0.4 BLEU points for DE-EN and +0.5 for AR-EN. • Linear interpolation for NNJM models was slightly behind its log-linear variant. Data Selection: • OSM-based selection performed better for AR-EN task giving an average improvement of +0.7 • NNJM performed better at the DE-EN task giving an average improvement of +0.6 points. • Both OSM- and NNJM-based selection gave slightly better results than Modified-MooreLewis (MML) selection (Axelrod et al., 2011). The rest of the paper is organized as follows. Section 2 briefly describes the OSM and the NNJM models. Section 3 describes mixture model and data selection techniques that we apply using the OSM and the NNJM models to carry out adaptation. Section 4 presents the results. Section 5 discusses related work and Section 6 concludes the paper. 2 Joint Sequence Models In this section, we revisit Operation Sequence and Neural Network Joint models briefly. 2.1 Operation Sequence Model The Operation Sequence Model (OSM) is a bilingual model that couples translation and reordering by representing them"
2015.mtsummit-papers.10,2014.iwslt-evaluation.6,1,0.864662,"52K 24K 32K 28K Table 2: Statistics of the German-English and Arabic-English training corpora in terms of Sentences and Tokens (Source/Target). Tokens are represented in Millions. ep = Europarl, cc = Common Crawl, un = United Nations and an output embedding layer of 750. Only one hidden layer is used with NCE4 to allow faster training and decoding. Training was done using mini-batch size of 1000 and using 100 noise samples. We train the out-domain NNJM models using the same vocabulary as the in-domain vocabulary. All models were trained for 25 epochs. Machine Translation Settings: We followed Birch et al. (2014) to train a Moses system Koehn et al. (2007) with the following settings: maximum sentence length of 80, Fast-Align (Dyer et al., 2013) for word-alignments, an interpolated Kneser-Ney smoothed 5-gram language model (Schwenk and Koehn, 2008) with KenLM (Heafield, 2011) for querying, lexicalized reordering (Galley and Manning, 2008) and other default parameters. We used Moses implementations of OSM and NNJM as a part of their respective baseline systems. Arabic OOVs were translated using an unsupervised transliteration module (Durrani et al., 2014b) in Moses. We used k-best batch MIRA (Cherry an"
2015.mtsummit-papers.10,2011.iwslt-evaluation.18,0,0.186872,"ata, an alternative way is to down-weight it and boost the data closer to the in-domain. It is robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013b) used vector space model for adaptation at phrase level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network training. They also used NNJM model as their basis. Other work on domain adaptation includes but n"
2015.mtsummit-papers.10,P13-1141,0,0.232794,"Missing"
2015.mtsummit-papers.10,N13-1114,0,0.591829,"EM-based weighting, (ii) using log-linear model inside the SMT pipeline. Secondly, we use cross-entropy difference (Moore and Lewis, 2010) between in- and out-domain models to perform data selection for domain adaptation. The bilingual property of the OSM and NNJM models gives them an edge over traditional LM-based methods, which do not capture source and target domain relevance jointly. The embedded reordering information modeled in OSM helps it to preserve reordering characteristic of the in-domain data. Capturing reordering variation across domains have been shown to be beneficial also by Chen et al. (2013a). NNJM adds a different dimension to it by semantically generalizing the data using distributed representation of words (Bengio et al., 2003). We evaluated our systems on a standard task of translating IWSLT TED talks for Germanto-English (DE-EN) and Arabic-to-English (AR-EN) language pairs. Below is a summary of our main findings: Model Weighting: • Linearly interpolating OSM models through EM-based weighting gave average BLEU (Papineni et al., 2002) improvements of up to +0.6 for DE-EN and +0.9 for AR-EN. • Log-linear variant performed better in the case of NNJM giving an average improveme"
2015.mtsummit-papers.10,P13-1126,0,0.590988,"EM-based weighting, (ii) using log-linear model inside the SMT pipeline. Secondly, we use cross-entropy difference (Moore and Lewis, 2010) between in- and out-domain models to perform data selection for domain adaptation. The bilingual property of the OSM and NNJM models gives them an edge over traditional LM-based methods, which do not capture source and target domain relevance jointly. The embedded reordering information modeled in OSM helps it to preserve reordering characteristic of the in-domain data. Capturing reordering variation across domains have been shown to be beneficial also by Chen et al. (2013a). NNJM adds a different dimension to it by semantically generalizing the data using distributed representation of words (Bengio et al., 2003). We evaluated our systems on a standard task of translating IWSLT TED talks for Germanto-English (DE-EN) and Arabic-to-English (AR-EN) language pairs. Below is a summary of our main findings: Model Weighting: • Linearly interpolating OSM models through EM-based weighting gave average BLEU (Papineni et al., 2002) improvements of up to +0.6 for DE-EN and +0.9 for AR-EN. • Log-linear variant performed better in the case of NNJM giving an average improveme"
2015.mtsummit-papers.10,N12-1047,0,0.0324171,"l. (2014) to train a Moses system Koehn et al. (2007) with the following settings: maximum sentence length of 80, Fast-Align (Dyer et al., 2013) for word-alignments, an interpolated Kneser-Ney smoothed 5-gram language model (Schwenk and Koehn, 2008) with KenLM (Heafield, 2011) for querying, lexicalized reordering (Galley and Manning, 2008) and other default parameters. We used Moses implementations of OSM and NNJM as a part of their respective baseline systems. Arabic OOVs were translated using an unsupervised transliteration module (Durrani et al., 2014b) in Moses. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning.5 4.1 Results: Model Weighting We first discuss the results of applying mixture modeling approach. The MT systems are trained on a concatenation of all in- and out-domain data. The OSM and NNJM models used in baseline MT systems were also trained on the concatenated data. Linear interpolation (OSMln ) based on EM-weighting shows significant improvements with average BLEU gains of +0.6 in DE-EN and +0.9 in AR-EN over the baseline system Bcat (see Table 3).6 One reason for better gains in AR-EN is the fact that the out-domain UN data 4 Training NNJM with backpropagation could be proh"
2015.mtsummit-papers.10,J81-4005,0,0.515198,"Missing"
2015.mtsummit-papers.10,P14-1129,0,0.128935,". Domain adaptation aims to preserve the identity of the in-domain data while using the best of the out-domain data. This is done by selecting a subset from the out-domain data, which is closer to the in-domain (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Bilingual sequence models (Mari˜no et al., 2006) have shown to be effective in improving the quality of machine translation and have achieved state-of-the-art performance recently (Le et al., 2012; Durrani et al., 2013; Devlin et al., 2014). Their ability to capture non-local dependencies makes them superior to the traditional phrase-based models, which do not consider contextual information across phrasal boundaries. Two such models that we explore in this paper are (i) the Operation Sequence Model or OSM (Durrani et al., 2011) — a markov translation model that integrates reordering, and (ii) the Neural Network Joint Model or NNJM (Devlin et al., 2014) — a continuous space model that learns neural network over augmented streams of source and target sequences. Both models are used as additional language model (LM) features insid"
2015.mtsummit-papers.10,P13-2119,0,0.300031,"based on the resulting scores. The MT system can then be trained on a subset of the out-domain data that is closer to in-domain. Selection based methods can be helpful to reduce computational cost when training is expensive and also when memory is constrained. Data selection was earlier done for language modeling using information retrieval techniques (Hildebrand et al., 2005) and using perplexity measure (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source side and target side language models. Duh et al. (2013) used recurrent neural network language model instead of an ngram-based language model to do the same. Translation model features were used recently by Liu et al. (2014); Hoang and Sima’an (2014) to do data selection. 5.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time consuming process. Therefore rather than filtering less useful data, an alternative way is to down-weight it and boost the data closer to the in-domain. It is robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards th"
2015.mtsummit-papers.10,P13-2071,1,0.880717,"g a sub-optimal system. Domain adaptation aims to preserve the identity of the in-domain data while using the best of the out-domain data. This is done by selecting a subset from the out-domain data, which is closer to the in-domain (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Bilingual sequence models (Mari˜no et al., 2006) have shown to be effective in improving the quality of machine translation and have achieved state-of-the-art performance recently (Le et al., 2012; Durrani et al., 2013; Devlin et al., 2014). Their ability to capture non-local dependencies makes them superior to the traditional phrase-based models, which do not consider contextual information across phrasal boundaries. Two such models that we explore in this paper are (i) the Operation Sequence Model or OSM (Durrani et al., 2011) — a markov translation model that integrates reordering, and (ii) the Neural Network Joint Model or NNJM (Devlin et al., 2014) — a continuous space model that learns neural network over augmented streams of source and target sequences. Both models are used as additional language mod"
2015.mtsummit-papers.10,C14-1041,1,0.833548,"epochs. Machine Translation Settings: We followed Birch et al. (2014) to train a Moses system Koehn et al. (2007) with the following settings: maximum sentence length of 80, Fast-Align (Dyer et al., 2013) for word-alignments, an interpolated Kneser-Ney smoothed 5-gram language model (Schwenk and Koehn, 2008) with KenLM (Heafield, 2011) for querying, lexicalized reordering (Galley and Manning, 2008) and other default parameters. We used Moses implementations of OSM and NNJM as a part of their respective baseline systems. Arabic OOVs were translated using an unsupervised transliteration module (Durrani et al., 2014b) in Moses. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning.5 4.1 Results: Model Weighting We first discuss the results of applying mixture modeling approach. The MT systems are trained on a concatenation of all in- and out-domain data. The OSM and NNJM models used in baseline MT systems were also trained on the concatenated data. Linear interpolation (OSMln ) based on EM-weighting shows significant improvements with average BLEU gains of +0.6 in DE-EN and +0.9 in AR-EN over the baseline system Bcat (see Table 3).6 One reason for better gains in AR-EN is the fact that the out-d"
2015.mtsummit-papers.10,P11-1105,1,0.931772,"y distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Bilingual sequence models (Mari˜no et al., 2006) have shown to be effective in improving the quality of machine translation and have achieved state-of-the-art performance recently (Le et al., 2012; Durrani et al., 2013; Devlin et al., 2014). Their ability to capture non-local dependencies makes them superior to the traditional phrase-based models, which do not consider contextual information across phrasal boundaries. Two such models that we explore in this paper are (i) the Operation Sequence Model or OSM (Durrani et al., 2011) — a markov translation model that integrates reordering, and (ii) the Neural Network Joint Model or NNJM (Devlin et al., 2014) — a continuous space model that learns neural network over augmented streams of source and target sequences. Both models are used as additional language model (LM) features inside the SMT decoder. Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 117 The diversity of the two models, i.e., OSM with embedded reordering information and NNJM with continuous space modeling, makes them interesting to be explored for domain adaptation."
2015.mtsummit-papers.10,N13-1073,0,0.0478691,"arget). Tokens are represented in Millions. ep = Europarl, cc = Common Crawl, un = United Nations and an output embedding layer of 750. Only one hidden layer is used with NCE4 to allow faster training and decoding. Training was done using mini-batch size of 1000 and using 100 noise samples. We train the out-domain NNJM models using the same vocabulary as the in-domain vocabulary. All models were trained for 25 epochs. Machine Translation Settings: We followed Birch et al. (2014) to train a Moses system Koehn et al. (2007) with the following settings: maximum sentence length of 80, Fast-Align (Dyer et al., 2013) for word-alignments, an interpolated Kneser-Ney smoothed 5-gram language model (Schwenk and Koehn, 2008) with KenLM (Heafield, 2011) for querying, lexicalized reordering (Galley and Manning, 2008) and other default parameters. We used Moses implementations of OSM and NNJM as a part of their respective baseline systems. Arabic OOVs were translated using an unsupervised transliteration module (Durrani et al., 2014b) in Moses. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning.5 4.1 Results: Model Weighting We first discuss the results of applying mixture modeling approach. The MT sy"
2015.mtsummit-papers.10,P12-2023,0,0.0681817,"adaptation (Mansour and Ney, 2013). Chen et al. (2013b) used vector space model for adaptation at phrase level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network training. They also used NNJM model as their basis. Other work on domain adaptation includes but not limited to studies that focus on topic modeling (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation where no in-domain data is available (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion We targeted an unexplored area of using bilingual language models for domain adaptation. We applied model weighting and data selection techniques using OSM and NNJM models. Both methods were shown to be effective in the target translation tasks. Interpolating multi-domain models gave an average improvement of up to +0.9 BLEU points using OSM and +0.5 using NNJM. We also used NNJM and OSM models for data select"
2015.mtsummit-papers.10,W08-0334,0,0.165313,"l cut-off threshold is a time consuming process. Therefore rather than filtering less useful data, an alternative way is to down-weight it and boost the data closer to the in-domain. It is robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013b) used vector space model for adaptation at phrase level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network traini"
2015.mtsummit-papers.10,D10-1044,0,0.19196,"ion model features were used recently by Liu et al. (2014); Hoang and Sima’an (2014) to do data selection. 5.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time consuming process. Therefore rather than filtering less useful data, an alternative way is to down-weight it and boost the data closer to the in-domain. It is robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013b) used vector space model for adaptation at phrase level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixtu"
2015.mtsummit-papers.10,W07-0717,0,0.155727,"enges of dealing with word-sense ambiguities and stylistic variance of other domains. When additional data, later referred as out-domain, is much larger than in-domain, the resultant distribution can get biased towards out-domain, yielding a sub-optimal system. Domain adaptation aims to preserve the identity of the in-domain data while using the best of the out-domain data. This is done by selecting a subset from the out-domain data, which is closer to the in-domain (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Bilingual sequence models (Mari˜no et al., 2006) have shown to be effective in improving the quality of machine translation and have achieved state-of-the-art performance recently (Le et al., 2012; Durrani et al., 2013; Devlin et al., 2014). Their ability to capture non-local dependencies makes them superior to the traditional phrase-based models, which do not consider contextual information across phrasal boundaries. Two such models that we explore in this paper are (i) the Operation Sequence Model or OSM (Durrani et al., 2011) — a markov translation model that integrates r"
2015.mtsummit-papers.10,W09-0439,0,0.123872,"filtering less useful data, an alternative way is to down-weight it and boost the data closer to the in-domain. It is robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013b) used vector space model for adaptation at phrase level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network training. They also used NNJM model as their basis. Other work on domain ada"
2015.mtsummit-papers.10,D08-1089,0,0.0222621,"r training and decoding. Training was done using mini-batch size of 1000 and using 100 noise samples. We train the out-domain NNJM models using the same vocabulary as the in-domain vocabulary. All models were trained for 25 epochs. Machine Translation Settings: We followed Birch et al. (2014) to train a Moses system Koehn et al. (2007) with the following settings: maximum sentence length of 80, Fast-Align (Dyer et al., 2013) for word-alignments, an interpolated Kneser-Ney smoothed 5-gram language model (Schwenk and Koehn, 2008) with KenLM (Heafield, 2011) for querying, lexicalized reordering (Galley and Manning, 2008) and other default parameters. We used Moses implementations of OSM and NNJM as a part of their respective baseline systems. Arabic OOVs were translated using an unsupervised transliteration module (Durrani et al., 2014b) in Moses. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning.5 4.1 Results: Model Weighting We first discuss the results of applying mixture modeling approach. The MT systems are trained on a concatenation of all in- and out-domain data. The OSM and NNJM models used in baseline MT systems were also trained on the concatenated data. Linear interpolation (OSMln ) ba"
2015.mtsummit-papers.10,P14-1066,0,0.0237562,"his allows the model to learn very rich translation and reordering patterns. Moreover, the model is based on minimal translation units (MTUs) and considers source and target contextual information across phrasal boundaries, thus addressing phrasal independence assumption and spurious segmentation problems in traditional phrase-based MT. 2.2 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to MT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013; Socher et al., 2013; Hinton et al., 2012). A bilingual Neural Network Joint model for MT was recently proposed by Devlin et al. (2014). It learns a feedforward neural network from augmented streams of source and target sequences. For a bilingual sentence pair (S, T ), NNJM defines a conditional probability distribution: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−n+1 , si ) i=1 where, si is an m-word source window for a target word ti based on the one-to-one alignment between T and S. Each input word in the context has a D dimensional ("
2015.mtsummit-papers.10,E14-1035,0,0.0822255,"Ney, 2013). Chen et al. (2013b) used vector space model for adaptation at phrase level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network training. They also used NNJM model as their basis. Other work on domain adaptation includes but not limited to studies that focus on topic modeling (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation where no in-domain data is available (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion We targeted an unexplored area of using bilingual language models for domain adaptation. We applied model weighting and data selection techniques using OSM and NNJM models. Both methods were shown to be effective in the target translation tasks. Interpolating multi-domain models gave an average improvement of up to +0.9 BLEU points using OSM and +0.5 using NNJM. We also used NNJM and OSM models for data selection using differences"
2015.mtsummit-papers.10,W11-2123,0,0.0290447,"Only one hidden layer is used with NCE4 to allow faster training and decoding. Training was done using mini-batch size of 1000 and using 100 noise samples. We train the out-domain NNJM models using the same vocabulary as the in-domain vocabulary. All models were trained for 25 epochs. Machine Translation Settings: We followed Birch et al. (2014) to train a Moses system Koehn et al. (2007) with the following settings: maximum sentence length of 80, Fast-Align (Dyer et al., 2013) for word-alignments, an interpolated Kneser-Ney smoothed 5-gram language model (Schwenk and Koehn, 2008) with KenLM (Heafield, 2011) for querying, lexicalized reordering (Galley and Manning, 2008) and other default parameters. We used Moses implementations of OSM and NNJM as a part of their respective baseline systems. Arabic OOVs were translated using an unsupervised transliteration module (Durrani et al., 2014b) in Moses. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning.5 4.1 Results: Model Weighting We first discuss the results of applying mixture modeling approach. The MT systems are trained on a concatenation of all in- and out-domain data. The OSM and NNJM models used in baseline MT systems were also tr"
2015.mtsummit-papers.10,2005.eamt-1.19,1,0.837721,"be an effective way to discard poor quality or irrelevant training instances, which when included in the MT systems, hurts its performance. The idea is to score the out-domain data using model trained from the in-domain data and apply a cut-off based on the resulting scores. The MT system can then be trained on a subset of the out-domain data that is closer to in-domain. Selection based methods can be helpful to reduce computational cost when training is expensive and also when memory is constrained. Data selection was earlier done for language modeling using information retrieval techniques (Hildebrand et al., 2005) and using perplexity measure (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source side and target side language models. Duh et al. (2013) used recurrent neural network language model instead of an ngram-based language model to do the same. Translation model features were used recently by Liu et al. (2014); Hoang and Sima’an (2014) to do data selection. 5.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time consuming process. Therefore rather th"
2015.mtsummit-papers.10,C14-1182,0,0.626827,"Missing"
2015.mtsummit-papers.10,D15-1147,1,0.427467,"e out-domain data that is unknown to the in-domain OSM, gets high probability7 and is ranked higher in the search space. On the contrary, the same gets down-weighted in a linearly interpolated global model. Both linear and log-linear interpolation of the NNJM models showed improvements over the baseline system Bcat (refer to Table 4). Log-linear interpolation (NNJMlg ) performed slightly better in both cases. Notice that NNJMlg does not face the same problem as OSMlg because all NNJM models are trained using the in-domain vocabulary with a low probability assigned to the out-domain UNKs.8 See Joty et al. (2015) for more details on our novel handling 7 Due to probability mass assigned to UNK sequences. order to reduce the training time and to learn better word representations, neural models are trained on most frequent vocabulary words only and low frequency words are represented under a class of unknown words, unk. This results in a large number of n-gram sequences containing at least one unk word and thereby, makes unk a highly probable word for the model. As a result of this discrepancy, sentences with more number of unk words will be selected. To solve this problem we created a separate class for"
2015.mtsummit-papers.10,D13-1176,0,0.0465297,"tion or reordering) decisions. This allows the model to learn very rich translation and reordering patterns. Moreover, the model is based on minimal translation units (MTUs) and considers source and target contextual information across phrasal boundaries, thus addressing phrasal independence assumption and spurious segmentation problems in traditional phrase-based MT. 2.2 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to MT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013; Socher et al., 2013; Hinton et al., 2012). A bilingual Neural Network Joint model for MT was recently proposed by Devlin et al. (2014). It learns a feedforward neural network from augmented streams of source and target sequences. For a bilingual sentence pair (S, T ), NNJM defines a conditional probability distribution: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−n+1 , si ) i=1 where, si is an m-word source window for a target word ti based on the one-to-one alignment between T and S. Each input word in the context has"
2015.mtsummit-papers.10,P07-2045,0,0.00623946,"erman-English and Arabic-English training corpora in terms of Sentences and Tokens (Source/Target). Tokens are represented in Millions. ep = Europarl, cc = Common Crawl, un = United Nations and an output embedding layer of 750. Only one hidden layer is used with NCE4 to allow faster training and decoding. Training was done using mini-batch size of 1000 and using 100 noise samples. We train the out-domain NNJM models using the same vocabulary as the in-domain vocabulary. All models were trained for 25 epochs. Machine Translation Settings: We followed Birch et al. (2014) to train a Moses system Koehn et al. (2007) with the following settings: maximum sentence length of 80, Fast-Align (Dyer et al., 2013) for word-alignments, an interpolated Kneser-Ney smoothed 5-gram language model (Schwenk and Koehn, 2008) with KenLM (Heafield, 2011) for querying, lexicalized reordering (Galley and Manning, 2008) and other default parameters. We used Moses implementations of OSM and NNJM as a part of their respective baseline systems. Arabic OOVs were translated using an unsupervised transliteration module (Durrani et al., 2014b) in Moses. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning.5 4.1 Results: Mo"
2015.mtsummit-papers.10,N12-1005,0,0.0261074,"t-domain, yielding a sub-optimal system. Domain adaptation aims to preserve the identity of the in-domain data while using the best of the out-domain data. This is done by selecting a subset from the out-domain data, which is closer to the in-domain (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Bilingual sequence models (Mari˜no et al., 2006) have shown to be effective in improving the quality of machine translation and have achieved state-of-the-art performance recently (Le et al., 2012; Durrani et al., 2013; Devlin et al., 2014). Their ability to capture non-local dependencies makes them superior to the traditional phrase-based models, which do not consider contextual information across phrasal boundaries. Two such models that we explore in this paper are (i) the Operation Sequence Model or OSM (Durrani et al., 2011) — a markov translation model that integrates reordering, and (ii) the Neural Network Joint Model or NNJM (Devlin et al., 2014) — a continuous space model that learns neural network over augmented streams of source and target sequences. Both models are used as a"
2015.mtsummit-papers.10,P14-2093,0,0.222498,"to reduce computational cost when training is expensive and also when memory is constrained. Data selection was earlier done for language modeling using information retrieval techniques (Hildebrand et al., 2005) and using perplexity measure (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source side and target side language models. Duh et al. (2013) used recurrent neural network language model instead of an ngram-based language model to do the same. Translation model features were used recently by Liu et al. (2014); Hoang and Sima’an (2014) to do data selection. 5.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time consuming process. Therefore rather than filtering less useful data, an alternative way is to down-weight it and boost the data closer to the in-domain. It is robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases ra"
2015.mtsummit-papers.10,N13-1074,0,0.30113,"ain. It is robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013b) used vector space model for adaptation at phrase level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network training. They also used NNJM model as their basis. Other work on domain adaptation includes but not limited to studies that focus on topic modeling (Eidelman et al., 2012; Hasler et a"
2015.mtsummit-papers.10,J06-4004,0,0.0704009,"Missing"
2015.mtsummit-papers.10,C14-1105,0,0.264728,"pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network training. They also used NNJM model as their basis. Other work on domain adaptation includes but not limited to studies that focus on topic modeling (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation where no in-domain data is available (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion We targeted an unexplored area of using bilingual language models for domain adaptation. We applied model weighting and data selection techniques using OSM and NNJM models. Both methods were shown to be effective in the target translation tasks. Interpolating multi-domain models gave an average improvement of up to +0.9 BLEU points using OSM and +0.5 using NNJM. We also used NNJM and OSM models for data selection using differences in cross entropy and showed improvements of up to +0.6 BLEU points. The code will be contributed to Mo"
2015.mtsummit-papers.10,D09-1074,0,0.345709,"tolo et al., 2014) or patents (Fujii et al., 2010) or educational content (Abdelali et al., 2014), and often come with the challenges of dealing with word-sense ambiguities and stylistic variance of other domains. When additional data, later referred as out-domain, is much larger than in-domain, the resultant distribution can get biased towards out-domain, yielding a sub-optimal system. Domain adaptation aims to preserve the identity of the in-domain data while using the best of the out-domain data. This is done by selecting a subset from the out-domain data, which is closer to the in-domain (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Bilingual sequence models (Mari˜no et al., 2006) have shown to be effective in improving the quality of machine translation and have achieved state-of-the-art performance recently (Le et al., 2012; Durrani et al., 2013; Devlin et al., 2014). Their ability to capture non-local dependencies makes them superior to the traditional phrase-based models, which do not consider contextual information across phrasal boundaries. Two such models that we explore"
2015.mtsummit-papers.10,P10-2041,0,0.680232,"tents (Fujii et al., 2010) or educational content (Abdelali et al., 2014), and often come with the challenges of dealing with word-sense ambiguities and stylistic variance of other domains. When additional data, later referred as out-domain, is much larger than in-domain, the resultant distribution can get biased towards out-domain, yielding a sub-optimal system. Domain adaptation aims to preserve the identity of the in-domain data while using the best of the out-domain data. This is done by selecting a subset from the out-domain data, which is closer to the in-domain (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Bilingual sequence models (Mari˜no et al., 2006) have shown to be effective in improving the quality of machine translation and have achieved state-of-the-art performance recently (Le et al., 2012; Durrani et al., 2013; Devlin et al., 2014). Their ability to capture non-local dependencies makes them superior to the traditional phrase-based models, which do not consider contextual information across phrasal boundaries. Two such models that we explore in this paper are (i) th"
2015.mtsummit-papers.10,D09-1141,0,0.0657375,"time consuming process. Therefore rather than filtering less useful data, an alternative way is to down-weight it and boost the data closer to the in-domain. It is robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013b) used vector space model for adaptation at phrase level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network training. They also used NN"
2015.mtsummit-papers.10,P02-1040,0,0.102972,"Missing"
2015.mtsummit-papers.10,C12-2104,0,0.064795,"el to learn very rich translation and reordering patterns. Moreover, the model is based on minimal translation units (MTUs) and considers source and target contextual information across phrasal boundaries, thus addressing phrasal independence assumption and spurious segmentation problems in traditional phrase-based MT. 2.2 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to MT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013; Socher et al., 2013; Hinton et al., 2012). A bilingual Neural Network Joint model for MT was recently proposed by Devlin et al. (2014). It learns a feedforward neural network from augmented streams of source and target sequences. For a bilingual sentence pair (S, T ), NNJM defines a conditional probability distribution: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−n+1 , si ) i=1 where, si is an m-word source window for a target word ti based on the one-to-one alignment between T and S. Each input word in the context has a D dimensional (continuous-valu"
2015.mtsummit-papers.10,I08-2089,0,0.0130472,"nd an output embedding layer of 750. Only one hidden layer is used with NCE4 to allow faster training and decoding. Training was done using mini-batch size of 1000 and using 100 noise samples. We train the out-domain NNJM models using the same vocabulary as the in-domain vocabulary. All models were trained for 25 epochs. Machine Translation Settings: We followed Birch et al. (2014) to train a Moses system Koehn et al. (2007) with the following settings: maximum sentence length of 80, Fast-Align (Dyer et al., 2013) for word-alignments, an interpolated Kneser-Ney smoothed 5-gram language model (Schwenk and Koehn, 2008) with KenLM (Heafield, 2011) for querying, lexicalized reordering (Galley and Manning, 2008) and other default parameters. We used Moses implementations of OSM and NNJM as a part of their respective baseline systems. Arabic OOVs were translated using an unsupervised transliteration module (Durrani et al., 2014b) in Moses. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning.5 4.1 Results: Model Weighting We first discuss the results of applying mixture modeling approach. The MT systems are trained on a concatenation of all in- and out-domain data. The OSM and NNJM models used in base"
2015.mtsummit-papers.10,E12-1055,0,0.662401,"ord-sense ambiguities and stylistic variance of other domains. When additional data, later referred as out-domain, is much larger than in-domain, the resultant distribution can get biased towards out-domain, yielding a sub-optimal system. Domain adaptation aims to preserve the identity of the in-domain data while using the best of the out-domain data. This is done by selecting a subset from the out-domain data, which is closer to the in-domain (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Bilingual sequence models (Mari˜no et al., 2006) have shown to be effective in improving the quality of machine translation and have achieved state-of-the-art performance recently (Le et al., 2012; Durrani et al., 2013; Devlin et al., 2014). Their ability to capture non-local dependencies makes them superior to the traditional phrase-based models, which do not consider contextual information across phrasal boundaries. Two such models that we explore in this paper are (i) the Operation Sequence Model or OSM (Durrani et al., 2011) — a markov translation model that integrates reordering, and (i"
2015.mtsummit-papers.10,P13-1082,0,0.207199,"se level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network training. They also used NNJM model as their basis. Other work on domain adaptation includes but not limited to studies that focus on topic modeling (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation where no in-domain data is available (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion We targeted an unexplored area of using bilingual language models for domain adaptation. We applied model weighting and data selection techniques using OSM and NNJM models. Both methods were shown to be effective in the target translation tasks. Interpolating multi-domain models gave an average improvement of up to +0.9 BLEU points using OSM and +0.5 using NNJM. We also used NNJM and OSM models for data selection using differences in cross entropy and showed improvements of up to +0.6 BLEU points. The code wil"
2015.mtsummit-papers.10,P13-1045,0,0.034229,"reover, the model is based on minimal translation units (MTUs) and considers source and target contextual information across phrasal boundaries, thus addressing phrasal independence assumption and spurious segmentation problems in traditional phrase-based MT. 2.2 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to MT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013; Socher et al., 2013; Hinton et al., 2012). A bilingual Neural Network Joint model for MT was recently proposed by Devlin et al. (2014). It learns a feedforward neural network from augmented streams of source and target sequences. For a bilingual sentence pair (S, T ), NNJM defines a conditional probability distribution: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−n+1 , si ) i=1 where, si is an m-word source window for a target word ti based on the one-to-one alignment between T and S. Each input word in the context has a D dimensional (continuous-valued) Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami"
2015.mtsummit-papers.10,D13-1140,0,0.293,"models are trained by randomly selecting corpora of same size as that of the in-domain data. 4 Experiments Data: We used TED talks (Cettolo et al., 2014) as our in-domain corpus. For German-toEnglish (DE-EN), we used the data made available for WMT’14.2 This contains News, Europarl and Common Crawl as out-domain data. For Arabic-English (AR-EN), we used the UN corpus as out-domain data. We concatenated dev- and test-2010 for tuning and used test2011-2013 for evaluation. Table 2 shows the size of the training and test data used. NNJM Settings: The NNJM models were trained using NPLM3 toolkit (Vaswani et al., 2013) with the following settings. We used a target context of 5 words and an aligned source window of 9 words, forming a joint stream of 14-grams for training. We restricted source and target side vocabularies to 20K and 40K most frequent words. We used an input embedding layer of 150 2 http://www.statmt.org/wmt14/ 3 http://nlg.isi.edu/software/nplm/ Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 122 German-English Arabic-English Corpus Sent. TokDE TokEN Corpus Sent. TokAR TokEN iwslt news ep cc 177K 200K 1.9M 2.3M 3.3M 5.1M 48.7M 53.9M 3.5M 5.0M 51.0M 57"
2020.acl-main.422,W19-4814,0,0.0311576,"a similar approach, but explores a range of similarity measures over different contextual word representation models. Questions of localization and distributivity of information have been under investigation for a long time in the connectionist cognitive science literature (Page, 2000; Bowers, 2002; Gayler and Levy, 2011). While neural language representations are thought to be densely distributed, several recent studies have pointed out the importance of individual neurons (Qian et al., 2016; Shi et al., 2016; Radford et al., 2017; Lakretz et al., 2019; Bau et al., 2019; Dalvi et al., 2019; Baan et al., 2019). Our study contributes to this line of work by designing measures of localization and distributivity of information in a collection of models. Such measures may facilitate incorporating neuron interactions in new training objectives (Li et al., 2020). 3 Similarity Measures We present five groups of similarity measures, each capturing a different similarity notion. Consider a collection of M models {f (m) }M m=1 , yielding (m) word representations hl and potentially atten(m) tion weights αl at each layer l. Let k index neu(m) (m) (m) rons hl [k] or attention heads αl [k]. hl [k], (m) αl [k] ar"
2020.acl-main.422,D19-1445,0,0.0290966,"sk finetuning setup. In contrast, in XLNet, fine-tuning on any task leads to top layers being very different from all layers of models fine-tuned on other tasks. This suggests that XLNet representations become very task-specific, and thus multi-task fine-tuning may be less effective with XLNet than with BERT. Observing the attnsim similarity based on Jensen–Shannon divergence for base and fine-tuned models (Figure 6), we again see that top layers have lower similarities, implying that they undergo greater changed during fine-tuning. Other attentionbased measures behaved similarly (not shown). Kovaleva et al. (2019) made a similar observation by comparing the cosine similarity of attention matrices in BERT, although they did not perform crosstask comparisons. In fact, the diagonals within each block indicate that bottom layers remain similar to one another even when fine-tuning on different tasks, while top layers diverge after finetuning. The vertical bands at layers 0 mean that many higher layers have a head that is very similar to a head from the first layer, that is, a form of redundancy, which can explain why many heads can be pruned (Michel et al., 2019; Voita et al., 2019b; Kovaleva et al., 2019)."
2020.acl-main.422,N19-1002,0,0.0433694,"rt-ofspeech tagger (Saphra and Lopez, 2019). Our work adopts a similar approach, but explores a range of similarity measures over different contextual word representation models. Questions of localization and distributivity of information have been under investigation for a long time in the connectionist cognitive science literature (Page, 2000; Bowers, 2002; Gayler and Levy, 2011). While neural language representations are thought to be densely distributed, several recent studies have pointed out the importance of individual neurons (Qian et al., 2016; Shi et al., 2016; Radford et al., 2017; Lakretz et al., 2019; Bau et al., 2019; Dalvi et al., 2019; Baan et al., 2019). Our study contributes to this line of work by designing measures of localization and distributivity of information in a collection of models. Such measures may facilitate incorporating neuron interactions in new training objectives (Li et al., 2020). 3 Similarity Measures We present five groups of similarity measures, each capturing a different similarity notion. Consider a collection of M models {f (m) }M m=1 , yielding (m) word representations hl and potentially atten(m) tion weights αl at each layer l. Let k index neu(m) (m) (m) ro"
2020.acl-main.422,N19-1112,1,0.849968,"s ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019) have led to impressive improvements in a variety of tasks. With this progress in breaking the state of the art, interest in the community has expanded to analyzing such models in an effort to illuminate their inner workings. A number of studies have analyzed the internal representations in such models and attempted to assess what linguistic properties they capture. A prominent methodology for this is to train supervised classifiers based on the models’ learned representations, and predict various linguistic properties. For instance, Liu et al. (2019a) train such classifiers on 16 linguistic tasks, including part-of-speech tagging, chunking, named ∗ Equal contribution The code is available at https://github.com/ johnmwu/contextual-corr-analysis. 1 entity recognition, and others. Such an approach may reveal how well representations from different models, and model layers, capture different properties. This approach, known as analysis by probing classifiers, has been used in numerous other studies (Belinkov and Glass, 2019). While the above approach yields compelling insights, its applicability is constrained by the availability of linguist"
2020.acl-main.422,2021.ccl-1.108,0,0.092394,"Missing"
2020.acl-main.422,J93-2004,0,0.0700399,"er-equivalent variant (Peters et al., 2018b). GPT variants We use both the original OpenAI Transformer (GPT; Radford et al. 2018) and its successor GPT2 (Radford et al., 2019), in the small and medium model sizes. These are all unidirectional Transformer LMs. BERT We use BERT-base/large (12/24 layers; Devlin et al. 2019): Transformer LMs trained with a masked LM objective function.6 XLNet We use XLNet-base/large (12/24 layers; Yang et al. 2019). Both are Transformer LM with a permutation-based objective function. Data For analyzing the models, we run them on the Penn Treebank development set (Marcus et al., 1993), following the setup taken by Liu et al. (2019a) in their probing classifier experiments.7 We collect representations and attention weights from each layer in each model for computing the similarity measures. We obtain representations for models used in Liu et al. (2019a) from their implementation and use the transformers library (Wolf et al., 2019) to extract other representations. We aggregate sub-word representations by taking the representation of the last sub-word, following Liu et al. (2019a), and sub-word attentions by summing up at6 BERT is also trained with a next sentence prediction"
2020.acl-main.422,D18-1179,0,0.154589,"the deep models, and facilitate the investigation of which design factors affect model similarity, without requiring any external linguistic annotation. The analysis reveals that models within the same family are more similar to one another, as may be expected. Surprisingly, different architectures have rather similar representations, but different individual neurons. We also observed differences in information localization in lower and higher layers and found that higher layers are more affected by fine-tuning on downstream tasks.1 1 Introduction Contextual word representations such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019) have led to impressive improvements in a variety of tasks. With this progress in breaking the state of the art, interest in the community has expanded to analyzing such models in an effort to illuminate their inner workings. A number of studies have analyzed the internal representations in such models and attempted to assess what linguistic properties they capture. A prominent methodology for this is to train supervised classifiers based on the models’ learned representations, and predict various linguistic properties. For instance, Liu et al. (2019a) train su"
2020.acl-main.422,D16-1079,0,0.0415815,"Missing"
2020.acl-main.422,D16-1264,0,0.0565186,"d Smith, 2019; Brunner et al., 2020). However, characterizing the effect of such concerns on our attention-based similarity measures is beyond the current scope. 6 Similarity of Fine-tuned Models How does fine-tuning on downstream tasks affect model similarity? In this section, we compare pretrained models and their fine-tuned versions. We use four of the GLUE tasks (Wang et al., 2019): MNLI A multi-genre natural language inference dataset (Williams et al., 2018), where the task is to predict whether a premise entails a hypothesis. QNLI A conversion of the Stanford question answering dataset (Rajpurkar et al., 2016), where the task is to determine whether a sentence contains the answer to a question. QQP A collection of question pairs from the Quora website, where the task is to determine whether two questions are semantically equivalent. SST-2 A binary sentiment analysis task using the Stanford sentiment treebank (Socher et al., 2013). 6.1 Results Top layers are more affected by fine-tuning Figure 5 shows representation-level ckasim similarity heatmaps of pre-trained (not fine-tuned) and fine-tuned versions of BERT and XLNet. The most striking pattern is that the top layers are more affected by fine-tun"
2020.acl-main.422,N19-1329,0,0.296007,"ies between model representations. Bau et al. (2019) used this approach to analyze the role of individual neurons in neural machine translation. They found that individual neurons are important and interpretable. However, their work was limited to a certain kind of architecture (specifically, a recurrent one). In contrast, we compare models of various architectures and objective functions. Other work used similarity measures to study learning dynamics in language models by comparing checkpoints of recurrent language models (Morcos et al., 2018), or a language model and a part-ofspeech tagger (Saphra and Lopez, 2019). Our work adopts a similar approach, but explores a range of similarity measures over different contextual word representation models. Questions of localization and distributivity of information have been under investigation for a long time in the connectionist cognitive science literature (Page, 2000; Bowers, 2002; Gayler and Levy, 2011). While neural language representations are thought to be densely distributed, several recent studies have pointed out the importance of individual neurons (Qian et al., 2016; Shi et al., 2016; Radford et al., 2017; Lakretz et al., 2019; Bau et al., 2019; Dal"
2020.acl-main.422,P19-1282,0,0.0196114,"RT-base and middle layers of BERT-large. This parallels the findings from comparing representations of XLNet and BERT, which we conjecture is the result of the permutation-based objective in XLNet. In general, we find the attention-based similarities to be mostly in line with the neuron- and representation-level similarities. Nevertheless, they appear to be harder to interpret, as fine-grained patterns are less noticeable. One might mention in this context concerns regarding the reliability of attention weights for interpreting the importance of input words in a model (Jain and Wallace, 2019; Serrano and Smith, 2019; Brunner et al., 2020). However, characterizing the effect of such concerns on our attention-based similarity measures is beyond the current scope. 6 Similarity of Fine-tuned Models How does fine-tuning on downstream tasks affect model similarity? In this section, we compare pretrained models and their fine-tuned versions. We use four of the GLUE tasks (Wang et al., 2019): MNLI A multi-genre natural language inference dataset (Williams et al., 2018), where the task is to predict whether a premise entails a hypothesis. QNLI A conversion of the Stanford question answering dataset (Rajpurkar et"
2020.acl-main.422,D16-1248,0,0.0236354,"al., 2018), or a language model and a part-ofspeech tagger (Saphra and Lopez, 2019). Our work adopts a similar approach, but explores a range of similarity measures over different contextual word representation models. Questions of localization and distributivity of information have been under investigation for a long time in the connectionist cognitive science literature (Page, 2000; Bowers, 2002; Gayler and Levy, 2011). While neural language representations are thought to be densely distributed, several recent studies have pointed out the importance of individual neurons (Qian et al., 2016; Shi et al., 2016; Radford et al., 2017; Lakretz et al., 2019; Bau et al., 2019; Dalvi et al., 2019; Baan et al., 2019). Our study contributes to this line of work by designing measures of localization and distributivity of information in a collection of models. Such measures may facilitate incorporating neuron interactions in new training objectives (Li et al., 2020). 3 Similarity Measures We present five groups of similarity measures, each capturing a different similarity notion. Consider a collection of M models {f (m) }M m=1 , yielding (m) word representations hl and potentially atten(m) tion weights αl at"
2020.acl-main.422,silveira-etal-2014-gold,0,0.0507562,"Missing"
2020.acl-main.422,D13-1170,0,0.0108017,"Missing"
2020.acl-main.422,P19-1452,0,0.0346209,"layers. In effect, we take the column-wise mean of each heatmap. We do this separately for svsim as the distributed measure and neuronsim as the localized measure, and we subtract the svsim means from the neuronsim means. This results in a measure of localization per layer. Figure 3 shows the results. In all models, the localization score mostly increases with layers, indicating that information tends to become more localized at higher layers.12 This pattern is quite consistent, but may be surprising given prior observations on lower layers capturing phenomena that operate at a local context (Tenney et al., 2019), which presumably require fewer neurons. However, this pattern is in line with observations made by Ethayarajh (2019), who reported that upper layers of pre-trained models produce more context-specific representations. There appears to be a correspondence between our localization score and Ethayarajh’s context-specificity score, which is based on the cosine similarity of representations of the same word in different contexts. Thus, more localized representations are also more context-specific. A direct comparison between context-specificity and localization may be fruitful avenue for future w"
2020.acl-main.422,D19-1448,0,0.380748,"also experimented with the RBF variant, which is computationally demanding. We found similar patterns in preliminary experiments, so we focus on the linear variant. has been used to analyze neural network representations (Bouchacourt and Baroni, 2018; Chrupała and Alishahi, 2019; Chrupała, 2019), or other variants of CCA, such as deep CCA (Andrew et al., 2013). We leave the explorations of such measures to future work. 3.4 Attention-level similarity Previous work analyzing network similarity has mostly focused on representation-based similarities (Morcos et al., 2018; Saphra and Lopez, 2019; Voita et al., 2019a). Here we consider similarity based on attention weights in Transformer models. Analogous to a neuron-level similarity measure, an attention-level similarity measure finds the most “correlated” other attention head. We consider three methods to correlate heads, based on the norm of (m) (m0 ) two attention matrices αl [k], αl0 [k 0 ], their Pearson correlation, and their Jensen–Shannon divergence.5 We then average over heads k in layer l, as before. These measures are similar to neuronsim in that they emphasize localization of information—if two layers have pairs of heads that are very simila"
2020.acl-main.422,P19-1580,0,0.383097,"also experimented with the RBF variant, which is computationally demanding. We found similar patterns in preliminary experiments, so we focus on the linear variant. has been used to analyze neural network representations (Bouchacourt and Baroni, 2018; Chrupała and Alishahi, 2019; Chrupała, 2019), or other variants of CCA, such as deep CCA (Andrew et al., 2013). We leave the explorations of such measures to future work. 3.4 Attention-level similarity Previous work analyzing network similarity has mostly focused on representation-based similarities (Morcos et al., 2018; Saphra and Lopez, 2019; Voita et al., 2019a). Here we consider similarity based on attention weights in Transformer models. Analogous to a neuron-level similarity measure, an attention-level similarity measure finds the most “correlated” other attention head. We consider three methods to correlate heads, based on the norm of (m) (m0 ) two attention matrices αl [k], αl0 [k 0 ], their Pearson correlation, and their Jensen–Shannon divergence.5 We then average over heads k in layer l, as before. These measures are similar to neuronsim in that they emphasize localization of information—if two layers have pairs of heads that are very simila"
2020.acl-main.422,W16-2524,0,\N,Missing
2020.acl-main.422,P17-1080,1,\N,Missing
2020.acl-main.422,D17-1169,0,\N,Missing
2020.acl-main.422,D18-1119,0,\N,Missing
2020.acl-main.422,Q19-1004,1,\N,Missing
2020.acl-main.422,N19-1357,0,\N,Missing
2020.acl-main.422,P19-1283,0,\N,Missing
2020.acl-main.422,N19-1423,0,\N,Missing
2020.acl-main.422,P19-1647,0,\N,Missing
2020.acl-main.422,N18-1101,0,\N,Missing
2020.acl-main.422,D19-1424,0,\N,Missing
2020.cl-1.1,D18-1313,0,0.0552683,"This implies that a higher BLEU score does not necessarily entail better morphological representations. In other words, a better translation model learns more informative representations, but only when it is actually learning to translate rather than merely memorizing the data as in the autoencoder case. We found these results to be consistent in other language pairs, that is, by changing the source from Arabic to German and Czech and also using character models instead of words (see Section A.2 in the Appendix for more details); however, more through study is required along this direction as Bisazza and Tump (2018) performed a similar experiment on a fine-grained tag level and found contrastive results. 7. Syntax Results To evaluate the NMT representations from a syntactic perspective, we consider two tasks. First, we made use of CCG supertagging, which is assumed to capture syntax at the word level. Second, we used dependency relations between any two words in the sentence for which a dependency edge exists, to investigate how words compose. Specifically, we ask the following questions: (i) Do NMT models acquire structural information while they are being trained on flat sequences of bilingual sentence"
2020.cl-1.1,C16-1333,0,0.0606149,"Missing"
2020.cl-1.1,W16-2308,0,0.232083,"hoices and performance. In current practice, their development is often limited to a trial-and-error process, without gaining a real understanding of what the system has learned. We aim to increase model transparency by analyzing the representations learned by NMT models at different levels of granularity in light of various linguistic phenomena—at morphological, syntactic, and semantic levels—that are considered important for the task of machine translation and for learning complex natural language processing (NLP) problems. We thus strive for post-hoc decomposability, in the sense of Lipton (2016). That is, we analyze models after they have been trained, to uncover what linguistic phenomena are captured within the underlying representations. More specifically, we aim to address the following questions in this article: • What linguistic information is captured in deep learning models? – – – Do the NMT representations capture word morphology? Do the NMT models, being trained on flat sequences of words, still acquire structural information? Do the NMT models learn informative semantic representations? • Is the language information well distributed across the network or are designated part"
2020.cl-1.1,W17-4705,0,0.0190207,"line of work visualizes hidden unit activations in recurrent neural networks (RNNs) that are trained for a given task (Elman 1991; Karpathy, Johnson, and Li 2015; K´ad´ar, Chrupała, and Alishahi 2017). Although such visualizations illuminate the inner workings of the network, they are often qualitative in nature and somewhat anecdotal. Other work aims to evaluate systems on specific linguistic phenomena represented in so-called challenge sets. Prominent examples include older work on MT evaluation (King and Falkedal 1990), as well as more recent evaluations via contrastive translation pairs (Burlot and Yvon 2017; Rios Gonzales, Mascarell, and 1 The learned parameters are implicitly shared by all the language pairs being modeled. 4 Belinkov, Durrani et al. Linguistic Representations in NMT Sennrich 2017; Sennrich 2017; Bawden et al. 2018). The latter line of work constructs minimal pairs of translations that differ by a known linguistic property, and evaluates whether the MT system assigns a higher score to the correct translation. The challenge set evaluation may produce informative results on the quality of the overall model for some linguistic property, but it does not directly assess the learned r"
2020.cl-1.1,P18-1008,0,0.025914,"on recurrent LSTM encoderdecoder models with attention. Although this is the first successful NMT architecture, and still a dominant one, it is certainly not the only one. Other sucessful architectures include fully convolutional (Gehring et al. 2017) and fully attentional, transformer encoder-decoder models (Vaswani et al. 2017). There are also non-autoregressive models, which are promising in terms of efficiency (Gu et al. 2018). At present, NMT systems based on transformer components appear to be the most successful. Combinations of transformer and recurrent components may also be helpful (Chen et al. 2018). The generalization of the particular results in this work to other architectures is a question of study. Recent efforts to analyze transformer-based NMT models include attempts to extract syntactic trees from self-attention weights (Mareˇcek and Rosa 2018; Raganato and Tiedemann 2018) and evaluating representations from the transformer encoder (Raganato and Tiedemann 2018). The latter found that lower layers tend to focus on POS and shallow syntax, whereas higher layers are more focused on semantic tagging. These results are in line with our findings. However, more work is needed to understa"
2020.cl-1.1,P05-1033,0,0.303982,"or Professor admit@@ s to shoot@@ ing his girl@@ friend Characters Professor admits to shooting his girlfriend 7 Computational Linguistics Volume 46, Number 1 3.2 Syntax Linguistic theories argue that words are hierarchically organized in syntactic constituents referred to as syntactic trees. It is therefore natural to think that translation models should be based on trees rather than a flat sequence representation of sentences. For more than a decade of research in machine translation, a tremendous amount of effort has been put into syntax-based machine translation (Yamada and Knight (2002); Chiang (2005), Galley et al. (2006), Zhang et al. (2007), Shen, Xu, and Weischedel (2010); Neubig and Duh (2014)), with notable success in languages such as Chinese and German, which are syntactically divergent compared to English. However, the sequence-to-sequence NMT systems were able to surpass the performance of the stateof-the-art syntax-based systems in recent MT competitions (Bojar et al. 2016). The LSTM-based RNN model with the help of the attention mechanism is able to handle long-distance dependencies. There have also been recent attempts to integrate syntax into NMT (Eriguchi, Hashimoto, and Tsu"
2020.cl-1.1,N09-1025,0,0.0771531,"Missing"
2020.cl-1.1,P16-1160,0,0.073408,"Missing"
2020.cl-1.1,C04-1041,0,0.0678557,"l. 2016). Third, as dependencies are bi-lexical relations between words, it is straightforward to obtain representations for them from an NMT model. This makes them amenable to the general methodology followed in this paper. Figure 1a shows an example sentence with syntactic dependencies. 3.3 Semantics The holy grail in MT has long been to achieve an interlingua-based translation model, where the goal is to capture the meaning of the source sentence and generate a target sentence with the same meaning. It has been believed since the inception of MT 3 Refer to Steedman and Baldridge (2011) and Clark and Curran (2004) for more information on CCG supertagging. 8 Belinkov, Durrani et al. Linguistic Representations in NMT Figure 1 Example sentence with syntactic and semantic relations. (a) Syntactic relations according to the Universal Dependencies formalism. Here “Obama” and “ Netanyahu” are the subject and object of “receives”, respectively, obl refers to an oblique relation of the locative modifier, nmod denotes the genitive relation, the prepositions “in” and “of” are treated as case-marking elements, and “the” is a determiner. See https://universaldependencies.org/guidelines.html for detailed definitions"
2020.cl-1.1,P18-1198,0,0.0310848,"oduce informative results on the quality of the overall model for some linguistic property, but it does not directly assess the learned representations. A different approach tries to provide a quantitative analysis by correlating parts of the neural network with linguistic properties, for example, by training a classifier to predict a feature of interest (Adi et al. 2017; Hupkes, Veldhoen, and Zuidema 2017; Conneau ¨ et al. 2018). Such an analysis has been conducted on word embeddings (Kohn 2015; Qian, Qiu, and Huang 2016b), sentence embeddings (Adi et al. 2017; Ganesh, Gupta, and Varma 2017; Conneau et al. 2018), and RNN states (Qian, Qiu, and Huang 2016a; Wu and King 2016; Wang, Chung, and Lee 2017). The language properties mainly analyzed are morphological (Qian, Qiu, and Huang 2016b; Vylomova et al. 2016; Belinkov et al. 2017a; Dalvi et al. 2017), semantic (Qian, Qiu, and Huang 2016b; Belinkov et al. 2017b), ¨ and syntactic (Tran, Bisazza, and Monz 2018; Kohn 2015; Conneau et al. 2018). Recent studies carried a more fine-grained neuron-level analysis for NMT and LM (Bau et al. 2019a; Dalvi et al. 2019a; Lakretz et al. 2019). In contrast to all of this work, we focus on the representations learned"
2020.cl-1.1,P16-2058,0,0.310821,"and Glass (2019) for a recent survey on the topic. 2.2 Subword Units One of the major challenges in training NMT systems is handling less frequent and out-of-vocabulary words. To address this issue, researchers have resorted to using subword units for training the neural network models. Luong and Manning (2016) trained a hybrid system that integrates character-level representation within a wordbased framework. Ling et al. (2015) used a bidirectional long short-term memory network (LSTM; Hochreiter and Schmidhuber 1997) to compose word embeddings from the character embeddings. Costa-juss`a and Fonollosa (2016) and Renduchintala et al. (2018) combined convolutional and highway layers to replace the standard lookupbased word representations in NMT systems with character-aware representations.2 Sennrich, Haddow, and Birch (2016) used byte-pair encoding (BPE), a data-compression algorithm, to segment words into smaller units. A variant of this method known as a wordpiece model is used by Google (Wu et al. 2016a). Shapiro and Duh (2018) used a similar convolutional architecture on top of BPE. Chung, Cho, and Bengio (2016) used a combination of BPE-based encoder and character-based decoder to improve 2 C"
2020.cl-1.1,I17-1015,1,0.836995,"eural network with linguistic properties, for example, by training a classifier to predict a feature of interest (Adi et al. 2017; Hupkes, Veldhoen, and Zuidema 2017; Conneau ¨ et al. 2018). Such an analysis has been conducted on word embeddings (Kohn 2015; Qian, Qiu, and Huang 2016b), sentence embeddings (Adi et al. 2017; Ganesh, Gupta, and Varma 2017; Conneau et al. 2018), and RNN states (Qian, Qiu, and Huang 2016a; Wu and King 2016; Wang, Chung, and Lee 2017). The language properties mainly analyzed are morphological (Qian, Qiu, and Huang 2016b; Vylomova et al. 2016; Belinkov et al. 2017a; Dalvi et al. 2017), semantic (Qian, Qiu, and Huang 2016b; Belinkov et al. 2017b), ¨ and syntactic (Tran, Bisazza, and Monz 2018; Kohn 2015; Conneau et al. 2018). Recent studies carried a more fine-grained neuron-level analysis for NMT and LM (Bau et al. 2019a; Dalvi et al. 2019a; Lakretz et al. 2019). In contrast to all of this work, we focus on the representations learned in neural machine translation in light of various linguistic properties (morphological, syntactic, and semantic) and phenomena such as handling low frequency words. Our work is most similar to Shi, Padhi, and Knight (2016) and Vylomova et al."
2020.cl-1.1,N19-1423,0,0.0237899,"he NMT encoder or decoder. We have already mentioned one work exploiting this idea, known as CoVE (McCann et al. 2017), which used NMT representations as features in other models to perform various NLP tasks. Other prominent contextualizers include ELMo (Peters et al. 2018a), which trains two separate, forward and backward LSTM language models (with a character CNN building block) and concatenates their representations across several layers; GPT (Radford et al. 2018) and GPT-2 (Radford et al. 2019), which use transformer language models based on self-attention (Vaswani et al. 2017); and BERT (Devlin et al. 2019), which uses a bidirectional transformer model trained on masked language modeling (filling the blanks). All these generate representations that feed into task-specific classifiers, potentially with fine-tuning the contextualizer weights.21 21 See Peters, Ruder, and Smith (2019) for an evaluation of when it is worthwhile to fine-tune. 38 Belinkov, Durrani et al. Linguistic Representations in NMT How do NMT representations compare with CWRs trained from raw text? Directly answering this question is beyond the scope of this work, and is also tricky to perform for two reasons. First, CWRs like EL"
2020.cl-1.1,P10-1048,1,0.738996,"representations.2 Sennrich, Haddow, and Birch (2016) used byte-pair encoding (BPE), a data-compression algorithm, to segment words into smaller units. A variant of this method known as a wordpiece model is used by Google (Wu et al. 2016a). Shapiro and Duh (2018) used a similar convolutional architecture on top of BPE. Chung, Cho, and Bengio (2016) used a combination of BPE-based encoder and character-based decoder to improve 2 Character-based systems have been used previously in phrase-based MT for handling morphologically rich (Luong, Nakov, and Kan 2010) and closely related language pairs (Durrani et al. 2010; Nakov and Tiedemann 2012) or for transliterating unknown words (Durrani et al. 2014). 5 Computational Linguistics Volume 46, Number 1 translation quality. Motivated by their findings, Lee, Cho, and Hofmann (2017) explored using fully character representations (with no word boundaries) on both the source and target sides. As BPE segmentation is not linguistically motivated, an alternative to using morpheme-based segmentation has been explored in Bradbury and Socher (2016). It is important to address what using different translation units (word, BPE, morpheme, character) entails. Sennrich (201"
2020.cl-1.1,E14-4029,1,0.825501,"a data-compression algorithm, to segment words into smaller units. A variant of this method known as a wordpiece model is used by Google (Wu et al. 2016a). Shapiro and Duh (2018) used a similar convolutional architecture on top of BPE. Chung, Cho, and Bengio (2016) used a combination of BPE-based encoder and character-based decoder to improve 2 Character-based systems have been used previously in phrase-based MT for handling morphologically rich (Luong, Nakov, and Kan 2010) and closely related language pairs (Durrani et al. 2010; Nakov and Tiedemann 2012) or for transliterating unknown words (Durrani et al. 2014). 5 Computational Linguistics Volume 46, Number 1 translation quality. Motivated by their findings, Lee, Cho, and Hofmann (2017) explored using fully character representations (with no word boundaries) on both the source and target sides. As BPE segmentation is not linguistically motivated, an alternative to using morpheme-based segmentation has been explored in Bradbury and Socher (2016). It is important to address what using different translation units (word, BPE, morpheme, character) entails. Sennrich (2017) performed a comparative evaluation of character- and BPE-based systems on carefully"
2020.cl-1.1,P11-1105,1,0.842798,"Missing"
2020.cl-1.1,P16-1078,0,0.0616503,"Missing"
2020.cl-1.1,P06-1121,0,0.0689132,"mit@@ s to shoot@@ ing his girl@@ friend Characters Professor admits to shooting his girlfriend 7 Computational Linguistics Volume 46, Number 1 3.2 Syntax Linguistic theories argue that words are hierarchically organized in syntactic constituents referred to as syntactic trees. It is therefore natural to think that translation models should be based on trees rather than a flat sequence representation of sentences. For more than a decade of research in machine translation, a tremendous amount of effort has been put into syntax-based machine translation (Yamada and Knight (2002); Chiang (2005), Galley et al. (2006), Zhang et al. (2007), Shen, Xu, and Weischedel (2010); Neubig and Duh (2014)), with notable success in languages such as Chinese and German, which are syntactically divergent compared to English. However, the sequence-to-sequence NMT systems were able to surpass the performance of the stateof-the-art syntax-based systems in recent MT competitions (Bojar et al. 2016). The LSTM-based RNN model with the help of the attention mechanism is able to handle long-distance dependencies. There have also been recent attempts to integrate syntax into NMT (Eriguchi, Hashimoto, and Tsuruoka 2016; Stahlberg"
2020.cl-1.1,D08-1089,0,0.0710678,"rtificial intelligence, including machine translation (MT). Compared with their traditional counterparts, these models are trained in an end-to-end fashion, providing a simple yet elegant mechanism. This simplicity, however, comes at the price of opaqueness. Unlike traditional systems that contain specialized modules carrying specific sub-tasks, neural MT (NMT) systems train one large network, optimized toward the overall task. For example, non-neural statistical MT systems have sub-components to handle fluency (Heafield 2011), lexical generation (Koehn, Och, and Marcu 2003), word reordering (Galley and Manning 2008; Durrani, Schmid, and Fraser 2011), rich morphology (Koehn and Hoang 2007), and a smorgasbord of features (Chiang, Knight, and Wang 2009) for modeling different phenomena. Neural MT systems, on the other hand, contain a single model based on an encoder-decoder mechanism (Sutskever, Vinyals, and Le 2014) with attention (Bahdanau, Cho, and Bengio 2014). Despite its simplicity, neural MT surpassed non-neural statistical MT within a few years of its emergence. Human evaluation and error analysis revealed that the improvements were obtained through more fluent outputs (Toral and S´anchez-Cartagena"
2020.cl-1.1,W11-2123,0,0.0433646,"ep neural networks have quickly become the predominant approach to most tasks in artificial intelligence, including machine translation (MT). Compared with their traditional counterparts, these models are trained in an end-to-end fashion, providing a simple yet elegant mechanism. This simplicity, however, comes at the price of opaqueness. Unlike traditional systems that contain specialized modules carrying specific sub-tasks, neural MT (NMT) systems train one large network, optimized toward the overall task. For example, non-neural statistical MT systems have sub-components to handle fluency (Heafield 2011), lexical generation (Koehn, Och, and Marcu 2003), word reordering (Galley and Manning 2008; Durrani, Schmid, and Fraser 2011), rich morphology (Koehn and Hoang 2007), and a smorgasbord of features (Chiang, Knight, and Wang 2009) for modeling different phenomena. Neural MT systems, on the other hand, contain a single model based on an encoder-decoder mechanism (Sutskever, Vinyals, and Le 2014) with attention (Bahdanau, Cho, and Bengio 2014). Despite its simplicity, neural MT surpassed non-neural statistical MT within a few years of its emergence. Human evaluation and error analysis revealed th"
2020.cl-1.1,P06-1064,0,0.0106613,"ifferent NLP tasks. Table 4 details the number of tags (or labels) in each task across different languages. 6. Morphology Results In this section, we investigate what kind of morphological information is captured within NMT models, using the tasks of POS and morphological tagging. To probe this, we annotated a subset of the training data (see Table 3) using POS or morphological 11 http://www.ims.uni-stuttgart.de/forschung/ressourcen/korpora/tiger.html. 12 There are no available CCG banks for the other languages we experiment with, except for a German CCG bank, which is not publicly available (Hockenmaier 2006). 13 The main differences between PSD and the original tectogrammatical annotation are the omission of elided elements, such that all nodes are surface tokens; the inclusion of functional and punctuation tokens; ignoring most cases of function word attachments to content words; ignoring coreference links; and ignoring grammatemes (tectogrammatical correlates of morphological categories). As a side effect, these simplifications make it straightforward to generate representations for surface tokens participating in dependency relations under the PSD formalism. See http://sdp.delph-in.net for mor"
2020.cl-1.1,J07-3004,0,0.0101183,"3 – – – – – – – – 14,006 5,640 14,006 5,640 – – – – – – – – German, Spanish, and Czech) we used RDRPOST (Nguyen et al. 2014), a state-of-the-art morphological tagger. For experiments using gold tags, we used the Arabic Treebank for Arabic (with the versions and splits described in the MADAMIRA manual) and the Tiger corpus for German.11 For semantic tagging, we used the semantic tags from the Groningen Parallel Meaning Bank (Abzianidze et al. 2017). For syntactic relation labeling we used the Universal Dependencies data set (Nivre et al. 2017). For CCG supertagging we used the English CCGBank (Hockenmaier and Steedman 2007).12 For semantic dependency labeling we used PSD, which is a reduction of the tectogrammatical analysis layer of the Prague Czech–English Dependency Treebank, and is made available as part of the Semantic Dependency Parsing data set (Oepen et al. 2014, 2015). Most of the PSD dependency labels mark semantic roles of arguments, which are called functors in the Prague dependency treebank.13 PSD annotations are available in English and Czech. Table 3 provides the amount of data used to train the MT classifiers for different NLP tasks. Table 4 details the number of tags (or labels) in each task acr"
2020.cl-1.1,E17-2059,0,0.0241302,"bit grammatical relations such as subject/object/predicate or gender agreement by only changing the word form, whereas others achieve the same through word order or addition of particles. Morphology (aka word structure), poses an exigent problem in machine translation and is at the heart of dealing with the challenge of data-sparsity. Although English is limited in morphology, other languages such as Czech, Arabic, and Russian have highly inflected morphology. This entails that for each lemma many possible word variants could exist, thus causing an out-of-vocabulary word problem. For example, Huck et al. (2017) found only one morphological variant of the Czech word “ˇce¨ s˘ ka” (plural of English “kneecap”) in a corpus of 50K parallel sentences. It required 50M sentences, a size of parallel corpus Table 1 Example sentence with different word-level annotations. The CCG supertags are taken from Nadejde et al. (2017). POS and semantic tags are our own annotation, as well as the German translation and its morphological tags. Words Obama receives Netanyahu in the capital of USA POS SEM CCG NP PER NP VBZ ENS ((S[dcl]NP) /PP)/NP NP PER NP IN REL PP/NP DT DEF NP/N NN REL N IN REL (NPNP) /NP NP GEO NP Word"
2020.cl-1.1,Q17-1024,0,0.0766313,"Missing"
2020.cl-1.1,C12-1083,0,0.0202664,"definitions. (b) Semantic relations according to the PSD formalism. Here ACT-arg and PAT-arg refer respectively to the originator and affected arguments of “receives”, LOC in the location, and APP is the thing that “capital” belongs to. For detailed definitions, see Cinkov´a et al. (2004). that without acquiring such meaning representations it will be impossible to generate human-like translations (Weaver 1955). Traditional statistical MT systems are weak at capturing meaning representations (e.g., “who does what to whom—namely, what are the agent, the action, and the patient in the sentence [Jones et al. 2012]). Although neural MT systems are also trained only on parallel data, without providing any direct supervision of word meaning, they are a continuous space model, and are believed to capture word meaning. Johnson et al. (2017), for example, found preliminary evidence that the shared architecture in their multilingual NMT systems learns a universal interlingua. There have also been some recent efforts to incorporate such information in NMT systems, either explicitly (Rios Gonzales, Mascarell, and Sennrich 2017) or implicitly (Liu, Lu, and Neubig 2018). Tagging task. In this article, we study h"
2020.cl-1.1,C90-2037,0,0.773283,"Missing"
2020.cl-1.1,D07-1091,0,0.249735,"Missing"
2020.cl-1.1,N03-1017,0,0.176326,"Missing"
2020.cl-1.1,2006.iwslt-evaluation.11,0,0.156188,"Missing"
2020.cl-1.1,N19-1002,0,0.0298792,"16b), sentence embeddings (Adi et al. 2017; Ganesh, Gupta, and Varma 2017; Conneau et al. 2018), and RNN states (Qian, Qiu, and Huang 2016a; Wu and King 2016; Wang, Chung, and Lee 2017). The language properties mainly analyzed are morphological (Qian, Qiu, and Huang 2016b; Vylomova et al. 2016; Belinkov et al. 2017a; Dalvi et al. 2017), semantic (Qian, Qiu, and Huang 2016b; Belinkov et al. 2017b), ¨ and syntactic (Tran, Bisazza, and Monz 2018; Kohn 2015; Conneau et al. 2018). Recent studies carried a more fine-grained neuron-level analysis for NMT and LM (Bau et al. 2019a; Dalvi et al. 2019a; Lakretz et al. 2019). In contrast to all of this work, we focus on the representations learned in neural machine translation in light of various linguistic properties (morphological, syntactic, and semantic) and phenomena such as handling low frequency words. Our work is most similar to Shi, Padhi, and Knight (2016) and Vylomova et al. (2016). The former used hidden vectors from a neural MT encoder to predict syntactic properties on the English source side, whereas we study multiple language properties in different languages. Vylomova et al. (2016) analyzed different representations for morphologically rich langu"
2020.cl-1.1,Q17-1026,0,0.0448226,"Missing"
2020.cl-1.1,N13-1060,0,0.0603544,"Missing"
2020.cl-1.1,N18-1121,0,0.0457537,"Missing"
2020.cl-1.1,N19-1112,1,0.910624,"Missing"
2020.cl-1.1,P16-1100,0,0.0292588,"for morphologically rich languages in MT, but they did not directly measure the quality of the learned representations. Surveying the work on analyzing neural networks in NLP is beyond the scope of the present paper. We have highlighted here several of the more relevant studies and refer to Belinkov and Glass (2019) for a recent survey on the topic. 2.2 Subword Units One of the major challenges in training NMT systems is handling less frequent and out-of-vocabulary words. To address this issue, researchers have resorted to using subword units for training the neural network models. Luong and Manning (2016) trained a hybrid system that integrates character-level representation within a wordbased framework. Ling et al. (2015) used a bidirectional long short-term memory network (LSTM; Hochreiter and Schmidhuber 1997) to compose word embeddings from the character embeddings. Costa-juss`a and Fonollosa (2016) and Renduchintala et al. (2018) combined convolutional and highway layers to replace the standard lookupbased word representations in NMT systems with character-aware representations.2 Sennrich, Haddow, and Birch (2016) used byte-pair encoding (BPE), a data-compression algorithm, to segment wor"
2020.cl-1.1,D10-1015,0,0.0774087,"Missing"
2020.cl-1.1,W18-5444,0,0.0348371,"Missing"
2020.cl-1.1,W06-2932,0,0.0276513,"Missing"
2020.cl-1.1,J11-1007,0,0.0157602,"), French (fr), German (de), Czech (cs), Arabic (ar), Russian (ru), and Hebrew (he). We trained NMT systems using data made available by the two popular machine translation campaigns, namely, WMT (Bojar et al. 2017) and IWSLT (Cettolo et al. 2016). The MT models were trained using a concatenation of NEWS, TED, and Europarl training data (≈ 2.5M sentence pairs). The multilingual systems were trained by simply concatenating data from different 5 It is also not unrealistic, as dependency parsers often work in two stages, first predicting an unlabeled dependency tree, and then labeling its edges (McDonald and Nivre 2011; McDonald, Lerman, and Pereira 2006). More complicated formulations can be conceived, from predicting the existence of dependencies independently to solving the full parsing task, but dependency labeling is a simple basic task to begin with. 6 Although we studied representations from a charCNN (Kim et al. 2015) in Belinkov et al. (2017a), the extracted features were still based on word representations produced by the charCNN. As a result, in that work we could not analyze and compare subword and character-based models that do not assume a segmentation into words. 7 One could envision more sop"
2020.cl-1.1,P12-2059,0,0.0181017,"ddow, and Birch (2016) used byte-pair encoding (BPE), a data-compression algorithm, to segment words into smaller units. A variant of this method known as a wordpiece model is used by Google (Wu et al. 2016a). Shapiro and Duh (2018) used a similar convolutional architecture on top of BPE. Chung, Cho, and Bengio (2016) used a combination of BPE-based encoder and character-based decoder to improve 2 Character-based systems have been used previously in phrase-based MT for handling morphologically rich (Luong, Nakov, and Kan 2010) and closely related language pairs (Durrani et al. 2010; Nakov and Tiedemann 2012) or for transliterating unknown words (Durrani et al. 2014). 5 Computational Linguistics Volume 46, Number 1 translation quality. Motivated by their findings, Lee, Cho, and Hofmann (2017) explored using fully character representations (with no word boundaries) on both the source and target sides. As BPE segmentation is not linguistically motivated, an alternative to using morpheme-based segmentation has been explored in Bradbury and Socher (2016). It is important to address what using different translation units (word, BPE, morpheme, character) entails. Sennrich (2017) performed a comparative"
2020.cl-1.1,P14-2024,0,0.0151066,"ting his girlfriend 7 Computational Linguistics Volume 46, Number 1 3.2 Syntax Linguistic theories argue that words are hierarchically organized in syntactic constituents referred to as syntactic trees. It is therefore natural to think that translation models should be based on trees rather than a flat sequence representation of sentences. For more than a decade of research in machine translation, a tremendous amount of effort has been put into syntax-based machine translation (Yamada and Knight (2002); Chiang (2005), Galley et al. (2006), Zhang et al. (2007), Shen, Xu, and Weischedel (2010); Neubig and Duh (2014)), with notable success in languages such as Chinese and German, which are syntactically divergent compared to English. However, the sequence-to-sequence NMT systems were able to surpass the performance of the stateof-the-art syntax-based systems in recent MT competitions (Bojar et al. 2016). The LSTM-based RNN model with the help of the attention mechanism is able to handle long-distance dependencies. There have also been recent attempts to integrate syntax into NMT (Eriguchi, Hashimoto, and Tsuruoka 2016; Stahlberg et al. 2016; Aharoni and Goldberg 2017; Chen et al. 2017; Wu et al. 2017), bu"
2020.cl-1.1,E14-2005,0,0.0130295,"and gold annotations for syntactic and semantics tasks. POS tags Morph tags CCG tags Syntactic dependency Semantic tags Semantic dependency Train Test Train Test Train Test Train Test Train Test Train Test de en cs ru fr es 14,498 8,172 14,498 8,172 – – 14,118 1,776 1,490 373 – – 14,498 8,172 14,498 8,172 41,586 2,407 12,467 4,049 14,084 12,168 12,000 9,692 14,498 8,172 14,498 8,172 – – 14,553 1,894 – – 11,999 10,010 11,824 5,999 11,824 5,999 – – 3,848 1,180 – – – – 11,495 3,003 11,495 3,003 – – – – – – – – 14,006 5,640 14,006 5,640 – – – – – – – – German, Spanish, and Czech) we used RDRPOST (Nguyen et al. 2014), a state-of-the-art morphological tagger. For experiments using gold tags, we used the Arabic Treebank for Arabic (with the versions and splits described in the MADAMIRA manual) and the Tiger corpus for German.11 For semantic tagging, we used the semantic tags from the Groningen Parallel Meaning Bank (Abzianidze et al. 2017). For syntactic relation labeling we used the Universal Dependencies data set (Nivre et al. 2017). For CCG supertagging we used the English CCGBank (Hockenmaier and Steedman 2007).12 For semantic dependency labeling we used PSD, which is a reduction of the tectogrammatical"
2020.cl-1.1,S15-2153,0,0.10891,"Missing"
2020.cl-1.1,S14-2008,0,0.146289,"can be noticed by the different graph structure (compare Figure 1b to Figure 1a). Predicate–argument relations have also been used in many (non-neural) MT systems (Komachi, Matsumoto, and Nagata 2006; Wu et al. 2011; Xiong, Zhang, and Li 2012; Li, Resnik, and Daum´e III 2013). Figure 1b shows an example sentence annotated with Prague Semantic Dependencies (PSD), a reduction of the tectogrammatical annotation in the Prague Czech–English Dependency Treebank (Cinkov´a et al. 2004; Cinkov´a et al. 2009), which was made available as part of the Semantic Dependency Parsing shared tasks in SemEval (Oepen et al. 2014, 2015). 4. Methodology We follow a 3-step process for studying linguistic information learned by the trained neural MT systems. The steps include: (i) training a neural MT system; (ii) using the trained model to generate feature representations for words in a language of interest; and (iii) training a classifier using generated features to make predictions for the different linguistic tagging tasks. The quality of the trained classifier on the given task serves as a proxy to the quality of the generated representations. It thus provides a quantitative measure of how well the original MT syste"
2020.cl-1.1,P02-1040,0,0.109365,"s from the forward and backward layers are concatenated. For the average method, all of the hidden states corresponding to subwords or characters of a given word are averaged together for each layer. For the last method, only the hidden state of the final subword or character is considered. language pairs (a total of ≈10M sentence pairs) and training a shared encoder-decoder pipeline. We used German, French, Spanish, and Czech to/from English to train multilingual systems. Language codes were added as prefixes before each sentence. We used official TED test sets to report translation quality (Papineni et al. 2002). We also used the fully aligned United Nations corpus (Ziemski, Junczys-Dowmunt, and Pouliquen 2016) for training the models in some of our experiments. It includes six languages: Arabic, Chinese, English, French, Spanish, and Russian. This data set has the benefit of multiple alignment of the several languages, which allows for comparable cross-linguistic analysis, for example, studying the effect of only changing the target language. We used the first 2 million sentences of the training set, using the official training/development/test split. 5.2 Neural MT Systems 5.2.1 Preprocessing. We us"
2020.cl-1.1,pasha-etal-2014-madamira,0,0.0332275,"Missing"
2020.cl-1.1,W19-4302,0,0.0794676,"Missing"
2020.cl-1.1,N18-1202,0,0.378949,"not all information may be extracted by a simple classifier), as the task-specific encoder-decoder performs better than a classifier trained on its representations. 10.2 Contextualized Word Representations The representations generated by NMT models may be thought of as contextualized word representations (CWRs), as they capture context via the NMT encoder or decoder. We have already mentioned one work exploiting this idea, known as CoVE (McCann et al. 2017), which used NMT representations as features in other models to perform various NLP tasks. Other prominent contextualizers include ELMo (Peters et al. 2018a), which trains two separate, forward and backward LSTM language models (with a character CNN building block) and concatenates their representations across several layers; GPT (Radford et al. 2018) and GPT-2 (Radford et al. 2019), which use transformer language models based on self-attention (Vaswani et al. 2017); and BERT (Devlin et al. 2019), which uses a bidirectional transformer model trained on masked language modeling (filling the blanks). All these generate representations that feed into task-specific classifiers, potentially with fine-tuning the contextualizer weights.21 21 See Peters"
2020.cl-1.1,D18-1179,0,0.0981855,"not all information may be extracted by a simple classifier), as the task-specific encoder-decoder performs better than a classifier trained on its representations. 10.2 Contextualized Word Representations The representations generated by NMT models may be thought of as contextualized word representations (CWRs), as they capture context via the NMT encoder or decoder. We have already mentioned one work exploiting this idea, known as CoVE (McCann et al. 2017), which used NMT representations as features in other models to perform various NLP tasks. Other prominent contextualizers include ELMo (Peters et al. 2018a), which trains two separate, forward and backward LSTM language models (with a character CNN building block) and concatenates their representations across several layers; GPT (Radford et al. 2018) and GPT-2 (Radford et al. 2019), which use transformer language models based on self-attention (Vaswani et al. 2017); and BERT (Devlin et al. 2019), which uses a bidirectional transformer model trained on masked language modeling (filling the blanks). All these generate representations that feed into task-specific classifiers, potentially with fine-tuning the contextualizer weights.21 21 See Peters"
2020.cl-1.1,W17-4737,0,0.1378,"t al. 2016a). Shapiro and Duh (2018) used a similar convolutional architecture on top of BPE. Chung, Cho, and Bengio (2016) used a combination of BPE-based encoder and character-based decoder to improve 2 Character-based systems have been used previously in phrase-based MT for handling morphologically rich (Luong, Nakov, and Kan 2010) and closely related language pairs (Durrani et al. 2010; Nakov and Tiedemann 2012) or for transliterating unknown words (Durrani et al. 2014). 5 Computational Linguistics Volume 46, Number 1 translation quality. Motivated by their findings, Lee, Cho, and Hofmann (2017) explored using fully character representations (with no word boundaries) on both the source and target sides. As BPE segmentation is not linguistically motivated, an alternative to using morpheme-based segmentation has been explored in Bradbury and Socher (2016). It is important to address what using different translation units (word, BPE, morpheme, character) entails. Sennrich (2017) performed a comparative evaluation of character- and BPE-based systems on carefully crafted synthetic tests and found that character-based models are effective in handling unknown words, but perform worse in cap"
2020.cl-1.1,D16-1079,0,0.12019,"Missing"
2020.cl-1.1,P16-1140,0,0.0902583,"Missing"
2020.cl-1.1,W18-5431,0,0.0485817,"Missing"
2020.cl-1.1,J82-2005,0,0.696897,"Missing"
2020.cl-1.1,W17-4702,0,0.0581143,"Missing"
2020.cl-1.1,C94-1027,0,0.359968,"k. 5.4 Supervised Data and Annotations We make use of gold-standard annotations wherever available, but in some cases we have to rely on using automatic taggers to obtain the annotations. In particular, to analyze the representations on the decoder side, we require parallel sentences.10 It is difficult to obtain gold-standard data with parallel sentences, so we rely on automatic annotation tools. An advantage of using automatic annotations, though, is that we can reduce the effect of domain mismatch and high out-of-vocabulary (OOV) rate in analyzing these representations. We used Tree-Tagger (Schmid 1994) for annotating Russian and the MADAMIRA tagger (Pasha et al. 2014) for annotating Arabic. For the remaining languages (French, 9 The sentence length was varied across different configurations, to keep the training data sizes the same for all systems. 10 We need source sentences to generate encoder states, which in turn are required for obtaining the decoder states that we want to analyze. 14 Belinkov, Durrani et al. Linguistic Representations in NMT Table 3 Train and test data (number of sentences) used to train MT classifiers to predict different tasks. We used automated tools to annotate da"
2020.cl-1.1,E17-2060,0,0.270394,". Although such visualizations illuminate the inner workings of the network, they are often qualitative in nature and somewhat anecdotal. Other work aims to evaluate systems on specific linguistic phenomena represented in so-called challenge sets. Prominent examples include older work on MT evaluation (King and Falkedal 1990), as well as more recent evaluations via contrastive translation pairs (Burlot and Yvon 2017; Rios Gonzales, Mascarell, and 1 The learned parameters are implicitly shared by all the language pairs being modeled. 4 Belinkov, Durrani et al. Linguistic Representations in NMT Sennrich 2017; Sennrich 2017; Bawden et al. 2018). The latter line of work constructs minimal pairs of translations that differ by a known linguistic property, and evaluates whether the MT system assigns a higher score to the correct translation. The challenge set evaluation may produce informative results on the quality of the overall model for some linguistic property, but it does not directly assess the learned representations. A different approach tries to provide a quantitative analysis by correlating parts of the neural network with linguistic properties, for example, by training a classifier to pred"
2020.cl-1.1,W17-4739,0,0.137065,"lly rich languages into constituents in a preprocessing step, using word segmentation in Arabic (Pasha et al. 2014; Abdelali et al. 2016) or compound splitting in German (Koehn and Knight 2003). Previous work also explored generative morphological models, known as Factored Translation Models, that explicitly integrate additional linguistic markup at the word level to learn morphology (Koehn and Hoang 2007). In NMT training, using subword units such as byte-pair encoding (Sennrich, Haddow, and Birch 2016) has become a de facto standard in training competition grade systems (Pinnis et al. 2017; Sennrich et al. 2017). A few have tried morpheme-based segmentation (Bradbury and Socher 2016), and several even used character-based systems (Chung, Cho, and Bengio 2016; Lee, Cho, and Hofmann 2017) to achieve similar performance as the BPE-segmented systems. Table 2 shows an example of each representation unit. BPE splits words into symbols (a symbol is a sequence of characters) and then iteratively replaces the most frequent sequences of symbols with a new merged symbol. In essence, frequent character n-gram sequences merge to form one symbol. The number of merge operations is controlled by a hyper-parameter OP"
2020.cl-1.1,P16-1162,0,0.213462,"Missing"
2020.cl-1.1,J10-4005,0,0.059411,"Missing"
2020.cl-1.1,D16-1159,0,0.108794,"Missing"
2020.cl-1.1,E14-2006,0,0.0330156,"Missing"
2020.cl-1.1,P16-2049,0,0.0243138,"al. (2006), Zhang et al. (2007), Shen, Xu, and Weischedel (2010); Neubig and Duh (2014)), with notable success in languages such as Chinese and German, which are syntactically divergent compared to English. However, the sequence-to-sequence NMT systems were able to surpass the performance of the stateof-the-art syntax-based systems in recent MT competitions (Bojar et al. 2016). The LSTM-based RNN model with the help of the attention mechanism is able to handle long-distance dependencies. There have also been recent attempts to integrate syntax into NMT (Eriguchi, Hashimoto, and Tsuruoka 2016; Stahlberg et al. 2016; Aharoni and Goldberg 2017; Chen et al. 2017; Wu et al. 2017), but sequence-to-sequence NMT models without explicit syntax are the state of the art at the moment (Pinnis et al. 2017; Sennrich et al. 2017). Tagging tasks. In this paper, we analyze whether NMT models trained on flat sequences acquire structural syntactic information. To answer this, we use two tagging tasks. First, we use CCG supertagging, which captures global syntactic information locally at the word level by assigning a label to each word annotating its syntactic role in the sentence. The process is almost equivalent to pars"
2020.cl-1.1,E17-1100,0,0.0444399,"Missing"
2020.cl-1.1,D18-1503,0,0.0645852,"Missing"
2020.cl-1.1,P17-1065,0,0.0226344,"ubig and Duh (2014)), with notable success in languages such as Chinese and German, which are syntactically divergent compared to English. However, the sequence-to-sequence NMT systems were able to surpass the performance of the stateof-the-art syntax-based systems in recent MT competitions (Bojar et al. 2016). The LSTM-based RNN model with the help of the attention mechanism is able to handle long-distance dependencies. There have also been recent attempts to integrate syntax into NMT (Eriguchi, Hashimoto, and Tsuruoka 2016; Stahlberg et al. 2016; Aharoni and Goldberg 2017; Chen et al. 2017; Wu et al. 2017), but sequence-to-sequence NMT models without explicit syntax are the state of the art at the moment (Pinnis et al. 2017; Sennrich et al. 2017). Tagging tasks. In this paper, we analyze whether NMT models trained on flat sequences acquire structural syntactic information. To answer this, we use two tagging tasks. First, we use CCG supertagging, which captures global syntactic information locally at the word level by assigning a label to each word annotating its syntactic role in the sentence. The process is almost equivalent to parsing (Bangalore and Joshi 1999). For example, the syntactic tag"
2020.cl-1.1,I11-1004,0,0.0211866,"ted with SEM tags. The semantic tag ENS describes a present-simple event category. The second semantic task is semantic dependency labeling, the task of assigning a type to each arc in a semantic dependency graph. Such dependencies are also known as predicate–argument relations, and may be seen as a first step toward semantic structure. They capture different aspects from syntactic relations, as can be noticed by the different graph structure (compare Figure 1b to Figure 1a). Predicate–argument relations have also been used in many (non-neural) MT systems (Komachi, Matsumoto, and Nagata 2006; Wu et al. 2011; Xiong, Zhang, and Li 2012; Li, Resnik, and Daum´e III 2013). Figure 1b shows an example sentence annotated with Prague Semantic Dependencies (PSD), a reduction of the tectogrammatical annotation in the Prague Czech–English Dependency Treebank (Cinkov´a et al. 2004; Cinkov´a et al. 2009), which was made available as part of the Semantic Dependency Parsing shared tasks in SemEval (Oepen et al. 2014, 2015). 4. Methodology We follow a 3-step process for studying linguistic information learned by the trained neural MT systems. The steps include: (i) training a neural MT system; (ii) using the tra"
2020.cl-1.1,P12-1095,0,0.0631512,"Missing"
2020.cl-1.1,P15-2041,0,0.0418831,"Missing"
2020.cl-1.1,P02-1039,0,0.235127,"s gir@@ l@@ friend Morfessor Professor admit@@ s to shoot@@ ing his girl@@ friend Characters Professor admits to shooting his girlfriend 7 Computational Linguistics Volume 46, Number 1 3.2 Syntax Linguistic theories argue that words are hierarchically organized in syntactic constituents referred to as syntactic trees. It is therefore natural to think that translation models should be based on trees rather than a flat sequence representation of sentences. For more than a decade of research in machine translation, a tremendous amount of effort has been put into syntax-based machine translation (Yamada and Knight (2002); Chiang (2005), Galley et al. (2006), Zhang et al. (2007), Shen, Xu, and Weischedel (2010); Neubig and Duh (2014)), with notable success in languages such as Chinese and German, which are syntactically divergent compared to English. However, the sequence-to-sequence NMT systems were able to surpass the performance of the stateof-the-art syntax-based systems in recent MT competitions (Bojar et al. 2016). The LSTM-based RNN model with the help of the attention mechanism is able to handle long-distance dependencies. There have also been recent attempts to integrate syntax into NMT (Eriguchi, Has"
2020.cl-1.1,W18-5448,0,0.177549,"corpus by training English-to-{French, Arabic, Spanish, Russian, and English} bilingual models. Comparing successive layers (for example, comparing layer 2 versus layer 3), in the majority of the cases, the higher layer performed statistically significantly better than the lower one (ρ &lt; 0.01), according to the approximate randomization test (Pado´ 2006).17 Similar to the results on morphological tagging, a combination of all layers achieved the best results. See the Combination bar in Figure 9a. This implies that although syntax is 16 In their study of NMT and language model representations, Zhang and Bowman (2018) noticed that POS is better represented at layer 1 whereas CCG supertags are sometimes, but not always, better represented at layer 2 (out of 2-layer encoders). 17 See Section 11 in the supplementary information for the detailed results. 24 Belinkov, Durrani et al. Linguistic Representations in NMT mainly learned at higher layers, syntactic information is at least partly distributed across the network. One possible concern with these results is that they may be appearing because of the stacked RNN layers, and not necessarily due to the translation task. In the extreme case, perhaps even a rand"
2020.cl-1.1,2007.mtsummit-papers.71,0,0.109127,"his girl@@ friend Characters Professor admits to shooting his girlfriend 7 Computational Linguistics Volume 46, Number 1 3.2 Syntax Linguistic theories argue that words are hierarchically organized in syntactic constituents referred to as syntactic trees. It is therefore natural to think that translation models should be based on trees rather than a flat sequence representation of sentences. For more than a decade of research in machine translation, a tremendous amount of effort has been put into syntax-based machine translation (Yamada and Knight (2002); Chiang (2005), Galley et al. (2006), Zhang et al. (2007), Shen, Xu, and Weischedel (2010); Neubig and Duh (2014)), with notable success in languages such as Chinese and German, which are syntactically divergent compared to English. However, the sequence-to-sequence NMT systems were able to surpass the performance of the stateof-the-art syntax-based systems in recent MT competitions (Bojar et al. 2016). The LSTM-based RNN model with the help of the attention mechanism is able to handle long-distance dependencies. There have also been recent attempts to integrate syntax into NMT (Eriguchi, Hashimoto, and Tsuruoka 2016; Stahlberg et al. 2016; Aharoni"
2020.cl-1.1,Q16-1027,0,0.0288016,"the BPE-based units. Comparing encoder representations with decoder representations, it is interesting to see that in several cases the decoder-side representations performed better than the encoder-side representations, even though they are trained using a unidirectional LSTM only. Because we did not see any notable trends in differences between encoder and decoder side representations, we only present the encoder-side results in the rest of the paper. 19 Computational Linguistics Volume 46, Number 1 6.3 Effect of Network Depth Modern NMT systems use very deep architectures (Wu et al. 2016b; Zhou et al. 2016). We are interested in understanding what kind of information different layers capture. Given a trained NMT model with multiple layers, we extract feature representations from the different layers in the encoder. We trained 4-layered models (using (NEW+TED+Europarl data). Figure 6 shows morphological tagging results using representations from different encoder and decoder layers across five language pairs. The general trend shows that representations from the first layer are better than those from the higher layers, for the purpose of capturing morphology. We found this observation to be true"
2020.cl-1.1,L16-1561,0,0.0590262,"Missing"
2020.cl-1.1,E03-1076,0,\N,Missing
2020.cl-1.1,P07-2045,0,\N,Missing
2020.cl-1.1,D15-1246,0,\N,Missing
2020.cl-1.1,J17-4003,0,\N,Missing
2020.cl-1.1,N16-3003,1,\N,Missing
2020.cl-1.1,W16-2301,0,\N,Missing
2020.cl-1.1,E17-2039,0,\N,Missing
2020.cl-1.1,P17-1080,1,\N,Missing
2020.cl-1.1,W17-4707,0,\N,Missing
2020.cl-1.1,D17-1304,0,\N,Missing
2020.cl-1.1,W17-4717,0,\N,Missing
2020.cl-1.1,Q19-1004,1,\N,Missing
2020.cl-1.1,N18-1118,0,\N,Missing
2020.coling-main.447,D19-1632,0,0.061235,"Missing"
2020.coling-main.447,W14-3627,0,0.0295886,"ous fine-grained categories of dialects. Other efforts outside of MT arena to build resources on Arabic dialect include, but are not limited to (Diab et al., 2014; Khalifa et al., 2016; Jarrar et al., 2017; Suwon et al., 2020; Mubarak et al., 2020). Machine Translation Machine Translation of Arabic dialects got attention for a short while due to BOLT project. Subsequent efforts were carried to improve MSA-to-English systems by appending Dialectto-MSA module as pre-processing step (Salloum and Habash, 2011; Salloum and Habash, 2013; Zbib et al., 2012; Sajjad et al., 2013; Durrani et al., 2014; Jeblee et al., 2014) or adapting the MSA-to-English systems towards in-domain dialectal data (Sajjad et al., 2016). Salloum et al. (2014) studied the use of sentence level dialect identification in optimizing MT system selection in mixed dialectal scenario. More recently Baniata et al. (2018) used multi-task learning in neural MT with individual encoders for MSA and dialects and a shared decoder. Despite the number of efforts in translating Arabic dialects to MSA, they are limited to a few dialects and the results among various studies are not comparable due to the difference of evaluation sets. In this paper, we"
2020.coling-main.447,L16-1679,0,0.0262907,"o be frequent in the MADAR testset where average sentence length is only 7 words. 6 Related Work Data Resources Numerous efforts have been made to build content for dialectal Arabic. Zbib et al. (2012) released Egyptian- and Levantine-English data gathered from weblogs and online user groups, translated through Amazon Mechanical Turk. Bouamor et al. (2014) and Bouamor et al. (2018) created multi-parallel data resources covering various fine-grained categories of dialects. Other efforts outside of MT arena to build resources on Arabic dialect include, but are not limited to (Diab et al., 2014; Khalifa et al., 2016; Jarrar et al., 2017; Suwon et al., 2020; Mubarak et al., 2020). Machine Translation Machine Translation of Arabic dialects got attention for a short while due to BOLT project. Subsequent efforts were carried to improve MSA-to-English systems by appending Dialectto-MSA module as pre-processing step (Salloum and Habash, 2011; Salloum and Habash, 2013; Zbib et al., 2012; Sajjad et al., 2013; Durrani et al., 2014; Jeblee et al., 2014) or adapting the MSA-to-English systems towards in-domain dialectal data (Sajjad et al., 2016). Salloum et al. (2014) studied the use of sentence level dialect iden"
2020.coling-main.447,P17-4012,0,0.0119468,"omain of MSA to maximize the benefit of large available MSA-English parallel data and the small amount of available dialectal training data. Lastly, we use back-translation (Sennrich et al., 2016a) to increase the size of the dialectal Arabic-English training data. We train an English-MSA MT system, fine-tune it on dialects and translate English monolingual data to dialectal Arabic. Then, we use this noisy dialect-English data as an additional training data to improve dialectal Arabic to English translation system. Model Settings We used transformer-based seq2seq model implemented in OpenNMT (Klein et al., 2017). We used default training and decoding settings: 6 encoder and 6 decoder layers, layer size 512, attention heads 8, dropout 0.1, Adam β1 0.9, β2 0.998 and batch size 4096 subwords. For fine-tuning, we additionally use a warmup step size of 800 and label smoothing 0.1.9 We train for 20 epochs and select the best model using the provided development sets. For example, in the case of a dialect specific system say, Egyptian, we choose the model that performs the best on Egyptian development sets. For a system targeting multiple dialects, we choose the model with the best average performance acros"
2020.coling-main.447,L16-1147,0,0.0203505,"text of MT performance in Section 5. 4 Evaluation In this section, we describe experimental setup and present our results using the evaluation suite. 4.1 Training Data and Evaluation Data Table 2 summarizes the available Arabic-English training data. The only reasonable sized dialectal training data are of Levantine and Egyptian which consist of 136k and 37k parallel sentences respectively. Rest of the dialect data is very small. We additionally use a large MSA-English corpus to explore the usefulness of MSA in translating dialectal Arabic effectively. The MSA-English corpus consists of OPUS (Lison and Tiedemann, 2016), UN (Ziemski et al., 2016), TED (Cettolo, 2016), NEWS, and QED (Guzm´an et al., 2013) corpora. In addition to the dialect testsets mentioned in Section 3, we use five MSA-English testsets – one from News domain, news04, and four from TED talks, test11-14 for some selected experiments. 4.2 Training and Model Settings Training Settings We build models using various training settings. First, we train systems using the available dialect training data and the MSA data listed in Table 2. Second, we apply the fine-tuning strategy which has shown to be effective in domain adaptation (Sajjad et al., 2"
2020.coling-main.550,C18-1169,0,0.147669,"y, 1975). This technique mitigates the misspellings challenge efficiently. Furthermore, Li and Sun (2014) and Li and Sun (2017), constructed noisy gazetteers using cross-posts on Twitter from Foursquare check-ins. To detect LMs, they developed a linear-chain CRF model and trained it over lexical, grammatical, and geographical features. Differently, Ghahremanlou et al. (2014) and Yin et al. (2014) retrain StandfordNER using tweet dataset, as it was originally trained on newswire articles (CoNLL-2003, MUC 66 , and MUC 77 ), to effectively identify the location mentions in tweets. More recently, Al-Olimat et al. (2018) proposed a statistical approach to construct regional language models. Their tagger identifies the LMs by traversing a tree of n-grams while matching them against region-specific gazetteers. Furthermore, in 2014, the topic of the fifth Australasian Language Technology Association ALTA shared task was on identifying LMs in tweets (Molla and Karimi, 2014). Participants explored several techniques such as feature engineering, ensemble classifiers, rule-based classification, knowledge infusion, CRFs sequence labelers, semi-supervision. As for features, they used different features including geosp"
2020.coling-main.550,C16-1111,0,0.0223532,"(iii) Crisis-related Twitter dataset. Table 2 shows various statistics of the datasets used in our experiments, which are described below. • General-purpose NER dataset: A well-known candidate for this category is the CoNLL-2003 NER dataset (Sang and De Meulder, 2003), which comprises of newswire text from Reuters, tagged with four different entity types, namely PER, LOC, ORG, and MISC. Overall, the dataset contains 22,137 sentences and 35,089 entities. We used the standard training segment for training. • Twitter NER dataset: We use the Broad Twitter Corpus (BTC) as our Twitter NER dataset (Derczynski et al., 2016). It consists of 9,515 tweets, which are tagged with three entity types, namely PER, LOC, and ORG. The dataset has a broad coverage of spatial, temporal, and social aspects. Various segments in the dataset represent different types of data collection and annotation methodologies. For instance, Segment A comprises of random samples of UK tweets about “New Year”. We used all segments for training in our experiments. • Crisis-related Twitter dataset: As the main focus of this work is to guide the development of a robust LMR system for toponym extraction from crisis-related tweets, we use several"
2020.coling-main.550,U14-1022,0,0.0297199,"emanlou et al. (2014) and Yin et al. (2014) retrain StandfordNER using tweet dataset, as it was originally trained on newswire articles (CoNLL-2003, MUC 66 , and MUC 77 ), to effectively identify the location mentions in tweets. More recently, Al-Olimat et al. (2018) proposed a statistical approach to construct regional language models. Their tagger identifies the LMs by traversing a tree of n-grams while matching them against region-specific gazetteers. Furthermore, in 2014, the topic of the fifth Australasian Language Technology Association ALTA shared task was on identifying LMs in tweets (Molla and Karimi, 2014). Participants explored several techniques such as feature engineering, ensemble classifiers, rule-based classification, knowledge infusion, CRFs sequence labelers, semi-supervision. As for features, they used different features including geospatial, structural, and lexical. StanfordNER was also used but after retraining it using tweet dataset. Among all the related work, there is no single study that explores the setups we propose in this paper. Typically, the proposed approaches are trained and tested on target events, assuming the training data is available at the onset of a disaster event,"
2020.coling-main.550,W09-1119,0,0.0217072,"t off the popular Jersey... [url] Table 1: Tweets from real-world disaster events with location mentions (highlighted) The problem is formally defined as follows: given a tweet t that is related to a disaster event e, the LMR system aims to identify all location mentions Lt = {li ; i ∈ [1, nt ]} in the tweet t, where li is the ith location mention and nt is the total number of location mentions in t if any. Each location mention may span one or more tokens. In this work, we follow the BILOU annotation scheme with 5 classes8 , due to its better performance over the commonly adopted BIO scheme (Ratinov and Roth, 2009; Dai et al., 2015; Yang et al., 2018). In the BIO scheme, labels identify the position of every term in LM: “B” denotes the beginning token of an LM, “I” denotes a token inside LM, and “O” denotes a token outside of LM. The BILOU scheme extends the BIO scheme to more positional tags: “L” denotes the last token in LM and “U” denotes the only token of a single-token LM. Therefore, we define the LMR as a multi-class classification task on the token level. 4 Experimental Setup In this section, we describe the details of our experimental setup. We present the datasets in Section 4.1 and the experi"
2020.coling-main.550,W00-0726,0,0.0100499,"d the tweets to remove ‘RT’, user mentions, non-ASCII characters, and URLs. We also segmented the hashtags using the word segment library9 , since some location mentions appear as subtokens of hashtags in the datasets. 4.4 Evaluation Measures To measure the effectiveness of the LMR model over different setups, we compute Precision (P), Recall (R), and their harmonic mean (F1 score) for each entity (i.e., location mention) using the seqeval (v0.0.12) package,10 which adopts the evaluation scripts used to evaluate the chunking tasks (e.g., namedentity recognition) in CoNLL-2000 NER shared task (Sang and Buchholz, 2000). The package evaluates the model’s output on entity-level rather than token-level. We use the default micro-average metric to account for the class imbalance issue in our datasets (see class distributions in table 2). 5 Results and Analysis In this section, we thoroughly discuss the results of our experiments to answer our research questions. We explore the usefulness of exploiting “out-domain” training data with either multiple entity types (such as person and organization) alongside the location entity (Section 5.1) or with location entity alone (Section 5.2). We further study the performan"
2020.coling-main.550,W03-0419,0,0.133502,"Missing"
2020.coling-main.550,C18-1327,0,0.0134725,"Tweets from real-world disaster events with location mentions (highlighted) The problem is formally defined as follows: given a tweet t that is related to a disaster event e, the LMR system aims to identify all location mentions Lt = {li ; i ∈ [1, nt ]} in the tweet t, where li is the ith location mention and nt is the total number of location mentions in t if any. Each location mention may span one or more tokens. In this work, we follow the BILOU annotation scheme with 5 classes8 , due to its better performance over the commonly adopted BIO scheme (Ratinov and Roth, 2009; Dai et al., 2015; Yang et al., 2018). In the BIO scheme, labels identify the position of every term in LM: “B” denotes the beginning token of an LM, “I” denotes a token inside LM, and “O” denotes a token outside of LM. The BILOU scheme extends the BIO scheme to more positional tags: “L” denotes the last token in LM and “U” denotes the only token of a single-token LM. Therefore, we define the LMR as a multi-class classification task on the token level. 4 Experimental Setup In this section, we describe the details of our experimental setup. We present the datasets in Section 4.1 and the experimental configurations in Section 4.2."
2020.emnlp-main.395,E17-2039,0,0.043767,"Missing"
2020.emnlp-main.395,J99-2004,0,0.159195,"The overall pattern remained similar in the task of chunking. Notice however, a shift in pattern – the contribution from lower layers decreased compared to previous tasks, in the case of BERT. For example, in the SEM task, top neurons were dominantly contributed from lower and middle layers, in chunking middle and higher layers contributed most. This could be attributed to the fact that chunking is a more complex syntactic task and is learned at relatively higher layers. CCG Supertagging: Compared to chunking, CCG supertagging is a richer syntactic tagging task, almost equivalent to parsing (Bangalore and Joshi, 1999). The complexity of the task is evident in our results as there is a clear shift in the distribution of top neurons moving from middle to higher layers. The only exception again is the BERT model where this information is well spread across the network, but still dominantly preserved in the final layers. Discussion: Our results are in line with and reinforce the layer-wise analysis presented in Liu et al. (2019). However, unlike their work and all other work on layer-wise probing analysis, which trains a classifier on each layer individually to compare the results, our method trains a single c"
2020.emnlp-main.395,P17-1080,1,0.852479,"tailment, etc. (Rajpurkar et al., 2016; Wang et al., 2018). Central to this revolution is the contextualized embedding, where each word is assigned a vector based on the entire input sequence, allowing it to capture not only a static semantic meaning but also a contextualized meaning. Previous work on analyzing neural networks showed that while learning rich NLP tasks such as machine translation and language modeling, these deep models capture fundamental linguistic phenomena such as word morphology, syntax and various other relevant properties of interest (Shi et al., 2016; Adi et al., 2016; Belinkov et al., 2017a,b; Dalvi et al., 2017; Blevins et al., 2018). More recently Liu et al. (2019) and Tenney et al. (2019) used probing classifiers to analyze pretrained neural language models on a variety of sequence labeling tasks and demonstrated that contextualized representations encode useful, transferable features of language. While most of the previous studies emphasize and analyze representations as a whole, very little work has been carried to analyze individual neurons in deep NLP models. Studying individual neurons can facilitate understanding of the inner workings of neural networks (Karpathy et al"
2020.emnlp-main.395,2020.cl-1.1,1,0.915589,"n, using a categorical cross-entropy loss, optimized by Adam (Kingma and Ba, 2014). Training is run with shuffled mini-batches of size 512 and stopped after 10 epochs. The regularization weights are trained using grid-search algorithm.8 For sub-word based models, we use the last activation value to be the representative of the word as prescribed for the embeddings extracted from Neural MT models (Durrani et al., 2019) and pre-trained Language Models (Liu et al., 2019). Linear classifiers are a popular choice in analyzing deep NLP models due to their better interpretability (Qian et al., 2016; Belinkov et al., 2020). Hewitt and Liang (2019) have also shown linear probes to have higher Selectivity, a property deemed desirable for more interpretable probes. Linear probes are particularly important for our method as we use the learned weights as a proxy to measure the importance of each neuron. 4 Evaluation 4.1 Minimal Neuron Set Now that we have established correctness of the rankings, we apply the algorithm incrementally to select minimal neurons for each linguistic task 8 XLNet All Top Random Bottom 96.04 90.16 28.45 16.86 96.13 92.28 58.17 44.64 All Top Random Bottom 92.09 84.32 64.28 59.02 92.64 90.70"
2020.emnlp-main.395,Q19-1004,1,0.843657,"zation methods to analyze learned representations (Karpathy et al., 2015; K´ad´ar et al., 2017), attention heads (Clark et al., 2019; Vig, 2019) of language compositionality (Li et al., 2016) etc. While such visualizations illuminate the inner workings of the network, they are often qualitative in nature and somewhat anecdotal. A more commonly used approach tries to provide a quantitative analysis by correlating parts of the neural network with linguistic properties, for example by training a classifier to predict a feature of interest (Adi et al., 2016; Conneau et al., 2018). Please refer to Belinkov and Glass (2019) for a comprehensive survey of work done in this direction. Liu et al. (2019) used probing classifiers for investigating the contextualized representations learned from a variety of neural language models on numerous word level linguistic tasks. A similar analysis was carried by Tenney et al. (2019) on a variety of sub-sentence linguistic tasks. We extend this line of work to carry out a more fine-grained neuron level analysis of neural language models. Our work is most similar to Dalvi et al. (2019) who conducted neuron analysis of representations learned from sequence-to-sequence machine tra"
2020.emnlp-main.395,I17-1001,1,0.927499,"Missing"
2020.emnlp-main.395,P18-2003,0,0.0181682,"t al., 2018). Central to this revolution is the contextualized embedding, where each word is assigned a vector based on the entire input sequence, allowing it to capture not only a static semantic meaning but also a contextualized meaning. Previous work on analyzing neural networks showed that while learning rich NLP tasks such as machine translation and language modeling, these deep models capture fundamental linguistic phenomena such as word morphology, syntax and various other relevant properties of interest (Shi et al., 2016; Adi et al., 2016; Belinkov et al., 2017a,b; Dalvi et al., 2017; Blevins et al., 2018). More recently Liu et al. (2019) and Tenney et al. (2019) used probing classifiers to analyze pretrained neural language models on a variety of sequence labeling tasks and demonstrated that contextualized representations encode useful, transferable features of language. While most of the previous studies emphasize and analyze representations as a whole, very little work has been carried to analyze individual neurons in deep NLP models. Studying individual neurons can facilitate understanding of the inner workings of neural networks (Karpathy et al., 2015; Dalvi et al., 2019; Suau et al., 2020"
2020.emnlp-main.395,N19-1112,1,0.712915,"ion is the contextualized embedding, where each word is assigned a vector based on the entire input sequence, allowing it to capture not only a static semantic meaning but also a contextualized meaning. Previous work on analyzing neural networks showed that while learning rich NLP tasks such as machine translation and language modeling, these deep models capture fundamental linguistic phenomena such as word morphology, syntax and various other relevant properties of interest (Shi et al., 2016; Adi et al., 2016; Belinkov et al., 2017a,b; Dalvi et al., 2017; Blevins et al., 2018). More recently Liu et al. (2019) and Tenney et al. (2019) used probing classifiers to analyze pretrained neural language models on a variety of sequence labeling tasks and demonstrated that contextualized representations encode useful, transferable features of language. While most of the previous studies emphasize and analyze representations as a whole, very little work has been carried to analyze individual neurons in deep NLP models. Studying individual neurons can facilitate understanding of the inner workings of neural networks (Karpathy et al., 2015; Dalvi et al., 2019; Suau et al., 2020) and have other potential benefi"
2020.emnlp-main.395,J93-2004,0,0.0764961,"4 dimensions. Its transformer equivalent (T-ELMo) is trained with 7 layers but with the same hidden layer size. The BERT model is trained as an auto-encoder with a dual objective function of predicting masked words and next sentence in auto-encoding fashion. We use base version (13 layers and 768 dimensions). Lastly we included XLNet-base which is trained with the same parameter settings (number and size of hidden layers) as BERT, but with a permutation based auto-regressive objective function. Language Tasks: We evaluated our method on 4 linguistic tasks: POS-tagging using the Penn TreeBank (Marcus et al., 1993), syntax tagging (CCG supertagging)7 using CCGBank (Hockenmaier, 2006), syntactic chunking using CoNLL 2000 shared task dataset (Tjong Kim Sang and Buchholz, 2000), and semantic tagging using the Parallel Meaning Bank data (Abzianidze et al., 2017). We used standard splits for training, de7 CCG captures global syntactic information locally at the word level by assigning a label to each word annotating its syntactic role in the sentence. The annotations can be thought of as a function that takes and return syntactic categories (like an NP: Noun phase). 4867 velopment and test data (See Appendix"
2020.emnlp-main.395,N18-1202,0,0.352548,"rons to play a role in the train4865 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4865–4880, c November 16–20, 2020. 2020 Association for Computational Linguistics ing of the classifier. Given a trained classifier, we consider the weights assigned to each neuron as a measure of their importance with respect to the understudied linguistic task. We use probes with high selectivity (Hewitt and Liang, 2019) to ensure that our results reflect the property of representations and not the probe’s capacity to learn. We choose 4 pre-trained models: ELMo (Peters et al., 2018a), its transformer variant T-ELMo (Peters et al., 2018b), BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019) – covering a varied set of modeling choices, including the building blocks (recurrent networks versus Transformers), optimization objective (auto-regressive versus nonautoregressive), and model depth and width. Our cross architectural analysis yields the following insights: • Information across networks is distributed, but it is possible to extract a very small subset of neurons to predict a linguistic task with the same accuracy as using the entire network. • Low level tasks suc"
2020.emnlp-main.395,D18-1179,0,0.0983389,"rons to play a role in the train4865 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4865–4880, c November 16–20, 2020. 2020 Association for Computational Linguistics ing of the classifier. Given a trained classifier, we consider the weights assigned to each neuron as a measure of their importance with respect to the understudied linguistic task. We use probes with high selectivity (Hewitt and Liang, 2019) to ensure that our results reflect the property of representations and not the probe’s capacity to learn. We choose 4 pre-trained models: ELMo (Peters et al., 2018a), its transformer variant T-ELMo (Peters et al., 2018b), BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019) – covering a varied set of modeling choices, including the building blocks (recurrent networks versus Transformers), optimization objective (auto-regressive versus nonautoregressive), and model depth and width. Our cross architectural analysis yields the following insights: • Information across networks is distributed, but it is possible to extract a very small subset of neurons to predict a linguistic task with the same accuracy as using the entire network. • Low level tasks suc"
2020.emnlp-main.395,W19-4302,0,0.0441719,"Missing"
2020.emnlp-main.395,2020.acl-main.420,0,0.0155308,"efined control tasks to analyze the role of training data and lexical memorization in probing experiments. Voita and Titov (2020) proposed an alternative that measures Minimal Description Length of labels given representations. It would be interesting to see how a probe’s complexity in their work (code length) compares with the number of selected neurons according to our method. The results are consistent at least in the ELMo POS example, where layer 1 was shown to have the shortest code length in their work. In our case, most top neurons are selected from layer 1 (see Figure 1d for example). Pimentel et al. (2020) discussed the complexity of the probes and argued for using highest performing probes for tighter estimates. However, complex probes are difficult to analyze. Linear models are preferable due to their explainability; especially in our work, as we use the learned weights as a proxy to get a measure of the importance of each neuron. We used linear classifiers with control tasks as described in Hewitt and Liang (2019). Although we mainly used probing accuracy to drive the neuron selection in this work, and Selectivity only to demonstrate that our results reflect the property learned by represent"
2020.emnlp-main.395,D16-1079,0,0.105339,"Missing"
2020.emnlp-main.395,D16-1264,0,0.0345889,"found small subsets of neurons to predict linguistic tasks, with lower level tasks (such as morphology) localized in fewer neurons, compared to higher level task of predicting syntax. Our study reveals interesting cross architectural comparisons. For example, we found neurons in XLNet to be more localized and disjoint when predicting properties compared to BERT and others, where they are more distributed and coupled. 1 Introduction Transformer-based neural language models have constantly pushed the state-of-the-art in downstream NLP tasks such as Question Answering, Textual Entailment, etc. (Rajpurkar et al., 2016; Wang et al., 2018). Central to this revolution is the contextualized embedding, where each word is assigned a vector based on the entire input sequence, allowing it to capture not only a static semantic meaning but also a contextualized meaning. Previous work on analyzing neural networks showed that while learning rich NLP tasks such as machine translation and language modeling, these deep models capture fundamental linguistic phenomena such as word morphology, syntax and various other relevant properties of interest (Shi et al., 2016; Adi et al., 2016; Belinkov et al., 2017a,b; Dalvi et al."
2020.emnlp-main.395,2020.emnlp-main.14,0,0.0116592,"to select the regularization parameters, compared to manual selection of lambdas, which is cumbersome and error-prone. In contemporaneous work, Suau et al. (2020) used max-pooling to identify relevant neurons (aka Expert units) in pre-trained models, with respect to a specific concept (for example word-sense). A pitfall to the approach of probing classifiers is whether the probe is faithfully reflecting the property of the representation or just learned the task? Hewitt and Liang (2019) defined control tasks to analyze the role of training data and lexical memorization in probing experiments. Voita and Titov (2020) proposed an alternative that measures Minimal Description Length of labels given representations. It would be interesting to see how a probe’s complexity in their work (code length) compares with the number of selected neurons according to our method. The results are consistent at least in the ELMo POS example, where layer 1 was shown to have the shortest code length in their work. In our case, most top neurons are selected from layer 1 (see Figure 1d for example). Pimentel et al. (2020) discussed the complexity of the probes and argued for using highest performing probes for tighter estimate"
2020.emnlp-main.395,W18-5446,0,0.0640162,"Missing"
2020.emnlp-main.395,2020.acl-main.422,1,0.786141,"ould be to use selectivity itself to drive the investigation. However, it is not trivial how to optimize for selectivity as it cannot be controlled/tuned directly – for example, removing some neurons may decrease accuracy but may not change selectivity. We leave this exploration for future work. Probing classifiers require supervision for the linguistic tasks of interest with annotations, limiting their applicability. Bau et al. (2019) used unsupervised approach to identify salient neurons in neural machine translation and manipulated translation output by controlling these neurons. Recently, Wu et al. (2020) measured similarity of internal representations and attention across prominent contextualized representations (from BERT, ELMo, etc.). They found that different architectures have similar representations, but different individual neurons. 7 Conclusion We analyzed individual neurons across a variety of neural language models using linguistic correlation analysis on the task of predicting core linguistic properties (morphology, syntax and semantics). Our results reinforce previous findings and also illuminate further insights: i) while the information in neural language models is massively dist"
2020.emnlp-main.398,W17-6901,0,0.0195206,"Missing"
2020.emnlp-main.398,N19-1112,1,0.846794,"Missing"
2020.emnlp-main.398,2020.acl-main.420,0,0.0204587,"asks. 2 Related Work A number of studies have analyzed representations at layer-level (Conneau et al., 2018; Liu et al., 2019; Tenney et al., 2019; Kim et al., 2020; Belinkov et al., 2020) and at neuron-level (Bau et al., 2019; Dalvi et al., 2019; Suau et al., 2020; Durrani et al., 2020). These studies aim at analyzing either the linguistic knowledge learned in representations and in neurons or the general importance of neurons in the model. The former is commonly done using a probing classifier (Shi et al., 2016a; Belinkov et al., 2017; Hupkes et al., 2018). Recently, Voita and Titov (2020); Pimentel et al. (2020) proposed probing methods based on information theoretic measures. The general importance of neurons is mainly captured using similarity and correlationbased methods (Raghu et al., 2017; Chrupała and Alishahi, 2019; Wu et al., 2020). Similar to the work on analyzing deep NLP models, we analyze pretrained models at representation-level and at neuron-level. Different from them, we analyze various forms of redundancy in these models. We draw upon various techniques from the literature and adapt them to perform a redundancy analysis. While the work on pretrained model compression (Cao et al., 2020"
2020.emnlp-main.398,D16-1079,0,0.0261294,"Missing"
2020.emnlp-main.398,D16-1264,0,0.0121626,"h (POS) tagging using the Penn TreeBank, ii) CCG super tagging using CCGBank (Hockenmaier, 2006), iii) semantic tagging (SEM) using Parallel Meaning Bank data (Abzianidze and Bos, 2017) and iv) syntactic chunking using CoNLL 2000 shared task dataset (Sang and Buchholz, 2000). For sequence classification, we study tasks from the GLUE benchmark (Wang et al., 2018), namely i) sentiment analysis (SST-2) (Socher et al., 2013), ii) semantic equivalence classification (MRPC) (Dolan and Brockett, 2005), iii) natural language inference (MNLI) (Williams et al., 2018), iv) question-answering NLI (QNLI) (Rajpurkar et al., 2016), iv) question pair similarity2 (QQP), v) textual entailment (RTE) (Bentivogli et al., 2009), and vi) semantic textual similarity (Cer et al., 2017).3 Complete statistics for all datasets is provided in Appendix A.1. Other Settings The neuron activations for each word in our dataset are extracted from the pretrained model for sequence labeling while the [CLS] token’s representation (from a fine-tuned model) is used for sequence classification. The fine-tuning step is essential to optimize the [CLS] token for sentence representation. In the case of sub-words, we pick the last sub-word’s represe"
2020.emnlp-main.398,W00-0726,0,0.0907869,"Missing"
2020.emnlp-main.398,D16-1248,0,0.360354,"ion in a pretrained model is necessary for specific downstream tasks? and v) can we exploit redundancy to 4908 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4908–4926, c November 16–20, 2020. 2020 Association for Computational Linguistics enable efficiency? We introduce several methods to analyze redundancy in the network. Specifically, for general redundancy, we use Center Kernel Alignment (Kornblith et al., 2019) for layer-level analysis, and Correlation Clustering for neuron-level analysis. For task-specific redundancy, we use Linear Probing (Shi et al., 2016a; Belinkov et al., 2017) to identify redundant layers, and Linguistic Correlation Analysis (Dalvi et al., 2019) to examine neuronlevel redundancy. We conduct our study on two pretrained language models, BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019). While these networks are similar in the number of parameters, they are trained using different training objectives, which accounts for interesting comparative analysis between these models. For task-specific analysis, we present our results across a wide suite of downstream tasks: four core NLP sequence labeling tasks and seven sequence"
2020.emnlp-main.398,D16-1159,0,0.250577,"ion in a pretrained model is necessary for specific downstream tasks? and v) can we exploit redundancy to 4908 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4908–4926, c November 16–20, 2020. 2020 Association for Computational Linguistics enable efficiency? We introduce several methods to analyze redundancy in the network. Specifically, for general redundancy, we use Center Kernel Alignment (Kornblith et al., 2019) for layer-level analysis, and Correlation Clustering for neuron-level analysis. For task-specific redundancy, we use Linear Probing (Shi et al., 2016a; Belinkov et al., 2017) to identify redundant layers, and Linguistic Correlation Analysis (Dalvi et al., 2019) to examine neuronlevel redundancy. We conduct our study on two pretrained language models, BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019). While these networks are similar in the number of parameters, they are trained using different training objectives, which accounts for interesting comparative analysis between these models. For task-specific analysis, we present our results across a wide suite of downstream tasks: four core NLP sequence labeling tasks and seven sequence"
2020.emnlp-main.398,D13-1170,0,0.00525291,"c analysis, we use two broad categories of downstream tasks – Sequence Labeling and Sequence Classification tasks. For the sequence labeling tasks, we study core linguistic tasks, i) part-of-speech (POS) tagging using the Penn TreeBank, ii) CCG super tagging using CCGBank (Hockenmaier, 2006), iii) semantic tagging (SEM) using Parallel Meaning Bank data (Abzianidze and Bos, 2017) and iv) syntactic chunking using CoNLL 2000 shared task dataset (Sang and Buchholz, 2000). For sequence classification, we study tasks from the GLUE benchmark (Wang et al., 2018), namely i) sentiment analysis (SST-2) (Socher et al., 2013), ii) semantic equivalence classification (MRPC) (Dolan and Brockett, 2005), iii) natural language inference (MNLI) (Williams et al., 2018), iv) question-answering NLI (QNLI) (Rajpurkar et al., 2016), iv) question pair similarity2 (QQP), v) textual entailment (RTE) (Bentivogli et al., 2009), and vi) semantic textual similarity (Cer et al., 2017).3 Complete statistics for all datasets is provided in Appendix A.1. Other Settings The neuron activations for each word in our dataset are extracted from the pretrained model for sequence labeling while the [CLS] token’s representation (from a fine-tun"
2020.emnlp-main.398,P19-1580,0,0.0800221,"e, BERT large (Devlin et al., 2019), NVIDIA’s Megatron model, and Google’s T5 model (Raffel et al., 2019) were trained using 340 million, 8.3 billion and 11 billion parameters respectively. An emerging body of work shows that these models are over-parameterized and do not require all the representational power lent by the rich architectural choices during inference. For example, these models can be distilled (Sanh et al., 2019; 1 The code for the experiments in this paper is available at https://github.com/fdalvi/analyzingredundancy-in-pretrained-transformermodels Sun et al., 2019) or pruned (Voita et al., 2019; Sajjad et al., 2020), with a minor drop in performance. Recent research (Mu et al., 2018; Ethayarajh, 2019) analyzed contextualized embeddings in pretrained models and showed that the representations learned within these models are highly anisotropic. While these approaches successfully exploited over-parameterization and redundancy in pretrained models, the choice of what to prune is empirically motivated and the work does not directly explore the redundancy in the network. Identifying and analyzing redundant parts of the network is useful in: i) developing a better understanding of these m"
2020.wmt-1.4,abdelali-etal-2014-amara,1,0.827823,"ne translated all comments using an in-house transformer-based model into Japanese and German. The goal of that was to be able to examine potential differences in source and (one example of) translation segments.3 We then pre-processed and automatically annotated all 17K segments with the following soft labels for catastrophic errors: Development Data The task specified the following data to help participants evaluate their system’s performance on unseen and multiple domains. • English-German: participants can use the development data from the News translation task, development data from QED (Abdelali et al., 2014) corpus, development data from WMT19 Medical translation task, and development data from the WMT16 IT translation task. 1. Introduction of toxicity: we checked both source and machine translation for toxic words (using in-house lists) and labelled as positive (i.e. potentially containing errors) cases where the source does not contain such words, but the translation does (at least one). • English-Japanese: participants can use the development data from the News translation task, and development data from the MTNT dataset, which contains noisy social media texts and their clean translations. 3."
2020.wmt-1.4,D17-1158,0,0.0130333,"Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively underexplored scenario, where examples in the target domain are either unavailable or relatively few. Other aspects of robustness are robustness to adversarial examples or noisy inputs. The"
2020.wmt-1.4,N19-1311,0,0.0437239,"Missing"
2020.wmt-1.4,P18-1128,0,0.014933,"Naver Labs and LIMSI made specific efforts towards the task of Robustness. Both of them used lightweight domain adaptors proposed by Bapna and Firat (2019). Both teams UEDIN: Team UEDIN also mainly trained their system towards News translation task, but added Gumbel noise to the output layer of the systems submitted to the Robustness task. They followed standard NMT training pipeline and boasted their systems with additional data filtered from the paracrawl corpus. The data was carefully selected using dual cross-entropy (Junczys-Dowmunt, 2018) and length-normalized cross-entropy. 81 script6 (Dror et al., 2018) with p <0.05. The result of significance test in likert score is used for the human judgement ranking. Interestingly, the correlation in the system rankings between human judgments and BLEU is not strong. In other words, the best performing systems in BLEU do not rank high according human judgement, sometimes even rank the lowest. For example, in Ja→En (set2), the online-B system ranks first in BLEU but last in likert score. OPPO outperforms all systems in both directions on set2, and is overall the best system among the constrained, zero-shot submissions. To get insight on the proportion of"
2020.wmt-1.4,2020.lrec-1.520,0,0.0937737,"raped, filtered for noisy comments and translated by professional translators. This year, data was collected for two translation directions: English→Japanese and Japanese→English. For English, comments were collected from the /r/all feed, which encompasses all communities, and filtered for English. Since Japanese is a minority language on Reddit, comments were scraped from a selection of japanese-speaking communities, detailed in Michel and Neubig (2018). Common Voices Test Set (set3): This data was obtained from from the CoVoST corpus (Wang et al., 2020). CoVoST is derived from Common Voice (Ardila et al., 2020), a crowdsourced speech recognition corpus with an open CC0 license. Transcripts were sent to professional translators and the quality of translations was controlled by automatic and manual checks (Guzm´an et al., 2019). For this task, we used the German→English test set with source German sentences deduplicated. 1 Bad: translation errors are so severe that they cause the target text to be incomprehensible. This may be mainly due to major grammatical, typographical, or lexical errors, or omission of critical or important salient information. 2 Poor: the target text contains translation errors,"
2020.wmt-1.4,N19-1154,1,0.82504,"luation and the results discussed in Section 5. We hope that this task leads to more efforts from the community in building robust MT models. Introduction In recent years, Machine Translation (MT) systems have seen great progress, with neural models becoming the de-facto methods and even approaching human quality in news domain (Hassan et al., 2018). However, like other deep learning models, neural machine translation (NMT) models are found to be sensitive to synthetic and natural noise in input, distributional shift, and adversarial examples (Koehn and Knowles, 2017; Belinkov and Bisk, 2018; Durrani et al., 2019; Anastasopoulos et al., 2019; Michel et al., 2019). From an application perspective, MT systems need to deal with non-standard, noisy text of the kind which is ubiquitous on social media and the internet, yet has different distributional signatures from corpora in common benchmark datasets. Following the first shared task on Machine Translation (MT) Robustness, we now propose a second edition, which aims at testing MT systems’ robustness towards domain diversity. Specifically, this year’s task aims to evaluate a general MT system’s performance in the following two scenarios: 2 Related Work Do"
2020.wmt-1.4,D19-1165,0,0.282315,"n a critical way. Critical errors are those that lead to misleading translations which may carry religious, health, safety, legal or financial implications, or introduce toxicity. The set of critical errors used for the guidelines (which also included examples of these errors) includes – but is not limited to – the cases below: Naver Labs (NLE): They participated in Chat and Biomedical tasks along with the Robustness task. They trained a general big-transformer model using FairSeq toolkit (Ott et al., 2019) and adapted it towards different tasks using lightweight adapter layers for each task (Bapna and Firat, 2019). They compared results against the more traditional finetuning method (Luong and Manning, 2015) to show that the former provides a viable alternative, while significantly reducing the amount of parameters per task. They also explored using embedding from pre-trained language models in their NMT system of which they tried two MLM variants: i) using NMT encoder’s setting, using Roberta (Liu et al., 2019). The latter was found more robust to novel domains and noise. The authors found that initializing with first 8 layers instead of the entire model to 80 OPPO: Team OPPO also trained their system"
2020.wmt-1.4,D19-1632,1,0.894086,"Missing"
2020.wmt-1.4,W17-4712,0,0.0183942,"ne-tune on datasets increasingly similar to the target domain (Sajjad et al., 2017), or to dynamically change the balance of data towards the target domain (van der Wees et al., 2017). Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively u"
2020.wmt-1.4,W17-3205,0,0.0203483,"to continuously fine-tune on datasets increasingly similar to the target domain (Sajjad et al., 2017), or to dynamically change the balance of data towards the target domain (van der Wees et al., 2017). Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task pr"
2020.wmt-1.4,P17-2061,0,0.0561223,"Missing"
2020.wmt-1.4,C18-1111,0,0.0137039,"6). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively underexplored scenario, where examples in the target domain are either unavailable or relatively few. Other aspects of robustness are robustness to adversarial examples or noisy inputs. The fragility of neural MT models has been previously demonstrated in various settings (Belinkov and Bisk, 2018; Heigold et al., 2017; Anastasopo"
2020.wmt-1.4,W18-6478,0,0.0263186,"different models to obtain further improvements. Only two teams, namely Naver Labs and LIMSI made specific efforts towards the task of Robustness. Both of them used lightweight domain adaptors proposed by Bapna and Firat (2019). Both teams UEDIN: Team UEDIN also mainly trained their system towards News translation task, but added Gumbel noise to the output layer of the systems submitted to the Robustness task. They followed standard NMT training pipeline and boasted their systems with additional data filtered from the paracrawl corpus. The data was carefully selected using dual cross-entropy (Junczys-Dowmunt, 2018) and length-normalized cross-entropy. 81 script6 (Dror et al., 2018) with p <0.05. The result of significance test in likert score is used for the human judgement ranking. Interestingly, the correlation in the system rankings between human judgments and BLEU is not strong. In other words, the best performing systems in BLEU do not rank high according human judgement, sometimes even rank the lowest. For example, in Ja→En (set2), the online-B system ranks first in BLEU but last in likert score. OPPO outperforms all systems in both directions on set2, and is overall the best system among the cons"
2020.wmt-1.4,P17-4012,0,0.0234194,"the decoder. This allows the test sets from known domains to use adapter layers and for novel domains to use the generic system. They created a noisy domain by adding synthetic noise to source data. The idea is that residual adapter layer learned from such data learns how to deal with noisy domain and is also able to preserve performance on the cleaner domains. However this did not work as well. The residual adapter fine-tuned using the ParaCrawl corpus gave better performance. PROMPT: Team PROMPT also participated mainly in the News translation task. Their systems were trained using OpenNMT (Klein et al., 2017) toolkit. They applied several stages of data preprocessing including length-based filtering, removing duplications, and using in-house classifier based on Hunalign aligner to identify and discard non-parallel sentences. They used two types of synthetic data to improve their systems: i) randomly selecting subset of Wikipedia equal to the size of news data and generating parallel corpus through back-translation, ii) creating synthetic data with unknown words using the procedure described in (Pinnis et al., 2017). Systems were trained with tags to differentiate between original data and syntheti"
2020.wmt-1.4,P02-1040,0,0.114721,"l can bias the selection to cases that are challenging for this particular model. In future work following this methodology, we recommend that multiple MT models be used. 4 https://cloud.google. com/natural-language/docs/ analyzing-sentiment 5 https://github.com/carpedm20/emoji/) 2 www.kaggle.com/c/ jigsaw-toxic-comment-classification- challenge 78 3.5 5. Presence of idioms: we checked if the source contains idiomatic expressions, using an inhouse list of idioms built from various sources, and labelled those cases as positive. Evaluation protocol Automatic evaluation: We first computed BLEU (Papineni et al., 2002) for each system using SacreBLEU (Post, 2018). For all language pairs except En→Ja, we used the original reference and SacreBLEU with the default options. In the case of En→Ja, we used the reference tokenized with KyTea and the option --tokenize none. We note that the automatic labelling using our various pre-processing techniques may have introduced errors, but we believe that basing the selection on such heuristics will still lead to higher chances of selecting very challenging source segments than arbitrarily sampling the data. Human evaluation: The system outputs were evaluated by professi"
2020.wmt-1.4,W17-3204,1,0.837046,"e evaluated both automatically and via a human evaluation and the results discussed in Section 5. We hope that this task leads to more efforts from the community in building robust MT models. Introduction In recent years, Machine Translation (MT) systems have seen great progress, with neural models becoming the de-facto methods and even approaching human quality in news domain (Hassan et al., 2018). However, like other deep learning models, neural machine translation (NMT) models are found to be sensitive to synthetic and natural noise in input, distributional shift, and adversarial examples (Koehn and Knowles, 2017; Belinkov and Bisk, 2018; Durrani et al., 2019; Anastasopoulos et al., 2019; Michel et al., 2019). From an application perspective, MT systems need to deal with non-standard, noisy text of the kind which is ubiquitous on social media and the internet, yet has different distributional signatures from corpora in common benchmark datasets. Following the first shared task on Machine Translation (MT) Robustness, we now propose a second edition, which aims at testing MT systems’ robustness towards domain diversity. Specifically, this year’s task aims to evaluate a general MT system’s performance in"
2020.wmt-1.4,W18-6459,0,0.0133196,"rvey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively underexplored scenario, where examples in the target domain are either unavailable or relatively few. Other aspects of robustness are robustness to adversarial examples or noisy inputs. The fragility of neural MT models has been previously demonstrated in various settings (Belinkov and Bisk, 2018; Heigold et al., 2017; Anastasopoulos et al., 2019; Lee et al., 2018). Michel and Neubig (2018) proposed a new dataset (MTNT) to test MT models for robustness to the types of noise encountered in the Internet. The previous iteration of the shared task focused on robustness of MT systems to such noise (Li et al., 2019). We refer to that report for more details. 3 participants to explore novel training and modeling approaches so that models have more robust performance at test time on multiple domains, including unseen and diversified domains. We offer two language pairs: English-German (En→De) and English-Japanese (En→Ja), with different test sets focusing on on"
2020.wmt-1.4,W19-5303,1,0.901081,"e 5th Conference on Machine Translation (WMT), pages 76–91 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics et al., 2017), to continuously fine-tune on datasets increasingly similar to the target domain (Sajjad et al., 2017), or to dynamically change the balance of data towards the target domain (van der Wees et al., 2017). Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predomin"
2020.wmt-1.4,W18-6319,0,0.0156248,"or this particular model. In future work following this methodology, we recommend that multiple MT models be used. 4 https://cloud.google. com/natural-language/docs/ analyzing-sentiment 5 https://github.com/carpedm20/emoji/) 2 www.kaggle.com/c/ jigsaw-toxic-comment-classification- challenge 78 3.5 5. Presence of idioms: we checked if the source contains idiomatic expressions, using an inhouse list of idioms built from various sources, and labelled those cases as positive. Evaluation protocol Automatic evaluation: We first computed BLEU (Papineni et al., 2002) for each system using SacreBLEU (Post, 2018). For all language pairs except En→Ja, we used the original reference and SacreBLEU with the default options. In the case of En→Ja, we used the reference tokenized with KyTea and the option --tokenize none. We note that the automatic labelling using our various pre-processing techniques may have introduced errors, but we believe that basing the selection on such heuristics will still lead to higher chances of selecting very challenging source segments than arbitrarily sampling the data. Human evaluation: The system outputs were evaluated by professional translators. The translators were presen"
2020.wmt-1.4,E17-2045,0,0.0383395,"o domain shift assume the existence of significant amounts of parallel data in both the source and target domain. In this scenario, a common approach is to first train an MT system on a (generic) source domain and then to fine-tune it on a (specific) target domain (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Servan et al., 2016; Chu 76 Proceedings of the 5th Conference on Machine Translation (WMT), pages 76–91 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics et al., 2017), to continuously fine-tune on datasets increasingly similar to the target domain (Sajjad et al., 2017), or to dynamically change the balance of data towards the target domain (van der Wees et al., 2017). Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of mono"
2020.wmt-1.4,2021.ccl-1.108,0,0.105338,"Missing"
2020.wmt-1.4,P16-1162,0,0.0120732,"ns at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively underexplored scenario, where examples in the target domain are either unavailable or relatively few. Other aspects of robustness are robustness to adversarial examples or noisy inputs. The fragility of neural MT models has been previously"
2020.wmt-1.4,2015.iwslt-evaluation.11,0,0.64331,"ims at testing MT systems’ robustness towards domain diversity. Specifically, this year’s task aims to evaluate a general MT system’s performance in the following two scenarios: 2 Related Work Domain mismatch is a key challenge in machine translation (Koehn and Knowles, 2017). Most approaches for improving robustness of MT systems to domain shift assume the existence of significant amounts of parallel data in both the source and target domain. In this scenario, a common approach is to first train an MT system on a (generic) source domain and then to fine-tune it on a (specific) target domain (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Servan et al., 2016; Chu 76 Proceedings of the 5th Conference on Machine Translation (WMT), pages 76–91 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics et al., 2017), to continuously fine-tune on datasets increasingly similar to the target domain (Sajjad et al., 2017), or to dynamically change the balance of data towards the target domain (van der Wees et al., 2017). Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approac"
2020.wmt-1.4,N19-1314,1,0.844424,"hope that this task leads to more efforts from the community in building robust MT models. Introduction In recent years, Machine Translation (MT) systems have seen great progress, with neural models becoming the de-facto methods and even approaching human quality in news domain (Hassan et al., 2018). However, like other deep learning models, neural machine translation (NMT) models are found to be sensitive to synthetic and natural noise in input, distributional shift, and adversarial examples (Koehn and Knowles, 2017; Belinkov and Bisk, 2018; Durrani et al., 2019; Anastasopoulos et al., 2019; Michel et al., 2019). From an application perspective, MT systems need to deal with non-standard, noisy text of the kind which is ubiquitous on social media and the internet, yet has different distributional signatures from corpora in common benchmark datasets. Following the first shared task on Machine Translation (MT) Robustness, we now propose a second edition, which aims at testing MT systems’ robustness towards domain diversity. Specifically, this year’s task aims to evaluate a general MT system’s performance in the following two scenarios: 2 Related Work Domain mismatch is a key challenge in machine transla"
2020.wmt-1.4,D18-1050,1,0.940705,"tation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively underexplored scenario, where examples in the target domain are either unavailable or relatively few. Other aspects of robustness are robustness to adversarial examples or noisy inputs. The fragility of neural MT models has been previously demonstrated in various settings (Belinkov and Bisk, 2018; Heigold et al., 2017; Anastasopoulos et al., 2019; Lee et al., 2018). Michel and Neubig (2018) proposed a new dataset (MTNT) to test MT models for robustness to the types of noise encountered in the Internet. The previous iteration of the shared task focused on robustness of MT systems to such noise (Li et al., 2019). We refer to that report for more details. 3 participants to explore novel training and modeling approaches so that models have more robust performance at test time on multiple domains, including unseen and diversified domains. We offer two language pairs: English-German (En→De) and English-Japanese (En→Ja), with different test sets focusing on one or both these language p"
2020.wmt-1.4,2020.lrec-1.517,1,0.814412,"comments from the social media website reddit.com were scraped, filtered for noisy comments and translated by professional translators. This year, data was collected for two translation directions: English→Japanese and Japanese→English. For English, comments were collected from the /r/all feed, which encompasses all communities, and filtered for English. Since Japanese is a minority language on Reddit, comments were scraped from a selection of japanese-speaking communities, detailed in Michel and Neubig (2018). Common Voices Test Set (set3): This data was obtained from from the CoVoST corpus (Wang et al., 2020). CoVoST is derived from Common Voice (Ardila et al., 2020), a crowdsourced speech recognition corpus with an open CC0 license. Transcripts were sent to professional translators and the quality of translations was controlled by automatic and manual checks (Guzm´an et al., 2019). For this task, we used the German→English test set with source German sentences deduplicated. 1 Bad: translation errors are so severe that they cause the target text to be incomprehensible. This may be mainly due to major grammatical, typographical, or lexical errors, or omission of critical or important salient inform"
2020.wmt-1.4,N19-4007,1,0.808507,"as well as an analysis of catastrophic errors (Section 5.2). 5.1 General Quality Overall, the correlation between human judgments and BLEU is not strong. For En→De (set1), the Pearson’s correlation coefficient is 0.97, while for the other four tasks the coefficients are lower, with 0.78, 0.65, 0.52, 0.79 for En→De (set1), Ja→En (set2), En→Ja (set2), and De→En (set3) respectively. Automatic Evaluation The automatic evaluation (BLEU) results of the Shared Task are summarized in Table 2, where we also include the three online translation systems. We performed significance test using compare-mt (Neubig et al., 2019) where systems are considered as significantly different at p <0.05. The result of significance test is used for the automatic evaluation ranking. Overall, the unconstrained online-B system provides strong results and outperforms most systems in the five language pairs, except the De→En (set3) and En→Ja (set1). Among the participating teams, the best zeroshot systems were OPPO, which outperforms other zero-shot systems in En→De (set1), Ja→En (set2), and En→Ja (set2) tasks, and NLE, which outperforms other systems in the other two tasks. Only Naver Labs participated in the few-shot stage (NLE-f"
2020.wmt-1.4,P17-2089,0,0.0494679,"Missing"
2020.wmt-1.4,D17-1155,0,0.0161114,"Association for Computational Linguistics et al., 2017), to continuously fine-tune on datasets increasingly similar to the target domain (Sajjad et al., 2017), or to dynamically change the balance of data towards the target domain (van der Wees et al., 2017). Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at traini"
2020.wmt-1.4,D17-1147,0,0.0351203,"Missing"
2020.wmt-1.4,D16-1160,0,0.025774,"ples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively underexplored scenario, where examples in the target domain are either unavailable or relatively few. Other aspects of robustness are robustness to adversarial examples or noisy inputs. The fragility of neural MT models has been previously demonstrated in various settings (Belinkov and Bisk, 2018; Heigold et"
2021.findings-acl.438,E17-2039,0,0.046241,"Missing"
2021.findings-acl.438,2020.cl-1.1,1,0.835724,"ned models retain the same amount of linguistic information, ii) how this information is redistributed across different layers and individual neurons. To this end, we use Diagnostic Classifiers (Hupkes et al., 2018; Conneau et al., 2018), a popular framework for probing knowledge in neural models. The central idea is to extract feature representations from the network and train an auxiliary classifier to predict the property of interest. The quality of the trained classifier on the given task serves as a proxy to the quality of the extracted representations w.r.t to the understudied property (Belinkov et al., 2020). We carry layer-wise (Liu et al., 2019a) and neuron-level probing analyses (Dalvi et al., 2019a) to study the fine-tuned representations. The former probes representations from individual layers w.r.t a linguistic property and the latter finds salient neurons in the network that capture the property. Finetuning involves adjusting feature weights, therefore it is important to look at the individual neurons to uncover important details, in addition to a more holistic layer-wise view. Our layer-wise analysis shows: i) that some GLUE tasks rely on core linguistic knowledge and 4947 Findings of th"
2021.findings-acl.438,I05-5002,0,0.104706,"019b) and XLNet (Yang et al., 2019) using the base versions (13 layers and 768 dimensions). This choice of architectures leads to an interesting comparison between auto-encoder versus auto-regressive models. The models were then fine-tuned towards GLUE tasks of which we experimented with SST-2 for sentiment analysis with the Stanford sentiment treebank (Socher et al., 2013), MNLI for natural language inference (Williams et al., 2018), QNLI for Question NLI (Rajpurkar et al., 2016), RTE for recognizing textual entailment (Bentivogli et al., 2009), MRPC for Microsoft Research paraphrase corpus (Dolan and Brockett, 2005), and STS-B 4948 (a) BERT – POS (b) RoBERTa – POS (c) XLNet – POS (d) BERT – Chunking (e) RoBERTa – Chunking (f) XLNet – Chunking Figure 1: Layer-wise Probing Performance. Baseline refers to the performance of the pre-trained models without any finetuning. for the semantic textual similarity benchmark (Cer et al., 2017). All the models were fine-tuned with the identical settings and we did 3 independent runs. work as we fine-tune it towards downstream tasks. Figure 1 shows results for POS and Chunking tasks.1 We found varying observations across different GLUE tasks. Linguistic Properties: We"
2021.findings-acl.438,I17-1001,1,0.895932,"Missing"
2021.findings-acl.438,S17-2001,0,0.0307445,"anford sentiment treebank (Socher et al., 2013), MNLI for natural language inference (Williams et al., 2018), QNLI for Question NLI (Rajpurkar et al., 2016), RTE for recognizing textual entailment (Bentivogli et al., 2009), MRPC for Microsoft Research paraphrase corpus (Dolan and Brockett, 2005), and STS-B 4948 (a) BERT – POS (b) RoBERTa – POS (c) XLNet – POS (d) BERT – Chunking (e) RoBERTa – Chunking (f) XLNet – Chunking Figure 1: Layer-wise Probing Performance. Baseline refers to the performance of the pre-trained models without any finetuning. for the semantic textual similarity benchmark (Cer et al., 2017). All the models were fine-tuned with the identical settings and we did 3 independent runs. work as we fine-tune it towards downstream tasks. Figure 1 shows results for POS and Chunking tasks.1 We found varying observations across different GLUE tasks. Linguistic Properties: We evaluated our method on 3 linguistic tasks: POS tagging using the Penn TreeBank (Marcus et al., 1993), syntactic chunking using CoNLL 2000 shared task dataset (Tjong Kim Sang and Buchholz, 2000), and semantic tagging using the Parallel Meaning Bank data (Abzianidze et al., 2017). We used standard splits for training, de"
2021.findings-acl.438,2020.emnlp-main.395,1,0.83941,"the neuron-analysis. We use the Linguistic Correlation Analysis as described in Dalvi et al. (2019a), to generate a neuron ranking with respect to the understudied linguistic property: Given the trained classifier θ ∈ RD×T , the algorithm extracts a ranking of the D neurons in the model M based on weight distribution. The elastic-net regularization (Zou and Hastie, 2005) – a combination of λ1 kθk1 and λ2 kθk22 is used to strike a balance between identifying focused (L1) versus distributed (L2) neurons. The weights for the regularization terms are tuned using a grid-search algorithm. Following Durrani et al. (2020), we extract salient neurons for a linguistic property by iteratively choosing the top N neurons from the ranked list and retrain the classifier using these neurons, until the classifier obtains an accuracy close (within a specified threshold δ) to the Oracle – accuracy of the classifier trained using all the features in the network. 3 Experimental Setup Pre-trained Neural Language Models: We experimented with 3 transformer models: BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019b) and XLNet (Yang et al., 2019) using the base versions (13 layers and 768 dimensions). This choice of archite"
2021.findings-acl.438,Q16-1037,0,0.0340836,"making them ubiquitous for transfer learning towards downstream NLP problems such as Natural Language Understanding tasks e.g. GLUE (Wang et al., 2018). The general idea is to pretrain representations on large scale unlabeled data and adapt these towards a downstream task using supervision. Descriptive methods in neural interpretability investigate what knowledge is learned within the representations through relevant extrinsic phenomenon varying from word morphology (Vylomova et al., 2016; Belinkov et al., 2017a; Dalvi et al., 2017) to high level concepts such as structure (Shi et al., 2016; Linzen et al., 2016) and semantics (Qian et al., 2016; Belinkov et al., 2017b) or more generic properties such as sentence length (Adi et al., 2016; Bau et al., 2019). These studies are carried towards analyzing representations from pre-trained models. However, it is important to investigate how this learned knowledge evolves as the models are adapted towards a specific task from the more generic task of language modeling (Peters et al., 2018) that they are primarily trained on. In this work, we analyze representations of 3 popular pre-trained models (BERT, RoBERTa and XLnet) with respect to morpho-syntactic and"
2021.findings-acl.438,N19-1112,0,0.0924138,"ic information, ii) how this information is redistributed across different layers and individual neurons. To this end, we use Diagnostic Classifiers (Hupkes et al., 2018; Conneau et al., 2018), a popular framework for probing knowledge in neural models. The central idea is to extract feature representations from the network and train an auxiliary classifier to predict the property of interest. The quality of the trained classifier on the given task serves as a proxy to the quality of the extracted representations w.r.t to the understudied property (Belinkov et al., 2020). We carry layer-wise (Liu et al., 2019a) and neuron-level probing analyses (Dalvi et al., 2019a) to study the fine-tuned representations. The former probes representations from individual layers w.r.t a linguistic property and the latter finds salient neurons in the network that capture the property. Finetuning involves adjusting feature weights, therefore it is important to look at the individual neurons to uncover important details, in addition to a more holistic layer-wise view. Our layer-wise analysis shows: i) that some GLUE tasks rely on core linguistic knowledge and 4947 Findings of the Association for Computational Linguis"
2021.findings-acl.438,J93-2004,0,0.0794756,"king (e) RoBERTa – Chunking (f) XLNet – Chunking Figure 1: Layer-wise Probing Performance. Baseline refers to the performance of the pre-trained models without any finetuning. for the semantic textual similarity benchmark (Cer et al., 2017). All the models were fine-tuned with the identical settings and we did 3 independent runs. work as we fine-tune it towards downstream tasks. Figure 1 shows results for POS and Chunking tasks.1 We found varying observations across different GLUE tasks. Linguistic Properties: We evaluated our method on 3 linguistic tasks: POS tagging using the Penn TreeBank (Marcus et al., 1993), syntactic chunking using CoNLL 2000 shared task dataset (Tjong Kim Sang and Buchholz, 2000), and semantic tagging using the Parallel Meaning Bank data (Abzianidze et al., 2017). We used standard splits for training, development and test data. Comparing GLUE tasks: We found that linguistic phenomena are more important for certain downstream tasks, for example STS, RTE and MRPC where they are preserved in the higher layers post fine-tuning, as opposed to others, for example SST, QNLI and MNLI where they are forgotten in the higher layers. It would be interesting to study this further by connec"
2021.findings-acl.438,2020.blackboxnlp-1.4,0,0.0215155,"layers, reinforcing our layer-wise results, ii) that linguistic information becomes less distributed and less redundant in the network post fine-tuning. Finally, we show how our analysis entails findings in layer pruning. Dropping higher layers of the models maintains comparable performance to finetuning the full network, with linguistic information regressed to the lower layers. Conversely, pruning the lower layers (which hold the core linguistic information) leads to substantial degradation in performance. In comparison to the related work done in this direction, our findings resonate with Merchant et al. (2020) who found that fine-tuning primarily affects top layers and does not lead to “catastrophic forgetting of linguistic phenomena” in BERT. However, we found that other models like RoBERTa and XLNet, which they did not study, see a substantial drop in accuracy even at the lower layers and start forgetting linguistic knowledge much earlier in the network. In contrast to Mosbach et al. (2020), we study core-linguistic phenomena whereas their study is based on sentence level probing tasks. Differently from both, we carry out a fine-grained neuron analysis which sheds light on how neurons are distrib"
2021.findings-acl.438,P16-1140,0,0.0655213,"Missing"
2021.findings-acl.438,D16-1264,0,0.025744,"al Setup Pre-trained Neural Language Models: We experimented with 3 transformer models: BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019b) and XLNet (Yang et al., 2019) using the base versions (13 layers and 768 dimensions). This choice of architectures leads to an interesting comparison between auto-encoder versus auto-regressive models. The models were then fine-tuned towards GLUE tasks of which we experimented with SST-2 for sentiment analysis with the Stanford sentiment treebank (Socher et al., 2013), MNLI for natural language inference (Williams et al., 2018), QNLI for Question NLI (Rajpurkar et al., 2016), RTE for recognizing textual entailment (Bentivogli et al., 2009), MRPC for Microsoft Research paraphrase corpus (Dolan and Brockett, 2005), and STS-B 4948 (a) BERT – POS (b) RoBERTa – POS (c) XLNet – POS (d) BERT – Chunking (e) RoBERTa – Chunking (f) XLNet – Chunking Figure 1: Layer-wise Probing Performance. Baseline refers to the performance of the pre-trained models without any finetuning. for the semantic textual similarity benchmark (Cer et al., 2017). All the models were fine-tuned with the identical settings and we did 3 independent runs. work as we fine-tune it towards downstream task"
2021.findings-acl.438,D16-1159,0,0.0204399,"guistic knowledge, making them ubiquitous for transfer learning towards downstream NLP problems such as Natural Language Understanding tasks e.g. GLUE (Wang et al., 2018). The general idea is to pretrain representations on large scale unlabeled data and adapt these towards a downstream task using supervision. Descriptive methods in neural interpretability investigate what knowledge is learned within the representations through relevant extrinsic phenomenon varying from word morphology (Vylomova et al., 2016; Belinkov et al., 2017a; Dalvi et al., 2017) to high level concepts such as structure (Shi et al., 2016; Linzen et al., 2016) and semantics (Qian et al., 2016; Belinkov et al., 2017b) or more generic properties such as sentence length (Adi et al., 2016; Bau et al., 2019). These studies are carried towards analyzing representations from pre-trained models. However, it is important to investigate how this learned knowledge evolves as the models are adapted towards a specific task from the more generic task of language modeling (Peters et al., 2018) that they are primarily trained on. In this work, we analyze representations of 3 popular pre-trained models (BERT, RoBERTa and XLnet) with respect to"
2021.findings-acl.438,2020.acl-main.422,1,0.840255,"c properties in XLNet to be localized to the lower layers4 and fewer neurons and mutually exclusive as compared to BERT where neurons are highly polysemous5 and therefore more redundant. Their finding helps us explain why XLNet forgets linguistic information that is unimportant to the downstream task more catastrophically. 5 Network Pruning Our layer and neuron-wise analyses showed that core linguistic knowledge is redundant and distributed in the large pre-trained models. But as they are fine-tuned towards a down-stream task, 3 See Appendix for all tasks and linguistic properties. Similarly (Wu et al., 2020) reported lower and middle layers of XLNet to have the most salient features. 5 attend to multiple linguistic phenomenon 4950 4 (a) BERT – SEM (b) RoBERTa – SEM (c) XLNet – SEM (d) BERT – Chunking (e) RoBERTa – Chunking (f) XLNet – Chunking Figure 2: Distribution of top neurons across layers it is relocated and localized to lower layers, with higher layers focusing on the task-specific information. In this section, we show that our findings explain patterns in layer pruning. We question How important is the linguistic knowledge for these downstream NLP tasks? Following Sajjad et al. (2020) we"
2021.findings-emnlp.56,2021.nlp4if-1.9,1,0.705503,"tions. Some of the larger datasets include the Liar, Liar dataset of 12.8K claims from PolitiFact (Wang, 2017), the ClaimsKG dataset and system (Tchechmedjiev et al., 2019) of 28K claims from eight factchecking organizations, the MultiFC dataset of 38K claims from 26 fact-checking organizations (Augenstein et al., 2019), and the 10K claims Truth of Various Shades dataset (Rashkin et al., 2017). There have been also datasets for other languages, • We develop a large manually annotated created in a similar fashion, e.g., for Arabic (Baly dataset of 16K tweets related to the COVID- et al., 2018; Alhindi et al., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers, and the society. mors (Derczynski et al., 2017; Gorrell et al., 2019), propaganda detection in news articles and memes • We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino"
2021.findings-emnlp.56,N19-1216,1,0.795082,"tweets in Arabic, Bulgarian, Dutch, and English, and we are making it freely available to the research community. We further reported a number of evaluation results for all languages using various transformer architectures. Moreover, we performed advanced experiments, including multilingual training, modeling the Twitter context, the use of propagandistic language, and whether the user is likely to be a bot, as well as multitask learning. In future work, we plan to explore multimodality and explainability (Yu et al., 2021). We further want to model the task as a multitask ordinal regression (Baly et al., 2019), as Q2–Q5 are defined on an ordinal scale. Moreover, we would like to put the data and the system in some practical use; in fact, we have already used them to analyze disinformation about COVID-19 in Bulgaria (Nakov et al., 2021a) and Qatar (Nakov et al., 2021b). Finally, the data will be used in a shared task at the CLEF2022 CheckThat! lab; part of it was used for the NLP4IF-2021 shared task (Shaar et al., 2021a). Acknowledgments We thank Akter Fatema, Al-Awthan Ahmed, AlDobashi Hussein, El Messelmani Jana, Fayoumi 6.3 Multitask Learning Sereen, Mohamed Esraa, Ragab Saleh, and Shurafa For th"
2021.findings-emnlp.56,N18-2004,1,0.90491,"Missing"
2021.findings-emnlp.56,2020.acl-main.747,0,0.0346481,"a URL, and the factuality of the website it points to.4 Models Large-scale pretrained Transformer models have achieved state-of-the-art performance for several NLP tasks. We experimented with several such models to evaluate their efficacy under various training scenarios such as, binary vs. multiclass classification, multilingual setup, etc. We used BERT (Devlin et al., 2019) and RoBERTa for English, AraBERT (Antoun et al., 2020) for Arabic, and BERTje (de Vries et al., 2019) for Dutch. We further used multilingual transformers such as (Liu et al., 2019), multilingual BERT (mBERT) and XLM-r (Conneau et al., 2020). Finally, we used static embeddings from FastText (Joulin et al., 2017). 616 4 From http://mediabiasfactcheck.com English Q. Cls. Arabic Maj. FT BT RT Bulgarian Maj. FT ArBT XLM-r Dutch Maj. FT mBT XLM-r Maj. FT BTje XLM-r Binary (Coarse-grained) Q1 Q2 Q3 Q4 Q5 Q6 Q7 2 2 2 2 2 2 2 Avg. 48.7 91.6 96.3 66.7 67.7 86.7 78.3 77.7 89.0 69.3 96.3 83.8 92.1 80.6 76.5 92.1 96.4 85.6 80.6 88.9 85.5 78.6 92.7 96.9 89.0 84.4 90.5 86.1 76.6 84.1 86.5 88.3 83.8 84.0 96.0 90.3 65.9 88.9 77.4 84.2 83.1 96.3 89.0 66.7 89.8 77.4 58.3 95.0 96.5 86.8 70.5 83.2 80.1 84.0 94.7 96.0 87.7 80.5 84.5 81.6 87.6 95.0 96"
2021.findings-emnlp.56,2020.semeval-1.186,1,0.850189,"., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers, and the society. mors (Derczynski et al., 2017; Gorrell et al., 2019), propaganda detection in news articles and memes • We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino et al., 2020a; Dimitrov et al., sentations (such as BERT), when using mul- 2021a,b), fact-checking in community question answering forums (Mihaylova et al., 2019), the CLEF titask learning, cross-language learning, and when modeling the social context of the tweet, CheckThat! lab on identification and verification of claims (Nakov et al., 2018; Elsayed et al., 2019; as well as the propagandistic nature of the Barrón-Cedeño et al., 2020; Shaar et al., 2020; language used. Nakov et al., 2021c; Shaar et al., 2021b,c), or (b) us• We make our data and code freely available.1 ing crowdsourcing, e.g., the FEVER"
2021.findings-emnlp.56,2020.acl-demos.32,1,0.926725,"., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers, and the society. mors (Derczynski et al., 2017; Gorrell et al., 2019), propaganda detection in news articles and memes • We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino et al., 2020a; Dimitrov et al., sentations (such as BERT), when using mul- 2021a,b), fact-checking in community question answering forums (Mihaylova et al., 2019), the CLEF titask learning, cross-language learning, and when modeling the social context of the tweet, CheckThat! lab on identification and verification of claims (Nakov et al., 2018; Elsayed et al., 2019; as well as the propagandistic nature of the Barrón-Cedeño et al., 2020; Shaar et al., 2020; language used. Nakov et al., 2021c; Shaar et al., 2021b,c), or (b) us• We make our data and code freely available.1 ing crowdsourcing, e.g., the FEVER"
2021.findings-emnlp.56,S19-2147,0,0.0285024,"s, • We develop a large manually annotated created in a similar fashion, e.g., for Arabic (Baly dataset of 16K tweets related to the COVID- et al., 2018; Alhindi et al., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers, and the society. mors (Derczynski et al., 2017; Gorrell et al., 2019), propaganda detection in news articles and memes • We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino et al., 2020a; Dimitrov et al., sentations (such as BERT), when using mul- 2021a,b), fact-checking in community question answering forums (Mihaylova et al., 2019), the CLEF titask learning, cross-language learning, and when modeling the social context of the tweet, CheckThat! lab on identification and verification of claims (Nakov et al., 2018; Elsayed et al., 2019; as well as the propagandistic nature of the Barrón-Cedeño et al., 2020; Shaar"
2021.findings-emnlp.56,2021.wanlp-1.9,0,0.0367297,"llected tweets by specifying a target language (English, Arabic, Bulgarian, or Dutch), a set of COVID-19 related keywords, as shown in Figure 2, and different time frames: from January 2020 till March 2021. We collected original tweets (no retweets or replies), we removed duplicates using a similarity-based approach (Alam et al., 2021b), and we filtered out tweets with less than five words. Finally, we selected the most frequently liked and retweeted tweets for annotation. COVID-19 Research There are a number of COVID-19 Twitter datasets: some unlabeled (Chen et al., 2020; Banda et al., 2021; Haouari et al., 2021), some automatically labeled with location information (Abdul-Mageed et al., 2021; Qazi et al., 2020), some labeled using distant supervision (Cinelli et al., 2020; Zhou et al., 2020), and some manually annotated (Song et al., 2020; Vidgen et al., 2020; Shahi and Nandini, 2020; Pulido et al., 2020; Dharawat et al., 2020). There is also work on credibility (Cinelli et al., 2020; Pulido et al., 2020; Zhou et al., 2020), racial prejudices and fear (Medford et al., 2020; Vidgen et al., 2020), as well as situational information, e.g., caution and advice (Li et al., 2020), as well as on detecting me"
2021.findings-emnlp.56,2020.nlpcovid19-2.11,0,0.043054,"Abdul-Mageed et al., 2021; Qazi et al., 2020), some labeled using distant supervision (Cinelli et al., 2020; Zhou et al., 2020), and some manually annotated (Song et al., 2020; Vidgen et al., 2020; Shahi and Nandini, 2020; Pulido et al., 2020; Dharawat et al., 2020). There is also work on credibility (Cinelli et al., 2020; Pulido et al., 2020; Zhou et al., 2020), racial prejudices and fear (Medford et al., 2020; Vidgen et al., 2020), as well as situational information, e.g., caution and advice (Li et al., 2020), as well as on detecting mentions and stance with respect to known misconceptions (Hossain et al., 2020). The closest work to ours is that of Song et al. (2020), who collected false and misleading claims about COVID-19 from IFCN Poynter, and annotated them as (1) Public authority, (2) Community spread and impact, (3) Medical advice, selftreatments, and virus effects, (4) Prominent actors, (5) Conspiracies, (6) Virus transmission, (7) Virus Figure 2: The keywords used to collect the tweets. origins and properties, (8) Public reaction, and (9) Vaccines, medical treatments, and tests. These categories partially overlap with ours, but account 3.2 Annotation Task for less perspectives. Moreover, we c"
2021.findings-emnlp.56,N18-5006,1,0.802211,"tweets (they used claims from news, speeches, political debates, community question answering fora, or were just made up by human annotators; RumourEval is a notable exception), targeted factuality only (we cover a number of other issues), were limited to a single language (typically English; except for CLEF), and did not focus on COVID-19. Check-Worthiness Estimation Another relevant research line is on detecting check-worthy claims in political debates using manual annotations (Hassan et al., 2015) or by observing the selection of fact-checkers (Gencheva et al., 2017; Patwari et al., 2017; Jaradat et al., 2018; Vasileva et al., 2019). 3 3.1 Dataset Data Collection We collected tweets by specifying a target language (English, Arabic, Bulgarian, or Dutch), a set of COVID-19 related keywords, as shown in Figure 2, and different time frames: from January 2020 till March 2021. We collected original tweets (no retweets or replies), we removed duplicates using a similarity-based approach (Alam et al., 2021b), and we filtered out tweets with less than five words. Finally, we selected the most frequently liked and retweeted tweets for annotation. COVID-19 Research There are a number of COVID-19 Twitter data"
2021.findings-emnlp.56,E17-2068,0,0.0257945,"le pretrained Transformer models have achieved state-of-the-art performance for several NLP tasks. We experimented with several such models to evaluate their efficacy under various training scenarios such as, binary vs. multiclass classification, multilingual setup, etc. We used BERT (Devlin et al., 2019) and RoBERTa for English, AraBERT (Antoun et al., 2020) for Arabic, and BERTje (de Vries et al., 2019) for Dutch. We further used multilingual transformers such as (Liu et al., 2019), multilingual BERT (mBERT) and XLM-r (Conneau et al., 2020). Finally, we used static embeddings from FastText (Joulin et al., 2017). 616 4 From http://mediabiasfactcheck.com English Q. Cls. Arabic Maj. FT BT RT Bulgarian Maj. FT ArBT XLM-r Dutch Maj. FT mBT XLM-r Maj. FT BTje XLM-r Binary (Coarse-grained) Q1 Q2 Q3 Q4 Q5 Q6 Q7 2 2 2 2 2 2 2 Avg. 48.7 91.6 96.3 66.7 67.7 86.7 78.3 77.7 89.0 69.3 96.3 83.8 92.1 80.6 76.5 92.1 96.4 85.6 80.6 88.9 85.5 78.6 92.7 96.9 89.0 84.4 90.5 86.1 76.6 84.1 86.5 88.3 83.8 84.0 96.0 90.3 65.9 88.9 77.4 84.2 83.1 96.3 89.0 66.7 89.8 77.4 58.3 95.0 96.5 86.8 70.5 83.2 80.1 84.0 94.7 96.0 87.7 80.5 84.5 81.6 87.6 95.0 96.5 88.4 82.9 85.1 81.7 36.5 64.9 62.3 63.9 44.4 84.7 65.6 75.4 75.1 76.9"
2021.findings-emnlp.56,2020.emnlp-demos.2,0,0.0153692,"80.2 69.2 68.3 Finally, we should note the strong performance Avg. 73.3 73.1 60.7 59.8 71.4 71.5 55.3 54.9 of context-free models such as FastText. We believe that it is suitable for the noisy text of Table 6: Multilingual experiments using mBERT. tweets due to its ability to model not only words Shown are results for monolingual vs. multilingual models (weighted F1 ). Mul is trained on the combined but also character n-grams. In future work, we English, Arabic, Bulgarian, and Dutch data. plan to try transformers specifically trained on tweets and/or on COVID-19 related data such as BERTweet (Nguyen et al., 2020) and COVID5 Twitter-BERT (Müller et al., 2020). We also tried XLM-r, but it performed worse. 618 6.2 Twitter/Propagandistic/Botometer We conducted experiments with Twitter, propaganda, and botness features alongside the posteriors from the BERT classifier, which we combined using XGBoost (Chen and Guestrin, 2016). The results are shown in Table 7. We can see that many of the combinations yielded improvements, with botness being the most useful, followed by propaganda, and finally by the Twitter object features. Binary (Coarse-grained) Q. Cls BERT B+TF B+Prop B+Bot B+All Q1 Q2 Q3 Q4 Q5 Q6 Q7 2"
2021.findings-emnlp.56,D17-1317,0,0.0286494,"onversations with a Ministry of Public Health. Our contributions can be summarized as follows: 2 Related Work Fact-Checking Research on fact-checking claims is largely based on datasets mined from major fact-checking organizations. Some of the larger datasets include the Liar, Liar dataset of 12.8K claims from PolitiFact (Wang, 2017), the ClaimsKG dataset and system (Tchechmedjiev et al., 2019) of 28K claims from eight factchecking organizations, the MultiFC dataset of 38K claims from 26 fact-checking organizations (Augenstein et al., 2019), and the 10K claims Truth of Various Shades dataset (Rashkin et al., 2017). There have been also datasets for other languages, • We develop a large manually annotated created in a similar fashion, e.g., for Arabic (Baly dataset of 16K tweets related to the COVID- et al., 2018; Alhindi et al., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers"
2021.findings-emnlp.56,2021.nlp4if-1.12,1,0.887854,"We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino et al., 2020a; Dimitrov et al., sentations (such as BERT), when using mul- 2021a,b), fact-checking in community question answering forums (Mihaylova et al., 2019), the CLEF titask learning, cross-language learning, and when modeling the social context of the tweet, CheckThat! lab on identification and verification of claims (Nakov et al., 2018; Elsayed et al., 2019; as well as the propagandistic nature of the Barrón-Cedeño et al., 2020; Shaar et al., 2020; language used. Nakov et al., 2021c; Shaar et al., 2021b,c), or (b) us• We make our data and code freely available.1 ing crowdsourcing, e.g., the FEVER task on fact ex1 traction and verification, focusing on claims about https://github.com/firojalam/ COVID-19-disinformation Wikipedia content (Thorne et al., 2018, 2019). 612 Unlike our work, the above datasets did not focus on tweets (they used claims from news, speeches, political debates, community question answering fora, or were just made up by human annotators; RumourEval is a notable exception), targeted factuality only (we cover a number of other issues), were limited to a single language (t"
2021.naacl-tutorials.2,2020.acl-tutorials.1,0,0.0442215,"behavioural studies to analyze models (Gulordava et al., 2018; Linzen et al., 2016; Marvin and Linzen, 2018). Moreover, a number of studies cover the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018a; Lundberg and Lee, 2017; Tran et al., 2018). The topic of interpretation of neural models has gained a lot of attention in a last couple of years. For example, it has been added as a regular track in major *CL conferences. There is an annual workshop, BlackboxNLP, dedicated for this purpose. The ACL 2020 and EMNLP 20201 featured tutorials on the topic (Belinkov et al., 2020). The ACL tutorial focused on two subareas of interpretation which are the representation analysis and the behavioral studies. The EMNLP tutorial is solely focused on behavioral studies i.e. assess a model’s behavior using constructed examples. Both of these tutorials serves as a great starting point for the new 1 https://2020.emnlp.org/tutorials 5 Proceedings of NAACL-HLT 2021: Tutorials, pages 5–10 June 6–11, 2021. ©2021 Association for Computational Linguistics sensitive attributes like gender, race or politeness (Bau et al., 2019; Vig et al., 2020). These recent works are not only enabling"
2021.naacl-tutorials.2,2020.aacl-main.46,0,0.0291845,"n of deep models has also been featured in several tech blogs including MIT News. Hassan co-organized BlackboxNLP 2020, and the WMT 2019/2020 machine translation robustness task. He served as an area chair for the analysis and interpretability, NLP Application, and machine translation tracks at various *CL conferences. In addition, Hassan has been regularly teaching courses on deep learning internationally at various spring and summer schools. Reading List • In order to get an overview of the interpretation field, trainees may look at the following survey papers: Belinkov and Glass (2019) and Danilevsky et al. (2020). • Fine-grained analysis and its Applications: Bau et al. (2019); Dalvi et al. (2019a); Mu and Andreas (2020b); Suau et al. (2020) etc. • Causation analysis: Lundberg and Lee (2017) provides an overview of various methods introduced in literature. For more details, see the following papers: Voita et al. (2020); Sundararajan et al. (2017); Dhamdhere et al. (2018b); Ribeiro et al. (2016); Janizek et al. (2020) In addition to the above list, interested trainees may look at the papers mentioned in Section 2. 6 Narine Kokhlikyan, Research Scientist, Facebook AI Instructor Information (Alphabetic o"
2021.naacl-tutorials.2,N19-1423,0,0.0276932,"age Modelling and Explainability in Deep Neural Networks. He also spends his time converting research into practical applications, with a focus on scalable web applications. Fahim also spends some time every year mentoring and teaching Deep Learning at Fall and Summer schools. 5. Discussion: The last part will discuss the overall challenges that the current work faces and suggest future directions. (10 minutes) 4 Prerequisites We assume basic knowledge of the deep learning and familiarity with the LSTM-based and transformer-based pre-trained models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). Additionally, some familiarity with natural language processing tasks such as, named entity tagging, natural language inference, etc. would be useful but not mandatory. We do not expect participants to have familiarity with the research on the interpretation and analysis of deep models. Familiarity with Python, Pytorch and Transformers library (Wolf et al., 2019) would be useful to understand the practical part. 5 Hassan Sajjad, Senior Research Scientist, Qatar Computing Research Institute, Qatar Email: hsajjad@hbku.edu.qa Website: https://hsajjad.github.io Hassan Sajjad is a Senior Research"
2021.naacl-tutorials.2,N19-1002,0,0.0117639,"ed methods lie, and provide ideas and recommendations around future directions. Description The tutorial is divided into two main parts: i) finegrained interpretation, and ii) causation analysis. The first part of the tutorial covers methods that align neurons to human interpretable concepts or study the most salient neurons in the network. We cluster these methods into four groups i) Visualization Methods (Karpathy et al., 2015; Li et al., 2016a), ii) Corpus Selection (Kádár et al., 2017; Poerner et al., 2018; Na et al., 2019; Mu and Andreas, 2020b), iii) Neuron Probing (Dalvi et al., 2019a; Lakretz et al., 2019; Valipour et al., 2019; Durrani et al., 2020) and iv) Unsupervised Methods (Bau et al., 2019; Torroba Hennigen et al., 2020; Wu et al., 2020; Michael et al., 2020). We will discuss evaluation methods that are used to measure the effectiveness of an interpretation method, such as accuracy, control tasks (Hewitt and Liang, 2019) and ablation studies (Li et al., 2016b; Lillian et al., 2018; Dalvi et al., 2019a; Lakretz et al., 2019). Moreover, we will cover various applications of these methods that go beyond interpretation such as efficient transfer learning (Dalvi et al., 2020), controlling sy"
2021.naacl-tutorials.2,2020.emnlp-main.395,1,0.756611,"ndations around future directions. Description The tutorial is divided into two main parts: i) finegrained interpretation, and ii) causation analysis. The first part of the tutorial covers methods that align neurons to human interpretable concepts or study the most salient neurons in the network. We cluster these methods into four groups i) Visualization Methods (Karpathy et al., 2015; Li et al., 2016a), ii) Corpus Selection (Kádár et al., 2017; Poerner et al., 2018; Na et al., 2019; Mu and Andreas, 2020b), iii) Neuron Probing (Dalvi et al., 2019a; Lakretz et al., 2019; Valipour et al., 2019; Durrani et al., 2020) and iv) Unsupervised Methods (Bau et al., 2019; Torroba Hennigen et al., 2020; Wu et al., 2020; Michael et al., 2020). We will discuss evaluation methods that are used to measure the effectiveness of an interpretation method, such as accuracy, control tasks (Hewitt and Liang, 2019) and ablation studies (Li et al., 2016b; Lillian et al., 2018; Dalvi et al., 2019a; Lakretz et al., 2019). Moreover, we will cover various applications of these methods that go beyond interpretation such as efficient transfer learning (Dalvi et al., 2020), controlling system’s behavior (Bau et al., 2019; Suau et al."
2021.naacl-tutorials.2,2021.naacl-main.308,0,0.164851,"ver important research questions such as i) how is knowledge distributed across the model components? ii) what knowledge learned within the model is used for specific predictions? iii) does the inhibition of specific knowledge in the model change predictions? iv) how do different modeling and optimization choices impact the underlying knowledge? Recent work on interpreting neurons has shown that in-addition to gaining better understanding of the inner workings of neural networks, the neuronlevel interpretation has applications in model distillation (Rethmeier et al., 2020), domain adaptation (Gu et al., 2021) or efficient feature selection (Dalvi et al., 2020) e.g., by removing unimportant neurons, facilitating architecture search, and mitigating model bias by identifying neurons responsible for Deep neural networks have constantly pushed the state-of-the-art performance in natural language processing and are considered as the de facto modeling approach in solving most complex NLP tasks such as machine translation, summarization and question-answering. Despite the benefits and the usefulness of deep neural networks at-large, their opaqueness is a major cause of concern. Interpreting neural network"
2021.naacl-tutorials.2,N18-1108,0,0.0196902,"oncern. Interpreting neural networks is considered important for increasing trust in AI systems, providing additional information to decision makers, and assisting ethical decision making (Lipton, 2016). Interpretation of neural network models is a broad area of research. Significant work has analyzed network at representation-level (Belinkov et al., 2017; Conneau et al., 2018; Adi et al., 2016; Tenney et al., 2019), and at neuron-level (Bau et al., 2020; Mu and Andreas, 2020a; Bau et al., 2019; Dalvi et al., 2019a). Others have experimented with various behavioural studies to analyze models (Gulordava et al., 2018; Linzen et al., 2016; Marvin and Linzen, 2018). Moreover, a number of studies cover the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018a; Lundberg and Lee, 2017; Tran et al., 2018). The topic of interpretation of neural models has gained a lot of attention in a last couple of years. For example, it has been added as a regular track in major *CL conferences. There is an annual workshop, BlackboxNLP, dedicated for this purpose. The ACL 2020 and EMNLP 20201 featured tutorials on the topic (Belinkov et al., 2020). The ACL tutorial focused on two subar"
2021.naacl-tutorials.2,Q16-1037,0,0.0215596,"ral networks is considered important for increasing trust in AI systems, providing additional information to decision makers, and assisting ethical decision making (Lipton, 2016). Interpretation of neural network models is a broad area of research. Significant work has analyzed network at representation-level (Belinkov et al., 2017; Conneau et al., 2018; Adi et al., 2016; Tenney et al., 2019), and at neuron-level (Bau et al., 2020; Mu and Andreas, 2020a; Bau et al., 2019; Dalvi et al., 2019a). Others have experimented with various behavioural studies to analyze models (Gulordava et al., 2018; Linzen et al., 2016; Marvin and Linzen, 2018). Moreover, a number of studies cover the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018a; Lundberg and Lee, 2017; Tran et al., 2018). The topic of interpretation of neural models has gained a lot of attention in a last couple of years. For example, it has been added as a regular track in major *CL conferences. There is an annual workshop, BlackboxNLP, dedicated for this purpose. The ACL 2020 and EMNLP 20201 featured tutorials on the topic (Belinkov et al., 2020). The ACL tutorial focused on two subareas of interpretation"
2021.naacl-tutorials.2,D19-1275,0,0.0117386,"in the network. We cluster these methods into four groups i) Visualization Methods (Karpathy et al., 2015; Li et al., 2016a), ii) Corpus Selection (Kádár et al., 2017; Poerner et al., 2018; Na et al., 2019; Mu and Andreas, 2020b), iii) Neuron Probing (Dalvi et al., 2019a; Lakretz et al., 2019; Valipour et al., 2019; Durrani et al., 2020) and iv) Unsupervised Methods (Bau et al., 2019; Torroba Hennigen et al., 2020; Wu et al., 2020; Michael et al., 2020). We will discuss evaluation methods that are used to measure the effectiveness of an interpretation method, such as accuracy, control tasks (Hewitt and Liang, 2019) and ablation studies (Li et al., 2016b; Lillian et al., 2018; Dalvi et al., 2019a; Lakretz et al., 2019). Moreover, we will cover various applications of these methods that go beyond interpretation such as efficient transfer learning (Dalvi et al., 2020), controlling system’s behavior (Bau et al., 2019; Suau et al., 2020), generating explanations (Mu and Andreas, 2020b) and domain adaptation (Gu et al., 2021). The second part, Causation Analysis, will focus on methods that seek to characterize the role of neurons and layers towards a specific prediction. More concretely, we will discuss gradi"
2021.naacl-tutorials.2,D18-1151,0,0.0121888,"dered important for increasing trust in AI systems, providing additional information to decision makers, and assisting ethical decision making (Lipton, 2016). Interpretation of neural network models is a broad area of research. Significant work has analyzed network at representation-level (Belinkov et al., 2017; Conneau et al., 2018; Adi et al., 2016; Tenney et al., 2019), and at neuron-level (Bau et al., 2020; Mu and Andreas, 2020a; Bau et al., 2019; Dalvi et al., 2019a). Others have experimented with various behavioural studies to analyze models (Gulordava et al., 2018; Linzen et al., 2016; Marvin and Linzen, 2018). Moreover, a number of studies cover the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018a; Lundberg and Lee, 2017; Tran et al., 2018). The topic of interpretation of neural models has gained a lot of attention in a last couple of years. For example, it has been added as a regular track in major *CL conferences. There is an annual workshop, BlackboxNLP, dedicated for this purpose. The ACL 2020 and EMNLP 20201 featured tutorials on the topic (Belinkov et al., 2020). The ACL tutorial focused on two subareas of interpretation which are the representat"
2021.naacl-tutorials.2,2020.emnlp-main.552,0,0.0295689,"Missing"
2021.naacl-tutorials.2,N18-1202,0,0.0548259,"ding Machine Translation, Language Modelling and Explainability in Deep Neural Networks. He also spends his time converting research into practical applications, with a focus on scalable web applications. Fahim also spends some time every year mentoring and teaching Deep Learning at Fall and Summer schools. 5. Discussion: The last part will discuss the overall challenges that the current work faces and suggest future directions. (10 minutes) 4 Prerequisites We assume basic knowledge of the deep learning and familiarity with the LSTM-based and transformer-based pre-trained models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). Additionally, some familiarity with natural language processing tasks such as, named entity tagging, natural language inference, etc. would be useful but not mandatory. We do not expect participants to have familiarity with the research on the interpretation and analysis of deep models. Familiarity with Python, Pytorch and Transformers library (Wolf et al., 2019) would be useful to understand the practical part. 5 Hassan Sajjad, Senior Research Scientist, Qatar Computing Research Institute, Qatar Email: hsajjad@hbku.edu.qa Website: https://hsajjad.github.io Has"
2021.naacl-tutorials.2,W18-5437,0,0.020493,"ial, our goal will also be to critically evaluate where the strengths and weakness of each of the presented methods lie, and provide ideas and recommendations around future directions. Description The tutorial is divided into two main parts: i) finegrained interpretation, and ii) causation analysis. The first part of the tutorial covers methods that align neurons to human interpretable concepts or study the most salient neurons in the network. We cluster these methods into four groups i) Visualization Methods (Karpathy et al., 2015; Li et al., 2016a), ii) Corpus Selection (Kádár et al., 2017; Poerner et al., 2018; Na et al., 2019; Mu and Andreas, 2020b), iii) Neuron Probing (Dalvi et al., 2019a; Lakretz et al., 2019; Valipour et al., 2019; Durrani et al., 2020) and iv) Unsupervised Methods (Bau et al., 2019; Torroba Hennigen et al., 2020; Wu et al., 2020; Michael et al., 2020). We will discuss evaluation methods that are used to measure the effectiveness of an interpretation method, such as accuracy, control tasks (Hewitt and Liang, 2019) and ablation studies (Li et al., 2016b; Lillian et al., 2018; Dalvi et al., 2019a; Lakretz et al., 2019). Moreover, we will cover various applications of these metho"
2021.naacl-tutorials.2,2020.acl-main.422,1,0.832703,"ained interpretation, and ii) causation analysis. The first part of the tutorial covers methods that align neurons to human interpretable concepts or study the most salient neurons in the network. We cluster these methods into four groups i) Visualization Methods (Karpathy et al., 2015; Li et al., 2016a), ii) Corpus Selection (Kádár et al., 2017; Poerner et al., 2018; Na et al., 2019; Mu and Andreas, 2020b), iii) Neuron Probing (Dalvi et al., 2019a; Lakretz et al., 2019; Valipour et al., 2019; Durrani et al., 2020) and iv) Unsupervised Methods (Bau et al., 2019; Torroba Hennigen et al., 2020; Wu et al., 2020; Michael et al., 2020). We will discuss evaluation methods that are used to measure the effectiveness of an interpretation method, such as accuracy, control tasks (Hewitt and Liang, 2019) and ablation studies (Li et al., 2016b; Lillian et al., 2018; Dalvi et al., 2019a; Lakretz et al., 2019). Moreover, we will cover various applications of these methods that go beyond interpretation such as efficient transfer learning (Dalvi et al., 2020), controlling system’s behavior (Bau et al., 2019; Suau et al., 2020), generating explanations (Mu and Andreas, 2020b) and domain adaptation (Gu et al., 2021"
2021.naacl-tutorials.2,P19-1452,0,0.0191797,"ost complex NLP tasks such as machine translation, summarization and question-answering. Despite the benefits and the usefulness of deep neural networks at-large, their opaqueness is a major cause of concern. Interpreting neural networks is considered important for increasing trust in AI systems, providing additional information to decision makers, and assisting ethical decision making (Lipton, 2016). Interpretation of neural network models is a broad area of research. Significant work has analyzed network at representation-level (Belinkov et al., 2017; Conneau et al., 2018; Adi et al., 2016; Tenney et al., 2019), and at neuron-level (Bau et al., 2020; Mu and Andreas, 2020a; Bau et al., 2019; Dalvi et al., 2019a). Others have experimented with various behavioural studies to analyze models (Gulordava et al., 2018; Linzen et al., 2016; Marvin and Linzen, 2018). Moreover, a number of studies cover the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018a; Lundberg and Lee, 2017; Tran et al., 2018). The topic of interpretation of neural models has gained a lot of attention in a last couple of years. For example, it has been added as a regular track in major *CL con"
2021.naacl-tutorials.2,2020.emnlp-main.15,0,0.013173,"o main parts: i) finegrained interpretation, and ii) causation analysis. The first part of the tutorial covers methods that align neurons to human interpretable concepts or study the most salient neurons in the network. We cluster these methods into four groups i) Visualization Methods (Karpathy et al., 2015; Li et al., 2016a), ii) Corpus Selection (Kádár et al., 2017; Poerner et al., 2018; Na et al., 2019; Mu and Andreas, 2020b), iii) Neuron Probing (Dalvi et al., 2019a; Lakretz et al., 2019; Valipour et al., 2019; Durrani et al., 2020) and iv) Unsupervised Methods (Bau et al., 2019; Torroba Hennigen et al., 2020; Wu et al., 2020; Michael et al., 2020). We will discuss evaluation methods that are used to measure the effectiveness of an interpretation method, such as accuracy, control tasks (Hewitt and Liang, 2019) and ablation studies (Li et al., 2016b; Lillian et al., 2018; Dalvi et al., 2019a; Lakretz et al., 2019). Moreover, we will cover various applications of these methods that go beyond interpretation such as efficient transfer learning (Dalvi et al., 2020), controlling system’s behavior (Bau et al., 2019; Suau et al., 2020), generating explanations (Mu and Andreas, 2020b) and domain adaptation"
2021.naacl-tutorials.2,D18-1503,0,0.0190094,"twork models is a broad area of research. Significant work has analyzed network at representation-level (Belinkov et al., 2017; Conneau et al., 2018; Adi et al., 2016; Tenney et al., 2019), and at neuron-level (Bau et al., 2020; Mu and Andreas, 2020a; Bau et al., 2019; Dalvi et al., 2019a). Others have experimented with various behavioural studies to analyze models (Gulordava et al., 2018; Linzen et al., 2016; Marvin and Linzen, 2018). Moreover, a number of studies cover the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018a; Lundberg and Lee, 2017; Tran et al., 2018). The topic of interpretation of neural models has gained a lot of attention in a last couple of years. For example, it has been added as a regular track in major *CL conferences. There is an annual workshop, BlackboxNLP, dedicated for this purpose. The ACL 2020 and EMNLP 20201 featured tutorials on the topic (Belinkov et al., 2020). The ACL tutorial focused on two subareas of interpretation which are the representation analysis and the behavioral studies. The EMNLP tutorial is solely focused on behavioral studies i.e. assess a model’s behavior using constructed examples. Both of these tutoria"
abdelali-etal-2014-amara,J93-1004,0,\N,Missing
abdelali-etal-2014-amara,tiedemann-2008-synchronizing,0,\N,Missing
abdelali-etal-2014-amara,J93-2003,0,\N,Missing
abdelali-etal-2014-amara,P02-1040,0,\N,Missing
abdelali-etal-2014-amara,P11-1105,0,\N,Missing
abdelali-etal-2014-amara,P13-2003,1,\N,Missing
abdelali-etal-2014-amara,P07-2045,0,\N,Missing
abdelali-etal-2014-amara,N04-1022,0,\N,Missing
abdelali-etal-2014-amara,N03-1017,0,\N,Missing
abdelali-etal-2014-amara,P12-1016,0,\N,Missing
abdelali-etal-2014-amara,2013.iwslt-papers.2,1,\N,Missing
abdelali-etal-2014-amara,tiedemann-2012-parallel,0,\N,Missing
abdelali-etal-2014-amara,W11-2123,0,\N,Missing
abdelali-etal-2014-amara,D11-1125,0,\N,Missing
abdelali-etal-2014-amara,2012.eamt-1.60,0,\N,Missing
abdelali-etal-2014-amara,C12-1121,1,\N,Missing
abdelali-etal-2014-amara,2010.iwslt-evaluation.1,0,\N,Missing
C16-1299,N16-3003,1,0.814437,"IWSLT (Cettolo et al., 2014). We used TED talks as our in-domain (≈ 177K sentences) corpus. For Arabic-to-English, we used the multiUN (≈ 3.7M sentences) (Eisele and Chen, 2010) as our out-domain corpora. For English-to-German, we used data made available (≈ 4.4M sentences) for the 9th Workshop on Machine Translation3 as our out-domain data. Language models were trained on all the available monolingual data (English: ≈ 287.3M and German: ≈ 59.5M sentences). Machine translation systems were tuned on concatenation of the dev- and test2010 and evaluated on test2011-2013 datasets. We used Farasa (Abdelali et al., 2016) to tokenize Arabic and the default Moses tokenizer for English-and German. All data was truecased. See Table 1 for data sizes. NN Training: The NNJM models were trained using the NPLM4 toolkit (Vaswani et al., 2013) with the following settings: a target context of 5 words and an aligned source window of 9 words. We restricted source and target side vocabularies to the 20K and 40K most frequent words in the in-domain data.5 The word vector size D and the hidden layer size were set to 150 and 750, respectively. Training was done using SGD with NCE using 100 noise samples and a mini-batch size o"
C16-1299,D11-1033,0,0.316495,"ata. The fine-tuning method first trains the NNJM on the concatenated data, then runs a few additional epochs on the in-domain data to tune the model towards in-domain. We found our method to outperform both the NDAM and finetuning methods. We also carried experiments against phrase-table weighting methods such as instance weighting (Sennrich, 2012), and phrase-table fill-up combination (Bisazza et al., 2011) and found our approach to outperform these. Our approach is complementary and the gains obtained were found to be additive on top of phrase-table adaptation and MML-based data-selection (Axelrod et al., 2011). The remainder of this paper is organized as follows. Section 2 briefly describes neural network joint model. Section 3 describes our fusion model for domain adaptation. Section 4 presents results and analysis. Section 5 gives an account on the related work and Section 6 concludes the paper. 2 Neural Network Joint Model Neural models are quickly becoming the state-of-the-art in machine translation. The ability to generalize and better capture non-local dependencies gives them edge over traditional models. The two most prevalent approaches are to use NNs as a feature inside SMT decoder (Vaswan"
C16-1299,2014.iwslt-evaluation.6,1,0.849612,"ne test-11 test-12 test-13 2437 1433 1700 993 51K 4K 28K 18K 48K 23K 26K 17K tune test-11 test-12 test-13 2456 1199 1702 1169 48K 21K 30K 26K 52K 24K 32K 28K Table 1: Statistics of the English-German and Arabic-English training corpora in terms of Sentences and Tokens (represented in millions). ep = Europarl, cc = Common Crawl, un = United Nations training to a subset of 1 million sentences containing all the in-domain data and a random selection of sentences from the out-domain data. Machine Translation Settings: We trained a Moses system (Koehn et al., 2007), with the settings described in (Birch et al., 2014): a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013) and other defaults. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. Arabic OOVs were transliterated using unsupervised transliteration module (Durrani et al., 2014) in Moses. Baselines: Baseline MT systems were trained by simply concatenating all the data. We included NNJM model trained on a plain concat"
C16-1299,2011.iwslt-evaluation.18,0,0.32717,". (2015) and the fine-tuning method of Luong and Manning (2015). The NDAM model uses data dependent regularization in the NNJM model to weight training instances, while training the model on the concatenated data. The fine-tuning method first trains the NNJM on the concatenated data, then runs a few additional epochs on the in-domain data to tune the model towards in-domain. We found our method to outperform both the NDAM and finetuning methods. We also carried experiments against phrase-table weighting methods such as instance weighting (Sennrich, 2012), and phrase-table fill-up combination (Bisazza et al., 2011) and found our approach to outperform these. Our approach is complementary and the gains obtained were found to be additive on top of phrase-table adaptation and MML-based data-selection (Axelrod et al., 2011). The remainder of this paper is organized as follows. Section 2 briefly describes neural network joint model. Section 3 describes our fusion model for domain adaptation. Section 4 presents results and analysis. Section 5 gives an account on the related work and Section 6 concludes the paper. 2 Neural Network Joint Model Neural models are quickly becoming the state-of-the-art in machine t"
C16-1299,P13-1141,0,0.0183407,"r combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion and Future Work We presented a deep fusion model based on the neural network joint model (NNJM) of Devlin et al. (2014). The model is learned by fusing in- and out-domain NNJM models into a composite model by adjusting their parameters in favor of the in-domain data. When used as a feature during decoding, our model obtains statistically significant improvements on top of a competition grade phrase-based baseline system. We also showed improvements compared to previous adaptation methods. Further gains were obtained when our models were combined with existing methods. Although t"
C16-1299,P13-1126,0,0.0143517,"complete out-domain data with intelligent weighting towards the in-domain. Our work falls in this line of research. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion and Future Work We presented a deep fusion model based on the neural network joint model (NNJM) of Devlin et al. (2014). The mod"
C16-1299,N12-1047,0,0.0163528,"ions training to a subset of 1 million sentences containing all the in-domain data and a random selection of sentences from the out-domain data. Machine Translation Settings: We trained a Moses system (Koehn et al., 2007), with the settings described in (Birch et al., 2014): a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013) and other defaults. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. Arabic OOVs were transliterated using unsupervised transliteration module (Durrani et al., 2014) in Moses. Baselines: Baseline MT systems were trained by simply concatenating all the data. We included NNJM model trained on a plain concatenation of the data as a feature in our baseline system. In the adapted systems, we either replaced it with the NDAM models trained on weighted concatenation, or with our fusion models (NFM*), where models are trained independently and adjusted towards in-domain data or by interpolating them linearly (EM-weighting) or log-linearly. We also tried th"
C16-1299,P14-1129,0,0.551242,"n to numerous NLP problems. The ability to generalize and to better capture non-local dependencies gives edge to the neural models over their traditional counter-part. Several researchers have also attempted to employ DNNs for domain adaptation in SMT. Duh et al. (2013) and Durrani et al. (2015) used DNNs for data selection. Joty et al. (2015) proposed a DNN-based adaptation model for SMT that regularizes the loss function with respect to the in-domain model. In this paper, we propose a deep fusion approach to domain adaptation for SMT. We use the Neural Network Joint Model (NNJM) proposed by Devlin et al. (2014) as our base model. However, rather than training the model on a plain concatenation of in- and out-domain data or a weighted concatenation (Joty et al., 2015), we first train in- and out-domain NNJM models, and then learn a composite model by readjusting their parameters through backpropagating errors from the output layer to the word embedding layer of each model. The intuition behind learning the models separately, is to learn in-domain model parameters without contaminating them with the out-domain data. In a variant of our model, we restrict backpropagation to only the outermost hidden la"
C16-1299,P13-2119,0,0.55228,"set from the out-domain data, which is closer to the in-domain data (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Recently, there has been a growing interest in deep neural networks (DNNs) and word embeddings with application to numerous NLP problems. The ability to generalize and to better capture non-local dependencies gives edge to the neural models over their traditional counter-part. Several researchers have also attempted to employ DNNs for domain adaptation in SMT. Duh et al. (2013) and Durrani et al. (2015) used DNNs for data selection. Joty et al. (2015) proposed a DNN-based adaptation model for SMT that regularizes the loss function with respect to the in-domain model. In this paper, we propose a deep fusion approach to domain adaptation for SMT. We use the Neural Network Joint Model (NNJM) proposed by Devlin et al. (2014) as our base model. However, rather than training the model on a plain concatenation of in- and out-domain data or a weighted concatenation (Joty et al., 2015), we first train in- and out-domain NNJM models, and then learn a composite model by readju"
C16-1299,P13-2071,1,0.844807,"ented in millions). ep = Europarl, cc = Common Crawl, un = United Nations training to a subset of 1 million sentences containing all the in-domain data and a random selection of sentences from the out-domain data. Machine Translation Settings: We trained a Moses system (Koehn et al., 2007), with the settings described in (Birch et al., 2014): a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013) and other defaults. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. Arabic OOVs were transliterated using unsupervised transliteration module (Durrani et al., 2014) in Moses. Baselines: Baseline MT systems were trained by simply concatenating all the data. We included NNJM model trained on a plain concatenation of the data as a feature in our baseline system. In the adapted systems, we either replaced it with the NDAM models trained on weighted concatenation, or with our fusion models (NFM*), where models are trained independently and adjusted towards in-domain data or by inte"
C16-1299,E14-4029,1,0.842113,"tences from the out-domain data. Machine Translation Settings: We trained a Moses system (Koehn et al., 2007), with the settings described in (Birch et al., 2014): a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013) and other defaults. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. Arabic OOVs were transliterated using unsupervised transliteration module (Durrani et al., 2014) in Moses. Baselines: Baseline MT systems were trained by simply concatenating all the data. We included NNJM model trained on a plain concatenation of the data as a feature in our baseline system. In the adapted systems, we either replaced it with the NDAM models trained on weighted concatenation, or with our fusion models (NFM*), where models are trained independently and adjusted towards in-domain data or by interpolating them linearly (EM-weighting) or log-linearly. We also tried the approach of Luong and Manning (2015) by Fine Tuning baseline model towards in-domain data (i.e., by trainin"
C16-1299,2015.mtsummit-papers.10,1,0.787326,"n data, which is closer to the in-domain data (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Recently, there has been a growing interest in deep neural networks (DNNs) and word embeddings with application to numerous NLP problems. The ability to generalize and to better capture non-local dependencies gives edge to the neural models over their traditional counter-part. Several researchers have also attempted to employ DNNs for domain adaptation in SMT. Duh et al. (2013) and Durrani et al. (2015) used DNNs for data selection. Joty et al. (2015) proposed a DNN-based adaptation model for SMT that regularizes the loss function with respect to the in-domain model. In this paper, we propose a deep fusion approach to domain adaptation for SMT. We use the Neural Network Joint Model (NNJM) proposed by Devlin et al. (2014) as our base model. However, rather than training the model on a plain concatenation of in- and out-domain data or a weighted concatenation (Joty et al., 2015), we first train in- and out-domain NNJM models, and then learn a composite model by readjusting their parameters thr"
C16-1299,N13-1073,0,0.0253079,"1 test-12 test-13 2456 1199 1702 1169 48K 21K 30K 26K 52K 24K 32K 28K Table 1: Statistics of the English-German and Arabic-English training corpora in terms of Sentences and Tokens (represented in millions). ep = Europarl, cc = Common Crawl, un = United Nations training to a subset of 1 million sentences containing all the in-domain data and a random selection of sentences from the out-domain data. Machine Translation Settings: We trained a Moses system (Koehn et al., 2007), with the settings described in (Birch et al., 2014): a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013) and other defaults. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. Arabic OOVs were transliterated using unsupervised transliteration module (Durrani et al., 2014) in Moses. Baselines: Baseline MT systems were trained by simply concatenating all the data. We included NNJM model trained on a plain concatenation of the data as a feature in our baseline system. In the adapted systems, we eit"
C16-1299,P12-2023,0,0.0239499,". Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion and Future Work We presented a deep fusion model based on the neural network joint model (NNJM) of Devlin et al. (2014). The model is learned by fusing in- and out-domain NNJM models into a composite model by adjusting their parameters in favor of the in-domain data. When used as a feature during decoding, our model obtains statistically significant improvements on top of a competition grade phrase-based baseline system. We"
C16-1299,eisele-chen-2010-multiun,0,0.0113535,"fusion and linear interpolation have the same number of parameters, which is the sum of the size of the base models. In fusion, we readjust all the parameters of the base models (or just the output layer weights in fusion-II), where in linear interpolation, we only learn their mixing weight. 4 Experiments Data: We experimented with the data made available for the translation task of the International Workshop on Spoken Language Translation IWSLT (Cettolo et al., 2014). We used TED talks as our in-domain (≈ 177K sentences) corpus. For Arabic-to-English, we used the multiUN (≈ 3.7M sentences) (Eisele and Chen, 2010) as our out-domain corpora. For English-to-German, we used data made available (≈ 4.4M sentences) for the 9th Workshop on Machine Translation3 as our out-domain data. Language models were trained on all the available monolingual data (English: ≈ 287.3M and German: ≈ 59.5M sentences). Machine translation systems were tuned on concatenation of the dev- and test2010 and evaluated on test2011-2013 datasets. We used Farasa (Abdelali et al., 2016) to tokenize Arabic and the default Moses tokenizer for English-and German. All data was truecased. See Table 1 for data sizes. NN Training: The NNJM model"
C16-1299,W08-0334,0,0.0309179,"uence model and NNJM models. An alternative to completely filtering out less useful data is to minimize its effect by down-weighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Our work falls in this line of research. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al."
C16-1299,W07-0717,0,0.2591,"´an et al., 2013). This is because of the difference in stylistic variations, vocabulary choices and word sense ambiguities across genres. Domain adaptation aims at finding the optimal point, that maximizes on the useful information available in the out-domain data, in favor of the in-domain data, while preventing it from degrading the performance of the system. This is either done by selecting a subset from the out-domain data, which is closer to the in-domain data (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Recently, there has been a growing interest in deep neural networks (DNNs) and word embeddings with application to numerous NLP problems. The ability to generalize and to better capture non-local dependencies gives edge to the neural models over their traditional counter-part. Several researchers have also attempted to employ DNNs for domain adaptation in SMT. Duh et al. (2013) and Durrani et al. (2015) used DNNs for data selection. Joty et al. (2015) proposed a DNN-based adaptation model for SMT that regularizes the loss function with respect to the in-domain model. In this"
C16-1299,W09-0439,0,0.0335122,"less useful data is to minimize its effect by down-weighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Our work falls in this line of research. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion"
C16-1299,D10-1044,0,0.0854332,"ame. Translation model features were used recently by (Liu et al., 2014; Hoang and Sima’an, 2014) for data selection. Durrani et al. (2015) performed data selection using operation sequence model and NNJM models. An alternative to completely filtering out less useful data is to minimize its effect by down-weighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Our work falls in this line of research. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Other work on domain adaptation"
C16-1299,D08-1089,0,0.0282,"lish training corpora in terms of Sentences and Tokens (represented in millions). ep = Europarl, cc = Common Crawl, un = United Nations training to a subset of 1 million sentences containing all the in-domain data and a random selection of sentences from the out-domain data. Machine Translation Settings: We trained a Moses system (Koehn et al., 2007), with the settings described in (Birch et al., 2014): a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013) and other defaults. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. Arabic OOVs were transliterated using unsupervised transliteration module (Durrani et al., 2014) in Moses. Baselines: Baseline MT systems were trained by simply concatenating all the data. We included NNJM model trained on a plain concatenation of the data as a feature in our baseline system. In the adapted systems, we either replaced it with the NDAM models trained on weighted concatenation, or with our fusion models (NFM*), where models are trained in"
C16-1299,2013.iwslt-papers.2,1,0.90169,"Missing"
C16-1299,E14-1035,0,0.0175463,"e carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion and Future Work We presented a deep fusion model based on the neural network joint model (NNJM) of Devlin et al. (2014). The model is learned by fusing in- and out-domain NNJM models into a composite model by adjusting their parameters in favor of the in-domain data. When used as a feature during decoding, our model obtains statistically significant improvements on top of a competition grade phrase-based baseline system. We also showed improveme"
C16-1299,W11-2123,0,0.00997717,"Statistics of the English-German and Arabic-English training corpora in terms of Sentences and Tokens (represented in millions). ep = Europarl, cc = Common Crawl, un = United Nations training to a subset of 1 million sentences containing all the in-domain data and a random selection of sentences from the out-domain data. Machine Translation Settings: We trained a Moses system (Koehn et al., 2007), with the settings described in (Birch et al., 2014): a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013) and other defaults. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. Arabic OOVs were transliterated using unsupervised transliteration module (Durrani et al., 2014) in Moses. Baselines: Baseline MT systems were trained by simply concatenating all the data. We included NNJM model trained on a plain concatenation of the data as a feature in our baseline system. In the adapted systems, we either replaced it with the NDAM models trained on weighted concatenation, or w"
C16-1299,2005.eamt-1.19,0,0.0559647,".3 in English-German and Arabic-English, respectively). 5 Related Work Previous work on domain adaptation in MT can be broken down broadly into two main categories namely data selection and model adaptation. Data selection has shown to be an effective way to discard poor quality or irrelevant training instances, which when included in an MT system, hurts its performance. Selection based methods can be helpful to reduce computational cost when training is expensive and also when memory is constrained. Data selection was done earlier for language modeling using information retrieval techniques (Hildebrand et al., 2005) and perplexity measures (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source- and target-side language models. Duh et al. (2013) used a recurrent neural language model instead of an ngram-based language model to do the same. Translation model features were used recently by (Liu et al., 2014; Hoang and Sima’an, 2014) for data selection. Durrani et al. (2015) performed data selection using operation sequence model and NNJM models. An alternative to completely filtering out less useful data is to m"
C16-1299,C14-1182,0,0.336565,"Missing"
C16-1299,D15-1147,1,0.338038,"ukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Recently, there has been a growing interest in deep neural networks (DNNs) and word embeddings with application to numerous NLP problems. The ability to generalize and to better capture non-local dependencies gives edge to the neural models over their traditional counter-part. Several researchers have also attempted to employ DNNs for domain adaptation in SMT. Duh et al. (2013) and Durrani et al. (2015) used DNNs for data selection. Joty et al. (2015) proposed a DNN-based adaptation model for SMT that regularizes the loss function with respect to the in-domain model. In this paper, we propose a deep fusion approach to domain adaptation for SMT. We use the Neural Network Joint Model (NNJM) proposed by Devlin et al. (2014) as our base model. However, rather than training the model on a plain concatenation of in- and out-domain data or a weighted concatenation (Joty et al., 2015), we first train in- and out-domain NNJM models, and then learn a composite model by readjusting their parameters through backpropagating errors from the output layer"
C16-1299,P07-2045,0,0.0037253,"est Set Sent. TokEN TokDE Corpus Sent. TokAR TokEN tune test-11 test-12 test-13 2437 1433 1700 993 51K 4K 28K 18K 48K 23K 26K 17K tune test-11 test-12 test-13 2456 1199 1702 1169 48K 21K 30K 26K 52K 24K 32K 28K Table 1: Statistics of the English-German and Arabic-English training corpora in terms of Sentences and Tokens (represented in millions). ep = Europarl, cc = Common Crawl, un = United Nations training to a subset of 1 million sentences containing all the in-domain data and a random selection of sentences from the out-domain data. Machine Translation Settings: We trained a Moses system (Koehn et al., 2007), with the settings described in (Birch et al., 2014): a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013) and other defaults. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. Arabic OOVs were transliterated using unsupervised transliteration module (Durrani et al., 2014) in Moses. Baselines: Baseline MT systems were trained by simply concatenating all the d"
C16-1299,W04-3250,0,0.0940932,"tion: In Table 4 we experiment with MML-based filtering and probe whether our model can also improve on top of data selection. Firstly, selecting no out-domain data degrades the English-to-German system. On the contrary, the Arabic-to-English system substantially improves. This shows that general domain data is useful for English-to-German and much of the outdomain data (UN corpus) used in these experiments is harmful in the case of Arabic-to-English. In comparison, data selection was found to be less useful in the case of English-to-German. But we found 9 p < 0.05 using bootstrap resampling (Koehn, 2004), with 1000 samples. 3183 English-to-German Arabic-to-English System tst11 tst12 tst13 Avg tst11 tst12 tst13 Avg Baselinecat BaselineID 27.3 26.7 22.9 22.5 24.5 23.6 24.9 24.3 26.1 27.2 29.4 30.0 30.5 30.2 28.7 29.1 MML +NFM-I 26.9 27.6 22.9 23.1 24.4 25.0 24.7 25.2 27.4 27.6 30.8 31.2 30.9 31.1 29.7 30.0 Table 4: Comparing with MML (Axelrod et al 2011) that using our fusion model instead of baseline NNJM in either system still gave improvements ( +0.5 and +0.3 in English-German and Arabic-English, respectively). 5 Related Work Previous work on domain adaptation in MT can be broken down broadl"
C16-1299,P14-2093,0,0.0440254,"Missing"
C16-1299,2015.iwslt-evaluation.11,0,0.400393,"ails: http://creativecommons.org/licenses/by/4.0/ 3177 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3177–3187, Osaka, Japan, December 11-17 2016. We evaluated our model on a standard task of translating TED talks for English-to-German and Arabicto-English language pairs. Compared to baseline NNJM models trained on a concatenation of in- and out-domain data, our model achieves an average improvement of up to 0.9 BLEU points. The most relevant to our work are the NDAM model of Joty et al. (2015) and the fine-tuning method of Luong and Manning (2015). The NDAM model uses data dependent regularization in the NNJM model to weight training instances, while training the model on the concatenated data. The fine-tuning method first trains the NNJM on the concatenated data, then runs a few additional epochs on the in-domain data to tune the model towards in-domain. We found our method to outperform both the NDAM and finetuning methods. We also carried experiments against phrase-table weighting methods such as instance weighting (Sennrich, 2012), and phrase-table fill-up combination (Bisazza et al., 2011) and found our approach to outperform thes"
C16-1299,D15-1166,0,0.0656008,"ibes neural network joint model. Section 3 describes our fusion model for domain adaptation. Section 4 presents results and analysis. Section 5 gives an account on the related work and Section 6 concludes the paper. 2 Neural Network Joint Model Neural models are quickly becoming the state-of-the-art in machine translation. The ability to generalize and better capture non-local dependencies gives them edge over traditional models. The two most prevalent approaches are to use NNs as a feature inside SMT decoder (Vaswani et al., 2013; Devlin et al., 2014), or as an end-to-end translation system (Luong et al., 2015; Bahdanau et al., 2015; Sennrich et al., 2016) designed as fully trainable model of which every component is tuned based on training corpora to maximize its translation performance. Our work falls in the former category and extends NNJM. The NNJM model learns a feed-forward neural network from augmented streams of source and target sequences. For a bilingual sentence pair (S, T ), NNJM defines a conditional probability distribution: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−n+1 , si ) (1) i=1 where, si is an m-word source window for a target word ti based on the one-to-one alignment between T an"
C16-1299,N13-1074,0,0.0203016,"takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Our work falls in this line of research. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion and Future Work We presented a deep fusion model based on the neural network joint model (NNJM) of Devlin et"
C16-1299,C14-1105,0,0.0141523,"Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion and Future Work We presented a deep fusion model based on the neural network joint model (NNJM) of Devlin et al. (2014). The model is learned by fusing in- and out-domain NNJM models into a composite model by adjusting their parameters in favor of the in-domain data. When used as a feature during decoding, our model obtains statistically significant improvements on top of a competition grade phrase-based baseline system. We also showed improvements compared to previous adaptation methods. Further gains were obtained when our models"
C16-1299,D09-1074,0,0.478744,"anslation tasks such as translating TED talks (Cettolo et al., 2014), patents (Fujii et al., 2010) and educational content (Guzm´an et al., 2013). This is because of the difference in stylistic variations, vocabulary choices and word sense ambiguities across genres. Domain adaptation aims at finding the optimal point, that maximizes on the useful information available in the out-domain data, in favor of the in-domain data, while preventing it from degrading the performance of the system. This is either done by selecting a subset from the out-domain data, which is closer to the in-domain data (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Recently, there has been a growing interest in deep neural networks (DNNs) and word embeddings with application to numerous NLP problems. The ability to generalize and to better capture non-local dependencies gives edge to the neural models over their traditional counter-part. Several researchers have also attempted to employ DNNs for domain adaptation in SMT. Duh et al. (2013) and Durrani et al. (2015) used DNNs for data selection. Joty et al. (2015"
C16-1299,P10-2041,0,0.291581,"translating TED talks (Cettolo et al., 2014), patents (Fujii et al., 2010) and educational content (Guzm´an et al., 2013). This is because of the difference in stylistic variations, vocabulary choices and word sense ambiguities across genres. Domain adaptation aims at finding the optimal point, that maximizes on the useful information available in the out-domain data, in favor of the in-domain data, while preventing it from degrading the performance of the system. This is either done by selecting a subset from the out-domain data, which is closer to the in-domain data (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Recently, there has been a growing interest in deep neural networks (DNNs) and word embeddings with application to numerous NLP problems. The ability to generalize and to better capture non-local dependencies gives edge to the neural models over their traditional counter-part. Several researchers have also attempted to employ DNNs for domain adaptation in SMT. Duh et al. (2013) and Durrani et al. (2015) used DNNs for data selection. Joty et al. (2015) proposed a DNN-based a"
C16-1299,D09-1141,0,0.0170088,"els. An alternative to completely filtering out less useful data is to minimize its effect by down-weighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Our work falls in this line of research. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense dis"
C16-1299,2013.iwslt-evaluation.8,1,0.909079,"Missing"
C16-1299,P13-1082,0,0.0755351,"nterpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion and Future Work We presented a deep fusion model based on the neural network joint model (NNJM) of Devlin et al. (2014). The model is learned by fusing in- and out-domain NNJM models into a composite model by adjusting their parameters in favor of the in-domain data. When used as a feature during decoding, our model obtains statistically significant improvements on top of a competition grade phrase-based baseline system. We also showed improvements compared to previous adaptation methods. Further gains were ob"
C16-1299,P16-1009,0,0.0306581,"describes our fusion model for domain adaptation. Section 4 presents results and analysis. Section 5 gives an account on the related work and Section 6 concludes the paper. 2 Neural Network Joint Model Neural models are quickly becoming the state-of-the-art in machine translation. The ability to generalize and better capture non-local dependencies gives them edge over traditional models. The two most prevalent approaches are to use NNs as a feature inside SMT decoder (Vaswani et al., 2013; Devlin et al., 2014), or as an end-to-end translation system (Luong et al., 2015; Bahdanau et al., 2015; Sennrich et al., 2016) designed as fully trainable model of which every component is tuned based on training corpora to maximize its translation performance. Our work falls in the former category and extends NNJM. The NNJM model learns a feed-forward neural network from augmented streams of source and target sequences. For a bilingual sentence pair (S, T ), NNJM defines a conditional probability distribution: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−n+1 , si ) (1) i=1 where, si is an m-word source window for a target word ti based on the one-to-one alignment between T and S. This is essentially an (m+n)-gram bilingua"
C16-1299,E12-1055,0,0.546508,"is because of the difference in stylistic variations, vocabulary choices and word sense ambiguities across genres. Domain adaptation aims at finding the optimal point, that maximizes on the useful information available in the out-domain data, in favor of the in-domain data, while preventing it from degrading the performance of the system. This is either done by selecting a subset from the out-domain data, which is closer to the in-domain data (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Recently, there has been a growing interest in deep neural networks (DNNs) and word embeddings with application to numerous NLP problems. The ability to generalize and to better capture non-local dependencies gives edge to the neural models over their traditional counter-part. Several researchers have also attempted to employ DNNs for domain adaptation in SMT. Duh et al. (2013) and Durrani et al. (2015) used DNNs for data selection. Joty et al. (2015) proposed a DNN-based adaptation model for SMT that regularizes the loss function with respect to the in-domain model. In this paper, we propos"
C16-1299,D13-1140,0,0.208513,"2011). The remainder of this paper is organized as follows. Section 2 briefly describes neural network joint model. Section 3 describes our fusion model for domain adaptation. Section 4 presents results and analysis. Section 5 gives an account on the related work and Section 6 concludes the paper. 2 Neural Network Joint Model Neural models are quickly becoming the state-of-the-art in machine translation. The ability to generalize and better capture non-local dependencies gives them edge over traditional models. The two most prevalent approaches are to use NNs as a feature inside SMT decoder (Vaswani et al., 2013; Devlin et al., 2014), or as an end-to-end translation system (Luong et al., 2015; Bahdanau et al., 2015; Sennrich et al., 2016) designed as fully trainable model of which every component is tuned based on training corpora to maximize its translation performance. Our work falls in the former category and extends NNJM. The NNJM model learns a feed-forward neural network from augmented streams of source and target sequences. For a bilingual sentence pair (S, T ), NNJM defines a conditional probability distribution: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−n+1 , si ) (1) i=1 where, si is an m-word"
D14-1154,cotterell-callison-burch-2014-multi,0,0.4921,"Missing"
D14-1154,N12-1006,0,0.042694,"rb suffixes such as “yn” instead of “wn” and “wA” instead of “wn” respectively. Also, so-called “five nouns”, are used in only one form (ex. “&gt;bw” (father of) instead of “&gt;bA” or “&gt;by”). 4 Detecting Dialectal Peculiarities ARZ is different from MSA lexically, morphologically, phonetically, and syntactically. Here, we present methods to handle such peculiarities. We chose not to handle syntactic differences, because they may be captured using word n-gram models. To capture lexical variations, we extracted and sorted by frequency all the unigrams from the Egyptian side of the LDC2012T09 corpus (Zbib et al., 2012), which has ≈ 38k Egyptian-English parallel sentences. A linguist was tasked with manually reviewing the words from the top until 1,300 dialectal words were found. Some of the words on the list included dialectal words, commonly used foreign words, words that exhibit morphological variations, and others with letter substitution. 1466 For morphological phenomenon, we employed three methods, namely: • Unsupervised Morphology Induction: We employed the unsupervised morpheme segmentation tool, Morfessor (Virpioja et al., 2013). It is a data driven tool that automatically learns morphemes from data"
D14-1154,I11-1062,0,0.0283516,"Missing"
D14-1154,C12-1160,0,0.0521899,"Missing"
D14-1154,P11-2007,0,\N,Missing
D14-1154,J14-1006,0,\N,Missing
D14-1154,darwish-etal-2014-using,1,\N,Missing
D14-1154,P13-2081,0,\N,Missing
D15-1097,P10-1048,1,0.881806,"Missing"
D15-1097,D12-1039,0,0.0683235,"Missing"
D15-1097,P14-2111,0,0.0235481,"dagy, zaindagee and zndagi. Specifically, the following normalization issues arise: (1) differently spelled words (see example above), (2) identically spelled words that are lexically different (e.g., bahar can be used for both [outside] and [spring], and (3) spellings that match words in English (e.g, had [limit] for the English word ‘had’). These inconsistencies cause a problem of data sparsity in basic natural language processing 2 Previous Work Normalization of short text messages and tweets has been in focus (Sproat et al., 2001; Wei et al., 2011; Clark and Araki, 2011; Roy et al., 2013; Chrupala, 2014; Kaufmann and Kalita, 2010; Sidarenka et al., 2013; Ling et al., 2013; Desai and Narvekar, 2015; Pinto et al., 2012). However, most of the work is limited to English or to other 823 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 823–828, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. resource-rich languages. In this paper, we focus on Roman Urdu, an under-resourced language, that does not have any gold standard corpus with standard word forms. Therefore, we are restricted to the task of finding lexical v"
D15-1097,W12-2109,0,0.141669,"ns in informal text. This is a rather more challenging problem since in this case every word is a possible variation of every other word in the corpus. Researchers have used phonetic, string, and contextual knowledge to find lexical variations in informal text.1 Pinto et al. (2012; Han et al. (2012; Zhang et al. (2015) used phonetic-based methods to find lexical variations. Han et al. (2012) also used word similarity and word context to enhance performance. Wang and Ng (2013) used normalization operations e.g., missing word recovery and punctuation correction to improve normalization process. Irvine et al. (2012) used manually prepared training data to build an automatic normalization system. Contractor et al. (2010) used string edit distance to find candidate lexical variations. Yang and Eisenstein (2013) used an unsupervised approach with log linear model and sequential Monte Carlo approximation. We propose an unsupervised method to find lexical variations. It uses string edit distance like Contractor et al. (2010), Sound-based encoding like Pinto et al. (2012) and context like Han et al. (2012) combined in a discriminative framework. However, in contrast, it does not use any corpus of standard word"
D15-1097,N13-1050,0,0.0141014,"that does not have any gold standard corpus with standard word forms. Therefore, we are restricted to the task of finding lexical variations in informal text. This is a rather more challenging problem since in this case every word is a possible variation of every other word in the corpus. Researchers have used phonetic, string, and contextual knowledge to find lexical variations in informal text.1 Pinto et al. (2012; Han et al. (2012; Zhang et al. (2015) used phonetic-based methods to find lexical variations. Han et al. (2012) also used word similarity and word context to enhance performance. Wang and Ng (2013) used normalization operations e.g., missing word recovery and punctuation correction to improve normalization process. Irvine et al. (2012) used manually prepared training data to build an automatic normalization system. Contractor et al. (2010) used string edit distance to find candidate lexical variations. Yang and Eisenstein (2013) used an unsupervised approach with log linear model and sequential Monte Carlo approximation. We propose an unsupervised method to find lexical variations. It uses string edit distance like Contractor et al. (2010), Sound-based encoding like Pinto et al. (2012)"
D15-1097,D13-1007,0,0.0263982,"and contextual knowledge to find lexical variations in informal text.1 Pinto et al. (2012; Han et al. (2012; Zhang et al. (2015) used phonetic-based methods to find lexical variations. Han et al. (2012) also used word similarity and word context to enhance performance. Wang and Ng (2013) used normalization operations e.g., missing word recovery and punctuation correction to improve normalization process. Irvine et al. (2012) used manually prepared training data to build an automatic normalization system. Contractor et al. (2010) used string edit distance to find candidate lexical variations. Yang and Eisenstein (2013) used an unsupervised approach with log linear model and sequential Monte Carlo approximation. We propose an unsupervised method to find lexical variations. It uses string edit distance like Contractor et al. (2010), Sound-based encoding like Pinto et al. (2012) and context like Han et al. (2012) combined in a discriminative framework. However, in contrast, it does not use any corpus of standard word forms to find lexical variations. 3 which in turn serves as grouping words of similar sounds (lexical variations) to one code. However, most of the schemes are designed for English and European la"
D15-1097,D13-1008,0,0.0128331,"on issues arise: (1) differently spelled words (see example above), (2) identically spelled words that are lexically different (e.g., bahar can be used for both [outside] and [spring], and (3) spellings that match words in English (e.g, had [limit] for the English word ‘had’). These inconsistencies cause a problem of data sparsity in basic natural language processing 2 Previous Work Normalization of short text messages and tweets has been in focus (Sproat et al., 2001; Wei et al., 2011; Clark and Araki, 2011; Roy et al., 2013; Chrupala, 2014; Kaufmann and Kalita, 2010; Sidarenka et al., 2013; Ling et al., 2013; Desai and Narvekar, 2015; Pinto et al., 2012). However, most of the work is limited to English or to other 823 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 823–828, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. resource-rich languages. In this paper, we focus on Roman Urdu, an under-resourced language, that does not have any gold standard corpus with standard word forms. Therefore, we are restricted to the task of finding lexical variations in informal text. This is a rather more challenging problem"
D15-1097,E09-1079,1,0.886844,"Missing"
D15-1097,N10-1077,0,\N,Missing
D15-1097,C10-2022,0,\N,Missing
D15-1147,abdelali-etal-2014-amara,1,0.184811,"Missing"
D15-1147,D13-1106,0,0.0123399,"rge to capture long-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T , the NNJM model computes the conditional probability P (T |S) as follows: P (T |S) ≈ |T | Y P (ti |ti−1 . . ."
D15-1147,D11-1033,0,0.0505111,"nces, which when included in an MT system, hurts its performance. The idea is to score the outdomain data using a model trained from the indomain data and apply a cut-off based on the resulting scores. The MT system can then be trained on a subset of the out-domain data that is closer to in-domain. Selection based methods can be helpful to reduce computational cost when training is expensive and also when memory is constrained. Data selection was done earlier for language modeling using information retrieval techniques (Hildebrand et al., 2005) and perplexity measures (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source- and target-side language models. Duh et al. (2013) used a recurrent neural language model instead of an ngram-based language model to do the same. Translation model features were used recently by (Liu et al., 2014; Hoang and Sima’an, 2014) for data selection. Durrani et al. (2015a) performed data selection using operation sequence model (OSM) and NNJM models. 2.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time consuming process. An al"
D15-1147,2014.iwslt-evaluation.6,1,0.906521,"Missing"
D15-1147,2011.iwslt-evaluation.18,0,0.0228131,"etely filtering out less useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasl"
D15-1147,N13-1114,0,0.33049,"domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural networ"
D15-1147,P13-1126,0,0.513472,"domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural networ"
D15-1147,N12-1047,0,0.0255873,"sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and DeNero, 2012) for Arabic-to-English and the default tokenizer provided with the Moses toolkit (Koehn et al., 2007) for the English-to-German pair. Arabic OOVs were translated using an unsupervised transliteration module in Moses (Durrani et al., 2014). We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. 5.3 Intrinsic Evaluation In this section, we compare the NNJM model and our NDAM models in terms of their perplexity numbers on the in-domain held-out dataset (i.e., dev+test2010). We choose Arabic-English language pair for the development experiments and train domain-wise models to measure the relatedness of each domain with respect to the in-domain. We later replicated selective experiments for the English-German language pair. The first part of Table 2 summarizes the results for Arabic-English. The perplexity numbers in the second column (NNJMb ) show that NEWS is the 1265 Doma"
D15-1147,P14-1129,0,0.286949,"the SMT pipeline, starting from corpus preparation to word-alignment, and then training a wide range of models opens a wide horizon to carry out domain specific adaptations. This is typically done using either data selection (Matsoukas et al., 2009) or model adaptation (Foster and Kuhn, 2007). In this paper, we further research in model adaptation using the neural network framework. In recent years, there has been a growing interest in deep neural networks (NNs) and word embeddings with application to numerous NLP problems. A notably successful attempt on the SMT frontier was recently made by Devlin et al. (2014). They proposed a neural network joint model (NNJM), which augments streams of source with target n-grams and learns a NN model over vector representation of such streams. The model is then integrated into the decoder and used as an additional language model feature. Our aim in this paper is to advance the state-ofthe-art in SMT by extending NNJM for domain adaptation to leverage the huge amount of out1259 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1259–1270, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguist"
D15-1147,P13-2119,0,0.178533,"a cut-off based on the resulting scores. The MT system can then be trained on a subset of the out-domain data that is closer to in-domain. Selection based methods can be helpful to reduce computational cost when training is expensive and also when memory is constrained. Data selection was done earlier for language modeling using information retrieval techniques (Hildebrand et al., 2005) and perplexity measures (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source- and target-side language models. Duh et al. (2013) used a recurrent neural language model instead of an ngram-based language model to do the same. Translation model features were used recently by (Liu et al., 2014; Hoang and Sima’an, 2014) for data selection. Durrani et al. (2015a) performed data selection using operation sequence model (OSM) and NNJM models. 2.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time consuming process. An alternative to completely filtering out less useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advanta"
D15-1147,E14-4029,1,0.0587384,"del (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and DeNero, 2012) for Arabic-to-English and the default tokenizer provided with the Moses toolkit (Koehn et al., 2007) for the English-to-German pair. Arabic OOVs were translated using an unsupervised transliteration module in Moses (Durrani et al., 2014). We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. 5.3 Intrinsic Evaluation In this section, we compare the NNJM model and our NDAM models in terms of their perplexity numbers on the in-domain held-out dataset (i.e., dev+test2010). We choose Arabic-English language pair for the development experiments and train domain-wise models to measure the relatedness of each domain with respect to the in-domain. We later replicated selective experiments for the English-German language pair. The first part of Table 2 summarizes the results for Arabic-English. The perplexity numbers in the s"
D15-1147,P13-1141,0,0.0984619,"ture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural network framework. In contrast to previous work, we perform it at the (bilingual) ngram level, where n is sufficiently large to capture long-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In r"
D15-1147,2015.mtsummit-papers.10,1,0.422186,"ensive and also when memory is constrained. Data selection was done earlier for language modeling using information retrieval techniques (Hildebrand et al., 2005) and perplexity measures (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source- and target-side language models. Duh et al. (2013) used a recurrent neural language model instead of an ngram-based language model to do the same. Translation model features were used recently by (Liu et al., 2014; Hoang and Sima’an, 2014) for data selection. Durrani et al. (2015a) performed data selection using operation sequence model (OSM) and NNJM models. 2.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time consuming process. An alternative to completely filtering out less useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting"
D15-1147,N13-1073,0,0.0372083,"9/125 IWSLT CC NEWS EP 177K 2.3M 200K 1.8M Tok. 3.5/3.3 57/53 2.8/3.4 51/48 Table 1: Statistics of the Arabic-English and English-German training corpora in terms of Sentences and Tokens (Source/Target). Tokens are represented in millions. 100 noise samples and a mini-batch size of 1000. All models were trained for 25 epochs. We used identical settings to train the NDAM models, except for the special handling of unk tokens. Machine Translation System: We trained a Moses system (Koehn et al., 2007), with the following settings: a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and DeNero, 2012) for Arabic-to-English and the default tokenizer provided with the Moses toolkit (Koehn et al., 2007) f"
D15-1147,P12-2023,0,0.0241725,"2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural network framework. In contrast to previous work, we perform it at the (bilingual) ngram level, where n is sufficiently large to capture long-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss"
D15-1147,eisele-chen-2010-multiun,0,0.0290387,"Missing"
D15-1147,W08-0334,0,0.257478,"that finding an optimal cut-off threshold is a time consuming process. An alternative to completely filtering out less useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptati"
D15-1147,W07-0717,0,0.0175294,"taken from out-domain data completely changes the meaning of the sentence. In this paper, we tackle this problem by proposing domain adaptation models that make use of all the data while preserving the in-domain preferences. A significant amount of research has been carried out recently in domain adaptation. The complexity of the SMT pipeline, starting from corpus preparation to word-alignment, and then training a wide range of models opens a wide horizon to carry out domain specific adaptations. This is typically done using either data selection (Matsoukas et al., 2009) or model adaptation (Foster and Kuhn, 2007). In this paper, we further research in model adaptation using the neural network framework. In recent years, there has been a growing interest in deep neural networks (NNs) and word embeddings with application to numerous NLP problems. A notably successful attempt on the SMT frontier was recently made by Devlin et al. (2014). They proposed a neural network joint model (NNJM), which augments streams of source with target n-grams and learns a NN model over vector representation of such streams. The model is then integrated into the decoder and used as an additional language model feature. Our a"
D15-1147,W09-0439,0,0.181637,"An alternative to completely filtering out less useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidel"
D15-1147,D10-1044,0,0.0298816,"Missing"
D15-1147,D08-1089,0,0.134778,"training corpora in terms of Sentences and Tokens (Source/Target). Tokens are represented in millions. 100 noise samples and a mini-batch size of 1000. All models were trained for 25 epochs. We used identical settings to train the NDAM models, except for the special handling of unk tokens. Machine Translation System: We trained a Moses system (Koehn et al., 2007), with the following settings: a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and DeNero, 2012) for Arabic-to-English and the default tokenizer provided with the Moses toolkit (Koehn et al., 2007) for the English-to-German pair. Arabic OOVs were translated using an unsupervised transliteration module in Moses (Durrani et al., 2014). We used"
D15-1147,P14-1066,0,0.0175969,"s. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T , the NNJM model computes the conditional probability P (T |S) as follows: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−p+1 , si ) (1) where φ(xn ) defines the transfo"
D15-1147,P12-1016,0,0.0157674,"al., 2007), with the following settings: a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and DeNero, 2012) for Arabic-to-English and the default tokenizer provided with the Moses toolkit (Koehn et al., 2007) for the English-to-German pair. Arabic OOVs were translated using an unsupervised transliteration module in Moses (Durrani et al., 2014). We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. 5.3 Intrinsic Evaluation In this section, we compare the NNJM model and our NDAM models in terms of their perplexity numbers on the in-domain held-out dataset (i.e., dev+test2010). We choose Arabic-English language pair for the development experiments and train domain-wise models to measure the"
D15-1147,2013.iwslt-papers.2,1,0.883493,"Missing"
D15-1147,E14-1035,0,0.111537,"2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural network framework. In contrast to previous work, we perform it at the (bilingual) ngram level, where n is sufficiently large to capture long-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptati"
D15-1147,W11-2123,0,0.0263896,"stics of the Arabic-English and English-German training corpora in terms of Sentences and Tokens (Source/Target). Tokens are represented in millions. 100 noise samples and a mini-batch size of 1000. All models were trained for 25 epochs. We used identical settings to train the NDAM models, except for the special handling of unk tokens. Machine Translation System: We trained a Moses system (Koehn et al., 2007), with the following settings: a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and DeNero, 2012) for Arabic-to-English and the default tokenizer provided with the Moses toolkit (Koehn et al., 2007) for the English-to-German pair. Arabic OOVs were translated using an unsupervised transl"
D15-1147,2005.eamt-1.19,1,0.289763,"o be an effective way to discard poor quality or irrelevant training instances, which when included in an MT system, hurts its performance. The idea is to score the outdomain data using a model trained from the indomain data and apply a cut-off based on the resulting scores. The MT system can then be trained on a subset of the out-domain data that is closer to in-domain. Selection based methods can be helpful to reduce computational cost when training is expensive and also when memory is constrained. Data selection was done earlier for language modeling using information retrieval techniques (Hildebrand et al., 2005) and perplexity measures (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source- and target-side language models. Duh et al. (2013) used a recurrent neural language model instead of an ngram-based language model to do the same. Translation model features were used recently by (Liu et al., 2014; Hoang and Sima’an, 2014) for data selection. Durrani et al. (2015a) performed data selection using operation sequence model (OSM) and NNJM models. 2.2 Model Adaptation The downside of data selection is that"
D15-1147,C14-1182,0,0.475615,"Missing"
D15-1147,D13-1176,0,0.0524999,"-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T , the NNJM model computes the conditional probability P (T |S) as follows: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−p+1 , si ) (1) where φ(xn ) d"
D15-1147,P07-2045,0,0.00828985,"are/nplm/ Corpus AR-EN Sent. IWSLT QED NEWS UN 150k 150k 203k 3.7M Tok. Corpus EN-DE Sent. 2.8/3.0 1.4/1.5 5.6/6.3 129/125 IWSLT CC NEWS EP 177K 2.3M 200K 1.8M Tok. 3.5/3.3 57/53 2.8/3.4 51/48 Table 1: Statistics of the Arabic-English and English-German training corpora in terms of Sentences and Tokens (Source/Target). Tokens are represented in millions. 100 noise samples and a mini-batch size of 1000. All models were trained for 25 epochs. We used identical settings to train the NDAM models, except for the special handling of unk tokens. Machine Translation System: We trained a Moses system (Koehn et al., 2007), with the following settings: a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and"
D15-1147,P14-2093,0,0.196766,"Missing"
D15-1147,C12-2104,0,0.0153129,"lized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T , the NNJM model computes the conditional probability P (T |S) as follows: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−p+1 , si ) (1) where φ(xn ) defines the transformations of xn"
D15-1147,N13-1074,0,0.197071,"It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Ma"
D15-1147,P13-1082,0,0.126125,"n (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural network framework. In contrast to previous work, we perform it at the (bilingual) ngram level, where n is sufficiently large to capture long-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data witho"
D15-1147,C14-1105,0,0.147949,"3). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural network framework. In contrast to previous work, we perform it at the (bilingual) ngram level, where n is sufficiently large to capture long-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability"
D15-1147,E12-1055,0,0.100976,"ss useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014),"
D15-1147,D09-1074,0,0.161736,"ice overload”. The sense of the Arabic phrase taken from out-domain data completely changes the meaning of the sentence. In this paper, we tackle this problem by proposing domain adaptation models that make use of all the data while preserving the in-domain preferences. A significant amount of research has been carried out recently in domain adaptation. The complexity of the SMT pipeline, starting from corpus preparation to word-alignment, and then training a wide range of models opens a wide horizon to carry out domain specific adaptations. This is typically done using either data selection (Matsoukas et al., 2009) or model adaptation (Foster and Kuhn, 2007). In this paper, we further research in model adaptation using the neural network framework. In recent years, there has been a growing interest in deep neural networks (NNs) and word embeddings with application to numerous NLP problems. A notably successful attempt on the SMT frontier was recently made by Devlin et al. (2014). They proposed a neural network joint model (NNJM), which augments streams of source with target n-grams and learns a NN model over vector representation of such streams. The model is then integrated into the decoder and used as"
D15-1147,P13-1045,0,0.0308553,"s the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T , the NNJM model computes the conditional probability P (T |S) as follows: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−p+1 , si ) (1) where φ(xn ) defines the transformations of xn through the hidden layers, and wk are the weights from the last hidd"
D15-1147,N13-1090,0,0.072173,"al network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T , the NNJM model computes the conditional probability P (T |S) as follows: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−p+1 , si ) (1) where φ(xn ) defines the transformations of xn through the hidden layers, and wk are the weig"
D15-1147,D13-1140,0,0.283823,"(yn = k) is an indicator variable (i.e., ynk =1 when yn =k, otherwise 0). Optimization is performed using first-order online methods, such as stochastic gradient ascent (SGA) with standard backpropagation algorithm. Unfortunately, training NNLMs are impractically slow because for each training instance (xn , yn ), the softmax output layer (see Equation 2) needs to compute a summation over all words in the output vocabulary.2 Noise contrastive estimation or NCE (Gutmann and Hyv¨arinen, 2010) provides an efficient and stable way to avoid this repetitive computation as recently applied to NNLMs (Vaswani et al., 2013; Mnih and Teh, 2012). We can re-write Equation 2 as follows: P (yn = k|xn , θ) = σ(yn = k|xn , θ) Z(φ(xn ), W) (4) where σ(.) is the un-normalized score and Z(.) is the normalization factor. In NCE, we consider 2 This would take few weeks for a modern CPU machine to train a single NNJM model on the whole data. 1261 Look-up layer Hidden layer Output layer U Source token 1 W Source token 2 C π yn Source token 3 Target token 1 ψ ynm Target token 2 xn M φ(xn ) Figure 1: A simplified neural network joint model with noise contrastive loss, where we use 3-gram target words (i.e., 2-words history) an"
D15-1147,P10-2041,0,0.0346668,"rrelevant training instances, which when included in an MT system, hurts its performance. The idea is to score the outdomain data using a model trained from the indomain data and apply a cut-off based on the resulting scores. The MT system can then be trained on a subset of the out-domain data that is closer to in-domain. Selection based methods can be helpful to reduce computational cost when training is expensive and also when memory is constrained. Data selection was done earlier for language modeling using information retrieval techniques (Hildebrand et al., 2005) and perplexity measures (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source- and target-side language models. Duh et al. (2013) used a recurrent neural language model instead of an ngram-based language model to do the same. Translation model features were used recently by (Liu et al., 2014; Hoang and Sima’an, 2014) for data selection. Durrani et al. (2015a) performed data selection using operation sequence model (OSM) and NNJM models. 2.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time c"
D15-1147,D09-1141,0,0.149564,"cut-off threshold is a time consuming process. An alternative to completely filtering out less useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not l"
D15-1147,P02-1040,0,0.103895,"he cross entropy by regularizing the loss function with respect to the in-domain model. The regularizer gives higher weight to the training instances that are similar to the in-domain data. Our second model takes a more conservative approach by additionally penalizing data instances similar to the out-domain data. We evaluate our models on the standard task of translating Arabic-English and English-German language pairs. Our adapted models achieve better perplexities (Chen and Goodman, 1999) than the models trained on in- and in+out-domain data. Improvements are also reflected in BLEU scores (Papineni et al., 2002) as we compare these models within the SMT pipeline. We obtain gains of up to 0.5 and 0.6 on Arabic-English and EnglishGerman pairs over a competitive baseline system. The remainder of this paper is organized as follows: Section 2 gives an account on related work. Section 3 revisits NNJM model and Section 4 discusses our models. Section 5 presents the experimental setup and the results. Section 6 concludes. 2 Related Work Previous work on domain adaptation in MT can be broken down broadly into two main categories namely data selection and model adaptation. 2.1 Data Selection Data selection has"
D15-1147,P13-2071,1,\N,Missing
D15-1147,P11-1105,1,\N,Missing
D15-1147,J15-2001,1,\N,Missing
D15-1147,W13-2212,1,\N,Missing
D15-1147,W14-3302,0,\N,Missing
D19-3037,W12-2301,0,0.0839234,"Missing"
D19-3037,N07-2014,0,0.106619,"Missing"
D19-3037,N16-3003,1,0.913237,"two varieties of Dialectal Arabic (DA), namely Moroccan (MA) and Tunisian (TN). Our system beats all previously reported SOTA results for the aforementioned varieties of Arabic. The underlying approach treats diacritic recovery as a translation problem, where a sequential encoder and a sequential decoder are employed with undiacritized characters as input and diacritized characters as output. The system is composed of four main componenets, namely: 1) a web application that efficiently handles concurrent user diacritization requests; 2) a text tokenization and cleaning module based on Farasa (Abdelali et al., 2016), a SOTA Arabic NLP toolkit; 3) Arabic variety identifier based on a fastText (Joulin et al., 2016), a deep learning classification toolkit, to properly ascertain the appropriate diacritization model; and 4) a Neural Machine Translation (NMT) based architecture, based on OpenNMT (Klein et al., 2017), to translate sequences of undiacritized characters to diacritized sequences. The contributions in this paper are: Short vowels, aka diacritics, are more often omitted when writing different varieties of Arabic including Modern Standard Arabic (MSA), Classical Arabic (CA), and Dialectal Arabic (DA)"
D19-3037,W17-1305,0,0.0467909,"Missing"
D19-3037,P17-4012,0,0.0277426,"er are employed with undiacritized characters as input and diacritized characters as output. The system is composed of four main componenets, namely: 1) a web application that efficiently handles concurrent user diacritization requests; 2) a text tokenization and cleaning module based on Farasa (Abdelali et al., 2016), a SOTA Arabic NLP toolkit; 3) Arabic variety identifier based on a fastText (Joulin et al., 2016), a deep learning classification toolkit, to properly ascertain the appropriate diacritization model; and 4) a Neural Machine Translation (NMT) based architecture, based on OpenNMT (Klein et al., 2017), to translate sequences of undiacritized characters to diacritized sequences. The contributions in this paper are: Short vowels, aka diacritics, are more often omitted when writing different varieties of Arabic including Modern Standard Arabic (MSA), Classical Arabic (CA), and Dialectal Arabic (DA). However, diacritics are required to properly pronounce words, which makes diacritic restoration (a.k.a. diacritization) essential for language learning and text-to-speech applications. In this paper, we present a system for diacritizing MSA, CA, and two varieties of DA, namely Moroccan and Tunisia"
D19-3037,W18-2507,0,0.0275433,"Missing"
D19-3037,D15-1274,0,0.0528553,"Missing"
D19-3037,D17-1151,0,0.0215157,"Missing"
D19-3037,N19-1248,1,0.599941,"ieties of Arabic Hamdy Mubarak Ahmed Abdelali Kareem Darwish Mohamed Eldesouki Younes Samih Hassan Sajjad {hmubarak,aabdelali}@qf.org.qa Qatar Computing Research Institute, HBKU Research Complex, Doha 5825, Qatar Abstract the diacritics, a prerequisite for Language Learning (Asadi, 2017) and Text to Speech (Sherif, 2018) among other applications. In this paper, we present a system that employs a character-based sequence-to-sequence model (seq2seq) (Britz et al., 2017; Cho et al., 2014; Kuchaiev et al., 2018) for diacritizing four different varieties of Arabic. We use the approach described by Mubarak et al. (2019), which they applied to MSA only, to build a system that effectively diacritizes MSA, CA, and and two varieties of Dialectal Arabic (DA), namely Moroccan (MA) and Tunisian (TN). Our system beats all previously reported SOTA results for the aforementioned varieties of Arabic. The underlying approach treats diacritic recovery as a translation problem, where a sequential encoder and a sequential decoder are employed with undiacritized characters as input and diacritized characters as output. The system is composed of four main componenets, namely: 1) a web application that efficiently handles con"
D19-3037,pasha-etal-2014-madamira,0,0.0626661,"Missing"
D19-3037,W17-1302,1,0.833503,"Missing"
D19-3037,W04-1612,0,0.189762,"Missing"
D19-3037,W02-0504,0,0.274384,"Missing"
E09-1079,H92-1022,0,0.208116,"Missing"
E09-1079,A88-1019,0,0.79281,"Missing"
E09-1079,P89-1015,0,0.431466,"Missing"
E09-1079,J93-2004,0,0.0449137,"kers, Urdu has noThere are various questions that need to be answered during the design of a tagset. The granularity of the tagset is the first problem in this regard. A tagset may consist either of general parts of speech only or it may consist of additional morpho-syntactic categories such as number, gender and case. In order to facilitate the tagger training and to reduce the lexical and syntactic ambiguity, we decided to concentrate on the syntactic categories of the language. Purely syntactic categories lead to a smaller number of tags which also improves the accuracy of manual tagging2 (Marcus et al., 1993). Urdu is influenced from Arabic, and can be considered as having three main parts of speech, namely noun, verb and particle (Platts, 1909; Javed, 1981; Haq, 1987). However, some grammarians proposed ten main parts of speech for Urdu (Schmidt, 1999). The work of Urdu grammar writers provides a full overview of all the features of the language. However, in the perspective of the tagset, their analysis is lacking the computational grounds. The semantic, morphological and syntactic categories are mixed in their distribution of parts of speech. For example, Haq (1987) divides the common nouns into"
E09-1079,C94-1027,1,0.754708,"Missing"
E09-1079,C08-1098,1,\N,Missing
E09-1079,A00-1031,0,\N,Missing
E09-1079,gimenez-marquez-2004-svmtool,0,\N,Missing
E14-4029,P13-2071,1,0.630639,"Missing"
E14-4029,P02-1051,0,0.0983573,"s to integrate transliterations, we observed improvements from 0.23-0.75 (∆ 0.41) BLEU points across 7 language pairs. We also show that our mined transliteration corpora provide better rule coverage and translation quality compared to the gold standard transliteration corpora. 1 Introduction All machine translation (MT) systems suffer from the existence of out-of-vocabulary (OOV) words, irrespective of the amount of data available for training. OOV words are mostly named entities, technical terms or foreign words that can be translated to the target language using transliteration. Much work (Al-Onaizan and Knight, 2002; Zhao et al., 2007; Kashani et al., 2007; Habash, 2009) has been done on transliterating named entities and OOVs, and transliteration has been shown to improve MT quality. Transliteration has also shown to be useful for translating closely related language pairs (Durrani et al., 2010; Nakov and Tiedemann, 2012), and for disambiguation (Hermjakob et al., 2008; Azab et al., 2013). However, despite its utility, a transliteration module does not exist in the commonly used MT toolkits, such as Moses (Koehn et al., 2007). One of the main reasons is that the training data, a corpus of transliteratio"
E14-4029,W13-2212,1,0.618295,"Missing"
E14-4029,P08-2014,0,0.0181782,"by tightening the mining threshold probability. However, our end goal is to improve end-to-end MT and not the transliteration system. We observed that recall is more important than precision for overall MT quality. We provide an empirical justification for this when discussing the final experiments. 3 Method 3 is desirable in cases where the decoder can translate or transliterate a word. For example Hindi word can be translated to “Border” and also transliterated to name “Seema”. Identifying such candidates that can be translated or transliterated is a challenge. Machine learning techniques (Goldwasser and Roth, 2008; Kirschenbaum and Wintner, 2009) and named entity recognizers (Klementiev and Roth, 2006; Hermjakob et al., 2008) have been used for this purpose. Though, we only focus on OOV words, method 3 can be used if such a classifier/NE tagger is available. 4 Arabic and Urdu are segmented using MADA (Habash and Sadat, 2006) and UWS (Durrani and Hussain, 2010). 5 Retuning the transliteration features was not helpful, default weights are used. 6 M-N/1-N alignments are less likely to be transliterations. 150 MT Experiments: Table 3 gives a comprehensive evaluation of the three methods of integration disc"
E14-4029,N06-2013,0,0.0489938,"irable in cases where the decoder can translate or transliterate a word. For example Hindi word can be translated to “Border” and also transliterated to name “Seema”. Identifying such candidates that can be translated or transliterated is a challenge. Machine learning techniques (Goldwasser and Roth, 2008; Kirschenbaum and Wintner, 2009) and named entity recognizers (Klementiev and Roth, 2006; Hermjakob et al., 2008) have been used for this purpose. Though, we only focus on OOV words, method 3 can be used if such a classifier/NE tagger is available. 4 Arabic and Urdu are segmented using MADA (Habash and Sadat, 2006) and UWS (Durrani and Hussain, 2010). 5 Retuning the transliteration features was not helpful, default weights are used. 6 M-N/1-N alignments are less likely to be transliterations. 150 MT Experiments: Table 3 gives a comprehensive evaluation of the three methods of integration discussed in Section 4 along with the number7 of OOV words (types) in different tests. We report BLEU gains (Papineni et al., 2002) obtained by each method. Method 1 (M1 ), that replaces OOV words with 1-best transliteration gave an average improvement of +0.13. This result can be attributed to the low precision of the"
E14-4029,2012.eamt-1.60,0,0.0120164,"ase tables and back-off models. As in method 2, we also use the LM-OOV feature in method 3.3 5 ized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, 100-best translation options, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), and the no-reordering-overpunctuation heuristic. We tuned with the k-best batch MIRA (Cherry and Foster, 2012).5 Evaluation Data: We experimented with 7 language pairs, namely: Arabic, Bengali, Farsi, Hindi, Russian, Telugu and Urdu-into-English. For Arabic4 and Farsi, we used the TED talks data (Cettolo et al., 2012) made available for IWSLT-13, and we used the dev2010 set for tuning and the test2011 and test2012 sets for evaluation. For Indian languages we used the Indic multi-parallel corpus (Post et al., 2012), and we used the dev and test sets provided with the parallel corpus. For Russian, we used WMT-13 data (Bojar et al., 2013), and we used half of the news-test2012 for tuning and other half for testing. We also evaluated on the newstest2013 set. For all, we trained the language model using the monolingual WMT-13 data. See Table 1 for data statistics. Lang AR BN FA HI RU TE UR Transliteration Miner"
E14-4029,2012.iwslt-papers.17,1,0.77685,"sing step. The transliteration model learns character alignment using expectation maximization (EM). See Sajjad et al. (2012) for more details. 1 Mining algorithm also makes this assumption. Tuning data is subtracted from the training corpus while tuning to avoid over-fitting. After the weights are tuned, we add it back, retrain GIZA, and estimate new models. 2 149 use the decoding-graph-backoff option in Moses, that allows multiple translation phrase tables and back-off models. As in method 2, we also use the LM-OOV feature in method 3.3 5 ized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, 100-best translation options, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), and the no-reordering-overpunctuation heuristic. We tuned with the k-best batch MIRA (Cherry and Foster, 2012).5 Evaluation Data: We experimented with 7 language pairs, namely: Arabic, Bengali, Farsi, Hindi, Russian, Telugu and Urdu-into-English. For Arabic4 and Farsi, we used the TED talks data (Cettolo et al., 2012) made available for IWSLT-13, and we used the dev2010 set for tuning and the test2011 and test2012 sets for evaluation. For Indian languages we use"
E14-4029,N12-1047,0,0.0529277,"s while tuning to avoid over-fitting. After the weights are tuned, we add it back, retrain GIZA, and estimate new models. 2 149 use the decoding-graph-backoff option in Moses, that allows multiple translation phrase tables and back-off models. As in method 2, we also use the LM-OOV feature in method 3.3 5 ized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, 100-best translation options, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), and the no-reordering-overpunctuation heuristic. We tuned with the k-best batch MIRA (Cherry and Foster, 2012).5 Evaluation Data: We experimented with 7 language pairs, namely: Arabic, Bengali, Farsi, Hindi, Russian, Telugu and Urdu-into-English. For Arabic4 and Farsi, we used the TED talks data (Cettolo et al., 2012) made available for IWSLT-13, and we used the dev2010 set for tuning and the test2011 and test2012 sets for evaluation. For Indian languages we used the Indic multi-parallel corpus (Post et al., 2012), and we used the dev and test sets provided with the parallel corpus. For Russian, we used WMT-13 data (Bojar et al., 2013), and we used half of the news-test2012 for tuning and other half f"
E14-4029,W11-2123,0,0.0165735,"Missing"
E14-4029,W10-2407,0,0.0431795,"53, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics 2 Transliteration Mining The main bottleneck in building a transliteration system is the lack of availability of transliteration training pairs. It is, however, fair to assume that any parallel data would contain a reasonable number of transliterated word pairs. Transliteration mining can be used to extract such word pairs from the parallel corpus. Most previous techniques on transliteration mining generally use supervised and semi-supervised methods (Sherif and Kondrak, 2007; Jiampojamarn et al., 2010; Darwish, 2010; Kahki et al., 2012). This constrains the mining solution to language pairs for which training data (seed data) is available. A few researchers proposed unsupervised approaches to mine transliterations (Lee and Choi, 1998; Sajjad et al., 2011; Lin et al., 2011). We adapted the work of Sajjad et al. (2012) as summarized below. i=1 pE (ei ) Integration to Machine Translation Method 1: involves replacing OOVs in the output with the 1-best transliteration. The success of Method 1 is solely contingent on the accuracy of the transliteration model. Also, it ignores context which may lead to incorrec"
E14-4029,P08-1045,0,0.0826147,"Missing"
E14-4029,P07-1019,0,0.0661469,"more details. 1 Mining algorithm also makes this assumption. Tuning data is subtracted from the training corpus while tuning to avoid over-fitting. After the weights are tuned, we add it back, retrain GIZA, and estimate new models. 2 149 use the decoding-graph-backoff option in Moses, that allows multiple translation phrase tables and back-off models. As in method 2, we also use the LM-OOV feature in method 3.3 5 ized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, 100-best translation options, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), and the no-reordering-overpunctuation heuristic. We tuned with the k-best batch MIRA (Cherry and Foster, 2012).5 Evaluation Data: We experimented with 7 language pairs, namely: Arabic, Bengali, Farsi, Hindi, Russian, Telugu and Urdu-into-English. For Arabic4 and Farsi, we used the TED talks data (Cettolo et al., 2012) made available for IWSLT-13, and we used the dev2010 set for tuning and the test2011 and test2012 sets for evaluation. For Indian languages we used the Indic multi-parallel corpus (Post et al., 2012), and we used the dev and test sets provided with the parallel corpus. For Russ"
E14-4029,P10-1048,1,0.84313,"n All machine translation (MT) systems suffer from the existence of out-of-vocabulary (OOV) words, irrespective of the amount of data available for training. OOV words are mostly named entities, technical terms or foreign words that can be translated to the target language using transliteration. Much work (Al-Onaizan and Knight, 2002; Zhao et al., 2007; Kashani et al., 2007; Habash, 2009) has been done on transliterating named entities and OOVs, and transliteration has been shown to improve MT quality. Transliteration has also shown to be useful for translating closely related language pairs (Durrani et al., 2010; Nakov and Tiedemann, 2012), and for disambiguation (Hermjakob et al., 2008; Azab et al., 2013). However, despite its utility, a transliteration module does not exist in the commonly used MT toolkits, such as Moses (Koehn et al., 2007). One of the main reasons is that the training data, a corpus of transliteration pairs, required to build a transliteration system, is not readily available for many language pairs. Even if such a training data is available, mechanisms to integrate transliterated words 148 Proceedings of the 14th Conference of the European Chapter of the Association for Computat"
E14-4029,W10-2405,0,0.109169,"Missing"
E14-4029,J03-1002,0,0.0188912,"Missing"
E14-4029,W07-0703,0,0.103886,"rovements from 0.23-0.75 (∆ 0.41) BLEU points across 7 language pairs. We also show that our mined transliteration corpora provide better rule coverage and translation quality compared to the gold standard transliteration corpora. 1 Introduction All machine translation (MT) systems suffer from the existence of out-of-vocabulary (OOV) words, irrespective of the amount of data available for training. OOV words are mostly named entities, technical terms or foreign words that can be translated to the target language using transliteration. Much work (Al-Onaizan and Knight, 2002; Zhao et al., 2007; Kashani et al., 2007; Habash, 2009) has been done on transliterating named entities and OOVs, and transliteration has been shown to improve MT quality. Transliteration has also shown to be useful for translating closely related language pairs (Durrani et al., 2010; Nakov and Tiedemann, 2012), and for disambiguation (Hermjakob et al., 2008; Azab et al., 2013). However, despite its utility, a transliteration module does not exist in the commonly used MT toolkits, such as Moses (Koehn et al., 2007). One of the main reasons is that the training data, a corpus of transliteration pairs, required to build a transliterat"
E14-4029,P02-1040,0,0.100942,"l., 2008) have been used for this purpose. Though, we only focus on OOV words, method 3 can be used if such a classifier/NE tagger is available. 4 Arabic and Urdu are segmented using MADA (Habash and Sadat, 2006) and UWS (Durrani and Hussain, 2010). 5 Retuning the transliteration features was not helpful, default weights are used. 6 M-N/1-N alignments are less likely to be transliterations. 150 MT Experiments: Table 3 gives a comprehensive evaluation of the three methods of integration discussed in Section 4 along with the number7 of OOV words (types) in different tests. We report BLEU gains (Papineni et al., 2002) obtained by each method. Method 1 (M1 ), that replaces OOV words with 1-best transliteration gave an average improvement of +0.13. This result can be attributed to the low precision of the transliteration system (Table 2). Method 2 (M2 ), that transliterates OOVs in second pass monotonic decoding, gave an average improvement of +0.39. Slightly higher gains were obtained using Method 3 (M3 ), that integrates transliteration phrase-table inside decoder on the fly. However, the efficacy of M3 in comparison to M2 is not as apparent, as M2 produced better results than M3 in half of the cases. both"
E14-4029,E09-1050,0,0.0166804,"hreshold probability. However, our end goal is to improve end-to-end MT and not the transliteration system. We observed that recall is more important than precision for overall MT quality. We provide an empirical justification for this when discussing the final experiments. 3 Method 3 is desirable in cases where the decoder can translate or transliterate a word. For example Hindi word can be translated to “Border” and also transliterated to name “Seema”. Identifying such candidates that can be translated or transliterated is a challenge. Machine learning techniques (Goldwasser and Roth, 2008; Kirschenbaum and Wintner, 2009) and named entity recognizers (Klementiev and Roth, 2006; Hermjakob et al., 2008) have been used for this purpose. Though, we only focus on OOV words, method 3 can be used if such a classifier/NE tagger is available. 4 Arabic and Urdu are segmented using MADA (Habash and Sadat, 2006) and UWS (Durrani and Hussain, 2010). 5 Retuning the transliteration features was not helpful, default weights are used. 6 M-N/1-N alignments are less likely to be transliterations. 150 MT Experiments: Table 3 gives a comprehensive evaluation of the three methods of integration discussed in Section 4 along with the"
E14-4029,W12-3152,0,0.0194846,"translation options, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), and the no-reordering-overpunctuation heuristic. We tuned with the k-best batch MIRA (Cherry and Foster, 2012).5 Evaluation Data: We experimented with 7 language pairs, namely: Arabic, Bengali, Farsi, Hindi, Russian, Telugu and Urdu-into-English. For Arabic4 and Farsi, we used the TED talks data (Cettolo et al., 2012) made available for IWSLT-13, and we used the dev2010 set for tuning and the test2011 and test2012 sets for evaluation. For Indian languages we used the Indic multi-parallel corpus (Post et al., 2012), and we used the dev and test sets provided with the parallel corpus. For Russian, we used WMT-13 data (Bojar et al., 2013), and we used half of the news-test2012 for tuning and other half for testing. We also evaluated on the newstest2013 set. For all, we trained the language model using the monolingual WMT-13 data. See Table 1 for data statistics. Lang AR BN FA HI RU TE UR Transliteration Miner: The miner extracts transliterations from a word-aligned parallel corpus. We only used word pairs with 1-to-1 alignments.6 Before feeding the list into the miner, we cleaned it by removing digits, sy"
E14-4029,N06-1011,0,0.03029,"to-end MT and not the transliteration system. We observed that recall is more important than precision for overall MT quality. We provide an empirical justification for this when discussing the final experiments. 3 Method 3 is desirable in cases where the decoder can translate or transliterate a word. For example Hindi word can be translated to “Border” and also transliterated to name “Seema”. Identifying such candidates that can be translated or transliterated is a challenge. Machine learning techniques (Goldwasser and Roth, 2008; Kirschenbaum and Wintner, 2009) and named entity recognizers (Klementiev and Roth, 2006; Hermjakob et al., 2008) have been used for this purpose. Though, we only focus on OOV words, method 3 can be used if such a classifier/NE tagger is available. 4 Arabic and Urdu are segmented using MADA (Habash and Sadat, 2006) and UWS (Durrani and Hussain, 2010). 5 Retuning the transliteration features was not helpful, default weights are used. 6 M-N/1-N alignments are less likely to be transliterations. 150 MT Experiments: Table 3 gives a comprehensive evaluation of the three methods of integration discussed in Section 4 along with the number7 of OOV words (types) in different tests. We rep"
E14-4029,P11-1044,1,0.785964,"t is, however, fair to assume that any parallel data would contain a reasonable number of transliterated word pairs. Transliteration mining can be used to extract such word pairs from the parallel corpus. Most previous techniques on transliteration mining generally use supervised and semi-supervised methods (Sherif and Kondrak, 2007; Jiampojamarn et al., 2010; Darwish, 2010; Kahki et al., 2012). This constrains the mining solution to language pairs for which training data (seed data) is available. A few researchers proposed unsupervised approaches to mine transliterations (Lee and Choi, 1998; Sajjad et al., 2011; Lin et al., 2011). We adapted the work of Sajjad et al. (2012) as summarized below. i=1 pE (ei ) Integration to Machine Translation Method 1: involves replacing OOVs in the output with the 1-best transliteration. The success of Method 1 is solely contingent on the accuracy of the transliteration model. Also, it ignores context which may lead to incorrect transliteration. For example, the Arabic word transliterates to “Bill” when followed by “Clinton” and “Bell” if preceded by “Alexander Graham”. Method 2: provides n-best transliterations to a monotonic decoder that uses a monolingual languag"
E14-4029,P12-1049,1,0.900033,"ain a reasonable number of transliterated word pairs. Transliteration mining can be used to extract such word pairs from the parallel corpus. Most previous techniques on transliteration mining generally use supervised and semi-supervised methods (Sherif and Kondrak, 2007; Jiampojamarn et al., 2010; Darwish, 2010; Kahki et al., 2012). This constrains the mining solution to language pairs for which training data (seed data) is available. A few researchers proposed unsupervised approaches to mine transliterations (Lee and Choi, 1998; Sajjad et al., 2011; Lin et al., 2011). We adapted the work of Sajjad et al. (2012) as summarized below. i=1 pE (ei ) Integration to Machine Translation Method 1: involves replacing OOVs in the output with the 1-best transliteration. The success of Method 1 is solely contingent on the accuracy of the transliteration model. Also, it ignores context which may lead to incorrect transliteration. For example, the Arabic word transliterates to “Bill” when followed by “Clinton” and “Bell” if preceded by “Alexander Graham”. Method 2: provides n-best transliterations to a monotonic decoder that uses a monolingual language model and a transliteration phrasetranslation table to rescore"
E14-4029,P07-1109,0,0.0454649,"ssociation for Computational Linguistics, pages 148–153, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics 2 Transliteration Mining The main bottleneck in building a transliteration system is the lack of availability of transliteration training pairs. It is, however, fair to assume that any parallel data would contain a reasonable number of transliterated word pairs. Transliteration mining can be used to extract such word pairs from the parallel corpus. Most previous techniques on transliteration mining generally use supervised and semi-supervised methods (Sherif and Kondrak, 2007; Jiampojamarn et al., 2010; Darwish, 2010; Kahki et al., 2012). This constrains the mining solution to language pairs for which training data (seed data) is available. A few researchers proposed unsupervised approaches to mine transliterations (Lee and Choi, 1998; Sajjad et al., 2011; Lin et al., 2011). We adapted the work of Sajjad et al. (2012) as summarized below. i=1 pE (ei ) Integration to Machine Translation Method 1: involves replacing OOVs in the output with the 1-best transliteration. The success of Method 1 is solely contingent on the accuracy of the transliteration model. Also, it"
E14-4029,N04-1022,0,0.0441673,"ion (EM). See Sajjad et al. (2012) for more details. 1 Mining algorithm also makes this assumption. Tuning data is subtracted from the training corpus while tuning to avoid over-fitting. After the weights are tuned, we add it back, retrain GIZA, and estimate new models. 2 149 use the decoding-graph-backoff option in Moses, that allows multiple translation phrase tables and back-off models. As in method 2, we also use the LM-OOV feature in method 3.3 5 ized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, 100-best translation options, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), and the no-reordering-overpunctuation heuristic. We tuned with the k-best batch MIRA (Cherry and Foster, 2012).5 Evaluation Data: We experimented with 7 language pairs, namely: Arabic, Bengali, Farsi, Hindi, Russian, Telugu and Urdu-into-English. For Arabic4 and Farsi, we used the TED talks data (Cettolo et al., 2012) made available for IWSLT-13, and we used the dev2010 set for tuning and the test2011 and test2012 sets for evaluation. For Indian languages we used the Indic multi-parallel corpus (Post et al., 2012), and we used the dev and test sets prov"
E14-4029,N07-1046,0,0.130438,"ns, we observed improvements from 0.23-0.75 (∆ 0.41) BLEU points across 7 language pairs. We also show that our mined transliteration corpora provide better rule coverage and translation quality compared to the gold standard transliteration corpora. 1 Introduction All machine translation (MT) systems suffer from the existence of out-of-vocabulary (OOV) words, irrespective of the amount of data available for training. OOV words are mostly named entities, technical terms or foreign words that can be translated to the target language using transliteration. Much work (Al-Onaizan and Knight, 2002; Zhao et al., 2007; Kashani et al., 2007; Habash, 2009) has been done on transliterating named entities and OOVs, and transliteration has been shown to improve MT quality. Transliteration has also shown to be useful for translating closely related language pairs (Durrani et al., 2010; Nakov and Tiedemann, 2012), and for disambiguation (Hermjakob et al., 2008; Azab et al., 2013). However, despite its utility, a transliteration module does not exist in the commonly used MT toolkits, such as Moses (Koehn et al., 2007). One of the main reasons is that the training data, a corpus of transliteration pairs, required t"
E14-4029,W10-2404,0,0.0966661,"Missing"
E14-4029,W11-2206,0,0.0183811,"o assume that any parallel data would contain a reasonable number of transliterated word pairs. Transliteration mining can be used to extract such word pairs from the parallel corpus. Most previous techniques on transliteration mining generally use supervised and semi-supervised methods (Sherif and Kondrak, 2007; Jiampojamarn et al., 2010; Darwish, 2010; Kahki et al., 2012). This constrains the mining solution to language pairs for which training data (seed data) is available. A few researchers proposed unsupervised approaches to mine transliterations (Lee and Choi, 1998; Sajjad et al., 2011; Lin et al., 2011). We adapted the work of Sajjad et al. (2012) as summarized below. i=1 pE (ei ) Integration to Machine Translation Method 1: involves replacing OOVs in the output with the 1-best transliteration. The success of Method 1 is solely contingent on the accuracy of the transliteration model. Also, it ignores context which may lead to incorrect transliteration. For example, the Arabic word transliterates to “Bill” when followed by “Clinton” and “Bell” if preceded by “Alexander Graham”. Method 2: provides n-best transliterations to a monotonic decoder that uses a monolingual language model and a trans"
E14-4029,N10-1077,1,\N,Missing
E14-4029,N12-1025,0,\N,Missing
E14-4029,P12-2059,0,\N,Missing
E14-4029,P07-2045,1,\N,Missing
E14-4029,N13-1046,0,\N,Missing
E14-4029,W13-2201,1,\N,Missing
E14-4029,2013.iwslt-evaluation.3,1,\N,Missing
E17-3016,E14-4029,1,0.83174,"f the systems.3 The results shown in Figure 3 depict the significant time gain we achieved using the pruned phrase based system. However, with a 5 BLEU point difference in translation quality, we decided to compromise and use the slower NMTCPU in our final demo. We also allow the user to switch to the phrase-based system, if translation speed is more important. We did not use NMTGPU since it is very costly to put into production with its requirement for a dedicated GPU card. Finally, we added a customized dictionary and translated unknown words by transliterating them in a post-decoding step (Durrani et al., 2014). PB-Pruned: The PB-best system is not suitable for real time translation and has high memory requirements. To increase the efficiency, we dropped the OSM and NNJM features, heavily pruned the language model and used MML-filtering to select a subset of training data. The resulting system was trained on 1.2 M sentences, 10 times less the original data. 2.4 NMT-GPU: This is our best system2 that we submitted to the IWSLT’16 campaign (Durrani et al., 2016). The advantage of Neural models is that their size does not scale linearly with the data, and hence we were able to train using all available"
E17-3016,W16-2323,0,0.0666475,"Missing"
E17-3016,P14-1129,0,0.018282,"eamlessly switch between them. We had four systems to choose from for our demo, two of which were Phrase-based systems, and the two were Neural MT systems trained using Nematus (Sennrich et al., 2016). Figure 3: Performance and Translation speed of various MT systems PB-Best: This is a competition-grade phrasebased system, also used for our participation at the IWSLT’16 campaign (Durrani et al., 2016). It was trained using all the freely available ArabicEnglish data with state-of-the-art features such as a large language model, lexical reordering, OSM (Durrani et al., 2011) and NNJM features (Devlin et al., 2014). We also computed the translation speed of each of the systems.3 The results shown in Figure 3 depict the significant time gain we achieved using the pruned phrase based system. However, with a 5 BLEU point difference in translation quality, we decided to compromise and use the slower NMTCPU in our final demo. We also allow the user to switch to the phrase-based system, if translation speed is more important. We did not use NMTGPU since it is very costly to put into production with its requirement for a dedicated GPU card. Finally, we added a customized dictionary and translated unknown words"
E17-3016,P11-1105,1,\N,Missing
E17-3029,P09-1039,0,0.0167833,"Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 Knowledge Base Construction This component provides a knowledge base of factual relations between entities, built with a model based on Universal Schemas (Riedel et al., 2013), a low-rank matrix factorization approach.The entity relations are extracted jointly across multiple languages, with entities pairs as rows and a set of structured relations and textual patterns as columns. The relations provide information about how various entities present in n"
E17-3029,N13-1008,1,0.77865,"Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 Knowledge Base Construction This component provides a knowledge base of factual relations between entities, built with a model based on Universal Schemas (Riedel et al., 2013), a low-rank matrix factorization approach.The entity relations are extracted jointly across multiple languages, with entities pairs as rows and a set of structured relations and textual patterns as columns. The relations provide information about how various entities present in news https://github.com/andre-martins/ TurboParser 118 Marcin Junczys-Dowmunt, Tomasz Dwojak, and Hieu Hoang. 2016. Is neural machine translation ready for deployment? A case study on 30 translation directions. CoRR, abs/1610.01108. documents are connected. 2.9 Storyline Construction and Summarization Storylines are co"
E17-3029,E17-3017,1,0.751559,"m a multilingual corpus of nearly 600k documents in 8 of the 9 SUMMA languages (all except Latvian), which were manually annotated by journalists at Deutsche Welle. The document model is a hierarchical attention network with attention at each level of the hierarchy, inspired by Yang et al. (2016), followed by a sigmoid classification layer. 2.4 Machine Translation 2.7 Deep Semantic Tagging The lingua franca within SUMMA is English. Machine translation based on neural networks is used to translate content into English automatically. The back-end MT systems are trained with the Nematus Toolkit (Sennrich et al., 2017); translation is performed with AmuNMT (Junczys-Dowmunt et al., 2016). The system also has a component that performs semantic parsing into Abstract Meaning Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is"
E17-3029,P13-1020,0,0.025126,"Missing"
E17-3029,E17-1051,1,0.815701,"lassification layer. 2.4 Machine Translation 2.7 Deep Semantic Tagging The lingua franca within SUMMA is English. Machine translation based on neural networks is used to translate content into English automatically. The back-end MT systems are trained with the Nematus Toolkit (Sennrich et al., 2017); translation is performed with AmuNMT (Junczys-Dowmunt et al., 2016). The system also has a component that performs semantic parsing into Abstract Meaning Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 K"
I11-1015,C08-1068,0,0.517934,"ration. They build a conditional probability model. The graphemebased model performs better than the phonemebased model and the hybrid model. This motivates our use of grapheme-based models. In this paper, we use a grapheme-based approach for transliteration from Hindi to Urdu. The phoneme-based approach would involve the conversion of Hindi and Urdu text into a phonemic representation which is not a trivial task as the short vowel ‘a’ is not written in Hindi text and no short vowels are written in Urdu text. The difficulty of this additional step would be likely to lead to additional errors. Malik et al. (2008) and Malik et al. (2009) work on transliteration from Hindi to Urdu and Urdu to Hindi respectively. They use the rules of SAMPA (Speech Assessment Methods Pho3 Extraction of Transliteration Pairs We automatically word-align the parallel corpus and extract a word list, later referred to as “list of word pairs“ (see Section 5, for details on training data). We use two methods to extract transliteration pairs from the list of word pairs. In the first 1 SAMPA and XSAMPA are used to represent the IPA symbols using 7-bit printable ASCII characters. 130 iteration which best predicts the held-out data"
I11-1015,W09-3536,0,0.117608,"ditional probability model. The graphemebased model performs better than the phonemebased model and the hybrid model. This motivates our use of grapheme-based models. In this paper, we use a grapheme-based approach for transliteration from Hindi to Urdu. The phoneme-based approach would involve the conversion of Hindi and Urdu text into a phonemic representation which is not a trivial task as the short vowel ‘a’ is not written in Hindi text and no short vowels are written in Urdu text. The difficulty of this additional step would be likely to lead to additional errors. Malik et al. (2008) and Malik et al. (2009) work on transliteration from Hindi to Urdu and Urdu to Hindi respectively. They use the rules of SAMPA (Speech Assessment Methods Pho3 Extraction of Transliteration Pairs We automatically word-align the parallel corpus and extract a word list, later referred to as “list of word pairs“ (see Section 5, for details on training data). We use two methods to extract transliteration pairs from the list of word pairs. In the first 1 SAMPA and XSAMPA are used to represent the IPA symbols using 7-bit printable ASCII characters. 130 iteration which best predicts the held-out data is selected as the stop"
I11-1015,P10-1048,1,0.879527,"Missing"
I11-1015,J03-1002,0,0.0156683,"Missing"
I11-1015,P09-1016,0,0.0358459,"Missing"
I11-1015,P06-2025,0,0.254507,"Missing"
I11-1015,P11-1044,1,0.904408,"In this paper, the term transliteration pair refers to a word pair where the words are transliterations of each other and the term transliteration unit refers to a character pair where the characters are transliterations of each other. We are interested in building joint source channel models for transliteration. Because we do not have a list of transliteration pairs to use as training data in building such a transliteration model, we use two methods to extract the list of transliteration pairs from a parallel corpus of Hindi/Urdu. The first method uses the transliteration mining algorithm of Sajjad et al. (2011) to automatically extract transliteration pairs. This approach does not use any language specific knowledge. The second method uses handcrafted transliteration rules specific to the mapping between Hindi and Urdu to extract transliteration pairs. We automatically align the two lists of extracted transliteration pairs at the character level and learn two transliteration models. We compare the results with three other transliteration systems. Both of our transliteration systems perform better than the other systems. The 1-best output of the transliteration system built on the list extracted usin"
I11-1015,P08-1045,0,0.30392,"Missing"
I11-1015,W98-1005,0,0.0866721,"n pairs. This method does not use any language dependent information. In the second approach, we use a rule-based method to extract transliteration pairs. Both processes are imperfect, meaning that there is noise in the extracted list of transliteration pairs. We build a joint source channel model as described by Li et al. (2004) and Ekbal et al. (2006) on the extracted list of transliteration pairs. The following sections describe the two mining approaches and the model in detail. Previous Work Transliteration can be done with phoneme-based or grapheme-based models. Knight and Graehl (1998), Stalls and Knight (1998), Al-Onaizan and Knight (2002) and Pervouchine et al. (2009) use the phoneme-based approach for transliteration. Kashani et al. (2007) and Al-Onaizan and Knight (2002) use a grapheme-based model to transliterate from Arabic into English. Al-Onaizan and Knight (2002) compare a grapheme-based approach, a phoneme-based approach and a linear combination of both for transliteration. They build a conditional probability model. The graphemebased model performs better than the phonemebased model and the hybrid model. This motivates our use of grapheme-based models. In this paper, we use a grapheme-bas"
I11-1015,N03-1017,0,0.0309845,"Missing"
I11-1015,P04-1021,0,0.16253,"Missing"
I11-1015,N10-1077,1,\N,Missing
I11-1015,W02-0505,0,\N,Missing
I11-1015,J98-4003,0,\N,Missing
I17-1001,P17-1080,1,0.491617,"ing Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks Yonatan Belinkov1 Llu´ıs M`arquez2 Hassan Sajjad2 Nadir Durrani2 Fahim Dalvi2 James Glass1 1 MIT Computer Science and Artificial Intelligence Laboratory, Cambridge, MA 02139, USA {belinkov, glass}@mit.edu 2 Qatar Computing Research Institute, HBKU, Doha, Qatar {lmarquez, hsajjad, ndurrani, faimaduddin}@qf.org.qa Abstract One observation that has been made is that lower layers in the neural MT network learn different kinds of information than higher layers. For example, Shi et al. (2016) and Belinkov et al. (2017) found that representations from lower layers of the NMT encoder are more predictive of word-level linguistic properties like part-ofspeech (POS) and morphological tags, whereas higher layer representations are more predictive of more global syntactic information. In this work, we take a first step towards understanding what NMT models learn about semantics. We evaluate NMT representations from different layers on a semantic tagging task and compare to the results on a POS tagging task. We believe that understanding the semantics learned in NMT can facilitate using semantic information for imp"
I17-1001,D17-1151,0,0.0125781,"al., 2017; Belinkov and Glass, 2017; Wang et al., 2017) and language processing models (K¨ohn, 2015; Qian et al., 2016a; Adi et al., 2016; Linzen et al., 2016; Qian et al., 2016b). Methodologically, our work is most similar to Shi et al. (2016) and Belinkov et al. (2017), who also used hidden vectors from neural MT models to predict linguistic properties. However, they focused on relatively low-level tasks (syntax and morphology, respectively), while we apply the approach to a semantic task and compare the results with a POS tagging task. Shallower MT models In comparing network depth in NMT, Britz et al. (2017) found that encoders with 2 to 4 layers performed the best. For completeness, we report here results using features extracted from models trained originally with 2 and 3 layers, in addition to our basic setting of 4 layers. Table 6 shows consistent trends with our previous observations: POS tagging does not benefit from upper layers, while SEM tagging does, although the improvement is rather small in the shallower models. 5 0 Related Work Techniques for analyzing neural network models include visualization of hidden units (Elman, 1991; Karpathy et al., 2015; K´ad´ar et al., 2016; Qian et al.,"
I17-1001,E17-2039,0,0.0170778,"Missing"
I17-1001,P07-1005,0,0.0220979,"coder are more predictive of word-level linguistic properties like part-ofspeech (POS) and morphological tags, whereas higher layer representations are more predictive of more global syntactic information. In this work, we take a first step towards understanding what NMT models learn about semantics. We evaluate NMT representations from different layers on a semantic tagging task and compare to the results on a POS tagging task. We believe that understanding the semantics learned in NMT can facilitate using semantic information for improving NMT systems, as previously shown for non-neural MT (Chan et al., 2007; Liu and Gildea, 2010; Gao and Vogel, 2011; Wu et al., 2011; Jones et al., 2012; Bazrafshan and Gildea, 2013, 2014). For the semantic (SEM) tagging task, we use the dataset recently introduced by Bjerva et al. (2016). This is a lexical semantics task: given a sentence, the goal is to assign to each word a tag representing a semantic class. The classes capture nuanced meanings that are ignored in most POS tag schemes. For instance, proximal and distal demonstratives (e.g., this and that) are typically assigned the same POS tag (DT) but receive different SEM tags (PRX and DST, respectively), an"
I17-1001,I17-1015,1,0.421519,"semantics while lower layers tend to be better for part-of-speech tagging. We also observe little effect of the target language on source-side representations, especially in higher quality models.1 1 Introduction Neural machine translation (NMT) offers an elegant end-to-end architecture, while at the same time improving translation quality. However, little is known about the inner workings of these models and their interpretability is limited. Recent work has started exploring what kind of linguistic information such models learn on morphological (Vylomova et al., 2016; Belinkov et al., 2017; Dalvi et al., 2017) and syntactic levels (Shi et al., 2016; Sennrich, 2017). (1) Sarah bought herself a book 1 Our code is available at http://github.com/ boknilev/nmt-repr-analysis. (2) Sarah herself bought a book 1 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 1–10, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP Figure 1: Illustration of our approach, after (Belinkov et al., 2017): (i) NMT system trained on parallel data; (ii) features extracted from pre-trained model; (iii) classifier trained using the extracted features. We train classifiers on e"
I17-1001,K17-1037,0,0.00554294,"ve similar trends as before: POS tagging does not benefit from features from the upper layers, while SEM tagging improves with layer 4 representations. 0 1 2 3 4 4 POS SEM 87.9 81.8 92.0 87.8 91.7 87.4 91.8 87.6 91.9 88.2 3 POS SEM 87.9 81.9 92.5 88.2 92.3 88.0 92.4 88.4 – – 2 POS SEM 87.9 82.0 92.7 88.5 92.7 88.7 – – – – Table 6: POS and SEM tagging accuracy with features from different layers of 2/3/4-layer encoders, averaged over all non-English target languages. tain quantitative correlations between parts of the neural network and linguistic properties, in both speech (Wu and King, 2016; Alishahi et al., 2017; Belinkov and Glass, 2017; Wang et al., 2017) and language processing models (K¨ohn, 2015; Qian et al., 2016a; Adi et al., 2016; Linzen et al., 2016; Qian et al., 2016b). Methodologically, our work is most similar to Shi et al. (2016) and Belinkov et al. (2017), who also used hidden vectors from neural MT models to predict linguistic properties. However, they focused on relatively low-level tasks (syntax and morphology, respectively), while we apply the approach to a semantic task and compare the results with a POS tagging task. Shallower MT models In comparing network depth in NMT, Britz et"
I17-1001,W11-1012,0,0.0281742,"inguistic properties like part-ofspeech (POS) and morphological tags, whereas higher layer representations are more predictive of more global syntactic information. In this work, we take a first step towards understanding what NMT models learn about semantics. We evaluate NMT representations from different layers on a semantic tagging task and compare to the results on a POS tagging task. We believe that understanding the semantics learned in NMT can facilitate using semantic information for improving NMT systems, as previously shown for non-neural MT (Chan et al., 2007; Liu and Gildea, 2010; Gao and Vogel, 2011; Wu et al., 2011; Jones et al., 2012; Bazrafshan and Gildea, 2013, 2014). For the semantic (SEM) tagging task, we use the dataset recently introduced by Bjerva et al. (2016). This is a lexical semantics task: given a sentence, the goal is to assign to each word a tag representing a semantic class. The classes capture nuanced meanings that are ignored in most POS tag schemes. For instance, proximal and distal demonstratives (e.g., this and that) are typically assigned the same POS tag (DT) but receive different SEM tags (PRX and DST, respectively), and proper nouns are assigned different SEM t"
I17-1001,P82-1020,0,0.755314,"Missing"
I17-1001,P13-2074,0,0.0196012,"gical tags, whereas higher layer representations are more predictive of more global syntactic information. In this work, we take a first step towards understanding what NMT models learn about semantics. We evaluate NMT representations from different layers on a semantic tagging task and compare to the results on a POS tagging task. We believe that understanding the semantics learned in NMT can facilitate using semantic information for improving NMT systems, as previously shown for non-neural MT (Chan et al., 2007; Liu and Gildea, 2010; Gao and Vogel, 2011; Wu et al., 2011; Jones et al., 2012; Bazrafshan and Gildea, 2013, 2014). For the semantic (SEM) tagging task, we use the dataset recently introduced by Bjerva et al. (2016). This is a lexical semantics task: given a sentence, the goal is to assign to each word a tag representing a semantic class. The classes capture nuanced meanings that are ignored in most POS tag schemes. For instance, proximal and distal demonstratives (e.g., this and that) are typically assigned the same POS tag (DT) but receive different SEM tags (PRX and DST, respectively), and proper nouns are assigned different SEM tags depending on their type (e.g., geopolitical entity, organizati"
I17-1001,C12-1083,0,0.0154324,"Missing"
I17-1001,P16-1140,0,0.0367,"Missing"
I17-1001,E17-2060,0,0.0165727,"peech tagging. We also observe little effect of the target language on source-side representations, especially in higher quality models.1 1 Introduction Neural machine translation (NMT) offers an elegant end-to-end architecture, while at the same time improving translation quality. However, little is known about the inner workings of these models and their interpretability is limited. Recent work has started exploring what kind of linguistic information such models learn on morphological (Vylomova et al., 2016; Belinkov et al., 2017; Dalvi et al., 2017) and syntactic levels (Shi et al., 2016; Sennrich, 2017). (1) Sarah bought herself a book 1 Our code is available at http://github.com/ boknilev/nmt-repr-analysis. (2) Sarah herself bought a book 1 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 1–10, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP Figure 1: Illustration of our approach, after (Belinkov et al., 2017): (i) NMT system trained on parallel data; (ii) features extracted from pre-trained model; (iii) classifier trained using the extracted features. We train classifiers on either SEM or POS tagging using features from different l"
I17-1001,D16-1159,0,0.531034,"tter for part-of-speech tagging. We also observe little effect of the target language on source-side representations, especially in higher quality models.1 1 Introduction Neural machine translation (NMT) offers an elegant end-to-end architecture, while at the same time improving translation quality. However, little is known about the inner workings of these models and their interpretability is limited. Recent work has started exploring what kind of linguistic information such models learn on morphological (Vylomova et al., 2016; Belinkov et al., 2017; Dalvi et al., 2017) and syntactic levels (Shi et al., 2016; Sennrich, 2017). (1) Sarah bought herself a book 1 Our code is available at http://github.com/ boknilev/nmt-repr-analysis. (2) Sarah herself bought a book 1 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 1–10, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP Figure 1: Illustration of our approach, after (Belinkov et al., 2017): (i) NMT system trained on parallel data; (ii) features extracted from pre-trained model; (iii) classifier trained using the extracted features. We train classifiers on either SEM or POS tagging using features"
I17-1001,D15-1246,0,0.0767367,"Missing"
I17-1001,W17-4115,0,0.0393383,"Missing"
I17-1001,Q16-1037,0,0.0323719,"0 1 2 3 4 4 POS SEM 87.9 81.8 92.0 87.8 91.7 87.4 91.8 87.6 91.9 88.2 3 POS SEM 87.9 81.9 92.5 88.2 92.3 88.0 92.4 88.4 – – 2 POS SEM 87.9 82.0 92.7 88.5 92.7 88.7 – – – – Table 6: POS and SEM tagging accuracy with features from different layers of 2/3/4-layer encoders, averaged over all non-English target languages. tain quantitative correlations between parts of the neural network and linguistic properties, in both speech (Wu and King, 2016; Alishahi et al., 2017; Belinkov and Glass, 2017; Wang et al., 2017) and language processing models (K¨ohn, 2015; Qian et al., 2016a; Adi et al., 2016; Linzen et al., 2016; Qian et al., 2016b). Methodologically, our work is most similar to Shi et al. (2016) and Belinkov et al. (2017), who also used hidden vectors from neural MT models to predict linguistic properties. However, they focused on relatively low-level tasks (syntax and morphology, respectively), while we apply the approach to a semantic task and compare the results with a POS tagging task. Shallower MT models In comparing network depth in NMT, Britz et al. (2017) found that encoders with 2 to 4 layers performed the best. For completeness, we report here results using features extracted from models t"
I17-1001,C10-1081,0,0.031874,"ictive of word-level linguistic properties like part-ofspeech (POS) and morphological tags, whereas higher layer representations are more predictive of more global syntactic information. In this work, we take a first step towards understanding what NMT models learn about semantics. We evaluate NMT representations from different layers on a semantic tagging task and compare to the results on a POS tagging task. We believe that understanding the semantics learned in NMT can facilitate using semantic information for improving NMT systems, as previously shown for non-neural MT (Chan et al., 2007; Liu and Gildea, 2010; Gao and Vogel, 2011; Wu et al., 2011; Jones et al., 2012; Bazrafshan and Gildea, 2013, 2014). For the semantic (SEM) tagging task, we use the dataset recently introduced by Bjerva et al. (2016). This is a lexical semantics task: given a sentence, the goal is to assign to each word a tag representing a semantic class. The classes capture nuanced meanings that are ignored in most POS tag schemes. For instance, proximal and distal demonstratives (e.g., this and that) are typically assigned the same POS tag (DT) but receive different SEM tags (PRX and DST, respectively), and proper nouns are ass"
I17-1001,L16-1561,0,0.00604061,"OS and SEM tags using the features hkj that are obtained from the English encoder and evaluate their accuracies. Figure 1 illustrates the process. • Consistent with previous work, we find that lower layer representations are usually better for POS tagging. However, we also find that representations from higher layers are better at capturing semantics, even though these are word-level labels. This is especially true with tags that are more semantic in nature such as discourse functions or noun concepts. 2 3 3.1 Data and Experimental Setup Data MT We use the fully-aligned United Nations corpus (Ziemski et al., 2016) for training NMT models, which includes 11 million multi-parallel sentences in six languages: Arabic (Ar), Chinese (Zh), English (En), French (Fr), Spanish (Es), and Russian (Ru). We train En-to-* models on the first 2 million sentences of the train set, using the official train/dev/test split. This dataset has the benefit of multiple alignment of the six languages, which allows for comparable cross-linguistic analysis. Note that the parallel dataset is only used for training the NMT model. The classifier is then trained on the supervised data (described next) and all accuracies are reported"
I17-1001,D16-1079,0,0.0193389,"Missing"
I17-1015,I17-1001,1,0.708774,"ng in the Neural Machine Translation Decoder Fahim Dalvi Nadir Durrani Hassan Sajjad Yonatan Belinkov∗ Stephan Vogel Qatar Computing Research Institute – HBKU, Doha, Qatar {faimaduddin, ndurrani, hsajjad, svogel}@qf.org.qa ∗ MIT Computer Science and Artificial Intelligence Laboratory, Cambridge, MA 02139, USA belinkov@mit.edu Abstract what NMT models learn about morphology (Belinkov et al., 2017a), syntax (Shi et al., 2016) and semantics (Belinkov et al., 2017b). Shi et al. (2016) used activations at various layers from the NMT encoder to predict syntactic properties on the source-side, while Belinkov et al. (2017a) and Belinkov et al. (2017b) used a similar approach to investigate the quality of word representations on the task of morphological and semantic tagging. Belinkov et al. (2017a) found that word representations learned from the encoder are rich in morphological information, while representations learned from the decoder are significantly poorer. However, the paper does not present a convincing explanation for this finding. Our first contribution in this work is to provide a more comprehensive analysis of morphological learning on the decoder side. We hypothesize that other components of the"
I17-1015,D16-1025,0,0.0176372,"d iii) multi-task learning. Our results show that explicit morphological information helps the decoder learn target language morphology and improves the translation quality by 0.2–0.6 BLEU points. 1 Introduction • What is the effect of attention on the performance of the decoder? Neural machine translation (NMT) offers an elegant end-to-end architecture, improving translation quality compared to traditional phrase-based machine translation. These improvements are attributed to more fluent output (Toral and S´anchezCartagena, 2017) and better handling of morphology and long-range dependencies (Bentivogli et al., 2016). However, systematic studies are required to understand what kinds of linguistic phenomena (morphology, syntax, semantics, etc.) are learned by these models and more importantly, which of the components is responsible for each phenomenon. A few attempts have been made to understand • How much does the encoder help the decoder in predicting the correct morphological variant of the word it generates? To answer these questions, we train NMT models for different language pairs, involving morphologically rich languages such as German and Czech. We then use the trained models to extract features fr"
I17-1015,2014.iwslt-evaluation.6,1,0.839752,"o integrate morphology into the decoder. Section 5 presents the results. Section 6 gives an account of related work and Section 7 concludes the paper. 2 Language-pair NMT Systems We used the seq2seq-attn implementation (Kim, 2016) with the following default settings: word embeddings and LSTM states with 500 dimensions, SGD with an initial learning rate of 1.0 and decay rate of 0.5 (after the 9th epoch), and dropout rate of 0.3. We use two uni-directional hidden layers for both the encoder and the decoder. 1 These have been used frequently to annotate data in the previous evaluation campaigns (Birch et al., 2014; Durrani et al., 2014a). 2 The difficulty with using these is that it is not straightforward to derive word representations out of a decoder that processes BPE-ed text, because the original words are split into subwords. We considered aggregating the representations of BPE subword units, but the choice of aggregation strategy may have an undesired impact on the analysis. For this reason we decided to leave exploration of BPE for future work. 3 Character-based models are becoming increasingly popular in Neural MT, for addressing the rare word problem – and they have been used previously also t"
I17-1015,P16-2058,0,0.023772,"Missing"
I17-1015,P17-2021,0,0.0187344,"er was tuned on a separate held out development set (test-11), and the results shown in Figure 3 are on blind test sets (test-12,13). Averages are reported in the figure. 6 Integrating Morphology Some work has also been done in injecting morphological or more general linguistic knowledge into an NMT system. Sennrich and Haddow (2016) proposed a factored model that incorporates linguistic features on the source side as additional factors. An embedding is learned for each factor, just like a source word, and then the word and factor embeddings are combined before being passed on to the encoder. Aharoni and Goldberg (2017) proposed a method to predict the target sentence along with its syntactic tree. They linearize the tree in order to use the existing sequence-to-sequence model. Nadejde et al. (2017) also evaluated several methods of incorporating syntactic knowledge on both the source and target. While they used factors on the source side, their best method for the target side was to linearize the information and interleave it between the target words. Garc´ıa-Mart´ınez et al. (2016) used a neural MT model with multiple outputs, like in our case of Multi-task learning. Their model predicts two properties at"
I17-1015,P15-1166,0,0.0240273,"Missing"
I17-1015,W14-3309,1,0.847572,"gy into the decoder. Section 5 presents the results. Section 6 gives an account of related work and Section 7 concludes the paper. 2 Language-pair NMT Systems We used the seq2seq-attn implementation (Kim, 2016) with the following default settings: word embeddings and LSTM states with 500 dimensions, SGD with an initial learning rate of 1.0 and decay rate of 0.5 (after the 9th epoch), and dropout rate of 0.3. We use two uni-directional hidden layers for both the encoder and the decoder. 1 These have been used frequently to annotate data in the previous evaluation campaigns (Birch et al., 2014; Durrani et al., 2014a). 2 The difficulty with using these is that it is not straightforward to derive word representations out of a decoder that processes BPE-ed text, because the original words are split into subwords. We considered aggregating the representations of BPE subword units, but the choice of aggregation strategy may have an undesired impact on the analysis. For this reason we decided to leave exploration of BPE for future work. 3 Character-based models are becoming increasingly popular in Neural MT, for addressing the rare word problem – and they have been used previously also to benefit MT for morph"
I17-1015,P17-1080,1,0.711845,"ng in the Neural Machine Translation Decoder Fahim Dalvi Nadir Durrani Hassan Sajjad Yonatan Belinkov∗ Stephan Vogel Qatar Computing Research Institute – HBKU, Doha, Qatar {faimaduddin, ndurrani, hsajjad, svogel}@qf.org.qa ∗ MIT Computer Science and Artificial Intelligence Laboratory, Cambridge, MA 02139, USA belinkov@mit.edu Abstract what NMT models learn about morphology (Belinkov et al., 2017a), syntax (Shi et al., 2016) and semantics (Belinkov et al., 2017b). Shi et al. (2016) used activations at various layers from the NMT encoder to predict syntactic properties on the source-side, while Belinkov et al. (2017a) and Belinkov et al. (2017b) used a similar approach to investigate the quality of word representations on the task of morphological and semantic tagging. Belinkov et al. (2017a) found that word representations learned from the encoder are rich in morphological information, while representations learned from the decoder are significantly poorer. However, the paper does not present a convincing explanation for this finding. Our first contribution in this work is to provide a more comprehensive analysis of morphological learning on the decoder side. We hypothesize that other components of the"
I17-1015,P10-1048,1,0.860476,"cesses BPE-ed text, because the original words are split into subwords. We considered aggregating the representations of BPE subword units, but the choice of aggregation strategy may have an undesired impact on the analysis. For this reason we decided to leave exploration of BPE for future work. 3 Character-based models are becoming increasingly popular in Neural MT, for addressing the rare word problem – and they have been used previously also to benefit MT for morphologically rich (Luong et al., 2010; Belinkov and Glass, 2016; Costa-juss`a and Fonollosa, 2016) and closely related languages (Durrani et al., 2010; Sajjad et al., 2013). Experimental Design Parallel Data We used the German-English and Czech-English datasets from the WIT3 TED corpus (Cettolo, 2016) made available for IWSLT 2016. We used the official training sets to analyze and evaluate the proposed methods for integrating morphology . The corpus also provides four test sets, test-11 through test-14. We used test-11 for tuning, and the other test sets for evaluation. The statistics for the sets are provided in Table 1. 143 weighted average of these hidden states from the previous decoder state (di−1 ), known as the context vector ci (Equ"
I17-1015,D10-1015,0,0.0577683,"lty with using these is that it is not straightforward to derive word representations out of a decoder that processes BPE-ed text, because the original words are split into subwords. We considered aggregating the representations of BPE subword units, but the choice of aggregation strategy may have an undesired impact on the analysis. For this reason we decided to leave exploration of BPE for future work. 3 Character-based models are becoming increasingly popular in Neural MT, for addressing the rare word problem – and they have been used previously also to benefit MT for morphologically rich (Luong et al., 2010; Belinkov and Glass, 2016; Costa-juss`a and Fonollosa, 2016) and closely related languages (Durrani et al., 2010; Sajjad et al., 2013). Experimental Design Parallel Data We used the German-English and Czech-English datasets from the WIT3 TED corpus (Cettolo, 2016) made available for IWSLT 2016. We used the official training sets to analyze and evaluate the proposed methods for integrating morphology . The corpus also provides four test sets, test-11 through test-14. We used test-11 for tuning, and the other test sets for evaluation. The statistics for the sets are provided in Table 1. 143 wei"
I17-1015,P17-2012,0,0.0111985,"raining is to learn several tasks simultaneously such that each task can benefit from the mutual information learned (Collobert and Weston, 2008). 5 With this motivation, we modified the NMT decoder to predict not only a word but also its corresponding tag. All of the layers below the output layers are shared. We have two output layers in parallel – the first to predict the target word, and the second to predict the morphological tag of the target word. Both ouput layJoint-data Learning Given the drawbacks of the first approach, we considered another data augmentation technique 5 For example, Eriguchi et al. (2017) jointly learned the tasks of parsing and translation. 146 Figure 3: Improvements from adding morphology. A y-value of zero represents the baseline ers have their own separate loss function. While training, we combine the losses from both output layers to jointly train the system. This is different from the Joint-data learning technique, where we predict entire sequences of words or tags without any dependence on each other. Formally, given a set of N tasks, sequence-tosequence multi-task learning involves an objective function minimizing the overall loss, which is a weighted combination of th"
I17-1015,D16-1079,0,0.0518352,"Missing"
I17-1015,Q17-1024,0,0.0293413,"Missing"
I17-1015,P16-1140,0,0.0270118,"Missing"
I17-1015,P13-2001,1,0.838603,"cause the original words are split into subwords. We considered aggregating the representations of BPE subword units, but the choice of aggregation strategy may have an undesired impact on the analysis. For this reason we decided to leave exploration of BPE for future work. 3 Character-based models are becoming increasingly popular in Neural MT, for addressing the rare word problem – and they have been used previously also to benefit MT for morphologically rich (Luong et al., 2010; Belinkov and Glass, 2016; Costa-juss`a and Fonollosa, 2016) and closely related languages (Durrani et al., 2010; Sajjad et al., 2013). Experimental Design Parallel Data We used the German-English and Czech-English datasets from the WIT3 TED corpus (Cettolo, 2016) made available for IWSLT 2016. We used the official training sets to analyze and evaluate the proposed methods for integrating morphology . The corpus also provides four test sets, test-11 through test-14. We used test-11 for tuning, and the other test sets for evaluation. The statistics for the sets are provided in Table 1. 143 weighted average of these hidden states from the previous decoder state (di−1 ), known as the context vector ci (Equation 2). The context"
I17-1015,D07-1091,0,0.203126,"Missing"
I17-1015,C94-1027,0,0.0784836,"logical information during training which can in turn improve the overall translation quality. In order to test this hypothesis, we experiment with three possible solutions: Sentences tokde/cz token De↔En Cz↔En 210K 122K 4M 2.1M 4.2M 2.5M Table 1: Statistics for the data used for training, tuning and testing Morphological Annotations In order to train and evaluate the external classifier on the extracted features, we required data annotated with morphological tags. We used the following tools recommended on the Moses website1 to annotate the data: LoPar (Schmid, 2000) for German, Tree-tagger (Schmid, 1994) for Czech and MXPOST (Ratnaparkhi, 1998) for English. The number of tags produced by these taggers is 214 for German and 368 for Czech. 1. Joint Generation: An NMT model is trained on the concatenation of words and morphological tags on the target side. 2. Joint-data learning: An NMT model is trained where each source sequence is used twice with an artificial token to either predict target words or morphological tags. Data preprocessing We used the standard MT pre-processing pipeline of tokenizing and truecasing the data using Moses (Koehn et al., 2007) scripts. We did not apply byte-pair enc"
I17-1015,P07-2045,0,0.0125609,"Missing"
I17-1015,W16-2209,0,0.0260724,"e at that point the model is only minimizing the tag objective function. Similarly at λ = 0, the model falls back to the baseline model with a single objective function minimizing translation error. For all language pairs, we consistently achieved the best BLEU score at λ = 0.2. The parameter was tuned on a separate held out development set (test-11), and the results shown in Figure 3 are on blind test sets (test-12,13). Averages are reported in the figure. 6 Integrating Morphology Some work has also been done in injecting morphological or more general linguistic knowledge into an NMT system. Sennrich and Haddow (2016) proposed a factored model that incorporates linguistic features on the source side as additional factors. An embedding is learned for each factor, just like a source word, and then the word and factor embeddings are combined before being passed on to the encoder. Aharoni and Goldberg (2017) proposed a method to predict the target sentence along with its syntactic tree. They linearize the tree in order to use the existing sequence-to-sequence model. Nadejde et al. (2017) also evaluated several methods of incorporating syntactic knowledge on both the source and target. While they used factors o"
I17-1015,D15-1246,0,0.0420013,"Missing"
I17-1015,N16-1005,0,0.0144274,"phological knowledge into the decoder inspired by multilingual NMT systems (Johnson et al., 2016). Instead of having multiple source and target languages, we used one source language and two target language variations. The training data consists of sequences of source→target words and source→target morphological tags. We added an artificial token in the beginning of each source sentence indicating whether we want to generate target words or morphological tags. Using an artificial token in the source sentence has been explored and shown to work well to control the style of the target language (Sennrich et al., 2016a). The objective function is the same as the one in usual sequence-to-sequence models, and is hence shared to minimize both morphological and translation error given the mixed data. encoder (Table 2) and the overall system does not learn as much about target morphology as source morphology, we investigated three ways to directly inject target morphology into the decoder, namely: i) Joint Generation, ii) Joint-data Learning, iii) Multi-task Learning. Figure 2 illustrates the approaches. 4.1 Joint Generation As our first approach, we considered a solution that uses the standard NMT architecture"
I17-1015,P16-1162,0,0.0453069,"phological knowledge into the decoder inspired by multilingual NMT systems (Johnson et al., 2016). Instead of having multiple source and target languages, we used one source language and two target language variations. The training data consists of sequences of source→target words and source→target morphological tags. We added an artificial token in the beginning of each source sentence indicating whether we want to generate target words or morphological tags. Using an artificial token in the source sentence has been explored and shown to work well to control the style of the target language (Sennrich et al., 2016a). The objective function is the same as the one in usual sequence-to-sequence models, and is hence shared to minimize both morphological and translation error given the mixed data. encoder (Table 2) and the overall system does not learn as much about target morphology as source morphology, we investigated three ways to directly inject target morphology into the decoder, namely: i) Joint Generation, ii) Joint-data Learning, iii) Multi-task Learning. Figure 2 illustrates the approaches. 4.1 Joint Generation As our first approach, we considered a solution that uses the standard NMT architecture"
I17-1015,D16-1159,0,0.118608,"sus 214 in German. We tuned the weight parameter on held-out data. 147 Figure 4: Multi-task learning: Translation vs. Morphological Tagging weight for En→De model relevant information about the input. K¨ohn (2015) and Qian et al. (2016b) analyzed linguistic information learned in word embeddings, while Qian et al. (2016a) went further and analyzed linguistic properties in the hidden states of a recurrent neural network. Adi et al. (2016) looked at the overall information learned in a sentence summary vector generated by an RNN using a similar approach. Our approach closely aligns with that of Shi et al. (2016) and Belinkov et al. (2017a), where the activations from various layers in a trained NMT system are used to predict linguistic properties. be handy if the morphological information quality is not very high. On the flip side, this additional explicit weight adjustment can also be viewed as a potential constraint that is not present in the jointdata learning approach. Multi-task Weight Hyper-Parameter As discussed, the multi-task learning approach has an additional weight hyper-parameter λ that adjusts the balance between word and tag prediction. Figure 4 shows the result of varying λ from no mo"
I17-1015,E17-1100,0,0.0361648,"Missing"
J17-2003,J84-3009,0,0.122732,"Missing"
J17-2003,2012.iwslt-papers.6,0,0.0419685,"Missing"
J17-2003,W10-2407,0,0.0148307,"threshold are extracted (Jiampojamarn et al. 2010; Noeman and Madkour 2010; Sajjad et al. 2011). Supervised transliteration mining systems (Nabende 2010; Noeman and Madkour 2010; El-Kahki et al. 2011) make use of an initial list of transliteration pairs that is automatically aligned at the character level. The systems are trained on the aligned data and applied to an unlabeled list of candidate word pairs. Word pairs with a probability greater than a certain threshold are classified as transliteration pairs. Similarly to supervised approaches, semi-supervised systems (Sherif and Kondrak 2007; Darwish 2010) also use a list of transliteration pairs for training. However, here the list is generally small. The systems thus do not solely rely on it to mine transliteration pairs. They use both the list of transliteration pairs and unlabeled data for training. We are only aware of two unsupervised systems (requiring no labeled data). One of them was proposed by Fei Huang (2005). He extracts named entity pairs from a bilingual corpus, converts all words into Latin script by romanization, and classifies them into transliterations and non-transliterations based on the edit distance. This system still req"
J17-2003,2014.eamt-1.17,0,0.0188764,"ations. Sajjad et al. (2013b) generated improved word alignment of the parallel training data by incorporating the transliteration mining module into GIZA++. Sajjad et al. (2013a), and Durrani et al. (2014) used transliteration mining to transliterate out-of-vocabulary words in a statistical machine translation system. They extracted transliteration pairs from the parallel corpus in an unsupervised fashion and trained a transliteration system on them. Kunchukuttan and Bhattacharyya (2015) showed the usefulness of unsupervised transliteration mining for the transliteration of Indian languages. Durrani and Koehn (2014) exploited the closeness between Urdu and Hindi using transliteration mining. They created synthetic Hindi-English parallel data by transliterating Urdu to Hindi. When they used the data for machine translation, it substantially improved translation quality. 10. Conclusions We presented a statistical model for mining transliteration pairs from a list of candidate pairs in a fully unsupervised fashion. Our model consists of sub-models for transliterations and non-transliterations that are interpolated. The transliteration sub-model is an n-gram model over 1–1, 0–1, and 1–0 character pairs and t"
J17-2003,P10-1048,1,0.805721,"language; (3) punctuation errors: one word has an additional punctuation symbol that makes the word pair a non-transliteration; (4) gold standard error: errors in the gold standard; (5) worst errors: word pairs that are far from being considered as transliteration pairs. Table 13 shows the number of errors of each type. The affix-based and pronunciation errors are the top errors made by the system. Both of them plus punctuation errors come under the broad definition of close transliterations. These word pairs are helpful because they provide useful character-level transliteration information. Durrani et al. (2010) incorporated our unsupervised transliteration mining system into machine translation. They showed that for language pairs with fewer transliterations, the close transliterations help to build a stronger transliteration system. Table 13 Types of errors made by the unsupervised transliteration mining system on the English/Arabic language pair. The numbers are based on randomly selected 100 word pairs that were wrongly classified by the mining system. Error Type Affix-based Error Gold Standard Error 372 Count Error Type Count Error Type Count 38 9 Pronunciation Error Worst Error 22 21 Punctuatio"
J17-2003,E14-4029,1,0.848098,"ost of the target language words in the cross-product list. The unsupervised system starts learning wrong transliterations because of their high frequency. Durrani et al. (2010) preprocess the candidate list before mining the transliteration pairs and remove words pairs with the most common source and target words. 9. Applications Our unsupervised transliteration mining system has been used in several NLP applications. Sajjad et al. (2013b) generated improved word alignment of the parallel training data by incorporating the transliteration mining module into GIZA++. Sajjad et al. (2013a), and Durrani et al. (2014) used transliteration mining to transliterate out-of-vocabulary words in a statistical machine translation system. They extracted transliteration pairs from the parallel corpus in an unsupervised fashion and trained a transliteration system on them. Kunchukuttan and Bhattacharyya (2015) showed the usefulness of unsupervised transliteration mining for the transliteration of Indian languages. Durrani and Koehn (2014) exploited the closeness between Urdu and Hindi using transliteration mining. They created synthetic Hindi-English parallel data by transliterating Urdu to Hindi. When they used the"
J17-2003,eisele-chen-2010-multiun,0,0.0445614,"Missing"
J17-2003,D11-1128,0,0.0606321,"Missing"
J17-2003,J93-1004,0,0.666995,"multigram sequences: p1 (e, f) = X p1 (a) (1) a∈Align(e,f ) where Align(e, f) returns all possible multigram sequences for the transliteration pair (e, f). In a unigram model, the probability of a multigram sequence a is the product of the probabilities of the multigrams it contains: p1 (a) = p1 (a1 a2 ...a|a |) = |a| Y p1 (aj ) (2) j=1 where |a |is the length of the sequence a. The non-transliteration sub-model generates source and target words that are unrelated. We model such pairs with two separate character unigram models (a source and a target model) whose probabilities are multiplied (Gale and Church 1993). Their parameters are learned from monolingual corpora and not updated during EM training. The non-transliteration sub-model is defined as follows: p2 (e, f) = pE (e)pF (f) (3) Q|e| Q|f| pE (e) = i=1 pE (ei ) and pF (f) = i=1 pF ( fi ) The transliteration mining model is obtained by interpolating the transliteration model p1 (e, f) and the non-transliteration model p2 (e, f): p(e, f) = (1 − λ )p1 (e, f) + λp2 (e, f) (4) where λ is the prior probability of non-transliteration. Interpolation with the non-transliteration model allows the transliteration model to concentrate on modeling translite"
J17-2003,W10-2405,0,0.150552,"it is attractive to extract transliteration pairs automatically from a noisy list of transliteration candidates, which can be obtained from aligned bilingual corpora, for instance. This extraction process is called transliteration mining. There are rule-based, supervised, semi-supervised, and unsupervised ways to mine transliteration pairs. Rule-based methods apply weighted handwritten rules that map characters between two languages, and compute a weighted edit distance metric that assigns a score to every candidate word pair. Pairs with an edit distance below a given threshold are extracted (Jiampojamarn et al. 2010; Noeman and Madkour 2010; Sajjad et al. 2011). Supervised transliteration mining systems (Nabende 2010; Noeman and Madkour 2010; El-Kahki et al. 2011) make use of an initial list of transliteration pairs that is automatically aligned at the character level. The systems are trained on the aligned data and applied to an unlabeled list of candidate word pairs. Word pairs with a probability greater than a certain threshold are classified as transliteration pairs. Similarly to supervised approaches, semi-supervised systems (Sherif and Kondrak 2007; Darwish 2010) also use a list of transliteration"
J17-2003,N03-1017,0,0.0598518,"Missing"
J17-2003,W15-3912,0,0.014598,"with the most common source and target words. 9. Applications Our unsupervised transliteration mining system has been used in several NLP applications. Sajjad et al. (2013b) generated improved word alignment of the parallel training data by incorporating the transliteration mining module into GIZA++. Sajjad et al. (2013a), and Durrani et al. (2014) used transliteration mining to transliterate out-of-vocabulary words in a statistical machine translation system. They extracted transliteration pairs from the parallel corpus in an unsupervised fashion and trained a transliteration system on them. Kunchukuttan and Bhattacharyya (2015) showed the usefulness of unsupervised transliteration mining for the transliteration of Indian languages. Durrani and Koehn (2014) exploited the closeness between Urdu and Hindi using transliteration mining. They created synthetic Hindi-English parallel data by transliterating Urdu to Hindi. When they used the data for machine translation, it substantially improved translation quality. 10. Conclusions We presented a statistical model for mining transliteration pairs from a list of candidate pairs in a fully unsupervised fashion. Our model consists of sub-models for transliterations and non-tr"
J17-2003,P04-1021,0,0.327325,"Missing"
J17-2003,W05-0809,0,0.0518742,"Missing"
J17-2003,W10-2412,0,0.014251,"ich can be obtained from aligned bilingual corpora, for instance. This extraction process is called transliteration mining. There are rule-based, supervised, semi-supervised, and unsupervised ways to mine transliteration pairs. Rule-based methods apply weighted handwritten rules that map characters between two languages, and compute a weighted edit distance metric that assigns a score to every candidate word pair. Pairs with an edit distance below a given threshold are extracted (Jiampojamarn et al. 2010; Noeman and Madkour 2010; Sajjad et al. 2011). Supervised transliteration mining systems (Nabende 2010; Noeman and Madkour 2010; El-Kahki et al. 2011) make use of an initial list of transliteration pairs that is automatically aligned at the character level. The systems are trained on the aligned data and applied to an unlabeled list of candidate word pairs. Word pairs with a probability greater than a certain threshold are classified as transliteration pairs. Similarly to supervised approaches, semi-supervised systems (Sherif and Kondrak 2007; Darwish 2010) also use a list of transliteration pairs for training. However, here the list is generally small. The systems thus do not solely rely on i"
J17-2003,W10-2408,0,0.0497492,"Missing"
J17-2003,J03-1002,0,0.0745413,"Missing"
J17-2003,I11-1015,1,0.907278,"utomatically from a noisy list of transliteration candidates, which can be obtained from aligned bilingual corpora, for instance. This extraction process is called transliteration mining. There are rule-based, supervised, semi-supervised, and unsupervised ways to mine transliteration pairs. Rule-based methods apply weighted handwritten rules that map characters between two languages, and compute a weighted edit distance metric that assigns a score to every candidate word pair. Pairs with an edit distance below a given threshold are extracted (Jiampojamarn et al. 2010; Noeman and Madkour 2010; Sajjad et al. 2011). Supervised transliteration mining systems (Nabende 2010; Noeman and Madkour 2010; El-Kahki et al. 2011) make use of an initial list of transliteration pairs that is automatically aligned at the character level. The systems are trained on the aligned data and applied to an unlabeled list of candidate word pairs. Word pairs with a probability greater than a certain threshold are classified as transliteration pairs. Similarly to supervised approaches, semi-supervised systems (Sherif and Kondrak 2007; Darwish 2010) also use a list of transliteration pairs for training. However, here the list is"
J17-2003,P11-1044,1,0.884668,"utomatically from a noisy list of transliteration candidates, which can be obtained from aligned bilingual corpora, for instance. This extraction process is called transliteration mining. There are rule-based, supervised, semi-supervised, and unsupervised ways to mine transliteration pairs. Rule-based methods apply weighted handwritten rules that map characters between two languages, and compute a weighted edit distance metric that assigns a score to every candidate word pair. Pairs with an edit distance below a given threshold are extracted (Jiampojamarn et al. 2010; Noeman and Madkour 2010; Sajjad et al. 2011). Supervised transliteration mining systems (Nabende 2010; Noeman and Madkour 2010; El-Kahki et al. 2011) make use of an initial list of transliteration pairs that is automatically aligned at the character level. The systems are trained on the aligned data and applied to an unlabeled list of candidate word pairs. Word pairs with a probability greater than a certain threshold are classified as transliteration pairs. Similarly to supervised approaches, semi-supervised systems (Sherif and Kondrak 2007; Darwish 2010) also use a list of transliteration pairs for training. However, here the list is"
J17-2003,P12-1049,1,0.795048,"Missing"
J17-2003,2013.iwslt-evaluation.8,1,0.860414,"ration Mining We observed that most of the word pairs in class 5 (worst errors) contain stop words. Because stop words are the most frequent words in a corpus, they occur with most of the target language words in the cross-product list. The unsupervised system starts learning wrong transliterations because of their high frequency. Durrani et al. (2010) preprocess the candidate list before mining the transliteration pairs and remove words pairs with the most common source and target words. 9. Applications Our unsupervised transliteration mining system has been used in several NLP applications. Sajjad et al. (2013b) generated improved word alignment of the parallel training data by incorporating the transliteration mining module into GIZA++. Sajjad et al. (2013a), and Durrani et al. (2014) used transliteration mining to transliterate out-of-vocabulary words in a statistical machine translation system. They extracted transliteration pairs from the parallel corpus in an unsupervised fashion and trained a transliteration system on them. Kunchukuttan and Bhattacharyya (2015) showed the usefulness of unsupervised transliteration mining for the transliteration of Indian languages. Durrani and Koehn (2014) ex"
J17-2003,W13-2228,1,0.851534,"ration Mining We observed that most of the word pairs in class 5 (worst errors) contain stop words. Because stop words are the most frequent words in a corpus, they occur with most of the target language words in the cross-product list. The unsupervised system starts learning wrong transliterations because of their high frequency. Durrani et al. (2010) preprocess the candidate list before mining the transliteration pairs and remove words pairs with the most common source and target words. 9. Applications Our unsupervised transliteration mining system has been used in several NLP applications. Sajjad et al. (2013b) generated improved word alignment of the parallel training data by incorporating the transliteration mining module into GIZA++. Sajjad et al. (2013a), and Durrani et al. (2014) used transliteration mining to transliterate out-of-vocabulary words in a statistical machine translation system. They extracted transliteration pairs from the parallel corpus in an unsupervised fashion and trained a transliteration system on them. Kunchukuttan and Bhattacharyya (2015) showed the usefulness of unsupervised transliteration mining for the transliteration of Indian languages. Durrani and Koehn (2014) ex"
J17-2003,P07-1109,0,0.0293419,"t distance below a given threshold are extracted (Jiampojamarn et al. 2010; Noeman and Madkour 2010; Sajjad et al. 2011). Supervised transliteration mining systems (Nabende 2010; Noeman and Madkour 2010; El-Kahki et al. 2011) make use of an initial list of transliteration pairs that is automatically aligned at the character level. The systems are trained on the aligned data and applied to an unlabeled list of candidate word pairs. Word pairs with a probability greater than a certain threshold are classified as transliteration pairs. Similarly to supervised approaches, semi-supervised systems (Sherif and Kondrak 2007; Darwish 2010) also use a list of transliteration pairs for training. However, here the list is generally small. The systems thus do not solely rely on it to mine transliteration pairs. They use both the list of transliteration pairs and unlabeled data for training. We are only aware of two unsupervised systems (requiring no labeled data). One of them was proposed by Fei Huang (2005). He extracts named entity pairs from a bilingual corpus, converts all words into Latin script by romanization, and classifies them into transliterations and non-transliterations based on the edit distance. This s"
J17-2003,W06-1630,0,0.0426347,"ent and accurate and can be used in three different training settings—unsupervised, semi-supervised, and supervised learning. Our method directly learns character correspondences between two scripts from a noisy unlabeled list of word pairs which contains both transliterations and non-transliterations. When such a list is extracted from an aligned bilingual corpus, for instance, it contains, 1 There are other approaches to transliteration mining that exploit phonetic similarity between languages (Aransa, Schwenk, and Barrault 2012) and make use of temporal information available with the data (Tao et al. 2006). We do not discuss them here because they are out of the scope of this work. 350 Sajjad et al. Statistical Models for Transliteration Mining apart from transliterations, also both translations and misalignments, which we will call non-transliterations. Our statistical model interpolates a transliteration sub-model and a nontransliteration sub-model. The intuition behind using two sub-models is that the transliteration pairs and non-transliteration pairs, which make up the unlabeled training data, have rather different characteristics and need to be modeled separately. Transliteration word pai"
J17-2003,W12-4410,0,\N,Missing
J17-2003,J93-2003,0,\N,Missing
J17-2003,C96-2141,0,\N,Missing
J17-2003,W98-1005,0,\N,Missing
J17-2003,W07-0703,0,\N,Missing
J17-2003,J07-3002,1,\N,Missing
J17-2003,W02-0505,0,\N,Missing
J17-2003,C08-1068,0,\N,Missing
J17-2003,N07-1046,0,\N,Missing
J17-2003,W09-3528,0,\N,Missing
J17-2003,N07-1047,0,\N,Missing
J17-2003,P06-1103,0,\N,Missing
J17-2003,P07-1119,0,\N,Missing
J17-2003,P06-1010,0,\N,Missing
J17-2003,P08-1045,0,\N,Missing
J17-2003,P06-2025,0,\N,Missing
J17-2003,W09-3525,0,\N,Missing
J17-2003,W09-3522,0,\N,Missing
J17-2003,W07-0711,0,\N,Missing
J17-2003,W04-3250,0,\N,Missing
J17-2003,2010.iwslt-papers.7,0,\N,Missing
J17-2003,I08-8003,0,\N,Missing
J17-2003,J98-4003,0,\N,Missing
J17-2003,W10-2404,0,\N,Missing
J17-2003,P00-1056,0,\N,Missing
J17-2003,W05-0606,0,\N,Missing
J17-2003,W09-3507,0,\N,Missing
J17-2003,W09-3504,0,\N,Missing
J17-2003,P03-1021,0,\N,Missing
N16-1125,N16-3004,1,0.864659,"Missing"
N16-1125,W07-0718,0,0.0456341,"res given by the evaluators. 1 Introduction Human evaluation has been the preferred method for tracking the progress of MT systems. In the past, the prevalent criterion was to judge the quality of a translation in terms of fluency and adequacy, on an absolute scale (White et al., 1994). However, different evaluators focused on different aspects of the translations, which increased the subjectivity of their judgments. As a result, evaluations suffered from low inter- and intra-annotator agreements (Turian et al., 2003; Snover et al., 2006). This caused a shift towards a ranking-based approach (Callison-Burch et al., 2007). Unfortunately, the disagreement between evaluators is still a challenge that cannot be easily resolved due to the non-transparent thought-process that evaluators follow to make a judgment. The eye-mind hypothesis (Just and Carpenter, 1980; Potter, 1983) states that when completing a task, people cognitively process objects that are in front of their eyes (i.e. where they fixate their gaze).1 Based on this assumption, it has been possible to study reading behavior and patterns (Rayner, 1998; Garrod, 2006; Hansen and Ji, 2010). The overall difficulty of a sentence and its syntactic complexity"
N16-1125,W12-3102,0,0.0857719,"Missing"
N16-1125,P11-1105,1,0.882545,"and distance between the start and end words. For subsequent words n, n + 1, this would mean a forward jump of distance equal to 1. All jumps with distance greater than 4 were sorted into a 5+ bucket. Additionally, we separate the features for reference and translation jumps. We also count the total number of jumps. Total jump distance We additionally aggregate jump distances2 to count the total distance covered while evaluating a sentence. We have reference distance and translation distance features. Again, the 2 Jump count and distance features have also shown to be useful in SMT decoders (Durrani et al., 2011). 1083 idea is that for a well-formed sentence, gaze distance should be less, compared to a poorly-formed one. Inter-region jumps While reading a translation, evaluators can jump between the translation and a reference to compare them. Intuitively, more jumps of this type could signify that the translation is harder to evaluate. Here we count the number of transitions between reference and translation. Dwell time The amount of time a person fixates on a region is a crucial marker for processing difficulty in sentence comprehension (Clifton et al., 2007) and moderately correlates with the quali"
N16-1125,W13-2305,0,0.0177286,"was performed by 6 different evaluators, resulting in 720 evaluations. The annotators were presented with a translationreference pair at a time. The two evaluation tasks corresponding to the same reference were presented at two different times with at least 40 other tasks in-between. This was done to prevent any possible spurious effects that may arise from remembering the content of a first translation, when evaluating the second translation of the same sentence. During each evaluation task, the evaluators were asked to assess the quality of a translation by providing a score between 0–100 (Graham et al., 2013). The observed inter-annotator agreement (Cohen’s kappa) among our annotators was 0.321. This is slightly higher than the overall inter-annotator agreement of 0.284 reported in WMT’12 for the Spanish-English.3 For reading patterns we use the EyeTribe eye-tracker at 3 For a rough comparison only. Note that these two numbers are not exactly comparable given that they are calculated on different subsets of the same data. Still, there is a fair agreement between the our evaluators and the expected wins from WMT’12 (avg. pairwise kappa of 0.381) 1084 Evaluation In our evaluation, we used eye-tracki"
N16-1125,W15-3059,1,0.875061,"Missing"
N16-1125,P02-1040,0,0.0970473,"n evaluation metric So far, we’ve shown that the individual sets of features based on reading patterns can help to predict translation quality, and that this goes beyond simple fluency. One question that remains to be answered is whether these features could be used as a whole to evaluate the quality of a translation semi-automatically. That is, whether we can use the gaze information, and other lexical information to anticipate the score that an evaluator will assign to a translation. Here, we present evaluation results combining several of these gaze features, and compare them against BLEU (Papineni et al., 2002), which uses lexical information and is designed to measure not only fluency but also adequacy. In Table 2, we present results in the following way: in (I) we present the best non-lexicalized feature combinations that improve the predictive power of the model. In (II) we re-introduce the results of lexicalized jumps feature. In (III) we present results of BLEU and the combination of eye-tracking features with it. Finally in (IV) we present the humanto-human agreement measured in average Kendall’s tau and in max human-to-human Kendall’s tau. Combinations of translation jumps In section I we pre"
N16-1125,2006.amta-papers.25,0,0.0363431,"patterns can be used to build semi-automatic metrics that anticipate the scores given by the evaluators. 1 Introduction Human evaluation has been the preferred method for tracking the progress of MT systems. In the past, the prevalent criterion was to judge the quality of a translation in terms of fluency and adequacy, on an absolute scale (White et al., 1994). However, different evaluators focused on different aspects of the translations, which increased the subjectivity of their judgments. As a result, evaluations suffered from low inter- and intra-annotator agreements (Turian et al., 2003; Snover et al., 2006). This caused a shift towards a ranking-based approach (Callison-Burch et al., 2007). Unfortunately, the disagreement between evaluators is still a challenge that cannot be easily resolved due to the non-transparent thought-process that evaluators follow to make a judgment. The eye-mind hypothesis (Just and Carpenter, 1980; Potter, 1983) states that when completing a task, people cognitively process objects that are in front of their eyes (i.e. where they fixate their gaze).1 Based on this assumption, it has been possible to study reading behavior and patterns (Rayner, 1998; Garrod, 2006; Hans"
N16-1125,stymne-etal-2012-eye,0,0.322545,"enomena remain to be explored in future work. Human performance On average, evaluators agreements with each other are fair (τ = 0.33) and below the best combination (CB3 ), while the maximum agreement of any two evaluators is relatively higher (τ = 0.53). This tells us that on average the semi-automatic approach to evaluation that we propose here is already competitive to predictions done by another (average) human. However, there is still room for improvement with respect to the mostagreeing pair of evaluators. 5 Related Work Eye-tracking devices have been used previously in the MT research. Stymne et al. (2012) used eye-tracking to identify and classify MT errors. 1086 SYS Feature Sets τ I. Combination of translation jumps EyeTrabj Backward jumps CTJ1 Backward jumps, total jumps CTJ2 Backward jumps, total jumps, distance 0.22 0.25 0.27 II. Eye-tracking: Best Lexicalized EyeLexall Lexicalized gaze jumps 0.22 III. Combinations with BLEU Bbleu BLEU CB1 Bbleu + EyeTrabj CB2 Bbleu + CTJ2 CB3 Bbleu + EyeLexall 0.34 0.38 0.39 0.42 IV. Human performance Avg Avg. human-to-human agreement Max Max. human-to-human agreement 0.33 0.53 Table 2: Result of combining several jump and lexicalized features with BLEU."
N16-1125,2003.mtsummit-papers.51,0,0.111887,"ts show that reading patterns can be used to build semi-automatic metrics that anticipate the scores given by the evaluators. 1 Introduction Human evaluation has been the preferred method for tracking the progress of MT systems. In the past, the prevalent criterion was to judge the quality of a translation in terms of fluency and adequacy, on an absolute scale (White et al., 1994). However, different evaluators focused on different aspects of the translations, which increased the subjectivity of their judgments. As a result, evaluations suffered from low inter- and intra-annotator agreements (Turian et al., 2003; Snover et al., 2006). This caused a shift towards a ranking-based approach (Callison-Burch et al., 2007). Unfortunately, the disagreement between evaluators is still a challenge that cannot be easily resolved due to the non-transparent thought-process that evaluators follow to make a judgment. The eye-mind hypothesis (Just and Carpenter, 1980; Potter, 1983) states that when completing a task, people cognitively process objects that are in front of their eyes (i.e. where they fixate their gaze).1 Based on this assumption, it has been possible to study reading behavior and patterns (Rayner, 19"
N16-1125,1994.amta-1.25,0,0.757488,"Missing"
N18-2079,N16-3003,1,0.868742,"Missing"
N18-2079,P07-2045,0,0.00773552,"Missing"
N18-2079,N12-1048,0,0.0553586,", or the end of sentence is not clearly marked, the system must operate on a buffered sequence. Generating translations for such incomplete sequences presents a considerable challenge for machine translation, more so in the case of syntactically divergent language pairs (such as German-English), where the context required to correctly translate a sentence, appears much later in the sequence, and prematurely committing to a translation leads to significant loss in quality. Various strategies to select appropriate segmentation points in a streaming input have been proposed (F¨ugen et al., 2007; Bangalore et al., 2012; Sridhar et al., 2013; Yarmohammadi et al., 2013; Oda et al., 2014). A downside of this approach is that the MT system translates sequences independent of each other, ignoring the context. Even if the segmenter decides perfect points to segment the input stream, an MT system requires lexical history to make the correct decision. The remaining paper is organized as follow: Section 2 describes modifications to the NMT decoder to enable stream decoding. Section 3 describes various agents to learn a READ/WRITE strategy. Section 4 presents evaluation and results. Section 5 describes modifications"
N18-2079,2015.iwslt-evaluation.11,0,0.0460089,"an agent introduced by Gu et al. (2017) and Satija and Pineau (2016), but without the overhead of expensive training for the agent. Figure 4: Averaged results on test-sets (2011-2014) using the models trained on small and large datasets using AP  0.75. Detailed test-wise results are available in the supplementary material. Scalability: The preliminary results were obtained using models trained on the TED corpus only. We conducted further experiments by training models on larger data-sets (See the supplementary section again for data sizes) to see if our findings are scalable. We fine-tuned (Luong and Manning, 2015; Sajjad et al., 2017b) our models with the in-domain data to avoid domain disparity. We then re-ran our agents with the best S,RW values (with an AP under 0.75) for each language pair. Figure 4 (“large” models) shows that the BLEU loss from the respective oracle increased when the models were trained with bigger data sizes. This could be attributed to the increased lexical ambiguity from the large amount of out-domain data, which can only be resolved with additional contextual information. However our results were still better than the WIW agent, which also has an AP value above 0.8. Allowing"
N18-2079,P14-2090,0,0.431005,"n a buffered sequence. Generating translations for such incomplete sequences presents a considerable challenge for machine translation, more so in the case of syntactically divergent language pairs (such as German-English), where the context required to correctly translate a sentence, appears much later in the sequence, and prematurely committing to a translation leads to significant loss in quality. Various strategies to select appropriate segmentation points in a streaming input have been proposed (F¨ugen et al., 2007; Bangalore et al., 2012; Sridhar et al., 2013; Yarmohammadi et al., 2013; Oda et al., 2014). A downside of this approach is that the MT system translates sequences independent of each other, ignoring the context. Even if the segmenter decides perfect points to segment the input stream, an MT system requires lexical history to make the correct decision. The remaining paper is organized as follow: Section 2 describes modifications to the NMT decoder to enable stream decoding. Section 3 describes various agents to learn a READ/WRITE strategy. Section 4 presents evaluation and results. Section 5 describes modifications to the NMT training to mimic corresponding decoding strategy, and Se"
N18-2079,P17-2095,1,0.839756,"u et al. (2017) and Satija and Pineau (2016), but without the overhead of expensive training for the agent. Figure 4: Averaged results on test-sets (2011-2014) using the models trained on small and large datasets using AP  0.75. Detailed test-wise results are available in the supplementary material. Scalability: The preliminary results were obtained using models trained on the TED corpus only. We conducted further experiments by training models on larger data-sets (See the supplementary section again for data sizes) to see if our findings are scalable. We fine-tuned (Luong and Manning, 2015; Sajjad et al., 2017b) our models with the in-domain data to avoid domain disparity. We then re-ran our agents with the best S,RW values (with an AP under 0.75) for each language pair. Figure 4 (“large” models) shows that the BLEU loss from the respective oracle increased when the models were trained with bigger data sizes. This could be attributed to the increased lexical ambiguity from the large amount of out-domain data, which can only be resolved with additional contextual information. However our results were still better than the WIW agent, which also has an AP value above 0.8. Allowing similar AP, our STAT"
N18-2079,P11-1105,1,0.811515,"escribed in Cho and Esipova (2016). The Wait-if-Worse (WIW) agent WRITES 495 Figure 3: Results for various streaming AGENTS (WID, WIW, WUE, C6 (Chunk decoding with a N=6) and S,RW for STATIC-RW) on the tune-set. For each AP bucket, we only show the Agents with the top 3 BLEU scores in that bucket, with remaining listed in descending order of their BLEU scores. bigger challenge and requires larger context than other language pairs. For example the conjugated verb in a German verb complex appears in the second position, while the main verb almost always occurs at the end of the sentence/phrase (Durrani et al., 2011). Our methods are also comparable to the more sophisticated techniques involving Reinforcement Learning to learn an agent introduced by Gu et al. (2017) and Satija and Pineau (2016), but without the overhead of expensive training for the agent. Figure 4: Averaged results on test-sets (2011-2014) using the models trained on small and large datasets using AP  0.75. Detailed test-wise results are available in the supplementary material. Scalability: The preliminary results were obtained using models trained on the TED corpus only. We conducted further experiments by training models on larger dat"
N18-2079,J15-2001,1,0.906195,"Missing"
N18-2079,E17-2045,0,0.0284211,"u et al. (2017) and Satija and Pineau (2016), but without the overhead of expensive training for the agent. Figure 4: Averaged results on test-sets (2011-2014) using the models trained on small and large datasets using AP  0.75. Detailed test-wise results are available in the supplementary material. Scalability: The preliminary results were obtained using models trained on the TED corpus only. We conducted further experiments by training models on larger data-sets (See the supplementary section again for data sizes) to see if our findings are scalable. We fine-tuned (Luong and Manning, 2015; Sajjad et al., 2017b) our models with the in-domain data to avoid domain disparity. We then re-ran our agents with the best S,RW values (with an AP under 0.75) for each language pair. Figure 4 (“large” models) shows that the BLEU loss from the respective oracle increased when the models were trained with bigger data sizes. This could be attributed to the increased lexical ambiguity from the large amount of out-domain data, which can only be resolved with additional contextual information. However our results were still better than the WIW agent, which also has an AP value above 0.8. Allowing similar AP, our STAT"
N18-2079,N13-1073,0,0.133373,"Missing"
N18-2079,eisele-chen-2010-multiun,0,0.0193242,"Missing"
N18-2079,P16-1162,0,0.318923,"Missing"
N18-2079,E17-1099,0,0.103077,"g with a N=6) and S,RW for STATIC-RW) on the tune-set. For each AP bucket, we only show the Agents with the top 3 BLEU scores in that bucket, with remaining listed in descending order of their BLEU scores. bigger challenge and requires larger context than other language pairs. For example the conjugated verb in a German verb complex appears in the second position, while the main verb almost always occurs at the end of the sentence/phrase (Durrani et al., 2011). Our methods are also comparable to the more sophisticated techniques involving Reinforcement Learning to learn an agent introduced by Gu et al. (2017) and Satija and Pineau (2016), but without the overhead of expensive training for the agent. Figure 4: Averaged results on test-sets (2011-2014) using the models trained on small and large datasets using AP  0.75. Detailed test-wise results are available in the supplementary material. Scalability: The preliminary results were obtained using models trained on the TED corpus only. We conducted further experiments by training models on larger data-sets (See the supplementary section again for data sizes) to see if our findings are scalable. We fine-tuned (Luong and Manning, 2015; Sajjad et al.,"
N18-2079,J81-4005,0,0.707264,"Missing"
N19-1154,E17-2039,0,0.0379246,"Missing"
N19-1154,P17-1080,1,0.899859,"r for modeling non-local syntactic and semantic dependencies, character-based ones are superior for morphology and are also more robust to noise. There is also value in combining different representations. 2 Related Work Representation analysis aims at demystifying what is learned inside the neural network blackbox. This includes analyzing word and sentence embeddings (Adi et al., 2017; Qian et al., 2016b; Ganesh et al., 2017; Conneau et al., 2018, among others), RNN states (Qian et al., 2016a; Shi et al., 2016; Wu and King, 2016; Wang et al., 2017), and NMT representations (Shi et al., 2016; Belinkov et al., 2017a), as applied to morphological (Vylomova et al., 2017; Dalvi et al., 2017), semantic (Qian et al., 2016b; Belinkov et al., 2017b) and syntactic (Linzen et al., 2016; Tran et al., 2018; Conneau et al., 2018) tasks. See Belinkov and Glass (2019) for a recent survey. Other studies carried a more fine-grained neuronlevel analysis for NMT and LM (Dalvi et al., 2019; Bau et al., 2019; Lakretz et al., 2019). While previous work focused on words, here we compare units of different granularities. Subword translation units aim at reducing the vocabulary size and the out-of-vocabulary (OOV) rate. Resear"
N19-1154,Q19-1004,1,0.851128,"ysis aims at demystifying what is learned inside the neural network blackbox. This includes analyzing word and sentence embeddings (Adi et al., 2017; Qian et al., 2016b; Ganesh et al., 2017; Conneau et al., 2018, among others), RNN states (Qian et al., 2016a; Shi et al., 2016; Wu and King, 2016; Wang et al., 2017), and NMT representations (Shi et al., 2016; Belinkov et al., 2017a), as applied to morphological (Vylomova et al., 2017; Dalvi et al., 2017), semantic (Qian et al., 2016b; Belinkov et al., 2017b) and syntactic (Linzen et al., 2016; Tran et al., 2018; Conneau et al., 2018) tasks. See Belinkov and Glass (2019) for a recent survey. Other studies carried a more fine-grained neuronlevel analysis for NMT and LM (Dalvi et al., 2019; Bau et al., 2019; Lakretz et al., 2019). While previous work focused on words, here we compare units of different granularities. Subword translation units aim at reducing the vocabulary size and the out-of-vocabulary (OOV) rate. Researchers have used BPE units (Sennrich et al., 2016), morphological segmentation (Bradbury and Socher, 2016), characters (Durrani et al., 2014; Lee et al., 2017), and hybrid units (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016) to address th"
N19-1154,I17-1001,1,0.895397,"Missing"
N19-1154,C16-1333,0,0.0223916,"U (Papineni et al., 2002). We trained the morphological classifiers and we tested them on a concatenation of the NEWS and the TED testsets, which were automatically tagged as described in the next paragraph. We trained and evaluated the semantic and the syntactic classifiers on existing annotated corpora. See Table 3 for details about the datasets. 1507 Taggers We used RDRPOST (Nguyen et al., 2014) to annotate data for the classifier. For semantic tagging, we used the gold-annotated semantic tags from the Groningen Parallel Meaning Bank (Abzianidze et al., 2017), which were made available by (Bjerva et al., 2016). The tags are grouped into coarse categories such as events, names, time, and logical expressions. There is enough data for English (≈42K), and we randomly sampled the same amount of data we used to train our morphological classifiers to train the semantic classifiers. Yet, only 1,863 annotated sentences (12,783 tokens) were available for German. Thus, in the experiments, we performed 5-fold cross-validation. For CCG supertagging, we used the English CCGBank (Hockenmaier and Steedman, 2007), which contains 41,586/2,407 train/test sentences.4 See Table 3 for more detailed statistics about the"
N19-1154,W16-2308,0,0.171762,"embeddings for 3M, 2M, and 2.5M words/phrases. The problem is typically addressed using byte-pair encoding (BPE), where words are segmented into pseudo-word sequences (Sennrich et al., 2016). 1504 Proceedings of NAACL-HLT 2019, pages 1504–1516 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics A less popular solution is to use characters as the basic unit (Chung et al., 2016; Lee et al., 2017), and in the case of morphologically complex languages, yet another alternative is to reduce the vocabulary size by using unsupervised morpheme segmentation (Bradbury and Socher, 2016). The impact of using different units of representation in NMT models has been studied in previous work (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016; Chung et al., 2016; Lee et al., 2017, among others), but the focus has been exclusively on the quality of the resulting translation output. However, it remains unclear what input and output units should be chosen if we are primarily interested in representation learning. Here, we aim at bridging this gap by evaluating the quality of NMT-derived embeddings originating from units of different granularity when used for modeling morphology, s"
N19-1154,P16-1160,0,0.0282665,"st NLP applications need to handle vocabularies of millions of words, e.g., word2vec (Mikolov et al., 2013b), GloVe (Pennington et al., 2014) and FastText (Mikolov et al., 2018) offer pre-trained embeddings for 3M, 2M, and 2.5M words/phrases. The problem is typically addressed using byte-pair encoding (BPE), where words are segmented into pseudo-word sequences (Sennrich et al., 2016). 1504 Proceedings of NAACL-HLT 2019, pages 1504–1516 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics A less popular solution is to use characters as the basic unit (Chung et al., 2016; Lee et al., 2017), and in the case of morphologically complex languages, yet another alternative is to reduce the vocabulary size by using unsupervised morpheme segmentation (Bradbury and Socher, 2016). The impact of using different units of representation in NMT models has been studied in previous work (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016; Chung et al., 2016; Lee et al., 2017, among others), but the focus has been exclusively on the quality of the resulting translation output. However, it remains unclear what input and output units should be chosen if we are primarily intere"
N19-1154,P18-1198,0,0.0296093,"Missing"
N19-1154,P16-2058,0,0.042031,"Missing"
N19-1154,I17-1015,1,0.857534,"ones are superior for morphology and are also more robust to noise. There is also value in combining different representations. 2 Related Work Representation analysis aims at demystifying what is learned inside the neural network blackbox. This includes analyzing word and sentence embeddings (Adi et al., 2017; Qian et al., 2016b; Ganesh et al., 2017; Conneau et al., 2018, among others), RNN states (Qian et al., 2016a; Shi et al., 2016; Wu and King, 2016; Wang et al., 2017), and NMT representations (Shi et al., 2016; Belinkov et al., 2017a), as applied to morphological (Vylomova et al., 2017; Dalvi et al., 2017), semantic (Qian et al., 2016b; Belinkov et al., 2017b) and syntactic (Linzen et al., 2016; Tran et al., 2018; Conneau et al., 2018) tasks. See Belinkov and Glass (2019) for a recent survey. Other studies carried a more fine-grained neuronlevel analysis for NMT and LM (Dalvi et al., 2019; Bau et al., 2019; Lakretz et al., 2019). While previous work focused on words, here we compare units of different granularities. Subword translation units aim at reducing the vocabulary size and the out-of-vocabulary (OOV) rate. Researchers have used BPE units (Sennrich et al., 2016), morphological segmentati"
N19-1154,N19-1423,0,0.121144,"modeling (LM) using long short-term memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997). It has been further argued that complex auxiliary tasks such as neural machine translation (NMT) are better tailored for representation learning, as the internal understanding of the input language that needs to be built by the network to be able to translate from one language to another needs to be much more comprehensive compared to what would be needed for a simple word prediction task. This idea is implemented in the seq2seqbased CoVe model (McCann et al., 2017). More recently, the BERT model (Devlin et al., 2019) proposed to use representation from another NMT model, the Transformer, while optimizing for two LM-related auxiliary tasks: (i) masked language model and (ii) next sentence prediction. Another important aspect of representation learning is the basic unit the model operates on. In word2vec-style embeddings, it is the word, but this does not hold for NMT-based models, as computational and memory limitations, as of present, prevent NMT from using a large vocabulary, typically limiting it to 30-50k words (Wu et al., 2016). This is a severe limitation, as most NLP applications need to handle voca"
N19-1154,E14-4029,1,0.793179,"al., 2017b) and syntactic (Linzen et al., 2016; Tran et al., 2018; Conneau et al., 2018) tasks. See Belinkov and Glass (2019) for a recent survey. Other studies carried a more fine-grained neuronlevel analysis for NMT and LM (Dalvi et al., 2019; Bau et al., 2019; Lakretz et al., 2019). While previous work focused on words, here we compare units of different granularities. Subword translation units aim at reducing the vocabulary size and the out-of-vocabulary (OOV) rate. Researchers have used BPE units (Sennrich et al., 2016), morphological segmentation (Bradbury and Socher, 2016), characters (Durrani et al., 2014; Lee et al., 2017), and hybrid units (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016) to address the OOV word problem in MT. The choice of translation unit impacts what the network learns. Sennrich (2017) carried a systematic error analysis by comparing subword versus character units and found the latter to be better at handling OOV and transliterations, whereas BPEbased subword units were better at capturing syntactic dependencies. In contrast, here we focus on representation learning, not translation quality. Robustness to noise is an important aspect in machine learning. It has been s"
N19-1154,P18-2006,0,0.0202511,"rns. Sennrich (2017) carried a systematic error analysis by comparing subword versus character units and found the latter to be better at handling OOV and transliterations, whereas BPEbased subword units were better at capturing syntactic dependencies. In contrast, here we focus on representation learning, not translation quality. Robustness to noise is an important aspect in machine learning. It has been studied for various models (Szegedy et al., 2014; Goodfellow et al., 2015), including NLP in general (Papernot et al., 2016; Samanta and Mehta, 2017; Liang et al., 2018; Jia and Liang, 2017; Ebrahimi et al., 2018; Gao et al., 2018), and character-based NMT in particular (Heigold et al., 2018; Belinkov and Bisk, 2018). Unlike this work, we compare robustness to noise for units of different granularity. Moreover, we focus on representation learning rather than on the quality of the translation output. 3 Methodology Our methodology is inspired by research on interpreting neural network (NN) models. A typical framework involves extracting feature representations from different components (e.g., encoder/decoder) of a trained model and then training a classifier to make predictions for an auxiliary task. Th"
N19-1154,W18-1807,0,0.473957,"sus character units and found the latter to be better at handling OOV and transliterations, whereas BPEbased subword units were better at capturing syntactic dependencies. In contrast, here we focus on representation learning, not translation quality. Robustness to noise is an important aspect in machine learning. It has been studied for various models (Szegedy et al., 2014; Goodfellow et al., 2015), including NLP in general (Papernot et al., 2016; Samanta and Mehta, 2017; Liang et al., 2018; Jia and Liang, 2017; Ebrahimi et al., 2018; Gao et al., 2018), and character-based NMT in particular (Heigold et al., 2018; Belinkov and Bisk, 2018). Unlike this work, we compare robustness to noise for units of different granularity. Moreover, we focus on representation learning rather than on the quality of the translation output. 3 Methodology Our methodology is inspired by research on interpreting neural network (NN) models. A typical framework involves extracting feature representations from different components (e.g., encoder/decoder) of a trained model and then training a classifier to make predictions for an auxiliary task. The performance of the trained classifier is considered to be a proxy for judging"
N19-1154,P06-1064,0,0.0101294,"ed the source side with word/BPE/Morfessor/character units. Similarly, when analyzing the representations from the decoder side, we trained the encoder representation with BPE units, and we varied the decoder side using word/BPE/char units. Our motivation for this setup is that we wanted to analyze the encoder/decoder side representations in isolation, keeping the other half of the network (i.e., the decoder/encoder) static across different settings.6 6 4 There are no available CCG banks for the other languages we experiment with, except for a German CCG bank, which is not publicly available (Hockenmaier, 2006). 5 The decoder has to be unidirectional as, at decoding time, the future is unknown. 6 Heigold et al. (2018) used a similar setup. Results We now present the evaluation results for using representations learned from different input units to predict morphology, semantics, and syntax. For subword and character units, we found the activation of the last subword/character unit of a word to be consistently better than using the average of all activations (See Table 4). Therefore, we report only the results using the Last method, for the remainder of the paper. de Last Avg cs ru sub char sub char s"
N19-1154,J07-3004,0,0.00913541,"ed semantic tags from the Groningen Parallel Meaning Bank (Abzianidze et al., 2017), which were made available by (Bjerva et al., 2016). The tags are grouped into coarse categories such as events, names, time, and logical expressions. There is enough data for English (≈42K), and we randomly sampled the same amount of data we used to train our morphological classifiers to train the semantic classifiers. Yet, only 1,863 annotated sentences (12,783 tokens) were available for German. Thus, in the experiments, we performed 5-fold cross-validation. For CCG supertagging, we used the English CCGBank (Hockenmaier and Steedman, 2007), which contains 41,586/2,407 train/test sentences.4 See Table 3 for more detailed statistics about the train/dev/test datasets we used. In our experiments, we used 50k BPE operations and we limited the vocabulary of all systems to 50k. Moreover, we trained the word, BPE, Morfessor, and character-based systems with maximum sentence lengths of 80, 100, 100, and 400 units, respectively. For the classification tasks, we used a logistic regression classifier whose input is either the hidden states in the case of the word-based models, or the Last or the Average representations in the case of chara"
N19-1154,D17-1215,0,0.019313,"what the network learns. Sennrich (2017) carried a systematic error analysis by comparing subword versus character units and found the latter to be better at handling OOV and transliterations, whereas BPEbased subword units were better at capturing syntactic dependencies. In contrast, here we focus on representation learning, not translation quality. Robustness to noise is an important aspect in machine learning. It has been studied for various models (Szegedy et al., 2014; Goodfellow et al., 2015), including NLP in general (Papernot et al., 2016; Samanta and Mehta, 2017; Liang et al., 2018; Jia and Liang, 2017; Ebrahimi et al., 2018; Gao et al., 2018), and character-based NMT in particular (Heigold et al., 2018; Belinkov and Bisk, 2018). Unlike this work, we compare robustness to noise for units of different granularity. Moreover, we focus on representation learning rather than on the quality of the translation output. 3 Methodology Our methodology is inspired by research on interpreting neural network (NN) models. A typical framework involves extracting feature representations from different components (e.g., encoder/decoder) of a trained model and then training a classifier to make predictions fo"
N19-1154,N19-1002,0,0.0260977,"016b; Ganesh et al., 2017; Conneau et al., 2018, among others), RNN states (Qian et al., 2016a; Shi et al., 2016; Wu and King, 2016; Wang et al., 2017), and NMT representations (Shi et al., 2016; Belinkov et al., 2017a), as applied to morphological (Vylomova et al., 2017; Dalvi et al., 2017), semantic (Qian et al., 2016b; Belinkov et al., 2017b) and syntactic (Linzen et al., 2016; Tran et al., 2018; Conneau et al., 2018) tasks. See Belinkov and Glass (2019) for a recent survey. Other studies carried a more fine-grained neuronlevel analysis for NMT and LM (Dalvi et al., 2019; Bau et al., 2019; Lakretz et al., 2019). While previous work focused on words, here we compare units of different granularities. Subword translation units aim at reducing the vocabulary size and the out-of-vocabulary (OOV) rate. Researchers have used BPE units (Sennrich et al., 2016), morphological segmentation (Bradbury and Socher, 2016), characters (Durrani et al., 2014; Lee et al., 2017), and hybrid units (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016) to address the OOV word problem in MT. The choice of translation unit impacts what the network learns. Sennrich (2017) carried a systematic error analysis by comparing subwo"
N19-1154,Q17-1026,0,0.182884,"need to handle vocabularies of millions of words, e.g., word2vec (Mikolov et al., 2013b), GloVe (Pennington et al., 2014) and FastText (Mikolov et al., 2018) offer pre-trained embeddings for 3M, 2M, and 2.5M words/phrases. The problem is typically addressed using byte-pair encoding (BPE), where words are segmented into pseudo-word sequences (Sennrich et al., 2016). 1504 Proceedings of NAACL-HLT 2019, pages 1504–1516 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics A less popular solution is to use characters as the basic unit (Chung et al., 2016; Lee et al., 2017), and in the case of morphologically complex languages, yet another alternative is to reduce the vocabulary size by using unsupervised morpheme segmentation (Bradbury and Socher, 2016). The impact of using different units of representation in NMT models has been studied in previous work (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016; Chung et al., 2016; Lee et al., 2017, among others), but the focus has been exclusively on the quality of the resulting translation output. However, it remains unclear what input and output units should be chosen if we are primarily interested in representat"
N19-1154,Q16-1037,0,0.0359514,"n combining different representations. 2 Related Work Representation analysis aims at demystifying what is learned inside the neural network blackbox. This includes analyzing word and sentence embeddings (Adi et al., 2017; Qian et al., 2016b; Ganesh et al., 2017; Conneau et al., 2018, among others), RNN states (Qian et al., 2016a; Shi et al., 2016; Wu and King, 2016; Wang et al., 2017), and NMT representations (Shi et al., 2016; Belinkov et al., 2017a), as applied to morphological (Vylomova et al., 2017; Dalvi et al., 2017), semantic (Qian et al., 2016b; Belinkov et al., 2017b) and syntactic (Linzen et al., 2016; Tran et al., 2018; Conneau et al., 2018) tasks. See Belinkov and Glass (2019) for a recent survey. Other studies carried a more fine-grained neuronlevel analysis for NMT and LM (Dalvi et al., 2019; Bau et al., 2019; Lakretz et al., 2019). While previous work focused on words, here we compare units of different granularities. Subword translation units aim at reducing the vocabulary size and the out-of-vocabulary (OOV) rate. Researchers have used BPE units (Sennrich et al., 2016), morphological segmentation (Bradbury and Socher, 2016), characters (Durrani et al., 2014; Lee et al., 2017), and h"
N19-1154,L18-1008,0,0.0277678,"y tasks: (i) masked language model and (ii) next sentence prediction. Another important aspect of representation learning is the basic unit the model operates on. In word2vec-style embeddings, it is the word, but this does not hold for NMT-based models, as computational and memory limitations, as of present, prevent NMT from using a large vocabulary, typically limiting it to 30-50k words (Wu et al., 2016). This is a severe limitation, as most NLP applications need to handle vocabularies of millions of words, e.g., word2vec (Mikolov et al., 2013b), GloVe (Pennington et al., 2014) and FastText (Mikolov et al., 2018) offer pre-trained embeddings for 3M, 2M, and 2.5M words/phrases. The problem is typically addressed using byte-pair encoding (BPE), where words are segmented into pseudo-word sequences (Sennrich et al., 2016). 1504 Proceedings of NAACL-HLT 2019, pages 1504–1516 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics A less popular solution is to use characters as the basic unit (Chung et al., 2016; Lee et al., 2017), and in the case of morphologically complex languages, yet another alternative is to reduce the vocabulary size by using unsupervised morph"
N19-1154,E14-2005,0,0.0123355,"7) and IWSLT (Cettolo et al., 2016). We trained the MT models using a concatenation of the NEWS and the TED training datasets, and we tested on official TED test sets (testsets-11-13) to perform the evaluation using BLEU (Papineni et al., 2002). We trained the morphological classifiers and we tested them on a concatenation of the NEWS and the TED testsets, which were automatically tagged as described in the next paragraph. We trained and evaluated the semantic and the syntactic classifiers on existing annotated corpora. See Table 3 for details about the datasets. 1507 Taggers We used RDRPOST (Nguyen et al., 2014) to annotate data for the classifier. For semantic tagging, we used the gold-annotated semantic tags from the Groningen Parallel Meaning Bank (Abzianidze et al., 2017), which were made available by (Bjerva et al., 2016). The tags are grouped into coarse categories such as events, names, time, and logical expressions. There is enough data for English (≈42K), and we randomly sampled the same amount of data we used to train our morphological classifiers to train the semantic classifiers. Yet, only 1,863 annotated sentences (12,783 tokens) were available for German. Thus, in the experiments, we pe"
N19-1154,P02-1040,0,0.105423,"er training data for English (en), German (de), Russian (ru), and Czech (cs). Here, CV stands for cross-validation. 5 Experimental Setup Data and Languages We trained NMT systems for four language pairs: German-English, CzechEnglish, Russian-English, and English-German, using data made available through two popular machine translation campaigns, namely, WMT (Bojar et al., 2017) and IWSLT (Cettolo et al., 2016). We trained the MT models using a concatenation of the NEWS and the TED training datasets, and we tested on official TED test sets (testsets-11-13) to perform the evaluation using BLEU (Papineni et al., 2002). We trained the morphological classifiers and we tested them on a concatenation of the NEWS and the TED testsets, which were automatically tagged as described in the next paragraph. We trained and evaluated the semantic and the syntactic classifiers on existing annotated corpora. See Table 3 for details about the datasets. 1507 Taggers We used RDRPOST (Nguyen et al., 2014) to annotate data for the classifier. For semantic tagging, we used the gold-annotated semantic tags from the Groningen Parallel Meaning Bank (Abzianidze et al., 2017), which were made available by (Bjerva et al., 2016). The"
N19-1154,D14-1162,0,0.0879568,"optimizing for two LM-related auxiliary tasks: (i) masked language model and (ii) next sentence prediction. Another important aspect of representation learning is the basic unit the model operates on. In word2vec-style embeddings, it is the word, but this does not hold for NMT-based models, as computational and memory limitations, as of present, prevent NMT from using a large vocabulary, typically limiting it to 30-50k words (Wu et al., 2016). This is a severe limitation, as most NLP applications need to handle vocabularies of millions of words, e.g., word2vec (Mikolov et al., 2013b), GloVe (Pennington et al., 2014) and FastText (Mikolov et al., 2018) offer pre-trained embeddings for 3M, 2M, and 2.5M words/phrases. The problem is typically addressed using byte-pair encoding (BPE), where words are segmented into pseudo-word sequences (Sennrich et al., 2016). 1504 Proceedings of NAACL-HLT 2019, pages 1504–1516 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics A less popular solution is to use characters as the basic unit (Chung et al., 2016; Lee et al., 2017), and in the case of morphologically complex languages, yet another alternative is to reduce the vocabul"
N19-1154,N18-1202,0,0.211861,"NLP task could be useful for other tasks as well. For example, word embeddings learned for a simple word prediction task in context, word2vec-style (Mikolov et al., 2013b), have now become almost obligatory in state-of-the-art NLP models. One issue with such word embeddings is that the resulting representation is context-independent. Recently, it has been shown that huge performance gains can be achieved by contextualizing the representations, so that the same word could have a different embedding in different contexts. This is best achieved by changing the auxiliary task. For example, ELMo (Peters et al., 2018) learns contextualized word embeddings from language modeling (LM) using long short-term memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997). It has been further argued that complex auxiliary tasks such as neural machine translation (NMT) are better tailored for representation learning, as the internal understanding of the input language that needs to be built by the network to be able to translate from one language to another needs to be much more comprehensive compared to what would be needed for a simple word prediction task. This idea is implemented in the seq2seqbased CoVe model (M"
N19-1154,P17-2095,1,0.861481,"rent word.2 Word Representation Units We consider four representation units: words, byte-pair encoding (BPE) units, morphological units, and characters. Table 2 shows an example of each representation unit. BPE splits words into symbols (a symbol is a sequence of characters) and then iteratively replaces the most frequent sequences of symbols with a new merged symbol. In essence, frequent character n-grams merge to form one symbol. The number of merge operations is controlled by a hyper-parameter OP; a high value of OP means coarse segmentation and a low value means fine-grained segmentation (Sajjad et al., 2017). For morphologically segmented units, we use an unsupervised morphological segmenter, Morfessor (Smit et al., 2014). Note that although BPE and Morfessor segment words at a similar level of granularity, the segmentation generated by Morfessor is linguistically motivated. For example, it splits the gerund verb shooting into root shoot and the suffix ing. Compare this to the BPE segmentation sho + oting, which has no linguistic connotation. On the extreme, the fully character-level units treat each word as a sequence of characters. Extracting Activations for Subword and Character Units (ii) Las"
N19-1154,E17-2060,0,0.0280466,"MT and LM (Dalvi et al., 2019; Bau et al., 2019; Lakretz et al., 2019). While previous work focused on words, here we compare units of different granularities. Subword translation units aim at reducing the vocabulary size and the out-of-vocabulary (OOV) rate. Researchers have used BPE units (Sennrich et al., 2016), morphological segmentation (Bradbury and Socher, 2016), characters (Durrani et al., 2014; Lee et al., 2017), and hybrid units (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016) to address the OOV word problem in MT. The choice of translation unit impacts what the network learns. Sennrich (2017) carried a systematic error analysis by comparing subword versus character units and found the latter to be better at handling OOV and transliterations, whereas BPEbased subword units were better at capturing syntactic dependencies. In contrast, here we focus on representation learning, not translation quality. Robustness to noise is an important aspect in machine learning. It has been studied for various models (Szegedy et al., 2014; Goodfellow et al., 2015), including NLP in general (Papernot et al., 2016; Samanta and Mehta, 2017; Liang et al., 2018; Jia and Liang, 2017; Ebrahimi et al., 201"
N19-1154,P16-1162,0,0.661548,"but this does not hold for NMT-based models, as computational and memory limitations, as of present, prevent NMT from using a large vocabulary, typically limiting it to 30-50k words (Wu et al., 2016). This is a severe limitation, as most NLP applications need to handle vocabularies of millions of words, e.g., word2vec (Mikolov et al., 2013b), GloVe (Pennington et al., 2014) and FastText (Mikolov et al., 2018) offer pre-trained embeddings for 3M, 2M, and 2.5M words/phrases. The problem is typically addressed using byte-pair encoding (BPE), where words are segmented into pseudo-word sequences (Sennrich et al., 2016). 1504 Proceedings of NAACL-HLT 2019, pages 1504–1516 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics A less popular solution is to use characters as the basic unit (Chung et al., 2016; Lee et al., 2017), and in the case of morphologically complex languages, yet another alternative is to reduce the vocabulary size by using unsupervised morpheme segmentation (Bradbury and Socher, 2016). The impact of using different units of representation in NMT models has been studied in previous work (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016; Chung e"
N19-1154,D16-1159,0,0.0278884,"respect to noise. We found that while representations derived from morphological segments are better for modeling non-local syntactic and semantic dependencies, character-based ones are superior for morphology and are also more robust to noise. There is also value in combining different representations. 2 Related Work Representation analysis aims at demystifying what is learned inside the neural network blackbox. This includes analyzing word and sentence embeddings (Adi et al., 2017; Qian et al., 2016b; Ganesh et al., 2017; Conneau et al., 2018, among others), RNN states (Qian et al., 2016a; Shi et al., 2016; Wu and King, 2016; Wang et al., 2017), and NMT representations (Shi et al., 2016; Belinkov et al., 2017a), as applied to morphological (Vylomova et al., 2017; Dalvi et al., 2017), semantic (Qian et al., 2016b; Belinkov et al., 2017b) and syntactic (Linzen et al., 2016; Tran et al., 2018; Conneau et al., 2018) tasks. See Belinkov and Glass (2019) for a recent survey. Other studies carried a more fine-grained neuronlevel analysis for NMT and LM (Dalvi et al., 2019; Bau et al., 2019; Lakretz et al., 2019). While previous work focused on words, here we compare units of different granularities. S"
N19-1154,E14-2006,0,0.157575,"Missing"
N19-1154,D16-1079,0,0.0226064,"Missing"
N19-1154,D18-1503,0,0.0447618,"Missing"
N19-1154,W17-4115,0,0.0210815,"encies, character-based ones are superior for morphology and are also more robust to noise. There is also value in combining different representations. 2 Related Work Representation analysis aims at demystifying what is learned inside the neural network blackbox. This includes analyzing word and sentence embeddings (Adi et al., 2017; Qian et al., 2016b; Ganesh et al., 2017; Conneau et al., 2018, among others), RNN states (Qian et al., 2016a; Shi et al., 2016; Wu and King, 2016; Wang et al., 2017), and NMT representations (Shi et al., 2016; Belinkov et al., 2017a), as applied to morphological (Vylomova et al., 2017; Dalvi et al., 2017), semantic (Qian et al., 2016b; Belinkov et al., 2017b) and syntactic (Linzen et al., 2016; Tran et al., 2018; Conneau et al., 2018) tasks. See Belinkov and Glass (2019) for a recent survey. Other studies carried a more fine-grained neuronlevel analysis for NMT and LM (Dalvi et al., 2019; Bau et al., 2019; Lakretz et al., 2019). While previous work focused on words, here we compare units of different granularities. Subword translation units aim at reducing the vocabulary size and the out-of-vocabulary (OOV) rate. Researchers have used BPE units (Sennrich et al., 2016), mor"
N19-1154,D16-1181,0,0.0642653,"Missing"
N19-1248,D15-1274,0,0.212375,"Missing"
N19-1248,D17-1151,0,0.0558737,"Missing"
N19-1248,W17-1302,1,0.866003,"Missing"
N19-1248,W02-0504,0,0.868168,"Missing"
N19-1248,P05-1071,0,0.215534,"Missing"
N19-1248,N07-2014,0,0.680665,"Missing"
N19-1248,P17-4012,0,0.0215192,"31 2.37 1.99 3.03 2.05 5.97 3.57 3.07 3.93 3.04 7.79 5.49 4.77 6.40 4.77 2.01 1.49 1.30 1.78 1.29 0.00 0.00 0.00 0.00 0.00 12 Combination ∗ 09 +† 11 1.89 2.89 4.49 1.21 0.00 Table 3: Diacritization results: *g represents ngram size e.g. 7g means 7-gram context. Experiment 09 and 11 are comparing NMT models – LSTM-based architecture with attention mechanism and Transformer model Setup and dropout rate = 0.3. The setting for the Transformer were: 6 encoder and 6 decoder layers each of size 512; number of attention heads = 8; feed forward dimension = 2048; and dropout = 0.1. We used the OpenNMT (Klein et al., 2017) implementation with tensorflow for all experiments. System Runs. We conducted a variety of experiments as follows, namely: Word-level experiments where the input is a sequence of words and the output is a sequence of diacritized words: – Baseline Word: uses the full sentences and shows the deficiency of using NMT directly. – Word 7g: uses non-overlapping windows of 7 words to compare to our best character-level model, which also uses a window of length 7. – Word 7g+overlap: uses a sliding window of 7 words. Character-level experiments where the input is represented as a sequence of character"
N19-1248,W18-2507,0,0.137075,"Missing"
N19-1248,W05-0711,0,0.22455,"Missing"
N19-1248,pasha-etal-2014-madamira,0,0.446003,"Missing"
N19-1248,W04-1612,0,0.362427,"Missing"
N19-1248,P06-1073,0,0.232166,"Missing"
N19-1248,E17-2060,0,0.0197172,"not fit in our scenario as it may create source and target segments of different lengths. In the Arabic diacritization problem, both source and target words and characters are strictly tied to each other and loosening it would result in sub-optimal performance and may generate unexpected errors. Context Window. The diacritization of Arabic words is highly sensitive to context. Character representations significantly increase the size of the source and target sequences. This leads to a well known limitation of character-based LSTM-based models, namely poor handling of long range dependencies (Sennrich, 2017). An easy fix is to split sentences greater than a certain length into multiple lines. However, boundary words may loose context in the newly created sequences. To handle this, we propose to keep a fixed size context window c for every word. Given a sentence, we use a sliding context window to split it into segments of overlapping windows of size c as in Table 1. This fixes the problems of both long range dependencies and context of neighboring words. We are further aided by the fact that local context can conclusively determine the correct diacritization in the vast majority of cases. Voting."
N19-1248,P16-1162,0,0.0460472,"split into a sequence of subword units each consisting of a letter and its diacritic(s). For example, source word “AlElm” would be represented as “A/l/E/l/m” and its diacritized target “AaloEalamu” as “Aa/lo/Ea/la/mu”. The character-level representation has several benefits, such as reducing the vocabulary size and avoiding OOV words. The splitting of diacritized 2391 words into subword units simplifies the problem as there will be identical number of source and target tokens in a parallel sentence. Later, we support our design decisions with results in the experiments section. Subwords (BPE (Sennrich et al., 2016)) have been used as a defacto standard in building NMT systems. They are a natural choice to handle unknown words. However, BPE does not fit in our scenario as it may create source and target segments of different lengths. In the Arabic diacritization problem, both source and target words and characters are strictly tied to each other and loosening it would result in sub-optimal performance and may generate unexpected errors. Context Window. The diacritization of Arabic words is highly sensitive to context. Character representations significantly increase the size of the source and target sequ"
P10-1048,P04-1021,0,0.0650915,"model decides whether Differently in Different Contexts to translate or transliterate and how it is able to choose among different valid transliterations given Hindi Urdu SAMPA Gloss the context. Section 8 concludes the paper. / simA Border/Seema / / 2 Previous Work There has been a significant amount of work on transliteration. We can break down previous work into three groups. The first group is generic transliteration work, which is evaluated outside of the context of translation. This work uses either grapheme or phoneme based models to transliterate words lists (Knight and Graehl, 1998; Li et al., 2004; Ekbal et al., 2006; Malik et al., 2008). The work by Malik et al. addresses Hindi to Urdu transliteration using hand-crafted rules and a phonemic representation; it ignores translation context. A second group deals with out-of-vocabulary words for SMT systems built on large parallel corpora, and therefore focuses on name transliteration, which is largely independent of context. AlOnaizan and Knight (2002) transliterate Arabic NEs into English and score them against their respective translations using a modified IBM Model 1. The options are further re-ranked based on different measures such a"
P10-1048,P02-1051,0,0.0189206,"Missing"
P10-1048,C08-1068,0,0.105223,"Different Contexts to translate or transliterate and how it is able to choose among different valid transliterations given Hindi Urdu SAMPA Gloss the context. Section 8 concludes the paper. / simA Border/Seema / / 2 Previous Work There has been a significant amount of work on transliteration. We can break down previous work into three groups. The first group is generic transliteration work, which is evaluated outside of the context of translation. This work uses either grapheme or phoneme based models to transliterate words lists (Knight and Graehl, 1998; Li et al., 2004; Ekbal et al., 2006; Malik et al., 2008). The work by Malik et al. addresses Hindi to Urdu transliteration using hand-crafted rules and a phonemic representation; it ignores translation context. A second group deals with out-of-vocabulary words for SMT systems built on large parallel corpora, and therefore focuses on name transliteration, which is largely independent of context. AlOnaizan and Knight (2002) transliterate Arabic NEs into English and score them against their respective translations using a modified IBM Model 1. The options are further re-ranked based on different measures such as web counts and using coreference to res"
P10-1048,P06-2025,0,0.116436,"ether Differently in Different Contexts to translate or transliterate and how it is able to choose among different valid transliterations given Hindi Urdu SAMPA Gloss the context. Section 8 concludes the paper. / simA Border/Seema / / 2 Previous Work There has been a significant amount of work on transliteration. We can break down previous work into three groups. The first group is generic transliteration work, which is evaluated outside of the context of translation. This work uses either grapheme or phoneme based models to transliterate words lists (Knight and Graehl, 1998; Li et al., 2004; Ekbal et al., 2006; Malik et al., 2008). The work by Malik et al. addresses Hindi to Urdu transliteration using hand-crafted rules and a phonemic representation; it ignores translation context. A second group deals with out-of-vocabulary words for SMT systems built on large parallel corpora, and therefore focuses on name transliteration, which is largely independent of context. AlOnaizan and Knight (2002) transliterate Arabic NEs into English and score them against their respective translations using a modified IBM Model 1. The options are further re-ranked based on different measures such as web counts and usi"
P10-1048,moore-2002-fast,0,0.0206337,"Missing"
P10-1048,J03-1002,0,0.0455825,"Missing"
P10-1048,2001.mtsummit-papers.68,0,0.0625189,"Missing"
P10-1048,P08-1045,0,0.0400247,"Missing"
P10-1048,W07-0703,0,0.192128,"es such as web counts and using coreference to resolve ambiguity. These re-ranking methodologies can not be performed in SMT at the decoding time. An efficient way to compute and re-rank the transliterations of NEs and integrate them on the fly might be possible. However, this is not practical in our case as our model considers transliterations of all input words and not just NEs. A log-linear block transliteration model is applied to OOV NEs in Arabic to English SMT by Zhao et al. (2007). This work is also transliterating only NEs and not doing any disambiguation. The best method proposed by Kashani et al. (2007) integrates translations provided by external sources such as transliteration or rule-base translation of numbers and dates, for an arbitrary number of entries within the input text. Our work is different from Kashani et al. (2007) in that our model compares transliterations with translations Table 2: Hindi Words That Can Be Translated or Transliterated in Different Contexts which focus primarily on name transliteration, because we need different transliterations in different contexts; in their case context is irrelevant. For example: consider the problem of transliterating the English word “r"
P10-1048,2009.mtsummit-caasl.12,0,0.0167695,"table dynamically such that they can directly compete with translations during decoding. This is closer to our approach except that we use transliteration as an alternative to translation for all Hindi words. Our focus is disambiguation of Hindi homonyms whereas they are concentrating only on transliterating NE’s. Moreover, they are working with a large bitext so they can rely on their translation model and only need to transliterate NEs and OOVs. Our translation model is based on data which is both sparse and noisy. Therefore we pit transliterations against translations for every input word. Sinha (2009) presents a rule-based MT system that uses Hindi as a pivot to translate from English to Urdu. This work also uses transliteration only for the translation of unknown words. Their work can not be used for direct translation from Hindi to Urdu (independently of English) “due to various ambiguous mappings that have to be resolved”. The third group uses transliteration models inside of a cross-lingual IR system (AbdulJaleel and Larkey, 2003; Virga and Khudanpur, 2003; Pirkola et al., 2003). Picking a single best transliteration or translation in context is not important in an IR system. Instead,"
P10-1048,W03-1508,0,0.0274178,"translation model is based on data which is both sparse and noisy. Therefore we pit transliterations against translations for every input word. Sinha (2009) presents a rule-based MT system that uses Hindi as a pivot to translate from English to Urdu. This work also uses transliteration only for the translation of unknown words. Their work can not be used for direct translation from Hindi to Urdu (independently of English) “due to various ambiguous mappings that have to be resolved”. The third group uses transliteration models inside of a cross-lingual IR system (AbdulJaleel and Larkey, 2003; Virga and Khudanpur, 2003; Pirkola et al., 2003). Picking a single best transliteration or translation in context is not important in an IR system. Instead, all the options are used by giving them weights and context is typically not taken into account. 3 ematical formulation of our two models, Model-1 and Model-2. 3.1 Model-1 : Conditional Probability Model Applying a noisy channel model to compute the most probable translation u ˆn1 , we get: p(un1 )p(hn1 |un1 ) p(un1 |hn1 ) = arg max arg max n n u1 u1 (1) 3.1.1 Language Model The language model (LM) p(un1 ) is implemented as an n-gram model using the SRILM-Toolkit"
P10-1048,N07-1046,0,0.178172,"ainst their respective translations using a modified IBM Model 1. The options are further re-ranked based on different measures such as web counts and using coreference to resolve ambiguity. These re-ranking methodologies can not be performed in SMT at the decoding time. An efficient way to compute and re-rank the transliterations of NEs and integrate them on the fly might be possible. However, this is not practical in our case as our model considers transliterations of all input words and not just NEs. A log-linear block transliteration model is applied to OOV NEs in Arabic to English SMT by Zhao et al. (2007). This work is also transliterating only NEs and not doing any disambiguation. The best method proposed by Kashani et al. (2007) integrates translations provided by external sources such as transliteration or rule-base translation of numbers and dates, for an arbitrary number of entries within the input text. Our work is different from Kashani et al. (2007) in that our model compares transliterations with translations Table 2: Hindi Words That Can Be Translated or Transliterated in Different Contexts which focus primarily on name transliteration, because we need different transliterations in d"
P10-1048,koen-2004-pharaoh,0,0.0404474,"ng convention in Urdu. For example (can go ; d ZA s@kt de) is alternathe previous section. Putting (11) and (12) in (10) we get p(hn1 |un1 ) = n Y λpw (hi , ui ) + (1 − λ)pc (hi , ui ) λpw (ui ) + (1 − λ)pc (ui ) (13) The idea is to interpolate joint probabilities and divide them by the interpolated marginals. The final equation for Model-2 is given as: i=1 u ˆn1 = arg max n u1 n Y pLM (ui |ui−1 i−k )× i=1 λpw (hi , ui ) + (1 − λ)pc (hi , ui ) λpw (ui ) + (1 − λ)pc (ui ) 3.3 (14) Search The decoder performs a stack-based search using a beam-search algorithm similar to the one used in Pharoah (Koehn, 2004a). It searches for an Urdu string that maximizes the product of translation probability and the language model probability (equation 1) by translating one Hindi word at a time. It is implemented as a two-level process. At the lower level, it computes n-best transliterations for each Hindi word hi according to pc (h, u). The joint probabilities given by pc (h, u) are marginalized for each Urdu transliteration to give pc (h|u). At the higher level, transliteration probabilities are interpolated with pw (h|u) and then multiplied with language model probabilities to give the probability of a hypo"
P10-1048,W04-3250,0,0.0158741,"ng convention in Urdu. For example (can go ; d ZA s@kt de) is alternathe previous section. Putting (11) and (12) in (10) we get p(hn1 |un1 ) = n Y λpw (hi , ui ) + (1 − λ)pc (hi , ui ) λpw (ui ) + (1 − λ)pc (ui ) (13) The idea is to interpolate joint probabilities and divide them by the interpolated marginals. The final equation for Model-2 is given as: i=1 u ˆn1 = arg max n u1 n Y pLM (ui |ui−1 i−k )× i=1 λpw (hi , ui ) + (1 − λ)pc (hi , ui ) λpw (ui ) + (1 − λ)pc (ui ) 3.3 (14) Search The decoder performs a stack-based search using a beam-search algorithm similar to the one used in Pharoah (Koehn, 2004a). It searches for an Urdu string that maximizes the product of translation probability and the language model probability (equation 1) by translating one Hindi word at a time. It is implemented as a two-level process. At the lower level, it computes n-best transliterations for each Hindi word hi according to pc (h, u). The joint probabilities given by pc (h, u) are marginalized for each Urdu transliteration to give pc (h|u). At the higher level, transliteration probabilities are interpolated with pw (h|u) and then multiplied with language model probabilities to give the probability of a hypo"
P10-1048,N10-1077,1,\N,Missing
P10-1048,P02-1040,0,\N,Missing
P10-1048,P07-2045,0,\N,Missing
P10-1048,J98-4003,0,\N,Missing
P11-1044,W10-2407,0,0.0865748,"Missing"
P11-1044,eisele-chen-2010-multiun,0,0.0441317,"corpora, we apply it to parallel corpora of English/Hindi and English/Arabic, and compare the transliteration mining results with a gold standard. 434 Table 2: Cognates from English/Russian corpus extracted by our system as transliteration pairs. None of them are correct transliteration pairs according to the gold standard. We use the English/Hindi corpus from the shared task on word alignment, organized as part of the ACL 2005 Workshop on Building and Using Parallel Texts (WA05) (Martin et al., 2005). For English/Arabic, we use a freely available parallel corpus from the United Nations (UN) (Eisele and Chen, 2010). We randomly take 200,000 parallel sentences from the UN corpus of the year 2000. We create gold standards for both language pairs by randomly selecting a few thousand word pairs from the lists of word pairs extracted from the two corpora. We manually tag them as either transliterations or non-transliterations. The English/Hindi gold standard contains 180 transliteration pairs and 2084 non-transliteration pairs and the English/Arabic gold standard contains 288 transliteration pairs and 6639 non-transliteration pairs. We have submitted these gold standards with the paper. They are available to"
P11-1044,W08-0509,0,0.0150714,"ation pairs as transliterations (see table 3, last column). Most of these word pairs are close transliterations and differ by only one or two characters from perfect transliteration pairs. The close transliteration pairs provide many valid multigrams which may be helpful for the mining system. 4.3 Integration into Word Alignment Model In the previous section, we presented a method for the extraction of transliteration pairs from a parallel corpus. In this section, we will explain how to build a transliteration module on the extracted transliteration pairs and how to integrate it into MGIZA++ (Gao and Vogel, 2008) by interpolating it with the ttable probabilities of the IBM models and the HMM model. MGIZA++ is an extension of GIZA++. It has the ability to resume training from any model rather than starting with Model1. 4.3.1 Modified EM Training of the Word Alignment Models GIZA++ applies the IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) in both directions, i.e., source to target and target to source. The alignments are refined using the grow-diag-final-and heuristic (Koehn et al., 2003). GIZA++ generates a list of translation pairs with alignment probabilities, which is called"
P11-1044,D09-1024,0,0.039788,"-supervised systems on three language pairs. We are only aware of one previous work which uses transliteration information for word alignment. 6 They use the seed data as positive examples. In order to obtain also negative examples, they generate all possible word pairs from the source and target words in the seed data and extract the ones which are not transliterations but have a common substring of some minimal length. 7 They use the phrase table of Moses to build a mapping table between source and target characters. The mapping table is then used to construct a finite state transducer. 438 Hermjakob (2009) proposed a linguistically focused word alignment system which uses many features including hand-crafted transliteration rules for Arabic/English alignment. His evaluation did not explicitly examine the effect of transliteration (alone) on word alignment. We show that the integration of a transliteration system based on unsupervised transliteration mining increases the word alignment quality for the two language pairs we tested. 6 Conclusion We proposed a method to automatically extract transliteration pairs from parallel corpora without supervision or linguistic knowledge. We evaluated it aga"
P11-1044,W10-2405,0,0.0779976,"the test data and add them to the training data. Our method is different from the method of Sherif and Kondrak (2007) as our method is fully unsupervised, and because in each iteration, they add the most probable transliteration pairs to the training data, while we filter out the least probable transliteration pairs from the training data. The transliteration mining systems of the four NEWS10 participants are either based on discriminative or on generative methods. All systems use manually labelled (seed) data for the initial training. The system based on the edit distance method submitted by Jiampojamarn et al. (2010) performs best for the English/Russian task. Jiampojamarn et al. (2010) submitted another system based on a standard n-gram kernel which ranked first for the English/Hindi and English/Tamil tasks.6 For the English/Arabic task, the transliteration mining system of Noeman and Madkour (2010) was best. They normalize the English and Arabic characters in the training data which increases the recall.7 Our transliteration extraction method differs in that we extract transliteration pairs from a parallel corpus without supervision. The results of the NEWS10 experiments (Kumaran et al., 2010) show that"
P11-1044,P06-1103,0,0.112777,"WA05. Three systems, one limited and two un-limited, participated in the English/Hindi task. We outperform the limited system and one un-limited system. 5 Previous Research Previous work on transliteration mining uses a manually labelled set of training data to extract transliteration pairs from a parallel corpus or comparable corpora. The training data may contain a few hundred randomly selected transliteration pairs from a transliteration dictionary (Yoon et al., 2007; Sproat et al., 2006; Lee and Chang, 2003) or just a few carefully selected transliteration pairs (Sherif and Kondrak, 2007; Klementiev and Roth, 2006). Our work is more challenging as we extract transliteration pairs without using transliteration dictionaries or gold standard transliteration pairs. Klementiev and Roth (2006) initialize their transliteration model with a list of 20 transliteration tion; so we did not interpolate in just those iterations of training where we were transitioning from one model to the next. pairs. Their model makes use of temporal scoring to rank the candidate transliterations. A lot of work has been done on discovering and learning transliterations from comparable corpora by using temporal and phonetic informat"
P11-1044,N03-1017,0,0.329806,"am models of order > 1 did not work well because these models tended to learn noise (information from non-transliteration pairs) in the training data. For our experiments, we only trained g2p with the unigram model. In test mode, we look for the best sequence of multigrams given a fixed source and target string and return the probability of this sequence. For the mining process, we trained g2p on lists containing both transliteration pairs and nontransliteration pairs. 2.2 Statistical Machine Transliteration System We build a phrase-based MT system for transliteration using the Moses toolkit (Koehn et al., 2003). We also tried using g2p for implementing the transliteration decoder but found Moses to perform better. Moses has the advantage of using Minimum Error Rate Training (MERT) which optimizes transliteration accuracy rather than the likelihood of the training data as g2p does. The training data contains more non-transliteration pairs than transliteration pairs. We don’t want to maximize the likelihood of the non-transliteration pairs. Instead we want to optimize the transliteration performance for test data. Secondly, it is easy to use a large language model (LM) with Moses. We build the LM on t"
P11-1044,W10-2404,0,0.0889357,"Missing"
P11-1044,W03-0317,0,0.0228532,"sion of our word alignment system We compared our word alignment results with the systems presented at WA05. Three systems, one limited and two un-limited, participated in the English/Hindi task. We outperform the limited system and one un-limited system. 5 Previous Research Previous work on transliteration mining uses a manually labelled set of training data to extract transliteration pairs from a parallel corpus or comparable corpora. The training data may contain a few hundred randomly selected transliteration pairs from a transliteration dictionary (Yoon et al., 2007; Sproat et al., 2006; Lee and Chang, 2003) or just a few carefully selected transliteration pairs (Sherif and Kondrak, 2007; Klementiev and Roth, 2006). Our work is more challenging as we extract transliteration pairs without using transliteration dictionaries or gold standard transliteration pairs. Klementiev and Roth (2006) initialize their transliteration model with a list of 20 transliteration tion; so we did not interpolate in just those iterations of training where we were transitioning from one model to the next. pairs. Their model makes use of temporal scoring to rank the candidate transliterations. A lot of work has been done"
P11-1044,W05-0809,0,0.0389042,"oportion of transliterations than a parallel corpus. In order to examine how well our method performs on parallel corpora, we apply it to parallel corpora of English/Hindi and English/Arabic, and compare the transliteration mining results with a gold standard. 434 Table 2: Cognates from English/Russian corpus extracted by our system as transliteration pairs. None of them are correct transliteration pairs according to the gold standard. We use the English/Hindi corpus from the shared task on word alignment, organized as part of the ACL 2005 Workshop on Building and Using Parallel Texts (WA05) (Martin et al., 2005). For English/Arabic, we use a freely available parallel corpus from the United Nations (UN) (Eisele and Chen, 2010). We randomly take 200,000 parallel sentences from the UN corpus of the year 2000. We create gold standards for both language pairs by randomly selecting a few thousand word pairs from the lists of word pairs extracted from the two corpora. We manually tag them as either transliterations or non-transliterations. The English/Hindi gold standard contains 180 transliteration pairs and 2084 non-transliteration pairs and the English/Arabic gold standard contains 288 transliteration pa"
P11-1044,W10-2408,0,0.0371531,"bable transliteration pairs from the training data. The transliteration mining systems of the four NEWS10 participants are either based on discriminative or on generative methods. All systems use manually labelled (seed) data for the initial training. The system based on the edit distance method submitted by Jiampojamarn et al. (2010) performs best for the English/Russian task. Jiampojamarn et al. (2010) submitted another system based on a standard n-gram kernel which ranked first for the English/Hindi and English/Tamil tasks.6 For the English/Arabic task, the transliteration mining system of Noeman and Madkour (2010) was best. They normalize the English and Arabic characters in the training data which increases the recall.7 Our transliteration extraction method differs in that we extract transliteration pairs from a parallel corpus without supervision. The results of the NEWS10 experiments (Kumaran et al., 2010) show that no single system performs well on all language pairs. Our unsupervised method seems robust as its performance is similar to or better than many of the semi-supervised systems on three language pairs. We are only aware of one previous work which uses transliteration information for word a"
P11-1044,J03-1002,0,0.0209882,"g criterion and return the filtered data set from this iteration. The stopping criterion uses unlabelled held-out data to predict the optimal stopping point. The following sections describe the transliteration mining method in detail. 3.1 Methodology We will first describe the iterative filtering algorithm (Algorithm 1) and then the algorithm for the stopping criterion (Algorithm 2). In practice, we first run Algorithm 2 for 100 iterations to determine the best number of iterations. Then, we run Algorithm 1 for that many iterations. Initially, the parallel corpus is word-aligned using GIZA++ (Och and Ney, 2003), and the alignments are refined using the grow-diag-final-and heuristic (Koehn et al., 2003). We extract all word pairs which occur as 1-to-1 alignments in the word-aligned corpus. We ignore non-1-to-1 alignments because they are less likely to be transliterations for most language pairs. The extracted set of word pairs will be called “list of word pairs” later on. We use the list of word pairs as the training data for Algorithm 1. Algorithm 1 builds a joint sequence model using g2p on the training data and computes the joint probability of all word pairs according to g2p. We normalize the pr"
P11-1044,P07-1109,0,0.223769,"the systems presented at WA05. Three systems, one limited and two un-limited, participated in the English/Hindi task. We outperform the limited system and one un-limited system. 5 Previous Research Previous work on transliteration mining uses a manually labelled set of training data to extract transliteration pairs from a parallel corpus or comparable corpora. The training data may contain a few hundred randomly selected transliteration pairs from a transliteration dictionary (Yoon et al., 2007; Sproat et al., 2006; Lee and Chang, 2003) or just a few carefully selected transliteration pairs (Sherif and Kondrak, 2007; Klementiev and Roth, 2006). Our work is more challenging as we extract transliteration pairs without using transliteration dictionaries or gold standard transliteration pairs. Klementiev and Roth (2006) initialize their transliteration model with a list of 20 transliteration tion; so we did not interpolate in just those iterations of training where we were transitioning from one model to the next. pairs. Their model makes use of temporal scoring to rank the candidate transliterations. A lot of work has been done on discovering and learning transliterations from comparable corpora by using te"
P11-1044,P06-1010,0,0.0192233,"and Pti is the precision of our word alignment system We compared our word alignment results with the systems presented at WA05. Three systems, one limited and two un-limited, participated in the English/Hindi task. We outperform the limited system and one un-limited system. 5 Previous Research Previous work on transliteration mining uses a manually labelled set of training data to extract transliteration pairs from a parallel corpus or comparable corpora. The training data may contain a few hundred randomly selected transliteration pairs from a transliteration dictionary (Yoon et al., 2007; Sproat et al., 2006; Lee and Chang, 2003) or just a few carefully selected transliteration pairs (Sherif and Kondrak, 2007; Klementiev and Roth, 2006). Our work is more challenging as we extract transliteration pairs without using transliteration dictionaries or gold standard transliteration pairs. Klementiev and Roth (2006) initialize their transliteration model with a list of 20 transliteration tion; so we did not interpolate in just those iterations of training where we were transitioning from one model to the next. pairs. Their model makes use of temporal scoring to rank the candidate transliterations. A lot"
P11-1044,W06-1630,0,0.179402,"work is more challenging as we extract transliteration pairs without using transliteration dictionaries or gold standard transliteration pairs. Klementiev and Roth (2006) initialize their transliteration model with a list of 20 transliteration tion; so we did not interpolate in just those iterations of training where we were transitioning from one model to the next. pairs. Their model makes use of temporal scoring to rank the candidate transliterations. A lot of work has been done on discovering and learning transliterations from comparable corpora by using temporal and phonetic information (Tao et al., 2006; Klementiev and Roth, 2006; Sproat et al., 2006). We do not have access to this information. Sherif and Kondrak (2007) train a probabilistic transducer on 14 manually constructed transliteration pairs of English/Arabic. They iteratively extract transliteration pairs from the test data and add them to the training data. Our method is different from the method of Sherif and Kondrak (2007) as our method is fully unsupervised, and because in each iteration, they add the most probable transliteration pairs to the training data, while we filter out the least probable transliteration pairs from the"
P11-1044,C96-2141,0,0.420695,"tion, we presented a method for the extraction of transliteration pairs from a parallel corpus. In this section, we will explain how to build a transliteration module on the extracted transliteration pairs and how to integrate it into MGIZA++ (Gao and Vogel, 2008) by interpolating it with the ttable probabilities of the IBM models and the HMM model. MGIZA++ is an extension of GIZA++. It has the ability to resume training from any model rather than starting with Model1. 4.3.1 Modified EM Training of the Word Alignment Models GIZA++ applies the IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) in both directions, i.e., source to target and target to source. The alignments are refined using the grow-diag-final-and heuristic (Koehn et al., 2003). GIZA++ generates a list of translation pairs with alignment probabilities, which is called the t-table. In this section, we propose a method to modify the translation probabilities of the t-table by interpolating the translation counts with transliteration counts. The interpolation is done in both directions. In the following, we will only consider the e-to-f direction. The transliteration module which is used to calculate the conditional tr"
P11-1044,P07-1015,0,0.0435323,"of baseline GIZA++ and Pti is the precision of our word alignment system We compared our word alignment results with the systems presented at WA05. Three systems, one limited and two un-limited, participated in the English/Hindi task. We outperform the limited system and one un-limited system. 5 Previous Research Previous work on transliteration mining uses a manually labelled set of training data to extract transliteration pairs from a parallel corpus or comparable corpora. The training data may contain a few hundred randomly selected transliteration pairs from a transliteration dictionary (Yoon et al., 2007; Sproat et al., 2006; Lee and Chang, 2003) or just a few carefully selected transliteration pairs (Sherif and Kondrak, 2007; Klementiev and Roth, 2006). Our work is more challenging as we extract transliteration pairs without using transliteration dictionaries or gold standard transliteration pairs. Klementiev and Roth (2006) initialize their transliteration model with a list of 20 transliteration tion; so we did not interpolate in just those iterations of training where we were transitioning from one model to the next. pairs. Their model makes use of temporal scoring to rank the candidate tr"
P11-1044,J93-2003,0,\N,Missing
P12-1049,eisele-chen-2010-multiun,0,0.0346249,"Missing"
P12-1049,W10-2405,0,0.0874072,"led the “seed data”) for initial training. All systems which participated in the NEWS10 shared task are either supervised or semi-supervised. They are described in (Kumaran et al., 2010a). Our transliteration mining model can mine transliterations without using any labelled data. However, if there is some labelled data available, our system is able to use it effectively. The transliteration mining systems evaluated on the NEWS10 dataset generally used heuristic methods, discriminative models or generative models for transliteration mining (Kumaran et al., 2010a). The heuristic-based system of Jiampojamarn et al. (2010) is based on the edit distance method which scores the similarity between source and target words. They presented two discriminative methods – an SVM-based classifier and alignment-based string similarity for transliteration mining. These methods model the conditional probability distribution and require supervised/semi-supervised information for learning. We propose a flexible generative model for transliteration mining usable for both unsupervised and semi-supervised learning. Previous work on generative approaches uses Hidden Markov Models (Nabende, 2010; Darwish, 2010; Jiampojamarn et al.,"
P12-1049,N03-1017,0,0.0184368,"ikipedia pages written in different languages, which may be transliterations or translations. The seed data is a list of 1000 transliteration pairs provided to semi-supervised systems for initial training. We use the seed data only in our semi-supervised system, and not in the unsupervised system. The reference data is a small subset of the training data which is manually annotated with positive and negative examples. 5.1.1 Training We word-aligned the parallel phrases of the training data using GIZA++ (Och and Ney, 2003), and symmetrized the alignments using the grow-diagfinal-and heuristic (Koehn et al., 2003). We extract all word pairs which occur as 1-to-1 alignments (like Sajjad et al. (2011)) and later refer to them as the word-aligned list. We compared the word-aligned list with the NEWS10 reference data and found that the word-aligned list is missing some transliteration pairs because of word-alignment errors. We built another list by adding a word pair for every source word that cooccurs with a target word in a parallel phrase/sentence and call it the cross-product list later on. The cross-product list is noisier but contains almost all transliteration pairs in the corpus. 473 EA EH ET ER 27"
P12-1049,P04-1021,0,0.212852,"Missing"
P12-1049,W05-0809,0,0.0377036,"Missing"
P12-1049,W10-2412,0,0.0242759,"uristic-based system of Jiampojamarn et al. (2010) is based on the edit distance method which scores the similarity between source and target words. They presented two discriminative methods – an SVM-based classifier and alignment-based string similarity for transliteration mining. These methods model the conditional probability distribution and require supervised/semi-supervised information for learning. We propose a flexible generative model for transliteration mining usable for both unsupervised and semi-supervised learning. Previous work on generative approaches uses Hidden Markov Models (Nabende, 2010; Darwish, 2010; Jiampojamarn et al., 2010), Finite State Automata (Noeman and Madkour, 2010) and Bayesian learning (Kahki et al., 2011) to learn transliteration pairs from labelled data. Our method is different from theirs as our generative story explains the unlabelled data using a combination of a transliteration and a non-transliteration sub-model. The transliteration model jointly generates source and target 470 strings, whereas the non-transliteration system generates them independently of each other. Sajjad et al. (2011) proposed a heuristic-based unsupervised transliteration mining sys"
P12-1049,R11-1053,0,0.0189504,"ases. This assumption is not followed for certain phrases like ”New York” and ”New Mexico”. EA EH ET ER Unsupervised SJD OU Semi-supervised/Supervised OS SBest GR DBN 87.4 92.2 90.1 76.0 92.7 96.3 94.6 83.1 92.4 95.7 93.2 79.4 91.5 94.4 91.4 87.5 94.1 93.2 95.5 92.3 95.5 93.9 82.5 Table 2: F-measure results on NEWS10 datasets where SJD is the unsupervised system of Sajjad11, OU is our unsupervised system built on the cross-product list, OS is our semi-supervised system, SBest is the best NEWS10 system, GR is the supervised system of Kahki et al. (2011) and DBN is the semi-supervised system of Nabende (2011) Our unsupervised mining system built on the cross-product list consistently outperforms the one built on the word-aligned list. Later, we consider only the system built on the cross-product list. Table 2 shows the results of our unsupervised system OU in comparison with the unsupervised system of Sajjad11 (SJD), the best semi-supervised systems presented at NEWS10 (SBEST ) and the best semi-supervised results reported on the NEWS10 dataset (GR, DBN ). On three language pairs, our unsupervised system performs better than all semisupervised systems which participated in NEWS10. It has competiti"
P12-1049,W10-2408,0,0.0248795,"method which scores the similarity between source and target words. They presented two discriminative methods – an SVM-based classifier and alignment-based string similarity for transliteration mining. These methods model the conditional probability distribution and require supervised/semi-supervised information for learning. We propose a flexible generative model for transliteration mining usable for both unsupervised and semi-supervised learning. Previous work on generative approaches uses Hidden Markov Models (Nabende, 2010; Darwish, 2010; Jiampojamarn et al., 2010), Finite State Automata (Noeman and Madkour, 2010) and Bayesian learning (Kahki et al., 2011) to learn transliteration pairs from labelled data. Our method is different from theirs as our generative story explains the unlabelled data using a combination of a transliteration and a non-transliteration sub-model. The transliteration model jointly generates source and target 470 strings, whereas the non-transliteration system generates them independently of each other. Sajjad et al. (2011) proposed a heuristic-based unsupervised transliteration mining system. We later call it Sajjad11. It is the only unsupervised mining system that was evaluated"
P12-1049,J03-1002,0,0.0287122,"ta, seed data and reference data. The NEWS10 data consists of pairs of titles of the same Wikipedia pages written in different languages, which may be transliterations or translations. The seed data is a list of 1000 transliteration pairs provided to semi-supervised systems for initial training. We use the seed data only in our semi-supervised system, and not in the unsupervised system. The reference data is a small subset of the training data which is manually annotated with positive and negative examples. 5.1.1 Training We word-aligned the parallel phrases of the training data using GIZA++ (Och and Ney, 2003), and symmetrized the alignments using the grow-diagfinal-and heuristic (Koehn et al., 2003). We extract all word pairs which occur as 1-to-1 alignments (like Sajjad et al. (2011)) and later refer to them as the word-aligned list. We compared the word-aligned list with the NEWS10 reference data and found that the word-aligned list is missing some transliteration pairs because of word-alignment errors. We built another list by adding a word pair for every source word that cooccurs with a target word in a parallel phrase/sentence and call it the cross-product list later on. The cross-product lis"
P12-1049,P11-1044,1,0.89978,"arning. Previous work on generative approaches uses Hidden Markov Models (Nabende, 2010; Darwish, 2010; Jiampojamarn et al., 2010), Finite State Automata (Noeman and Madkour, 2010) and Bayesian learning (Kahki et al., 2011) to learn transliteration pairs from labelled data. Our method is different from theirs as our generative story explains the unlabelled data using a combination of a transliteration and a non-transliteration sub-model. The transliteration model jointly generates source and target 470 strings, whereas the non-transliteration system generates them independently of each other. Sajjad et al. (2011) proposed a heuristic-based unsupervised transliteration mining system. We later call it Sajjad11. It is the only unsupervised mining system that was evaluated on the NEWS10 dataset up until now, as far as we know. That system is computationally expensive. We show in Section 5 that its runtime is much higher than that of our system. In this paper, we propose a novel model-based approach to transliteration mining. Our approach is language pair independent – at least for alphabetic languages – and efficient. Unlike the previous unsupervised system, and unlike the supervised and semi-supervised s"
P12-1049,W10-2407,0,\N,Missing
P12-1049,D11-1128,0,\N,Missing
P12-1049,W10-2403,0,\N,Missing
P12-1049,W10-2404,0,\N,Missing
P13-2001,E06-1047,0,0.149589,"012), or by concatenating the parallel data for both languages (Nakov and Ng, 2009). These translation methods generally require parallel data, for which hardly any exists between dialects and MSA. Instead of translating between a dialect and MSA, we tried to narrow down the lexical, morphological and phonetic gap between them using a character-level conversion model, which we trained on a small set of parallel dialect/MSA word pairs. In the context of Arabic dialects3 , most previous work focused on converting dialects to MSA and vice versa to improve the processing of dialects (Sawaf, 2010; Chiang et al., 2006; Mohamed et al., 2012; Utiyama and Isahara, 2008). Sawaf (2010) proposed a dialect to MSA normalization that used character-level rules and morphological analysis. Salloum and Habash (2011) also used a rule-based method to generate MSA paraphrases of dialectal out-of-vocabulary (OOV) and low frequency words. Instead of rules, we automatically We constructed baselines that were based on the following training data: - An Egyptian/English parallel corpus consisting of ≈38k sentences, which is part of the LDC2012T09 corpus (Zbib et al., 2012). We randomly divided it into 32k sentences for trainin"
P13-2001,P10-1048,1,0.686515,"LM experiments also affirmed the importance of in-domain English LMs. We also showed that a conversion does not imply a straight forward usage of MSA resources and there is a need for adaptation which we fulfilled using phrase-table merging (Nakov and Ng, 2009). 2 2.1 Previous Work Baseline Our work is related to research on MT from a resource poor language (to other languages) by pivoting on a closely related resource rich language. This can be done by either translating between the related languages using word-level translation, character level transformations, and language specific rules (Durrani et al., 2010; Hajiˇc et al., 2000; Nakov and Tiedemann, 2012), or by concatenating the parallel data for both languages (Nakov and Ng, 2009). These translation methods generally require parallel data, for which hardly any exists between dialects and MSA. Instead of translating between a dialect and MSA, we tried to narrow down the lexical, morphological and phonetic gap between them using a character-level conversion model, which we trained on a small set of parallel dialect/MSA word pairs. In the context of Arabic dialects3 , most previous work focused on converting dialects to MSA and vice versa to impr"
P13-2001,N13-1044,0,0.12911,"actic challenge in this sentence, since the Egyptian word order in interrogative sentences is normally different from the MSA word order: the interrogative particle appears at the end of the sentence instead of at the beginning. Addressing this problem might have improved translation. The above analysis suggests that incorporating deeper linguistic information in the conversion procedure could improve translation quality. In particular, using a morphological analyzer seeems like a promising possibility. One approach could be to run a morphological analyzer for dialectal Arabic (e.g. MADA-ARZ (Habash et al., 2013)) on the original EG sentence and another analyzer for MSA (such as MADA) on the converted EG0 sentence, and then to compare the morphological features. Discrepancies should be probabilistically incorporated in the conversion. Exploring this approach is left for future work. 4 Conclusion We presented an Egyptian to English MT system. In contrast to previous work, we used an automatic conversion method to map Egyptian close to MSA. The converted Egyptian EG0 had fewer OOV words and spelling mistakes and improved language handling. The MT system built on the adapted parallel data showed an impro"
P13-2001,A00-1002,0,0.0541606,"Missing"
P13-2001,W11-2123,0,0.0145373,"(Och and Ney, 2003), and symmetrized the alignments using grow-diag-final-and heuristic (Koehn et al., 2003). We trained a phrasal MT system (Koehn et al., 2003). We built five-gram LMs using KenLM 3 Due to space limitations, we restrict discussion to work on dialects only. 4 Arabic News (LDC2004T17), eTIRR (LDC2004E72), and parallel corpora the GALE program 2 B1 B2 B3 B4 Train LM AR EG EG EG GW GW EGen EGen GW BLEU OOV 7.48 12.82 13.94 14.23 6.7 5.2 5.2 5.2 Table 1: Baseline results using the EG and AR training sets with GW and EGen corpora for LM training with modified Kneser-Ney smoothing (Heafield, 2011). In case of more than one LM, we tuned their weights on a development set using Minimum Error Rate Training (Och and Ney, 2003). We built several baseline systems as follows: – B1 used AR for training a translation model and GW for LM. – B2-B4 systems used identical training data, namely EG, with the GW, EGen , or both for B2, B3, and B4 respectively for language modeling. Table 1 reports the baseline results. The system trained on AR (B1) performed poorly compared to the one trained on EG (B2) with a 6.75 BLEU points difference. This highlights the difference between MSA and Egyptian. Using"
P13-2001,N03-1017,0,0.00522573,"g of 200k sentences from LDC4 . We refer to this corpus as the AR corpus. For language modeling, we used either EGen or the English side of the AR corpus plus the English side of NIST12 training data and English GigaWord v5. We refer to this corpus as GW. We tokenized Egyptian and Arabic according to the ATB tokenization scheme using the MADA+TOKAN morphological analyzer and tokenizer v3.1 (Roth et al., 2008). Word elongations were already fixed in the corpus. We wordaligned the parallel data using GIZA++ (Och and Ney, 2003), and symmetrized the alignments using grow-diag-final-and heuristic (Koehn et al., 2003). We trained a phrasal MT system (Koehn et al., 2003). We built five-gram LMs using KenLM 3 Due to space limitations, we restrict discussion to work on dialects only. 4 Arabic News (LDC2004T17), eTIRR (LDC2004E72), and parallel corpora the GALE program 2 B1 B2 B3 B4 Train LM AR EG EG EG GW GW EGen EGen GW BLEU OOV 7.48 12.82 13.94 14.23 6.7 5.2 5.2 5.2 Table 1: Baseline results using the EG and AR training sets with GW and EGen corpora for LM training with modified Kneser-Ney smoothing (Heafield, 2011). In case of more than one LM, we tuned their weights on a development set using Minimum Erro"
P13-2001,P12-2035,0,0.0480451,"ting the parallel data for both languages (Nakov and Ng, 2009). These translation methods generally require parallel data, for which hardly any exists between dialects and MSA. Instead of translating between a dialect and MSA, we tried to narrow down the lexical, morphological and phonetic gap between them using a character-level conversion model, which we trained on a small set of parallel dialect/MSA word pairs. In the context of Arabic dialects3 , most previous work focused on converting dialects to MSA and vice versa to improve the processing of dialects (Sawaf, 2010; Chiang et al., 2006; Mohamed et al., 2012; Utiyama and Isahara, 2008). Sawaf (2010) proposed a dialect to MSA normalization that used character-level rules and morphological analysis. Salloum and Habash (2011) also used a rule-based method to generate MSA paraphrases of dialectal out-of-vocabulary (OOV) and low frequency words. Instead of rules, we automatically We constructed baselines that were based on the following training data: - An Egyptian/English parallel corpus consisting of ≈38k sentences, which is part of the LDC2012T09 corpus (Zbib et al., 2012). We randomly divided it into 32k sentences for training, 2k for development"
P13-2001,N12-1006,0,0.379251,"r, we applied an adaptation method to incorporate MSA/English parallel data. The contributions of this paper are as follows: – We trained an Egyptian/MSA transformation model to make Egyptian look similar to MSA. We publicly released the training data. – We built a phrasal Machine Translation (MT) system on adapted Egyptian/English parallel data, which outperformed a non-adapted baseline by 1.87 BLEU points. – We used phrase-table merging (Nakov and Ng, 2009) to utilize MSA/English parallel data with the available in-domain parallel data. learnt character mappings from dialect/MSA word pairs. Zbib et al. (2012) explored several methods for dialect/English MT. Their best Egyptian/English system was trained on dialect/English parallel data. They used two language models built from the English GigaWord corpus and from a large web crawl. Their best system outperformed manually translating Egyptian to MSA then translating using an MSA/English system. In contrast, we showed that training on in-domain dialectal data irrespective of its small size is better than training on large MSA/English data. Our LM experiments also affirmed the importance of in-domain English LMs. We also showed that a conversion does"
P13-2001,D09-1141,0,0.390382,"using character-level transformations and word n-gram models that handle spelling mistakes, phonological variations, and morphological transformations. Later, we applied an adaptation method to incorporate MSA/English parallel data. The contributions of this paper are as follows: – We trained an Egyptian/MSA transformation model to make Egyptian look similar to MSA. We publicly released the training data. – We built a phrasal Machine Translation (MT) system on adapted Egyptian/English parallel data, which outperformed a non-adapted baseline by 1.87 BLEU points. – We used phrase-table merging (Nakov and Ng, 2009) to utilize MSA/English parallel data with the available in-domain parallel data. learnt character mappings from dialect/MSA word pairs. Zbib et al. (2012) explored several methods for dialect/English MT. Their best Egyptian/English system was trained on dialect/English parallel data. They used two language models built from the English GigaWord corpus and from a large web crawl. Their best system outperformed manually translating Egyptian to MSA then translating using an MSA/English system. In contrast, we showed that training on in-domain dialectal data irrespective of its small size is bett"
P13-2001,P12-2059,0,0.0156256,"e of in-domain English LMs. We also showed that a conversion does not imply a straight forward usage of MSA resources and there is a need for adaptation which we fulfilled using phrase-table merging (Nakov and Ng, 2009). 2 2.1 Previous Work Baseline Our work is related to research on MT from a resource poor language (to other languages) by pivoting on a closely related resource rich language. This can be done by either translating between the related languages using word-level translation, character level transformations, and language specific rules (Durrani et al., 2010; Hajiˇc et al., 2000; Nakov and Tiedemann, 2012), or by concatenating the parallel data for both languages (Nakov and Ng, 2009). These translation methods generally require parallel data, for which hardly any exists between dialects and MSA. Instead of translating between a dialect and MSA, we tried to narrow down the lexical, morphological and phonetic gap between them using a character-level conversion model, which we trained on a small set of parallel dialect/MSA word pairs. In the context of Arabic dialects3 , most previous work focused on converting dialects to MSA and vice versa to improve the processing of dialects (Sawaf, 2010; Chia"
P13-2001,J03-1002,0,0.00581558,"(2012) to directly compare to their results. - An MSA/English parallel corpus consisting of 200k sentences from LDC4 . We refer to this corpus as the AR corpus. For language modeling, we used either EGen or the English side of the AR corpus plus the English side of NIST12 training data and English GigaWord v5. We refer to this corpus as GW. We tokenized Egyptian and Arabic according to the ATB tokenization scheme using the MADA+TOKAN morphological analyzer and tokenizer v3.1 (Roth et al., 2008). Word elongations were already fixed in the corpus. We wordaligned the parallel data using GIZA++ (Och and Ney, 2003), and symmetrized the alignments using grow-diag-final-and heuristic (Koehn et al., 2003). We trained a phrasal MT system (Koehn et al., 2003). We built five-gram LMs using KenLM 3 Due to space limitations, we restrict discussion to work on dialects only. 4 Arabic News (LDC2004T17), eTIRR (LDC2004E72), and parallel corpora the GALE program 2 B1 B2 B3 B4 Train LM AR EG EG EG GW GW EGen EGen GW BLEU OOV 7.48 12.82 13.94 14.23 6.7 5.2 5.2 5.2 Table 1: Baseline results using the EG and AR training sets with GW and EGen corpora for LM training with modified Kneser-Ney smoothing (Heafield, 2011). In"
P13-2001,P08-2030,0,0.0379788,"is corpus as EG and the English part of it as EGen . We did not have access to the training/test splits of Zbib et al. (2012) to directly compare to their results. - An MSA/English parallel corpus consisting of 200k sentences from LDC4 . We refer to this corpus as the AR corpus. For language modeling, we used either EGen or the English side of the AR corpus plus the English side of NIST12 training data and English GigaWord v5. We refer to this corpus as GW. We tokenized Egyptian and Arabic according to the ATB tokenization scheme using the MADA+TOKAN morphological analyzer and tokenizer v3.1 (Roth et al., 2008). Word elongations were already fixed in the corpus. We wordaligned the parallel data using GIZA++ (Och and Ney, 2003), and symmetrized the alignments using grow-diag-final-and heuristic (Koehn et al., 2003). We trained a phrasal MT system (Koehn et al., 2003). We built five-gram LMs using KenLM 3 Due to space limitations, we restrict discussion to work on dialects only. 4 Arabic News (LDC2004T17), eTIRR (LDC2004E72), and parallel corpora the GALE program 2 B1 B2 B3 B4 Train LM AR EG EG EG GW GW EGen EGen GW BLEU OOV 7.48 12.82 13.94 14.23 6.7 5.2 5.2 5.2 Table 1: Baseline results using the EG"
P13-2001,W11-2602,0,0.266595,"ects and MSA. Instead of translating between a dialect and MSA, we tried to narrow down the lexical, morphological and phonetic gap between them using a character-level conversion model, which we trained on a small set of parallel dialect/MSA word pairs. In the context of Arabic dialects3 , most previous work focused on converting dialects to MSA and vice versa to improve the processing of dialects (Sawaf, 2010; Chiang et al., 2006; Mohamed et al., 2012; Utiyama and Isahara, 2008). Sawaf (2010) proposed a dialect to MSA normalization that used character-level rules and morphological analysis. Salloum and Habash (2011) also used a rule-based method to generate MSA paraphrases of dialectal out-of-vocabulary (OOV) and low frequency words. Instead of rules, we automatically We constructed baselines that were based on the following training data: - An Egyptian/English parallel corpus consisting of ≈38k sentences, which is part of the LDC2012T09 corpus (Zbib et al., 2012). We randomly divided it into 32k sentences for training, 2k for development and 4k for testing. We henceforth refer to this corpus as EG and the English part of it as EGen . We did not have access to the training/test splits of Zbib et al. (201"
P13-2001,2010.amta-papers.5,0,0.351178,"Tiedemann, 2012), or by concatenating the parallel data for both languages (Nakov and Ng, 2009). These translation methods generally require parallel data, for which hardly any exists between dialects and MSA. Instead of translating between a dialect and MSA, we tried to narrow down the lexical, morphological and phonetic gap between them using a character-level conversion model, which we trained on a small set of parallel dialect/MSA word pairs. In the context of Arabic dialects3 , most previous work focused on converting dialects to MSA and vice versa to improve the processing of dialects (Sawaf, 2010; Chiang et al., 2006; Mohamed et al., 2012; Utiyama and Isahara, 2008). Sawaf (2010) proposed a dialect to MSA normalization that used character-level rules and morphological analysis. Salloum and Habash (2011) also used a rule-based method to generate MSA paraphrases of dialectal out-of-vocabulary (OOV) and low frequency words. Instead of rules, we automatically We constructed baselines that were based on the following training data: - An Egyptian/English parallel corpus consisting of ≈38k sentences, which is part of the LDC2012T09 corpus (Zbib et al., 2012). We randomly divided it into 32k"
P13-2001,P11-2007,0,0.209222,"Missing"
P13-2001,D11-1128,1,\N,Missing
P17-1080,2015.iwslt-evaluation.11,0,0.0948384,"Missing"
P17-1080,P16-1100,0,0.0161522,"Koehn and Hoang, 2007; Durrani et al., 2014). Characters and other sub-word units have become increasingly popular in neural MT, although they had also been used in phrase-based MT for handling morphologically-rich (Luong et al., 2010) or closely related language pairs (Durrani et al., 2010; Nakov and Tiedemann, 2012). In neural MT, such units are obtained in a pre-processing step—e.g. by byte-pair encoding (Sennrich et al., 2016) or the word-piece model (Wu et al., 2016)— or learned during training using a character-based convolutional/recurrent sub-network (Costa-juss`a and Fonollosa, 2016; Luong and Manning, 2016; Vylomova et al., 2016). The latter approach has the advantage of keeping the original word boundaries without requiring pre- and post-processing. Here we focus on a character CNN which has been used in language modeling and machine translation (Kim et al., 2015; Belinkov and Glass, 2016; Costa-juss`a and Fonollosa, 2016; Jozefowicz et al., 2016; Sajjad et al., 2017). We evaluate the quality of different representations learned by an MT system augmented with a character CNN in terms of POS and morphological tagging, and contrast them with a purely word-based system. 7 Conclusion Neural networ"
P17-1080,D10-1015,0,0.0631953,"echniques for representing morphological knowledge, such as word segmentation (Nieflen and Ney, 2000; Koehn and Word Char POS Accuracy ENC DEC BLEU Ar-En En-Ar 89.62 95.35 24.69 28.42 43.93 44.54 13.37 13.00 Table 4: POS tagging accuracy using word-based and char-based encoder/decoder representations. Knight, 2003; Badr et al., 2008) and factored translation and reordering models (Koehn and Hoang, 2007; Durrani et al., 2014). Characters and other sub-word units have become increasingly popular in neural MT, although they had also been used in phrase-based MT for handling morphologically-rich (Luong et al., 2010) or closely related language pairs (Durrani et al., 2010; Nakov and Tiedemann, 2012). In neural MT, such units are obtained in a pre-processing step—e.g. by byte-pair encoding (Sennrich et al., 2016) or the word-piece model (Wu et al., 2016)— or learned during training using a character-based convolutional/recurrent sub-network (Costa-juss`a and Fonollosa, 2016; Luong and Manning, 2016; Vylomova et al., 2016). The latter approach has the advantage of keeping the original word boundaries without requiring pre- and post-processing. Here we focus on a character CNN which has been used in language"
P17-1080,D13-1032,0,0.0236083,"Missing"
P17-1080,P12-2059,0,0.0233233,"(Nieflen and Ney, 2000; Koehn and Word Char POS Accuracy ENC DEC BLEU Ar-En En-Ar 89.62 95.35 24.69 28.42 43.93 44.54 13.37 13.00 Table 4: POS tagging accuracy using word-based and char-based encoder/decoder representations. Knight, 2003; Badr et al., 2008) and factored translation and reordering models (Koehn and Hoang, 2007; Durrani et al., 2014). Characters and other sub-word units have become increasingly popular in neural MT, although they had also been used in phrase-based MT for handling morphologically-rich (Luong et al., 2010) or closely related language pairs (Durrani et al., 2010; Nakov and Tiedemann, 2012). In neural MT, such units are obtained in a pre-processing step—e.g. by byte-pair encoding (Sennrich et al., 2016) or the word-piece model (Wu et al., 2016)— or learned during training using a character-based convolutional/recurrent sub-network (Costa-juss`a and Fonollosa, 2016; Luong and Manning, 2016; Vylomova et al., 2016). The latter approach has the advantage of keeping the original word boundaries without requiring pre- and post-processing. Here we focus on a character CNN which has been used in language modeling and machine translation (Kim et al., 2015; Belinkov and Glass, 2016; Costa"
P17-1080,C00-2162,0,0.212466,"om a neural MT encoder to predict syntactic properties on the English source side. In contrast, we focus on representations in morphologically-rich languages and evaluate both source and target sides across several criteria. Vylomova et al. (2016) also analyze different representations for morphologically-rich languages in MT, but do not directly measure the quality of the learned representations. Word representations in MT Machine translation systems that deal with morphologically-rich languages resort to various techniques for representing morphological knowledge, such as word segmentation (Nieflen and Ney, 2000; Koehn and Word Char POS Accuracy ENC DEC BLEU Ar-En En-Ar 89.62 95.35 24.69 28.42 43.93 44.54 13.37 13.00 Table 4: POS tagging accuracy using word-based and char-based encoder/decoder representations. Knight, 2003; Badr et al., 2008) and factored translation and reordering models (Koehn and Hoang, 2007; Durrani et al., 2014). Characters and other sub-word units have become increasingly popular in neural MT, although they had also been used in phrase-based MT for handling morphologically-rich (Luong et al., 2010) or closely related language pairs (Durrani et al., 2010; Nakov and Tiedemann, 20"
P17-1080,E03-1076,0,0.199308,"Missing"
P17-1080,pasha-etal-2014-madamira,0,0.026797,"Missing"
P17-1080,D15-1246,0,0.0820648,"Missing"
P17-1080,D16-1079,0,0.0296875,"itself can be modeled in different ways. For example, it may be an LSTM over outputs of the encoder. However, as we are interested in assessing the quality of the representations learned by the MT system, we choose to model the classifier as a simple feed-forward neural network with one hidden layer and a ReLU non-linearity. Arguably, if the learned representations are good, then a non-linear classifier should be able to extract useful information from them.2 We empha2 We also experimented with a linear classifier and observed similar trends to the non-linear case, but overall lower results; Qian et al. (2016b) reported similar findings. 862 Train Tokens Dev Tokens Test Tokens POS Tags Morph Tags Ar De Fr Cz Gold Pred BLEU Gold/Pred Gold/Pred Pred Pred 0.5M/2.7M 63K/114K 62K/16K 0.9M/4M 45K/50K 44K/25K 5.2M 55K 23K 2M 35K 20K Word/Char Word/Char Word/Char 42 1969 54 214 33 – 368 – 80.31/93.66 78.20/92.48 87.68/94.57 – – 89.62/95.35 88.33/94.66 93.54/94.63 94.61/95.55 75.71/79.10 24.7/28.4 9.9/10.7 29.6/30.4 37.8/38.8 23.2/25.4 Table 1: Statistics for annotated corpora in Arabic (Ar), German (De), French (Fr), and Czech (Cz). size that our goal is not to beat the state-of-the-art on a given task, b"
P17-1080,P16-1140,0,0.150611,"itself can be modeled in different ways. For example, it may be an LSTM over outputs of the encoder. However, as we are interested in assessing the quality of the representations learned by the MT system, we choose to model the classifier as a simple feed-forward neural network with one hidden layer and a ReLU non-linearity. Arguably, if the learned representations are good, then a non-linear classifier should be able to extract useful information from them.2 We empha2 We also experimented with a linear classifier and observed similar trends to the non-linear case, but overall lower results; Qian et al. (2016b) reported similar findings. 862 Train Tokens Dev Tokens Test Tokens POS Tags Morph Tags Ar De Fr Cz Gold Pred BLEU Gold/Pred Gold/Pred Pred Pred 0.5M/2.7M 63K/114K 62K/16K 0.9M/4M 45K/50K 44K/25K 5.2M 55K 23K 2M 35K 20K Word/Char Word/Char Word/Char 42 1969 54 214 33 – 368 – 80.31/93.66 78.20/92.48 87.68/94.57 – – 89.62/95.35 88.33/94.66 93.54/94.63 94.61/95.55 75.71/79.10 24.7/28.4 9.9/10.7 29.6/30.4 37.8/38.8 23.2/25.4 Table 1: Statistics for annotated corpora in Arabic (Ar), German (De), French (Fr), and Czech (Cz). size that our goal is not to beat the state-of-the-art on a given task, b"
P17-1080,P17-2095,1,0.861424,"Missing"
P17-1080,Q16-1027,0,0.0145163,"PUNC) whose accuracy improves quite a lot. Then there are plural nouns (NNS, DT+NNS) where the char-based model really shines, which makes sense linguistically as plurality in Arabic is usually expressed by certain suffixes (“-wn/yn” for masc. plural, “-At” for fem. plural). The charbased model is thus especially good with frequent tags and infrequent words, which is understandable given that infrequent words typically belong to frequent open categories like nouns and verbs. 4.2 Effect of encoder depth Modern NMT systems use very deep architectures with up to 8 or 16 layers (Wu et al., 2016; Zhou et al., 2016). We would like to understand what kind of information different layers capture. Given a trained NMT model with multiple layers, we extract feature representations from the different layers in the encoder. Let ENCli (s) denote the encoded representation of word wi after the l-th layer. We can vary l and train different classifiers to predict POS or morphological tags. Here we focus on the case of a 2-layer encoder-decoder model for simplicity (l 2 {1, 2}). Figure 6: POS tagging accuracy using representations from layers 0 (word vectors), 1, and 2, taken from encoders of different language pair"
P17-1080,C94-1027,0,0.0759209,"Missing"
P17-1080,P16-1162,0,0.118047,"Missing"
P17-1080,E17-1100,0,0.0558147,"Missing"
P17-1080,D07-1091,0,\N,Missing
P17-1080,P08-2039,1,\N,Missing
P17-1080,P10-1048,1,\N,Missing
P17-1080,C14-1041,1,\N,Missing
P17-1080,P16-2058,0,\N,Missing
P17-1080,C16-1124,0,\N,Missing
P17-1080,D16-1159,0,\N,Missing
P17-1080,2012.eamt-1.60,0,\N,Missing
P17-2095,P10-1048,1,0.856786,"ub-word segmentation based on BPE, and iii) two variants of character-based segmentation. We first map each source word to its corresponding segments (depending on the segmentation scheme), embed all segments of a word in vector space and feed them one-by-one to an encoder-decoder model. See Figure 1 for illustration. 2.1 Figure 1: Segmentation approaches for the word “b$rhm” “ ÑëQå.”; the blue vectors indicate the embedding(s) used before the encoding layer. 2.3 Morphological Segmentation Character-based models have been found to be effective in translating closely related language pairs (Durrani et al., 2010; Nakov and Tiedemann, 2012) and OOV words (Durrani et al., 2014). Ling et al. (2016) used character embeddings to address the OOV word problem. We explored them as an alternative to morphological segmentation. Their advantage is that character embeddings do not require any complicated pre- and post-processing step other than segmenting words into characters. The fully character-level encoder treats the source sentence as a sequence of letters, encoding each letter (including white-space) in the LSTM encoder (see Figure 1). The decoding may follow identical settings. We restricted the characte"
P17-2095,D11-1033,0,0.0328528,"ng several segmentation strategies. 3 Experiments In the following, we describe the data and system settings and later present the results of machine translation and POS tagging. LSTM in the (bidirectional) encoder and the decoder, with a size of 500. We limit the sentence length to 100 for MORPH, UNSEG, BPE, cCNN, and 500 for CHAR experiments. The source and target vocabularies are limited to 50k each. 3.1 3.2 Settings Data The MT systems were trained on 1.2 Million sentences, a concatenation of TED corpus (Cettolo et al., 2012), LDC NEWS data, QED (Guzm´an et al., 2013) and an MML-filtered (Axelrod et al., 2011) UN corpus.1 We used dev+test10 for tuning and tst11-14 for testing. For EnglishArabic, outputs were detokenized using MADA detokenizer. Before scoring the output, we normalized them and reference translations using the QCRI normalizer (Sajjad et al., 2013). Machine Translation Results Table 1 presents MT results using various segmentation strategies. Compared to the UNSEG system, the MORPH system2 improved translation quality by 4.6 and 1.6 BLEU points in Ar-to-En and Ento-Ar systems, respectively. The results also improved by up to 3 BLEU points for cCNN and CHAR systems in the Ar-to-En dire"
P17-2095,E14-4029,1,0.887411,"acter-based segmentation. We first map each source word to its corresponding segments (depending on the segmentation scheme), embed all segments of a word in vector space and feed them one-by-one to an encoder-decoder model. See Figure 1 for illustration. 2.1 Figure 1: Segmentation approaches for the word “b$rhm” “ ÑëQå.”; the blue vectors indicate the embedding(s) used before the encoding layer. 2.3 Morphological Segmentation Character-based models have been found to be effective in translating closely related language pairs (Durrani et al., 2010; Nakov and Tiedemann, 2012) and OOV words (Durrani et al., 2014). Ling et al. (2016) used character embeddings to address the OOV word problem. We explored them as an alternative to morphological segmentation. Their advantage is that character embeddings do not require any complicated pre- and post-processing step other than segmenting words into characters. The fully character-level encoder treats the source sentence as a sequence of letters, encoding each letter (including white-space) in the LSTM encoder (see Figure 1). The decoding may follow identical settings. We restricted the character-level representation to the Arabic side of the parallel corpus"
P17-2095,C96-1017,0,0.0492124,"ranslation system is sensitive to the ratio of source and target tokens, and a ratio close to 1 or greater, gives optimal performance. 1 Introduction Arabic word segmentation has shown to significantly improve output quality in NLP tasks such as machine translation (Habash and Sadat, 2006; Almahairi et al., 2016), part-of-speech tagging (Diab et al., 2004; Habash and Rambow, 2005), and information retrieval (M. Aljlayl and Grossman, 2002). A considerable amount of research has therefore been spent on Arabic morphological segmentation in the past two decades, ranging from rule-based analyzers (Beesley, 1996) to state-of-the-art statistical segmenters (Pasha et al., 2014; Abdelali et al., 2016; Khalifa et al., 2016). Morphological segmentation splits words into morphemes. For example, ‘‘wktAbnA” “ AJK . AJ»ð” (gloss: and our book) is decomposed into its stem H AJ» + ð”. and affixes as: “w+ ktAb +nA” “ AK+ . 601 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 601–607 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2095 2 Segmentation Approaches We experimente"
P17-2095,P17-1080,1,0.841253,"ing a CNN over characters. The embedding are then provided to the encoder as input. The intuition is that the character-based word embedding should be able to learn the morphological phenomena a word inherits. Compared to fully characterlevel encoding, the encoder gets word-level embeddings as in the case of unsegmented words (see Figure 1). However, the word embedding is intuitively richer than the embedding learned over unsegmented words because of the convolution over characters. The method was previously shown to help neural MT (Belinkov and Glass, 2016; Costa-juss`a and Fonollosa, 2016). Belinkov et al. (2017) also showed character-based representations learned using a CNN to be superior, at learning word morphology, than their word-based counter-parts. However, they did not compare these against BPE-based segmentation. We use character-CNN to aid Arabic word segmentation. 602 # SEG tst11 Arabic-to-English tst12 tst13 tst14 AVG. tst11 English-to-Arabic tst12 tst13 tst14 UNSEG 25.7 28.2 27.3 23.9 26.3 15.8 17.1 18.1 15.5 16.6 MORPH cCNN CHAR BPE 29.2 29.0 28.8 29.7 33.0 32.0 31.8 32.5 32.9 32.5 32.5 33.6 28.3 28.0 27.8 28.4 30.9 30.3 30.2 31.1 16.5 14.3 15.3 17.5 18.8 12.8 17.1 18.0 20.4 13.6 18.0 2"
P17-2095,fishel-kirik-2010-linguistically,0,0.0293358,"ntext). The analyses are provided with the original text to a Feature Modeling component that applies an SVM and a language model to make predictions, which are scored by an Analysis Ranking component. Farasa on the other hand is a light weight segmenter, which ignores context and instead uses a variety of features and lexicons for segmentation. 2.2 Character-level Encoding Data Driven Sub-word Units A number of data-driven approaches have been proposed that learn to segment words into smaller units from data (Demberg, 2007; Sami Virpioja and Kurimo, 2013) and shown to improve phrasebased MT (Fishel and Kirik, 2010; Stallard et al., 2012). Recently, with the advent of neural MT, a few sub-word-based techniques have been proposed that segment words into smaller units to tackle the limited vocabulary and unknown word problems (Sennrich et al., 2016; Wu et al., 2016). In this work, we explore Byte-Pair Encoding (BPE), a data compression algorithm (Gage, 1994) as an alternative to morphological segmentation of Arabic. BPE splits words into symbols (a sequence of characters) and then iteratively replaces the most frequent symbols with their merged variants. In essence, frequent character n-gram sequences wil"
P17-2095,A00-1031,0,0.0614725,"this problem is; at test time, BPE is applied to those words only which were known to the full vocabulary of the training corpus. In this way, the sub-word units created by BPE for the word are already seen in a similar context during training and the model has learned to translate them correctly. The downside of this method is that it limits BPE’s power to segment unknown words to their correct sub-word units and outputs them as UNK in translation. 3.3 We also experimented with the aforementioned segmentation strategies for the task of Arabic POS tagging. Probabilistic taggers like HMMbased (Brants, 2000) and sequence learning models like CRF (Lafferty et al., 2001) consider previous words and/or tags to predict the tag of the current word. We mimic a similar setting but in a sequence-to-sequence learning framework. Figure 3 describes a step by step procedure to train a neural encoder-decoder tagger. Consider an Arabic phrase “klm >SdqA}k b$rhm” Discussion: Though BPE performed well for machine translation, there are a few reservations that we would like to discuss here. Since the main goal of the algorithm is to compress data and segmentation comes as a by-product, it often produces different"
P17-2095,2013.iwslt-papers.2,1,0.905117,"Missing"
P17-2095,2012.eamt-1.60,0,0.0196508,"3.6 18.0 20.0 17.2 12.6 15.3 16.6 18.2 13.3 16.4 18.0 AVG. Table 1: Results of comparing several segmentation strategies. 3 Experiments In the following, we describe the data and system settings and later present the results of machine translation and POS tagging. LSTM in the (bidirectional) encoder and the decoder, with a size of 500. We limit the sentence length to 100 for MORPH, UNSEG, BPE, cCNN, and 500 for CHAR experiments. The source and target vocabularies are limited to 50k each. 3.1 3.2 Settings Data The MT systems were trained on 1.2 Million sentences, a concatenation of TED corpus (Cettolo et al., 2012), LDC NEWS data, QED (Guzm´an et al., 2013) and an MML-filtered (Axelrod et al., 2011) UN corpus.1 We used dev+test10 for tuning and tst11-14 for testing. For EnglishArabic, outputs were detokenized using MADA detokenizer. Before scoring the output, we normalized them and reference translations using the QCRI normalizer (Sajjad et al., 2013). Machine Translation Results Table 1 presents MT results using various segmentation strategies. Compared to the UNSEG system, the MORPH system2 improved translation quality by 4.6 and 1.6 BLEU points in Ar-to-En and Ento-Ar systems, respectively. The resul"
P17-2095,P05-1071,0,0.072442,"racter CNN (Convolution Neural Network). On the tasks of Machine Translation and POS tagging, we found these methods to achieve close to, and occasionally surpass state-of-the-art performance. In our analysis, we show that a neural machine translation system is sensitive to the ratio of source and target tokens, and a ratio close to 1 or greater, gives optimal performance. 1 Introduction Arabic word segmentation has shown to significantly improve output quality in NLP tasks such as machine translation (Habash and Sadat, 2006; Almahairi et al., 2016), part-of-speech tagging (Diab et al., 2004; Habash and Rambow, 2005), and information retrieval (M. Aljlayl and Grossman, 2002). A considerable amount of research has therefore been spent on Arabic morphological segmentation in the past two decades, ranging from rule-based analyzers (Beesley, 1996) to state-of-the-art statistical segmenters (Pasha et al., 2014; Abdelali et al., 2016; Khalifa et al., 2016). Morphological segmentation splits words into morphemes. For example, ‘‘wktAbnA” “ AJK . AJ»ð” (gloss: and our book) is decomposed into its stem H AJ» + ð”. and affixes as: “w+ ktAb +nA” “ AK+ . 601 Proceedings of the 55th Annual Meeting of the Association"
P17-2095,P16-2058,0,0.109475,"Missing"
P17-2095,N13-1044,0,0.0576645,"Missing"
P17-2095,P07-1116,0,0.100776,"al analyzer that generates a list of possible word-level analyses (independent of context). The analyses are provided with the original text to a Feature Modeling component that applies an SVM and a language model to make predictions, which are scored by an Analysis Ranking component. Farasa on the other hand is a light weight segmenter, which ignores context and instead uses a variety of features and lexicons for segmentation. 2.2 Character-level Encoding Data Driven Sub-word Units A number of data-driven approaches have been proposed that learn to segment words into smaller units from data (Demberg, 2007; Sami Virpioja and Kurimo, 2013) and shown to improve phrasebased MT (Fishel and Kirik, 2010; Stallard et al., 2012). Recently, with the advent of neural MT, a few sub-word-based techniques have been proposed that segment words into smaller units to tackle the limited vocabulary and unknown word problems (Sennrich et al., 2016; Wu et al., 2016). In this work, we explore Byte-Pair Encoding (BPE), a data compression algorithm (Gage, 1994) as an alternative to morphological segmentation of Arabic. BPE splits words into symbols (a sequence of characters) and then iteratively replaces the most fre"
P17-2095,N06-2013,0,0.0488146,"rd units, ii) characters as a unit of learning, and iii) word embeddings learned using a character CNN (Convolution Neural Network). On the tasks of Machine Translation and POS tagging, we found these methods to achieve close to, and occasionally surpass state-of-the-art performance. In our analysis, we show that a neural machine translation system is sensitive to the ratio of source and target tokens, and a ratio close to 1 or greater, gives optimal performance. 1 Introduction Arabic word segmentation has shown to significantly improve output quality in NLP tasks such as machine translation (Habash and Sadat, 2006; Almahairi et al., 2016), part-of-speech tagging (Diab et al., 2004; Habash and Rambow, 2005), and information retrieval (M. Aljlayl and Grossman, 2002). A considerable amount of research has therefore been spent on Arabic morphological segmentation in the past two decades, ranging from rule-based analyzers (Beesley, 1996) to state-of-the-art statistical segmenters (Pasha et al., 2014; Abdelali et al., 2016; Khalifa et al., 2016). Morphological segmentation splits words into morphemes. For example, ‘‘wktAbnA” “ AJK . AJ»ð” (gloss: and our book) is decomposed into its stem H AJ» + ð”. and af"
P17-2095,C16-2047,0,0.0156449,"ater, gives optimal performance. 1 Introduction Arabic word segmentation has shown to significantly improve output quality in NLP tasks such as machine translation (Habash and Sadat, 2006; Almahairi et al., 2016), part-of-speech tagging (Diab et al., 2004; Habash and Rambow, 2005), and information retrieval (M. Aljlayl and Grossman, 2002). A considerable amount of research has therefore been spent on Arabic morphological segmentation in the past two decades, ranging from rule-based analyzers (Beesley, 1996) to state-of-the-art statistical segmenters (Pasha et al., 2014; Abdelali et al., 2016; Khalifa et al., 2016). Morphological segmentation splits words into morphemes. For example, ‘‘wktAbnA” “ AJK . AJ»ð” (gloss: and our book) is decomposed into its stem H AJ» + ð”. and affixes as: “w+ ktAb +nA” “ AK+ . 601 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 601–607 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2095 2 Segmentation Approaches We experimented with three data-driven segmentation schemes: i) morphological segmentation, ii) sub-word segmentation based"
P17-2095,P16-1162,0,0.504845,"bic translation (El Kholy and Habash, 2012)). More importantly, these tools are dialect- and domain-specific. A segmenter trained for modern standard Arabic (MSA) performs significantly worse on dialectal Arabic (Habash et al., 2013), or when it is applied to a new domain. In this work, we explore whether we can avoid the language-dependent pre/post-processing components and learn segmentation directly from the training data being used for a given task. We investigate data-driven alternatives to morphological segmentation using i) unsupervised sub-word units obtained using byte-pair encoding (Sennrich et al., 2016), ii) purely character-based segmentation (Ling et al., 2015), and iii) a convolutional neural network over characters (Kim et al., 2016). We evaluate these techniques on the tasks of machine translation (MT) and part-of-speech (POS) tagging and compare them against morphological segmenters MADAMIRA (Pasha et al., 2014) and Farasa (Abdelali et al., 2016). On the MT task, byte-pair encoding (BPE) performs the best among the three methods, achieving very similar performance to morphological segmentation in the Arabic-to-English direction and slightly worse in the other direction. Character-based"
P17-2095,P12-2063,0,0.0265535,"provided with the original text to a Feature Modeling component that applies an SVM and a language model to make predictions, which are scored by an Analysis Ranking component. Farasa on the other hand is a light weight segmenter, which ignores context and instead uses a variety of features and lexicons for segmentation. 2.2 Character-level Encoding Data Driven Sub-word Units A number of data-driven approaches have been proposed that learn to segment words into smaller units from data (Demberg, 2007; Sami Virpioja and Kurimo, 2013) and shown to improve phrasebased MT (Fishel and Kirik, 2010; Stallard et al., 2012). Recently, with the advent of neural MT, a few sub-word-based techniques have been proposed that segment words into smaller units to tackle the limited vocabulary and unknown word problems (Sennrich et al., 2016; Wu et al., 2016). In this work, we explore Byte-Pair Encoding (BPE), a data compression algorithm (Gage, 1994) as an alternative to morphological segmentation of Arabic. BPE splits words into symbols (a sequence of characters) and then iteratively replaces the most frequent symbols with their merged variants. In essence, frequent character n-gram sequences will be merged to form one"
P17-2095,P12-2059,0,0.0607745,"ased on BPE, and iii) two variants of character-based segmentation. We first map each source word to its corresponding segments (depending on the segmentation scheme), embed all segments of a word in vector space and feed them one-by-one to an encoder-decoder model. See Figure 1 for illustration. 2.1 Figure 1: Segmentation approaches for the word “b$rhm” “ ÑëQå.”; the blue vectors indicate the embedding(s) used before the encoding layer. 2.3 Morphological Segmentation Character-based models have been found to be effective in translating closely related language pairs (Durrani et al., 2010; Nakov and Tiedemann, 2012) and OOV words (Durrani et al., 2014). Ling et al. (2016) used character embeddings to address the OOV word problem. We explored them as an alternative to morphological segmentation. Their advantage is that character embeddings do not require any complicated pre- and post-processing step other than segmenting words into characters. The fully character-level encoder treats the source sentence as a sequence of letters, encoding each letter (including white-space) in the LSTM encoder (see Figure 1). The decoding may follow identical settings. We restricted the character-level representation to th"
P17-2095,pasha-etal-2014-madamira,0,0.112051,"Missing"
P17-2095,N04-4038,0,\N,Missing
P17-2095,N16-3003,1,\N,Missing
P17-2095,2013.iwslt-evaluation.8,1,\N,Missing
W13-2213,D11-1033,0,0.0919379,"Missing"
W13-2213,P05-1066,0,0.0443099,"a are also automatically tagged and phrases with words and POS tags on both sides are extracted. The POSbased OSM model is only used in the German-toEnglish and English-to-German experiments.4 So far, we only used coarse POS tags without gender and case information. 4 Constituent Parse Reordering Our German-to-English system used constituent parses for pre-ordering of the input. We parsed all of the parallel German to English data available, and the tuning, test and blind-test sets. We then applied reordering rules to these parses. We used the rules for reordering German constituent parses of Collins et al. (2005) together with the additional rules described by Fraser (2009). These are applied as a preprocess to all German data (training, tuning and test data). To produce the parses, we started with the generative BitPar parser trained on the Tiger treebank with optimizations of the grammar, as described by (Fraser et al., 2013). We then performed self-training using the high quality Europarl corpus - we parsed it, and then retrained the parser on the output. Decoder The decoding framework used in the operation sequence model is based on Pharaoh (Koehn, 2004). The decoder uses beam search to build up t"
W13-2213,P11-1105,1,0.934423,"rd Farkas4 1 2 University of Edinburgh – dnadir@inf.ed.ac.uk Ludwig Maximilian University Munich – schmid,fraser@cis.uni-muenchen.de 3 Qatar Computing Research Institute – hsajjad@qf.org.qa 4 University of Szeged – rfarkas@inf.u-szeged.hu Abstract reordering operations and learns a Markov model over a sequence of operations. Our decoder uses the beam search algorithm in a stack-based decoder like most sequence-based SMT frameworks. Although the model is based on minimal translation units, we use phrases during search because they improve the search accuracy of our system. The earlier decoder (Durrani et al., 2011) was based on minimal units. But we recently showed that using phrases during search gives better coverage of translation, better future cost estimation and lesser search errors (Durrani et al., 2013a) than MTU-based decoding. We have therefore shifted to phrase-based search on top of the OSM model. This paper is organized as follows. Section 2 gives a short description of the model and search as used in the OSM decoder. In Section 3 we give a description of the POS-based operation sequence model that we test for our German-English and English-German experiments. Section 4 describes our proces"
W13-2213,N13-1001,1,0.917412,"ty of Szeged – rfarkas@inf.u-szeged.hu Abstract reordering operations and learns a Markov model over a sequence of operations. Our decoder uses the beam search algorithm in a stack-based decoder like most sequence-based SMT frameworks. Although the model is based on minimal translation units, we use phrases during search because they improve the search accuracy of our system. The earlier decoder (Durrani et al., 2011) was based on minimal units. But we recently showed that using phrases during search gives better coverage of translation, better future cost estimation and lesser search errors (Durrani et al., 2013a) than MTU-based decoding. We have therefore shifted to phrase-based search on top of the OSM model. This paper is organized as follows. Section 2 gives a short description of the model and search as used in the OSM decoder. In Section 3 we give a description of the POS-based operation sequence model that we test for our German-English and English-German experiments. Section 4 describes our processing of the German and English data for German-English and English-German experiments. In Section 5 we describe the unsupervised transliteration mining that has been done for the Russian-English and"
W13-2213,P13-2071,1,0.919884,"ty of Szeged – rfarkas@inf.u-szeged.hu Abstract reordering operations and learns a Markov model over a sequence of operations. Our decoder uses the beam search algorithm in a stack-based decoder like most sequence-based SMT frameworks. Although the model is based on minimal translation units, we use phrases during search because they improve the search accuracy of our system. The earlier decoder (Durrani et al., 2011) was based on minimal units. But we recently showed that using phrases during search gives better coverage of translation, better future cost estimation and lesser search errors (Durrani et al., 2013a) than MTU-based decoding. We have therefore shifted to phrase-based search on top of the OSM model. This paper is organized as follows. Section 2 gives a short description of the model and search as used in the OSM decoder. In Section 3 we give a description of the POS-based operation sequence model that we test for our German-English and English-German experiments. Section 4 describes our processing of the German and English data for German-English and English-German experiments. In Section 5 we describe the unsupervised transliteration mining that has been done for the Russian-English and"
W13-2213,W13-2212,1,0.928384,"ty of Szeged – rfarkas@inf.u-szeged.hu Abstract reordering operations and learns a Markov model over a sequence of operations. Our decoder uses the beam search algorithm in a stack-based decoder like most sequence-based SMT frameworks. Although the model is based on minimal translation units, we use phrases during search because they improve the search accuracy of our system. The earlier decoder (Durrani et al., 2011) was based on minimal units. But we recently showed that using phrases during search gives better coverage of translation, better future cost estimation and lesser search errors (Durrani et al., 2013a) than MTU-based decoding. We have therefore shifted to phrase-based search on top of the OSM model. This paper is organized as follows. Section 2 gives a short description of the model and search as used in the OSM decoder. In Section 3 we give a description of the POS-based operation sequence model that we test for our German-English and English-German experiments. Section 4 describes our processing of the German and English data for German-English and English-German experiments. In Section 5 we describe the unsupervised transliteration mining that has been done for the Russian-English and"
W13-2213,J13-1005,1,0.871716,"Missing"
W13-2213,W09-0420,1,0.89655,"both sides are extracted. The POSbased OSM model is only used in the German-toEnglish and English-to-German experiments.4 So far, we only used coarse POS tags without gender and case information. 4 Constituent Parse Reordering Our German-to-English system used constituent parses for pre-ordering of the input. We parsed all of the parallel German to English data available, and the tuning, test and blind-test sets. We then applied reordering rules to these parses. We used the rules for reordering German constituent parses of Collins et al. (2005) together with the additional rules described by Fraser (2009). These are applied as a preprocess to all German data (training, tuning and test data). To produce the parses, we started with the generative BitPar parser trained on the Tiger treebank with optimizations of the grammar, as described by (Fraser et al., 2013). We then performed self-training using the high quality Europarl corpus - we parsed it, and then retrained the parser on the output. Decoder The decoding framework used in the operation sequence model is based on Pharaoh (Koehn, 2004). The decoder uses beam search to build up the translation from left to right. The hypotheses are arranged"
W13-2213,W10-1734,1,0.864372,"Missing"
W13-2213,schmid-etal-2004-smor,1,0.739052,"Missing"
W13-2213,W11-2123,0,0.045987,"source-side cepts and source-word deletion. However, it doesn’t provide a mechanism to deal with unaligned and discontinuous target cepts. These are handled through a 3-step process3 in which we modify the alignments to remove discontinuous and unaligned target MTUs. Please see Durrani et al. (2011) for details. After modifying the alignments, we convert each bilingual sentence pair and its alignments into a sequence of operations as described above and learn an OSM model. To this end, a Kneser-Ney (Kneser and Ney, 1995) smoothed 9-gram model is trained with SRILM (Stolcke, 2002) while KenLM (Heafield, 2011) is used at runtime. Figure 1: Bilingual Sentence with Alignments sentence pair and its alignments as a unique sequence of operations. An operation either jointly generates source and target words, or it performs reordering by inserting gaps or jumping to gaps. We then learn a Markov model over a sequence of operations o1 , o2 , . . . , oJ that encapsulate MTUs and reordering information as: posm (o1 , ..., oJ ) = J Y j=1 p(oj |oj−n+1 , ..., oj−1 ) 2.3 We use additional features for our model and employ the standard log-linear approach (Och and Ney, 2004) to combine and tune them. We search fo"
W13-2213,I08-2089,0,0.179256,"estimation of the translation models is: de–en ≈ 4.5M and ru–en ≈ 2M parallel sentences. We were able to use all the available data for cs-to-en (≈ 15.6M sentences). However, sub-sampled data was used for en-to-cs (≈ 3M sentences), en-to-fr (≈ 7.8M sentences) and es–en (≈ 3M sentences). Monolingual Language Model: We used all the available training data (including LDC Gigaword data) for the estimation of monolingual language models: en ≈ 287.3M sentences, fr ≈ 91M, es ≈ 65.7M, cs ≈ 43.4M and ru ≈ 21.7M sentences. All data except for ru-en and en-ru was true-cased. We followed the approach of Schwenk and Koehn (2008) by training language models from each sub-corpus separately and then linearly interpolated them using SRILM with weights optimized on the held-out dev-set. We concatenated the news-test sets from four years (2008-2011) to obtain a large dev-set5 in order to obtain more stable weights (Koehn and Haddow, 2012). Decoder Settings: For each extracted input phrase only 15-best translation options were used during decoding.6 We used a hard reordering limit 5 For Russian-English and English-Russian language pairs, we divided the tuning-set news-test 2012 into two halves and used the first half for tu"
W13-2213,W13-2230,1,0.864769,"Missing"
W13-2213,W12-3139,0,0.0378184,"odel: We used all the available training data (including LDC Gigaword data) for the estimation of monolingual language models: en ≈ 287.3M sentences, fr ≈ 91M, es ≈ 65.7M, cs ≈ 43.4M and ru ≈ 21.7M sentences. All data except for ru-en and en-ru was true-cased. We followed the approach of Schwenk and Koehn (2008) by training language models from each sub-corpus separately and then linearly interpolated them using SRILM with weights optimized on the held-out dev-set. We concatenated the news-test sets from four years (2008-2011) to obtain a large dev-set5 in order to obtain more stable weights (Koehn and Haddow, 2012). Decoder Settings: For each extracted input phrase only 15-best translation options were used during decoding.6 We used a hard reordering limit 5 For Russian-English and English-Russian language pairs, we divided the tuning-set news-test 2012 into two halves and used the first half for tuning and second for test. 6 We could not experiment with higher n-best translation options due to a bug that was not fixed in time and hindered us from scaling. Sub-sampling Because of scalability problems we were not able to use the entire data made available for build125 of 16 words which disallows a jump b"
W13-2213,E03-1076,0,0.439555,"Missing"
W13-2213,koen-2004-pharaoh,0,0.0671416,"ering German constituent parses of Collins et al. (2005) together with the additional rules described by Fraser (2009). These are applied as a preprocess to all German data (training, tuning and test data). To produce the parses, we started with the generative BitPar parser trained on the Tiger treebank with optimizations of the grammar, as described by (Fraser et al., 2013). We then performed self-training using the high quality Europarl corpus - we parsed it, and then retrained the parser on the output. Decoder The decoding framework used in the operation sequence model is based on Pharaoh (Koehn, 2004). The decoder uses beam search to build up the translation from left to right. The hypotheses are arranged in m stacks such that stack i maintains hypotheses that have already translated i many foreign words. The ultimate goal is to find the best scoring hypothesis, that translates all the words in the foreign sentence. During the hypothesis extension each extracted phrase is translated into a sequence of operations. The reordering opera4 This work is ongoing and we will present detailed experiments in the future. 124 ing the translation model in some cases. We used modified Moore-Lewis sampli"
W13-2213,J06-4004,0,0.0984652,"Missing"
W13-2213,J04-4002,0,0.0848361,"d with SRILM (Stolcke, 2002) while KenLM (Heafield, 2011) is used at runtime. Figure 1: Bilingual Sentence with Alignments sentence pair and its alignments as a unique sequence of operations. An operation either jointly generates source and target words, or it performs reordering by inserting gaps or jumping to gaps. We then learn a Markov model over a sequence of operations o1 , o2 , . . . , oJ that encapsulate MTUs and reordering information as: posm (o1 , ..., oJ ) = J Y j=1 p(oj |oj−n+1 , ..., oj−1 ) 2.3 We use additional features for our model and employ the standard log-linear approach (Och and Ney, 2004) to combine and tune them. We search for a target string E which maximizes a linear combination of feature functions: By coupling reordering with lexical generation, each (translation or reordering) decision depends on n − 1 previous (translation and reordering) decisions spanning across phrasal boundaries. The reordering decisions therefore influence lexical selection and vice versa. A heterogeneous mixture of translation and reordering operations enables us to memorize reordering patterns and lexicalized triggers unlike the classic N-gram model where translation and reordering are modeled se"
W13-2213,P11-1044,1,0.696753,"ing. We do not have such a list and making one is a cumbersome process. Instead, we use the unsupervised transliteration mining system of Sajjad et al. (2012) that takes a list of word pairs for training and extracts transliteration pairs that can be used for the training of the transliteration system. The procedure of mining transliteration pairs and transliterating OOVs is described as follows: We word-align the parallel corpus using GIZA++ in both direction and symmetrize the alignments using the grow-diag-final-and heuristic. We extract all word pairs which occur as 1to-1 alignments (like Sajjad et al. (2011)) and later refer to them as the list of word pairs. We train the unsupervised transliteration mining system on the list of word pairs and extract transliteration pairs. We use these mined pairs to build a transliteration system using the Moses toolkit. The transliteration system is applied in a post-processing step to transliterate OOVs. Please refer to Sajjad et al. (2013) for further details on our transliteration work. 6 7 Experiments Parallel Corpus: The amount of bitext used for the estimation of the translation models is: de–en ≈ 4.5M and ru–en ≈ 2M parallel sentences. We were able to u"
W13-2213,P12-1049,1,0.737125,"on system fails to translate out-of-vocabulary words (OOVs) as they are unknown to the training data. Most of the OOVs are named entities and simply passing them to the output often produces correct translations if source and target language use the same script. If the scripts are different transliterating them to the target language script could solve this problem. However, building a transliteration system requires a list of transliteration pairs for training. We do not have such a list and making one is a cumbersome process. Instead, we use the unsupervised transliteration mining system of Sajjad et al. (2012) that takes a list of word pairs for training and extracts transliteration pairs that can be used for the training of the transliteration system. The procedure of mining transliteration pairs and transliterating OOVs is described as follows: We word-align the parallel corpus using GIZA++ in both direction and symmetrize the alignments using the grow-diag-final-and heuristic. We extract all word pairs which occur as 1to-1 alignments (like Sajjad et al. (2011)) and later refer to them as the list of word pairs. We train the unsupervised transliteration mining system on the list of word pairs and"
W13-2213,D11-1017,0,\N,Missing
W13-2213,E12-1074,1,\N,Missing
W13-2213,C04-1024,1,\N,Missing
W13-2213,W13-2228,1,\N,Missing
W13-2228,C12-1121,0,0.0548234,"Missing"
W13-2228,J03-1002,0,0.0274782,"n of PRO that optimizes BLEU+1 at corpus level. Section 5 and Section 6 present English/Russian and Russian/English machine translation experiments respectively. Section 7 concludes. Introduction We describe the QCRI-Munich-EdinburghStuttgart (QCRI-MES) English to Russian and Russian to English systems submitted to the Eighth Workshop on Statistical Machine Translation. We experimented using the standard Phrase-based Statistical Machine Translation System (PSMT) as implemented in the Moses toolkit (Koehn et al., 2007). The typical pipeline for translation involves word alignment using GIZA++ (Och and Ney, 2003), phrase extraction, tuning and phrase-based decoding. Our system is different from standard PSMT in three ways: • We integrate an unsupervised transliteration mining system (Sajjad et al., 2012) into the GIZA++ word aligner (Sajjad et al., 2011). 219 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 219–224, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics 2 Transliteration Mining 2.1.1 Estimating Transliteration Probabilities We use the algorithm for the estimation of transliteration probabilities of Sajjad et al. (2011). We modify"
W13-2228,P03-1021,0,0.0050144,"as a list of word pairs. The unsupervised transliteration mining system trains on the list of word pairs and mines transliteration pairs. We use the mined pairs to build a transliteration system using the Moses toolkit. The transliteration system is used in Algorithm 1 to generate transliteration probabilities of candidate word pairs and is also used in the postprocessing step to transliterate OOVs. We run GIZA++ with identical settings as described in Section 5.2. We interpolate for evPRO: Corpus-level BLEU Pairwise Ranking Optimization (PRO) (Hopkins and May, 2011) is an extension of MERT (Och, 2003) that can scale to thousands of parameters. It optimizes sentence-level BLEU+1 which is an add-one smoothed version of BLEU (Lin and Och, 2004). The sentence-level BLEU+1 has a bias towards producing short translations as add-one smoothing improves precision but does not change the brevity penalty. Nakov et al. (2012) fixed this by using several heuristics on brevity penalty, reference length and grounding the precision length. In our experiments, we use the improved version of PRO as provided by Nakov et al. (2012). We 221 MERT MIRA PRO PROv1 GIZA++ TA-GIZA++ OOV-TI 23.41 23.60 23.57 23.65 23"
W13-2228,P11-1044,1,0.923855,"QCRI-MES) English to Russian and Russian to English systems submitted to the Eighth Workshop on Statistical Machine Translation. We experimented using the standard Phrase-based Statistical Machine Translation System (PSMT) as implemented in the Moses toolkit (Koehn et al., 2007). The typical pipeline for translation involves word alignment using GIZA++ (Och and Ney, 2003), phrase extraction, tuning and phrase-based decoding. Our system is different from standard PSMT in three ways: • We integrate an unsupervised transliteration mining system (Sajjad et al., 2012) into the GIZA++ word aligner (Sajjad et al., 2011). 219 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 219–224, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics 2 Transliteration Mining 2.1.1 Estimating Transliteration Probabilities We use the algorithm for the estimation of transliteration probabilities of Sajjad et al. (2011). We modify it to improve efficiency. In step 6 of Algorithm 1 instead of taking all f that coocur with e, we take only those that have a word length ratio in range of 0.8-1.2.1 This reduces cooc(e) by more than half and speeds up step 9 of Algorithm 1. The"
W13-2228,J93-2003,0,0.0261744,"Missing"
W13-2228,P11-1105,1,0.819552,"tion system on that. We compare its result with the Russian to English system trained on the un-processed parallel data. 6.1.2 Morphological Reduction English in comparison to Slavic group of languages is morphologically poor. For example, English has no morphological attributes for nouns and adjectives to express gender or case; verbs in English have no gender either. Russian, on the contrary, has rich morphology. It suffices to say that the Russian has 6 cases and 3 grammatical genders, which manifest themselves in different 2 We see similar gain in BLEU when using operation sequence model (Durrani et al., 2011) for decoding and transliterating OOVs in a post-processing step (Durrani et al., 2013). 222 Original corpus suffixes for nouns, pronouns, adjectives and some verb forms. When translating from Russian into English, a lot of these attributes become meaningless and excessive. It makes sense to reduce the number of morphological attributes before the text is supplied for the training of the MT system. We apply morphological reduction to nouns, pronouns, verbs, adjectives, prepositions and conjunctions. The rest of the POS (adverbs, particles, interjections and abbreviations) have no morphological"
W13-2228,P12-1049,1,0.919427,"on We describe the QCRI-Munich-EdinburghStuttgart (QCRI-MES) English to Russian and Russian to English systems submitted to the Eighth Workshop on Statistical Machine Translation. We experimented using the standard Phrase-based Statistical Machine Translation System (PSMT) as implemented in the Moses toolkit (Koehn et al., 2007). The typical pipeline for translation involves word alignment using GIZA++ (Och and Ney, 2003), phrase extraction, tuning and phrase-based decoding. Our system is different from standard PSMT in three ways: • We integrate an unsupervised transliteration mining system (Sajjad et al., 2012) into the GIZA++ word aligner (Sajjad et al., 2011). 219 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 219–224, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics 2 Transliteration Mining 2.1.1 Estimating Transliteration Probabilities We use the algorithm for the estimation of transliteration probabilities of Sajjad et al. (2011). We modify it to improve efficiency. In step 6 of Algorithm 1 instead of taking all f that coocur with e, we take only those that have a word length ratio in range of 0.8-1.2.1 This reduces cooc(e) by more"
W13-2228,C08-1098,1,0.691413,"f English to Russian machine translation system evaluated on tst2012 and tst2013 using baseline GIZA++ alignment and transliteration augmented-GIZA++ alignment and post-processed the output by transliterating OOVs. Human evaluation in WMT13 is performed on TA-GIZA++ tested on tst2013 (marked with *) Table 1: BLEU scores of English to Russian machine translation system evaluated on tst2012 using baseline GIZA++ alignment and transliteration augmented-GIZA++. OOV-TI presents the score of the system trained using TA-GIZA++ after transliterating OOVs 5.4 tst2012 6.1.1 POS Tagging We use RFTagger (Schmid and Laws, 2008) for POS tagging. Despite the good quality of tagging provided by RFTagger, some errors seem to be unavoidable due to the ambiguity of certain grammatical forms in Russian. A good example of this is neuter nouns that have the same form in all cases, or feminine nouns, which have identical forms in singular genitive and plural nominative (Sharoff et al., 2008). Since Russian sentences have free word order, and the case of nouns cannot be determined on that basis, this imperfection can not be corrected during tagging or by postprocessing the tagger output. Russian/English Experiments In this sec"
W13-2228,W13-2213,1,0.727212,"n the un-processed parallel data. 6.1.2 Morphological Reduction English in comparison to Slavic group of languages is morphologically poor. For example, English has no morphological attributes for nouns and adjectives to express gender or case; verbs in English have no gender either. Russian, on the contrary, has rich morphology. It suffices to say that the Russian has 6 cases and 3 grammatical genders, which manifest themselves in different 2 We see similar gain in BLEU when using operation sequence model (Durrani et al., 2011) for decoding and transliterating OOVs in a post-processing step (Durrani et al., 2013). 222 Original corpus suffixes for nouns, pronouns, adjectives and some verb forms. When translating from Russian into English, a lot of these attributes become meaningless and excessive. It makes sense to reduce the number of morphological attributes before the text is supplied for the training of the MT system. We apply morphological reduction to nouns, pronouns, verbs, adjectives, prepositions and conjunctions. The rest of the POS (adverbs, particles, interjections and abbreviations) have no morphological attributes and are left unchanged. We apply morphological reduction to train, tune, de"
W13-2228,D11-1125,0,0.0171179,"ke Sajjad et al. (2011)) and later refer to them as a list of word pairs. The unsupervised transliteration mining system trains on the list of word pairs and mines transliteration pairs. We use the mined pairs to build a transliteration system using the Moses toolkit. The transliteration system is used in Algorithm 1 to generate transliteration probabilities of candidate word pairs and is also used in the postprocessing step to transliterate OOVs. We run GIZA++ with identical settings as described in Section 5.2. We interpolate for evPRO: Corpus-level BLEU Pairwise Ranking Optimization (PRO) (Hopkins and May, 2011) is an extension of MERT (Och, 2003) that can scale to thousands of parameters. It optimizes sentence-level BLEU+1 which is an add-one smoothed version of BLEU (Lin and Och, 2004). The sentence-level BLEU+1 has a bias towards producing short translations as add-one smoothing improves precision but does not change the brevity penalty. Nakov et al. (2012) fixed this by using several heuristics on brevity penalty, reference length and grounding the precision length. In our experiments, we use the improved version of PRO as provided by Nakov et al. (2012). We 221 MERT MIRA PRO PROv1 GIZA++ TA-GIZA"
W13-2228,I08-2089,0,0.333709,"s are less likely to be transliterations. 220 very sensitive to the value of λ. We use λ = 50 for our experiments. The procedure we described of estimation of transliteration probabilities and modification of EM is also followed in the opposite direction f-to-e. 3 call it PROv1 later on. 5 5.1 Dataset The amount of bitext used for the estimation of the translation model is ≈ 2M parallel sentences. We use newstest2012a for tuning and newstest2012b (tst2012) as development set. The language model is estimated using large monolingual corpus of Russian ≈ 21.7M sentences. We follow the approach of Schwenk and Koehn (2008) by training domain-specific language models separately and then linearly interpolate them using SRILM with weights optimized on the held-out development set. We divide the tuning set newstest2012a into two halves and use the first half for tuning and second for test in order to obtain stable weights (Koehn and Haddow, 2012). Transliteration System The unsupervised transliteration mining system (as described in Section 2) outputs a list of transliteration pairs. We consider transliteration word pairs as parallel sentences by putting a space after every character of the words and train a PSMT s"
W13-2228,W12-3139,0,0.0320278,"d for the estimation of the translation model is ≈ 2M parallel sentences. We use newstest2012a for tuning and newstest2012b (tst2012) as development set. The language model is estimated using large monolingual corpus of Russian ≈ 21.7M sentences. We follow the approach of Schwenk and Koehn (2008) by training domain-specific language models separately and then linearly interpolate them using SRILM with weights optimized on the held-out development set. We divide the tuning set newstest2012a into two halves and use the first half for tuning and second for test in order to obtain stable weights (Koehn and Haddow, 2012). Transliteration System The unsupervised transliteration mining system (as described in Section 2) outputs a list of transliteration pairs. We consider transliteration word pairs as parallel sentences by putting a space after every character of the words and train a PSMT system for transliteration. We apply the transliteration system to OOVs in a post-processing step on the output of the machine translation system. Russian is a morphologically rich language. Different cases of a word are generally represented by adding suffixes to the root form. For OOVs that are named entities, transliterati"
W13-2228,sharoff-etal-2008-designing,0,0.0184257,"n system evaluated on tst2012 using baseline GIZA++ alignment and transliteration augmented-GIZA++. OOV-TI presents the score of the system trained using TA-GIZA++ after transliterating OOVs 5.4 tst2012 6.1.1 POS Tagging We use RFTagger (Schmid and Laws, 2008) for POS tagging. Despite the good quality of tagging provided by RFTagger, some errors seem to be unavoidable due to the ambiguity of certain grammatical forms in Russian. A good example of this is neuter nouns that have the same form in all cases, or feminine nouns, which have identical forms in singular genitive and plural nominative (Sharoff et al., 2008). Since Russian sentences have free word order, and the case of nouns cannot be determined on that basis, this imperfection can not be corrected during tagging or by postprocessing the tagger output. Russian/English Experiments In this section, we present translation experiments in Russian to English direction. We morphologically reduce the Russian side of the parallel data in a pre-processing step and train the translation system on that. We compare its result with the Russian to English system trained on the un-processed parallel data. 6.1.2 Morphological Reduction English in comparison to S"
W13-2228,N03-1017,0,0.00764806,", , , , ) and transliterate the stemmed form. For morphologically reduced Russian (see Section 6.1), we follow the same procedure as OOVs are unknown to the POS tagger too and are (incorrectly) not reduced to their root forms. For OOVs that are not identified as named entities, we transliterate them without any pre-processing. 4 English/Russian Experiments 5.2 Baseline Settings We word-aligned the parallel corpus using GIZA++ (Och and Ney, 2003) with 5 iterations of Model1, 4 iterations of HMM and 4 iterations of Model4, and symmetrized the alignments using the grow-diag-final-and heuristic (Koehn et al., 2003). We built a phrase-based machine translation system using the Moses toolkit. Minimum error rate training (MERT), margin infused relaxed algorithm (MIRA) and PRO are used to optimize the parameters. 5.3 Main System Settings Our main system involves a pre-processing step – unsupervised transliteration mining, and a postprocessing step – transliteration of OOVs. For the training of the unsupervised transliteration mining system, we take the word alignments from our baseline settings and extract all word pairs which occur as 1-to-1 alignments (like Sajjad et al. (2011)) and later refer to them as"
W13-2228,C96-2141,0,0.0435274,"word alignment models. They combined the translation probabilities of the IBM models and the HMM model with the transliteration probabilities. Consider pta (f |e) = fta (f, e)/fta (e) is the translation probability of the word alignment models. The interpolated probability is calculated by adding the smoothed alignment frequency fta (f, e) to the transliteration probability weight by the factor λ. The modified translation probabilities is given by: Transliteration Augmented-GIZA++ GIZA++ aligns parallel sentences at word level. It applies the IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) in both directions i.e. source to target and target to source. It generates a list of translation pairs with translation probabilities, which is called the t-table. Sajjad et al. (2011) used a heuristic-based transliteration mining system and integrated it into the GIZA++ word aligner. We follow a similar procedure but use the unsupervised transliteration mining system of Sajjad et al. (2012). pˆ(f |e) = We define a transliteration sub-model and train it on the transliteration pairs mined by the unsupervised transliteration mining system. We integrate it into the GIZA++ word aligner. The prob"
W13-2228,W13-2230,1,0.735961,"ng The linguistic processing of Russian involves POS tagging and morphological reduction. We first tag the Russian data using a fine grained tagset. The tagger identifies lemmas and the set of morphological attributes attached to each word. We reduce the number of these attributes by deleting some of them, that are not relevant for English (for example, gender agreement of verbs). This generates a morphologically reduced Russian which is used in parallel with English for the training of the machine translation system. Further details on the morphological processing of Russian are described in Weller et al. (2013). Results Table 1 summarizes English/Russian results on tst2012. Improved word alignment gives up to 0.13 BLEU points improvement. PROv1 improves translation quality and shows 0.08 BLEU point increase in BLEU in comparison to the parameters tuned using PRO. The transliteration of OOVs consistently improve translation quality by at least 0.1 BLEU point for all systems.2 This adds to a cumulative gain of up to 0.2 BLEU points. We summarize results of our systems trained on GIZA++ and transliteration augmented-GIZA++ (TA-GIZA++) and tested on tst2012 and tst2013 in Table 2. Both systems use PROv1"
W13-2228,P07-2045,0,\N,Missing
W13-2228,C04-1072,0,\N,Missing
W13-2230,W10-1749,0,0.0262519,"highest usefulness scores for the reordering task. Then we trained a new grammar on the concatenation of the Tiger corpus and the automatic parses from Europarl. The usefulness score estimates the value of a parse tree for the reordering task. We calculated this score as the similarity between the word order achieved by applying the parse tree-based reordering rules of Fraser (2009) and the word order indicated by the automatic word alignment between the German and English sentences in Europarl. We used the Kendall’s Tau Distance as the similarity metric of two word orderings (as suggested by Birch and Osborne (2010)). Following this, we performed linguisticallyinformed compound splitting, using the system of Fritzinger and Fraser (2010), which disambiguates competing analyses from the high-recall Stuttgart Morphological Analyzer SMOR (Schmid et al., 2004) using corpus statistics. We also split German portmanteaus like zum → zu dem (meaning to the). DE-EN (OSM) DE-EN (OSM) BitPar not self-trained DE-EN (Moses) DE-EN (Moses) BitPar not self-trained EN-DE (Moses) BLEU (ci) 27.60 27.48 BLEU (cs) 26.12 25.99 system name 27.14 25.65 26.82 25.36 MES-Szegedreorder-split not submitted 19.68 18.97 MES-reorder MES"
W13-2230,P12-1050,0,0.0143602,"tuent parses for pre-reordering. For DE-EN we also deal with word formation issues such as compound splitting. We did not perform inflectional normalization or generation for German due to time constraints, instead focusing 236 system our efforts on these issues for French and Russian as previously described. German to English German has a wider diversity of clausal orderings than English, all of which need to be mapped to the English SVO order. This is a difficult problem to solve during inference, as shown for hierarchical SMT by Fabienne Braune and Fraser (2012) and for phrase-based SMT by Bisazza and Federico (2012). We syntactically parsed all of the source side sentences of the parallel German to English data available, and the tuning, test and blindtest sets. We then applied reordering rules to these parses. We use the rules for reordering German constituent parses of Collins et al. (2005) together with the additional rules described by Fraser (2009). These are applied as a preprocess to all German data. For parsing the German sentences, we used the generative phrase-structure parser BitPar with optimizations of the grammar, as described by Fraser et al. (2013). The parser was trained on the Tiger Tre"
W13-2230,P05-1022,0,0.0609343,".36 MES-Szegedreorder-split not submitted 19.68 18.97 MES-reorder MES not submitted Table 8: Results on WMT-2013 (blindtest) English to German The task of mapping English SVO order to the different clausal orders in German is difficult. For our English to German systems, we solved this by parsing the English and applying the system of Gojun and Fraser (2012) to reorder English into the correct German clausal order (depending on the clause type which is detected using the English parse, see (Gojun and Fraser, 2012) for further details). We primarily used the Charniak-Johnson generative parser (Charniak and Johnson, 2005) to parse the English Europarl data and the test data. However, due to time constraints we additionally used Berkeley parses of about 400K Europarl sentences and the other English parallel training data. We also left a small amount of the English parallel training data unparsed, which means that it was not reordered. For tune, test and blindtest (WMT2013), we used the Charniak-Johnson generative parser. Experiments and results We used all available training data for constrained systems; results for the WMT-2013 set are given in table 8. For the contrastive BitPar results, we reparsed WMT-2013."
W13-2230,N12-1047,0,0.0466778,"Moses system (DE-EN, EN-DE, EN-FR and FR-EN) or with an operation sequence model (RU-EN, DEEN), cf. Durrani et al. (2013b) for more details. An operation sequence model (OSM) is a stateof-the-art SMT-system that learns translation and reordering patterns by representing a sentence pair and its word alignment as a unique sequence of operations (see e.g. Durrani et al. (2011), Durrani et al. (2013a) for more details). For the Moses systems we used the old train-model perl scripts rather than the EMS, so we did not perform Good-Turing smoothing; parameter tuning was carried out with batch-mira (Cherry and Foster, 2012). 232 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 232–239, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics 1 2 3 4 5 System Baseline Simplified French* Removal of empty lines Conversion of HTML special characters like &quot; to the corresponding characters Unification of words that were written both with an œ or with an oe to only one spelling Punctuation normalization and tokenization Putting together clitics and apostrophes like l ’ or d ’ to l’ and d’ la / l’ / les → le un / une → un Infl. form → lemma e. g. au → a` le Redu"
W13-2230,P05-1066,0,0.216656,"Missing"
W13-2230,P11-1105,1,0.789613,"eneral translation model, this method also allows the generation of inflected word forms which do not occur in the training data. 2 Experimental setup The translation experiments in this paper are carried out with either a standard phrase-based Moses system (DE-EN, EN-DE, EN-FR and FR-EN) or with an operation sequence model (RU-EN, DEEN), cf. Durrani et al. (2013b) for more details. An operation sequence model (OSM) is a stateof-the-art SMT-system that learns translation and reordering patterns by representing a sentence pair and its word alignment as a unique sequence of operations (see e.g. Durrani et al. (2011), Durrani et al. (2013a) for more details). For the Moses systems we used the old train-model perl scripts rather than the EMS, so we did not perform Good-Turing smoothing; parameter tuning was carried out with batch-mira (Cherry and Foster, 2012). 232 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 232–239, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics 1 2 3 4 5 System Baseline Simplified French* Removal of empty lines Conversion of HTML special characters like &quot; to the corresponding characters Unification of words that we"
W13-2230,N13-1001,1,0.816849,"lly complex target language, we describe a two-step translation system built on non-inflected word stems with a post-processing component for predicting morphological features and the generation of inflected forms. In addition to the advantage of a more general translation model, this method also allows the generation of inflected word forms which do not occur in the training data. 2 Experimental setup The translation experiments in this paper are carried out with either a standard phrase-based Moses system (DE-EN, EN-DE, EN-FR and FR-EN) or with an operation sequence model (RU-EN, DEEN), cf. Durrani et al. (2013b) for more details. An operation sequence model (OSM) is a stateof-the-art SMT-system that learns translation and reordering patterns by representing a sentence pair and its word alignment as a unique sequence of operations (see e.g. Durrani et al. (2011), Durrani et al. (2013a) for more details). For the Moses systems we used the old train-model perl scripts rather than the EMS, so we did not perform Good-Turing smoothing; parameter tuning was carried out with batch-mira (Cherry and Foster, 2012). 232 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 232–239, c Sof"
W13-2230,2012.eamt-1.42,1,0.834142,"bmitted systems for DE-EN and EN-DE which used constituent parses for pre-reordering. For DE-EN we also deal with word formation issues such as compound splitting. We did not perform inflectional normalization or generation for German due to time constraints, instead focusing 236 system our efforts on these issues for French and Russian as previously described. German to English German has a wider diversity of clausal orderings than English, all of which need to be mapped to the English SVO order. This is a difficult problem to solve during inference, as shown for hierarchical SMT by Fabienne Braune and Fraser (2012) and for phrase-based SMT by Bisazza and Federico (2012). We syntactically parsed all of the source side sentences of the parallel German to English data available, and the tuning, test and blindtest sets. We then applied reordering rules to these parses. We use the rules for reordering German constituent parses of Collins et al. (2005) together with the additional rules described by Fraser (2009). These are applied as a preprocess to all German data. For parsing the German sentences, we used the generative phrase-structure parser BitPar with optimizations of the grammar, as described by Frase"
W13-2230,E12-1068,1,0.847569,"ng all necessary morphological features for the translation output, which are then used to generate fully inflected forms. This two-step setup decreases the complexity of the translation task by removing languagespecific features from the translation model. Furthermore, generating inflected forms based on word stems and morphological features allows to gener233 ate forms which do not occur in the parallel training data – this is not possible in a standard SMT setup. The idea of separating the translation into two steps to deal with complex morphology was introduced by Toutanova et al. (2008). Fraser et al. (2012) applied this method to the language pair English-German with an additional special focus on word formation issues such as the splitting and merging of portmanteau prepositions and compounds. The presented inflection prediction systems focuses on nominal inflection; verbal inflection is not addressed. Morphological analysis and resources The morphological analysis of the French training data is obtained using RFTagger, which is designed for annotating fine-grained morphological tags (Schmid and Laws, 2008). For generating inflected forms based on stems and morphological features, we use an ext"
W13-2230,J13-1005,1,0.858728,"Missing"
W13-2230,W09-0420,1,0.87823,"of clausal orderings than English, all of which need to be mapped to the English SVO order. This is a difficult problem to solve during inference, as shown for hierarchical SMT by Fabienne Braune and Fraser (2012) and for phrase-based SMT by Bisazza and Federico (2012). We syntactically parsed all of the source side sentences of the parallel German to English data available, and the tuning, test and blindtest sets. We then applied reordering rules to these parses. We use the rules for reordering German constituent parses of Collins et al. (2005) together with the additional rules described by Fraser (2009). These are applied as a preprocess to all German data. For parsing the German sentences, we used the generative phrase-structure parser BitPar with optimizations of the grammar, as described by Fraser et al. (2013). The parser was trained on the Tiger Treebank (Brants et al., 2002) along with utilizing the Europarl corpus as unlabeled data. At the training of Bitpar, we followed the targeted self-training approach (Katz-Brown et al., 2011) as follows. We parsed the whole Europarl corpus using a grammar trained on the Tiger corpus and extracted the 100best parse trees for each sentence. We sel"
W13-2230,W10-1734,1,0.788681,"us and the automatic parses from Europarl. The usefulness score estimates the value of a parse tree for the reordering task. We calculated this score as the similarity between the word order achieved by applying the parse tree-based reordering rules of Fraser (2009) and the word order indicated by the automatic word alignment between the German and English sentences in Europarl. We used the Kendall’s Tau Distance as the similarity metric of two word orderings (as suggested by Birch and Osborne (2010)). Following this, we performed linguisticallyinformed compound splitting, using the system of Fritzinger and Fraser (2010), which disambiguates competing analyses from the high-recall Stuttgart Morphological Analyzer SMOR (Schmid et al., 2004) using corpus statistics. We also split German portmanteaus like zum → zu dem (meaning to the). DE-EN (OSM) DE-EN (OSM) BitPar not self-trained DE-EN (Moses) DE-EN (Moses) BitPar not self-trained EN-DE (Moses) BLEU (ci) 27.60 27.48 BLEU (cs) 26.12 25.99 system name 27.14 25.65 26.82 25.36 MES-Szegedreorder-split not submitted 19.68 18.97 MES-reorder MES not submitted Table 8: Results on WMT-2013 (blindtest) English to German The task of mapping English SVO order to the diffe"
W13-2230,E12-1074,1,0.864449,"s. We also split German portmanteaus like zum → zu dem (meaning to the). DE-EN (OSM) DE-EN (OSM) BitPar not self-trained DE-EN (Moses) DE-EN (Moses) BitPar not self-trained EN-DE (Moses) BLEU (ci) 27.60 27.48 BLEU (cs) 26.12 25.99 system name 27.14 25.65 26.82 25.36 MES-Szegedreorder-split not submitted 19.68 18.97 MES-reorder MES not submitted Table 8: Results on WMT-2013 (blindtest) English to German The task of mapping English SVO order to the different clausal orders in German is difficult. For our English to German systems, we solved this by parsing the English and applying the system of Gojun and Fraser (2012) to reorder English into the correct German clausal order (depending on the clause type which is detected using the English parse, see (Gojun and Fraser, 2012) for further details). We primarily used the Charniak-Johnson generative parser (Charniak and Johnson, 2005) to parse the English Europarl data and the test data. However, due to time constraints we additionally used Berkeley parses of about 400K Europarl sentences and the other English parallel training data. We also left a small amount of the English parallel training data unparsed, which means that it was not reordered. For tune, test"
W13-2230,D11-1017,0,0.051958,"d reordering rules to these parses. We use the rules for reordering German constituent parses of Collins et al. (2005) together with the additional rules described by Fraser (2009). These are applied as a preprocess to all German data. For parsing the German sentences, we used the generative phrase-structure parser BitPar with optimizations of the grammar, as described by Fraser et al. (2013). The parser was trained on the Tiger Treebank (Brants et al., 2002) along with utilizing the Europarl corpus as unlabeled data. At the training of Bitpar, we followed the targeted self-training approach (Katz-Brown et al., 2011) as follows. We parsed the whole Europarl corpus using a grammar trained on the Tiger corpus and extracted the 100best parse trees for each sentence. We selected the parse tree among the 100 candidates which got the highest usefulness scores for the reordering task. Then we trained a new grammar on the concatenation of the Tiger corpus and the automatic parses from Europarl. The usefulness score estimates the value of a parse tree for the reordering task. We calculated this score as the similarity between the word order achieved by applying the parse tree-based reordering rules of Fraser (2009"
W13-2230,P10-1052,0,0.0314476,"Missing"
W13-2230,C12-1121,0,0.0602483,"Missing"
W13-2230,P11-1044,1,0.749892,"problem by stemming the OOVs based on a list of suffixes ( , , , , , ) and transliterating the stemmed forms. Voice Aspect Type Degree Type Formation Table 6: Rules for simplifying the morphological complexity for RU. training and extracts transliteration pairs that can be used for the training of the transliteration system. The procedure of mining transliteration pairs and transliterating OOVs is described as follows: We word-align the parallel corpus using GIZA++ and symmetrize the alignments using the grow-diagfinal-and heuristic. We extract all word pairs which occur as 1-to-1 alignments (Sajjad et al., 2011) and later refer to them as a list of word pairs. We train the unsupervised transliteration mining system on the list of word pairs and extract transliteration pairs. We use these mined pairs to build a transliteration system using the Moses toolkit. The transliteration system is applied as a post-processing step to transliterate OOVs. The morphological reduction of Russian (cf. section 5) does not process most of the OOVs as they are also unknown to the POS tagger. So OOVs that we get are in their original form. When translitExperiments and results We trained the systems separately on GIZA++"
W13-2230,P12-1049,1,0.744729,", particles, interjections and abbreviations) have no morphological attributes. The list of the original and the reduced attributes is given in Table 6. Transliteration mining to handle OOVs The machine translation system fails to translate out-ofvocabulary words (OOVs) as they are unknown to the training data. Most of the OOVs are named entities and transliterating them to the target language script could solve this problem. The transliteration system requires a list of transliteration pairs for training. As we do not have such a list, we use the unsupervised transliteration mining system of Sajjad et al. (2012) that takes a list of word pairs for Part of Speech Noun Pronoun Verb Adjective Preposition Conjunction Attributes RFTagger Type Gender Number Case Reduced attributes Type Gender Number Case nom,gen,dat,acc,instr,prep gen,notgen Animate Case 2 Person Gender Number Case Person Gender Number Case nom,gen,dat,acc,instr,prep nom,notnom Syntactic type Animated Type VForm Tense Person Number Gender Voice Definiteness Aspect Case Type Degree Gender Number Case Definiteness Type Formation Case Type Formation SYS GIZA++ TA-GIZA++ Original corpus WMT-2012 WMT-2013 32.51 25.5 33.40 25.9* SYS GIZA++ TA-GI"
W13-2230,W13-2228,1,0.735928,"ng system on the list of word pairs and extract transliteration pairs. We use these mined pairs to build a transliteration system using the Moses toolkit. The transliteration system is applied as a post-processing step to transliterate OOVs. The morphological reduction of Russian (cf. section 5) does not process most of the OOVs as they are also unknown to the POS tagger. So OOVs that we get are in their original form. When translitExperiments and results We trained the systems separately on GIZA++ and transliteration augmented-GIZA++ (TA-GIZA++) to compare their results; for more details see Sajjad et al. (2013). All systems are tuned using PROv1 (Nakov et al., 2012). The translation output is postprocessed to transliterate OOVs. Table 7 summarizes the results of RU-EN translation systems trained on the original corpus and on the morph-reduced corpus. Using TA-GIZA++ alignment gives the best results for both WMT2012 and WMT-2013, leading to an improvement of 0.4 BLEU points. The system built on the morph-reduced data leads to decreased BLEU results. However, the percentage of OOVs is reduced for both test sets when using the morph-reduced data set compared to the original data. An analysis of the out"
W13-2230,C08-1098,1,0.776757,"reas adjectives in English are not inflected at all. This causes data sparsity in coverage of French inflected forms. We try to overcome this problem by simplifying French inflected forms in a pre-processing step in order to adapt the French input better to the English output. Processing of the training and test data The pre-processing of the French input consists of two steps: (1) normalizing not well-formed data (cf. table 1) and (2) morphological simplification. In the second step, the normalized training data is annotated with Part-of-Speech tags (PoS-tags) and word lemmas using RFTagger (Schmid and Laws, 2008) which was trained on the French treebank (Abeill´e et al., 2003). French forms are then simplified according to the rules given in table 2. Data and experiments We trained a French to English Moses system on the preprocessed and BLEU (ci) 31.02 30.83 Table 3: Results of the French to English system (WMT-2012). The marked system (*) corresponds to the system submitted for manual evaluation. (cs: case-sensitive, ci: case-insensitive) Table 1: Text normalization for FR-EN. Definite determiners Indefinite determiners Adjectives Portmanteaus Verb participles inflected for gender and number ending"
W13-2230,schmid-etal-2004-smor,1,0.771053,"Missing"
W13-2230,I08-2089,0,0.135299,"h an œ or with an oe to only one spelling Punctuation normalization and tokenization Putting together clitics and apostrophes like l ’ or d ’ to l’ and d’ la / l’ / les → le un / une → un Infl. form → lemma e. g. au → a` le Reduced to non-inflected verb participle form ending in e´ d’ → de, qu’ → que, n’ → ne, ... Table 2: Rules for morphological simplification. The development data consists of the concatenated news-data sets from the years 2008-2011. Unless otherwise stated, we use all constrained data (parallel and monolingual). For the target-side language models, we follow the approach of Schwenk and Koehn (2008) and train a separate language model for each corpus and then interpolate them using weights optimized on development data. 3 French to English French has a much richer morphology than English; for example, adjectives in French are inflected with respect to gender and number whereas adjectives in English are not inflected at all. This causes data sparsity in coverage of French inflected forms. We try to overcome this problem by simplifying French inflected forms in a pre-processing step in order to adapt the French input better to the English output. Processing of the training and test data Th"
W13-2230,P08-1059,0,0.0269016,"step consists of predicting all necessary morphological features for the translation output, which are then used to generate fully inflected forms. This two-step setup decreases the complexity of the translation task by removing languagespecific features from the translation model. Furthermore, generating inflected forms based on word stems and morphological features allows to gener233 ate forms which do not occur in the parallel training data – this is not possible in a standard SMT setup. The idea of separating the translation into two steps to deal with complex morphology was introduced by Toutanova et al. (2008). Fraser et al. (2012) applied this method to the language pair English-German with an additional special focus on word formation issues such as the splitting and merging of portmanteau prepositions and compounds. The presented inflection prediction systems focuses on nominal inflection; verbal inflection is not addressed. Morphological analysis and resources The morphological analysis of the French training data is obtained using RFTagger, which is designed for annotating fine-grained morphological tags (Schmid and Laws, 2008). For generating inflected forms based on stems and morphological f"
W13-2230,W13-2213,1,\N,Missing
W14-3628,bouamor-etal-2014-multidialectal,0,0.0565805,"a for each of these subdialects, and we release this data to the research community. Related Work NLP for DA is still in its early stages of development and many challenges need to be overcomed such as the lack of suitable tools and resources. Collecting resources for dialectal Arabic: Several researchers have directed efforts to develop DA computational resources (Maamouri et al., 2006; Al-Sabbagh and Girju, 2010; Zaidan and Callison-Burch, 2011; Salama et al., 2014). Zbib et al. (2012) built two dialectal Arabic-English parallel corpora for Egyptian and Levantine Arabic using crowdsourcing. Bouamor et al. (2014) presented a multi-dialectal Arabic parallel corpus, which covers five Arabic dialects besides MSA and English. Mubarak and Darwish (2014) collected a multi-dialectal corpus using Twitter. Unlike previous work, we focus on Gulf subdialects, particularly Qatari Arabic. The monolingual data that we collected is a high-quality dialectal resource and originates from dialect-specific sources such as novels and forums. We train an unsupervised segmentation tool, Morphessor, and its MAP model (Creutz and Lagus, 2007), using different variations of the collected Qatari data. We optimize the single hyp"
W14-3628,J93-2003,0,0.0394221,"Missing"
W14-3628,2012.eamt-1.60,0,0.0204419,"n help to better adapt resources for dialects and MSA for SMT. This section describes our experimental setup. to Qº, and we reduce character elongations to be just two characters long. In order to maintain consistency among different resources, we re  move supplementary diacritics, e.g., Y®« ‘knots’ Datasets: We divided the QCA corpus into 1k sentences each for development and testing, and we used the remaining 12k for training. We adapted parallel corpora for Egyptian, Levantine and MSA to English to be used for Qatari Arabic to English SMT. For MSA, we used parallel corpora of TED talks (Cettolo et al., 2012) and the AMARA corpus (Abdelali et al., 2014), which consists of educational videos. Since the QCA corpus is in the speech domain, we believe that an MSA corpus of spoken domain would be more helpful than a text domain such as News. For Egyptian and Levantine, we used the parallel corpus provided by Zbib et al. (2012). There is no Gulf–English parallel data available in the literature. The data that we found was a very small collection of subdialects of Gulf Arabic; we did not use it for MT experiments. However, we used the Qatari part of the AVIA corpus to train Morfessor.  /Euqad/ is normal"
W14-3628,E06-1047,0,0.0771248,"so explain our experimental setup and we present the results (Section 5). We then discuss translating in the reverse direction, i.e., into Qatari Arabic (Section 6). Finally, we point to possible directions for future work and we conclude the paper (Section 7). Building morphological segmenters for the Arabic dialects: Researchers have already focused efforts on crafting and extending existing MSA tools to DA by mainly using a set of rules (Habash et al., 2012). Habash and Rambow (2006) presented MAGEAD, a knowledge-based morphological analyzer and generator for Egyptian and Levantine Arabic. Chiang et al. (2006) developed a Levantine morphological analyzer on top of an existing MSA analyzer using an explicit knowledge base. 208 Meanwhile, the MSA consonant /Z/ is realized as /D/ in EGY. For example, the MSA ‘luck’ is maintained pronunciation /HaZ/ of ¡k in QA and transformed to /HaD/ in EGY. This change is consistent in all words within each dialect. However, such phonological variations between dialects have the potential to add ambiguity to dialectal Arabic. The MSA consonant h. /j/ can be used to distinguish between different dialects, particularly Gulf subdialects. h. /j/ is pronounced as ø /y/ i"
W14-3628,P11-1105,0,0.0332246,"Missing"
W14-3628,elmahdy-etal-2014-development,0,0.0277134,"More detailed description follows below. Lexical variations are among the most obvious differences between Arabic dialects. For exam ple, the MSA word @ XAÓ ‘what’ /mA*A/ would be 3.4 QCA Table 1: Statistics about the collected parallel corpora (in thousands). AVIAO shows the statistics about the AVIA corpus excluding Qatari data. /taEal˜am/ becomes ÕÎªK @ /AitEalim/ in EGY, while the MSA form is preserved in QA. 3.3 Corpus Bilingual corpora: – The QCA speech corpus, comprises 14.7k sentences that are phonetically transcribed from TV broadcasts in Qatari Arabic and translated to English; see (Elmahdy et al., 2014) for more detail. The corpus was designed for speech recognition and we faced several normalization-related issues that we had to resolve before it could be used for machine translation and language modeling. One example is the usage of five Persian characters to represent some sounds in Arabic words. Moreover, the English side had some grammatical and spelling errors. We normalized the Arabic side and corrected the English side of the corpus as described in Section 4.2. The corpus can be found at http://sprosig.isle. illinois.edu/corpora/1. – The AVIA corpus1 is designed as a reference source"
W14-3628,P12-1016,0,0.0171831,"/V/ to ¬ H /P/ to H. /b/, and P and h /J/ to h. /j/. /f/, For the English texts, the orthographic variations were already normalized. However, the English side of the QCA corpus had some spelling and grammatical errors, which we corrected manually. On the grammatical side, we only corrected a subset of the data, which we used for tuning and testing our SMT system (see Section 5). 4.3 Morphological Decomposition There is no general Arabic morphological segmenter that works for all variations of Arabic. The most commonly used segmenters for Arabic were designed for MSA (Habash et al., 2009; Green and DeNero, 2012). Due to the lexical and morphological differences between dialects and MSA, these MSA-based morphological tools do not work well for dialects. 5 Experimental Setup Machine translation system settings: We used a phrase-based statistical machine translation model as implemented in the Moses toolkit (Koehn et al., 2007) for machine translation. 6 This is an extension of the basic Morfessor method and is based on a Maximum a Posteriori model. This issue relates to the QCA corpus. 211 We built separate directed word alignments for source-to-target and target-to-source using IBM model 4 (Brown et a"
W14-3628,abdelali-etal-2014-amara,1,0.75838,"s and MSA for SMT. This section describes our experimental setup. to Qº, and we reduce character elongations to be just two characters long. In order to maintain consistency among different resources, we re  move supplementary diacritics, e.g., Y®« ‘knots’ Datasets: We divided the QCA corpus into 1k sentences each for development and testing, and we used the remaining 12k for training. We adapted parallel corpora for Egyptian, Levantine and MSA to English to be used for Qatari Arabic to English SMT. For MSA, we used parallel corpora of TED talks (Cettolo et al., 2012) and the AMARA corpus (Abdelali et al., 2014), which consists of educational videos. Since the QCA corpus is in the speech domain, we believe that an MSA corpus of spoken domain would be more helpful than a text domain such as News. For Egyptian and Levantine, we used the parallel corpus provided by Zbib et al. (2012). There is no Gulf–English parallel data available in the literature. The data that we found was a very small collection of subdialects of Gulf Arabic; we did not use it for MT experiments. However, we used the Qatari part of the AVIA corpus to train Morfessor.  /Euqad/ is normalized to Y®«, and we map Persian letters to th"
W14-3628,P06-1086,0,0.176067,"uses morphological segmentation to combine resources for other Arabic dialects in a QA-EN SMT system effectively (Section 4.3). We also explain our experimental setup and we present the results (Section 5). We then discuss translating in the reverse direction, i.e., into Qatari Arabic (Section 6). Finally, we point to possible directions for future work and we conclude the paper (Section 7). Building morphological segmenters for the Arabic dialects: Researchers have already focused efforts on crafting and extending existing MSA tools to DA by mainly using a set of rules (Habash et al., 2012). Habash and Rambow (2006) presented MAGEAD, a knowledge-based morphological analyzer and generator for Egyptian and Levantine Arabic. Chiang et al. (2006) developed a Levantine morphological analyzer on top of an existing MSA analyzer using an explicit knowledge base. 208 Meanwhile, the MSA consonant /Z/ is realized as /D/ in EGY. For example, the MSA ‘luck’ is maintained pronunciation /HaZ/ of ¡k in QA and transformed to /HaD/ in EGY. This change is consistent in all words within each dialect. However, such phonological variations between dialects have the potential to add ambiguity to dialectal Arabic. The MSA conso"
W14-3628,al-sabbagh-girju-2010-mining,0,0.0365102,"me additional monolingual data for Qatari Arabic. Qatari Arabic is a subdialect of the more general Gulf dialect, among with Saudi, Kuwaiti, Emirati, Bahraini, and Omani; we collected additional monologual data for each of these subdialects, and we release this data to the research community. Related Work NLP for DA is still in its early stages of development and many challenges need to be overcomed such as the lack of suitable tools and resources. Collecting resources for dialectal Arabic: Several researchers have directed efforts to develop DA computational resources (Maamouri et al., 2006; Al-Sabbagh and Girju, 2010; Zaidan and Callison-Burch, 2011; Salama et al., 2014). Zbib et al. (2012) built two dialectal Arabic-English parallel corpora for Egyptian and Levantine Arabic using crowdsourcing. Bouamor et al. (2014) presented a multi-dialectal Arabic parallel corpus, which covers five Arabic dialects besides MSA and English. Mubarak and Darwish (2014) collected a multi-dialectal corpus using Twitter. Unlike previous work, we focus on Gulf subdialects, particularly Qatari Arabic. The monolingual data that we collected is a high-quality dialectal resource and originates from dialect-specific sources such a"
W14-3628,W14-3601,0,0.0778929,"ages of development and many challenges need to be overcomed such as the lack of suitable tools and resources. Collecting resources for dialectal Arabic: Several researchers have directed efforts to develop DA computational resources (Maamouri et al., 2006; Al-Sabbagh and Girju, 2010; Zaidan and Callison-Burch, 2011; Salama et al., 2014). Zbib et al. (2012) built two dialectal Arabic-English parallel corpora for Egyptian and Levantine Arabic using crowdsourcing. Bouamor et al. (2014) presented a multi-dialectal Arabic parallel corpus, which covers five Arabic dialects besides MSA and English. Mubarak and Darwish (2014) collected a multi-dialectal corpus using Twitter. Unlike previous work, we focus on Gulf subdialects, particularly Qatari Arabic. The monolingual data that we collected is a high-quality dialectal resource and originates from dialect-specific sources such as novels and forums. We train an unsupervised segmentation tool, Morphessor, and its MAP model (Creutz and Lagus, 2007), using different variations of the collected Qatari data. We optimize the single hyperparameter of the MAP model by maximizing the translation quality of the QA-EN SMT system in terms of BLEU. Our experimental results demo"
W14-3628,D09-1141,1,0.822284,"ith the QCA bitext for Qatari Arabic to English machine translation. We explored three segmentation options for the Arabic side of the data: (i) no segmentation, (ii) ATB segmentation, and (iii) unsupervised segmentation using Morfessor. The QCA corpus is of much smaller size compared to other Arabic variants, say MSA. It is possible that in the training of the machine translation models, the large corpus dominates the QCA corpus. In order to avoid that, we balanced the two corpora by replicating the smaller corpus X number of times in order to make it approximately equal to the large corpus (Nakov and Ng, 2009).8 The complete procedure is described below. In a nutshell, for building a machine translation system using the MSA plus Qatari corpus, we first balanced the Qatari corpus to make it approximately equal to MSA and concatenated them. For training Morfessor, the Qatari Arabic data consisted of QCA, Novels and AVIAQA , while for SMT, it consisted of QCA only. In both cases, we balanced it to be approximately equal to MSA. We then trained Morfessor on the balanced (QCA, Novels, AVIAQA ) plus MSA data and we segmented the Arabic side of the balanced QCA plus MSA training data for machine translati"
W14-3628,C12-1121,1,0.888804,"Missing"
W14-3628,W11-2123,0,0.0741014,"Missing"
W14-3628,P02-1040,0,0.0966622,"Missing"
W14-3628,D11-1125,0,0.0296435,"Missing"
W14-3628,W14-3627,0,0.108906,"ialectal training data, which consists of Qatari Arabic, Egyptian Arabic (EGY), Levantine Arabic (LEV) and MSA to English, i.e., a scaled combination of all the available parallel data. We train a QA-EN SMT system using the segmented multi-dialectal data, and we show an absolute gain of 1.5 BLEU points compared to a baseline that uses no segmentation. Adapting SMT resources for other Arabic dialects: Many researchers have explored the potential of using MSA as a pivot language for improving SMT of Arabic dialects (Bakr et al., 2008; Sawaf, 2010; Salloum and Habash, 2011; Sajjad et al., 2013a; Jeblee et al., 2014). This often involves DA-MSA conversion schemes as an alternative in the absence of DA-MSA parallel resources. In contrast, limited work has been done on leveraging available resources for other dialects. Recently, Zbib et al. (2012) have shown that using a small amount of dialectal data could yield great improvements for SMT. Here, we investigate the potential of improving the resource adaptability of Arabic dialects. Our work is different as we use an unsupervised segmenter that helps in improving the lexical overlap between dialects and MSA. The rest of the paper is organized as follows: Fi"
W14-3628,N09-1024,0,0.0276176,"s the first collection of monolingual corpora for Gulf Arabic subdialects. It can be helpful for, e.g., language modeling when translating into Arabic, for learning the similarities and differences between Gulf subdialects, etc. Table 2 shows some statistics about the data after punctuation tokenization. In this work, we used an unsupervised morphological segmenter, Morfessor-categories MAP6 , an unsupervised model with a single hyperparameter (Creutz and Lagus, 2007). We chose Morfessor because of its superior performance on Arabic compared to other unsupervised models (Siivola et al., 2007; Poon et al., 2009). The model has a single hyperparameter, the perplexity threshold parameter B, which controls the granularity of segmentation. The recommended value ranges from 1 to 400 where 1 means maximum fine-grained segmentation, and 400 restricts it to the least segmented output. We set the threshold empirically to 70, as shown in Section 5.1. 4.2 5 Corpus Tokens Types AE BH Novel KW OM QA SA Forum QA 573 43 244 22 178 27 412 43 614 71 69 15 372 27 Table 2: Statistics about the collected monolingual corpora (in thousands of words). Orthographic Normalization The inconsistency in the orthographic spellin"
W14-3628,2006.amta-papers.21,0,0.0275349,"oped a Levantine morphological analyzer on top of an existing MSA analyzer using an explicit knowledge base. 208 Meanwhile, the MSA consonant /Z/ is realized as /D/ in EGY. For example, the MSA ‘luck’ is maintained pronunciation /HaZ/ of ¡k in QA and transformed to /HaD/ in EGY. This change is consistent in all words within each dialect. However, such phonological variations between dialects have the potential to add ambiguity to dialectal Arabic. The MSA consonant h. /j/ can be used to distinguish between different dialects, particularly Gulf subdialects. h. /j/ is pronounced as ø /y/ in KW, Riesa and Yarowsky (2006) trained a supervised trie-based model using a small lexicon of dialectal affixes. In our work, we eliminate the need for linguistic knowledge by training an unsupervised model using available resources. The unsupervised mode of learning allowed us to develop a multi-dialectal morphological segmenter. 3 Arabic Dialects In this section, we highlight some of the linguistic differences between Arabic dialects and MSA, with a focus on the Qatari dialect. 3.1 BH, QA, AE,  /q/ in OM, much like in EGY, h. /j/ in SA, much like in LEV. For example, the MSA word Yj.Ó ‘mosque’ /masjid/ is  pronounced"
W14-3628,N03-1017,0,0.00752633,"these MSA-based morphological tools do not work well for dialects. 5 Experimental Setup Machine translation system settings: We used a phrase-based statistical machine translation model as implemented in the Moses toolkit (Koehn et al., 2007) for machine translation. 6 This is an extension of the basic Morfessor method and is based on a Maximum a Posteriori model. This issue relates to the QCA corpus. 211 We built separate directed word alignments for source-to-target and target-to-source using IBM model 4 (Brown et al., 1993), and we symmetrized them using the grow-diag-final-and heuristics (Koehn et al., 2003). We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing (Kneser and Ney, 1995). We also built a lexicalized reordering model, msd-bidirectional-fe. We built a 5-gram language model on the English side of QCA-train using KenLM (Heafield, 2011). Finally, we built a log-linear model using the above features. We tuned the model weights by optimizing BLEU (Papineni et al., 2002) on the tuning set, using PRO (Hopkins and May, 2011) with sentencelevel BLEU+1 optimization (Nakov et al., 2012). In testing, we used"
W14-3628,P13-2001,1,0.937556,"ic side of the multi-dialectal training data, which consists of Qatari Arabic, Egyptian Arabic (EGY), Levantine Arabic (LEV) and MSA to English, i.e., a scaled combination of all the available parallel data. We train a QA-EN SMT system using the segmented multi-dialectal data, and we show an absolute gain of 1.5 BLEU points compared to a baseline that uses no segmentation. Adapting SMT resources for other Arabic dialects: Many researchers have explored the potential of using MSA as a pivot language for improving SMT of Arabic dialects (Bakr et al., 2008; Sawaf, 2010; Salloum and Habash, 2011; Sajjad et al., 2013a; Jeblee et al., 2014). This often involves DA-MSA conversion schemes as an alternative in the absence of DA-MSA parallel resources. In contrast, limited work has been done on leveraging available resources for other dialects. Recently, Zbib et al. (2012) have shown that using a small amount of dialectal data could yield great improvements for SMT. Here, we investigate the potential of improving the resource adaptability of Arabic dialects. Our work is different as we use an unsupervised segmenter that helps in improving the lexical overlap between dialects and MSA. The rest of the paper is o"
W14-3628,N04-1022,0,0.0213042,"Missing"
W14-3628,salama-etal-2014-youdacc,0,0.1266,"ic is a subdialect of the more general Gulf dialect, among with Saudi, Kuwaiti, Emirati, Bahraini, and Omani; we collected additional monologual data for each of these subdialects, and we release this data to the research community. Related Work NLP for DA is still in its early stages of development and many challenges need to be overcomed such as the lack of suitable tools and resources. Collecting resources for dialectal Arabic: Several researchers have directed efforts to develop DA computational resources (Maamouri et al., 2006; Al-Sabbagh and Girju, 2010; Zaidan and Callison-Burch, 2011; Salama et al., 2014). Zbib et al. (2012) built two dialectal Arabic-English parallel corpora for Egyptian and Levantine Arabic using crowdsourcing. Bouamor et al. (2014) presented a multi-dialectal Arabic parallel corpus, which covers five Arabic dialects besides MSA and English. Mubarak and Darwish (2014) collected a multi-dialectal corpus using Twitter. Unlike previous work, we focus on Gulf subdialects, particularly Qatari Arabic. The monolingual data that we collected is a high-quality dialectal resource and originates from dialect-specific sources such as novels and forums. We train an unsupervised segmentat"
W14-3628,maamouri-etal-2006-developing,0,0.115294,"l, we also collected some additional monolingual data for Qatari Arabic. Qatari Arabic is a subdialect of the more general Gulf dialect, among with Saudi, Kuwaiti, Emirati, Bahraini, and Omani; we collected additional monologual data for each of these subdialects, and we release this data to the research community. Related Work NLP for DA is still in its early stages of development and many challenges need to be overcomed such as the lack of suitable tools and resources. Collecting resources for dialectal Arabic: Several researchers have directed efforts to develop DA computational resources (Maamouri et al., 2006; Al-Sabbagh and Girju, 2010; Zaidan and Callison-Burch, 2011; Salama et al., 2014). Zbib et al. (2012) built two dialectal Arabic-English parallel corpora for Egyptian and Levantine Arabic using crowdsourcing. Bouamor et al. (2014) presented a multi-dialectal Arabic parallel corpus, which covers five Arabic dialects besides MSA and English. Mubarak and Darwish (2014) collected a multi-dialectal corpus using Twitter. Unlike previous work, we focus on Gulf subdialects, particularly Qatari Arabic. The monolingual data that we collected is a high-quality dialectal resource and originates from dia"
W14-3628,W11-2602,0,0.0453054,"which we train on the Arabic side of the multi-dialectal training data, which consists of Qatari Arabic, Egyptian Arabic (EGY), Levantine Arabic (LEV) and MSA to English, i.e., a scaled combination of all the available parallel data. We train a QA-EN SMT system using the segmented multi-dialectal data, and we show an absolute gain of 1.5 BLEU points compared to a baseline that uses no segmentation. Adapting SMT resources for other Arabic dialects: Many researchers have explored the potential of using MSA as a pivot language for improving SMT of Arabic dialects (Bakr et al., 2008; Sawaf, 2010; Salloum and Habash, 2011; Sajjad et al., 2013a; Jeblee et al., 2014). This often involves DA-MSA conversion schemes as an alternative in the absence of DA-MSA parallel resources. In contrast, limited work has been done on leveraging available resources for other dialects. Recently, Zbib et al. (2012) have shown that using a small amount of dialectal data could yield great improvements for SMT. Here, we investigate the potential of improving the resource adaptability of Arabic dialects. Our work is different as we use an unsupervised segmenter that helps in improving the lexical overlap between dialects and MSA. The r"
W14-3628,2010.amta-papers.5,0,0.355818,"ation model, which we train on the Arabic side of the multi-dialectal training data, which consists of Qatari Arabic, Egyptian Arabic (EGY), Levantine Arabic (LEV) and MSA to English, i.e., a scaled combination of all the available parallel data. We train a QA-EN SMT system using the segmented multi-dialectal data, and we show an absolute gain of 1.5 BLEU points compared to a baseline that uses no segmentation. Adapting SMT resources for other Arabic dialects: Many researchers have explored the potential of using MSA as a pivot language for improving SMT of Arabic dialects (Bakr et al., 2008; Sawaf, 2010; Salloum and Habash, 2011; Sajjad et al., 2013a; Jeblee et al., 2014). This often involves DA-MSA conversion schemes as an alternative in the absence of DA-MSA parallel resources. In contrast, limited work has been done on leveraging available resources for other dialects. Recently, Zbib et al. (2012) have shown that using a small amount of dialectal data could yield great improvements for SMT. Here, we investigate the potential of improving the resource adaptability of Arabic dialects. Our work is different as we use an unsupervised segmenter that helps in improving the lexical overlap betwe"
W14-3628,P11-2007,0,0.145365,"ta for Qatari Arabic. Qatari Arabic is a subdialect of the more general Gulf dialect, among with Saudi, Kuwaiti, Emirati, Bahraini, and Omani; we collected additional monologual data for each of these subdialects, and we release this data to the research community. Related Work NLP for DA is still in its early stages of development and many challenges need to be overcomed such as the lack of suitable tools and resources. Collecting resources for dialectal Arabic: Several researchers have directed efforts to develop DA computational resources (Maamouri et al., 2006; Al-Sabbagh and Girju, 2010; Zaidan and Callison-Burch, 2011; Salama et al., 2014). Zbib et al. (2012) built two dialectal Arabic-English parallel corpora for Egyptian and Levantine Arabic using crowdsourcing. Bouamor et al. (2014) presented a multi-dialectal Arabic parallel corpus, which covers five Arabic dialects besides MSA and English. Mubarak and Darwish (2014) collected a multi-dialectal corpus using Twitter. Unlike previous work, we focus on Gulf subdialects, particularly Qatari Arabic. The monolingual data that we collected is a high-quality dialectal resource and originates from dialect-specific sources such as novels and forums. We train an"
W14-3628,N12-1006,0,0.715837,"the more general Gulf dialect, among with Saudi, Kuwaiti, Emirati, Bahraini, and Omani; we collected additional monologual data for each of these subdialects, and we release this data to the research community. Related Work NLP for DA is still in its early stages of development and many challenges need to be overcomed such as the lack of suitable tools and resources. Collecting resources for dialectal Arabic: Several researchers have directed efforts to develop DA computational resources (Maamouri et al., 2006; Al-Sabbagh and Girju, 2010; Zaidan and Callison-Burch, 2011; Salama et al., 2014). Zbib et al. (2012) built two dialectal Arabic-English parallel corpora for Egyptian and Levantine Arabic using crowdsourcing. Bouamor et al. (2014) presented a multi-dialectal Arabic parallel corpus, which covers five Arabic dialects besides MSA and English. Mubarak and Darwish (2014) collected a multi-dialectal corpus using Twitter. Unlike previous work, we focus on Gulf subdialects, particularly Qatari Arabic. The monolingual data that we collected is a high-quality dialectal resource and originates from dialect-specific sources such as novels and forums. We train an unsupervised segmentation tool, Morphessor"
W14-3628,N06-2051,1,0.790504,"word forms of a root word may not be always possible. Considering the different variants of Arabic, the problem is exacerabated as dialects could use different choices of affixes for the same function. For example, the MSA . ªÊK /yalEabuwn/, meaning ‘they are playword àñJ ing’, could be found as . ªÊK /ylEbuwn/ in Gulf, àñJ @ñJ.ªÊK Ñ« /Eam yilEabuA/ in Levantine, and as @ñJ.ªÊJ K. /biylEabwA/ in Egyptian Arabic. as Introduction One possible solution is to use a morphological segmenter that segments words into simpler units such as stems and affixes, which might be covered in the training set (Zollmann et al., 2006; Tsai et al., 2010). When applied to dialects, this may reduce the lexical gap between dialects and MSA by matching the common stems. Unfortunately, there are no standard morphological segmentation tools for dialects. Due to the difference in morphology, tools designed for MSA do not work well for dialects. Developing rule-based segmenters for each dialect might appear to be the ideal solution, but, as the orthography of dialects is not standardized, crafting linguistic rules for them is very hard. The Arabic language has many varieties, where the Modern Standard Arabic (MSA) coexists with va"
W14-3628,P07-2045,0,\N,Missing
W14-3628,W12-2301,0,\N,Missing
W14-3628,2013.iwslt-evaluation.8,1,\N,Missing
W15-3059,W11-2101,0,0.0448672,"Missing"
W15-3059,W07-0718,0,0.744642,"ls with only target language information. 1 Introduction Each year thousands of human judgments are used to evaluate the quality of Machine Translation (MT) systems to determine which algorithms and techniques are to be considered the new state-ofthe-art. In a typical scenario human judges evaluate a system’s output (or hypothesis) by comparing it to a source sentence and/or to a reference translation. Then, they score the hypothesis according to a set of defined criteria such as fluency and adequacy (White et al., 1994); or rank a set of hypotheses in order of preference (Vilar et al., 2007; Callison-Burch et al., 2007). Evaluating MT output can be a challenging task for a number of reasons: it is tedious and therefore evaluators can lose interest quickly; it is complex, especially if the guidelines are not well defined; and evaluators can have difficulty distinguishing between different aspects of the translations (Callison-Burch et al., 2007). • Given different scenarios, what source of information do evaluators use to evaluate a translation? Do they use the source text, the target text, or both? Does the availability of specific information changes the consistency of the evaluation? • Are there difference"
W15-3059,W12-3102,0,0.110664,"Missing"
W15-3059,P14-1065,1,0.885127,"Missing"
W15-3059,2013.mtsummit-wptp.5,0,0.0518021,"Missing"
W15-3059,2006.amta-papers.25,0,0.201332,"Missing"
W15-3059,stymne-etal-2012-eye,0,0.215144,"information do evaluators use to evaluate a translation? Do they use the source text, the target text, or both? Does the availability of specific information changes the consistency of the evaluation? • Are there differences of behavior between bilinguals (i.e. evaluators fluent in both source and target languages) and monolinguals (i.e. evaluators fluent only in the target language)? Which group is more consistent? 457 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 457–466, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. Stymne et al. (2012) applied eye-tracking to machine translation error analysis. They found that longer gaze time and and a higher number of fixations correlate with high number of errors in the MT output. Doherty and O’Brien (2014) used eyetracking to evaluate the quality of raw machine translation output in terms of its usability by an end user. They concluded that eye-tracking correlates well with the other measures which they used for their study. In this work, we use eye-tracking to observe which sources of information evaluators use while performing an MT evaluation task and how this impacts the task comple"
W15-3059,2003.mtsummit-papers.51,0,0.22915,"Missing"
W15-3059,W07-0713,0,0.0259127,"er to use monolinguals with only target language information. 1 Introduction Each year thousands of human judgments are used to evaluate the quality of Machine Translation (MT) systems to determine which algorithms and techniques are to be considered the new state-ofthe-art. In a typical scenario human judges evaluate a system’s output (or hypothesis) by comparing it to a source sentence and/or to a reference translation. Then, they score the hypothesis according to a set of defined criteria such as fluency and adequacy (White et al., 1994); or rank a set of hypotheses in order of preference (Vilar et al., 2007; Callison-Burch et al., 2007). Evaluating MT output can be a challenging task for a number of reasons: it is tedious and therefore evaluators can lose interest quickly; it is complex, especially if the guidelines are not well defined; and evaluators can have difficulty distinguishing between different aspects of the translations (Callison-Burch et al., 2007). • Given different scenarios, what source of information do evaluators use to evaluate a translation? Do they use the source text, the target text, or both? Does the availability of specific information changes the consistency of the eval"
W15-3059,H93-1040,0,0.574996,"Missing"
W15-3059,1994.amta-1.25,0,0.487622,"Missing"
W15-3059,1993.mtsummit-1.24,0,\N,Missing
W15-3059,W14-3352,1,\N,Missing
W15-3059,W12-4906,0,\N,Missing
W15-3217,W08-0509,0,0.0366323,"Missing"
W15-3217,I08-2131,0,0.527985,"uction With the increased usage of computers in the processing of various languages comes the need for correcting errors introduced at different stages. Hence, the topic of text correction has seen a lot of interest in the past several years (Haddad and Yaseen, 2007; Rozovskaya et al., 2013). Numerous approaches have been explored to correct spelling errors in texts using NLP tools and resources (Kukich, 1992; Oflazer, 1996). The spelling correction for Arabic is an understudied problem in comparison to English, although small amount of research has been done previously (Shaalan et al., 2003; Hassan et al., 2008). The reason for this is the complexity of Arabic language and unavailability of language resources. For example, the Arabic spell checker in Microsoft Word gives incorrect suggests for even simple errors. First shared task on automatic Arabic text 2 Data Resources QALB: We trained and evaluated our system using the data provided for the shared task and the m2Scorer (Dahlmeier and Ng, 2012). These datasets are extracted from the QALB corpus of human-edited Arabic text produced by native speakers, non-native speakers and machines (Zaghouani et al., 2014). The corpus contains a large 144 Proceed"
W15-3217,E09-2008,0,0.0206763,"r for a regular set over a, b described by the regular expression (aba + bab)*, and we want to recognize the inputs that are slightly corrupted, for example, abaaaba may be matched to abaaba (correcting for a spurious a), or babbb may be matched to babbab (correcting for a deletion), or ababba may be matched to either abaaba (correcting a b to an a) or to ababab (correcting the reversal of the last two symbols). This method is perfect for handling mainly transposition errors resulting from swapping two letters , or typing errors of neighboring letters in the keyboard. We use the Foma library (Hulden, 2009) to build the finite-state tranducer using the Arabic Word-list as a dictionary.4 For each word, our system checks if the word is analyzed and recognized by the finite-state transducer. It then generates a list of correction candidates for the nonrecognized ones. The candidates are words having an edit distance lower than a certain threshold. We score the different candidates using a LM and consider the best one as the possible correction for each word. 4 ous system configurations on the L2 dev and test 2014 sets are given in Table 3. The results clearly show different modules are complementry"
W15-3217,C12-2011,0,0.494671,"Missing"
W15-3217,W14-3618,1,0.86265,"Missing"
W15-3217,N12-1067,0,0.163355,"Missing"
W15-3217,W14-3620,0,0.536639,"Missing"
W15-3217,P07-2045,0,0.00732275,"Missing"
W15-3217,W14-3309,1,0.929547,"f the statistics reported in Table 1 is taken from Diab et al. (2014) 2 The list is freely available at: http: //sourceforge.net/projects/ arabic-wordlist/ 145 Original Target English Characters à@ ñë ... à @ ñë ‘... Source Source Target H. ñKñJ Ë @ H. ñKñJ Ë@ ú ¯ ú ¯ éKYëA éKYëA ø YË @ ø YË@ which I have seen in Youtube is that è H X è @  # ø X È @ ... # à @# ð è# H. ð H ð ø È @# ø ¬#    ... # à @# ð è# H. ð H ð ø È @# ø ¬# è H X è @ # ø X È @ Table 2: Preparing the training and tuning and test corpus for alignment 3.2 Rule-based Corrector (Rules) verse (Sajjad et al., 2013a; Durrani et al., 2014a). The conversion of Arabic dialects to MSA at character-level can be seen as a spelling correction task where small character-level changes are made to convert a dialectal word into an MSA word. We also formulate our correction problem as a character-level machine translation problem, where the pre-processed incorrect Arabic text is considered as the source, and our target is the correct Arabic text provided by the Shared task organizers. The goal is to learn correspondences between errors and their corrections. All the train data is used to train our the phrase-based model. We treat sentenc"
W15-3217,W14-3605,0,0.0929078,"Missing"
W15-3217,E14-4029,1,0.927876,"f the statistics reported in Table 1 is taken from Diab et al. (2014) 2 The list is freely available at: http: //sourceforge.net/projects/ arabic-wordlist/ 145 Original Target English Characters à@ ñë ... à @ ñë ‘... Source Source Target H. ñKñJ Ë @ H. ñKñJ Ë@ ú ¯ ú ¯ éKYëA éKYëA ø YË @ ø YË@ which I have seen in Youtube is that è H X è @  # ø X È @ ... # à @# ð è# H. ð H ð ø È @# ø ¬#    ... # à @# ð è# H. ð H ð ø È @# ø ¬# è H X è @ # ø X È @ Table 2: Preparing the training and tuning and test corpus for alignment 3.2 Rule-based Corrector (Rules) verse (Sajjad et al., 2013a; Durrani et al., 2014a). The conversion of Arabic dialects to MSA at character-level can be seen as a spelling correction task where small character-level changes are made to convert a dialectal word into an MSA word. We also formulate our correction problem as a character-level machine translation problem, where the pre-processed incorrect Arabic text is considered as the source, and our target is the correct Arabic text provided by the Shared task organizers. The goal is to learn correspondences between errors and their corrections. All the train data is used to train our the phrase-based model. We treat sentenc"
W15-3217,J03-1002,0,0.00661281,"Missing"
W15-3217,J96-1003,1,0.68369,"and yeilds better correction quality with an F-score of 68.12 on L1test-2015 testset and 38.90 on the L2-test2015. This ranks us 2nd in the L2 subtask and 5th in the L1 subtask. 1 Introduction With the increased usage of computers in the processing of various languages comes the need for correcting errors introduced at different stages. Hence, the topic of text correction has seen a lot of interest in the past several years (Haddad and Yaseen, 2007; Rozovskaya et al., 2013). Numerous approaches have been explored to correct spelling errors in texts using NLP tools and resources (Kukich, 1992; Oflazer, 1996). The spelling correction for Arabic is an understudied problem in comparison to English, although small amount of research has been done previously (Shaalan et al., 2003; Hassan et al., 2008). The reason for this is the complexity of Arabic language and unavailability of language resources. For example, the Arabic spell checker in Microsoft Word gives incorrect suggests for even simple errors. First shared task on automatic Arabic text 2 Data Resources QALB: We trained and evaluated our system using the data provided for the shared task and the m2Scorer (Dahlmeier and Ng, 2012). These dataset"
W15-3217,pasha-etal-2014-madamira,0,0.131692,"Missing"
W15-3217,W13-3602,0,0.0694364,"We trained and tested our spelling corrector using the dataset provided by the shared task organizers. Our system outperforms the baseline system and yeilds better correction quality with an F-score of 68.12 on L1test-2015 testset and 38.90 on the L2-test2015. This ranks us 2nd in the L2 subtask and 5th in the L1 subtask. 1 Introduction With the increased usage of computers in the processing of various languages comes the need for correcting errors introduced at different stages. Hence, the topic of text correction has seen a lot of interest in the past several years (Haddad and Yaseen, 2007; Rozovskaya et al., 2013). Numerous approaches have been explored to correct spelling errors in texts using NLP tools and resources (Kukich, 1992; Oflazer, 1996). The spelling correction for Arabic is an understudied problem in comparison to English, although small amount of research has been done previously (Shaalan et al., 2003; Hassan et al., 2008). The reason for this is the complexity of Arabic language and unavailability of language resources. For example, the Arabic spell checker in Microsoft Word gives incorrect suggests for even simple errors. First shared task on automatic Arabic text 2 Data Resources QALB:"
W15-3217,W14-3622,0,0.351338,"Missing"
W15-3217,P13-2001,1,0.907081,"Missing"
W15-3217,2013.iwslt-evaluation.8,1,0.897486,"Missing"
W15-3217,zaghouani-etal-2014-large,1,\N,Missing
W15-3217,W11-2123,0,\N,Missing
W16-4828,bouamor-etal-2014-multidialectal,0,0.275321,"Missing"
W16-4828,D14-1154,1,0.882221,"Missing"
W16-4828,W16-4801,0,0.043439,"Missing"
W16-4828,W14-3601,1,0.917262,"Missing"
W16-4828,P13-2001,1,0.868736,"rams, bigrams with trigrams, all three n-grams, etc. This resulted in a very high-dimensional feature vector. 3.2.2 Character level features These features are more fine-grained than word level features, which would enable our models to learn morphological and utterance based features. Working with more fine-grained features was also shown to be useful in other natural language processing tasks such as machine translation (Sennrich et al., 2016). Character-based models have also been used in literature to convert Egyptian dialect to MSA in order to aid machine translation of Egyptian dialect (Sajjad et al., 2013; Durrani et al., 2014). Hence, this motivates the use of character level features for this task. Character N-grams: Similar to word level features, we experimented with character-level bigrams, trigrams, 4-grams and 5-grams. The motivation behind this was drawn from word examples from different dialects that only differ in a few characters. The average word length in the dataset for the closed task is around 4.5 characters. Thus, we decided not to try values of n that are higher than 5. Character N-gram combinations: Again, similar to word level features, we noticed that each of the n-gram fe"
W16-4828,P16-1162,0,0.0178969,"unigrams, bigrams and trigrams provides its own advantage over the dataset, we decided to experiment with different combinations of these features, such as unigrams with bigrams, bigrams with trigrams, all three n-grams, etc. This resulted in a very high-dimensional feature vector. 3.2.2 Character level features These features are more fine-grained than word level features, which would enable our models to learn morphological and utterance based features. Working with more fine-grained features was also shown to be useful in other natural language processing tasks such as machine translation (Sennrich et al., 2016). Character-based models have also been used in literature to convert Egyptian dialect to MSA in order to aid machine translation of Egyptian dialect (Sajjad et al., 2013; Durrani et al., 2014). Hence, this motivates the use of character level features for this task. Character N-grams: Similar to word level features, we experimented with character-level bigrams, trigrams, 4-grams and 5-grams. The motivation behind this was drawn from word examples from different dialects that only differ in a few characters. The average word length in the dataset for the closed task is around 4.5 characters. T"
W16-4828,P11-2007,0,0.0968841,"Missing"
W16-4828,J14-1006,0,0.183833,"Missing"
W19-5303,2012.eamt-1.60,0,0.0358,"indicate the data domain. For data augmentation, they back-translate from a target language to its noisy source. The intuition, also observed by Michel and Neubig (2018), is that the source sentences are noisier than their target translations. They include out-ofdomain clean data during this step and differentiate data types with a special symbol on the target side. In addition, they also run a model ensemble. Training Data In the constrained setting, participants were allowed to use the WMT15 training data3 for Eng↔Fra and any of the KFTT (Neubig, 2011), JESC (Pryzant et al.) and TED talks (Cettolo et al., 2012) corpora for Jpn↔Eng. Additionally, the use of the MTNT corpus (Michel and Neubig, 2018) was allowed in order to adapt models on limited in-domain data. 3.3 Evaluation protocol Test Data The test sets were collected following the same protocol as the MTNT dataset, i.e. collected from 3 http://www.statmt.org/wmt15/ translation-task.html 93 Figure 1: Annotation interface for human evaluations. 94 Eng-Fra Fra-Eng Eng-Jpn Jpn-Eng # samples 1,401 1,233 1,392 1,111 # source tokens # target tokens 20.0k 22.8k 19.8k 19.2k 20.0k 33.6k 18.7k 13.4k Table 1: Statistics of the test sets. model which was al"
W19-5303,P19-1425,0,0.113812,"inkov and Glass (2019) for a categorization of such work. In particular, some have focused on specific variations of naturally-occurring noise, such as grammatical errors produced by non-native speakers (Anastasopoulos et al., 2019) or errors extracted from Wikipedia edits (Belinkov and Bisk, 2018). It has also been shown that adding synthetic noise does not trivially increase robustness to natural noise (Belinkov and Bisk, 2018) and may require specific recipes (Karpukhin et al., 2019). Michel et al. (2019) recently emphasized the importance of meaning-preserving perturbations and along with Cheng et al. (2019) demonstrated the utility of adversarial training without significantly impairing performance on clean data and domain. Durrani et al. (2019) showed that character-based representations are more robust towards noise compared to such learned using BPE-based sub-word units in the task of machine translation. Jpn→Eng. We describe the dataset and the task setup in Section 3. The shared-task attracted a total of 23 submissions from 11 teams. The teams employed a variety of methods to improve robustness. A specific challenge was the small size of the in-domain noisy parallel dataset. We summarize th"
W19-5303,P17-4012,0,0.0548466,"data, improve existing semisupervised approach such as backtranslation. We provide both in-domain (MTNT) and outof-domain (News Commentary, News Crawl, etc) monolingual data. 3.2 Participants and System Descriptions We received 23 submissions from 11 teams. Except two submissions on the Eng-Fra language pair, all systems used the constrained setup. Below we briefly describe the systems from the 8 teams which submitted corresponding system description papers: Baidu & Oregon State University’s submission (Zheng et al., 2019): Their system is based on the Transformer implementation in OpenNMTpy (Klein et al., 2017). The main methods applied in their submission are: domain-sensitive data mixing and data augmentation with backtranslation. For data mixing, they used a special symbol on the source side to indicate the data domain. For data augmentation, they back-translate from a target language to its noisy source. The intuition, also observed by Michel and Neubig (2018), is that the source sentences are noisier than their target translations. They include out-ofdomain clean data during this step and differentiate data types with a special symbol on the target side. In addition, they also run a model ensem"
W19-5303,P18-1163,0,0.189473,"Missing"
W19-5303,W17-3204,1,0.848798,"re efforts from the community in building robust MT models. 2 Related Work The fragility of neural networks (Szegedy et al., 2013) has been shown to extend to neural machine translation models (Belinkov and Bisk, 2018; Heigold et al., 2017) and recent work focused on various aspects of the problem. From the identification of the causes of this brittleness, to the induction of (adversarial) inputs that trigger the unwanted behavior (attacks) and making such models robust against various types of noisy inputs (defenses); improving robustness has been receiving increasing attention in NMT. While Koehn and Knowles (2017) mentioned domain mismatch as a challenge for neural machine translation, Khayrallah and Koehn (2018) addressed noisy training data and focus on the types of noise occurring in web-crawled corpora. Michel and Neubig (2018) proposed a new dataset (MTNT) to test MT models for robustness to the types of noise encountered in the Internet and demonstrated that these challenges cannot be overcome by simple domain adaptation techniques alone. Belinkov and Bisk (2018) and Heigold et al. (2017) showed that NMT systems are very sensitive to slightly perturbed input forms, and hinted at the importance of"
W19-5303,W19-5362,0,0.149193,"this campaign. Unlike other participants, the winning team Naver Labs B´erard et al. (2019) and NTT (Murakami et al., 2019) applied data cleaning techniques in order to filter noisy parallel sentences. They filtered i) identical sentences on source and target side, ii) sentences that belonged to a language other than the source and target language, iii) sentences with length mismatch, and iv) also applied attention-based filtering. Data cleaning gave an improvement of more than 5 BLEU points with substantial reduction in the hallucination of the model for the winning team. NICT’s submission (Dabre and Sumita, 2019): The authors used Transformer models to train their systems and employed two strategies namely: i) mixed fine-tuning and ii) multilingual models for making the systems robust. The former helps as the in-domain data is available in a very small quantity. Using a mix of in-domain and outdomain data for fine-tuning helps overcome the problem of adjusting learning rate, applying better regularization and other complicated strategies. It is not clear how these two methods contributed towards making the models more robust. According to the authors, mixed fine-tuning and multilingual training (bidir"
W19-5303,D18-2012,0,0.020004,"mer-Big architecture, whereas improvements were substantially larger when the base models were RNN-Based MTNT baselines, about 8+ BLEU points. Participants emphasized the importance of their strong Transformer-Big base JHU’s submission (Post and Duh, 2019): This submission participated in the Fra→Eng and Jpn↔Eng tasks. The participants used data dual cross-entropy filtering for reducing the monolingual data, then back-translate these, and train their Transformer models (Vaswani et al., 2017). They compared Moses tokenization+Byte Pair Encoding (BPE) (Sennrich et al., 2016), and sentencepiece (Kudo and Richardson, 2018) (without any pre-processing) and found the two comparable, and that using larger sentence-piece models improved over smaller ones. For Jpn↔Eng (both di4 http://www.statmt.org/wmt19/biomedical-translationtask.html 95 sis, they found that their system performs poorly in translating emojis. The segmentation errors generated by KyTea resulted in further errors in the translation. rections) they first used both in-domain (MTNT) and out-of-domain data (other constrained), and then continued training (fine-tune) using MTNT only. They also reported many results from their hyper-parameter search (albe"
W19-5303,N19-1154,1,0.767588,"ise, such as grammatical errors produced by non-native speakers (Anastasopoulos et al., 2019) or errors extracted from Wikipedia edits (Belinkov and Bisk, 2018). It has also been shown that adding synthetic noise does not trivially increase robustness to natural noise (Belinkov and Bisk, 2018) and may require specific recipes (Karpukhin et al., 2019). Michel et al. (2019) recently emphasized the importance of meaning-preserving perturbations and along with Cheng et al. (2019) demonstrated the utility of adversarial training without significantly impairing performance on clean data and domain. Durrani et al. (2019) showed that character-based representations are more robust towards noise compared to such learned using BPE-based sub-word units in the task of machine translation. Jpn→Eng. We describe the dataset and the task setup in Section 3. The shared-task attracted a total of 23 submissions from 11 teams. The teams employed a variety of methods to improve robustness. A specific challenge was the small size of the in-domain noisy parallel dataset. We summarize the participating systems in Section 4 and the notable methods in Section 5. The contributions were evaluated both automatically and via a huma"
W19-5303,W18-6459,0,0.0413765,"ish (Eng) and French (Fra) and English and Japanese (Jpn), in four translation directions: Eng→Fra, Fra→Eng, Eng→Jpn, and www.reddit.com https://github.com/neulab/compare-mt 91 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 91–102 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics examples are generated with access to the model parameters (Ebrahimi et al., 2018; Cheng et al., 2018a,b, 2019) and ii) black-box attacks, where examples are generated without accessing model internals (Zhao et al., 2018; Lee et al., 2018; ?; Anastasopoulos et al., 2019; Vaibhav et al., 2019); see Belinkov and Glass (2019) for a categorization of such work. In particular, some have focused on specific variations of naturally-occurring noise, such as grammatical errors produced by non-native speakers (Anastasopoulos et al., 2019) or errors extracted from Wikipedia edits (Belinkov and Bisk, 2018). It has also been shown that adding synthetic noise does not trivially increase robustness to natural noise (Belinkov and Bisk, 2018) and may require specific recipes (Karpukhin et al., 2019). Michel et al. (2019) recently emphasized th"
W19-5303,N19-1314,1,0.8427,"ernals (Zhao et al., 2018; Lee et al., 2018; ?; Anastasopoulos et al., 2019; Vaibhav et al., 2019); see Belinkov and Glass (2019) for a categorization of such work. In particular, some have focused on specific variations of naturally-occurring noise, such as grammatical errors produced by non-native speakers (Anastasopoulos et al., 2019) or errors extracted from Wikipedia edits (Belinkov and Bisk, 2018). It has also been shown that adding synthetic noise does not trivially increase robustness to natural noise (Belinkov and Bisk, 2018) and may require specific recipes (Karpukhin et al., 2019). Michel et al. (2019) recently emphasized the importance of meaning-preserving perturbations and along with Cheng et al. (2019) demonstrated the utility of adversarial training without significantly impairing performance on clean data and domain. Durrani et al. (2019) showed that character-based representations are more robust towards noise compared to such learned using BPE-based sub-word units in the task of machine translation. Jpn→Eng. We describe the dataset and the task setup in Section 3. The shared-task attracted a total of 23 submissions from 11 teams. The teams employed a variety of methods to improve ro"
W19-5303,C18-1055,0,0.064971,"n this first iteration, the shared-task used the MTNT dataset (Michel and Neubig, 2018) that contains noisy social media texts and their translations between English (Eng) and French (Fra) and English and Japanese (Jpn), in four translation directions: Eng→Fra, Fra→Eng, Eng→Jpn, and www.reddit.com https://github.com/neulab/compare-mt 91 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 91–102 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics examples are generated with access to the model parameters (Ebrahimi et al., 2018; Cheng et al., 2018a,b, 2019) and ii) black-box attacks, where examples are generated without accessing model internals (Zhao et al., 2018; Lee et al., 2018; ?; Anastasopoulos et al., 2019; Vaibhav et al., 2019); see Belinkov and Glass (2019) for a categorization of such work. In particular, some have focused on specific variations of naturally-occurring noise, such as grammatical errors produced by non-native speakers (Anastasopoulos et al., 2019) or errors extracted from Wikipedia edits (Belinkov and Bisk, 2018). It has also been shown that adding synthetic noise does not trivially increase"
W19-5303,D18-1050,1,0.625161,"erstand the overall challenges in translating social media text and identify major themes of efforts which needs more research from the community. In recent years, Machine Translation (MT) systems have seen great progress, with neural models becoming the de-facto methods and even approaching human quality in news domain (Hassan et al., 2018). However, like other deep learning models, neural machine translation (NMT) models are found to be sensitive to synthetic and natural noise in input, distributional shift, and adversarial 1 2 In this first iteration, the shared-task used the MTNT dataset (Michel and Neubig, 2018) that contains noisy social media texts and their translations between English (Eng) and French (Fra) and English and Japanese (Jpn), in four translation directions: Eng→Fra, Fra→Eng, Eng→Jpn, and www.reddit.com https://github.com/neulab/compare-mt 91 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 91–102 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics examples are generated with access to the model parameters (Ebrahimi et al., 2018; Cheng et al., 2018a,b, 2019) and ii) black-box attacks, where ex"
W19-5303,W19-5363,0,0.0447228,"e of tied multitask learning, where the noisy source sentences are first decoded by a same-language denoising decoder, and both information is passed on to the translation decoder. This approach requires data triples of noisy source, clean source, translation, which they created by data augmentation over the provided data, using tag-informed translation systems trained on either noisy (MTNT) or clean (Europarl) data. As the participants point out though, their performance improvements seems to be attributed to data augmentation and not to the intermediate denoising decoder. FOKUS’ submission (Grozea, 2019): This team participated in three directions: Eng→Fra, Fra→Eng and Jpn→Eng. For the Eng→Fra and Fra→Eng language pairs, the submissions are unconstrained systems, where the model was trained on the medical domain corpus provided by the WMT biomedical shared task 4 . Despite the training data being out-of-domain, removing “lowquality” parallel data such as “Subtitles” as the author hypothesized helped to bring 2 to 4 BLEU points improvement over the baseline models. Their Jpn→Eng submission is a constrained system, using the same model architecture as the Eng→Fra language pair. To improve robus"
W19-5303,N19-4007,1,0.827549,"ype tags (real or backtranslated) for further categorization of the training data. Compared to fine-tuning, adding tags provides them additional flexibility, resulting in a generalized system, robust towards a variety of input data. Human Evaluation The results of human evaluation following the evaluation protocol described in Section 3.4 are outlined in Table 2. Automatic Evaluation The automatic evaluation (BLEU) results of the Shared Task are summarized in Table 3. 6.2 Qualitative Analysis In order to discover salient differences between the methods, we performed analysis using compare-mt (Neubig et al., 2019), and present a few of the salient findings below. Fine-tuning Along with the noisy in-domain MTNT data, general domain data typically made available for WMT campaign was also allowed for this task. Most participants (Murakami et al., 2019; Dabre and Sumita, 2019; Helcl et al., 2019) trained on general domain data and fine-tuned the models towards the task. Murakami et al. (2019) did not see a consistent improvement with finetuning. Due to the small size of the in-domain data, Dabre and Sumita (2019) fine-tuned on a mix of in-domain and a subset of the out-of-domain data. Stronger Submissions"
W19-5303,W19-5364,0,0.144816,"Missing"
W19-5303,P02-1040,0,0.110507,"he translators were presented the original source sentence, the reference and the system output side by side. The order between the reference and the system output was randomized by the user interface. The translators rated both the reference and the translation on a scale from 1 to 100. For both the original source sentence and the reference, the original text was presented except for Eng-Jpn where the Japanese reference tokenized with KyTea was presented in order to be consistent with the systems’ outputs. The user interface for annotation is illustrated in Figure 1. We also evaluated BLEU (Papineni et al., 2002) for each system using SacreBLEU (Post, 2018). For all language pairs except Eng-Jpn, we used the original reference and SacreBLEU with the default options. In the case of Eng-Jpn, we used the reference tokenized with KyTea and the option --tokenize none. Task Setup The task includes two tracks, constrained and unconstrained depending on whether the system is trained on a predefined training datasets or not. The two tracks are evaluated by the same automatic and human evaluation protocol, however, they are compared separately. For the constrained system track, the task specifies two types of t"
W19-5303,D19-5506,0,0.114843,"thout accessing model internals (Zhao et al., 2018; Lee et al., 2018; ?; Anastasopoulos et al., 2019; Vaibhav et al., 2019); see Belinkov and Glass (2019) for a categorization of such work. In particular, some have focused on specific variations of naturally-occurring noise, such as grammatical errors produced by non-native speakers (Anastasopoulos et al., 2019) or errors extracted from Wikipedia edits (Belinkov and Bisk, 2018). It has also been shown that adding synthetic noise does not trivially increase robustness to natural noise (Belinkov and Bisk, 2018) and may require specific recipes (Karpukhin et al., 2019). Michel et al. (2019) recently emphasized the importance of meaning-preserving perturbations and along with Cheng et al. (2019) demonstrated the utility of adversarial training without significantly impairing performance on clean data and domain. Durrani et al. (2019) showed that character-based representations are more robust towards noise compared to such learned using BPE-based sub-word units in the task of machine translation. Jpn→Eng. We describe the dataset and the task setup in Section 3. The shared-task attracted a total of 23 submissions from 11 teams. The teams employed a variety of"
W19-5303,W18-2709,1,0.860168,"networks (Szegedy et al., 2013) has been shown to extend to neural machine translation models (Belinkov and Bisk, 2018; Heigold et al., 2017) and recent work focused on various aspects of the problem. From the identification of the causes of this brittleness, to the induction of (adversarial) inputs that trigger the unwanted behavior (attacks) and making such models robust against various types of noisy inputs (defenses); improving robustness has been receiving increasing attention in NMT. While Koehn and Knowles (2017) mentioned domain mismatch as a challenge for neural machine translation, Khayrallah and Koehn (2018) addressed noisy training data and focus on the types of noise occurring in web-crawled corpora. Michel and Neubig (2018) proposed a new dataset (MTNT) to test MT models for robustness to the types of noise encountered in the Internet and demonstrated that these challenges cannot be overcome by simple domain adaptation techniques alone. Belinkov and Bisk (2018) and Heigold et al. (2017) showed that NMT systems are very sensitive to slightly perturbed input forms, and hinted at the importance of injecting noisy examples during training, also known as adversarial examples. Further research propo"
W19-5303,W18-6319,0,0.0373317,"ce, the reference and the system output side by side. The order between the reference and the system output was randomized by the user interface. The translators rated both the reference and the translation on a scale from 1 to 100. For both the original source sentence and the reference, the original text was presented except for Eng-Jpn where the Japanese reference tokenized with KyTea was presented in order to be consistent with the systems’ outputs. The user interface for annotation is illustrated in Figure 1. We also evaluated BLEU (Papineni et al., 2002) for each system using SacreBLEU (Post, 2018). For all language pairs except Eng-Jpn, we used the original reference and SacreBLEU with the default options. In the case of Eng-Jpn, we used the reference tokenized with KyTea and the option --tokenize none. Task Setup The task includes two tracks, constrained and unconstrained depending on whether the system is trained on a predefined training datasets or not. The two tracks are evaluated by the same automatic and human evaluation protocol, however, they are compared separately. For the constrained system track, the task specifies two types of training data in addition to MTNT train set: •"
W19-5303,P16-1162,0,0.0719206,"on top of the base models with the Transformer-Big architecture, whereas improvements were substantially larger when the base models were RNN-Based MTNT baselines, about 8+ BLEU points. Participants emphasized the importance of their strong Transformer-Big base JHU’s submission (Post and Duh, 2019): This submission participated in the Fra→Eng and Jpn↔Eng tasks. The participants used data dual cross-entropy filtering for reducing the monolingual data, then back-translate these, and train their Transformer models (Vaswani et al., 2017). They compared Moses tokenization+Byte Pair Encoding (BPE) (Sennrich et al., 2016), and sentencepiece (Kudo and Richardson, 2018) (without any pre-processing) and found the two comparable, and that using larger sentence-piece models improved over smaller ones. For Jpn↔Eng (both di4 http://www.statmt.org/wmt19/biomedical-translationtask.html 95 sis, they found that their system performs poorly in translating emojis. The segmentation errors generated by KyTea resulted in further errors in the translation. rections) they first used both in-domain (MTNT) and out-of-domain data (other constrained), and then continued training (fine-tune) using MTNT only. They also reported many"
W19-5303,N19-1190,1,0.747855,"e (Jpn), in four translation directions: Eng→Fra, Fra→Eng, Eng→Jpn, and www.reddit.com https://github.com/neulab/compare-mt 91 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 91–102 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics examples are generated with access to the model parameters (Ebrahimi et al., 2018; Cheng et al., 2018a,b, 2019) and ii) black-box attacks, where examples are generated without accessing model internals (Zhao et al., 2018; Lee et al., 2018; ?; Anastasopoulos et al., 2019; Vaibhav et al., 2019); see Belinkov and Glass (2019) for a categorization of such work. In particular, some have focused on specific variations of naturally-occurring noise, such as grammatical errors produced by non-native speakers (Anastasopoulos et al., 2019) or errors extracted from Wikipedia edits (Belinkov and Bisk, 2018). It has also been shown that adding synthetic noise does not trivially increase robustness to natural noise (Belinkov and Bisk, 2018) and may require specific recipes (Karpukhin et al., 2019). Michel et al. (2019) recently emphasized the importance of meaning-preserving perturbations and al"
W19-5303,D18-1316,0,0.0279519,"ations between English (Eng) and French (Fra) and English and Japanese (Jpn), in four translation directions: Eng→Fra, Fra→Eng, Eng→Jpn, and www.reddit.com https://github.com/neulab/compare-mt 91 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 91–102 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics examples are generated with access to the model parameters (Ebrahimi et al., 2018; Cheng et al., 2018a,b, 2019) and ii) black-box attacks, where examples are generated without accessing model internals (Zhao et al., 2018; Lee et al., 2018; ?; Anastasopoulos et al., 2019; Vaibhav et al., 2019); see Belinkov and Glass (2019) for a categorization of such work. In particular, some have focused on specific variations of naturally-occurring noise, such as grammatical errors produced by non-native speakers (Anastasopoulos et al., 2019) or errors extracted from Wikipedia edits (Belinkov and Bisk, 2018). It has also been shown that adding synthetic noise does not trivially increase robustness to natural noise (Belinkov and Bisk, 2018) and may require specific recipes (Karpukhin et al., 2019). Michel et al. (2019) rece"
W19-5303,W19-5368,1,0.878409,"not experimented. Finally, participants point out one peculiarity they’ve noticed in the train/validation partitioning of the original MTNT dataset; validation source sentences being started with the letter “Y” followed by alphabetically sorted sentences (test partition not effected). The team experimented with the Fra→Eng and Eng→Fra translation directions, obtaining 43.6 and 36.4 BLEU-cased, respectively (3rd place in both). Their ablations show significant benefit from domain-sensitive training (+3 BLEU), with additional improvements from back-translation and ensembling. CMU’s submission (Zhou et al., 2019): This submission only participated in the Fra→Eng direction. They proposed the use of tied multitask learning, where the noisy source sentences are first decoded by a same-language denoising decoder, and both information is passed on to the translation decoder. This approach requires data triples of noisy source, clean source, translation, which they created by data augmentation over the provided data, using tag-informed translation systems trained on either noisy (MTNT) or clean (Europarl) data. As the participants point out though, their performance improvements seems to be attributed to da"
W19-5303,Q19-1004,1,\N,Missing
W19-5303,N19-1311,1,\N,Missing
W19-5303,W19-5366,0,\N,Missing
