2020.acl-main.231,P08-1090,0,0.103392,"Missing"
2020.acl-main.231,W02-1002,0,\N,Missing
2020.acl-main.231,P08-1100,0,\N,Missing
2020.conll-1.24,2014.lilt-9.5,0,0.0492336,"a semantic function composition rather than different types of representations for words. Our work is different from these, since we start our learning procedure by taking the grammatical types of words into account and then compose these initially learnt representations with each other based on the structure of phrases they are part of, rather then by adding or learning different composition operators, or learning the entire phrase/sentence at once. On the other hand, formal distributional models, e.g. the categorial framework of Coecke et al. (2010, 2013), the linear regression approach of Baroni et al. (2014), and the Combinatory Categorial Grammar (CCG) tensor contraction model of Maillard et al. (2014), directly take the grammatical types of words into account, but fail to scale up to sentences of any length and complexity, and do not 314 perform as well as their neural embedding counterparts. To remedy these issues, our model makes use of a simple neural network to learn the typedriven word representations in such a way that their composition leads to improved results. 2 and the type-raising combinators are used in cases of coordination and gapping. Following the tensor semantics of CCG, develo"
2020.conll-1.24,D10-1115,0,0.0610635,"RT embeddings. However, it does not outperform BERT embeddings fine-tuned on NLI data. In the subset of SICK, our model only outperforms all previous type-driven models. Despite that, our model is motivated by linguistic theory, is simple and quick to train, and has the potential for improvement (which we expand on in the conclusion). Code and data to train representations and reproduce our work is available online.1 Background There is a plethora of methods for word embeddings, with few of them distinguishing the grammatical types of the words. For adjectives, we have the regression model of Baroni and Zamparelli (2010) that approximates the holistic adjective-noun vectors to learn adjective matrices; we also have the skipgram model of Maillard and Clark (2015) that learns a transformation between fixed vectors for nouns and adjective-noun combi1 github.com/gijswijnholds/ tensorskipgram-torch nations. The model of Grefenstette and Sadrzadeh (2011) takes the sum of the outer products of the vectors of subjects and objects, and the Kronecker product of the verb vector with itself, to learn verb matrices. Later work uses multi-step regression to learn a verb cube, i.e. a multidimensional array of depth 1, by it"
2020.conll-1.24,P12-1015,0,0.0345488,"a), and (5) the transitive sentence similarity dataset of Kartsaklis et al. (2013) (KS13b). (6,7) We additionally test on two recent datasets (Wijnholds and Sadrzadeh, 2019) (ELLDIS and ELLSIM), which extend the KS13a and KS13b datasets to sentences with verb phrase ellipsis in them. The datasets ML08 and ML10, respectively, contain pairs of subject-verb, and verb-object phrases. Next to the additive baseline, we apply the unary map representations of verbs to the subject (or 4 CA Evaluation and Datasets We considered five verb similarity datasets of varying size: pairs of words from the MEN (Bruni et al., 2012) and SimLex-999 (Hill et al., 2015) datasets that were labelled as verbs, obtaining 22 and 222 verb similarity pairs, respectively. Next to these partial datasets, we considered VerbSim (Yang and Powers, 2006), a dataset of 130 verb pairs, and the more recent SimVerb-3500 dataset (Gerz et al., 2016), containing 3500 verb pairs. 3.4 Formula CAS We evaluate our verb representations on four types of tasks: verb similarity, verb disambiguation, sentence similarity, including SVO sentences and SVO sentences with elliptical phrases, and a subset of the SICK sentence relatedness task. 3.3 Model We ar"
2020.conll-1.24,D18-2029,0,0.098413,"ishes between observed subject-verb-object triples and randomly generated ones. Our work is different from these, since we use a skipgram-style model rather than combining the subject and object vectors or the verb vectors, as done by Grefenstette and Sadrzadeh (2011), or performing regression, as done by Paperno et al. (2014) and Polajnar et al. (2014). Sentence embeddings are either learnt by mixing word embeddings e.g. the additive models of (Mitchell and Lapata, 2010; Mikolov et al., 2013), or as a whole, e.g. the supervised InferSent (Conneau et al., 2017) and Universal Sentence Encoder (Cer et al., 2018), and the unsupervised ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) models. None, however, explicitly take into account grammatical information. TreeRNNs (Socher et al., 2013), Tree-LSTMs (Tai et al., 2015), and Lifted Matrix Space model (Chung et al., 2018), do use the constituency tree of a sentence as a guide, but to learn a semantic function composition rather than different types of representations for words. Our work is different from these, since we start our learning procedure by taking the grammatical types of words into account and then compose these initially learnt rep"
2020.conll-1.24,D16-1205,0,0.0542574,"Missing"
2020.conll-1.24,K18-1049,0,0.0188585,"or performing regression, as done by Paperno et al. (2014) and Polajnar et al. (2014). Sentence embeddings are either learnt by mixing word embeddings e.g. the additive models of (Mitchell and Lapata, 2010; Mikolov et al., 2013), or as a whole, e.g. the supervised InferSent (Conneau et al., 2017) and Universal Sentence Encoder (Cer et al., 2018), and the unsupervised ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) models. None, however, explicitly take into account grammatical information. TreeRNNs (Socher et al., 2013), Tree-LSTMs (Tai et al., 2015), and Lifted Matrix Space model (Chung et al., 2018), do use the constituency tree of a sentence as a guide, but to learn a semantic function composition rather than different types of representations for words. Our work is different from these, since we start our learning procedure by taking the grammatical types of words into account and then compose these initially learnt representations with each other based on the structure of phrases they are part of, rather then by adding or learning different composition operators, or learning the entire phrase/sentence at once. On the other hand, formal distributional models, e.g. the categorial framew"
2020.conll-1.24,J07-4004,1,0.449893,"ntly, W(n) is an (n + 1)th-order tensor Wi1 ...in+1 in the space V1 ⊗ ... ⊗ Vn ⊗ Vn+1 . Given a functional word W of n arguments and representations d1 , ..., dn of its arguments, we denote by W(n) d1 ...dn the application of the representation of W to its arguments’ representations. The model that learns the maps has the following objective function: X log σ(W(n) d1 ...dn · c) c∈C c∈C + We generalise the skipgram model following the typing of Combinatorial Categorial Grammar (CCG, Steedman (2000)). CCG has a transparent interface between syntax and semantics and robust wide-coverage parsers (Clark and Curran, 2007; Hockenmaier and Steedman, 2007). Syntactic types of CCG are either atomic, e.g. nouns/noun phrases: NP and sentences: S , or functional. Functional types are either of the form Y /X or Y X ; they take an argument of type X and return an argument of type Y , where for  the argument occurs to the left and for / it occurs to the right. Examples of functional types are adjectives: NP /NP , intransitive verbs: S NP and transitive verbs: (S NP )/NP . Types are composed with each other through the combinatorial rules of CCG, which include forward and backward application and composition, type-r"
2020.conll-1.24,D17-1070,0,0.178563,"a verb matrix/cube by optimising a model that distinguishes between observed subject-verb-object triples and randomly generated ones. Our work is different from these, since we use a skipgram-style model rather than combining the subject and object vectors or the verb vectors, as done by Grefenstette and Sadrzadeh (2011), or performing regression, as done by Paperno et al. (2014) and Polajnar et al. (2014). Sentence embeddings are either learnt by mixing word embeddings e.g. the additive models of (Mitchell and Lapata, 2010; Mikolov et al., 2013), or as a whole, e.g. the supervised InferSent (Conneau et al., 2017) and Universal Sentence Encoder (Cer et al., 2018), and the unsupervised ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) models. None, however, explicitly take into account grammatical information. TreeRNNs (Socher et al., 2013), Tree-LSTMs (Tai et al., 2015), and Lifted Matrix Space model (Chung et al., 2018), do use the constituency tree of a sentence as a guide, but to learn a semantic function composition rather than different types of representations for words. Our work is different from these, since we start our learning procedure by taking the grammatical types of words into a"
2020.conll-1.24,N19-1423,0,0.177768,"s. Our work is different from these, since we use a skipgram-style model rather than combining the subject and object vectors or the verb vectors, as done by Grefenstette and Sadrzadeh (2011), or performing regression, as done by Paperno et al. (2014) and Polajnar et al. (2014). Sentence embeddings are either learnt by mixing word embeddings e.g. the additive models of (Mitchell and Lapata, 2010; Mikolov et al., 2013), or as a whole, e.g. the supervised InferSent (Conneau et al., 2017) and Universal Sentence Encoder (Cer et al., 2018), and the unsupervised ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) models. None, however, explicitly take into account grammatical information. TreeRNNs (Socher et al., 2013), Tree-LSTMs (Tai et al., 2015), and Lifted Matrix Space model (Chung et al., 2018), do use the constituency tree of a sentence as a guide, but to learn a semantic function composition rather than different types of representations for words. Our work is different from these, since we start our learning procedure by taking the grammatical types of words into account and then compose these initially learnt representations with each other based on the structure of phrases they are part of,"
2020.conll-1.24,D16-1235,0,0.0602759,"Missing"
2020.conll-1.24,W13-0112,0,0.0244741,"e matrices; we also have the skipgram model of Maillard and Clark (2015) that learns a transformation between fixed vectors for nouns and adjective-noun combi1 github.com/gijswijnholds/ tensorskipgram-torch nations. The model of Grefenstette and Sadrzadeh (2011) takes the sum of the outer products of the vectors of subjects and objects, and the Kronecker product of the verb vector with itself, to learn verb matrices. Later work uses multi-step regression to learn a verb cube, i.e. a multidimensional array of depth 1, by iteratively approximating a holistic subject-verb and verb-object vector (Grefenstette et al., 2013). The model of Paperno et al. (2014) overcomes the sparsity issues of this technique and approximates the cubes by two matrices. The plausibility model of Polajnar et al. (2014), learns a verb matrix/cube by optimising a model that distinguishes between observed subject-verb-object triples and randomly generated ones. Our work is different from these, since we use a skipgram-style model rather than combining the subject and object vectors or the verb vectors, as done by Grefenstette and Sadrzadeh (2011), or performing regression, as done by Paperno et al. (2014) and Polajnar et al. (2014). Sen"
2020.conll-1.24,D11-1129,1,0.838526,"Missing"
2020.conll-1.24,J15-4004,0,0.0277272,"similarity dataset of Kartsaklis et al. (2013) (KS13b). (6,7) We additionally test on two recent datasets (Wijnholds and Sadrzadeh, 2019) (ELLDIS and ELLSIM), which extend the KS13a and KS13b datasets to sentences with verb phrase ellipsis in them. The datasets ML08 and ML10, respectively, contain pairs of subject-verb, and verb-object phrases. Next to the additive baseline, we apply the unary map representations of verbs to the subject (or 4 CA Evaluation and Datasets We considered five verb similarity datasets of varying size: pairs of words from the MEN (Bruni et al., 2012) and SimLex-999 (Hill et al., 2015) datasets that were labelled as verbs, obtaining 22 and 222 verb similarity pairs, respectively. Next to these partial datasets, we considered VerbSim (Yang and Powers, 2006), a dataset of 130 verb pairs, and the more recent SimVerb-3500 dataset (Gerz et al., 2016), containing 3500 verb pairs. 3.4 Formula CAS We evaluate our verb representations on four types of tasks: verb similarity, verb disambiguation, sentence similarity, including SVO sentences and SVO sentences with elliptical phrases, and a subset of the SICK sentence relatedness task. 3.3 Model We argue that this is because contexts i"
2020.conll-1.24,J07-3004,0,0.0234275,"th-order tensor Wi1 ...in+1 in the space V1 ⊗ ... ⊗ Vn ⊗ Vn+1 . Given a functional word W of n arguments and representations d1 , ..., dn of its arguments, we denote by W(n) d1 ...dn the application of the representation of W to its arguments’ representations. The model that learns the maps has the following objective function: X log σ(W(n) d1 ...dn · c) c∈C c∈C + We generalise the skipgram model following the typing of Combinatorial Categorial Grammar (CCG, Steedman (2000)). CCG has a transparent interface between syntax and semantics and robust wide-coverage parsers (Clark and Curran, 2007; Hockenmaier and Steedman, 2007). Syntactic types of CCG are either atomic, e.g. nouns/noun phrases: NP and sentences: S , or functional. Functional types are either of the form Y /X or Y X ; they take an argument of type X and return an argument of type Y , where for  the argument occurs to the left and for / it occurs to the right. Examples of functional types are adjectives: NP /NP , intransitive verbs: S NP and transitive verbs: (S NP )/NP . Types are composed with each other through the combinatorial rules of CCG, which include forward and backward application and composition, type-raising, and backward-cross and fo"
2020.conll-1.24,D13-1166,1,0.844865,"Missing"
2020.conll-1.24,W13-3513,1,0.808844,"unct and finally combine the representations by addition; formally Verb Disambiguation and Sentence Similarity We considered seven tasks. (1,2) The two datasets introduced by Mitchell and Lapata (2008, 2010), dubbed ML08 and ML10. These datasets contain pairs of intransitive sentences; the 2008 dataset aims to disambiguate the verb of each sentence, the 2010 dataset is for computing sentence similarity. (3,4) The transitive verb disambiguation datasets of Grefenstette and Sadrzadeh (2011) (GS11) and Kartsaklis and Sadrzadeh (2013) (KS13a), and (5) the transitive sentence similarity dataset of Kartsaklis et al. (2013) (KS13b). (6,7) We additionally test on two recent datasets (Wijnholds and Sadrzadeh, 2019) (ELLDIS and ELLSIM), which extend the KS13a and KS13b datasets to sentences with verb phrase ellipsis in them. The datasets ML08 and ML10, respectively, contain pairs of subject-verb, and verb-object phrases. Next to the additive baseline, we apply the unary map representations of verbs to the subject (or 4 CA Evaluation and Datasets We considered five verb similarity datasets of varying size: pairs of words from the MEN (Bruni et al., 2012) and SimLex-999 (Hill et al., 2015) datasets that were labelled"
2020.conll-1.24,2020.tacl-1.50,0,0.0249999,"ion and sentence similarity tasks. The unary map approximations significantly outperformed previous type-driven verb representations. They also outperformed sentence encoders and pre-trained BERT embeddings. When moving to datasets of longer sentences, e.g. sentences with elliptical phrases and the SICK relatedness, some sentence encoders and fine-tuned BERT representations were superior. Our multilinear skipgram model paves the way for a new generation of type-driven representations, in line with recent research highlighting benefits of syntactic biases injected into representation learning (Kuncoro et al., 2020). Furthermore, our model is fast to train, guided by a linguistic calculus (CCG), and produces syntax-aware sentence embeddings. Performance could potentially be improved by adding non-linearities to the model, as in Socher et al. (2013) and by modelling complex syntactic phenomena such as auxiliaries and negation. USE BERTp BERTf 0.31 0.30 0.37 0.56 0.34 0.27 0.36 0.67 0.52 0.65 0.76 0.80 0.68 0.67 0.71 0.58 0.44 0.70 0.74 0.76 0.70 0.65 0.79 0.76 Table 9: Spearman ρ scores on the ELLDIS (top), ELLSIM (middle), and SICK relatedness (bottom) tasks. was 0.53, and provide equal results to the st"
2020.conll-1.24,P14-2050,0,0.0298455,"verb, given The map V (1) a fixed object, and is learnt as follows: X e s|o o · s) + log σ(V (1) s∈S X e s|o o · s) (4) log σ(−V (1) s∈S Here, S and O are the sets of observed subjects and objects of V , and S and O are the sets of V ’s unobserved subjects and objects. We push the approximation one level further to also produce three 0-ary maps, i.e. vectors, for eo , V es , V e o,s ; the verb. We denote these by V (0) (0) (0) they respectively represent a verb vector by only considering its objects, subjects, or both as context. These vectors are similar to the dependency based embeddings of Levy and Goldberg (2014). We summarise all trained models by the arity of their maps and the choice of their contexts in Table 1. As baselines, we additionally train unary maps sent|s sent|o V(1) and V(1) , which predict a full sentence context given the subject or object of the verb, and vskip for the original skipgram vector of the verb. 2.2 Fusion We consider two ways of combining our unary map skipgram verb representations into a single representation: the middle and late fusion methods of Bruni et al. (2014). Middle fusion takes a weighted average of the two verb representations, using the result to compute simi"
2020.conll-1.24,Q15-1016,0,0.0440252,"lard et al. (2014), in our model, we represent a word W with a functional type of n arguments by a n-ary map W from the argument spaces to the result space: Multilinear Skipgram Embeddings The skipgram model with negative sampling generates word embeddings by optimising a logistic regression objective in which target vectors have high inner product with context vectors for positive contexts, and low inner product with negative ones. Given a target word n and a set of positive contexts C, a set of negative contexts C is sampled from a unigram distribution raised to some power (here: 3/4, after Levy et al. (2015)). Initially, both target vectors n and context vectors c are randomly intialised, and during training the model updates both target and context vectors to maximise the following objective function: X X log σ(n · c) + log σ(−n · c) (1) c∈C W(n) : V1 × ... × Vn → Vn+1 where Vi ’s are (finite dimensional) vector spaces over the field of reals and the subscript n denotes the arity of the map W. Equivalently, W(n) is an (n + 1)th-order tensor Wi1 ...in+1 in the space V1 ⊗ ... ⊗ Vn ⊗ Vn+1 . Given a functional word W of n arguments and representations d1 , ..., dn of its arguments, we denote by W(n)"
2020.conll-1.24,K15-1035,1,0.894384,"certain number of the arguments to predict the contexts of the remaining arguments. A transitive verb is now represented by two unary maps of one argument each; one of them transforms the object representation to predict its subject contexts, and the other transforms its subject representations to predict its object contexts. These lower order approximations are combined with each other to produce one single representation for the word with functional type. Our generalised skipgram algorithm is modular, i.e. the skipgram model of Mikolov et al. (2013) and its extension to adjective matrices (Maillard and Clark, 2015) are special cases of it. We instantiate our model on binary and unary maps for transitive verbs. After learning these representations, we evaluate them on verb similarity, compositional sentence similarity and disambiguation tasks, and a subset of the SICK relatedness dataset (Marelli et al., 2014). In the verb and sentence similarity and verb disambiguation datasets, our model outperforms all previous type-driven models, and in most cases it also outperforms InferSent and Universal Sentence encoders, as well as pre-trained ELMo and BERT embeddings. However, it does not outperform BERT embedd"
2020.conll-1.24,W14-1406,1,0.779707,"work is different from these, since we start our learning procedure by taking the grammatical types of words into account and then compose these initially learnt representations with each other based on the structure of phrases they are part of, rather then by adding or learning different composition operators, or learning the entire phrase/sentence at once. On the other hand, formal distributional models, e.g. the categorial framework of Coecke et al. (2010, 2013), the linear regression approach of Baroni et al. (2014), and the Combinatory Categorial Grammar (CCG) tensor contraction model of Maillard et al. (2014), directly take the grammatical types of words into account, but fail to scale up to sentences of any length and complexity, and do not 314 perform as well as their neural embedding counterparts. To remedy these issues, our model makes use of a simple neural network to learn the typedriven word representations in such a way that their composition leads to improved results. 2 and the type-raising combinators are used in cases of coordination and gapping. Following the tensor semantics of CCG, developed in Maillard et al. (2014), in our model, we represent a word W with a functional type of n ar"
2020.conll-1.24,marelli-etal-2014-sick,0,0.117428,"ict its object contexts. These lower order approximations are combined with each other to produce one single representation for the word with functional type. Our generalised skipgram algorithm is modular, i.e. the skipgram model of Mikolov et al. (2013) and its extension to adjective matrices (Maillard and Clark, 2015) are special cases of it. We instantiate our model on binary and unary maps for transitive verbs. After learning these representations, we evaluate them on verb similarity, compositional sentence similarity and disambiguation tasks, and a subset of the SICK relatedness dataset (Marelli et al., 2014). In the verb and sentence similarity and verb disambiguation datasets, our model outperforms all previous type-driven models, and in most cases it also outperforms InferSent and Universal Sentence encoders, as well as pre-trained ELMo and BERT embeddings. However, it does not outperform BERT embeddings fine-tuned on NLI data. In the subset of SICK, our model only outperforms all previous type-driven models. Despite that, our model is motivated by linguistic theory, is simple and quick to train, and has the potential for improvement (which we expand on in the conclusion). Code and data to trai"
2020.conll-1.24,D14-1079,1,0.814552,"correlation on verb similarity datasets. The subscript v indicates that we are looking at the partial verb-only dataset. For SimVerb we distinguish between the development and test set. State of the art scores are taken from (Chersoni et al. (2016), VS) and (Gerz et al. (2016), SLv , SVd , SVt ). For MEN, we did not find any results on the verb subset. 3.6 0.07 0.25 0.11 0.06 Table 6: Spearman ρ correlation of verbs of SVO sentence level tasks. Each score is a maximum score out of possible clusters and fusion weights. State of the art scores are taken from (Mitchell and Lapata (2008),ML08), (Milajevs et al. (2014),GS11,KS13b) and (Kartsaklis and Sadrzadeh (2013),ML10,KS13a). as, Universal Sentence Encoder (Cer et al., 2018). For these latter, we take off-the-shelf encoders to map the sentence pairs in our evaluation datasets to a pair of embeddings, and compute the cosine similarity between these. We moreover compare to state-of-the-art contextualised encoders ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). For ELMo, we use a pre-trained model and apply mean pooling6 . For BERT, we take the implementation of Reimers and Gurevych (2019)7 , as it implements both the original pre-trained BERT m"
2020.conll-1.24,P08-1028,0,0.393341,"ifferent models: CA for Copy Argument, CAS for Copy Argument Sum and CATA for Categorical Argument; see Table 4. The ELLDIS and ELLSIM datasets of Wijnholds and Sadrzadeh (2019) contain sentences of the form subj verb obj and subj∗ does too. We first resolve the ellipsis by replacing the marker does too with its antecedent verb object, then apply a transitive model to the resulting subj verb obj and subj∗ verb object conjunct and finally combine the representations by addition; formally Verb Disambiguation and Sentence Similarity We considered seven tasks. (1,2) The two datasets introduced by Mitchell and Lapata (2008, 2010), dubbed ML08 and ML10. These datasets contain pairs of intransitive sentences; the 2008 dataset aims to disambiguate the verb of each sentence, the 2010 dataset is for computing sentence similarity. (3,4) The transitive verb disambiguation datasets of Grefenstette and Sadrzadeh (2011) (GS11) and Kartsaklis and Sadrzadeh (2013) (KS13a), and (5) the transitive sentence similarity dataset of Kartsaklis et al. (2013) (KS13b). (6,7) We additionally test on two recent datasets (Wijnholds and Sadrzadeh, 2019) (ELLDIS and ELLSIM), which extend the KS13a and KS13b datasets to sentences with ver"
2020.conll-1.24,P14-1009,0,0.0186279,"el of Maillard and Clark (2015) that learns a transformation between fixed vectors for nouns and adjective-noun combi1 github.com/gijswijnholds/ tensorskipgram-torch nations. The model of Grefenstette and Sadrzadeh (2011) takes the sum of the outer products of the vectors of subjects and objects, and the Kronecker product of the verb vector with itself, to learn verb matrices. Later work uses multi-step regression to learn a verb cube, i.e. a multidimensional array of depth 1, by iteratively approximating a holistic subject-verb and verb-object vector (Grefenstette et al., 2013). The model of Paperno et al. (2014) overcomes the sparsity issues of this technique and approximates the cubes by two matrices. The plausibility model of Polajnar et al. (2014), learns a verb matrix/cube by optimising a model that distinguishes between observed subject-verb-object triples and randomly generated ones. Our work is different from these, since we use a skipgram-style model rather than combining the subject and object vectors or the verb vectors, as done by Grefenstette and Sadrzadeh (2011), or performing regression, as done by Paperno et al. (2014) and Polajnar et al. (2014). Sentence embeddings are either learnt b"
2020.conll-1.24,N18-1202,0,0.336599,"ples and randomly generated ones. Our work is different from these, since we use a skipgram-style model rather than combining the subject and object vectors or the verb vectors, as done by Grefenstette and Sadrzadeh (2011), or performing regression, as done by Paperno et al. (2014) and Polajnar et al. (2014). Sentence embeddings are either learnt by mixing word embeddings e.g. the additive models of (Mitchell and Lapata, 2010; Mikolov et al., 2013), or as a whole, e.g. the supervised InferSent (Conneau et al., 2017) and Universal Sentence Encoder (Cer et al., 2018), and the unsupervised ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) models. None, however, explicitly take into account grammatical information. TreeRNNs (Socher et al., 2013), Tree-LSTMs (Tai et al., 2015), and Lifted Matrix Space model (Chung et al., 2018), do use the constituency tree of a sentence as a guide, but to learn a semantic function composition rather than different types of representations for words. Our work is different from these, since we start our learning procedure by taking the grammatical types of words into account and then compose these initially learnt representations with each other based on the structu"
2020.conll-1.24,D19-1410,0,0.0159649,"art scores are taken from (Mitchell and Lapata (2008),ML08), (Milajevs et al. (2014),GS11,KS13b) and (Kartsaklis and Sadrzadeh (2013),ML10,KS13a). as, Universal Sentence Encoder (Cer et al., 2018). For these latter, we take off-the-shelf encoders to map the sentence pairs in our evaluation datasets to a pair of embeddings, and compute the cosine similarity between these. We moreover compare to state-of-the-art contextualised encoders ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). For ELMo, we use a pre-trained model and apply mean pooling6 . For BERT, we take the implementation of Reimers and Gurevych (2019)7 , as it implements both the original pre-trained BERT models and fine-tuned sentence embedding models. To this, we apply max, mean, and CLS token pooling, and report the best scores out of all models and pooling types, for the pre-trained models and the fine-tuned models. 4 Results 4.1 Verb Level Tasks The correlation results on verb similarity tasks are displayed in Table 5. Here, for the case of verb vectors, the general skipgram model is outperformed by the vectors trained using our partial model on the verb arguments as context, and in fact these show the highest performance on the VerbS"
2020.conll-1.24,D13-1170,0,0.0515904,"nd object vectors or the verb vectors, as done by Grefenstette and Sadrzadeh (2011), or performing regression, as done by Paperno et al. (2014) and Polajnar et al. (2014). Sentence embeddings are either learnt by mixing word embeddings e.g. the additive models of (Mitchell and Lapata, 2010; Mikolov et al., 2013), or as a whole, e.g. the supervised InferSent (Conneau et al., 2017) and Universal Sentence Encoder (Cer et al., 2018), and the unsupervised ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) models. None, however, explicitly take into account grammatical information. TreeRNNs (Socher et al., 2013), Tree-LSTMs (Tai et al., 2015), and Lifted Matrix Space model (Chung et al., 2018), do use the constituency tree of a sentence as a guide, but to learn a semantic function composition rather than different types of representations for words. Our work is different from these, since we start our learning procedure by taking the grammatical types of words into account and then compose these initially learnt representations with each other based on the structure of phrases they are part of, rather then by adding or learning different composition operators, or learning the entire phrase/sentence a"
2020.conll-1.24,P15-1150,0,0.0967435,"Missing"
2020.conll-1.24,N19-1023,1,0.87492,"phrases, V For the separate subject-verb and verb-object maps, we apply middle and late fusion. To model a transitive sentence of the form subj verb obj, we compare verb-only and additive baselines with n-ary map models as described in Table 3. In the Two model e o to the subject vecin this table, we first apply V (1) e s to the tor, then mix it with the application of V (1) object vector. We then mix in the object/subject vectors and obtain three different models: CA for Copy Argument, CAS for Copy Argument Sum and CATA for Categorical Argument; see Table 4. The ELLDIS and ELLSIM datasets of Wijnholds and Sadrzadeh (2019) contain sentences of the form subj verb obj and subj∗ does too. We first resolve the ellipsis by replacing the marker does too with its antecedent verb object, then apply a transitive model to the resulting subj verb obj and subj∗ verb object conjunct and finally combine the representations by addition; formally Verb Disambiguation and Sentence Similarity We considered seven tasks. (1,2) The two datasets introduced by Mitchell and Lapata (2008, 2010), dubbed ML08 and ML10. These datasets contain pairs of intransitive sentences; the 2008 dataset aims to disambiguate the verb of each sentence,"
C00-1029,C94-2195,0,0.0627943,"Missing"
C00-1029,W99-0631,1,0.834754,"chnique tends to dissipate as counts are passed up the hierarchy. 4 Table 1: Maximum Likelihood Estimates { freq(c; v; r) is the number of (n; v; r) triples in the data in which n is being used to denote c. p^(cjr) = freq(c;r ) freq(r ) =P p^(v jr) = freq(v;r ) freq(r ) =P p^(v jc0; r) = freq(c0 ;v;r ) freq(c0 ;r ) P P 0 v 0 2V freq(c;v ;r ) 0 0 v 0 2V c0 2C freq(c ;v ;r ) P P P P P = 0 c0 2C freq(c ;v;r ) 0 0 v 0 2V c0 2C freq(c ;v ;r ) c00 2c0 v 0 2V freq(c00 ;v;r ) c00 2c0 freq(c00 ;v 0 ;r ) The method used for comparing the p(v jc00; r) for c00 in some set c0, is based on the technique in Clark and Weir (1999) used for nding homogeneous sets of concepts in the WordNet noun hierarchy. Rather than directly compare estimates of p(v jc00 ; r), which are likely to be unreliable, we consider the children of c0 , and use estimates based on counts which have accumulated at the children. If c0 has children c01; c02; : : : ; c0 , we compare p(v jc0 ; r) for each i. This is an approximation, but if the p(v jc0 ; r) are similar, then we assume that the p(v jc00 ; r) for c00 in c0 are similar too. To determine whether the children of some hypernym c0 have similar p(v jc0 ), where c0 is the ith child, we apply a"
C00-1029,W95-0103,0,0.0342469,"Missing"
C00-1029,P96-1025,0,0.0322117,"Missing"
C00-1029,P97-1003,0,0.109999,"Missing"
C00-1029,J93-1003,0,0.0179403,"equencies for c0 equal to hnutrimenti, in the object position of eat. The gures in brackets are the expected values, based on the marginal totals in the table. The null hypothesis of the test is that p(v jc0 ; r) is the same for each i. For Table 2 the null hypothesis is that for every child, c0 , of hnutrimenti, the probability p(eat jc0 ; obj) is the same. The log-likelihood 2 statistic corresponding to Table 2 is 4:8. The log-likelihood 2 statistic is used rather than the Pearson's 2 statistic because it is thought to be more appropriate when the counts in the contingency table are low (Dunning, 1993). This tends to occur when the test is being applied to a set of concepts near the foot of the hierarchy.5 We compared n i i i i i i i Fisher's exact test could be used for tables with low counts, but we do not do so because tables dominated by low counts are likely to have a high percentage of noise, due to the way counts for a noun are split among 5 Table 2: Contingency table for children of hnutrimenti c i ^ c ; eat ; obj) freq( ^ c ; obj), freq( ^ c ; eat ; obj) freq( i i i hmilki hmeali 0.0 8.5 hcoursei 1.3 hdishi 5.3 hdelicacyi 0.3 15.4 (0.6) (5.6) (1.7) (5.7) (1.8) 9.0 78.0 24.7 82.3 27"
C00-1029,J93-1005,0,0.166435,"cn(n2 ) c The sense of n2 is chosen which maximises the relevant probability in each potential attachment case. If p(c ; prjv ) is greater than p(c 1 ; prjn1), the attachment is made to v , otherwise to n1 . If n2 is not in WordNet we compare p(prjv ) and p(prjn1). Probabilities of the form p(c; prjv ) and p(c; prjn1) are used rather than p(cjv; pr) and p(cjn1; pr), because the association between the preposition and v and n1 contains useful information. In fact, for a lot of cases this information alone can be used to decide on the correct attachment site. The original corpus-based method of Hindle and Rooth (1993) used exactly this information. Thus the method described here can be thought of as Hindle and Rooth's method with additional classbased information about n2 . In order to estimate p(c ; prjv ) (and p(c 1 ; prjn1)) we apply the same procedure as described in Section 3, rst rewriting the probability using Bayes' rule: p(c ; pr) p(c ; prjv ) = p(v jc ; pr) p(v ) = p(v jc ; pr) p(prjc )p(c ) p(v ) v n v n v v v v v v The probabilities p(c ) and p(v ) can be estimated using maximum likelihood estimates, and p(v jc ; pr) and p(prjc ) can be estimated using maximum likelihood estimates of p(v jtop(c"
C00-1029,J98-2002,0,0.181475,"ch would be too high is hentityi, fact that dog, rather than prize, is often as not all entities are semantically similar with the subject of run, can be used to decide respect to the object position of eat. The problem of choosing an appropriate level on the attachment site of the relative clause: Fred awarded a prize for the dog that ran the fastest in the hierarchy at which to represent a parWe describe a proposal for acquiring such ticular noun sense (given a predicate and arguknowledge, and as in other recent work in this ment position) has been investigated by Resnik area (Resnik, 1993; Li and Abe, 1998), a prob- (1993), Li and Abe (1998) and Ribas (1995). abilistic approach is taken. Using probabilities The learning mechanism presented here is a accords with the intuition that there are no ab- novel approach based on nding semantically solute constraints on the arguments of predi- similar sets of concepts in a hierarchy. We cates, but rather that constraints are satis ed demonstrate the e ectiveness of our approach to a certain degree (Resnik, 1993). Unfortu- using a PP-attachment experiment. nately, de ning probabilities in terms of words leads to a model with a vast number of param- 2 The"
C00-1029,H94-1048,0,0.0209356,"'s method with additional classbased information about n2 . In order to estimate p(c ; prjv ) (and p(c 1 ; prjn1)) we apply the same procedure as described in Section 3, rst rewriting the probability using Bayes' rule: p(c ; pr) p(c ; prjv ) = p(v jc ; pr) p(v ) = p(v jc ; pr) p(prjc )p(c ) p(v ) v n v n v v v v v v The probabilities p(c ) and p(v ) can be estimated using maximum likelihood estimates, and p(v jc ; pr) and p(prjc ) can be estimated using maximum likelihood estimates of p(v jtop(c ; v; pr); pr) and p(prjtop(c ; pr)) respectively.6 We used the training and test data described in Ratnaparkhi et al. (1994), which was taken from the Penn Treebank and has now become the standard data set for this task. The data set consists of tuples of the form (v , n1 , pr, n2 ), together with the attachment site for each tuple. There is also a development set to prevent implicit training on the test set during development. We extracted (v , pr, n2 ) and (n1 , pr, n2 ) v v v v v 6 In Section 4 we only gave the procedure for determining top(cv ; v; pr), but top(cv ; pr) can be determined in an analogous fashion. triples from the training set, and in order to increase the number of training triples, we also extra"
C00-1029,P98-2177,0,0.0346051,"Missing"
C00-1029,E95-1016,0,0.207944,"han prize, is often as not all entities are semantically similar with the subject of run, can be used to decide respect to the object position of eat. The problem of choosing an appropriate level on the attachment site of the relative clause: Fred awarded a prize for the dog that ran the fastest in the hierarchy at which to represent a parWe describe a proposal for acquiring such ticular noun sense (given a predicate and arguknowledge, and as in other recent work in this ment position) has been investigated by Resnik area (Resnik, 1993; Li and Abe, 1998), a prob- (1993), Li and Abe (1998) and Ribas (1995). abilistic approach is taken. Using probabilities The learning mechanism presented here is a accords with the intuition that there are no ab- novel approach based on nding semantically solute constraints on the arguments of predi- similar sets of concepts in a hierarchy. We cates, but rather that constraints are satis ed demonstrate the e ectiveness of our approach to a certain degree (Resnik, 1993). Unfortu- using a PP-attachment experiment. nately, de ning probabilities in terms of words leads to a model with a vast number of param- 2 The Input Data and Semantic eters, resulting in a sparse"
C00-1029,W97-0109,0,0.0393604,"Missing"
C00-1029,C92-2070,0,0.0658946,"ins how we determine similarity classes. The maximum likelihood estimates for the relevant probabilities are given in Table 1.4 4 Finding Similarity-classes First we explain how we determine if a set of concepts has similar p(v jc00 ; r) for each concept c00 in the set. Then we explain how we determine top(c; v; r). Since we are assuming the data is not sense disambiguated, freq(c; v; r) cannot be obtained by simply counting senses. The standard approach, which is adopted here, is to estimate freq(c;v; r) by distributing the count for each noun n in syn(c) evenly among all senses of the noun. Yarowsky (1992) and Resnik (1993) explain how the noise introduced by this technique tends to dissipate as counts are passed up the hierarchy. 4 Table 1: Maximum Likelihood Estimates { freq(c; v; r) is the number of (n; v; r) triples in the data in which n is being used to denote c. p^(cjr) = freq(c;r ) freq(r ) =P p^(v jr) = freq(v;r ) freq(r ) =P p^(v jc0; r) = freq(c0 ;v;r ) freq(c0 ;r ) P P 0 v 0 2V freq(c;v ;r ) 0 0 v 0 2V c0 2C freq(c ;v ;r ) P P P P P = 0 c0 2C freq(c ;v;r ) 0 0 v 0 2V c0 2C freq(c ;v ;r ) c00 2c0 v 0 2V freq(c00 ;v;r ) c00 2c0 freq(c00 ;v 0 ;r ) The method used for comparing the p(v"
C00-1029,P97-1056,0,0.0349289,"Missing"
C00-1029,C98-2172,0,\N,Missing
C04-1041,J99-2004,0,0.827347,"ation space using constraints on category combination. The result is an accurate wide-coverage CCG parser which is an order of magnitude faster than comparable systems for other linguistically motivated formalisms. 1 Introduction Lexicalised grammar formalisms such as Lexicalized Tree Adjoining Grammar (LTAG) and Combinatory Categorial Grammar (CCG) assign one or more syntactic structures to each word in a sentence which are then manipulated by the parser. Supertagging was introduced for LTAG as a way of increasing parsing efficiency by reducing the number of structures assigned to each word (Bangalore and Joshi, 1999). Supertagging has more recently been applied to CCG (Clark, 2002; Curran and Clark, 2003). Supertagging accuracy is relatively high for manually constructed LTAGs (Bangalore and Joshi, 1999). However, for LTAGs extracted automatically from the Penn Treebank, performance is much lower (Chen et al., 1999; Chen et al., 2002). In fact, performance for such grammars is below that needed for successful integration into a full parser (Sarkar et al., 2000). In this paper we demonstrate that CCG supertagging accuracy is not only sufficient for accurate and robust parsing using an automatically extract"
C04-1041,2000.iwpt-1.9,0,0.114603,"Missing"
C04-1041,E99-1025,0,0.0287391,"d Combinatory Categorial Grammar (CCG) assign one or more syntactic structures to each word in a sentence which are then manipulated by the parser. Supertagging was introduced for LTAG as a way of increasing parsing efficiency by reducing the number of structures assigned to each word (Bangalore and Joshi, 1999). Supertagging has more recently been applied to CCG (Clark, 2002; Curran and Clark, 2003). Supertagging accuracy is relatively high for manually constructed LTAGs (Bangalore and Joshi, 1999). However, for LTAGs extracted automatically from the Penn Treebank, performance is much lower (Chen et al., 1999; Chen et al., 2002). In fact, performance for such grammars is below that needed for successful integration into a full parser (Sarkar et al., 2000). In this paper we demonstrate that CCG supertagging accuracy is not only sufficient for accurate and robust parsing using an automatically extracted grammar, but also offers several practical advantages. James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au Our wide-coverage CCG parser uses a log-linear model to select an analysis. The model paramaters are estimated using a discriminative"
C04-1041,W02-2236,0,0.0512536,"orial Grammar (CCG) assign one or more syntactic structures to each word in a sentence which are then manipulated by the parser. Supertagging was introduced for LTAG as a way of increasing parsing efficiency by reducing the number of structures assigned to each word (Bangalore and Joshi, 1999). Supertagging has more recently been applied to CCG (Clark, 2002; Curran and Clark, 2003). Supertagging accuracy is relatively high for manually constructed LTAGs (Bangalore and Joshi, 1999). However, for LTAGs extracted automatically from the Penn Treebank, performance is much lower (Chen et al., 1999; Chen et al., 2002). In fact, performance for such grammars is below that needed for successful integration into a full parser (Sarkar et al., 2000). In this paper we demonstrate that CCG supertagging accuracy is not only sufficient for accurate and robust parsing using an automatically extracted grammar, but also offers several practical advantages. James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au Our wide-coverage CCG parser uses a log-linear model to select an analysis. The model paramaters are estimated using a discriminative method, that is, on"
C04-1041,W03-1013,1,0.301602,"using L - BFGS (Nocedal and Wright, 1999), an iterative algorithm from the numerical optimisation literature. The algorithm requires the gradient of the objective function, and the value of the objective function, at each iteration. Calculation of these values requires all derivations for each sentence in the training data. In Clark and Curran (2004) we describe efficient methods for performing the calculations using packed charts. However, a very large amount of memory is still needed to store the packed charts for the complete training data even though the representation is very compact; in Clark and Curran (2003) we report a memory usage of 30 GB. To handle this we have developed a parallel implementation of the estimation algorithm which runs on a Beowulf cluster. The need for large high-performance computing resources is a disadvantage of our earlier approach. In the next section we show how use of the supertagger, combined with normal-form constraints on the derivations, can significantly reduce the memory requirements for the model estimation. 4 Generating Parser Training Data Since the training data contains the correct lexical categories, we ensure the correct category is assigned to each word w"
C04-1041,P04-1014,1,0.60491,"on the word’s POS tag is used. The table demonstrates the significant reduction in the average number of categories that can be achieved through the use of a supertagger. To give one example, the number of categories in the tag dictionary’s entry for the word is is 45 (only considering categories which have appeared at least 10 times in the training data). However, in the sentence Mr. Vinken is chairman of Elsevier N.V., the Dutch publishing group., the supertagger correctly assigns 1 category to is for β = 0.1, and 3 categories for β = 0.01. 3 The Parser The parser is described in detail in Clark and Curran (2004). It takes POS tagged sentences as input with each word assigned a set of lexical categories. A packed chart is used to efficiently represent all of the possible analyses for a sentence, and the CKY chart parsing algorithm described in Steedman (2000) is used to build the chart. Clark and Curran (2004) evaluate a number of log-linear parsing models for CCG. In this paper we use the normal-form model, which defines probabilities with the conditional log-linear form in (1), where y is a derivation and x is a sentence. Features are defined in terms of the local trees in the derivation, including"
C04-1041,P02-1042,1,0.547487,"erivation and x is a sentence. Features are defined in terms of the local trees in the derivation, including lexical head information and wordword dependencies. The normal-form derivations in CCGbank provide the gold standard training data. The feature set we use is from the best performing normal-form model in Clark and Curran (2004). For a given sentence the output of the parser is a dependency structure corresponding to the most probable derivation, which can be found using the Viterbi algorithm. The dependency relations are defined in terms of the argument slots of CCG lexical categories. Clark et al. (2002) and Clark and Curran (2004) give a detailed description of the dependency structures. 3.1 Model Estimation In Clark and Curran (2004) we describe a discriminative method for estimating the parameters of a log-linear parsing model. The estimation method maximises the following objective function: L0 (Λ) = L(Λ) − G(Λ) m n Y X λ2i = log PΛ (d j |S j ) − 2σ2 j=1 i=1 (2) The data consists of sentences S 1 , . . . , S m , together with gold standard normal-form derivations, d1 , . . . , dm . L(Λ) is the log-likelihood of model Λ, and G(Λ) is a Gaussian prior term used to avoid overfitting (n is the"
C04-1041,E03-1071,1,0.684772,"e CCG parser which is an order of magnitude faster than comparable systems for other linguistically motivated formalisms. 1 Introduction Lexicalised grammar formalisms such as Lexicalized Tree Adjoining Grammar (LTAG) and Combinatory Categorial Grammar (CCG) assign one or more syntactic structures to each word in a sentence which are then manipulated by the parser. Supertagging was introduced for LTAG as a way of increasing parsing efficiency by reducing the number of structures assigned to each word (Bangalore and Joshi, 1999). Supertagging has more recently been applied to CCG (Clark, 2002; Curran and Clark, 2003). Supertagging accuracy is relatively high for manually constructed LTAGs (Bangalore and Joshi, 1999). However, for LTAGs extracted automatically from the Penn Treebank, performance is much lower (Chen et al., 1999; Chen et al., 2002). In fact, performance for such grammars is below that needed for successful integration into a full parser (Sarkar et al., 2000). In this paper we demonstrate that CCG supertagging accuracy is not only sufficient for accurate and robust parsing using an automatically extracted grammar, but also offers several practical advantages. James R. Curran School of Inform"
C04-1041,P96-1011,0,0.297857,"how normal-form constraints can further reduce the derivation space. 4.1 Normal-Form Constraints As well as the supertagger, we use two additional strategies for reducing the derivation space. The first, following Hockenmaier (2003), is to only allow categories to combine if the combination has been seen in sections 2-21 of CCGbank. For example, NP/NP could combine with NP/NP according to CCG’s combinatory rules (by forward composition), but since this particular combination does not appear in CCGbank the parser does not allow it. The second strategy is to use Eisner’s normalform constraints (Eisner, 1996). The constraints SUPERTAGGING / PARSING CONSTRAINTS β = 0.01 → 0.05 → 0.1 CCGbank constraints Eisner constraints β = 0.05 → 0.1 USAGE DISK 17 GB 13 GB 9 GB 2 GB MEMORY 31 GB 23 GB 16 GB 4 GB Table 3: Space requirements for model training data prevent any constituent which is the result of a forward (backward) composition serving as the primary functor in another forward (backward) composition or a forward (backward) application. Eisner only deals with a grammar without type-raising, and so the constraints do not guarantee a normalform parse when using a grammar extracted from CCGbank. However"
C04-1041,hockenmaier-steedman-2002-acquiring,0,0.0823348,"Missing"
C04-1041,N04-1013,0,0.149848,"rk and Curran (2004). 5.2 Comparison with Other Work The only other work we are aware of to investigate the impact of supertagging on parsing efficiency is the work of Sarkar et al. (2000) for LTAG. Sarkar et al. did find that LTAG supertagging increased parsing speed, but at a significant cost in coverage: only 1,324 sentences out of a test set of 2,250 received a parse. The parse times reported are also not as good as those reported here: the time taken to parse the 2,250 test sentences was over 5 hours. 5 Multiplying by an estimate of the outside score may improve the efficacy of the beam. Kaplan et al. (2004) report high parsing speeds for a deep parsing system which uses an LFG grammar: 1.9 sentences per second for 560 sentences from section 23 of the Penn Treebank. They also report speeds for the publicly available Collins parser (Collins, 1999): 2.8 sentences per second for the same set. The best speeds we have reported for the CCG parser are an order of magnitude faster. 6 Conclusions This paper has shown that by tightly integrating a supertagger with a CCG parser, very fast parse times can be achieved for Penn Treebank WSJ text. As far as we are aware, the times reported here are an order of"
C04-1041,C00-1085,0,0.0958703,"Missing"
C04-1041,W00-1605,0,0.0451858,". Supertagging was introduced for LTAG as a way of increasing parsing efficiency by reducing the number of structures assigned to each word (Bangalore and Joshi, 1999). Supertagging has more recently been applied to CCG (Clark, 2002; Curran and Clark, 2003). Supertagging accuracy is relatively high for manually constructed LTAGs (Bangalore and Joshi, 1999). However, for LTAGs extracted automatically from the Penn Treebank, performance is much lower (Chen et al., 1999; Chen et al., 2002). In fact, performance for such grammars is below that needed for successful integration into a full parser (Sarkar et al., 2000). In this paper we demonstrate that CCG supertagging accuracy is not only sufficient for accurate and robust parsing using an automatically extracted grammar, but also offers several practical advantages. James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au Our wide-coverage CCG parser uses a log-linear model to select an analysis. The model paramaters are estimated using a discriminative method, that is, one which requires all incorrect parses for a sentence as well as the correct parse. Since an automatically extracted CCG grammar c"
C04-1041,J03-4003,0,\N,Missing
C04-1180,briscoe-carroll-2002-robust,0,0.0181404,"Missing"
C04-1180,E99-1042,0,0.00750892,"Missing"
C04-1180,2003.mtsummit-papers.6,0,0.0315977,"Missing"
C04-1180,A00-2018,0,0.0299319,"Missing"
C04-1180,W03-1013,1,0.567339,"mma and returns a sentential modifier of the same type. Type-raising is applied to the categories NP, PP and S adj NP (adjectival phrase), and is implemented by adding the relevant set of type-raised categories to the chart whenever an NP, PP or S adj NP is present. The sets of type-raised categories are based on the most commonly used typeraising rule instantiations in sections 2-21 of CCGbank, and currently contain 8 type-raised categories for NP and 1 each for PP and S adj NP. For a given sentence, the automatically extracted grammar can produce a very large number of derivations. Clark and Curran (2003) and Clark and Curran (2004b) describe how a packed chart can be used to efficiently represent the derivation space, and also efficient algorithms for finding the most probable derivation. The parser uses a log-linear model over normal-form derivations.3 Features are defined in terms of the local trees in the derivation, including lexical head information and word-word dependencies. The normal-form derivations in CCGbank provide the gold standard training data. For a given sentence, the output of the parser is a set of syntactic dependencies corresponding to the 3 most probable derivation. How"
C04-1180,C04-1041,1,0.441407,"es are suppressed, since these are developed beWhile the proliferation of surface constituents allowed by CCG adds to derivational ambiguity (since the constituent taxpayers £15 million to install is also allowed in the non-coordinate sentence It could cost taxpayers £15 million to install), previous work has shown that standard techniques from the statistical parsing literature can be used for practical widecoverage parsing with state-of-the-art performance. 3 The Parser A number of statistical parsers have recently been developed for CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b; Clark and Curran, 2004b). All of these parsers use a grammar derived from CCGbank (Hockenmaier and Steedman, 2002a; Hockenmaier, 2003), a treebank of normalform CCG derivations derived semi-automatically from the Penn Treebank. In this paper we use the Clark and Curran (2004b) parser, which uses a loglinear model of normal-form derivations to select an analysis. The parser takes a POS tagged sentence as input with a set of lexical categories assigned to each word. A CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. The supertagger uses a log-linear model of the target word’s context to dec"
C04-1180,P04-1014,1,0.669575,"es are suppressed, since these are developed beWhile the proliferation of surface constituents allowed by CCG adds to derivational ambiguity (since the constituent taxpayers £15 million to install is also allowed in the non-coordinate sentence It could cost taxpayers £15 million to install), previous work has shown that standard techniques from the statistical parsing literature can be used for practical widecoverage parsing with state-of-the-art performance. 3 The Parser A number of statistical parsers have recently been developed for CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b; Clark and Curran, 2004b). All of these parsers use a grammar derived from CCGbank (Hockenmaier and Steedman, 2002a; Hockenmaier, 2003), a treebank of normalform CCG derivations derived semi-automatically from the Penn Treebank. In this paper we use the Clark and Curran (2004b) parser, which uses a loglinear model of normal-form derivations to select an analysis. The parser takes a POS tagged sentence as input with a set of lexical categories assigned to each word. A CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. The supertagger uses a log-linear model of the target word’s context to dec"
C04-1180,P02-1042,1,0.779112,"of the derivation and of the semantics of noun phrases are suppressed, since these are developed beWhile the proliferation of surface constituents allowed by CCG adds to derivational ambiguity (since the constituent taxpayers £15 million to install is also allowed in the non-coordinate sentence It could cost taxpayers £15 million to install), previous work has shown that standard techniques from the statistical parsing literature can be used for practical widecoverage parsing with state-of-the-art performance. 3 The Parser A number of statistical parsers have recently been developed for CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b; Clark and Curran, 2004b). All of these parsers use a grammar derived from CCGbank (Hockenmaier and Steedman, 2002a; Hockenmaier, 2003), a treebank of normalform CCG derivations derived semi-automatically from the Penn Treebank. In this paper we use the Clark and Curran (2004b) parser, which uses a loglinear model of normal-form derivations to select an analysis. The parser takes a POS tagged sentence as input with a set of lexical categories assigned to each word. A CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. The supertagger us"
C04-1180,hockenmaier-steedman-2002-acquiring,1,0.878485,"nd of the semantics of noun phrases are suppressed, since these are developed beWhile the proliferation of surface constituents allowed by CCG adds to derivational ambiguity (since the constituent taxpayers £15 million to install is also allowed in the non-coordinate sentence It could cost taxpayers £15 million to install), previous work has shown that standard techniques from the statistical parsing literature can be used for practical widecoverage parsing with state-of-the-art performance. 3 The Parser A number of statistical parsers have recently been developed for CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b; Clark and Curran, 2004b). All of these parsers use a grammar derived from CCGbank (Hockenmaier and Steedman, 2002a; Hockenmaier, 2003), a treebank of normalform CCG derivations derived semi-automatically from the Penn Treebank. In this paper we use the Clark and Curran (2004b) parser, which uses a loglinear model of normal-form derivations to select an analysis. The parser takes a POS tagged sentence as input with a set of lexical categories assigned to each word. A CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. The supertagger uses a log-linear model of the tar"
C04-1180,P02-1043,1,0.826238,"nd of the semantics of noun phrases are suppressed, since these are developed beWhile the proliferation of surface constituents allowed by CCG adds to derivational ambiguity (since the constituent taxpayers £15 million to install is also allowed in the non-coordinate sentence It could cost taxpayers £15 million to install), previous work has shown that standard techniques from the statistical parsing literature can be used for practical widecoverage parsing with state-of-the-art performance. 3 The Parser A number of statistical parsers have recently been developed for CCG (Clark et al., 2002; Hockenmaier and Steedman, 2002b; Clark and Curran, 2004b). All of these parsers use a grammar derived from CCGbank (Hockenmaier and Steedman, 2002a; Hockenmaier, 2003), a treebank of normalform CCG derivations derived semi-automatically from the Penn Treebank. In this paper we use the Clark and Curran (2004b) parser, which uses a loglinear model of normal-form derivations to select an analysis. The parser takes a POS tagged sentence as input with a set of lexical categories assigned to each word. A CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. The supertagger uses a log-linear model of the tar"
C04-1180,N04-1013,0,0.0413167,"Missing"
C04-1180,C02-1105,0,0.0414764,"Missing"
C04-1180,J03-4003,0,\N,Missing
C10-2168,J99-2004,0,0.347837,"s on the Combinatory Categorial Grammar (CCG) parser of Clark and Curran (2007). One advantage of the CCG parser is that it is able to assign rich structural descriptions to sentences, from a variety of representations, e.g. CCG derivations, CCG dependency structures, grammatical relations (Carroll et al., 1998), and ﬁrst-order logical forms (Bos et al., 2004). One of the properties of the grammar formalism is that it is lexicalised, associating CCG lexical categories, or CCG supertags, with the words in a sentence (Steedman, 2000). Clark and Curran (2004) adapt the technique of supertagging (Bangalore and Joshi, 1999) to CCG, using a standard maximum entropy tagger to assign small sets of supertags to each word. The reduction in ambiguity resulting from the supertagging stage results in a surprisingly efﬁcient parser, given the rich structural output, operating at tens of newspaper sentences per second. In this paper we demonstrate that the CCG parser can be made more than twice as fast, with little or no loss in accuracy. A noteworthy feature of the CCG parser is that, after the supertagging 1471 Coling 2010: Poster Volume, pages 1471–1479, Beijing, August 2010 stage, the parser builds a complete packed c"
C10-2168,C04-1180,1,0.805962,"is a system that performs open information extraction on the web (Lin et al., 2009). However, the text processing that is performed by TextRunner, in particular the parsing, is rudimentary: ﬁnite-state shallow parsing technology that In this paper we focus on the Combinatory Categorial Grammar (CCG) parser of Clark and Curran (2007). One advantage of the CCG parser is that it is able to assign rich structural descriptions to sentences, from a variety of representations, e.g. CCG derivations, CCG dependency structures, grammatical relations (Carroll et al., 1998), and ﬁrst-order logical forms (Bos et al., 2004). One of the properties of the grammar formalism is that it is lexicalised, associating CCG lexical categories, or CCG supertags, with the words in a sentence (Steedman, 2000). Clark and Curran (2004) adapt the technique of supertagging (Bangalore and Joshi, 1999) to CCG, using a standard maximum entropy tagger to assign small sets of supertags to each word. The reduction in ambiguity resulting from the supertagging stage results in a surprisingly efﬁcient parser, given the rich structural output, operating at tens of newspaper sentences per second. In this paper we demonstrate that the CCG pa"
C10-2168,P06-2006,0,0.0364055,"Missing"
C10-2168,P05-1022,0,0.187649,"ate that the CCG parser can be made more than twice as fast, with little or no loss in accuracy. A noteworthy feature of the CCG parser is that, after the supertagging 1471 Coling 2010: Poster Volume, pages 1471–1479, Beijing, August 2010 stage, the parser builds a complete packed chart, storing all sentences consistent with the assigned supertags and the parser’s CCG combinatory rules, with no chart pruning whatsoever. The use of chart pruning techniques, typically some form of beam search, is essential for practical parsing using Penn Treebank parsers (Collins, 1999; Petrov and Klein, 2007; Charniak and Johnson, 2005), as well as practical parsers based on linguistic formalisms, such as HPSG (Ninomiya et al., 2005) and LFG (Kaplan et al., 2004). However, in the CCG case, the use of the supertagger means that enough ambiguity has already been resolved to allow the complete chart to be represented. Despite the effectiveness of the supertagging stage, the number of derivations stored in a packed chart can still be enormous for typical newspaper sentences. Hence it is an obvious question whether chart pruning techniques can be profitably applied to the CCG parser. Some previous work (Djordjevic et al., 2007) h"
C10-2168,A00-2018,0,0.336784,"he supertagger output can be stored in a packed chart. However, wide-coverage CCG parsers still produce a very large number of derivations for typical newspaper or Wikipedia sentences. In this paper we investigate two forms of chart pruning, and develop a novel method for pruning complete cells in a parse chart. The result is a widecoverage CCG parser that can process almost 100 sentences per second, with little or no loss in accuracy over the baseline with no pruning. Many of the popular wide-coverage parsers available today operate at around one newspaper sentence per second (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007). There are dependency parsers that operate orders of magnitude faster, by exploiting the fact that accurate dependency parsing can be achieved by using a shift-reduce linear-time process which makes a single decision at each point in the parsing process (Nivre and Scholz, 2004). Introduction Many NLP tasks and applications require the processing of massive amounts of textual data. For example, knowledge acquisition efforts can involve processing billions of words of text (Curran, 2004). Also, the increasing need to process large amounts of web data places an efﬁciency"
C10-2168,C04-1041,1,0.967384,"e-state shallow parsing technology that In this paper we focus on the Combinatory Categorial Grammar (CCG) parser of Clark and Curran (2007). One advantage of the CCG parser is that it is able to assign rich structural descriptions to sentences, from a variety of representations, e.g. CCG derivations, CCG dependency structures, grammatical relations (Carroll et al., 1998), and ﬁrst-order logical forms (Bos et al., 2004). One of the properties of the grammar formalism is that it is lexicalised, associating CCG lexical categories, or CCG supertags, with the words in a sentence (Steedman, 2000). Clark and Curran (2004) adapt the technique of supertagging (Bangalore and Joshi, 1999) to CCG, using a standard maximum entropy tagger to assign small sets of supertags to each word. The reduction in ambiguity resulting from the supertagging stage results in a surprisingly efﬁcient parser, given the rich structural output, operating at tens of newspaper sentences per second. In this paper we demonstrate that the CCG parser can be made more than twice as fast, with little or no loss in accuracy. A noteworthy feature of the CCG parser is that, after the supertagging 1471 Coling 2010: Poster Volume, pages 1471–1479, B"
C10-2168,P10-1036,1,0.767973,"Missing"
C10-2168,N06-1020,0,0.0609905,"taggers. 5.2.1 Final experiments using gold training and self training In this section we report our ﬁnal tests using Wikipedia data. We used two methods to derive training data for the taggers. The ﬁrst is the standard method, which is to transform gold-standard parse trees into begin and end tag sequences. This method is the method that we used for all previous experiments, and we call it “gold training”. In addition to gold training, we also investigate an alternative method, which is to obtain training data for the taggers from the output of the parser itself, in a form of self-training (McClosky et al., 2006). The intuition is that the tagger will learn what constituents a trained parser will eventually choose, and as long as the constituents favoured by the parsing model are not pruned, no reduction in accuracy can occur. There is the potential for an increase in speed, however, due to the pruning effect. For gold training, we used sections 02-21 of 1477 Model baseline binary gold binary 40K binary 200K binary 1M level gold level 40K level 200K level 1M Speed 47.6 80.8 75.5 77.4 78.6 93.7 92.8 92.5 96.6 CCGbank) did not improve the self-training results. We did see the usual speed improvements fr"
C10-2168,W05-1511,0,0.0222118,"orthy feature of the CCG parser is that, after the supertagging 1471 Coling 2010: Poster Volume, pages 1471–1479, Beijing, August 2010 stage, the parser builds a complete packed chart, storing all sentences consistent with the assigned supertags and the parser’s CCG combinatory rules, with no chart pruning whatsoever. The use of chart pruning techniques, typically some form of beam search, is essential for practical parsing using Penn Treebank parsers (Collins, 1999; Petrov and Klein, 2007; Charniak and Johnson, 2005), as well as practical parsers based on linguistic formalisms, such as HPSG (Ninomiya et al., 2005) and LFG (Kaplan et al., 2004). However, in the CCG case, the use of the supertagger means that enough ambiguity has already been resolved to allow the complete chart to be represented. Despite the effectiveness of the supertagging stage, the number of derivations stored in a packed chart can still be enormous for typical newspaper sentences. Hence it is an obvious question whether chart pruning techniques can be profitably applied to the CCG parser. Some previous work (Djordjevic et al., 2007) has investigated this question but with little success. In this paper we investigate two types of ch"
C10-2168,J07-4004,1,0.948597,"essing of massive amounts of textual data. For example, knowledge acquisition efforts can involve processing billions of words of text (Curran, 2004). Also, the increasing need to process large amounts of web data places an efﬁciency demand on existing NLP tools. TextRunner, for example, is a system that performs open information extraction on the web (Lin et al., 2009). However, the text processing that is performed by TextRunner, in particular the parsing, is rudimentary: ﬁnite-state shallow parsing technology that In this paper we focus on the Combinatory Categorial Grammar (CCG) parser of Clark and Curran (2007). One advantage of the CCG parser is that it is able to assign rich structural descriptions to sentences, from a variety of representations, e.g. CCG derivations, CCG dependency structures, grammatical relations (Carroll et al., 1998), and ﬁrst-order logical forms (Bos et al., 2004). One of the properties of the grammar formalism is that it is lexicalised, associating CCG lexical categories, or CCG supertags, with the words in a sentence (Steedman, 2000). Clark and Curran (2004) adapt the technique of supertagging (Bangalore and Joshi, 1999) to CCG, using a standard maximum entropy tagger to a"
C10-2168,C04-1010,0,0.0481003,"in a parse chart. The result is a widecoverage CCG parser that can process almost 100 sentences per second, with little or no loss in accuracy over the baseline with no pruning. Many of the popular wide-coverage parsers available today operate at around one newspaper sentence per second (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007). There are dependency parsers that operate orders of magnitude faster, by exploiting the fact that accurate dependency parsing can be achieved by using a shift-reduce linear-time process which makes a single decision at each point in the parsing process (Nivre and Scholz, 2004). Introduction Many NLP tasks and applications require the processing of massive amounts of textual data. For example, knowledge acquisition efforts can involve processing billions of words of text (Curran, 2004). Also, the increasing need to process large amounts of web data places an efﬁciency demand on existing NLP tools. TextRunner, for example, is a system that performs open information extraction on the web (Lin et al., 2009). However, the text processing that is performed by TextRunner, in particular the parsing, is rudimentary: ﬁnite-state shallow parsing technology that In this paper"
C10-2168,N07-1051,0,0.469875,"utput can be stored in a packed chart. However, wide-coverage CCG parsers still produce a very large number of derivations for typical newspaper or Wikipedia sentences. In this paper we investigate two forms of chart pruning, and develop a novel method for pruning complete cells in a parse chart. The result is a widecoverage CCG parser that can process almost 100 sentences per second, with little or no loss in accuracy over the baseline with no pruning. Many of the popular wide-coverage parsers available today operate at around one newspaper sentence per second (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007). There are dependency parsers that operate orders of magnitude faster, by exploiting the fact that accurate dependency parsing can be achieved by using a shift-reduce linear-time process which makes a single decision at each point in the parsing process (Nivre and Scholz, 2004). Introduction Many NLP tasks and applications require the processing of massive amounts of textual data. For example, knowledge acquisition efforts can involve processing billions of words of text (Curran, 2004). Also, the increasing need to process large amounts of web data places an efﬁciency demand on existing NLP t"
C10-2168,E03-1071,1,0.80584,"Missing"
C10-2168,W96-0213,0,0.368336,"Missing"
C10-2168,N09-1073,0,0.236863,"epresented. Despite the effectiveness of the supertagging stage, the number of derivations stored in a packed chart can still be enormous for typical newspaper sentences. Hence it is an obvious question whether chart pruning techniques can be profitably applied to the CCG parser. Some previous work (Djordjevic et al., 2007) has investigated this question but with little success. In this paper we investigate two types of chart pruning: a standard beam search, similar to that used in the Collins parser (Collins, 1999), and a more aggressive strategy in which complete cells are pruned, following Roark and Hollingshead (2009). Roark and Hollingshead use a ﬁnite-state tagger to decide which words in a sentence can end or begin constituents, from which whole cells in the chart can be removed. We develop a novel extension to this approach, in which a tagger is trained to infer the maximum length constituent that can begin or end at a particular word. These lengths can then be used in a more agressive pruning strategy which we show to be signiﬁcantly more effective than the basic approach. Both beam search and cell pruning are highly effective, with the resulting CCG parser able to process almost 100 sentences per sec"
C10-2168,W07-2206,1,0.892852,"harniak and Johnson, 2005), as well as practical parsers based on linguistic formalisms, such as HPSG (Ninomiya et al., 2005) and LFG (Kaplan et al., 2004). However, in the CCG case, the use of the supertagger means that enough ambiguity has already been resolved to allow the complete chart to be represented. Despite the effectiveness of the supertagging stage, the number of derivations stored in a packed chart can still be enormous for typical newspaper sentences. Hence it is an obvious question whether chart pruning techniques can be profitably applied to the CCG parser. Some previous work (Djordjevic et al., 2007) has investigated this question but with little success. In this paper we investigate two types of chart pruning: a standard beam search, similar to that used in the Collins parser (Collins, 1999), and a more aggressive strategy in which complete cells are pruned, following Roark and Hollingshead (2009). Roark and Hollingshead use a ﬁnite-state tagger to decide which words in a sentence can end or begin constituents, from which whole cells in the chart can be removed. We develop a novel extension to this approach, in which a tagger is trained to infer the maximum length constituent that can be"
C10-2168,J07-3004,0,0.0352215,"with pointers to the children used in the creation. Equivalence is deﬁned in terms of the category and head of the constituent, to enable the Viterbi algorithm to efﬁciently ﬁnd the highest scoring derivation.1 A textbook treatment of CKY applied to statistical parsing is given in Jurafsky and Martin (2000). 2 We performed efﬁciency and accuracy tests on newspaper and Wikipedia data. For the newspaper data, we used the standard test sections from The CCG Parser The parser is described in detail in Clark and Curran (2007). It is based on CCGbank, a CCG version of the Penn Treebank developed by Hockenmaier and Steedman (2007). 3 Data and Evaluation Metrics 1 Use of the Viterbi algorithm in this way requires the features in the parser model to be local to a single rule application; Clark and Curran (2007) has more discussion. 1472 (ncmod num hundred 1 Seven 0) (conj and 2 sixty-one 3) (conj and 2 hundred 1) (dobj in 6 total 7) (ncmod made 5 in 6) (aux made 5 were 4) (ncsubj made 5 and 2 obj) (passive made 5) β Baseline 0.001 0.002 0.005 0.01 Seven hundred and sixty-one were made in total. Figure 1: Example Wikipedia test sentence annotated with grammatical relations. CCGbank. Following Clark and Curran (2007) we us"
C10-2168,J00-4006,0,0.00657188,"starting with two-word constituents (assuming the supertagging phase has been completed), incrementally increasing the span until the whole sentence is covered. The chart is packed in the standard sense that any two equivalent constituents created during the parsing process are placed in the same equivalence class, with pointers to the children used in the creation. Equivalence is deﬁned in terms of the category and head of the constituent, to enable the Viterbi algorithm to efﬁciently ﬁnd the highest scoring derivation.1 A textbook treatment of CKY applied to statistical parsing is given in Jurafsky and Martin (2000). 2 We performed efﬁciency and accuracy tests on newspaper and Wikipedia data. For the newspaper data, we used the standard test sections from The CCG Parser The parser is described in detail in Clark and Curran (2007). It is based on CCGbank, a CCG version of the Penn Treebank developed by Hockenmaier and Steedman (2007). 3 Data and Evaluation Metrics 1 Use of the Viterbi algorithm in this way requires the features in the parser model to be local to a single rule application; Clark and Curran (2007) has more discussion. 1472 (ncmod num hundred 1 Seven 0) (conj and 2 sixty-one 3) (conj and 2 h"
C10-2168,N04-1013,0,\N,Missing
C10-2168,J03-4003,0,\N,Missing
C12-1031,N10-1084,1,0.172762,"two fundamental requirements: high imperceptibility and high payload capacity. The former aims at imposing minimum embedding distortion to the cover text so that the resulting stegotext in which a message is camouflaged is inconspicuous. The latter aims at providing sufficient embedding capacity in order to achieve efficient information transmission. There is a trade-off between imperceptibility and payload, since any attempt to embed additional information via changes to the cover text increases the chance of introducing anomalies into the text and thus raising the suspicion of an observer (Chang and Clark, 2010a). Another cryptographic method is secret sharing. Secret sharing (Blakley, 1979; Shamir, 1979) refers to methods for distributing a secret amongst a group of n people, each of whom is allocated a share of the secret. Individual shares are of no use on their own; only when any group of t (for threshold) or more shares are combined together can the secret be reconstructed. Such a system is called a (t, n)-threshold scheme. For example, a simple (3,3)-threshold scheme for a secret number s can be achieved by splitting s into three numerical shares s1 , s2 and s3 such that s = s1 + s2 + s3 . Not"
C12-1031,D10-1116,1,0.113924,"two fundamental requirements: high imperceptibility and high payload capacity. The former aims at imposing minimum embedding distortion to the cover text so that the resulting stegotext in which a message is camouflaged is inconspicuous. The latter aims at providing sufficient embedding capacity in order to achieve efficient information transmission. There is a trade-off between imperceptibility and payload, since any attempt to embed additional information via changes to the cover text increases the chance of introducing anomalies into the text and thus raising the suspicion of an observer (Chang and Clark, 2010a). Another cryptographic method is secret sharing. Secret sharing (Blakley, 1979; Shamir, 1979) refers to methods for distributing a secret amongst a group of n people, each of whom is allocated a share of the secret. Individual shares are of no use on their own; only when any group of t (for threshold) or more shares are combined together can the secret be reconstructed. Such a system is called a (t, n)-threshold scheme. For example, a simple (3,3)-threshold scheme for a secret number s can be achieved by splitting s into three numerical shares s1 , s2 and s3 such that s = s1 + s2 + s3 . Not"
C12-1031,J90-1003,0,0.234655,"n 3.1. The second to the fifth scores are the scores calculated by only considering a specific window size n, where n = 2 to 5, using the same method as for the Score function. Again, each score is provided with an additional boolean feature to indicate whether the CountBefore average is equal to zero. There are a total of 58 features contributed from the n-gram counts. 4.2 Lexical Association Measures In addition to n-gram features, we exploit some standard lexical association measures to determine the degree of association between an adjective and a noun. Pointwise Mutual Information (PMI) (Church and Hanks, 1990) is roughly a measure of how much one word tells us about the other. In order to calculate PMI, we need the joint frequency of the noun-adjective pair, the frequency of the noun modified by any adjective and the frequency of the adjective modifying any noun. We collect (adjective, noun) pairs and their frequency counts from grammatical relations (GRs). The GRs we use are derived by parsing a Wikipedia dump (dated October 2007) with Clark and Curran (2007)’s CCG parser. We first consider GRs having the pattern (ncmod _ noun adjective) and extract the (adjective, noun) pair. Next we extract pair"
C12-1031,J07-4004,1,0.861068,"aticality and naturalness checks. In order to prevent an ungrammatical adjective deletion, we use the syntactic filter proposed in Chang and Clark (2010a) to certify the deletion grammaticality. This is only a preliminary grammaticality check and does not guarantee sentence fluency. For generating the modified sentence, we also use Minnen et al. (2001)’s tools for correcting the form of an indefinite. For example, after deleting alternative, the phrase “an alternative choice” would be modified to “a choice”. The original and modified sentences are then parsed using a wide-coverage CCG parser (Clark and Curran, 2007). After parsing, each lexical token is associated with a syntactic description, called a lexical category, or supertag. With the significant amount of information included in supertags, comparing two sequences of supertags is similar to comparing two syntax trees. Thus we require a deletion to retain the same sequence of supertags as that of the original sentence in order to ensure grammaticality. Table 1 shows two adjective deletion examples and their supertags,2 where last is the target adjective. The first deletion case passes the grammaticality check since all the supertags remain the same"
C12-1031,C08-1018,0,0.0438281,"egosystem may find more words that can be substituted in a fairy tale than in a medical paper since there are usually many terminologies in a medical paper which cannot be changed or even cannot be found in a standard dictionary. To the best of our knowledge, there is no study on the practical issue of using different types of cover text for the steganography application. 2.3 Sentence Compression Sentence compression, text simplification and text summarisation usually involve removing unimportant words in a sentence in order to make the text more concise. For example, Knight and Marcu (2002), Cohn and Lapata (2008), Filippova and Strube (2008) and Zhu et al. (2010) have used the word deletion operation in their systems. However, to our knowledge, there is no work looking at redundant adjectives in text in particular. The proposed adjective deletion methods can be applied before and/or after a sentence compression system. Deleting unnecessary adjectives before can help the system focus on other content of a sentence. Deleting unnecessary adjectives after can generate an even more concise sentence. 496 Sentence Supertags before deletion Supertags after deletion Sentence Supertags before deletion Supertags"
C12-1031,J93-1003,0,0.056692,"Next we extract pairs that match patterns (xcomp _ be adjective) and (ncsubj be noun _) in a sentence. For instance, the GRs of the sentence “The car is red” are (det car_1 the_0) (xcomp _ be_2 red_3) and (ncsubj be_2 car_1 _), and since car and red match the two patterns, (red, car) is seen as an eligible pair for our database. A total of 63,896,006 adjective-noun pairs are extracted form the parsed Wikipedia corpus which includes 832,320 noun types and 792,914 adjective types. We also use the log likelihood ratio (LLR), an alternative to PMI, which is reported to handle rare events better (Dunning, 1993). Again, the contingency table for computing LLR can be derived from the parsed Wikipedia corpus described above. In the study of collocation extraction, both high PMI and LLR values are treated as evidence that the collocation components occur together more often than by chance. In this paper, we use PMI and LLR as features in the SVM. 4.3 Noun and Adjective Entropy Suppose we observe a noun N1 as being modified by adjective J1 five times, J2 twice and J3 three times. The modifier entropy of N1 is H(N1 ) = −((0.5 log 0.5)+(0.2 log 0.2)+(0.3 log 0.3)) = 1.5. Now suppose there is a noun N2 modi"
C12-1031,W08-1105,0,0.0248955,"words that can be substituted in a fairy tale than in a medical paper since there are usually many terminologies in a medical paper which cannot be changed or even cannot be found in a standard dictionary. To the best of our knowledge, there is no study on the practical issue of using different types of cover text for the steganography application. 2.3 Sentence Compression Sentence compression, text simplification and text summarisation usually involve removing unimportant words in a sentence in order to make the text more concise. For example, Knight and Marcu (2002), Cohn and Lapata (2008), Filippova and Strube (2008) and Zhu et al. (2010) have used the word deletion operation in their systems. However, to our knowledge, there is no work looking at redundant adjectives in text in particular. The proposed adjective deletion methods can be applied before and/or after a sentence compression system. Deleting unnecessary adjectives before can help the system focus on other content of a sentence. Deleting unnecessary adjectives after can generate an even more concise sentence. 496 Sentence Supertags before deletion Supertags after deletion Sentence Supertags before deletion Supertags after deletion Those awaitin"
C12-1031,P99-1004,0,0.0536426,"α-Skew Divergence We assume that if an adjective in a noun phrase is deletable, the noun should have a similar n-gram distribution to the original adjective-noun phrase across the various n-gram counts. Figure 1 shows the logarithmic n-gram counts of joint collaboration and collaboration being in the same context of the sentence “The task force will be a joint collaboration between the cities of Sterling Heights and Warren.” In this example sentence, joint is determined as deletable. We can see that the counts have similar distributions before and after the deletion. We use α-skew divergence (Lee, 1999) to calculate the n-gram distributional similarity between the original and the modified sentences. The α-skew divergence is a non-symmetric measure of the difference between two probability distributions P and Q. In our application, P is a probability vector containing normalised logarithmic counts derived from the contextual n-grams before removing the adjective, and Q is a probability vector obtained after deleting the adjective. The α-skew divergence measure is defined as: Sα (Q, P) = D(Pkα·Q + (1 − α)·P), P P(v) where 0 ≤ α ≤ 1 and D is the Kullback-Leibler divergence D(PkQ) = v P(v) log"
C12-1031,D11-1126,0,0.0450846,"l., 2005; Meral et al., 2007; Murphy, 2001; Murphy and Vogel, 2007b; Topkara et al., 2006b) and semantic transformations (Atallah et al., 2002; Vybornova and Macq, 2007). For details of the transformations mentioned above, readers can refer to our previous papers: Chang and Clark (2010a) and Chang and Clark (2010b). Another group of studies aim to embed information into translated text. Stutsman et al. (2006) use multiple translation systems to provide alternative candidates for a sentence. The secret information is then embedded into the choice of translation. Another recent work proposed by Venugopal et al. (2011) introduces a watermark as a parameter in the machine translation algorithm and probabilistically identifies the watermarked translation. The motivation of watermarking machine translation outputs is to distinguish machine and human generated translations so a machine translation system is unlikely to learn from self-generated data. These transformations often rely on sophisticated NLP tools and resources. For example, a lexical substitution-based stegosystem may require synonym dictionaries, POS taggers, word sense disambiguation tools and language models; a syntactic transformation-based ste"
C12-1031,C10-1152,0,0.0324421,"a fairy tale than in a medical paper since there are usually many terminologies in a medical paper which cannot be changed or even cannot be found in a standard dictionary. To the best of our knowledge, there is no study on the practical issue of using different types of cover text for the steganography application. 2.3 Sentence Compression Sentence compression, text simplification and text summarisation usually involve removing unimportant words in a sentence in order to make the text more concise. For example, Knight and Marcu (2002), Cohn and Lapata (2008), Filippova and Strube (2008) and Zhu et al. (2010) have used the word deletion operation in their systems. However, to our knowledge, there is no work looking at redundant adjectives in text in particular. The proposed adjective deletion methods can be applied before and/or after a sentence compression system. Deleting unnecessary adjectives before can help the system focus on other content of a sentence. Deleting unnecessary adjectives after can generate an even more concise sentence. 496 Sentence Supertags before deletion Supertags after deletion Sentence Supertags before deletion Supertags after deletion Those awaiting execution spent thei"
C12-1032,J96-1002,0,0.239217,"sentence. Stutsman et al. (2006) showed that their translation-based stegosystem has a payload of 0.33 bits per sentence. In this paper, we exploit word ordering as the linguistic transformation. A cover sentence is first used to provide a bag-of-words as input to the Zhang et al. (2012) word ordering realisation system. The generated permutations provide alternatives to the original and thus the cover sentence can play the role of an information carrier. However, not all the permutations are grammatical and semantically meaningful. To solve this problem we train a Maximum Entropy classifier (Berger et al., 1996) to distinguish natural word orders from awkward wordings, and evaluate the classifier using human judgements. Note that the proposed maximum entropy classifier is trained to classify sentence-leval naturalness and thus, it is possible that even individual natural sentences might lead to an unnatural document. Modeling the documentlevel coherence of modified text would be useful but is outside the scope of our study. In addition, we review some translation-based stegosystems and show that the word ordering transformation can be used with the existing translation-based embedding algorithms. For"
C12-1032,N10-1084,1,0.292572,"tic steganography scheme should allow sufficient embedding capacity to achieve efficient information transmission, resulting in a large payload capacity. There is a fundamental trade-off between security and payload since any attempt to embed additional information is likely to increase the chance of introducing anomalies into the text, thus degrading the security level. Existing studies have exploited different linguistic transformations for the application of steganography, such as lexical substitution (Chapman and Davida, 1997; Bolshakov, 2004; Taskiran et al., 2006; Topkara et al., 2006c; Chang and Clark, 2010b), phrase paraphrasing (Chang and Clark, 2010a), sentence structure manipulations (Atallah et al., 2001a,b; Liu et al., 2005; Meral et al., 2007; Murphy, 2001; Murphy and Vogel, 2007b; Topkara et al., 2006b), semantic transformations (Atallah et al., 2002; Vybornova and Macq, 2007) and text translation (Grothoff et al., 2005; Stutsman et al., 2006; Meng et al., 2011). These transformations often rely on sophisticated NLP tools and resources. For example, a lexical substitution-based stegosystem may require synonym dictionaries, POS taggers, word sense disambiguation tools and language models;"
C12-1032,D10-1116,1,0.748375,"tic steganography scheme should allow sufficient embedding capacity to achieve efficient information transmission, resulting in a large payload capacity. There is a fundamental trade-off between security and payload since any attempt to embed additional information is likely to increase the chance of introducing anomalies into the text, thus degrading the security level. Existing studies have exploited different linguistic transformations for the application of steganography, such as lexical substitution (Chapman and Davida, 1997; Bolshakov, 2004; Taskiran et al., 2006; Topkara et al., 2006c; Chang and Clark, 2010b), phrase paraphrasing (Chang and Clark, 2010a), sentence structure manipulations (Atallah et al., 2001a,b; Liu et al., 2005; Meral et al., 2007; Murphy, 2001; Murphy and Vogel, 2007b; Topkara et al., 2006b), semantic transformations (Atallah et al., 2002; Vybornova and Macq, 2007) and text translation (Grothoff et al., 2005; Stutsman et al., 2006; Meng et al., 2011). These transformations often rely on sophisticated NLP tools and resources. For example, a lexical substitution-based stegosystem may require synonym dictionaries, POS taggers, word sense disambiguation tools and language models;"
C12-1032,J07-4004,1,0.70937,"Missing"
C12-1032,E03-1071,1,0.752644,"etermine the acceptability of a permutation. Even though this baseline method is only an n-gram count comparison, Bergsma et al. (2009) show that the approach works well for lexical disambiguation tasks and produces comparable performance to other more complex methods. 4.2 Maximum Entropy Classifier In addition to the baseline method, we propose a machine learning approach to classify natural and unnatural permutations. We choose the method of maximum entropy modelling (MaxEnt for short) because of its proven performance for NLP tasks, such as part-of-speech tagging (Ratnaparkhi et al., 1996; Curran and Clark, 2003), parsing (Ratnaparkhi, 1999; Johnson et al., 1999) and language modelling (Rosenfeld, 1996), and the ease with which features can be included in the model (Ratnaparkhi, 1999). In addition, some work has shown that MaxEnt is viable for ranking the fluency of machine generated sentences (Nakanishi et al., 2005; Velldal and Oepen, 2006; Velldal, 2008). The concept of MaxEnt is to use observed features about a certain event (y) occurring in the context (x) to estimate a probability model p(y|x). Its canonical form is: n X 1 p( y|x) = ex p λi f i (x, y) Z(x) i=1 where Z(x) is a normalisation const"
C12-1032,de-marneffe-etal-2006-generating,0,0.00884975,"Missing"
C12-1032,W08-1301,0,0.0225528,"Missing"
C12-1032,D09-1129,0,0.125792,"sing some syntactic features to train a Maximum Entropy classifier (Berger et al., 1996). Both methods require a score/probability threshold to decide the acceptability of a permutation. 4.1 Google N-gram Method The Google n-gram corpus (Brants and Franz, 2006) contains frequency counts for n-grams from unigrams through five-grams obtained from over 1 trillion word tokens of English Web text collected in January 2006. Only n-grams appearing more than 40 times were kept in the corpus. The Google n-gram corpus has been applied to many NLP tasks such as spelling correction (Carlson et al., 2008; Islam and Inkpen, 2009), multi-word expression classification 517 (Kummerfeld and Curran, 2008) and lexical disambiguation (Bergsma et al., 2009). Recently, in Chang and Clark (2010b) and Chang and Clark (2010a) we have used the corpus to check the text paraphrasing grammaticality and the synonym acceptability in context in our earlier steganography systems. Therefore, we propose a baseline method similar to Chang and Clark (2010b) and Chang and Clark (2010a) using the Google n-gram corpus to calculate a score based on the n-gram counts before and after word ordering. The task is as follows: given a cover sentence a"
C12-1032,P99-1069,0,0.0354159,"ugh this baseline method is only an n-gram count comparison, Bergsma et al. (2009) show that the approach works well for lexical disambiguation tasks and produces comparable performance to other more complex methods. 4.2 Maximum Entropy Classifier In addition to the baseline method, we propose a machine learning approach to classify natural and unnatural permutations. We choose the method of maximum entropy modelling (MaxEnt for short) because of its proven performance for NLP tasks, such as part-of-speech tagging (Ratnaparkhi et al., 1996; Curran and Clark, 2003), parsing (Ratnaparkhi, 1999; Johnson et al., 1999) and language modelling (Rosenfeld, 1996), and the ease with which features can be included in the model (Ratnaparkhi, 1999). In addition, some work has shown that MaxEnt is viable for ranking the fluency of machine generated sentences (Nakanishi et al., 2005; Velldal and Oepen, 2006; Velldal, 2008). The concept of MaxEnt is to use observed features about a certain event (y) occurring in the context (x) to estimate a probability model p(y|x). Its canonical form is: n X 1 p( y|x) = ex p λi f i (x, y) Z(x) i=1 where Z(x) is a normalisation constant over all events in context x and λi is the weig"
C12-1032,U08-1008,0,0.133901,"(Berger et al., 1996). Both methods require a score/probability threshold to decide the acceptability of a permutation. 4.1 Google N-gram Method The Google n-gram corpus (Brants and Franz, 2006) contains frequency counts for n-grams from unigrams through five-grams obtained from over 1 trillion word tokens of English Web text collected in January 2006. Only n-grams appearing more than 40 times were kept in the corpus. The Google n-gram corpus has been applied to many NLP tasks such as spelling correction (Carlson et al., 2008; Islam and Inkpen, 2009), multi-word expression classification 517 (Kummerfeld and Curran, 2008) and lexical disambiguation (Bergsma et al., 2009). Recently, in Chang and Clark (2010b) and Chang and Clark (2010a) we have used the corpus to check the text paraphrasing grammaticality and the synonym acceptability in context in our earlier steganography systems. Therefore, we propose a baseline method similar to Chang and Clark (2010b) and Chang and Clark (2010a) using the Google n-gram corpus to calculate a score based on the n-gram counts before and after word ordering. The task is as follows: given a cover sentence and corresponding permutation, decide if the permutation is acceptable. T"
C12-1032,J93-2004,0,0.0419311,"t permutations for a cover sentence, but, in practice, any word ordering realisation system can be integrated into the proposed word ordering-based stegosystem. 3.2 Human Judgement Corpus Since not all the sentence permutations generated by the Zhang et al. (2012) system are grammatical and semantically meaningful, we develop a maximum entropy classifier to determine the naturalness of permutations. In order to have a labelled corpus for training and testing the classifier, we first randomly selected 765 sentences having length between 8 and 25 tokens from sections 02-21 of the Penn Treebank (Marcus et al., 1993) as the cover sentences. The restriction on the sentence length is because a short sentence may not have enough good permu515 Score Explanation 1 2 Completely or largely non-fluent, and/or completely or largely lacking in meaning. Very awkward wording, major punctuation errors, and/or logical errors, but still possible to understand. Slightly awkward but still relatively fluent, clear and logical; may contain slightly awkward wording and/or minor punctuation errors. Perfectly natural – both grammatical and semantically meaningful. 3 4 Score 3 1 1 4 1 Figure 2: Rating scale and guidelines for h"
C12-1032,W05-1510,0,0.0123686,"n to the baseline method, we propose a machine learning approach to classify natural and unnatural permutations. We choose the method of maximum entropy modelling (MaxEnt for short) because of its proven performance for NLP tasks, such as part-of-speech tagging (Ratnaparkhi et al., 1996; Curran and Clark, 2003), parsing (Ratnaparkhi, 1999; Johnson et al., 1999) and language modelling (Rosenfeld, 1996), and the ease with which features can be included in the model (Ratnaparkhi, 1999). In addition, some work has shown that MaxEnt is viable for ranking the fluency of machine generated sentences (Nakanishi et al., 2005; Velldal and Oepen, 2006; Velldal, 2008). The concept of MaxEnt is to use observed features about a certain event (y) occurring in the context (x) to estimate a probability model p(y|x). Its canonical form is: n X 1 p( y|x) = ex p λi f i (x, y) Z(x) i=1 where Z(x) is a normalisation constant over all events in context x and λi is the weight of the feature f i (x, y). The standard way to train a maximum entropy model is to use conditional maximum likelihood (with a Gaussian prior for smoothing), which is equivalent to picking the most uniform model subject to constraints on the feature expecta"
C12-1032,W96-0213,0,0.0405384,"Missing"
C12-1032,W06-1661,0,0.0128614,", we propose a machine learning approach to classify natural and unnatural permutations. We choose the method of maximum entropy modelling (MaxEnt for short) because of its proven performance for NLP tasks, such as part-of-speech tagging (Ratnaparkhi et al., 1996; Curran and Clark, 2003), parsing (Ratnaparkhi, 1999; Johnson et al., 1999) and language modelling (Rosenfeld, 1996), and the ease with which features can be included in the model (Ratnaparkhi, 1999). In addition, some work has shown that MaxEnt is viable for ranking the fluency of machine generated sentences (Nakanishi et al., 2005; Velldal and Oepen, 2006; Velldal, 2008). The concept of MaxEnt is to use observed features about a certain event (y) occurring in the context (x) to estimate a probability model p(y|x). Its canonical form is: n X 1 p( y|x) = ex p λi f i (x, y) Z(x) i=1 where Z(x) is a normalisation constant over all events in context x and λi is the weight of the feature f i (x, y). The standard way to train a maximum entropy model is to use conditional maximum likelihood (with a Gaussian prior for smoothing), which is equivalent to picking the most uniform model subject to constraints on the feature expectations (Berger et al., 199"
C12-1032,E09-1097,0,0.13128,"e have been some word ordering realisation systems that take a 513 Rank (binary) Permutation Secret Bitstring s=1 s=2 s=3 1 (001) 2 (010) 3 (011) 4 (100) 5 (101) 6 (110) In our products now there is no asbestos. No asbestos there is now in our products. Now in our products there is no asbestos. There is no asbestos in our products now. There no asbestos in our products is now. There now is no asbestos in our products. 1 0 1 0 1 0 01 10 11 00 01 10 001 010 011 100 101 110 Figure 1: Ranked sentence permutations and their secret bits bag-of-words as input and automatically generate permutations (Wan et al., 2009; Zhang and Clark, 2011; Zhang et al., 2012). Any of these word ordering realisation systems can be treated as a module integrated into the proposed secret embedding method. For linguistic steganography, there exists a convenient modularity between the linguistic transformation and the embedding method. In other words, a secret embedding method can treat the linguistic transformation as a black box for outputting alternatives for a given cover text. How an alternative encodes secret bits is decided by the embedding algorithm. In the next section, we propose an embedding method designed to use"
C12-1032,D12-1023,0,0.0137669,"me secret bit(s). Therefore, it is crucial to develop a method that can distinguish acceptable permutations from those having awkward wordings in a word ordering-based stegosystem. It is important to note that the checking method can take the original sentence into consideration because the permutation selection happens at the secret embedding stage and is not needed during the decoding. Having the cover sentence available at the checking stage is a feature we will exploit. A research area that relates to the proposed permutation checking method is realisation ranking (Cahill and Forst, 2010; White and Rajkumar, 2012) where a system is given a set of text realisations and is asked to rate each text in the set. However, in the realisation ranking task there is no “cover text”. Since our methods require the knowledge of the original text, they cannot be applied to the realisation ranking task. In this section we first explain a baseline method using the Google n-gram corpus (Brants and Franz, 2006) to check whether a particular word ordering has been used frequently on the Web. Then we propose another approach using some syntactic features to train a Maximum Entropy classifier (Berger et al., 1996). Both met"
C12-1032,E12-1075,1,0.910869,"y syntactic transformations was around 0.5 to 1.0 bits per sentence (Atallah et al., 2001b; Topkara et al., 2006b; Meral et al., 2009). Since the ontological semantic transformation is currently impractical, the empirical payload is not available. Another semantic method (Vybornova and Macq, 2007) achieves a payload of 1 bit per sentence. Stutsman et al. (2006) showed that their translation-based stegosystem has a payload of 0.33 bits per sentence. In this paper, we exploit word ordering as the linguistic transformation. A cover sentence is first used to provide a bag-of-words as input to the Zhang et al. (2012) word ordering realisation system. The generated permutations provide alternatives to the original and thus the cover sentence can play the role of an information carrier. However, not all the permutations are grammatical and semantically meaningful. To solve this problem we train a Maximum Entropy classifier (Berger et al., 1996) to distinguish natural word orders from awkward wordings, and evaluate the classifier using human judgements. Note that the proposed maximum entropy classifier is trained to classify sentence-leval naturalness and thus, it is possible that even individual natural sen"
C12-1032,D11-1106,1,0.911663,"ord ordering realisation systems that take a 513 Rank (binary) Permutation Secret Bitstring s=1 s=2 s=3 1 (001) 2 (010) 3 (011) 4 (100) 5 (101) 6 (110) In our products now there is no asbestos. No asbestos there is now in our products. Now in our products there is no asbestos. There is no asbestos in our products now. There no asbestos in our products is now. There now is no asbestos in our products. 1 0 1 0 1 0 01 10 11 00 01 10 001 010 011 100 101 110 Figure 1: Ranked sentence permutations and their secret bits bag-of-words as input and automatically generate permutations (Wan et al., 2009; Zhang and Clark, 2011; Zhang et al., 2012). Any of these word ordering realisation systems can be treated as a module integrated into the proposed secret embedding method. For linguistic steganography, there exists a convenient modularity between the linguistic transformation and the embedding method. In other words, a secret embedding method can treat the linguistic transformation as a black box for outputting alternatives for a given cover text. How an alternative encodes secret bits is decided by the embedding algorithm. In the next section, we propose an embedding method designed to use permutations from a wor"
D08-1050,J99-2004,0,0.671526,"signs a single POS tag to each word in a sentence. POS tags are fairly coarse-grained grammatical labels indicating part-of-speech; the Penn Treebank set, used here, contains approximately 50 labels. Second, a maximum entropy supertagger assigns CCG lexical categories to the words in the sentence. 476 Lexical categories can be thought of as fine-grained POS tags expressing subcategorization information, i.e. information about the argument frame of the word. There are 425 categories in the set used by the CCG parser. Supertagging was originally developed for Lexicalized Tree Adjoining Grammar (Bangalore and Joshi, 1999), but has been particularly successful for wide-coverage CCG parsing (Clark and Curran, 2007b). Rather than assign a single category to each word, the supertagger operates as a multitagger, sometimes assigning more than one category if the context is not sufficiently discriminating to suggest a single tag (Curran et al., 2006). Since the taggers have linear time complexity, the first two stages can be performed extremely quickly. Finally, the parsing stage combines the lexical categories, using a small set of combinatory rules that are part of the grammar of CCG, and builds a packed chart repr"
D08-1050,P06-4020,0,0.0412227,"stion data started from a much lower baseline figure, however. 4.4 Parser We evaluated the parser on the 500 questions annotated with Stanford GRs and on the 500 evaluation sentences from the BioInfer corpus. We used the original newspaper pipeline, a pipeline with a retrained POS tagger, and a pipeline with both a retrained POS tagger and supertagger. In order to perform these evaluations we develooped a mapping from the parser’s native CCG syntactic dependencies to GRs in the Stanford format. The mapping was based on the same principles as the mapping that produces GR output in the style of Briscoe et al. (2006). These principles are discussed in detail in Clark and Curran (2007a); in summary, the argument slots in the CCG dependencies are mapped to argument slots in Stanford GRs, a fairly complex, many-to-many mapping. An additional post-processing script applies some manually developed rules to bring the output closer to the Stanford format. Figure 2 gives an example of Stanford GRs, where the label of the relation is followed by two arguments, head and dependent. Table 3 gives the results of the parser evaluation on GRs. Since the parser model was not retrained, the improvements in accuracy are du"
D08-1050,A00-2018,0,0.319476,"Missing"
D08-1050,P07-1032,1,0.949594,"rather than a set of documents about a particular topic. However, we consider question data to be interesting in the context of domain adaptation for the following reasons: 1) there are few examples in the Penn Treebank (PTB) and so PTB parsers typically perform poorly on them; 2) questions form a fairly homogeneous set with respect to the syntactic constructions employed, and it is an interesting question how easy it is to adapt a parser to such data; and 3) QA is becoming an important example of NLP technology, and question parsing is an important task for QA systems. The CCG parser we use (Clark and Curran, 2007b) makes use of three levels of representation: one, a POS tag level based on the fairly coarse-grained POS tags in the Penn Treebank; two, a lexical category level based on the more fine-grained CCG lexical categories, which are assigned to words by a CCG su475 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 475–484, c Honolulu, October 2008. 2008 Association for Computational Linguistics pertagger; and three, a hierarchical level consisting of CCG derivations. A key idea in this paper, following a pilot study in Clark et al. (2004), is to perform"
D08-1050,J07-4004,1,0.945364,"rather than a set of documents about a particular topic. However, we consider question data to be interesting in the context of domain adaptation for the following reasons: 1) there are few examples in the Penn Treebank (PTB) and so PTB parsers typically perform poorly on them; 2) questions form a fairly homogeneous set with respect to the syntactic constructions employed, and it is an interesting question how easy it is to adapt a parser to such data; and 3) QA is becoming an important example of NLP technology, and question parsing is an important task for QA systems. The CCG parser we use (Clark and Curran, 2007b) makes use of three levels of representation: one, a POS tag level based on the fairly coarse-grained POS tags in the Penn Treebank; two, a lexical category level based on the more fine-grained CCG lexical categories, which are assigned to words by a CCG su475 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 475–484, c Honolulu, October 2008. 2008 Association for Computational Linguistics pertagger; and three, a hierarchical level consisting of CCG derivations. A key idea in this paper, following a pilot study in Clark et al. (2004), is to perform"
D08-1050,W04-3215,1,0.92299,"ser we use (Clark and Curran, 2007b) makes use of three levels of representation: one, a POS tag level based on the fairly coarse-grained POS tags in the Penn Treebank; two, a lexical category level based on the more fine-grained CCG lexical categories, which are assigned to words by a CCG su475 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 475–484, c Honolulu, October 2008. 2008 Association for Computational Linguistics pertagger; and three, a hierarchical level consisting of CCG derivations. A key idea in this paper, following a pilot study in Clark et al. (2004), is to perform manual annotation only at the first two levels. Since the lexical category level consists of sequences of tags, rather than hierarchical derivations, the annotation can be performed relatively quickly. For the biomedical and question domains we manually annotated approximately 1,000 and 2,000 sentences, respectively, with CCG lexical categories. We also created a gold standard set of grammatical relations (GR) in the Stanford format (de Marneffe et al., 2006), using 500 of the questions. For the biomedical domain we used the BioInfer corpus (Pyysalo et al., 2007a), an existing"
D08-1050,P97-1003,0,0.30846,"Missing"
D08-1050,P06-1088,1,0.851992,"ht of as fine-grained POS tags expressing subcategorization information, i.e. information about the argument frame of the word. There are 425 categories in the set used by the CCG parser. Supertagging was originally developed for Lexicalized Tree Adjoining Grammar (Bangalore and Joshi, 1999), but has been particularly successful for wide-coverage CCG parsing (Clark and Curran, 2007b). Rather than assign a single category to each word, the supertagger operates as a multitagger, sometimes assigning more than one category if the context is not sufficiently discriminating to suggest a single tag (Curran et al., 2006). Since the taggers have linear time complexity, the first two stages can be performed extremely quickly. Finally, the parsing stage combines the lexical categories, using a small set of combinatory rules that are part of the grammar of CCG, and builds a packed chart representation containing all the derivations which can be built from the lexical categories. The Viterbi algorithm efficiently finds the highest scoring derivation from the packed chart, using a loglinear model to score the derivations. The grammar and training data for the newspaper version of the CCG parser are obtained from CC"
D08-1050,de-marneffe-etal-2006-generating,0,0.032399,"Missing"
D08-1050,W07-2202,0,0.134325,"Missing"
D08-1050,P01-1037,0,0.0337072,"ently for parser adaptation. 1 Introduction Most state-of-the-art wide-coverage parsers are based on the Penn Treebank (Marcus et al., 1993), making such parsers highly tuned to newspaper text. A pressing question facing the parsing community is how to adapt these parsers to other domains, such as biomedical research papers and web pages. A related question is how to improve the performance of these parsers on constructions that are rare in the Penn Treebank, such as questions. Questions are particularly important since a question parser is a component in most Question Answering (QA) systems (Harabagiu et al., 2001). In this paper we investigate parser adaptation in the context of lexicalized grammars, by using a The two domains we consider are the biomedical domain and questions for a QA system. We use the term “domain” somewhat loosely here, since questions are best described as a particular set of syntactic constructions, rather than a set of documents about a particular topic. However, we consider question data to be interesting in the context of domain adaptation for the following reasons: 1) there are few examples in the Penn Treebank (PTB) and so PTB parsers typically perform poorly on them; 2) qu"
D08-1050,J07-3004,0,0.176838,"the taggers have linear time complexity, the first two stages can be performed extremely quickly. Finally, the parsing stage combines the lexical categories, using a small set of combinatory rules that are part of the grammar of CCG, and builds a packed chart representation containing all the derivations which can be built from the lexical categories. The Viterbi algorithm efficiently finds the highest scoring derivation from the packed chart, using a loglinear model to score the derivations. The grammar and training data for the newspaper version of the CCG parser are obtained from CCGbank (Hockenmaier and Steedman, 2007), a CCG version of the Penn Treebank. The aspect of the pipeline which is most relevant to this paper is the supertagging phase. Figure 1 gives an example sentence from each target domain, with the CCG lexical category assigned to each word shown below the word, and the POS tag to the right. Note that the categories contain a significant amount of grammatical information, in particular subcategorization information. The verb acts in the biomedical sentence, for example, looks for a prepositional phrase (PP, as a linkage protein) to its right and a noun phrase (NP, Talin) to its left, with the"
D08-1050,P06-1063,0,0.157178,"Missing"
D08-1050,W03-2401,0,0.0695246,"Missing"
D08-1050,I05-1006,0,0.0752745,"Missing"
D08-1050,J93-2004,0,0.0330833,"cal text and questions for a QA system, by using manually-annotated training data at the POS and lexical category levels only. This approach achieves parser accuracy comparable to that on newspaper data without the need for annotated parse trees in the new domain. We find that retraining at the lexical category level yields a larger performance increase for questions than for biomedical text and analyze the two datasets to investigate why different domains might behave differently for parser adaptation. 1 Introduction Most state-of-the-art wide-coverage parsers are based on the Penn Treebank (Marcus et al., 1993), making such parsers highly tuned to newspaper text. A pressing question facing the parsing community is how to adapt these parsers to other domains, such as biomedical research papers and web pages. A related question is how to improve the performance of these parsers on constructions that are rare in the Penn Treebank, such as questions. Questions are particularly important since a question parser is a component in most Question Answering (QA) systems (Harabagiu et al., 2001). In this paper we investigate parser adaptation in the context of lexicalized grammars, by using a The two domains w"
D08-1050,P05-1011,0,0.088417,"Missing"
D08-1050,W06-1619,0,0.0403654,"Missing"
D08-1050,W07-1004,0,0.0497193,"a pilot study in Clark et al. (2004), is to perform manual annotation only at the first two levels. Since the lexical category level consists of sequences of tags, rather than hierarchical derivations, the annotation can be performed relatively quickly. For the biomedical and question domains we manually annotated approximately 1,000 and 2,000 sentences, respectively, with CCG lexical categories. We also created a gold standard set of grammatical relations (GR) in the Stanford format (de Marneffe et al., 2006), using 500 of the questions. For the biomedical domain we used the BioInfer corpus (Pyysalo et al., 2007a), an existing gold-standard GR resource also in the Stanford format. We evaluated the parser on both lexical category assignment and recovery of GRs. The results show that the domain adaptation approach used here is successful in two very different domains, achieving parsing accuracy comparable to state-of-the-art accuracy for newspaper text. The results also show, however, that the two domains have different profiles with regard to the levels of representation used by the parser. We find that simply retraining the POS tagger used by the parser leads to a large improvement in performance for"
D08-1059,W06-2920,0,0.344212,"use an approximate decoder while a transition-based parser is not necessarily deterministic. To make the concepts clear, we classify the two types of parser by the following two criteria: 1. whether or not the outputs are built by explicit transition-actions, such as ”Shift” and ”Reduce”; 2. whether it is dependency graphs or transitionactions that the parsing model assigns scores to. By this classification, beam-search can be applied to both graph-based and transition-based parsers. Representative of each method, MSTParser and MaltParser gave comparable accuracies in the CoNLL-X shared task (Buchholz and Marsi, 2006). However, they make different types of errors, which can be seen as a reflection of their theoretical differences (McDonald and Nivre, 2007). MSTParser has the strength of exact inference, but its choice of features is constrained by the requirement of efficient dynamic programming. MaltParser is deterministic, yet its comparatively larger feature range is an advantage. By comparing the two, three interesting research questions arise: (1) how to increase the flexibility in defining features for graph-based parsing; (2) how to add search to transition-based parsing; and (3) how to combine the"
D08-1059,W06-2925,0,0.0306015,"Missing"
D08-1059,P04-1015,0,0.708526,"s, the agenda contains an empty sentence. At each processing stage, existing partial candidates from the agenda are extended in all possible ways according to the Covington algorithm. The top B newly generated candidates are then put to the agenda. After all input words are processed, the best candidate output from the agenda is taken as the final output. The projectivity of the output dependency trees is guaranteed by the incremental Covington process. The time complexity of this algorithm is O(n2 ), where n is the length of the input sentence. During training, the “early update” strategy of Collins and Roark (2004) is used: when the correct state item falls out of the beam at any stage, parsing is stopped immediately, and the model is updated using the current best partial item. The intuition is to improve learning by avoiding irrelevant information: when all the items in the current agenda are incorrect, further parsing steps will be irrelevant because the correct partial output no longer exists in the candidate ranking. Table 1 shows the feature templates from the MSTParser (McDonald and Pereira, 2006), which are defined in terms of the context of a word, its parent and its sibling. To give more templ"
D08-1059,W02-1001,0,0.921413,"(x) = arg max Score(y) y∈GEN(x) where GEN(x) denotes the set of possible parses for the input x. To repeat our earlier comments, in this paper we do not consider the method of finding the arg max to be part of the definition of graph-based parsing, only the fact that the dependency graph itself is being scored, and factored into scores attached to the dependency links. The score of an output parse y is given by a linear model: Score(y) = Φ(y) · w ~ where Φ(y) is the global feature vector from y and w ~ is the weight vector of the model. We use the discriminative perceptron learning algorithm (Collins, 2002; McDonald et al., 2005) to train the values of w. ~ The algorithm is shown in Figure 1. Averaging parameters is a way to reduce overfitting for perceptron training (Collins, 2002), and is applied to all our experiments. While the MSTParser uses exact-inference (Eisner, 1996), we apply beam-search to decoding. This is done by extending the deterministic Covington algorithm for projective dependency parsing (Covington, 2001). As shown in Figure 2, the decoder works incrementally, building a state item (i.e. partial parse tree) word by word. When each word is processed, links are added between t"
D08-1059,C96-1058,0,0.736463,"ndency graph itself is being scored, and factored into scores attached to the dependency links. The score of an output parse y is given by a linear model: Score(y) = Φ(y) · w ~ where Φ(y) is the global feature vector from y and w ~ is the weight vector of the model. We use the discriminative perceptron learning algorithm (Collins, 2002; McDonald et al., 2005) to train the values of w. ~ The algorithm is shown in Figure 1. Averaging parameters is a way to reduce overfitting for perceptron training (Collins, 2002), and is applied to all our experiments. While the MSTParser uses exact-inference (Eisner, 1996), we apply beam-search to decoding. This is done by extending the deterministic Covington algorithm for projective dependency parsing (Covington, 2001). As shown in Figure 2, the decoder works incrementally, building a state item (i.e. partial parse tree) word by word. When each word is processed, links are added between the current word and its predecessors. Beam-search is applied by keeping the B best items in the agenda at each processing stage, while partial candidates are compared by scores from the graph-based model, according to partial graph up to the current word. Before decoding star"
D08-1059,D07-1097,0,0.037297,"approximated large-margin algorithm. This model is the most similar to our transition-based model, while the differences include the choice of learning and decoding algorithms, the definition of feature templates and our application of the “early update” strategy. Our combined parser makes the biggest contribution of this paper. In contrast to the models above, it includes both graph-based and transition-based components. An existing method to combine multiple parsing algorithms is the ensemble approach (Sagae and Lavie, 2006a), which was reported to be useful in improving dependency parsing (Hall et al., 2007). A more recent approach (Nivre and McDonald, 2008) combined MSTParser and MaltParser by using the output of one parser for features in the other. Both Hall et al. (2007) and Nivre and McDonald (2008) can be seen as methods to combine separately defined models. In contrast, our parser combines two components in a single model, in which all parameters are trained consistently. 7 the combined parser was highly competitive compared to the best systems in the literature. The idea of combining different approaches to the same problem using beam-search and a global model could be applied to other pa"
D08-1059,P07-1050,0,0.0153262,"uracy on the data. Our observations on parsing Chinese are essentially the same as for English. Our combined parser outperforms both the pure graph-based and the pure transition-based parsers. It gave the best accuracy we are aware of for dependency parsing using CTB. 6 Related work Our graph-based parser is derived from the work of McDonald and Pereira (2006). Instead of performing exact inference by dynamic programming, we incorporated the linear model and feature templates from McDonald and Pereira (2006) into our beam-search framework, while adding new global features. Nakagawa (2007) and Hall (2007) also showed the effectiveness of global features in improving the accuracy of graph-based parsing, using the approximate Gibbs sampling method and a reranking approach, respectively. Our transition-based parser is derived from the deterministic parser of Nivre et al. (2006). We incorporated the transition process into our beamsearch framework, in order to study the influence of search on this algorithm. Existing efforts to add search to deterministic parsing include Sagae and Lavie (2006b), which applied best-first search to constituent parsing, and Johansson and Nugues (2006) and Duan et al."
D08-1059,W06-2930,0,0.0455436,"l features. Nakagawa (2007) and Hall (2007) also showed the effectiveness of global features in improving the accuracy of graph-based parsing, using the approximate Gibbs sampling method and a reranking approach, respectively. Our transition-based parser is derived from the deterministic parser of Nivre et al. (2006). We incorporated the transition process into our beamsearch framework, in order to study the influence of search on this algorithm. Existing efforts to add search to deterministic parsing include Sagae and Lavie (2006b), which applied best-first search to constituent parsing, and Johansson and Nugues (2006) and Duan et al. (2007), which applied beamsearch to dependency parsing. All three methods estimate the probability of each transition action, and score a state item by the product of the probabilities of all its corresponding actions. But different from our transition-based parser, which trains all transitions for a parse globally, these models train the probability of each action separately. Based on the work of Johansson and Nugues (2006), Johansson and Nugues (2007) studied global training with an approximated large-margin algorithm. This model is the most similar to our transition-based m"
D08-1059,D07-1123,0,0.0831456,"search to deterministic parsing include Sagae and Lavie (2006b), which applied best-first search to constituent parsing, and Johansson and Nugues (2006) and Duan et al. (2007), which applied beamsearch to dependency parsing. All three methods estimate the probability of each transition action, and score a state item by the product of the probabilities of all its corresponding actions. But different from our transition-based parser, which trains all transitions for a parse globally, these models train the probability of each action separately. Based on the work of Johansson and Nugues (2006), Johansson and Nugues (2007) studied global training with an approximated large-margin algorithm. This model is the most similar to our transition-based model, while the differences include the choice of learning and decoding algorithms, the definition of feature templates and our application of the “early update” strategy. Our combined parser makes the biggest contribution of this paper. In contrast to the models above, it includes both graph-based and transition-based components. An existing method to combine multiple parsing algorithms is the ensemble approach (Sagae and Lavie, 2006a), which was reported to be useful"
D08-1059,N03-1017,0,0.00622671,"t its choice of features is constrained by the requirement of efficient dynamic programming. MaltParser is deterministic, yet its comparatively larger feature range is an advantage. By comparing the two, three interesting research questions arise: (1) how to increase the flexibility in defining features for graph-based parsing; (2) how to add search to transition-based parsing; and (3) how to combine the two parsing approaches so that the strengths of each are utilized. In this paper, we study these questions under one framework: beam-search. Beam-search has been successful in many NLP tasks (Koehn et al., 2003; 562 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 562–571, c Honolulu, October 2008. 2008 Association for Computational Linguistics Inputs: training examples (xi , yi ) Initialization: set w ~ =0 Algorithm: // R training iterations; N examples for t = 1..R, i = 1..N : zi = arg maxy∈GEN(xi ) Φ(y) · w ~ if zi 6= yi : w ~ =w ~ + Φ(yi ) − Φ(zi ) Outputs: w ~ Figure 1: The perceptron learning algorithm Collins and Roark, 2004), and can achieve accuracy that is close to exact inference. Moreover, a beamsearch decoder does not impose restrictions on t"
D08-1059,P08-1068,0,0.276446,"Missing"
D08-1059,D07-1013,0,0.278812,"g that, in contrast to MaltParser, which trains each action decision individually, our training algorithm globally optimizes all action decisions for a parse. Again, “early update” and averaging parameters are applied to the training process. 4 Training Dev Test P 0 T 0 ∈act(y) Score(T , sT 0 ) P 0 ~T T 0 ∈act(y) Φ(T , sT 0 ) · w = w~T · P T 0 ∈act(y) Φ(T Words 950,028 40,117 56,684 PTB We therefore combine the two models to give: ScoreC (y) = ScoreG (y) + ScoreT (y) = ΦG (y) · w~G + ΦT (y) · w~T The graph-based and transition-based approaches adopt very different views of dependency parsing. McDonald and Nivre (2007) showed that the MSTParser and MaltParser produce different errors. This observation suggests a combined approach: by using both graph-based information and transition-based information, parsing accuracy can be improved. The beam-search framework we have developed facilitates such a combination. Our graph-based and transition-based parsers share many similarities. Both build a parse tree incrementally, keeping an agenda of comparable state items. Both rank state items by their current scores, and use the averaged perceptron with early update for training. The key differences are the scoring mo"
D08-1059,E06-1011,0,0.909394,"is O(n2 ), where n is the length of the input sentence. During training, the “early update” strategy of Collins and Roark (2004) is used: when the correct state item falls out of the beam at any stage, parsing is stopped immediately, and the model is updated using the current best partial item. The intuition is to improve learning by avoiding irrelevant information: when all the items in the current agenda are incorrect, further parsing steps will be irrelevant because the correct partial output no longer exists in the candidate ranking. Table 1 shows the feature templates from the MSTParser (McDonald and Pereira, 2006), which are defined in terms of the context of a word, its parent and its sibling. To give more templates, features from templates 1 – 5 are also conjoined with 564 1 2 3 Parent word (P) Child word (C) 4 A tag Bt between P, C Neighbour words of P, C, left (PL/CL) and right (PR/CR) P and C 5 sibling (S) of C 6 Pw; Pt; Pwt Cw; Ct; Cwt PwtCwt; PwtCw; PwCwt; PwtCt; PtCwt; PwCw; PtCt PtBtCt PtPLtCtCLt; PtPLtCtCRt; PtPRtCtCLt; PtPRtCtCRt; PtPLtCLt; PtPLtCRt; PtPRtCLt; PtPRtCRt; PLtCtCLt; PLtCtCRt; PRtCtCLt; PRtCtCRt; PtCtCLt; PtCtCRt; PtPLtCt; PtPRtCt CwSw; CtSt; CwSt; CtSw; PtCtSt; Table 1: Feature"
D08-1059,P05-1012,0,0.976325,"core(y) y∈GEN(x) where GEN(x) denotes the set of possible parses for the input x. To repeat our earlier comments, in this paper we do not consider the method of finding the arg max to be part of the definition of graph-based parsing, only the fact that the dependency graph itself is being scored, and factored into scores attached to the dependency links. The score of an output parse y is given by a linear model: Score(y) = Φ(y) · w ~ where Φ(y) is the global feature vector from y and w ~ is the weight vector of the model. We use the discriminative perceptron learning algorithm (Collins, 2002; McDonald et al., 2005) to train the values of w. ~ The algorithm is shown in Figure 1. Averaging parameters is a way to reduce overfitting for perceptron training (Collins, 2002), and is applied to all our experiments. While the MSTParser uses exact-inference (Eisner, 1996), we apply beam-search to decoding. This is done by extending the deterministic Covington algorithm for projective dependency parsing (Covington, 2001). As shown in Figure 2, the decoder works incrementally, building a state item (i.e. partial parse tree) word by word. When each word is processed, links are added between the current word and its"
D08-1059,D07-1100,0,0.0320952,"he previous best accuracy on the data. Our observations on parsing Chinese are essentially the same as for English. Our combined parser outperforms both the pure graph-based and the pure transition-based parsers. It gave the best accuracy we are aware of for dependency parsing using CTB. 6 Related work Our graph-based parser is derived from the work of McDonald and Pereira (2006). Instead of performing exact inference by dynamic programming, we incorporated the linear model and feature templates from McDonald and Pereira (2006) into our beam-search framework, while adding new global features. Nakagawa (2007) and Hall (2007) also showed the effectiveness of global features in improving the accuracy of graph-based parsing, using the approximate Gibbs sampling method and a reranking approach, respectively. Our transition-based parser is derived from the deterministic parser of Nivre et al. (2006). We incorporated the transition process into our beamsearch framework, in order to study the influence of search on this algorithm. Existing efforts to add search to deterministic parsing include Sagae and Lavie (2006b), which applied best-first search to constituent parsing, and Johansson and Nugues (2006)"
D08-1059,P08-1108,0,0.177125,"del is the most similar to our transition-based model, while the differences include the choice of learning and decoding algorithms, the definition of feature templates and our application of the “early update” strategy. Our combined parser makes the biggest contribution of this paper. In contrast to the models above, it includes both graph-based and transition-based components. An existing method to combine multiple parsing algorithms is the ensemble approach (Sagae and Lavie, 2006a), which was reported to be useful in improving dependency parsing (Hall et al., 2007). A more recent approach (Nivre and McDonald, 2008) combined MSTParser and MaltParser by using the output of one parser for features in the other. Both Hall et al. (2007) and Nivre and McDonald (2008) can be seen as methods to combine separately defined models. In contrast, our parser combines two components in a single model, in which all parameters are trained consistently. 7 the combined parser was highly competitive compared to the best systems in the literature. The idea of combining different approaches to the same problem using beam-search and a global model could be applied to other parsing tasks, such as constituent parsing, and possi"
D08-1059,W06-2933,0,0.042082,"Missing"
D08-1059,N06-2033,0,0.0971182,"om McDonald and Pereira (2006) into our beam-search framework, while adding new global features. Nakagawa (2007) and Hall (2007) also showed the effectiveness of global features in improving the accuracy of graph-based parsing, using the approximate Gibbs sampling method and a reranking approach, respectively. Our transition-based parser is derived from the deterministic parser of Nivre et al. (2006). We incorporated the transition process into our beamsearch framework, in order to study the influence of search on this algorithm. Existing efforts to add search to deterministic parsing include Sagae and Lavie (2006b), which applied best-first search to constituent parsing, and Johansson and Nugues (2006) and Duan et al. (2007), which applied beamsearch to dependency parsing. All three methods estimate the probability of each transition action, and score a state item by the product of the probabilities of all its corresponding actions. But different from our transition-based parser, which trains all transitions for a parse globally, these models train the probability of each action separately. Based on the work of Johansson and Nugues (2006), Johansson and Nugues (2007) studied global training with an ap"
D08-1059,P06-2089,0,0.0111382,"om McDonald and Pereira (2006) into our beam-search framework, while adding new global features. Nakagawa (2007) and Hall (2007) also showed the effectiveness of global features in improving the accuracy of graph-based parsing, using the approximate Gibbs sampling method and a reranking approach, respectively. Our transition-based parser is derived from the deterministic parser of Nivre et al. (2006). We incorporated the transition process into our beamsearch framework, in order to study the influence of search on this algorithm. Existing efforts to add search to deterministic parsing include Sagae and Lavie (2006b), which applied best-first search to constituent parsing, and Johansson and Nugues (2006) and Duan et al. (2007), which applied beamsearch to dependency parsing. All three methods estimate the probability of each transition action, and score a state item by the product of the probabilities of all its corresponding actions. But different from our transition-based parser, which trains all transitions for a parse globally, these models train the probability of each action separately. Based on the work of Johansson and Nugues (2006), Johansson and Nugues (2007) studied global training with an ap"
D08-1059,N04-1032,0,0.0174794,"Missing"
D08-1059,W03-3023,0,0.901952,"wing McDonald et al. (2005). Bracketed sentences from the Penn Treebank (PTB) 3 are split into training, development and test sets MSTParser 1 Graph [M] Transition Graph [MA] MSTParser 2 Combined [TM] Combined [TMA] Word 90.7 91.2 91.4 91.4 91.5 92.0 92.1 Complete 36.7 40.8 41.8 42.5 42.1 45.0 45.4 Table 5: Accuracy comparisons using PTB 3 Figure 6: The influence of beam size on the transitionbased parser, using the development data X-axis: number of training iterations Y-axis: word precision as shown in Table 4, and then translated into dependency structures using the head-finding rules from Yamada and Matsumoto (2003). Before parsing, POS tags are assigned to the input sentence using our reimplementation of the POStagger from Collins (2002). Like McDonald et al. (2005), we evaluate the parsing accuracy by the precision of lexical heads (the percentage of input words, excluding punctuation, that have been assigned the correct parent) and by the percentage of complete matches, in which all words excluding punctuation have been assigned the correct parent. 5.1 Development experiments Since the beam size affects all three parsers, we study its influence first; here we show the effect on the transition-based pa"
D08-1059,D07-1101,0,\N,Missing
D09-1085,de-marneffe-etal-2006-generating,0,0.240733,"Missing"
D09-1085,H91-1060,0,0.0344846,"Missing"
D09-1085,W01-0521,0,0.0557389,"Missing"
D09-1085,J07-3004,1,0.896337,"e removed. 3.2 The parsers The parsers that we chose to evaluate are the C & C CCG parser (Clark and Curran, 2007), the Enju HPSG parser (Miyao and Tsujii, 2005), the RASP parser (Briscoe et al., 2006), the Stanford parser (Klein and Manning, 2003), and the DCU postprocessor of PTB parsers (Cahill et al., 2004), based on LFG and applied to the output of the Charniak and Johnson reranking parser. Of course we were unable to evaluate every publicly available parser, but we believe these are representative of current wide-coverage robust parsing technology.1 The C & C parser is based on CCGbank (Hockenmaier and Steedman, 2007), a CCG version of the Penn Treebank. It is ideally suited for this evaluation because CCG was designed to capture the unbounded dependencies being considered. The Enju parser was designed with a similar motivation to C & C, and is also based on an automatically extracted grammar derived from the PTB, but the grammar formalism is HPSG rather than CCG . Both parsers produce head-word dependencies reflecting the underlying predicate-argument structure of a sentence, and so in theory should be straightforward to evaluate. The RASP parser is based on a manually constructed POS tag-sequence grammar"
D09-1085,E03-1005,0,0.0607821,"Missing"
D09-1085,P08-1067,0,0.0408422,"Missing"
D09-1085,P06-4020,0,0.0556321,"it does not produce explicit dependencies. However, the DCU post-processor is ideal because it does produce dependencies in a GR format. It has also obtained competitive scores on general GR evaluation corpora (Cahill et al., 2004). Table 3: Distance between head and dependent. subject embedded construction: the same stump which had impaled the car of many a guest in the past thirty years and which he refused to have removed. 3.2 The parsers The parsers that we chose to evaluate are the C & C CCG parser (Clark and Curran, 2007), the Enju HPSG parser (Miyao and Tsujii, 2005), the RASP parser (Briscoe et al., 2006), the Stanford parser (Klein and Manning, 2003), and the DCU postprocessor of PTB parsers (Cahill et al., 2004), based on LFG and applied to the output of the Charniak and Johnson reranking parser. Of course we were unable to evaluate every publicly available parser, but we believe these are representative of current wide-coverage robust parsing technology.1 The C & C parser is based on CCGbank (Hockenmaier and Steedman, 2007), a CCG version of the Penn Treebank. It is ideally suited for this evaluation because CCG was designed to capture the unbounded dependencies being considered. The Enju p"
D09-1085,P02-1018,0,0.309025,"resented. Fourth, we show that current parsing technology is very poor at representing some important elements of the argument structure of sentences, and argue for a more focused constructionbased parser evaluation as a complement to existing grammatical relation-based evaluations. We also perform an error-analysis for one of the more successful parsers. There has been some prior work on evaluating parsers on long-range dependencies, but no work we are aware of that has the scope and focus of this paper. Clark et al. (2004) evaluated a CCG parser on a small corpus of object extraction cases. Johnson (2002) began the body of work on inserting traces into the output of Penn Treebank (PTB) parsers, followed by Levy and Manning (2004), among others. This PTB work focused heavily on the representation in the Treebank, evaluating against patterns in the trace annotation. In this paper we have tried to be more “formalismindependent” and construction focused. 2 2.1 in object position has apparently been extracted: the paper which I wrote. Our corpus includes cases where the extracted word is (semantically) the object of a preposition in the verb phrase: the agency that I applied to. Object extraction f"
D09-1085,P04-1041,0,0.125932,"Missing"
D09-1085,P03-1054,0,0.0330272,"However, the DCU post-processor is ideal because it does produce dependencies in a GR format. It has also obtained competitive scores on general GR evaluation corpora (Cahill et al., 2004). Table 3: Distance between head and dependent. subject embedded construction: the same stump which had impaled the car of many a guest in the past thirty years and which he refused to have removed. 3.2 The parsers The parsers that we chose to evaluate are the C & C CCG parser (Clark and Curran, 2007), the Enju HPSG parser (Miyao and Tsujii, 2005), the RASP parser (Briscoe et al., 2006), the Stanford parser (Klein and Manning, 2003), and the DCU postprocessor of PTB parsers (Cahill et al., 2004), based on LFG and applied to the output of the Charniak and Johnson reranking parser. Of course we were unable to evaluate every publicly available parser, but we believe these are representative of current wide-coverage robust parsing technology.1 The C & C parser is based on CCGbank (Hockenmaier and Steedman, 2007), a CCG version of the Penn Treebank. It is ideally suited for this evaluation because CCG was designed to capture the unbounded dependencies being considered. The Enju parser was designed with a similar motivation to"
D09-1085,W08-2102,0,0.0339446,"Missing"
D09-1085,P04-1042,0,0.104635,"argument structure of sentences, and argue for a more focused constructionbased parser evaluation as a complement to existing grammatical relation-based evaluations. We also perform an error-analysis for one of the more successful parsers. There has been some prior work on evaluating parsers on long-range dependencies, but no work we are aware of that has the scope and focus of this paper. Clark et al. (2004) evaluated a CCG parser on a small corpus of object extraction cases. Johnson (2002) began the body of work on inserting traces into the output of Penn Treebank (PTB) parsers, followed by Levy and Manning (2004), among others. This PTB work focused heavily on the representation in the Treebank, evaluating against patterns in the trace annotation. In this paper we have tried to be more “formalismindependent” and construction focused. 2 2.1 in object position has apparently been extracted: the paper which I wrote. Our corpus includes cases where the extracted word is (semantically) the object of a preposition in the verb phrase: the agency that I applied to. Object extraction from a reduced relative clause is essentially the same, except that there is no overt relative pronoun: the paper I wrote; the a"
D09-1085,N06-1020,0,0.109981,"Missing"
D09-1085,P05-1022,0,0.186351,"Missing"
D09-1085,A00-2018,0,0.148425,"he dependencies in our corpus; for example, the tagsequence grammar has no explicit representation of verb subcategorisation, and so may not know that there is a missing object in the case of extraction from a relative clause (though it does recover some of these dependencies). However, RASP is a popular parser used in a number of applications, and it returns dependencies in a suitable format for evaluation, and so we considered it to be an appropriate and useful member of our parser set. The Stanford parser is representative of a large number of PTB parsers, exemplified by Collins (1997) and Charniak (2000). The Parseval scores reported for the Stanford parser are not the highest in the literature, but are competitive enough for our purposes. The advantage of the Stanford parser is that it returns dependencies in a suitable format for our evaluation. The dependencies are obtained by a set of manually defined rules operating over the phrase-structure trees returned by the parser (de Marneffe et al., 2006). Like RASP, the Stanford parser has not been designed to capture unbounded dependencies; in particular it does not make use of any of the trace information in the PTB. However, we wanted to incl"
D09-1085,P05-1012,0,0.0354573,"bounded dependencies being considered. The Enju parser was designed with a similar motivation to C & C, and is also based on an automatically extracted grammar derived from the PTB, but the grammar formalism is HPSG rather than CCG . Both parsers produce head-word dependencies reflecting the underlying predicate-argument structure of a sentence, and so in theory should be straightforward to evaluate. The RASP parser is based on a manually constructed POS tag-sequence grammar, with a statistical parse selection component and a robust 3.3 1 One obvious omission is any form of dependency parser (McDonald et al., 2005; Nivre and Scholz, 2004). However, the dependencies returned by these parsers are local, and it would be non-trivial to infer from a series of links whether a long-range dependency had been correctly represented. Also, dependency parsers are not significantly better at recovering head-based dependencies than constituent parsers based on the PTB (McDonald et al., 2005). Parser evaluation The parsers were run essentially out-of-the-box when parsing the test sentences. The one exception was C & C, which required some minor adjusting of parameters, as described in the parser documentation, to obt"
D09-1085,J07-4004,1,0.953553,"representation directly, such as Johnson (2002), is difficult for us to evaluate because it does not produce explicit dependencies. However, the DCU post-processor is ideal because it does produce dependencies in a GR format. It has also obtained competitive scores on general GR evaluation corpora (Cahill et al., 2004). Table 3: Distance between head and dependent. subject embedded construction: the same stump which had impaled the car of many a guest in the past thirty years and which he refused to have removed. 3.2 The parsers The parsers that we chose to evaluate are the C & C CCG parser (Clark and Curran, 2007), the Enju HPSG parser (Miyao and Tsujii, 2005), the RASP parser (Briscoe et al., 2006), the Stanford parser (Klein and Manning, 2003), and the DCU postprocessor of PTB parsers (Cahill et al., 2004), based on LFG and applied to the output of the Charniak and Johnson reranking parser. Of course we were unable to evaluate every publicly available parser, but we believe these are representative of current wide-coverage robust parsing technology.1 The C & C parser is based on CCGbank (Hockenmaier and Steedman, 2007), a CCG version of the Penn Treebank. It is ideally suited for this evaluation beca"
D09-1085,P05-1011,0,0.136149,"), is difficult for us to evaluate because it does not produce explicit dependencies. However, the DCU post-processor is ideal because it does produce dependencies in a GR format. It has also obtained competitive scores on general GR evaluation corpora (Cahill et al., 2004). Table 3: Distance between head and dependent. subject embedded construction: the same stump which had impaled the car of many a guest in the past thirty years and which he refused to have removed. 3.2 The parsers The parsers that we chose to evaluate are the C & C CCG parser (Clark and Curran, 2007), the Enju HPSG parser (Miyao and Tsujii, 2005), the RASP parser (Briscoe et al., 2006), the Stanford parser (Klein and Manning, 2003), and the DCU postprocessor of PTB parsers (Cahill et al., 2004), based on LFG and applied to the output of the Charniak and Johnson reranking parser. Of course we were unable to evaluate every publicly available parser, but we believe these are representative of current wide-coverage robust parsing technology.1 The C & C parser is based on CCGbank (Hockenmaier and Steedman, 2007), a CCG version of the Penn Treebank. It is ideally suited for this evaluation because CCG was designed to capture the unbounded d"
D09-1085,W04-3215,1,0.872505,"cause of different conventions across the parsers as to how the underlying grammar is represented. Fourth, we show that current parsing technology is very poor at representing some important elements of the argument structure of sentences, and argue for a more focused constructionbased parser evaluation as a complement to existing grammatical relation-based evaluations. We also perform an error-analysis for one of the more successful parsers. There has been some prior work on evaluating parsers on long-range dependencies, but no work we are aware of that has the scope and focus of this paper. Clark et al. (2004) evaluated a CCG parser on a small corpus of object extraction cases. Johnson (2002) began the body of work on inserting traces into the output of Penn Treebank (PTB) parsers, followed by Levy and Manning (2004), among others. This PTB work focused heavily on the representation in the Treebank, evaluating against patterns in the trace annotation. In this paper we have tried to be more “formalismindependent” and construction focused. 2 2.1 in object position has apparently been extracted: the paper which I wrote. Our corpus includes cases where the extracted word is (semantically) the object of"
D09-1085,C04-1010,0,0.0309107,"ing considered. The Enju parser was designed with a similar motivation to C & C, and is also based on an automatically extracted grammar derived from the PTB, but the grammar formalism is HPSG rather than CCG . Both parsers produce head-word dependencies reflecting the underlying predicate-argument structure of a sentence, and so in theory should be straightforward to evaluate. The RASP parser is based on a manually constructed POS tag-sequence grammar, with a statistical parse selection component and a robust 3.3 1 One obvious omission is any form of dependency parser (McDonald et al., 2005; Nivre and Scholz, 2004). However, the dependencies returned by these parsers are local, and it would be non-trivial to infer from a series of links whether a long-range dependency had been correctly represented. Also, dependency parsers are not significantly better at recovering head-based dependencies than constituent parsers based on the PTB (McDonald et al., 2005). Parser evaluation The parsers were run essentially out-of-the-box when parsing the test sentences. The one exception was C & C, which required some minor adjusting of parameters, as described in the parser documentation, to obtain close to full coverag"
D09-1085,N07-1051,0,0.0681442,"Missing"
D09-1085,P97-1003,0,0.154492,"o capture many of the dependencies in our corpus; for example, the tagsequence grammar has no explicit representation of verb subcategorisation, and so may not know that there is a missing object in the case of extraction from a relative clause (though it does recover some of these dependencies). However, RASP is a popular parser used in a number of applications, and it returns dependencies in a suitable format for evaluation, and so we considered it to be an appropriate and useful member of our parser set. The Stanford parser is representative of a large number of PTB parsers, exemplified by Collins (1997) and Charniak (2000). The Parseval scores reported for the Stanford parser are not the highest in the literature, but are competitive enough for our purposes. The advantage of the Stanford parser is that it returns dependencies in a suitable format for our evaluation. The dependencies are obtained by a set of manually defined rules operating over the phrase-structure trees returned by the parser (de Marneffe et al., 2006). Like RASP, the Stanford parser has not been designed to capture unbounded dependencies; in particular it does not make use of any of the trace information in the PTB. Howeve"
D09-1085,D08-1050,1,0.903466,"clause is a so-called small clause, i.e. one with a null copula verb: the plan that she considered foolish, where plan is the semantic subject of foolish. 2.2 With the exception of the question construction, all sentences were taken from the PTB, with roughly half from the WSJ sections (excluding 2-21 which provided the training data for many The data The corpus consists of approximately 100 sentences for each of the seven constructions; 80 of 815 of the parsers in our set) and half from Brown (roughly balanced across the different sections). The questions were taken from the question data in Rimell and Clark (2008), which was obtained from various years of the TREC QA track. We chose to use the PTB as the main source because the use of traces in the PTB annotation provides a starting point for the identification of unbounded dependencies. Sentences were selected for the corpus by a combination of automatic and manual processes. A regular expression applied to PTB trees, searching for appropriate traces for a particular construction, was first used to extract a set of candidate sentences. All candidates were manually reviewed and, if selected, annotated with one or more grammatical relations representing"
D09-1085,J03-4003,0,\N,Missing
D10-1082,N06-1022,0,0.0154052,"of partially built structures for each character, which can be seen as a dynamic programming chart which is greatly reduced by pruning. In this paper we follow the line of single-model research, in particular the global linear model of Z&C08. We show that effective decoding can be achieved with standard beam-search, which gives significant speed improvements compared to the decoding algorithm of Z&C08, and achieves accuracies that are competitive with the state-of-the-art. Our research is also in line with recent research on improving the speed of NLP systems with little or no accuracy loss (Charniak et al., 2006; Roark and Hollingshead, 2008). Our speed improvement is achieved by the use of a single-beam decoder. Given an input sentence, candidate outputs are built incrementally, one character at a time. When each character is processed, it is combined with existing candidates in all possible ways to generate new candidates, and an agenda is used to keep the N -best candidate outputs from 843 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 843–852, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics the begining of"
D10-1082,P04-1015,0,0.768721,"word. Before a word is confirmed as a full word, we only apply to it features that represent its current partial status, such as character bigrams, its starting character and its part-ofspeech, etc. Full word features, including the first and last characters of a word, are applied immediately after a word is confirmed as complete. An important component for our proposed system is the training process, which needs to ensure that the model scores a partial word with predicted POS properly. We use the averaged perceptron (Collins, 2002) for training, together with the “early update” mechanism of Collins and Roark (2004). Rather than updating the parameters after decoding is complete, the modified algorithm updates parameters at any processing step if the correct partial candidate falls out of the beam. In our experiments using the Chinese Treebank 1 The next incoming characters are also a useful source of information for predicting the POS. However, our system achieved competitive accuracy with Z&C08 without such character lookahead features. 845 data, our system ran an order of magnitude faster than our Z&C08 system with little loss of accuracy. The accuracy of our system was competitive with other recent m"
D10-1082,W02-1001,0,0.816261,"l the next incoming character is separated from the last character of this word. Before a word is confirmed as a full word, we only apply to it features that represent its current partial status, such as character bigrams, its starting character and its part-ofspeech, etc. Full word features, including the first and last characters of a word, are applied immediately after a word is confirmed as complete. An important component for our proposed system is the training process, which needs to ensure that the model scores a partial word with predicted POS properly. We use the averaged perceptron (Collins, 2002) for training, together with the “early update” mechanism of Collins and Roark (2004). Rather than updating the parameters after decoding is complete, the modified algorithm updates parameters at any processing step if the correct partial candidate falls out of the beam. In our experiments using the Chinese Treebank 1 The next incoming characters are also a useful source of information for predicting the POS. However, our system achieved competitive accuracy with Z&C08 without such character lookahead features. 845 data, our system ran an order of magnitude faster than our Z&C08 system with li"
D10-1082,P08-1102,0,0.66486,"Missing"
D10-1082,C08-1049,0,0.762106,"t partial candidate as well as a full output. Effective scoring of partial structures allows the decoder to give high accuracy with a small beam-size of 16. In our 10-fold crossvalidation experiments with the Chinese Treebank, our system performed over 10 times as fast as Zhang and Clark (2008) with little accuracy loss. The accuracy of our system on the standard CTB 5 test was competitive with the best in the literature. 1 Introduction and Motivation Several approaches have been proposed to solve word segmentation and POS-tagging jointly, including the reranking approach (Shi and Wang, 2007; Jiang et al., 2008b), the hybrid approach (Nakagawa and Uchimoto, 2007; Jiang et al., 2008a), and the single-model approach (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009). These methods led to accuracy improvements over the traditional, pipelined segmentation and POS-tagging baseline by avoiding segmentation error propagation and making use of part-of-speech information to improve segmentation. The single-model approach to joint segmentation and POS-tagging offers consistent training of all information, concerning words, characters and partsof-speech. However, exact inference with dynamic pr"
D10-1082,P09-1058,0,0.765266,"-fold crossvalidation experiments with the Chinese Treebank, our system performed over 10 times as fast as Zhang and Clark (2008) with little accuracy loss. The accuracy of our system on the standard CTB 5 test was competitive with the best in the literature. 1 Introduction and Motivation Several approaches have been proposed to solve word segmentation and POS-tagging jointly, including the reranking approach (Shi and Wang, 2007; Jiang et al., 2008b), the hybrid approach (Nakagawa and Uchimoto, 2007; Jiang et al., 2008a), and the single-model approach (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009). These methods led to accuracy improvements over the traditional, pipelined segmentation and POS-tagging baseline by avoiding segmentation error propagation and making use of part-of-speech information to improve segmentation. The single-model approach to joint segmentation and POS-tagging offers consistent training of all information, concerning words, characters and partsof-speech. However, exact inference with dynamic programming can be infeasible if features are defined over a large enough range of the output, such as over a two-word history. In our previous work (Zhang and Clark, 2008),"
D10-1082,P07-2055,0,0.539688,"t. Effective scoring of partial structures allows the decoder to give high accuracy with a small beam-size of 16. In our 10-fold crossvalidation experiments with the Chinese Treebank, our system performed over 10 times as fast as Zhang and Clark (2008) with little accuracy loss. The accuracy of our system on the standard CTB 5 test was competitive with the best in the literature. 1 Introduction and Motivation Several approaches have been proposed to solve word segmentation and POS-tagging jointly, including the reranking approach (Shi and Wang, 2007; Jiang et al., 2008b), the hybrid approach (Nakagawa and Uchimoto, 2007; Jiang et al., 2008a), and the single-model approach (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009). These methods led to accuracy improvements over the traditional, pipelined segmentation and POS-tagging baseline by avoiding segmentation error propagation and making use of part-of-speech information to improve segmentation. The single-model approach to joint segmentation and POS-tagging offers consistent training of all information, concerning words, characters and partsof-speech. However, exact inference with dynamic programming can be infeasible if features are defined"
D10-1082,W04-3236,0,0.49528,"y with a small beam-size of 16. In our 10-fold crossvalidation experiments with the Chinese Treebank, our system performed over 10 times as fast as Zhang and Clark (2008) with little accuracy loss. The accuracy of our system on the standard CTB 5 test was competitive with the best in the literature. 1 Introduction and Motivation Several approaches have been proposed to solve word segmentation and POS-tagging jointly, including the reranking approach (Shi and Wang, 2007; Jiang et al., 2008b), the hybrid approach (Nakagawa and Uchimoto, 2007; Jiang et al., 2008a), and the single-model approach (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009). These methods led to accuracy improvements over the traditional, pipelined segmentation and POS-tagging baseline by avoiding segmentation error propagation and making use of part-of-speech information to improve segmentation. The single-model approach to joint segmentation and POS-tagging offers consistent training of all information, concerning words, characters and partsof-speech. However, exact inference with dynamic programming can be infeasible if features are defined over a large enough range of the output, such as over a two-word histor"
D10-1082,C08-1094,0,0.0139416,"uctures for each character, which can be seen as a dynamic programming chart which is greatly reduced by pruning. In this paper we follow the line of single-model research, in particular the global linear model of Z&C08. We show that effective decoding can be achieved with standard beam-search, which gives significant speed improvements compared to the decoding algorithm of Z&C08, and achieves accuracies that are competitive with the state-of-the-art. Our research is also in line with recent research on improving the speed of NLP systems with little or no accuracy loss (Charniak et al., 2006; Roark and Hollingshead, 2008). Our speed improvement is achieved by the use of a single-beam decoder. Given an input sentence, candidate outputs are built incrementally, one character at a time. When each character is processed, it is combined with existing candidates in all possible ways to generate new candidates, and an agenda is used to keep the N -best candidate outputs from 843 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 843–852, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics the begining of the sentence to the current ch"
D10-1082,P07-1106,1,0.772654,"ing methods. Kruengkrai et al. (2009) and Zhang and Clark (2008) are the most similar to our system among related work. Both systems use a discriminatively trained linear model to score candidate outputs. The work of Kruengkrai et al. (2009) is based on Nakagawa and Uchimoto (2007), which separates the processing of known words and unknown words, and uses a set of segmentation tags to represent the segmentation of characters. In contrast, our model is conceptually simpler, and does not differentiate known words and unknown words. Moreover, our model is based on our previous work, in line with Zhang and Clark (2007), which does not treat word segmentation as character sequence labeling. Our learning and decoding algorithms are also different from Kruengkrai et al. (2009). While Kruengkrai et al. (2009) perform dynamic programming and MIRA learning, we use beam-search to perform incremental decoding, and the early-update version of the perceptron algorithm to train the model. Dynamic programming is exact inference, for which the time complexity is decided by the locality of feature templates. In contrast, beam-search is approximate and can run in linear time. The parameter updating for our algorithm is co"
D10-1082,P08-1101,1,0.880027,"rd features so that feature templates can be instantiated incrementally, according to whether the current character is separated or appended; (2) deciding the POS-tag of a potential word when its first character is processed. Early-update is used with perceptron training so that the linear model gives a high score to a correct partial candidate as well as a full output. Effective scoring of partial structures allows the decoder to give high accuracy with a small beam-size of 16. In our 10-fold crossvalidation experiments with the Chinese Treebank, our system performed over 10 times as fast as Zhang and Clark (2008) with little accuracy loss. The accuracy of our system on the standard CTB 5 test was competitive with the best in the literature. 1 Introduction and Motivation Several approaches have been proposed to solve word segmentation and POS-tagging jointly, including the reranking approach (Shi and Wang, 2007; Jiang et al., 2008b), the hybrid approach (Nakagawa and Uchimoto, 2007; Jiang et al., 2008a), and the single-model approach (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009). These methods led to accuracy improvements over the traditional, pipelined segmentation and POS-tagging"
D10-1116,N10-1084,1,0.342487,"ication, a linguistic stegosystem should allow sufficient embedding capacity, known as the payload. There is a fundamental tradeoff between imperceptibility and payload, since any attempt to embed more information via changes to the cover text increases the chance of introducing anomalies into the text and therefore raising the suspicion of an observer. A linguistic transformation is required to embed information. Transformations studied in previous work include lexical substitution (Chapman and Davida, 1997; Bolshakov, 2004; Taskiran et al., 2006; Topkara et al., 2006b), phrase paraphrasing (Chang and Clark, 2010), sentence structure manipulations (Atallah et al., 2001a; Atallah et al., 2001b; 1194 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1194–1203, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics Liu et al., 2005; Meral et al., 2007; Murphy, 2001; Murphy and Vogel, 2007; Topkara et al., 2006a) and semantic transformations (Atallah et al., 2002; Vybornova and Macq, 2007). Many of these transformations require some sophisticated NLP tools; for example, in order to perform semantic transformations on text, wo"
D10-1116,D09-1129,0,0.0963096,"d ‘over’ or the phrase ‘all over’. Table 1 shows the statistics of synsets used in our stegosystem. # of synsets # of entries average set size max set size noun 16,079 30,933 2.56 25 verb 4,529 6,495 2.79 16 adj 6,655 14,151 2.72 21 adv 964 2,025 2.51 8 Table 1: Statistics of synsets used in our stegosystem For the contextual check we use the Google Web 1T 5-gram Corpus (Brants and Franz, 2006) which contains counts for n-grams from unigrams through to five-grams obtained from over 1 trillion word tokens of English Web text. The corpus has been used for many tasks such as spelling correction (Islam and Inkpen, 2009; Carlson et al., 2008) and multi-word expression classification (Kummerfeld and Curran, 2008). Moreover, for the SemEval-2007 English Lexical Substitution Task, which is similar to our substitution task, six out of ten participating teams utilised the Web 1T corpus. 3.2 Synonym Checking Method In order to measure the degree of acceptability in a substitution, the proposed filter calculates a substitution score for a synonym by using the observed frequency counts in the Web n-gram corpus. The method first extracts contextual n-grams around the synonym and queries the n-gram frequency counts fr"
D10-1116,U08-1008,0,0.146703,"Missing"
D10-1116,S07-1009,0,0.164128,"Missing"
D11-1106,W00-1401,0,0.0252535,"is used to control the volume of accepted hypotheses, so that only a very small portion of the whole search space is explored. The search algorithm is guided by perceptron training, which ensures that the explored path in the search space consists of highly probable hypotheses. This framework of best-first search guided by learning is a general contribution of the paper, which could be applied to problems outside grammaticality improvement. We evaluate our system using the generation task of word-order recovery, which is to recover the original word order of a fully scrambled input sentence (Bangalore et al., 2000; Wan et al., 2009). This problem is an instance of our general task formulation, but without any input constraints, or content word selection (since all input words are used). It is straightforward to use this task to evaluate our system and compare with existing approaches. Our system gave 40.1 BLEU score, higher than the dependency-based system of Wan et al. (2009), for which a BLEU score of 33.7 was reported. Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1147–1157, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computationa"
D11-1106,C10-1009,0,0.245301,"Missing"
D11-1106,J98-2004,0,0.0321789,"all that can be constructed via selecting and ordering a subset of words from the input multiset. The complexity of this problem is much higher than a typical parsing problem, since there is an exponential number of word choices for the output sentence, each 1148 with a factorial number of orderings. Moreover, dynamic programming packing for parsers, such as a CKY chart, is not applicable, because of the lack of a fixed word order. We perform approximate search using a bestfirst algorithm. Starting from single words, candidate parses are constructed bottom-up. Similar to a best-first parser (Caraballo and Charniak, 1998), the highest scored hypothesis is expanded first. A hypothesis is expanded by applying CCG unary rules to the hypothesis, or by combining the hypothesis with existing hypotheses using CCG binary rules. We use beam search to control the number of accepted hypotheses, so that the computational complexity of expanding each hypothesis is linear in the size of the beam. Since there is no guarantee that a goal hypothesis will be found in polynomial time, we apply a robustness mechanism (Riezler et al., 2002; White, 2004), and construct a default output when no goal hypothesis is found within a time"
D11-1106,J07-4004,1,0.78207,"based system of Wan et al. (2009), for which a BLEU score of 33.7 was reported. Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1147–1157, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics 2 The Grammar Combinatory Categorial Grammar (CCG; Steedman (2000)) is a lexicalized grammar formalism, which associates words with lexical categories. Lexical categories are detailed grammatical labels, typically expressing subcategorisation information. CCG, and parsing with CCG, has been described in detail elsewhere (Clark and Curran, 2007; Hockenmaier, 2003); here we provide only a short description. During CCG parsing, adjacent categories are combined using CCG’s combinatory rules. For example, a verb phrase in English (S NP ) can combine with an NP to its left: NP S NP ⇒ S In addition to binary rule instances, such as the one above, there are also unary rules which operate on a single category in order to change its type. For example, forward type-raising can change a subject NP into a complex category looking to the right for a verb phrase: NP ⇒ S /(S NP ) Following Hockenmaier (2003), we extract the grammar by reading r"
D11-1106,W02-1001,0,0.00347054,"+ lmost POS + rmost POS the binary rule the binary rule + head word rule + head word + non-head word bigrams resulting from combination POS bigrams resulting from combi. word trigrams resulting from combi. POS trigrams resulting from combi. resulting lexical categary trigrams resulting word + POS bigrams resulting POS + word bigrams resulting POS + word + POS trigrams unary rule unary rule + headw Table 1: Feature template definitions. edge is recovered. We use the perceptron (Rosenblatt, 1958) to perform parameter updates. The traditional perceptron has been adapted to structural prediction (Collins, 2002) and search optimization problems (Daum´e III and Marcu, 2005; Shen et al., 2007). Our training algorithm can be viewed as an adaptation of the perceptron to our best-first framework for search efficiency and accuracy. We choose to update parameters as soon as the best edge from the agenda is not a gold-standard edge. The intuition is that all gold edges are forced to be above all non-gold edges on the agenda. This is a strong precondition for parameter updates. An alternative is to update when a gold-standard edge falls off the chart, which corresponds to the precondition for parameter update"
D11-1106,P08-1022,0,0.0177719,"Missing"
D11-1106,N10-1115,0,0.017723,"Missing"
D11-1106,J07-3004,0,0.0988866,"iption. During CCG parsing, adjacent categories are combined using CCG’s combinatory rules. For example, a verb phrase in English (S NP ) can combine with an NP to its left: NP S NP ⇒ S In addition to binary rule instances, such as the one above, there are also unary rules which operate on a single category in order to change its type. For example, forward type-raising can change a subject NP into a complex category looking to the right for a verb phrase: NP ⇒ S /(S NP ) Following Hockenmaier (2003), we extract the grammar by reading rule instances directly from the derivations in CCGbank (Hockenmaier and Steedman, 2007), rather than defining the combinatory rule schema manually as in Clark and Curran (2007). 3 The Search Algorithm The input to the search algorithm is a set of words, each word having a count that specifies the maximum number of times it can appear in the output. Typically, most input words can occur only once in the output. However, punctuation marks and function words can be given a higher count. Depending on the fluency of the base output (e.g. the output of the base SMT system), some constraints can be given to specific input words, limiting their order or identifying them as an atomic phr"
D11-1106,P03-1046,0,0.0347058,"l. (2009), for which a BLEU score of 33.7 was reported. Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1147–1157, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics 2 The Grammar Combinatory Categorial Grammar (CCG; Steedman (2000)) is a lexicalized grammar formalism, which associates words with lexical categories. Lexical categories are detailed grammatical labels, typically expressing subcategorisation information. CCG, and parsing with CCG, has been described in detail elsewhere (Clark and Curran, 2007; Hockenmaier, 2003); here we provide only a short description. During CCG parsing, adjacent categories are combined using CCG’s combinatory rules. For example, a verb phrase in English (S NP ) can combine with an NP to its left: NP S NP ⇒ S In addition to binary rule instances, such as the one above, there are also unary rules which operate on a single category in order to change its type. For example, forward type-raising can change a subject NP into a complex category looking to the right for a verb phrase: NP ⇒ S /(S NP ) Following Hockenmaier (2003), we extract the grammar by reading rule instances direct"
D11-1106,2007.mtsummit-ucnlg.1,0,0.202786,"yntax-based algorithm based on CCG. The goal of the search problem is to find an optimal parse tree among all that can be constructed through selection and ordering of the input words. The search problem, which is significantly harder than parsing, is solved by guided learning for best-first search. In a standard word ordering task, our system gives a BLEU score of 40.1, higher than the previous result of 33.7 achieved by a dependency-based system. 1 Introduction Machine-produced text, such as SMT output, often lacks grammaticality and fluency, especially when using n-gram language modelling (Knight, 2007). Recent efforts have been made to improve grammaticality using local language models (Blackwood et al., 2010) and global dependency structures (Wan et al., 2009). We study grammaticality improvement using a syntax-based system. The task is effectively a text-to-text generation problem where the goal is to produce a grammatical sentence from an ungrammatical and fragmentary input. The input can range from a bag-ofwords (Wan et al., 2009) to a fully-ordered sentence (Blackwood et al., 2010). A general form of the problem is to construct a grammatical sentence from a set of un-ordered input word"
D11-1106,J10-4005,0,0.0101328,"ce most input words typically occur once, they can be indexed and represented by a bitvector, which allows a constant time input check. The few multiple-occurrence words are stored in a count array. In the best-first process, edges to be expanded are ordered by their scores, and stored in an agenda. Edges that have been expanded are stored in a chart. There are many ways in which edges could be ordered and compared. Here the chart is organised as a set of beams, each containing a fixed number of edges with a particular size. This is similar to typical decoding algorithms for phrase-based SMT (Koehn, 2010). In each beam, edges are ordered by their scores, and low score edges are pruned. In addition to pruning by the beam, only the highest scored edge is kept among all that share the same signature. 3.2 The Search Process Figure 1 shows pseudocode for the search algorithm. During initialization, the agenda (a) and chart (c) are cleared. All candidate lexical categories are assigned to each input word, and the resulting leaf edges are put onto the agenda. In the main loop, the best edge (e) is popped from the agenda. If e is a goal hypothesis, it is appended to a list of goals (goal), and the loo"
D11-1106,P02-1040,0,0.101893,"as the model. In our experiments, N is chosen according to results on development data. 6 Experiments We use CCGBank (Hockenmaier and Steedman, 2007) for experimental data. CCGbank is the CCG version of the Penn Treebank. Sections 02–21 are used for training, section 00 is used for development and section 23 for the final test. Original sentences from CCGBank are transformed into bags of words, with sequence information removed, and passed to our system as input data. The system outputs are compared to the original sentences for evaluation. Following Wan et al. (2009), we use the BLEU metric (Papineni et al., 2002) for string comparison. Whilst BLEU is not an ideal measure of fluency or grammaticality, being based on n-gram precision, it is currently widely used for automatic evaluation and allows us to compare directly with existing work (Wan et al., 2009). In addition to the surface string, our system also produces the CCG parse given an input bag of words. The quality of the parse tree can reflect both the grammaticality of the surface string and the quality of the trained grammar model. However, there is no direct way to automatically evaluate parse trees since output word choice and order can be di"
D11-1106,P02-1035,0,0.0119772,"words, candidate parses are constructed bottom-up. Similar to a best-first parser (Caraballo and Charniak, 1998), the highest scored hypothesis is expanded first. A hypothesis is expanded by applying CCG unary rules to the hypothesis, or by combining the hypothesis with existing hypotheses using CCG binary rules. We use beam search to control the number of accepted hypotheses, so that the computational complexity of expanding each hypothesis is linear in the size of the beam. Since there is no guarantee that a goal hypothesis will be found in polynomial time, we apply a robustness mechanism (Riezler et al., 2002; White, 2004), and construct a default output when no goal hypothesis is found within a time limit. 3.1 Data Structures Edges are the basic structures that represent hypotheses. Each edge is a CCG constituent, spanning a sequence of words. Similar to partial parses in a typical chart parser, edges have recursive structures. Depending on the number of subedges, edges can be classified into leaf edges, unary edges and binary edges. Leaf edges, which represent input words, are constructed first in the search process. Existing edges are expanded to generate new edges via unary and binary CCG rule"
D11-1106,D08-1052,0,0.14834,"Missing"
D11-1106,P07-1096,0,0.401666,"ad word + non-head word bigrams resulting from combination POS bigrams resulting from combi. word trigrams resulting from combi. POS trigrams resulting from combi. resulting lexical categary trigrams resulting word + POS bigrams resulting POS + word bigrams resulting POS + word + POS trigrams unary rule unary rule + headw Table 1: Feature template definitions. edge is recovered. We use the perceptron (Rosenblatt, 1958) to perform parameter updates. The traditional perceptron has been adapted to structural prediction (Collins, 2002) and search optimization problems (Daum´e III and Marcu, 2005; Shen et al., 2007). Our training algorithm can be viewed as an adaptation of the perceptron to our best-first framework for search efficiency and accuracy. We choose to update parameters as soon as the best edge from the agenda is not a gold-standard edge. The intuition is that all gold edges are forced to be above all non-gold edges on the agenda. This is a strong precondition for parameter updates. An alternative is to update when a gold-standard edge falls off the chart, which corresponds to the precondition for parameter updates of Daum´e III and Marcu (2005). However, due to the complexity of our search ta"
D11-1106,E09-1097,0,0.514386,"dering of the input words. The search problem, which is significantly harder than parsing, is solved by guided learning for best-first search. In a standard word ordering task, our system gives a BLEU score of 40.1, higher than the previous result of 33.7 achieved by a dependency-based system. 1 Introduction Machine-produced text, such as SMT output, often lacks grammaticality and fluency, especially when using n-gram language modelling (Knight, 2007). Recent efforts have been made to improve grammaticality using local language models (Blackwood et al., 2010) and global dependency structures (Wan et al., 2009). We study grammaticality improvement using a syntax-based system. The task is effectively a text-to-text generation problem where the goal is to produce a grammatical sentence from an ungrammatical and fragmentary input. The input can range from a bag-ofwords (Wan et al., 2009) to a fully-ordered sentence (Blackwood et al., 2010). A general form of the problem is to construct a grammatical sentence from a set of un-ordered input words. However, in cases where the base system produces fluent subsequences within the sentence, constraints on the choice and 1147 Stephen Clark University of Cambri"
D11-1106,D09-1043,0,0.217373,"Missing"
D13-1147,W03-1809,0,0.0106348,"fashion, and by more closely examining the impact of light verbs for the compositionality of a phrase. 5 Related Work The past decade has seen extensive work on computational and statistical methods in detecting the compositionality of MWEs (Lin, 1999; Schone and Jurafsky, 2001; Katz and Giesbrecht, 2006; Sporleder and Li, 2009; Biemann and Giesbrecht, 2011). Many of these methods rely on distributional models and vector space models (Sch¨utze, 1993; Turney and Pantel, 2010; Erk, 2012). Work has been done on different types of phrases, including work on particle verbs (McCarthy et al., 2003; Bannard et al., 2003), verb-noun collocations (Venkatapathy and Joshi, 2005; McCarthy et al., 2007), adjectivenoun combinations (Vecchi et al., 2011) and nounnoun compounds (Reddy et al., 2011), as well as on languages other than English (Schulte im Walde et al., 2013). Recent developments in distributional compositional models (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Socher et al., 2011) have opened up a number of possibilities for constructing vectors for phrases, which have also been applied to compositionality tests (Giesbrecht, 2009; Kochmar and Briscoe, 201"
D13-1147,D10-1115,0,0.27911,"of verb-noun pairs: 84 phrases contain pronouns, while there are also several examples containing words that WordNet considers to be adjectives rather than nouns. This problem was mitigated by part-of-speech tagging the dataset. As neighbours for pronouns (which are not included in WordNet), we used the other pronouns present in the dataset. For the remaining words, we included the part-of-speech when looking up the word in WordNet. 3.1 Average distance compositionality score We considered several different ways of constructing phrasal vectors. We chose not to use the compositional models of Baroni and Zamparelli (2010) and Socher et al. (2011) because we believe that it is important that our methods are completely unsupervised and do not require any initial learning phase. 1429 Hence, we experimented with different ways of constructing phrasal vectors according to Mitchell and Lapata (2010) and found that pointwise multiplication worked best in our experiments. Thus, we −−−−→ define the composed vector eat hat as: −→ −→ eat hat We can now compute a compositionality score sc by averaging the distance between the original phrase vector and its substituted neighbour phrase vectors via the following formula: k"
D13-1147,W06-1203,0,0.0168707,"al. (2011) weighted the contribution of individual constituents in such a way that the modifier’s contribution is included but is weighted less highly than the head’s contribution, which led to an improvement in performance. Our results might be improved by weighting the contribution of constituent words in a similar fashion, and by more closely examining the impact of light verbs for the compositionality of a phrase. 5 Related Work The past decade has seen extensive work on computational and statistical methods in detecting the compositionality of MWEs (Lin, 1999; Schone and Jurafsky, 2001; Katz and Giesbrecht, 2006; Sporleder and Li, 2009; Biemann and Giesbrecht, 2011). Many of these methods rely on distributional models and vector space models (Sch¨utze, 1993; Turney and Pantel, 2010; Erk, 2012). Work has been done on different types of phrases, including work on particle verbs (McCarthy et al., 2003; Bannard et al., 2003), verb-noun collocations (Venkatapathy and Joshi, 2005; McCarthy et al., 2007), adjectivenoun combinations (Vecchi et al., 2011) and nounnoun compounds (Reddy et al., 2011), as well as on languages other than English (Schulte im Walde et al., 2013). Recent developments in distribution"
D13-1147,P99-1041,0,0.038644,"ty scores for compound nouns: Reddy et al. (2011) weighted the contribution of individual constituents in such a way that the modifier’s contribution is included but is weighted less highly than the head’s contribution, which led to an improvement in performance. Our results might be improved by weighting the contribution of constituent words in a similar fashion, and by more closely examining the impact of light verbs for the compositionality of a phrase. 5 Related Work The past decade has seen extensive work on computational and statistical methods in detecting the compositionality of MWEs (Lin, 1999; Schone and Jurafsky, 2001; Katz and Giesbrecht, 2006; Sporleder and Li, 2009; Biemann and Giesbrecht, 2011). Many of these methods rely on distributional models and vector space models (Sch¨utze, 1993; Turney and Pantel, 2010; Erk, 2012). Work has been done on different types of phrases, including work on particle verbs (McCarthy et al., 2003; Bannard et al., 2003), verb-noun collocations (Venkatapathy and Joshi, 2005; McCarthy et al., 2007), adjectivenoun combinations (Vecchi et al., 2011) and nounnoun compounds (Reddy et al., 2011), as well as on languages other than English (Schulte im Wa"
D13-1147,W03-1810,0,0.764549,"Missing"
D13-1147,D07-1039,0,0.875809,"ble 2. Substituting the verb in the non-compositional phrase yields semantically anomalous vectors, which leads to very small changes in the distance between it and the original phrase vector. This is a result of using pointwise multiplication, where overlapping components are stressed: since the vectors for take and breath have little overlap outside of Neighbour get breath find breath use breath work breath hold breath run breath carry breath look breath play breath buy breath AvgDist Dist 0.049 0.051 0.050 0.060 0.094 0.079 0.076 0.065 0.071 0.100 0.069 System Venkatapathy and Joshi (2005) McCarthy et al. (2007) AvgDist VSM neighbours-both AvgDist VSM neighbours-verb AvgDist VSM neighbours-noun AvgDist WN-ranked neighbours-both AvgDist WN-ranked neighbours-verb AvgDist WN-ranked neighbours-noun Table 3: Spearman ρs results Table 1: Example take breath Neighbour pay money put money bring money provide money owe money sell money cost money look money distribute money offer money AvgDist Dist 0.446 0.432 0.405 0.442 0.559 0.404 0.482 0.425 0.544 0.428 0.457 Table 2: Example lend money the idiomatic sense in take breath, its neighboursubstituted phrases also have little overlap, resulting in a smaller ch"
D13-1147,I11-1024,0,0.278924,"ct this not to be the case, since e.g. eat trousers, where the noun has been substituted, does not make a lot of sense either — which we would expect to be informative for determining compositionality. There are two possible explanations for this, which might be at play simultaneously: since our dataset consists of verbobject pairs, the verb constituent is always the head word of the phrase, and the dataset contains several so-called “light verbs”, which have little semantic content of their own. Head words have been found to have a higher impact on compositionality scores for compound nouns: Reddy et al. (2011) weighted the contribution of individual constituents in such a way that the modifier’s contribution is included but is weighted less highly than the head’s contribution, which led to an improvement in performance. Our results might be improved by weighting the contribution of constituent words in a similar fashion, and by more closely examining the impact of light verbs for the compositionality of a phrase. 5 Related Work The past decade has seen extensive work on computational and statistical methods in detecting the compositionality of MWEs (Lin, 1999; Schone and Jurafsky, 2001; Katz and Gi"
D13-1147,W01-0513,0,0.0227808,"or compound nouns: Reddy et al. (2011) weighted the contribution of individual constituents in such a way that the modifier’s contribution is included but is weighted less highly than the head’s contribution, which led to an improvement in performance. Our results might be improved by weighting the contribution of constituent words in a similar fashion, and by more closely examining the impact of light verbs for the compositionality of a phrase. 5 Related Work The past decade has seen extensive work on computational and statistical methods in detecting the compositionality of MWEs (Lin, 1999; Schone and Jurafsky, 2001; Katz and Giesbrecht, 2006; Sporleder and Li, 2009; Biemann and Giesbrecht, 2011). Many of these methods rely on distributional models and vector space models (Sch¨utze, 1993; Turney and Pantel, 2010; Erk, 2012). Work has been done on different types of phrases, including work on particle verbs (McCarthy et al., 2003; Bannard et al., 2003), verb-noun collocations (Venkatapathy and Joshi, 2005; McCarthy et al., 2007), adjectivenoun combinations (Vecchi et al., 2011) and nounnoun compounds (Reddy et al., 2011), as well as on languages other than English (Schulte im Walde et al., 2013). Recent d"
D13-1147,W11-0806,0,0.0270309,"en.clark@cl.cam.ac.uk 1. consume her hat 2. eat her trousers Introduction Multi-word expressions (MWEs) are defined as “idiosyncratic interpretations that cross word boundaries” (Sag et al., 2002). They tend to have a standard syntactic structure but are often semantically non-compositional; i.e. their meaning is not fully determined by their syntactic structure and the meanings of their constituents. A classic example is kick the bucket, which means to die rather than to hit a bucket with the foot. These types of expressions account for a large proportion of day-to-day language interactions (Schuler and Joshi, 2011) and present a significant problem for natural language processing systems (Sag et al., 2002). This paper presents a novel unsupervised approach to detecting the compositionality of MWEs, specifically of verb-noun collocations. The idea is Both phrases are semantically anomalous, implying that eat hat is a highly non-compositional verb-noun collocation. Following a similar procedure for eat apple, however, would not lead to an anomaly: consume apple and eat pear are perfectly meaningful, leading us to believe that eat apple is compositional. In the context of distributional models, this idea c"
D13-1147,S13-1038,0,0.0751707,"Missing"
D13-1147,E09-1086,0,0.0153543,"ntribution of individual constituents in such a way that the modifier’s contribution is included but is weighted less highly than the head’s contribution, which led to an improvement in performance. Our results might be improved by weighting the contribution of constituent words in a similar fashion, and by more closely examining the impact of light verbs for the compositionality of a phrase. 5 Related Work The past decade has seen extensive work on computational and statistical methods in detecting the compositionality of MWEs (Lin, 1999; Schone and Jurafsky, 2001; Katz and Giesbrecht, 2006; Sporleder and Li, 2009; Biemann and Giesbrecht, 2011). Many of these methods rely on distributional models and vector space models (Sch¨utze, 1993; Turney and Pantel, 2010; Erk, 2012). Work has been done on different types of phrases, including work on particle verbs (McCarthy et al., 2003; Bannard et al., 2003), verb-noun collocations (Venkatapathy and Joshi, 2005; McCarthy et al., 2007), adjectivenoun combinations (Vecchi et al., 2011) and nounnoun compounds (Reddy et al., 2011), as well as on languages other than English (Schulte im Walde et al., 2013). Recent developments in distributional compositional models"
D13-1147,W11-1301,0,0.0148097,"ecade has seen extensive work on computational and statistical methods in detecting the compositionality of MWEs (Lin, 1999; Schone and Jurafsky, 2001; Katz and Giesbrecht, 2006; Sporleder and Li, 2009; Biemann and Giesbrecht, 2011). Many of these methods rely on distributional models and vector space models (Sch¨utze, 1993; Turney and Pantel, 2010; Erk, 2012). Work has been done on different types of phrases, including work on particle verbs (McCarthy et al., 2003; Bannard et al., 2003), verb-noun collocations (Venkatapathy and Joshi, 2005; McCarthy et al., 2007), adjectivenoun combinations (Vecchi et al., 2011) and nounnoun compounds (Reddy et al., 2011), as well as on languages other than English (Schulte im Walde et al., 2013). Recent developments in distributional compositional models (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010; Socher et al., 2011) have opened up a number of possibilities for constructing vectors for phrases, which have also been applied to compositionality tests (Giesbrecht, 2009; Kochmar and Briscoe, 2013). This paper takes that work a step further: by constructing phrase vectors and evaluating these vectors on a dataset of human"
D13-1147,H05-1113,0,0.823443,"e, are shown in Table 1 and Table 2. Substituting the verb in the non-compositional phrase yields semantically anomalous vectors, which leads to very small changes in the distance between it and the original phrase vector. This is a result of using pointwise multiplication, where overlapping components are stressed: since the vectors for take and breath have little overlap outside of Neighbour get breath find breath use breath work breath hold breath run breath carry breath look breath play breath buy breath AvgDist Dist 0.049 0.051 0.050 0.060 0.094 0.079 0.076 0.065 0.071 0.100 0.069 System Venkatapathy and Joshi (2005) McCarthy et al. (2007) AvgDist VSM neighbours-both AvgDist VSM neighbours-verb AvgDist VSM neighbours-noun AvgDist WN-ranked neighbours-both AvgDist WN-ranked neighbours-verb AvgDist WN-ranked neighbours-noun Table 3: Spearman ρs results Table 1: Example take breath Neighbour pay money put money bring money provide money owe money sell money cost money look money distribute money offer money AvgDist Dist 0.446 0.432 0.405 0.442 0.559 0.404 0.482 0.425 0.544 0.428 0.457 Table 2: Example lend money the idiomatic sense in take breath, its neighboursubstituted phrases also have little overlap, re"
D13-1147,R13-1047,0,\N,Missing
D13-1147,W11-1300,0,\N,Missing
D14-1111,D10-1115,0,0.322199,"al arrays, or tensors). An obstacle to full implementation of the framework is the size of these tensors. We examine the performance of lower dimensional approximations of transitive verb tensors on a sentence plausibility/selectional preference task. We find that the matrices perform as well as, and sometimes even better than, full tensors, allowing a reduction in the number of parameters needed to model the framework. 1 Introduction An emerging subfield of computational linguistics is concerned with learning compositional distributional representations of meaning (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Clarke, 2012; Socher et al., 2012; Clark, 2013). The advantage of such representations lies in their potential to combine the benefits of distributional approachs to word meaning (Sch¨utze, 1998; Turney and Pantel, 2010) with the more traditional compositional methods from formal semantics (Dowty et al., 1981). Distributional representations have the properties of robustness, learnability from data, ease of handling ambiguity, and the ability to represent gradations of meaning; whereas compositional models handle the unbounded nature of"
D14-1111,P10-1046,0,0.0501282,"mples were extracted from the GSN corpus. More precisely, we collected all distinct syntactic trigrams of the form nsubj ROOT dobj, where the root of the phrase was one of our target verbs. We lemmatised the words using the NLTK4 lemmatiser and filtered these examples to retain only the ones that contain nouns that also occur in Wikipedia, obtaining the counts reported in Table 2. For every positive training example, we constructed a negative (implausible) one by replacing both the subject and the object with a confounder, using a standard technique from the selectional preference literature (Chambers and Jurafsky, 2010). A confounder was generated by choosing a random noun from the same frequency bucket as the original noun.5 Frequency buckets of size 10 were constructed by collecting noun frequency counts from the Wikipedia corpus. For ex4 http://nltk.org/ Note that the random selection of the confounder could result in a plausible negative example by chance, but manual inspection of a subset of the data suggests this happens infrequently for those verbs which select strongly for their arguments, but more often for those verbs that don’t. 1041 5 Verb Tensor APPLY CENSOR COMB DEPOSE EAT IDEALIZE INCUBATE JUS"
D14-1111,J07-4004,1,0.831221,"lication (N /N N ⇒ N ), which is an instance of function application. Function application is also used in the tensor-based semantics, which, for a matrix and vector argument, corresponds to matrix multiplication. Figure 1 shows how the syntactic types combine with a transitive verb, and the corresponding tensor-based semantic types. Note that, after the verb has combined with its object NP , the type of the verb phrase is S NP , with a corresponding meaning tensor (matrix) in S ⊗ N. This matrix then combines with the subject vector, through 2 In practice, for example using the CCG parser of Clark and Curran (2007), there will be additional atomic categories, such as PP , but not many more. 1037 people NP N eat fish V Θ (S NP )/NP NP S⊗N⊗N N S NP S⊗N S S 2Mat 4K 8 SKMat 2K 4 KKMat K2 0 DMat K2 0 > Table 1: Number of parameters per method. < Figure 1: Syntactic reduction and tensor-based semantic types for a transitive verb sentence matrix multiplication, to give a sentence vector. In practice, using for example the widecoverage grammar from CCGbank (Hockenmaier and Steedman, 2007), there will be many types with more than 3 slashes, with corresponding higher-order tensors. For example, a common categor"
D14-1111,J02-2003,1,0.607679,"80 150 300 600 800 1000 2000 # Training Examples Figure 4: Comparison of DMat, Tensor, and 2Mat methods as the number of training instances increases. logic examples. Here, we go beyond this by learning tensors using corpus data and by deriving several different matrix representations for the verb in the subject-verb-object (SVO) sentence. This work can also be thought of as applying neural network learning techniques to the classic problem of selectional preference acquisition, since the design of the pseudo-disambiguation experiments is taken from the literature on selectional preferences (Clark and Weir, 2002; Chambers and Jurafsky, 2010). We do not compare directly with methods from this literature, e.g. those based on WordNet (Resnik, 1996; Clark and Weir, 2002) or topic modelling techniques (Seaghdha, 2010), since our goal in this paper is not to extend the state-of-the-art in that area, but rather to use selectional preference acquisition as a test bed for the tensor-based semantic framework. 7 Conclusion In this paper we introduced three dimensionally reduced representations of the transitive verb tensor defined in the type-driven framework for compositional distributional semantics (Coecke e"
D14-1111,J12-1002,0,0.0563467,"ize of these tensors. We examine the performance of lower dimensional approximations of transitive verb tensors on a sentence plausibility/selectional preference task. We find that the matrices perform as well as, and sometimes even better than, full tensors, allowing a reduction in the number of parameters needed to model the framework. 1 Introduction An emerging subfield of computational linguistics is concerned with learning compositional distributional representations of meaning (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Clarke, 2012; Socher et al., 2012; Clark, 2013). The advantage of such representations lies in their potential to combine the benefits of distributional approachs to word meaning (Sch¨utze, 1998; Turney and Pantel, 2010) with the more traditional compositional methods from formal semantics (Dowty et al., 1981). Distributional representations have the properties of robustness, learnability from data, ease of handling ambiguity, and the ability to represent gradations of meaning; whereas compositional models handle the unbounded nature of natural language, as well as providing established accounts of logica"
D14-1111,S13-1035,0,0.017072,"t word matrix. We use context selection (with N = 140) and row normalisation as described in Polajnar and Clark (2014) to markedly improve the performance of SVD on smaller dimensions (K) and enable us to train the verb tensors using very low-dimensional 3 http://nlp.stanford.edu/software/index.shtml noun vectors. Performance of the noun vectors was measured on standard word similarity datasets and the results were comparable to those reported by Polajnar and Clark (2014). 4.2 Training data In order to generate training data we made use of two large corpora: the Google Syntactic Ngrams (GSN) (Goldberg and Orwant, 2013) and the Wikipedia October 2013 dump. We first chose ten transitive verbs with different concreteness scores (Brysbaert et al., 2013) and frequencies, in order to obtain a variety of verb types. Then the positive (plausible) SVO examples were extracted from the GSN corpus. More precisely, we collected all distinct syntactic trigrams of the form nsubj ROOT dobj, where the root of the phrase was one of our target verbs. We lemmatised the words using the NLTK4 lemmatiser and filtered these examples to retain only the ones that contain nouns that also occur in Wikipedia, obtaining the counts repor"
D14-1111,D11-1129,0,0.156943,"Missing"
D14-1111,W13-0112,0,0.114966,"Missing"
D14-1111,J07-3004,0,0.0383431,"r (matrix) in S ⊗ N. This matrix then combines with the subject vector, through 2 In practice, for example using the CCG parser of Clark and Curran (2007), there will be additional atomic categories, such as PP , but not many more. 1037 people NP N eat fish V Θ (S NP )/NP NP S⊗N⊗N N S NP S⊗N S S 2Mat 4K 8 SKMat 2K 4 KKMat K2 0 DMat K2 0 > Table 1: Number of parameters per method. < Figure 1: Syntactic reduction and tensor-based semantic types for a transitive verb sentence matrix multiplication, to give a sentence vector. In practice, using for example the widecoverage grammar from CCGbank (Hockenmaier and Steedman, 2007), there will be many types with more than 3 slashes, with corresponding higher-order tensors. For example, a common category for a preposition is the following: ((S NP )(S NP ))/NP , which would be assigned to WITH in eat WITH a fork. (The way to read the syntactic type is as follows: with requires an NP argument to the right – a fork in this example – and then a verb phrase to the left – eat with type S NP – resulting in a verb phrase S NP .) The corresponding meaning tensor lives in the tensor space S ⊗ N ⊗ S ⊗ N ⊗ N, i.e. a 5th-order tensor. Categories with even more slashes are not un"
D14-1111,W13-3201,0,0.132789,"Missing"
D14-1111,W14-1406,1,0.647355,", for example, is represented by a matrix (a 2nd-order tensor)1 and the meaning of a transitive verb is represented by a 3rd-order tensor. Coecke et al. use the grammar of pregroups as the syntactic machinery to construct distributional meaning representations, since both pregroups and vector spaces can be seen as examples of the same abstract structure, which leads to a particularly clean mathematical description of the compositional process. However, the approach applies more generally, for example to other forms of categorial grammar, such as Combinatory Categorial Grammar (Steedman, 2000; Maillard et al., 2014), and also to phrase-structure grammars in a way that a formal linguist would recognize (Baroni et al., 2014). Clark (2013) provides a description of the tensor-based framework aimed more at computational linguists, relying only on the mathematics of multi-linear algebra rather than the category theory used in Coecke et al. (2010). Section 2 repeats some of this description. A major open question associated with the tensor-based semantic framework is how to learn 1 This same insight lies behind the work of Baroni and Zamparelli (2010). 1036 Proceedings of the 2014 Conference on Empirical Metho"
D14-1111,P08-1028,0,0.0971303,"maps (i.e. multi-dimensional arrays, or tensors). An obstacle to full implementation of the framework is the size of these tensors. We examine the performance of lower dimensional approximations of transitive verb tensors on a sentence plausibility/selectional preference task. We find that the matrices perform as well as, and sometimes even better than, full tensors, allowing a reduction in the number of parameters needed to model the framework. 1 Introduction An emerging subfield of computational linguistics is concerned with learning compositional distributional representations of meaning (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Clarke, 2012; Socher et al., 2012; Clark, 2013). The advantage of such representations lies in their potential to combine the benefits of distributional approachs to word meaning (Sch¨utze, 1998; Turney and Pantel, 2010) with the more traditional compositional methods from formal semantics (Dowty et al., 1981). Distributional representations have the properties of robustness, learnability from data, ease of handling ambiguity, and the ability to represent gradations of meaning; whereas compositional models ha"
D14-1111,E14-1025,1,0.741848,"ran, 2004), where fwi cj is the number of times target noun wi occurs with context word cj : tT est(w ~i , cj ) = p(wi , cj ) − p(wi )p(cj ) p p(wi )p(cj ) P j fwi cj P P where p(wi ) = , p(cj ) k l fwk cl fw i c j and p(wi , cj ) = P P fw c . k l k l = (3) P i fwi cj P P , k l fwk cl Using all 10,000 context words would result in a large number of parameters for each verb tensor, and so we apply singular value decomposition (SVD) (Turney and Pantel, 2010) with 40 latent dimensions to the target-context word matrix. We use context selection (with N = 140) and row normalisation as described in Polajnar and Clark (2014) to markedly improve the performance of SVD on smaller dimensions (K) and enable us to train the verb tensors using very low-dimensional 3 http://nlp.stanford.edu/software/index.shtml noun vectors. Performance of the noun vectors was measured on standard word similarity datasets and the results were comparable to those reported by Polajnar and Clark (2014). 4.2 Training data In order to generate training data we made use of two large corpora: the Google Syntactic Ngrams (GSN) (Goldberg and Orwant, 2013) and the Wikipedia October 2013 dump. We first chose ten transitive verbs with different con"
D14-1111,J98-1004,0,0.736458,"Missing"
D14-1111,P10-1045,0,0.0146973,"s using corpus data and by deriving several different matrix representations for the verb in the subject-verb-object (SVO) sentence. This work can also be thought of as applying neural network learning techniques to the classic problem of selectional preference acquisition, since the design of the pseudo-disambiguation experiments is taken from the literature on selectional preferences (Clark and Weir, 2002; Chambers and Jurafsky, 2010). We do not compare directly with methods from this literature, e.g. those based on WordNet (Resnik, 1996; Clark and Weir, 2002) or topic modelling techniques (Seaghdha, 2010), since our goal in this paper is not to extend the state-of-the-art in that area, but rather to use selectional preference acquisition as a test bed for the tensor-based semantic framework. 7 Conclusion In this paper we introduced three dimensionally reduced representations of the transitive verb tensor defined in the type-driven framework for compositional distributional semantics (Coecke et al., 2010). In a comprehensive experiment on ten different verbs we find no significant difference between the full tensor representation and the reduced representations. The SKMat and 2Mat representatio"
D14-1111,D12-1110,0,0.644778,"ensors. We examine the performance of lower dimensional approximations of transitive verb tensors on a sentence plausibility/selectional preference task. We find that the matrices perform as well as, and sometimes even better than, full tensors, allowing a reduction in the number of parameters needed to model the framework. 1 Introduction An emerging subfield of computational linguistics is concerned with learning compositional distributional representations of meaning (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Clarke, 2012; Socher et al., 2012; Clark, 2013). The advantage of such representations lies in their potential to combine the benefits of distributional approachs to word meaning (Sch¨utze, 1998; Turney and Pantel, 2010) with the more traditional compositional methods from formal semantics (Dowty et al., 1981). Distributional representations have the properties of robustness, learnability from data, ease of handling ambiguity, and the ability to represent gradations of meaning; whereas compositional models handle the unbounded nature of natural language, as well as providing established accounts of logical words, quantificati"
D14-1111,D13-1170,0,0.00790297,"e between 0 and 1, or as a two-dimensional probability distribution over the classes plausible (>) and implausible (⊥). Whether we consider a one- or two-dimensional sentence space depends on the architecture of the logistic regression classifier that is used to learn the verb (Section 3). We begin with this simple plausibility sentence space to determine if, in fact, the tensor-based representation can be learned to a sufficiently useful degree. Other simple sentence spaces which can perhaps be represented using one or two variables include a “sentence space” for the sentiment analysis task (Socher et al., 2013), where one variable represents positive sentiment and the other negative. We also expect that the insights gained from research on this task can be applied to more complex sentence spaces, for example a semantic similarity space which will require more than two variables. 2 Syntactic Types to Tensors The syntactic type of a transitive verb in English is (S NP )/NP (using notation from Steedman (2000)), meaning that a transitive verb is a function which takes an NP argument to the right, an NP argument to the left, and results in a sentence S . Such categories with slashes are complex categor"
D14-1111,C12-1165,0,0.0226685,"Missing"
D14-1111,2014.lilt-9.5,0,\N,Missing
D15-1015,N09-1003,0,0.0221124,"l similarity metrics on a relatedness (MEN) and a genuine similarity (SimLex-999) dataset. aggregated visual representation-based metrics of CNN-M EAN and CNN-M AX, despite the fact that Kiela and Bottou (2014) achieved optimal performance using the latter metrics on a well-known conceptual relatedness dataset. It has been noted before that there is a clear distinction between similarity and relatedness. This is one of the reasons that, for example, WordSim353 (Finkelstein et al., 2002) has been criticized: it gives high similarity scores to cases of genuine similarity as well as relatedness (Agirre et al., 2009; Hill et al., 2014). The MEN dataset (Bruni et al., 2014) that Kiela and Bottou (2014) evaluate on explicitly measures word relatedness. In contrast, the current lexicon learning task seems to require something else than relatedness: whilst a chair and table are semantically related, a translation for chair is not a good translation for table. For example, we want to make sure we translate chair to stuhl in German, and not to tisch. In other words, what we are interIt is clear that the per-image similarity metrics perform better on genuine similarity, as measured by SimLex-999, than on relate"
D15-1015,P98-1069,0,0.0757273,"aning across different languages. It plays an important role in a variety of tasks in information retrieval and natural language processing, including cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005) and statistical machine translation (Och and Ney, 2003). Although parallel corpora have been used successfully for inducing bilingual lexicons for some languages (Och and Ney, 2003), these corpora are either too small or unavailable for many language pairs. Consequently, mono-lingual approaches that rely on comparable instead of parallel corpora have been developed (Fung and Yee, 1998; Koehn and Knight, 2002). These approaches work by mapping language pairs to a shared bilingual space and ex148 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 148–158, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. parallel data, and the induced lexicons were typically a by-product of word alignment models (Och and Ney, 2003). To alleviate the issue of low coverage, a large body of work has been dedicated to lexicon learning from more abundant and less restricted comparable data, e.g., (Fung and Yee, 199"
D15-1015,P04-1067,0,0.0822964,"Missing"
D15-1015,R11-1055,0,0.188877,"Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other examples of multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2014) or auto-encoders (Wu et al., 2013; Silberer and Lapata, 2014). 3.1 Image Representations We use Google Images to extract the top n ranked images for each lexical item in the evaluation datasets. It has been shown that images from Google yield higher quality representations than comparable sources such as Flickr (Bergsma and Goebel, 2011) and that Google-derived datasets are competitive with “hand prepared datasets” (Fergus et al., 2005). Google Images also has the advantage that it has full coverage and is multi-lingual, as opposed to other potential image sources such as ImageNet (Deng et al., 2009) or the ESP Game Dataset (von Ahn and Dabbish, 2004). For each Google search we specify the target language corresponding to the lexical item’s language. Figure 2 gives some example images retrieved using the same query terms in different languages. For each image, we extract the presoftmax layer of an AlexNet (Krizhevsky et al.,"
D15-1015,P08-1088,0,0.297869,"when they are grounded in the visual modality (Silberer and Lapata, 2012; Bruni et al., 2014). Having an intermediate visual space means that words in different languages can be grounded in the same space. Third, it is natural to use vision as an intermediate: when we communicate with someone who does not speak our language, we often communicate by directly referring to our surroundings. Languages that are linguistically far apart will, by cognitive necessity, still refer to objects in the same visual space. While some approaches to bilingual lexicon induction rely on orthographic properties (Haghighi et al., 2008; Koehn and Knight, 2002) or properties of frequency distributions (Schafer and Yarowsky, 2002) that will work only for Introduction Bilingual lexicon induction is the task of finding words that share a common meaning across different languages. It plays an important role in a variety of tasks in information retrieval and natural language processing, including cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005) and statistical machine translation (Och and Ney, 2003). Although parallel corpora have been used successfully for inducing bilingual lexicons for some langu"
D15-1015,D14-1032,0,0.0322433,"ulti-Modal Semantics Multi-modal semantics is motivated by parallels with human concept acquisition. It has been found that semantic knowledge, from a very early age, relies heavily on perceptual information (Louwerse, 2008), and there exists substantial evidence that many concepts are grounded in the perceptual system (Barsalou, 2008). One way to accomplish such grounding is by combining linguistic representations with information from a perceptual modality, obtained from, e.g., property norming experiments (Silberer and Lapata, 2012; Silberer et al., 2013; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or extracting features from raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (F"
D15-1015,D09-1092,0,0.0138313,"n induction models to learn translations from comparable data (see sect. 3.3). We do not necessarily expect visual methods to outperform linguistic ones, but it is instructive to see the comparison. We compare our visual models against the current state-of-the-art lexicon induction model using comparable data (Vuli´c and Moens, 2013b). This model induces translations from comparable Wikipedia data in two steps: (1) It learns a set of highly reliable one-to-one translation pairs using a shared bilingual space obtained by applying the multilingual probabilistic topic modeling (MuPTM) framework (Mimno et al., 2009). (2) These highly reliable one-to-one translation pairs serve as dimensions of a word-based bilingual semantic space (Gaussier et al., 2004; Tamura et al., 2012). The model then bootstraps from the high-precision seed lexicon of translations and learns new dimensions of the bilingual space until convergence. This model, which we call B OOTS TRAP, obtains the current best results on the evaluation dataset. For more details about the bootstrapping model and its comparison against other approaches, we refer to Vuli´c and Moens (2013b). Table 4 shows the results for the language pairs in the V UL"
D15-1015,D14-1005,1,0.742838,"orming experiments (Silberer and Lapata, 2012; Silberer et al., 2013; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or extracting features from raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other examples of multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2014) or auto-encoders (Wu et al., 2013; Silberer and Lapata, 2014). 3.1 Image Representations We use Google Images to extract the top n ranked images for each lexical item in the evaluation datasets. It has been shown that images from Google yield higher quality representations than comparable sou"
D15-1015,P14-2135,1,0.898971,"edge, from a very early age, relies heavily on perceptual information (Louwerse, 2008), and there exists substantial evidence that many concepts are grounded in the perceptual system (Barsalou, 2008). One way to accomplish such grounding is by combining linguistic representations with information from a perceptual modality, obtained from, e.g., property norming experiments (Silberer and Lapata, 2012; Silberer et al., 2013; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or extracting features from raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other examples of multi-modal deep learning use"
D15-1015,J03-1002,0,0.00373644,"isual space. While some approaches to bilingual lexicon induction rely on orthographic properties (Haghighi et al., 2008; Koehn and Knight, 2002) or properties of frequency distributions (Schafer and Yarowsky, 2002) that will work only for Introduction Bilingual lexicon induction is the task of finding words that share a common meaning across different languages. It plays an important role in a variety of tasks in information retrieval and natural language processing, including cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005) and statistical machine translation (Och and Ney, 2003). Although parallel corpora have been used successfully for inducing bilingual lexicons for some languages (Och and Ney, 2003), these corpora are either too small or unavailable for many language pairs. Consequently, mono-lingual approaches that rely on comparable instead of parallel corpora have been developed (Fung and Yee, 1998; Koehn and Knight, 2002). These approaches work by mapping language pairs to a shared bilingual space and ex148 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 148–158, c Lisbon, Portugal, 17-21 September 2015. 2015 Assoc"
D15-1015,W02-0902,0,0.108859,"in the visual modality (Silberer and Lapata, 2012; Bruni et al., 2014). Having an intermediate visual space means that words in different languages can be grounded in the same space. Third, it is natural to use vision as an intermediate: when we communicate with someone who does not speak our language, we often communicate by directly referring to our surroundings. Languages that are linguistically far apart will, by cognitive necessity, still refer to objects in the same visual space. While some approaches to bilingual lexicon induction rely on orthographic properties (Haghighi et al., 2008; Koehn and Knight, 2002) or properties of frequency distributions (Schafer and Yarowsky, 2002) that will work only for Introduction Bilingual lexicon induction is the task of finding words that share a common meaning across different languages. It plays an important role in a variety of tasks in information retrieval and natural language processing, including cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005) and statistical machine translation (Och and Ney, 2003). Although parallel corpora have been used successfully for inducing bilingual lexicons for some languages (Och and Ney, 2003),"
D15-1015,P99-1067,0,0.0295644,"oehn and Knight, 2002). These approaches work by mapping language pairs to a shared bilingual space and ex148 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 148–158, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. parallel data, and the induced lexicons were typically a by-product of word alignment models (Och and Ney, 2003). To alleviate the issue of low coverage, a large body of work has been dedicated to lexicon learning from more abundant and less restricted comparable data, e.g., (Fung and Yee, 1998; Rapp, 1999; Gaussier et al., 2004; Shezaf and Rappoport, 2010; Tamura et al., 2012). However, these models typically rely on the availability of bilingual seed lexicons to produce shared bilingual spaces, as well as large repositories of comparable data. Therefore, several approaches attempt to learn lexicons from large monolingual data sets in two languages (Koehn and Knight, 2002; Haghighi et al., 2008), but their performance again relies on language pair-dependent clues such as orthographic similarity. A further approach removed the requirement of seed lexicons, and induced lexicons using bilingual s"
D15-1015,J99-4009,0,0.872954,"n down by language, are shown in Table 6. B ERGSMA 500 has a lower average image dispersion score in general, and thus is more concrete than V ULIC 1000. It also has less variance. This may explain why we score higher, in absolute terms, on that dataset than on the more abstract one. When examining individual languages in the datasets, we note that the worst performing language on V ULIC 1000 is Italian, which is also the most abstract dataset, with the highest average image dispersion score and the lowest variance. There is some evidence that abstract concepts are also perceptually grounded (Lakoff and Johnson, 1999), but in a more complex way, since abstract concepts express more varied situations (Barsalou and Wiemer-Hastings, 2005). Using an image resource like Google Images that has full coverage for almost any word, means that we can retrieve what we might call “associated” images (such as images of voters for words like democracy) as opposed to “extensional” images (such as images of cats for cat). This explains why we still obtain good performance on the more abstract V ULIC 1000 dataset, in some cases outperforming linguistic methods: even abstract concepts can have a clear visual representation,"
D15-1015,P14-1132,0,0.0141507,"ata (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other examples of multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2014) or auto-encoders (Wu et al., 2013; Silberer and Lapata, 2014). 3.1 Image Representations We use Google Images to extract the top n ranked images for each lexical item in the evaluation datasets. It has been shown that images from Google yield higher quality representations than comparable sources such as Flickr (Bergsma and Goebel, 2011) and that Google-derived datasets are competitive with “hand prepared datasets” (Fergus et al., 2005). Google Images also h"
D15-1015,D13-1115,0,0.258685,"Missing"
D15-1015,I11-1162,0,0.0481252,"isition. It has been found that semantic knowledge, from a very early age, relies heavily on perceptual information (Louwerse, 2008), and there exists substantial evidence that many concepts are grounded in the perceptual system (Barsalou, 2008). One way to accomplish such grounding is by combining linguistic representations with information from a perceptual modality, obtained from, e.g., property norming experiments (Silberer and Lapata, 2012; Silberer et al., 2013; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or extracting features from raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other e"
D15-1015,W02-2026,0,0.0492709,"014). Having an intermediate visual space means that words in different languages can be grounded in the same space. Third, it is natural to use vision as an intermediate: when we communicate with someone who does not speak our language, we often communicate by directly referring to our surroundings. Languages that are linguistically far apart will, by cognitive necessity, still refer to objects in the same visual space. While some approaches to bilingual lexicon induction rely on orthographic properties (Haghighi et al., 2008; Koehn and Knight, 2002) or properties of frequency distributions (Schafer and Yarowsky, 2002) that will work only for Introduction Bilingual lexicon induction is the task of finding words that share a common meaning across different languages. It plays an important role in a variety of tasks in information retrieval and natural language processing, including cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005) and statistical machine translation (Och and Ney, 2003). Although parallel corpora have been used successfully for inducing bilingual lexicons for some languages (Och and Ney, 2003), these corpora are either too small or unavailable for many language p"
D15-1015,P10-1011,0,0.0134096,"aches work by mapping language pairs to a shared bilingual space and ex148 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 148–158, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. parallel data, and the induced lexicons were typically a by-product of word alignment models (Och and Ney, 2003). To alleviate the issue of low coverage, a large body of work has been dedicated to lexicon learning from more abundant and less restricted comparable data, e.g., (Fung and Yee, 1998; Rapp, 1999; Gaussier et al., 2004; Shezaf and Rappoport, 2010; Tamura et al., 2012). However, these models typically rely on the availability of bilingual seed lexicons to produce shared bilingual spaces, as well as large repositories of comparable data. Therefore, several approaches attempt to learn lexicons from large monolingual data sets in two languages (Koehn and Knight, 2002; Haghighi et al., 2008), but their performance again relies on language pair-dependent clues such as orthographic similarity. A further approach removed the requirement of seed lexicons, and induced lexicons using bilingual spaces spanned by multilingual probabilistic topic m"
D15-1015,lin-etal-2010-new,0,0.0150856,"ambiamento (change) in Italian. Using the two evaluation datasets can potentially provide Evaluations Test Sets. Bergsma and Van Durme’s primary evaluation dataset consists of a set of five hundred matching lexical items for fifteen language pairs, based on six languages. (The fifteen pairs results from all ways of pairing six languages). The data is publicly available online.1 In order to get the five hundred lexical items, they first rank nouns by the conditional probability of them occurring in the pattern “{image,photo,photograph,picture} of {a,an} ” in the web-scale Google N-gram corpus (Lin et al., 2010), and take the top five hundred words as their English lexicon. For each item 1 1 n 2 http://www.clsp.jhu.edu/˜sbergsma/LexImg/ 151 http://people.cs.kuleuven.be/˜ivan.vulic/software/ Figure 2: Example images for the languages in the Bergsma and Van Durme dataset. Method P@1 P@5 P@20 MRR B&VD Visual-Only B&VD Visual + NED 31.1 48.0 41.4 59.5 53.7 68.7 0.367 0.536 CNN-AVG M AX CNN-M AX M AX CNN-M EAN CNN-M AX 56.7 42.8 50.5 51.4 69.2 60.0 62.7 64.9 77.4 64.5 71.1 74.8 0.658 0.529 0.586 0.608 4 We evaluate the four similarity metrics on the B ERGSMA 500 dataset and compare the results to the syst"
D15-1015,D12-1130,0,0.113474,"ith using multiple layers from the same network in an attempt to improve performance. 1 There are several advantages to having a visual rather than a linguistic intermediate bilingual space: First, while labeled images are readily available for many languages through resources such as Google Images, language pairs that have sizeable comparable, let alone parallel, corpora are relatively scarce. Second, it has been found that meaning is often grounded in the perceptual system, and that the quality of semantic representations improves significantly when they are grounded in the visual modality (Silberer and Lapata, 2012; Bruni et al., 2014). Having an intermediate visual space means that words in different languages can be grounded in the same space. Third, it is natural to use vision as an intermediate: when we communicate with someone who does not speak our language, we often communicate by directly referring to our surroundings. Languages that are linguistically far apart will, by cognitive necessity, still refer to objects in the same visual space. While some approaches to bilingual lexicon induction rely on orthographic properties (Haghighi et al., 2008; Koehn and Knight, 2002) or properties of frequenc"
D15-1015,W13-3523,0,0.0364082,"ver, these models typically rely on the availability of bilingual seed lexicons to produce shared bilingual spaces, as well as large repositories of comparable data. Therefore, several approaches attempt to learn lexicons from large monolingual data sets in two languages (Koehn and Knight, 2002; Haghighi et al., 2008), but their performance again relies on language pair-dependent clues such as orthographic similarity. A further approach removed the requirement of seed lexicons, and induced lexicons using bilingual spaces spanned by multilingual probabilistic topic models (Vuli´c et al., 2011; Liu et al., 2013; Vuli´c and Moens, 2013b). However, these models require document alignments as initial bilingual signals. In this work, following recent research in multi-modal semantics and image representation learning—in particular deep learning and convolutional neural networks—we test the ability of purely visual data to induce shared bilingual spaces and to consequently learn bilingual word correspondences in these spaces. By compiling images related to linguistic concepts given in different languages, the potentially prohibitive data requirements and language pair-dependence from prior work is remove"
D15-1015,P14-1068,0,0.101262,"(Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other examples of multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2014) or auto-encoders (Wu et al., 2013; Silberer and Lapata, 2014). 3.1 Image Representations We use Google Images to extract the top n ranked images for each lexical item in the evaluation datasets. It has been shown that images from Google yield higher quality representations than comparable sources such as Flickr (Bergsma and Goebel, 2011) and that Google-derived datasets are competitive with “hand prepared datasets” (Fergus et al., 2005). Google Images also has the advantage that it has full coverage and is multi-lingual, as opposed to other potential image sources such as ImageNet (Deng et al., 2009) or the ESP Game Dataset (von Ahn and Dabbish, 2004)."
D15-1015,P13-1056,0,0.0195593,"3 A Purely Visual Approach to Bilingual Lexicon Learning Multi-Modal Semantics Multi-modal semantics is motivated by parallels with human concept acquisition. It has been found that semantic knowledge, from a very early age, relies heavily on perceptual information (Louwerse, 2008), and there exists substantial evidence that many concepts are grounded in the perceptual system (Barsalou, 2008). One way to accomplish such grounding is by combining linguistic representations with information from a perceptual modality, obtained from, e.g., property norming experiments (Silberer and Lapata, 2012; Silberer et al., 2013; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or extracting features from raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques"
D15-1015,Q14-1017,0,0.0416793,"ures from raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other examples of multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2014) or auto-encoders (Wu et al., 2013; Silberer and Lapata, 2014). 3.1 Image Representations We use Google Images to extract the top n ranked images for each lexical item in the evaluation datasets. It has been shown that images from Google yield higher quality representations than comparable sources such as Flickr (Bergsma and Goebel, 2011) and that Google-derived datasets are competitive with “hand prepared datasets” (Fergus et al., 200"
D15-1015,D12-1003,0,0.136163,"ge pairs to a shared bilingual space and ex148 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 148–158, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. parallel data, and the induced lexicons were typically a by-product of word alignment models (Och and Ney, 2003). To alleviate the issue of low coverage, a large body of work has been dedicated to lexicon learning from more abundant and less restricted comparable data, e.g., (Fung and Yee, 1998; Rapp, 1999; Gaussier et al., 2004; Shezaf and Rappoport, 2010; Tamura et al., 2012). However, these models typically rely on the availability of bilingual seed lexicons to produce shared bilingual spaces, as well as large repositories of comparable data. Therefore, several approaches attempt to learn lexicons from large monolingual data sets in two languages (Koehn and Knight, 2002; Haghighi et al., 2008), but their performance again relies on language pair-dependent clues such as orthographic similarity. A further approach removed the requirement of seed lexicons, and induced lexicons using bilingual spaces spanned by multilingual probabilistic topic models (Vuli´c et al.,"
D15-1015,N13-1011,1,0.87388,"Missing"
D15-1015,D13-1168,1,0.763446,"Missing"
D15-1015,P11-2084,1,0.863258,"Missing"
D15-1015,N10-1011,0,\N,Missing
D15-1015,J15-4004,0,\N,Missing
D15-1015,C98-1066,0,\N,Missing
D15-1242,N09-1003,0,0.103463,"dness better than unspecialized spaces. We also show that using specialized spaces in NLP tasks and applications leads to clear improvements, for document classification and synonym selection, which rely on either similarity or relatedness but not both. 1 Introduction Most current models of semantic word representation exploit the distributional hypothesis: the idea that words occurring in similar contexts have similar meanings (Harris, 1954; Turney and Pantel, 2010; Clark, 2015). Such representations (or embeddings) can reflect human intuitions about similarity and relatedness (Turney, 2006; Agirre et al., 2009), and have been applied to a wide variety of NLP tasks, including bilingual lexicon induction (Mikolov et al., 2013b), sentiment analysis (Socher et al., 2013) and named entity recognition (Turian et al., 2010; Guo et al., 2014). Arguably, one of the reasons behind the popularity of word embeddings is that they are “general purpose”: they can be used in a variety of tasks without modification. Although this behavior is sometimes desirable, it may in other cases be detrimental to downstream performance. For example, when classifying documents by topic, we are particularly interested in related"
D15-1242,P14-1023,0,0.0819888,"e same objective function that was used to learn the original skip-gram embeddings. In other words, we first train a standard skip-gram model, and then learn from the additional contexts in a second training stage as if they form a separate corpus: t=1 −c≤j≤c u> w ∈Awt We exclude articles with multiple topic labels in order to avoid multi-class document classification. The dataset contains a total of 78 topic labels and 33,226 news articles. We call this approach skip-gram retrofitting. In all cases, our embeddings have 300 dimensions, which has been found to work well (Mikolov et al., 2013a; Baroni et al., 2014) 3 Results for Intrinsic Evaluation We compare standard skip-gram embeddings with retrofitted and jointly learned specialized embeddings, as well as with “fitted” embeddings that were randomly initialized and learned only from the additional semantic resource. In each case, the 2045 Method SimLex-999 MEN Skip-gram 0.31 0.68 Fit-Norms 0.08 0.14 Fit-Thesaurus 0.26 0.14 Joint-Norms-Sampled 0.43 0.72 Joint-Norms-All 0.42 0.67 Joint-Thesaurus-Sampled 0.38 0.69 Joint-Thesaurus-All 0.44 0.60 GB-Retrofit-Norms 0.32 0.71 GB-Retrofit-Thesaurus 0.38 0.68 SG-Retrofit-Norms 0.35 0.71 SG-Retrofit-Thesaurus"
D15-1242,N15-1184,0,0.673336,"well occur in similar contexts. Corpus-driven approaches based on the distributional hypothesis therefore generally learn embeddings that capture both similarity and relatedness reasonably well, but neither perfectly. In this work we demonstrate the advantage of specializing semantic spaces for either similarity or relatedness. Specializing for similarity is achieved by learning from both a corpus and a thesaurus, and for relatedness by learning from both a corpus and a collection of psychological association norms. We also compare the recentlyintroduced technique of graph-based retrofitting (Faruqui et al., 2015) with a skip-gram retrofitting and a skip-gram joint-learning approach. All three methods yield specialized semantic spaces that capture human intuitions regarding similarity and relatedness significantly better than unspecialized spaces, in one case yielding state-of-the-art results for word similarity. More importantly, we show clear improvements in downstream tasks and applications: specialized similarity spaces improve synonym detection, while association spaces work better than both general-purpose and similarityspecialized spaces for document classification. 2 Approach The underlying ass"
D15-1242,W05-0604,0,0.0998654,"zation when encountered early in training (in the joint learning condition) because the model has not acquired the basic concepts on which it builds. However, with retrofitting the model first acquires good representations for frequent words from the raw text, after which it can better understand, and learn from, the information in the thesaurus. 4 4.1 Downstream Tasks and Applications TOEFL Synonym Task Unsupervised synonym selection has many applications including the generation of thesauri and other lexical resources from raw text (Kageura et al., 2000). In the well-known TOEFL evaluation (Freitag et al., 2005) models are required to identify true synonyms to question words from a selection of possible answers. To test our models on this task, for each question in the dataset, we rank the multiple-choice answers according to the cosine similarity between their word embeddings and that of the target word, and choose the highestranked option. 4 Hill et al. (2014a) obtain a score of 0.52 using neural translation embeddings. 2046 Figure 1: Varying the number of iterations when retrofitting Method TOEFL Doc Skip-gram 77.50 83.96 Joint-Norms-Sampled 78.75 84.46 Joint-Norms-All 66.25 84.82 Joint-Thesaurus-"
D15-1242,D14-1012,0,0.0133372,"atedness but not both. 1 Introduction Most current models of semantic word representation exploit the distributional hypothesis: the idea that words occurring in similar contexts have similar meanings (Harris, 1954; Turney and Pantel, 2010; Clark, 2015). Such representations (or embeddings) can reflect human intuitions about similarity and relatedness (Turney, 2006; Agirre et al., 2009), and have been applied to a wide variety of NLP tasks, including bilingual lexicon induction (Mikolov et al., 2013b), sentiment analysis (Socher et al., 2013) and named entity recognition (Turian et al., 2010; Guo et al., 2014). Arguably, one of the reasons behind the popularity of word embeddings is that they are “general purpose”: they can be used in a variety of tasks without modification. Although this behavior is sometimes desirable, it may in other cases be detrimental to downstream performance. For example, when classifying documents by topic, we are particularly interested in related words rather than similar ones: knowing that dog is associated with cat is much more informative of the topic than knowing that it is a synonym of canine. Conversely, if our embeddings indicate that table is closely related to c"
D15-1242,C00-1058,0,0.0445341,"source. Its information may be detrimental to model optimization when encountered early in training (in the joint learning condition) because the model has not acquired the basic concepts on which it builds. However, with retrofitting the model first acquires good representations for frequent words from the raw text, after which it can better understand, and learn from, the information in the thesaurus. 4 4.1 Downstream Tasks and Applications TOEFL Synonym Task Unsupervised synonym selection has many applications including the generation of thesauri and other lexical resources from raw text (Kageura et al., 2000). In the well-known TOEFL evaluation (Freitag et al., 2005) models are required to identify true synonyms to question words from a selection of possible answers. To test our models on this task, for each question in the dataset, we rank the multiple-choice answers according to the cosine similarity between their word embeddings and that of the target word, and choose the highestranked option. 4 Hill et al. (2014a) obtain a score of 0.52 using neural translation embeddings. 2046 Figure 1: Varying the number of iterations when retrofitting Method TOEFL Doc Skip-gram 77.50 83.96 Joint-Norms-Sampl"
D15-1242,D14-1162,0,0.0909535,"Missing"
D15-1242,D13-1170,0,0.00230888,"sification and synonym selection, which rely on either similarity or relatedness but not both. 1 Introduction Most current models of semantic word representation exploit the distributional hypothesis: the idea that words occurring in similar contexts have similar meanings (Harris, 1954; Turney and Pantel, 2010; Clark, 2015). Such representations (or embeddings) can reflect human intuitions about similarity and relatedness (Turney, 2006; Agirre et al., 2009), and have been applied to a wide variety of NLP tasks, including bilingual lexicon induction (Mikolov et al., 2013b), sentiment analysis (Socher et al., 2013) and named entity recognition (Turian et al., 2010; Guo et al., 2014). Arguably, one of the reasons behind the popularity of word embeddings is that they are “general purpose”: they can be used in a variety of tasks without modification. Although this behavior is sometimes desirable, it may in other cases be detrimental to downstream performance. For example, when classifying documents by topic, we are particularly interested in related words rather than similar ones: knowing that dog is associated with cat is much more informative of the topic than knowing that it is a synonym of canine. Conv"
D15-1242,P10-1040,0,0.035032,"her similarity or relatedness but not both. 1 Introduction Most current models of semantic word representation exploit the distributional hypothesis: the idea that words occurring in similar contexts have similar meanings (Harris, 1954; Turney and Pantel, 2010; Clark, 2015). Such representations (or embeddings) can reflect human intuitions about similarity and relatedness (Turney, 2006; Agirre et al., 2009), and have been applied to a wide variety of NLP tasks, including bilingual lexicon induction (Mikolov et al., 2013b), sentiment analysis (Socher et al., 2013) and named entity recognition (Turian et al., 2010; Guo et al., 2014). Arguably, one of the reasons behind the popularity of word embeddings is that they are “general purpose”: they can be used in a variety of tasks without modification. Although this behavior is sometimes desirable, it may in other cases be detrimental to downstream performance. For example, when classifying documents by topic, we are particularly interested in related words rather than similar ones: knowing that dog is associated with cat is much more informative of the topic than knowing that it is a synonym of canine. Conversely, if our embeddings indicate that table is c"
D15-1242,J06-3003,0,0.00916113,"ity and relatedness better than unspecialized spaces. We also show that using specialized spaces in NLP tasks and applications leads to clear improvements, for document classification and synonym selection, which rely on either similarity or relatedness but not both. 1 Introduction Most current models of semantic word representation exploit the distributional hypothesis: the idea that words occurring in similar contexts have similar meanings (Harris, 1954; Turney and Pantel, 2010; Clark, 2015). Such representations (or embeddings) can reflect human intuitions about similarity and relatedness (Turney, 2006; Agirre et al., 2009), and have been applied to a wide variety of NLP tasks, including bilingual lexicon induction (Mikolov et al., 2013b), sentiment analysis (Socher et al., 2013) and named entity recognition (Turian et al., 2010; Guo et al., 2014). Arguably, one of the reasons behind the popularity of word embeddings is that they are “general purpose”: they can be used in a variety of tasks without modification. Although this behavior is sometimes desirable, it may in other cases be detrimental to downstream performance. For example, when classifying documents by topic, we are particularly"
D15-1242,J15-4004,1,\N,Missing
D15-1293,P14-1023,0,0.0562673,"re searching for audio files of cars, we retrieve up to 50 files with a maximum duration of 1 minute per file that have been tagged with the label “car”. 4.1 Linguistic Representations For the linguistic representations we use the continuous vector representations from the log-linear skip-gram model of Mikolov et al. (2013). Specifically, we trained 300-dimensional vector representations trained on a dump of the English Wikipedia plus newswire (8 billion words in total).5 These types of representations have been found to yield the highest performance on a variety of semantic similarity tasks (Baroni et al., 2014). 3 http://www.freesound.org. http://www.vorbis.com. 5 We used the demo-train-big-model-v1.sh script from http://word2vec.googlecode.com to obtain this corpus. 2463 4 4.2 Auditory Representations els in each category. A common approach to obtaining acoustic features of audio files is the Mel-scale Frequency Cepstral Coefficient (MFCC) (O’Shaughnessy, 1987). MFCC features are abundant in a wide variety of applications in audio signal processing, ranging from audio information retrieval, to speech and speaker recognition, and music analysis (Eronen, 2003). Such features are derived from the mel-"
D15-1293,R11-1055,0,0.0770156,"ade publicly available at http://www.cl.cam.ac.uk/˜dk427/audio.html. 2 To avoid introducing another parameter, we set the number of latent variables in the cross-modal PLSR map to a third of the number of dimensions of the perceptual representation. 4 Approach One reason for using raw image data in multimodal models is that there is a wide variety of resources that contain tagged images, such as ImageNet (Deng et al., 2009) and the ESP Game dataset (Von Ahn and Dabbish, 2004). However, such resources do not exist for audio files, and so we follow a similar approach to Fergus et al. (2005) and Bergsma and Goebel (2011), who use Google Images to obtain images. We use the online search engine Freesound3 to obtain audio files. Freesound is a collaborative database released under Creative Commons licenses, in the form of snippets, samples and recordings, that is aimed at sound artists. The Freesound API allows users to easily search for audio files that have been tagged using certain keywords. For each of the concepts in the evaluation datasets, we used the Freesound API to obtain samples encoded in the standard open source OGG format4 . Because the database contains variable numbers of files, with varying dura"
D15-1293,N10-1011,0,0.0208359,"leads to conceptual representations that can be processed and reasoned with? MEN automobile-car 1.00 taxi-cab 0.92 A key observation is that concepts are, through perception, grounded in physical reality and sensorimotor experience (Harnad, 1990; Louwerse, 2008), and there has been a surge of recent work on perceptually grounded semantic models that try to account for this fact. These models learn semantic representations from both textual and perceptual input, using either feature norms (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014) as the source of perceptual information. A popular approach in the latter case is to collect images associated with a concept, and then lay out each image as a set of keypoints on a dense grid, where each keypoint is represented by a robust local feature descriptor such as SIFT (Lowe, 2004). These local descriptors are subsequently clustered into a set of “visual words” using a standard clustering algorithm such as k-means and then quantized into vector representations by comparing the descriptors with the centroids. An alternative to this bag of"
D15-1293,D14-1005,1,0.908705,"popular approach in the latter case is to collect images associated with a concept, and then lay out each image as a set of keypoints on a dense grid, where each keypoint is represented by a robust local feature descriptor such as SIFT (Lowe, 2004). These local descriptors are subsequently clustered into a set of “visual words” using a standard clustering algorithm such as k-means and then quantized into vector representations by comparing the descriptors with the centroids. An alternative to this bag of visual words (BoVW) approach is transferring features from convolutional neural networks (Kiela and Bottou, 2014). rain-storm 0.98 plane-jet 0.81 cat-feline 0.96 horse-mare 0.83 jazz-musician 0.88 sheep-lamb 0.84 bird-eagle 0.88 bird-hawk 0.79 highway-traffic 0.88 band-orchestra 0.71 guitar-piano 0.86 music-melody 0.70 Various ways of aggregating images into visual representations have been proposed, such as taking the mean or the elementwise maximum. Ideally, one would jointly learn multi-modal representations from parallel multi-modal data, such as text containing images (Silberer and Lapata, 2014) or images described with speech (Synnaeve et al., 2014), but such data is hard to obtain, has limited cov"
D15-1293,P14-2135,1,0.917865,"genuine similarity (e.g., teacher-instructor) and the latter focuses more on relatedness (e.g., riverwater). In addition, following previous work in cross-modal semantics, we evaluate on the zeroshot learning task of inducing a cross-modal mapping to the correct label in the auditory modality from the linguistic one and vice-versa. 3.1 Multi-modal Semantics Evidence suggests that the inclusion of visual representations only improves performance for certain concepts, and that in some cases the introduction of visual information is detrimental to performance on similarity and relatedness tasks (Kiela et al., 2014). The same is likely to be true for other perceptual modalities: in the case of comparisons such as guitar-piano, the auditory modal2462 Dataset MEN AMEN SLex ASLex Linguistic 3000 258 999 296 Auditory 2590 233 534 216 ever having heard a guitar; or map it to the appropriate place in linguistic space without ever having read about a guitar (having only heard it). Table 2: Number of concept pairs for which representations are available in each modality. ity is certainly meaningful, whereas in the case of democracy-anarchism it is probably less so. Therefore, we had two graduate students annotat"
D15-1293,P15-2038,1,0.292746,"ng strategies for the early fusion joint-learning approach and to investigate more sophisticated mixing strategies for the middle and late fusion models, e.g. using the “audio dispersion” of a word to determine how much auditory input should be included in the multi-modal representation (Kiela et al., 2014). Another interesting possibility is to improve auditory representations by training a neural network classifier on the audio files and subsequently transferring the hidden representations to tasks in semantics. Lastly, now that the perceptual modalities of vision, audio and even olfaction (Kiela et al., 2015) have been investigated in the context of distributional semantics, the logical next step for future work is to explore different fusion strategies for multi-modal models that combine various sources of perceptual input into a single grounded model. Acknowledgments DK is supported by EPSRC grant EP/I037512/1. SC is supported by ERC Starting Grant DisCoTex (306920) and EPSRC grant EP/I037512/1. We are grateful to Xavier Serra, Frederic Font Corbera, Alessandro Lopopolo and Emiel van Miltenburg 2468 for useful suggestions and thank the anonymous reviewers for their helpful comments. Felix Hill a"
D15-1293,P14-1132,0,0.0876778,". Hence, image representations are often learned independently. Aggregated visual representations are subsequently combined with a traditional linguistic space to form a multi-modal model. This mixing can be done in a variety of ways, ranging from simple concatenation to more sophisticated fusion methods (Bruni et al., 2014). Cross-modal semantics, instead of being concerned with improving semantic representations through grounding, focuses on the problem of reference. Using, for instance, mappings between visual and textual space, the objective is to learn which words refer to which objects (Lazaridou et al., 2014). This problem is very much rescore SimLex-999 score Table 1: Examples of pairs in the datasets where auditory is relevant, with the similarity score. lated to the object recognition task in computer vision, but instead of using just visual data and labels, these cross-modal models also utilize textual information (Socher et al., 2014; Frome et al., 2013). This allows for zero-shot learning, where the model can predict how an object relates to other concepts just from seeing an image of the object, but without ever having previously encountered an image of that particular object (Lazaridou et"
D15-1293,I11-1162,0,0.0860397,"resentations that can be processed and reasoned with? MEN automobile-car 1.00 taxi-cab 0.92 A key observation is that concepts are, through perception, grounded in physical reality and sensorimotor experience (Harnad, 1990; Louwerse, 2008), and there has been a surge of recent work on perceptually grounded semantic models that try to account for this fact. These models learn semantic representations from both textual and perceptual input, using either feature norms (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014) as the source of perceptual information. A popular approach in the latter case is to collect images associated with a concept, and then lay out each image as a set of keypoints on a dense grid, where each keypoint is represented by a robust local feature descriptor such as SIFT (Lowe, 2004). These local descriptors are subsequently clustered into a set of “visual words” using a standard clustering algorithm such as k-means and then quantized into vector representations by comparing the descriptors with the centroids. An alternative to this bag of visual words (BoVW) appro"
D15-1293,W15-0110,0,0.0886942,"Missing"
D15-1293,D13-1115,0,0.262768,"Missing"
D15-1293,D07-1043,0,0.0310278,"We used Wikipedia to collect a total of 52 instruments and divided them into 5 classes: brass, percussion, piano-based, string and woodwind instruments. For each of the instruments, we collected as many audio files from FreeSound as possible, and used the MM - MIDDLE model with parameter settings that yielded good results in the previous experiments (k = 300 and α = 0.6). We then performed k-means clustering with five cluster centroids and compared results between auditory, linguistic and multi-modal, evaluating the clustering quality using the standard V-measure clustering evaluation metric (Rosenberg and Hirschberg, 2007). This is an interesting problem because instrument classes are determined somewhat by conven2467 Model Auditory Linguistic MM - MIDDLE 0.39 0.47 0.54 V-measure Linguistic 1 baritone 2 lute, zither, xylophone, lyre, cymbals 3 piano, trombone, clarinet, cello, violin 4 castanets, tambourine, claves, maracas 5 trumpet, horn, bugle, cowbell, carillon Multi-modal Figure 4: Performance of uni-modal auditory representations on the four datasets when varying the maximum duration. tion (is a saxophone a brass or a woodwind instrument?). What is more, how instruments actually sound is rarely described"
D15-1293,D12-1130,0,0.254234,"for a variety of NLP tasks, the fact that the meaning of a word is represented as a distribution over other words implies that they suffer from the grounding problem (Harnad, 1990); i.e. they do not account for the fact that human semantic knowledge is grounded in the perceptual system (Louwerse, 2008). Motivated by human concept acquisition, multi-modal semantics enhances linguistic representations with extra-linguistic perceptual input. These models outperform language-only models on a range of tasks, including modelling semantic similarity and relatedness, and predicting compositionality (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Bruni et al., 2014). Although feature norms have also been used, raw image data has become the de-facto perceptual modality in multi-modal models. However, if the objective is to ground semantic representations in perceptual information, why stop at image data? The meaning of violin is surely not only grounded in its visual properties, such as shape, color and texture, but also in its sound, pitch and timbre. To understand how perceptual input leads to conceptual representation, we should use as many perceptual modalities as possible. A recent preliminary s"
D15-1293,P14-1068,0,0.20521,"ive to this bag of visual words (BoVW) approach is transferring features from convolutional neural networks (Kiela and Bottou, 2014). rain-storm 0.98 plane-jet 0.81 cat-feline 0.96 horse-mare 0.83 jazz-musician 0.88 sheep-lamb 0.84 bird-eagle 0.88 bird-hawk 0.79 highway-traffic 0.88 band-orchestra 0.71 guitar-piano 0.86 music-melody 0.70 Various ways of aggregating images into visual representations have been proposed, such as taking the mean or the elementwise maximum. Ideally, one would jointly learn multi-modal representations from parallel multi-modal data, such as text containing images (Silberer and Lapata, 2014) or images described with speech (Synnaeve et al., 2014), but such data is hard to obtain, has limited coverage and can be noisy. Hence, image representations are often learned independently. Aggregated visual representations are subsequently combined with a traditional linguistic space to form a multi-modal model. This mixing can be done in a variety of ways, ranging from simple concatenation to more sophisticated fusion methods (Bruni et al., 2014). Cross-modal semantics, instead of being concerned with improving semantic representations through grounding, focuses on the problem of reference"
D15-1293,Q14-1017,0,0.0260286,"semantics, instead of being concerned with improving semantic representations through grounding, focuses on the problem of reference. Using, for instance, mappings between visual and textual space, the objective is to learn which words refer to which objects (Lazaridou et al., 2014). This problem is very much rescore SimLex-999 score Table 1: Examples of pairs in the datasets where auditory is relevant, with the similarity score. lated to the object recognition task in computer vision, but instead of using just visual data and labels, these cross-modal models also utilize textual information (Socher et al., 2014; Frome et al., 2013). This allows for zero-shot learning, where the model can predict how an object relates to other concepts just from seeing an image of the object, but without ever having previously encountered an image of that particular object (Lazaridou et al., 2014). Multi-modal and cross-modal approaches have outperformed state-of-the-art textbased methods on a variety of tasks (Bruni et al., 2014; Silberer and Lapata, 2014). 3 Evaluations Following previous work in multi-modal semantics, we evaluate on two standard similarity and relatedness datasets: SimLex-999 (Hill et al., 2014) a"
D15-1293,N15-1016,0,\N,Missing
D15-1293,J15-4004,0,\N,Missing
D15-1293,D14-1032,0,\N,Missing
D16-1043,R11-1055,0,0.265608,"sentational quality (Bullinaria and Levy, 2007; Kiela and Clark, 2014). The same is likely to hold in the case of Google Bing Flickr ImageNet ESP Game Type Search engine Search engine Photo sharing Image database Game Annotation Automatic Automatic Human Human Human Coverage Unlimited Unlimited Unlimited Limited Limited Multi-lingual Yes Yes No No No Sorted Yes Yes Yes No No Tag specificity Unknown Unknown Loose Specific Loose Table 2: Sources of image data. visual representations. Various sources of image data have been used in multi-modal semantics, but there have not been many comparisons: Bergsma and Goebel (2011) compare Google and Flickr, and Kiela and Bottou (2014) compare ImageNet (Deng et al., 2009) and the ESP Game dataset (von Ahn and Dabbish, 2004), but most works use a single data source. In this study, one of our objectives is to asses the quality of various sources of image data. Table 2 provides an overview of the data sources, and Figure 1 shows some example images. We examine the following corpora: Google Images Google’s image search2 results have been found to be comparable to hand-crafted image datasets (Fergus et al., 2005). Bing Images An alternative image search engine is Bing Images"
D16-1043,D15-1172,0,0.0251272,"Missing"
D16-1043,P12-1015,0,0.0410624,"n representation as the sampled image representations. We use the same method for the ESP Game dataset. In all cases, images are resized and centercropped to ensure that they are the correct size input. 4 Evaluation Representation quality in semantics is usually evaluated using intrinsic datasets of human similarity and relatedness judgments. Model performance is assessed through the Spearman ρs rank correlation between the system’s similarity scores for a given pair of words, together with human judgments. Here, we evaluate on two well-known similarity and relatedness judgment datasets: MEN (Bruni et al., 2012) and SimLex-999 (Hill et al., 2015). MEN focuses explicitly on relatedness (i.e. coffee-tea and coffee-mug get high scores, while bakery-zebra gets a low score), while SimLex-999 focuses on what it calls “genuine” similarity (i.e., coffee-tea gets a high score, while both coffee-mug and bakery-zebra get low scores). They are standard evaluations for evaluating representational quality in semantics. In each experiment, we examine performance of the visual representations compared to text-based representations, as well as performance of the multimodal representation that fuses the two. In this A"
D16-1043,N16-1071,1,0.890774,"Missing"
D16-1043,J15-4004,0,0.0909181,"e representations. We use the same method for the ESP Game dataset. In all cases, images are resized and centercropped to ensure that they are the correct size input. 4 Evaluation Representation quality in semantics is usually evaluated using intrinsic datasets of human similarity and relatedness judgments. Model performance is assessed through the Spearman ρs rank correlation between the system’s similarity scores for a given pair of words, together with human judgments. Here, we evaluate on two well-known similarity and relatedness judgment datasets: MEN (Bruni et al., 2012) and SimLex-999 (Hill et al., 2015). MEN focuses explicitly on relatedness (i.e. coffee-tea and coffee-mug get high scores, while bakery-zebra gets a low score), while SimLex-999 focuses on what it calls “genuine” similarity (i.e., coffee-tea gets a high score, while both coffee-mug and bakery-zebra get low scores). They are standard evaluations for evaluating representational quality in semantics. In each experiment, we examine performance of the visual representations compared to text-based representations, as well as performance of the multimodal representation that fuses the two. In this Arch. AlexNet Agg. Mean GoogLeNet Ma"
D16-1043,D14-1005,1,0.941874,"in the data source. Introduction Multi-modal distributional semantics addresses the fact that text-based semantic models, which represent word meanings as a distribution over other words (Turney and Pantel, 2010; Clark, 2015), suffer from the grounding problem (Harnad, 1990). Recent work has shown that this theoretical motivation can be successfully exploited for practical gain. Indeed, multi-modal representation learning leads to improvements over language-only models in a range of tasks, including modelling semantic similarity and relatedness (Bruni et al., 2014; Silberer and Lapata, 2014; Kiela and Bottou, 2014; Lazaridou et Traditionally, representations for images were learned through bag-of-visual words (Sivic and Zisserman, 2003), using SIFT-based local feature descriptors (Lowe, 2004). Kiela and Bottou (2014) showed that transferring representations from deep convolutional neural networks (ConvNets) yield much better performance than bag-of-visual-words in multi-modal semantics. ConvNets (LeCun et al., 1998) have become very popular in recent years: they are now the dominant approach for almost all recognition and detection tasks in the computer vision community (LeCun et al., 2015), approachin"
D16-1043,W14-1503,1,0.828012,"yers. These networks were selected because they are very well-known in the computer vision community. They exhibit interesting qualitative differences in terms of their depth (i.e., the number of layers), the number of parameters, regularization methods and the use of fully connected layers. They have all been winning network architectures in the ILSVRC ImageNet classification challenges. 3 Sources of Image Data Some systematic studies of parameters for textbased distributional methods have found that the source corpus has a large impact on representational quality (Bullinaria and Levy, 2007; Kiela and Clark, 2014). The same is likely to hold in the case of Google Bing Flickr ImageNet ESP Game Type Search engine Search engine Photo sharing Image database Game Annotation Automatic Automatic Human Human Human Coverage Unlimited Unlimited Unlimited Limited Limited Multi-lingual Yes Yes No No No Sorted Yes Yes Yes No No Tag specificity Unknown Unknown Loose Specific Loose Table 2: Sources of image data. visual representations. Various sources of image data have been used in multi-modal semantics, but there have not been many comparisons: Bergsma and Goebel (2011) compare Google and Flickr, and Kiela and Bot"
D16-1043,P15-2020,1,0.865352,"Missing"
D16-1043,D15-1015,1,0.832353,"Missing"
D16-1043,P16-4010,1,0.817635,"hese findings extend to different languages beyond English? We evaluate semantic representation quality through examining how well a system’s similarity scores correlate with human similarity and relatedness judgments. We examine both the visual representations themselves as well as the multi-modal representations that fuse visual representations with linguistic input, in this case using middle fusion (i.e., concatenation). To the best of our knowledge, this work is the first to systematically compare these aspects of visual representation learning. 2 Architectures We use the MMFeat toolkit1 (Kiela, 2016) to obtain image representations for three different convolutional network architectures: AlexNet (Krizhevsky 1 https://github.com/douwekiela/mmfeat 448 et al., 2012), GoogLeNet (Szegedy et al., 2015) and VGGNet (Simonyan and Zisserman, 2014). Image representations are turned into an overall word-level visual representation by either taking the mean or the elementwise maximum of the relevant image representations. All three networks are trained to maximize the multinomial logistic regression objective using mini-batch gradient descent with momentum: − D X K X exp(θ(k)&gt; x(i) ) 1{y (i) = k} log"
D16-1043,D13-1115,0,0.237121,"Missing"
D16-1043,N16-1020,1,0.845642,"and Architectures for Deep Visual Representation Learning in Semantics Douwe Kiela, Anita L. Ver˝o and Stephen Clark Computer Laboratory University of Cambridge douwe.kiela,alv34,stephen.clark@cl.cam.ac.uk Abstract al., 2015), improving lexical entailment (Kiela et al., 2015a), predicting compositionality (Roller and Schulte im Walde, 2013), bilingual lexicon induction (Bergsma and Van Durme, 2011), selectional preference prediction (Bergsma and Goebel, 2011), linguistic ambiguity resolution (Berzak et al., 2015), visual information retrieval (Bulat et al., 2016) and metaphor identification (Shutova et al., 2016). Multi-modal distributional models learn grounded representations for improved performance in semantics. Deep visual representations, learned using convolutional neural networks, have been shown to achieve particularly high performance. In this study, we systematically compare deep visual representation learning techniques, experimenting with three well-known network architectures. In addition, we explore the various data sources that can be used for retrieving relevant images, showing that images from search engines perform as well as, or better than, those from manually crafted resources su"
D16-1043,P14-1068,0,0.062773,"ted with the target word(s) in the data source. Introduction Multi-modal distributional semantics addresses the fact that text-based semantic models, which represent word meanings as a distribution over other words (Turney and Pantel, 2010; Clark, 2015), suffer from the grounding problem (Harnad, 1990). Recent work has shown that this theoretical motivation can be successfully exploited for practical gain. Indeed, multi-modal representation learning leads to improvements over language-only models in a range of tasks, including modelling semantic similarity and relatedness (Bruni et al., 2014; Silberer and Lapata, 2014; Kiela and Bottou, 2014; Lazaridou et Traditionally, representations for images were learned through bag-of-visual words (Sivic and Zisserman, 2003), using SIFT-based local feature descriptors (Lowe, 2004). Kiela and Bottou (2014) showed that transferring representations from deep convolutional neural networks (ConvNets) yield much better performance than bag-of-visual-words in multi-modal semantics. ConvNets (LeCun et al., 1998) have become very popular in recent years: they are now the dominant approach for almost all recognition and detection tasks in the computer vision community (LeCun e"
D16-1043,P16-2031,1,0.602892,"Missing"
D16-1043,N15-1016,0,\N,Missing
D17-1113,D13-1202,0,0.404187,"enchmark for evaluating semantic model performance in terms of their ability to represent human semantic memory. Mitchell et al. (2008) were the first to demonstrate that distributional semantic models encode some of the patterns found in the fMRI data. Other researchers followed in their steps, evaluating traditional count-based distributional models (Devereux et al., 2010; Murphy et al., 2012), topic model-based semantic features (Pereira et al., 2013), psycholinguistic and behavioural features (Palatucci et al., 2009; Chang et al., 2010; Fernandino et al., 2015) and visual representations (Anderson et al., 2013, 2017). While all of these studies report correlation between the investigated semantic models and patterns found in the brain imaging data, their focus on individual models and the use of different datasets and prediction methods make their results difficult to compare and to integrate into a coherent evaluation landscape. The work of Murphy et al. (2012) is an exception, in that the authors systematically compare several distributional models with a range of parameters on the same brain imaging dataset. However, they focus on the traditional count-based distributional models only. We take i"
D17-1113,Q17-1002,1,0.767429,"s improves performance on a variety of tasks (Silberer and Lapata, 2012; Bruni et al., 2012; Kiela and Bottou, 2014; Bulat et al., 2016). Anderson et al. (2013) show that semantic models built from visual data correlate highly with fMRIbased brain activation patterns. Anderson et al. (2015) find that similarity in activity in the brain areas related to linguistic processing can be better predicted from text-based semantic representations, whilst image-based representations perform better at predicting similarity in the visual processing areas of the brain. In line with the dual coding theory, Anderson et al. (2017) demonstrate an advantage in decoding brain activity patterns of abstract words for text-based semantic models over the image-based ones. Contrary to previous findings, Anderson et al. (2017) find no advantage in decoding neural activity patterns associated with concrete words for image-based models. Murphy et al. (2012) present the first study systematically comparing several text-based semantic models on the brain activity prediction task. They focus on the traditional count-based distributional models and achieve the best performance using dependency-based features. Our study is more extens"
D17-1113,R11-1055,0,0.0453686,"as semantic features, and counts are replaced by the sum of primary, secondary and tertiary association frequencies between the target word and the responses. Counts are re-weighted using PPMI and vectors are L2normalised. The association-based representations obtained for the 60 target words in the Mitchell et al. (2008) dataset under this model are 9854-dimensional. 4.3 Image-based semantic model We also build state-of-the-art deep visual semantic representations (henceforth VISUAL) for the 60 concepts in the Mitchell et al. (2008) dataset. Following previous work in multi-modal semantics (Bergsma and Goebel, 2011; Kiela and Bottou, 2014) and the findings of a recent study of system architectures and data sources for constructing visual representations (Kiela et al., 2016), we retrieve 10 images per concept from Google Images. We use the MMFeat toolkit6 (Kiela, 2016) to build our image representations. We extract the 4096-dimensional pre-softmax layer from a for4 https://smallworldofwords.org/ Total of 9854 words (appearing as both target and responses) and 1092251 association pairs 6 https://github.com/douwekiela/mmfeat 1084 5 ward pass through a convolutional neural network (Krizhevsky et al., 2012),"
D17-1113,P12-1015,0,0.217624,"xt of natural language processing system performance. In this paper, we present a systematic evaluation and comparison of a range of widely-used, stateof-the-art semantic models in their ability to predict patterns of conceptual representation in the human brain. Our results provide new insights both for the design of computational semantic models and for further research in cognitive neuroscience. 1 Introduction Recent years have witnessed many breakthroughs in data-driven semantic modelling: from the loglinear skip-gram model of Mikolov et al. (2013a) to multi-modal meaning representations (Bruni et al., 2012; Kiela and Bottou, 2014; Kiela and Clark, 2015; Kiela et al., 2015a). These models boast of a higher performance accuracy in numerous semantic tasks, including modeling semantic similarity and relatedness (Silberer and Lapata, 2012), lexical entailment (Kiela et al., 2015b), analogy (Mikolov et al., 2013b) and metaphor (Shutova et al., 2016). However, less is known about the extent to which such models correlate with and reflect human conceptual representation. Much research in the cognitive neuroscience community has been concerned with uncovering how the brain represents conceptual knowledg"
D17-1113,N16-1071,1,0.845696,"Missing"
D17-1113,J07-4004,1,0.714176,"ooccurrence based semantic vectors developed in the Mitchell et al. (2008) study. The features of this semantic space are 25 sensory-motor verbs. Co-occurrence statistics were collected using a window size of 5 words either side of the target word, on a trillion-word corpus provided by Google. 4.1 Text-based semantic models We train a variety2 of context-counting and context-predicting text-based semantic models on the January 2016 dump of Wikipedia, which was tokenised using the Stanford NLP tools3 , lemmatised with the Morpha lemmatiser (Minnen et al., 2001), and parsed with the C&C parser (Clark and Curran, 2007). DISTRIB We obtain count-based distributional semantic models, using the top 10K most frequent lemmatised words in the corpus (excluding stopwords) as contexts. The context window is defined as sentence boundaries. Counts are reweighted using positive pointwise mutual information (PPMI) and vectors are L2-normalised. SVD 300 We also construct 300-dimensional dense semantic representations by applying singular value decomposition (SVD) (Deerwester et al., 1990) to DISTRIB. 1 https://www.cs.cmu.edu/afs/cs/ project/theo-73/www/science2008/data. html 2 We have experimented with different paramete"
D17-1113,C16-1175,0,0.0972466,"Missing"
D17-1113,W10-0609,0,0.474057,"brain activation data associated with the meanings of concepts obtained during functional magnetic resonance imaging (fMRI) experiments. In the computational linguistics community, the availability of such fMRI data provides researchers with a benchmark for evaluating semantic model performance in terms of their ability to represent human semantic memory. Mitchell et al. (2008) were the first to demonstrate that distributional semantic models encode some of the patterns found in the fMRI data. Other researchers followed in their steps, evaluating traditional count-based distributional models (Devereux et al., 2010; Murphy et al., 2012), topic model-based semantic features (Pereira et al., 2013), psycholinguistic and behavioural features (Palatucci et al., 2009; Chang et al., 2010; Fernandino et al., 2015) and visual representations (Anderson et al., 2013, 2017). While all of these studies report correlation between the investigated semantic models and patterns found in the brain imaging data, their focus on individual models and the use of different datasets and prediction methods make their results difficult to compare and to integrate into a coherent evaluation landscape. The work of Murphy et al. (2"
D17-1113,D14-1032,0,0.0337962,"tput by the C&C parser, we create word-context pairs using all words and contexts occurring more than 400 times in the corpus. This resulted in a vocabulary of about 92,000 words, with over 250,000 distinct syntactic contexts. We use 10 negative samples per word-context pair and 15 iterations over the corpus. 4.2 Association-based semantic model Free word association datasets (Nelson et al., 2004; De Deyne et al., 2016) represent a rich source of semantic information and have been successfully used in NLP, including research on semantic memory (Steyvers et al., 2004) and multimodal semantics (Hill and Korhonen, 2014). Recent studies have shown the superiority of semantic models built using data collected from multiple-response free association tasks — where subjects are asked to list multiple associative cues for every target word rather than a single association — over the models built from single-response ones (De Deyne et al., 2013). Moreover, such association-based semantic models have been shown to outperform current stateof-the-art text-based language models on concept relatedness and similarity judgments (De Deyne et al., 2016). We make use of the word association dataset collected as part of the S"
D17-1113,P16-4010,0,0.563309,"for the 60 target words in the Mitchell et al. (2008) dataset under this model are 9854-dimensional. 4.3 Image-based semantic model We also build state-of-the-art deep visual semantic representations (henceforth VISUAL) for the 60 concepts in the Mitchell et al. (2008) dataset. Following previous work in multi-modal semantics (Bergsma and Goebel, 2011; Kiela and Bottou, 2014) and the findings of a recent study of system architectures and data sources for constructing visual representations (Kiela et al., 2016), we retrieve 10 images per concept from Google Images. We use the MMFeat toolkit6 (Kiela, 2016) to build our image representations. We extract the 4096-dimensional pre-softmax layer from a for4 https://smallworldofwords.org/ Total of 9854 words (appearing as both target and responses) and 1092251 association pairs 6 https://github.com/douwekiela/mmfeat 1084 5 ward pass through a convolutional neural network (Krizhevsky et al., 2012), which has been pretrained on the ImageNet classification task using Caffe (Jia et al., 2014). We obtain the visual representation for a given concept by taking the mean of the 10 resulting image representations. 4.4 Multi-modal semantic models We also inclu"
D17-1113,D14-1005,0,0.704069,"ge processing system performance. In this paper, we present a systematic evaluation and comparison of a range of widely-used, stateof-the-art semantic models in their ability to predict patterns of conceptual representation in the human brain. Our results provide new insights both for the design of computational semantic models and for further research in cognitive neuroscience. 1 Introduction Recent years have witnessed many breakthroughs in data-driven semantic modelling: from the loglinear skip-gram model of Mikolov et al. (2013a) to multi-modal meaning representations (Bruni et al., 2012; Kiela and Bottou, 2014; Kiela and Clark, 2015; Kiela et al., 2015a). These models boast of a higher performance accuracy in numerous semantic tasks, including modeling semantic similarity and relatedness (Silberer and Lapata, 2012), lexical entailment (Kiela et al., 2015b), analogy (Mikolov et al., 2013b) and metaphor (Shutova et al., 2016). However, less is known about the extent to which such models correlate with and reflect human conceptual representation. Much research in the cognitive neuroscience community has been concerned with uncovering how the brain represents conceptual knowledge, by leveraging brain a"
D17-1113,P15-2038,1,0.838305,", we present a systematic evaluation and comparison of a range of widely-used, stateof-the-art semantic models in their ability to predict patterns of conceptual representation in the human brain. Our results provide new insights both for the design of computational semantic models and for further research in cognitive neuroscience. 1 Introduction Recent years have witnessed many breakthroughs in data-driven semantic modelling: from the loglinear skip-gram model of Mikolov et al. (2013a) to multi-modal meaning representations (Bruni et al., 2012; Kiela and Bottou, 2014; Kiela and Clark, 2015; Kiela et al., 2015a). These models boast of a higher performance accuracy in numerous semantic tasks, including modeling semantic similarity and relatedness (Silberer and Lapata, 2012), lexical entailment (Kiela et al., 2015b), analogy (Mikolov et al., 2013b) and metaphor (Shutova et al., 2016). However, less is known about the extent to which such models correlate with and reflect human conceptual representation. Much research in the cognitive neuroscience community has been concerned with uncovering how the brain represents conceptual knowledge, by leveraging brain activation data associated with the meanings"
D17-1113,D15-1293,1,0.883032,"formance. In this paper, we present a systematic evaluation and comparison of a range of widely-used, stateof-the-art semantic models in their ability to predict patterns of conceptual representation in the human brain. Our results provide new insights both for the design of computational semantic models and for further research in cognitive neuroscience. 1 Introduction Recent years have witnessed many breakthroughs in data-driven semantic modelling: from the loglinear skip-gram model of Mikolov et al. (2013a) to multi-modal meaning representations (Bruni et al., 2012; Kiela and Bottou, 2014; Kiela and Clark, 2015; Kiela et al., 2015a). These models boast of a higher performance accuracy in numerous semantic tasks, including modeling semantic similarity and relatedness (Silberer and Lapata, 2012), lexical entailment (Kiela et al., 2015b), analogy (Mikolov et al., 2013b) and metaphor (Shutova et al., 2016). However, less is known about the extent to which such models correlate with and reflect human conceptual representation. Much research in the cognitive neuroscience community has been concerned with uncovering how the brain represents conceptual knowledge, by leveraging brain activation data associat"
D17-1113,P15-2020,1,0.902858,"Missing"
D17-1113,D16-1043,1,0.907927,"Missing"
D17-1113,P14-2050,0,0.0362204,"resentations by applying SVD to DEPS. EMBED - BOW We train 300-dimensional embeddings using the standard log-linear skipgram model with negative sampling of Mikolov et al. (2013a). The embeddings were trained using linear bag-of-words contexts, with the window defined as k = 2 (EMBED - BOW 2) or k = 5 (EMBED - BOW 5) words either side of the target word. We use 10 negative samples per wordcontext pair and 15 iterations over the corpus. EMBED - DEPS In addition to the embeddings trained with linear bag-of-words contexts, we also obtain 300-dimensional dependency-based word embeddings using the Levy and Goldberg (2014) implementation of the generalised skip-gram with arbitrary contexts model. Using both incoming and outgoing dependency relations output by the C&C parser, we create word-context pairs using all words and contexts occurring more than 400 times in the corpus. This resulted in a vocabulary of about 92,000 words, with over 250,000 distinct syntactic contexts. We use 10 negative samples per word-context pair and 15 iterations over the corpus. 4.2 Association-based semantic model Free word association datasets (Nelson et al., 2004; De Deyne et al., 2016) represent a rich source of semantic informat"
D17-1113,N16-1020,1,0.875798,"c models and for further research in cognitive neuroscience. 1 Introduction Recent years have witnessed many breakthroughs in data-driven semantic modelling: from the loglinear skip-gram model of Mikolov et al. (2013a) to multi-modal meaning representations (Bruni et al., 2012; Kiela and Bottou, 2014; Kiela and Clark, 2015; Kiela et al., 2015a). These models boast of a higher performance accuracy in numerous semantic tasks, including modeling semantic similarity and relatedness (Silberer and Lapata, 2012), lexical entailment (Kiela et al., 2015b), analogy (Mikolov et al., 2013b) and metaphor (Shutova et al., 2016). However, less is known about the extent to which such models correlate with and reflect human conceptual representation. Much research in the cognitive neuroscience community has been concerned with uncovering how the brain represents conceptual knowledge, by leveraging brain activation data associated with the meanings of concepts obtained during functional magnetic resonance imaging (fMRI) experiments. In the computational linguistics community, the availability of such fMRI data provides researchers with a benchmark for evaluating semantic model performance in terms of their ability to re"
D17-1113,D12-1130,0,0.214005,"ual representation in the human brain. Our results provide new insights both for the design of computational semantic models and for further research in cognitive neuroscience. 1 Introduction Recent years have witnessed many breakthroughs in data-driven semantic modelling: from the loglinear skip-gram model of Mikolov et al. (2013a) to multi-modal meaning representations (Bruni et al., 2012; Kiela and Bottou, 2014; Kiela and Clark, 2015; Kiela et al., 2015a). These models boast of a higher performance accuracy in numerous semantic tasks, including modeling semantic similarity and relatedness (Silberer and Lapata, 2012), lexical entailment (Kiela et al., 2015b), analogy (Mikolov et al., 2013b) and metaphor (Shutova et al., 2016). However, less is known about the extent to which such models correlate with and reflect human conceptual representation. Much research in the cognitive neuroscience community has been concerned with uncovering how the brain represents conceptual knowledge, by leveraging brain activation data associated with the meanings of concepts obtained during functional magnetic resonance imaging (fMRI) experiments. In the computational linguistics community, the availability of such fMRI data"
D17-1113,N13-1090,0,0.167424,"ng. However, semantic models are typically studied in the context of natural language processing system performance. In this paper, we present a systematic evaluation and comparison of a range of widely-used, stateof-the-art semantic models in their ability to predict patterns of conceptual representation in the human brain. Our results provide new insights both for the design of computational semantic models and for further research in cognitive neuroscience. 1 Introduction Recent years have witnessed many breakthroughs in data-driven semantic modelling: from the loglinear skip-gram model of Mikolov et al. (2013a) to multi-modal meaning representations (Bruni et al., 2012; Kiela and Bottou, 2014; Kiela and Clark, 2015; Kiela et al., 2015a). These models boast of a higher performance accuracy in numerous semantic tasks, including modeling semantic similarity and relatedness (Silberer and Lapata, 2012), lexical entailment (Kiela et al., 2015b), analogy (Mikolov et al., 2013b) and metaphor (Shutova et al., 2016). However, less is known about the extent to which such models correlate with and reflect human conceptual representation. Much research in the cognitive neuroscience community has been concerned"
D17-1113,S12-1019,0,0.207323,"ssociated with the meanings of concepts obtained during functional magnetic resonance imaging (fMRI) experiments. In the computational linguistics community, the availability of such fMRI data provides researchers with a benchmark for evaluating semantic model performance in terms of their ability to represent human semantic memory. Mitchell et al. (2008) were the first to demonstrate that distributional semantic models encode some of the patterns found in the fMRI data. Other researchers followed in their steps, evaluating traditional count-based distributional models (Devereux et al., 2010; Murphy et al., 2012), topic model-based semantic features (Pereira et al., 2013), psycholinguistic and behavioural features (Palatucci et al., 2009; Chang et al., 2010; Fernandino et al., 2015) and visual representations (Anderson et al., 2013, 2017). While all of these studies report correlation between the investigated semantic models and patterns found in the brain imaging data, their focus on individual models and the use of different datasets and prediction methods make their results difficult to compare and to integrate into a coherent evaluation landscape. The work of Murphy et al. (2012) is an exception,"
D19-1233,P16-1231,0,0.0436299,"rn parsing datasets, with 347 training documents. Previous approaches to RST parsing (Ji and Eisenstein, 2014; Feng and Hirst, 2014a; Joty et al., 2015; Braud et al., 2017) have used locally normalized discriminative models. However, these are known to have worse performance than generative models when there is little training data (Ng and Jordan, 2002; Yogatama et al., 2017). Unlike locally normalised discriminative models, generative models are not susceptible to label bias (Lafferty et al., 2001). The success of generative (Dyer et al., 2016; Charniak et al., 2016) and globally normalised (Andor et al., 2016) syntactic parsers suggests that reducing label bias leads to better performance. We hypothesize that using a generative parser would also lead to improved performance on RST parsing. However, while they are free from label bias, generative parsers require more sophisticated search algorithms for decoding. Fried et al. (2017) presented a word-level beam search algorithm that made it possible to decode directly from neural generative parsers rather than using them as rerankers. In this paper, we present the first generative RST parser1 . Our model is a document-level version of an RNN Grammar ("
D19-1233,D15-1263,0,0.205956,"ting this branching bias and results in absolute improvements of 6.8 and 2.9 on unlabelled and labelled F1 over previous algorithms. Overall, our generative model outperforms a discriminative model with the same features by 2.6 F1 points and achieves performance comparable to the state-of-the-art, outperforming all published parsers from a recent replication study that do not use additional training data. 1 Introduction Understanding a document’s discourse-level organization is important for correctly interpreting it, and discourse analyses have been shown to be helpful for several NLP tasks (Bhatia et al., 2015; Ji and Smith, 2017; Feng and Hirst, 2014b; Ferracane et al., 2017). A popular formalism for discourse analysis is Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) (Fig. 1) which represents a document as a tree of discourse units recursively built by connecting smaller units through rhetorical relations. Learning to predict RST trees is difficult because it depends on pragmatics as well as literal meaning, and the English RST Discourse Treebank (RST-DT) (Carlson et al., 2003) is small by the standards of modern parsing datasets, with 347 training documents. Previous approaches to R"
D19-1233,E17-1028,0,0.39538,"formalism for discourse analysis is Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) (Fig. 1) which represents a document as a tree of discourse units recursively built by connecting smaller units through rhetorical relations. Learning to predict RST trees is difficult because it depends on pragmatics as well as literal meaning, and the English RST Discourse Treebank (RST-DT) (Carlson et al., 2003) is small by the standards of modern parsing datasets, with 347 training documents. Previous approaches to RST parsing (Ji and Eisenstein, 2014; Feng and Hirst, 2014a; Joty et al., 2015; Braud et al., 2017) have used locally normalized discriminative models. However, these are known to have worse performance than generative models when there is little training data (Ng and Jordan, 2002; Yogatama et al., 2017). Unlike locally normalised discriminative models, generative models are not susceptible to label bias (Lafferty et al., 2001). The success of generative (Dyer et al., 2016; Charniak et al., 2016) and globally normalised (Andor et al., 2016) syntactic parsers suggests that reducing label bias leads to better performance. We hypothesize that using a generative parser would also lead to improv"
D19-1233,C16-1179,0,0.312021,"Missing"
D19-1233,N18-1086,0,0.0200786,"contains a single tree. Table 2 shows an example of a completed computation for our transition system. 2286 3.3 Transition Model In initial experiments we found, as did Kuncoro et al. (2017) for syntactic parsing, that conditioning only on the stack led to better parsing accuracy, so we specify the next action distribution as p(aj |Sj ). To handle the unbounded number of possible EDUs, we parametrize the probabilities of GEN(e) actions using a neural language model. The next action distribution is factorised into a structural action distribution ptrans and a generation distribution pgen as in Buys and Blunsom (2018), so that p(RE(r, n)|S) = ptrans (RE(r, n)|S) and p(GEN(e)|S) = ptrans (GEN|S)·pgen (e|S) where pgen is the neural language model. We parametrize ptrans as a feedforward neural network on an embedding of the stack hS (S). In initial experiments we found, consistent with Morey et al. (2017), that a model with neural embeddings as its only features performed poorly. We therefore compute the representation using both neural embeddings of the discourse units on the stack (Section 3.3.1) and a set of structural features extracted from the stack (Section 3.3.2). 3.3.1 Neural Embeddings To produce th"
D19-1233,P15-1033,0,0.018673,"vely as the nucleus if the nucleus is an EDU, or the nuclear EDU of the nucleus if the nucleus is itself a unit. For multinuclear relations, we take the left-most nucleus. Then, if Unit(r, n) UL UR is a unit and eN is its nuclear EDU, hEDU (eN ) is the embedding of the nuclear EDU, and hR (r, n) is an embedding of the nuclearity-relation pair (r, n) in a lookup table: hU (U ) = TREELSTM([hEDU (eN ); hR (r, n)], hU (UL ), hU (UR )) (5) where hU (UL ) and hU (UR ) are the hidden state and memory cell of the left and right argument of the unit respectively. We embed the stack with a stack LSTM (Dyer et al., 2015). If the stack contents are D1 |· · · |Dm with each Di being a discourse unit, then S S hN S (S) = LSTM (hU (D1:m ), h0 ) (6) 3.3.2 Structural Features We extract additional features from the stack that have been found to be useful in prior work. As in Braud et al. (2017), for each discourse unit, we extract the word embeddings of up to three words whose syntactic head is not in the unit, adding padding if there are fewer than three. We concatenate these features for the top two discourse units on the stack, using a dummy embedding if the stack only contains one discourse unit. We write hhead"
D19-1233,N16-1024,0,0.486855,"k (RST-DT) (Carlson et al., 2003) is small by the standards of modern parsing datasets, with 347 training documents. Previous approaches to RST parsing (Ji and Eisenstein, 2014; Feng and Hirst, 2014a; Joty et al., 2015; Braud et al., 2017) have used locally normalized discriminative models. However, these are known to have worse performance than generative models when there is little training data (Ng and Jordan, 2002; Yogatama et al., 2017). Unlike locally normalised discriminative models, generative models are not susceptible to label bias (Lafferty et al., 2001). The success of generative (Dyer et al., 2016; Charniak et al., 2016) and globally normalised (Andor et al., 2016) syntactic parsers suggests that reducing label bias leads to better performance. We hypothesize that using a generative parser would also lead to improved performance on RST parsing. However, while they are free from label bias, generative parsers require more sophisticated search algorithms for decoding. Fried et al. (2017) presented a word-level beam search algorithm that made it possible to decode directly from neural generative parsers rather than using them as rerankers. In this paper, we present the first generative RS"
D19-1233,P14-1048,0,0.588331,"bsolute improvements of 6.8 and 2.9 on unlabelled and labelled F1 over previous algorithms. Overall, our generative model outperforms a discriminative model with the same features by 2.6 F1 points and achieves performance comparable to the state-of-the-art, outperforming all published parsers from a recent replication study that do not use additional training data. 1 Introduction Understanding a document’s discourse-level organization is important for correctly interpreting it, and discourse analyses have been shown to be helpful for several NLP tasks (Bhatia et al., 2015; Ji and Smith, 2017; Feng and Hirst, 2014b; Ferracane et al., 2017). A popular formalism for discourse analysis is Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) (Fig. 1) which represents a document as a tree of discourse units recursively built by connecting smaller units through rhetorical relations. Learning to predict RST trees is difficult because it depends on pragmatics as well as literal meaning, and the English RST Discourse Treebank (RST-DT) (Carlson et al., 2003) is small by the standards of modern parsing datasets, with 347 training documents. Previous approaches to RST parsing (Ji and Eisenstein, 2014; Feng"
D19-1233,I17-1059,0,0.0226243,"6.8 and 2.9 on unlabelled and labelled F1 over previous algorithms. Overall, our generative model outperforms a discriminative model with the same features by 2.6 F1 points and achieves performance comparable to the state-of-the-art, outperforming all published parsers from a recent replication study that do not use additional training data. 1 Introduction Understanding a document’s discourse-level organization is important for correctly interpreting it, and discourse analyses have been shown to be helpful for several NLP tasks (Bhatia et al., 2015; Ji and Smith, 2017; Feng and Hirst, 2014b; Ferracane et al., 2017). A popular formalism for discourse analysis is Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) (Fig. 1) which represents a document as a tree of discourse units recursively built by connecting smaller units through rhetorical relations. Learning to predict RST trees is difficult because it depends on pragmatics as well as literal meaning, and the English RST Discourse Treebank (RST-DT) (Carlson et al., 2003) is small by the standards of modern parsing datasets, with 347 training documents. Previous approaches to RST parsing (Ji and Eisenstein, 2014; Feng and Hirst, 2014a; Joty et"
D19-1233,P17-2025,0,0.479973,"g data (Ng and Jordan, 2002; Yogatama et al., 2017). Unlike locally normalised discriminative models, generative models are not susceptible to label bias (Lafferty et al., 2001). The success of generative (Dyer et al., 2016; Charniak et al., 2016) and globally normalised (Andor et al., 2016) syntactic parsers suggests that reducing label bias leads to better performance. We hypothesize that using a generative parser would also lead to improved performance on RST parsing. However, while they are free from label bias, generative parsers require more sophisticated search algorithms for decoding. Fried et al. (2017) presented a word-level beam search algorithm that made it possible to decode directly from neural generative parsers rather than using them as rerankers. In this paper, we present the first generative RST parser1 . Our model is a document-level version of an RNN Grammar (RNNG, Dyer et al. (2016)) defined through a transition system with both word- and structure-generating actions. It uses distributed representations of discourse units and transition probabilities parametrized by RNNs to model unbounded dependencies in a document. For our discourse parser, we find that Fried et al. (2017)’s wo"
D19-1233,W16-3616,0,0.0541394,"Missing"
D19-1233,D16-1257,0,0.0571551,"Missing"
D19-1233,P14-1002,0,0.567919,"th, 2017; Feng and Hirst, 2014b; Ferracane et al., 2017). A popular formalism for discourse analysis is Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) (Fig. 1) which represents a document as a tree of discourse units recursively built by connecting smaller units through rhetorical relations. Learning to predict RST trees is difficult because it depends on pragmatics as well as literal meaning, and the English RST Discourse Treebank (RST-DT) (Carlson et al., 2003) is small by the standards of modern parsing datasets, with 347 training documents. Previous approaches to RST parsing (Ji and Eisenstein, 2014; Feng and Hirst, 2014a; Joty et al., 2015; Braud et al., 2017) have used locally normalized discriminative models. However, these are known to have worse performance than generative models when there is little training data (Ng and Jordan, 2002; Yogatama et al., 2017). Unlike locally normalised discriminative models, generative models are not susceptible to label bias (Lafferty et al., 2001). The success of generative (Dyer et al., 2016; Charniak et al., 2016) and globally normalised (Andor et al., 2016) syntactic parsers suggests that reducing label bias leads to better performance. We hypot"
D19-1233,N16-1037,0,0.0524378,"gorithm that made it possible to decode directly from neural generative parsers rather than using them as rerankers. In this paper, we present the first generative RST parser1 . Our model is a document-level version of an RNN Grammar (RNNG, Dyer et al. (2016)) defined through a transition system with both word- and structure-generating actions. It uses distributed representations of discourse units and transition probabilities parametrized by RNNs to model unbounded dependencies in a document. For our discourse parser, we find that Fried et al. (2017)’s word-level beam search algorithm is bi1 Ji et al. (2016) introduced a neural generative discourse parser, but they used the annotation scheme of the Penn Discourse Treebank (Prasad et al., 2008) and Switchboard Dialog Act (Godfrey et al., 1992) corpora, predicting flat discourse representations between adjacent sentences, rather than hierarchical relations among clauses. 2284 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2284–2295, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 3 Rhetorica"
D19-1233,P17-1092,0,0.0506627,"ias and results in absolute improvements of 6.8 and 2.9 on unlabelled and labelled F1 over previous algorithms. Overall, our generative model outperforms a discriminative model with the same features by 2.6 F1 points and achieves performance comparable to the state-of-the-art, outperforming all published parsers from a recent replication study that do not use additional training data. 1 Introduction Understanding a document’s discourse-level organization is important for correctly interpreting it, and discourse analyses have been shown to be helpful for several NLP tasks (Bhatia et al., 2015; Ji and Smith, 2017; Feng and Hirst, 2014b; Ferracane et al., 2017). A popular formalism for discourse analysis is Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) (Fig. 1) which represents a document as a tree of discourse units recursively built by connecting smaller units through rhetorical relations. Learning to predict RST trees is difficult because it depends on pragmatics as well as literal meaning, and the English RST Discourse Treebank (RST-DT) (Carlson et al., 2003) is small by the standards of modern parsing datasets, with 347 training documents. Previous approaches to RST parsing (Ji and E"
D19-1233,J15-3002,0,0.126699,"., 2017). A popular formalism for discourse analysis is Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) (Fig. 1) which represents a document as a tree of discourse units recursively built by connecting smaller units through rhetorical relations. Learning to predict RST trees is difficult because it depends on pragmatics as well as literal meaning, and the English RST Discourse Treebank (RST-DT) (Carlson et al., 2003) is small by the standards of modern parsing datasets, with 347 training documents. Previous approaches to RST parsing (Ji and Eisenstein, 2014; Feng and Hirst, 2014a; Joty et al., 2015; Braud et al., 2017) have used locally normalized discriminative models. However, these are known to have worse performance than generative models when there is little training data (Ng and Jordan, 2002; Yogatama et al., 2017). Unlike locally normalised discriminative models, generative models are not susceptible to label bias (Lafferty et al., 2001). The success of generative (Dyer et al., 2016; Charniak et al., 2016) and globally normalised (Andor et al., 2016) syntactic parsers suggests that reducing label bias leads to better performance. We hypothesize that using a generative parser woul"
D19-1233,D17-1136,0,0.825311,"y the next action distribution as p(aj |Sj ). To handle the unbounded number of possible EDUs, we parametrize the probabilities of GEN(e) actions using a neural language model. The next action distribution is factorised into a structural action distribution ptrans and a generation distribution pgen as in Buys and Blunsom (2018), so that p(RE(r, n)|S) = ptrans (RE(r, n)|S) and p(GEN(e)|S) = ptrans (GEN|S)·pgen (e|S) where pgen is the neural language model. We parametrize ptrans as a feedforward neural network on an embedding of the stack hS (S). In initial experiments we found, consistent with Morey et al. (2017), that a model with neural embeddings as its only features performed poorly. We therefore compute the representation using both neural embeddings of the discourse units on the stack (Section 3.3.1) and a set of structural features extracted from the stack (Section 3.3.2). 3.3.1 Neural Embeddings To produce the stack embedding, we first require embeddings for both EDUs and units. We embed EDUs with bidirectional LSTMs4 . If e is an EDU consisting of the word sequence w1:k , then (→) (w1:k , h→ h→ 0 ) k = LSTM (←) h← (wk:1 , h← 0 ) k = LSTM (2) where wt is the word embedding of wt . The embeddin"
D19-1233,E17-1117,0,0.0210675,"results in absolute improvements of 6.8 and 2.9 on unlabelled and labelled F1 over previous algorithms. Overall, our generative model outperforms a discriminative model with the same features by 2.6 F1 points and achieves performance comparable to the state-of-the-art, outperforming all published parsers from a recent replication study that do not use additional training data. 1 Introduction Understanding a document’s discourse-level organization is important for correctly interpreting it, and discourse analyses have been shown to be helpful for several NLP tasks (Bhatia et al., 2015; Ji and Smith, 2017; Feng and Hirst, 2014b; Ferracane et al., 2017). A popular formalism for discourse analysis is Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) (Fig. 1) which represents a document as a tree of discourse units recursively built by connecting smaller units through rhetorical relations. Learning to predict RST trees is difficult because it depends on pragmatics as well as literal meaning, and the English RST Discourse Treebank (RST-DT) (Carlson et al., 2003) is small by the standards of modern parsing datasets, with 347 training documents. Previous approaches to RST parsing (Ji and E"
D19-1233,P18-1132,1,0.896753,"Missing"
D19-1233,prasad-etal-2008-penn,0,0.0603041,"present the first generative RST parser1 . Our model is a document-level version of an RNN Grammar (RNNG, Dyer et al. (2016)) defined through a transition system with both word- and structure-generating actions. It uses distributed representations of discourse units and transition probabilities parametrized by RNNs to model unbounded dependencies in a document. For our discourse parser, we find that Fried et al. (2017)’s word-level beam search algorithm is bi1 Ji et al. (2016) introduced a neural generative discourse parser, but they used the annotation scheme of the Penn Discourse Treebank (Prasad et al., 2008) and Switchboard Dialog Act (Godfrey et al., 1992) corpora, predicting flat discourse representations between adjacent sentences, rather than hierarchical relations among clauses. 2284 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2284–2295, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 3 Rhetorical Structure RNNGs J USTIFY In this section, we present a generative model for predicting RST trees given a document segmented into a seque"
D19-1233,N03-1030,0,0.190355,"than three. We concatenate these features for the top two discourse units on the stack, using a dummy embedding if the stack only contains one discourse unit. We write hhead (S) for these features. S We use a categorical feature for whether the top two discourse units are: in the same sentence; in different sentences; or incomparable since one of them spans multiple sentences. We also use an equivalent feature for paragraphs. Feature values are represented by embeddings in a lookup table. We write hcomp (S) for these features. S Finally, we extract features describing the dominance relation (Soricut and Marcu, 2003) between the top two discourse units on the stack. If there is a word in one discourse unit whose syntactic head is in the other, we extract the word embeddings of these two words as well as an embedding of the dependency relation between them, otherwise we use a single dummy embedding. We write hdom S (S) for these features. 2287 The structural feature representation is then the concatenation of these three features: hFS (S) = [hhead (S); hcomp (S); hdom S S (S)] S (7) and the full stack representation is the concatenation of the neural embedding and the feature representation: F hS (S) = [hN"
D19-1233,D17-1178,0,0.0243961,"ring model and the search algorithm. We can isolate bias in search algorithms by studying the trees they return when the scoring model contains no information. Intuitively, if the scoring model has no preference over trees, then any preference shown by the parser is the result of biases in the search algorithm. We tested whether the left-branching bias came from the word-level beam search (the search algorithm of Fried et al. (2017)) by using it to parse sequences of various lengths using a bottom-up RNNG with a uniform scoring model. We broke 6 We used candidate fast-tracking as described in Stern et al. (2017)’s extension to Fried et al. (2017)’s algorithm. 2288 1 Algorithm 1 Word-level Beam Search ties at beam cut-offs by uniform sampling without replacement. We measured branching bias using Sampson (1997)’s production-based measure of left-branching for parse trees which we write as PL (T ) for a tree T . The measure is the fraction of non-terminals whose left child is also a non-terminal, and varies from 0 for a fully right-branching tree to n−2 n−1 → 1 for a fully leftbranching tree, where n is the number of leaves. Figure 2 shows the median value of this measure for 100 trees each for sequence"
D19-1233,D16-1035,0,0.43598,"Missing"
D19-1233,N15-3001,0,0.0640996,"Missing"
D19-1233,P19-1410,0,0.310533,"eraged F1 scores on labelled attachment decisions as calculated by the EDUCE python package8 . We report F1 for predicting span attachments (S), span attachments with nuclearity (N), span attachments with relation labels (R) and span attachments with nuclearity and relation labels (F). We compare our results against the numbers from Morey et al. (2017), since they include several competitive parers under a consistent evaluation scheme.9 As a baseline, we use a discriminative version of 8 https://github.com/irit-melodi/educe 9 We do not compare against Yu et al. (2018), Zhang et al. (2018) and Lin et al. (2019)’s recent neural RST parsers since they do not evaluate labelled attachment decisions so their results are not comparable to ours. Training and Hyperparameters We use 300-dimensional word embeddings initialized to word2vec vectors (Mikolov et al., 2013). We tie the embeddings in the EDU LSTM and the decoder LSTM input and output embeddings. We use a 2-layer bidirectional LSTM with 512-dimensional hidden state for the EDU LSTM. The TreeLSTM composition function also has a 512-dimensional (in total) hidden state with 100-dimensional relation embeddings. The stack LSTM and decoder LSTM also have"
D19-1233,Q17-1012,0,0.0176644,"(Section 3.3.1) and a set of structural features extracted from the stack (Section 3.3.2). 3.3.1 Neural Embeddings To produce the stack embedding, we first require embeddings for both EDUs and units. We embed EDUs with bidirectional LSTMs4 . If e is an EDU consisting of the word sequence w1:k , then (→) (w1:k , h→ h→ 0 ) k = LSTM (←) h← (wk:1 , h← 0 ) k = LSTM (2) where wt is the word embedding of wt . The embedding for e, hEDU (e), is the concatenation of the final forward and backward hidden states: ← hEDU (e) = [h→ k ; hk ] (3) We embed units by composing their arguments with a Tree LSTM5 (Teng and Zhang, 2017). A Tree LSTM recursively composes vectors while using memory cells to track long-term dependencies. We produce a new representation for each EDU e by applying a linear transformation 4 We track memory cells and use them when updating the hidden state in LSTMs and Tree LSTMs, but use only the hidden states for stack embeddings. Initial hidden states and memory cells are learned parameters. 5 Since constituency trees are n-ary branching, RNNGs for constituency parsing have used a bidirectional LSTM composition function (Dyer et al., 2016; Kuncoro et al., 2017, 2018) to compose the variable numb"
D19-1233,C18-1047,0,0.159014,"wing this study we evaluate using micro-averaged F1 scores on labelled attachment decisions as calculated by the EDUCE python package8 . We report F1 for predicting span attachments (S), span attachments with nuclearity (N), span attachments with relation labels (R) and span attachments with nuclearity and relation labels (F). We compare our results against the numbers from Morey et al. (2017), since they include several competitive parers under a consistent evaluation scheme.9 As a baseline, we use a discriminative version of 8 https://github.com/irit-melodi/educe 9 We do not compare against Yu et al. (2018), Zhang et al. (2018) and Lin et al. (2019)’s recent neural RST parsers since they do not evaluate labelled attachment decisions so their results are not comparable to ours. Training and Hyperparameters We use 300-dimensional word embeddings initialized to word2vec vectors (Mikolov et al., 2013). We tie the embeddings in the EDU LSTM and the decoder LSTM input and output embeddings. We use a 2-layer bidirectional LSTM with 512-dimensional hidden state for the EDU LSTM. The TreeLSTM composition function also has a 512-dimensional (in total) hidden state with 100-dimensional relation embeddings."
D19-1233,D08-1059,1,0.674659,"= argmax p(x, y) (17) y∈Y(x) The search space grows exponentially with the input length, so we must perform inexact search as our model conditions on the entire relation structure of every subtree on the stack. Search is generally more difficult for generative models than for discriminative ones, requiring more complex search algorithms. For this reason, Dyer et al. (2016) used RNNGs only to rerank the output of a discriminative parser. Fried et al. (2017) presented the first algorithm for decoding directly from RNNGs to give competitive performance. They found that action-level beam search (Zhang and Clark, 2008) gave poor performance for constituency parsing with RNNGs. The problem was that GEN actions almost always have lower probabilities than structure-generating actions, causing computations where GEN actions come earlier to “fall off the beam” even if the completed computation would have a higher probability than other completed computations. To address this problem, Fried et al. (2017) proposed word-level beam search (Algorithm 1). Briefly, the algorithm keeps an array of beams indexed by the current position in the sequence and the number of structure-generating actions taken since this positi"
E03-1008,H91-1060,0,0.0320313,"Missing"
E03-1008,W99-0613,0,0.0609689,"Missing"
E03-1008,W01-0521,0,0.161063,"Missing"
E03-1008,J93-2004,0,0.0278308,"Missing"
E03-1008,W01-0501,0,0.185139,"Missing"
E03-1008,N01-1023,1,0.616825,"Missing"
E03-1008,P95-1026,0,0.323358,"Missing"
E03-1008,J03-4003,0,\N,Missing
E03-1008,P02-1046,0,\N,Missing
E03-1071,J96-1002,0,0.00385442,"have been successfully modelled in the ME framework, including POS tagging, with state of the art performance (van Halteren et al., 2001), ""supertagging"" (Clark, 2002) and chunking (Koeling, 2000). Generalised Iterative Scaling (GIs) is a very simple algorithm for estimating the parameters of a ME model. The original formulation of GIS (Darroch and Ratcliff, 1972) required the sum of the feature values for each event to be constant. Since this is not the case for many applications, the standard method is to add a ""correction"", or ""slack"", feature to each event Improved Iterative Scaling (us) (Berger et al., 1996; Della Pietra et al., 1997) eliminated the correction feature to improve the convergence rate of the algorithm. However, the extra book keeping required for us means that GIS is often faster in practice (Malouf, 2002). This paper shows, by a simple adaptation of Berger&apos;s proof for the convergence of HS (Berger, 1997), that GIS does not require a correction feature. We also investigate how the use of a correction feature affects the performance of ME taggers. GIS and HS obtain a maximum likelihood estimate (mLE) of the parameters, and, like other MLE methods, are susceptible to overfitting. A"
E03-1071,A00-1031,0,0.106095,"Missing"
E03-1071,W02-1001,0,0.051123,"Missing"
E03-1071,W00-1308,0,0.0407202,"Missing"
E03-1071,J01-2002,0,0.0399112,"Missing"
E03-1071,P02-1002,0,0.0122848,"Missing"
E03-1071,hockenmaier-steedman-2002-acquiring,0,0.0121682,"Missing"
E03-1071,P99-1069,0,0.166136,"es for model smoothing. We show analytically and empirically that the correction feature, assumed to be required for the correctness of GIS, is unnecessary. We also explore the use of a Gaussian prior and a simple cutoff for smoothing. The experiments are performed with two tagsets: the standard Penn Treebank POS tagset and the larger set of lexical types from Combinatory Categorial Grammar. 1 Introduction The use of maximum entropy (ME) models has become popular in Statistical NLP; some example applications include part-of-speech (Pos) tagging (Ratnaparkhi, 1996), parsing (Ratnaparkhi, 1999; Johnson et al., 1999) and language modelling (Rosenfeld, 1996). Many tagging problems have been successfully modelled in the ME framework, including POS tagging, with state of the art performance (van Halteren et al., 2001), ""supertagging"" (Clark, 2002) and chunking (Koeling, 2000). Generalised Iterative Scaling (GIs) is a very simple algorithm for estimating the parameters of a ME model. The original formulation of GIS (Darroch and Ratcliff, 1972) required the sum of the feature values for each event to be constant. Since this is not the case for many applications, the standard method is to add a ""correction"", or"
E03-1071,W02-2018,0,0.312726,"ing (GIs) is a very simple algorithm for estimating the parameters of a ME model. The original formulation of GIS (Darroch and Ratcliff, 1972) required the sum of the feature values for each event to be constant. Since this is not the case for many applications, the standard method is to add a ""correction"", or ""slack"", feature to each event Improved Iterative Scaling (us) (Berger et al., 1996; Della Pietra et al., 1997) eliminated the correction feature to improve the convergence rate of the algorithm. However, the extra book keeping required for us means that GIS is often faster in practice (Malouf, 2002). This paper shows, by a simple adaptation of Berger&apos;s proof for the convergence of HS (Berger, 1997), that GIS does not require a correction feature. We also investigate how the use of a correction feature affects the performance of ME taggers. GIS and HS obtain a maximum likelihood estimate (mLE) of the parameters, and, like other MLE methods, are susceptible to overfitting. A simple technique used to avoid overfitting is a frequency cutoff, in which only frequently occurring features are included in the model (Ratnaparkhi, 1998). However, more sophisticated smoothing techniques exist, such"
E03-1071,W00-0729,0,\N,Missing
E12-1075,J05-3002,0,0.0596554,"her when the N -gram model is incorporated. Table 6 compares our system with Z&C using lexical category pruning (β = 0.0001) and a 5s timeout for fair comparison. The results are similar to Table 5: our large-margin training systems outperforms the baseline by 1.5 BLEU points, and adding the N -gram model gave a further 1.4 point improvement. The scores could be significantly increased by using a larger timeout, as shown in our earlier development experiments. 7 Related Work There is a recent line of research on text-totext generation, which studies the linearization of dependency structures (Barzilay and McKeown, 2005; Filippova and Strube, 2007; Filippova and Strube, 2009; Bohnet et al., 2010; Guo et al., 2011). Unlike our system, and Wan et al. (2009), input dependencies provide additional information to these systems. Although the search space can be constrained by the assumption of projectivity, permutation of modifiers of the same head word makes exact inference for tree linearization intractable. The above systems typically apply approximate inference, such as beam-search. While syntax-based features are commonly used by these systems for linearization, Filippova and Strube (2009) apply a trigram mod"
E12-1075,C10-1009,1,0.500779,"Missing"
E12-1075,C10-1012,0,0.122116,"ng lexical category pruning (β = 0.0001) and a 5s timeout for fair comparison. The results are similar to Table 5: our large-margin training systems outperforms the baseline by 1.5 BLEU points, and adding the N -gram model gave a further 1.4 point improvement. The scores could be significantly increased by using a larger timeout, as shown in our earlier development experiments. 7 Related Work There is a recent line of research on text-totext generation, which studies the linearization of dependency structures (Barzilay and McKeown, 2005; Filippova and Strube, 2007; Filippova and Strube, 2009; Bohnet et al., 2010; Guo et al., 2011). Unlike our system, and Wan et al. (2009), input dependencies provide additional information to these systems. Although the search space can be constrained by the assumption of projectivity, permutation of modifiers of the same head word makes exact inference for tree linearization intractable. The above systems typically apply approximate inference, such as beam-search. While syntax-based features are commonly used by these systems for linearization, Filippova and Strube (2009) apply a trigram model to control local fluency within constituents. A dependencybased N-gram mod"
E12-1075,J93-2003,0,0.0273705,"+ Φ(e+ ) − Φ(e− ) 2 k Φ(e+ ) − Φ(e− ) k In this update, the global feature vectors Φ(e+ ) and Φ(e− ) are used. Unlike Z&C, the scores of sub-edges of e+ and e− are also udpated, so that the sub-edges of e− are less prioritized than those of e+ . We show empirically that this training algorithm significantly outperforms the perceptron training of the baseline system in Section 5. An advantage of our new training algorithm is that it enables the accommodation of a separately trained N -gram model into the system. 4 Incorporating an N-gram language model Since the seminal work of the IBM models (Brown et al., 1993), N -gram language models es ∈e Here gδ (e) = α · gδfour (e) + β · gδtri (e) + γ · gδbi (e) is the sum of log-probabilities of the new N grams resulting from the construction of e. For leaf edges and unary-branching edges, no new N grams result from their construction (i.e. gδ = 0). For a binary-branching edge, new N -grams result from the surface-string concatenation of its subedges. The sum of log-probabilities of the new fourgrams, trigrams and bigrams contribute to gδ with weights α, β and γ, respectively. For training, there are at least three methods to tune α, β, γ and θ. One simple met"
E12-1075,J98-2004,0,0.0347926,"D(new, e′ ) end if end for for e′ ∈ new do A DD(a, e′ ) end for A DD(c, e) end while decoding finishes. Otherwise it is extended with unary rules, and combined with existing edges in the chart using binary rules to produce new edges. The resulting edges are scored and put onto the agenda, while the original edge is put onto the chart. The process repeats until a goal edge is found, or a timeout limit is reached. In the latter case, a default output is produced using existing edges in the chart. Pseudocode for the decoder is shown as Algorithm 1. Again it is reminiscent of a best-first parser (Caraballo and Charniak, 1998) in the use of an agenda and a chart, but is fundamentally different due to the fact that there is no input order. 2.3 Statistical model and feature templates The baseline system uses a linear model to score hypotheses. For an edge e, its score is defined as: f (e) = Φ(e) · θ, where Φ(e) represents the feature vector of e and θ is the parameter vector of the model. During decoding, feature vectors are computed incrementally. When an edge is constructed, its score is computed from the scores of its sub-edges and the incrementally added structure: f (e) = Φ(e) · θ   X  Φ(es ) + φ(e) · θ = es"
E12-1075,J07-2003,0,0.382627,"ifficult problem. Finding the best permutation for a set of words according to a bigram language model, for example, is NP-hard, which can be proved by linear reduction from the traveling salesman problem. In practice, exploring the whole search space of permutations is often prevented by adding constraints. Stephen Clark University of Cambridge Computer Laboratory sc609@cam.ac.uk In phrase-based machine translation (Koehn et al., 2003; Koehn et al., 2007), a distortion limit is used to constrain the position of output phrases. In syntax-based machine translation systems such as Wu (1997) and Chiang (2007), synchronous grammars limit the search space so that polynomial time inference is feasible. In fluency improvement (Blackwood et al., 2010), parts of translation hypotheses identified as having high local confidence are held fixed, so that word ordering elsewhere is strictly local. Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search. Wan et al. (2009) uses a dependency grammar to solve word ordering, and Zhang and Clark (2011) uses CCG (Steedman, 2000) for word ordering and word choice. The use of syntax models makes th"
E12-1075,J07-4004,1,0.915656,"t of input words, the baseline system builds a CCG derivation by choosing and ordering words from the input set. The scoring model is trained using CCGBank (Hockenmaier and Steedman, 2007), and best-first decoding is applied. We apply the same decoding framework in this paper, but apply an improved training process, and incorporate an N -gram language model into the syntax model. In this section, we describe and discuss the baseline statistical model and decoding framework, motivating our extensions. 2.1 Combinatory Categorial Grammar CCG , and parsing with CCG , has been described elsewhere (Clark and Curran, 2007; Hockenmaier and Steedman, 2002); here we provide only a short description. CCG (Steedman, 2000) is a lexicalized grammar formalism, which associates each word in a sentence with a lexical category. There is a small number of basic lexical categories, such as noun (N), noun phrase (NP), and prepositional phrase (PP). Complex lexical categories are formed recursively from basic categories and slashes, which indicate the directions of arguments. The CCG grammar used by our system is read off the derivations in CCGbank, following Hockenmaier and Steedman (2002), meaning that the CCG combinatory"
E12-1075,P07-1041,0,0.123404,"s incorporated. Table 6 compares our system with Z&C using lexical category pruning (β = 0.0001) and a 5s timeout for fair comparison. The results are similar to Table 5: our large-margin training systems outperforms the baseline by 1.5 BLEU points, and adding the N -gram model gave a further 1.4 point improvement. The scores could be significantly increased by using a larger timeout, as shown in our earlier development experiments. 7 Related Work There is a recent line of research on text-totext generation, which studies the linearization of dependency structures (Barzilay and McKeown, 2005; Filippova and Strube, 2007; Filippova and Strube, 2009; Bohnet et al., 2010; Guo et al., 2011). Unlike our system, and Wan et al. (2009), input dependencies provide additional information to these systems. Although the search space can be constrained by the assumption of projectivity, permutation of modifiers of the same head word makes exact inference for tree linearization intractable. The above systems typically apply approximate inference, such as beam-search. While syntax-based features are commonly used by these systems for linearization, Filippova and Strube (2009) apply a trigram model to control local fluency"
E12-1075,N09-2057,0,0.0459596,"ares our system with Z&C using lexical category pruning (β = 0.0001) and a 5s timeout for fair comparison. The results are similar to Table 5: our large-margin training systems outperforms the baseline by 1.5 BLEU points, and adding the N -gram model gave a further 1.4 point improvement. The scores could be significantly increased by using a larger timeout, as shown in our earlier development experiments. 7 Related Work There is a recent line of research on text-totext generation, which studies the linearization of dependency structures (Barzilay and McKeown, 2005; Filippova and Strube, 2007; Filippova and Strube, 2009; Bohnet et al., 2010; Guo et al., 2011). Unlike our system, and Wan et al. (2009), input dependencies provide additional information to these systems. Although the search space can be constrained by the assumption of projectivity, permutation of modifiers of the same head word makes exact inference for tree linearization intractable. The above systems typically apply approximate inference, such as beam-search. While syntax-based features are commonly used by these systems for linearization, Filippova and Strube (2009) apply a trigram model to control local fluency within constituents. A depen"
E12-1075,N10-1115,0,0.0219988,"score of an edge e becomes: the perspective of correctness, it is unnecessary to find a margin between the sub-edges of e+ and those of e− , since both are gold-standard edges. However, since the score of an edge not only represents its correctness, but also affects its priority on the agenda, promoting the sub-edge of e+ can lead to “easier” edges being constructed before “harder” ones (i.e. those that are less likely to be correct), and therefore improve the output accuracy. This perspective has been observed by other works of learning-guided-search (Shen et al., 2007; Shen and Joshi, 2008; Goldberg and Elhadad, 2010). Intuitively, the score difference between easy gold-standard and harder gold-standard edges should not be as great as the difference between gold-standard and non-goldstandard edges. The perceptron update cannot provide such control of separation, because the amount of update is fixed to 1. As described earlier, we treat parameter update as finding a separation between correct and incorrect edges, in which the global feature vectors Φ, rather than φ, are considered. Given a positive example e+ and a negative example e− , we make a minimum update so that the score of e+ is higher than that of"
E12-1075,W11-2833,0,0.102068,"Missing"
E12-1075,P02-1043,0,0.0497799,"seline system builds a CCG derivation by choosing and ordering words from the input set. The scoring model is trained using CCGBank (Hockenmaier and Steedman, 2007), and best-first decoding is applied. We apply the same decoding framework in this paper, but apply an improved training process, and incorporate an N -gram language model into the syntax model. In this section, we describe and discuss the baseline statistical model and decoding framework, motivating our extensions. 2.1 Combinatory Categorial Grammar CCG , and parsing with CCG , has been described elsewhere (Clark and Curran, 2007; Hockenmaier and Steedman, 2002); here we provide only a short description. CCG (Steedman, 2000) is a lexicalized grammar formalism, which associates each word in a sentence with a lexical category. There is a small number of basic lexical categories, such as noun (N), noun phrase (NP), and prepositional phrase (PP). Complex lexical categories are formed recursively from basic categories and slashes, which indicate the directions of arguments. The CCG grammar used by our system is read off the derivations in CCGbank, following Hockenmaier and Steedman (2002), meaning that the CCG combinatory rules are encoded as rule instanc"
E12-1075,J07-3004,0,0.0135301,"m for the challenging task of the general word ordering problem. Second, we develop a novel method for incorporating a large-scale language model into a syntax-based generation system. Finally, we analyse large-margin training in the context of learning-guided best-first search, offering a novel solution to this computationally hard problem. 2 The statistical model and decoding algorithm We take Z&C as our baseline system. Given a multi-set of input words, the baseline system builds a CCG derivation by choosing and ordering words from the input set. The scoring model is trained using CCGBank (Hockenmaier and Steedman, 2007), and best-first decoding is applied. We apply the same decoding framework in this paper, but apply an improved training process, and incorporate an N -gram language model into the syntax model. In this section, we describe and discuss the baseline statistical model and decoding framework, motivating our extensions. 2.1 Combinatory Categorial Grammar CCG , and parsing with CCG , has been described elsewhere (Clark and Curran, 2007; Hockenmaier and Steedman, 2002); here we provide only a short description. CCG (Steedman, 2000) is a lexicalized grammar formalism, which associates each word in a"
E12-1075,N03-1017,0,0.0251502,"order for a multiset of words. The word ordering problem can also include word choice, where only a subset of the input words are used to produce the output. Word ordering is a difficult problem. Finding the best permutation for a set of words according to a bigram language model, for example, is NP-hard, which can be proved by linear reduction from the traveling salesman problem. In practice, exploring the whole search space of permutations is often prevented by adding constraints. Stephen Clark University of Cambridge Computer Laboratory sc609@cam.ac.uk In phrase-based machine translation (Koehn et al., 2003; Koehn et al., 2007), a distortion limit is used to constrain the position of output phrases. In syntax-based machine translation systems such as Wu (1997) and Chiang (2007), synchronous grammars limit the search space so that polynomial time inference is feasible. In fluency improvement (Blackwood et al., 2010), parts of translation hypotheses identified as having high local confidence are held fixed, so that word ordering elsewhere is strictly local. Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search. Wan et al. (200"
E12-1075,P07-2045,0,0.00438473,"t of words. The word ordering problem can also include word choice, where only a subset of the input words are used to produce the output. Word ordering is a difficult problem. Finding the best permutation for a set of words according to a bigram language model, for example, is NP-hard, which can be proved by linear reduction from the traveling salesman problem. In practice, exploring the whole search space of permutations is often prevented by adding constraints. Stephen Clark University of Cambridge Computer Laboratory sc609@cam.ac.uk In phrase-based machine translation (Koehn et al., 2003; Koehn et al., 2007), a distortion limit is used to constrain the position of output phrases. In syntax-based machine translation systems such as Wu (1997) and Chiang (2007), synchronous grammars limit the search space so that polynomial time inference is feasible. In fluency improvement (Blackwood et al., 2010), parts of translation hypotheses identified as having high local confidence are held fixed, so that word ordering elsewhere is strictly local. Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search. Wan et al. (2009) uses a dependency"
E12-1075,P02-1040,0,0.0899325,"and does not add to its asymptotic complexity, due to the heuristic nature of the decoder. 5 Experiments We use sections 2–21 of CCGBank to train our syntax model, section 00 for development and section 23 for the final test. Derivations from CCGBank are transformed into inputs by turning their surface strings into multi-sets of words. Following Z&C, we treat base noun phrases (i.e. NP s that do not recursively contain other NPs) as atomic units for the input. Output sequences are compared with the original sentences to evaluate their quality. We follow previous work and use the BLEU metric (Papineni et al., 2002) to compare outputs with references. Z&C use two methods to construct leaf edges. The first is to assign lexical categories according to a dictionary. There are 26.8 lexical categories for each word on average using this method, corresponding to 26.8 leaf edges. The other method is to use a pre-processing step — a CCG supertagger (Clark and Curran, 2007) — to prune candidate lexical categories according to the goldCCGBank training development GigaWord v4 AFP XIN Sentences 39,604 1,913 Sentences 30,363,052 15,982,098 Tokens 929,552 45,422 Tokens 684,910,697 340,666,976 Table 1: Number of senten"
E12-1075,P11-1008,0,0.0150784,"achine translation systems, both of which apply a score from the N gram model component in a derivation-building process. As discussed earlier, polynomial-time decoding is typically feasible for syntax-based machine translation systems without an N -gram language model, due to constraints from the grammar. In these cases, incorporation of N gram language models can significantly increase the complexity of a dynamic-programming decoder (Bar-Hillel et al., 1961). Efficient search has been achieved using chart pruning (Chiang, 2007) and iterative numerical approaches to constrained optimization (Rush and Collins, 2011). In contrast, the incorporation of an N -gram language model into our decoder is more straightforward, and does not add to its asymptotic complexity, due to the heuristic nature of the decoder. 5 Experiments We use sections 2–21 of CCGBank to train our syntax model, section 00 for development and section 23 for the final test. Derivations from CCGBank are transformed into inputs by turning their surface strings into multi-sets of words. Following Z&C, we treat base noun phrases (i.e. NP s that do not recursively contain other NPs) as atomic units for the input. Output sequences are compared w"
E12-1075,D08-1052,0,0.118457,"ax model, so that the score of an edge e becomes: the perspective of correctness, it is unnecessary to find a margin between the sub-edges of e+ and those of e− , since both are gold-standard edges. However, since the score of an edge not only represents its correctness, but also affects its priority on the agenda, promoting the sub-edge of e+ can lead to “easier” edges being constructed before “harder” ones (i.e. those that are less likely to be correct), and therefore improve the output accuracy. This perspective has been observed by other works of learning-guided-search (Shen et al., 2007; Shen and Joshi, 2008; Goldberg and Elhadad, 2010). Intuitively, the score difference between easy gold-standard and harder gold-standard edges should not be as great as the difference between gold-standard and non-goldstandard edges. The perceptron update cannot provide such control of separation, because the amount of update is fixed to 1. As described earlier, we treat parameter update as finding a separation between correct and incorrect edges, in which the global feature vectors Φ, rather than φ, are considered. Given a positive example e+ and a negative example e− , we make a minimum update so that the score"
E12-1075,P07-1096,0,0.0346909,"cores into our syntax model, so that the score of an edge e becomes: the perspective of correctness, it is unnecessary to find a margin between the sub-edges of e+ and those of e− , since both are gold-standard edges. However, since the score of an edge not only represents its correctness, but also affects its priority on the agenda, promoting the sub-edge of e+ can lead to “easier” edges being constructed before “harder” ones (i.e. those that are less likely to be correct), and therefore improve the output accuracy. This perspective has been observed by other works of learning-guided-search (Shen et al., 2007; Shen and Joshi, 2008; Goldberg and Elhadad, 2010). Intuitively, the score difference between easy gold-standard and harder gold-standard edges should not be as great as the difference between gold-standard and non-goldstandard edges. The perceptron update cannot provide such control of separation, because the amount of update is fixed to 1. As described earlier, we treat parameter update as finding a separation between correct and incorrect edges, in which the global feature vectors Φ, rather than φ, are considered. Given a positive example e+ and a negative example e− , we make a minimum up"
E12-1075,E09-1097,0,0.233416,"Ordering Incorporating a Large-Scale Language Model Yue Zhang University of Cambridge Computer Laboratory yz360@cam.ac.uk Graeme Blackwood University of Cambridge Engineering Department gwb24@eng.cam.ac.uk Abstract A fundamental problem in text generation is word ordering. Word ordering is a computationally difficult problem, which can be constrained to some extent for particular applications, for example by using synchronous grammars for statistical machine translation. There have been some recent attempts at the unconstrained problem of generating a sentence from a multi-set of input words (Wan et al., 2009; Zhang and Clark, 2011). By using CCG and learning guided search, Zhang and Clark reported the highest scores on this task. One limitation of their system is the absence of an N-gram language model, which has been used by text generation systems to improve fluency. We take the Zhang and Clark system as the baseline, and incorporate an N-gram model by applying online large-margin training. Our system significantly improved on the baseline by 3.7 BLEU points. 1 Introduction One fundamental problem in text generation is word ordering, which can be abstractly formulated as finding a grammatical o"
E12-1075,J97-3002,0,0.247543,"rdering is a difficult problem. Finding the best permutation for a set of words according to a bigram language model, for example, is NP-hard, which can be proved by linear reduction from the traveling salesman problem. In practice, exploring the whole search space of permutations is often prevented by adding constraints. Stephen Clark University of Cambridge Computer Laboratory sc609@cam.ac.uk In phrase-based machine translation (Koehn et al., 2003; Koehn et al., 2007), a distortion limit is used to constrain the position of output phrases. In syntax-based machine translation systems such as Wu (1997) and Chiang (2007), synchronous grammars limit the search space so that polynomial time inference is feasible. In fluency improvement (Blackwood et al., 2010), parts of translation hypotheses identified as having high local confidence are held fixed, so that word ordering elsewhere is strictly local. Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search. Wan et al. (2009) uses a dependency grammar to solve word ordering, and Zhang and Clark (2011) uses CCG (Steedman, 2000) for word ordering and word choice. The use of synt"
E12-1075,D11-1106,1,0.660638,"ting a Large-Scale Language Model Yue Zhang University of Cambridge Computer Laboratory yz360@cam.ac.uk Graeme Blackwood University of Cambridge Engineering Department gwb24@eng.cam.ac.uk Abstract A fundamental problem in text generation is word ordering. Word ordering is a computationally difficult problem, which can be constrained to some extent for particular applications, for example by using synchronous grammars for statistical machine translation. There have been some recent attempts at the unconstrained problem of generating a sentence from a multi-set of input words (Wan et al., 2009; Zhang and Clark, 2011). By using CCG and learning guided search, Zhang and Clark reported the highest scores on this task. One limitation of their system is the absence of an N-gram language model, which has been used by text generation systems to improve fluency. We take the Zhang and Clark system as the baseline, and incorporate an N-gram model by applying online large-margin training. Our system significantly improved on the baseline by 3.7 BLEU points. 1 Introduction One fundamental problem in text generation is word ordering, which can be abstractly formulated as finding a grammatical order for a multiset of w"
E14-1025,W13-3211,0,0.0118183,"arity judgements than the original full vectors. The second method is a weighted l2 normalisation of the vectors prior to application of singular value decomposition (SVD) (Deerwester et al., 1990) or compositional vector operators. It has the effect of drastically improving SVD with 100 or fewer dimensions. For example, we find that applying normalisation before SVD improves correlation from ρ  0.48 to ρ  0.70 for 20 dimensions, on the word similarity task. This is an essential finding as many more complex models of compositional semantics (Coecke et al., 2010; Baroni and Zamparelli, 2010; Andreas and Ghahramani, 2013) work with tensor objects and require good quality low-dimensional representations of words in order to lower computational costs. This technique also improves the performance of vector addition on texts of any length and vector elementwise product on shorter texts, on both the similarity and definitions tasks. The definition task and dataset are an additional contribution. We produced a new dataset of words and their definitions, which is separated into nine parts, each consisting of definitions of a particular length. This allows us to examine how compositional operators interact with CS and"
E14-1025,R13-1047,0,0.0209395,"tional vectors learned through co-occurrence statistics, infrequent phrases and novel constructions are impossible to represent in that way. The goal of compositional DSMs is to find methods of combining word vectors, or perhaps higher-order tensors, into a single vector that represents the meaning of the whole segment of text. Elementary approaches to composition employ simple operations, such as addition and elementwise product, directly on the word vectors. These have been shown to be effective for phrase similarity evaluation (Mitchell and Lapata, 2010) and detection of anomalous phrases (Kochmar and Briscoe, 2013). The methods that will be introduced in this paper can be applied to co-occurrence vectors to produce improvements on word similarity and compositional tasks with simple operators. We chose to examine the use of sum, elementwise product, and circular convolution (Jones and Mewhort, 2007), because they are often used due to their simplicity, or as components of more complex models (Zanzotto and Dell’Arciprete, 2011). The first method is context selection (CS), in which the top N highest weighted context words per vector are selected, and the rest of the values are discarded (by setting to zero"
E14-1025,D10-1115,0,0.779174,"onstrate these effects on standard word and phrase datasets, and on a new definition retrieval task and dataset. 1 Introduction Distributional semantic models (DSMs) (Turney and Pantel, 2010; Clarke, 2012) encode word meaning by counting co-occurrences with other words within a context window and recording these counts in a vector. Various IR and NLP tasks, such as word sense disambiguation, query expansion, and paraphrasing, take advantage of DSMs at a word level. More recently, researchers have been exploring methods that combine word vectors to represent phrases (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010) and sentences (Coecke et al., 2010; Socher et al., 2012). In this paper, we introduce two techniques that improve the quality of word vectors and can be easily tuned to adapt the vectors to particular lexical and compositional tasks. The quality of the word vectors is generally assessed on standard datasets that consist of a list of word pairs and a corresponding list of gold standard scores. These scores are gathered through an annotation task and reflect the similarity between the words as perceived by human judges (Bruni et 230 Proceedings of the 14th Conference of the European Chapter of"
E14-1025,P12-1015,0,0.0179659,"eighting scheme these are: 140 (tTest), 240 (PPMI), and 20 (Freq). Full ρ indicates the correlation when using full vectors without CS. ppmi freq 0.68 max max 0.66 max 0.64 0.62 0.6 0.58 0 1000 2000 3000 4000 5000 6000 0.8 8000 9000 10000 0.8 The cosine, Jaccard, and Lin similarity measures (Curran, 2004) were all used to ensure the results reflect genuine effects of context selection, and not an artefact of any particular similarity measure. The similarity measure and value of N were chosen, given a particular weighting scheme, to maximise correlation on the development part of the MEN data (Bruni et al., 2012) (MENdev). Testing was performed on the remaining section of MEN and the entire WS353 dataset (Finkelstein et al., 2002). The MEN dataset consists of 3,000 word pairs rated for similarity, which is divided into a 2,000-pair development set and a 1,000-pair test set. WS353 consists only of 353 pairs, but has been consistently used as a benchmark word similarity dataset throughout the past decade. ttest ppmi freq max max max 0.7 0.7 0.6 Spearman Spearman 0.6 0.5 0.4 ttest ppmi freq max max max 0.3 0.2 0.1 0 1000 2000 3000 0.5 0.4 0.3 0.2 0.1 4000 5000 6000 7000 8000 9000 Maximum nonzero elements"
E14-1025,J12-1002,0,0.0153269,"evel, and research has recently moved on to building distributional representations for larger segments of text. In this paper, we introduce novel ways of applying context selection and normalisation to vary model sparsity and the range of values of the DSM vectors. We show how these methods enhance the quality of the vectors and thus result in improved low dimensional and composed representations. We demonstrate these effects on standard word and phrase datasets, and on a new definition retrieval task and dataset. 1 Introduction Distributional semantic models (DSMs) (Turney and Pantel, 2010; Clarke, 2012) encode word meaning by counting co-occurrences with other words within a context window and recording these counts in a vector. Various IR and NLP tasks, such as word sense disambiguation, query expansion, and paraphrasing, take advantage of DSMs at a word level. More recently, researchers have been exploring methods that combine word vectors to represent phrases (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010) and sentences (Coecke et al., 2010; Socher et al., 2012). In this paper, we introduce two techniques that improve the quality of word vectors and can be easily tuned to adapt t"
E14-1025,D12-1110,0,0.0194373,"on a new definition retrieval task and dataset. 1 Introduction Distributional semantic models (DSMs) (Turney and Pantel, 2010; Clarke, 2012) encode word meaning by counting co-occurrences with other words within a context window and recording these counts in a vector. Various IR and NLP tasks, such as word sense disambiguation, query expansion, and paraphrasing, take advantage of DSMs at a word level. More recently, researchers have been exploring methods that combine word vectors to represent phrases (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010) and sentences (Coecke et al., 2010; Socher et al., 2012). In this paper, we introduce two techniques that improve the quality of word vectors and can be easily tuned to adapt the vectors to particular lexical and compositional tasks. The quality of the word vectors is generally assessed on standard datasets that consist of a list of word pairs and a corresponding list of gold standard scores. These scores are gathered through an annotation task and reflect the similarity between the words as perceived by human judges (Bruni et 230 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 230–"
E14-1025,W11-1302,0,0.0820026,"Missing"
E17-2029,D16-1230,0,0.136762,"Missing"
E17-2029,K16-1002,0,0.11223,"al., 2014; Sohn et al., 2015; Miao et al., 2016) , and approximate the posterior P (z|X, Y ) with a proposal distribution Q(z|X, Y ), which in our case is a diagonal Gaussian whose parameters depend on X and Y . We thus have the following evidence lower bound (ELBO) for the loglikelihood of the data: Figure 1: A schematic of how our model is implemented. Please see the text for full details. terior and the prior, and the cross-entropy loss between the model distribution and the data distribution. If the model can encode useful information into z, then the KL divergence term will be non-zero (Bowman et al., 2016). As our model decoder is given a deterministic representation of X already, z will then encode information about the variation in replies to X. 2.2 Model Implementation Given an input sentence X and a response Y , we run two separate bidirectional RNNs over their word embeddings xi and yi . We concatenate the final states of each and pass them through a single nonlinear layer to obtain our representations hx and hy of X and Y . We use GRUs (Cho et al., 2014) as our RNN cell as a compromise between expressive power and computational cost. We calculate the mean and variance of Q as: log P (Y |X"
E17-2029,tiedemann-2012-parallel,0,0.0322626,"Missing"
E17-2084,W15-0107,1,0.825793,"n concept representation relies on salient attributes or properties1 (Tyler et al., 2000; Randall et al., 2004). Property norm datasets (McRae et al., 2005; Devereux et al., 2013) are constructed by asking human participants to identify the most important attributes of a concept (see Table 1) and are widely used to test models of conceptual representation (McRae et al., 1997; Randall et al., 2004; Cree et al., 2006; Tyler et al., 2000; Grondin et al., 2009). Yet, to the best of our knowledge, such property norms have not been investigated in the context of metaphor processing. Recent studies (Fagarasan et al., 2015; Bulat et al., 2016) have shown that wide-coverage property-norm based semantic representations can be automatically constructed using cross-modal maps and that these perform comparably to dense semantic representations (Mikolov et al., 2013) One of the key problems in computational metaphor modelling is finding the optimal level of abstraction of semantic representations, such that these are able to capture and generalise metaphorical mechanisms. In this paper we present the first metaphor identification method that uses representations constructed from property norms. Such norms have been p"
E17-2084,W13-0908,0,0.0966211,"Missing"
E17-2084,C10-1113,1,0.895153,"ut the paper we will be using the terms properties and attributes interchangeably. 523 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 523–528, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics SHOES has_heels, 15 has_laces, 13 worn_on_feet, 13 ANT an_insect, 18 is_small, 18 is_black 15 cepts in terms of more concrete or physical experiences. They developed a method to automatically measure concreteness of words and applied it to identify verbal and adjectival metaphors. Shutova et al. (2010) pointed out that the metaphorical uses of words constitute a large portion of the dependency features extracted for abstract concepts from corpora. As a result, distributional clustering of abstract nouns with such features identifies groups of diverse concepts metaphorically associated with the same source domain. Shutova et al. (2010) exploit this property of co-occurrence vectors to identify new metaphorical mappings starting from a set of examples. Shutova and Sun (2013) used hierarchical clustering to derive a network of concepts in which metaphorical associations are learned in an unsup"
E17-2084,W13-0907,0,0.366933,"m entropy classifier and the verbs’ nominal arguments and their semantic roles as features. Dunn (2013) used a logistic regression classifier and high-level properties of concepts extracted from the SUMO ontology, including domain types (ABSTRACT, PHYSICAL , SOCIAL , MENTAL ) and event status (PROCESS , STATE , OBJECT). Tsvetkov et al. (2013) also used logistic regression and coarse semantic features, such as concreteness, animateness, named entity types and WordNet supersenses. They have shown that the model learned with such coarse semantic features is portable across languages. The work of Hovy et al. (2013) is notable as they focused on compositional features. They trained an SVM with dependency-tree kernels to capture compositional information, using lexical, part-of-speech tag and WordNet supersense representations of parse trees. Mohler et al. (2013) derived semantic signatures of texts as sets of highly-related and interlinked WordNet synsets. The semantic signatures served as features to train a set of classifiers (maximum entropy, decision trees, SVM, random forest) that map new metaphors to the semantic signatures of the known ones. Turney et al. (2011) hypothesized that metaphor is commo"
E17-2084,N16-1020,1,0.812966,"sets of highly-related and interlinked WordNet synsets. The semantic signatures served as features to train a set of classifiers (maximum entropy, decision trees, SVM, random forest) that map new metaphors to the semantic signatures of the known ones. Turney et al. (2011) hypothesized that metaphor is commonly used to describe abstract conMethod Learning dense linguistic representations We construct two types of linguistic representations: context-predicting – based on the skip-gram model of Mikolov et al. (2013) – and contextcounting. We employ 100-dimensional word embeddings constructed by Shutova et al. (2016) from Wikipedia using the standard log-linear skipgram model with negative sampling of Mikolov et al. (2013). The embeddings were trained using a symmetric window of 5 words either side of the target word, 10 negative samples per word-context pair and number of epochs set to 3. EMBED SVD We use Wikipedia to build count-based distributional vectors, using the top 10K most frequent lemmatised words (excluding stopwords) as contexts. Context windows are defined as sentence boundaries and counts are re-weighted using positive pointwise mutual information (PPMI). We obtain 100-dimensional dense sem"
E17-2084,W13-0906,0,0.533176,"odel of cross-domain property projection in metaphorical language. 2 3 Related work 3.1 Much previous research on metaphor processing casts the problem as classification of linguistic expressions as metaphorical or literal. Gedigian et al. (2006) classified verbs using a maximum entropy classifier and the verbs’ nominal arguments and their semantic roles as features. Dunn (2013) used a logistic regression classifier and high-level properties of concepts extracted from the SUMO ontology, including domain types (ABSTRACT, PHYSICAL , SOCIAL , MENTAL ) and event status (PROCESS , STATE , OBJECT). Tsvetkov et al. (2013) also used logistic regression and coarse semantic features, such as concreteness, animateness, named entity types and WordNet supersenses. They have shown that the model learned with such coarse semantic features is portable across languages. The work of Hovy et al. (2013) is notable as they focused on compositional features. They trained an SVM with dependency-tree kernels to capture compositional information, using lexical, part-of-speech tag and WordNet supersense representations of parse trees. Mohler et al. (2013) derived semantic signatures of texts as sets of highly-related and interli"
E17-2084,P14-1024,0,0.63249,". 3.3 Metaphor classification We compare the performance of the aforementioned semantic representations (SVD, EMBED, ATTR - SVD and ATTR - EMBED ) on a metaphor classification task, in order to test our hypothesis as to whether attribute-based semantic representations provide better concept generalisations for metaphor modelling than the widely-used dense linguistic representations. We use an SVM (Joachims, 1998) to perform the classification3 . 4 4.1 Experiments Experimental data We evaluate our method using the dataset of adjective–noun pairs manually annotated for metaphoricity, created by Tsvetkov et al. (2014). This corpus was created by extracting the nouns that co-occur with a list of 1000 frequent adjectives in the TenTen Web Corpus4 using SketchEngine and in collections of metaphor on the Web. The data is divided into a training set (TSV- TRAIN) and test set (TSV- TEST). TSV- TRAIN contains 884 literal and 884 metaphorical pairs annotated for metaphoricity. TSV- TEST contains 100 literal and 100 metaphorical pairs, annotated by 5 annotators with an inter-annotator agreement of κ = 0.76. Table 3 shows a portion of the test set. Metaphorical phrases that depend on wider context for their interpre"
E17-2084,W13-0904,0,0.147262,"HYSICAL , SOCIAL , MENTAL ) and event status (PROCESS , STATE , OBJECT). Tsvetkov et al. (2013) also used logistic regression and coarse semantic features, such as concreteness, animateness, named entity types and WordNet supersenses. They have shown that the model learned with such coarse semantic features is portable across languages. The work of Hovy et al. (2013) is notable as they focused on compositional features. They trained an SVM with dependency-tree kernels to capture compositional information, using lexical, part-of-speech tag and WordNet supersense representations of parse trees. Mohler et al. (2013) derived semantic signatures of texts as sets of highly-related and interlinked WordNet synsets. The semantic signatures served as features to train a set of classifiers (maximum entropy, decision trees, SVM, random forest) that map new metaphors to the semantic signatures of the known ones. Turney et al. (2011) hypothesized that metaphor is commonly used to describe abstract conMethod Learning dense linguistic representations We construct two types of linguistic representations: context-predicting – based on the skip-gram model of Mikolov et al. (2013) – and contextcounting. We employ 100-dim"
E17-2084,D11-1063,0,0.450583,"ortable across languages. The work of Hovy et al. (2013) is notable as they focused on compositional features. They trained an SVM with dependency-tree kernels to capture compositional information, using lexical, part-of-speech tag and WordNet supersense representations of parse trees. Mohler et al. (2013) derived semantic signatures of texts as sets of highly-related and interlinked WordNet synsets. The semantic signatures served as features to train a set of classifiers (maximum entropy, decision trees, SVM, random forest) that map new metaphors to the semantic signatures of the known ones. Turney et al. (2011) hypothesized that metaphor is commonly used to describe abstract conMethod Learning dense linguistic representations We construct two types of linguistic representations: context-predicting – based on the skip-gram model of Mikolov et al. (2013) – and contextcounting. We employ 100-dimensional word embeddings constructed by Shutova et al. (2016) from Wikipedia using the standard log-linear skipgram model with negative sampling of Mikolov et al. (2013). The embeddings were trained using a symmetric window of 5 words either side of the target word, 10 negative samples per word-context pair and"
E17-2084,N13-1118,1,0.900525,"oped a method to automatically measure concreteness of words and applied it to identify verbal and adjectival metaphors. Shutova et al. (2010) pointed out that the metaphorical uses of words constitute a large portion of the dependency features extracted for abstract concepts from corpora. As a result, distributional clustering of abstract nouns with such features identifies groups of diverse concepts metaphorically associated with the same source domain. Shutova et al. (2010) exploit this property of co-occurrence vectors to identify new metaphorical mappings starting from a set of examples. Shutova and Sun (2013) used hierarchical clustering to derive a network of concepts in which metaphorical associations are learned in an unsupervised way. DISHWASHER an_appliance, 19 requires_soap, 15 is_electrical, 14 Table 1: Examples of properties from McRae et al. (2005) together with their production frequencies on standard word similarity tasks. In this paper we hypothesise that such attribute-based representations provide a suitable means for generalisation over the source and target domains in metaphorical language and test this hypothesis. Our results show that these property-based representations can perf"
E17-2084,shutova-teufel-2010-metaphor,1,0.789164,"ratory University of Cambridge es407@cam.ac.uk Introduction According to the Conceptual Metaphor Theory (Lakoff and Johnson, 1980), metaphors are not merely a linguistic, but also a cognitive phenomenon. They arise when one concept (or conceptual domain) can be understood in terms of the properties of another. For example, we interpret the metaphorical expression “He shot down my argument” by projecting our knowledge about battles (the source domain) onto our reasoning about arguments (the target domain). Multiple studies have established the prevalence of metaphor in language (Cameron, 2003; Shutova and Teufel, 2010) and confirmed the key role that it plays in human reasoning (Thibodeau and Boroditsky, 2011). These findings make computational processing of metaphor essential for any NLP application that is focused on semantics, from machine translation (Shutova, 2011) to 1 Throughout the paper we will be using the terms properties and attributes interchangeably. 523 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 523–528, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics SHOES has_he"
E17-2084,C08-2001,0,\N,Missing
E17-2084,W06-3506,0,\N,Missing
E17-2084,N16-1071,1,\N,Missing
J02-2003,W99-0901,0,0.744355,"h to the problem of estimating the frequencies of senses, by distributing the count for each noun in the data evenly among all senses of the noun:  f (n, v, r) ˆf (c, v, r) = (15) |cn(n)| n∈syn(c) where ˆf (c, v, r) is an estimate of the number of times that concept c appears in position r of verb v, and |cn(n) |is the cardinality of cn(n). This is the approach taken by Li and Abe (1998), Ribas (1995), and McCarthy (2000).7 Resnik (1998) explains how this apparently crude technique works surprisingly well. Alternative approaches are described in Clark and Weir (1999) (see also Clark [2001]), Abney and Light (1999), and Ciaramita and Johnson (2000). 4. Using a Chi-Square Test to Compare Probabilities In this section we show how to test whether p(v |c , r) changes significantly when considering a node higher in the hierarchy. Consider the problem of deciding whether p(run |canine, subj) is a good approximation of p(run |dog, subj). (canine is the parent of dog in WordNet.) To do this, the probabilities p(run |ci , subj) are compared using a chi-square test, where the ci are the children of canine. In this case, the null hypothesis of the test is that the probabilities p(run |ci , subj) are t"
J02-2003,W01-0703,0,0.0408907,"c, can be used to estimate p(c |v, r). (Recall that c denotes the set of concepts dominated by c , including c itself.) One possible approach would be simply to substitute c for the individual concept c. This is a poor solution, however, since p(c |v, r) is the conditional probability that 2 Angled brackets are used to denote concepts in the hierarchy. 3 The term predicate is used loosely here, in that the predicate does not have to be a semantic object but can simply be a word form. 4 A recent paper that extends the acquisition of selectional preferences to sense-sense relationships is Agirre and Martinez (2001). 189 Computational Linguistics Volume 28, Number 2 some noun denoting a concept in c appears in position r of verb v. For example, p(animal |run, subj) is the probability that some noun denoting a kind of animal appears in the subject position of the verb run. Probabilities of sets of concepts are obtained by summing over the concepts in the set: p(c |v, r) =  p(c |v, r) (1) c ∈c This means that p(animal |run, subj) is likely to be much greater than p(dog | run, subj) and thus is not a good approximation of p(dog |run, subj). What can be done, though, is to condition on sets o"
J02-2003,W00-1320,0,0.012491,"the extent of generalization, which can be tailored to particular tasks. We have also shown that the task performance is at least as good when using the Pearson chi-square statistic as when using the log-likelihood chi-square statistic. There are a number of ways in which this work could be extended. One possibility would be to use all the classes dominated by the hypernyms of a concept, rather than just one, to estimate the probability of the concept. An estimate would be obtained for each hypernym, and the estimates combined in a linear interpolation. An approach similar to this is taken by Bikel (2000), in the context of statistical parsing. There is still room for investigation of the hidden-data problem when data are used that have not been sense disambiguated. In this article, a very simple approach is taken, 13 χ2 performed slightly better than G2 using the smaller data set also. 204 Clark and Weir Class-Based Probability Estimation which is to split the count for a noun evenly among the noun’s senses. Abney and Light (1999) have tried a more motivated approach, using the expectation maximization algorithm, but with little success. The approach described in Clark and Weir (1999) is show"
J02-2003,A97-1052,0,0.201662,"= = f (r)   v ∈V c ∈C f (c , v , r)    c ∈c f (c , v, r)  ˆ(v |c , r) = f (c ,v,r) =  p   f (c ,r) v ∈V c ∈c f (c , v , r) (12) (13) (14) where f (c, v, r) is the number of (n, v, r) triples in the data in which n is being used to denote c, and V is the set of verbs in the data. The problem is that the estimates are defined in terms of frequencies of senses, whereas the data are assumed to be in the form of (n, v, r) triples: a noun, verb, and argument position. All the data used in this work have been obtained from the British National Corpus (BNC), using the system of Briscoe and Carroll (1997), which consists of a shallow-parsing component that is able to identify verbal arguments. We take a simple approach to the problem of estimating the frequencies of senses, by distributing the count for each noun in the data evenly among all senses of the noun:  f (n, v, r) ˆf (c, v, r) = (15) |cn(n)| n∈syn(c) where ˆf (c, v, r) is an estimate of the number of times that concept c appears in position r of verb v, and |cn(n) |is the cardinality of cn(n). This is the approach taken by Li and Abe (1998), Ribas (1995), and McCarthy (2000).7 Resnik (1998) explains how this apparently crude techniq"
J02-2003,J90-1003,0,0.047983,"ich, obj). A feature of the proposed generalization procedure is that comparing probabilities of the form p(v |C, r), where C is a class, is closely related to comparing ratios of probabilities of the form p(C |v, r)/p(C |r) (for a given verb and argument position): p(v |C, r) = p(C |v, r) p(v |r) p(C |r) (10) Note that, for a given verb and argument position, p(v |r) is constant across classes. Equation (10) is of interest because the ratio p(C |v, r)/p(C |r) can be interpreted as a measure of association between the verb v and class C. This ratio is similar to pointwise mutual information (Church and Hanks 1990) and also forms part of Resnik’s association score, which will be introduced in Section 6. Thus the generalization procedure can be thought of as one that finds “homogeneous” areas of the hierarchy, that is, areas consisting of classes that are associated to a similar degree with the verb (Clark and Weir 1999). Finally, we note that the proposed estimation method does not guarantee that the estimates form a probability distribution over the concepts in the hierarchy, and so a normalization factor is required: psc (c |v, r) =  (c|r) ˆp(v |[c, v, r], r) ˆpˆp(v|r) c ∈C  |r) ˆp(v |[c , v, r],"
J02-2003,C00-1028,0,0.172921,"ng the frequencies of senses, by distributing the count for each noun in the data evenly among all senses of the noun:  f (n, v, r) ˆf (c, v, r) = (15) |cn(n)| n∈syn(c) where ˆf (c, v, r) is an estimate of the number of times that concept c appears in position r of verb v, and |cn(n) |is the cardinality of cn(n). This is the approach taken by Li and Abe (1998), Ribas (1995), and McCarthy (2000).7 Resnik (1998) explains how this apparently crude technique works surprisingly well. Alternative approaches are described in Clark and Weir (1999) (see also Clark [2001]), Abney and Light (1999), and Ciaramita and Johnson (2000). 4. Using a Chi-Square Test to Compare Probabilities In this section we show how to test whether p(v |c , r) changes significantly when considering a node higher in the hierarchy. Consider the problem of deciding whether p(run |canine, subj) is a good approximation of p(run |dog, subj). (canine is the parent of dog in WordNet.) To do this, the probabilities p(run |ci , subj) are compared using a chi-square test, where the ci are the children of canine. In this case, the null hypothesis of the test is that the probabilities p(run |ci , subj) are the same for each child ci . By jud"
J02-2003,W99-0631,1,0.900416,"Note that, for a given verb and argument position, p(v |r) is constant across classes. Equation (10) is of interest because the ratio p(C |v, r)/p(C |r) can be interpreted as a measure of association between the verb v and class C. This ratio is similar to pointwise mutual information (Church and Hanks 1990) and also forms part of Resnik’s association score, which will be introduced in Section 6. Thus the generalization procedure can be thought of as one that finds “homogeneous” areas of the hierarchy, that is, areas consisting of classes that are associated to a similar degree with the verb (Clark and Weir 1999). Finally, we note that the proposed estimation method does not guarantee that the estimates form a probability distribution over the concepts in the hierarchy, and so a normalization factor is required: psc (c |v, r) =  (c|r) ˆp(v |[c, v, r], r) ˆpˆp(v|r) c ∈C  |r) ˆp(v |[c , v, r], r) ˆpˆp(c(v|r) (11) We use psc to denote an estimate obtained using our method (since the technique finds sets of semantically similar senses, or “similarity classes”) and [c, v, r] to denote the class chosen for concept c in position r of verb v; ˆp denotes a relative frequency estimate, and C denotes the set"
J02-2003,C00-1029,1,0.933173,"(NLP) tasks, such as structural disambiguation and statistical parsing, word sense disambiguation, anaphora resolution, and language modeling. To see how such knowledge can be used to resolve structural ambiguities, consider the following prepositional phrase attachment ambiguity: Example 1 Fred ate strawberries with a spoon. The ambiguity arises because the prepositional phrase with a spoon can attach to either strawberries or ate. The ambiguity can be resolved by noting that the correct sense of spoon is more likely to be an argument of “ate-with” than “strawberries-with” (Li and Abe 1998; Clark and Weir 2000). The problem with estimating a probability model defined over a large vocabulary of predicates and noun senses is that this involves a huge number of parameters, which results in a sparse-data problem. In order to reduce the number of parameters, we propose to define a probability model over senses in a semantic hierarchy and ∗ Division of Informatics, University of Edinburgh, 2 Buccleuch Place, Edinburgh, EH8 9LW, UK. E-mail: stephenc@cogsci.ed.ac.uk. † School of Cognitive and Computing Sciences, University of Sussex, Brighton, BN1 9QH, UK. E-mail: david.weir@cogs.susx.ac.uk. c 2002 Associat"
J02-2003,N01-1013,1,0.0995285,"Missing"
J02-2003,J93-1003,0,0.230062,"re statistic, denoted X2 :  (oij − eij )2 X2 = (16) eij i,j where oij is the observed value for the cell in row i and column j, and eij is the corresponding expected value. An alternative statistic is the log-likelihood chi-square statistic, denoted G2 :8  oij G2 = 2 oij loge (17) eij i,j The two statistics have similar values when the counts in the contingency table are large (Agresti 1996). The statistics behave differently, however, when the table contains low counts, and, since corpus data are likely to lead to some low counts, the question of which statistic to use is an important one. Dunning (1993) argues for the use of G2 rather than X2 , based on an analysis of the sampling distributions of G2 and X2 , and results obtained when using the statistics to acquire highly associated bigrams. We consider Dunning’s analysis at the end of this section, and the question of whether to use G2 or X2 will be discussed further there. For now, we continue with the discussion of how the chi-square test is used in the generalization procedure. For Table 1, the value of G2 is 3.8, and the value of X2 is 2.5. Assuming a level of significance of α = 0.05, the critical value is 12.6 (for six degrees of fre"
J02-2003,C96-1003,0,0.0305241,"Missing"
J02-2003,J98-2002,0,0.313739,"nguage processing (NLP) tasks, such as structural disambiguation and statistical parsing, word sense disambiguation, anaphora resolution, and language modeling. To see how such knowledge can be used to resolve structural ambiguities, consider the following prepositional phrase attachment ambiguity: Example 1 Fred ate strawberries with a spoon. The ambiguity arises because the prepositional phrase with a spoon can attach to either strawberries or ate. The ambiguity can be resolved by noting that the correct sense of spoon is more likely to be an argument of “ate-with” than “strawberries-with” (Li and Abe 1998; Clark and Weir 2000). The problem with estimating a probability model defined over a large vocabulary of predicates and noun senses is that this involves a huge number of parameters, which results in a sparse-data problem. In order to reduce the number of parameters, we propose to define a probability model over senses in a semantic hierarchy and ∗ Division of Informatics, University of Edinburgh, 2 Buccleuch Place, Edinburgh, EH8 9LW, UK. E-mail: stephenc@cogsci.ed.ac.uk. † School of Cognitive and Computing Sciences, University of Sussex, Brighton, BN1 9QH, UK. E-mail: david.weir@cogs.susx."
J02-2003,W97-0808,0,0.0608576,"pseudo-disambiguation task. Our method outperforms these alternatives on the pseudo-disambiguation task, and an analysis of the results shows that the generalization methods of Resnik and Li and Abe appear to be overgeneralizing, at least for this task. Note that the problem being addressed here is the engineering problem of estimating predicate argument probabilities, with the aim of producing estimates that will be useful for NLP applications. In particular, we are not addressing the problem of acquiring selectional restrictions in the way this is usually construed (Resnik 1993; Ribas 1995; McCarthy 1997; Li and Abe 1998; Wagner 2000). The purpose of using a semantic hierarchy for generalization is to overcome the sparse data problem, rather than find a level of abstraction that best represents the selectional restrictions of some predicate. This point is considered further in Section 5. The next section describes the noun hierarchy from WordNet and gives a more precise description of the probabilities to be estimated. Section 3 shows how a class from WordNet can be used to estimate the probability of a noun sense. Section 4 shows how a chi-square test is used as part of the generalization pr"
J02-2003,A00-2034,0,0.25818,"timate the probability of the sense? And second, given a particular noun sense, how can a suitable class be determined? This article offers novel solutions to both problems, and there is a particular focus on the second question, which can be thought of as how to find a suitable level of generalization in the hierarchy.1 The semantic hierarchy used here is the noun hierarchy of WordNet (Fellbaum 1998), version 1.6. Previous work has considered how to estimate probabilities using classes from WordNet in the context of acquiring selectional preferences (Resnik 1998; Ribas 1995; Li and Abe 1998; McCarthy 2000), and this previous work has also addressed the question of how to determine a suitable level of generalization in the hierarchy. Li and Abe use the minimum description length principle to obtain a level of generalization, and Resnik uses a simple technique based on a statistical measure of selectional preference. (The work by Ribas builds on that by Resnik, and the work by McCarthy builds on that by Li and Abe.) We compare our estimation method with those of Resnik and Li and Abe, using a pseudo-disambiguation task. Our method outperforms these alternatives on the pseudo-disambiguation task,"
J02-2003,N01-1011,0,0.00941752,"problem, moving up a node is desirable, and therefore we do not modify the test for tables with low counts. The final issue to consider is which chi-square statistic to use. Dunning (1993) argues for the use of G2 rather than X2 , based on the claim that the sampling distribution of G2 approaches the true chi-square distribution quicker than the sampling distribution of X2 . However, Agresti (1996, page 34) makes the opposite claim: “The sampling distributions of X2 and G2 get closer to chi-squared as the sample size n increases. . . . The convergence is quicker for X2 than G2 .” In addition, Pedersen (2001) questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read (1984), who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic. Finally, the results of the pseudo-disambiguation experiments presented in Section 7 are at least as good, if not better, when using X2 rather than G2 , and so we conclude that the question of which statistic to use should be answered on a per application basis. 5. The Generalization Procedure The procedure for finding a suitable class, c , to ge"
J02-2003,P93-1024,0,0.557123,"e approach described in Clark and Weir (1999) is shown in Clark (2001) to have some impact on the pseudo-disambiguation task, but only with certain values of the α parameter, and ultimately does not improve on the best performance. Finally, an issue that has not been much addressed in the literature (except by Li and Abe [1996]) is how the accuracy of class-based estimation techniques compare when automatically acquired classes, as opposed to the manually created classes from WordNet, are used. The pseudo-disambiguation task described here has also been used to evaluate clustering algorithms (Pereira, Tishby, and Lee, 1993; Rooth et al., 1999), but with different data, and so it is difficult to compare the results. A related issue is how the structure of WordNet affects the accuracy of the probability estimates. We have taken the structure of the hierarchy for granted, without any analysis, but it may be that an alternative design could be more conducive to probability estimation. Acknowledgments This article is an extended and updated version of a paper that appeared in the proceedings of NAACL 2001. The work on which it is based was carried out while the first author was a D.Phil. student at the University of"
J02-2003,E95-1016,0,0.27864,"can that class be used to estimate the probability of the sense? And second, given a particular noun sense, how can a suitable class be determined? This article offers novel solutions to both problems, and there is a particular focus on the second question, which can be thought of as how to find a suitable level of generalization in the hierarchy.1 The semantic hierarchy used here is the noun hierarchy of WordNet (Fellbaum 1998), version 1.6. Previous work has considered how to estimate probabilities using classes from WordNet in the context of acquiring selectional preferences (Resnik 1998; Ribas 1995; Li and Abe 1998; McCarthy 2000), and this previous work has also addressed the question of how to determine a suitable level of generalization in the hierarchy. Li and Abe use the minimum description length principle to obtain a level of generalization, and Resnik uses a simple technique based on a statistical measure of selectional preference. (The work by Ribas builds on that by Resnik, and the work by McCarthy builds on that by Li and Abe.) We compare our estimation method with those of Resnik and Li and Abe, using a pseudo-disambiguation task. Our method outperforms these alternatives on"
J02-2003,P99-1014,0,0.608779,"ich is close to entity and dominated by entity, has two parents: life form and causal agent. This DAG-like property was responsible for the overgeneralization, and so we removed the link between person and causal agent. This appeared to solve the problem, and the results presented later for the average degree of generalization do not show an overgeneralization compared with those given in Li and Abe (1998). 7. Pseudo-Disambiguation Experiments The task we used to compare the class-based estimation techniques is a decision task previously used by Pereira, Tishby, and Lee (1993) and Rooth et al. (1999). The task is to decide which of two verbs, v and v , is more likely to take a given noun, n, as an object. The test and training data were obtained as follows. A number of verb–direct object pairs were extracted from a subset of the BNC, using the system of Briscoe and Carroll. All those pairs containing a noun not in WordNet were removed, and each verb and argument was lemmatized. This resulted in a data set of around 1.3 million (v, n) pairs. To form a test set, 3,000 of these pairs were randomly selected such that each selected pair contained a fairly frequent verb. (Following Pereira, Ti"
J07-4004,J97-4005,0,0.0199829,"lla Pietra, Della Pietra, and Lafferty (1997), Ratnaparkhi motivates log-linear models from the perspective of maximizing entropy, subject to certain constraints. Ratnaparkhi models the various decisions made by a shift-reduce parser, using log-linear distributions defined over features of the local context in which a decision is made. The probabilities of each decision are multiplied together to give a score for the complete sequence of decisions, and beam search is used to find the most probable sequence, which corresponds to the most probable derivation. A different approach is proposed by Abney (1997), who develops log-linear models for attribute-value grammars, such as Head-driven Phrase Structure Grammar (HPSG). Rather than define a model in terms of parser moves, Abney defines a model directly over the syntactic structures licensed by the grammar. Another difference is that Abney uses a global model, in which a single log-linear model is defined over the complete space of attribute–value structures. Abney’s motivation for using log-linear models is to overcome various problems in applying models based on PCFGs directly to attributevalue grammars. A further motivation for using global mo"
J07-4004,E03-1036,0,0.136917,"Missing"
J07-4004,J99-2004,0,0.894269,"from CCGbank. A key component of the parsing system is a Maximum Entropy CCG supertagger (Ratnaparkhi 1996; Curran and Clark 2003) which assigns lexical categories to words in a sentence. The role of the supertagger is twofold. First, it makes discriminative estimation feasible by limiting the number of incorrect derivations for each training sentence; the supertagger can be thought of as supplying a number of incorrect but plausible lexical categories for each word in the sentence. Second, it greatly increases the efficiency of the parser, which was the original motivation for supertagging (Bangalore and Joshi 1999). One possible criticism of CCG has been that highly efficient parsing is not possible because of the additional “spurious” derivations. In fact, we show that a novel method which tightly integrates the supertagger and parser leads to parse times significantly faster than those reported for comparable parsers in the literature. The parser is evaluated on CCGbank (available through the Linguistic Data Consortium). In order to facilitate comparisons with parsers using different formalisms, we also evaluate on the publicly available DepBank (King et al. 2003), using the Briscoe and Carroll annota"
J07-4004,C04-1180,1,0.78349,"tical parsing with CCG will be described in Section 3. 3. Combinatory Categorial Grammar Combinatory Categorial Grammar (CCG) (Steedman 1996, 2000) is a type-driven lexicalized theory of grammar based on Categorial Grammar (Wood 1993). CCG lexical 498 Clark and Curran Wide-Coverage Efficient Statistical Parsing entries consist of a syntactic category, which defines valency and directionality, and a semantic interpretation. In this article we are concerned with the syntactic component; see Steedman (2000) for how a semantic interpretation can be composed during a syntactic derivation, and also Bos et al. (2004) for how semantic interpretations can be built for newspaper text using the wide-coverage parser described in this article. Categories can be either basic or complex. Examples of basic categories are S (sentence), N (noun), NP (noun phrase), and PP (prepositional phrase). Complex categories are built recursively from basic categories, and indicate the type and directionality of arguments (using slashes), and the type of the result. For example, the following category for the transitive verb bought specifies its first argument as a noun phrase to its right, its second argument as a noun phrase"
J07-4004,briscoe-carroll-2002-robust,0,0.00406905,"et al. 2003). Cahill et al. (2004) evaluate an LFG parser, which uses an automatically extracted grammar, against DepBank. Miyao and Tsujii (2004) evaluate their HPSG parser against PropBank (Palmer, Gildea, and Kingsbury 2005). Kaplan et al. (2004) compare the Collins parser with the Parc LFG parser by mapping Penn Treebank parses into the dependencies of DepBank, claiming that the LFG parser is more accurate with only a slight reduction in speed. Preiss (2003) compares the parsers of Collins and Charniak, the grammatical relations finder of Buchholz, Veenstra, and Daelemans (1999), and the Briscoe and Carroll (2002) parser, using the gold-standard grammatical relations (GRs) from Carroll, Briscoe, and Sanfilippo (1998). The Penn Treebank trees of the Collins and Charniak parsers, and the GRs of the Buchholz parser, are mapped into the required grammatical relations, with the result that the GR finder of Buchholz is the most accurate. There are a number of problems with such evaluations. The first is that, when converting the output of the Collins parser, for example, into the output of another parser, the Collins parser is at an immediate disadvantage. This is especially true if the alternative output is"
J07-4004,P06-2006,0,0.177279,"ent grammar formalisms. One question we are often asked is whether the CCG derivations 530 Clark and Curran Wide-Coverage Efficient Statistical Parsing output by the parser could be converted to Penn Treebank–style trees to enable a comparison with, for example, the Collins and Charniak parsers. The difficulty is that CCG derivations often have a different shape to the Penn Treebank analyses (coordination being a prime example) and reversing the mapping used by Hockenmaier to create CCGbank is a far from trivial task. There is some existing work comparing parser performance across formalisms. Briscoe and Carroll (2006) evaluate the RASP parser on the Parc Dependency Bank (DepBank; King et al. 2003). Cahill et al. (2004) evaluate an LFG parser, which uses an automatically extracted grammar, against DepBank. Miyao and Tsujii (2004) evaluate their HPSG parser against PropBank (Palmer, Gildea, and Kingsbury 2005). Kaplan et al. (2004) compare the Collins parser with the Parc LFG parser by mapping Penn Treebank parses into the dependencies of DepBank, claiming that the LFG parser is more accurate with only a slight reduction in speed. Preiss (2003) compares the parsers of Collins and Charniak, the grammatical re"
J07-4004,P06-4020,0,0.321388,"Missing"
J07-4004,W99-0629,0,0.0609457,"Missing"
J07-4004,P04-1047,0,0.0178952,"Missing"
J07-4004,P04-1041,0,0.0227422,"Missing"
J07-4004,A00-2018,0,0.30964,"article is to incorporate the multi-modal approach; Baldridge suggests that, as well as having theoretical motivation, a multi-modal approach can improve the efficiency of CCG parsing. 3.1 Why Use CCG for Statistical Parsing? CCG was designed to deal with the long-range dependencies inherent in certain constructions, such as coordination and extraction, and arguably provides the most linguistically satisfactory account of these phenomena. Long-range dependencies are relatively common in text such as newspaper text, but are typically not recovered by treebank parsers such as Collins (2003) and Charniak (2000). This has led to a number of proposals for post-processing the output of the Collins and Charniak parsers, in which trace sites are located and the antecedent of the trace determined (Johnson 2002; Dienes and Dubey 2003; Levy and Manning 2004). An advantage of using CCG is that 501 Computational Linguistics Volume 33, Number 4 the recovery of long-range dependencies can be integrated into the parsing process in a straightforward manner, rather than be relegated to such a post-processing phase (Clark, Hockenmaier, and Steedman 2002; Hockenmaier 2003a; Clark, Steedman, and Curran 2004). Another"
J07-4004,W02-2236,0,0.0645173,"Missing"
J07-4004,2000.iwpt-1.9,0,0.0787687,"Missing"
J07-4004,P00-1058,0,0.135957,"Missing"
J07-4004,W03-1013,1,0.6045,"vidual entry at the root of the subderivation which has the highest score for the class. The equivalence classes were defined so that any other individual entry cannot be part  of the highest scoring derivation for the sentence. The score for a subderivation d is i λi fi (d) where fi (d) is the number of times the ith feature occurs in the subderivation. The highest-scoring subderivations can be calculated recursively using the highest-scoring equivalence classes that were combined to create the individual entry. For the dependency model, the highest scoring dependency structure is required. Clark and Curran (2003) outline an algorithm for finding the most probable dependency structure, which keeps track of the highest scoring set of dependencies for each node in the chart. For a set of equivalent entries in the chart (a disjunctive node), this involves summing over all conjunctive node daughters which head sub-derivations leading to the same set of high scoring dependencies. In practice large numbers of such conjunctive nodes lead to very long parse times. As an alternative to finding the most probable dependency structure, we have developed an algorithm which maximizes the expected labeled recall over"
J07-4004,C04-1041,1,0.740435,"h is required to produce reasonable parse times. Thus the reduced accuracy could be due to implementation difficulties rather than the model itself. The use of conditional log-linear models in this article is designed to overcome some of the weaknesses identified in the approach of Clark, Hockenmaier, and Steedman 503 Computational Linguistics Volume 33, Number 4 (2002), and to offer a more flexible framework for including features than the generative models of Hockenmaier (2003a). For example, adding long-range dependency features to the log-linear model is straightforward. We also showed in Clark and Curran (2004b) that, in contrast with Hockenmaier (2003a), adding distance to the dependency features in the log-linear model does improve parsing accuracy. Another feature of conditional log-linear models is that they are trained discriminatively, by maximizing the conditional probability of each gold-standard parse relative to the incorrect parses for the sentence. Generative models, in contrast, are typically trained by maximizing the joint probability of the training sentence, parse pairs, even though the sentence does not need to be inferred. 3.3 CCGbank The treebank used in this article performs t"
J07-4004,P04-1014,1,0.852435,"h is required to produce reasonable parse times. Thus the reduced accuracy could be due to implementation difficulties rather than the model itself. The use of conditional log-linear models in this article is designed to overcome some of the weaknesses identified in the approach of Clark, Hockenmaier, and Steedman 503 Computational Linguistics Volume 33, Number 4 (2002), and to offer a more flexible framework for including features than the generative models of Hockenmaier (2003a). For example, adding long-range dependency features to the log-linear model is straightforward. We also showed in Clark and Curran (2004b) that, in contrast with Hockenmaier (2003a), adding distance to the dependency features in the log-linear model does improve parsing accuracy. Another feature of conditional log-linear models is that they are trained discriminatively, by maximizing the conditional probability of each gold-standard parse relative to the incorrect parses for the sentence. Generative models, in contrast, are typically trained by maximizing the joint probability of the training sentence, parse pairs, even though the sentence does not need to be inferred. 3.3 CCGbank The treebank used in this article performs t"
J07-4004,N06-1019,1,0.710915,"Missing"
J07-4004,P07-1032,1,0.715316,"Missing"
J07-4004,P02-1042,1,0.84187,"Missing"
J07-4004,W04-3215,1,0.736675,"Missing"
J07-4004,P96-1025,0,0.191085,"Parsing The work in this article began as part of the Edinburgh wide-coverage CCG parsing project (2000–2004). There has been some other work on defining stochastic categorial grammars, but mainly in the context of grammar learning (Osborne and Briscoe 1997; Watkinson and Manandhar 2001; Zettlemoyer and Collins 2005). An early attempt from the Edinburgh project at wide-coverage CCG parsing is presented in Clark, Hockenmaier, and Steedman (2002). In order to deal with the problem of the additional, nonstandard CCG derivations, a conditional model of dependency structures is presented, based on Collins (1996), in which the dependencies are modeled directly and derivations are not modeled at all. The conditional probability of a dependency structure π, given a sentence S, is factored into two parts. The first part is the probability of the lexical category sequence, C, and the second part is the dependency structure, D, giving P(π|S) = P(C|S)P(D|C, S). Intuitively, the category sequence is gen502 Clark and Curran Wide-Coverage Efficient Statistical Parsing erated first, conditioned on the sentence, and then attachment decisions are made to form the dependency links. The probability of the category"
J07-4004,J03-4003,0,0.685283,"described in this article is to incorporate the multi-modal approach; Baldridge suggests that, as well as having theoretical motivation, a multi-modal approach can improve the efficiency of CCG parsing. 3.1 Why Use CCG for Statistical Parsing? CCG was designed to deal with the long-range dependencies inherent in certain constructions, such as coordination and extraction, and arguably provides the most linguistically satisfactory account of these phenomena. Long-range dependencies are relatively common in text such as newspaper text, but are typically not recovered by treebank parsers such as Collins (2003) and Charniak (2000). This has led to a number of proposals for post-processing the output of the Collins and Charniak parsers, in which trace sites are located and the antecedent of the trace determined (Johnson 2002; Dienes and Dubey 2003; Levy and Manning 2004). An advantage of using CCG is that 501 Computational Linguistics Volume 33, Number 4 the recovery of long-range dependencies can be integrated into the parsing process in a straightforward manner, rather than be relegated to such a post-processing phase (Clark, Hockenmaier, and Steedman 2002; Hockenmaier 2003a; Clark, Steedman, and C"
J07-4004,W95-0103,0,0.0326873,"t. There is considerable flexibility in defining the features for a parsing model in our log-linear framework, as the long-range dependency example demonstrates, but the need for dynamic programming for both estimation and decoding reduces the range of features which can be used. Any extension to the “locality” of the features would reduce the effectiveness of the chart packing and any dynamic programming performed over the chart. Two possible extensions, which we have not investigated, include defining dependency features which account for all three elements of the triple in a PP-attachment (Collins and Brooks 1995), and defining a rule feature which includes the grandparent node (Johnson 1998). Another alternative for future work is to compare the dynamic programming approach taken here with the beam-search approach of Collins and Roark (2004), which allows more “global” features. 5.3 Calculating Feature Expectations For estimating both the normal-form model and the dependency model, the following expectation of each feature fi , with respect to some model Λ, is required: EΛ fi =  S 1 ZS  e λ · f ( ω ) fi ( ω ) (20) ω∈ρ(S) where ρ(S) is the set of all parses for sentence S, and λ is the vector of weig"
J07-4004,J05-1003,0,0.0949806,"hly one half. Future work will also look at maintaing the POS tag ambiguity through to the parsing stage. Currently we do not use the probabilities assigned to the lexical categories by the supertagger as part of the parse selection process. These scores could be incorporated as real-valued features, or as auxiliary functions, as in Johnson and Riezler (2000). We would also like to investigate using the generative model of Hockenmaier and Steedman (2002b) in a similar way. Using a generative model’s score as a feature in a discriminative framework has been beneficial for reranking approaches (Collins and Koo 2005). Because the generative model uses local features similar to those in our log-linear models, it could be incorporated into the estimation and decoding processes without the need for reranking. One way of improving the accuracy of a supertagger is to use the parser to provide large amounts of additional training data, by taking the lexical categories chosen by the parser as gold-standard training data. If enough unlabeled data is parsed, then the large volume can overcome the noise in the data (Steedman et al. 2002; Prins and van Noord 2003). We plan to investigate this idea in the context of"
J07-4004,P04-1015,0,0.0512372,"educes the range of features which can be used. Any extension to the “locality” of the features would reduce the effectiveness of the chart packing and any dynamic programming performed over the chart. Two possible extensions, which we have not investigated, include defining dependency features which account for all three elements of the triple in a PP-attachment (Collins and Brooks 1995), and defining a rule feature which includes the grandparent node (Johnson 1998). Another alternative for future work is to compare the dynamic programming approach taken here with the beam-search approach of Collins and Roark (2004), which allows more “global” features. 5.3 Calculating Feature Expectations For estimating both the normal-form model and the dependency model, the following expectation of each feature fi , with respect to some model Λ, is required: EΛ fi =  S 1 ZS  e λ · f ( ω ) fi ( ω ) (20) ω∈ρ(S) where ρ(S) is the set of all parses for sentence S, and λ is the vector of weights for Λ. This is essentially the same calculation for both models, even though for the dependency model, features can be defined in terms of dependencies as well as the derivations. Dependencies can be stored as part of the individ"
J07-4004,E03-1071,1,0.616549,"umerating all derivations is infeasible. To solve this problem, we have adapted the dynamic programming method of Miyao and Tsujii (2002) to packed CCG charts. A packed chart efficiently represents all derivations for a sentence. The dynamic programming method uses inside and outside scores to calculate expectations, similar to the inside–outside algorithm for estimating the parameters of a PCFG from unlabeled data (Lari and Young 1990). Generalized Iterative Scaling (Darroch and Ratcliff 1972) is a common choice in the NLP literature for estimating a log-linear model (e.g., Ratnaparkhi 1998; Curran and Clark 2003). Initially we used generalized iterative scaling (GIS) for the parsing models described here, but found that convergence was extremely slow; Sha and Pereira (2003) present a similar finding for globally optimized log-linear models for sequences. As an alternative to GIS, we use the limited-memory BFGS algorithm (Nocedal and Wright 1999). As Malouf (2002) demonstrates, general purpose numerical optimization algorithms such as BFGS can converge much faster than iterative scaling algorithms (including Improved Iterative Scaling; Della Pietra, Della Pietra, and Lafferty 1997). Despite the use of"
J07-4004,P06-1088,1,0.562802,"Missing"
J07-4004,P03-1055,0,0.00870733,"al Parsing? CCG was designed to deal with the long-range dependencies inherent in certain constructions, such as coordination and extraction, and arguably provides the most linguistically satisfactory account of these phenomena. Long-range dependencies are relatively common in text such as newspaper text, but are typically not recovered by treebank parsers such as Collins (2003) and Charniak (2000). This has led to a number of proposals for post-processing the output of the Collins and Charniak parsers, in which trace sites are located and the antecedent of the trace determined (Johnson 2002; Dienes and Dubey 2003; Levy and Manning 2004). An advantage of using CCG is that 501 Computational Linguistics Volume 33, Number 4 the recovery of long-range dependencies can be integrated into the parsing process in a straightforward manner, rather than be relegated to such a post-processing phase (Clark, Hockenmaier, and Steedman 2002; Hockenmaier 2003a; Clark, Steedman, and Curran 2004). Another advantage of CCG is that providing a compositional semantics for the grammar is relatively straightforward. It has a completely transparent interface between syntax and semantics and, because CCG is a lexicalized gramma"
J07-4004,P96-1011,0,0.730648,"literature to deal with certain linguistic phenomena, but we chose not to implement them. The reason is that adding new combinatory rules reduces the efficiency of the parser, and we felt that, in the case of substitution, for example, the small gain in grammatical coverage was not worth the reduction in speed. Section 9.3 discusses some of the choices we made when implementing the grammar. One way of dealing with the additional ambiguity in CCG is to only consider normal-form derivations. Informally, a normal-form derivation is one which uses typeraising and composition only when necessary. Eisner (1996) describes a technique for eliminating spurious ambiguity entirely, by defining exactly one normal-form derivation for each semantic equivalence class of derivations. The idea is to restrict the combination of categories produced by composition; more specifically, any constituent which is the result of a forward composition cannot serve as the primary (left) functor in another forward composition or forward application. Similarly, any constituent which is the result of a backward composition cannot serve as the primary (right) functor in another backward composition or backward application. Ei"
J07-4004,P02-1036,0,0.0208432,"to enumerate the analyses for each sentence in the training data. Osborne (2000) investigates training on a sample of the analyses for each sentence, for example the top-n most probable according to some other probability model, or simply a random sample. The CCG grammar used in this article is automatically extracted, has wide coverage, and can produce an extremely large number of derivations for some sentences, far too many to enumerate. We adapt the feature-forest method of Miyao and Tsujii (2002), which involves using dynamic programming to efficiently calculate the feature expectations. Geman and Johnson (2002) propose a similar method in the context of LFG parsing; an implementation is described in Kaplan et al. (2004). Miyao and Tsujii have carried out a number of investigations similar to the work in this article. In Miyao and Tsujii (2003b, 2003a) log-linear models are developed for automatically extracted grammars for Lexicalized Tree Adjoining Grammar (LTAG) and Head Driven Phrase Structure Grammar (HPSG). One of Miyao and Tsujii’s motivations is to model predicate–argument dependencies, including long-range dependencies, which was one of the original motivations of the wide-coverage CCG parsi"
J07-4004,W01-0521,0,0.101766,"Missing"
J07-4004,P96-1024,0,0.449382,"Missing"
J07-4004,1997.iwpt-1.13,0,0.151441,"Missing"
J07-4004,P03-1046,0,0.119668,"yao and Tsujii (2002) to a packed chart; however, because the grammar is automatically extracted, the packed charts require a considerable amount of memory: up to 25 GB. We solve this massive estimation problem by developing a parallelized version of the estimation algorithm which runs on a Beowulf cluster. The lexicalized grammar formalism we use is Combinatory Categorial Grammar (CCG; Steedman 2000). A number of statistical parsing models have recently been developed for CCG and used in parsers applied to newspaper text (Clark, Hockenmaier, and Steedman 2002; Hockenmaier and Steedman 2002b; Hockenmaier 2003b). In this article we extend existing parsing techniques by developing log-linear models for CCG, as well as a new model and efficient parsing algorithm which exploits all CCG’s derivations, including the nonstandard ones. Estimating a log-linear model involves computing expectations of feature values. For the conditional log-linear models used in this article, computing expectations requires a sum over all derivations for each sentence in the training data. Because there can be a massive number of derivations for some sentences, enumerating all derivations is infeasible. To solve this proble"
J07-4004,hockenmaier-steedman-2002-acquiring,0,0.052539,"babilities to decide which tags to maintain. We were able to reduce the drop 540 Clark and Curran Wide-Coverage Efficient Statistical Parsing in supertagger accuracy by roughly one half. Future work will also look at maintaing the POS tag ambiguity through to the parsing stage. Currently we do not use the probabilities assigned to the lexical categories by the supertagger as part of the parse selection process. These scores could be incorporated as real-valued features, or as auxiliary functions, as in Johnson and Riezler (2000). We would also like to investigate using the generative model of Hockenmaier and Steedman (2002b) in a similar way. Using a generative model’s score as a feature in a discriminative framework has been beneficial for reranking approaches (Collins and Koo 2005). Because the generative model uses local features similar to those in our log-linear models, it could be incorporated into the estimation and decoding processes without the need for reranking. One way of improving the accuracy of a supertagger is to use the parser to provide large amounts of additional training data, by taking the lexical categories chosen by the parser as gold-standard training data. If enough unlabeled data is pa"
J07-4004,P02-1043,0,0.825906,"dynamic programming method of Miyao and Tsujii (2002) to a packed chart; however, because the grammar is automatically extracted, the packed charts require a considerable amount of memory: up to 25 GB. We solve this massive estimation problem by developing a parallelized version of the estimation algorithm which runs on a Beowulf cluster. The lexicalized grammar formalism we use is Combinatory Categorial Grammar (CCG; Steedman 2000). A number of statistical parsing models have recently been developed for CCG and used in parsers applied to newspaper text (Clark, Hockenmaier, and Steedman 2002; Hockenmaier and Steedman 2002b; Hockenmaier 2003b). In this article we extend existing parsing techniques by developing log-linear models for CCG, as well as a new model and efficient parsing algorithm which exploits all CCG’s derivations, including the nonstandard ones. Estimating a log-linear model involves computing expectations of feature values. For the conditional log-linear models used in this article, computing expectations requires a sum over all derivations for each sentence in the training data. Because there can be a massive number of derivations for some sentences, enumerating all derivations is infeasible. T"
J07-4004,J98-4004,0,0.173189,"tion; hence the parser can produce non-normal-form derivations. However, because the parsing model is estimated over normal-form derivations, any non-normal-form derivations will receive low probabilities and are unlikely to be returned as the most probable parse. Hockenmaier (2003a) compares a number of generative models, starting with a baseline model based on a PCFG. Various extensions to the baseline are considered: increasing the amount of lexicalization; generating a lexical category at its maximal projection; conditioning the probability of a rule instantiation on the grandparent node (Johnson 1998); adding features designed to deal with coordination; and adding distance to the dependency features. Some of these extensions, such as increased lexicalization and generating a lexical category at its maximal projection, improved performance, whereas others, such as the coordination and distance features, reduced performance. Hockenmaier (2003a) conjectures that the reduced performance is due to the problem of data sparseness, which becomes particularly severe for the generative model when the number of features is increased. The best performing model outperforms that of Clark, Hockenmaier, a"
J07-4004,P02-1018,0,0.109019,"ate the analyses for each sentence in the training data. Osborne (2000) investigates training on a sample of the analyses for each sentence, for example the top-n most probable according to some other probability model, or simply a random sample. The CCG grammar used in this article is automatically extracted, has wide coverage, and can produce an extremely large number of derivations for some sentences, far too many to enumerate. We adapt the feature-forest method of Miyao and Tsujii (2002), which involves using dynamic programming to efficiently calculate the feature expectations. Geman and Johnson (2002) propose a similar method in the context of LFG parsing; an implementation is described in Kaplan et al. (2004). Miyao and Tsujii have carried out a number of investigations similar to the work in this article. In Miyao and Tsujii (2003b, 2003a) log-linear models are developed for automatically extracted grammars for Lexicalized Tree Adjoining Grammar (LTAG) and Head Driven Phrase Structure Grammar (HPSG). One of Miyao and Tsujii’s motivations is to model predicate–argument dependencies, including long-range dependencies, which was one of the original motivations of the wide-coverage CCG parsi"
J07-4004,P99-1069,0,0.393766,"ark@comlab.ox.ac.uk. ∗∗ School of Information Technologies, University of Sydney, NSW 2006, Australia. E-mail: james@it.usyd.edu.au. Submission received: 27 April 2006; revised submission received: 30 November 2006; accepted for publication: 16 March 2007. © 2007 Association for Computational Linguistics Computational Linguistics Volume 33, Number 4 1. Introduction Log-linear models have been applied to a number of problems in NLP, for example, POS tagging (Ratnaparkhi 1996; Lafferty, McCallum, and Pereira 2001), named entity recognition (Borthwick 1999), chunking (Koeling 2000), and parsing (Johnson et al. 1999). Log-linear models are also referred to as maximum entropy models and random fields in the NLP literature. They are popular because of the ease with which complex discriminating features can be included in the model, and have been shown to give good performance across a range of NLP tasks. Log-linear models have previously been applied to statistical parsing (Johnson et al. 1999; Toutanova et al. 2002; Riezler et al. 2002; Malouf and van Noord 2004), but typically under the assumption that all possible parses for a sentence can be enumerated. For manually constructed grammars, this assumption"
J07-4004,A00-2021,0,0.0207646,"to maintain some POS tag ambiguity for later parts of the parsing process, using the tag probabilities to decide which tags to maintain. We were able to reduce the drop 540 Clark and Curran Wide-Coverage Efficient Statistical Parsing in supertagger accuracy by roughly one half. Future work will also look at maintaing the POS tag ambiguity through to the parsing stage. Currently we do not use the probabilities assigned to the lexical categories by the supertagger as part of the parse selection process. These scores could be incorporated as real-valued features, or as auxiliary functions, as in Johnson and Riezler (2000). We would also like to investigate using the generative model of Hockenmaier and Steedman (2002b) in a similar way. Using a generative model’s score as a feature in a discriminative framework has been beneficial for reranking approaches (Collins and Koo 2005). Because the generative model uses local features similar to those in our log-linear models, it could be incorporated into the estimation and decoding processes without the need for reranking. One way of improving the accuracy of a supertagger is to use the parser to provide large amounts of additional training data, by taking the lexica"
J07-4004,N04-1013,0,0.0809683,"of the analyses for each sentence, for example the top-n most probable according to some other probability model, or simply a random sample. The CCG grammar used in this article is automatically extracted, has wide coverage, and can produce an extremely large number of derivations for some sentences, far too many to enumerate. We adapt the feature-forest method of Miyao and Tsujii (2002), which involves using dynamic programming to efficiently calculate the feature expectations. Geman and Johnson (2002) propose a similar method in the context of LFG parsing; an implementation is described in Kaplan et al. (2004). Miyao and Tsujii have carried out a number of investigations similar to the work in this article. In Miyao and Tsujii (2003b, 2003a) log-linear models are developed for automatically extracted grammars for Lexicalized Tree Adjoining Grammar (LTAG) and Head Driven Phrase Structure Grammar (HPSG). One of Miyao and Tsujii’s motivations is to model predicate–argument dependencies, including long-range dependencies, which was one of the original motivations of the wide-coverage CCG parsing project. Miyao and Tsujii (2003a) present another log-linear model for an automatically extracted LTAG which"
J07-4004,W00-0729,0,0.0293961,"3QD, UK. E-mail: stephen.clark@comlab.ox.ac.uk. ∗∗ School of Information Technologies, University of Sydney, NSW 2006, Australia. E-mail: james@it.usyd.edu.au. Submission received: 27 April 2006; revised submission received: 30 November 2006; accepted for publication: 16 March 2007. © 2007 Association for Computational Linguistics Computational Linguistics Volume 33, Number 4 1. Introduction Log-linear models have been applied to a number of problems in NLP, for example, POS tagging (Ratnaparkhi 1996; Lafferty, McCallum, and Pereira 2001), named entity recognition (Borthwick 1999), chunking (Koeling 2000), and parsing (Johnson et al. 1999). Log-linear models are also referred to as maximum entropy models and random fields in the NLP literature. They are popular because of the ease with which complex discriminating features can be included in the model, and have been shown to give good performance across a range of NLP tasks. Log-linear models have previously been applied to statistical parsing (Johnson et al. 1999; Toutanova et al. 2002; Riezler et al. 2002; Malouf and van Noord 2004), but typically under the assumption that all possible parses for a sentence can be enumerated. For manually co"
J07-4004,I05-1006,0,0.131553,"Missing"
J07-4004,P04-1042,0,0.00540724,"igned to deal with the long-range dependencies inherent in certain constructions, such as coordination and extraction, and arguably provides the most linguistically satisfactory account of these phenomena. Long-range dependencies are relatively common in text such as newspaper text, but are typically not recovered by treebank parsers such as Collins (2003) and Charniak (2000). This has led to a number of proposals for post-processing the output of the Collins and Charniak parsers, in which trace sites are located and the antecedent of the trace determined (Johnson 2002; Dienes and Dubey 2003; Levy and Manning 2004). An advantage of using CCG is that 501 Computational Linguistics Volume 33, Number 4 the recovery of long-range dependencies can be integrated into the parsing process in a straightforward manner, rather than be relegated to such a post-processing phase (Clark, Hockenmaier, and Steedman 2002; Hockenmaier 2003a; Clark, Steedman, and Curran 2004). Another advantage of CCG is that providing a compositional semantics for the grammar is relatively straightforward. It has a completely transparent interface between syntax and semantics and, because CCG is a lexicalized grammar formalism, providing a"
J07-4004,W02-2018,0,0.164791,"imating the parameters of a PCFG from unlabeled data (Lari and Young 1990). Generalized Iterative Scaling (Darroch and Ratcliff 1972) is a common choice in the NLP literature for estimating a log-linear model (e.g., Ratnaparkhi 1998; Curran and Clark 2003). Initially we used generalized iterative scaling (GIS) for the parsing models described here, but found that convergence was extremely slow; Sha and Pereira (2003) present a similar finding for globally optimized log-linear models for sequences. As an alternative to GIS, we use the limited-memory BFGS algorithm (Nocedal and Wright 1999). As Malouf (2002) demonstrates, general purpose numerical optimization algorithms such as BFGS can converge much faster than iterative scaling algorithms (including Improved Iterative Scaling; Della Pietra, Della Pietra, and Lafferty 1997). Despite the use of a packed representation, the complete set of derivations for the sentences in the training data requires up to 25 GB of RAM for some of the models in this article. There are a number of ways to solve this problem. Possibilities include using a subset of the training data; repeatedly parsing the training data for each iteration of the estimation algorithm;"
J07-4004,J93-2004,0,0.0402583,"Missing"
J07-4004,W03-0401,0,0.0135774,"y a random sample. The CCG grammar used in this article is automatically extracted, has wide coverage, and can produce an extremely large number of derivations for some sentences, far too many to enumerate. We adapt the feature-forest method of Miyao and Tsujii (2002), which involves using dynamic programming to efficiently calculate the feature expectations. Geman and Johnson (2002) propose a similar method in the context of LFG parsing; an implementation is described in Kaplan et al. (2004). Miyao and Tsujii have carried out a number of investigations similar to the work in this article. In Miyao and Tsujii (2003b, 2003a) log-linear models are developed for automatically extracted grammars for Lexicalized Tree Adjoining Grammar (LTAG) and Head Driven Phrase Structure Grammar (HPSG). One of Miyao and Tsujii’s motivations is to model predicate–argument dependencies, including long-range dependencies, which was one of the original motivations of the wide-coverage CCG parsing project. Miyao and Tsujii (2003a) present another log-linear model for an automatically extracted LTAG which uses a simple unigram model of the elementary trees together with a loglinear model of the attachments. Miyao and Tsujii (20"
J07-4004,C04-1204,0,0.0242163,"Missing"
J07-4004,P05-1011,0,0.0315053,"yao and Tsujii (2003b, 2003a) log-linear models are developed for automatically extracted grammars for Lexicalized Tree Adjoining Grammar (LTAG) and Head Driven Phrase Structure Grammar (HPSG). One of Miyao and Tsujii’s motivations is to model predicate–argument dependencies, including long-range dependencies, which was one of the original motivations of the wide-coverage CCG parsing project. Miyao and Tsujii (2003a) present another log-linear model for an automatically extracted LTAG which uses a simple unigram model of the elementary trees together with a loglinear model of the attachments. Miyao and Tsujii (2005) address the issue of practical estimation using an automatically extracted HPSG grammar. A simple unigram model of lexical categories is used to limit the size of the charts for training, in a similar way to how we use a CCG supertagger to restrict the size of the charts. The main differences between Miyao and Tsujii’s work and ours, aside from the different grammar formalisms, are as follows. The CCG supertagger is a key component of our parsing system. It allows practical estimation of the log-linear models as well as highly efficient parsing. The Maximum Entropy supertagger we use could al"
J07-4004,W04-3308,0,0.0202605,"Missing"
J07-4004,C00-1085,0,0.123783,"alizes models whose weights get too large in absolute value. This smoothing method for log-linear models is also proposed by Chen and Rosenfeld (1999). Calculating the conditional feature expectations can still be problematic if the grammar licenses a large number of analyses for some sentences. This is not a problem for Johnson et al. (1999) because their grammars are hand-written and constraining enough to allow the analyses for each sentence to be enumerated. However, for grammars with wider coverage it is often not possible to enumerate the analyses for each sentence in the training data. Osborne (2000) investigates training on a sample of the analyses for each sentence, for example the top-n most probable according to some other probability model, or simply a random sample. The CCG grammar used in this article is automatically extracted, has wide coverage, and can produce an extremely large number of derivations for some sentences, far too many to enumerate. We adapt the feature-forest method of Miyao and Tsujii (2002), which involves using dynamic programming to efficiently calculate the feature expectations. Geman and Johnson (2002) propose a similar method in the context of LFG parsing;"
J07-4004,W97-1010,0,0.0561094,"he CCG grammars used in this article, the automatically extracted LTAG grammars have, as yet, been too large to enable effective supertagging (as discussed in the previous section). We are not aware of any other work which has demonstrated the parsing efficiency benefits of supertagging using an automatically extracted grammar. 3.2 Previous Work on CCG Statistical Parsing The work in this article began as part of the Edinburgh wide-coverage CCG parsing project (2000–2004). There has been some other work on defining stochastic categorial grammars, but mainly in the context of grammar learning (Osborne and Briscoe 1997; Watkinson and Manandhar 2001; Zettlemoyer and Collins 2005). An early attempt from the Edinburgh project at wide-coverage CCG parsing is presented in Clark, Hockenmaier, and Steedman (2002). In order to deal with the problem of the additional, nonstandard CCG derivations, a conditional model of dependency structures is presented, based on Collins (1996), in which the dependencies are modeled directly and derivations are not modeled at all. The conditional probability of a dependency structure π, given a sentence S, is factored into two parts. The first part is the probability of the lexical"
J07-4004,J05-1004,0,0.113316,"Missing"
J07-4004,E03-1025,0,0.0157022,"ork comparing parser performance across formalisms. Briscoe and Carroll (2006) evaluate the RASP parser on the Parc Dependency Bank (DepBank; King et al. 2003). Cahill et al. (2004) evaluate an LFG parser, which uses an automatically extracted grammar, against DepBank. Miyao and Tsujii (2004) evaluate their HPSG parser against PropBank (Palmer, Gildea, and Kingsbury 2005). Kaplan et al. (2004) compare the Collins parser with the Parc LFG parser by mapping Penn Treebank parses into the dependencies of DepBank, claiming that the LFG parser is more accurate with only a slight reduction in speed. Preiss (2003) compares the parsers of Collins and Charniak, the grammatical relations finder of Buchholz, Veenstra, and Daelemans (1999), and the Briscoe and Carroll (2002) parser, using the gold-standard grammatical relations (GRs) from Carroll, Briscoe, and Sanfilippo (1998). The Penn Treebank trees of the Collins and Charniak parsers, and the GRs of the Buchholz parser, are mapped into the required grammatical relations, with the result that the GR finder of Buchholz is the most accurate. There are a number of problems with such evaluations. The first is that, when converting the output of the Collins p"
J07-4004,P02-1035,0,0.396294,"in NLP, for example, POS tagging (Ratnaparkhi 1996; Lafferty, McCallum, and Pereira 2001), named entity recognition (Borthwick 1999), chunking (Koeling 2000), and parsing (Johnson et al. 1999). Log-linear models are also referred to as maximum entropy models and random fields in the NLP literature. They are popular because of the ease with which complex discriminating features can be included in the model, and have been shown to give good performance across a range of NLP tasks. Log-linear models have previously been applied to statistical parsing (Johnson et al. 1999; Toutanova et al. 2002; Riezler et al. 2002; Malouf and van Noord 2004), but typically under the assumption that all possible parses for a sentence can be enumerated. For manually constructed grammars, this assumption is usually sufficient for efficient estimation and decoding. However, for wide-coverage grammars extracted from a treebank, enumerating all parses is infeasible. In this article we apply the dynamic programming method of Miyao and Tsujii (2002) to a packed chart; however, because the grammar is automatically extracted, the packed charts require a considerable amount of memory: up to 25 GB. We solve this massive estimation"
J07-4004,W05-1513,0,0.0101864,"ample by changing the beam parameter in the Collins (2003) parser, but that any increase in speed is typically associated with a reduction in accuracy. For the CCG parser, the accuracy did not degrade when using the new adaptive parsing strategy. Thus the accuracy and efficiency of the parser were not tuned separately: The configuration used to obtain the speed results was also used to obtain the accuracy results in Sections 10.2 and 11. To give some idea of how these parsing speeds compare with existing parsers, Table 12 gives the parse times on Section 23 for a number of well-known parsers. Sagae and Lavie (2005) is a classifier-based linear time parser. The times for the Sagae, Collins, and Charniak parsers were taken from the Sagae and Lavie paper, and were obtained using a 1.8 GHz P4, compared to a 3.2 GHz P4 for the CCG numbers. Comparing parser speeds is especially problematic because of implementation differences and the fact that the accuracy of the parsers is not being controlled. Thus we are not making any strong claims about the efficiency of parsing with CCG compared to other formalisms. However, the results in Table 12 add considerable weight to one of our main claims in this article, name"
J07-4004,N03-1028,0,0.0458489,"cked chart efficiently represents all derivations for a sentence. The dynamic programming method uses inside and outside scores to calculate expectations, similar to the inside–outside algorithm for estimating the parameters of a PCFG from unlabeled data (Lari and Young 1990). Generalized Iterative Scaling (Darroch and Ratcliff 1972) is a common choice in the NLP literature for estimating a log-linear model (e.g., Ratnaparkhi 1998; Curran and Clark 2003). Initially we used generalized iterative scaling (GIS) for the parsing models described here, but found that convergence was extremely slow; Sha and Pereira (2003) present a similar finding for globally optimized log-linear models for sequences. As an alternative to GIS, we use the limited-memory BFGS algorithm (Nocedal and Wright 1999). As Malouf (2002) demonstrates, general purpose numerical optimization algorithms such as BFGS can converge much faster than iterative scaling algorithms (including Improved Iterative Scaling; Della Pietra, Della Pietra, and Lafferty 1997). Despite the use of a packed representation, the complete set of derivations for the sentences in the training data requires up to 25 GB of RAM for some of the models in this article."
J07-4004,W04-3201,0,0.0262102,"estimation, could be applied to log-linear parsing models using other grammar formalisms. Despite memory requirements of up to 25 GB we have shown how a parallelized version of the estimation process can limit the estimation time to under three hours, resulting in a practical framework for parser development. One of the problems with modeling approaches which require very long estimation times is that it is difficult to test different configurations of the system, for example different feature sets. It may also not be possible to train or run the system on anything other than short sentences (Taskar et al. 2004). The supertagger is a key component in our parsing system. It reduces the size of the charts considerably compared with naive methods for assigning lexical categories, which is crucial for practical discriminative training. The tight integration of the supertagger and parser enables highly efficient as well as accurate parsing. The parser is significantly faster than comparable parsers in the NLP literature. The supertagger we have developed can be applied to other lexicalized grammar formalisms. Another contribution of the article is the development of log-linear parsing models for CCG. In p"
J07-4004,W04-3222,0,0.0151497,"Missing"
J07-4004,W05-1517,0,0.00904801,"ive node daughters which head sub-derivations leading to the same set of high scoring dependencies. In practice large numbers of such conjunctive nodes lead to very long parse times. As an alternative to finding the most probable dependency structure, we have developed an algorithm which maximizes the expected labeled recall over dependencies. Our algorithm is based on Goodman’s (1996) labeled recall algorithm for the phrase-structure PARSEVAL measures. As far as we know, this is the first application of Goodman’s approach to finding highest scoring dependency structures. Watson, Carroll, and Briscoe (2005) have also applied our algorithm to the grammatical relations output by the RASP parser. The dependency structure, πmax , which maximizes the expected recall is: πmax = argmax π  P(πi |S)|π ∩ πi | (29) πi where πi ranges over the dependency structures for S. The expectation for a single dependency structure π is realized as a weighted intersection over all possible dependency structures πi for S. The intuition is that, if πi is the gold standard, then the number of dependencies recalled in π is |π ∩ πi |. Because we do not know which πi is the gold 517 Computational Linguistics Volume 33, Num"
J07-4004,W00-1307,0,0.042199,"Missing"
J07-4004,W03-2401,0,\N,Missing
J11-1005,W00-1201,0,0.124367,"Missing"
J11-1005,J93-1002,0,0.128678,"a stacking framework. Both Hall et al. (2007) and Nivre and McDonald (2008) can be seen as methods to combine separately defined models. In contrast, our parser combines two components in a single model, in which all parameters are trained consistently. 6. Phrase-Structure Parsing Phrase-structure parsing is the problem of producing the syntactic structure of an input sentence according to a phrase-structure grammar. An example phrase-structure parse tree is shown in Figure 11. Similar to dependency parsing, dominant approaches to phrase-structure parsing include the transition-based method (Briscoe and Carroll 1993), which builds an output parse tree by choosing a series of transition actions such as SHIFT and REDUCE, and the graph-based method (Collins 1999; Charniak 2000), which explores the search space of possible parse trees to find the best output according to graph-based scores. For English constituent parsing using the Penn Treebank, the best performing transition-based parser lags behind the current state-of-the-art (Sagae and Lavie 2005). However, for Chinese constituent parsing using the Chinese Treebank, Wang et al. (2006) have shown that a shift-reduce parser can give competitive accuracy sc"
J11-1005,W06-2920,0,0.123179,"Missing"
J11-1005,W08-2102,0,0.0193679,"Missing"
J11-1005,W06-2925,0,0.029396,"Missing"
J11-1005,A00-2018,0,0.030083,"wo components in a single model, in which all parameters are trained consistently. 6. Phrase-Structure Parsing Phrase-structure parsing is the problem of producing the syntactic structure of an input sentence according to a phrase-structure grammar. An example phrase-structure parse tree is shown in Figure 11. Similar to dependency parsing, dominant approaches to phrase-structure parsing include the transition-based method (Briscoe and Carroll 1993), which builds an output parse tree by choosing a series of transition actions such as SHIFT and REDUCE, and the graph-based method (Collins 1999; Charniak 2000), which explores the search space of possible parse trees to find the best output according to graph-based scores. For English constituent parsing using the Penn Treebank, the best performing transition-based parser lags behind the current state-of-the-art (Sagae and Lavie 2005). However, for Chinese constituent parsing using the Chinese Treebank, Wang et al. (2006) have shown that a shift-reduce parser can give competitive accuracy scores together with high speeds by using an SVM to make a single decision at each point in the parsing process. In Zhang and Clark (2009) we proposed a transition"
J11-1005,D09-1060,0,0.00765181,"are shown together with other systems in Table 16. In the table, each row represents a parsing model. Rows Yamada 2003 and MSTParser represent Yamada and Matsumoto (2003), and MSTParser with templates 1–6 from Table 14 (McDonald and Pereira 2006), respectively. Rows Transition and Combined represent our pure transition-based and combined parsers, respectively. Row Huang 2010 shows the recent work of Huang and Sagae (2010), which applies dynamic-programming packing to transition-based dependency parsing. Rows Koo 2008 and Chen 2009 represent the models of Koo, Carreras, and Collins (2008) and Chen et al. (2009), which perform semi-supervised learning by word-clustering and self-training, respectively. Columns Word and Complete show the precision of lexical Table 15 The training, development, and test data for English dependency parsing. Training Development Test Sections Sentences Words 2–21 22 23 39,832 1,700 2,416 950,028 40,117 56,684 135 Computational Linguistics Volume 37, Number 1 Table 16 Accuracy comparisons between various dependency parsers on English data. Word Complete Yamada 2003 Transition MSTParser Combined Huang 2010 90.3 91.4 91.5 92.1 92.1 38.4 41.8 42.1 45.4 – Koo 2008 Chen 2009 9"
J11-1005,C02-1126,0,0.0191446,"Missing"
J11-1005,J07-4004,1,0.840862,"g only on a decoder for each problem and using a trivial online update procedure for each training example. An advantage of the linear perceptron models we use is that they are global models, assigning a score to a complete hypothesis for each problem rather than assigning scores to parts which are then combined under statistical independence assumptions. Here we are following a recent line of work applying global discriminative models to tagging and wide-coverage parsing problems (Lafferty, McCallum, and Pereira 2001; Collins 2002; Collins and Roark 2004; McDonald, Crammer, and Pereira 2005; Clark and Curran 2007; Carreras, Collins, and Koo 2008; Finkel, Kleeman, and Manning 2008). The flexibility of our framework leads to competitive accuracies for each of the tasks we consider. For word segmentation, we show how the framework can accommodate a word-based approach, rather than the standard and more restrictive character-based tagging approaches. For POS-tagging, we consider joint segmentation and POS-tagging, showing that a single beam-search decoder can be used to achieve a significant accuracy boost over the pipeline baseline. For Chinese and English dependency parsing, we show how both graph-based"
J11-1005,W02-1001,0,0.598348,"Missing"
J11-1005,P04-1015,0,0.786358,"tures. The generalized perceptron is equally flexible, relying only on a decoder for each problem and using a trivial online update procedure for each training example. An advantage of the linear perceptron models we use is that they are global models, assigning a score to a complete hypothesis for each problem rather than assigning scores to parts which are then combined under statistical independence assumptions. Here we are following a recent line of work applying global discriminative models to tagging and wide-coverage parsing problems (Lafferty, McCallum, and Pereira 2001; Collins 2002; Collins and Roark 2004; McDonald, Crammer, and Pereira 2005; Clark and Curran 2007; Carreras, Collins, and Koo 2008; Finkel, Kleeman, and Manning 2008). The flexibility of our framework leads to competitive accuracies for each of the tasks we consider. For word segmentation, we show how the framework can accommodate a word-based approach, rather than the standard and more restrictive character-based tagging approaches. For POS-tagging, we consider joint segmentation and POS-tagging, showing that a single beam-search decoder can be used to achieve a significant accuracy boost over the pipeline baseline. For Chinese"
J11-1005,I05-3017,0,0.190435,"ur system without combination of character-based information, we call our segmentor in Section 3.1 the pure wordbased segmentor and the segmentor that uses character-based features the combined segmentor in our experimental sections. 3.4 Experiments We performed two sets of experiments. In the first set of experiments, we used the Chinese Treebank (CTB) data to study the speed/accuracy tradeoff by varying the size of the beam. In the second set of experiments, we used training and testing sets from the first and second international Chinese word segmentation bakeoffs (Sproat and Emerson 2003; Emerson 2005) to compare the accuracies to other models in the literature, including our segmentor of Zhang and Clark (2007). F-score is used as the accuracy measure: 2pr/(p + r), where precision p is the percentage of words in the decoder output that are segmented correctly, and recall r is the percentage of gold-standard output words that are correctly segmented by the decoder. CWS systems are evaluated by two types of tests. The closed tests require that the system is trained only with a designated training corpus. Any extra knowledge is not allowed, including common surnames, Chinese and Arabic numbers"
J11-1005,P08-1109,0,0.0637353,"Missing"
J11-1005,D07-1097,0,0.0132965,"Missing"
J11-1005,P08-1067,0,0.0131453,"ncy of a dynamicprogramming decoder is restricted by the range of features, due to its requirement for optimal substructure. For our combined dependency parser, the feature set makes a dynamic-programming decoder infeasibly slow. From this perspective, beam-search is in line with other recent research on the improvement of accuracies by incorporating non-local features via approximation, such as belief propagation for dependency parsing (Smith and Eisner 2008), integer linear programming for dependency parsing (Martins, Smith, and Xing 2009), and forest reranking for phrase-structure parsing (Huang 2008). The only prerequisite of the framework is an incremental process, which consumes the input sentence and builds the output structure using a sequence of actions. All four problems studied in the article were first turned into an incremental process, and then solved by applying the framework. The number of distinct actions for a problem is dependent on the complexity of the output. For word segmentation, there are only two actions (append or separate). For transition-based unlabeled dependency parsing, there are four actions (shift, arc-left, arc-right, and reduce). For joint segmentation and"
J11-1005,P10-1110,0,0.19138,"Missing"
J11-1005,P08-1102,0,0.0828904,"ith the current character in the middle. We call these methods character-based word segmentation. The advantage of character-based segmentation is that well-known tagging approaches can be applied directly to the CWS problem. There are various character-based models in the literature. They differ mainly in the learning algorithm and the features used. Several discriminative learning algorithms have been applied to the character-based systems. Examples include Xue (2003), Peng, Feng, and McCallum (2004), and Wang et al. (2006), which use maximum entropy and conditional random field models, and Jiang et al. (2008), which uses the perceptron model. The standard feature set is that defined by Ng and Low (2004), though other feature sets are reported to improve the accuracy (Zhao, Huang, and Li 2006). Zhao, Huang, and Li (2006) also showed that the best accuracy for conditional random field (CRF) models is given by using a set of six character segmentation tags, rather than the standard set {beginning, middle, end, single} shown previously. Standard search algorithms for sequence tagging have been applied to the decoding process, such as the dynamic-programming algorithm and beam-search. A disadvantage of"
J11-1005,C08-1049,0,0.0454952,"ith the current character in the middle. We call these methods character-based word segmentation. The advantage of character-based segmentation is that well-known tagging approaches can be applied directly to the CWS problem. There are various character-based models in the literature. They differ mainly in the learning algorithm and the features used. Several discriminative learning algorithms have been applied to the character-based systems. Examples include Xue (2003), Peng, Feng, and McCallum (2004), and Wang et al. (2006), which use maximum entropy and conditional random field models, and Jiang et al. (2008), which uses the perceptron model. The standard feature set is that defined by Ng and Low (2004), though other feature sets are reported to improve the accuracy (Zhao, Huang, and Li 2006). Zhao, Huang, and Li (2006) also showed that the best accuracy for conditional random field (CRF) models is given by using a set of six character segmentation tags, rather than the standard set {beginning, middle, end, single} shown previously. Standard search algorithms for sequence tagging have been applied to the decoding process, such as the dynamic-programming algorithm and beam-search. A disadvantage of"
J11-1005,D07-1123,0,0.0274584,"Missing"
J11-1005,P08-1068,0,0.0258046,"Missing"
J11-1005,P09-1058,0,0.188763,"ction 3. Due to the large accuracy gain from the baseline, our joint model performed much better. In summary, when compared with existing joint word segmentation and POStagging systems using cross-validation tests, our proposed model achieved the best accuracy boost from the pipelined baseline, and competitive overall accuracy. Our system based on the general framework of this article gave comparable accuracies to our multiple-beam system in Zhang and Clark (2008a), and a speed that is over an order of magnitude higher than the multiple-beam algorithm. 4.4.3 Test Results Using CTB5. We follow Kruengkrai et al. (2009) and split the CTB5 into training, development testing, and testing sets, as shown in Table 11. The data are used 128 Zhang and Clark Syntactic Processing Table 11 Training, development, and test data from CTB5 for joint word segmentation and POS-tagging. Training Dev Test Sections Sentences Words 1–270, 400–931, 1001–1151 301–325 271–300 18,085 350 348 493,892 6,821 8,008 to compare the accuracies of our joint system with models in the literature, and to draw the speed/accuracy tradeoff graph. Kruengkrai et al. (2009) made use of character type knowledge for spaces, numerals, symbols, alphabe"
J11-1005,P03-1056,0,0.0966093,"Missing"
J11-1005,P09-1039,0,0.0172078,"Missing"
J11-1005,P05-1012,0,0.175465,"Missing"
J11-1005,D07-1013,0,0.0352932,"Missing"
J11-1005,E06-1011,0,0.696994,"s on projective dependency parsing. An unlabeled dependency tree is a dependency tree without dependency labels such as Subj and Obj in Figure 8. The same techniques used by unlabeled dependency parsers can be applied to labeled dependency parsing. For example, a shift-reduce unlabeled dependency parser can be extended to perform labeled dependency parsing by splitting a single reduce action into a set of reduce actions each associated with a dependency label. Here we focus on unlabeled dependency parsing. Graph-based (McDonald, Crammer, and Pereira 2005; Carreras, Surdeanu, and Marquez 2006; McDonald and Pereira 2006) and transition-based (Yamada and Matsumoto 2003; Nivre et al. 2006) parsing algorithms offer two different approaches to data-driven dependency parsing. Given an input sentence, a graph-based algorithm finds the highest scoring parse tree from all possible outputs, scoring each complete tree, while a transition-based algorithm builds a parse by a sequence of actions, scoring each action individually. Although graph-based and transition-based parsers can be differentiated in various ways, we prefer to think in terms of the features used in the two approaches as the differentiating factor. In Z"
J11-1005,H05-1066,0,0.0583269,"Missing"
J11-1005,P07-2055,0,0.0184038,"tradeoff graph. Kruengkrai et al. (2009) made use of character type knowledge for spaces, numerals, symbols, alphabets, and Chinese and other characters. In the previous experiments, our system did not use any knowledge beyond the training data. To make the comparison fairer, we included knowledge of English letters and Arabic numbers in this experiment. During both training and decoding, English letters and Arabic numbers are segmented using rules, treating consecutive English letters or Arabic numbers as a single word. The results are shown in Table 12, where row N07 refers to the model of Nakagawa and Uchimoto (2007), rows J08a and J08b refer to the models of Jiang et al. (2008) and Jiang, Mi, and Liu (2008), and row K09 refers to the models of Kruengkrai et al. (2009). Columns SF and JF refer to segmentation and joint segmentation and tagging accuracies, respectively. Our system gave comparable accuracies to these recent works, obtaining the best (same as the error-driven version of K09) joint F-score. The accuracy/speed tradeoff graphs for the joint segmentor and POS-taggers, together with the baseline pipeline system, are shown in Figure 7. For each point in each curve, the development test data were u"
J11-1005,W04-3236,0,0.105735,"The advantage of character-based segmentation is that well-known tagging approaches can be applied directly to the CWS problem. There are various character-based models in the literature. They differ mainly in the learning algorithm and the features used. Several discriminative learning algorithms have been applied to the character-based systems. Examples include Xue (2003), Peng, Feng, and McCallum (2004), and Wang et al. (2006), which use maximum entropy and conditional random field models, and Jiang et al. (2008), which uses the perceptron model. The standard feature set is that defined by Ng and Low (2004), though other feature sets are reported to improve the accuracy (Zhao, Huang, and Li 2006). Zhao, Huang, and Li (2006) also showed that the best accuracy for conditional random field (CRF) models is given by using a set of six character segmentation tags, rather than the standard set {beginning, middle, end, single} shown previously. Standard search algorithms for sequence tagging have been applied to the decoding process, such as the dynamic-programming algorithm and beam-search. A disadvantage of character-based models is the use of limited contextual information. For these methods, context"
J11-1005,W06-2933,0,0.0101416,"Missing"
J11-1005,P08-1108,0,0.0120114,"l search. Johansson and Nugues (2007) also use beam search. Second, we use the perceptron to train whole sequences of transition actions globally, whereas MaltParser uses SVM to train each transition action locally. Our global training corresponds to beamsearch decoding, which searches for a globally optimal sequence of transition actions rather than an optimal action at each step. An existing method to combine multiple parsing algorithms is the ensemble approach (Sagae and Lavie 2006), which was reported to be useful in improving dependency parsing (Hall et al. 2007). A more recent approach (Nivre and McDonald 2008) combined MSTParser and MaltParser by using the output of one parser for features in the other in a stacking framework. Both Hall et al. (2007) and Nivre and McDonald (2008) can be seen as methods to combine separately defined models. In contrast, our parser combines two components in a single model, in which all parameters are trained consistently. 6. Phrase-Structure Parsing Phrase-structure parsing is the problem of producing the syntactic structure of an input sentence according to a phrase-structure grammar. An example phrase-structure parse tree is shown in Figure 11. Similar to dependen"
J11-1005,C04-1081,0,0.0388234,"Missing"
J11-1005,N07-1051,0,0.0622914,"Missing"
J11-1005,J01-2004,0,0.0167206,"classifier that optimizes each individual choice. Instead of greedy local decoding, we used beamsearch in the decoder. An early work that applies beam-search to constituent parsing is Ratnaparkhi (1999). The main difference between our parser and Ratnaparkhi’s is that we use a global discriminative model, whereas Ratnaparkhi’s parser has separate probabilities of actions chained together in a conditional model. Both our parser and the parser from Collins and Roark (2004) use a global discriminative model and an incremental parsing process. The major difference is that Collins and Roark, like Roark (2001), follow a top–down derivation strategy, whereas we chose to use a shift-reduce process which has been shown to give state-of-the-art accuracies for Chinese (Wang et al. 2006). In addition, we did not include a generative baseline model in the discriminative model, as did Collins and Roark (2004). 7. Discussion We have demonstrated in the previous sections that accuracies competitive with the state-of-the-art can be achieved by our general framework for Chinese word segmentation, joint word segmentation and POS-tagging, Chinese and English dependency parsing, and Chinese phrase-structure parsi"
J11-1005,W05-1513,0,0.160312,"cture parse tree is shown in Figure 11. Similar to dependency parsing, dominant approaches to phrase-structure parsing include the transition-based method (Briscoe and Carroll 1993), which builds an output parse tree by choosing a series of transition actions such as SHIFT and REDUCE, and the graph-based method (Collins 1999; Charniak 2000), which explores the search space of possible parse trees to find the best output according to graph-based scores. For English constituent parsing using the Penn Treebank, the best performing transition-based parser lags behind the current state-of-the-art (Sagae and Lavie 2005). However, for Chinese constituent parsing using the Chinese Treebank, Wang et al. (2006) have shown that a shift-reduce parser can give competitive accuracy scores together with high speeds by using an SVM to make a single decision at each point in the parsing process. In Zhang and Clark (2009) we proposed a transition-based constituent parser for Chinese, which is based on the transition process of Wang et al. (2006). Rather than making a single decision at each processing step, our parser uses a global linear model Figure 11 An example Chinese lexicalized phrase-structure parse tree. 139 Co"
J11-1005,N06-2033,0,0.046777,"fferent from MaltParser in two aspects. First, we applied beam-search in decoding, which helps to prevent error propagation of local search. Johansson and Nugues (2007) also use beam search. Second, we use the perceptron to train whole sequences of transition actions globally, whereas MaltParser uses SVM to train each transition action locally. Our global training corresponds to beamsearch decoding, which searches for a globally optimal sequence of transition actions rather than an optimal action at each step. An existing method to combine multiple parsing algorithms is the ensemble approach (Sagae and Lavie 2006), which was reported to be useful in improving dependency parsing (Hall et al. 2007). A more recent approach (Nivre and McDonald 2008) combined MSTParser and MaltParser by using the output of one parser for features in the other in a stacking framework. Both Hall et al. (2007) and Nivre and McDonald (2008) can be seen as methods to combine separately defined models. In contrast, our parser combines two components in a single model, in which all parameters are trained consistently. 6. Phrase-Structure Parsing Phrase-structure parsing is the problem of producing the syntactic structure of an inp"
J11-1005,D08-1016,0,0.0094007,"Missing"
J11-1005,J96-3004,0,0.0957261,"Missing"
J11-1005,W03-1719,0,0.0675137,"nguish this system from our system without combination of character-based information, we call our segmentor in Section 3.1 the pure wordbased segmentor and the segmentor that uses character-based features the combined segmentor in our experimental sections. 3.4 Experiments We performed two sets of experiments. In the first set of experiments, we used the Chinese Treebank (CTB) data to study the speed/accuracy tradeoff by varying the size of the beam. In the second set of experiments, we used training and testing sets from the first and second international Chinese word segmentation bakeoffs (Sproat and Emerson 2003; Emerson 2005) to compare the accuracies to other models in the literature, including our segmentor of Zhang and Clark (2007). F-score is used as the accuracy measure: 2pr/(p + r), where precision p is the percentage of words in the decoder output that are segmented correctly, and recall r is the percentage of gold-standard output words that are correctly segmented by the decoder. CWS systems are evaluated by two types of tests. The closed tests require that the system is trained only with a designated training corpus. Any extra knowledge is not allowed, including common surnames, Chinese and"
J11-1005,W06-0121,0,0.1268,"ti-character word. The context for disambiguation is normally a five-character window with the current character in the middle. We call these methods character-based word segmentation. The advantage of character-based segmentation is that well-known tagging approaches can be applied directly to the CWS problem. There are various character-based models in the literature. They differ mainly in the learning algorithm and the features used. Several discriminative learning algorithms have been applied to the character-based systems. Examples include Xue (2003), Peng, Feng, and McCallum (2004), and Wang et al. (2006), which use maximum entropy and conditional random field models, and Jiang et al. (2008), which uses the perceptron model. The standard feature set is that defined by Ng and Low (2004), though other feature sets are reported to improve the accuracy (Zhao, Huang, and Li 2006). Zhao, Huang, and Li (2006) also showed that the best accuracy for conditional random field (CRF) models is given by using a set of six character segmentation tags, rather than the standard set {beginning, middle, end, single} shown previously. Standard search algorithms for sequence tagging have been applied to the decodi"
J11-1005,I05-1007,0,0.0212555,"Missing"
J11-1005,O03-4002,0,0.0606094,"Missing"
J11-1005,W03-3023,0,0.502499,"ore, the decoding process consists of 2n − 1 steps, and in each step all state items in the agenda have been built using the same number of actions. Our experiments showed that start-of-the-art accuracy can be achieved by this intuitive method of candidate comparison. 5.2 Experiments for English We used Penn Treebank 3 for our experiments, which was separated into the training, development, and test sets in the same way as McDonald, Crammer, and Pereira (2005), shown in Table 15. Bracketed sentences from the Treebank were translated into dependency structures using the head-finding rules from Yamada and Matsumoto (2003). Before parsing, POS-tags are assigned to the input sentence using our baseline POS -tagger of Zhang and Clark (2008a), which can be seen as the perceptron tagger of Collins (2002) with beam-search. Like McDonald, Crammer, and Pereira (2005), we evaluated the parsing accuracy by the precision of lexical heads (the percentage of input words, excluding punctuation, that have been assigned the correct parent) and by the percentage of complete matches, in which all words excluding punctuation have been assigned the correct parent. A set of development tests, including the convergence of the perce"
J11-1005,N06-2049,0,0.0265563,"Missing"
J11-1005,P07-1106,1,0.282662,"pproaches into a single model which outperforms both in isolation. Finally, for Chinese phrase-structure parsing, we describe a global model for a shift-reduce parsing algorithm, in contrast to current deterministic approaches which use only local models at each step of the parsing process. For all these tasks we present results competitive with the best results in the literature. In Section 2 we describe our general framework of the generic beam-search algorithm and the generalized perceptron. Then in the subsequent sections we describe each task in turn, based on conference papers including Zhang and Clark (2007, 2008a, 2008b, 2009, 2010), presented in our single coherent framework. We give an updated set of results, plus a number of additional experiments which probe further into the advantages and disadvantages of our framework. For the segmentation task, we also compare our beam-search framework with alternative decoding algorithms including an exact dynamic-programming method, showing that the beam-search method is significantly faster with comparable accuracy. For the joint segmentation and POS-tagging task, we present a novel solution using the framework in this article, and show that it gives"
J11-1005,P08-1101,1,0.0511217,"r single coherent framework. We give an updated set of results, plus a number of additional experiments which probe further into the advantages and disadvantages of our framework. For the segmentation task, we also compare our beam-search framework with alternative decoding algorithms including an exact dynamic-programming method, showing that the beam-search method is significantly faster with comparable accuracy. For the joint segmentation and POS-tagging task, we present a novel solution using the framework in this article, and show that it gives comparable accuracies to our previous work (Zhang and Clark 2008a), while being more than an order of magnitude faster. In Section 7 we provide further discussion of the framework based on the studies of the individual tasks. We present the main advantages of the framework, and give an analysis of the main reasons for the high speeds and accuracies achieved. We also discuss how this framework can be applied to a potential new task, and show that the comparability of candidates in the incremental process is an important factor to consider. In summary, we study a general framework for incremental structural prediction, showing how the framework can be tailor"
J11-1005,D08-1059,1,0.450126,"mpared with Ng and Low (2004), our baseline model gave slightly better accuracy, consistent with our previous observations about the word segmentors in Section 3. Due to the large accuracy gain from the baseline, our joint model performed much better. In summary, when compared with existing joint word segmentation and POStagging systems using cross-validation tests, our proposed model achieved the best accuracy boost from the pipelined baseline, and competitive overall accuracy. Our system based on the general framework of this article gave comparable accuracies to our multiple-beam system in Zhang and Clark (2008a), and a speed that is over an order of magnitude higher than the multiple-beam algorithm. 4.4.3 Test Results Using CTB5. We follow Kruengkrai et al. (2009) and split the CTB5 into training, development testing, and testing sets, as shown in Table 11. The data are used 128 Zhang and Clark Syntactic Processing Table 11 Training, development, and test data from CTB5 for joint word segmentation and POS-tagging. Training Dev Test Sections Sentences Words 1–270, 400–931, 1001–1151 301–325 271–300 18,085 350 348 493,892 6,821 8,008 to compare the accuracies of our joint system with models in the li"
J11-1005,W09-3825,1,0.917397,"raph-based method (Collins 1999; Charniak 2000), which explores the search space of possible parse trees to find the best output according to graph-based scores. For English constituent parsing using the Penn Treebank, the best performing transition-based parser lags behind the current state-of-the-art (Sagae and Lavie 2005). However, for Chinese constituent parsing using the Chinese Treebank, Wang et al. (2006) have shown that a shift-reduce parser can give competitive accuracy scores together with high speeds by using an SVM to make a single decision at each point in the parsing process. In Zhang and Clark (2009) we proposed a transition-based constituent parser for Chinese, which is based on the transition process of Wang et al. (2006). Rather than making a single decision at each processing step, our parser uses a global linear model Figure 11 An example Chinese lexicalized phrase-structure parse tree. 139 Computational Linguistics Volume 37, Number 1 and beam-search decoding, and achieved competitive accuracy. This phrase-structure parser can be expressed as an instance of our general framework. 6.1 Instantiating the General Framework The incremental parsing process of our parser is based on the sh"
J11-1005,D10-1082,1,0.801835,"e-beam decoder, and our new way to decide the number of training iterations in this article. The single-beam results correspond to “Zhang 2007*” in Tables 4 and 5. 120 Zhang and Clark Syntactic Processing POS -tagged output. Joint word segmentation and POS -tagging is a method that addresses these problems. In Zhang and Clark (2008a) we proposed a joint word segmentor and POS-tagger using a multiple-beam decoder, and showed that it outperformed a pipelined baseline. We recently showed that comparable accuracies can be achieved by a single-beam decoder, which runs an order of magnitude faster (Zhang and Clark 2010). In this section, we describe our single-beam system using our general framework, and provide a detailed comparison with our multiple-beam and baseline systems of Zhang and Clark (2008a). 4.1 Instantiating the General Framework Given an input sentence, our joint segmentor and POS-tagger builds an output incrementally, one character at a time. When a character is processed, it is either concatenated with the last word in the partially built output, or taken as a new word. In the latter case, a POS-tag is assigned to the new word. When more characters are concatenated to a word, the POS-tag of"
J11-1005,W06-0127,0,0.0268495,"Missing"
J11-1005,J03-4003,0,\N,Missing
J11-1005,D07-1096,0,\N,Missing
J11-1005,W03-1726,0,\N,Missing
J14-2006,D08-1021,0,0.0494841,"Missing"
J14-2006,N10-1084,1,0.767452,"Missing"
J14-2006,J14-2006,1,0.0512257,"Missing"
J14-2006,C12-1031,1,0.828707,"e comparison of the statistical distribution of features obtained from a normal text and from a stego text might be able to detect the existence of the secret message (Meng et al. 2010; Chen et al. 2011). Instead of obtaining alternative translations from multiple translation systems, Meng et al. (2011) and Venugopal et al. (2011) used a statistical machine translation system to generate the n-best translations for a given cover sentence. Because translations are from one system, each of them is more similar to the rest than that derived from another translation system. Another of our papers (Chang and Clark 2012b) proposed a word-ordering-based stegosystem, where the word-ordering technique can be seen as a “monolingual translation” that translates a cover sentence into different permutations. Because not all the sentence permutations generated by a word-ordering system are grammatical and semantically meaningful, we developed a maximum entropy classifier to distinguish natural word orders from awkward ones. Another possible semantic transformation for linguistic steganography is sentence compression (Dorr, Zajic, and Schwartz 2003; Cohn and Lapata 2008; Zhu, Bernhard, and Gurevych 2010). Different c"
J14-2006,C12-1032,1,0.814933,"e comparison of the statistical distribution of features obtained from a normal text and from a stego text might be able to detect the existence of the secret message (Meng et al. 2010; Chen et al. 2011). Instead of obtaining alternative translations from multiple translation systems, Meng et al. (2011) and Venugopal et al. (2011) used a statistical machine translation system to generate the n-best translations for a given cover sentence. Because translations are from one system, each of them is more similar to the rest than that derived from another translation system. Another of our papers (Chang and Clark 2012b) proposed a word-ordering-based stegosystem, where the word-ordering technique can be seen as a “monolingual translation” that translates a cover sentence into different permutations. Because not all the sentence permutations generated by a word-ordering system are grammatical and semantically meaningful, we developed a maximum entropy classifier to distinguish natural word orders from awkward ones. Another possible semantic transformation for linguistic steganography is sentence compression (Dorr, Zajic, and Schwartz 2003; Cohn and Lapata 2008; Zhu, Bernhard, and Gurevych 2010). Different c"
J14-2006,J07-4004,1,0.719662,"ransformation Original sentence Transformed sentence Passivization Topicalization Clefting Extraposition Preposing There-construction Pronominalization Fronting The dog kissed Peter. I like pasta. He won a new bike. To achieve that is impossible. I like cheese bagels. A cat is in the garden. I put the cake in the fridge. “What!” Peter said. Peter was kissed by the dog. Pasta, I like. It was a new bike that he won. It is impossible to achieve that. Cheese bagels are what I like. There is a cat in the garden. I put it there. “What!” said Peter. and a combinatory categorial grammar (CCG) parser (Clark and Curran 2007) to certify the paraphrasing grammaticality. 2.1.2 Syntactic Transformations. Syntactic transformation methods are based on the fact that a sentence can be transformed into more than one semantically equivalent syntactic structure, using transformations such as passivization, topicalization, and clefting. Table 2 lists some of the common syntactic transformations in English.6 The first syntactic transformation method was presented by Atallah et al. (2000). Later, Atallah et al. (2001) generated alternative sentences by adjusting the structural properties of intermediate representations of a co"
J14-2006,C08-1018,0,0.0606772,"nslation evaluation metrics BLEU (Papineni et al. 2002) and NIST (Doddington 2002), automatically measuring how close a stego sentence is to the original. Topkara, Topkara, and Atallah (2006a) admitted that machine translation evaluation metrics are not sufficient for evaluating stegosystems; for example, BLEU relies on word sequences in the stego sentence matching those in the cover sentence and thus is not suitable for evaluating transformations that change the word order significantly. The other widely adopted evaluation method is based on human judgments. Meral et al. (2007, 2009) and Kim (2008, 2009) asked participants to edit stego text for improving intelligibility and style. The fewer edit-hits a transformed text received, the higher the reported security level. Murphy and Vogel (2007a, 2007b) first asked subjects to rate the acceptability (in terms of plausibility, grammaticality, and style) of the stego sentences on a seven-point scale. Then participants were provided with the originals and asked to judge to what extent meaning was preserved, also on a seven-point scale. In Chang and Clark (2010a) we asked participants to judge whether a paraphrased sentence is grammatical and whethe"
J14-2006,D10-1113,0,0.123131,"Missing"
J14-2006,W03-0501,0,0.297924,"Missing"
J14-2006,P10-2017,0,0.114284,"Missing"
J14-2006,D09-1129,0,0.226383,"Missing"
J14-2006,U08-1008,0,0.103007,"dule and present the vertex coding method. Finally, we use an example to demonstrate the complete stegosystem. 3.1.1 n-Gram Count Method (NGM). The basic checking method, referred to as NGM, utilizes the Google n-gram corpus to calculate a substitution score for a candidate word in context based on Bergsma, Lin, and Goebel (2009). The Google n-gram corpus was collected by Google Research for statistical language modeling, and has been used for many tasks such as spelling correction (Carlson, Mitchell, and Fette 2008; Islam and Inkpen 2009), multi-word expression classification (Kummerfeld and Curran 2008), and lexical disambiguation (Bergsma, Lin, and Goebel 2009). It contains frequency counts for n-grams from uni-grams through to 5-grams obtained from over 1 trillion word tokens of English Web text. Only n-grams appearing more than 40 times were kept in the corpus. The checking method first extracts contextual bi- to 5-grams around the word to be tested and uses the Minnen, Carroll, and Pearce (2001) tools for correcting the form of an indefinite and a verb’s tense. For example, if the word to be tested is maverick and it is going to replace unorthodox in the phrase the help of an unorthodox"
J14-2006,P99-1004,0,0.107509,"e the one that represents the secret bit as the stego word. 3.1 Substitution Checkers The aim of the proposed checkers is to filter out inapplicable substitutes given the original word in context. The substitution checkers must not only work with the proposed linguistic stegosystem, but can also be integrated into other synonym substitutionbased applications to certify the transformation quality. The following sections are organized so that the basic substitution checker using the Google n-gram corpus (Brants and Franz 2006) is described first. Then we introduce the α-skew divergence measure (Lee 1999) that can be combined with the basic n-gram method. The proposed checkers are evaluated using data from the SemEval lexical substitution task (McCarthy and Navigli 2007), which is independent of the steganography application. We also perform a more direct evaluation of the imperceptibility of the steganography application by asking human judges to evaluate the naturalness of sentences. After explaining the linguistic transformation module in our stegosystem, we proceed with the encoder generation module and present the vertex coding method. Finally, we use an example to demonstrate the complet"
J14-2006,S07-1009,0,0.100708,"Missing"
J14-2006,D11-1097,0,0.494498,"ltiple machine translation systems to generate alternative translations, which leads to a stego text containing a mixture of translations generated from different systems and each stego sentence may have different statistical distribution of features (e.g., percentage of high-frequency words), a simple comparison of the statistical distribution of features obtained from a normal text and from a stego text might be able to detect the existence of the secret message (Meng et al. 2010; Chen et al. 2011). Instead of obtaining alternative translations from multiple translation systems, Meng et al. (2011) and Venugopal et al. (2011) used a statistical machine translation system to generate the n-best translations for a given cover sentence. Because translations are from one system, each of them is more similar to the rest than that derived from another translation system. Another of our papers (Chang and Clark 2012b) proposed a word-ordering-based stegosystem, where the word-ordering technique can be seen as a “monolingual translation” that translates a cover sentence into different permutations. Because not all the sentence permutations generated by a word-ordering system are grammatical and"
J14-2006,P02-1040,0,0.0896436,"stic steganography is that evaluation of linguistic stegosystems is much more difficult than that of image, audio, or video stegosystems because such evaluation requires us to consider many controversial linguistic issues, such as meaning, grammaticality, fluency, and style. The current state-of-the-art techniques for automatically evaluating the fluency and grammaticality of natural language generation systems are based on techniques for evaluating the output of machine translation systems, such as comparing the n-grams in the machine translation output with those in a reference translation (Papineni et al. 2002; Zhang and Clark 2011). Although some computational steganalysis systems have been developed to identify stego text from innocent text using statistical methods, these systems can only be applied to text that undergoes certain linguistic transformations such as translation and lexical substitution, and they are not accurate enough for practical evaluation. Therefore, most of the current linguistic stegosystems were evaluated by human judges (Murphy and Vogel 2007a, 2007b; Meral et al. 2007, 2009; Kim 2008, 2009; Chang and Clark 2010a, 2012a, 2012b), where a human assessor was provided with st"
J14-2006,P07-1096,0,0.066588,"Missing"
J14-2006,P10-2038,0,0.159023,"Missing"
J14-2006,E09-1087,0,0.0584119,"Missing"
J14-2006,N03-1033,0,0.119269,"Missing"
J14-2006,D11-1126,0,0.204616,"ine translation systems to generate alternative translations, which leads to a stego text containing a mixture of translations generated from different systems and each stego sentence may have different statistical distribution of features (e.g., percentage of high-frequency words), a simple comparison of the statistical distribution of features obtained from a normal text and from a stego text might be able to detect the existence of the secret message (Meng et al. 2010; Chen et al. 2011). Instead of obtaining alternative translations from multiple translation systems, Meng et al. (2011) and Venugopal et al. (2011) used a statistical machine translation system to generate the n-best translations for a given cover sentence. Because translations are from one system, each of them is more similar to the rest than that derived from another translation system. Another of our papers (Chang and Clark 2012b) proposed a word-ordering-based stegosystem, where the word-ordering technique can be seen as a “monolingual translation” that translates a cover sentence into different permutations. Because not all the sentence permutations generated by a word-ordering system are grammatical and semantically meaningful, we"
J14-2006,D11-1106,1,0.907131,"Missing"
J14-2006,C10-1152,0,0.0906649,"Missing"
J14-2006,D10-1116,1,\N,Missing
J15-3005,D11-1031,0,0.0548798,"Missing"
J15-3005,J05-3002,0,0.0496661,"Building, 15 JJ Thomson Avenue, Cambridge, UK. E-mail: stephen.clark@cl.cam.ac.uk. Submission received: 17 April 2013; revised version received: 23 April 2014; accepted for publication: 22 June 2014. doi:10.1162/COLI a 00229 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 3 those words. Additional annotation may also be provided with the input—for example, part-of-speech (POS) tags or syntactic dependencies. Applications that can benefit from better text generation algorithms include machine translation (Koehn 2010), abstractive text summarization (Barzilay and McKeown 2005), and grammar correction (Lee and Seneff 2006). Typically, statistical machine translation (SMT) systems (Chiang 2007; Koehn 2010) perform generation into the target language as part of an integrated system, which avoids the high computational complexity of independent word ordering. On the other hand, performing word ordering separately in a pipeline has many potential advantages. For SMT, it offers better modularity between adequacy (translation) and fluency (linearization), and can potentially improve target grammaticality for syntactically different languages (e.g., Chinese and English). M"
J15-3005,W11-2832,0,0.0325166,"Missing"
J15-3005,C10-1009,0,0.143121,"Missing"
J15-3005,C10-1012,0,0.236432,"Missing"
J15-3005,C04-1180,1,0.0644539,"hat, with this new approach, negative examples can be expanded during training and a single beam applied to the chart, resulting in a conceptually simpler and more effective training algorithm and decoder. 3. CCG-Based Word Ordering 3.1 The CCG Grammar We were motivated to use CCG as one of the grammar formalisms for our syntax-based realization system because of its successful application to a number of related tasks, such as wide-coverage parsing (Hockenmaier 2003; Clark and Curran 2007b; Auli and Lopez 2011), semantic parsing (Zettlemoyer and Collins 2005), wide-coverage semantic analysis (Bos et al. 2004), and generation itself (Espinosa, White, and Mehay 2008). The grammar formalism has been described in detail in those papers, and so here we provide only a short description. CCG (Steedman 2000) is a lexicalized grammar formalism that associates words with lexical categories. Lexical categories are detailed grammatical labels, typically expressing subcategorization information. During CCG parsing, and during our search procedure, categories are combined using CCG’s combinatory rules. For example, a verb phrase in English (SNP) can combine with an NP to its left, in this case using the combin"
J15-3005,J98-2004,0,0.113266,"derivation. Hypotheses are constructed bottom–up: starting from single words, smaller phrases are combined into larger ones according to CCG rules. To allow the combination of hypotheses, we use an additional structure to store a set of hypotheses that have been expanded, which we call accepted 506 Zhang and Clark Discriminative Syntax-Based Word Ordering hypotheses. When a hypothesis from the agenda is expanded, it is combined with all accepted hypotheses in all possible ways to produce new hypotheses. The data structure for accepted hypotheses is similar to that used for best-first parsing (Caraballo and Charniak 1998), and we adopt the term chart for this structure. However, note there are important differences to the parsing problem. First, the parsing problem has a fixed word order and is considerably simpler than the word ordering problem we are tackling. Second, although we use the term chart, the structure for accepted hypotheses is not a dynamic programming chart in the same way as for the parsing problem. In our previous papers (Zhang and Clark 2011; Zhang, Blackwood, and Clark 2012), we applied a set of beams to this structure, which makes it similar to the data structure used for phrase-based MT d"
J15-3005,I05-1015,0,0.200152,"Missing"
J15-3005,P07-1002,0,0.0661321,"uage as part of an integrated system, which avoids the high computational complexity of independent word ordering. On the other hand, performing word ordering separately in a pipeline has many potential advantages. For SMT, it offers better modularity between adequacy (translation) and fluency (linearization), and can potentially improve target grammaticality for syntactically different languages (e.g., Chinese and English). More importantly, a standalone word ordering component can in principle be applied to a wide range of text generation tasks, including transfer-based machine translation (Chang and Toutanova 2007). Most word ordering systems use an n-gram language model, which is effective at controling local fluency. Syntax-based language models, in particular dependency language models (Xu, Chelba, and Jelinek 2002), are sometimes used in an attempt to improve global fluency through the capturing of long-range dependencies. In this article, we take a syntax-based approach and consider two grammar formalisms: Combinatory Categorial Grammar (CCG) and dependency grammar. Our system also employs a discriminative model. Coupled with heuristic search, a strength of the model is that arbitrary features can"
J15-3005,J07-2003,0,0.343996,"on received: 23 April 2014; accepted for publication: 22 June 2014. doi:10.1162/COLI a 00229 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 3 those words. Additional annotation may also be provided with the input—for example, part-of-speech (POS) tags or syntactic dependencies. Applications that can benefit from better text generation algorithms include machine translation (Koehn 2010), abstractive text summarization (Barzilay and McKeown 2005), and grammar correction (Lee and Seneff 2006). Typically, statistical machine translation (SMT) systems (Chiang 2007; Koehn 2010) perform generation into the target language as part of an integrated system, which avoids the high computational complexity of independent word ordering. On the other hand, performing word ordering separately in a pipeline has many potential advantages. For SMT, it offers better modularity between adequacy (translation) and fluency (linearization), and can potentially improve target grammaticality for syntactically different languages (e.g., Chinese and English). More importantly, a standalone word ordering component can in principle be applied to a wide range of text generation"
J15-3005,W07-1202,1,0.867426,"Missing"
J15-3005,J07-4004,1,0.74908,"eses are expanded before the gold-standard goal hypothesis is found. In this case, the minimum number of hypotheses is expanded and the output is correct. The best-first search decoder is optimal not only with respect to accuracy but also speed. This ideal situation can hardly be met in practice, but it determines the goal of the training algorithm: to find a model that scores gold-standard hypotheses higher than non-gold ones. Learning-guided search places more challenges on the training of a discriminative model than standard structured prediction problems, for example, CKY parsing for CCG (Clark and Curran 2007b). If we take gold-standard hypotheses as positive training examples, and non-gold hypotheses as negative examples, then the training goal is to find a large separating margin between the scores of all positive examples and all negative examples. For CKY parsing, the highest-scored negative example can be found via optimal Viterbi decoding, according to the current model, and this negative example can be used in place of all negative examples during the updating of parameters. In contrast, our best-first search algorithm cannot find an output in reasonable time unless a good model has already"
J15-3005,P04-1015,0,0.359558,"Missing"
J15-3005,P08-1022,0,0.103695,"Missing"
J15-3005,P07-1041,0,0.0635731,"Missing"
J15-3005,N09-2057,0,0.152421,"Missing"
J15-3005,P10-1035,0,0.0140586,". For example, a verb phrase in English (SNP) can combine with an NP to its left, in this case using the combinatory rule of (backward) function application: NP SNP ⇒ S In addition to binary rule instances, such as this one, there are also unary rules that operate on a single category in order to change its type. For example, forward typeraising can change a subject NP into a complex category looking to the right for a verb phrase: NP ⇒ S/(SNP) Such a type-raised category can then combine with a transitive verb type using the rule of forward composition: S/(SNP) (SNP)/NP ⇒ S/NP Following Fowler and Penn (2010), we extract the grammar by reading rule instances directly from the derivations in CCGbank (Hockenmaier and Steedman 2007), rather than defining the combinatory rule schema manually as in Clark and Curran 508 Zhang and Clark Discriminative Syntax-Based Word Ordering (2007b). Hence the grammar we use can be thought of as a context-free approximation to the mildly content sensitive grammar arising from the use of generalized composition rules (Weir 1988). Hockenmaier (2003) contains a detailed description of the grammar that is obtained in this way, including the various unary type-changing rul"
J15-3005,N10-1115,0,0.0282392,"Missing"
J15-3005,W11-2833,0,0.0541311,"Missing"
J15-3005,P09-1091,0,0.367407,"onstration of the applicability of the system to more realistic text generation scenarios, we consider two further tasks for the dependency-based realization system. The first task considers a variety of input conditions for the dependency-based system, determined by two parameters. The first is whether POS information is provided for each word in the input multi-set. The second is whether syntactic dependencies between the words are provided. The extreme case is when all dependencies are provided, in which case the problem reduces to the tree linearization problem (Filippova and Strube 2009; He et al. 2009). However, the input can also lie between the two extremes of no- and full-dependency information. The second task is the NLG 2011 shared task, which provides a further demonstration of the practical utility of our system. The shared task is closer to a real realization scenario, in that lemmas, rather than inflected words, are provided as input. Hence some modifications are required to our system in order that it can perform some word inflection, as well as deciding on the ordering. The shared task data also uses labeled, rather than unlabeled, syntactic dependencies, and so the system was mo"
J15-3005,J07-3004,0,0.278928,"papers (Zhang and Clark 2011; Zhang, Blackwood, and Clark 2012), we applied a set of beams to this structure, which makes it similar to the data structure used for phrase-based MT decoding (Koehn 2010). However, we will show later that this structure is unnecessary when the model has more discriminative power, and a conceptually simpler single beam can be used. We will also investigate the possibility of applying dynamic-programming-style pruning to the chart. We now give an overview of the training algorithm, which is crucial to both the speed and accuracy of the resulting decoder. CCGBank (Hockenmaier and Steedman 2007) is used to train the model. For each training sentence, the corresponding CCGBank derivation together with all its sub-derivations are treated as gold-standard hypotheses. All other hypotheses that can be constructed from the same bag of words are non-gold hypotheses. From the generation perspective this assumption is too strong, because sentences can have multiple orderings (with multiple derivations) that are both grammatical and fluent. Nevertheless, it is the most feasible choice given the training data available. The efficiency of the decoding algorithm is dependent on the training algor"
J15-3005,N12-1015,0,0.0610609,"Missing"
J15-3005,P10-1110,0,0.0790259,"Missing"
J15-3005,W07-2416,0,0.0158856,"Missing"
J15-3005,P96-1027,0,0.192408,"ng. The shared task data also uses labeled, rather than unlabeled, syntactic dependencies, and so the system was modified to incorporate labels. The final result is that our system gives competitive BLEU scores, compared to the best-performing systems on the shared task. The structured prediction problem we solve is a very hard problem. Due to the use of syntax, and the search for a sentence together with a single CCG derivation or dependency tree, the search space is exponentially larger than the n-gram word permutation problem. No efficient algorithm exists for finding the optimal solution. Kay (1996) recognized the computational difficulty of chart-based generation, which has many similarities to the problem we address in his seminal paper. We tackle the high complexity by using learning-guided best-first search, exploring a small path in the whole search space, which contains the most likely structures according to the discriminative model. One of the contributions of this article is to introduce, and provide a discriminative solution to, this difficult structured prediction problem, which is an interesting machine learning problem in its own right. This article is based on, and signific"
J15-3005,J10-4005,0,0.178397,"Cambridge Computer Laboratory, William Gates Building, 15 JJ Thomson Avenue, Cambridge, UK. E-mail: stephen.clark@cl.cam.ac.uk. Submission received: 17 April 2013; revised version received: 23 April 2014; accepted for publication: 22 June 2014. doi:10.1162/COLI a 00229 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 3 those words. Additional annotation may also be provided with the input—for example, part-of-speech (POS) tags or syntactic dependencies. Applications that can benefit from better text generation algorithms include machine translation (Koehn 2010), abstractive text summarization (Barzilay and McKeown 2005), and grammar correction (Lee and Seneff 2006). Typically, statistical machine translation (SMT) systems (Chiang 2007; Koehn 2010) perform generation into the target language as part of an integrated system, which avoids the high computational complexity of independent word ordering. On the other hand, performing word ordering separately in a pipeline has many potential advantages. For SMT, it offers better modularity between adequacy (translation) and fluency (linearization), and can potentially improve target grammaticality for synt"
J15-3005,P07-2045,0,0.00311972,"features can be defined to capture complex syntactic patterns in output hypotheses. The discriminative model is trained using syntactically annotated data. From the perspective of search, word ordering is a computationally difficult problem. Finding the best permutation for a set of words according to a bigram language model, for example, is NP-hard, which can be proved by linear reduction from the traveling salesman problem. In practice, exploring the whole search space of permutations is often prevented by adding constraints. In phrase-based machine translation (Koehn, Och, and Marcu 2003; Koehn et al. 2007), a distortion limit is used to constrain the position of output phrases. In syntax-based machine translation systems such as Wu (1997) and Chiang (2007), synchronous grammars limit the search space so that polynomialtime inference is feasible. In fluency improvement (Blackwood, de Gispert, and Byrne 2010), parts of translation hypotheses identified as having high local confidence are held fixed, so that word ordering elsewhere is strictly local. In this article we begin by proposing a general system to solve the word ordering problem, which does not rely on constraints (which are typically ta"
J15-3005,N03-1017,0,0.0413498,"Missing"
J15-3005,P10-1001,0,0.0132787,"e decoding algorithm. Here a leaf edge refers to an input word with a POS tag, and a non-leaf edge refers to a phrase or sentence with its dependency tree. Edges are constructed bottom–up, by recursively joining two existing edges and adding an unlabeled dependency link between their head words. As for the CCG system, edges are scored by a global linear model: f (e) =  Φ(e) · θ |e|  is the parameter vector of the model. where Φ(e) represents the feature vector of e and θ Table 2 shows the feature templates we use, which are inspired by the rich feature templates used for dependency parsing (Koo and Collins 2010; Zhang and Nivre 2011). In the table, h, m, s, hl , hr , ml , and mr are the indices of words in the newly constructed edge, where h and m refer to the head and dependent of the newly constructed arc, s refers to the nearest sibling of m (on the same side of h), and hl , hr , ml , and mr refer to the left and rightmost dependents of h and m, respectively. WORD, POS, LVAL, and RVAL are maps from indices to word forms, POS , left valencies, and right valencies of words, respectively. Example feature instances extracted from the sentence in Figure 3 are shown in the example column. Because of th"
J15-3005,P13-1001,0,0.0356189,"Missing"
J15-3005,J93-2004,0,0.0516151,"Missing"
J15-3005,P02-1040,0,0.0981861,"lti-set (bag) of words and the output is an ordered sentence, together with its syntactic analysis (either CCG derivation or dependency tree, depending on the grammar formalism being used). Given an input, our system searches for the highestscored output, according to a syntax-based discriminative model. One advantage of this formulation of the reordering problem, which can perhaps be thought of as a “pure” text realization task, is that systems for solving it are easily evaluated, because all that is required is a set of sentences for reordering and a standard evaluation metric such as BLEU (Papineni et al. 2002). However, one potential criticism of the “pure” problem is that it is unclear how it relates to real realization tasks, since in practice (e.g., in statistical machine translation systems) the input does provide constraints on the possible output orderings. Our general formulation still allows task-specific contraints to be added if appropriate. Hence as a test of the flexibility of our system, 504 Zhang and Clark Discriminative Syntax-Based Word Ordering and a demonstration of the applicability of the system to more realistic text generation scenarios, we consider two further tasks for the d"
J15-3005,W96-0213,0,0.358539,"ng and Clark Discriminative Syntax-Based Word Ordering Table 2 (continued) dependency syntax example surface string patterns for complete sentences WORD (0), WORD (0) · WORD (1), WORD (size − 1), WORD (size − 1) · WORD (size − 2), POS (0), POS (0) · POS (1), POS (0) · POS (1) · POS (2), POS (size − 1), POS (size − 1) · POS (size − 2), POS (size − 1) · POS (size − 2) · POS (size − 3), (VBD, NNP, NN) the allowed POS tag is assigned. For unconstrained words, we assign all possible POS tags according to a tag dictionary compiled from the training data, following standard practice for POS-tagging (Ratnaparkhi 1996). When an edge is expanded, it is combined with all edges in the chart in all possible ways to generate new edges. Two edges can be combined by concatenation of the surface strings in both orders and, in each case, constructing a dependency link between their heads in two ways (corresponding to the two options for the head of the new link). When there is a head constraint on the dependent word, a dependency link can be constructed only if it is consistent with the constraint. This algorithm implements abstract word ordering, partial-tree linearization, and full tree linearization—all generaliz"
J15-3005,D08-1052,0,0.0633386,"Missing"
J15-3005,P07-1096,0,0.0899193,"Missing"
J15-3005,W08-2121,0,0.0158951,"Missing"
J15-3005,E09-1097,0,0.311155,"teedman 2007) and the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993) for CCG and dependency data, respectively. CCGbank is the CCG version of the Penn Treebank. Standard splits were used for both: Sections 02–21 for training, Section 00 for development, and Section 23 for the final test. Table 3 gives statistics for the Penn Treebank. For the CCG experiments, original sentences from CCGBank are transformed into bags of words, with sequence information removed, and passed to our system as input data. The system outputs are compared to the original sentences for evaluation. Following Wan et al. (2009), we use the BLEU metric (Papineni et al. 2002) for string comparison. Although BLEU is not the perfect measure of fluency or grammaticality, being based on n-gram precision, it is currently widely used for automatic evaluation and allows us to compare directly with existing work (Wan et al. 2009). Note also that one criticism of BLEU for evaluating machine translation systems (i.e., that it can only register exact matches between the same words in the system and reference translation), does not apply here, because the system output always contains the same words as the original reference sent"
J15-3005,D09-1043,0,0.173991,"Missing"
J15-3005,J97-3002,0,0.105001,"otated data. From the perspective of search, word ordering is a computationally difficult problem. Finding the best permutation for a set of words according to a bigram language model, for example, is NP-hard, which can be proved by linear reduction from the traveling salesman problem. In practice, exploring the whole search space of permutations is often prevented by adding constraints. In phrase-based machine translation (Koehn, Och, and Marcu 2003; Koehn et al. 2007), a distortion limit is used to constrain the position of output phrases. In syntax-based machine translation systems such as Wu (1997) and Chiang (2007), synchronous grammars limit the search space so that polynomialtime inference is feasible. In fluency improvement (Blackwood, de Gispert, and Byrne 2010), parts of translation hypotheses identified as having high local confidence are held fixed, so that word ordering elsewhere is strictly local. In this article we begin by proposing a general system to solve the word ordering problem, which does not rely on constraints (which are typically task-specific). In particular, we treat syntax-based word ordering as a structured prediction problem, for which the input is a multi-set"
J15-3005,P02-1025,0,0.121191,"Missing"
J15-3005,E12-1075,1,0.860952,"Missing"
J15-3005,P08-1101,1,0.743215,"Missing"
J15-3005,D10-1082,1,0.886146,"Missing"
J15-3005,J11-1005,1,0.0512939,"of chart-based generation, which has many similarities to the problem we address in his seminal paper. We tackle the high complexity by using learning-guided best-first search, exploring a small path in the whole search space, which contains the most likely structures according to the discriminative model. One of the contributions of this article is to introduce, and provide a discriminative solution to, this difficult structured prediction problem, which is an interesting machine learning problem in its own right. This article is based on, and significantly extends, three conference papers (Zhang and Clark 2011; Zhang, Blackwood, and Clark 2012; Zhang 2013). It includes a more detailed description and discussion of our guided-search approach to syntax-based word ordering, bringing together the CCG- and dependency-based systems under one unified framework. In addition, we discuss the limitations of our previous work, and show that a better model can be developed through scaling of the feature vectors. The resulting model allows fair comparison of constituents of different sizes, and enables the learning algorithms to expand negative examples during training, which leads to significantly improved resu"
J15-3005,P11-2033,1,0.903535,"Missing"
J16-4004,S15-2045,0,0.0693849,"0; Vecchi, Baroni, and Zamparelli 2011; Grefenstette and Sadrzadeh 2011; Boleda et al. 2012; Boleda et al. 2013; Lazaridou, Vecchi, and Baroni 2013). Such data sets have been essential for evaluating the first generation of compositional distributional models, but the relative grammatical simplicity of the phrases—in particular, the absence of function words—leaves open a wide range of compositional phenomena in natural language against which models must be evaluated.1 Other data sets, including those used in recent SemEval and *SEM Shared Tasks (Agirre et al. 2012, 2013; Marelli et al. 2014; Agirre 2015), focus on pairs of full sentences, some as long as 20 words or more. The evaluation is based on a numeric similarity measure for each sentence pair. These data sets represent a more realistic task for language technology applications, but reducing sentence similarity to a single value makes it difficult to identify how a model performs on subparts of a sentence, or on specific grammatical constructions, masking the areas where models need improvement.2 In the domain of syntactic parsing, a call has been made for “grammatical construction-focused” parser evaluation (Rimell, Clark, and Steedman"
J16-4004,S12-1051,0,0.0892441,"Missing"
J16-4004,S13-1004,0,0.0620904,"Missing"
J16-4004,P14-1023,0,0.0567732,"Missing"
J16-4004,D10-1115,0,0.0940594,", and Sadrzadeh 2008; Baroni, Bernardi, and Zamparelli 2014) integrates ¨ distributional semantic representations of words (Schutze 1998; Turney and Pantel 2010; Clark 2015) with formal methods for composing word representations into larger phrases and sentences (Montague 1970; Dowty, Wall, and Peters 1981). In recent years a number of composition methods have been proposed, including simple arithmetic operations on distributional word vectors (Mitchell and Lapata 2008, 2010), multi-linear operations involving higher-order representations of argument-taking words such as verbs and adjectives (Baroni and Zamparelli 2010; Coecke, Sadrzadeh, and Clark 2010), and composition of distributed word vectors learned with neural networks (Socher, Manning, and Ng 2010; Mikolov, Yih, and Zweig 2013). To compare such approaches it is important to have high-quality data sets for evaluating composed phrase representations at different levels of granularity and complexity. Existing evaluation data sets fall largely into two categories. Some data sets focus on two to three word phrases consisting of content words only (i.e., no closedclass function words), including subject–verb, verb–object, subject–verb–object, and adjecti"
J16-4004,D11-1037,0,0.0195336,"on pairs of full sentences, some as long as 20 words or more. The evaluation is based on a numeric similarity measure for each sentence pair. These data sets represent a more realistic task for language technology applications, but reducing sentence similarity to a single value makes it difficult to identify how a model performs on subparts of a sentence, or on specific grammatical constructions, masking the areas where models need improvement.2 In the domain of syntactic parsing, a call has been made for “grammatical construction-focused” parser evaluation (Rimell, Clark, and Steedman 2009; Bender et al. 2011), focusing on individual, often challenging syntactic structures, in order to tease out parser performance in these areas from overall accuracy scores. We make an analogous call here, for a wider range of compositional phenomena to be investigated in compositional distributional semantics in the near future. 1 Exceptions are the data sets of Bernardi et al. (2013) and Pham et al. (2013), which include determiners such as two, most, and no in the phrases to be composed; see Section 2.1. 2 The pilot subtask on interpretable Semantic Textual Similarity (Agirre 2015) begins to address this problem"
J16-4004,P13-2010,0,0.0664342,"atical constructions, masking the areas where models need improvement.2 In the domain of syntactic parsing, a call has been made for “grammatical construction-focused” parser evaluation (Rimell, Clark, and Steedman 2009; Bender et al. 2011), focusing on individual, often challenging syntactic structures, in order to tease out parser performance in these areas from overall accuracy scores. We make an analogous call here, for a wider range of compositional phenomena to be investigated in compositional distributional semantics in the near future. 1 Exceptions are the data sets of Bernardi et al. (2013) and Pham et al. (2013), which include determiners such as two, most, and no in the phrases to be composed; see Section 2.1. 2 The pilot subtask on interpretable Semantic Textual Similarity (Agirre 2015) begins to address this problem. 662 Rimell et al. A Relative Clause Evaluation Data Set This article begins to answer that call by presenting RELPRON, a data set of noun phrases consisting of a noun modified by a relative clause. For example, building that hosts premieres is a noun phrase containing a subject relative clause (a relative clause with an extracted subject), and in our data set de"
J16-4004,W11-1304,0,0.0739665,"Missing"
J16-4004,W13-0104,0,0.0712458,"atical constructions, masking the areas where models need improvement.2 In the domain of syntactic parsing, a call has been made for “grammatical construction-focused” parser evaluation (Rimell, Clark, and Steedman 2009; Bender et al. 2011), focusing on individual, often challenging syntactic structures, in order to tease out parser performance in these areas from overall accuracy scores. We make an analogous call here, for a wider range of compositional phenomena to be investigated in compositional distributional semantics in the near future. 1 Exceptions are the data sets of Bernardi et al. (2013) and Pham et al. (2013), which include determiners such as two, most, and no in the phrases to be composed; see Section 2.1. 2 The pilot subtask on interpretable Semantic Textual Similarity (Agirre 2015) begins to address this problem. 662 Rimell et al. A Relative Clause Evaluation Data Set This article begins to answer that call by presenting RELPRON, a data set of noun phrases consisting of a noun modified by a relative clause. For example, building that hosts premieres is a noun phrase containing a subject relative clause (a relative clause with an extracted subject), and in our data set de"
J16-4004,D12-1112,0,0.0462126,"Missing"
J16-4004,D14-1082,0,0.0128309,"ence, and 100-dimensional target and context vectors. All lemmas occurring fewer than 100 times in the corpus were ignored.12 The context vectors produced during word vector learning were retained for use during training of the holistic vectors for phrases. Verb matrices and relative pronoun matrices and tensors require holistic vectors to serve as training targets for linear regression. These vectors represent the observed contexts in which instances of verbs or relative pronouns with their arguments are found in the Wikipedia corpus. We tagged and parsed the corpus with the Stanford Parser (Chen and Manning 2014). For learning of verb matrices, we extracted verb– subject and verb–object pairs, ensuring that each pair occurred within a transitive verb 12 The word slipping, which occurs once in the RELPRON test set, was missing from the resulting corpus. We substituted the vector for slip at test time. 682 Rimell et al. A Relative Clause Evaluation Data Set construction. For learning of relative pronoun matrices and tensors, we extracted hhead noun, verb, argumenti tuples that occurred within relative clause constructions. To identify relative clauses, we looked for characteristic dependency subtrees co"
J16-4004,E12-1005,0,0.0422427,"Missing"
J16-4004,W13-3005,1,0.909848,"Missing"
J16-4004,J07-4004,1,0.656198,"pha (Minnen, Carroll, and Pearce 4 For example, for person the WordNet command for viewing the hyponym hierarchy was: wn person -n1 -treen. 668 Rimell et al. A Relative Clause Evaluation Data Set Table 1 Sample SVO triples extracted from the source corpus for the candidate term traveler. Object Subject it allow traveler website allow traveler inn serve traveler business serve traveler she become traveler student become traveler traveler make decision traveler make journey traveler take road traveler take advantage traveler have option traveler have luggage 2001) and parsed with the C&C Tools (Clark and Curran 2007) to obtain the subject and object relations. A minimum frequency cutoff of six occurrences per verb, per grammatical function (subject/object), per candidate term was applied. Properties containing proper noun arguments were removed. Table 1 shows some sample triples extracted from the corpus prior to manual annotation. 3.4 Conversion to Relative Clause Form The candidate properties were converted to relative clause form prior to annotation, by joining the head noun with the relevant part of the SVO triple, using the template term: headnoun that V O for a subject property, or term: headnoun th"
J16-4004,P15-2120,1,0.912259,"Missing"
J16-4004,W11-0112,0,0.140913,"Missing"
J16-4004,W13-0112,0,0.0367118,"e, by copying the subject or object slices of the original tensor; for example, in the copy-object method the representation for V is NV − → −→ −→ X (s) (o) (o) ni ⊗ ni ⊗ ni V= (13) i=1 Representing sentence meaning in the noun space confers the advantage that words, phrases, and sentences of any grammatical structure can be compared with one another, but it is an open question whether the features in the noun space, whether distributional or not, are appropriate or adequate for representing sentence meaning. An alternative way of learning the transitive verb tensor parameters is presented in Grefenstette et al. (2013), using a process analogous to B&Z’s process for learning adjectives. Two linear regression steps are performed. In the first step, a matrix is learned representing verb–object phrases, that is, a verb that has already been paired with its object. For example, the matrix for eat meat is learned as a mapping from corpus instances such as dogs and dogs eat meat. The full tensor for eat is then learned with meat as input and the eat meat matrix as output. Essentially, the subject mapping is learned in the first step, and the object mapping in the second. Polajnar, Fagarasan, and Clark (2014), fol"
J16-4004,D11-1129,0,0.590929,"Manning, and Ng 2010; Mikolov, Yih, and Zweig 2013). To compare such approaches it is important to have high-quality data sets for evaluating composed phrase representations at different levels of granularity and complexity. Existing evaluation data sets fall largely into two categories. Some data sets focus on two to three word phrases consisting of content words only (i.e., no closedclass function words), including subject–verb, verb–object, subject–verb–object, and adjective–noun combinations (Mitchell and Lapata 2008, 2010; Baroni and Zamparelli 2010; Vecchi, Baroni, and Zamparelli 2011; Grefenstette and Sadrzadeh 2011; Boleda et al. 2012; Boleda et al. 2013; Lazaridou, Vecchi, and Baroni 2013). Such data sets have been essential for evaluating the first generation of compositional distributional models, but the relative grammatical simplicity of the phrases—in particular, the absence of function words—leaves open a wide range of compositional phenomena in natural language against which models must be evaluated.1 Other data sets, including those used in recent SemEval and *SEM Shared Tasks (Agirre et al. 2012, 2013; Marelli et al. 2014; Agirre 2015), focus on pairs of full sentences, some as long as 20 word"
J16-4004,W11-0114,1,0.846413,"nd Zweig (2013) can be adapted to learn adjective matrices as part of a neural embedding objective. For verb–argument composition, the question of sentence spaces arises.9 A transitive verb such as chase lives in S ⊗ N ⊗ N, and can therefore be represented as X → → → Cijk (− si ⊗ − nj ⊗ − nk ) (11) ijk 9 We consider a verb with all of its argument positions saturated—for example, an SVO triple—to be a sentence, although real-world sentences are much longer and contain determiners and other function words. 677 Computational Linguistics Volume 42, Number 4 for some choice of coefficients Cijk . Grefenstette et al. (2011) choose S = N ⊗ N, using the tensor product of the noun space with itself as the sentence space. Their concrete implementation learns the verb by counting observed n(s) , n(o) pairs where n(s) is the subject and n(o) the object of V. The final representation for V is then V= NV − → −→ X (s) (o) ni ⊗ ni (12) i=1 (s) (o) where NV is the number of instances of V in the corpus, and (ni , ni ) are the subject and object nouns for the ith instance of V. This method of learning the verb tensor is known as the relational method, because it captures relations between subject and object features. Compos"
J16-4004,S15-1017,0,0.0322441,"Missing"
J16-4004,W13-3209,0,0.0258491,"Missing"
J16-4004,J07-3004,0,0.0304096,"judgment is produced. Polajnar, Rimell, and Clark (2015) investigate a distributional sentence space based on the wider discourse context, in which the meaning of a sentence is represented as a distributional vector based on context words in the surrounding discourse, making the sentence space completely distinct from the noun space.10 Existing implementations of the Categorial framework have investigated learning up to third-order tensors (for transitive verbs). However, in practice, syntactic categories such as ((N/N)/(N/N))/((N/N)/(N/N)) are not uncommon in the wide-coverage CCG grammar of Hockenmaier and Steedman (2007); such a category would require an eighth-order tensor. The combination of many word–category pairs and higher-order tensors results in a huge number of parameters to be learned in any implementation. As a solution to this problem, various ways of reducing the number of parameters are being investigated, for example, using tensor decomposition techniques (Kolda and Bader 2009; Fried, Polajnar, and Clark 2015), and removing some of the interactions encoded in the tensor by using only matrices to encode each predicate–argument combination (Paperno, Pham, and Baroni 2014; Polajnar, Fagarasan, and"
J16-4004,S14-2003,0,0.0544604,"Missing"
J16-4004,D13-1166,0,0.0142849,".471 0.563 0.490 0.394 0.415 0.413 0.393 0.463 0.360 0.390 0.592 0.632 0.502 0.542 0.595 0.546 0.448 0.443 0.462 0.512 0.378 0.384 head noun. Several document terms (lease, form, assignment, bond) exhibit very low AP. The term form provides an example; it has an extremely low AP of 0.01. Although form was chosen by the annotators in its document sense, with correct properties including document that parent signs and document that applicant completes, other senses may be more dominant in the source corpus, confusing the similarity ranking.17 Prior disambiguation of words in the data set, as in Kartsaklis and Sadrzadeh (2013), might improve performance. We believe that polysemy may come into play with device terms as well, though the terms have high concreteness. One term with a very low AP is watch. Among the properties that SPLF ranks high for watch are person that police hunt (killer) and device that views stars (telescope), both of which are related to different senses of watch. 7.3 Capturing the Semantics of Relative Clauses A system that successfully captures the semantics of relative clauses must integrate the semantic contribution of the head noun with the contributions of the verb and argument; for exampl"
J16-4004,C12-2054,0,0.0520044,"Missing"
J16-4004,W13-3201,0,0.0181076,"a process analogous to B&Z’s process for learning adjectives. Two linear regression steps are performed. In the first step, a matrix is learned representing verb–object phrases, that is, a verb that has already been paired with its object. For example, the matrix for eat meat is learned as a mapping from corpus instances such as dogs and dogs eat meat. The full tensor for eat is then learned with meat as input and the eat meat matrix as output. Essentially, the subject mapping is learned in the first step, and the object mapping in the second. Polajnar, Fagarasan, and Clark (2014), following Krishnamurthy and Mitchell (2013), investigate a non-distributional sentence space, the “plausibility space” described by Clark (2013, 2015). Here the sentence space is one- or two-dimensional, with sentence meaning either a real number between 0 and 1, or a probability distribution over the classes plausible and implausible. A verb tensor is learned using single-step linear regression, with the training data consisting of positive (plausible) and negative (implausible) SVO examples. Positive examples are attested in the corpus, and negative 678 Rimell et al. A Relative Clause Evaluation Data Set examples for a given verb hav"
J16-4004,D13-1196,0,0.0269502,"Missing"
J16-4004,Q15-1016,0,0.0286903,"ntations are built. Many of the composition techniques we describe can be equally applied to “classical” distributional vectors built using count methods (Turney and Pantel 2010; Clark 2015) and vectors built using neural embedding techniques. It may be that some composition techniques work better with one type of vector or the other—for example, there is evidence that elementwise multiplication performs better with count methods—although research comparing the two vector-building methods is still in its infancy (Baroni, Dinu, and Kruszewski 2014; Levy and Goldberg 2014; Milajevs et al. 2014; Levy et al. 2015). Section 5 provides details of how our vector representations are built in practice. 4.1 Simple Operators on Vectors The simplest method of combining two distributional representations, for example, vectors for fast and car, is to perform some elementwise combination, such as vector Table 4 Total number of subject and object properties by head noun in RELPRON. Test Set Head noun Sbj Obj Development Set Head noun Sbj Obj phenomenon activity material room vehicle mammal woman scientist quality organization device building document person player TOTAL 672 38 46 30 38 38 41 20 58 42 37 52 42 41 2"
J16-4004,Q13-1015,0,0.106794,"rk (Coecke, Sadrzadeh, and Clark 2010), which uses higher-order tensors—a generalization of matrices—for words which take more than one argument. The Categorial framework has vectors and their multi-linear analogues as native elements, and provides a natural operation for composition, namely, tensor contraction (see Section 4.2.2). It therefore stands in contrast to frameworks for compositional distributional semantics that effectively take the logic of formal semantics as a starting point, and use distributional semantics to enrich the logical representations (Garrette, Erk, and Mooney 2011; Lewis and Steedman 2013; Herbelot and Copestake 2015). It is important to note that the Categorial framework makes no commitment to how the multi-linear algebraic objects are realized, only a commitment to their shape and how they are combined. In particular, the vector spaces corresponding to atomic categories, such as noun and sentence, do not have to be distributional in the classical sense; they could be distributed in the connectionist sense (Smolensky 1990), where the basis vectors themselves are not readily interpretable (the neural embeddings that we use in this article fall into this category). 4.2.1 Compos"
J16-4004,K15-1035,1,0.498264,"ial Framework. There are a few existing implementations of the Categorial framework, focusing mostly on adjectives and transitive verbs. The adjective implementation is that of B&Z as described in Section 4.2.1, where linear regression is used to learn each adjective as a mapping from noun contexts to adjective–noun contexts, with the observed context vectors for the noun and adjective– noun instances as training data. Because an adjective–noun combination has noun phrase meaning, the noun space is the obvious choice for the space in which the composed meanings should live. Maillard and Clark (2015) extend this idea by showing how the skip-gram model of Mikolov, Yih, and Zweig (2013) can be adapted to learn adjective matrices as part of a neural embedding objective. For verb–argument composition, the question of sentence spaces arises.9 A transitive verb such as chase lives in S ⊗ N ⊗ N, and can therefore be represented as X → → → Cijk (− si ⊗ − nj ⊗ − nk ) (11) ijk 9 We consider a verb with all of its argument positions saturated—for example, an SVO triple—to be a sentence, although real-world sentences are much longer and contain determiners and other function words. 677 Computational"
J16-4004,W14-1406,1,0.883269,"Missing"
J16-4004,S14-2001,0,0.0341338,"Every word has both associated types, but the network may learn to put more weight on one type or the other for a given word. 674 Rimell et al. A Relative Clause Evaluation Data Set − → car red R11  R21   R31 R 41 R51  R12 R22 R32 R42 R52 R13 R23 R33 R43 R53 R14 R24 R34 R44 R54 −−−→ red car     R15 c1 rc1 R25   c2   rc2    =   R35   c3   rc3  R45   c4   rc4  R55 c5 rc5 Figure 2 Matrix mulitiplication for adjective–noun combinations. an instance of the Categorial framework (and also an instance of the general framework described in Baroni, Bernardi, and Zamparelli [2014]). The insight in B&Z is that, in theoretical linguistics, adjectives are typically thought of as having a functional role, mapping noun denotations to noun denotations. B&Z make the transition to vector spaces by arguing that, in linear algebra, functions are represented as matrices (the linear maps). This insight provides an obvious answer to the question of what the composition operator should be in this framework, namely, matrix multiplication. Figure 2 shows how the context vector for car is multiplied by the matrix for red to produce the → = (c , . . . , c )T vector for red car. In this"
J16-4004,N13-1090,0,0.0881022,"Missing"
J16-4004,D14-1079,0,0.333945,"how the vector representations are built. Many of the composition techniques we describe can be equally applied to “classical” distributional vectors built using count methods (Turney and Pantel 2010; Clark 2015) and vectors built using neural embedding techniques. It may be that some composition techniques work better with one type of vector or the other—for example, there is evidence that elementwise multiplication performs better with count methods—although research comparing the two vector-building methods is still in its infancy (Baroni, Dinu, and Kruszewski 2014; Levy and Goldberg 2014; Milajevs et al. 2014; Levy et al. 2015). Section 5 provides details of how our vector representations are built in practice. 4.1 Simple Operators on Vectors The simplest method of combining two distributional representations, for example, vectors for fast and car, is to perform some elementwise combination, such as vector Table 4 Total number of subject and object properties by head noun in RELPRON. Test Set Head noun Sbj Obj Development Set Head noun Sbj Obj phenomenon activity material room vehicle mammal woman scientist quality organization device building document person player TOTAL 672 38 46 30 38 38 41 20"
J16-4004,P08-1028,0,0.304733,"roperties. This is ¨ analogous to an Information Retrieval task (Manning, Raghavan, and Schutze 2008), where the term is the query and the properties are the documents, with the identifying properties for a given term as the relevant documents. Evaluation can make use of standard Information Retrieval measures; in this article we use Mean Average Precision (MAP). 2.1 Existing Evaluation Data Sets Most existing evaluation data sets for compositional distributional semantics have been based on small, fixed syntactic contexts, typically adjective–noun or subject–verb– object. Mitchell and Lapata (2008) introduce an evaluation that has been adopted by a number of other researchers. The task is to predict human similarity judgments on subject–verb phrases, where the similarity rating depends on word sense disambiguation in context. For example, horse run is more similar to horse gallop than to horse dissolve, whereas colors run is more similar to colors dissolve than to colors gallop. Compositional models are evaluated on how well their similarity judgments correlate with the human judgments. Mitchell and Lapata (2010) introduce a phrase similarity data set consisting of adjective–noun, verb–"
J16-4004,P14-1009,0,0.0659531,"Missing"
J16-4004,W13-0603,0,0.105766,"tructions, masking the areas where models need improvement.2 In the domain of syntactic parsing, a call has been made for “grammatical construction-focused” parser evaluation (Rimell, Clark, and Steedman 2009; Bender et al. 2011), focusing on individual, often challenging syntactic structures, in order to tease out parser performance in these areas from overall accuracy scores. We make an analogous call here, for a wider range of compositional phenomena to be investigated in compositional distributional semantics in the near future. 1 Exceptions are the data sets of Bernardi et al. (2013) and Pham et al. (2013), which include determiners such as two, most, and no in the phrases to be composed; see Section 2.1. 2 The pilot subtask on interpretable Semantic Textual Similarity (Agirre 2015) begins to address this problem. 662 Rimell et al. A Relative Clause Evaluation Data Set This article begins to answer that call by presenting RELPRON, a data set of noun phrases consisting of a noun modified by a relative clause. For example, building that hosts premieres is a noun phrase containing a subject relative clause (a relative clause with an extracted subject), and in our data set describes a theater; whil"
J16-4004,E14-1025,1,0.940471,"which projects the tensor product of u and v onto the space of p (which is assumed to be the same space as that containing u and v). Under similar simplifying assumptions to the additive model, the multiplicative model has the form: p=u v (8) where represents elementwise multiplication, that is, pi = ui · vi , where pi is the ith coefficient of p. In this simpler model, C simply picks out the diagonal in the u ⊗ v tensor. One obvious question to consider is how well such simple operators scale when applied to phrases or sentences longer than a few words. Polajnar, Rimell, and Clark (2014) and Polajnar and Clark (2014) suggest that elementwise multiplication performs badly in this respect, with the quality of the composed representation degrading quickly as the number of composition operations increases. Vector addition is more stable, but Polajnar, Rimell, and Clark suggest that the quality of the composed representation degrades after around 10 binary composition operations, even with addition. M&L also introduce a method that aims to interpret one of the constituent vectors as an operator, despite its lying in the same semantic space as the argument. A dilation 673 Computational Linguistics Volume 42, Nu"
J16-4004,D14-1111,1,0.858615,"Missing"
J16-4004,polajnar-etal-2014-evaluation,1,0.903613,"Missing"
J16-4004,W15-2701,1,0.862359,"Missing"
J16-4004,I11-1024,0,0.0685427,"Missing"
J16-4004,D09-1085,1,0.888354,"Missing"
J16-4004,J98-1004,0,0.0303909,"providing a qualitative analysis highlighting some of the more common errors. Our hope is that the competitive results presented here, in which the best systems are on average ranking one out of every two properties correctly for a given term, will inspire new approaches to the RELPRON ranking task and other tasks based on linguistically interesting constructions. 1. Introduction The field of compositional distributional semantics (Mitchell and Lapata 2008; Clark, Coecke, and Sadrzadeh 2008; Baroni, Bernardi, and Zamparelli 2014) integrates ¨ distributional semantic representations of words (Schutze 1998; Turney and Pantel 2010; Clark 2015) with formal methods for composing word representations into larger phrases and sentences (Montague 1970; Dowty, Wall, and Peters 1981). In recent years a number of composition methods have been proposed, including simple arithmetic operations on distributional word vectors (Mitchell and Lapata 2008, 2010), multi-linear operations involving higher-order representations of argument-taking words such as verbs and adjectives (Baroni and Zamparelli 2010; Coecke, Sadrzadeh, and Clark 2010), and composition of distributed word vectors learned with neural networks"
J16-4004,D12-1110,0,0.0934855,"Missing"
J16-4004,D13-1170,0,0.00668451,"WordNet hyponyms with sufficient corpus frequency, according to the criteria in Section 3.2. 671 Computational Linguistics Volume 42, Number 4 Table 3 Total number of terms and properties by head noun in RELPRON. Test Set Head noun Terms Props Development Set Head noun Terms Props phenomenon activity material room vehicle mammal woman scientist 10 10 10 10 10 10 5 8 80 83 82 80 79 70 37 58 quality organization device building document person player 10 10 10 10 10 10 5 81 99 76 69 69 86 38 TOTAL 73 569 TOTAL 65 518 specifically designed for composition, such as Socher, Manning, and Ng (2010), Socher et al. (2013), and Hermann, Grefenstette, and Blunsom (2013). In this section, we are agnostic about how the vector representations are built. Many of the composition techniques we describe can be equally applied to “classical” distributional vectors built using count methods (Turney and Pantel 2010; Clark 2015) and vectors built using neural embedding techniques. It may be that some composition techniques work better with one type of vector or the other—for example, there is evidence that elementwise multiplication performs better with count methods—although research comparing the two vector-building meth"
J16-4004,W11-1301,0,0.0554801,"Missing"
J16-4004,J15-1010,0,0.180285,"Missing"
J16-4004,2014.lilt-9.5,0,\N,Missing
K15-1035,D10-1115,0,0.199086,"Missing"
K15-1035,D12-1110,0,0.138463,"ectors (Mitchell and Lapata, 2008). In these relatively simple methods, the composition function does not typically depend on its arguments or their syntactic role in the sentence. An alternative which makes more use of grammatical structure is the recursive neural network approach of Socher et al. (2010). Constituent vectors in a phrase are combined using a matrix and non-linearity, with the resulting vector living in the same vector space as the inputs. The matrices can be parameterised by the syntactic type of the combining words or phrases (Socher et al., 2013; Hermann and Blunsom, 2013). Socher et al. (2012) extend this idea by representing the meanings of words and phrases as both a vector and a matrix, introducing a form of lexicalisation into the model. 1 Baroni et al. (2014) have developed a similar approach. 327 Proceedings of the 19th Conference on Computational Language Learning, pages 327–331, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics PICK THE ONE GREEN UNRIPE RIPE X APPLE = FROM APPLE SMALL TASTED = UNRIPE APPLE THE VERY TREE SOUR CICERO SAYS SEXTANT BRAILLE DEFER OMEN Figure 1: Learning the vector for apple in the context pick one ripe apple from"
K15-1035,P14-1023,0,0.112867,"Missing"
K15-1035,J07-4004,1,0.728194,"ilarity measure described in Section 2.3. Table 1 shows that the noun vectors we use are of a high quality, performing comparably to the S KIP G RAM noun vectors on the noun-noun similarity data. Table 2 shows our T B S G adjective matrices, plus new similarity measure, to also perform comparably to the S KIP G RAM adjective vectors on the adjective-adjective similarity data. matsimpA, Bq “ median vecsimpAn, Bnq, nPN (3) where the median is taken over the set of cluster centroids N .2 Evaluation The model is trained on a dump of the English Wikipedia, automatically parsed with the C&C parser (Clark and Curran, 2007). The corpus contains around 200 million noun examples, and 30 million adjective-noun examples. For every context word in the corpus, 5 negative words are sampled from the unigram distribution. Subsampling is used to decrease the number of frequent words (Mikolov et al., 2013). We train 100-dimensional noun vectors and 100ˆ100-dimensional adjective matrices. 3.1 0.776 0.769 Table 1: Spearman rank correlation on noun similarity task. n¨m . }n} }m} Based on tests using a development set, we found that using cosine to measure the similarity of adjective matrices leads to no correlation with golds"
K15-1035,W10-2805,0,0.0208603,"by their syntactic type. Coecke et al. (2011) achieve this by treating relational words such as verbs and adjectives as functions in the semantic space. The functions are assumed to be multilinear maps, and are therefore realised as tensors, with composition being achieved through tensor contraction.1 While the framework specifies the “shape” or semantic type of these tensors, it makes no assumption about how the values of these tensors should be interpreted (nor how they can be learned). A proposal for the case of adjective-noun combinations is given by Baroni and Zamparelli (2010) (and also Guevara (2010)). Their model represents adjectives as matrices over noun space, trained via linear regression to approximate the “holistic” adjective-noun vectors from the corpus. In this paper we propose a new solution to the problem of learning adjective meaning representations. The model is an implementation of the tensor framework of Coecke et al. (2011), here applied to adjective-noun combinations as a starting point. Like Baroni and Zamparelli (2010), our model also learn nouns as vectors and adjectives as matrices, but uses a skip-gram approach with negative sampling (Mikolov et al., 2013), extended"
K15-1035,P13-1088,0,0.0183279,"to combine the constituent vectors (Mitchell and Lapata, 2008). In these relatively simple methods, the composition function does not typically depend on its arguments or their syntactic role in the sentence. An alternative which makes more use of grammatical structure is the recursive neural network approach of Socher et al. (2010). Constituent vectors in a phrase are combined using a matrix and non-linearity, with the resulting vector living in the same vector space as the inputs. The matrices can be parameterised by the syntactic type of the combining words or phrases (Socher et al., 2013; Hermann and Blunsom, 2013). Socher et al. (2012) extend this idea by representing the meanings of words and phrases as both a vector and a matrix, introducing a form of lexicalisation into the model. 1 Baroni et al. (2014) have developed a similar approach. 327 Proceedings of the 19th Conference on Computational Language Learning, pages 327–331, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics PICK THE ONE GREEN UNRIPE RIPE X APPLE = FROM APPLE SMALL TASTED = UNRIPE APPLE THE VERY TREE SOUR CICERO SAYS SEXTANT BRAILLE DEFER OMEN Figure 1: Learning the vector for apple in the context pi"
K15-1035,W14-1406,1,0.882201,"Missing"
K15-1035,P13-1045,0,\N,Missing
K15-1035,P08-1028,0,\N,Missing
K15-1035,2014.lilt-9.5,0,\N,Missing
K15-1035,W11-1301,0,\N,Missing
N01-1013,W99-0631,1,\N,Missing
N01-1013,J90-1003,0,\N,Missing
N01-1013,A00-2034,0,\N,Missing
N01-1013,J98-2002,0,\N,Missing
N01-1013,W97-0808,0,\N,Missing
N01-1013,C96-1003,0,\N,Missing
N01-1013,E95-1016,0,\N,Missing
N01-1013,W00-1320,0,\N,Missing
N01-1013,C00-1029,1,\N,Missing
N01-1013,C00-1028,0,\N,Missing
N01-1013,N01-1011,0,\N,Missing
N01-1013,A97-1052,0,\N,Missing
N01-1013,P99-1014,0,\N,Missing
N01-1013,W01-0703,0,\N,Missing
N01-1013,P93-1024,0,\N,Missing
N03-1031,A00-2018,0,0.0290607,"s are iteratively re-trained on each other’s output; and a semi-supervised approach, corrected co-training, in which a human corrects each parser’s output before adding it to the training data. The selection of labeled training examples is an integral part of both frameworks. We propose several selection methods based on the criteria of minimizing errors in the data and maximizing training utility. We show that incorporating the utility criterion into the selection method results in better parsers for both frameworks. 1 Introduction Current state-of-the-art statistical parsers (Collins, 1999; Charniak, 2000) are trained on large annotated corpora such as the Penn Treebank (Marcus et al., 1993). However, the production of such corpora is expensive and labor-intensive. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human"
N03-1031,P96-1042,0,0.012772,"Missing"
N03-1031,N01-1023,1,0.558581,"e. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human to label1 . Sentences with high training utility are those most likely to improve the parser. The other approach, and the focus of this paper, is co-training (Sarkar, 2001), a mostlyunsupervised algorithm that replaces the human by having two (or more) parsers label training examples for each other. The goal is for both parsers to improve by bootstrapping off each other’s strengths. Because the parsers may label examples incorrectly, only a subset of their output, chosen by some selection mechanism, is used in order to minimize errors. The choice of selection method significantly affects the quality of the resulting parsers. We investigate a novel approach of selecting training examples for co-training parsers by incorporating the idea of maximizing training uti"
N03-1031,E03-1008,1,0.765705,"Missing"
N03-1031,W00-1306,1,0.314843,"ility criterion into the selection method results in better parsers for both frameworks. 1 Introduction Current state-of-the-art statistical parsers (Collins, 1999; Charniak, 2000) are trained on large annotated corpora such as the Penn Treebank (Marcus et al., 1993). However, the production of such corpora is expensive and labor-intensive. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human to label1 . Sentences with high training utility are those most likely to improve the parser. The other approach, and the focus of this paper, is co-training (Sarkar, 2001), a mostlyunsupervised algorithm that replaces the human by having two (or more) parsers label training examples for each other. The goal is for both parsers to improve by bootstrapping off each other’s strengths. Because the parsers may label examples inc"
N03-1031,P02-1016,0,0.00757357,"rion into the selection method results in better parsers for both frameworks. 1 Introduction Current state-of-the-art statistical parsers (Collins, 1999; Charniak, 2000) are trained on large annotated corpora such as the Penn Treebank (Marcus et al., 1993). However, the production of such corpora is expensive and labor-intensive. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human to label1 . Sentences with high training utility are those most likely to improve the parser. The other approach, and the focus of this paper, is co-training (Sarkar, 2001), a mostlyunsupervised algorithm that replaces the human by having two (or more) parsers label training examples for each other. The goal is for both parsers to improve by bootstrapping off each other’s strengths. Because the parsers may label examples incorrectly, only a sub"
N03-1031,J93-2004,0,0.0255646,"h, corrected co-training, in which a human corrects each parser’s output before adding it to the training data. The selection of labeled training examples is an integral part of both frameworks. We propose several selection methods based on the criteria of minimizing errors in the data and maximizing training utility. We show that incorporating the utility criterion into the selection method results in better parsers for both frameworks. 1 Introduction Current state-of-the-art statistical parsers (Collins, 1999; Charniak, 2000) are trained on large annotated corpora such as the Penn Treebank (Marcus et al., 1993). However, the production of such corpora is expensive and labor-intensive. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human to label1 . Sentences with high training utility are those most likely to improve the"
N03-1031,P00-1016,0,0.0133509,"Missing"
N03-1031,W01-0501,0,0.0173865,"have different goals, their selection methods focus on different criteria: co-training typically favors selecting accurately labeled examples, while sample selection typically favors selecting examples with high training utility, which often are not sentences that the parsers already label accurately. In this work, we investigate selection methods for co-training that explore the trade-off between maximizing training utility and minimizing errors. Empirical studies were conducted to compare selection methods under both co-training and a semi-supervised framework called corrected co-training (Pierce and Cardie, 2001), in which the selected examples are manually checked and corrected before being added to the 1 In the context of training parsers, a labeled example is a sentence with its parse tree. Throughout this paper, we use the term “label” and “parse” interchangeably. training data. For co-training, we show that the benefit of selecting examples with high training utility can offset the additional errors they contain. For corrected co-training, we show that selecting examples with high training utility reduces the number of sentences the human annotator has to check. For both frameworks, we show that"
N03-1031,J03-4003,0,\N,Missing
N03-1031,P02-1046,0,\N,Missing
N06-1019,P04-1041,0,0.0413665,"Missing"
N06-1019,C02-1013,0,0.0613012,"Missing"
N06-1019,P00-1058,0,0.0604462,"Missing"
N06-1019,W03-1013,1,0.882703,"tions leading to each gold-standard dependency structure, and the second is over all derivations for each sentence in the training data. The estimation process attempts to make the expectations in (5) equal (ignoring the Gaussian prior term). Another way to think of the estimation process is that it attempts to put as much mass as possible on the derivations leading to the gold-standard structures (Riezler et al., 2002). Calculation of the feature expectations requires summing over all derivations for a sentence, and summing over all derivations leading to a goldstandard dependency structure. Clark and Curran (2003) shows how the sum over the complete derivation space can be performed efficiently using a packed chart and the inside-outside algorithm, and Clark and Curran (2004b) extends this method to sum over all derivations leading to a gold-standard dependency structure. 146 The partial data we use for training the dependency model is derived from CCG lexical category sequences only. Figure 1 gives an example sentence adapted from CCGbank (Hockenmaier, 2003) together with its lexical category sequence. Note that, although the attachment of the prepositional phrase to the noun phrase is not explicitly"
N06-1019,C04-1041,1,0.249881,"parsers for new domains and languages. Previous work has attempted parser adaptation without relying on treebank data from the new domain (Steedman et al., 2003; Lease and Charniak, 2005). In this paper we propose the use of annotated data in the new domain, but only partially annotated data, which reduces the annotation effort required (Hwa, 1999). We develop a parsing model which can be trained using partial data, by exploiting the properties of lexicalized grammar formalisms. The formalism we use is Combinatory Categorial Grammar (Steedman, 2000), together with a parsing model described in Clark and Curran (2004b) which we adapt for use with partial data. Parsing with Combinatory Categorial Grammar (CCG) takes place in two stages: first, CCG lexical categories are assigned to the words in the sentence, and then the categories are combined by the parser (Clark and Curran, 2004a). The lexical categories can be thought of as detailed part of speech tags and typically express subcategorization information. We exploit the fact that CCG lexical categories contain a lot of syntactic information, and can therefore be used for training a full parser, even though attachment information is not explicitly repres"
N06-1019,P04-1014,1,0.906989,"Missing"
N06-1019,W04-3215,1,0.878626,"he parser (Clark and Curran, 2004a). The lexical categories can be thought of as detailed part of speech tags and typically express subcategorization information. We exploit the fact that CCG lexical categories contain a lot of syntactic information, and can therefore be used for training a full parser, even though attachment information is not explicitly represented in a category sequence. Our partial training regime only requires sentences to be annotated with lexical categories, rather than full parse trees; therefore the data can be produced much more quickly for a new domain or language (Clark et al., 2004). The partial training method uses the log-linear dependency model described in Clark and Curran (2004b), which uses sets of predicate-argument de144 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 144–151, c New York, June 2006. 2006 Association for Computational Linguistics pendencies, rather than derivations, for training. Our novel idea is that, since there is so much information in the lexical category sequence, most of the correct dependencies can be easily inferred from the categories alone. More specifically, for a given sentence"
N06-1019,W97-1505,0,0.0545329,"Missing"
N06-1019,P02-1043,0,0.0924825,"Missing"
N06-1019,P99-1010,0,0.0471612,". The labour-intensive nature of the treebank development process, which can take many James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au years, creates a significant barrier for the development of parsers for new domains and languages. Previous work has attempted parser adaptation without relying on treebank data from the new domain (Steedman et al., 2003; Lease and Charniak, 2005). In this paper we propose the use of annotated data in the new domain, but only partially annotated data, which reduces the annotation effort required (Hwa, 1999). We develop a parsing model which can be trained using partial data, by exploiting the properties of lexicalized grammar formalisms. The formalism we use is Combinatory Categorial Grammar (Steedman, 2000), together with a parsing model described in Clark and Curran (2004b) which we adapt for use with partial data. Parsing with Combinatory Categorial Grammar (CCG) takes place in two stages: first, CCG lexical categories are assigned to the words in the sentence, and then the categories are combined by the parser (Clark and Curran, 2004a). The lexical categories can be thought of as detailed pa"
N06-1019,I05-1006,0,0.064504,"Clark and Curran, 2004b; Hockenmaier, 2003), HPSG (Miyao et al., 2004) and LFG (Riezler et al., 2002; Cahill et al., 2004), often use training data derived from the Penn Treebank. The labour-intensive nature of the treebank development process, which can take many James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au years, creates a significant barrier for the development of parsers for new domains and languages. Previous work has attempted parser adaptation without relying on treebank data from the new domain (Steedman et al., 2003; Lease and Charniak, 2005). In this paper we propose the use of annotated data in the new domain, but only partially annotated data, which reduces the annotation effort required (Hwa, 1999). We develop a parsing model which can be trained using partial data, by exploiting the properties of lexicalized grammar formalisms. The formalism we use is Combinatory Categorial Grammar (Steedman, 2000), together with a parsing model described in Clark and Curran (2004b) which we adapt for use with partial data. Parsing with Combinatory Categorial Grammar (CCG) takes place in two stages: first, CCG lexical categories are assigned"
N06-1019,W02-2018,0,0.0210346,"ing a log-linear form: P (ω|S) = P 1 λ.f (ω) e ZS (3) where λ.f (ω) = i λi fi (ω). The function fi is the integer-valued frequency function of the ith feature; λi is the weight of the ith feature; and ZS is a normalising constant. Clark and Curran (2004b) describes the training procedure for the dependency model, which uses a discriminative estimation method by maximising the conditional likelihood of the model given the data (Riezler et al., 2002). The optimisation of the objective function is performed using the limited-memory BFGS numerical optimisation algorithm (Nocedal and Wright, 1999; Malouf, 2002), which requires calculation of the objective function and the gradient of the objective function at each iteration. The objective function is defined below, where L(Λ) is the likelihood and G(Λ) is a Gaussian prior term for smoothing. He anticipates growth NP (S [dcl ]NP )/NP NP for the auto maker (NP NP )/NP NP [nb]/N N /N N Figure 1: Example sentence with CCG lexical categories 3 Partial Training L0 (Λ) = L(Λ) − G(Λ) = m X j=1 − X log m X (4) eλ.f (d,πj ) d∈∆(πj ) log j=1 n X λ2i eλ.f (ω) − 2σ 2 i=1 ω∈ρ(S ) X j S1 , . . . , Sm are the sentences in the training data; π1 , . . . , πm are th"
N06-1019,P92-1017,0,0.265797,"ncy F-score than the fulldata model, despite the fact that the partial data does not contain any explicit attachment information. 2 The CCG Parsing Model Clark and Curran (2004b) describes two log-linear parsing models for CCG: a normal-form derivation model and a dependency model. In this paper we use the dependency model, which requires sets of predicate-argument dependencies for training.1 1 Hockenmaier and Steedman (2002) describe a generative model of normal-form derivations; one possibility for training this model on partial data, which has not been explored, is to use the EM algorithm (Pereira and Schabes, 1992). 145 The predicate-argument dependencies are represented as 5-tuples: hhf , f, s, ha , li, where hf is the lexical item of the lexical category expressing the dependency relation; f is the lexical category; s is the argument slot; ha is the head word of the argument; and l encodes whether the dependency is non-local. For example, the dependency encoding company as the object of bought (as in IBM bought the company) is represented as follows: hbought2 , (S NP1 )/NP2 , 2, company4 , −i (1) CCG dependency structures are sets of predicateargument dependencies. We define the probability of a depe"
N06-1019,P02-1035,0,0.358514,": P (π|S) = X P (d, π|S) (2) d∈∆(π) where ∆(π) is the set of derivations which lead to π. The probability of a hd, πi pair, ω, conditional on a sentence S, is defined using a log-linear form: P (ω|S) = P 1 λ.f (ω) e ZS (3) where λ.f (ω) = i λi fi (ω). The function fi is the integer-valued frequency function of the ith feature; λi is the weight of the ith feature; and ZS is a normalising constant. Clark and Curran (2004b) describes the training procedure for the dependency model, which uses a discriminative estimation method by maximising the conditional likelihood of the model given the data (Riezler et al., 2002). The optimisation of the objective function is performed using the limited-memory BFGS numerical optimisation algorithm (Nocedal and Wright, 1999; Malouf, 2002), which requires calculation of the objective function and the gradient of the objective function at each iteration. The objective function is defined below, where L(Λ) is the likelihood and G(Λ) is a Gaussian prior term for smoothing. He anticipates growth NP (S [dcl ]NP )/NP NP for the auto maker (NP NP )/NP NP [nb]/N N /N N Figure 1: Example sentence with CCG lexical categories 3 Partial Training L0 (Λ) = L(Λ) − G(Λ) = m X j=1 − X"
N06-1019,E03-1008,1,0.846865,"G (Chiang, 2000), CCG (Clark and Curran, 2004b; Hockenmaier, 2003), HPSG (Miyao et al., 2004) and LFG (Riezler et al., 2002; Cahill et al., 2004), often use training data derived from the Penn Treebank. The labour-intensive nature of the treebank development process, which can take many James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au years, creates a significant barrier for the development of parsers for new domains and languages. Previous work has attempted parser adaptation without relying on treebank data from the new domain (Steedman et al., 2003; Lease and Charniak, 2005). In this paper we propose the use of annotated data in the new domain, but only partially annotated data, which reduces the annotation effort required (Hwa, 1999). We develop a parsing model which can be trained using partial data, by exploiting the properties of lexicalized grammar formalisms. The formalism we use is Combinatory Categorial Grammar (Steedman, 2000), together with a parsing model described in Clark and Curran (2004b) which we adapt for use with partial data. Parsing with Combinatory Categorial Grammar (CCG) takes place in two stages: first, CCG lexic"
N10-1084,P01-1008,0,0.036709,". Each paraphrase also has a probability, based on a statistical machine translation model, but we do not use that feature here. The examples show that, while some of the paraphrases are of a high quality, some are not. For example, differences is unlikely to be a suitable paraphrase for a number of people in any context. Moreover, there are some hphrase, paraphrasei pairs which are only suitable in particular contexts. For example, year end is an unsuitable paraphrase for the end of this year in the sentence The chart compares the gold price at the end of last year with the end of this year. Barzilay and McKeown (2001) also note that the applicability of paraphrases is strongly influenced by context. Section 4 describes our method for determining if a paraphrase is suitable in a given context. 3.2 Google N-gram Data The Google n-gram data was collected by Google Research for statistical language modelling, and has been used for many tasks such as lexical disambiguation (Bergsma et al., 2009), and contains English n-grams and their observed frequency counts, for counts of at least 40. The striking feature of 594 the n-gram corpus is the large number of n-grams and the size of the counts, since the counts wer"
N10-1084,D08-1021,0,0.121551,"embed hidden bits in text. Section 2 describes some of the previous transformations used in Linguistic Steganography. Note that we are concerned with transformations which are 2 The message may have been encrypted initially also, as in the figure, but this is not important in this paper; the key point is that the hidden message is a sequence of bits. 592 linguistic in nature, rather than dealing with superficial properties of the text, e.g. the amount of white space between words (Por et al., 2008). Our proposed method is based on the automatically acquired paraphrase dictionary described in Callison-Burch (2008), in which the application of paraphrases from the dictionary encodes secret bits. One advantage of the dictionary is that it has wide coverage, being automatically extracted; however, a disadvantage is that it contains many paraphrases which are either inappropriate, or only appropriate in certain contexts. Since we require any changes to be imperceptible to a human observer, it is crucial to our system that any uses of paraphrasing are grammatical and retain the meaning of the original cover text. In order to test the grammaticality and meaning preserving nature of a paraphrase, we employ a"
N10-1084,J07-4004,1,0.375826,"account, namely W32 . However, If the paraphrase P 0 fails the current (n, C) check the checking procedure will terminate and report that the paraphrase fails. In contrast, if the paraphrase passes all the (n, C) checks where C = 1 to maxC, the procedure determines the paraphrase as acceptable. What is happening is that an ngram window is effectively being shifted across the paraphrase boundary to include different amounts of context and paraphrase. 4.2 Syntactic Filter In order to improve the grammaticality checking, we use a parser as an addition to the basic Google ngram method. We use the Clark and Curran (2007) CCG parser to analyse the sentence before and after paraphrasing. Combinatory Categorial Grammar (CCG) is a lexicalised grammar formalism, in which CCG lexical categories — typically expressing subcategorisation information — are assigned to each word in a sentence. The grammatical check works by checking if the words in the sentence outside of the phrase and paraphrase receive the same lexical categories before and after paraphrasing. If there is any change in lexical category assignment to these words then the paraphrase is judged ungrammatical. Hence the grammar check is at the word, rathe"
N10-1084,J93-2004,0,0.0504636,"Missing"
N16-1025,D11-1031,1,0.961006,"f the parsing process during inference (Chen and Manning, 2014; Dyer et al., 2015). In the beam search setting, Zhou et al. (2015) show that sentence-level likelihood, together with contrastive learning (Hinton, 2002), can be used to derive a global model which incorporates beam In this paper, we present a global neural network parsing model, optimized for a task-specific loss based on expected F-measure. The model naturally incorporates beam search during training, and is globally optimized, to learn shift-reduce action sequences that lead to parses with high expected Fscores. In contrast to Auli and Lopez (2011), who optimize a CCG parser for F-measure via softmaxmargin (Gimpel and Smith, 2010), we directly optimize an expected F-measure objective, derivable from only a set of shift-reduce action sequences and sentence-level F-scores. More generally, our method can be seen as an alternative approach for training a neural beam search parsing model (Watanabe and Sumita, 2015; Weiss et al., 2015; Zhou et al., 2015), combining the benefits of global learning and taskspecific optimization. We also introduce a simple recurrent neural network (RNN) model to shift-reduce parsing on which the greedy baseline"
N16-1025,D14-1132,1,0.719403,"11) optimized the C & C parser for F-measure. However, they used the softmax-margin (Gimpel and Smith, 2010) objective, which required decomposing precision and recall statistics over parse forests. Instead, we directly optimize for an F-measure loss. In MT, task-specific optimization has also received much attention (e.g., see Och (2003)). Closely related to our work, Gao and He (2013) proposed training a Markov random field translation model as an additional component in a log-linear phrase-based translation system using a k-best list based expected BLEU objective; using the same objective, Auli et al. (2014) and Auli and Gao (2014) trained a large scale phrase-based reordering model and a RNN language model respectively, all as additional components within a log-linear translation model. In contrast, our RNN parsing model is trained in an end-toend fashion with an expected F-measure loss and all parameters of the model are optimized using backpropagation and SGD. Parsing with RNNs. A line of work is devoted to parsing with RNN models, including using RNNs (Miikkulainen, 1996; Mayberry and Miikkulainen, 1999; Legrand and Collobert, 2015; Watanabe and Sumita, 2015) and LSTM (Hochreiter and Schmidhu"
N16-1025,D15-1041,0,0.0238985,"e scale phrase-based reordering model and a RNN language model respectively, all as additional components within a log-linear translation model. In contrast, our RNN parsing model is trained in an end-toend fashion with an expected F-measure loss and all parameters of the model are optimized using backpropagation and SGD. Parsing with RNNs. A line of work is devoted to parsing with RNN models, including using RNNs (Miikkulainen, 1996; Mayberry and Miikkulainen, 1999; Legrand and Collobert, 2015; Watanabe and Sumita, 2015) and LSTM (Hochreiter and Schmidhuber, 1997) RNNs (Vinyals et al., 2015; Ballesteros et al., 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016). Legrand and Collobert (2015) used RNNs to learn conditional distributions over syntactic rules; Vinyals et al. (2015) explored sequenceto-sequence learning (Sutskever et al., 2014) for parsing; Ballesteros et al. (2015) utilized characterlevel representations and Kiperwasser and Goldberg (2016) built an easy-first dependency parser using tree-structured compositional LSTMs. However, all these parsers use greedy search and are trained using the maximum likelihood criterion (except Kiperwasser and Goldberg (2016), who used a margin-based obje"
N16-1025,J99-2004,0,0.0559903,"op element and replaces it with a unary subtree rooted in x. The Transition System The transition system we use in this work is based on the CCG transition system of Zhang and Clark (2011). We denote parse items as (j, δ, β, ∆)3 , where δ is the stack (with top element δ|s0 ), β is the queue (with top element xwj |β), j is the positional index of the word at the front of the queue, and ∆ is the set of CCG dependencies realized for the input consumed so far (needed to calculate the expected F-score). We also assume a set of lexical categories has been assigned to each word using a supertagger (Bangalore and Joshi, 1999; Clark and Curran, 2004). The transition system is specified using three action types: • S HIFT (sh) removes one of the lexical categories xwj of the front word wj in the queue, and pushes it onto the stack; and removes wj from the queue. 3 We partly adopt standard notations from dependency parsing (Nivre, 2008). 214 The deduction system (Fig. 2) of our shift-reduce parser follows from the transition system.4 Each parse item is associated with a step indicator ω, which denotes the number of actions used to build it. Given a sentence of length n, a full derivation requires 2n − 1 + µ steps to"
N16-1025,P05-1022,0,0.0475868,", directed dependencies, denoted as ∆, associated with its parse item. (We assume F1 over labeled, directed dependencies is also the parser evaluation metric.) 3. We compute the negative expected F1 objective (-xF1, defined below) for xn using the scores obtained in the above step and minimize this objective using SGD (maximizing the expected F1 for xn ). These three steps repeat for other sentences in the training data, updating θ after processing each sentence, and training iterates in epochs until convergence. We note that the above process is different from parse reranking (Collins, 2000; Charniak and Johnson, 2005), in which Λ(xn ) would stay the same for each xn in the training data across all epochs, and a reranker is trained on all fixed Λ(xn ); whereas the xF1 training procedure is on-line learning with parameters updated after processing each sentence and each Λ(xn ) is generated with a new θ. More formally, we define the loss J(θ), which incorporates all action scores in each action sequence, and all action sequences in Λ(xn ), for each xn as J(θ) = −xF1(θ) X =− p(yi |θ)F1(∆yi , ∆G xn ), (1) yi ∈Λ(xn ) where F1(∆yi , ∆G xn ) is the sentence level F1 of the parse derived by yi , with respect to the"
N16-1025,D14-1082,0,0.461101,"ased. Compared with feed-forward networks, RNNs have the potential to capture and use an unbounded history, and they have been used to learn explicit representations for parser states as well as actions 210 Proceedings of NAACL-HLT 2016, pages 210–220, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics performed on the stack and queue in shift-reduce parsers (Dyer et al., 2015; Watanabe and Sumita, 2015), following Miikkulainen (1996) and Mayberry and Miikkulainen (1999). In comparison, our model is a natural extension of the feed-forward architecture in Chen and Manning (2014) using Elman RNNs (Elman, 1990). We apply our models to CCG, and evaluate the resulting parsers on standard CCGBank data (Hockenmaier and Steedman, 2007). More specifically, by combining the global RNN parsing model with a bidirectional RNN CCG supertagger that we have developed (§4) — building on the supertagger of Xu et al. (2015), we obtain accuracies higher than the shift-reduce CCG parsers of Zhang and Clark (2011) and Xu et al. (2014). Finally, although we choose to focus on shift-reduce parsing for CCG, we expect the methods to generalize to other shift-reduce parsers. 2 RNN Models In t"
N16-1025,C04-1041,1,0.796621,"Missing"
N16-1025,J07-4004,1,0.53161,"lication. Given CCGBank (Hockenmaier and Steedman, 2007), there are two approaches to extract a grammar from this data. The first is to treat all CCG derivations as phrase-structure trees, and a binary, context-free “cover” grammar, consisting of all CCG rule instances in the treebank, is extracted from local trees in all the derivations (Fowler and Penn, 2010; Zhang and Clark, 2011). In contrast, one can extract the lexicon from the treebank and define only the rule schemas, without explicitly enumerating any rule instances (Hockenmaier, 2003). This is the approach taken in the C & C parser (Clark and Curran, 2007) and the one we use here. Moreover, following Zhang and Clark (2011), our CCG parsing model is also a normal-form model, which models action sequences of normal-form derivations in CCGBank. 3.2 ω : (j, δ, xwj |β, ∆) ω + 1 : (j + 1, δ|xwj , β, ∆) (sh; 0 ≤ j &lt; n) ω : (j, δ|s1 |s0 , β, ∆) ω + 1 : (j, δ|x, β, ∆ ∪ hxi)) (re; s1 s0 → x) ω : (j, δ|s0 , β, ∆) ω + 1 : (j, δ|x, β, ∆) (un; s0 → x) Figure 2: The shift-reduce deduction system. • R EDUCE (re) combines the top two subtrees s0 and s1 on the stack using a CCG rule (s1 s0 → x) and replaces them with a subtree rooted in x. It also appends the se"
N16-1025,P04-1015,0,0.13841,"15) utilized characterlevel representations and Kiperwasser and Goldberg (2016) built an easy-first dependency parser using tree-structured compositional LSTMs. However, all these parsers use greedy search and are trained using the maximum likelihood criterion (except Kiperwasser and Goldberg (2016), who used a margin-based objective). For learning global models, Watanabe and Sumita (2015) used a marginbased objective, which was not optimized for the evaluation metric; although not using RNNs, Weiss et al. (2015) proposed a method using the averaged perceptron with beam search (Collins, 2002; Collins and Roark, 2004; Zhang and Clark, 2008), which required fixing the neural network representations, and 218 thus their model parameters were not learned using end-to-end backpropagation. Finally, a number of recent work (Bengio et al., 2015; Vaswani and Sagae, 2016) explored training neural network models for parsing and other tasks such that the network learns from the oracle as well as its own predictions, and are hence more robust to search errors during inference. In principle, these techniques are largely orthogonal to both global learning and task-based optimization, and we would expect further accuracy"
N16-1025,W02-1001,0,0.0353133,"eros et al. (2015) utilized characterlevel representations and Kiperwasser and Goldberg (2016) built an easy-first dependency parser using tree-structured compositional LSTMs. However, all these parsers use greedy search and are trained using the maximum likelihood criterion (except Kiperwasser and Goldberg (2016), who used a margin-based objective). For learning global models, Watanabe and Sumita (2015) used a marginbased objective, which was not optimized for the evaluation metric; although not using RNNs, Weiss et al. (2015) proposed a method using the averaged perceptron with beam search (Collins, 2002; Collins and Roark, 2004; Zhang and Clark, 2008), which required fixing the neural network representations, and 218 thus their model parameters were not learned using end-to-end backpropagation. Finally, a number of recent work (Bengio et al., 2015; Vaswani and Sagae, 2016) explored training neural network models for parsing and other tasks such that the network learns from the oracle as well as its own predictions, and are hence more robust to search errors during inference. In principle, these techniques are largely orthogonal to both global learning and task-based optimization, and we woul"
N16-1025,P14-1129,0,0.00697777,"Missing"
N16-1025,P15-1033,0,0.179087,"combining distributed representations and neural network models (Chen and Manning, 2014), accurate and efficient shift-reduce parsing models can be obtained with little feature engineering, largely alleviating the feature sparsity problem of linear models. In practice, the most common objective for optimizing neural network shift-reduce parsing models is maximum likelihood. In the greedy search setting, the log-likelihood of each target action is maximized during training, and the most likely action is committed to at each step of the parsing process during inference (Chen and Manning, 2014; Dyer et al., 2015). In the beam search setting, Zhou et al. (2015) show that sentence-level likelihood, together with contrastive learning (Hinton, 2002), can be used to derive a global model which incorporates beam In this paper, we present a global neural network parsing model, optimized for a task-specific loss based on expected F-measure. The model naturally incorporates beam search during training, and is globally optimized, to learn shift-reduce action sequences that lead to parses with high expected Fscores. In contrast to Auli and Lopez (2011), who optimize a CCG parser for F-measure via softmaxmargin ("
N16-1025,P10-1035,0,0.13279,"cat NP /N N /N N /N N /N N NP &gt;B N axiom: 0 : (0, , β, φ) goal: 2n − 1 + µ : (n, δ, , ∆) &gt; &gt; Figure 1: An example CCG derivation. forward rules; for example, N /N N /N → N /N is an instance of forward composition and N /N N → N is an instance of forward application. Given CCGBank (Hockenmaier and Steedman, 2007), there are two approaches to extract a grammar from this data. The first is to treat all CCG derivations as phrase-structure trees, and a binary, context-free “cover” grammar, consisting of all CCG rule instances in the treebank, is extracted from local trees in all the derivations (Fowler and Penn, 2010; Zhang and Clark, 2011). In contrast, one can extract the lexicon from the treebank and define only the rule schemas, without explicitly enumerating any rule instances (Hockenmaier, 2003). This is the approach taken in the C & C parser (Clark and Curran, 2007) and the one we use here. Moreover, following Zhang and Clark (2011), our CCG parsing model is also a normal-form model, which models action sequences of normal-form derivations in CCGBank. 3.2 ω : (j, δ, xwj |β, ∆) ω + 1 : (j + 1, δ|xwj , β, ∆) (sh; 0 ≤ j &lt; n) ω : (j, δ|s1 |s0 , β, ∆) ω + 1 : (j, δ|x, β, ∆ ∪ hxi)) (re; s1 s0 → x) ω : (j"
N16-1025,N13-1048,0,0.0235347,"computation. The speed results for the C & C parser were obtained using the per-compiled C & C binary for Linux available from http://svn.ask.it.usyd.edu.au/trac/ candc/wiki/Download. 10 formance improvements. Auli and Lopez (2011) optimized the C & C parser for F-measure. However, they used the softmax-margin (Gimpel and Smith, 2010) objective, which required decomposing precision and recall statistics over parse forests. Instead, we directly optimize for an F-measure loss. In MT, task-specific optimization has also received much attention (e.g., see Och (2003)). Closely related to our work, Gao and He (2013) proposed training a Markov random field translation model as an additional component in a log-linear phrase-based translation system using a k-best list based expected BLEU objective; using the same objective, Auli et al. (2014) and Auli and Gao (2014) trained a large scale phrase-based reordering model and a RNN language model respectively, all as additional components within a log-linear translation model. In contrast, our RNN parsing model is trained in an end-toend fashion with an expected F-measure loss and all parameters of the model are optimized using backpropagation and SGD. Parsing"
N16-1025,P14-1066,0,0.0261525,"Missing"
N16-1025,N10-1112,0,0.439849,". In the beam search setting, Zhou et al. (2015) show that sentence-level likelihood, together with contrastive learning (Hinton, 2002), can be used to derive a global model which incorporates beam In this paper, we present a global neural network parsing model, optimized for a task-specific loss based on expected F-measure. The model naturally incorporates beam search during training, and is globally optimized, to learn shift-reduce action sequences that lead to parses with high expected Fscores. In contrast to Auli and Lopez (2011), who optimize a CCG parser for F-measure via softmaxmargin (Gimpel and Smith, 2010), we directly optimize an expected F-measure objective, derivable from only a set of shift-reduce action sequences and sentence-level F-scores. More generally, our method can be seen as an alternative approach for training a neural beam search parsing model (Watanabe and Sumita, 2015; Weiss et al., 2015; Zhou et al., 2015), combining the benefits of global learning and taskspecific optimization. We also introduce a simple recurrent neural network (RNN) model to shift-reduce parsing on which the greedy baseline and the global model is based. Compared with feed-forward networks, RNNs have the po"
N16-1025,P13-2111,0,0.0405929,"Missing"
N16-1025,P96-1024,0,0.569337,"4) for reference only, since it uses a more sophisticated dependency, rather than normal-form derivation, model. At test time, we also used the precomputation trick of Devlin et al. (2014) to speed up the RNN models by caching the top 20K word embeddings and all POS embeddings,9 and this made the greedy RNN parser more than 3 times faster than the C & C parser (all speed experiments were measured on a workstation with an Intel Core i7 4.0GHz CPU).10 6 Related Work Optimizing for Task-specific Metrics. Our training objective is largely inspired by task-specific optimization for parsing and MT. Goodman (1996) proposed algorithms for optimizing a parser for various constituent matching criteria, and it was one of the earliest work that we are aware of on optimizing a parser for evaluation metrics. Smith and Eisner (2006) proposed a framework for minimizing expected loss for log-linear models and applied it to dependency parsing by optimizing for labeled attachment scores, although they obtained little per9 8 The C & C parser fails to produce spanning analyses for a very small number of sentences (Clark and Curran, 2007) on both dev and test sets, which is not the case for any of the shiftreduce par"
N16-1025,P12-1031,0,0.0256443,"Missing"
N16-1025,J07-3004,0,0.292569,"icit representations for parser states as well as actions 210 Proceedings of NAACL-HLT 2016, pages 210–220, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics performed on the stack and queue in shift-reduce parsers (Dyer et al., 2015; Watanabe and Sumita, 2015), following Miikkulainen (1996) and Mayberry and Miikkulainen (1999). In comparison, our model is a natural extension of the feed-forward architecture in Chen and Manning (2014) using Elman RNNs (Elman, 1990). We apply our models to CCG, and evaluate the resulting parsers on standard CCGBank data (Hockenmaier and Steedman, 2007). More specifically, by combining the global RNN parsing model with a bidirectional RNN CCG supertagger that we have developed (§4) — building on the supertagger of Xu et al. (2015), we obtain accuracies higher than the shift-reduce CCG parsers of Zhang and Clark (2011) and Xu et al. (2014). Finally, although we choose to focus on shift-reduce parsing for CCG, we expect the methods to generalize to other shift-reduce parsers. 2 RNN Models In this section, we start by describing the baseline model, which is also taken as the pretrained model to train the global model (§2.4). We abstract away fr"
N16-1025,Q16-1032,0,0.0266013,"Missing"
N16-1025,Q14-1032,0,0.0779368,"nd the latter gives schemas which dictate whether two categories can be combined. Given the lexicon and the rules, the syntactic types of complete constituents can be obtained by recursive combination of categories using the rules. More generally, both lexical and non-lexical CCG categories can be either atomic or complex: atomic categories are categories without any slashes, and complex categories are constructed recursively from atomic ones using forward (/) and backward slashes () as two binary operators. As such, all categories can be represented as follows (Vijay-Shanker and Weir, 1993; Kuhlmann and Satta, 2014): x := α|1 z1 |2 z2 . . . |m zm , where m ≥ 0, α is an atomic category, |1 , . . . , |m ∈ {, /} and zi are meta-variables for categories. CCG rules have the following two schematic forms, each a generalized version of functional composition (Vijay-Shanker and Weir, 1993): x/y y|1 z1 . . . |m zm → x|1 z1 . . . |m zm , y|1 z1 . . . |m zm xy → x|1 z1 . . . |m zm . The first schematic form above instantiates into a forward application rule (&gt;) for m = 0, and forward composition rules (&gt;B ) for m &gt; 0. Similarly, the second schematic form, which is symmetric to the first, instantiates into backwar"
N16-1025,C04-1010,0,0.15768,"Missing"
N16-1025,J08-4003,0,0.00498973,"e positional index of the word at the front of the queue, and ∆ is the set of CCG dependencies realized for the input consumed so far (needed to calculate the expected F-score). We also assume a set of lexical categories has been assigned to each word using a supertagger (Bangalore and Joshi, 1999; Clark and Curran, 2004). The transition system is specified using three action types: • S HIFT (sh) removes one of the lexical categories xwj of the front word wj in the queue, and pushes it onto the stack; and removes wj from the queue. 3 We partly adopt standard notations from dependency parsing (Nivre, 2008). 214 The deduction system (Fig. 2) of our shift-reduce parser follows from the transition system.4 Each parse item is associated with a step indicator ω, which denotes the number of actions used to build it. Given a sentence of length n, a full derivation requires 2n − 1 + µ steps to terminate, where µ is the total number of un actions applied. In Zhang and Clark (2011), a finish action is used to indicate termination, which we do not use in our parser: an item finishes when no further action can be taken. Another difference between the transition systems is that Zhang and Clark (2011) omit t"
N16-1025,P03-1021,0,0.125757,"e results. 217 We used b = 8 to do the precomputation. The speed results for the C & C parser were obtained using the per-compiled C & C binary for Linux available from http://svn.ask.it.usyd.edu.au/trac/ candc/wiki/Download. 10 formance improvements. Auli and Lopez (2011) optimized the C & C parser for F-measure. However, they used the softmax-margin (Gimpel and Smith, 2010) objective, which required decomposing precision and recall statistics over parse forests. Instead, we directly optimize for an F-measure loss. In MT, task-specific optimization has also received much attention (e.g., see Och (2003)). Closely related to our work, Gao and He (2013) proposed training a Markov random field translation model as an additional component in a log-linear phrase-based translation system using a k-best list based expected BLEU objective; using the same objective, Auli et al. (2014) and Auli and Gao (2014) trained a large scale phrase-based reordering model and a RNN language model respectively, all as additional components within a log-linear translation model. In contrast, our RNN parsing model is trained in an end-toend fashion with an expected F-measure loss and all parameters of the model are"
N16-1025,W10-1748,0,0.0680122,"Missing"
N16-1025,P06-2089,0,0.0663224,"Missing"
N16-1025,P06-2101,0,0.191119,"NN models by caching the top 20K word embeddings and all POS embeddings,9 and this made the greedy RNN parser more than 3 times faster than the C & C parser (all speed experiments were measured on a workstation with an Intel Core i7 4.0GHz CPU).10 6 Related Work Optimizing for Task-specific Metrics. Our training objective is largely inspired by task-specific optimization for parsing and MT. Goodman (1996) proposed algorithms for optimizing a parser for various constituent matching criteria, and it was one of the earliest work that we are aware of on optimizing a parser for evaluation metrics. Smith and Eisner (2006) proposed a framework for minimizing expected loss for log-linear models and applied it to dependency parsing by optimizing for labeled attachment scores, although they obtained little per9 8 The C & C parser fails to produce spanning analyses for a very small number of sentences (Clark and Curran, 2007) on both dev and test sets, which is not the case for any of the shiftreduce parsers; and for brevity, we omit C & C coverage results. 217 We used b = 8 to do the precomputation. The speed results for the C & C parser were obtained using the per-compiled C & C binary for Linux available from ht"
N16-1025,P10-1040,0,0.0103662,"agger by all RNN parsing models for both training and testing. F-score over directed, labeled CCG predicate-argument dependencies was used as the parser evaluation metric, obtained using the script from C & C. Hyperparameters. For the BRNN supertagging model, we used identical hyperparameter settings as in Xu et al. (2015). For all RNN parsing models, the weights were uniformly initialized using the interval [−2.0, 2.0], and scaled by their fanin (Bengio, 2012); the hidden layer size was 220, and 50-dimensional embeddings were used for all feature types and scaled Turian embeddings were used (Turian et al., 2010) for word embeddings. We also pretrained CCG lexcial category and POS embeddings by using the GENSIM word2vec implementation.7 The data used for this was obtained by parsing a Wikipedia dump using the C & C parser and concatenating the output with CCGBank Sections 02-21. Embeddings for unknown words and CCG categories outside of the lexical category set were uniformly initialized ([−2.0, 2.0]) without scaling. 6 Training: Sections 02-21; development: Section 00; test Section 23. 7 https://radimrehurek.com/gensim/ Supertagger C & C (gold POS ) C & C (auto POS ) RNN BRNN Dev 92.60 91.50 93.07 93"
N16-1025,Q16-1014,0,0.0402546,"d criterion (except Kiperwasser and Goldberg (2016), who used a margin-based objective). For learning global models, Watanabe and Sumita (2015) used a marginbased objective, which was not optimized for the evaluation metric; although not using RNNs, Weiss et al. (2015) proposed a method using the averaged perceptron with beam search (Collins, 2002; Collins and Roark, 2004; Zhang and Clark, 2008), which required fixing the neural network representations, and 218 thus their model parameters were not learned using end-to-end backpropagation. Finally, a number of recent work (Bengio et al., 2015; Vaswani and Sagae, 2016) explored training neural network models for parsing and other tasks such that the network learns from the oracle as well as its own predictions, and are hence more robust to search errors during inference. In principle, these techniques are largely orthogonal to both global learning and task-based optimization, and we would expect further accuracy gains are possible by combining these techniques in a single model. 7 Conclusion Neural network shift-reduce parsers are often trained by maximizing likelihood, which does not optimize towards the final evaluation metric. In this paper, we addressed"
N16-1025,J93-4002,0,0.105107,"epresenting syntactic types, and the latter gives schemas which dictate whether two categories can be combined. Given the lexicon and the rules, the syntactic types of complete constituents can be obtained by recursive combination of categories using the rules. More generally, both lexical and non-lexical CCG categories can be either atomic or complex: atomic categories are categories without any slashes, and complex categories are constructed recursively from atomic ones using forward (/) and backward slashes () as two binary operators. As such, all categories can be represented as follows (Vijay-Shanker and Weir, 1993; Kuhlmann and Satta, 2014): x := α|1 z1 |2 z2 . . . |m zm , where m ≥ 0, α is an atomic category, |1 , . . . , |m ∈ {, /} and zi are meta-variables for categories. CCG rules have the following two schematic forms, each a generalized version of functional composition (Vijay-Shanker and Weir, 1993): x/y y|1 z1 . . . |m zm → x|1 z1 . . . |m zm , y|1 z1 . . . |m zm xy → x|1 z1 . . . |m zm . The first schematic form above instantiates into a forward application rule (&gt;) for m = 0, and forward composition rules (&gt;B ) for m &gt; 0. Similarly, the second schematic form, which is symmetric to the first"
N16-1025,P15-1113,0,0.210249,"k-specific loss based on expected F-measure. The model naturally incorporates beam search during training, and is globally optimized, to learn shift-reduce action sequences that lead to parses with high expected Fscores. In contrast to Auli and Lopez (2011), who optimize a CCG parser for F-measure via softmaxmargin (Gimpel and Smith, 2010), we directly optimize an expected F-measure objective, derivable from only a set of shift-reduce action sequences and sentence-level F-scores. More generally, our method can be seen as an alternative approach for training a neural beam search parsing model (Watanabe and Sumita, 2015; Weiss et al., 2015; Zhou et al., 2015), combining the benefits of global learning and taskspecific optimization. We also introduce a simple recurrent neural network (RNN) model to shift-reduce parsing on which the greedy baseline and the global model is based. Compared with feed-forward networks, RNNs have the potential to capture and use an unbounded history, and they have been used to learn explicit representations for parser states as well as actions 210 Proceedings of NAACL-HLT 2016, pages 210–220, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics"
N16-1025,P15-1032,0,0.262661,"pected F-measure. The model naturally incorporates beam search during training, and is globally optimized, to learn shift-reduce action sequences that lead to parses with high expected Fscores. In contrast to Auli and Lopez (2011), who optimize a CCG parser for F-measure via softmaxmargin (Gimpel and Smith, 2010), we directly optimize an expected F-measure objective, derivable from only a set of shift-reduce action sequences and sentence-level F-scores. More generally, our method can be seen as an alternative approach for training a neural beam search parsing model (Watanabe and Sumita, 2015; Weiss et al., 2015; Zhou et al., 2015), combining the benefits of global learning and taskspecific optimization. We also introduce a simple recurrent neural network (RNN) model to shift-reduce parsing on which the greedy baseline and the global model is based. Compared with feed-forward networks, RNNs have the potential to capture and use an unbounded history, and they have been used to learn explicit representations for parser states as well as actions 210 Proceedings of NAACL-HLT 2016, pages 210–220, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics performed on the sta"
N16-1025,P14-1021,1,0.744248,"lowing Miikkulainen (1996) and Mayberry and Miikkulainen (1999). In comparison, our model is a natural extension of the feed-forward architecture in Chen and Manning (2014) using Elman RNNs (Elman, 1990). We apply our models to CCG, and evaluate the resulting parsers on standard CCGBank data (Hockenmaier and Steedman, 2007). More specifically, by combining the global RNN parsing model with a bidirectional RNN CCG supertagger that we have developed (§4) — building on the supertagger of Xu et al. (2015), we obtain accuracies higher than the shift-reduce CCG parsers of Zhang and Clark (2011) and Xu et al. (2014). Finally, although we choose to focus on shift-reduce parsing for CCG, we expect the methods to generalize to other shift-reduce parsers. 2 RNN Models In this section, we start by describing the baseline model, which is also taken as the pretrained model to train the global model (§2.4). We abstract away from the details of CCG and present the models in a canonical shift-reduce parsing framework (Aho and Ullman, 1972), which is henceforth assumed: partially constructed derivations are maintained on a stack, and a queue stores remaining words from the input string; the initial parse item has a"
N16-1025,P15-2041,1,0.871297,"s performed on the stack and queue in shift-reduce parsers (Dyer et al., 2015; Watanabe and Sumita, 2015), following Miikkulainen (1996) and Mayberry and Miikkulainen (1999). In comparison, our model is a natural extension of the feed-forward architecture in Chen and Manning (2014) using Elman RNNs (Elman, 1990). We apply our models to CCG, and evaluate the resulting parsers on standard CCGBank data (Hockenmaier and Steedman, 2007). More specifically, by combining the global RNN parsing model with a bidirectional RNN CCG supertagger that we have developed (§4) — building on the supertagger of Xu et al. (2015), we obtain accuracies higher than the shift-reduce CCG parsers of Zhang and Clark (2011) and Xu et al. (2014). Finally, although we choose to focus on shift-reduce parsing for CCG, we expect the methods to generalize to other shift-reduce parsers. 2 RNN Models In this section, we start by describing the baseline model, which is also taken as the pretrained model to train the global model (§2.4). We abstract away from the details of CCG and present the models in a canonical shift-reduce parsing framework (Aho and Ullman, 1972), which is henceforth assumed: partially constructed derivations are"
N16-1025,D08-1059,1,0.740405,"easure loss, which gives further significant accuracy improvements (§5). 2.4 Expected F1 Training The RNN we use to train the global model has the same Elman architecture as the greedy model. Given the greedy model, we summarize its weights as θ = {U, V, W} and initialize the weights of the global model to θ, and training proceeds as follows: 1. We use a beam-search decoder to parse a sentence xn in the training data and let the decoder generate a k-best list2 of output parses using the current θ, denoted as Λ(xn ). Similar to other structured training approaches that use inexact beam search (Zhang and Clark, 2008; Weiss et al., 2015; Watanabe and Sumita, 2015; Zhou et al., 2015), Λ(xn ) is as an approximation to the set of all possible parses of an input sentence. 2. Let yi be the shift-reduce action sequence of a parse in the k-best list Λ(xn ), and let |yi |be its total number of actions and yij be the j th action in yi , for 1 ≤ j ≤ |yi |. We compute the log-linear action sequence score of yi , ρ(yi ), as a sum of individual action scores in that 2 We do not put a limit on k, and whenever an item is finished, it is appended to the k-best list. We found the size of the k-best lists were on average t"
N16-1025,P11-1069,1,0.364226,"nabe and Sumita, 2015), following Miikkulainen (1996) and Mayberry and Miikkulainen (1999). In comparison, our model is a natural extension of the feed-forward architecture in Chen and Manning (2014) using Elman RNNs (Elman, 1990). We apply our models to CCG, and evaluate the resulting parsers on standard CCGBank data (Hockenmaier and Steedman, 2007). More specifically, by combining the global RNN parsing model with a bidirectional RNN CCG supertagger that we have developed (§4) — building on the supertagger of Xu et al. (2015), we obtain accuracies higher than the shift-reduce CCG parsers of Zhang and Clark (2011) and Xu et al. (2014). Finally, although we choose to focus on shift-reduce parsing for CCG, we expect the methods to generalize to other shift-reduce parsers. 2 RNN Models In this section, we start by describing the baseline model, which is also taken as the pretrained model to train the global model (§2.4). We abstract away from the details of CCG and present the models in a canonical shift-reduce parsing framework (Aho and Ullman, 1972), which is henceforth assumed: partially constructed derivations are maintained on a stack, and a queue stores remaining words from the input string; the ini"
N16-1025,P15-1117,0,0.232056,"l network models (Chen and Manning, 2014), accurate and efficient shift-reduce parsing models can be obtained with little feature engineering, largely alleviating the feature sparsity problem of linear models. In practice, the most common objective for optimizing neural network shift-reduce parsing models is maximum likelihood. In the greedy search setting, the log-likelihood of each target action is maximized during training, and the most likely action is committed to at each step of the parsing process during inference (Chen and Manning, 2014; Dyer et al., 2015). In the beam search setting, Zhou et al. (2015) show that sentence-level likelihood, together with contrastive learning (Hinton, 2002), can be used to derive a global model which incorporates beam In this paper, we present a global neural network parsing model, optimized for a task-specific loss based on expected F-measure. The model naturally incorporates beam search during training, and is globally optimized, to learn shift-reduce action sequences that lead to parses with high expected Fscores. In contrast to Auli and Lopez (2011), who optimize a CCG parser for F-measure via softmaxmargin (Gimpel and Smith, 2010), we directly optimize an"
N16-1025,P13-1043,0,0.0191998,"Missing"
N16-1025,P14-2023,1,\N,Missing
N16-1071,R11-1055,0,0.192846,"ataset apply to it (e.g. is animal, has 4 legs) and which don’t (e.g. a bird, made of metal). 3.1 Building modality-specific representations We obtain distributed representations of concepts in the property-norm semantic space (henceforth PROPNORM ) by simply treating MCRAE as a bag of 2526 properties, with the production frequencies representing the “co-occurrence counts” (Table 3). Our visual space (henceforth VISUAL) consists of visual representations for all the 541 concepts in MCRAE , built as follows. First, we retrieve 10 images per concept from Google Images,2 following previous work (Bergsma and Goebel, 2011; Kiela and Bottou, 2014). The image representations are then obtained by extracting the pre-softmax layer 2 www.google.com/imghp (images were retrieved on 10 April 2015) from a forward pass in a convolutional neural network that has been trained on the ImageNet classification task using Caffe (Jia et al., 2014). We aggregate images associated with a concept into an overall visually grounded representation by taking the mean of the individual image representations. The dimensionality of the visual vectors is 4096. We also build three linguistic spaces (DISTRIB, SVD and EMBED ), along the lines"
N16-1071,P12-1015,0,0.359172,"Missing"
N16-1071,W15-0107,1,0.822125,"al Linguistics one where we take advantage of the abundance of freely available textual corpora. There are two strands of research that attempt to automatically obtain property norm data for new concepts. One approach is to automatically generate feature norms from text corpora by mining text data for a set of generalised property patterns (Kelly et al., 2014; Baroni et al., 2010; Barbu, 2008). Another avenue of research is inspired by Lazaridou et al. (2014) and Mikolov et al. (2013b) and tries to increase the coverage of feature norms through cross-modal mapping from linguistic information (Fagarasan et al., 2015). Here, we follow recent trends in multi-modal semantics and explore automatic property norm extraction from visual, rather than textual, data. Obtaining property norms from visual information makes intuitive sense: information contained in the property norm datasets can often be attributed to extra-linguistic modalities—a large proportion of relevant properties are visual, auditory or tactile, rather than linguistic (e.g. is round, makes noise, is yellow). We show that such conceptual properties can be more accurately predicted through cross-modal mappings from raw perceptual information (i.e"
N16-1071,D14-1032,0,0.0436018,"bridge ltf24,douwe.kiela,stephen.clark@cl.cam.ac.uk Abstract et al., 2013). After having been used to test models of conceptual representation in cognitive science for decades (Randall et al., 2004; Cree et al., 2006), these datasets have proved to be useful in a wide range of semantic NLP tasks as well, including text simplification for limited vocabulary groups. More recently, property norms have been used as a proxy for perceptual information in a number of studies on multi-modal semantics (Andrews et al., 2009; Riordan and Jones, 2011; Silberer and Lapata, 2012; Roller and Im Walde, 2013; Hill and Korhonen, 2014). Such models aim to addres the grounding problem (Harnad, 1990) that distributional semantic models of language (Turney and Pantel, 2010; Clark, 2015) suffer from. Property norms have the potential to aid a wide range of semantic tasks, provided that they can be obtained for large numbers of concepts. Recent work has focused on text as the main source of information for automatic property extraction. In this paper we examine property norm prediction from visual, rather than textual, data, using cross-modal maps learnt between property norm and visual spaces. We also investigate the importance"
N16-1071,D14-1005,1,0.876581,"s animal, has 4 legs) and which don’t (e.g. a bird, made of metal). 3.1 Building modality-specific representations We obtain distributed representations of concepts in the property-norm semantic space (henceforth PROPNORM ) by simply treating MCRAE as a bag of 2526 properties, with the production frequencies representing the “co-occurrence counts” (Table 3). Our visual space (henceforth VISUAL) consists of visual representations for all the 541 concepts in MCRAE , built as follows. First, we retrieve 10 images per concept from Google Images,2 following previous work (Bergsma and Goebel, 2011; Kiela and Bottou, 2014). The image representations are then obtained by extracting the pre-softmax layer 2 www.google.com/imghp (images were retrieved on 10 April 2015) from a forward pass in a convolutional neural network that has been trained on the ImageNet classification task using Caffe (Jia et al., 2014). We aggregate images associated with a concept into an overall visually grounded representation by taking the mean of the individual image representations. The dimensionality of the visual vectors is 4096. We also build three linguistic spaces (DISTRIB, SVD and EMBED ), along the lines of Fagarasan et al. (201"
N16-1071,P15-2038,1,0.847704,"or representations from the log-linear skipgram model of Mikolov et al. (2013a). We used the publicly-available3 representations that were trained on part of the Google News dataset (about 100 billion words). We will also employ three multi-modal semantic spaces (VISUAL + DISTRIB , VISUAL + SVD , VI SUAL + EMBED ), in which the visual ( VISUAL) and respective linguistic representations (DISTRIB , SVD , EMBED ) are combined into a multi-modal representation by concatenating their respective L2normalized representations. 3.2 Method and evaluation Following previous work (Fagarasan et al., 2015; Kiela et al., 2015) we use partial least squares regression (PLSR)4 to learn cross-modal maps to the property-norm space (PROPNORM) from the visual (VISUAL), linguistic (DISTRIB , SVD , EMBED) and multi-modal semantic spaces (VISUAL + DISTRIB , VISUAL + SVD , VISUAL + EMBED ). At training time, we take advantage of the fact that we possess both visual/linguistic/multi-modal and property norm information for the concepts in MCRAE. Let’s consider the VISUAL→PROPNORM setting as an example. We use this cross-modal vocabulary to learn a mapping function between VISUAL and PROP 3 https://code.google.com/p/word2vec/ Th"
N16-1071,P14-1132,0,0.079927,"Missing"
N16-1071,D13-1115,0,0.158678,"Missing"
N16-1071,D12-1130,0,0.0931638,"nd Stephen Clark Computer Laboratory University of Cambridge ltf24,douwe.kiela,stephen.clark@cl.cam.ac.uk Abstract et al., 2013). After having been used to test models of conceptual representation in cognitive science for decades (Randall et al., 2004; Cree et al., 2006), these datasets have proved to be useful in a wide range of semantic NLP tasks as well, including text simplification for limited vocabulary groups. More recently, property norms have been used as a proxy for perceptual information in a number of studies on multi-modal semantics (Andrews et al., 2009; Riordan and Jones, 2011; Silberer and Lapata, 2012; Roller and Im Walde, 2013; Hill and Korhonen, 2014). Such models aim to addres the grounding problem (Harnad, 1990) that distributional semantic models of language (Turney and Pantel, 2010; Clark, 2015) suffer from. Property norms have the potential to aid a wide range of semantic tasks, provided that they can be obtained for large numbers of concepts. Recent work has focused on text as the main source of information for automatic property extraction. In this paper we examine property norm prediction from visual, rather than textual, data, using cross-modal maps learnt between property norm"
N16-1071,P13-1056,0,0.0389244,"(those marked with * in Table 6) are highly plausible properties for the given concepts: tastes sweet for BANANA or has legs for TORTOISE. This also means that the model is being unfairly penalised. In order to obtain a complete version of MCRAE, every possible (CONCEPT, property) pair would have to be checked for validity and annotated accordingly depending on whether property is a valid attribute of CONCEPT. 3.5 Importance of complete data We were interested in measuring the impact that a complete dataset of features would have on the performance of the cross-modal zero-shot learning task. Silberer et al. (2013) conducted a study using a subset of the concepts and properties in MCRAE, whereby every property was annotated if it was a plausible attribute of the concept. The published dataset (SILBERER) consists of visual attribute annotations for 512 concepts (that also occur in MCRAE ) and 693 visual properties. The an585 Dataset #concs #props #(conc,prop) pairs SILBERER 512 693 7743 SILB - VIS 512 283 5335 M - VIS 512 283 2140 MCRAE 541 2526 7259 Table 7: Comparison of various datasets, according to the number of concepts and properties covered, as well as the pairs of (CONCEPT, property) contained T"
N16-1071,Q14-1017,0,0.044416,"ED is yellow 29 7 0 a fruit 25 24 0 is edible 13 0 0 is soft 12 0 13 Table 3: Subspace of PROPNORM. Important to note that MCRAE is not complete, meaning that even though some properties are true of a given concept, they have not been produced by the human participants (e.g. the is edible property for APPLE holds the value 0). text-based distributional vectors) (Lazaridou et al., 2014). This represents an extension of the object recognition problem, since we want to associate images with semantic representations of their depicted objects, rather than just with their label (Frome et al., 2013; Socher et al., 2014). The benefit of this approach lies in its generalisation power: once a function between the two semantic spaces is learnt, it can be used to see how an unseen concept relates to other concepts, just by looking at an image of that concept. This is referred to as the zero-shot learning task (Palatucci et al., 2009; Lazaridou et al., 2014). Our task is to increase the coverage of the property norm datasets, meaning that we want to predict properties for new (unseen) concepts. For example, the concept WOLF is not included in MCRAE, but it would be desirable to know which of the properties in the"
N19-1223,W13-2322,0,0.333892,"problem, as many syntactic decisions are not constrained by the semantic graph. To explicitly account for this underspecification, we break down generating from AMR into two steps: first generate a syntactic structure, and then generate the surface form. We show that decomposing the generation process this way leads to state-of-the-art single model performance generating from AMR without additional unlabelled data. We also demonstrate that we can generate meaning-preserving syntactic paraphrases of the same AMR graph, as judged by humans. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic annotation framework which abstracts away from the surface form of text to capture the core ‘who did what to whom’ structure. As a result, generating from AMR is underspecified (see Figure 1 for an example). Single-step approaches to AMR generation (Flanigan et al., 2016; Konstas et al., 2017; Song et al., 2016, 2017) therefore have to decide the syntax and surface form of the AMR realisation in one go. We instead explicitly try and capture this syntactic variation and factor the generation process through a syntactic representation (Walker et al., 2001; Duˇsek and Jurcicek, 201"
N19-1223,P18-1026,0,0.320813,"POS tags) before calculating span F1s. We also report the results of running our aligner on the most probable parse tree as estimated by an unconditional LSTM as a baseline both to control for our aligner and also to see how much extra signal is in the AMR graph. The results in Table 1 show that predicting a syntactic structure from an AMR graph is a much harder task than predicting from the text, but there is information in the AMR graph to improve over a blind baseline. 5 Dev BLEU Test BLEU Trained on LDC2017T10 Our model 26.1 26.8 Our model + oracle parse 57.5 Baseline s2s + copy 23.7 23.5 Beck et al. (2018) 23.3 Trained on LDC2015E86 Our model 23.6 23.5 Our model + oracle parse 53.1 Konstas et al. (2017) 21.7 22.0 Song et al. (2018) 22.8 23.3 Trained on LDC2015E86 or earlier + additional unlabelled data Song et al. (2018) 33.0 Konstas et al. (2017) 33.1 33.8 Pourdamghani et al. (2016) 27.2 26.9 Song et al. (2017) 25.2 25.6 Experiment 2: Generating natural language from AMR Table 3 shows the results of our model on the AMR generation task. We evaluate using BLEU score (Papineni et al., 2002) against the reference realisations. As a baseline, we train a straight AMR-to-text model with the same arc"
N19-1223,D18-1327,0,0.0164831,"h abstracts away from the surface form of text to capture the core ‘who did what to whom’ structure. As a result, generating from AMR is underspecified (see Figure 1 for an example). Single-step approaches to AMR generation (Flanigan et al., 2016; Konstas et al., 2017; Song et al., 2016, 2017) therefore have to decide the syntax and surface form of the AMR realisation in one go. We instead explicitly try and capture this syntactic variation and factor the generation process through a syntactic representation (Walker et al., 2001; Duˇsek and Jurcicek, 2016; Gardent and Perez-Beltrachini, 2017; Currey and Heafield, 2018). First, we generate a delexicalised constituency structure from the AMR graph using a syntax model. Then, we fill out the constituency structure with the semantic content in the AMR graph using a lexicalisation model to generate the final surface form. Breaking down the AMR generation process this way provides us with several advantages: we disentangle the variance caused by the choice of syntax from that caused by the choice of words. We can therefore realise the same AMR graph with a variety of syntactic structures by sampling from the syntax model, and deterministically decoding using the"
N19-1223,P16-2008,0,0.0534605,"Missing"
N19-1223,N16-1087,0,0.203175,"eration process this way leads to state-of-the-art single model performance generating from AMR without additional unlabelled data. We also demonstrate that we can generate meaning-preserving syntactic paraphrases of the same AMR graph, as judged by humans. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic annotation framework which abstracts away from the surface form of text to capture the core ‘who did what to whom’ structure. As a result, generating from AMR is underspecified (see Figure 1 for an example). Single-step approaches to AMR generation (Flanigan et al., 2016; Konstas et al., 2017; Song et al., 2016, 2017) therefore have to decide the syntax and surface form of the AMR realisation in one go. We instead explicitly try and capture this syntactic variation and factor the generation process through a syntactic representation (Walker et al., 2001; Duˇsek and Jurcicek, 2016; Gardent and Perez-Beltrachini, 2017; Currey and Heafield, 2018). First, we generate a delexicalised constituency structure from the AMR graph using a syntax model. Then, we fill out the constituency structure with the semantic content in the AMR graph using a lexicalisation model to"
N19-1223,J17-1001,0,0.0200158,"a semantic annotation framework which abstracts away from the surface form of text to capture the core ‘who did what to whom’ structure. As a result, generating from AMR is underspecified (see Figure 1 for an example). Single-step approaches to AMR generation (Flanigan et al., 2016; Konstas et al., 2017; Song et al., 2016, 2017) therefore have to decide the syntax and surface form of the AMR realisation in one go. We instead explicitly try and capture this syntactic variation and factor the generation process through a syntactic representation (Walker et al., 2001; Duˇsek and Jurcicek, 2016; Gardent and Perez-Beltrachini, 2017; Currey and Heafield, 2018). First, we generate a delexicalised constituency structure from the AMR graph using a syntax model. Then, we fill out the constituency structure with the semantic content in the AMR graph using a lexicalisation model to generate the final surface form. Breaking down the AMR generation process this way provides us with several advantages: we disentangle the variance caused by the choice of syntax from that caused by the choice of words. We can therefore realise the same AMR graph with a variety of syntactic structures by sampling from the syntax model, and determini"
N19-1223,P82-1020,0,0.78857,"Missing"
N19-1223,N18-1170,0,0.0518653,"Missing"
N19-1223,P17-1014,0,0.505717,"y leads to state-of-the-art single model performance generating from AMR without additional unlabelled data. We also demonstrate that we can generate meaning-preserving syntactic paraphrases of the same AMR graph, as judged by humans. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic annotation framework which abstracts away from the surface form of text to capture the core ‘who did what to whom’ structure. As a result, generating from AMR is underspecified (see Figure 1 for an example). Single-step approaches to AMR generation (Flanigan et al., 2016; Konstas et al., 2017; Song et al., 2016, 2017) therefore have to decide the syntax and surface form of the AMR realisation in one go. We instead explicitly try and capture this syntactic variation and factor the generation process through a syntactic representation (Walker et al., 2001; Duˇsek and Jurcicek, 2016; Gardent and Perez-Beltrachini, 2017; Currey and Heafield, 2018). First, we generate a delexicalised constituency structure from the AMR graph using a syntax model. Then, we fill out the constituency structure with the semantic content in the AMR graph using a lexicalisation model to generate the final su"
N19-1223,P17-2031,0,0.0446542,"Missing"
N19-1223,D15-1166,0,0.00896447,"h token x1 , . . . , xn . Then we feed this into a stacked bidirectional LSTM encoder to obtain contextualised representations of each input token ci . As far as possible, we share parameters between our two models. Concretely, this means that the syntax model uses the same AMR and parse embeddings, and AMR encoder, as the lexicalisation model. We find that this speeds up model inference, as we only have to encode the AMR sequence once for both models. Further, it regularises the joint model by reducing the number of parameters. In our decoder, we use the dot-product formulation of attention (Luong et al., 2015): the attention potentials ai at timestep t are given by ai = hTt−1 Watt ci where ht−1 is the decoder hidden state at the previous timestep, and ci is the context representation at position i given by the encoder. The attention weight wi is then given by a softmax over the attention potentials, and P the overall context representation st is given by wi ci . The syntax model only attends over the input AMR graph; the linearisation model attends over both the input AMR and syntax tree independently, and the resulting context representation st is given by the concatenation of the AMR context repr"
N19-1223,P02-1040,0,0.104212,"est BLEU Trained on LDC2017T10 Our model 26.1 26.8 Our model + oracle parse 57.5 Baseline s2s + copy 23.7 23.5 Beck et al. (2018) 23.3 Trained on LDC2015E86 Our model 23.6 23.5 Our model + oracle parse 53.1 Konstas et al. (2017) 21.7 22.0 Song et al. (2018) 22.8 23.3 Trained on LDC2015E86 or earlier + additional unlabelled data Song et al. (2018) 33.0 Konstas et al. (2017) 33.1 33.8 Pourdamghani et al. (2016) 27.2 26.9 Song et al. (2017) 25.2 25.6 Experiment 2: Generating natural language from AMR Table 3 shows the results of our model on the AMR generation task. We evaluate using BLEU score (Papineni et al., 2002) against the reference realisations. As a baseline, we train a straight AMR-to-text model with the same architecture as above to control for the extra regularisation in our model compared to previous work. Our results Table 3: BLEU results for generation. show that adding syntax into the model dramatically boosts performance, resulting in state-ofthe-art single model performance on both datasets without using external training data. As an oracle experiment, we also generate from the realisation model conditioned on the ground truth parse. The outstanding result here – BLEU scores in the 50s –"
N19-1223,W16-6603,0,0.129389,"Missing"
N19-1223,P17-1099,0,0.333195,"othesise that this generates better paraphrases of the reference realisation than sampling from a singlestep model. We linearise both the AMR graphs (Konstas et al., 2017) and constituency trees (Vinyals et al., 2015b) to allow us to use sequence-to-sequence models (Sutskever et al., 2014; Bahdanau et al., 2015) for the syntax and lexicalisation models. Further, as the AMR dataset is relatively small, we have issues with data sparsity causing poor parameter estimation for rarely seen words. We deal with this by anonymizing named entities, and including a copy mechanism (Vinyals et al., 2015a; See et al., 2017; Song et al., 2018) into our decoder, which allows open-vocabulary token generation. We show that factorising the generation process in this way leads to improvements in AMR generation, setting a new state of the art for single-model AMR generation performance training only on labelled data. We also verify our diverse generation hypothesis with a human annotation study. 2 Data Abstract Meaning Repreentation Abstract Meaning Representation is a semantic annotation formalism which represents the meaning of an English utterance as a rooted directed acyclic graph. Nodes in the graph represent ent"
N19-1223,P17-2002,0,0.164345,"ntactic structure from an AMR graph is a much harder task than predicting from the text, but there is information in the AMR graph to improve over a blind baseline. 5 Dev BLEU Test BLEU Trained on LDC2017T10 Our model 26.1 26.8 Our model + oracle parse 57.5 Baseline s2s + copy 23.7 23.5 Beck et al. (2018) 23.3 Trained on LDC2015E86 Our model 23.6 23.5 Our model + oracle parse 53.1 Konstas et al. (2017) 21.7 22.0 Song et al. (2018) 22.8 23.3 Trained on LDC2015E86 or earlier + additional unlabelled data Song et al. (2018) 33.0 Konstas et al. (2017) 33.1 33.8 Pourdamghani et al. (2016) 27.2 26.9 Song et al. (2017) 25.2 25.6 Experiment 2: Generating natural language from AMR Table 3 shows the results of our model on the AMR generation task. We evaluate using BLEU score (Papineni et al., 2002) against the reference realisations. As a baseline, we train a straight AMR-to-text model with the same architecture as above to control for the extra regularisation in our model compared to previous work. Our results Table 3: BLEU results for generation. show that adding syntax into the model dramatically boosts performance, resulting in state-ofthe-art single model performance on both datasets without using extern"
N19-1223,D16-1224,0,0.055559,"e-art single model performance generating from AMR without additional unlabelled data. We also demonstrate that we can generate meaning-preserving syntactic paraphrases of the same AMR graph, as judged by humans. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic annotation framework which abstracts away from the surface form of text to capture the core ‘who did what to whom’ structure. As a result, generating from AMR is underspecified (see Figure 1 for an example). Single-step approaches to AMR generation (Flanigan et al., 2016; Konstas et al., 2017; Song et al., 2016, 2017) therefore have to decide the syntax and surface form of the AMR realisation in one go. We instead explicitly try and capture this syntactic variation and factor the generation process through a syntactic representation (Walker et al., 2001; Duˇsek and Jurcicek, 2016; Gardent and Perez-Beltrachini, 2017; Currey and Heafield, 2018). First, we generate a delexicalised constituency structure from the AMR graph using a syntax model. Then, we fill out the constituency structure with the semantic content in the AMR graph using a lexicalisation model to generate the final surface form. Breakin"
N19-1223,P18-1150,0,0.543326,"generates better paraphrases of the reference realisation than sampling from a singlestep model. We linearise both the AMR graphs (Konstas et al., 2017) and constituency trees (Vinyals et al., 2015b) to allow us to use sequence-to-sequence models (Sutskever et al., 2014; Bahdanau et al., 2015) for the syntax and lexicalisation models. Further, as the AMR dataset is relatively small, we have issues with data sparsity causing poor parameter estimation for rarely seen words. We deal with this by anonymizing named entities, and including a copy mechanism (Vinyals et al., 2015a; See et al., 2017; Song et al., 2018) into our decoder, which allows open-vocabulary token generation. We show that factorising the generation process in this way leads to improvements in AMR generation, setting a new state of the art for single-model AMR generation performance training only on labelled data. We also verify our diverse generation hypothesis with a human annotation study. 2 Data Abstract Meaning Repreentation Abstract Meaning Representation is a semantic annotation formalism which represents the meaning of an English utterance as a rooted directed acyclic graph. Nodes in the graph represent entities, events, prope"
N19-1223,P14-5010,0,0.0069384,"th share dev and test sets, facilitating comparison. Constituency syntax While there are many syntactic annotation formalisms, we use delexicalised Penn treebank-style constituency trees to represent syntax. Constituency trees have the advantage of a well-defined linearization order compared to dependency trees. Further, constituency trees may be easier to realise, as they effectively correspond to a bracketing of the surface form. Unfortunately, AMR annotated data does not come with syntactic annotation. We therefore parse the training and dev splits of both corpora with the Stanford parser (Manning et al., 2014) to provide silver-standard reference parse trees. We then delexicalise the parse trees by trimming the trees of the surface words; after this stage, the leaves of the tree are the preterminal POS tags. After this, we linearise the delexicalised constituency trees with depth-first traversal, following Vinyals et al. (2015b). 3 3.1 Model implementation and training Model details We wish to estimate P (Y, Z|X), the joint probability of a parse Y and surface form Z given an AMR graph X. We model this in two parts, using the chain rule to decompose the joint distribution. The first model, which we"
N19-1223,N01-1003,0,0.224217,"Representation (AMR) (Banarescu et al., 2013) is a semantic annotation framework which abstracts away from the surface form of text to capture the core ‘who did what to whom’ structure. As a result, generating from AMR is underspecified (see Figure 1 for an example). Single-step approaches to AMR generation (Flanigan et al., 2016; Konstas et al., 2017; Song et al., 2016, 2017) therefore have to decide the syntax and surface form of the AMR realisation in one go. We instead explicitly try and capture this syntactic variation and factor the generation process through a syntactic representation (Walker et al., 2001; Duˇsek and Jurcicek, 2016; Gardent and Perez-Beltrachini, 2017; Currey and Heafield, 2018). First, we generate a delexicalised constituency structure from the AMR graph using a syntax model. Then, we fill out the constituency structure with the semantic content in the AMR graph using a lexicalisation model to generate the final surface form. Breaking down the AMR generation process this way provides us with several advantages: we disentangle the variance caused by the choice of syntax from that caused by the choice of words. We can therefore realise the same AMR graph with a variety of synta"
N19-1223,P18-1070,0,0.0210653,"f the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 182–187. Association for Computational Linguistics. Conclusions and further work We present an AMR generation model that factors the generation process through a syntactic decision, and show that this leads to improved AMR generation performance. In addition, we show that separating the syntactic decisions from the lexicalisation decisions allows the model to generate higher quality paraphrases of a given AMR graph. In future work, we would like to integrate a semantic parser into our model (Yin et al., 2018). Annotating data with AMR is expensive, and existing AMR treebanks are small. By integrating a component which parses into AMR into our model, we can do semi-supervised learning on plentiful unannotated natural language sentences, and improve our AMR generation performance even further. In addition, we would be able to generate text-to-text paraphrases by parsing into AMR first and then carrying out the paraphrase generation procedure described in this paper (Iyyer et al., 2018). This opens up scope for data augmentation for downstream NLP tasks, such as machine transAcknowledgements The auth"
P02-1042,J99-2004,0,\N,Missing
P02-1042,A00-2018,0,\N,Missing
P02-1042,J97-4005,0,\N,Missing
P02-1042,C96-1058,0,\N,Missing
P02-1042,hockenmaier-steedman-2002-acquiring,1,\N,Missing
P02-1042,J03-4003,0,\N,Missing
P02-1042,P02-1043,1,\N,Missing
P02-1042,P96-1011,0,\N,Missing
P02-1042,P00-1058,0,\N,Missing
P02-1042,P96-1025,0,\N,Missing
P02-1042,1997.iwpt-1.17,0,\N,Missing
P02-1042,P99-1069,0,\N,Missing
P04-1014,J96-1002,0,0.0131687,"he gradient of the objective function to be computed at each iteration. The components of the gradient vector are as follows: ∂L0 (Λ) ∂λi = m X X eλ. f (d,π j ) fi (d, π j ) P λ. f (d,π j ) j=1 d∈∆(π j ) d∈∆(π j ) e − (5) m X X λi eλ. f (ω) fi (ω) − 2 P λ. f (ω) σi j=1 ω∈ρ(S j ) ω∈ρ(S j ) e The first two terms in (5) are expectations of feature fi : the first expectation is over all derivations leading to each gold standard dependency structure; the second is over all derivations for each sentence in the training data. Setting the gradient to zero yields the usual maximum entropy constraints (Berger et al., 1996), except that in this case the empirical values are themselves expectations (over all derivations leading to each gold standard dependency structure). The estimation process attempts to make the expectations equal, by putting as much mass as possible on the derivations leading to the gold standard structures.1 The Gaussian prior term penalises any model whose weights get too large in absolute value. Calculation of the feature expectations requires summing over all derivations for a sentence, and summing over all derivations leading to a gold standard dependency structure. In both cases there c"
P04-1014,W03-1013,1,0.539725,"evelop a new efficient parsing algorithm for CCG which maximises expected recall of dependencies. We compare models which use all CCG derivations, including nonstandard derivations, with normal-form models. The performances of the two models are comparable and the results are competitive with existing wide-coverage CCG parsers. 1 Introduction A number of statistical parsing models have recently been developed for Combinatory Categorial Grammar (CCG; Steedman, 2000) and used in parsers applied to the WSJ Penn Treebank (Clark et al., 2002; Hockenmaier and Steedman, 2002; Hockenmaier, 2003b). In Clark and Curran (2003) we argued for the use of log-linear parsing models for CCG. However, estimating a log-linear model for a widecoverage CCG grammar is very computationally expensive. Following Miyao and Tsujii (2002), we showed how the estimation can be performed efficiently by applying the inside-outside algorithm to a packed chart. We also showed how the complete WSJ Penn Treebank can be used for training by developing a parallel version of Generalised Iterative Scaling (GIS) to perform the estimation. This paper significantly extends our earlier work in a number of ways. First, we evaluate a number of log-l"
P04-1014,C04-1041,1,0.564233,"Missing"
P04-1014,P02-1042,1,0.918306,"ter allowing the complete Penn Treebank to be used for estimation. We also develop a new efficient parsing algorithm for CCG which maximises expected recall of dependencies. We compare models which use all CCG derivations, including nonstandard derivations, with normal-form models. The performances of the two models are comparable and the results are competitive with existing wide-coverage CCG parsers. 1 Introduction A number of statistical parsing models have recently been developed for Combinatory Categorial Grammar (CCG; Steedman, 2000) and used in parsers applied to the WSJ Penn Treebank (Clark et al., 2002; Hockenmaier and Steedman, 2002; Hockenmaier, 2003b). In Clark and Curran (2003) we argued for the use of log-linear parsing models for CCG. However, estimating a log-linear model for a widecoverage CCG grammar is very computationally expensive. Following Miyao and Tsujii (2002), we showed how the estimation can be performed efficiently by applying the inside-outside algorithm to a packed chart. We also showed how the complete WSJ Penn Treebank can be used for training by developing a parallel version of Generalised Iterative Scaling (GIS) to perform the estimation. This paper significantly e"
P04-1014,P96-1025,0,0.0485102,"CG is unusual among grammar formalisms in that, for each derived structure for a sentence, there can be many derivations leading to that structure. The presence of such ambiguity, sometimes referred to as spurious ambiguity, enables CCG to produce elegant analyses of coordination and extraction phenomena (Steedman, 2000). However, the introduction of extra derivations increases the complexity of the modelling and parsing problem. Clark et al. (2002) handle the additional derivations by modelling the derived structure, in their case dependency structures. They use a conditional model, based on Collins (1996), which, as the authors acknowledge, has a number of theoretical deficiencies; thus the results of Clark et al. provide a useful baseline for the new models presented here. Hockenmaier (2003a) uses a model which favours only one of the derivations leading to a derived structure, namely the normal-form derivation (Eisner, 1996). In this paper we compare the normal-form approach with a dependency model. For the dependency model, we define the probability of a dependency structure as follows: X P(π|S ) = P(d, π|S ) (1) d∈∆(π) where π is a dependency structure, S is a sentence and ∆(π) is the set"
P04-1014,E03-1071,1,0.264655,"Missing"
P04-1014,P96-1011,0,0.135802,"er, the introduction of extra derivations increases the complexity of the modelling and parsing problem. Clark et al. (2002) handle the additional derivations by modelling the derived structure, in their case dependency structures. They use a conditional model, based on Collins (1996), which, as the authors acknowledge, has a number of theoretical deficiencies; thus the results of Clark et al. provide a useful baseline for the new models presented here. Hockenmaier (2003a) uses a model which favours only one of the derivations leading to a derived structure, namely the normal-form derivation (Eisner, 1996). In this paper we compare the normal-form approach with a dependency model. For the dependency model, we define the probability of a dependency structure as follows: X P(π|S ) = P(d, π|S ) (1) d∈∆(π) where π is a dependency structure, S is a sentence and ∆(π) is the set of derivations which lead to π. This extends the approach of Clark et al. (2002) who modelled the dependency structures directly, not using any information from the derivations. In contrast to the dependency model, the normal-form model simply defines a distribution over normalform derivations. The dependency structures consid"
P04-1014,W01-0521,0,0.0802913,"Missing"
P04-1014,P96-1024,0,0.155575,"rt for CCG parsing. We also compare log-linear models which use all CCG derivations, including non-standard derivations, with normal-form models. Second, we find that GIS is unsuitable for estimating a model of the size being considered, and develop a parallel version of the L - BFGS algorithm (Nocedal and Wright, 1999). And finally, we show that the parsing algoJames R. Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au rithm described in Clark and Curran (2003) is extremely slow in some cases, and suggest an efficient alternative based on Goodman (1996). The development of parsing and estimation algorithms for models which use all derivations extends existing CCG parsing techniques, and allows us to test whether there is useful information in the additional derivations. However, we find that the performance of the normal-form model is at least as good as the all-derivations model, in our experiments todate. The normal-form approach allows the use of additional constraints on rule applications, leading to a smaller model, reducing the computational resources required for estimation, and resulting in an extremely efficient parser. This paper a"
P04-1014,P02-1043,0,0.4146,"plete Penn Treebank to be used for estimation. We also develop a new efficient parsing algorithm for CCG which maximises expected recall of dependencies. We compare models which use all CCG derivations, including nonstandard derivations, with normal-form models. The performances of the two models are comparable and the results are competitive with existing wide-coverage CCG parsers. 1 Introduction A number of statistical parsing models have recently been developed for Combinatory Categorial Grammar (CCG; Steedman, 2000) and used in parsers applied to the WSJ Penn Treebank (Clark et al., 2002; Hockenmaier and Steedman, 2002; Hockenmaier, 2003b). In Clark and Curran (2003) we argued for the use of log-linear parsing models for CCG. However, estimating a log-linear model for a widecoverage CCG grammar is very computationally expensive. Following Miyao and Tsujii (2002), we showed how the estimation can be performed efficiently by applying the inside-outside algorithm to a packed chart. We also showed how the complete WSJ Penn Treebank can be used for training by developing a parallel version of Generalised Iterative Scaling (GIS) to perform the estimation. This paper significantly extends our earlier work in a num"
P04-1014,P03-1046,0,0.78231,"or estimation. We also develop a new efficient parsing algorithm for CCG which maximises expected recall of dependencies. We compare models which use all CCG derivations, including nonstandard derivations, with normal-form models. The performances of the two models are comparable and the results are competitive with existing wide-coverage CCG parsers. 1 Introduction A number of statistical parsing models have recently been developed for Combinatory Categorial Grammar (CCG; Steedman, 2000) and used in parsers applied to the WSJ Penn Treebank (Clark et al., 2002; Hockenmaier and Steedman, 2002; Hockenmaier, 2003b). In Clark and Curran (2003) we argued for the use of log-linear parsing models for CCG. However, estimating a log-linear model for a widecoverage CCG grammar is very computationally expensive. Following Miyao and Tsujii (2002), we showed how the estimation can be performed efficiently by applying the inside-outside algorithm to a packed chart. We also showed how the complete WSJ Penn Treebank can be used for training by developing a parallel version of Generalised Iterative Scaling (GIS) to perform the estimation. This paper significantly extends our earlier work in a number of ways. First,"
P04-1014,P02-1035,0,0.652293,"as in the company that IBM bought) is represented as the following 5-tuple: hbought, (S[dcl]NP1 )/NP2 , 2, company, ∗i where ∗ is the category (NPNP)/(S[dcl]/NP) assigned to the relative pronoun. For local dependencies l is assigned a null value. A dependency structure is a multiset of these dependencies. 3 Log-Linear Parsing Models Log-linear models (also known as Maximum Entropy models) are popular in NLP because of the ease with which discriminating features can be included in the model. Log-linear models have been applied to the parsing problem across a range of grammar formalisms, e.g. Riezler et al. (2002) and Toutanova et al. (2002). One motivation for using a log-linear model is that long-range dependencies which CCG was designed to handle can easily be encoded as features. A conditional log-linear model of a parse ω ∈ Ω, given a sentence S , is defined as follows: P(ω|S ) = 1 λ. f (ω) e ZS (2) P where λ. f (ω) = i λi fi (ω). The function fi is a feature of the parse which can be any real-valued function over the space of parses Ω. Each feature fi has an associated weight λi which is a parameter of the model to be estimated. ZS is a normalising constant which ensures that P(ω|S ) is a probabi"
P04-1014,N03-1028,0,0.0466571,"Missing"
P06-1088,W02-2018,0,0.0139263,"crease in POS tagging accuracy can be achieved with only a tiny increase in ambiguity; and second that maintaining some POS ambiguity can significantly improve the accuracy of the supertagger. The parser uses the CCG lexical categories to build syntactic structure, and the POS tags are used by the supertagger and parser as part of their statisical models. We show that using a multitagger for supertagging results in an effective preprocessor for CCG parsing, and that using a multitagger for POS tagging results in more accurate CCG supertagging. 2 timisation algorithm (Nocedal and Wright, 1999; Malouf, 2002) to perform the estimation. MLE has a tendency to overfit the training data. We adopt the standard approach of Chen and Rosenfeld (1999) by introducing a Gaussian prior term to the objective function which penalises feature weights with large absolute values. A parameter defined in terms of the standard deviation of the Gaussian determines the degree of smoothing. The conditional probability of a sequence of tags, y1 , . . . , yn , given a sentence, w1 , . . . , wn , is defined as the product of the individual probabilities for each tag: Maximum Entropy Tagging P (y1 , . . . , yn |w1 , . . . ,"
P06-1088,J99-2004,0,0.790823,"Missing"
P06-1088,briscoe-carroll-2002-robust,0,0.0156975,"nough to serve as a front-end to a CCG parser, and we retain some POS ambiguity since POS tags are used as features in the statistical models of the supertagger and parser. Charniak et al. (1996) investigated multi-POS tagging in the context of PCFG parsing. It was found that multi-tagging provides only a minor improvement in accuracy, with a significant loss in efficiency; hence it was concluded that, given the particular parser and tagger used, a single-tag POS tagger is preferable to a multi-tagger. More recently, Watson (2006) has revisited this question in the context of the RASP parser (Briscoe and Carroll, 2002) and found that, similar to Charniak et al. (1996), multi-tagging at the POS level results in a small increase in parsing accuracy but at some cost in efficiency. For lexicalized grammars, such as CCG and TAG , the motivation for using a multi-tagger to assign the elementary structures (supertags) is more compelling. Since the set of supertags is typically much larger than a standard POS tag set, the tagging problem becomes much harder. In where Z(x) is a normalisation constant which ensures a proper probability distribution for each context x. The feature functions fi (x, y) are binaryvalued,"
P06-1088,W04-2401,0,0.0219275,"Missing"
P06-1088,C04-1041,1,0.6649,"Missing"
P06-1088,N03-1033,0,0.052147,"Missing"
P06-1088,P04-1014,1,0.81675,"Missing"
P06-1088,U05-1007,1,0.823674,"om the optimisation algorithm; for example, GIS only allows non-negative values. Real-valued features are commonly used with other machine learning algorithms. Binary features suffer from certain limitations of the representation, which make them unsuitable for modelling some properties. For example, POS taggers have difficulty determining if capitalised, sentence initial words are proper nouns. A useful way to model this property is to determine the ratio of capitalised and non-capitalised instances of a particular word in a large corpus and use a realvalued feature which encodes this ratio (Vadas and Curran, 2005). The only way to include this feature in a binary representation is to discretize (or bin) the feature values. For this type of feature, choosing appropriate bins is difficult and it may be hard to find a discretization scheme that performs optimally. Another problem with discretizing feature values is that it imposes artificial boundaries to define the bins. For the example above, we may choose the bins 0 ≤ x < 1 and 1 ≤ x < 2, which separate the values 0.99 and 1.01 even though they are close in value. At the same time, the model does not distinguish between 0.01 and 0.99 even though they a"
P06-1088,W02-1001,0,0.102537,"Missing"
P06-1088,E03-1071,1,0.792415,"Missing"
P06-1088,P02-1043,0,0.0515943,"Missing"
P07-1032,A00-2018,0,0.094588,"Missing"
P07-1032,C04-1041,1,0.858706,"from the same resource used to develop the parser. This allows the many parsers based on the Penn Treebank, for example, to be meaningfully compared. However, there are two drawbacks to this approach. First, parser evaluations using different resources cannot be compared; for example, the Parseval scores obtained by Penn Treebank parsers cannot be compared with the dependency F-scores obtained by evaluating on the Parc Dependency Bank. Second, using the same resource for development and testing can lead to an over-optimistic view of parser performance. In this paper we evaluate a CCG parser (Clark and Curran, 2004b) on the Briscoe and Carroll version of DepBank (Briscoe and Carroll, 2006). The CCG parser produces head-dependency relations, so evaluating the parser should simply be a matter of converting the CCG dependencies into those in DepBank. Such conversions have been performed for other parsers, including parsers producing phrase structure output (Kaplan et al., 2004; Preiss, 2003). However, we found that performing such a conversion is a time-consuming and non-trivial task. The contributions of this paper are as follows. First, we demonstrate the considerable difficulties associated with formali"
P07-1032,P04-1014,1,0.354702,"from the same resource used to develop the parser. This allows the many parsers based on the Penn Treebank, for example, to be meaningfully compared. However, there are two drawbacks to this approach. First, parser evaluations using different resources cannot be compared; for example, the Parseval scores obtained by Penn Treebank parsers cannot be compared with the dependency F-scores obtained by evaluating on the Parc Dependency Bank. Second, using the same resource for development and testing can lead to an over-optimistic view of parser performance. In this paper we evaluate a CCG parser (Clark and Curran, 2004b) on the Briscoe and Carroll version of DepBank (Briscoe and Carroll, 2006). The CCG parser produces head-dependency relations, so evaluating the parser should simply be a matter of converting the CCG dependencies into those in DepBank. Such conversions have been performed for other parsers, including parsers producing phrase structure output (Kaplan et al., 2004; Preiss, 2003). However, we found that performing such a conversion is a time-consuming and non-trivial task. The contributions of this paper are as follows. First, we demonstrate the considerable difficulties associated with formali"
P07-1032,J03-4003,0,0.0376609,"Missing"
P07-1032,E03-1071,1,0.521697,"Missing"
P07-1032,P02-1043,0,0.336984,"Missing"
P07-1032,E03-1005,0,0.0158967,"Missing"
P07-1032,N04-1013,0,0.202683,"dependency F-scores obtained by evaluating on the Parc Dependency Bank. Second, using the same resource for development and testing can lead to an over-optimistic view of parser performance. In this paper we evaluate a CCG parser (Clark and Curran, 2004b) on the Briscoe and Carroll version of DepBank (Briscoe and Carroll, 2006). The CCG parser produces head-dependency relations, so evaluating the parser should simply be a matter of converting the CCG dependencies into those in DepBank. Such conversions have been performed for other parsers, including parsers producing phrase structure output (Kaplan et al., 2004; Preiss, 2003). However, we found that performing such a conversion is a time-consuming and non-trivial task. The contributions of this paper are as follows. First, we demonstrate the considerable difficulties associated with formalism-independent parser evaluation, highlighting the problems in converting the Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 248–255, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics output of a parser from one representation to another. Second, we develop a method for measuring how"
P07-1032,P06-2006,0,0.609096,"parsers based on the Penn Treebank, for example, to be meaningfully compared. However, there are two drawbacks to this approach. First, parser evaluations using different resources cannot be compared; for example, the Parseval scores obtained by Penn Treebank parsers cannot be compared with the dependency F-scores obtained by evaluating on the Parc Dependency Bank. Second, using the same resource for development and testing can lead to an over-optimistic view of parser performance. In this paper we evaluate a CCG parser (Clark and Curran, 2004b) on the Briscoe and Carroll version of DepBank (Briscoe and Carroll, 2006). The CCG parser produces head-dependency relations, so evaluating the parser should simply be a matter of converting the CCG dependencies into those in DepBank. Such conversions have been performed for other parsers, including parsers producing phrase structure output (Kaplan et al., 2004; Preiss, 2003). However, we found that performing such a conversion is a time-consuming and non-trivial task. The contributions of this paper are as follows. First, we demonstrate the considerable difficulties associated with formalism-independent parser evaluation, highlighting the problems in converting th"
P07-1032,P06-4020,0,0.0691834,"against our results; the GRs making up the annotation share some similarities with the predicateargument dependencies output by the CCG parser; and we can directly compare our parser against a non-CCG parser, namely the RASP parser. We chose not to use the corpus based on the Susanne corpus (Carroll et al., 1998) because the GRs are less like the CCG dependencies; the corpus is not based on the Penn Treebank, making comparison more difficult because of tokenisation differences, for example; and the latest results for RASP are on DepBank. The GRs are described in Briscoe and Carroll (2006) and Briscoe et al. (2006). Table 1 lists the GR s used in the evaluation. As an example, the sentence The parent sold Imperial produces three GRs: (det parent The), (ncsubj sold parent ) and (dobj sold Imperial). Note that some GRs — in this example ncsubj — have a subtype slot, giving extra information. The subtype slot for ncsubj is used to indicate passive subjects, with the null value “ ” for active subjects and obj for passive subjects. Other subtype slots are discussed in Section 4.2. The CCG dependencies were transformed into GR s in two stages. The first stage was to create a mapping between the CCG dependenci"
P07-1032,W99-0629,0,0.122663,"Missing"
P07-1032,P04-1041,0,0.0597084,"Missing"
P07-1032,P05-1022,0,0.0133165,"Missing"
P07-1032,C04-1010,0,0.00561439,"e HPSG (Toutanova et al., 2002; Malouf and van Noord, 2004), LFG (Kaplan et al., 2004; Cahill et al., 2004), TAG (Sarkar and Joshi, 2003), CCG (Hockenmaier and Steedman, 2002; Clark and Curran, 2004b), and variants of phrase-structure grammar (Briscoe et al., 2006), including the phrase-structure grammar implicit in the Penn Treebank (Collins, 2003; Charniak, 2000). Different parsers produce different output, for ex248 James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au ample phrase structure trees (Collins, 2003), dependency trees (Nivre and Scholz, 2004), grammatical relations (Briscoe et al., 2006), and formalismspecific dependencies (Clark and Curran, 2004b). This variety of formalisms and output creates a challenge for parser evaluation. The majority of parser evaluations have used test sets drawn from the same resource used to develop the parser. This allows the many parsers based on the Penn Treebank, for example, to be meaningfully compared. However, there are two drawbacks to this approach. First, parser evaluations using different resources cannot be compared; for example, the Parseval scores obtained by Penn Treebank parsers cannot b"
P07-1032,E03-1025,0,0.222492,"btained by evaluating on the Parc Dependency Bank. Second, using the same resource for development and testing can lead to an over-optimistic view of parser performance. In this paper we evaluate a CCG parser (Clark and Curran, 2004b) on the Briscoe and Carroll version of DepBank (Briscoe and Carroll, 2006). The CCG parser produces head-dependency relations, so evaluating the parser should simply be a matter of converting the CCG dependencies into those in DepBank. Such conversions have been performed for other parsers, including parsers producing phrase structure output (Kaplan et al., 2004; Preiss, 2003). However, we found that performing such a conversion is a time-consuming and non-trivial task. The contributions of this paper are as follows. First, we demonstrate the considerable difficulties associated with formalism-independent parser evaluation, highlighting the problems in converting the Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 248–255, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics output of a parser from one representation to another. Second, we develop a method for measuring how effective the c"
P07-1032,W03-2401,0,\N,Missing
P07-1106,P04-1015,0,0.119517,"ocal window. However, the correct decision can be made by comparison of the two three-word windows containing this character. In order to explore the potential of word-based models, we adapt the perceptron discriminative learning algorithm to the CWS problem. Collins (2002) proposed the perceptron as an alternative to the CRF method for HMM-style taggers. However, our model does not map the segmentation problem to a tag sequence learning problem, but defines features on segmented sentences directly. Hence we use a beam-search decoder during training and testing; our idea is similar to that of Collins and Roark (2004) who used a beam-search decoder as part of a perceptron parsing model. Our work can also be seen as part of the recent move towards search-based learning methods which do not rely on dynamic programming and are thus able to exploit larger parts of the context for making decisions (Daume III, 2006). We study several factors that influence the performance of the perceptron word segmentor, including the averaged perceptron method, the size of the 841 -ý ý  -ý  ¡  v beam and the importance of word-based features. We compare the accuracy of our final system to the state-of-the-art CWS sys"
P07-1106,W02-1001,0,0.878774,"especially useful is information about surrounding words. Consider the sentence “ ”, which can be from “ (among which) (foreign) (companies)”, or “ (in China) (foreign companies) (business)”. Note that the five-character window surrounding “ ” is the same in both cases, making the tagging decision for that character difficult given the local window. However, the correct decision can be made by comparison of the two three-word windows containing this character. In order to explore the potential of word-based models, we adapt the perceptron discriminative learning algorithm to the CWS problem. Collins (2002) proposed the perceptron as an alternative to the CRF method for HMM-style taggers. However, our model does not map the segmentation problem to a tag sequence learning problem, but defines features on segmented sentences directly. Hence we use a beam-search decoder during training and testing; our idea is similar to that of Collins and Roark (2004) who used a beam-search decoder as part of a perceptron parsing model. Our work can also be seen as part of the recent move towards search-based learning methods which do not rely on dynamic programming and are thus able to exploit larger parts of th"
P07-1106,I05-3017,0,0.737479,"Missing"
P07-1106,I05-3023,0,0.0194707,"vector according to these templates. There are 356, 337 features with non-zero values after 6 training iterations using the development data. For this particular feature set, the longest range features are word bigrams. Therefore, among partial candidates ending with the same bigram, the best one will also be in the best final candidate. The decoder can be optimized accordingly: when an incoming character is combined with candidate items as a new word, only the best candidate is kept among those having the same last word. 5 Comparison with Previous Work Among the character-tagging CWS models, Li et al. (2005) uses an uneven margin alteration of the traditional perceptron classifier (Li et al., 2002). Each character is classified independently, using information in the neighboring five-character window. Liang (2005) uses the discriminative perceptron algorithm (Collins, 2002) to score whole character tag sequences, finding the best candidate by the global score. It can be seen as an alternative to the ME and CRF models (Xue, 2003; Peng et al., 2004), which 1 2 3 4 5 6 7 8 9 10 11 12 13 14 word w word bigram w1 w2 single-character word w a word starting with character c and having length l a word en"
P07-1106,C04-1081,0,0.829011,"Missing"
P07-1106,W03-1719,0,0.0169565,"Missing"
P07-1106,J96-3004,0,0.0625506,"Missing"
P07-1106,W06-0121,0,0.0468868,"bigram w1 w2 single-character word w a word starting with character c and having length l a word ending with character c and having length l space-separated characters c1 and c2 character bigram c1 c2 in any word the first and last characters c1 and c2 of any word word w immediately before character c character c immediately before word w the starting characters c1 and c2 of two consecutive words the ending characters c1 and c2 of two consecutive words a word of length l and the previous word w a word of length l and the next word w Table 1: feature templates do not involve word information. Wang et al. (2006) incorporates an N-gram language model in ME tagging, making use of word information to improve the character tagging model. The key difference between our model and the above models is the wordbased nature of our system. One existing method that is based on sub-word information, Zhang et al. (2006), combines a CRF and a rule-based model. Unlike the character-tagging models, the CRF submodel assigns tags to subwords, which include single-character words and the most frequent multiple-character words from the training corpus. Thus it can be seen as a step towards a word-based model. However, su"
P07-1106,O03-4002,0,0.835363,"Missing"
P07-1106,N06-2049,0,0.0974195,"er c character c immediately before word w the starting characters c1 and c2 of two consecutive words the ending characters c1 and c2 of two consecutive words a word of length l and the previous word w a word of length l and the next word w Table 1: feature templates do not involve word information. Wang et al. (2006) incorporates an N-gram language model in ME tagging, making use of word information to improve the character tagging model. The key difference between our model and the above models is the wordbased nature of our system. One existing method that is based on sub-word information, Zhang et al. (2006), combines a CRF and a rule-based model. Unlike the character-tagging models, the CRF submodel assigns tags to subwords, which include single-character words and the most frequent multiple-character words from the training corpus. Thus it can be seen as a step towards a word-based model. However, sub-words do not necessarily contain full word information. Moreover, sub-word extraction is performed separately from feature extraction. Another difference from our model is the rule-based submodel, which uses a dictionary-based forward maximum match method described by Sproat et al. (1996). 6 Exper"
P07-1106,W03-1726,0,\N,Missing
P07-2009,P04-1014,1,0.880545,"vels of robustness and efficiency in NLP systems and made linguistically motivated large-scale language processing a possibility (Matsuzaki et al., 2007; Kaplan et al., 2004). This paper describes an NLP system which is based on syntactic and semantic formalisms from theoretical linguistics, and which we have used to analyse the entire Gigaword corpus (1 billion words) in less than 5 days using only 18 processors. This combination of detail and speed of analysis represents a breakthrough in NLP technology. The system is built around a wide-coverage Combinatory Categorial Grammar (CCG) parser (Clark and Curran, 2004b). The parser not only recovers the local dependencies output by treebank parsers such as Collins (2003), but also the long-range depdendencies inherent in constructions such as extraction and coordination. CCG is a lexicalized grammar formalism, so that each word in a sentence is assigned an elementary syntactic structure, in CCG’s case a lexical category expressing subcategorisation information. Statistical tagging techniques can assign lexical categories with high accuracy and low ambiguity (Curran et al., 2006). The combination of finite-state supertagging and highly engineered C++ leads"
P07-2009,P07-1032,1,0.634558,"teedman, 2000). A Maximum Entropy supertagger first assigns lexical categories to the words in a sentence (Curran et al., 2006), which are then combined by the parser using the combinatory rules and the CKY algorithm. Clark and Curran (2004b) describes log-linear parsing models for CCG. The features in the models are defined over local parts of CCG derivations and include word-word dependencies. A disadvantage of the log-linear models is that they require cluster computing resources for practical training (Clark and Curran, 2004b). We have also investigated perceptron training for the parser (Clark and Curran, 2007b), obtaining comparable accuracy scores and similar training times (a few hours) compared with the log-linear models. The significant advantage of Proceedings of the ACL 2007 Demo and Poster Sessions, pages 33–36, c Prague, June 2007. 2007 Association for Computational Linguistics the perceptron training is that it only requires a single processor. The training is online, updating the model parameters one sentence at a time, and it converges in a few passes over the CCGbank data. A packed chart representation allows efficient decoding, with the same algorithm — the Viterbi algorithm — finding"
P07-2009,W07-1202,1,0.65614,"teedman, 2000). A Maximum Entropy supertagger first assigns lexical categories to the words in a sentence (Curran et al., 2006), which are then combined by the parser using the combinatory rules and the CKY algorithm. Clark and Curran (2004b) describes log-linear parsing models for CCG. The features in the models are defined over local parts of CCG derivations and include word-word dependencies. A disadvantage of the log-linear models is that they require cluster computing resources for practical training (Clark and Curran, 2004b). We have also investigated perceptron training for the parser (Clark and Curran, 2007b), obtaining comparable accuracy scores and similar training times (a few hours) compared with the log-linear models. The significant advantage of Proceedings of the ACL 2007 Demo and Poster Sessions, pages 33–36, c Prague, June 2007. 2007 Association for Computational Linguistics the perceptron training is that it only requires a single processor. The training is online, updating the model parameters one sentence at a time, and it converges in a few passes over the CCGbank data. A packed chart representation allows efficient decoding, with the same algorithm — the Viterbi algorithm — finding"
P07-2009,W04-3215,1,0.743379,"Missing"
P07-2009,J03-4003,0,0.0233363,"g a possibility (Matsuzaki et al., 2007; Kaplan et al., 2004). This paper describes an NLP system which is based on syntactic and semantic formalisms from theoretical linguistics, and which we have used to analyse the entire Gigaword corpus (1 billion words) in less than 5 days using only 18 processors. This combination of detail and speed of analysis represents a breakthrough in NLP technology. The system is built around a wide-coverage Combinatory Categorial Grammar (CCG) parser (Clark and Curran, 2004b). The parser not only recovers the local dependencies output by treebank parsers such as Collins (2003), but also the long-range depdendencies inherent in constructions such as extraction and coordination. CCG is a lexicalized grammar formalism, so that each word in a sentence is assigned an elementary syntactic structure, in CCG’s case a lexical category expressing subcategorisation information. Statistical tagging techniques can assign lexical categories with high accuracy and low ambiguity (Curran et al., 2006). The combination of finite-state supertagging and highly engineered C++ leads to a parser which can analyse up to 30 sentences per second on standard hardware (Clark and Curran, 2004a"
P07-2009,E03-1071,1,0.503429,"xicalized grammar formalism, so that each word in a sentence is assigned an elementary syntactic structure, in CCG’s case a lexical category expressing subcategorisation information. Statistical tagging techniques can assign lexical categories with high accuracy and low ambiguity (Curran et al., 2006). The combination of finite-state supertagging and highly engineered C++ leads to a parser which can analyse up to 30 sentences per second on standard hardware (Clark and Curran, 2004a). The C & C tools also contain a number of Maximum Entropy taggers, including the CCG supertagger, a POS tagger (Curran and Clark, 2003a), chun33 bos@di.uniroma1.it ker, and named entity recogniser (Curran and Clark, 2003b). The taggers are highly efficient, with processing speeds of over 100,000 words per second. Finally, the various components, including the morphological analyser morpha (Minnen et al., 2001), are combined into a single program. The output from this program — a CCG derivation, POS tags, lemmas, and named entity tags — is used by the module Boxer (Bos, 2005) to produce interpretable structure in the form of Discourse Representation Structures (DRSs). 2 The CCG Parser The grammar used by the parser is extract"
P07-2009,W03-0424,1,0.650833,"xicalized grammar formalism, so that each word in a sentence is assigned an elementary syntactic structure, in CCG’s case a lexical category expressing subcategorisation information. Statistical tagging techniques can assign lexical categories with high accuracy and low ambiguity (Curran et al., 2006). The combination of finite-state supertagging and highly engineered C++ leads to a parser which can analyse up to 30 sentences per second on standard hardware (Clark and Curran, 2004a). The C & C tools also contain a number of Maximum Entropy taggers, including the CCG supertagger, a POS tagger (Curran and Clark, 2003a), chun33 bos@di.uniroma1.it ker, and named entity recogniser (Curran and Clark, 2003b). The taggers are highly efficient, with processing speeds of over 100,000 words per second. Finally, the various components, including the morphological analyser morpha (Minnen et al., 2001), are combined into a single program. The output from this program — a CCG derivation, POS tags, lemmas, and named entity tags — is used by the module Boxer (Bos, 2005) to produce interpretable structure in the form of Discourse Representation Structures (DRSs). 2 The CCG Parser The grammar used by the parser is extract"
P07-2009,P06-1088,1,0.722633,"is built around a wide-coverage Combinatory Categorial Grammar (CCG) parser (Clark and Curran, 2004b). The parser not only recovers the local dependencies output by treebank parsers such as Collins (2003), but also the long-range depdendencies inherent in constructions such as extraction and coordination. CCG is a lexicalized grammar formalism, so that each word in a sentence is assigned an elementary syntactic structure, in CCG’s case a lexical category expressing subcategorisation information. Statistical tagging techniques can assign lexical categories with high accuracy and low ambiguity (Curran et al., 2006). The combination of finite-state supertagging and highly engineered C++ leads to a parser which can analyse up to 30 sentences per second on standard hardware (Clark and Curran, 2004a). The C & C tools also contain a number of Maximum Entropy taggers, including the CCG supertagger, a POS tagger (Curran and Clark, 2003a), chun33 bos@di.uniroma1.it ker, and named entity recogniser (Curran and Clark, 2003b). The taggers are highly efficient, with processing speeds of over 100,000 words per second. Finally, the various components, including the morphological analyser morpha (Minnen et al., 2001),"
P07-2009,N04-1013,0,0.00471736,"School of Information Technologies Computing Laboratory Dipartimento di Informatica University of Sydney Oxford University Universit`a di Roma “La Sapienza” NSW 2006, Australia Wolfson Building, Parks Road via Salaria 113 james@it.usyd.edu.au Oxford, OX1 3QD, UK 00198 Roma, Italy stephen.clark@comlab.ox.ac.uk 1 Introduction The statistical modelling of language, together with advances in wide-coverage grammar development, have led to high levels of robustness and efficiency in NLP systems and made linguistically motivated large-scale language processing a possibility (Matsuzaki et al., 2007; Kaplan et al., 2004). This paper describes an NLP system which is based on syntactic and semantic formalisms from theoretical linguistics, and which we have used to analyse the entire Gigaword corpus (1 billion words) in less than 5 days using only 18 processors. This combination of detail and speed of analysis represents a breakthrough in NLP technology. The system is built around a wide-coverage Combinatory Categorial Grammar (CCG) parser (Clark and Curran, 2004b). The parser not only recovers the local dependencies output by treebank parsers such as Collins (2003), but also the long-range depdendencies inheren"
P07-2009,P06-4020,0,0.0246004,"Missing"
P07-2009,C04-1041,1,\N,Missing
P08-1101,W02-1001,0,0.200367,"nd c2 character bigram c1 c2 in any word the first / last characters c1 / c2 of any word word w immediately before character c character c immediately before word w the starting characters c1 and c2 of two consecutive words the ending characters c1 and c2 of two consecutive words a word of length l with previous word w a word of length l with next word w Table 1: Feature templates for the baseline segmentor 2 The Baseline System We built a two-stage baseline system, using the perceptron segmentation model from our previous work (Zhang and Clark, 2007) and the perceptron POS tagging model from Collins (2002). We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segmentor to refer to the segmentor from (Zhang and Clark, 2007) which performs segmentation only; and baseline POS tagger to refer to the Collins tagger which performs POS tagging only (given segmentation). The features used by the baseline segmentor are shown in Table 1. The features used by the POS tagger, some of which are different to those from Collins (2002) and are specific to Chinese, are shown in Table 2. The word segmentation featu"
P08-1101,W06-1673,0,0.0345774,"Missing"
P08-1101,P07-2055,0,0.0135112,"entor 0.9 0.89 F-score the large search space by imposing strong restrictions on the form of search candidates. In particular, Ng and Low (2004) used character-based POS tagging, which prevents some important POS tagging features such as word + POS tag; Shi and Wang (2007) used an N -best reranking approach, which limits the influence of POS tagging on segmentation to the N -best list. In comparison, our joint model does not impose any hard limitations on the interaction between segmentation and POS information.4 Fast decoding speed is achieved by using a novel multiple-beam search algorithm. Nakagawa and Uchimoto (2007) proposed a hybrid model for word segmentation and POS tagging using an HMM-based approach. Word information is used to process known-words, and character information is used for unknown words in a similar way to Ng and Low (2004). In comparison, our model handles character and word information simultaneously in a single perceptron model. 0.88 0.87 0.86 1 Experiments 2 3 4 5 6 7 8 9 10 Number of training iterations The Chinese Treebank (CTB) 4 is used for the experiments. It is separated into two parts: CTB 3 (420K characters in 150K words / 10364 sentences) is used for the final 10-fold cross"
P08-1101,W04-3236,0,0.531691,"gmentation is a necessary step before POS tagging can be performed. Typically, a Chinese POS tagger takes segmented inputs, which are produced by a separate word segmentor. This two-step approach, however, has an obvious flaw of error propagation, since word segmentation errors cannot be corrected by the POS tagger. A better approach would be to utilize POS inRecent research on Chinese POS tagging has started to investigate joint segmentation and tagging, reporting accuracy improvements over the pipeline approach. Various decoding approaches have been used to reduce the combined search space. Ng and Low (2004) mapped the joint segmentation and POS tagging task into a single character sequence tagging problem. Two types of tags are assigned to each character to represent its segmentation and POS. For example, the tag “b NN” indicates a character at the beginning of a noun. Using this method, POS features are allowed to interact with segmentation. 888 Proceedings of ACL-08: HLT, pages 888–896, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics Since tagging is restricted to characters, the search space is reduced to O((4T )n ), and beam search decoding is effective with"
P08-1101,W96-0213,0,0.781993,"linear model. Denoting the global feature vector for the tagged sentence y with Φ(y), we have: Score(y) = Φ(y) · w ~ Templates 15 and 16 in Table 2 are inspired by the CTBMorph feature templates in Tseng et al. (2005), which gave the most accuracy improvement in their experiments. Here the category of a character is the set of tags seen on the character during training. Other morphological features from Tseng et al. (2005) are not used because they require extra web corpora besides the training data. During training, the baseline POS tagger stores special word-tag pairs into a tag dictionary (Ratnaparkhi, 1996). Such information is used by the decoder to prune unlikely tags. For each word occurring more than N times in the training data, the decoder can only assign a tag the word has been seen with in the training data. This method led to improvement in the decoding speed as well as the output accuracy for English POS tagging (Ratnaparkhi, 1996). Besides tags for frequent words, our baseline POS tagger also uses the tag dictionary to store closed-set tags (Xia, 2000) – those associated only with a limited number of Chinese words. 890 where w ~ is the parameter vector in the model. Each element in w"
P08-1101,P06-2098,0,0.0341201,"Missing"
P08-1101,I05-3005,0,0.0149716,"rticipates in word segmentation. 3.1 Formulation of the joint model We formulate joint word segmentation and POS tagging as a single problem, which maps a raw Chinese sentence to a segmented and POS tagged output. Given an input sentence x, the output F (x) satisfies: F (x) = arg max Score(y) y∈GEN(x) where GEN(x) represents the set of possible outputs for x. Score(y) is computed by a feature-based linear model. Denoting the global feature vector for the tagged sentence y with Φ(y), we have: Score(y) = Φ(y) · w ~ Templates 15 and 16 in Table 2 are inspired by the CTBMorph feature templates in Tseng et al. (2005), which gave the most accuracy improvement in their experiments. Here the category of a character is the set of tags seen on the character during training. Other morphological features from Tseng et al. (2005) are not used because they require extra web corpora besides the training data. During training, the baseline POS tagger stores special word-tag pairs into a tag dictionary (Ratnaparkhi, 1996). Such information is used by the decoder to prune unlikely tags. For each word occurring more than N times in the training data, the decoder can only assign a tag the word has been seen with in the"
P08-1101,P07-1106,1,0.914671,"length l with ending character c space-separated characters c1 and c2 character bigram c1 c2 in any word the first / last characters c1 / c2 of any word word w immediately before character c character c immediately before word w the starting characters c1 and c2 of two consecutive words the ending characters c1 and c2 of two consecutive words a word of length l with previous word w a word of length l with next word w Table 1: Feature templates for the baseline segmentor 2 The Baseline System We built a two-stage baseline system, using the perceptron segmentation model from our previous work (Zhang and Clark, 2007) and the perceptron POS tagging model from Collins (2002). We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segmentor to refer to the segmentor from (Zhang and Clark, 2007) which performs segmentation only; and baseline POS tagger to refer to the Collins tagger which performs POS tagging only (given segmentation). The features used by the baseline segmentor are shown in Table 1. The features used by the POS tagger, some of which are different to those from Collins (2002) and are specific to C"
P09-2014,P06-2006,0,0.0191376,"ences of the WSJ, and evaluating accuracy according to the Parseval metrics. Collins (1999) is a seminal example. The second approach is to apply statistical methods to parsers based on linguistic formalisms, such as HPSG, LFG, TAG, and CCG, with the grammar being defined manually or extracted from a formalism-specific treebank. Evaluation is typically performed by comparing against predicate-argument structures extracted from the treebank, or against a test set of manually annotated grammatical relations (GRs). Examples of this approach include Riezler et al. (2002), Miyao and Tsujii (2005), Briscoe and Carroll (2006), and Clark and Curran (2007).1 Despite the many examples from both approaches, there has been little comparison across the two groups, which we refer to as PTB parsing and formalism-based parsing, respectively. The 2 The CCG to PTB Conversion There has been much recent work in attempting to convert native parser output into alternative representations for evaluation purposes, e.g. (Clark and Curran, 2007; Matsuzaki and Tsujii, 2008). The conclusion is that such conversions are surprisingly difficult. Clark and Curran (2007) 2 Since this short paper reports a small, focused research contributi"
P09-2014,J07-4004,1,0.958666,"difficulty of mapping from a grammatical resource based on the PTB back to the PTB , and we also comment on the (non-)suitability of the PTB as a general formalism-independent evaluation resource. A second contribution is to provide the first accuracy comparison of the CCG parser with a PTB parser, obtaining competitive scores for the CCG parser on a representative subset of the PTB test sections. It is important to note that the purpose of this evaluation is comparison with a PTB parser, rather than evaluation of the CCG parser per se. The CCG parser has been extensively evaluated elsewhere (Clark and Curran, 2007), and arguably GRs or predicate-argument structures provide a more suitable test set for the CCG parser than PTB phrase-structure trees. We compare the CCG parser of Clark and Curran (2007) with a state-of-the-art Penn Treebank (PTB) parser. An accuracy comparison is performed by converting the CCG derivations into PTB trees. We show that the conversion is extremely difficult to perform, but are able to fairly compare the parsers on a representative subset of the PTB test section, obtaining results for the CCG parser that are statistically no different to those for the Berkeley parser. 1 James"
P09-2014,P05-1011,0,0.0552886,"ure trees for unseen sentences of the WSJ, and evaluating accuracy according to the Parseval metrics. Collins (1999) is a seminal example. The second approach is to apply statistical methods to parsers based on linguistic formalisms, such as HPSG, LFG, TAG, and CCG, with the grammar being defined manually or extracted from a formalism-specific treebank. Evaluation is typically performed by comparing against predicate-argument structures extracted from the treebank, or against a test set of manually annotated grammatical relations (GRs). Examples of this approach include Riezler et al. (2002), Miyao and Tsujii (2005), Briscoe and Carroll (2006), and Clark and Curran (2007).1 Despite the many examples from both approaches, there has been little comparison across the two groups, which we refer to as PTB parsing and formalism-based parsing, respectively. The 2 The CCG to PTB Conversion There has been much recent work in attempting to convert native parser output into alternative representations for evaluation purposes, e.g. (Clark and Curran, 2007; Matsuzaki and Tsujii, 2008). The conclusion is that such conversions are surprisingly difficult. Clark and Curran (2007) 2 Since this short paper reports a small,"
P09-2014,N07-1051,0,0.163768,"Missing"
P09-2014,P02-1035,0,0.0729183,"skeletal phrase-structure trees for unseen sentences of the WSJ, and evaluating accuracy according to the Parseval metrics. Collins (1999) is a seminal example. The second approach is to apply statistical methods to parsers based on linguistic formalisms, such as HPSG, LFG, TAG, and CCG, with the grammar being defined manually or extracted from a formalism-specific treebank. Evaluation is typically performed by comparing against predicate-argument structures extracted from the treebank, or against a test set of manually annotated grammatical relations (GRs). Examples of this approach include Riezler et al. (2002), Miyao and Tsujii (2005), Briscoe and Carroll (2006), and Clark and Curran (2007).1 Despite the many examples from both approaches, there has been little comparison across the two groups, which we refer to as PTB parsing and formalism-based parsing, respectively. The 2 The CCG to PTB Conversion There has been much recent work in attempting to convert native parser output into alternative representations for evaluation purposes, e.g. (Clark and Curran, 2007; Matsuzaki and Tsujii, 2008). The conclusion is that such conversions are surprisingly difficult. Clark and Curran (2007) 2 Since this sho"
P09-2014,C08-1069,0,\N,Missing
P09-2014,J03-4003,0,\N,Missing
P09-2014,J07-3004,0,\N,Missing
P10-1036,J99-2004,0,0.71339,"racy and speed improvements for Wikipedia and biomedical text. 1 Introduction In many NLP tasks and applications, e.g. distributional similarity (Curran, 2004) and question answering (Dumais et al., 2002), large volumes of text and detailed syntactic information are both critical for high performance. To avoid a tradeoff between these two, we need to increase parsing speed, but without losing accuracy. Parsing with lexicalised grammar formalisms, such as Lexicalised Tree Adjoining Grammar and Combinatory Categorial Grammar (CCG; Steedman, 2000), can be made more efficient using a supertagger. Bangalore and Joshi (1999) call supertagging almost parsing because of the significant reduction in ambiguity which occurs once the supertags have been assigned. In this paper, we focus on the CCG parser and supertagger described in Clark and Curran (2007). 1 We use supertag and lexical category interchangeably. 345 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 345–355, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics I Using the parser to generate training data also has the advantage that it is not a domain specific process. Previous wo"
P10-1036,E99-1025,0,0.096565,"Missing"
P10-1036,W02-2236,0,0.0204483,"NP S NP &gt; < &gt; < Figure 1: Two CCG derivations with PP ambiguity. can be used to find the most probable supertag sequence. Alternatively the Forward-Backward algorithm can be used to efficiently sum over all sequences, giving a probability distribution over supertags for each word which is conditional only on the input sentence. Background Supertaggers can be made accurate enough for wide coverage parsing using multi-tagging (Chen et al., 1999), in which more than one supertag can be assigned to a word; however, as more supertags are supplied by the supertagger, parsing efficiency decreases (Chen et al., 2002), demonstrating the influence of lexical ambiguity on parsing complexity (Sarkar et al., 2000). Many statistical parsers use two stages: a tagging stage that labels each word with its grammatical role, and a parsing stage that uses the tags to form a parse tree. Lexicalised grammars typically contain a much smaller set of rules than phrase-structure grammars, relying on tags (supertags) that contain a more detailed description of each word’s role in the sentence. This leads to much larger tag sets, and shifts a large proportion of the search for an optimal derivation to the tagging component o"
P10-1036,M95-1004,0,0.192466,"idering a word correct if the correct tag is amongst any of the assigned tags. For the biomedical parser evaluation we have used the parsing model and grammatical relation conversion script from Rimell and Clark (2009). Our timing measurements are calculated in two ways. Overall times were measured using the C&C parser’s timers. Individual sentence measurements were made using the Intel timing registers, since standard methods are not accurate enough for the short time it takes to parse a single sentence. To check whether changes were statistically significant we applied the test described by Chinchor (1995). This measures the probability that two sets of responses are drawn from the same distribution, where a score below 0.05 is considered significant. Models were trained on an Intel Core2Duo 3GHz with 4GB of RAM. The evaluation was performed on a dual quad-core Intel Xeon 2.27GHz with 16GB of RAM. 5.1 6 Results We have performed four primary sets of experiments to explore the ability of an adaptive supertagger to improve parsing speed or accuracy. In the first two experiments, we explore performance on the newswire domain, which is the source of training data for the parsing model and the basel"
P10-1036,C04-1041,1,0.923426,"ng component of the parser. Figure 1 gives two sentences and their CCG derivations, showing how some of the syntactic ambiguity is transferred to the supertagging component in a lexicalised grammar. Note that the lexical category assigned to with is different in each case, reflecting the fact that the prepositional phrase attaches differently. Either we need a tagging model that can resolve this ambiguity, or both lexical categories must be supplied to the parser which can then attempt to resolve the ambiguity by eventually selecting between them. 2.1 ate NP (S NP)/NP NP ((S NP)(S NP))/NP Clark and Curran (2004) applied supertagging to CCG, using a flexible multi-tagging approach. The supertagger assigns to a word all lexical categories whose probabilities are within some factor, β, of the most probable category for that word. When the supertagger is integrated with the C&C parser, several progressively lower β values are considered. If a sentence is not parsed on one pass then the parser attempts to parse the sentence again with a lower β value, using a larger set of categories from the supertagger. Since most sentences are parsed at the first level (in which the average number of supertags assigned"
P10-1036,J07-4004,1,0.901182,"etailed syntactic information are both critical for high performance. To avoid a tradeoff between these two, we need to increase parsing speed, but without losing accuracy. Parsing with lexicalised grammar formalisms, such as Lexicalised Tree Adjoining Grammar and Combinatory Categorial Grammar (CCG; Steedman, 2000), can be made more efficient using a supertagger. Bangalore and Joshi (1999) call supertagging almost parsing because of the significant reduction in ambiguity which occurs once the supertags have been assigned. In this paper, we focus on the CCG parser and supertagger described in Clark and Curran (2007). 1 We use supertag and lexical category interchangeably. 345 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 345–355, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics I Using the parser to generate training data also has the advantage that it is not a domain specific process. Previous work has shown that parsers typically perform poorly outside of their training domain (Gildea, 2001). Using a newspapertrained parser, we constructed new training sets for Wikipedia and biomedical text. These were used to create new"
P10-1036,P07-1120,0,0.0547606,"Missing"
P10-1036,W03-0407,1,0.787353,"ng analysis cannot be found by the parser, the number of lexical categories supplied by the supertagger is increased. The supertagger-parser interaction influences speed in two ways: first, the larger the lexical ambiguity, the more derivations the parser must consider; second, each further pass is as costly as parsing a whole extra sentence. Our goal is to increase parsing speed without loss of accuracy. The technique we use is a form of self-training, in which the output of the parser is used to train the supertagger component. The existing literature on self-training reports mixed results. Clark et al. (2003) were unable to improve the accuracy of POS tagging using self-training. In contrast, McClosky et al. (2006a) report improved accuracy through self-training for a twostage parser and re-ranker. Here our goal is not to improve accuracy, only to maintain it, which we achieve through an adaptive supertagger. The adaptive supertagger produces lexical categories that the parser would have used in the final derivation when using the baseline model. However, it does so with much lower ambiguity levels, and potentially during an earlier pass, which means sentences are parsed faster. By increasing the"
P10-1036,W03-2401,0,0.0235798,"ed as they would be most similar in style to the evaluation corpus. In all experiments the sentences from 1989 were excluded to ensure no overlap occurred with CCGbank. As Wikipedia text we have used 794,024,397 tokens (51,673,069 sentences) from Wikipedia articles. This text was processed in the same way as the NANC data to produce parser-annotated training data. For supertagger evaluation, one thousand sentences were manually annotated with CCG lexical categories and POS tags. For parser evaluation, three hundred of these sentences were manually annotated with DepBank grammatical relations (King et al., 2003) in the style of Briscoe and Carroll (2006). Both sets of annotations were produced by manually correcting the output of the baseline system. The annotation was performed by Stephen Clark and Laura Rimell. For the biomedical domain we have used several different resources. As gold standard data for supertagger evaluation we have used supertagged GENIA data (Kim et al., 2003), annotated by Rimell and Clark (2008). For parsing evaluation, grammatical relations from the BioInfer corpus were used (Pyysalo et al., 2007), with the same post-processing process as Rimell and Clark (2009) to convert th"
P10-1036,W02-1001,0,0.197012,"Missing"
P10-1036,J93-2004,0,0.0350434,"-20 14.04 17.41 39.2 21-40 28.76 29.27 49.4 41-250 49.73 86.73 10.2 All 24.83 152.15 100.0 0-4 2.81 0.60 22.4 5-20 11.64 21.56 48.9 21-40 28.02 28.48 24.3 41-250 49.69 77.70 4.5 All 15.33 154.57 100.0 0-4 2.98 0.75 0.9 5-20 14.54 15.14 41.3 21-40 28.49 29.34 48.0 41-250 49.17 68.34 9.8 All 24.53 139.35 100.0 Table 1: Statistics for sentences in the supertagger training data. Sentences containing more than 250 tokens were not included in our data sets. Training and accuracy evaluation We have used Sections 02-21 of CCGbank (Hockenmaier and Steedman, 2007), the CCG version of the Penn Treebank (Marcus et al., 1993), as training data for the newspaper domain. Sections 00 and 23 were used for development and test evaluation. A further 113,346,430 tokens (4,566,241 sentences) of raw data from the Wall Street Journal section of the North American News Corpus (Graff, 1995) were parsed to produce the training data for adaptation. This text was tokenised using the C&C tools tokeniser and parsed using our baseline models. For the smaller training sets, sentences from 1988 were used as they would be most similar in style to the evaluation corpus. In all experiments the sentences from 1989 were excluded to ensure"
P10-1036,N06-1020,0,0.0472281,"s increased. The supertagger-parser interaction influences speed in two ways: first, the larger the lexical ambiguity, the more derivations the parser must consider; second, each further pass is as costly as parsing a whole extra sentence. Our goal is to increase parsing speed without loss of accuracy. The technique we use is a form of self-training, in which the output of the parser is used to train the supertagger component. The existing literature on self-training reports mixed results. Clark et al. (2003) were unable to improve the accuracy of POS tagging using self-training. In contrast, McClosky et al. (2006a) report improved accuracy through self-training for a twostage parser and re-ranker. Here our goal is not to improve accuracy, only to maintain it, which we achieve through an adaptive supertagger. The adaptive supertagger produces lexical categories that the parser would have used in the final derivation when using the baseline model. However, it does so with much lower ambiguity levels, and potentially during an earlier pass, which means sentences are parsed faster. By increasing the ambiguity level of the adaptive models to match the baseline system, we can also slightly increase supertag"
P10-1036,de-marneffe-etal-2006-generating,0,0.0116686,"Missing"
P10-1036,P06-1043,0,0.0232405,"s increased. The supertagger-parser interaction influences speed in two ways: first, the larger the lexical ambiguity, the more derivations the parser must consider; second, each further pass is as costly as parsing a whole extra sentence. Our goal is to increase parsing speed without loss of accuracy. The technique we use is a form of self-training, in which the output of the parser is used to train the supertagger component. The existing literature on self-training reports mixed results. Clark et al. (2003) were unable to improve the accuracy of POS tagging using self-training. In contrast, McClosky et al. (2006a) report improved accuracy through self-training for a twostage parser and re-ranker. Here our goal is not to improve accuracy, only to maintain it, which we achieve through an adaptive supertagger. The adaptive supertagger produces lexical categories that the parser would have used in the final derivation when using the baseline model. However, it does so with much lower ambiguity levels, and potentially during an earlier pass, which means sentences are parsed faster. By increasing the ambiguity level of the adaptive models to match the baseline system, we can also slightly increase supertag"
P10-1036,U08-1013,1,0.808259,"r the biomedical domain we have used several different resources. As gold standard data for supertagger evaluation we have used supertagged GENIA data (Kim et al., 2003), annotated by Rimell and Clark (2008). For parsing evaluation, grammatical relations from the BioInfer corpus were used (Pyysalo et al., 2007), with the same post-processing process as Rimell and Clark (2009) to convert the C&C parser output to Stanford format grammatical relations (de Marneffe et al., 2006). For adaptive training we have used 1,900,618,859 tokens (76,739,723 sentences) from the MEDLINE abstracts tokenised by McIntosh and Curran (2008). These sentences were POS -tagged and parsed twice, once as for the newswire and Wikipedia data, and then again, using the bio-specific models developed by Rimell and Clark (2009). Statistics for the sentences in the training sets are given in Table 1. 4.2 Speed evaluation data For speed evaluation we held out three sets of sentences from each domain-specific corpus. Specifically, we used 30,000, 4,000 and 2,000 unique sentences of length 5-20, 21-40 and 41-250 tokens respectively. Speeds on these length controlled sets were combined to calculate an overall parsing speed for the text in each"
P10-1036,W01-0521,0,0.0241195,"ery case the new models perform worse than the baseline on domains other than the one they were trained on. In some cases the models in Table 7 are less accurate than those in Table 5. This is because as well as optimising the β levels we have changed training methods. All of the training methods were tried, but only the method with the best results in newswire is included here, which for F-score when trained on 400,000 sentences was GIS. The accuracy presented so far for the biomediCross-domain speed improvement When applying parsers out of domain they are typically slower and less accurate (Gildea, 2001). In this experiment, we attempt to increase speed on out-of-domain data. Note that for some of the results presented here it may appear that the C&C parser does not lose speed when out of domain, since the Wikipedia and biomedical corpora contain shorter sentences on average than the news corpus. However, by testing on balanced sets it is clear that speed does decrease, particularly for longer sentences, as shown in Table 9. For our domain adaptation development experiments, we considered a collection of different models; here we only present results for the best set of models. For speed impr"
P10-1036,P07-1037,0,0.0368961,"Missing"
P10-1036,W07-1004,0,0.0185156,"red of these sentences were manually annotated with DepBank grammatical relations (King et al., 2003) in the style of Briscoe and Carroll (2006). Both sets of annotations were produced by manually correcting the output of the baseline system. The annotation was performed by Stephen Clark and Laura Rimell. For the biomedical domain we have used several different resources. As gold standard data for supertagger evaluation we have used supertagged GENIA data (Kim et al., 2003), annotated by Rimell and Clark (2008). For parsing evaluation, grammatical relations from the BioInfer corpus were used (Pyysalo et al., 2007), with the same post-processing process as Rimell and Clark (2009) to convert the C&C parser output to Stanford format grammatical relations (de Marneffe et al., 2006). For adaptive training we have used 1,900,618,859 tokens (76,739,723 sentences) from the MEDLINE abstracts tokenised by McIntosh and Curran (2008). These sentences were POS -tagged and parsed twice, once as for the newswire and Wikipedia data, and then again, using the bio-specific models developed by Rimell and Clark (2009). Statistics for the sentences in the training sets are given in Table 1. 4.2 Speed evaluation data For sp"
P10-1036,J07-3004,0,0.343689,"Missing"
P10-1036,D08-1050,1,0.880391,"ntences were manually annotated with CCG lexical categories and POS tags. For parser evaluation, three hundred of these sentences were manually annotated with DepBank grammatical relations (King et al., 2003) in the style of Briscoe and Carroll (2006). Both sets of annotations were produced by manually correcting the output of the baseline system. The annotation was performed by Stephen Clark and Laura Rimell. For the biomedical domain we have used several different resources. As gold standard data for supertagger evaluation we have used supertagged GENIA data (Kim et al., 2003), annotated by Rimell and Clark (2008). For parsing evaluation, grammatical relations from the BioInfer corpus were used (Pyysalo et al., 2007), with the same post-processing process as Rimell and Clark (2009) to convert the C&C parser output to Stanford format grammatical relations (de Marneffe et al., 2006). For adaptive training we have used 1,900,618,859 tokens (76,739,723 sentences) from the MEDLINE abstracts tokenised by McIntosh and Curran (2008). These sentences were POS -tagged and parsed twice, once as for the newswire and Wikipedia data, and then again, using the bio-specific models developed by Rimell and Clark (2009)."
P10-1036,W00-1605,0,0.0316057,"ost probable supertag sequence. Alternatively the Forward-Backward algorithm can be used to efficiently sum over all sequences, giving a probability distribution over supertags for each word which is conditional only on the input sentence. Background Supertaggers can be made accurate enough for wide coverage parsing using multi-tagging (Chen et al., 1999), in which more than one supertag can be assigned to a word; however, as more supertags are supplied by the supertagger, parsing efficiency decreases (Chen et al., 2002), demonstrating the influence of lexical ambiguity on parsing complexity (Sarkar et al., 2000). Many statistical parsers use two stages: a tagging stage that labels each word with its grammatical role, and a parsing stage that uses the tags to form a parse tree. Lexicalised grammars typically contain a much smaller set of rules than phrase-structure grammars, relying on tags (supertags) that contain a more detailed description of each word’s role in the sentence. This leads to much larger tag sets, and shifts a large proportion of the search for an optimal derivation to the tagging component of the parser. Figure 1 gives two sentences and their CCG derivations, showing how some of the"
P10-1036,N01-1023,0,0.0924336,"Missing"
P10-1036,E03-1008,1,0.886743,"Missing"
P10-1036,E09-1093,0,0.063093,"Missing"
P10-1036,W09-3832,0,0.0147488,"age (Clark and Curran, 2004). Supertagging Supertaggers typically use standard linear-time tagging algorithms, and only consider words in the local context when assigning a supertag. The C&C supertagger is similar to the Ratnaparkhi (1996) tagger, using features based on words and POS tags in a five-word window surrounding the target word, and defining a local probability distribution over supertags for each word in the sentence, given the previous two supertags. The Viterbi algorithm Supertagging has since been effectively applied to other formalisms, such as HPSG (Blunsom and Baldwin, 2006; Zhang et al., 2009), and as an information source for tasks such as Statistical Machine Translation (Hassan et al., 2007). The use of parser output for supertagger training has been explored for LTAG by Sarkar (2007). However, the focus of that work was on improving parser and supertagger accuracy rather than speed. 346 Previously , watch imports S/S N , N /N N were N S [adj ]NP denied (S [dcl ]NP )/(S [pss]NP ) (S [pss]NP )/NP (S[dcl]NP)/NP S [pss]NP (S [dcl ]NP )/(S [adj ]NP ) (S [pss]NP )/NP such NP/NP duty-free treatment N/N N (N /N )/(N /N ) N /N (S [pt]NP )/NP (S[dcl]NP)/NP Figure 2: An example"
P10-1036,W96-0213,0,\N,Missing
P10-1036,W06-1620,0,\N,Missing
P10-1036,J05-1003,0,\N,Missing
P10-1036,P06-2006,0,\N,Missing
P11-1069,C04-1180,1,0.665729,"gives competitive accuracies compared to C&C. Considering our use of a small beam, and given the high ambiguity levels in an automatically-extracted grammar and the amount of information in the CCG lexical categories which form the shift actions, this is a surprising result. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman (2000)) is a lexicalised theory of grammar which has been successfully applied to a range of problems in NLP, including treebank creation (Hockenmaier and Steedman, 2007), syntactic parsing (Hockenmaier, 2003; Clark and Curran, 2007), logical form construction (Bos et al., 2004) and surface realization (White and Rajkumar, 2009). From a parsing perspective, the C&C parser (Clark and Curran, 2007) has been shown to be competitive with state-of-theart statistical parsers on a variety of test suites, including those consisting of grammatical relations (Clark and Curran, 2007), Penn Treebank phrase683 Stephen Clark University of Cambridge Computer Laboratory stephen.clark@cl.cam.ac.uk structure trees (Clark and Curran, 2009), and unbounded dependencies (Rimell et al., 2009). The binary branching nature of CCG means that it is naturally compatible with bottom-up parsing a"
P11-1069,J07-4004,1,0.750723,"e two parsers, and show that the shift-reduce parser gives competitive accuracies compared to C&C. Considering our use of a small beam, and given the high ambiguity levels in an automatically-extracted grammar and the amount of information in the CCG lexical categories which form the shift actions, this is a surprising result. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman (2000)) is a lexicalised theory of grammar which has been successfully applied to a range of problems in NLP, including treebank creation (Hockenmaier and Steedman, 2007), syntactic parsing (Hockenmaier, 2003; Clark and Curran, 2007), logical form construction (Bos et al., 2004) and surface realization (White and Rajkumar, 2009). From a parsing perspective, the C&C parser (Clark and Curran, 2007) has been shown to be competitive with state-of-theart statistical parsers on a variety of test suites, including those consisting of grammatical relations (Clark and Curran, 2007), Penn Treebank phrase683 Stephen Clark University of Cambridge Computer Laboratory stephen.clark@cl.cam.ac.uk structure trees (Clark and Curran, 2009), and unbounded dependencies (Rimell et al., 2009). The binary branching nature of CCG means that it is"
P11-1069,P09-2014,1,0.884833,"Missing"
P11-1069,P04-1015,0,0.0376825,"t are put on the agenda. Then the list is cleared and the parser moves on to the next step. This process repeats until the agenda is empty (which means that no new items have been generated in the previous step), and the candidate output is the final derivation. Pseudocode for the algorithm is shown in Figure 3. 687 5 Model and Training We use a global linear model to score candidate items, trained discriminatively with the averaged perceptron (Collins, 2002). Features for a (finished or partial) candidate are extracted from each action that have been applied to build the candidate. Following Collins and Roark (2004), we apply the “early update” strategy to perceptron training: at any step during decoding, if neither the candidate output nor any item in the agenda is correct, decoding is stopped and the parameters are updated using the current highest scored item in the agenda or the candidate output, whichever has the higher score. Table 1 shows the feature templates used by the parser. The symbols S0 , S1 , S2 and S3 in the table represent the top four nodes on the stack (if existent), and Q0 , Q1 , Q2 and Q3 represent the front four words in the incoming queue (if existent). S0 H and S1 H represent the"
P11-1069,W02-1001,0,0.0167382,"y generated partial candidates. After all candidate items from the agenda have been processed, the agenda is cleared and the N -best items from the list are put on the agenda. Then the list is cleared and the parser moves on to the next step. This process repeats until the agenda is empty (which means that no new items have been generated in the previous step), and the candidate output is the final derivation. Pseudocode for the algorithm is shown in Figure 3. 687 5 Model and Training We use a global linear model to score candidate items, trained discriminatively with the averaged perceptron (Collins, 2002). Features for a (finished or partial) candidate are extracted from each action that have been applied to build the candidate. Following Collins and Roark (2004), we apply the “early update” strategy to perceptron training: at any step during decoding, if neither the candidate output nor any item in the agenda is correct, decoding is stopped and the parameters are updated using the current highest scored item in the agenda or the candidate output, whichever has the higher score. Table 1 shows the feature templates used by the parser. The symbols S0 , S1 , S2 and S3 in the table represent the t"
P11-1069,P08-1109,0,0.0309352,"Missing"
P11-1069,P10-1035,0,0.0929355,"theart statistical parsers on a variety of test suites, including those consisting of grammatical relations (Clark and Curran, 2007), Penn Treebank phrase683 Stephen Clark University of Cambridge Computer Laboratory stephen.clark@cl.cam.ac.uk structure trees (Clark and Curran, 2009), and unbounded dependencies (Rimell et al., 2009). The binary branching nature of CCG means that it is naturally compatible with bottom-up parsing algorithms such as shift-reduce and CKY (Ades and Steedman, 1982; Steedman, 2000). However, the parsing work by Clark and Curran (2007), and also Hockenmaier (2003) and Fowler and Penn (2010), has only considered chart-parsing. In this paper we fill a gap in the CCG literature by developing a shiftreduce parser for CCG. Shift-reduce parsers have become popular for dependency parsing, building on the initial work of Yamada and Matsumoto (2003) and Nivre and Scholz (2004). One advantage of shift-reduce parsers is that the scoring model can be defined over actions, allowing highly efficient parsing by using a greedy algorithm in which the highest scoring action (or a small number of possible actions) is taken at each step. In addition, high accuracy can be maintained by using a model"
P11-1069,J07-3004,0,0.122469,"s with the chart-based C&C parser. We study different errors made by the two parsers, and show that the shift-reduce parser gives competitive accuracies compared to C&C. Considering our use of a small beam, and given the high ambiguity levels in an automatically-extracted grammar and the amount of information in the CCG lexical categories which form the shift actions, this is a surprising result. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman (2000)) is a lexicalised theory of grammar which has been successfully applied to a range of problems in NLP, including treebank creation (Hockenmaier and Steedman, 2007), syntactic parsing (Hockenmaier, 2003; Clark and Curran, 2007), logical form construction (Bos et al., 2004) and surface realization (White and Rajkumar, 2009). From a parsing perspective, the C&C parser (Clark and Curran, 2007) has been shown to be competitive with state-of-theart statistical parsers on a variety of test suites, including those consisting of grammatical relations (Clark and Curran, 2007), Penn Treebank phrase683 Stephen Clark University of Cambridge Computer Laboratory stephen.clark@cl.cam.ac.uk structure trees (Clark and Curran, 2009), and unbounded dependencies (Rimell et"
P11-1069,D09-1127,0,0.0122833,"Missing"
P11-1069,D07-1123,0,0.0178413,"shifted as an NP. Then “bought” is combined with its object “Lotus” resulting in a verb phrase looking for its subject on the left (S[dcl]NP). Finally, the resulting verb phrase is combined with its subject, resulting in a declarative sentence (S[dcl]). A key difference with previous work on shiftreduce dependency (Nivre et al., 2006) and CFG 686 Greedy local search (Yamada and Matsumoto, 2003; Sagae and Lavie, 2005; Nivre and Scholz, 2004) has typically been used for decoding in shift-reduce parsers, while beam-search has recently been applied as an alternative to reduce error-propagation (Johansson and Nugues, 2007; Zhang and Clark, 2008; Zhang and Clark, 2009; Huang et al., 2009). Both greedy local search and beam-search have linear time complexity. We use beam-search in our CCG parser. To formulate the decoding algorithm, we define a candidate item as a tuple hS, Q, F i, where S represents the stack with partial derivations that have been built, Q represents the queue of incoming words that have not been processed, and F is a boolean value that represents whether the candidate item has been finished. A candidate item is finished if and only if the FINISH action has been applied to it, and no more acti"
P11-1069,D07-1013,0,0.0286029,"Missing"
P11-1069,P05-1012,0,0.0343332,"l. (2009) (and Ninomiya et al. (2010)) describe a greedy shift-reduce parser for HPSG , in which a single action is chosen at each parsing step, allowing the possibility of highly efficient parsing. Since the HPSG grammar has relatively tight constraints, similar to CCG, the possibility arises that a spanning analysis cannot be found for some sentences. Our approach to this problem was to allow the parser to return a fragmentary analysis; Ninomiya et al. (2009) adopt a different approach based on default unification. Finally, our work is similar to the comparison of the chart-based MSTParser (McDonald et al., 2005) and shift-reduce MaltParser (Nivre et al., 2006) for dependency parsing. MSTParser can perform exhaustive search, given certain feature restrictions, because the complexity of the parsing task is lower than for constituent parsing. C&C can perform exhaustive search because the supertagger has already reduced the search space. We also found that approximate heuristic search for shift-reduce parsing, utilising a rich feature space, can match the performance of the optimal chart-based parser, as well as similar error profiles for the two CCG parsers compared to the two dependency parsers. 8 Conc"
P11-1069,P05-1011,0,0.0344085,"Missing"
P11-1069,E09-1069,0,0.051403,"search; we use a global model rather than local models chained together; and finally, our results surpass the best published results on the CCG parsing task, whereas Sagae and Lavie (2006a) matched the best PTB results only by using a parser combination. Matsuzaki et al. (2007) describes similar work to ours but using an automatically-extracted HPSG, rather than CCG, grammar. They also use the generalised perceptron to train a disambiguation model. One difference is that Matsuzaki et al. (2007) use an approximating CFG, in addition to the supertagger, to improve the efficiency of the parser. Ninomiya et al. (2009) (and Ninomiya et al. (2010)) describe a greedy shift-reduce parser for HPSG , in which a single action is chosen at each parsing step, allowing the possibility of highly efficient parsing. Since the HPSG grammar has relatively tight constraints, similar to CCG, the possibility arises that a spanning analysis cannot be found for some sentences. Our approach to this problem was to allow the parser to return a fragmentary analysis; Ninomiya et al. (2009) adopt a different approach based on default unification. Finally, our work is similar to the comparison of the chart-based MSTParser (McDonald"
P11-1069,C04-1010,0,0.187938,"ed onto the stack as an NP; then “bought” is shifted as a transitive verb looking for its object NP on the right and subject NP on the left ((S[dcl]NP)/NP); and then “Lotus” is shifted as an NP. Then “bought” is combined with its object “Lotus” resulting in a verb phrase looking for its subject on the left (S[dcl]NP). Finally, the resulting verb phrase is combined with its subject, resulting in a declarative sentence (S[dcl]). A key difference with previous work on shiftreduce dependency (Nivre et al., 2006) and CFG 686 Greedy local search (Yamada and Matsumoto, 2003; Sagae and Lavie, 2005; Nivre and Scholz, 2004) has typically been used for decoding in shift-reduce parsers, while beam-search has recently been applied as an alternative to reduce error-propagation (Johansson and Nugues, 2007; Zhang and Clark, 2008; Zhang and Clark, 2009; Huang et al., 2009). Both greedy local search and beam-search have linear time complexity. We use beam-search in our CCG parser. To formulate the decoding algorithm, we define a candidate item as a tuple hS, Q, F i, where S represents the stack with partial derivations that have been built, Q represents the queue of incoming words that have not been processed, and F is"
P11-1069,W06-2933,0,0.0134265,"Missing"
P11-1069,N07-1051,0,0.0312963,"Missing"
P11-1069,P02-1035,0,0.0292003,"the derivations, by extracting combinatory rule instances from the local trees consisting of a parent category and one or two child categories, and applying only those instances during parsing. (These rule instances also include rules to deal with punctuation and unary type-changing rules, in addition to instances of the combinatory rule schemas.) This is the method used by Hockenmaier (2003) and is the method we adopt in this paper. Fowler and Penn (2010) demonstrate that the second extraction method results in a context-free approximation to the grammar resulting from the first 2 1 See e.g. Riezler et al. (2002) and Zhang et al. (2007) for chartbased parsers which can produce fragmentary analyses. 684 Although the C&C default mode applies a restriction for efficiency reasons in which only rule instances seen in CCGbank can be applied, making the grammar of the second type. method, which has the potential to produce a mildlycontext sensitive grammar (given the existence of certain combinatory rules) (Weir, 1988). However, it is important to note that the advantages of CCG, in particular the tight relationship between syntax and semantic interpretation, are still maintained with the second approach, as"
P11-1069,D09-1085,1,0.891416,"Missing"
P11-1069,W05-1513,0,0.183074,"the word “IBM” is shifted onto the stack as an NP; then “bought” is shifted as a transitive verb looking for its object NP on the right and subject NP on the left ((S[dcl]NP)/NP); and then “Lotus” is shifted as an NP. Then “bought” is combined with its object “Lotus” resulting in a verb phrase looking for its subject on the left (S[dcl]NP). Finally, the resulting verb phrase is combined with its subject, resulting in a declarative sentence (S[dcl]). A key difference with previous work on shiftreduce dependency (Nivre et al., 2006) and CFG 686 Greedy local search (Yamada and Matsumoto, 2003; Sagae and Lavie, 2005; Nivre and Scholz, 2004) has typically been used for decoding in shift-reduce parsers, while beam-search has recently been applied as an alternative to reduce error-propagation (Johansson and Nugues, 2007; Zhang and Clark, 2008; Zhang and Clark, 2009; Huang et al., 2009). Both greedy local search and beam-search have linear time complexity. We use beam-search in our CCG parser. To formulate the decoding algorithm, we define a candidate item as a tuple hS, Q, F i, where S represents the stack with partial derivations that have been built, Q represents the queue of incoming words that have not"
P11-1069,P06-2089,0,0.0157233,"d is not directly comparable with ours, especially considering that their test set is smaller and potentially slightly easier. The final comparison is parser speed. The shiftreduce parser is linear-time (in both sentence length and beam size), and can analyse over 10 sentences per second on a 2GHz CPU, with a beam of 16, which compares very well with other constituency parsers. However, this is no faster than the chart690 based C&C parser, although speed comparisons are difficult because of implementation differences (C&C uses heavily engineered C++ with a focus on efficiency). 7 Related Work Sagae and Lavie (2006a) describes a shift-reduce parser for the Penn Treebank parsing task which uses best-first search to allow some ambiguity into the parsing process. Differences with our approach are that we use a beam, rather than best-first, search; we use a global model rather than local models chained together; and finally, our results surpass the best published results on the CCG parsing task, whereas Sagae and Lavie (2006a) matched the best PTB results only by using a parser combination. Matsuzaki et al. (2007) describes similar work to ours but using an automatically-extracted HPSG, rather than CCG, gra"
P11-1069,N06-2033,0,0.0165163,"d is not directly comparable with ours, especially considering that their test set is smaller and potentially slightly easier. The final comparison is parser speed. The shiftreduce parser is linear-time (in both sentence length and beam size), and can analyse over 10 sentences per second on a 2GHz CPU, with a beam of 16, which compares very well with other constituency parsers. However, this is no faster than the chart690 based C&C parser, although speed comparisons are difficult because of implementation differences (C&C uses heavily engineered C++ with a focus on efficiency). 7 Related Work Sagae and Lavie (2006a) describes a shift-reduce parser for the Penn Treebank parsing task which uses best-first search to allow some ambiguity into the parsing process. Differences with our approach are that we use a beam, rather than best-first, search; we use a global model rather than local models chained together; and finally, our results surpass the best published results on the CCG parsing task, whereas Sagae and Lavie (2006a) matched the best PTB results only by using a parser combination. Matsuzaki et al. (2007) describes similar work to ours but using an automatically-extracted HPSG, rather than CCG, gra"
P11-1069,D09-1043,0,0.00927653,"&C. Considering our use of a small beam, and given the high ambiguity levels in an automatically-extracted grammar and the amount of information in the CCG lexical categories which form the shift actions, this is a surprising result. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman (2000)) is a lexicalised theory of grammar which has been successfully applied to a range of problems in NLP, including treebank creation (Hockenmaier and Steedman, 2007), syntactic parsing (Hockenmaier, 2003; Clark and Curran, 2007), logical form construction (Bos et al., 2004) and surface realization (White and Rajkumar, 2009). From a parsing perspective, the C&C parser (Clark and Curran, 2007) has been shown to be competitive with state-of-theart statistical parsers on a variety of test suites, including those consisting of grammatical relations (Clark and Curran, 2007), Penn Treebank phrase683 Stephen Clark University of Cambridge Computer Laboratory stephen.clark@cl.cam.ac.uk structure trees (Clark and Curran, 2009), and unbounded dependencies (Rimell et al., 2009). The binary branching nature of CCG means that it is naturally compatible with bottom-up parsing algorithms such as shift-reduce and CKY (Ades and St"
P11-1069,W03-3023,0,0.162666,"e “IBM bought Lotus”. First the word “IBM” is shifted onto the stack as an NP; then “bought” is shifted as a transitive verb looking for its object NP on the right and subject NP on the left ((S[dcl]NP)/NP); and then “Lotus” is shifted as an NP. Then “bought” is combined with its object “Lotus” resulting in a verb phrase looking for its subject on the left (S[dcl]NP). Finally, the resulting verb phrase is combined with its subject, resulting in a declarative sentence (S[dcl]). A key difference with previous work on shiftreduce dependency (Nivre et al., 2006) and CFG 686 Greedy local search (Yamada and Matsumoto, 2003; Sagae and Lavie, 2005; Nivre and Scholz, 2004) has typically been used for decoding in shift-reduce parsers, while beam-search has recently been applied as an alternative to reduce error-propagation (Johansson and Nugues, 2007; Zhang and Clark, 2008; Zhang and Clark, 2009; Huang et al., 2009). Both greedy local search and beam-search have linear time complexity. We use beam-search in our CCG parser. To formulate the decoding algorithm, we define a candidate item as a tuple hS, Q, F i, where S represents the stack with partial derivations that have been built, Q represents the queue of incomi"
P11-1069,D08-1059,1,0.111383,"ght” is combined with its object “Lotus” resulting in a verb phrase looking for its subject on the left (S[dcl]NP). Finally, the resulting verb phrase is combined with its subject, resulting in a declarative sentence (S[dcl]). A key difference with previous work on shiftreduce dependency (Nivre et al., 2006) and CFG 686 Greedy local search (Yamada and Matsumoto, 2003; Sagae and Lavie, 2005; Nivre and Scholz, 2004) has typically been used for decoding in shift-reduce parsers, while beam-search has recently been applied as an alternative to reduce error-propagation (Johansson and Nugues, 2007; Zhang and Clark, 2008; Zhang and Clark, 2009; Huang et al., 2009). Both greedy local search and beam-search have linear time complexity. We use beam-search in our CCG parser. To formulate the decoding algorithm, we define a candidate item as a tuple hS, Q, F i, where S represents the stack with partial derivations that have been built, Q represents the queue of incoming words that have not been processed, and F is a boolean value that represents whether the candidate item has been finished. A candidate item is finished if and only if the FINISH action has been applied to it, and no more actions can be applied to a"
P11-1069,W09-3825,1,0.737168,"ts object “Lotus” resulting in a verb phrase looking for its subject on the left (S[dcl]NP). Finally, the resulting verb phrase is combined with its subject, resulting in a declarative sentence (S[dcl]). A key difference with previous work on shiftreduce dependency (Nivre et al., 2006) and CFG 686 Greedy local search (Yamada and Matsumoto, 2003; Sagae and Lavie, 2005; Nivre and Scholz, 2004) has typically been used for decoding in shift-reduce parsers, while beam-search has recently been applied as an alternative to reduce error-propagation (Johansson and Nugues, 2007; Zhang and Clark, 2008; Zhang and Clark, 2009; Huang et al., 2009). Both greedy local search and beam-search have linear time complexity. We use beam-search in our CCG parser. To formulate the decoding algorithm, we define a candidate item as a tuple hS, Q, F i, where S represents the stack with partial derivations that have been built, Q represents the queue of incoming words that have not been processed, and F is a boolean value that represents whether the candidate item has been finished. A candidate item is finished if and only if the FINISH action has been applied to it, and no more actions can be applied to a candidate item after i"
P11-1069,W07-1217,0,0.0179799,"ting combinatory rule instances from the local trees consisting of a parent category and one or two child categories, and applying only those instances during parsing. (These rule instances also include rules to deal with punctuation and unary type-changing rules, in addition to instances of the combinatory rule schemas.) This is the method used by Hockenmaier (2003) and is the method we adopt in this paper. Fowler and Penn (2010) demonstrate that the second extraction method results in a context-free approximation to the grammar resulting from the first 2 1 See e.g. Riezler et al. (2002) and Zhang et al. (2007) for chartbased parsers which can produce fragmentary analyses. 684 Although the C&C default mode applies a restriction for efficiency reasons in which only rule instances seen in CCGbank can be applied, making the grammar of the second type. method, which has the potential to produce a mildlycontext sensitive grammar (given the existence of certain combinatory rules) (Weir, 1988). However, it is important to note that the advantages of CCG, in particular the tight relationship between syntax and semantic interpretation, are still maintained with the second approach, as Fowler and Penn (2010)"
P13-2009,J93-2003,0,0.0286326,"ecoding: given any sequence of decorated MRL tokens, we can always reconstruct the corresponding tree structure (if one exists). Arity labeling additionally allows functions with variable numbers of arguments (e.g. cityid, which in some training examples is unary) to align with different natural language strings depending on context. LINEARIZE state border texa state1 next to1 state1 stateid1 texas0 ⇓ ALIGN state border texa state1 next to1 state1 stateid1 texas0 Alignment Following the linearization of the MRs, we find alignments between the MR tokens and the NL tokens using the IBM Model 4 (Brown et al., 1993). Once the alignment algorithm is run in both directions (NL to MRL, MRL to NL), we symmetrize the resulting alignments to obtain a consensus many-to-many alignment (Och and Ney, 2000; Koehn et al., 2005). ⇓ EXTRACT ( PHRASE ) h state , state1 i h state border , state1 border1 i h texa , state1 stateid1 texas0 i .. . ⇓ EXTRACT ( HIER ) [X] → hstate , state1 i Rule extraction From the many-to-many alignment we need to extract a translation rule table, consisting of corresponding phrases in NL and MRL. We consider a phrase-based translation model (Koehn et al., 2003) and a hierarchical translati"
P13-2009,P11-1149,0,0.0308621,"he use of machine translation methods as an informative baseline in semantic parsing evaluations, and suggest that research in semantic parsing could benefit from advances in machine translation. 1 Stephen Clark Computer Laboratory University of Cambridge sc609@cam.ac.uk Introduction Semantic parsing (SP) is the problem of transforming a natural language (NL) utterance into a machine-interpretable meaning representation (MR). It is well-studied in NLP, and a wide variety of methods have been proposed to tackle it, e.g. rule-based (Popescu et al., 2003), supervised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al., 2011). At least superficially, SP is simply a machine translation (MT) task: we transform an NL utterance in one language into a statement of another (un-natural) meaning representation language (MRL). Indeed, successful semantic parsers often resemble MT systems in several important respects, including the use of word alignment models as a starting point for rule extraction (Wong and Mooney, 2006; Kwiatkowski et al., 2010) and the use of automata such as tree transducers (Jones et al., 2012) to encode the relationship between NL and MRL. 2 MT-based semantic"
P13-2009,P12-1051,0,0.666025,"opescu et al., 2003), supervised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al., 2011). At least superficially, SP is simply a machine translation (MT) task: we transform an NL utterance in one language into a statement of another (un-natural) meaning representation language (MRL). Indeed, successful semantic parsers often resemble MT systems in several important respects, including the use of word alignment models as a starting point for rule extraction (Wong and Mooney, 2006; Kwiatkowski et al., 2010) and the use of automata such as tree transducers (Jones et al., 2012) to encode the relationship between NL and MRL. 2 MT-based semantic parsing The input is a corpus of NL utterances paired with MRs. In order to learn a semantic parser using MT we linearize the MRs, learn alignments between the MRL and the NL, extract translation rules, and learn a language model for the MRL. We also specify a decoding procedure that will return structured MRs for an utterance during prediction. 47 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 47–52, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguist"
P13-2009,N03-1017,0,0.0364768,"okens using the IBM Model 4 (Brown et al., 1993). Once the alignment algorithm is run in both directions (NL to MRL, MRL to NL), we symmetrize the resulting alignments to obtain a consensus many-to-many alignment (Och and Ney, 2000; Koehn et al., 2005). ⇓ EXTRACT ( PHRASE ) h state , state1 i h state border , state1 border1 i h texa , state1 stateid1 texas0 i .. . ⇓ EXTRACT ( HIER ) [X] → hstate , state1 i Rule extraction From the many-to-many alignment we need to extract a translation rule table, consisting of corresponding phrases in NL and MRL. We consider a phrase-based translation model (Koehn et al., 2003) and a hierarchical translation model (Chiang, 2005). Rules for the phrase-based model consist of pairs of aligned source and target sequences, while hierarchical rules are SCFG productions containing at most two instances of a single nonterminal symbol. Note that both extraction algorithms can learn rules which a traditional tree-transducer-based approach cannot—for example the right hand side [X] → hstate [X] texa , state1 [X] state1 stateid1 texas0 i .. . Figure 1: Illustration of preprocessing and rule extraction. Linearization We assume that the MRL is variable-free (that is, the meaning"
P13-2009,2005.iwslt-1.8,0,0.0119875,"ents (e.g. cityid, which in some training examples is unary) to align with different natural language strings depending on context. LINEARIZE state border texa state1 next to1 state1 stateid1 texas0 ⇓ ALIGN state border texa state1 next to1 state1 stateid1 texas0 Alignment Following the linearization of the MRs, we find alignments between the MR tokens and the NL tokens using the IBM Model 4 (Brown et al., 1993). Once the alignment algorithm is run in both directions (NL to MRL, MRL to NL), we symmetrize the resulting alignments to obtain a consensus many-to-many alignment (Och and Ney, 2000; Koehn et al., 2005). ⇓ EXTRACT ( PHRASE ) h state , state1 i h state border , state1 border1 i h texa , state1 stateid1 texas0 i .. . ⇓ EXTRACT ( HIER ) [X] → hstate , state1 i Rule extraction From the many-to-many alignment we need to extract a translation rule table, consisting of corresponding phrases in NL and MRL. We consider a phrase-based translation model (Koehn et al., 2003) and a hierarchical translation model (Chiang, 2005). Rules for the phrase-based model consist of pairs of aligned source and target sequences, while hierarchical rules are SCFG productions containing at most two instances of a singl"
P13-2009,P07-2045,0,0.00547291,"tely discard malformed MRs; for the experiments in this paper we simply filter the regular n-best list until we find a well-formed MR. This filtering can be done with time linear in the length of the example by exploiting the argument label numbers introduced during linearization. Finally, we insert the brackets according to the tree structure specified by the argument number labels. 3 Implementation In all experiments, we use the IBM Model 4 implementation from the GIZA++ toolkit (Och and Ney, 2000) for alignment, and the phrase-based and hierarchical models implemented in the Moses toolkit (Koehn et al., 2007) for rule extraction. The best symmetrization algorithm, translation and language model weights for each language are selected using cross-validation on the development set. In the case of English and German, we also found that stemming (Bird et al., 2009; Porter, 1980) was hepful in reducing data sparsity. 4 Results We first compare the results for the two translation rule extraction models, phrase-based and hierarchical (“MT-phrase” and “MT-hier” respectively in Table 1). We find that the hierarchical model performs better in all languages apart from Greek, indicating that the long-range reo"
P13-2009,D10-1119,0,0.0752731,"wide variety of methods have been proposed to tackle it, e.g. rule-based (Popescu et al., 2003), supervised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al., 2011). At least superficially, SP is simply a machine translation (MT) task: we transform an NL utterance in one language into a statement of another (un-natural) meaning representation language (MRL). Indeed, successful semantic parsers often resemble MT systems in several important respects, including the use of word alignment models as a starting point for rule extraction (Wong and Mooney, 2006; Kwiatkowski et al., 2010) and the use of automata such as tree transducers (Jones et al., 2012) to encode the relationship between NL and MRL. 2 MT-based semantic parsing The input is a corpus of NL utterances paired with MRs. In order to learn a semantic parser using MT we linearize the MRs, learn alignments between the MRL and the NL, extract translation rules, and learn a language model for the MRL. We also specify a decoding procedure that will return structured MRs for an utterance during prediction. 47 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 47–52, c Sofia,"
P13-2009,P11-1060,0,0.0670688,"formative baseline in semantic parsing evaluations, and suggest that research in semantic parsing could benefit from advances in machine translation. 1 Stephen Clark Computer Laboratory University of Cambridge sc609@cam.ac.uk Introduction Semantic parsing (SP) is the problem of transforming a natural language (NL) utterance into a machine-interpretable meaning representation (MR). It is well-studied in NLP, and a wide variety of methods have been proposed to tackle it, e.g. rule-based (Popescu et al., 2003), supervised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al., 2011). At least superficially, SP is simply a machine translation (MT) task: we transform an NL utterance in one language into a statement of another (un-natural) meaning representation language (MRL). Indeed, successful semantic parsers often resemble MT systems in several important respects, including the use of word alignment models as a starting point for rule extraction (Wong and Mooney, 2006; Kwiatkowski et al., 2010) and the use of automata such as tree transducers (Jones et al., 2012) to encode the relationship between NL and MRL. 2 MT-based semantic parsing The input is a corpus of NL utte"
P13-2009,D11-1149,0,0.0250669,"scribes a generative model over derivations of MRL trees. The remaining system discussed in this paper, UBL (Kwiatkowski et al., 2010), leverages the fact that the MRL does not simply encode trees, but rather λ-calculus expressions. It employs resolution procedures specific to the λ-calculus such as splitting and unification in order to generate rule templates. Like other systems described, it uses GIZA alignments for initialization. Other work which generalizes from variable-free meaning representations to λ-calculus expressions includes the natural language generation procedure described by Lu and Ng (2011). UBL , like an MT system (and unlike most of the other systems discussed in this section), extracts rules at multiple levels of granularity by means of this splitting and unification procedure. hybridtree similarly benefits from the introduction of We also compare the MT-based semantic parsers to several recently published ones: WASP (Wong and Mooney, 2006), which like the hierarchical model described here learns a SCFG to translate between NL and MRL; tsVB (Jones et al., 2012), which uses variational Bayesian inference to learn weights for a tree transducer; UBL (Kwiatkowski et al., 2010), w"
P13-2009,D08-1082,0,0.0842745,"ctic MT to extract rules. tsVB also uses a piece of standard MT machinery, specifically tree transducers, which have been profitably employed for syntax-based machine translation (Maletti, 2010). In that work, however, the usual MT parameter-estimation technique of simply counting the number of rule occurrences does not improve scores, and the authors instead resort to a variational inference procedure to acquire rule weights. The present work is also the first we are aware of which uses phrasebased rather than tree-based machine translation techniques to learn a semantic parser. hybrid-tree (Lu et al., 2008) similarly describes a generative model over derivations of MRL trees. The remaining system discussed in this paper, UBL (Kwiatkowski et al., 2010), leverages the fact that the MRL does not simply encode trees, but rather λ-calculus expressions. It employs resolution procedures specific to the λ-calculus such as splitting and unification in order to generate rule templates. Like other systems described, it uses GIZA alignments for initialization. Other work which generalizes from variable-free meaning representations to λ-calculus expressions includes the natural language generation procedure"
P13-2009,P00-1056,0,0.0603827,"le numbers of arguments (e.g. cityid, which in some training examples is unary) to align with different natural language strings depending on context. LINEARIZE state border texa state1 next to1 state1 stateid1 texas0 ⇓ ALIGN state border texa state1 next to1 state1 stateid1 texas0 Alignment Following the linearization of the MRs, we find alignments between the MR tokens and the NL tokens using the IBM Model 4 (Brown et al., 1993). Once the alignment algorithm is run in both directions (NL to MRL, MRL to NL), we symmetrize the resulting alignments to obtain a consensus many-to-many alignment (Och and Ney, 2000; Koehn et al., 2005). ⇓ EXTRACT ( PHRASE ) h state , state1 i h state border , state1 border1 i h texa , state1 stateid1 texas0 i .. . ⇓ EXTRACT ( HIER ) [X] → hstate , state1 i Rule extraction From the many-to-many alignment we need to extract a translation rule table, consisting of corresponding phrases in NL and MRL. We consider a phrase-based translation model (Koehn et al., 2003) and a hierarchical translation model (Chiang, 2005). Rules for the phrase-based model consist of pairs of aligned source and target sequences, while hierarchical rules are SCFG productions containing at most two"
P13-2009,N06-1056,0,0.940236,"l-studied in NLP, and a wide variety of methods have been proposed to tackle it, e.g. rule-based (Popescu et al., 2003), supervised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al., 2011). At least superficially, SP is simply a machine translation (MT) task: we transform an NL utterance in one language into a statement of another (un-natural) meaning representation language (MRL). Indeed, successful semantic parsers often resemble MT systems in several important respects, including the use of word alignment models as a starting point for rule extraction (Wong and Mooney, 2006; Kwiatkowski et al., 2010) and the use of automata such as tree transducers (Jones et al., 2012) to encode the relationship between NL and MRL. 2 MT-based semantic parsing The input is a corpus of NL utterances paired with MRs. In order to learn a semantic parser using MT we linearize the MRs, learn alignments between the MRL and the NL, extract translation rules, and learn a language model for the MRL. We also specify a decoding procedure that will return structured MRs for an utterance during prediction. 47 Proceedings of the 51st Annual Meeting of the Association for Computational Linguist"
P13-2009,P05-1033,0,\N,Missing
P14-1021,P11-1048,0,0.432956,"Missing"
P14-1021,P06-2006,0,0.0360593,"provides an elegant solution to the spurious ambiguity problem (Clark and Curran, 2007). Second, obtaining training data for dependencies is likely to be easier than for syntactic derivations, especially for incomplete data (Schneider et al., 2013). Clark and Curran (2006) show how the dependency model from Clark and Curran (2007) extends naturally to the partialtraining case, and also how to obtain dependency data cheaply from gold-standard lexical category sequences alone. And third, it has been argued that dependencies are an ideal representation for parser evaluation, especially for CCG (Briscoe and Carroll, 2006; Clark and Hockenmaier, 2002), and so optimizing for dependency recovery makes sense from an evaluation perspective. In this paper, we fill a gap in the literature by developing the first dependency model for a shiftreduce CCG parser. Shift-reduce parsing applies naturally to CCG (Zhang and Clark, 2011), and the left-to-right, incremental nature of the decoding fits with CCG’s cognitive claims. The discriminative model is global and trained with the structured perceptron. The decoder is based on beam-search This paper presents the first dependency model for a shift-reduce CCG parser. Modellin"
P14-1021,N06-1019,1,0.718846,"plying constraints on grammaticality, is the semantic interpretation. The early dependency model of Clark et al. (2002), in which model features were defined over only dependency structures, was partly motivated by these theoretical observations. More generally, dependency models are desirable for a number of reasons. First, modelling dependencies provides an elegant solution to the spurious ambiguity problem (Clark and Curran, 2007). Second, obtaining training data for dependencies is likely to be easier than for syntactic derivations, especially for incomplete data (Schneider et al., 2013). Clark and Curran (2006) show how the dependency model from Clark and Curran (2007) extends naturally to the partialtraining case, and also how to obtain dependency data cheaply from gold-standard lexical category sequences alone. And third, it has been argued that dependencies are an ideal representation for parser evaluation, especially for CCG (Briscoe and Carroll, 2006; Clark and Hockenmaier, 2002), and so optimizing for dependency recovery makes sense from an evaluation perspective. In this paper, we fill a gap in the literature by developing the first dependency model for a shiftreduce CCG parser. Shift-reduce"
P14-1021,N12-1015,0,0.0171276,"e arises from the potentially exponential number of derivations leading to a gold-standard dependency structure, which the oracle needs to keep track of. Our solution is an integration of a packed parse forest, which efficiently stores all the derivations, with the beam-search decoder at training time. The derivations are not explicitly part of the data, since the forest is built from the gold-standard dependencies. We also show how perceptron learning with beam-search (Collins and Roark, 2004) can be extended to handle the additional ambiguity, by adapting the “violationfixing” perceptron of Huang et al. (2012). Results on the standard CCGBank tests show that our parser achieves absolute labeled F-score gains of up to 0.5 over the shift-reduce parser of Zhang and Clark (2011); and up to 1.05 and 0.64 over the normal-form and hybrid models of Clark and Curran (2007), respectively. 2 0 1 2 3 4 5 6 7 8 9 stack (sn , ..., s1 , s0 ) N /N N /N N N NP NP (S [dcl]NP)/NP NP (S [dcl]NP)/NP N NP (S [dcl]NP)/NP NP NP S [dcl]NP S [dcl] queue (q0 , q1 , ..., qm ) Mr. President visited Paris President visited Paris visited Paris visited Paris visited Paris Paris action SHIFT SHIFT REDUCE UNARY SHIFT SHIFT UNAR"
P14-1021,J07-4004,1,0.0940301,"ry of CCG, in which Steedman (2000) argues that the derivation is merely a “trace” of the underlying syntactic process, and that the structure which is built, and predicated over when applying constraints on grammaticality, is the semantic interpretation. The early dependency model of Clark et al. (2002), in which model features were defined over only dependency structures, was partly motivated by these theoretical observations. More generally, dependency models are desirable for a number of reasons. First, modelling dependencies provides an elegant solution to the spurious ambiguity problem (Clark and Curran, 2007). Second, obtaining training data for dependencies is likely to be easier than for syntactic derivations, especially for incomplete data (Schneider et al., 2013). Clark and Curran (2006) show how the dependency model from Clark and Curran (2007) extends naturally to the partialtraining case, and also how to obtain dependency data cheaply from gold-standard lexical category sequences alone. And third, it has been argued that dependencies are an ideal representation for parser evaluation, especially for CCG (Briscoe and Carroll, 2006; Clark and Hockenmaier, 2002), and so optimizing for dependenc"
P14-1021,P08-1108,0,0.0223216,"en tracing out a single, correct derivation; then we describe how a model of normal-form derivations — or, more accurately, a sequence of shift-reduce actions leading to a normal-form derivation — can be used with beam-search to develop a nondeterministic parser which selects the highest scoring sequence of actions. Note this section only describes a normal-form derivation model for shiftreduce parsing. Section 3 explains how we extend the approach to dependency models. The shift-reduce algorithm adapted to CCG is similar to that of shift-reduce dependency parsing (Yamada and Matsumoto, 2003; Nivre and McDonald, 2008; Zhang and Clark, 2008; Huang and Sagae, 2010). Following Zhang and Clark (2011), we define each item in the parser as a pair hs, qi, where q is a queue of remaining input, consisting of words and a set of possible lexical categories for each word (with q0 being the front word), and s is the stack that holds subtrees s0 , s1 , ... (with s0 at the top). Subtrees on the stack are partial deriva1 See Hockenmaier (2003) and Clark and Curran (2007) for a description of CCG rules. 219 Mr. President visited Paris N /N N (S [dcl ]NP )/NP NP N NP &gt; S [dcl ]NP &gt;TC Mr. N /N President visited Paris N ("
P14-1021,P02-1042,1,0.258244,"Computer Laboratory sc609@cam.ac.uk Abstract and Curran, 2007) is to model derivations directly, restricting the gold-standard to be the normal-form derivations (Eisner, 1996) from CCGBank (Hockenmaier and Steedman, 2007). Modelling dependencies, as a proxy for the semantic interpretation, fits well with the theory of CCG, in which Steedman (2000) argues that the derivation is merely a “trace” of the underlying syntactic process, and that the structure which is built, and predicated over when applying constraints on grammaticality, is the semantic interpretation. The early dependency model of Clark et al. (2002), in which model features were defined over only dependency structures, was partly motivated by these theoretical observations. More generally, dependency models are desirable for a number of reasons. First, modelling dependencies provides an elegant solution to the spurious ambiguity problem (Clark and Curran, 2007). Second, obtaining training data for dependencies is likely to be easier than for syntactic derivations, especially for incomplete data (Schneider et al., 2013). Clark and Curran (2006) show how the dependency model from Clark and Curran (2007) extends naturally to the partialtrai"
P14-1021,C04-1010,0,0.0382713,"nitial item hs, qi0 (row 0), which has an empty stack and a full queue, a total of nine actions are applied to produce the complete derivation. Applying beam-search to a statistical shiftreduce parser is a straightforward extension to the deterministic example. At each step, a beam is used to store the top-k highest-scoring items, resulting from expanding all items in the previous beam. An item becomes a candidate output once it has an empty queue, and the parser keeps track of the highest scored candidate output and returns the best one as the final output. Compared with greedy local-search (Nivre and Scholz, 2004), the use of a beam allows the parser to explore a larger search space and delay difficult ambiguity-resolving decisions by considering multiple items in parallel. We refer to the shift-reduce model of Zhang and Clark (2011) as the normal-form model, where the oracle for each sentence specifies a unique sequence of gold-standard actions which produces the corresponding normal-form derivation. No dependency structures are involved at training and test time, except for evaluation. In the next section, we describe a dependency oracle which considers all sequences of actions producing a goldstanda"
P14-1021,P04-1015,0,0.624603,"paper is a novel technique for training the parser using a dependency oracle, in which all derivations are hidden. A challenge arises from the potentially exponential number of derivations leading to a gold-standard dependency structure, which the oracle needs to keep track of. Our solution is an integration of a packed parse forest, which efficiently stores all the derivations, with the beam-search decoder at training time. The derivations are not explicitly part of the data, since the forest is built from the gold-standard dependencies. We also show how perceptron learning with beam-search (Collins and Roark, 2004) can be extended to handle the additional ambiguity, by adapting the “violationfixing” perceptron of Huang et al. (2012). Results on the standard CCGBank tests show that our parser achieves absolute labeled F-score gains of up to 0.5 over the shift-reduce parser of Zhang and Clark (2011); and up to 1.05 and 0.64 over the normal-form and hybrid models of Clark and Curran (2007), respectively. 2 0 1 2 3 4 5 6 7 8 9 stack (sn , ..., s1 , s0 ) N /N N /N N N NP NP (S [dcl]NP)/NP NP (S [dcl]NP)/NP N NP (S [dcl]NP)/NP NP NP S [dcl]NP S [dcl] queue (q0 , q1 , ..., qm ) Mr. President visited Paris"
P14-1021,C10-1094,0,0.00918542,"over three existing, competitive CCG parsing models. 1 Yue Zhang Singapore University of Technology and Design yue zhang@sutd.edu.sg Introduction Combinatory Categorial Grammar (CCG; Steedman (2000)) is able to derive typed dependency structures (Hockenmaier, 2003; Clark and Curran, 2007), providing a useful approximation to the underlying predicate-argument relations of “who did what to whom”. To date, CCG remains the most competitive formalism for recovering “deep” dependencies arising from many linguistic phenomena such as raising, control, extraction and coordination (Rimell et al., 2009; Nivre et al., 2010). To achieve its expressiveness, CCG exhibits so-called “spurious” ambiguity, permitting many non-standard surface derivations which ease the recovery of certain dependencies, especially those arising from type-raising and composition. But this raises the question of what is the most suitable model for CCG: should we model the derivations, the dependencies, or both? The choice for some existing parsers (Hockenmaier, 2003; Clark 218 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 218–227, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Associatio"
P14-1021,W02-1001,0,0.422492,"if there is a disjunctive node d ∈ D(c) s.t. clex0 ∈ γ(d). With this implementation, the complexity of checking each valid SHIFT action is then O(|pL (cs0 )|). Definition 7. Let hs, qi be an item and let hs0 , q 0 i = hs, qi ◦ (x, c). We define the shared ancestor set R(cs01 , cs00 ) of cs00 , after applying action (x, c), as: • {c0 |c0 ∈ pL (cs0 ) ∩ A(c)}, if s is frontier and x = 3.3 SHIFT 0 0 • {c |c ∈ pL (cs0 ) ∩ A(c) and there is some c ∈ R(cs1 , cs0 ) s.t. c00 ∈ A(c0 )}, if s is non-frontier and x = SHIFT • {c0 |c0 ∈ R(cs2 , cs1 ) ∩ A(c)}, if x = Training We use the averaged perceptron (Collins, 2002) to train a global linear model and score each action. The normal-form model of Zhang and Clark (2011) uses an early update mechanism (Collins and Roark, 2004), where decoding is stopped to update model weights whenever the single gold action falls outside the beam. In our parser, there can be multiple gold items in a beam. One option would be to apply early update whenever at least 00 REDUCE • {c0 |c0 ∈ R(cs1 , cs0 ) ∩ A(c)}, if s is non-frontier and x = UNARY • R(, c0s0 ) = ∅ where c0s0 is the conjunctive node corresponding to the gold-standard lexical category of the 223 Algorithm 3 Depend"
P14-1021,D09-1085,1,0.84161,"F-score improvements over three existing, competitive CCG parsing models. 1 Yue Zhang Singapore University of Technology and Design yue zhang@sutd.edu.sg Introduction Combinatory Categorial Grammar (CCG; Steedman (2000)) is able to derive typed dependency structures (Hockenmaier, 2003; Clark and Curran, 2007), providing a useful approximation to the underlying predicate-argument relations of “who did what to whom”. To date, CCG remains the most competitive formalism for recovering “deep” dependencies arising from many linguistic phenomena such as raising, control, extraction and coordination (Rimell et al., 2009; Nivre et al., 2010). To achieve its expressiveness, CCG exhibits so-called “spurious” ambiguity, permitting many non-standard surface derivations which ease the recovery of certain dependencies, especially those arising from type-raising and composition. But this raises the question of what is the most suitable model for CCG: should we model the derivations, the dependencies, or both? The choice for some existing parsers (Hockenmaier, 2003; Clark 218 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 218–227, c Baltimore, Maryland, USA, June 23-25"
P14-1021,P96-1011,0,0.436747,"Missing"
P14-1021,W13-2307,0,0.038113,"Missing"
P14-1021,P10-1035,0,0.130723,"t our shift-reduce parser on top of the core C & C code base (Clark and Curran, 2007) and evaluate it against the shift-reduce parser of Zhang and Clark (2011) (henceforth Z & C) and the chartbased normal-form and hybrid models of Clark and Curran (2007). For all experiments, we use CCGBank with the standard split: sections 2-21 for training (39,604 sentences), section 00 for development (1,913 sentences) and section 23 (2,407 sentences) for testing. The way that the CCG grammar is implemented in C & C has some implications for our parser. First, unlike Z & C, which uses a context-free cover (Fowler and Penn, 2010) and hence is able to use all sentences in the training data, we are only able to use 36,036 sentences. The reason is that the grammar in C & C does not have complete coverage of CCGBank, due to the fact that e.g. not all rules in CCGBank conform to the combinatory rules of CCG. Second, our parser uses the unification mechanism from C & C to output dependencies directly, and hence does not need a separate postprocessing step to convert derivations into CCG dependencies, as required by Z & C. The feature templates of our model consist of all of those in Z & C, except the ones which require lexi"
P14-1021,W03-3023,0,0.090749,"ich a parser would follow when tracing out a single, correct derivation; then we describe how a model of normal-form derivations — or, more accurately, a sequence of shift-reduce actions leading to a normal-form derivation — can be used with beam-search to develop a nondeterministic parser which selects the highest scoring sequence of actions. Note this section only describes a normal-form derivation model for shiftreduce parsing. Section 3 explains how we extend the approach to dependency models. The shift-reduce algorithm adapted to CCG is similar to that of shift-reduce dependency parsing (Yamada and Matsumoto, 2003; Nivre and McDonald, 2008; Zhang and Clark, 2008; Huang and Sagae, 2010). Following Zhang and Clark (2011), we define each item in the parser as a pair hs, qi, where q is a queue of remaining input, consisting of words and a set of possible lexical categories for each word (with q0 being the front word), and s is the stack that holds subtrees s0 , s1 , ... (with s0 at the top). Subtrees on the stack are partial deriva1 See Hockenmaier (2003) and Clark and Curran (2007) for a description of CCG rules. 219 Mr. President visited Paris N /N N (S [dcl ]NP )/NP NP N NP &gt; S [dcl ]NP &gt;TC Mr. N /N P"
P14-1021,C12-1059,0,0.0227654,"ts all derivations as hidden, and defines a probabilistic model for a dependency structure by summing probabilities of all derivations leading to a particular structure. Features are defined over both derivations and CCG predicate-argument dependencies. We follow a similar approach, but rather than define a probabilistic model (which requires summing), we define a linear model over sequences of shiftreduce actions, as for the normal-form shift-reduce model. However, the difference compared to the normal-form model is that we do not assume a single gold-standard sequence of actions. Similar to Goldberg and Nivre (2012), we define an oracle which determines, for a goldstandard dependency structure, G, what the valid transition sequences are (i.e. those sequences corresponding to derivations leading to G). More specifically, the oracle can determine, given G and an item hs, qi, what the valid actions are for that item (i.e. what actions can potentially lead to G, starting with hs, qi and the dependencies already built on s). However, there can be exponentially many valid action sequences for G, which we represent efficiently using a packed parse forest. We show how the forest can be used, during beamsearch de"
P14-1021,D13-1112,0,0.0389206,"Missing"
P14-1021,P13-2111,0,0.0183734,"d surface derivations which ease the recovery of certain dependencies, especially those arising from type-raising and composition. But this raises the question of what is the most suitable model for CCG: should we model the derivations, the dependencies, or both? The choice for some existing parsers (Hockenmaier, 2003; Clark 218 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 218–227, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics step (Zhang and Clark, 2008) with the advantage of linear-time decoding (Goldberg et al., 2013). A main contribution of the paper is a novel technique for training the parser using a dependency oracle, in which all derivations are hidden. A challenge arises from the potentially exponential number of derivations leading to a gold-standard dependency structure, which the oracle needs to keep track of. Our solution is an integration of a packed parse forest, which efficiently stores all the derivations, with the beam-search decoder at training time. The derivations are not explicitly part of the data, since the forest is built from the gold-standard dependencies. We also show how perceptro"
P14-1021,D08-1059,1,0.142872,"xhibits so-called “spurious” ambiguity, permitting many non-standard surface derivations which ease the recovery of certain dependencies, especially those arising from type-raising and composition. But this raises the question of what is the most suitable model for CCG: should we model the derivations, the dependencies, or both? The choice for some existing parsers (Hockenmaier, 2003; Clark 218 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 218–227, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics step (Zhang and Clark, 2008) with the advantage of linear-time decoding (Goldberg et al., 2013). A main contribution of the paper is a novel technique for training the parser using a dependency oracle, in which all derivations are hidden. A challenge arises from the potentially exponential number of derivations leading to a gold-standard dependency structure, which the oracle needs to keep track of. Our solution is an integration of a packed parse forest, which efficiently stores all the derivations, with the beam-search decoder at training time. The derivations are not explicitly part of the data, since the forest is bu"
P14-1021,J07-3004,0,0.14906,"Missing"
P14-1021,P11-1069,1,0.337827,"m Clark and Curran (2007) extends naturally to the partialtraining case, and also how to obtain dependency data cheaply from gold-standard lexical category sequences alone. And third, it has been argued that dependencies are an ideal representation for parser evaluation, especially for CCG (Briscoe and Carroll, 2006; Clark and Hockenmaier, 2002), and so optimizing for dependency recovery makes sense from an evaluation perspective. In this paper, we fill a gap in the literature by developing the first dependency model for a shiftreduce CCG parser. Shift-reduce parsing applies naturally to CCG (Zhang and Clark, 2011), and the left-to-right, incremental nature of the decoding fits with CCG’s cognitive claims. The discriminative model is global and trained with the structured perceptron. The decoder is based on beam-search This paper presents the first dependency model for a shift-reduce CCG parser. Modelling dependencies is desirable for a number of reasons, including handling the “spurious” ambiguity of CCG; fitting well with the theory of CCG; and optimizing for structures which are evaluated at test time. We develop a novel training technique using a dependency oracle, in which all derivations are hidde"
P14-1021,W05-1506,0,0.0337318,"N /N , sh N , re N , un NP , sh (S [dcl ]NP )/NP , sh NP , re S [dcl ]NP , re S [dcl ].3 The order of traversal is left-child, right-child and parent. For a single parse, the corresponding shift-reduce action sequence is unique, and for a given item this canonical order restricts the possible derivations that can be formed using further actions. We now extend this observation to the more general case of an oracle forest, where there may be more than one gold-standard action for a given item. Definition 1. Given a gold-standard dependency 2 Under the hypergraph framework (Gallo et al., 1993; Huang and Chiang, 2005), a conjunctive node corresponds to a hyperedge and a disjunctive node corresponds to the head of a hyperedge or hyperedge bundle. 3 The derivation is “upside down”, following the convention used for CCG, where the root is S [dcl ]. We use sh, re and un to denote the three types of shift-reduce action. 221 Mr. President visited Paris N/N N (S [dcl ]NP )/NP NP Mr. President visited Paris N /N N (S [dcl ]NP )/NP NP N &gt; S[dcl]NP &gt; S[dcl]NP (a) &gt; (b) Figure 3: Example subtrees on two stacks, with two subtrees in (a) and three in (b); roots of subtrees are in bold. It is trivial to determine th"
P14-1021,P10-1110,0,\N,Missing
P14-2135,N09-1003,0,0.0572959,"Missing"
P14-2135,P12-1092,0,0.0431552,"e average pairwise cosine distance between all the image representations {w~1 . . . w~n } in the set of images for that concept: d(w) = X 1 w ~i · w~j 1− 2n(n − 1) i<j≤n |w ~i ||w~j | (1) We use an average pairwise distance-based metric because this emphasizes the total variation more than e.g. the mean distance from the centroid. In all experiments we set n = 50. Generating Visual Representations Visual vector representations for each image were obtained using the well-known bag of visual words (BoVW) approach (Sivic and Zisserman, 2003). BoVW obtains a vector representation for an 836 e.g. (Huang et al., 2012; Bruni et al., 2012)). As a complementary gold-standard, we use the University of South Florida Norms (USF) (Nelson et al., 2004). This dataset contains scores for free association, an experimental measure of cognitive association, between over 40,000 concept pairs. The USF norms have been used in many previous studies to evaluate semantic representations (Andrews et al., 2009; Feng and Lapata, 2010; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). The USF evaluation set is particularly appropriate in the present context because concepts in the dataset are also rated for concept"
P14-2135,Y08-1023,0,0.0218403,"vements on other tasks in semantic processing and representation. Table 1: Concepts with highest and lowest image dispersion scores in our evaluation set, and concreteness ratings from the USF dataset. the very abstract or very concrete concepts. As Table 1 illustrates, the concepts with the lowest dispersion in this sample are, without exception, highly concrete, and the concepts of highest dispersion are clearly very abstract. It should be noted that all previous approaches to the automatic measurement of concreteness rely on annotator ratings, dictionaries or manuallyconstructed resources. Kwong (2008) proposes a method based on the presence of hard-coded phrasal features in dictionary entries corresponding to each concept. By contrast, S´anchez et al. (2011) present an approach based on the position of word senses corresponding to each concept in the WordNet ontology (Fellbaum, 1999). Turney et al. (2011) propose a method that extends a large set of concreteness ratings similar to those in the USF dataset. The Turney et al. algorithm quantifies the concreteness of concepts that lack such a rating based on their proximity to rated concepts in a semantic vector space. In contrast to each of"
P14-2135,C94-1103,0,0.022202,"et al., 2007). We resize the images in our dataset to 100x100 pixels and compute PHOW descriptors using VLFeat (Vedaldi and Fulkerson, 2008). The descriptors for the images were subsequently clustered using mini-batch k-means (Sculley, 2010) with k = 50 to obtain histograms of visual words, yielding 50-dimensional visual vectors for each of the images. Generating Linguistic Representations We extract continuous vector representations (also of 50 dimensions) for concepts using the continuous log-linear skipgram model of Mikolov et al. (2013a), trained on the 100M word British National Corpus (Leech et al., 1994). This model learns high quality lexical semantic representations based on the distributional properties of words in text, and has been shown to outperform simple distributional models on applications such as semantic composition and analogical mapping (Mikolov et al., 2013b). 2.2 3 Improving Multi-Modal Representations We apply image dispersion-based filtering as follows: if both concepts in an evaluation pair have an image dispersion below a given threshold, both the linguistic and the visual representations are included. If not, in accordance with the Dual Coding Theory of human concept pro"
P14-2135,R11-1055,0,0.208132,"ments focus on multi-modal models that extract their perceptual input automatically from images. Image-based models more naturally mirror the process of human concept acquisition than those whose input derives from experimental datasets or expert annotation. They are also more scalable since high-quality tagged images are freely available in several web-scale image datasets. We use Google Images as our image source, and extract the first n image results for each concept word. It has been shown that images from Google yield higher-quality representations than comparable sources such as Flickr (Bergsma and Goebel, 2011). Other potential sources, such as ImageNet (Deng et al., 2009) or the ESP Game Dataset (Von Ahn and Dabbish, 2004), either do not contain images for abstract concepts or do not contain sufficient images for the concepts in our evaluation sets. 2.1 Figure 1: Example images for a concrete (elephant – little diversity, low dispersion) and an abstract concept (happiness – greater diversity, high dispersion). Figure 2: Computation of PHOW descriptors using dense SIFT for levels l = 0 to l = 2 and the corresponding histogram representations (Bosch et al., 2007). Image Dispersion-Based Filtering Fol"
P14-2135,P12-1015,0,0.642782,"be applied to a variety of other NLP tasks. 1 Introduction Multi-modal models that learn semantic concept representations from both linguistic and perceptual input were originally motivated by parallels with human concept acquisition, and evidence that many concepts are grounded in the perceptual system (Barsalou et al., 2003). Such models extract information about the perceptible characteristics of words from data collected in property norming experiments (Roller and Schulte im Walde, 2013; Silberer and Lapata, 2012) or directly from ‘raw’ data sources such as images (Feng and Lapata, 2010; Bruni et al., 2012). This input is combined with information from linguistic corpora to produce enhanced representations of concept meaning. Multi-modal models outperform languageonly models on a range of tasks, including modelling conceptual association and predicting compositionality (Bruni et al., 2012; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). Despite these results, the advantage of multimodal over linguistic-only models has only been 835 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 835–841, c Baltimore, Maryland, USA, June"
P14-2135,D13-1115,0,0.470058,"Missing"
P14-2135,N10-1011,0,0.377871,"on image data, and can be applied to a variety of other NLP tasks. 1 Introduction Multi-modal models that learn semantic concept representations from both linguistic and perceptual input were originally motivated by parallels with human concept acquisition, and evidence that many concepts are grounded in the perceptual system (Barsalou et al., 2003). Such models extract information about the perceptible characteristics of words from data collected in property norming experiments (Roller and Schulte im Walde, 2013; Silberer and Lapata, 2012) or directly from ‘raw’ data sources such as images (Feng and Lapata, 2010; Bruni et al., 2012). This input is combined with information from linguistic corpora to produce enhanced representations of concept meaning. Multi-modal models outperform languageonly models on a range of tasks, including modelling conceptual association and predicting compositionality (Bruni et al., 2012; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). Despite these results, the advantage of multimodal over linguistic-only models has only been 835 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 835–841, c Baltimore"
P14-2135,D12-1130,0,0.48871,"lti-modal models to learn and represent word meanings. The method relies solely on image data, and can be applied to a variety of other NLP tasks. 1 Introduction Multi-modal models that learn semantic concept representations from both linguistic and perceptual input were originally motivated by parallels with human concept acquisition, and evidence that many concepts are grounded in the perceptual system (Barsalou et al., 2003). Such models extract information about the perceptible characteristics of words from data collected in property norming experiments (Roller and Schulte im Walde, 2013; Silberer and Lapata, 2012) or directly from ‘raw’ data sources such as images (Feng and Lapata, 2010; Bruni et al., 2012). This input is combined with information from linguistic corpora to produce enhanced representations of concept meaning. Multi-modal models outperform languageonly models on a range of tasks, including modelling conceptual association and predicting compositionality (Bruni et al., 2012; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). Despite these results, the advantage of multimodal over linguistic-only models has only been 835 Proceedings of the 52nd Annual Meeting of the Associatio"
P14-2135,D11-1063,0,0.245846,"s spectrum), we observed a high correlation between abstractness and dispersion (Spearman ρ = 0.61, p < 0.001). On this more diverse sample, which reflects the range of concepts typically found in linguistic corpora, image dispersion is a particularly useful diagnostic for identifying Concreteness and Image Dispersion The filtering approach described thus far improves multi-modal representations because image dispersion provides a means to distinguish concrete concepts from more abstract concepts. Since research has demonstrated the applicability of concreteness to a range of other NLP tasks (Turney et al., 2011; Kwong, 2008), it is important to examine the connection between image dispersion and concreteness in more detail. 4.1 Quantifying Concreteness To evaluate the effectiveness of image dispersion as a proxy for concreteness we evaluated our algorithm on a binary classification task based on the set of 100 concrete and 100 abstract concepts A∪C introduced in Section 2. By classifying con838 Concept shirt bed knife dress car ego nonsense memory potential know Image Dispersion .488 .495 .560 .578 .580 1.000 .999 .999 .997 .996 Conc. (USF) 6.05 5.91 6.08 6.59 6.35 1.93 1.90 1.78 1.90 2.70 racy of 7"
P14-2135,W13-2609,1,\N,Missing
P15-2020,W11-0112,0,0.0323545,"for lexical entailment detection by examining a concept’s generality. We introduce three unsupervised methods for determining a concept’s generality, based on its related images, and obtain state-ofthe-art performance on two standard semantic evaluation datasets. We also introduce a novel task that combines hypernym detection and directionality, significantly outperforming a competitive frequencybased baseline. 1 Introduction Automatic detection of lexical entailment is useful for a number of NLP tasks including search query expansion (Shekarpour et al., 2013), recognising textual entailment (Garrette et al., 2011), metaphor detection (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Given two semantically related words, a key aspect of detecting lexical entailment, or the hyponym-hypernym relation, is the generality of the hypernym compared to the hyponym. For example, bird is more general than eagle, having a broader intension and a larger extension. This property has led to the introduction of lexical entailment measures that compare the entropy of distributional word representations, under the assumption that a more general term has a higher-entropy distribution (Herbelot and Gan"
P15-2020,P05-1014,0,0.905176,"Missing"
P15-2020,P13-2078,0,0.234332,"te et al., 2011), metaphor detection (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Given two semantically related words, a key aspect of detecting lexical entailment, or the hyponym-hypernym relation, is the generality of the hypernym compared to the hyponym. For example, bird is more general than eagle, having a broader intension and a larger extension. This property has led to the introduction of lexical entailment measures that compare the entropy of distributional word representations, under the assumption that a more general term has a higher-entropy distribution (Herbelot and Ganesalingam, 2013; Santus et al., 2014). A strand of distributional semantics has recently emerged that exploits the fact that meaning is often grounded in the perceptual system, known as multi-modal distributional semantics (Bruni et al., 2014). Such models enhance purely linguistic models with extra-linguistic perceptual information, and outperform language-only models on a 2 Related Work In the linguistic modality, the most closely related work is by Herbelot and Ganesalingam (2013) and Santus et al. (2014), who use unsupervised distributional generality measures to identify the hypernym in a hyponym-hypern"
P15-2020,W98-0718,0,0.195269,"Missing"
P15-2020,D14-1005,1,0.765841,"ailment Detection Douwe Kiela Computer Laboratory University of Cambridge douwe.kiela@cl.cam.ac.uk Laura Rimell Computer Laboratory University of Cambridge laura.rimell@cl.cam.ac.uk Ivan Vuli´c Department of Computer Science KU Leuven ivan.vulic@cs.kuleuven.be Stephen Clark Computer Laboratory University of Cambridge stephen.clark@cl.cam.ac.uk Abstract range of tasks, including modelling semantic similarity and conceptual relatedness (Silberer and Lapata, 2014). In fact, under some conditions uni-modal visual representations outperform traditional linguistic representations on semantic tasks (Kiela and Bottou, 2014). We hypothesize that visual representations can be particularly useful for lexical entailment detection. Deselaers and Ferrari (2011) have shown that sets of images corresponding to terms at higher levels in the WordNet hierarchy have greater visual variability than those at lower levels. We exploit this tendency using sets of images returned by Google’s image search. The intuition is that the set of images returned for animal will consist of pictures of different kinds of animals, the set of images for bird will consist of pictures of different birds, while the set for owl will mostly consis"
P15-2020,W11-2501,0,0.154227,"ging dataset BIBLESS . Examples of pairs in the respective datasets can be found in Table 1. Approach We use two standard evaluations for lexical entailment: hypernym directionality, where the task is to predict which of two words is the hypernym; and hypernym detection, where the task is to predict whether two words are in a hypernym-hyponym relation (Weeds et al., 2014; Santus et al., 2014). We also introduce a third, more challenging, evaluation that combines detection and directionality. For the directionality experiment, we evaluate on the hypernym subset of the well-known BLESS dataset (Baroni and Lenci, 2011), which consists of 1337 hyponym-hypernym pairs. In this case, it is known that the words are in an entailment relation and the task is to predict the directionality of the relation. BLESS data is always presented with the hyponym first, so we report how often our measures predict that the second term in the pair is more general than the first. For the detection experiment, we evaluate on the BLESS -based dataset of Weeds et al. (2014), which consists of 1168 word pairs and which we call WBLESS . In this dataset, the positive examples are hyponym-hypernym pairs. The negative examples 3.1 Image"
P15-2020,P14-2135,1,0.908604,"in an entailment relation and the task is to predict the directionality of the relation. BLESS data is always presented with the hyponym first, so we report how often our measures predict that the second term in the pair is more general than the first. For the detection experiment, we evaluate on the BLESS -based dataset of Weeds et al. (2014), which consists of 1168 word pairs and which we call WBLESS . In this dataset, the positive examples are hyponym-hypernym pairs. The negative examples 3.1 Image representations Following previous work in multi-modal semantics (Bergsma and Goebel, 2011; Kiela et al., 2014), we obtain images from Google Images1 for the words in the evaluation datasets. It has been shown that images from Google yield higherquality representations than comparable resources such as Flickr and are competitive with “hand prepared datasets” (Bergsma and Goebel, 2011; Fergus et al., 2005). 1 www.google.com/imghp. Images were retrieved on 10 April, 2015 from Cambridge in the United Kingdom. 120 For each image, we extract the pre-softmax layer from a forward pass in a convolutional neural network (CNN) that has been trained on the ImageNet classification task using Caffe (Jia et al., 201"
P15-2020,R11-1055,0,0.077426,"s known that the words are in an entailment relation and the task is to predict the directionality of the relation. BLESS data is always presented with the hyponym first, so we report how often our measures predict that the second term in the pair is more general than the first. For the detection experiment, we evaluate on the BLESS -based dataset of Weeds et al. (2014), which consists of 1168 word pairs and which we call WBLESS . In this dataset, the positive examples are hyponym-hypernym pairs. The negative examples 3.1 Image representations Following previous work in multi-modal semantics (Bergsma and Goebel, 2011; Kiela et al., 2014), we obtain images from Google Images1 for the words in the evaluation datasets. It has been shown that images from Google yield higherquality representations than comparable resources such as Flickr and are competitive with “hand prepared datasets” (Bergsma and Goebel, 2011; Fergus et al., 2005). 1 www.google.com/imghp. Images were retrieved on 10 April, 2015 from Cambridge in the United Kingdom. 120 For each image, we extract the pre-softmax layer from a forward pass in a convolutional neural network (CNN) that has been trained on the ImageNet classification task using C"
P15-2020,I13-1095,0,0.207838,"e unsupervised methods for determining a concept’s generality, based on its related images, and obtain state-ofthe-art performance on two standard semantic evaluation datasets. We also introduce a novel task that combines hypernym detection and directionality, significantly outperforming a competitive frequencybased baseline. 1 Introduction Automatic detection of lexical entailment is useful for a number of NLP tasks including search query expansion (Shekarpour et al., 2013), recognising textual entailment (Garrette et al., 2011), metaphor detection (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Given two semantically related words, a key aspect of detecting lexical entailment, or the hyponym-hypernym relation, is the generality of the hypernym compared to the hyponym. For example, bird is more general than eagle, having a broader intension and a larger extension. This property has led to the introduction of lexical entailment measures that compare the entropy of distributional word representations, under the assumption that a more general term has a higher-entropy distribution (Herbelot and Ganesalingam, 2013; Santus et al., 2014). A strand of distributional semantics has recently"
P15-2020,H05-1079,0,0.0594447,"Missing"
P15-2020,S12-1012,0,0.539959,"Missing"
P15-2020,W09-0215,0,0.65594,"Missing"
P15-2020,N15-1098,0,0.0260618,"Missing"
P15-2020,E14-1054,1,0.310309,"Missing"
P15-2020,E14-4008,0,0.567911,"ion (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Given two semantically related words, a key aspect of detecting lexical entailment, or the hyponym-hypernym relation, is the generality of the hypernym compared to the hyponym. For example, bird is more general than eagle, having a broader intension and a larger extension. This property has led to the introduction of lexical entailment measures that compare the entropy of distributional word representations, under the assumption that a more general term has a higher-entropy distribution (Herbelot and Ganesalingam, 2013; Santus et al., 2014). A strand of distributional semantics has recently emerged that exploits the fact that meaning is often grounded in the perceptual system, known as multi-modal distributional semantics (Bruni et al., 2014). Such models enhance purely linguistic models with extra-linguistic perceptual information, and outperform language-only models on a 2 Related Work In the linguistic modality, the most closely related work is by Herbelot and Ganesalingam (2013) and Santus et al. (2014), who use unsupervised distributional generality measures to identify the hypernym in a hyponym-hypernym pair. Herbelot and"
P15-2020,P14-1068,0,0.140668,"Missing"
P15-2020,D11-1063,0,0.0217261,"Missing"
P15-2020,C04-1146,0,0.81603,"Missing"
P15-2020,C14-1212,0,0.405241,"relations, but does not require detection of directionality, since reversed pairs are grouped with the other negatives. For the combined experiment, we assign reversed hyponym-hypernym pairs a value of -1 instead of 0. We call this more challenging dataset BIBLESS . Examples of pairs in the respective datasets can be found in Table 1. Approach We use two standard evaluations for lexical entailment: hypernym directionality, where the task is to predict which of two words is the hypernym; and hypernym detection, where the task is to predict whether two words are in a hypernym-hyponym relation (Weeds et al., 2014; Santus et al., 2014). We also introduce a third, more challenging, evaluation that combines detection and directionality. For the directionality experiment, we evaluate on the hypernym subset of the well-known BLESS dataset (Baroni and Lenci, 2011), which consists of 1337 hyponym-hypernym pairs. In this case, it is known that the words are in an entailment relation and the task is to predict the directionality of the relation. BLESS data is always presented with the hyponym first, so we report how often our measures predict that the second term in the pair is more general than the first. For"
P15-2020,J09-3004,0,0.0525782,"Missing"
P15-2020,W13-0904,0,\N,Missing
P15-2038,D14-1005,1,0.811658,"Missing"
P15-2038,P14-2135,1,0.844431,"-999 (Hill et al., 2014) and the MEN test collection (Bruni et al., 2014). These datasets consist of concept pairs together with a human-annotated similarity score. Model performance is evaluated using the Spearman ρs correlation between the ranking produced by the cosine of the model-derived vectors and that produced by the gold-standard similarity scores. Evidence suggests that the inclusion of visual representations only improves performance for certain concepts, and that in some cases the introduction of visual information is detrimental to performance on similarity and relatedness tasks (Kiela et al., 2014). The same is likely to be true for other perceptual modalities: in the case of a comparison such as lily-rose, the olfactory modality certainly is meaningful, while this is probably not the case for skateboard-swimsuit. Some examples of relevant pairs can be found in Table 1. Hence, we had two annotators rate the two datasets according to whether smell is relevant to the pairwise comparison. The annotation criterion was as follows: if both concepts in a pairwise comparison have a distinctive associated smell, then the comparison is relevant to the olfactory modality. Only if both annotators a"
P15-2038,P14-1132,0,0.0397793,"ection and κ = 0.96 for SimLex-999).1 Olfactory-Relevant Examples MEN sim SimLex-999 sim bakery bread 0.96 steak meat 0.75 grass lawn 0.96 flower violet 0.70 dog terrier 0.90 tree maple 0.55 bacon meat 0.88 grass moss 0.50 oak wood 0.84 beach sea 0.47 daisy violet 0.76 cereal wheat 0.38 daffodil rose 0.74 bread flour 0.33 2.2 Cross-modal semantics, instead of being concerned with improving semantic representations through grounding, focuses on the problem of reference. Using, for instance, mappings between visual and textual space, the objective is to learn which words refer to which objects (Lazaridou et al., 2014). This problem is very much related to the object recognition task in computer vision, but instead of using just visual data and labels, these cross-modal models also utilize textual information (Socher et al., 2014; Frome et al., 2013). This approach allows for zero-shot learning, where the model can predict how an object relates to other concepts just from seeing an image of the object, but without ever having seen the object previously (Lazaridou et al., 2014). We evaluate cross-modal zero-shot learning performance through the average percentage correct at N (P@N), which measures how many o"
P15-2038,I11-1162,0,0.397813,"Missing"
P15-2038,W15-0107,1,0.830114,"partial least squares regression (PLSR) to induce cross-modal mappings from the linguistic to the olfactory space and vice versa.2 Due to the nature of the olfactory data source (see Section 3), it is not possible to build olfactory representations for all concepts in the test sets. However, cross-modal mappings yield an additional benefit: since linguistic representations have full coverage over the datasets, we can project from linguistic space to perceptual space to also obtain full coverage for the perceptual modalities. This technique has been used to increase coverage for feature norms (Fagarasan et al., 2015). Consequently, we are in a position to compare perceptual spaces directly to each other, and to linguistic Table 1: Examples of pairs in the evaluation datasets where olfactory information is relevant, together with the gold-standard similarity score. 2 Tasks Following previous work in grounded semantics, we evaluate performance on two tasks: conceptual similarity and cross-modal zero-shot learning. 2.1 Cross-modal zero-shot learning Conceptual similarity We evaluate the performance of olfactory multimodal representations on two well-known similarity datasets: SimLex-999 (Hill et al., 2014) a"
P15-2038,N10-1011,0,0.048341,"Missing"
P15-2038,D13-1115,0,0.31721,"Missing"
P15-2038,D12-1130,0,0.138536,"ution over other words implies they suffer from the grounding problem (Harnad, 1990); i.e. they do not account for the fact that human semantic knowledge is grounded in physical reality and sensori-motor experience (Louwerse, 2008). Multi-modal semantics attempts to address this issue and there has been a surge of recent work on perceptually grounded semantic models. These models learn semantic representations from both textual and perceptual input and outperform language-only models on a range of tasks, including modelling semantic similarity and relatedness, and predicting compositionality (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Bruni et al., 2014). Perceptual information is obtained from either feature norms (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; 231 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 231–236, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics MEN test collection MEN (3000 pairs) and its olfactory-relevant subset OMEN (311 pairs); and the SimLex-999 dataset SLex (999 pairs) and i"
P15-2038,Q14-1017,0,0.096799,".50 oak wood 0.84 beach sea 0.47 daisy violet 0.76 cereal wheat 0.38 daffodil rose 0.74 bread flour 0.33 2.2 Cross-modal semantics, instead of being concerned with improving semantic representations through grounding, focuses on the problem of reference. Using, for instance, mappings between visual and textual space, the objective is to learn which words refer to which objects (Lazaridou et al., 2014). This problem is very much related to the object recognition task in computer vision, but instead of using just visual data and labels, these cross-modal models also utilize textual information (Socher et al., 2014; Frome et al., 2013). This approach allows for zero-shot learning, where the model can predict how an object relates to other concepts just from seeing an image of the object, but without ever having seen the object previously (Lazaridou et al., 2014). We evaluate cross-modal zero-shot learning performance through the average percentage correct at N (P@N), which measures how many of the test instances were ranked within the top N highest ranked nearest neighbors. A chance baseline is obtained by randomly ranking a concept’s nearest neighbors. We use partial least squares regression (PLSR) to"
P15-2038,J15-4004,0,\N,Missing
P15-2038,D14-1032,0,\N,Missing
P15-2041,Q14-1026,0,0.565729,"he size of the output layer being equal to the size of the lexical category set. The parameterization of the network consists of three matrices which are learned during supervised training. Matrix U contains weights between the input and hidden layers, V contains weights between the hidden and output layers, and W contains weights between the previous hidden state and the current hidden state. The following recurrence2 is used to compute the activations of the hidden state at word position t: ht = f (xt U + ht−1 W), ewt = bj Lw ∈ R1×n , where j is the look-up index for wt . In addition, as in Lewis and Steedman (2014), for every word we also include its 2-character suffix and capitalization as features. Two more lookup tables are used for these features. Ls ∈ R|s|×m is the look-up table for suffix embeddings, where |s |is the suffix vocabulary size. Lc ∈ R2×m is the look-up table for the capitalization embeddings. Lc contains only two embeddings, representing whether or not a given word is capitalized. We extract features from a context window surrounding the current word to make a tagging decision. Concretely, with a context window of size k, bk/2c words either side of the target word are included. For a"
P15-2041,D11-1031,1,0.94047,"Missing"
P15-2041,J99-2004,0,0.735192,"pedia and biomedical text. 1 Stephen Clark University of Cambridge Computer Laboratory sc609@cam.ac.uk Introduction Combinatory Categorial Grammar (CCG; Steedman, 2000) is a highly lexicalized formalism; the standard parsing model of Clark and Curran (2007) uses over 400 lexical categories (or supertags), compared to about 50 POS tags for typical CFG parsers. This makes accurate disambiguation of lexical types much more challenging. However, the assignment of lexical categories can still be solved reasonably well by treating it as a sequence tagging problem, often referred to as supertagging (Bangalore and Joshi, 1999). Clark and Curran (2004) show that high tagging accuracy can be achieved by leaving some ambiguity to the parser to resolve, but with enough of a reduction in the number of tags assigned to each word so that parsing efficiency is greatly increased. In addition to improving parsing efficiency, supertagging also has a large impact on parsing accuracy (Curran et al., 2006; Kummerfeld et al., 2010), since the derivation space of the parser is determined by the supertagger, at both train*All work was completed before the author joined Facebook. 250 Proceedings of the 53rd Annual Meeting of the Ass"
P15-2041,C04-1041,1,0.83064,"Stephen Clark University of Cambridge Computer Laboratory sc609@cam.ac.uk Introduction Combinatory Categorial Grammar (CCG; Steedman, 2000) is a highly lexicalized formalism; the standard parsing model of Clark and Curran (2007) uses over 400 lexical categories (or supertags), compared to about 50 POS tags for typical CFG parsers. This makes accurate disambiguation of lexical types much more challenging. However, the assignment of lexical categories can still be solved reasonably well by treating it as a sequence tagging problem, often referred to as supertagging (Bangalore and Joshi, 1999). Clark and Curran (2004) show that high tagging accuracy can be achieved by leaving some ambiguity to the parser to resolve, but with enough of a reduction in the number of tags assigned to each word so that parsing efficiency is greatly increased. In addition to improving parsing efficiency, supertagging also has a large impact on parsing accuracy (Curran et al., 2006; Kummerfeld et al., 2010), since the derivation space of the parser is determined by the supertagger, at both train*All work was completed before the author joined Facebook. 250 Proceedings of the 53rd Annual Meeting of the Association for Computationa"
P15-2041,J07-4004,1,0.644157,"Missing"
P15-2041,D08-1050,1,0.934316,"Missing"
P15-2041,P06-1088,1,0.843558,"te disambiguation of lexical types much more challenging. However, the assignment of lexical categories can still be solved reasonably well by treating it as a sequence tagging problem, often referred to as supertagging (Bangalore and Joshi, 1999). Clark and Curran (2004) show that high tagging accuracy can be achieved by leaving some ambiguity to the parser to resolve, but with enough of a reduction in the number of tags assigned to each word so that parsing efficiency is greatly increased. In addition to improving parsing efficiency, supertagging also has a large impact on parsing accuracy (Curran et al., 2006; Kummerfeld et al., 2010), since the derivation space of the parser is determined by the supertagger, at both train*All work was completed before the author joined Facebook. 250 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 250–255, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics tions (§2.2). Our model is highly accurate, and by integrating it with the C & C parser as its adaptive supertagger, we obtain substantial accura"
P15-2041,P07-2009,1,0.651356,"Missing"
P15-2041,J07-3004,0,0.433218,"learning and better generalization of the trained model with dropout. Similar to other forms of droput (Srivastava et al., 2014), we randomly drop units and their connections to other units at training time. Concretely, we apply a binary dropout mask to xt , with a dropout rate of 0.25, and at test time no mask is applied, but the input to the network, xt , at each word position is scaled by 0.75. We experimented during development with different dropout rates, but found the above choice to be optimal in our setting. Experiments Datasets and Baseline. We follow the standard splits of CCGBank (Hockenmaier and Steedman, 2007) for all experiments using sections 2-21 for training, section 00 for development and section 23 as in-domain test set. The Wikipedia corpus from Honnibal et al. (2009) and the Bioinfer corpus (Pyysalo et al., 2007) are used as two outof-domain test sets. We compare supertagging accuracy with the MaxEnt C & C supertagger and the neural network tagger of Lewis and Steedman (2014) (henceforth NN), and we also evaluate parsing accuracy using these three supertaggers as a front-end to the C & C parser. We use the same 425 supertag set used in both C & C and NN. Hyperparameters and Training. For Lw"
P15-2041,P10-1040,0,0.0323139,"s from a context window surrounding the current word to make a tagging decision. Concretely, with a context window of size k, bk/2c words either side of the target word are included. For a word wt , its continuous feature representation is: fwt = [ewt ; swt ; cwt ], (2) xt = [fwt−bk/2c ; . . . fwt ; . . . ; fwt+bk/2c ], where g is the softmax activation function g(zi ) = z Pe i zj that squeezes raw output activations into a e (5) where xt ∈ R1×k(n+2m) and the right-hand side is the concatenation of all feature representations in a size k context window. We use pre-trained word embeddings from Turian et al. (2010) to initialize lookup table Lw , and we apply a set of word pre-processing techniques at both training and test time to reduce sparsity. All words are first lower-cased, and all numbers are collapsed into a single digit ‘0’. If a lower-cased hyphenated j probability distribution. 2.2 (4) where ewt ∈ R1×n , swt ∈ R1×m and cwt ∈ R1×m are the output vectors from the three different look-up tables, and [ewt ; swt ; cwt ] denotes the concatenation of three vectors and hence fwt ∈ R1×(n+2m) . At word position t, the input layer of the network xt is: (1) where f is a non-linear activation function; h"
P15-2041,W09-3306,0,0.0176849,"other units at training time. Concretely, we apply a binary dropout mask to xt , with a dropout rate of 0.25, and at test time no mask is applied, but the input to the network, xt , at each word position is scaled by 0.75. We experimented during development with different dropout rates, but found the above choice to be optimal in our setting. Experiments Datasets and Baseline. We follow the standard splits of CCGBank (Hockenmaier and Steedman, 2007) for all experiments using sections 2-21 for training, section 00 for development and section 23 as in-domain test set. The Wikipedia corpus from Honnibal et al. (2009) and the Bioinfer corpus (Pyysalo et al., 2007) are used as two outof-domain test sets. We compare supertagging accuracy with the MaxEnt C & C supertagger and the neural network tagger of Lewis and Steedman (2014) (henceforth NN), and we also evaluate parsing accuracy using these three supertaggers as a front-end to the C & C parser. We use the same 425 supertag set used in both C & C and NN. Hyperparameters and Training. For Lw , we use the scaled 50-dimensional Turian embeddings (n = 50 for Lw ) as initialization. We have experimented during development with using 100dimensional embeddings a"
P15-2041,P14-1021,1,0.848389,"Missing"
P15-2041,P11-1069,1,0.927753,"Missing"
P15-2041,P10-1036,1,0.88165,"lexical types much more challenging. However, the assignment of lexical categories can still be solved reasonably well by treating it as a sequence tagging problem, often referred to as supertagging (Bangalore and Joshi, 1999). Clark and Curran (2004) show that high tagging accuracy can be achieved by leaving some ambiguity to the parser to resolve, but with enough of a reduction in the number of tags assigned to each word so that parsing efficiency is greatly increased. In addition to improving parsing efficiency, supertagging also has a large impact on parsing accuracy (Curran et al., 2006; Kummerfeld et al., 2010), since the derivation space of the parser is determined by the supertagger, at both train*All work was completed before the author joined Facebook. 250 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 250–255, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics tions (§2.2). Our model is highly accurate, and by integrating it with the C & C parser as its adaptive supertagger, we obtain substantial accuracy improvements, outperfor"
P15-2120,D10-1115,0,0.076225,"mantic methods represent word meanings by their contextual distributions, for example by computing word-context co-ocurrence statistics (Sch¨utze, 1998; Turney and Pantel, 2010) or by learning vector representations for words as part of a context prediction model (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013). Recent research has also focused on compositional distributional semantics (CDS): combining the distributional representations for words, often in a syntax-driven fashion, to produce distributional representations of phrases and sentences (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Socher et al., 2012; Zanzotto and Dell’Arciprete, 2012). One method for CDS is the Categorial framework (Coecke et al., 2011; Baroni et al., 2014), 731 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 731–736, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics vector for the SVO triple is given by X V (s, o)l = Vljk ok sj also introduce several alternative ways of reducing the number of tensor parameters by using matrices. The"
P15-2120,2014.lilt-9.5,0,0.277859,"rney and Pantel, 2010) or by learning vector representations for words as part of a context prediction model (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013). Recent research has also focused on compositional distributional semantics (CDS): combining the distributional representations for words, often in a syntax-driven fashion, to produce distributional representations of phrases and sentences (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Socher et al., 2012; Zanzotto and Dell’Arciprete, 2012). One method for CDS is the Categorial framework (Coecke et al., 2011; Baroni et al., 2014), 731 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 731–736, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics vector for the SVO triple is given by X V (s, o)l = Vljk ok sj also introduce several alternative ways of reducing the number of tensor parameters by using matrices. The best performing method uses two matrices, one representing the subject-verb interactions and the other the verb-object interactions. Some interactio"
P15-2120,N13-1105,0,0.0241602,"s and thus we are able to improve on both memory usage and efficiency. This approach follows recent work on using low-rank tensors to parameterize models for dependency parsing (Lei et al., 2014) and semantic role labelling (Lei et al., 2015). Our work applies the same tensor rank decompositions, and similar optimization algorithms, to the task of constructing a syntax-driven model for CDS. Although we focus on the Categorial framework, the low-rank decomposition methods are also applicable to other tensor-based semantic models including Van de Cruys (2010), Smolensky and Legendre (2006), and Blacoe et al. (2013). 2 (1) Low-Rank Tensor Representations Following Lei et al. (2014), we represent each verb’s tensor using a low-rank canonical polyadic (CP) decomposition to reduce the numbers of parameters that must be learned during training. As a higher-order analogue to singular value decomposition for matrices, CP decomposition factors a tensor into a sum of R tensor products of vectors.1 Given a third-order tensor V ∈ RS×N ×N , the CP decomposition of V is: V= R X Pr ⊗ Qr ⊗ Rr (2) r=1 where P ∈ RR×S , Q ∈ RR×N , R ∈ RR×N are parameter matrices, Pr gives the rth row of matrix P, and ⊗ is the tensor prod"
P15-2120,P07-2009,1,0.826276,"Missing"
P15-2120,W11-2507,0,0.510058,"more complex types have corresponding tensors of higher order, and therefore a barrier to the practical implementation of this framework is the large number of parameters required to represent an extended vocabulary and a variety of grammatical constructions. We aim to reduce the size of the models by demonstrating that reduced-rank tensors, which can be represented in a form requiring fewer parameters, can capture the semantics of complex types as well as the full-rank tensors do. We base our experiments on the transitive verb construction for which there are established tasks and datasets (Grefenstette and Sadrzadeh, 2011; Kartsaklis and Sadrzadeh, 2014). Previous work on the transitive verb construction within the Categorial framework includes a two-step linear-regression method for the construction of the full verb tensors (Grefenstette et al., 2013) and a multi-linear regression method combined with a two-dimensional plausibility space (Polajnar et al., 2014). Polajnar et al. (2014) Several compositional distributional semantic methods use tensors to model multi-way interactions between vectors. Unfortunately, the size of the tensors can make their use impractical in large-scale implementations. In this pap"
P15-2120,W13-0112,0,0.241686,"ical constructions. We aim to reduce the size of the models by demonstrating that reduced-rank tensors, which can be represented in a form requiring fewer parameters, can capture the semantics of complex types as well as the full-rank tensors do. We base our experiments on the transitive verb construction for which there are established tasks and datasets (Grefenstette and Sadrzadeh, 2011; Kartsaklis and Sadrzadeh, 2014). Previous work on the transitive verb construction within the Categorial framework includes a two-step linear-regression method for the construction of the full verb tensors (Grefenstette et al., 2013) and a multi-linear regression method combined with a two-dimensional plausibility space (Polajnar et al., 2014). Polajnar et al. (2014) Several compositional distributional semantic methods use tensors to model multi-way interactions between vectors. Unfortunately, the size of the tensors can make their use impractical in large-scale implementations. In this paper, we investigate whether we can match the performance of full tensors with low-rank approximations that use a fraction of the original number of parameters. We investigate the effect of low-rank tensors on the transitive verb constru"
P15-2120,P08-1028,0,0.69344,"roduction Distributional semantic methods represent word meanings by their contextual distributions, for example by computing word-context co-ocurrence statistics (Sch¨utze, 1998; Turney and Pantel, 2010) or by learning vector representations for words as part of a context prediction model (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013). Recent research has also focused on compositional distributional semantics (CDS): combining the distributional representations for words, often in a syntax-driven fashion, to produce distributional representations of phrases and sentences (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Socher et al., 2012; Zanzotto and Dell’Arciprete, 2012). One method for CDS is the Categorial framework (Coecke et al., 2011; Baroni et al., 2014), 731 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 731–736, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics vector for the SVO triple is given by X V (s, o)l = Vljk ok sj also introduce several alternative ways of reducing the number of tensor param"
P15-2120,P14-1009,0,0.0295455,"ational Joint Conference on Natural Language Processing (Short Papers), pages 731–736, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics vector for the SVO triple is given by X V (s, o)l = Vljk ok sj also introduce several alternative ways of reducing the number of tensor parameters by using matrices. The best performing method uses two matrices, one representing the subject-verb interactions and the other the verb-object interactions. Some interaction between the subject and the object is re-introduced through a softmax layer. A similar method is presented in Paperno et al. (2014). Milajevs et al. (2014) use vectors generated by a neural language model to construct verb matrices and several different composition operators to generate the composed subject-verb-object sentence representation. j,k We aim to learn distributional vectors s and o for subjects and objects, and tensors V for verbs, such that the output vectors V (s, o) are distributional representations of the entire SVO triple. While there are several possible definitions of the sentence space (Clark, 2013; Baroni et al., 2014), we follow previous work (Grefenstette et al., 2013) by using a contextual sentenc"
P15-2120,E14-1025,1,0.844006,"viously unseen expressions. However, they can be used to generate data to train such a model, as we will describe. 1) Count vectors (SVD): we count the number of times each noun or SVO triple co-occurs with each of the 10,000 most frequent words (excluding stopwords) in the Wikipedia corpus, using sentences as context boundaries. If the verb in the SVO triple is itself a content word, we do not include it as context for the triple. This produces one set of context vectors for nouns and another for SVO triples. We weight entries in these vectors using the t-test weighting scheme (Curran, 2004; Polajnar and Clark, 2014), and then reduce the vectors to 100 dimensions via singular value decomposition (SVD), decomposing the noun vectors and SVO vectors separately. 2) Prediction vectors (PV): we train vector embeddings for nouns and SVO triples by adaptMV 1 X L(V ) = ||V (s(i) , o(i) ) − t(i) ||22 MV (4) i=1 V (s, o) is given by Eq. (1) for full tensors, and by Eq. (3) for tensors represented in low-rank form. In both the low-rank and full-rank tensor learning, we use mini-batch ADADELTA optimization (Zeiler, 2012) up to a maximum of 500 iterations through the training data, which we found to be sufficient for c"
P15-2120,P14-1130,0,0.0571163,"2014), we follow previous work (Grefenstette et al., 2013) by using a contextual sentence space consisting of content words that occur within the same sentences as the SVO triple. In this paper, we use tensor rank decomposition (Kolda and Bader, 2009) to represent each verb’s tensor as a sum of tensor products of vectors. We learn the component vectors and apply the composition without ever constructing the full tensors and thus we are able to improve on both memory usage and efficiency. This approach follows recent work on using low-rank tensors to parameterize models for dependency parsing (Lei et al., 2014) and semantic role labelling (Lei et al., 2015). Our work applies the same tensor rank decompositions, and similar optimization algorithms, to the task of constructing a syntax-driven model for CDS. Although we focus on the Categorial framework, the low-rank decomposition methods are also applicable to other tensor-based semantic models including Van de Cruys (2010), Smolensky and Legendre (2006), and Blacoe et al. (2013). 2 (1) Low-Rank Tensor Representations Following Lei et al. (2014), we represent each verb’s tensor using a low-rank canonical polyadic (CP) decomposition to reduce the numbe"
P15-2120,D14-1111,1,0.925396,"Semantics Daniel Fried, Tamara Polajnar, and Stephen Clark University of Cambridge Computer Laboratory {df345,tp366,sc609}@cam.ac.uk Abstract where each word is represented by a tensor whose order is determined by the Categorial Grammar type of the word. For example, nouns are an atomic type represented by a vector, and adjectives are matrices that act as functions transforming a noun vector into another noun vector (Baroni and Zamparelli, 2010). A transitive verb is a thirdorder tensor that takes the noun vectors representing the subject and object and returns a vector in the sentence space (Polajnar et al., 2014). However, a concrete implementation of the Categorial framework requires setting and storing the values, or parameters, defining these matrices and tensors. These parameters can be quite numerous for even low-dimensional sentence spaces. For example, a third-order tensor for a given transitive verb, mapping two 100-dimensional noun spaces to a 100-dimensional sentence space, would have 1003 parameters in its full form. All of the more complex types have corresponding tensors of higher order, and therefore a barrier to the practical implementation of this framework is the large number of param"
P15-2120,N15-1121,0,0.0314077,"al., 2013) by using a contextual sentence space consisting of content words that occur within the same sentences as the SVO triple. In this paper, we use tensor rank decomposition (Kolda and Bader, 2009) to represent each verb’s tensor as a sum of tensor products of vectors. We learn the component vectors and apply the composition without ever constructing the full tensors and thus we are able to improve on both memory usage and efficiency. This approach follows recent work on using low-rank tensors to parameterize models for dependency parsing (Lei et al., 2014) and semantic role labelling (Lei et al., 2015). Our work applies the same tensor rank decompositions, and similar optimization algorithms, to the task of constructing a syntax-driven model for CDS. Although we focus on the Categorial framework, the low-rank decomposition methods are also applicable to other tensor-based semantic models including Van de Cruys (2010), Smolensky and Legendre (2006), and Blacoe et al. (2013). 2 (1) Low-Rank Tensor Representations Following Lei et al. (2014), we represent each verb’s tensor using a low-rank canonical polyadic (CP) decomposition to reduce the numbers of parameters that must be learned during tr"
P15-2120,J98-1004,0,0.440819,"Missing"
P15-2120,Q15-1016,0,0.11915,"Missing"
P15-2120,W14-1406,1,0.866381,"sing gradient-based optimization, we learn a low-rank tensor requiring fewer parameters without ever having to store the full tensor. In addition to reducing the number of parameters, representing tensors in this form allows us to formulate the verb tensor’s action on noun vectors as matrix multiplication. For a tensor in the form of Eq. (2), the output SVO vector is given by Model Tensor Models for Verbs We model each transitive verb as a bilinear function mapping subject and object noun vectors, each of dimensionality N , to a single sentence vector of dimensionality S (Coecke et al., 2011; Maillard et al., 2014) representing the composed subject-verb-object (SVO) triple. Each transitive verb has its own thirdorder tensor, which defines this bilinear function. Consider a verb V with associated tensor V ∈ RS×N ×N , and vectors s ∈ RN , o ∈ RN for subject and object nouns, respectively. Then the compositional representation for the subject, verb, and object is a vector V (s, o) ∈ RS , produced by applying tensor contraction (the higher-order analogue of matrix multiplication) to the verb tensor and two noun vectors. The lth component of the V (s, o) = P> (Qs Ro) (3) where is the elementwise vector produ"
P15-2120,D12-1110,0,0.124712,"meanings by their contextual distributions, for example by computing word-context co-ocurrence statistics (Sch¨utze, 1998; Turney and Pantel, 2010) or by learning vector representations for words as part of a context prediction model (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013). Recent research has also focused on compositional distributional semantics (CDS): combining the distributional representations for words, often in a syntax-driven fashion, to produce distributional representations of phrases and sentences (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Socher et al., 2012; Zanzotto and Dell’Arciprete, 2012). One method for CDS is the Categorial framework (Coecke et al., 2011; Baroni et al., 2014), 731 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 731–736, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics vector for the SVO triple is given by X V (s, o)l = Vljk ok sj also introduce several alternative ways of reducing the number of tensor parameters by using matrices. The best performing metho"
P15-2120,P14-5010,0,0.00310259,"or nouns and SVO triples with a modified version of word2vec,2 using the hierarchical sampling method with the default hyperparameters and 20 iterations through the training data. We train the compositional model for verbs in three steps: extracting transitive verbs and their subject and object nouns from corpus data, producing distributional vectors for the nouns and the SVO triples, and then learning parameters of the verb functions, which map the nouns to the SVO triple vectors. Corpus Data We extract SVO triples from an October 2013 download of Wikipedia, tokenized using Stanford CoreNLP (Manning et al., 2014), lemmatized with the Morpha lemmatizer (Minnen et al., 2001), and parsed using the C&C parser (Curran et al., 2007). We filter the SVO triples to a set containing 345 distinct verbs: the verbs from our test datasets, along with some additional high-frequency verbs included to produce more representative sentence spaces. For each verb, we selected up to 600 triples which occurred more than once and contained subject and object nouns that occurred at least 100 times (to allow sufficient context to produce a distributional representation for the triple). This resulted in approximately 150,000 SV"
P15-2120,D14-1079,0,0.545985,"e on Natural Language Processing (Short Papers), pages 731–736, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics vector for the SVO triple is given by X V (s, o)l = Vljk ok sj also introduce several alternative ways of reducing the number of tensor parameters by using matrices. The best performing method uses two matrices, one representing the subject-verb interactions and the other the verb-object interactions. Some interaction between the subject and the object is re-introduced through a softmax layer. A similar method is presented in Paperno et al. (2014). Milajevs et al. (2014) use vectors generated by a neural language model to construct verb matrices and several different composition operators to generate the composed subject-verb-object sentence representation. j,k We aim to learn distributional vectors s and o for subjects and objects, and tensors V for verbs, such that the output vectors V (s, o) are distributional representations of the entire SVO triple. While there are several possible definitions of the sentence space (Clark, 2013; Baroni et al., 2014), we follow previous work (Grefenstette et al., 2013) by using a contextual sentence space consisting of co"
P16-2031,P14-1006,0,0.112167,"that share a common meaning across different languages. It plays an important role in a variety of fundamental tasks in IR and NLP, e.g. cross-lingual information retrieval and statistical machine translation. The majority of current BLL models aim to learn lexicons from comparable data. These approaches work by (1) mapping language pairs to a shared crosslingual vector space (SCLVS) such that words are close when they have similar meanings; and (2) extracting close lexical items from the induced SCLVS. Bilingual word embedding (BWE) induced models currently hold the state-of-the-art on BLL (Hermann and Blunsom, 2014; Gouws et al., 2015; Vuli´c and Moens, 2016). Although methods for learning SCLVSs are predominantly text-based, this space need not be linguistic in nature: Bergsma and van Durme (2011) and Kiela et al. (2015) used labeled images from 2 2.1 Methodology Linguistic Representations We use three representative linguistic BWE models. Given a source and target vocabulary V S and V T , BWE models learn a representation of each word w ∈ V S ∪ V T as a real-valued vec188 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 188–194, c Berlin, Germany, August 7"
P16-2031,D14-1005,1,0.413644,"eneral performance of linguistic BLL models from comparable Wikipedia data (Vuli´c and Moens, 2013), this is considered a benchmarking test set for (linguistic) BLL models from comparable data (Vuli´c and Moens, 2016)5 . It comprises 1, 000 nouns in ES, IT, and NL, along with their oneto-one ground-truth word translations in EN compiled semi-automatically. Translation direction is ES/IT /N L → EN . Multi-Modal Representations We experiment with two ways of fusing information stemming from the linguistic and visual modalities. Following recent work in multi-modal semantics (Bruni et al., 2014; Kiela and Bottou, 2014), we construct representations by concatenating the centered and L2 -normalized linguistic and visual feature vectors: wmm = α × wling ||(1 − α) × wvis Experimental Setup Training Data and Setup We used standard training data and suggested settings to learn M/G/V-EMB model representations. M-EMB and G-EMB were trained on the full cleaned and tokenized Wikipedias from the Polyglot website (AlRfou et al., 2013). V-EMB was trained on the full tokenized document-aligned Wikipedias from (1) where ||denotes concatenation and α is a parameter governing the contributions of each unimodal representatio"
P16-2031,P14-2135,1,0.557318,"setting. Our contributions are: We introduce bilingual multi-modal semantic spaces that merge linguistic and visual components to obtain semantically-enriched bilingual multi-modal word representations. These representations display significant improvements for three language pairs on two benchmarking BLL test sets in comparison to three different bilingual linguistic representations (Mikolov et al., 2013; Gouws et al., 2015; Vuli´c and Moens, 2016), as well as over the uni-modal visual representations from Kiela et al. (2015). We also propose a weighting technique based on image dispersion (Kiela et al., 2014) that governs the influence of visual information in fused representations, and show that this technique leads to robust multi-modal models which do not require fine tuning of the fusion parameter. Recent work has revealed the potential of using visual representations for bilingual lexicon learning (BLL). Such image-based BLL methods, however, still fall short of linguistic approaches. In this paper, we propose a simple yet effective multimodal approach that learns bilingual semantic representations that fuse linguistic and visual input. These new bilingual multi-modal embeddings display signi"
P16-2031,D15-1015,1,0.471003,"Missing"
P16-2031,J99-4009,0,0.810588,"image dispersion (ID) (Kiela et al., 2014). ID is defined as the average pairwise cosine distance between all the image representations/vectors {i1 . . . in } in the set of images for a given word w: id(w) = X 2 ij · ik 1− n(n − 1) |ij ||ik | (2) j<k≤n 5 Intuitively, more concrete words display more coherent visual representations and consequently lower ID scores (see Footnote 9 again). The lowest improvements on V ULIC 1000 are reported for the IT-EN language pair, which is incidentally the most abstract test set. There is some evidence that abstract concepts are also perceptually grounded (Lakoff and Johnson, 1999), albeit in a more complex way, since abstract concepts will relate more varied situations (Barsalou and Wiemer-Hastings, 2005). Consequently, uni-modal visual representations are not powerful enough to capture all the semantic intricacies of such abstract concepts, and the linguistic components are more beneficial in such cases. This explains an improved performance with α = 0.7, but also calls for a more intelligent decision mechanism on how much perceptual information to include in the multi-modal models. The decision should be closely related to the degree of a concept’s concreteness, e.g."
P16-2031,P15-1027,0,0.289784,"feature for w. Similarity between w, v ∈ V S ∪ V T is computed through a similarity function (SF), simling (w, v) = SF (wling , vling ), e.g., cosine. BOWA model from Gouws et al. (2015) as the representative model to be included in the comparisons, due to its solid performance and robustness in the BLL task (Luong et al., 2015), its reduced complexity reflected in fast computations on massive datasets and its public availability.2 Type 1: M-EMB This type of BWE induction model assumes the following setup for learning the SCLVS (Mikolov et al., 2013; Faruqui and Dyer, 2014; Dinu et al., 2015; Lazaridou et al., 2015a): First, two monolingual spaces, RdS and RdT , are induced separately in each language using a standard monolingual embedding model. The bilingual signal is provided in the form of word translation pairs (xi , yi ), where xi ∈ V S , yi ∈ V T , and xi ∈ RdS , yi ∈ RdT . Training is cast as a multivariate regression problem: it implies learning a function that maps the source language vectors to their corresponding target language vectors. A standard approach (Mikolov et al., 2013; Dinu et al., 2015) is to assume a linear map W ∈ RdS ×dT , which is learned through an L2 -regularized least-squa"
P16-2031,W16-3210,0,0.0149091,"Missing"
P16-2031,E14-1049,0,0.0830889,"l R is the value of the k-th cross-lingual feature for w. Similarity between w, v ∈ V S ∪ V T is computed through a similarity function (SF), simling (w, v) = SF (wling , vling ), e.g., cosine. BOWA model from Gouws et al. (2015) as the representative model to be included in the comparisons, due to its solid performance and robustness in the BLL task (Luong et al., 2015), its reduced complexity reflected in fast computations on massive datasets and its public availability.2 Type 1: M-EMB This type of BWE induction model assumes the following setup for learning the SCLVS (Mikolov et al., 2013; Faruqui and Dyer, 2014; Dinu et al., 2015; Lazaridou et al., 2015a): First, two monolingual spaces, RdS and RdT , are induced separately in each language using a standard monolingual embedding model. The bilingual signal is provided in the form of word translation pairs (xi , yi ), where xi ∈ V S , yi ∈ V T , and xi ∈ RdS , yi ∈ RdT . Training is cast as a multivariate regression problem: it implies learning a function that maps the source language vectors to their corresponding target language vectors. A standard approach (Mikolov et al., 2013; Dinu et al., 2015) is to assume a linear map W ∈ RdS ×dT , which is le"
P16-2031,W15-1521,0,0.168909,"Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 188–194, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics tor: wling = [f1ling , . . . , fdling ], where fkling ∈ l R is the value of the k-th cross-lingual feature for w. Similarity between w, v ∈ V S ∪ V T is computed through a similarity function (SF), simling (w, v) = SF (wling , vling ), e.g., cosine. BOWA model from Gouws et al. (2015) as the representative model to be included in the comparisons, due to its solid performance and robustness in the BLL task (Luong et al., 2015), its reduced complexity reflected in fast computations on massive datasets and its public availability.2 Type 1: M-EMB This type of BWE induction model assumes the following setup for learning the SCLVS (Mikolov et al., 2013; Faruqui and Dyer, 2014; Dinu et al., 2015; Lazaridou et al., 2015a): First, two monolingual spaces, RdS and RdT , are induced separately in each language using a standard monolingual embedding model. The bilingual signal is provided in the form of word translation pairs (xi , yi ), where xi ∈ V S , yi ∈ V T , and xi ∈ RdS , yi ∈ RdT . Training is cast as a multivariate r"
P16-2031,P04-1067,0,0.0295097,"SCLVS: simvis (w, v) = SF (wvis , vvis ), e.g. cosine. (2) CNN-AVG M AX: An alternative strategy, introduced by Bergsma and van Durme (2011), is to consider the similarities between individual images from the two sets and take the average of the maximum similarity scores as the final similarity simvis (w, v). 2.3 3 Task: Bilingual Lexicon Learning Given a source language word ws , the task is to find a target language word wt closest to ws in the SCLVS, and the resulting pair (ws , wt ) is a bilingual lexicon entry. Performance is measured using the BLL standard Top 1 accuracy (Acc1 ) metric (Gaussier et al., 2004; Gouws et al., 2015). Test Sets We work with three language pairs: English-Spanish/Dutch/Italian (EN-ES/NL/IT), and two benchmarking BLL test sets: (1) B ERGSMA 500: consisting of a set of 500 ground truth noun pairs for the three language pairs, it is considered a benchmarking test set in prior work on BLL using vision (Bergsma and van Durme, 2011)4 . Translation direction in our tests is EN → ES/IT /N L. (2) V ULIC 1000: constructed to measure the general performance of linguistic BLL models from comparable Wikipedia data (Vuli´c and Moens, 2013), this is considered a benchmarking test set"
P16-2031,N16-1021,0,0.0159524,"le. As future work, we plan to analyse the ability of multi-view representation learning algorithms to yield fused multi-modal representations in bilingual settings (Lazaridou et al., 2015b; Rastogi et al., 2015; Wang et al., 2015), as well as to apply multi-modal bilingual spaces in other tasks such as zero-short learning (Frome et al., 2013) or cross-lingual MM information search and retrieval following paradigms from monolingual settings (Pereira et al., 2014; Vuli´c and Moens, 2015). The inclusion of perceptual data, as this paper reveals, seems especially promising in bilingual settings (Rajendran et al., 2016; Elliott et al., 2016), since the perceptual information demonstrates the ability to transcend linguistic borders. Image Dispersion Weighting The intuition that the inclusion of visual information may lead to negative effects in MM modeling has been exploited by Kiela et al. (2014) in their work on image-dispersion filtering: Although the filtering method displays some clear benefits, its shortcoming lies in the fact that it performs a binary decision which can potentially discard valuable perceptual information for less concrete concepts. Here, we introduce a weighting scheme where the perce"
P16-2031,N15-1058,0,0.0380203,"Missing"
P16-2031,P14-1068,0,0.159578,"ven.be Abstract the Web to learn bilingual lexicons based on visual features, with features derived from deep convolutional neural networks (CNNs) leading to the best results (Kiela et al., 2015). However, vision-based BLL does not yet perform at the same level as state-of-the-art linguistic models. Here, we unify the strengths of both approaches into one single multi-modal vision-language SCLVS. It has been found in multi-modal semantics that linguistic and visual representations are often complementary in terms of the information they encode (Deselaers and Ferrari, 2011; Bruni et al., 2014; Silberer and Lapata, 2014). This is the first work to test the effectiveness of the multi-modal approach in a BLL setting. Our contributions are: We introduce bilingual multi-modal semantic spaces that merge linguistic and visual components to obtain semantically-enriched bilingual multi-modal word representations. These representations display significant improvements for three language pairs on two benchmarking BLL test sets in comparison to three different bilingual linguistic representations (Mikolov et al., 2013; Gouws et al., 2015; Vuli´c and Moens, 2016), as well as over the uni-modal visual representations from"
P16-2031,tiedemann-2012-parallel,0,0.0181405,"ting from the BNC word frequency list (Kilgarriff, 1997), the 6, 318 most frequent EN words were translated to the three other languages using Google Translate. The lists were subsequently cleaned, removing all pairs that contain IT/ES/NL words occurring in the test sets and least frequent pairs, to build the final 3×5K training pairs. We trained two monolingual SGNS models, using SGD with a global learning rate of 0.025. For G-EMB, as in the original work (Gouws et al., 2015), the bilingual signal for the cross-lingual regularization was provided in the first 500K sentences from Europarl.v7 (Tiedemann, 2012). We used SGD with a global learning rate 0.15. For V-EMB, monolingual SGNS was trained on pseudo-bilingual documents using SGD with a global learning rate 0.025. All BWEs were trained with d = 300.7 Other parameters are: 15 epochs, 15 negatives, subsampling rate 1e − 4. We report results with two α standard values: 0.5 and 0.7 (more weight assigned to the linguistic part). 4 sions8 . There is a marked difference in performance on B ERGSMA 500 and V ULIC 1000: visual-only BLL models on V ULIC 1000 perform two times worse than linguistic-only BLL models. This is easily explained by the increase"
P16-2031,D13-1168,1,0.367109,"Missing"
P16-2031,W13-3520,0,\N,Missing
P18-1132,D10-1066,0,0.025447,"The fox eats worms Stack 0 1 2 3 4 5 6 7 8 9 10 11 12 The (NP |The (NP |The |fox (NP The fox) (S |(NP The fox) (S |(NP The fox) |eats (S |(NP The fox) |(VP |eats (S |(NP The fox) |(VP |eats |worms (S |(NP The fox) |(VP |eats |(NP |worms (S |(NP The fox) |(VP |eats |(NP worms) (S |(NP The fox) |(VP eats (NP worms)) (S (NP The fox) (VP eats (NP worms))) Action GEN (The) NT SW (NP) GEN (fox) anticipatory representations, it is said, could explain the rapid, incremental processing that humans seem to exhibit (Marslen-Wilson, 1973; Tanenhaus et al., 1995); this line of thinking similarly motivates Charniak (2010), among others. While most work in this domain has been concerned with the parsing problem, our findings suggest that anticipatory mechanisms are also beneficial in capturing structural dependencies in language modeling. We note that our results are achieved using models that, in theory, are able to condition on the entire derivation history, while earlier work in sentence processing has focused on cognitive memory considerations, such as the memory-bounded model of Schuler et al. (2010). REDUCE 5 NT SW (S) Conclusion GEN (eats) NT SW (VP) GEN (worms) NT SW (NP) REDUCE REDUCE REDUCE N/A Figure"
P18-1132,D16-1257,0,0.420859,"that explicitly models syntactic structures, the recurrent neural network grammars (Dyer et al., 2016, RNNGs), considerably outperforms sequential LSTM language models for cases with multiple attractors (§3). We present experiments affirming that this gain is due to an explicit composition operator rather than the presence of predicted syntactic annotations. Rather surprisingly, syntactic LSTM language models without explicit composition have no advantage over sequential LSTMs that operate on word sequences, although these models can nevertheless be excellent predictors of phrase structures (Choe and Charniak, 2016). Having established the importance of modeling structures, we explore the hypothesis that how we build the structure affects the model’s ability to identify structural dependencies in English. As RNNGs build phrase-structure trees through top-down operations, we propose extensions to the structure-building sequences and model architecture that enable left-corner (Henderson, 2003, 2004) and bottom-up (Chelba and Jelinek, 2000; Emami and Jelinek, 2005) generation orders (§4). Extensive prior work has characterized topdown, left-corner, and bottom-up parsing strategies in terms of cognitive plau"
P18-1132,E17-1117,1,0.823907,"Missing"
P18-1132,E17-2020,0,0.0449163,"Missing"
P18-1132,1997.iwpt-1.18,0,0.450127,"y of strings and phrase-structure trees, thereby imposing different biases on the learner. Earlier work in parsing has characterized the plausibility of top-down, left-corner, and bottom-up strategies as viable candidates of human sentence processing, especially in terms of memory constraints and human difficulties with center embedding constructions (Johnson-Laird, 1983; Pulman, 1986; Abney and Johnson, 1991; Resnik, 1992, inter alia), along with neurophysiological evidence in human sentence processing (Nelson et al., 2017). Here we cast the three strategies as models of language generation (Manning and Carpenter, 1997), and focus on the empirical question: which generation order has the most appropriate bias in modeling non-local structural dependencies in English? These alternative orders organize the learning problem so as to yield intermediate states in generation that condition on different aspects of the grammatical structure. In number agreement, this amounts to making an agreement controller, such as the word flowers in Fig. 3, more or less salient. If it is more salient, the model should be better-able to inflect the main verb in agreement with this controller, without getting distracted by the attr"
P18-1132,N16-1024,1,0.564713,"in the number agreement task. Given the strong performance of word-based LSTM language models, are there are any substantial benefits, in terms of number agreement accuracy, to explicitly modeling hierarchical structures as an inductive bias? We discover that a 1426 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1426–1436 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics certain class of LSTM language models that explicitly models syntactic structures, the recurrent neural network grammars (Dyer et al., 2016, RNNGs), considerably outperforms sequential LSTM language models for cases with multiple attractors (§3). We present experiments affirming that this gain is due to an explicit composition operator rather than the presence of predicted syntactic annotations. Rather surprisingly, syntactic LSTM language models without explicit composition have no advantage over sequential LSTMs that operate on word sequences, although these models can nevertheless be excellent predictors of phrase structures (Choe and Charniak, 2016). Having established the importance of modeling structures, we explore the hyp"
P18-1132,J93-2004,0,0.0611748,"rather than vase. In some sense, the composition operator can be understood as injecting a structural recency bias into the model design, as subjects and verbs that are sequentially apart are encouraged to be close together in the RNNGs’ representation. Recurrent Neural Network Grammars Experiments Here we summarize the experimental settings of running RNNGs on the number agreement dataset and discuss the empirical findings. Experimental settings. We obtain phrasestructure trees for the Linzen et al. (2016) dataset using a publicly available discriminative model8 trained on the Penn Treebank (Marcus et al., 1993). At training time, we use these predicted trees to derive action sequences on the training set, and train the RNNG model on these sequences.9 At test time, we compare the probabilities of the correct and incorrect verb forms given the prefix, which now includes both nonterminal and terminal symbols. An example of the stack contents (i.e. the prefix) when predicting the verb is provided in Fig. 3(a). We similarly run a grid search over the same hyper-parameter range as the sequential 7 For a complete example of action sequences, we refer the reader to the example provided by Dyer et al. (2016)"
P18-1132,P17-2025,0,0.0870533,"Missing"
P18-1132,N18-1108,0,0.0498081,"ale language model, the primary difference is that we do not map infrequent word types to their POS tags and that we subsample to obtain 500 test instances of each number of attractor due to computation cost; both preprocessing were also done by Linzen et al. (2016). 4 The pretrained large-scale language model is obtained from https://github.com/tensorflow/models/ tree/master/research/lm_1b. 5 This trend is also observed by comparing results with H=150 and H=250. While both models achieve near-identical performance for zero attractor, the model with H=250 persame finding as the recent work of Gulordava et al. (2018), who also find that LSTMs trained with language modeling objectives are able to learn number agreement well; here we additionally identify model capacity as one of the reasons for the discrepancy with the Linzen et al. (2016) results. While the pretrained large-scale language model of Jozefowicz et al. (2016) has certain advantages in terms of model capacity, more training data, and richer vocabulary, we suspect that the poorer performance is due to differences between their training domain and the number agreement testing domain, although the model still performs reasonably well in the numbe"
P18-1132,N03-1014,0,0.0402388,"c LSTM language models without explicit composition have no advantage over sequential LSTMs that operate on word sequences, although these models can nevertheless be excellent predictors of phrase structures (Choe and Charniak, 2016). Having established the importance of modeling structures, we explore the hypothesis that how we build the structure affects the model’s ability to identify structural dependencies in English. As RNNGs build phrase-structure trees through top-down operations, we propose extensions to the structure-building sequences and model architecture that enable left-corner (Henderson, 2003, 2004) and bottom-up (Chelba and Jelinek, 2000; Emami and Jelinek, 2005) generation orders (§4). Extensive prior work has characterized topdown, left-corner, and bottom-up parsing strategies in terms of cognitive plausibility (Pulman, 1986; Abney and Johnson, 1991; Resnik, 1992) and neurophysiological evidence in human sentence processing (Nelson et al., 2017). Here we move away from the realm of parsing and evaluate the three strategies as models of generation instead, and address the following empirical question: which generation order is most appropriately biased to model structural depend"
P18-1132,P04-1013,0,0.258321,"Missing"
P18-1132,D17-1215,0,0.0146658,", more training data, and richer vocabulary, we suspect that the poorer performance is due to differences between their training domain and the number agreement testing domain, although the model still performs reasonably well in the number agreement test set. Prior work has confirmed the notion that, in many cases, statistical models are able to achieve good performance under some aggregate metric by overfitting to patterns that are predictive in most cases, often at the expense of more difficult, infrequent instances that require deeper language understanding abilities (Rimell et al., 2009; Jia and Liang, 2017). In the vast majority of cases, structural dependencies between subjects and verbs highly overlap with sequential dependencies (Table 1). Nevertheless, the fact that number agreement accuracy gets worse as the number of attractors increases is consistent with a sequential recency bias in LSTMs: under this conjecture, identifying the correct structural dependency becomes harder when there are more adjacent nouns of different number forms than the true subject. If the sequential recency conjecture is correct, then LSTMs would perform worse when the structural dependency is more distant in the s"
P18-1132,C92-1032,0,0.803938,"tructures, we explore the hypothesis that how we build the structure affects the model’s ability to identify structural dependencies in English. As RNNGs build phrase-structure trees through top-down operations, we propose extensions to the structure-building sequences and model architecture that enable left-corner (Henderson, 2003, 2004) and bottom-up (Chelba and Jelinek, 2000; Emami and Jelinek, 2005) generation orders (§4). Extensive prior work has characterized topdown, left-corner, and bottom-up parsing strategies in terms of cognitive plausibility (Pulman, 1986; Abney and Johnson, 1991; Resnik, 1992) and neurophysiological evidence in human sentence processing (Nelson et al., 2017). Here we move away from the realm of parsing and evaluate the three strategies as models of generation instead, and address the following empirical question: which generation order is most appropriately biased to model structural dependencies in English, as indicated by number agreement accuracy? Our key finding is that the top-down generation outperforms left-corner and bottom-up variants for difficult cases with multiple attractors. In theory, the three traversal strategies approximate the same chain rule tha"
P18-1132,D09-1085,1,0.694538,"rms of model capacity, more training data, and richer vocabulary, we suspect that the poorer performance is due to differences between their training domain and the number agreement testing domain, although the model still performs reasonably well in the number agreement test set. Prior work has confirmed the notion that, in many cases, statistical models are able to achieve good performance under some aggregate metric by overfitting to patterns that are predictive in most cases, often at the expense of more difficult, infrequent instances that require deeper language understanding abilities (Rimell et al., 2009; Jia and Liang, 2017). In the vast majority of cases, structural dependencies between subjects and verbs highly overlap with sequential dependencies (Table 1). Nevertheless, the fact that number agreement accuracy gets worse as the number of attractors increases is consistent with a sequential recency bias in LSTMs: under this conjecture, identifying the correct structural dependency becomes harder when there are more adjacent nouns of different number forms than the true subject. If the sequential recency conjecture is correct, then LSTMs would perform worse when the structural dependency is"
P18-1132,J10-1001,0,0.0137268,"humans seem to exhibit (Marslen-Wilson, 1973; Tanenhaus et al., 1995); this line of thinking similarly motivates Charniak (2010), among others. While most work in this domain has been concerned with the parsing problem, our findings suggest that anticipatory mechanisms are also beneficial in capturing structural dependencies in language modeling. We note that our results are achieved using models that, in theory, are able to condition on the entire derivation history, while earlier work in sentence processing has focused on cognitive memory considerations, such as the memory-bounded model of Schuler et al. (2010). REDUCE 5 NT SW (S) Conclusion GEN (eats) NT SW (VP) GEN (worms) NT SW (NP) REDUCE REDUCE REDUCE N/A Figure 6: Example Derivation for left-corner traversal. Each NT SW(X) action adds the open nonterminal symbol (X to the stack, followed by a deterministic swap operator that swaps the top two elements on the stack. Discussion. In Table 5, we focus on empirical results for cases where the structural dependencies matter the most, corresponding to cases with two, three, and four attractors. All three RNNG variants outperform the sequential LSTM language model baseline for these cases. Nevertheles"
P18-1132,P17-1099,0,0.01002,"-up variants in capturing long-distance structural dependencies. 1 Figure 1: An example of the number agreement task with two attractors and a subject-verb distance of five. Introduction Recurrent neural networks (RNNs) are remarkably effective models of sequential data. Recent years have witnessed the widespread adoption of recurrent architectures such as LSTMs (Hochreiter and Schmidhuber, 1997) in various NLP tasks, with state of the art results in language modeling (Melis et al., 2018) and conditional generation tasks like machine translation (Bahdanau et al., 2015) and text summarization (See et al., 2017). Here we revisit the question asked by Linzen et al. (2016): as RNNs model word sequences without explicit notions of hierarchical structure, to what extent are these models able to learn non-local syntactic dependencies in natural language? Identifying number agreement between subjects and verbs—especially in the presence of attractors—can be understood as a cognitivelymotivated probe that seeks to distinguish hierarchical theories from sequential ones, as models that rely on sequential cues like the most recent noun would favor the incorrect verb form. We provide an example of this task in"
P18-1132,E17-2060,0,0.0252701,"to resolve dependencies between word tokens. Second, by nature of modeling characters, non-local structural dependencies are sequentially further apart than in the wordbased language model. On the other hand, character LSTMs have the ability to exploit and share informative morphological cues, such as the fact that plural nouns in English tend to end with ‘s’. As demonstrated on the last row of Table 2, we find that the character LSTM language model performs much worse at number agreement with multiple attractors compared to its word-based counterparts. This finding is consistent with that of Sennrich (2017), who find that character-level decoders in neural machine translation perform worse than subword models in capturing morphosyntactic agreement. To some extent, our finding demonstrates the limitations that character LSTMs face in learning structure from language modeling objectives, despite earlier evidence that character LSTM language models are able to implicitly acquire a lexicon (Le Godais et al., 2017). 3.1 RNNGs (Dyer et al., 2016) are language models that estimate the joint probability of string terminals and phrase-structure tree nonterminals. Here we use stack-only RNNGs that achieve"
P18-1132,C00-2137,0,0.311374,"cal results for cases where the structural dependencies matter the most, corresponding to cases with two, three, and four attractors. All three RNNG variants outperform the sequential LSTM language model baseline for these cases. Nevertheless, the top-down variant outperforms both left-corner and bottom-up strategies for difficult cases with three or more attractors, suggesting that the top-down strategy is most appropriately biased to model difficult number agreement dependencies in English. We run an approximate randomization test by stratifying the output and permuting within each stratum (Yeh, 2000) and find that, for four attractors, the performance difference between the top-down RNNG and the other variants is statistically significant at p &lt; 0.05. The success of the top-down traversal in the domain of number-agreement prediction is consistent with a classical view in parsing that argues top-down parsing is the most human-like parsing strategy since it is the most anticipatory. Only Given enough capacity, LSTMs trained on language modeling objectives are able to learn syntax-sensitive dependencies, as evidenced by accurate number agreement accuracy with multiple attractors. Despite thi"
P19-1337,P17-2021,0,0.0203852,"elinek, 2000; Roark, 2001; Henderson, 2004; Emami and Jelinek, 2005; Buys and Blunsom, 2015; Mirowski and Vlachos, 2015; Dyer et al., 2016; Kim et al., 2019). In contrast to these approaches, the DSA-LSTM only models the probability of surface strings, albeit with an auxiliary loss that distills the next-word predictive distribution of a syntactic language model. Earlier work has also explored multi-task learning with syntactic objectives as an auxiliary loss in language modelling and machine translation (Luong et al., 2016; Eriguchi et al., 2016; Nadejde et al., 2017; Enguehard et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017). Our approach of injecting syntactic bias through a KD objective is orthogonal to this approach, with the primary difference that here the student DSALSTM has no direct access to syntactic annotations; it does, however, have access to the teacher RNNG’s softmax distribution over the next word. Our approach is also closely related to recent work that introduces structurally-motivated inductive biases into language models. Chung et al. (2017) segmented the hidden state update of an RNN through a multi-scale hierarchical recurrence, thereby providing a shortcut to the gra"
P19-1337,J99-2004,0,0.176336,"0.77 (±0.02) 0.99 (±0.01) 0.93 (±0.02) 0.96 (±0.02) 0.94 (±0.03) 0.95 (±0.01) 0.95 (±0.03) 0.95 (±0.03) 0.93 (±0.02) 0.96 (±0.01) 0.96 (±0.02) 0.95 (±0.01) 0.96 0.93 0.94 0.82 0.85 0.88 0.85 0.82 0.78 0.79 0.86 0.93 (±0.01) 0.77 (±0.03) 0.63 (±0.02) 0.78 (±0.01) 0.83 (±0.02) 0.46 (±0.05) 0.82 (±0.02) 0.70 (±0.02) 0.96 0.91 0.87 0.91 0.93 (±0.06) 0.82 (±0.09) 0.88 (±0.05) 0.79 (±0.02) 0.28 (±0.05) 0.78 (±0.06) 0.53 (±0.04) 0.85 (±0.02) 0.98 0.81 0.90 0.88 Table 1: Replication of Marvin and Linzen (2018) results. M&L-Multi is the Marvin and Linzen (2018) LSTM trained on LM and CCG supertagging (Bangalore and Joshi, 1999; Clark and Curran, 2007) losses with an interpolation factor of 0.5. We report our LSTM LM, small LSTM† , and RNNG† performance († smaller training data; §3) in the format of mean (±standard deviation) of 10 identical models from different seeds. Results in bold denote the best among models trained on similar amounts of training data. 3.1 Recurrent Neural Network Grammars An RNNG defines the joint probability of surface string x and phrase-structure tree y, denoted as t(x, y). The model generates phrase-structure trees in a top-down, left-to-right manner through a series of action sequences i"
P19-1337,P17-1080,0,0.02996,"times as much data as the DSA-LSTM, this finding suggests that, at least in terms of syntactic competence, structural biases continue to be relevant even as the current generation of sequential LMs is able to exploit increasingly large amounts of data. 4.3 Probing for Hierarchical Information Having established the advantages of the DSALSTM on targeted syntactic evaluations, we turn to the question of analysing how its internal representation differs from that of a standard LSTM LM. To this end, we adopt the method of Blevins et al. (2018) and use a probe (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017; Conneau et al., 2018; Hewitt and Manning, 2019, inter alia) that 11 Goldberg (2019) applies an additional pre-processing step, removing sentences in which the focus verb does not appear as a single word in the word piece-based vocabulary; hence, the evaluation sentences are slightly different. predicts the grandparent constituent of a word token xt , based on its encoding ht under the pretrained LSTM. Under this framework, the accuracy of the probe on a held-out set can be understood as an indication of how well the hidden states encode the relevant syntactic information required to succeed"
P19-1337,P18-2003,0,0.0912624,"Association for Computational Linguistics faster to train. On targeted syntactic evaluations, it achieves better accuracy than: (i) a strong LSTM LM which, through careful hyperparameter tuning, performs much better than previously thought (§2); (ii) the teacher RNNG that exploits a hierarchical inductive bias but lacks scalability (§3); and (iii) a born-again network (Furlanello et al., 2018) that similarly learns from KD, albeit without a hierarchical bias from the teacher. We analyse the DSA-LSTM’s internal representation through the syntactic probe (Shi et al., 2016; Adi et al., 2017) of Blevins et al. (2018), and find that the learned representations encode hierarchical information to a large extent, despite the DSA-LSTM lacking direct access to syntactic annotation. While not directly comparable, on subject-verb agreement both the teacher RNNG and student DSA-LSTM outperform BERT (Devlin et al., 2019; Goldberg, 2019), which benefits from bidirectional information and is trained on 30 times as much data. Altogether, these findings suggest that structural biases continue to play an important role, even at massive data scales, in improving the linguistic competence of LMs. 2 Replication of Targeted"
P19-1337,W15-2108,1,0.890398,"Missing"
P19-1337,N16-1024,1,0.952594,"STMs (Hochreiter and Schmidhuber, 1997) have numerous practical applications, but it has also been shown that they do not always develop accurate syntactic generalisations (Marvin and Linzen, 2018). Thus, one strategy for improving LSTMs is to change their biases to facilitate more linguistically valid generalisations. This paper introduces a scalable method for introducing syntactic biases to LSTMs (and indeed, to any left-to-right language model trained with a cross-entropy objective) by distilling knowledge (Bucilˇa et al., 2006; Hinton et al., 2015) from recurrent neural network grammars (Dyer et al., 2016, RNNGs). RNNGs have been shown to successfully capture non-local syntactic dependencies (Kuncoro et al., 2018), achieve excellent parsing performance (Kuncoro et al., 2017; Fried et al., 2017), and correlate well with encephalography signals (Hale et al., 2018). Unfortunately, these benefits come at the expense of scalability, since the hierarchical constituent composition process (§3) within RNNGs means that the structure of the computation graph for a sentence varies according to its tree structure. Even with the help of automatic dynamic batching (Neubig et al., 2017a,b), RNNGs can be ten"
P19-1337,K17-1003,0,0.0736648,"Missing"
P19-1337,P16-1078,0,0.0251677,"some form of syntactic structure (Jurafsky et al., 1995; Chelba and Jelinek, 2000; Roark, 2001; Henderson, 2004; Emami and Jelinek, 2005; Buys and Blunsom, 2015; Mirowski and Vlachos, 2015; Dyer et al., 2016; Kim et al., 2019). In contrast to these approaches, the DSA-LSTM only models the probability of surface strings, albeit with an auxiliary loss that distills the next-word predictive distribution of a syntactic language model. Earlier work has also explored multi-task learning with syntactic objectives as an auxiliary loss in language modelling and machine translation (Luong et al., 2016; Eriguchi et al., 2016; Nadejde et al., 2017; Enguehard et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017). Our approach of injecting syntactic bias through a KD objective is orthogonal to this approach, with the primary difference that here the student DSALSTM has no direct access to syntactic annotations; it does, however, have access to the teacher RNNG’s softmax distribution over the next word. Our approach is also closely related to recent work that introduces structurally-motivated inductive biases into language models. Chung et al. (2017) segmented the hidden state update of an RNN through a mu"
P19-1337,P17-2012,0,0.0147635,"enderson, 2004; Emami and Jelinek, 2005; Buys and Blunsom, 2015; Mirowski and Vlachos, 2015; Dyer et al., 2016; Kim et al., 2019). In contrast to these approaches, the DSA-LSTM only models the probability of surface strings, albeit with an auxiliary loss that distills the next-word predictive distribution of a syntactic language model. Earlier work has also explored multi-task learning with syntactic objectives as an auxiliary loss in language modelling and machine translation (Luong et al., 2016; Eriguchi et al., 2016; Nadejde et al., 2017; Enguehard et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017). Our approach of injecting syntactic bias through a KD objective is orthogonal to this approach, with the primary difference that here the student DSALSTM has no direct access to syntactic annotations; it does, however, have access to the teacher RNNG’s softmax distribution over the next word. Our approach is also closely related to recent work that introduces structurally-motivated inductive biases into language models. Chung et al. (2017) segmented the hidden state update of an RNN through a multi-scale hierarchical recurrence, thereby providing a shortcut to the gradient propagation of lon"
P19-1337,P17-2025,0,0.0199916,"2018). Thus, one strategy for improving LSTMs is to change their biases to facilitate more linguistically valid generalisations. This paper introduces a scalable method for introducing syntactic biases to LSTMs (and indeed, to any left-to-right language model trained with a cross-entropy objective) by distilling knowledge (Bucilˇa et al., 2006; Hinton et al., 2015) from recurrent neural network grammars (Dyer et al., 2016, RNNGs). RNNGs have been shown to successfully capture non-local syntactic dependencies (Kuncoro et al., 2018), achieve excellent parsing performance (Kuncoro et al., 2017; Fried et al., 2017), and correlate well with encephalography signals (Hale et al., 2018). Unfortunately, these benefits come at the expense of scalability, since the hierarchical constituent composition process (§3) within RNNGs means that the structure of the computation graph for a sentence varies according to its tree structure. Even with the help of automatic dynamic batching (Neubig et al., 2017a,b), RNNGs can be ten times slower to train than a comparable LSTM as they benefit less from specialised hardware like GPUs. As such, RNNGs are an impractical alternative to computationally convenient architectures"
P19-1337,N18-1108,0,0.304663,"ecoding (Kuncoro et al., 2018). 1 By modelling each sentence separately, our setup is consistent with that of Marvin and Linzen (2018) but differs from those with cross-sentential context (Mikolov et al., 2010). 2 While BERT (Devlin et al., 2019) achieves even better number agreement performance (Goldberg, 2019), the results are not directly comparable since BERT operates nonincrementally and was trained on 500 times as much data. The current state of the art among models trained on the Linzen et al. (2016) training set is the adaptive universal transformer model (Dehghani et al., 2019). 3473 Gulordava et al. (2018) test perplexity Simple In a sentential complement Short VP coordination Long VP coordination Across a prepositional phrase Across a subject relative clause Across an object relative clause Across an object relative clause (no that) In an object relative clause In an object relative clause (no that) Average of subject-verb agreement Simple In a sentential complement Across a relative clause Average of reflexive anaphora Simple Across a relative clause Average of negative polarity items Average of all constructions Marvin & Linzen models Ours M&L-LSTM M&L-Multi Our LSTM 78.65 61.10 53.73 (±0.16"
P19-1337,D16-1257,0,0.022929,"orical distribution defined by an affine transformation and a softmax: at ∼ softmax(Wa ht + ba ). • If at ∈ {GEN, NT}, the model samples a terminal x or a non-terminal n from each respective categorical distribution as the next input: x ∼ softmax(Wx ht + bx ), n ∼ softmax(Wn ht + bn ). • If at = REDUCE, the topmost stack elements going back to the last incomplete non-terminal are popped, and a composition function (here a bidirectional LSTM) is executed to represent the completed phrase on the stack. This recursive composition function constitutes a primary difference with the syntactic LM of Choe and Charniak (2016) that operates sequentially, and has been found to be crucial for achieving good number agreement (Kuncoro et al., 2018) and correlation with brain signals (Hale et al., 2018). The stack LSTM, composition function, lookup embeddings, and pairs of affine transformation weights and biases {W, b} are model parameters. 3.2 Experiments Here we outline the experimental settings and present our RNNG findings. Experimental settings. We implement the RNNG with DyNet and enable autobatching on GPU. Predicted phrase-structure trees for the training and validation sets of the Gulordava et al. (2018) Wikip"
P19-1337,P18-1254,1,0.940646,"es to facilitate more linguistically valid generalisations. This paper introduces a scalable method for introducing syntactic biases to LSTMs (and indeed, to any left-to-right language model trained with a cross-entropy objective) by distilling knowledge (Bucilˇa et al., 2006; Hinton et al., 2015) from recurrent neural network grammars (Dyer et al., 2016, RNNGs). RNNGs have been shown to successfully capture non-local syntactic dependencies (Kuncoro et al., 2018), achieve excellent parsing performance (Kuncoro et al., 2017; Fried et al., 2017), and correlate well with encephalography signals (Hale et al., 2018). Unfortunately, these benefits come at the expense of scalability, since the hierarchical constituent composition process (§3) within RNNGs means that the structure of the computation graph for a sentence varies according to its tree structure. Even with the help of automatic dynamic batching (Neubig et al., 2017a,b), RNNGs can be ten times slower to train than a comparable LSTM as they benefit less from specialised hardware like GPUs. As such, RNNGs are an impractical alternative to computationally convenient architectures that are used to build language models from massive corpora (Peters e"
P19-1337,P04-1013,0,0.195863,"Missing"
P19-1337,J07-4004,1,0.60422,".93 (±0.02) 0.96 (±0.02) 0.94 (±0.03) 0.95 (±0.01) 0.95 (±0.03) 0.95 (±0.03) 0.93 (±0.02) 0.96 (±0.01) 0.96 (±0.02) 0.95 (±0.01) 0.96 0.93 0.94 0.82 0.85 0.88 0.85 0.82 0.78 0.79 0.86 0.93 (±0.01) 0.77 (±0.03) 0.63 (±0.02) 0.78 (±0.01) 0.83 (±0.02) 0.46 (±0.05) 0.82 (±0.02) 0.70 (±0.02) 0.96 0.91 0.87 0.91 0.93 (±0.06) 0.82 (±0.09) 0.88 (±0.05) 0.79 (±0.02) 0.28 (±0.05) 0.78 (±0.06) 0.53 (±0.04) 0.85 (±0.02) 0.98 0.81 0.90 0.88 Table 1: Replication of Marvin and Linzen (2018) results. M&L-Multi is the Marvin and Linzen (2018) LSTM trained on LM and CCG supertagging (Bangalore and Joshi, 1999; Clark and Curran, 2007) losses with an interpolation factor of 0.5. We report our LSTM LM, small LSTM† , and RNNG† performance († smaller training data; §3) in the format of mean (±standard deviation) of 10 identical models from different seeds. Results in bold denote the best among models trained on similar amounts of training data. 3.1 Recurrent Neural Network Grammars An RNNG defines the joint probability of surface string x and phrase-structure tree y, denoted as t(x, y). The model generates phrase-structure trees in a top-down, left-to-right manner through a series of action sequences in a process reminiscent o"
P19-1337,N19-1419,0,0.0365748,"ding suggests that, at least in terms of syntactic competence, structural biases continue to be relevant even as the current generation of sequential LMs is able to exploit increasingly large amounts of data. 4.3 Probing for Hierarchical Information Having established the advantages of the DSALSTM on targeted syntactic evaluations, we turn to the question of analysing how its internal representation differs from that of a standard LSTM LM. To this end, we adopt the method of Blevins et al. (2018) and use a probe (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017; Conneau et al., 2018; Hewitt and Manning, 2019, inter alia) that 11 Goldberg (2019) applies an additional pre-processing step, removing sentences in which the focus verb does not appear as a single word in the word piece-based vocabulary; hence, the evaluation sentences are slightly different. predicts the grandparent constituent of a word token xt , based on its encoding ht under the pretrained LSTM. Under this framework, the accuracy of the probe on a held-out set can be understood as an indication of how well the hidden states encode the relevant syntactic information required to succeed in this task. We use a linear classifier for the"
P19-1337,P18-1198,0,0.0312272,"the DSA-LSTM, this finding suggests that, at least in terms of syntactic competence, structural biases continue to be relevant even as the current generation of sequential LMs is able to exploit increasingly large amounts of data. 4.3 Probing for Hierarchical Information Having established the advantages of the DSALSTM on targeted syntactic evaluations, we turn to the question of analysing how its internal representation differs from that of a standard LSTM LM. To this end, we adopt the method of Blevins et al. (2018) and use a probe (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017; Conneau et al., 2018; Hewitt and Manning, 2019, inter alia) that 11 Goldberg (2019) applies an additional pre-processing step, removing sentences in which the focus verb does not appear as a single word in the word piece-based vocabulary; hence, the evaluation sentences are slightly different. predicts the grandparent constituent of a word token xt , based on its encoding ht under the pretrained LSTM. Under this framework, the accuracy of the probe on a held-out set can be understood as an indication of how well the hidden states encode the relevant syntactic information required to succeed in this task. We use a"
P19-1337,P18-1031,0,0.024229,"ly, these benefits come at the expense of scalability, since the hierarchical constituent composition process (§3) within RNNGs means that the structure of the computation graph for a sentence varies according to its tree structure. Even with the help of automatic dynamic batching (Neubig et al., 2017a,b), RNNGs can be ten times slower to train than a comparable LSTM as they benefit less from specialised hardware like GPUs. As such, RNNGs are an impractical alternative to computationally convenient architectures that are used to build language models from massive corpora (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019; Radford et al., 2019). As RNNGs are hard to scale, we instead use the predictions of an RNNG teacher model trained on a small training set, to guide the learning of syntactic structure in a sequential LSTM student model, which is trained on the training set in its entirety. We denote the resulting lanugage model (i.e., the student LSTM) as a distilled syntaxaware LSTM LM (DSA-LSTM). Intuitively, the RNNG teacher is an expert on syntactic generalisation, although it lacks the opportunity to learn the relevant semantic and common-sense knowledge from a large training corpu"
P19-1337,N19-1423,0,0.170169,"at the expense of scalability, since the hierarchical constituent composition process (§3) within RNNGs means that the structure of the computation graph for a sentence varies according to its tree structure. Even with the help of automatic dynamic batching (Neubig et al., 2017a,b), RNNGs can be ten times slower to train than a comparable LSTM as they benefit less from specialised hardware like GPUs. As such, RNNGs are an impractical alternative to computationally convenient architectures that are used to build language models from massive corpora (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019; Radford et al., 2019). As RNNGs are hard to scale, we instead use the predictions of an RNNG teacher model trained on a small training set, to guide the learning of syntactic structure in a sequential LSTM student model, which is trained on the training set in its entirety. We denote the resulting lanugage model (i.e., the student LSTM) as a distilled syntaxaware LSTM LM (DSA-LSTM). Intuitively, the RNNG teacher is an expert on syntactic generalisation, although it lacks the opportunity to learn the relevant semantic and common-sense knowledge from a large training corpus. By learning from b"
P19-1337,P15-1033,1,0.812139,"aining data; §3) in the format of mean (±standard deviation) of 10 identical models from different seeds. Results in bold denote the best among models trained on similar amounts of training data. 3.1 Recurrent Neural Network Grammars An RNNG defines the joint probability of surface string x and phrase-structure tree y, denoted as t(x, y). The model generates phrase-structure trees in a top-down, left-to-right manner through a series of action sequences in a process reminiscent of shift-reduce parsing. At any given state, the decision over which action to take is parameterised by a stack LSTM (Dyer et al., 2015) encoding partially-completed constituents. Let ht be the stack LSTM hidden state at time t. The next action at ∈ {GEN, NT, REDUCE} is sampled according to a categorical distribution defined by an affine transformation and a softmax: at ∼ softmax(Wa ht + ba ). • If at ∈ {GEN, NT}, the model samples a terminal x or a non-terminal n from each respective categorical distribution as the next input: x ∼ softmax(Wx ht + bx ), n ∼ softmax(Wn ht + bn ). • If at = REDUCE, the topmost stack elements going back to the last incomplete non-terminal are popped, and a composition function (here a bidirection"
P19-1337,D16-1139,0,0.28832,"interthat syntactic regularities are inferred. In contrast, polate the distillation (left) and LM (right) losses: the interpolated target assigns a minimum probability of 0.5 to the correct label, but crucially con5 This procedure of training a student LSTM LM on string tains additional information about the plausibility samples from the RNNG with K ≈ 3, 000, 000 yields a high of every alternative based on the teacher RNNG’s validation perplexity of above 1,000, due to the enormity of the sample space and the use of discrete samples. predictions. Under this objective, the plural verbs 6 While Kim and Rush (2016) proposed a technique for sequence-level KD for machine translation through beam search, the same technique is not directly applicable to LM, which is an unconditional language generation problem. 7 Recall that `KD (x; θ) does not depend on the true next word x∗j . 8 We use the same pre-trained Berkeley parser to obtain training and validation trees in §3. 9 ˆ berk The resulting syntactic prefix y &lt;j (x) for approximating ∗ t(w |x &lt;j ) under the RNNG is obtained from a Berkeley parser that has access to yet unseen words x&gt;j . 3476 Figure 1: Example of the KD target (top), the standard LM targe"
P19-1337,N19-1114,1,0.836935,"esting that the means by which the DSA-LSTM achieves better syntactic competence is by tracking more hierarchical information during sequential processing. 5 Related Work Augmenting language models with syntactic information and structural inductive bias has been a long-standing area of research. To this end, syntactic language models estimate the joint probability of surface strings and some form of syntactic structure (Jurafsky et al., 1995; Chelba and Jelinek, 2000; Roark, 2001; Henderson, 2004; Emami and Jelinek, 2005; Buys and Blunsom, 2015; Mirowski and Vlachos, 2015; Dyer et al., 2016; Kim et al., 2019). In contrast to these approaches, the DSA-LSTM only models the probability of surface strings, albeit with an auxiliary loss that distills the next-word predictive distribution of a syntactic language model. Earlier work has also explored multi-task learning with syntactic objectives as an auxiliary loss in language modelling and machine translation (Luong et al., 2016; Eriguchi et al., 2016; Nadejde et al., 2017; Enguehard et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017). Our approach of injecting syntactic bias through a KD objective is orthogonal to this approach, with the"
P19-1337,E17-1117,1,0.884318,"Missing"
P19-1337,P18-1132,1,0.922299,"at they do not always develop accurate syntactic generalisations (Marvin and Linzen, 2018). Thus, one strategy for improving LSTMs is to change their biases to facilitate more linguistically valid generalisations. This paper introduces a scalable method for introducing syntactic biases to LSTMs (and indeed, to any left-to-right language model trained with a cross-entropy objective) by distilling knowledge (Bucilˇa et al., 2006; Hinton et al., 2015) from recurrent neural network grammars (Dyer et al., 2016, RNNGs). RNNGs have been shown to successfully capture non-local syntactic dependencies (Kuncoro et al., 2018), achieve excellent parsing performance (Kuncoro et al., 2017; Fried et al., 2017), and correlate well with encephalography signals (Hale et al., 2018). Unfortunately, these benefits come at the expense of scalability, since the hierarchical constituent composition process (§3) within RNNGs means that the structure of the computation graph for a sentence varies according to its tree structure. Even with the help of automatic dynamic batching (Neubig et al., 2017a,b), RNNGs can be ten times slower to train than a comparable LSTM as they benefit less from specialised hardware like GPUs. As such,"
P19-1337,P18-1129,0,0.0160896,"eralisations (and we have shown that it largely does in §3), every training instance provides the student LSTM with a wealth of information about all the possible legitimate continuations according to the predictions of the hierarchical teacher, thereby making it easier for the student to learn the appropriate hierarchical constraints and generalisations. Differences with other KD work. Our approach departs from the predominant view of distillation primarily as a means of compressing knowledge from a bigger teacher or an ensemble to a compact student (Ba and Caruana, 2014; Kim and Rush, 2016; Liu et al., 2018, inter alia) in two important ways. First, here the teacher and student models are different in character, and not just in size: we transfer knowledge from a teacher that models the joint probability of strings and phrasestructure trees through hierarchical operations, to a student that only models surface strings through sequential operations. This setup presents an interesting dynamic since the DSA-LSTM has to mimic the predictions of the RNNG, which conditions on syntactic annotation to guide hierarchical operations, even though the DSA-LSTM itself has no direct access to any syntactic ann"
P19-1337,D18-1151,0,0.402372,"training data it learns from. On targeted syntactic evaluations, we find that, while sequential LSTMs perform much better than previously reported, our proposed technique substantially improves on this baseline, yielding a new state of the art. Our findings and analysis affirm the importance of structural biases, even in models that learn from large amounts of data. 1 Introduction Language models (LMs) based on sequential LSTMs (Hochreiter and Schmidhuber, 1997) have numerous practical applications, but it has also been shown that they do not always develop accurate syntactic generalisations (Marvin and Linzen, 2018). Thus, one strategy for improving LSTMs is to change their biases to facilitate more linguistically valid generalisations. This paper introduces a scalable method for introducing syntactic biases to LSTMs (and indeed, to any left-to-right language model trained with a cross-entropy objective) by distilling knowledge (Bucilˇa et al., 2006; Hinton et al., 2015) from recurrent neural network grammars (Dyer et al., 2016, RNNGs). RNNGs have been shown to successfully capture non-local syntactic dependencies (Kuncoro et al., 2018), achieve excellent parsing performance (Kuncoro et al., 2017; Fried"
P19-1337,P15-2084,0,0.0621342,"Missing"
P19-1337,W17-4707,0,0.0161857,"structure (Jurafsky et al., 1995; Chelba and Jelinek, 2000; Roark, 2001; Henderson, 2004; Emami and Jelinek, 2005; Buys and Blunsom, 2015; Mirowski and Vlachos, 2015; Dyer et al., 2016; Kim et al., 2019). In contrast to these approaches, the DSA-LSTM only models the probability of surface strings, albeit with an auxiliary loss that distills the next-word predictive distribution of a syntactic language model. Earlier work has also explored multi-task learning with syntactic objectives as an auxiliary loss in language modelling and machine translation (Luong et al., 2016; Eriguchi et al., 2016; Nadejde et al., 2017; Enguehard et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017). Our approach of injecting syntactic bias through a KD objective is orthogonal to this approach, with the primary difference that here the student DSALSTM has no direct access to syntactic annotations; it does, however, have access to the teacher RNNG’s softmax distribution over the next word. Our approach is also closely related to recent work that introduces structurally-motivated inductive biases into language models. Chung et al. (2017) segmented the hidden state update of an RNN through a multi-scale hierarchical"
P19-1337,N18-1202,0,0.0669526,"., 2018). Unfortunately, these benefits come at the expense of scalability, since the hierarchical constituent composition process (§3) within RNNGs means that the structure of the computation graph for a sentence varies according to its tree structure. Even with the help of automatic dynamic batching (Neubig et al., 2017a,b), RNNGs can be ten times slower to train than a comparable LSTM as they benefit less from specialised hardware like GPUs. As such, RNNGs are an impractical alternative to computationally convenient architectures that are used to build language models from massive corpora (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019; Radford et al., 2019). As RNNGs are hard to scale, we instead use the predictions of an RNNG teacher model trained on a small training set, to guide the learning of syntactic structure in a sequential LSTM student model, which is trained on the training set in its entirety. We denote the resulting lanugage model (i.e., the student LSTM) as a distilled syntaxaware LSTM LM (DSA-LSTM). Intuitively, the RNNG teacher is an expert on syntactic generalisation, although it lacks the opportunity to learn the relevant semantic and common-sense knowledge fro"
P19-1337,N07-1051,0,0.207262,"Missing"
P19-1337,J01-2004,0,0.073207,"Missing"
P19-1337,D16-1159,0,0.185934,", Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics faster to train. On targeted syntactic evaluations, it achieves better accuracy than: (i) a strong LSTM LM which, through careful hyperparameter tuning, performs much better than previously thought (§2); (ii) the teacher RNNG that exploits a hierarchical inductive bias but lacks scalability (§3); and (iii) a born-again network (Furlanello et al., 2018) that similarly learns from KD, albeit without a hierarchical bias from the teacher. We analyse the DSA-LSTM’s internal representation through the syntactic probe (Shi et al., 2016; Adi et al., 2017) of Blevins et al. (2018), and find that the learned representations encode hierarchical information to a large extent, despite the DSA-LSTM lacking direct access to syntactic annotation. While not directly comparable, on subject-verb agreement both the teacher RNNG and student DSA-LSTM outperform BERT (Devlin et al., 2019; Goldberg, 2019), which benefits from bidirectional information and is trained on 30 times as much data. Altogether, these findings suggest that structural biases continue to play an important role, even at massive data scales, in improving the linguistic"
P19-1337,D17-1178,0,0.0710463,"Missing"
P19-1337,D18-1503,0,0.0447877,"Missing"
polajnar-etal-2014-evaluation,D10-1115,0,\N,Missing
polajnar-etal-2014-evaluation,P08-1028,0,\N,Missing
polajnar-etal-2014-evaluation,P12-1015,0,\N,Missing
polajnar-etal-2014-evaluation,S13-1004,0,\N,Missing
polajnar-etal-2014-evaluation,S12-1051,0,\N,Missing
Q14-1042,P13-2009,1,0.592847,"Missing"
Q14-1042,D11-1039,0,0.0147709,"as training signal (Liang et al., 2011; Kwiatkowski et al., 2013; Berant and Liang, 2014). However such an approach would not be straightforward to implement in our application, since the response from the system is not always the result of a database query but, e.g., a navigation instruction that is context-dependent and thus difficult to assess its correctness. Furthermore, it would require the development of a user simulator (Keizer et al., 2012), a non-trivial task which is beyond the scope of this work. A different approach is to use dialogs between a system and its users as proposed by Artzi and Zettlemoyer (2011) using the DARPA communicator corpus (Walker et al., 2002). However, in that work utterances were selected to be shorter than 6 words and to include one noun phrase present in the lexicon used during learning while ignoring short but common phrases such as “yes” and “no”; 557 thus it is unclear whether it would be applicable to our dataset. Finally, dialog context is only taken into account in predicting the dialog act for each utterance. Even though our corpus contains coreference information, we did not attempt this task as it is difficult to evaluate and our performance on node prediction o"
Q14-1042,W13-2322,0,0.0150392,"s. Finally, our approach to annotating coreference avoids repeating the MR of previous utterances, thus resulting in shorter expressions that are closer to the semantics of the NL utterances. The datasets developed in the recent dialog state tracking challenge (Henderson et al., 2014) also consist of dialogs between a user and a tourism information system. However the task is easier since only three entity types are considered (restaurant, coffeeshop and pub), a slot-filling MRL is used and the argument slots take values from fixed lists. The abstract meaning representation (AMR) described by Banarescu et al. (2013) was developed to provide a semantic interpretation layer to improve machine translation (MT) systems. It has similar predicate argument structure to the MRL proposed here, including a lack of cover for temporal relations and scoping. However, due to the different application domains (MT vs. tourism-related activities), there are some differences. Since MT systems operate at the sentence-level, each sentence is interpreted in isolation in AMR, whilst our proposed MRL takes context into account. Also, AMR tries to account for all the words in a sentence, whilst our MRL only tries to capture the"
Q14-1042,P14-1133,0,0.0329311,"to avoid error propagation, but also how to infer latent variables. The main bottleneck is training data sparsity. Some node types appear only a few times in relatively long utterances, and thus it is difficult to infer appropriate alignments for them. Unlike machine translation between natural languages, it is unrealistic to expect large quantities of utterances to be annotated with MR expressions. An appealing alternative would be to use response-based learning, i.e. use the response from the system instead of MR expressions as training signal (Liang et al., 2011; Kwiatkowski et al., 2013; Berant and Liang, 2014). However such an approach would not be straightforward to implement in our application, since the response from the system is not always the result of a database query but, e.g., a navigation instruction that is context-dependent and thus difficult to assess its correctness. Furthermore, it would require the development of a user simulator (Keizer et al., 2012), a non-trivial task which is beyond the scope of this work. A different approach is to use dialogs between a system and its users as proposed by Artzi and Zettlemoyer (2011) using the DARPA communicator corpus (Walker et al., 2002). Ho"
Q14-1042,P13-1042,0,0.124549,"ned classifiers by 9.0 and 4.8 points in F-score on the development and test sets respectively. 1 Introduction Semantic parsing is the task of translating natural language utterances into a machine-interpretable meaning representation (MR). Progress in semantic parsing has been facilitated by the existence of corpora containing utterances annotated with MRs, the most commonly used being ATIS (Dahl et al., 1994) and GeoQuery (Zelle, 1995). As these corpora cover rather narrow application domains, recent work has developed corpora to support natural language interfaces to the Freebase database (Cai and Yates, 2013), as well as the development of MT systems (Banarescu et al., 2013). However, these existing corpora have some important limitations. The MRs accompanying the utterances are typically restricted to some form of database query. Furthermore, in most cases each utterance is interpreted in isolation; thus utterances that use coreference or whose semantics are contextdependent are typically ignored. In this paper we present a new corpus for context-dependent semantic parsing to support the development of an interactive navigation and exploration system for tourismrelated activities. The new corpus"
Q14-1042,H94-1010,0,0.770624,"on system. We develop a semantic parser for this corpus by adapting the imitation learning algorithm DAGGER without requiring alignment information during training. DAGGER improves upon independently trained classifiers by 9.0 and 4.8 points in F-score on the development and test sets respectively. 1 Introduction Semantic parsing is the task of translating natural language utterances into a machine-interpretable meaning representation (MR). Progress in semantic parsing has been facilitated by the existence of corpora containing utterances annotated with MRs, the most commonly used being ATIS (Dahl et al., 1994) and GeoQuery (Zelle, 1995). As these corpora cover rather narrow application domains, recent work has developed corpora to support natural language interfaces to the Freebase database (Cai and Yates, 2013), as well as the development of MT systems (Banarescu et al., 2013). However, these existing corpora have some important limitations. The MRs accompanying the utterances are typically restricted to some form of database query. Furthermore, in most cases each utterance is interpreted in isolation; thus utterances that use coreference or whose semantics are contextdependent are typically ignor"
Q14-1042,P14-1134,0,0.0277687,"testing in the cross-scenario setup. 8 Comparison with Related Work Previous work on semantic parsing handled the lack of alignments during training in a variety of ways. Zettlemoyer and Collins (2009) manually engineered a CCG lexicon for the ATIS corpus. Kwiatkowski et al. (2011) used a dedicated algorithm to infer a similar dictionary and used alignments from Giza++ (Och and Ney, 2000) to initialize the relevant features. Most recent work on GeoQuery uses an alignment dictionary that includes for each geographical entity all noun phrases referring to it (Jones et al., 2012). More recently, Flanigan et al. (2014) developed a dedicated alignment model on top of which they learned a semantic parser for the AMR formalism. In our approach, we learn the alignments together with the semantic parser without requiring a dictionary. In terms of structured prediction frameworks, most previous work uses hidden variable linear (Zettlemoyer and Collins, 2007) or log-linear (Liang et al., 2011) models with beam search. In terms of direct comparisons with existing work, the goal of this paper is to introduce the new corpus and provide a competitive first attempt at the new semantic parsing task. However, we believe"
Q14-1042,Q13-1033,0,0.0358589,"Missing"
Q14-1042,D13-1152,0,0.046212,"Missing"
Q14-1042,P12-1051,0,0.0115051,"ogs are being used for training and testing in the cross-scenario setup. 8 Comparison with Related Work Previous work on semantic parsing handled the lack of alignments during training in a variety of ways. Zettlemoyer and Collins (2009) manually engineered a CCG lexicon for the ATIS corpus. Kwiatkowski et al. (2011) used a dedicated algorithm to infer a similar dictionary and used alignments from Giza++ (Och and Ney, 2000) to initialize the relevant features. Most recent work on GeoQuery uses an alignment dictionary that includes for each geographical entity all noun phrases referring to it (Jones et al., 2012). More recently, Flanigan et al. (2014) developed a dedicated alignment model on top of which they learned a semantic parser for the AMR formalism. In our approach, we learn the alignments together with the semantic parser without requiring a dictionary. In terms of structured prediction frameworks, most previous work uses hidden variable linear (Zettlemoyer and Collins, 2007) or log-linear (Liang et al., 2011) models with beam search. In terms of direct comparisons with existing work, the goal of this paper is to introduce the new corpus and provide a competitive first attempt at the new sema"
Q14-1042,D11-1140,0,0.0179634,"sting on the dialogs from the first scenario and training on the dialogs from the second, the overall performance using Imit+align was 36.9 points in F-score, while in the reverse experiment it was 41.7. Note that direct comparisons against the performances in Tbl. 3 are 556 not meaningful since fewer dialogs are being used for training and testing in the cross-scenario setup. 8 Comparison with Related Work Previous work on semantic parsing handled the lack of alignments during training in a variety of ways. Zettlemoyer and Collins (2009) manually engineered a CCG lexicon for the ATIS corpus. Kwiatkowski et al. (2011) used a dedicated algorithm to infer a similar dictionary and used alignments from Giza++ (Och and Ney, 2000) to initialize the relevant features. Most recent work on GeoQuery uses an alignment dictionary that includes for each geographical entity all noun phrases referring to it (Jones et al., 2012). More recently, Flanigan et al. (2014) developed a dedicated alignment model on top of which they learned a semantic parser for the AMR formalism. In our approach, we learn the alignments together with the semantic parser without requiring a dictionary. In terms of structured prediction frameworks"
Q14-1042,D13-1161,0,0.0254847,"GGER to learn not only how to avoid error propagation, but also how to infer latent variables. The main bottleneck is training data sparsity. Some node types appear only a few times in relatively long utterances, and thus it is difficult to infer appropriate alignments for them. Unlike machine translation between natural languages, it is unrealistic to expect large quantities of utterances to be annotated with MR expressions. An appealing alternative would be to use response-based learning, i.e. use the response from the system instead of MR expressions as training signal (Liang et al., 2011; Kwiatkowski et al., 2013; Berant and Liang, 2014). However such an approach would not be straightforward to implement in our application, since the response from the system is not always the result of a database query but, e.g., a navigation instruction that is context-dependent and thus difficult to assess its correctness. Furthermore, it would require the development of a user simulator (Keizer et al., 2012), a non-trivial task which is beyond the scope of this work. A different approach is to use dialogs between a system and its users as proposed by Artzi and Zettlemoyer (2011) using the DARPA communicator corpus"
Q14-1042,P11-1060,0,0.0773233,"nd Ney, 2000) to initialize the relevant features. Most recent work on GeoQuery uses an alignment dictionary that includes for each geographical entity all noun phrases referring to it (Jones et al., 2012). More recently, Flanigan et al. (2014) developed a dedicated alignment model on top of which they learned a semantic parser for the AMR formalism. In our approach, we learn the alignments together with the semantic parser without requiring a dictionary. In terms of structured prediction frameworks, most previous work uses hidden variable linear (Zettlemoyer and Collins, 2007) or log-linear (Liang et al., 2011) models with beam search. In terms of direct comparisons with existing work, the goal of this paper is to introduce the new corpus and provide a competitive first attempt at the new semantic parsing task. However, we believe it is non-trivial to apply existing approaches to the new task, since, assuming a decomposition similar to that of Sec. 5.1, exhaustive search would be too expensive, and applying vanilla beam search would be difficult since different predictions result in beams of (sometimes radically) different lengths that are not comparable. We have attempted applying the MT-based sema"
Q14-1042,P14-5010,0,0.00320201,"Missing"
Q14-1042,P00-1056,0,0.0211856,"ing Imit+align was 36.9 points in F-score, while in the reverse experiment it was 41.7. Note that direct comparisons against the performances in Tbl. 3 are 556 not meaningful since fewer dialogs are being used for training and testing in the cross-scenario setup. 8 Comparison with Related Work Previous work on semantic parsing handled the lack of alignments during training in a variety of ways. Zettlemoyer and Collins (2009) manually engineered a CCG lexicon for the ATIS corpus. Kwiatkowski et al. (2011) used a dedicated algorithm to infer a similar dictionary and used alignments from Giza++ (Och and Ney, 2000) to initialize the relevant features. Most recent work on GeoQuery uses an alignment dictionary that includes for each geographical entity all noun phrases referring to it (Jones et al., 2012). More recently, Flanigan et al. (2014) developed a dedicated alignment model on top of which they learned a semantic parser for the AMR formalism. In our approach, we learn the alignments together with the semantic parser without requiring a dictionary. In terms of structured prediction frameworks, most previous work uses hidden variable linear (Zettlemoyer and Collins, 2007) or log-linear (Liang et al.,"
Q14-1042,J00-3003,0,0.0235402,"Missing"
Q14-1042,D07-1071,0,0.0153974,"tionary and used alignments from Giza++ (Och and Ney, 2000) to initialize the relevant features. Most recent work on GeoQuery uses an alignment dictionary that includes for each geographical entity all noun phrases referring to it (Jones et al., 2012). More recently, Flanigan et al. (2014) developed a dedicated alignment model on top of which they learned a semantic parser for the AMR formalism. In our approach, we learn the alignments together with the semantic parser without requiring a dictionary. In terms of structured prediction frameworks, most previous work uses hidden variable linear (Zettlemoyer and Collins, 2007) or log-linear (Liang et al., 2011) models with beam search. In terms of direct comparisons with existing work, the goal of this paper is to introduce the new corpus and provide a competitive first attempt at the new semantic parsing task. However, we believe it is non-trivial to apply existing approaches to the new task, since, assuming a decomposition similar to that of Sec. 5.1, exhaustive search would be too expensive, and applying vanilla beam search would be difficult since different predictions result in beams of (sometimes radically) different lengths that are not comparable. We have a"
Q14-1042,P09-1110,0,0.0196491,"er evaluated on the other (still respecting the train/test split from before). When testing on the dialogs from the first scenario and training on the dialogs from the second, the overall performance using Imit+align was 36.9 points in F-score, while in the reverse experiment it was 41.7. Note that direct comparisons against the performances in Tbl. 3 are 556 not meaningful since fewer dialogs are being used for training and testing in the cross-scenario setup. 8 Comparison with Related Work Previous work on semantic parsing handled the lack of alignments during training in a variety of ways. Zettlemoyer and Collins (2009) manually engineered a CCG lexicon for the ATIS corpus. Kwiatkowski et al. (2011) used a dedicated algorithm to infer a similar dictionary and used alignments from Giza++ (Och and Ney, 2000) to initialize the relevant features. Most recent work on GeoQuery uses an alignment dictionary that includes for each geographical entity all noun phrases referring to it (Jones et al., 2012). More recently, Flanigan et al. (2014) developed a dedicated alignment model on top of which they learned a semantic parser for the AMR formalism. In our approach, we learn the alignments together with the semantic pa"
Q14-1042,bunt-etal-2012-iso,0,\N,Missing
Q14-1042,P13-1163,0,\N,Missing
Q17-1002,D13-1202,1,0.763078,"onal modelling there has been increasing importance attributed to grounding semantic models in sensory modalities, e.g., Bruni et al. (2014), Kiela and Bottou (2014). Andrews et al. (2009) demonstrated that multi-modal models formed by combining text-based distributional information with behaviourally generated conceptual properties (as a surrogate for perceptual experience) provide a better proxy for human-like intelligence. However, both the text-based and behaviourallybased components of their model were ultimately derived from linguistic information. Since then, in analyses of brain data, Anderson et al. (2013) have applied multi-modal models incorporating features that are truly grounded in natural image statistics to further support this claim. In addition, Anderson et al. (2015) have demonstrated that visually grounded models describe brain activity associated with internally induced visual features of objects as the ob17 Transactions of the Association for Computational Linguistics, vol. 5, pp. 17–30, 2017. Action Editor: Daichi Mochihashi. Submission batch: 2/2016; Revision batch: 7/2016; Published 1/2017. c 2017 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license."
Q17-1002,W04-2214,0,0.0431589,"domains (columns), and taxonomic categories (groups of 5 rows). The most concrete half of the words are indicated in bold font. Strike-throughs indicate words for which we did not have semantic model coverage. words in the norms of Barca et al. (2002). They then linked these to WordNet to identify the taxonomic category of the dominant sense of each word. Six taxonomic categories that were heavily populated with abstract words, as well as one unambiguously concrete category, were chosen. All categories supported ample coverage of Law and Music domains (determined according to WordNet Domains (Bentivogli et al., 2004)). Five law words and five music words were selected from each taxonomic category. Taxonomic categories and example stimulus words (translated into English) are as below: Ur-abstract: Anderson et al.’s term for concepts that are classified as abstract in WordNet but do not belong to a clear subcategory, e.g., law or music. At19 tribute: A construct whereby objects or individuals can be distinguished, e.g., legality, tonality. Communication: Something that is communicated by, to or between groups, e.g., accusation, symphony. Event/action: Something that happens at a given place and time, e.g.,"
Q17-1002,P13-1153,0,0.0342749,"on native Italians, and because approximately comparable text corpora in content were available in English and Italian (English and Italian Wikipedia), we were able to compare how well English and Italian text-based semantic models can decode neural activity patterns. Whilst Italian Wikipedia could reasonably be expected to be advantaged by supporting culturally appropriate nuances of semantic structure, it is disadvantaged by being considerably smaller than English Wikipedia. Taking inspiration from previous work exploiting cross-lingual resources (Richman and Schone, 2008; Shi et al., 2010; Darwish, 2013) we combined Italian and English text-based models in our decoding analyses in an attempt to leverage the benefits of both. Although combined language and English models tended to yield marginally better decoding accuracies, there were no significant differences between the different language models. Whilst we expect semantic structure on a grand scale to broadly straddle language boundaries for most concrete and abstract concepts (albeit with cultural specificities), this is proof of principle that cross linguistic commonalities are reflected in neural activity patterns measurable with curren"
Q17-1002,W10-0609,0,0.432764,"sity of Rochester aander41@ur.rochester.edu Douwe Kiela Computer Laboratory University of Cambridge dk427@cam.ac.uk Stephen Clark Massimo Poesio Computer Laboratory School of Computer Science and Electronic Engineering University of Cambridge University of Essex sc609@cam.ac.uk poesio@essex.ac.uk Abstract scanned as participants engage in conceptual tasks. This research has almost exclusively focused on brain activity elicited as participants comprehend concrete nouns as experimental stimuli. Different modelling approaches — predominantly distributional semantic models (Mitchell et al., 2008; Devereux et al., 2010; Murphy et al., 2012; Pereira et al., 2013; Carlson et al., 2014) and semantic models based on human behavioural estimation of conceptual features (Palatucci et al., 2009; Sudre et al., 2012; Chang et al., 2010; Bruffaerts et al., 2013; Fernandino et al., 2015) — have elucidated how different brain regions contribute to semantic representation of concrete nouns; however, how these results extend to non-concrete nouns is unknown. Important advances have recently been made using computational semantic models to decode brain activity patterns associated with concepts; however, this work has almo"
Q17-1002,P14-1046,0,0.010655,"computational representations derived from these images in the analysis. Secondary results are that we have exploited rep28 resentational similarity space to build group-level neural representations which better match our inherently group-level computational semantic models. In so doing, this exposes group-level commonalities in neural representation for both concrete and abstract words. Such group-level representations may prove both a useful test-bed for evaluating computational semantic models, as well as a potentially useful information source to incorporate into computational models (see Fyshe et al. (2014) for related work). Finally we have demonstrated that English and Italian text-based models are roughly interchangeable in our neural decoding task. That the English text-based model tended to return marginally higher results on our Italian brain data than the Italian model provides a cautionary note for future studies wishing to use semantic models from different languages to identify culturally specific aspects of neural semantic representation e.g., as a follow up to Zinszer et al. (2016). However we also note that the English Wikipedia data was larger than the corresponding Italian corpus."
Q17-1002,D14-1005,1,0.772322,"er, accuracy is significantly greater using the text-based models for the most abstract nouns. More generally this confirms that current computational models are sufficiently advanced to assist in investigating the representational structure of abstract concepts in the brain. 1 Introduction Since the work of Mitchell et al. (2008), there has been increasing interest in using computational semantic models to interpret neural activity patterns In computational modelling there has been increasing importance attributed to grounding semantic models in sensory modalities, e.g., Bruni et al. (2014), Kiela and Bottou (2014). Andrews et al. (2009) demonstrated that multi-modal models formed by combining text-based distributional information with behaviourally generated conceptual properties (as a surrogate for perceptual experience) provide a better proxy for human-like intelligence. However, both the text-based and behaviourallybased components of their model were ultimately derived from linguistic information. Since then, in analyses of brain data, Anderson et al. (2013) have applied multi-modal models incorporating features that are truly grounded in natural image statistics to further support this claim. In a"
Q17-1002,D15-1293,1,0.410712,"owing two factors. First, the dataset analysed was for a small sample of 67 words, and it is reasonable to conjecture that some of these words are also encoded in modalities other than vision and language. For example, musical words may be encoded in acoustic and motor features (see also Fernandino et al. (2015)). Future work will be necessary to verify that the findings generalise more broadly to words from domains beyond law and music. In work in progress the authors are undertaking more focused analyses on the current dataset, using textual, visual and newly developed audio semantic modes (Kiela and Clark, 2015) to tease apart linguistic, visual and acoustic contributions to semantic representation and how these vary throughout different regions of the brain. A second limitation of the current approach, as pointed out by a reviewer, is that the Google image search algorithm (the workings of which are unknown to the authors) may not perform as well for abstract words as it does for concrete words. Consequently, the visual model may have been handicapped compared to the textual model when decoding neural representations associated with more abstract words. We have no current measure of the degree of th"
Q17-1002,P14-2135,1,0.318578,"-wordout decoding procedure detailed later in Section 4 using the same method as Mitchell et al. (2008): Pearson’s correlation of each voxel’s activity between matched word lists in all scanning run pairs (10 unique run pairs giving 10 correlation coefficients of 68/70 words, where the other 2 words were test words to be decoded) was computed. The mean coefficient was used as stability measure. Voxels associated with the 500 largest stability measures were selected. 3 Semantic Models 3.1 Image-based semantic models Following previous work in multi-modal semantics (Bergsma and Van Durme, 2011; Kiela et al., 2014), we obtain a total of 20 images for each of the stimulus words from Google Images1 . Images from Google have been shown to yield representations that are competitive in quality compared to alternative resources (Bergsma and Van Durme, 2011; Fergus et al., 2005). Image representations are obtained by extracting the pre-softmax layer from a forward pass in a convolutional neural network (CNN) that has been trained on the ImageNet classification task using Caffe (Jia et al., 2014). This approach is similar to e.g., Kriegeskorte (2015), except that we only use the pre-softmax layer, which has bee"
Q17-1002,S12-1019,0,0.0487973,"r41@ur.rochester.edu Douwe Kiela Computer Laboratory University of Cambridge dk427@cam.ac.uk Stephen Clark Massimo Poesio Computer Laboratory School of Computer Science and Electronic Engineering University of Cambridge University of Essex sc609@cam.ac.uk poesio@essex.ac.uk Abstract scanned as participants engage in conceptual tasks. This research has almost exclusively focused on brain activity elicited as participants comprehend concrete nouns as experimental stimuli. Different modelling approaches — predominantly distributional semantic models (Mitchell et al., 2008; Devereux et al., 2010; Murphy et al., 2012; Pereira et al., 2013; Carlson et al., 2014) and semantic models based on human behavioural estimation of conceptual features (Palatucci et al., 2009; Sudre et al., 2012; Chang et al., 2010; Bruffaerts et al., 2013; Fernandino et al., 2015) — have elucidated how different brain regions contribute to semantic representation of concrete nouns; however, how these results extend to non-concrete nouns is unknown. Important advances have recently been made using computational semantic models to decode brain activity patterns associated with concepts; however, this work has almost exclusively focuse"
Q17-1002,P08-1001,0,0.0512656,"fMRI experiments were performed in Italian on native Italians, and because approximately comparable text corpora in content were available in English and Italian (English and Italian Wikipedia), we were able to compare how well English and Italian text-based semantic models can decode neural activity patterns. Whilst Italian Wikipedia could reasonably be expected to be advantaged by supporting culturally appropriate nuances of semantic structure, it is disadvantaged by being considerably smaller than English Wikipedia. Taking inspiration from previous work exploiting cross-lingual resources (Richman and Schone, 2008; Shi et al., 2010; Darwish, 2013) we combined Italian and English text-based models in our decoding analyses in an attempt to leverage the benefits of both. Although combined language and English models tended to yield marginally better decoding accuracies, there were no significant differences between the different language models. Whilst we expect semantic structure on a grand scale to broadly straddle language boundaries for most concrete and abstract concepts (albeit with cultural specificities), this is proof of principle that cross linguistic commonalities are reflected in neural activi"
Q17-1002,D10-1103,0,0.0189407,"formed in Italian on native Italians, and because approximately comparable text corpora in content were available in English and Italian (English and Italian Wikipedia), we were able to compare how well English and Italian text-based semantic models can decode neural activity patterns. Whilst Italian Wikipedia could reasonably be expected to be advantaged by supporting culturally appropriate nuances of semantic structure, it is disadvantaged by being considerably smaller than English Wikipedia. Taking inspiration from previous work exploiting cross-lingual resources (Richman and Schone, 2008; Shi et al., 2010; Darwish, 2013) we combined Italian and English text-based models in our decoding analyses in an attempt to leverage the benefits of both. Although combined language and English models tended to yield marginally better decoding accuracies, there were no significant differences between the different language models. Whilst we expect semantic structure on a grand scale to broadly straddle language boundaries for most concrete and abstract concepts (albeit with cultural specificities), this is proof of principle that cross linguistic commonalities are reflected in neural activity patterns measur"
S10-1060,J07-4004,1,0.813506,"urately reflects the parser’s analysis of the data. This paper describes the Cambridge submission to the SemEval-2010 Parser Evaluation using Textual Entailment (PETE) task. We used a simple definition of entailment, parsing both T and H with the C & C parser and checking whether the core grammatical relations (subject and object) produced for H were a subset of those for T. This simple system achieved the top score for the task out of those systems submitted. We analyze the errors made by the system and the potential role of the task in parser evaluation. 1 2 System We used the C & C parser (Clark and Curran, 2007), which can produce output in the form of grammatical relations (GRs), i.e. labelled headdependencies. For example, (nsubj tired man) for the example in Section 1 represents the fact that the NP headed by man is the subject of the predicate headed by tired. We chose to use the Stanford Dependency GR scheme (de Marneffe et al., 2006), but the same approach should work for other schemes (and other parsers producing GRs). Our entailment system was very simple, and based on the assumption that H is a simplified version of T (true for this task though not for RTE in general). We parsed both T and H"
S10-1060,de-marneffe-etal-2006-generating,0,0.0801363,"Missing"
S10-1060,S10-1009,0,0.125278,"Missing"
S10-1060,W00-1427,0,\N,Missing
W02-2203,C94-1024,0,0.0170602,"Missing"
W02-2203,J99-2004,0,0.833573,"ing a subset of the 1 206 category types by applying a frequency cut-off. Table 1 shows how many category tokens in Sections 2-21 do not appear in the category set entailed by the cut-off, and also shows the percentage of sentences that have at least one category token not in the set. The same figures are given for Section 00, which has 1 900 sentences and 44 544 category tokens. The figures show that the size of the category set can be reduced by as much as 3 & 4, without greatly increasing the percentage of missing category tokens in the data (unseen and seen). The LTAG supertag set used by Bangalore and Joshi (1999) contained 300 supertags (for their WSJ experiments). To obtain a CCG category set as compact as that requires a cut-off as high as 20. However, Bangalore and Joshi took their supertags from the manually created XTAG grammar, which presumably contains a much cleaner set of supertags than an automatically extracted grammar. A more useful comparison is with the work of Chen and Vijay-Shanker (2000), who extract an LTAG automatically from the Penn Treebank. A number of strategies are used for extracting the supertags (referred to as tree 1. There is a distinction between lexical categories and ca"
W02-2203,A00-1031,0,0.117845,"Missing"
W02-2203,E99-1025,0,0.063537,"Missing"
W02-2203,2000.iwpt-1.9,0,0.038098,"how that the size of the category set can be reduced by as much as 3 & 4, without greatly increasing the percentage of missing category tokens in the data (unseen and seen). The LTAG supertag set used by Bangalore and Joshi (1999) contained 300 supertags (for their WSJ experiments). To obtain a CCG category set as compact as that requires a cut-off as high as 20. However, Bangalore and Joshi took their supertags from the manually created XTAG grammar, which presumably contains a much cleaner set of supertags than an automatically extracted grammar. A more useful comparison is with the work of Chen and Vijay-Shanker (2000), who extract an LTAG automatically from the Penn Treebank. A number of strategies are used for extracting the supertags (referred to as tree 1. There is a distinction between lexical categories and categories created during a derivation. Since we are only interested in lexical categories in this paper, we may sometimes use category to mean lexical category. 2. The category for join has a PP-complement, which is arguably incorrect. Since the distinction between complements and adjuncts is not always reliably marked in the Penn Treebank, the procedure for identifying complements and adjuncts do"
W02-2203,hockenmaier-steedman-2002-acquiring,0,0.0292391,"Missing"
W02-2203,P02-1043,0,0.0580502,"Missing"
W02-2203,J93-2004,0,0.0254889,"Missing"
W03-0407,A00-1031,0,0.703418,"data. Previous co-training approaches have typically used the score assigned by the model as an indicator of the reliability of a newly labelled example. In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy algorithm which explicitly maximises the POS taggers’ agreement on unlabelled data. We investigate whether co-training based upon directly maximising agreement can be successfully applied to a pair of part-of-speech (POS) taggers: the Markov model T N T tagger (Brants, 2000) and the maximum entropy C&C tagger (Curran and Clark, 2003). There has been some previous work on boostrapping POS taggers (e.g., Zavrel and Daelemans (2000) and Cucerzan and Yarowsky (2002)), but to our knowledge no previous work on co-training POS taggers. The idea behind co-training the POS taggers is very simple: use output from the T N T tagger as additional labelled data for the maximum entropy tagger, and vice versa, in the hope that one tagger can learn useful information from the output of the other. Since the output of both taggers is noisy, there is a question of which newly labell"
W03-0407,W99-0613,0,0.226777,"ment-based co-training can significantly improve tagging performance for small seed datasets. Further results show that this form of co-training considerably outperforms self-training. However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost. 1 Introduction Co-training (Blum and Mitchell, 1998), and several variants of co-training, have been applied to a number of NLP problems, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001) and statistical parsing (Sarkar, 2001; Steedman et al., 2003). In each case, co-training was used successfully to bootstrap a model from only a small amount of labelled data and a much larger pool of unlabelled data. Previous co-training approaches have typically used the score assigned by the model as an indicator of the reliability of a newly labelled example. In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy"
W03-0407,W02-2018,0,0.0219854,"d to define the conditional probabilities of a tag given some context. The advantage of ME models over the Markov model used by T N T is that arbitrary features can easily be included in the context; so as well as considering the target word and the previous two tags (which is the information T N T uses), the ME models also consider the words either side of the target word and, for unknown and infrequent words, various properties of the string of the target word. A disadvantage is that the training times for ME models are usually relatively slow, especially with iterative scaling methods (see Malouf (2002) for alternative methods). Here we use Generalised Iterative Scaling (Darroch and Ratcliff, 1972), but our implementation is much faster than Ratnaparkhi’s publicly available tagger. The C&C tagger trains in less than 7 minutes on the 1 million words of the Penn Treebank, and tags slightly faster than T N T. Since the taggers share many common features, one might think they are not different enough for effective co-training to be possible. In fact, both taggers are sufficiently different for co-training to be effective. Section 4 shows that both taggers can benefit significantly from the infor"
W03-0407,J94-2001,0,0.329951,"Missing"
W03-0407,W01-0501,0,0.305384,"gging performance for small seed datasets. Further results show that this form of co-training considerably outperforms self-training. However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost. 1 Introduction Co-training (Blum and Mitchell, 1998), and several variants of co-training, have been applied to a number of NLP problems, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001) and statistical parsing (Sarkar, 2001; Steedman et al., 2003). In each case, co-training was used successfully to bootstrap a model from only a small amount of labelled data and a much larger pool of unlabelled data. Previous co-training approaches have typically used the score assigned by the model as an indicator of the reliability of a newly labelled example. In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy algorithm which explicitly maximises the POS tagge"
W03-0407,N01-1023,0,0.328731,"results show that this form of co-training considerably outperforms self-training. However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost. 1 Introduction Co-training (Blum and Mitchell, 1998), and several variants of co-training, have been applied to a number of NLP problems, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001) and statistical parsing (Sarkar, 2001; Steedman et al., 2003). In each case, co-training was used successfully to bootstrap a model from only a small amount of labelled data and a much larger pool of unlabelled data. Previous co-training approaches have typically used the score assigned by the model as an indicator of the reliability of a newly labelled example. In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy algorithm which explicitly maximises the POS taggers’ agreement on unlabelled data. We i"
W03-0407,E03-1008,1,0.830279,"that this form of co-training considerably outperforms self-training. However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost. 1 Introduction Co-training (Blum and Mitchell, 1998), and several variants of co-training, have been applied to a number of NLP problems, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001) and statistical parsing (Sarkar, 2001; Steedman et al., 2003). In each case, co-training was used successfully to bootstrap a model from only a small amount of labelled data and a much larger pool of unlabelled data. Previous co-training approaches have typically used the score assigned by the model as an indicator of the reliability of a newly labelled example. In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy algorithm which explicitly maximises the POS taggers’ agreement on unlabelled data. We investigate whether co-tr"
W03-0407,W02-2006,0,0.0259342,"a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy algorithm which explicitly maximises the POS taggers’ agreement on unlabelled data. We investigate whether co-training based upon directly maximising agreement can be successfully applied to a pair of part-of-speech (POS) taggers: the Markov model T N T tagger (Brants, 2000) and the maximum entropy C&C tagger (Curran and Clark, 2003). There has been some previous work on boostrapping POS taggers (e.g., Zavrel and Daelemans (2000) and Cucerzan and Yarowsky (2002)), but to our knowledge no previous work on co-training POS taggers. The idea behind co-training the POS taggers is very simple: use output from the T N T tagger as additional labelled data for the maximum entropy tagger, and vice versa, in the hope that one tagger can learn useful information from the output of the other. Since the output of both taggers is noisy, there is a question of which newly labelled examples to add to the training set. The additional data should be accurate, but also useful, providing the tagger with new information. Our work differs from the Blum and Mitchell (1998)"
W03-0407,P95-1026,0,0.0370871,"ing literature. Our results show that agreement-based co-training can significantly improve tagging performance for small seed datasets. Further results show that this form of co-training considerably outperforms self-training. However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost. 1 Introduction Co-training (Blum and Mitchell, 1998), and several variants of co-training, have been applied to a number of NLP problems, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001) and statistical parsing (Sarkar, 2001; Steedman et al., 2003). In each case, co-training was used successfully to bootstrap a model from only a small amount of labelled data and a much larger pool of unlabelled data. Previous co-training approaches have typically used the score assigned by the model as an indicator of the reliability of a newly labelled example. In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly la"
W03-0407,E03-1071,1,0.88395,"ly used the score assigned by the model as an indicator of the reliability of a newly labelled example. In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy algorithm which explicitly maximises the POS taggers’ agreement on unlabelled data. We investigate whether co-training based upon directly maximising agreement can be successfully applied to a pair of part-of-speech (POS) taggers: the Markov model T N T tagger (Brants, 2000) and the maximum entropy C&C tagger (Curran and Clark, 2003). There has been some previous work on boostrapping POS taggers (e.g., Zavrel and Daelemans (2000) and Cucerzan and Yarowsky (2002)), but to our knowledge no previous work on co-training POS taggers. The idea behind co-training the POS taggers is very simple: use output from the T N T tagger as additional labelled data for the maximum entropy tagger, and vice versa, in the hope that one tagger can learn useful information from the output of the other. Since the output of both taggers is noisy, there is a question of which newly labelled examples to add to the training set. The additional data"
W03-0407,zavrel-daelemans-2000-bootstrapping,0,0.10034,"example. In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy algorithm which explicitly maximises the POS taggers’ agreement on unlabelled data. We investigate whether co-training based upon directly maximising agreement can be successfully applied to a pair of part-of-speech (POS) taggers: the Markov model T N T tagger (Brants, 2000) and the maximum entropy C&C tagger (Curran and Clark, 2003). There has been some previous work on boostrapping POS taggers (e.g., Zavrel and Daelemans (2000) and Cucerzan and Yarowsky (2002)), but to our knowledge no previous work on co-training POS taggers. The idea behind co-training the POS taggers is very simple: use output from the T N T tagger as additional labelled data for the maximum entropy tagger, and vice versa, in the hope that one tagger can learn useful information from the output of the other. Since the output of both taggers is noisy, there is a question of which newly labelled examples to add to the training set. The additional data should be accurate, but also useful, providing the tagger with new information. Our work differs f"
W03-0407,A94-1009,0,0.124495,"Missing"
W03-0407,P02-1046,0,\N,Missing
W03-0424,W02-2003,0,0.0166223,"entify named entities with very high accuracy. The tagger uses features which can be obtained for a variety of languages and works effectively not only for English, but also for other languages such as German and Dutch. 1 Introduction Named Entity Recognition1 (NER) can be treated as a tagging problem where each word in a sentence is assigned a label indicating whether it is part of a named entity and the entity type. Thus methods used for part of speech (POS) tagging and chunking can also be used for NER. The papers from the CoNLL-2002 shared task which used such methods (e.g. Malouf (2002), Burger et al. (2002)) reported results significantly lower than the best system (Carreras et al., 2002). However, Zhou and Su (2002) have reported state of the art results on the MUC-6 and MUC-7 data using a HMM-based tagger. Zhou and Su (2002) used a wide variety of features, which suggests that the relatively poor performance of the taggers used in CoNLL-2002 was largely due to the feature sets used rather than the machine learning method. We demonstrate this to be the case by improving on the best Dutch results from CoNLL-2002 using a maximum entropy (ME) tagger. We report reasonable precision and recall (84.9"
W03-0424,E99-1025,0,0.0103392,". yn given a sentence w1 . . . wn is approximated as follows: p(y1 . . . yn |w1 . . . wn ) ≈ n Y p(yi |xi ) (2) i=1 where xi is the context for word wi . The tagger uses beam search to find the most probable sequence given the sentence. The features are binary valued functions which pair a tag with various elements of the context; for example:  1 if word(x) = Moody & y = I-PER fj (x, y) = 0 otherwise (3) word(x) = Moody is an example of a contextual predicate. Generalised Iterative Scaling (GIS) is used to estimate the values of the weights. The tagger uses a Gaussian prior over the weights (Chen et al., 1999) which allows a large number of rare, but informative, features to be used without overfitting. Condition freq(wi ) < 5 ∀wi ∀wi ∀wi Contextual predicate X is prefix of wi , |X |≤ 4 X is suffix of wi , |X |≤ 4 wi contains a digit wi contains uppercase character wi contains a hyphen wi = X wi−1 = X, wi−2 = X wi+1 = X, wi+2 = X POS i = X POS i−1 = X, POS i−2 = X POS i+1 = X, POS i+2 = X NEi−1 = X NEi−2 NEi−1 = XY Condition freq(wi ) < 5 ∀wi ∀wi Table 1: Contextual predicates in baseline system 3 The Data We used three data sets: the English and German data for the CoNLL-2003 shared task (Tjong Ki"
W03-0424,E03-1071,1,0.458487,"t data, and an F-score of 68.4 for the CoNLL-2003 German test data. 1 We assume that NER involves assigning the correct label to an entity as well as identifying its boundaries. Incorporating a diverse set of overlapping features in a HMM-based tagger is difficult and complicates the smoothing typically used for such taggers. In contrast, a ME tagger can easily deal with diverse, overlapping features. We also use a Gaussian prior on the parameters for effective smoothing over the large feature space. 2 The ME Tagger The ME tagger is based on Ratnaparkhi (1996)’s POS tagger and is described in Curran and Clark (2003) . The tagger uses models of the form: ! n X 1 exp λi fi (x, y) (1) p(y|x) = Z(x) i=1 where y is the tag, x is the context and the fi (x, y) are the features with associated weights λi . The probability of a tag sequence y1 . . . yn given a sentence w1 . . . wn is approximated as follows: p(y1 . . . yn |w1 . . . wn ) ≈ n Y p(yi |xi ) (2) i=1 where xi is the context for word wi . The tagger uses beam search to find the most probable sequence given the sentence. The features are binary valued functions which pair a tag with various elements of the context; for example:  1 if word(x) = Moody & y"
W03-0424,W02-2024,0,0.0213805,"tting. Condition freq(wi ) < 5 ∀wi ∀wi ∀wi Contextual predicate X is prefix of wi , |X |≤ 4 X is suffix of wi , |X |≤ 4 wi contains a digit wi contains uppercase character wi contains a hyphen wi = X wi−1 = X, wi−2 = X wi+1 = X, wi+2 = X POS i = X POS i−1 = X, POS i−2 = X POS i+1 = X, POS i+2 = X NEi−1 = X NEi−2 NEi−1 = XY Condition freq(wi ) < 5 ∀wi ∀wi Table 1: Contextual predicates in baseline system 3 The Data We used three data sets: the English and German data for the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003) and the Dutch data for the CoNLL2002 shared task (Tjong Kim Sang, 2002). Each word in the data sets is annotated with a named entity tag plus POS tag, and the words in the German and English data also have a chunk tag. Our system does not currently exploit the chunk tags. There are 4 types of entities to be recognised: persons, locations, organisations, and miscellaneous entities not belonging to the other three classes. The 2002 data uses the IOB-2 format in which a B - XXX tag indicates the first word of an entity of type XXX and I - XXX is used for subsequent words in an entity of type XXX. The tag O indicates words outside of a named entity. The 2003 data use"
W03-0424,W02-2008,1,0.652247,"name and last name gazetteers as shown in Table 6. These gazetteers are used for predicates applied to the current, previous and next word in the window. Collins (2002) includes a number of interesting contextual predicates for NER. One feature we have adapted encodes whether the current word is more frequently seen lowercase than uppercase in a large external corpus. This feature is useful for disambiguating beginning of sentence capitalisation and tagging sentences which are all capitalised. The frequency counts have been obtained from 1 billion words of English newspaper text collected by Curran and Osborne (2002). Collins (2002) also describes a mapping from words to word types which groups words with similar orthographic forms into classes. This involves mapping characters to classes and merging adjacent characters of the same type. For example, Moody becomes Aa, A.B.C. becomes A.A.A. and 1,345.05 becomes 0,0.0. The classes are used to define unigram, bigram and trigram contextual predicates over the window. We have also defined additional composite features which are a combination of atomic features; for example, a feature which is active for mid-sentence titlecase words seen more frequently as lowe"
W03-0424,W02-2019,0,0.134021,"ormation and identify named entities with very high accuracy. The tagger uses features which can be obtained for a variety of languages and works effectively not only for English, but also for other languages such as German and Dutch. 1 Introduction Named Entity Recognition1 (NER) can be treated as a tagging problem where each word in a sentence is assigned a label indicating whether it is part of a named entity and the entity type. Thus methods used for part of speech (POS) tagging and chunking can also be used for NER. The papers from the CoNLL-2002 shared task which used such methods (e.g. Malouf (2002), Burger et al. (2002)) reported results significantly lower than the best system (Carreras et al., 2002). However, Zhou and Su (2002) have reported state of the art results on the MUC-6 and MUC-7 data using a HMM-based tagger. Zhou and Su (2002) used a wide variety of features, which suggests that the relatively poor performance of the taggers used in CoNLL-2002 was largely due to the feature sets used rather than the machine learning method. We demonstrate this to be the case by improving on the best Dutch results from CoNLL-2002 using a maximum entropy (ME) tagger. We report reasonable prec"
W03-0424,W02-2031,0,0.0155258,"o words with mixed lower- and uppercase (e.g. CityBank). The length predicates encode the number of characters in the word from 1 to 15, with a single predicate for lengths greater than 15. The next set of contextual predicates encode extra information about NE tags in the current context. The memory NE tag predicate (see e.g. Malouf (2002)) records the NE tag that was most recently assigned to the current word. The use of beam-search tagging means that tags can only be recorded from previous sentences. This memory is cleared at the beginning of each document. The unigram predicates (see e.g. Tsukamoto et al. (2002)) encode the most probable tag for the next words in the window. The unigram probabilities are relative frequencies obtained from the training data. This feature enables us to know something about the likely NE tag of the next word before reaching it. Most systems use gazetteers to encode information about personal and organisation names, locations and trigger words. There is considerable variation in the size of the gazetteers used. Some studies found that gazetteers did not improve performance (e.g. Malouf (2002)) whilst others gained significant improvement using gazetteers and triggers (e."
W03-0424,P02-1060,0,0.0610272,"guages and works effectively not only for English, but also for other languages such as German and Dutch. 1 Introduction Named Entity Recognition1 (NER) can be treated as a tagging problem where each word in a sentence is assigned a label indicating whether it is part of a named entity and the entity type. Thus methods used for part of speech (POS) tagging and chunking can also be used for NER. The papers from the CoNLL-2002 shared task which used such methods (e.g. Malouf (2002), Burger et al. (2002)) reported results significantly lower than the best system (Carreras et al., 2002). However, Zhou and Su (2002) have reported state of the art results on the MUC-6 and MUC-7 data using a HMM-based tagger. Zhou and Su (2002) used a wide variety of features, which suggests that the relatively poor performance of the taggers used in CoNLL-2002 was largely due to the feature sets used rather than the machine learning method. We demonstrate this to be the case by improving on the best Dutch results from CoNLL-2002 using a maximum entropy (ME) tagger. We report reasonable precision and recall (84.9 F-score) for the CoNLL-2003 English test data, and an F-score of 68.4 for the CoNLL-2003 German test data. 1 We"
W03-0424,W02-2004,0,\N,Missing
W03-0424,W03-0419,0,\N,Missing
W03-0424,P02-1062,0,\N,Missing
W03-1013,J97-4005,0,0.203373,"ebank to be used for estimation. 1 Introduction Statistical parsing models have recently been developed for Combinatory Categorial Grammar (CCG, Steedman (2000)) and used in wide-coverage parsers applied to the WSJ Penn Treebank (Clark et al., 2002; Hockenmaier and Steedman, 2002). An attraction of CCG is its elegant treatment of coordination and extraction, allowing recovery of the long-range dependencies inherent in these constructions. We would like the parsing model to include long-range dependencies, but this introduces problems for generative parsing models similar to those described by Abney (1997) for attribute-value grammars; hence Hockenmaier and Steedman do not include such dependencies in their model, and Clark et al. include the dependencies but use an inconsistent model. Following Abney, we propose a loglinear framework which incorporates long-range dependencies as features without loss of consistency. Log-linear models have previously been applied to statistical parsing (Johnson et al., 1999; Toutanova et al., 2002; Riezler et al., 2002; Osborne, 2000). Typically, these approaches have enumerated all possible parses for model estimation and finding the most probable parse. For g"
W03-1013,P02-1042,1,0.863843,"stical parsing, under the assumption that all possible parses for a sentence can be enumerated. Enumerating all parses is infeasible for large grammars; however, dynamic programming over a packed chart can be used to efficiently estimate the model parameters. We describe a parellelised implementation which runs on a Beowulf cluster and allows the complete WSJ Penn Treebank to be used for estimation. 1 Introduction Statistical parsing models have recently been developed for Combinatory Categorial Grammar (CCG, Steedman (2000)) and used in wide-coverage parsers applied to the WSJ Penn Treebank (Clark et al., 2002; Hockenmaier and Steedman, 2002). An attraction of CCG is its elegant treatment of coordination and extraction, allowing recovery of the long-range dependencies inherent in these constructions. We would like the parsing model to include long-range dependencies, but this introduces problems for generative parsing models similar to those described by Abney (1997) for attribute-value grammars; hence Hockenmaier and Steedman do not include such dependencies in their model, and Clark et al. include the dependencies but use an inconsistent model. Following Abney, we propose a loglinear framework wh"
W03-1013,W02-2018,0,0.0497924,"obability mass for the outside (at least for the feature forests we have defined). 8 {d |d ∈δ(c),d ,d} i The normalisation constant ZS is the sum of the inside scores for the root disjunctive nodes: X ZS = φdr (16) dr ∈R In order to calculate inside scores, the scores for daughter nodes need to be calculated before the scores for mother nodes (and vice versa for the outside scores). This can easily be achieved by ordering the nodes in the bottom-up CKY parsing order. Note that the inside-outside approach can be combined with any maximum entropy estimation procedure, such as those evaluated by Malouf (2002). Finally, in order to avoid overfitting, we use a Gaussian prior on the parameters of the model (Chen and Rosenfeld, 1999), which requires a slight modification to the update rule in (10). A Gaussian prior also handles the problem of “pseudo-maximal” features (Johnson et al., 1999). 6 The Parser The parser is based on Clark et al. (2002) and takes as input a POS-tagged sentence with a set of possible lexical categories assigned to each word. The supertagger of Clark (2002) provides the lexical categories, with a parameter setting which assigns around 4 categories per word on average. The pars"
W03-1013,C00-1085,0,0.205358,"model to include long-range dependencies, but this introduces problems for generative parsing models similar to those described by Abney (1997) for attribute-value grammars; hence Hockenmaier and Steedman do not include such dependencies in their model, and Clark et al. include the dependencies but use an inconsistent model. Following Abney, we propose a loglinear framework which incorporates long-range dependencies as features without loss of consistency. Log-linear models have previously been applied to statistical parsing (Johnson et al., 1999; Toutanova et al., 2002; Riezler et al., 2002; Osborne, 2000). Typically, these approaches have enumerated all possible parses for model estimation and finding the most probable parse. For grammars extracted from the Penn Treebank (in our case CCGbank (Hockenmaier, 2003)), enumerating all parses is infeasible. One approach to this problem is to sample the parse space for estimation, e.g. Osborne (2000). In this paper we use a dynamic programming technique applied to a packed chart, similar to those proposed by Geman and Johnson (2002) and Miyao and Tsujii (2002), which efficiently estimates the model parameters over the complete space without enumeratin"
W03-1013,E03-1071,1,0.193226,"value of fi for a parse is then the sum of the values of fi for each conjunctive node in the parse. 5 Estimation using GIS GIS is a very simple algorithm for estimating the parameters of a log-linear model. The parameters are initialised to some arbitrary constant and the following update rule is applied until convergence: µ(t+1) i = µ(t) i E p˜ fi E p(t) fi ! C1 (10) where (t) is the iteration index and the constant C P is defined as maxω,S i fi (ω). In practice C is maximised over the sentences in the training data. Implementations of GIS typically use a “correction feature”, but following Curran and Clark (2003) we do not use such a feature, which simplifies the algorithm. Calculating E p(t) fi requires summing over all derivations which include fi for each packed chart in the training data. The key to performing this sum efficiently is to write the sum in terms of inside and outside scores for each conjunctive node. The inside and outside scores can be defined recursively, as in the inside-outside algorithm for PCFGs. If the inside score for a conjunctive node c is denoted φc , and the 5 Miyao and Tsujii have a single root conjunctive node; the disjunctive root nodes we define correspond to the root"
W03-1013,P02-1035,0,0.0917698,"ould like the parsing model to include long-range dependencies, but this introduces problems for generative parsing models similar to those described by Abney (1997) for attribute-value grammars; hence Hockenmaier and Steedman do not include such dependencies in their model, and Clark et al. include the dependencies but use an inconsistent model. Following Abney, we propose a loglinear framework which incorporates long-range dependencies as features without loss of consistency. Log-linear models have previously been applied to statistical parsing (Johnson et al., 1999; Toutanova et al., 2002; Riezler et al., 2002; Osborne, 2000). Typically, these approaches have enumerated all possible parses for model estimation and finding the most probable parse. For grammars extracted from the Penn Treebank (in our case CCGbank (Hockenmaier, 2003)), enumerating all parses is infeasible. One approach to this problem is to sample the parse space for estimation, e.g. Osborne (2000). In this paper we use a dynamic programming technique applied to a packed chart, similar to those proposed by Geman and Johnson (2002) and Miyao and Tsujii (2002), which efficiently estimates the model parameters over the complete space wi"
W03-1013,P02-1036,0,0.314858,"ar models have previously been applied to statistical parsing (Johnson et al., 1999; Toutanova et al., 2002; Riezler et al., 2002; Osborne, 2000). Typically, these approaches have enumerated all possible parses for model estimation and finding the most probable parse. For grammars extracted from the Penn Treebank (in our case CCGbank (Hockenmaier, 2003)), enumerating all parses is infeasible. One approach to this problem is to sample the parse space for estimation, e.g. Osborne (2000). In this paper we use a dynamic programming technique applied to a packed chart, similar to those proposed by Geman and Johnson (2002) and Miyao and Tsujii (2002), which efficiently estimates the model parameters over the complete space without enumerating parses. The estimation method is similar to the inside-outside algorithm used for estimating a PCFG (Lari and Young, 1990). Miyao and Tsujii (2002) apply their estimation technique to an automatically extracted Tree Adjoining Grammar using Improved Iterative Scaling (IIS, Della Pietra et al. (1997)). However, their model has significant memory requirements which limits them to using 868 sentences as training data. We use a parallelised version of Generalised Iterative Scal"
W03-1013,P02-1043,0,0.51676,"r the assumption that all possible parses for a sentence can be enumerated. Enumerating all parses is infeasible for large grammars; however, dynamic programming over a packed chart can be used to efficiently estimate the model parameters. We describe a parellelised implementation which runs on a Beowulf cluster and allows the complete WSJ Penn Treebank to be used for estimation. 1 Introduction Statistical parsing models have recently been developed for Combinatory Categorial Grammar (CCG, Steedman (2000)) and used in wide-coverage parsers applied to the WSJ Penn Treebank (Clark et al., 2002; Hockenmaier and Steedman, 2002). An attraction of CCG is its elegant treatment of coordination and extraction, allowing recovery of the long-range dependencies inherent in these constructions. We would like the parsing model to include long-range dependencies, but this introduces problems for generative parsing models similar to those described by Abney (1997) for attribute-value grammars; hence Hockenmaier and Steedman do not include such dependencies in their model, and Clark et al. include the dependencies but use an inconsistent model. Following Abney, we propose a loglinear framework which incorporates long-range depen"
W03-1013,P99-1069,0,0.571643,"ndencies inherent in these constructions. We would like the parsing model to include long-range dependencies, but this introduces problems for generative parsing models similar to those described by Abney (1997) for attribute-value grammars; hence Hockenmaier and Steedman do not include such dependencies in their model, and Clark et al. include the dependencies but use an inconsistent model. Following Abney, we propose a loglinear framework which incorporates long-range dependencies as features without loss of consistency. Log-linear models have previously been applied to statistical parsing (Johnson et al., 1999; Toutanova et al., 2002; Riezler et al., 2002; Osborne, 2000). Typically, these approaches have enumerated all possible parses for model estimation and finding the most probable parse. For grammars extracted from the Penn Treebank (in our case CCGbank (Hockenmaier, 2003)), enumerating all parses is infeasible. One approach to this problem is to sample the parse space for estimation, e.g. Osborne (2000). In this paper we use a dynamic programming technique applied to a packed chart, similar to those proposed by Geman and Johnson (2002) and Miyao and Tsujii (2002), which efficiently estimates t"
W03-1013,J96-1002,0,\N,Missing
W04-3215,J99-2004,0,\N,Missing
W04-3215,A00-2018,0,\N,Missing
W04-3215,C04-1180,1,\N,Missing
W04-3215,C04-1041,1,\N,Missing
W04-3215,J03-4003,0,\N,Missing
W04-3215,P02-1043,1,\N,Missing
W04-3215,P02-1018,0,\N,Missing
W04-3215,briscoe-carroll-2002-robust,0,\N,Missing
W04-3215,P02-1042,1,\N,Missing
W04-3215,P03-1055,0,\N,Missing
W04-3215,P03-1046,0,\N,Missing
W04-3215,P04-1014,1,\N,Missing
W04-3215,W97-1505,0,\N,Missing
W07-1202,P96-1011,0,0.11745,"s. In summary, the perceptron learner appears highly robust to the order of the training examples, at least for this parsing task. 6 Comparison with Other Work Taskar et al. (2004) investigate discriminative training methods for a phrase-structure parser, and also use dynamic programming for the decoder. The key difference between our work and theirs is that they are only able to train on sentences of 15 words or less, because of the expense of the decoding. There is work on discriminative models for dependency parsing (McDonald, 2006); since there are efficient decoding algorithms available (Eisner, 1996b), complete resources such as the Penn Treebank can used for estimation, leading to accurate parsers. There is also work on discriminative models for parse reranking (Collins and Koo, 2005). The main drawback with this approach is that the correct 15 parse may get lost in the first phase. The existing work most similar to ours is Collins and Roark (2004). They use a beam-search decoder as part of a phrase-structure parser to allow practical estimation. The main difference is that we are able to store the complete forests for training, and can guarantee that the forest contains the correct der"
W07-1202,C96-1058,0,0.0283044,"s. In summary, the perceptron learner appears highly robust to the order of the training examples, at least for this parsing task. 6 Comparison with Other Work Taskar et al. (2004) investigate discriminative training methods for a phrase-structure parser, and also use dynamic programming for the decoder. The key difference between our work and theirs is that they are only able to train on sentences of 15 words or less, because of the expense of the decoding. There is work on discriminative models for dependency parsing (McDonald, 2006); since there are efficient decoding algorithms available (Eisner, 1996b), complete resources such as the Penn Treebank can used for estimation, leading to accurate parsers. There is also work on discriminative models for parse reranking (Collins and Koo, 2005). The main drawback with this approach is that the correct 15 parse may get lost in the first phase. The existing work most similar to ours is Collins and Roark (2004). They use a beam-search decoder as part of a phrase-structure parser to allow practical estimation. The main difference is that we are able to store the complete forests for training, and can guarantee that the forest contains the correct der"
W07-1202,P02-1043,0,0.03414,"s for the complete CCGbank require over 20 GB of RAM . Reading the training instances into memory one at a time and keeping a record of the relevant feature counts would be too slow for practical development, since the log-linear model requires hundreds of iterations to converge. Hence the packed charts need to be stored in memory. In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of the BFGS training algorithm, to solve this problem. The need for cluster computing resources presents a barrier to the development of further CCG parsing models. Hockenmaier and Steedman (2002) describe a generative model for CCG, which only requires a non-iterative counting process for training, but it is generally acknowledged that discriminative models provide greater flexibility and typically higher performance. In this paper we propose the perceptron algorithm as a solution. The perceptron is an online learning algorithm, and so the parameters are updated one training instance at a time. However, the key difference compared with the loglinear training is that the perceptron converges in many fewer iterations, and so it is practical to read the training instances into memory one"
W07-1202,J99-2004,0,0.580946,"me training criterion. Hence, for efficient training, these methods require an efficient decoder; in fact, for methods like the perceptron, the update procedure is so trivial that the training algorithm essentially is decoding. This paper describes a decoder for a lexicalizedgrammar parser which is efficient enough for practical discriminative training. We use a lexicalized phrase-structure parser, the CCG parser of Clark and Curran (2004b), together with a DP-based decoder. The key idea is to exploit the properties of lexicalized grammars by using a finite-state supertagger prior to parsing (Bangalore and Joshi, 1999; Clark and Curran, 2004a). The decoder still uses the CKY algorithm, so the worst case complexity of 9 Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 9–16, c Prague, Czech Republic, June, 2007. 2007 Association for Computational Linguistics the parsing is unchanged; however, by allowing the supertagger to do much of the parsing work, the efficiency of the decoder is greatly increased in practice. We chose the perceptron for the training algorithm because it has shown good performance on other NLP tasks; in particular, Collins (2002) reported good performance for a p"
W07-1202,P06-4020,0,0.0508906,"Missing"
W07-1202,C04-1041,1,0.0464384,"of RAM for practical training compared to 20 GB for the log-linear model. We also investigate the order in which the training examples are presented to the online perceptron learner, and find that order does not significantly affect the results. 1 Introduction A recent development in data-driven parsing is the use of discriminative training methods (Riezler et al., 2002; Taskar et al., 2004; Collins and Roark, 2004; Turian and Melamed, 2006). One popular approach is to use a log-linear parsing model and maximise the conditional likelihood function (Johnson et al., 1999; Riezler et al., 2002; Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au Tsujii, 2005). Maximising the likelihood involves calculating feature expectations, which is computationally expensive. Dynamic programming (DP) in the form of the inside-outside algorithm can be used to calculate the expectations, if the features are sufficiently local (Miyao and Tsujii, 2002); however, the memory requirements can be prohibitive, especially for automatically extracted, wide-coverage grammars. In Clark and Curran (2004b) we u"
W07-1202,P04-1014,1,0.920905,"Missing"
W07-1202,P07-1032,1,0.881951,"Missing"
W07-1202,J05-1003,0,0.02199,"investigate discriminative training methods for a phrase-structure parser, and also use dynamic programming for the decoder. The key difference between our work and theirs is that they are only able to train on sentences of 15 words or less, because of the expense of the decoding. There is work on discriminative models for dependency parsing (McDonald, 2006); since there are efficient decoding algorithms available (Eisner, 1996b), complete resources such as the Penn Treebank can used for estimation, leading to accurate parsers. There is also work on discriminative models for parse reranking (Collins and Koo, 2005). The main drawback with this approach is that the correct 15 parse may get lost in the first phase. The existing work most similar to ours is Collins and Roark (2004). They use a beam-search decoder as part of a phrase-structure parser to allow practical estimation. The main difference is that we are able to store the complete forests for training, and can guarantee that the forest contains the correct derivation (assuming the grammar is able to generate it given the correct lexical categories). The downside of our approach is the restriction on the locality of the features, to allow dynamic"
W07-1202,P04-1015,0,0.333524,"h of the parsing work, resulting in a highly efficient decoder. The perceptron performs as well as the log-linear model; it trains in a few hours on a single machine; and it requires only a few hundred MB of RAM for practical training compared to 20 GB for the log-linear model. We also investigate the order in which the training examples are presented to the online perceptron learner, and find that order does not significantly affect the results. 1 Introduction A recent development in data-driven parsing is the use of discriminative training methods (Riezler et al., 2002; Taskar et al., 2004; Collins and Roark, 2004; Turian and Melamed, 2006). One popular approach is to use a log-linear parsing model and maximise the conditional likelihood function (Johnson et al., 1999; Riezler et al., 2002; Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au Tsujii, 2005). Maximising the likelihood involves calculating feature expectations, which is computationally expensive. Dynamic programming (DP) in the form of the inside-outside algorithm can be used to calculate the expectations, if the feat"
W07-1202,W02-1001,0,0.914374,"gger prior to parsing (Bangalore and Joshi, 1999; Clark and Curran, 2004a). The decoder still uses the CKY algorithm, so the worst case complexity of 9 Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 9–16, c Prague, Czech Republic, June, 2007. 2007 Association for Computational Linguistics the parsing is unchanged; however, by allowing the supertagger to do much of the parsing work, the efficiency of the decoder is greatly increased in practice. We chose the perceptron for the training algorithm because it has shown good performance on other NLP tasks; in particular, Collins (2002) reported good performance for a perceptron tagger compared to a Maximum Entropy tagger. Like Collins (2002), the decoder is the same for both the perceptron and the log-linear parsing models; the only change is the method for setting the weights. The perceptron model performs as well as the loglinear model, but is considerably easier to train. Another contribution of this paper is to advance wide-coverage CCG parsing. Previous discriminative models for CCG (Clark and Curran, 2004b) required cluster computing resources to train. In this paper we reduce the memory requirements from 20 GB of RAM"
W07-1202,P99-1069,0,0.0551292,"chine; and it requires only a few hundred MB of RAM for practical training compared to 20 GB for the log-linear model. We also investigate the order in which the training examples are presented to the online perceptron learner, and find that order does not significantly affect the results. 1 Introduction A recent development in data-driven parsing is the use of discriminative training methods (Riezler et al., 2002; Taskar et al., 2004; Collins and Roark, 2004; Turian and Melamed, 2006). One popular approach is to use a log-linear parsing model and maximise the conditional likelihood function (Johnson et al., 1999; Riezler et al., 2002; Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au Tsujii, 2005). Maximising the likelihood involves calculating feature expectations, which is computationally expensive. Dynamic programming (DP) in the form of the inside-outside algorithm can be used to calculate the expectations, if the features are sufficiently local (Miyao and Tsujii, 2002); however, the memory requirements can be prohibitive, especially for automatically extracted, wide-cover"
W07-1202,P05-1011,0,0.063773,"Missing"
W07-1202,P02-1035,0,0.0246402,"using a finite-state supertagger to do much of the parsing work, resulting in a highly efficient decoder. The perceptron performs as well as the log-linear model; it trains in a few hours on a single machine; and it requires only a few hundred MB of RAM for practical training compared to 20 GB for the log-linear model. We also investigate the order in which the training examples are presented to the online perceptron learner, and find that order does not significantly affect the results. 1 Introduction A recent development in data-driven parsing is the use of discriminative training methods (Riezler et al., 2002; Taskar et al., 2004; Collins and Roark, 2004; Turian and Melamed, 2006). One popular approach is to use a log-linear parsing model and maximise the conditional likelihood function (Johnson et al., 1999; Riezler et al., 2002; Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au Tsujii, 2005). Maximising the likelihood involves calculating feature expectations, which is computationally expensive. Dynamic programming (DP) in the form of the inside-outside algorithm can be u"
W07-1202,N06-1021,0,0.0186896,"n Standard order Shortest first Longest first 1 86.14 85.98 86.25 2 86.30 86.41 86.48 3 86.53 86.57 86.66 4 86.61 86.56 86.72 5 86.69 86.54 86.74 6 86.72 86.53 86.75 Table 5: F-score of the averaged perceptron on the development data for different data orderings (β = 0.002) perceptron model LP LR F CAT standard order 87.50 86.62 87.06 94.08 best random order 87.52 86.72 87.12 94.12 averaged 87.53 86.67 87.10 94.09 Table 6: Comparison of various perceptron models on the test data Finally, we used the 10 models (including the model from the original training set) to investigate model averaging. Corston-Oliver et al. (2006) motivate model averaging for the perceptron in terms of Bayes Point Machines. The averaged perceptron weights resulting from each permutation of the training data were simply averaged to produce a new model. Table 6 shows that the averaged model again performs only marginally better than the original model, and not as well as the best-performing “random” model, which is perhaps not surprising given the small variation among the performances of the component models. In summary, the perceptron learner appears highly robust to the order of the training examples, at least for this parsing task. 6"
W07-1202,W04-3201,0,0.476772,"supertagger to do much of the parsing work, resulting in a highly efficient decoder. The perceptron performs as well as the log-linear model; it trains in a few hours on a single machine; and it requires only a few hundred MB of RAM for practical training compared to 20 GB for the log-linear model. We also investigate the order in which the training examples are presented to the online perceptron learner, and find that order does not significantly affect the results. 1 Introduction A recent development in data-driven parsing is the use of discriminative training methods (Riezler et al., 2002; Taskar et al., 2004; Collins and Roark, 2004; Turian and Melamed, 2006). One popular approach is to use a log-linear parsing model and maximise the conditional likelihood function (Johnson et al., 1999; Riezler et al., 2002; Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au Tsujii, 2005). Maximising the likelihood involves calculating feature expectations, which is computationally expensive. Dynamic programming (DP) in the form of the inside-outside algorithm can be used to calculate the"
W07-1202,P06-1088,1,0.931104,"he model are defined over local parts of the derivation and include wordword dependencies. A packed chart representation allows efficient decoding, with the Viterbi algorithm finding the most probable derivation. The supertagger is a key part of the system. It uses a log-linear model to define a distribution over the lexical category set for each word and the previous two categories (Ratnaparkhi, 1996) and the forward backward algorithm efficiently sums over all histories to give a distibution for each word. These distributions are then used to assign a set of lexical categories to each word (Curran et al., 2006). Supertagging was first defined for LTAG (Bangalore and Joshi, 1999), and was designed to increase parsing speed for lexicalized grammars by allowing a finite-state tagger to do some of the parsing work. Since the elementary syntactic units in a lexicalized grammar — in LTAG’s case elementary trees and in CCG’s case lexical categories – contain a significant amount of grammatical information, combining them together is easier than the parsing typically performed by phrase-structure parsers. Hence Bangalore and Joshi (1999) refer to supertagging as almost parsing. Supertagging has been especia"
W07-1202,P06-1110,0,0.0177231,"sulting in a highly efficient decoder. The perceptron performs as well as the log-linear model; it trains in a few hours on a single machine; and it requires only a few hundred MB of RAM for practical training compared to 20 GB for the log-linear model. We also investigate the order in which the training examples are presented to the online perceptron learner, and find that order does not significantly affect the results. 1 Introduction A recent development in data-driven parsing is the use of discriminative training methods (Riezler et al., 2002; Taskar et al., 2004; Collins and Roark, 2004; Turian and Melamed, 2006). One popular approach is to use a log-linear parsing model and maximise the conditional likelihood function (Johnson et al., 1999; Riezler et al., 2002; Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au Tsujii, 2005). Maximising the likelihood involves calculating feature expectations, which is computationally expensive. Dynamic programming (DP) in the form of the inside-outside algorithm can be used to calculate the expectations, if the features are sufficiently local"
W07-2206,J99-2004,0,0.158496,"an, 2004b). The attraction of linguistically motivated parsers is the potential to produce rich output, in particular the predicate-argument structure representing the underlying meaning of a sentence. The disadvantage of such parsers is that they are typically not very efficient, parsing a few sentences per second on commodity hardware (Kaplan et al., 2004). The C&C CCG parser (Clark and Curran, 2004b) is an order of magnitude faster, but is still limited to around 25 sentences per second. The key to efficient CCG parsing is a finite-state supertagger which performs much of the parsing work (Bangalore and Joshi, 1999). CCG is a lexicalised grammar formalism, in which elementary syntactic structures — in CCG’s case lexical categories expressing subcategorisation information — are assigned to the words in a sentence. CCG supertagging can be performed accurately and efficiently by a Maximum Entropy tagger (Clark and Curran, 2004a). Since the lexical categories contain so much grammatical information, assigning them with low average ambiguity leaves the parser, which combines them together, with much less work to do at parse time. Hence Bangalore and Joshi (1999), in the context of LTAG parsing, refer to super"
W07-2206,P04-1041,0,0.0286954,"Missing"
W07-2206,C04-1041,1,0.950093,"mar formalism, in which elementary syntactic structures — in CCG’s case lexical categories expressing subcategorisation information — are assigned to the words in a sentence. CCG supertagging can be performed accurately and efficiently by a Maximum Entropy tagger (Clark and Curran, 2004a). Since the lexical categories contain so much grammatical information, assigning them with low average ambiguity leaves the parser, which combines them together, with much less work to do at parse time. Hence Bangalore and Joshi (1999), in the context of LTAG parsing, refer to supertagging as almost parsing. Clark and Curran (2004a) presents a novel method of integrating the supertagger and parser: initially only a small number of categories, on average, is assigned to each word, and the parser attempts to find a spanning analysis using the CKY chart-parsing algorithm. If one cannot be found, the parser requests more categories from the supertagger and builds the chart again from scratch. This process repeats until the parser is able to build a chart containing a spanning analysis.1 1 Tsuruoka and Tsujii (2004) investigate a similar idea in the context of the CKY algorithm for a PCFG. 39 Proceedings of the 10th Confere"
W07-2206,P04-1014,1,0.932691,"mar formalism, in which elementary syntactic structures — in CCG’s case lexical categories expressing subcategorisation information — are assigned to the words in a sentence. CCG supertagging can be performed accurately and efficiently by a Maximum Entropy tagger (Clark and Curran, 2004a). Since the lexical categories contain so much grammatical information, assigning them with low average ambiguity leaves the parser, which combines them together, with much less work to do at parse time. Hence Bangalore and Joshi (1999), in the context of LTAG parsing, refer to supertagging as almost parsing. Clark and Curran (2004a) presents a novel method of integrating the supertagger and parser: initially only a small number of categories, on average, is assigned to each word, and the parser attempts to find a spanning analysis using the CKY chart-parsing algorithm. If one cannot be found, the parser requests more categories from the supertagger and builds the chart again from scratch. This process repeats until the parser is able to build a chart containing a spanning analysis.1 1 Tsuruoka and Tsujii (2004) investigate a similar idea in the context of the CKY algorithm for a PCFG. 39 Proceedings of the 10th Confere"
W07-2206,J03-4003,0,0.0181837,"le, e.g. punctuation constraints. We find that the punctuation constraints are particularly effective while the gold standard chunks are required to gain any benefit for the NP constraints. Adding constraints also has the potential to increase coverage because the reduced search space means that longer sentences can 43 be parsed without exceeding the pre-defined limits on chart size. 5 Selective Beam Search Beam search involves greedy elimination of low probability partial derivations before they can form complete derivations. It is used in many parsers to reduce the search space, for example Collins (2003). We use a variable width beam where all categories c in a particular cell C that satisfy score(c) &lt; max{score(x)|x ∈ C} − B, for some beam cutoff B, are removed. The category scores score(c) are log probabilities. In the C&C parser, the entire packed chart is constructed first and then the spanning derivations are marked. Only the partial derivations that form part of spanning derivations are scored to select the best parse, which is a small fraction of the categories in the chart. Because the categories are scored with a complex statistical model with a large number of features, the time spe"
W07-2206,E03-1071,1,0.862477,"straints How can we know in advance that the correct derivation must yield specific spans, since this appears to require knowledge of the parse itself? We have explored constraints derived from shallow parsing and from the raw sentence. Our results demonstrate that simple constraints can reduce parsing time significantly without loss of coverage or accuracy. Chunk tags were used to create constraints. We experimented with both gold standard chunks from the Penn Treebank and also chunker output from the C&C chunk tagger. The tagger is very similar to the Maximum Entropy POS tagger described in Curran and Clark (2003). Only NP chunks were used because the accuracy of the tagger for other chunks is lower. The Penn Treebank chunks required modification because CCGbank analyses some constructions differently. We also created longer NPs by concatenating adjacent base NPs, for example in the case of possessives. A number of punctuation constraints were used and had a significant impact especially for longer sentences. There are a number of punctuation rules in CCGbank which absorb a punctuation mark by combining it with a category and returning a category of the same type. These rules are very productive, combi"
W07-2206,P06-1088,1,0.807349,"Missing"
W07-2206,P02-1043,0,0.0756201,"Missing"
W07-2206,N04-1013,0,0.0682503,"Missing"
W07-2206,N03-1016,0,0.0643485,"Missing"
W08-1307,P06-2006,0,0.175004,"the representations of the individual constructions, but also about the interfaces between constructions. For example, in the sentence Mary likes apples and pears, the coordination structure apples and pears serves as direct object of likes, and it must be determined which word(s) are used to represent the coordination in the direct object relation. We will illustrate some of the consequences of the decisions described here with detailed examples of three construction types. We focus on passive, coordination, and relative clause constructions, as analysed in the PARC (King et al., 2003), GR (Briscoe and Carroll, 2006), and Stanford (de Marneffe et al., 2006) evaluation schemes, using sentences from the shared task of the COLING 2008 parser evaluation workshop.1 These three constructions were chosen because we believe they provide particularly good illustrations of the various decisions and their consequences for scoring. Furthermore, they are constructions whose representation differs across at least two of the three grammatical relation schemes under dicsussion, which makes them more interesting as examples. We believe that the principles involved, however, The penultimate question, also a source of disag"
W08-1307,J07-4004,1,0.828594,"score reflecting the overall accuracy of a parser, which means that the construction’s overall contribution to the score is relevant.5 Observe also that partial credit is assigned differently in the three schemes. If the parser recognises the subject of made but misses the fact that the construction is a passive, for example, it will earn one out of two possible points in PARC, one out of three in GR (if it recognizes the auxiliary), but zero out of two in Stanford. This type of error may seem unlikely, yet examples are readily available. In related work we have evaluated the C & C parser of Clark and Curran (2007) on the BioInfer corpus of biomedical abstracts (Pyysalo et al., 2007), which includes the following sentence: Acanthamoeba profilin was cross-linked to actin via a zero-length isopeptide bond using carbodiimide. The parser correctly identifies profilin as the subject of cross-linked, yet because it misidentifies cross-linked as an adjectival rather than verbal predicate, it misses the passive construction. Finally, note an asymmetry in the partial credit scoring: a parser that misidentifies the subject (e.g. by selecting the wrong head), but basically gets the construction correct, will recei"
W08-1307,de-marneffe-etal-2006-generating,0,0.0830358,"Missing"
W08-1307,W03-2401,0,0.214311,"Introduction In this paper we examine the various decisions made by designers of parser evaluation schemes based on grammatical relations (Lin, 1995; Carroll et al., 1998). Following Carroll et al. (1998), we use the term grammatical relations to refer to syntactic dependencies between heads and dependents. We assume that grammatical relation schemes are currently the best method available for parser evaluation due to their relative independence of any particular parser or linguistic theory. There are several grammatical relation schemes currently available, for example Carroll et al. (1998), King et al. (2003), and de Marneffe et al. (2006). However, there has been little analysis of the decisions made by the designers in creating c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 44 Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 44–50 Manchester, August 2008 pendencies. A useful example here is tense and mood information for verbs. This is included in the PARC scheme, for example, but not in the Briscoe and Carroll or"
W09-3825,W00-1201,0,0.0810676,"Missing"
W09-3825,P05-1012,0,0.0755544,"ce. One advantage of this approach is that the parsing can be highly efficient, for example by pursuing a greedy strategy in which a single action is chosen at each decision point. The alternative approach, exemplified by Collins (1997) and Charniak (2000), is to use a chart-based algorithm to build the space of possible parses, together with pruning of lowprobability constituents and the Viterbi algorithm to find the highest scoring parse. For English dependency parsing, the two approaches give similar Stephen Clark Cambridge University Computer Laboratory stephen.clark@cl.cam.ac.uk results (McDonald et al., 2005; Nivre et al., 2006). For English constituent-based parsing using the Penn Treebank, the best performing transitionbased parser lags behind the current state-of-theart (Sagae and Lavie, 2005). In contrast, for Chinese, the best dependency parsers are currently transition-based (Duan et al., 2007; Zhang and Clark, 2008). For constituent-based parsing using the Chinese Treebank (CTB), Wang et al. (2006) have shown that a shift-reduce parser can give competitive accuracy scores together with high speeds, by using an SVM to make a single decision at each point in the parsing process. In this pape"
W09-3825,W06-2933,0,0.0384175,"Missing"
W09-3825,J93-1002,0,0.11149,"shift-reduce parsing model on parsing the Chinese Treebank 2 data (Wang et al., 2006). In this paper, we propose a global discriminative model based on the shift-reduce parsing process, combined with a beam-search decoder, obtaining competitive accuracies on CTB2. We also report the performance of the parser on CTB5 data, obtaining the highest scores in the literature for a dependencybased evaluation. 1 Introduction Transition-based statistical parsing associates scores with each decision in the parsing process, selecting the parse which is built by the highest scoring sequence of decisions (Briscoe and Carroll, 1993; Nivre et al., 2006). The parsing algorithm is typically some form of bottom-up shiftreduce algorithm, so that scores are associated with actions such as shift and reduce. One advantage of this approach is that the parsing can be highly efficient, for example by pursuing a greedy strategy in which a single action is chosen at each decision point. The alternative approach, exemplified by Collins (1997) and Charniak (2000), is to use a chart-based algorithm to build the space of possible parses, together with pruning of lowprobability constituents and the Viterbi algorithm to find the highest s"
W09-3825,N07-1051,0,0.257214,"Missing"
W09-3825,W08-2102,0,0.0334691,". The main difference is that our parser in this paper is for constituent parsing. In fact, our parser is one of only a few constituent parsers which have successfully applied global discriminative models, certainly without a generative baseline as a feature, whereas global models for dependency parsing have been comparatively easier to develop. 7 Conclusion The contributions of this paper can be summarized as follows. First, we defined a global discriminative model for Chinese constituent-based parsing, continuing recent work in this area which has focused on English (Clark and Curran, 2007; Carreras et al., 2008; Finkel et al., 2008). Second, we showed how such a model can be applied to shiftreduce parsing and combined with beam search, resulting in an accurate linear-time parser. In standard tests using CTB2 data, our parser achieved comparable Parseval F-score to the state-of-theart systems. Moreover, we observed that more training data lead to improvements on both accuracy and stability against feature variations, and reported performance of the parser using CTB5 data. By converting constituent-based output to dependency relations using standard head-finding rules, our parser also obtained the hig"
W09-3825,A00-2018,0,0.0368523,"ed statistical parsing associates scores with each decision in the parsing process, selecting the parse which is built by the highest scoring sequence of decisions (Briscoe and Carroll, 1993; Nivre et al., 2006). The parsing algorithm is typically some form of bottom-up shiftreduce algorithm, so that scores are associated with actions such as shift and reduce. One advantage of this approach is that the parsing can be highly efficient, for example by pursuing a greedy strategy in which a single action is chosen at each decision point. The alternative approach, exemplified by Collins (1997) and Charniak (2000), is to use a chart-based algorithm to build the space of possible parses, together with pruning of lowprobability constituents and the Viterbi algorithm to find the highest scoring parse. For English dependency parsing, the two approaches give similar Stephen Clark Cambridge University Computer Laboratory stephen.clark@cl.cam.ac.uk results (McDonald et al., 2005; Nivre et al., 2006). For English constituent-based parsing using the Penn Treebank, the best performing transitionbased parser lags behind the current state-of-theart (Sagae and Lavie, 2005). In contrast, for Chinese, the best depend"
W09-3825,C02-1126,0,0.0345237,"Missing"
W09-3825,J07-4004,1,0.534681,"ve model and beam search. The main difference is that our parser in this paper is for constituent parsing. In fact, our parser is one of only a few constituent parsers which have successfully applied global discriminative models, certainly without a generative baseline as a feature, whereas global models for dependency parsing have been comparatively easier to develop. 7 Conclusion The contributions of this paper can be summarized as follows. First, we defined a global discriminative model for Chinese constituent-based parsing, continuing recent work in this area which has focused on English (Clark and Curran, 2007; Carreras et al., 2008; Finkel et al., 2008). Second, we showed how such a model can be applied to shiftreduce parsing and combined with beam search, resulting in an accurate linear-time parser. In standard tests using CTB2 data, our parser achieved comparable Parseval F-score to the state-of-theart systems. Moreover, we observed that more training data lead to improvements on both accuracy and stability against feature variations, and reported performance of the parser using CTB5 data. By converting constituent-based output to dependency relations using standard head-finding rules, our parse"
W09-3825,P04-1015,0,0.743677,"ses the current model to decode training examples (the parse function in the pseudo-code). If the output is correct, it passes on to the next example. If the output is incorrect, it adjusts the weight values by adding the feature vector from the goldstandard output and subtracting the feature vector from the parser output. Weight values are updated for each example (making the process online) and the training data is iterated over T times. In order to avoid overfitting we used the now-standard averaged version of this algorithm (Collins, 2002). We also apply the early update modification from Collins and Roark (2004). If the agenda, at any point during the decoding process, does not contain the correct partial parse, it is not possible for the decoder to produce the correct output. In this case, decoding is stopped early and the weight values are updated using the highest scoring partial parse on the agenda. 4.1 Feature set Table 1 shows the set of feature templates for the model. Individual features are generated from 165 Description Unigrams Feature templates S0 tc, S0 wc, S1 tc, S1 wc, S2 tc, S2 wc, S3 tc, S3 wc, N0 wt, N1 wt, N2 wt, N3 wt, S0 lwc, S0 rwc, S0 uwc, S1 lwc, S1 rwc, S1 uwc, Bigrams S0 wS1"
W09-3825,P97-1003,0,0.0181104,"tion Transition-based statistical parsing associates scores with each decision in the parsing process, selecting the parse which is built by the highest scoring sequence of decisions (Briscoe and Carroll, 1993; Nivre et al., 2006). The parsing algorithm is typically some form of bottom-up shiftreduce algorithm, so that scores are associated with actions such as shift and reduce. One advantage of this approach is that the parsing can be highly efficient, for example by pursuing a greedy strategy in which a single action is chosen at each decision point. The alternative approach, exemplified by Collins (1997) and Charniak (2000), is to use a chart-based algorithm to build the space of possible parses, together with pruning of lowprobability constituents and the Viterbi algorithm to find the highest scoring parse. For English dependency parsing, the two approaches give similar Stephen Clark Cambridge University Computer Laboratory stephen.clark@cl.cam.ac.uk results (McDonald et al., 2005; Nivre et al., 2006). For English constituent-based parsing using the Penn Treebank, the best performing transitionbased parser lags behind the current state-of-theart (Sagae and Lavie, 2005). In contrast, for Chin"
W09-3825,W05-1513,0,0.139096,"ative approach, exemplified by Collins (1997) and Charniak (2000), is to use a chart-based algorithm to build the space of possible parses, together with pruning of lowprobability constituents and the Viterbi algorithm to find the highest scoring parse. For English dependency parsing, the two approaches give similar Stephen Clark Cambridge University Computer Laboratory stephen.clark@cl.cam.ac.uk results (McDonald et al., 2005; Nivre et al., 2006). For English constituent-based parsing using the Penn Treebank, the best performing transitionbased parser lags behind the current state-of-theart (Sagae and Lavie, 2005). In contrast, for Chinese, the best dependency parsers are currently transition-based (Duan et al., 2007; Zhang and Clark, 2008). For constituent-based parsing using the Chinese Treebank (CTB), Wang et al. (2006) have shown that a shift-reduce parser can give competitive accuracy scores together with high speeds, by using an SVM to make a single decision at each point in the parsing process. In this paper we describe a global discriminative model for Chinese shift-reduce parsing, and compare it with Wang et al.’s approach. We apply the same shift-reduce procedure as Wang et al. (2006), but in"
W09-3825,W03-1706,0,0.0223096,"re vector. q represents the count of any separator punctuation between S0 and S1 . Whenever an action is being considered at each point in the beam-search process, templates from Table 1 are matched with the context defined by the parser state and combined with the action to generate features. Negative features, which are the features from incorrect parser outputs but not from any training example, are included in the model. There are around a million features in our experiments with the CTB2 dataset. Wang et al. (2006) used a range of other features, including rhythmic features of S0 and S1 (Sun and Jurafsky, 2003), features from the most recently found node that is to the left or right of S0 and S1 , the number of words and the number of punctuations in S0 and S1 , the distance between S0 and S1 and so on. We did not include these features in our parser, because they did not lead to improved performance during development experiments. 5 Experiments The experiments were performed using the Chinese Treebank 2 and Chinese Treebank 5 data. Standard data preparation was performed before the experiments: empty terminal nodes were removed; any non-terminal nodes with no children were removed; any unary X → X"
W09-3825,P06-1054,0,0.106074,"m to find the highest scoring parse. For English dependency parsing, the two approaches give similar Stephen Clark Cambridge University Computer Laboratory stephen.clark@cl.cam.ac.uk results (McDonald et al., 2005; Nivre et al., 2006). For English constituent-based parsing using the Penn Treebank, the best performing transitionbased parser lags behind the current state-of-theart (Sagae and Lavie, 2005). In contrast, for Chinese, the best dependency parsers are currently transition-based (Duan et al., 2007; Zhang and Clark, 2008). For constituent-based parsing using the Chinese Treebank (CTB), Wang et al. (2006) have shown that a shift-reduce parser can give competitive accuracy scores together with high speeds, by using an SVM to make a single decision at each point in the parsing process. In this paper we describe a global discriminative model for Chinese shift-reduce parsing, and compare it with Wang et al.’s approach. We apply the same shift-reduce procedure as Wang et al. (2006), but instead of using a local classifier for each transition-based action, we train a generalized perceptron model over complete sequences of actions, so that the parameters are learned in the context of complete parses."
W09-3825,I05-1007,0,0.0525411,"Missing"
W09-3825,D08-1059,1,0.770866,"le parses, together with pruning of lowprobability constituents and the Viterbi algorithm to find the highest scoring parse. For English dependency parsing, the two approaches give similar Stephen Clark Cambridge University Computer Laboratory stephen.clark@cl.cam.ac.uk results (McDonald et al., 2005; Nivre et al., 2006). For English constituent-based parsing using the Penn Treebank, the best performing transitionbased parser lags behind the current state-of-theart (Sagae and Lavie, 2005). In contrast, for Chinese, the best dependency parsers are currently transition-based (Duan et al., 2007; Zhang and Clark, 2008). For constituent-based parsing using the Chinese Treebank (CTB), Wang et al. (2006) have shown that a shift-reduce parser can give competitive accuracy scores together with high speeds, by using an SVM to make a single decision at each point in the parsing process. In this paper we describe a global discriminative model for Chinese shift-reduce parsing, and compare it with Wang et al.’s approach. We apply the same shift-reduce procedure as Wang et al. (2006), but instead of using a local classifier for each transition-based action, we train a generalized perceptron model over complete sequenc"
W09-3825,W02-1001,0,0.239499,"Outputs: w ~ Figure 4: the perceptron learning algorithm for state item Y is defined by: Score(Y ) = w ~ · Φ(Y ) = X λi fi (Y ) i where Φ(Y ) is the global feature vector from Y , and w ~ is the weight vector defined by the model. Each element from Φ(Y ) represents the global count of a particular feature from Y . The feature set consists of a large number of features which pick out various configurations from the stack and queue, based on the words and subtrees in the state item. The features are described in Section 4.1. The weight values are set using the generalized perceptron algorithm (Collins, 2002). The perceptron algorithm is shown in Figure 4. It initializes weight values as all zeros, and uses the current model to decode training examples (the parse function in the pseudo-code). If the output is correct, it passes on to the next example. If the output is incorrect, it adjusts the weight values by adding the feature vector from the goldstandard output and subtracting the feature vector from the parser output. Weight values are updated for each example (making the process online) and the training data is iterated over T times. In order to avoid overfitting we used the now-standard aver"
W09-3825,P08-1109,0,0.0724614,"Missing"
W09-3825,P03-1056,0,0.0287834,"Missing"
W11-0114,D10-1115,0,0.579701,"Missing"
W11-0114,D08-1094,0,0.605185,"mathematical model of [3] can be implemented in a concrete setting which introduces a richer, not necessarily truth-theoretic, notion of natural language semantics which is closer to the ideas underlying standard distributional models of word meaning. We leave full evaluation to future work, in order to determine whether the following method in conjunction with word vectors built from large corpora leads to improved results on language processing tasks, such as computing sentence similarity and paraphrase evaluation. Nouns and Transitive Verbs. We take N to be a structured vector space, as in [4, 5]. The bases of N are annotated by ‘properties’ obtained by combining dependency relations with nouns, verbs and adjectives. For example, basis vectors might be associated with properties such as “arg-fluffy”, denoting the argument of the adjective fluffy, “subj-chase” denoting the subject of the verb chase, “obj-buy” denoting the object of the verb buy, and so on. We construct the vector for a noun by counting how many times in the corpus a word has been the argument of ‘fluffy’, the subject of ‘chase’, the object of ‘buy’, and so on. The framework in [3] offers no guidance as to what the sent"
W11-0114,P08-1028,0,0.801448,"ination of run and catch, such as: 1 2 chase = run + catch 3 3 Again, the meaning map ensures that these degrees propagate to the meaning of the whole sentence. For a worked out example see [3]. But neither of these examples provide a distributional sentence meaning. Here we take a first step towards a corpus-based distributional model, by attempting to recover a meaning for a sentence based on the meanings of the words derived from a corpus. But crucially this meaning goes beyond just composing the meanings of words using a vector operator, such as tensor product, summation or multiplication [8]. Our computation of sentence meaning treats some vectors as functions and others as 127 function arguments, according to how the words in the sentence are typed, and uses the syntactic structure as a guide to determine how the functions are applied to their arguments. The intuition behind this approach is that syntactic analysis guides semantic vector composition. The contribution of this paper is to introduce some concrete constructions for a compositional distributional model of meaning. These constructions demonstrate how the mathematical model of [3] can be implemented in a concrete setti"
W11-0114,J00-4006,0,\N,Missing
W11-0114,J98-1004,0,\N,Missing
W13-3005,D10-1115,0,0.49855,"and verbs. Hence, if one applies the contextual co-occurrence methods of distributional semantics to them, the result will be a set of dense vectors which do not discriminate between different meanings. The current state-of-the-art in compositional distributional semantics either adopts a simple method to obtain a vector for a sequence of words, such as adding or mutliplying the contextual vectors of the words (Mitchell and Lapata, 2008), or, based on the grammatical structure, builds linear maps for some words and applies these to the vector representations of the other words in the string (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011a). Neither of these approaches produce vectors which provide a good representation for the meanings of relative clauses. In the grammar-based approach, one has to assign a linear map to the relative pronoun, for instance a map f as follows: −−−−−−−−−−−−−→ −−−−−→ −→ − men who like Mary = f (− men, like Mary) This paper develops a compositional vector-based semantics of relative pronouns within a categorical framework. Frobenius algebras are used to formalise the operations required to model the semantics of relative pronouns, including passing information betw"
W13-3005,D11-1129,1,0.840637,"lies the contextual co-occurrence methods of distributional semantics to them, the result will be a set of dense vectors which do not discriminate between different meanings. The current state-of-the-art in compositional distributional semantics either adopts a simple method to obtain a vector for a sequence of words, such as adding or mutliplying the contextual vectors of the words (Mitchell and Lapata, 2008), or, based on the grammatical structure, builds linear maps for some words and applies these to the vector representations of the other words in the string (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011a). Neither of these approaches produce vectors which provide a good representation for the meanings of relative clauses. In the grammar-based approach, one has to assign a linear map to the relative pronoun, for instance a map f as follows: −−−−−−−−−−−−−→ −−−−−→ −→ − men who like Mary = f (− men, like Mary) This paper develops a compositional vector-based semantics of relative pronouns within a categorical framework. Frobenius algebras are used to formalise the operations required to model the semantics of relative pronouns, including passing information between the relative clause and the mo"
W13-3005,W11-2507,1,0.726222,"lies the contextual co-occurrence methods of distributional semantics to them, the result will be a set of dense vectors which do not discriminate between different meanings. The current state-of-the-art in compositional distributional semantics either adopts a simple method to obtain a vector for a sequence of words, such as adding or mutliplying the contextual vectors of the words (Mitchell and Lapata, 2008), or, based on the grammatical structure, builds linear maps for some words and applies these to the vector representations of the other words in the string (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011a). Neither of these approaches produce vectors which provide a good representation for the meanings of relative clauses. In the grammar-based approach, one has to assign a linear map to the relative pronoun, for instance a map f as follows: −−−−−−−−−−−−−→ −−−−−→ −→ − men who like Mary = f (− men, like Mary) This paper develops a compositional vector-based semantics of relative pronouns within a categorical framework. Frobenius algebras are used to formalise the operations required to model the semantics of relative pronouns, including passing information between the relative clause and the mo"
W13-3005,P08-1028,0,0.0837992,"The difficulty with pronouns is that the contexts in which they occur do not seem to provide a suitable representation of their meanings: pronouns tend to occur with a great many nouns and verbs. Hence, if one applies the contextual co-occurrence methods of distributional semantics to them, the result will be a set of dense vectors which do not discriminate between different meanings. The current state-of-the-art in compositional distributional semantics either adopts a simple method to obtain a vector for a sequence of words, such as adding or mutliplying the contextual vectors of the words (Mitchell and Lapata, 2008), or, based on the grammatical structure, builds linear maps for some words and applies these to the vector representations of the other words in the string (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011a). Neither of these approaches produce vectors which provide a good representation for the meanings of relative clauses. In the grammar-based approach, one has to assign a linear map to the relative pronoun, for instance a map f as follows: −−−−−−−−−−−−−→ −−−−−→ −→ − men who like Mary = f (− men, like Mary) This paper develops a compositional vector-based semantics of relative"
W13-3005,J98-1004,0,0.170028,"Missing"
W13-3005,D11-1014,0,0.0318106,"Missing"
W13-3005,C10-1142,0,0.0218262,"Missing"
W14-1406,D10-1115,0,0.705961,"to a distributional semantics (Clarke, 2012; Pulman, 2013; Erk, 2012). 1.1 Distributional Semantics We assume a basic knowledge of distributional semantics (Grefenstette, 1994; Sch¨utze, 1998). Re46 Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 46–54, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics ing recent techniques from “recursive” neural networks (Socher et al., 2010). Another possibility is suggested by Grefenstette et al. (2013), extending the learning technique based on linear regression from Baroni and Zamparelli (2010) in which “gold-standard” distributional representations are assumed to be available for some phrases and larger units. cent inroductions to the topic include Turney and Pantel (2010) and Clark (2014). A potentially useful distinction for this paper, and one not commonly made, is between distributional and distributed representations. Distributional representations are inherently contextual, and rely on the frequently quoted dictum from Firth that “you shall know a word from the company it keeps” (Firth, 1957; Pulman, 2013). This leads to the so-called distributional hypothesis that words that"
W14-1406,C04-1180,1,0.619778,"Steedman (2007), which in this case would require an 8th-order tensor. This combination of many word-category pairs and higher-order tensors results in a huge number of parameters. As a solution to this problem, we are investigating ways to reduce the number of parameters, for example using tensor decomposition techniques (Kolda and Bader, 2009). It may also be possible to reduce the size of some of the complex categories in the grammar. Many challenges remain before a type-driven compositional distributional semantics can be realised, similar to the work of Bos for the model-theoretic case (Bos et al., 2004; Bos, 2005), but in this paper we have set out the theoretical framework for such an implementation. Finally, we repeat a comment made earlier that the compositional framework makes no assumptions about the underlying vector spaces, or how they are to be interpreted. On the one hand, this flexibility is welcome, since it means the framework can encompass many techniques for building word vectors (and tensors). On the other hand, it means that a description of the framework is necessarily abstract, and it leaves open the question Acknowledgments Jean Maillard is supported by an EPSRC MPhil stu"
W14-1406,J07-4004,1,0.737835,"ors and then perform the inner product on these vectors. 52 5 Conclusion of what the meaning spaces represent. The latter question is particularly pressing in the case of the sentence space, and providing an interpretation of such spaces remains a challenge for the distributional semantics community, as well as relating distributional semantics to more traditional topics in semantics such as quantification and inference. This paper provides a theoretical framework for the development of a compositional distributional semantics for CCG. Given the existence of robust, wide-coverage CCG parsers (Clark and Curran, 2007; Hockenmaier and Steedman, 2002), together with various techniques for learning the tensors, the opportunity exists for a practical implementation. However, there are significant engineering difficulties which need to be overcome. Consider adapting the neural-network learning techniques of Socher et al. (2012) to this problem.4 In terms of the number of tensors, the lexicon would need to contain a tensor for every wordcategory pair; this is at least an order of magnitude more tensors then the number of matrices learnt in existing work (Socher et al., 2012; Hermann and Blunsom, 2013). Furtherm"
W14-1406,W13-3201,0,0.468385,"Missing"
W14-1406,P08-1028,0,0.730088,"Missing"
W14-1406,J98-1004,0,0.724694,"Missing"
W14-1406,W13-0112,0,0.2619,"tility in NLP applications. There are a number of suggestions for how to add compositionality to a distributional semantics (Clarke, 2012; Pulman, 2013; Erk, 2012). 1.1 Distributional Semantics We assume a basic knowledge of distributional semantics (Grefenstette, 1994; Sch¨utze, 1998). Re46 Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 46–54, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics ing recent techniques from “recursive” neural networks (Socher et al., 2010). Another possibility is suggested by Grefenstette et al. (2013), extending the learning technique based on linear regression from Baroni and Zamparelli (2010) in which “gold-standard” distributional representations are assumed to be available for some phrases and larger units. cent inroductions to the topic include Turney and Pantel (2010) and Clark (2014). A potentially useful distinction for this paper, and one not commonly made, is between distributional and distributed representations. Distributional representations are inherently contextual, and rely on the frequently quoted dictum from Firth that “you shall know a word from the company it keeps” (Fi"
W14-1406,D12-1110,0,0.460936,"ors, and then combine those vectors using some operation, such as vector addition, element-wise multiplication, or tensor product (Clark and Pulman, 2007; Mitchell and Lapata, 2008). A more sophisticated approach, which is the subject of this paper, is to adapt the compositional process from formal semantics (Dowty et al., 1981) and attempt to build a distributional representation in step with the syntactic derivation (Coecke et al., 2010; Baroni et al., 2013). Finally, there is a third approach using neural networks, which perhaps lies in between the two described above (Socher et al., 2010; Socher et al., 2012). Here compositional distributed representations are built using matrices operating on vectors, with all parameters learnt through a supervised learning procedure intended to optimise performance on some NLP task, such as syntactic parsing or sentiment analysis. The approach of Hermann and Blunsom (2013) conditions the vector combination operation on the syntactic type of the combinands, moving it a little closer to the more formal semantics-inspired approaches. The remainder of the Introduction gives a short summary of distributional semantics. The rest of the paper introduces some mathematic"
W14-1406,P13-1088,0,0.179592,"semantics (Dowty et al., 1981) and attempt to build a distributional representation in step with the syntactic derivation (Coecke et al., 2010; Baroni et al., 2013). Finally, there is a third approach using neural networks, which perhaps lies in between the two described above (Socher et al., 2010; Socher et al., 2012). Here compositional distributed representations are built using matrices operating on vectors, with all parameters learnt through a supervised learning procedure intended to optimise performance on some NLP task, such as syntactic parsing or sentiment analysis. The approach of Hermann and Blunsom (2013) conditions the vector combination operation on the syntactic type of the combinands, moving it a little closer to the more formal semantics-inspired approaches. The remainder of the Introduction gives a short summary of distributional semantics. The rest of the paper introduces some mathematical notation from multi-linear algebra, including Einstein notation, and then shows how the combinatory rules of CCG, including type-raising and composition, can be applied directly to tensor-based semantic representations. As well as describing a tensor-based semantics for CCG, a further goal of this pap"
W14-1406,P02-1043,0,0.0139148,"inner product on these vectors. 52 5 Conclusion of what the meaning spaces represent. The latter question is particularly pressing in the case of the sentence space, and providing an interpretation of such spaces remains a challenge for the distributional semantics community, as well as relating distributional semantics to more traditional topics in semantics such as quantification and inference. This paper provides a theoretical framework for the development of a compositional distributional semantics for CCG. Given the existence of robust, wide-coverage CCG parsers (Clark and Curran, 2007; Hockenmaier and Steedman, 2002), together with various techniques for learning the tensors, the opportunity exists for a practical implementation. However, there are significant engineering difficulties which need to be overcome. Consider adapting the neural-network learning techniques of Socher et al. (2012) to this problem.4 In terms of the number of tensors, the lexicon would need to contain a tensor for every wordcategory pair; this is at least an order of magnitude more tensors then the number of matrices learnt in existing work (Socher et al., 2012; Hermann and Blunsom, 2013). Furthermore, the order of the tensors is"
W14-1406,J07-3004,0,0.0667567,"here are significant engineering difficulties which need to be overcome. Consider adapting the neural-network learning techniques of Socher et al. (2012) to this problem.4 In terms of the number of tensors, the lexicon would need to contain a tensor for every wordcategory pair; this is at least an order of magnitude more tensors then the number of matrices learnt in existing work (Socher et al., 2012; Hermann and Blunsom, 2013). Furthermore, the order of the tensors is now higher. Syntactic categories such as ((N /N )/(N /N ))/((N /N )/(N /N )) are not uncommon in the wide-coverage grammar of Hockenmaier and Steedman (2007), which in this case would require an 8th-order tensor. This combination of many word-category pairs and higher-order tensors results in a huge number of parameters. As a solution to this problem, we are investigating ways to reduce the number of parameters, for example using tensor decomposition techniques (Kolda and Bader, 2009). It may also be possible to reduce the size of some of the complex categories in the grammar. Many challenges remain before a type-driven compositional distributional semantics can be realised, similar to the work of Bos for the model-theoretic case (Bos et al., 2004"
W14-1406,J12-1002,0,\N,Missing
W14-1406,2014.lilt-9.5,0,\N,Missing
W14-1503,S13-1035,0,0.0271096,"e also consider three different corpora from which to build the vectors, varying in size and domain. These include the BNC (Burnard, 2007) (106 word types, 108 tokens) and the larger ukWaC (Baroni et al., 2009) (107 types, 109 tokens). We also include a sub-spaced Wikipedia corpus (Stone et al., 2008): for all words in the evaluation datasets, we build a subcorpus by querying the top 10-ranked Wikipedia documents using the words as search terms, resulting in a corpus with 106 word types and 107 tokens. For examining the dependency-based contexts, we include the Google Syntactic N-gram corpus (Goldberg and Orwant, 2013), with 107 types and 1011 tokens. 2.1 Context There are two main approaches to modelling context: window-based and dependencybased. For window-based methods, contexts are determined by word co-occurrences within a window of a given size, where the window simply spans a number of words occurring around instances of a target word. For dependency-based methods, the contexts are determined by word co-occurrences in a particular syntactic relation with a target word (e.g. target word dog is the subject of run, where run subj is the context). We consider different window sizes and compare window-bas"
W14-1503,D12-1050,0,0.0142917,"f parameters, and using a limited set of source corpora and tasks for evaluation (Curran and Moens, 2002a; Curran and Moens, 2002b; Curran, 2004; Grefenstette, 1994; Pado and Lapata, 2007; Sahlgren, 2006; Turney and Pantel, 2010; Schulte im Walde et al., 2013). Rohde et al. (2006) considered several weighting schemes for a large variety of tasks, while Weeds et al. (2004) did the same for similarity metrics. Stone et al. (2008) investigated the effectiveness of sub-spacing corpora, where a larger corpus is queried in order to construct a smaller sub-spaced corpus (Zelikovitz and Kogan, 2006). Blacoe and Lapata (2012) compare several types of vector representations for semantic composition tasks. The most comprehensive existing studies of VSM parameters — encompassing window sizes, feature granularity, stopwords and dimensionality reduction — are by Bullinaria and Levy (2007; 2012) and Lapesa and Evert (2013). Section 2 introduces the various parameters of vector space model construction. We then attempt, in Section 3, to answer some of the fundamental questions for building VSMs through a number of experiments that consider each of the selected parameters. In Section 4 we examine how these findings relate"
W14-1503,P12-1015,0,0.025116,"n the literature, but a typical value is in the low thousands. Here we consider vector sizes ranging from 50,000 to 500,000, to see whether larger vectors lead to better performance. Table 1: Datasets for evaluation (Landauer and Dumais, 1997). There is a risk that semantic similarity studies have been overfitting to their idiosyncracies, so in this study we evaluate on a variety of datasets: in addition to WordSim353 (W353) and TOEFL, we also use the Rubenstein & Goodenough (RG) (1965) and Miller & Charles (MC) (1991) data, as well as a much larger set of similarity ratings: the MEN dataset (Bruni et al., 2012). All these datasets consist of human similarity ratings for word pairings, except TOEFL, which consists of multiple choice questions where the task is to select the correct synonym for a target word. In Section 4 we examine our parameters in the context of distributional compositional semantics, using the evaluation dataset from Mitchell and Lapata (2010). Table 1 gives statistics for the number of words and word pairings in each of the datasets. As well as using a variety of datasets, we also consider three different corpora from which to build the vectors, varying in size and domain. These"
W14-1503,W13-2608,0,0.146911,"al weighting schemes for a large variety of tasks, while Weeds et al. (2004) did the same for similarity metrics. Stone et al. (2008) investigated the effectiveness of sub-spacing corpora, where a larger corpus is queried in order to construct a smaller sub-spaced corpus (Zelikovitz and Kogan, 2006). Blacoe and Lapata (2012) compare several types of vector representations for semantic composition tasks. The most comprehensive existing studies of VSM parameters — encompassing window sizes, feature granularity, stopwords and dimensionality reduction — are by Bullinaria and Levy (2007; 2012) and Lapesa and Evert (2013). Section 2 introduces the various parameters of vector space model construction. We then attempt, in Section 3, to answer some of the fundamental questions for building VSMs through a number of experiments that consider each of the selected parameters. In Section 4 we examine how these findings relate to the recent development of distributional compositional semantics (Baroni et al., 2013; Clark, 2014), where vectors for words are combined into vectors for phrases. Introduction Vector space models (VSMs) represent the meanings of lexical items as vectors in a “semantic space”. The benefit of"
W14-1503,J07-4004,1,0.5597,"pears to work best (given the small window size). 3.4 Feature granularity Stemming and lemmatisation are standard techniques in NLP and IR to reduce data sparsity. However, with large enough corpora it may be that the loss of information through generalisation hurts performance. In fact, it may be that increased granularity – through the use of grammatical tags – can lead to improved performance. We test these hypotheses by comparing four types of processed context words: lemmatised, stemmed, POS-tagged, and tagged with CCG lexical categories (which can be thought of as fine-grained POS tags (Clark and Curran, 2007)).4 The source corpora are BNC and ukWaC, using a windowbased method with windows of size 5, Positive Mutual Information weighting, vectors of size 50,000 and Cosine similarity. The results are reported in Figure 4. The ukWaC-generated vectors outperform the BNC-generated ones on all but a single instance for each of the granularities. Stemming yields the best overall performance, and increasing the granularity does not lead to better results. Even with a very large corpus like ukWaC, stemming yields signficantly better results than not reducing the feature granularity at all. Conversely, apar"
W14-1503,W02-0908,0,0.307509,"sent a systematic study of parameters used in the construction of semantic vector space models. Evaluation is carried out on a variety of similarity tasks, including a compositionality dataset, using several source corpora. In addition to recommendations for optimal parameters, we present some novel findings, including a similarity metric that outperforms the alternatives on all tasks considered. 1 • dataset for evaluation; • source corpus. Previous studies have been limited to investigating only a small number of parameters, and using a limited set of source corpora and tasks for evaluation (Curran and Moens, 2002a; Curran and Moens, 2002b; Curran, 2004; Grefenstette, 1994; Pado and Lapata, 2007; Sahlgren, 2006; Turney and Pantel, 2010; Schulte im Walde et al., 2013). Rohde et al. (2006) considered several weighting schemes for a large variety of tasks, while Weeds et al. (2004) did the same for similarity metrics. Stone et al. (2008) investigated the effectiveness of sub-spacing corpora, where a larger corpus is queried in order to construct a smaller sub-spaced corpus (Zelikovitz and Kogan, 2006). Blacoe and Lapata (2012) compare several types of vector representations for semantic composition tasks."
W14-1503,J07-2002,0,0.919036,"ace models. Evaluation is carried out on a variety of similarity tasks, including a compositionality dataset, using several source corpora. In addition to recommendations for optimal parameters, we present some novel findings, including a similarity metric that outperforms the alternatives on all tasks considered. 1 • dataset for evaluation; • source corpus. Previous studies have been limited to investigating only a small number of parameters, and using a limited set of source corpora and tasks for evaluation (Curran and Moens, 2002a; Curran and Moens, 2002b; Curran, 2004; Grefenstette, 1994; Pado and Lapata, 2007; Sahlgren, 2006; Turney and Pantel, 2010; Schulte im Walde et al., 2013). Rohde et al. (2006) considered several weighting schemes for a large variety of tasks, while Weeds et al. (2004) did the same for similarity metrics. Stone et al. (2008) investigated the effectiveness of sub-spacing corpora, where a larger corpus is queried in order to construct a smaller sub-spaced corpus (Zelikovitz and Kogan, 2006). Blacoe and Lapata (2012) compare several types of vector representations for semantic composition tasks. The most comprehensive existing studies of VSM parameters — encompassing window si"
W14-1503,P02-1030,0,0.0373706,"sent a systematic study of parameters used in the construction of semantic vector space models. Evaluation is carried out on a variety of similarity tasks, including a compositionality dataset, using several source corpora. In addition to recommendations for optimal parameters, we present some novel findings, including a similarity metric that outperforms the alternatives on all tasks considered. 1 • dataset for evaluation; • source corpus. Previous studies have been limited to investigating only a small number of parameters, and using a limited set of source corpora and tasks for evaluation (Curran and Moens, 2002a; Curran and Moens, 2002b; Curran, 2004; Grefenstette, 1994; Pado and Lapata, 2007; Sahlgren, 2006; Turney and Pantel, 2010; Schulte im Walde et al., 2013). Rohde et al. (2006) considered several weighting schemes for a large variety of tasks, while Weeds et al. (2004) did the same for similarity metrics. Stone et al. (2008) investigated the effectiveness of sub-spacing corpora, where a larger corpus is queried in order to construct a smaller sub-spaced corpus (Zelikovitz and Kogan, 2006). Blacoe and Lapata (2012) compare several types of vector representations for semantic composition tasks."
W14-1503,S13-1038,0,0.0100052,"Missing"
W14-1503,C04-1146,0,0.386865,"ent some novel findings, including a similarity metric that outperforms the alternatives on all tasks considered. 1 • dataset for evaluation; • source corpus. Previous studies have been limited to investigating only a small number of parameters, and using a limited set of source corpora and tasks for evaluation (Curran and Moens, 2002a; Curran and Moens, 2002b; Curran, 2004; Grefenstette, 1994; Pado and Lapata, 2007; Sahlgren, 2006; Turney and Pantel, 2010; Schulte im Walde et al., 2013). Rohde et al. (2006) considered several weighting schemes for a large variety of tasks, while Weeds et al. (2004) did the same for similarity metrics. Stone et al. (2008) investigated the effectiveness of sub-spacing corpora, where a larger corpus is queried in order to construct a smaller sub-spaced corpus (Zelikovitz and Kogan, 2006). Blacoe and Lapata (2012) compare several types of vector representations for semantic composition tasks. The most comprehensive existing studies of VSM parameters — encompassing window sizes, feature granularity, stopwords and dimensionality reduction — are by Bullinaria and Levy (2007; 2012) and Lapesa and Evert (2013). Section 2 introduces the various parameters of vect"
W14-1503,2014.lilt-9.5,0,\N,Missing
W14-1503,W13-2609,1,\N,Missing
W14-4501,P07-1073,0,0.0340759,"s who were the correct answers did not always have a dedicated Wikipedia page. Even though combining a search engine with distant supervision results in a highly imbalanced learning task, it increases the potential coverage of our system. In this process we rely on the keywords used in the queries in order to find pages containing the entities intended rather than synonymous ones, e.g. the keyword “building” helps avoid extracting sentences mentioning saints instead of churches. Nevertheless, building names such as churches named after saints were often ambiguous resulting in false positives. Bunescu and Mooney (2007) also used a small seed set and a search engine, but they collected sentences via queries containing both the question and the answer entities, thus (unreallistically) assuming knowledge of all the correct answers. Instead we rely on simple heuristics to identify candidate answers. These heuristics are relation-dependent and different types of answers can be easily accommodated, e.g. in completed year relation they are single-token numbers. Finally, the entity filters learned jointly with relation extraction in our approach, while they perform a role similar to NER, they are learned so that th"
W14-4501,D13-1152,0,0.128656,"Missing"
W14-4501,P11-1055,0,0.103475,"tailored NER system. This is reasonable for the example, since such a list is relatively easy to acquire. In order to create training data, queries containing words from the seeds are sent to a search engine. Sentences from the returned pages are then processed to find examples which contain mentions of both a building and the corresponding architect. Applying the distant supervision hypothesis, we assume that such sentences are indeed expressing the desired relation, and these are positive examples. While such data contains noise, it has been shown to be useful in practice (Yao et al., 2010; Hoffmann et al., 2011). At test time the input is the name of a historical building. Now the web is searched to find example sentences containing this name, and the classifier is applied to each sentence, returning either the name of the architect, or none. Note that different sentences could provide evidence for different architects; hence assuming only one architect for each building, a procedure is required to decide between the possible answers (see Sec. 5). 3 Entity Filtering for Relation Extraction Each relation extraction instance consists of a sentence containing a question entity (e.g. Bute House) and a ca"
W14-4501,P09-1113,0,0.269373,"eal-world application. The application is a dialog-based city tour guide, based in Edinburgh. One of the features of the system is its pro-active nature, offering information which may be of interest to the user. In order to be pro-active in this way, as well as answer users’ questions, the system requires a large amount of knowledge about the city. Part of that knowledge is stored in a database, which is time-consuming and difficult to populate manually. Hence, we have explored the use of an automatic knowledge base population technique based on distant supervision (Craven and Kumlien, 1999; Mintz et al., 2009). The attraction of this approach is that the only input required is a list of seed instances of the relation in question and a corpus of sentences expressing new instances of that relation. However, existing studies typically assume a large seed set, whereas in our application such sets are often not readily available, e.g. Mintz et al. (2009) reported using 7K-140K seed instances per relation as input. In this paper, the two relations that we evaluate on are architect name and completion year of buildings. These were chosen because they are highly relevant to our application, but also somewh"
W14-4501,D10-1099,0,0.335865,"ings instead of a tailored NER system. This is reasonable for the example, since such a list is relatively easy to acquire. In order to create training data, queries containing words from the seeds are sent to a search engine. Sentences from the returned pages are then processed to find examples which contain mentions of both a building and the corresponding architect. Applying the distant supervision hypothesis, we assume that such sentences are indeed expressing the desired relation, and these are positive examples. While such data contains noise, it has been shown to be useful in practice (Yao et al., 2010; Hoffmann et al., 2011). At test time the input is the name of a historical building. Now the web is searched to find example sentences containing this name, and the classifier is applied to each sentence, returning either the name of the architect, or none. Note that different sentences could provide evidence for different architects; hence assuming only one architect for each building, a procedure is required to decide between the possible answers (see Sec. 5). 3 Entity Filtering for Relation Extraction Each relation extraction instance consists of a sentence containing a question entity (e"
W14-4501,P13-2141,0,0.101059,"me and completion year of buildings. These were chosen because they are highly relevant to our application, but also somewhat non-standard compared to the existing literature; and crucially they do not come with a readily-available set of seed instances. Furthermore, previous approaches typically assume named entity recognition (NER) as a preprocessing step in order to construct the training and testing instances. However, since these tools are not tailored to the relations of interest, they introduce spurious entity matches that are harmful to performance as shown by Ling and Weld (2012) and Zhang et al. (2013). These authors ameliorated this issue by learning fine-grained entity recognizers and filters using supervised learning. The labeled data used was extracted from the anchor text of entity mentions annotated in Wikipedia, however this is not possible for entities not annotated in this resource. In this work, instead of relying on labeled data to construct entity filters, we learn them jointly with the relation extraction component. For this purpose we use the imitation learning algorithm DAGGER (Ross et al., 2011), which can handle the dependencies between actions taken in a sequence, and use"
W15-0107,Q14-1023,0,0.0277825,"ts in order to allow for empirical testing of psychological semantic theories. In these studies humans are asked to identify the most important attributes of a concept; e.g. given AIRPLANE, its most important features could be to_fly, has_wings and is_used_for_transport. These datasets provide a valuable insight into human concept representation and have been successfully used for tasks such as text simplification for limited vocabulary groups, personality modelling and metaphor processing, as well as a proxy for modelling perceptual information (Riordan and Jones, 2011; Andrews et al., 2009; Hill et al., 2014). Feature norms provide an interesting source of semantic information because they capture higher level conceptual knowledge in comparison to the low level perceptual information represented in images, for example. Despite their advantages, semantic feature norms are not widely used in computational linguistics, mainly because they are expensive to produce and have only been collected for small sets of words; moreover there is no finite list of features that can be produced for a given concept. In Roller and Schulte im 52 Proceedings of the 11th International Conference on Computational Semant"
W15-0107,P14-1132,0,0.172379,"his work has been set in Johns and Jones (2012), but whilst they predict feature representations through global lexical similarity, we infer them through learning a cross-modal mapping. 2 Mapping between semantic spaces The integration of perceptual and linguistic information is supported by a large body of work in the cognitive science literature (Riordan and Jones, 2011; Andrews et al., 2009) that shows that models that include both types of information perform better at fitting human semantic data. The idea of learning a mapping between semantic spaces appears in previous work; for example Lazaridou et al. (2014) learn a cross-modal mapping between text and images and Mikolov et al. (2013) show that a linear mapping between vector spaces of different languages can be learned by only relying on a small amount of bilingual information from which missing dictionary entries can be inferred. Following the approach in Mikolov et al. (2013), we learn a linear mapping between the distributional space and the feature-based space. 2.1 Feature norm datasets One of the largest and most widely used feature-norm datasets is from McRae et al. (2005). Participants were asked to produce a list of features for a given"
W15-0107,N13-1090,0,0.251606,"an automatic method for predicting feature norms for new concepts by learning a mapping from a text-based distributional semantic space to a space built using feature norms. Our experimental results show that we are able to generalise feature-based concept representations, which opens up the possibility of developing large-scale semantic models grounded in a proxy for human perceptual data. 1 Introduction Distributional semantic models (Turney and Pantel, 2010; Sahlgren, 2006) represent the meanings of words by relying on their statistical distribution in text (Erk, 2012; Bengio et al., 2006; Mikolov et al., 2013; Clark, 2015). Despite performing well in a wide range of semantic tasks, a common criticism is that by only representing meaning through linguistic input these models are not grounded in perception, since the words only exist in relation to each other and are not in relation to the physical world. This concern is motivated by the increasing evidence in the cognitive science literature that the semantics of words is derived not only from our exposure to the language, but also through our interactions with the world. One way to overcome this issue would be to include perceptual information in"
W15-0107,E14-1025,1,0.482504,"presentation of CAT in the feature-based and distributional spaces features in the McRae dataset as contexts (FS 1) and one obtained by reducing the dimensions of FS 1 to 300 using SVD (FS 2). For the distributional spaces (DS), we experimented with various parameter settings, and built four spaces using Wikipedia as a corpus and sentence-like windows together with the following parameters: • DS 1: contexts are the top 10K most frequent content words in Wikipedia, values are raw cooccurrence counts. • DS 2: same contexts as DS 1, counts are re-weighted using PPMI and normalised as detailed in Polajnar and Clark (2014). • DS 3: perform SVD to 300 dimensions on DS 2. • DS 4: same as DS 3 but with row normalisation performed after dimensionality reduction. We also use the context-predicting vectors available as part of the word2vec1 project (Mikolov et al., 2013) (DS 5). These vectors are 300 dimensional and are trained on a Google News dataset (100 billion words). 2.3 The mapping function Our goal is to learn a function f : DS → FS that maps a distributional vector for a concept to its featurebased vector. Following Mikolov et al. (2013), we learn the mapping as a linear relationship between the distribution"
W15-0107,D13-1115,0,0.0662633,"Missing"
W15-0107,P13-4006,0,\N,Missing
W15-2701,P08-1090,0,0.00536391,"ets. St : He photographed stairwells and architectural details. St+1 : His interests also extended to the environs of Paris. St+2 : He also photographed street-hawkers and small tradesmen, as well as popular amusements. IDist: stairwell, architectural, detail DDist: capture, old, paris, picture, photograph, show, city, various, interest, extend, popular, amusement DVerb: capture, show, extend, photograph space, which takes as context features any verbs occurring in the two sentences either side of St . This space was loosely inspired by work on unsupervised learning of narrative event chains (Chambers and Jurafsky, 2008, 2009), in which sequences of events such as accuse – claim – argue – dismiss or appoint – work – oversee – retire are extracted from text. That work links event types which share a protagonist in a connected discourse; in contrast, we do not check whether neighbouring verbs share arguments, but simply hypothesise that verbs near the target verb represent related events and are therefore particularly suited to be context features. Figure 2 shows examples of DVerb and DDist. We expect DVerb to suffer from a certain amount of data sparsity since the number of verbs in a window of two sentences"
W15-2701,P09-1068,0,0.0157049,"Missing"
W15-2701,J07-4004,1,0.502847,"P , which defines it as a function that takes a noun phrase as an input from the right, and then another noun phrase from the left, to produce a sentence. Interpreting such categories under the Categorial framework is straightforward. First, for each atomic category there is a corresponding vector space; in this case the sentence space S and the noun space N.1 Hence the meaning of a noun or noun phrase, for example people, will be a vector −−−→ in the noun space: people ∈ N. In order to obtain the meaning of a transitive verb, each slash is re1 In practice, for example using the CCG parser of Clark and Curran (2007), there will be additional atomic categories, such as PP , but not many more. 2 people NP N eat fish result, the sentence representation is a purely compositional function of the context vectors of its component words; the observed contexts of SVO triples are not part of the representation. One effect of reducing a verb tensor to a matrix is to reduce the number of parameters required to learn the verb. However, recent work in the Categorial framework offers other ways to reduce the number of parameters while retaining the higher type of the tensor. Fried et al. (2015) introduce low-rank appro"
W15-2701,C12-2054,0,0.561492,"tial contextual sentence space similar to that proposed in Grefenstette et al. (2013). We situate our work within the Categorial framework (Coecke et al., 2010; Baroni et al., 2014; Clark, 2013, 2015) where nouns and sentences are considered atomic types, represented as vectors, and other words as functions, represented as tensors. This framework provides a natural setting in which the sentence space can differ from the spaces of sentence constituents, since argument-taking words such as verbs are maps from argument space into sentence space. Following Grefenstette and Sadrzadeh (2011a,b) and Kartsaklis et al. (2012) we focus on simplified sentences consisting of a subject, transitive verb, and object (SVO). We train transitive verb tensors using a single-step multilinear regression algorithm. We evaluate our composed representations on two standard SVO sentence similarity tasks. The results show that the discourse-based sentence spaces perform competitively, both with the intraThis paper investigates whether the wider context in which a sentence is located can contribute to a distributional representation of sentence meaning. We compare a vector space for sentences in which the features are words occurri"
W15-2701,P15-2120,1,0.802951,"ious work in compositional distributional semantics largely defines the sentence vector space to be the same as the noun space (Kartsaklis et al., 2012; Socher et al., 2011b, 2012), and produces sentence vectors in that space by a sequence 1 Proceedings of the EMNLP 2015 Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 1–11, c Lisboa, Portugal, 18 September 2015. 2015 Association for Computational Linguistics. dings for larger text segments, including sentences, based on n-grams internal to the text segments. A variant of this approach was also adopted by Fried et al. (2015) to learn verb tensors mapping to an intra-sentential sentence space for the Categorial framework using single-step linear regression. Sentence spaces need not be contextual, but may also represent other aspects of meaning relevant to propositions, such as plausibility or feature norms (McRae et al., 1997). A non-contextual option that has been previously implemented is a two-dimensional “plausibility space”, in which the sentence vector represents a plausibility judgement. This type of space was explored in theory in Clark (2013, 2015) and implemented with multilinear regression training for"
W15-2701,D11-1129,0,0.031853,"Missing"
W15-2701,W11-2507,0,0.517185,"bs, and compare them with an intra-sentential contextual sentence space similar to that proposed in Grefenstette et al. (2013). We situate our work within the Categorial framework (Coecke et al., 2010; Baroni et al., 2014; Clark, 2013, 2015) where nouns and sentences are considered atomic types, represented as vectors, and other words as functions, represented as tensors. This framework provides a natural setting in which the sentence space can differ from the spaces of sentence constituents, since argument-taking words such as verbs are maps from argument space into sentence space. Following Grefenstette and Sadrzadeh (2011a,b) and Kartsaklis et al. (2012) we focus on simplified sentences consisting of a subject, transitive verb, and object (SVO). We train transitive verb tensors using a single-step multilinear regression algorithm. We evaluate our composed representations on two standard SVO sentence similarity tasks. The results show that the discourse-based sentence spaces perform competitively, both with the intraThis paper investigates whether the wider context in which a sentence is located can contribute to a distributional representation of sentence meaning. We compare a vector space for sentences in whi"
W15-2701,E14-1025,1,0.775256,"entations of their contexts. For each verb we therefore selected up to 2 4 http://nlp.stanford.edu/software/index.shtml 600 triples which occurred more than once and contained subject and object nouns that occurred at least 100 times. This resulted in M ≈ 150, 000 triples overall. We first generated distributional vectors for all the nouns contained in the training triples and the test datasets. We used Wikipedia as the source corpus, with sentences as the context window and the top N = 10, 000 most frequent words (excluding stopwords) as the context words. Following the procedure outlined in Polajnar and Clark (2014), we employed t-test weighting (Curran, 2004) and context selection, and reduced our noun vectors (n) to K = 100 dimensions using Singular Value Decomposition (SVD). For each verb V we have a set of MV training instances, where each instance i ∈ MV consists of subject and object noun vectors n(s) i , n(o) i and a true sentence space representation vector ti . The vector ti is the SVD-reduced version of the Wikipedia context vector for the triple n(s) i V n(o) i . The true IDist and DDist vectors were generated using the same N = 10, 000 context words as for the nouns, weighted by t-test. The e"
W15-2701,D14-1111,1,0.940405,"ensors mapping to an intra-sentential sentence space for the Categorial framework using single-step linear regression. Sentence spaces need not be contextual, but may also represent other aspects of meaning relevant to propositions, such as plausibility or feature norms (McRae et al., 1997). A non-contextual option that has been previously implemented is a two-dimensional “plausibility space”, in which the sentence vector represents a plausibility judgement. This type of space was explored in theory in Clark (2013, 2015) and implemented with multilinear regression training for verb tensors by Polajnar et al. (2014a). Contemporaneously with our work, Kiros et al. (2015) have used an encoder-decoder recurrent neural network architecture to encode a sentence vector conditioned on the previous and following sentences, providing further support for the utility of extra-sentential context for sentence meaning. sentential contextual space and with previous work on SVO composition, although not beating the state of the art on these tasks. We then provide a qualitative analysis of the topics resulting from Singular Value Decomposition in each sentence space, showing that both intra- and extrasentential spaces c"
W15-2701,W15-4001,0,0.314644,"e Analysis In this section we provide a qualitative analysis of how the sentence spaces represent meaning. We contrast the space that has been used in previous literature (IDist) with the extra-sentential spaces (DDist, DVerb) to highlight the differences encoded by different contextual information. 6.1 Topic Comparison In a word-context matrix, it is common to perform qualitative analyses of dimensionally reduced spaces by looking at the top-weighted words per topic, where the topics are induced by a dimensionality reduction technique. In our case, 4 The results of Milajevs et al. (2014) and Hashimoto and Tsuruoka (2015) are not comparable, as they average across annotators for each SVO pair. The standard treatment of these datasets considers each annotator judgement as a separate test point, which leads to lower results overall. 6 1 2 3 4 5 20 21 22 23 24 IDist Topic 5 fire destroy building - fire building downtown rebuild disastrous main fire damage building - building fire severely rebuild badly disastrous building suffer fire - fire building rebuild restore severe porch remodel Fire destroy building - fire building great salem rebuilt displacement building replace building - building consulate construct c"
W15-2701,prasad-etal-2008-penn,0,0.0764418,"Missing"
W15-2701,D12-1110,0,0.298268,"Missing"
W15-2701,P13-1045,0,0.0305373,"Missing"
W15-2701,J14-4007,0,\N,Missing
W15-2701,D14-1079,0,\N,Missing
W15-2701,2014.lilt-9.5,0,\N,Missing
W18-2903,N16-1024,0,0.0420896,"(2016) and Choi et al. (2017) when trained on two natural language inference corpora, and analyse the results. They find that the former model induces almost entirely leftbranching trees, while the latter performs well but has inconsistent trees across re-runs with different parameter initializations. A number of other neural models have also been proposed which create a tree encoding during parsing, but unlike the above architectures rely on traditional parse trees. Le and Zuidema (2015) propose a sentence embedding model based on CKY, taking as input a parse forest from an automatic parser. Dyer et al. (2016) propose RNNG, a probabilistic model of phrase-structure trees and sentences, with an integrated parser that is trained on gold standard trees. 3 of all possible options, using the normalised scores as weights. In order for this weighted sum to approximate a discrete selection, a temperature hyperparameter is used in the softmax. This process is repeated for the whole chart, and the sentence representation is given by the topmost cell. We noticed in our experiments that the weighted sum still occasionally assigned non-trivial weight to more than one option. The model was thus able to utilize m"
W18-2903,D15-1137,0,0.118696,"d are thus able to train with backpropagation. Williams et al. (2017a) investigate the trees produced by Yogatama et al. (2016) and Choi et al. (2017) when trained on two natural language inference corpora, and analyse the results. They find that the former model induces almost entirely leftbranching trees, while the latter performs well but has inconsistent trees across re-runs with different parameter initializations. A number of other neural models have also been proposed which create a tree encoding during parsing, but unlike the above architectures rely on traditional parse trees. Le and Zuidema (2015) propose a sentence embedding model based on CKY, taking as input a parse forest from an automatic parser. Dyer et al. (2016) propose RNNG, a probabilistic model of phrase-structure trees and sentences, with an integrated parser that is trained on gold standard trees. 3 of all possible options, using the normalised scores as weights. In order for this weighted sum to approximate a discrete selection, a temperature hyperparameter is used in the softmax. This process is repeated for the whole chart, and the sentence representation is given by the topmost cell. We noticed in our experiments that"
W18-2903,D15-1075,0,0.188585,"k, our model is trained via standard backpropagation, which is made possible by exploiting beam search to obtain an approximate gradient. We show that this model performs well compared to baselines, and induces trees that are not as trivial as those learned by the Yogatama et al. model in the experiments of Williams et al. (2017a). This paper also presents an analysis of the trees learned by our model, in the style of Williams et al. (2017a). We further analyse the trees learned by the model of Maillard et al. (2017), which had not yet been done, and perform evaluations on both the SNLI data (Bowman et al., 2015) and the MultiNLI data (Williams et al., 2017b). The former corpus had not been used for the evaluation of trees of Williams et al. (2017a), and we find that it leads to more consistent induced trees. Latent tree learning models represent sentences by composing their words according to an induced parse tree, all based on a downstream task. These models often outperform baselines which use (externally provided) syntax trees to drive the composition order. This work contributes (a) a new latent tree learning model based on shift-reduce parsing, with competitive downstream performance and non-tri"
W18-2903,P16-1139,0,0.0293177,"raining on a downstream task such as natural language inference. At the heart of these models is a mechanism to assign trees to sentences – effectively, a natural language parser. Williams et al. (2017a) have recently investigated the tree structures induced by two of these models, trained for 2 Related work The first neural model which learns to both parse a sentence and embed it for a downstream task is by Socher et al. (2011). The authors train the model’s parsing component on an auxiliary task, based on recursive autoencoders, while the rest of the model is trained for sentiment analysis. Bowman et al. (2016) propose the “Shiftreduce Parser-Interpreter Neural Network”, a model which obtains syntax trees using an integrated shift-reduce parser (trained on gold13 Proceedings of the Workshop on the Relevance of Linguistic Structure in Neural Architectures for NLP, pages 13–18 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics standard trees), and uses the resulting structure to drive composition with Tree-LSTMs. Yogatama et al. (2016) is the first model to jointly train its parsing and sentence embedding components. They base their model on shiftreduce parsing. Thei"
W18-2903,D14-1162,0,0.0875731,"Missing"
W18-2903,D11-1014,0,0.140456,"Missing"
W18-2903,P15-1150,0,0.304396,"Missing"
W99-0631,E95-1016,0,0.188611,"oose a h y p e r n y m for each alternative sense, we employ a novel technique which uses a X2 test to measure the homogeneity of sets of concepts in the hierarchy. 1 Introduction Knowledge of the constraints a verb places on the semantic types of its a r g u m e n t s (variously called selectional restrictions, selectional preferences, selectional constraints) is of use in m a n y areas of n a t u r a l language processing, particularly structural disambiguation. Recent t r e a t m e n t s of selectional restrictions have been probabilistic in n a t u r e (Resnik, 1993), (Li and Abe, 1998), (Ribas, 1995), (McCarthy, 1997), and estimation of the relevant probabilities has required corpus-based counts of the n u m b e r of times word senses, or concepts, appear in the different a r g u m e n t positions of verbs. A difficulty arises due to the absence of a large volume of sense d i s a m b i g u a t e d data, as the counts have to be estimated from the nouns which appear in the corpus, most of which will have more t h a n one sense. The techniques in Resnik (1993), Li and Abe (1998) and Ribas (1995) simply distribute the count equally a m o n g the alternative senses of a noun. A b n e y and Li"
W99-0631,A97-1052,0,0.135345,". Using the previous example, < e n t i t y &gt; is too high to represent either sense of chicken because the children of < e n t i t y &gt; are not all associated in the same way with eat. The set consisting of the children of <meat&gt;, however, is homogeneous with respect to the object position of eat, and so <meat&gt; is not too high a level of representation. The measure of homogeneity we use is detailed in Section 5. 2 The Input Data and Semantic Hierarchy The input data used to estimate frequencies and probabilities over the semantic hierarchy has been obtained from the shallow parser described in Briscoe and Carroll (1997). The data consists of a multiset of 'co-occurrence triples', each triple consisting of a noun lemma, verb lemma, and argument position. We refer to the data as follows: let the universe of verbs, argument positions and nouns that can appear in the input data be denoted = { V l , . . . ,Vkv }, 1Z---- { r l , . . . , r k n } and Af = { n l , . . . , nk~¢ }, respectively. Note that in our treatment of selectional restrictions, we do not attempt to distinguish between alternative senses of verbs. We also assume that each instance of a noun in the data refers to one, and only one, concept. We use"
W99-0631,C92-2070,0,0.0774675,"v and position r, in the following formula we use [c, v, r] to denote the set of concepts top(c, v, r). The re_ ^ rn+l. estimated frequency treq (c, v, r) is given as follows. freq(n, v, r) I cn(n)l nEsyn(c) where freq(n, v, r) is the number times the triple (n,v,r) appears in the data, and [ cn(n)] is the cardinality of an(n). Although this approach can give inaccurate estimates, the counts given to the incorrect senses will disperse randomly throughout the hierarchy as noise, and by accumulating counts up the hierarchy we will tend to gather counts from the correct senses of related words (Yarowsky, 1992; Resnik, 1993). To see why, consider two instances of possible triples in the data, drink wine and drink water. (This example is adapted from Resnik (1993).) The word water is a member of seven synsets in WordNet 1.6, and wine is a member of two synsets. Thus each sense of water will be incremented by 0.14 counts, and each sense of wine will be incremented by 0.5 counts. Now although the incorrect senses of these words will receive counts, those concepts in the hierarchy which dominate more than one of the senses, such as <beverage&gt;, will accumulate more substantial counts. However, although"
W99-0631,J93-1003,0,0.085173,"< e n t i t y &gt; is {entity, something}, and the words entity and something may well appear in the argument positions of verbs in the corpus. Furthermore, for a concept c, we distinguish between the set of words t h a t can be used to denote c (the synset of c), and the set of words t h a t can be used to denote concepts in L 6 3 p(nlc, v, r) Now we have a model for the input data: p(n, v, r) = p(v,r)p(niv ,r) = p(v,r) p(clv, rlp(ntc, v,r) cecn(n) Note t h a t for c ¢ cn(n), p(nlc, v, r) = O. The association norm (and similar measures such as the mutual information score) have been criticised (Dunning, 1993) because these scores can be greatly over-estimated when frequency counts are low. This problem is overcome to some extent in the scheme presented below since, generally speaking, we only calculate the association norms for concepts that have accumulated a significant count. The association norm can be estimated using m a x i m u m likelihood estimates of the probabilities as follows. The Measure of A s s o c i a t i o n We measure the association between argum e n t positions of verbs and sets of concepts using the a s s o c i a t i o n n o r m (Abe and Li, 1996). 7 For C C C, v E V a n d r E"
W99-0631,J98-2002,0,0.160036,"ount. In order to choose a h y p e r n y m for each alternative sense, we employ a novel technique which uses a X2 test to measure the homogeneity of sets of concepts in the hierarchy. 1 Introduction Knowledge of the constraints a verb places on the semantic types of its a r g u m e n t s (variously called selectional restrictions, selectional preferences, selectional constraints) is of use in m a n y areas of n a t u r a l language processing, particularly structural disambiguation. Recent t r e a t m e n t s of selectional restrictions have been probabilistic in n a t u r e (Resnik, 1993), (Li and Abe, 1998), (Ribas, 1995), (McCarthy, 1997), and estimation of the relevant probabilities has required corpus-based counts of the n u m b e r of times word senses, or concepts, appear in the different a r g u m e n t positions of verbs. A difficulty arises due to the absence of a large volume of sense d i s a m b i g u a t e d data, as the counts have to be estimated from the nouns which appear in the corpus, most of which will have more t h a n one sense. The techniques in Resnik (1993), Li and Abe (1998) and Ribas (1995) simply distribute the count equally a m o n g the alternative senses of a noun. A"
W99-0631,W97-0808,0,0.724897,"r n y m for each alternative sense, we employ a novel technique which uses a X2 test to measure the homogeneity of sets of concepts in the hierarchy. 1 Introduction Knowledge of the constraints a verb places on the semantic types of its a r g u m e n t s (variously called selectional restrictions, selectional preferences, selectional constraints) is of use in m a n y areas of n a t u r a l language processing, particularly structural disambiguation. Recent t r e a t m e n t s of selectional restrictions have been probabilistic in n a t u r e (Resnik, 1993), (Li and Abe, 1998), (Ribas, 1995), (McCarthy, 1997), and estimation of the relevant probabilities has required corpus-based counts of the n u m b e r of times word senses, or concepts, appear in the different a r g u m e n t positions of verbs. A difficulty arises due to the absence of a large volume of sense d i s a m b i g u a t e d data, as the counts have to be estimated from the nouns which appear in the corpus, most of which will have more t h a n one sense. The techniques in Resnik (1993), Li and Abe (1998) and Ribas (1995) simply distribute the count equally a m o n g the alternative senses of a noun. A b n e y and Light (1998) have at"
