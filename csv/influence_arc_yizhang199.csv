2020.acl-main.294,D18-1045,0,0.166186,"aging the GEC training data as the augmented parallel data to help improve formality. An example is illustrated in Figure 1 in which the annotated data for GEC provides knowledge to help the model rewrite the ungrammatical informal sentence. 2.2 Pre-training with Augmented Data In general, massive augmented parallel data can help a seq2seq model to learn contextualized representations, sentence generation and source-target alignments better. When the augmented parallel 3 https://translate.google.com/ 3222 σ = 0.6 in our experiments. data is available, previous studies (Sennrich et al., 2016a; Edunov et al., 2018; Karakanta et al., 2018; Wang et al., 2018) for seq2seq tasks are inclined to train a seq2seq model with original training data and augmented data simultaneously. However, augmented data is usually noisier and less valuable than original training data. In simultaneous training, the massive augmented data tends to overwhelm the original data and introduce unnecessary and even erroneous editing knowledge, which is undesirable for our task. To better exploit the augmented data, we propose to first pre-train the model with augmented parallel data and then fine-tune the model with the original tra"
2020.acl-main.294,P17-2090,0,0.0227105,"rse augmented data with various formality style transfer knowledge. The augmented data can significantly help improve the performance when it is used for pre-training the model and leads to the state-of-the-art results in the formality style transfer benchmark dataset. Acknowledgements We thank all the reviewers for providing the constructive suggestions. This work is partly supported by Beijing Academy of Artificial Intelligence. Xu Sun is the corresponding author of this paper. Related Work Data augmentation has been much explored for seq2seq tasks like Machine Translation (He et al., 2016; Fadaee et al., 2017; Zhang et al., 2018b; PonReferences Albert C Baugh and Thomas Cable. 1993. A history of the English language. Routledge. 3225 Kyunghyun Cho, Bart van Merrienboer, C¸aglar G¨ulc¸ehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation. CoRR, abs/1406.1078. Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu. 2013. Building a large annotated corpus of learner english: The nus corpus of learner english. In Proceedings of the eighth workshop on innovative use of NLP for building educational applications"
2020.acl-main.294,P18-1097,1,0.860365,"entation ambiguity problem but brings fair improvement because of its largest size. In contrast, the German-based augmented data has relatively high quality and a moderate size, leading to the best result in our experiments. 4 E&M BLEU 69.44 70.09 71.15 70.51 F&R BLEU 74.19 74.52 75.18 74.79 Table 6: Performances of formality discrimination based on different pivot languages: French (Fr), German (De) and Chinese (Zh). celas et al., 2018; Edunov et al., 2018; Li et al., 2019) and Grammatical Error Correction (Kiyono et al., 2019; Grundkiewicz et al., 2019; Zhao et al., 2019; Zhou et al., 2019; Ge et al., 2018a,b; Xie et al., 2018; Yuan et al., 2016; Rei et al., 2017). For text style transfer, however, due to the lack of parallel data, many studies focus on unsupervised approaches (Luo et al., 2019; Wu et al., 2019; Zhang et al., 2018a) and there is little related work concerning data augmentation. As a result, most recent work (Jhamtani et al., 2017; Xu et al., 2012) that models text style transfer as MT suffers from a lack of parallel data for training, which seriously limits the performance of powerful models. To solve this pain point, we propose novel data augmentation methods and study the bes"
2020.acl-main.294,P19-1609,1,0.851924,"valuable rewriting knowledge that is not covered by the original parallel data. 2.1.3 Multi-task transfer In addition to back translation and formality discrimination that use artificially generated sentence pairs for data augmentation, we introduce multitask transfer that uses annotated sentence pairs from other seq2seq tasks. We observe that informal texts are usually ungrammatical while formal texts are almost grammatically correct. Therefore, a desirable FST model should possess the ability to detect and rewrite ungrammatical texts, which has been verified by the previous empirical study (Ge et al., 2019) showing that using a state-of-theart grammatical error correction (GEC) model to post-process the outputs of an FST model can improve the result. Inspired by this observation, we propose to transfer the knowledge from GEC to FST by leveraging the GEC training data as the augmented parallel data to help improve formality. An example is illustrated in Figure 1 in which the annotated data for GEC provides knowledge to help the model rewrite the ungrammatical informal sentence. 2.2 Pre-training with Augmented Data In general, massive augmented parallel data can help a seq2seq model to learn conte"
2020.acl-main.294,W19-4427,0,0.0182589,"d augmented data introduces more noise due to the additional segmentation ambiguity problem but brings fair improvement because of its largest size. In contrast, the German-based augmented data has relatively high quality and a moderate size, leading to the best result in our experiments. 4 E&M BLEU 69.44 70.09 71.15 70.51 F&R BLEU 74.19 74.52 75.18 74.79 Table 6: Performances of formality discrimination based on different pivot languages: French (Fr), German (De) and Chinese (Zh). celas et al., 2018; Edunov et al., 2018; Li et al., 2019) and Grammatical Error Correction (Kiyono et al., 2019; Grundkiewicz et al., 2019; Zhao et al., 2019; Zhou et al., 2019; Ge et al., 2018a,b; Xie et al., 2018; Yuan et al., 2016; Rei et al., 2017). For text style transfer, however, due to the lack of parallel data, many studies focus on unsupervised approaches (Luo et al., 2019; Wu et al., 2019; Zhang et al., 2018a) and there is little related work concerning data augmentation. As a result, most recent work (Jhamtani et al., 2017; Xu et al., 2012) that models text style transfer as MT suffers from a lack of parallel data for training, which seriously limits the performance of powerful models. To solve this pain point, we pr"
2020.acl-main.294,W17-4902,0,0.0494237,"n based on different pivot languages: French (Fr), German (De) and Chinese (Zh). celas et al., 2018; Edunov et al., 2018; Li et al., 2019) and Grammatical Error Correction (Kiyono et al., 2019; Grundkiewicz et al., 2019; Zhao et al., 2019; Zhou et al., 2019; Ge et al., 2018a,b; Xie et al., 2018; Yuan et al., 2016; Rei et al., 2017). For text style transfer, however, due to the lack of parallel data, many studies focus on unsupervised approaches (Luo et al., 2019; Wu et al., 2019; Zhang et al., 2018a) and there is little related work concerning data augmentation. As a result, most recent work (Jhamtani et al., 2017; Xu et al., 2012) that models text style transfer as MT suffers from a lack of parallel data for training, which seriously limits the performance of powerful models. To solve this pain point, we propose novel data augmentation methods and study the best way to utilize the augmented data, which not only achieves a success in formality style transfer, but also would be inspiring for other text style transfer tasks. 5 Conclusion In this paper, we propose novel data augmentation methods for formality style transfer. Our proposed data augmentation methods can effectively generate diverse augmented"
2020.acl-main.294,D19-1119,0,0.0144967,"ages, the Chinesebased augmented data introduces more noise due to the additional segmentation ambiguity problem but brings fair improvement because of its largest size. In contrast, the German-based augmented data has relatively high quality and a moderate size, leading to the best result in our experiments. 4 E&M BLEU 69.44 70.09 71.15 70.51 F&R BLEU 74.19 74.52 75.18 74.79 Table 6: Performances of formality discrimination based on different pivot languages: French (Fr), German (De) and Chinese (Zh). celas et al., 2018; Edunov et al., 2018; Li et al., 2019) and Grammatical Error Correction (Kiyono et al., 2019; Grundkiewicz et al., 2019; Zhao et al., 2019; Zhou et al., 2019; Ge et al., 2018a,b; Xie et al., 2018; Yuan et al., 2016; Rei et al., 2017). For text style transfer, however, due to the lack of parallel data, many studies focus on unsupervised approaches (Luo et al., 2019; Wu et al., 2019; Zhang et al., 2018a) and there is little related work concerning data augmentation. As a result, most recent work (Jhamtani et al., 2017; Xu et al., 2012) that models text style transfer as MT suffers from a lack of parallel data for training, which seriously limits the performance of powerful models. To s"
2020.acl-main.294,I11-1017,0,0.0654449,"Missing"
2020.acl-main.294,C18-1086,0,0.160822,"by GPT (Radford et al., E&M BLEU 50.28 60.37 66.88 58.27 67.51 71.29 72.01 72.70 69.86 72.63 74.24 F&R BLEU 51.67 66.40 72.40 68.26 73.78 74.51 75.33 77.26 76.32 77.01 77.97 Table 3: The comparison of our approach to the stateof-the-art results. * denotes the ensemble results. 2019). Specifically, GPT-CAT concatenates the original input sentence and the input sentence preprocessed by rules as input, while GPT-Ensemble is the ensemble of two GPT-based encoder-decoder models: one takes the original input sentence as input, the other takes the preprocssed sentence as input. Following Niu et al. (2018), we train 4 independent models with different initializations for ensemble decoding. According to Table 3, our single model performs comparably to the state-ofthe-art GPT-based encoder-decoder models (more than 200M parameters) with only 54M parameters. Our ensemble model further advances the state-ofthe-art result only with a comparable model size to the GPT-based single model (i.e., GPT-CAT). We also conduct human evaluation. Following Rao and Tetreault (2018), we assess the model output on three criteria: formality, fluency and meaning preservation. We compare our baseline model trained wi"
2020.acl-main.294,N18-1012,0,0.427327,"data. In this paper, we study how to augment parallel data and propose novel and simple data augmentation methods for this task to obtain useful sentence pairs with easily accessible models and systems. Experiments demonstrate that our augmented parallel data largely helps improve formality style transfer when it is used to pre-train the model, leading to the state-of-the-art results in the GYAFC benchmark dataset1 . 1 F-Dis MT M-Task Introduction Formality style transfer (FST) is defined as the task of automatically transforming a piece of text in one particular formality style into another (Rao and Tetreault, 2018). For example, given an informal sentence, FST aims to preserve the styleindependent content and output a formal sentence. Previous work tends to leverage neural networks (Xu et al., 2019; Niu et al., 2018; Wang et al., 2019) such as seq2seq models to address this challenge due to their powerful capability and large improvement over the traditional rule-based approaches (Rao and Tetreault, 2018). However, the performance of the neural network approaches is still limited by the inadequacy of training data: the public parallel corpus for FST training – GYAFC (Rao and Tetreault, 2018) – contains"
2020.acl-main.294,W17-5032,0,0.0298358,"cause of its largest size. In contrast, the German-based augmented data has relatively high quality and a moderate size, leading to the best result in our experiments. 4 E&M BLEU 69.44 70.09 71.15 70.51 F&R BLEU 74.19 74.52 75.18 74.79 Table 6: Performances of formality discrimination based on different pivot languages: French (Fr), German (De) and Chinese (Zh). celas et al., 2018; Edunov et al., 2018; Li et al., 2019) and Grammatical Error Correction (Kiyono et al., 2019; Grundkiewicz et al., 2019; Zhao et al., 2019; Zhou et al., 2019; Ge et al., 2018a,b; Xie et al., 2018; Yuan et al., 2016; Rei et al., 2017). For text style transfer, however, due to the lack of parallel data, many studies focus on unsupervised approaches (Luo et al., 2019; Wu et al., 2019; Zhang et al., 2018a) and there is little related work concerning data augmentation. As a result, most recent work (Jhamtani et al., 2017; Xu et al., 2012) that models text style transfer as MT suffers from a lack of parallel data for training, which seriously limits the performance of powerful models. To solve this pain point, we propose novel data augmentation methods and study the best way to utilize the augmented data, which not only achieve"
2020.acl-main.294,P16-1009,0,0.592342,"ne chance. Target I don't know ... Good luck. Source I think she like cat too. Target I think she likes cat too. Figure 1: An example that Formality Style Transfer (FST) benefits from data augmented via formality discrimination (F-Dis) and multi-task transfer (MTask). The mapping knowledge indicated by the color (blue→pink) in FST test instance occur in the pairs augmented by F-Dis and M-Task. F-Dis identifies useful sentence pairs from paraphrased sentence pairs generated by cross-lingual MT, while M-Task utilizes training data from GEC to help formality improvement. translation (BT) method (Sennrich et al., 2016a) in Machine Translation (MT) to FST, our data augmentation methods include formality discrimination (F-Dis) and multi-task transfer (M-Task). They are both novel and effective in generating parallel data that introduces additional formality transfer knowledge that cannot be derived from the original training data. Specifically, F-Dis identifies useful pairs from the paraphrased pairs generated by cross-lingual MT; while M-task leverages the training data of Grammatical Error Correction (GEC) task to improve formality, as shown in Figure 1. Experimental results show that our proposed data aug"
2020.acl-main.294,P16-1162,0,0.739721,"ne chance. Target I don't know ... Good luck. Source I think she like cat too. Target I think she likes cat too. Figure 1: An example that Formality Style Transfer (FST) benefits from data augmented via formality discrimination (F-Dis) and multi-task transfer (MTask). The mapping knowledge indicated by the color (blue→pink) in FST test instance occur in the pairs augmented by F-Dis and M-Task. F-Dis identifies useful sentence pairs from paraphrased sentence pairs generated by cross-lingual MT, while M-Task utilizes training data from GEC to help formality improvement. translation (BT) method (Sennrich et al., 2016a) in Machine Translation (MT) to FST, our data augmentation methods include formality discrimination (F-Dis) and multi-task transfer (M-Task). They are both novel and effective in generating parallel data that introduces additional formality transfer knowledge that cannot be derived from the original training data. Specifically, F-Dis identifies useful pairs from the paraphrased pairs generated by cross-lingual MT; while M-task leverages the training data of Grammatical Error Correction (GEC) task to improve formality, as shown in Figure 1. Experimental results show that our proposed data aug"
2020.acl-main.294,C12-1177,0,0.0594951,"vot languages: French (Fr), German (De) and Chinese (Zh). celas et al., 2018; Edunov et al., 2018; Li et al., 2019) and Grammatical Error Correction (Kiyono et al., 2019; Grundkiewicz et al., 2019; Zhao et al., 2019; Zhou et al., 2019; Ge et al., 2018a,b; Xie et al., 2018; Yuan et al., 2016; Rei et al., 2017). For text style transfer, however, due to the lack of parallel data, many studies focus on unsupervised approaches (Luo et al., 2019; Wu et al., 2019; Zhang et al., 2018a) and there is little related work concerning data augmentation. As a result, most recent work (Jhamtani et al., 2017; Xu et al., 2012) that models text style transfer as MT suffers from a lack of parallel data for training, which seriously limits the performance of powerful models. To solve this pain point, we propose novel data augmentation methods and study the best way to utilize the augmented data, which not only achieves a success in formality style transfer, but also would be inspiring for other text style transfer tasks. 5 Conclusion In this paper, we propose novel data augmentation methods for formality style transfer. Our proposed data augmentation methods can effectively generate diverse augmented data with various"
2020.acl-main.294,W16-0530,0,0.0653181,"Missing"
2020.acl-main.294,P12-2039,0,0.0701122,"Missing"
2020.acl-main.294,D18-1138,1,0.851243,"E&M BLEU 69.44 70.09 71.15 70.51 F&R BLEU 74.19 74.52 75.18 74.79 Table 6: Performances of formality discrimination based on different pivot languages: French (Fr), German (De) and Chinese (Zh). celas et al., 2018; Edunov et al., 2018; Li et al., 2019) and Grammatical Error Correction (Kiyono et al., 2019; Grundkiewicz et al., 2019; Zhao et al., 2019; Zhou et al., 2019; Ge et al., 2018a,b; Xie et al., 2018; Yuan et al., 2016; Rei et al., 2017). For text style transfer, however, due to the lack of parallel data, many studies focus on unsupervised approaches (Luo et al., 2019; Wu et al., 2019; Zhang et al., 2018a) and there is little related work concerning data augmentation. As a result, most recent work (Jhamtani et al., 2017; Xu et al., 2012) that models text style transfer as MT suffers from a lack of parallel data for training, which seriously limits the performance of powerful models. To solve this pain point, we propose novel data augmentation methods and study the best way to utilize the augmented data, which not only achieves a success in formality style transfer, but also would be inspiring for other text style transfer tasks. 5 Conclusion In this paper, we propose novel data augmentation m"
2020.acl-main.294,D18-1100,0,0.0205002,"parallel data to help improve formality. An example is illustrated in Figure 1 in which the annotated data for GEC provides knowledge to help the model rewrite the ungrammatical informal sentence. 2.2 Pre-training with Augmented Data In general, massive augmented parallel data can help a seq2seq model to learn contextualized representations, sentence generation and source-target alignments better. When the augmented parallel 3 https://translate.google.com/ 3222 σ = 0.6 in our experiments. data is available, previous studies (Sennrich et al., 2016a; Edunov et al., 2018; Karakanta et al., 2018; Wang et al., 2018) for seq2seq tasks are inclined to train a seq2seq model with original training data and augmented data simultaneously. However, augmented data is usually noisier and less valuable than original training data. In simultaneous training, the massive augmented data tends to overwhelm the original data and introduce unnecessary and even erroneous editing knowledge, which is undesirable for our task. To better exploit the augmented data, we propose to first pre-train the model with augmented parallel data and then fine-tune the model with the original training data. In our pre-training & finetuning"
2020.acl-main.294,N19-1014,0,0.023737,"more noise due to the additional segmentation ambiguity problem but brings fair improvement because of its largest size. In contrast, the German-based augmented data has relatively high quality and a moderate size, leading to the best result in our experiments. 4 E&M BLEU 69.44 70.09 71.15 70.51 F&R BLEU 74.19 74.52 75.18 74.79 Table 6: Performances of formality discrimination based on different pivot languages: French (Fr), German (De) and Chinese (Zh). celas et al., 2018; Edunov et al., 2018; Li et al., 2019) and Grammatical Error Correction (Kiyono et al., 2019; Grundkiewicz et al., 2019; Zhao et al., 2019; Zhou et al., 2019; Ge et al., 2018a,b; Xie et al., 2018; Yuan et al., 2016; Rei et al., 2017). For text style transfer, however, due to the lack of parallel data, many studies focus on unsupervised approaches (Luo et al., 2019; Wu et al., 2019; Zhang et al., 2018a) and there is little related work concerning data augmentation. As a result, most recent work (Jhamtani et al., 2017; Xu et al., 2012) that models text style transfer as MT suffers from a lack of parallel data for training, which seriously limits the performance of powerful models. To solve this pain point, we propose novel data au"
2020.acl-main.294,D19-1365,0,0.251114,"our augmented parallel data largely helps improve formality style transfer when it is used to pre-train the model, leading to the state-of-the-art results in the GYAFC benchmark dataset1 . 1 F-Dis MT M-Task Introduction Formality style transfer (FST) is defined as the task of automatically transforming a piece of text in one particular formality style into another (Rao and Tetreault, 2018). For example, given an informal sentence, FST aims to preserve the styleindependent content and output a formal sentence. Previous work tends to leverage neural networks (Xu et al., 2019; Niu et al., 2018; Wang et al., 2019) such as seq2seq models to address this challenge due to their powerful capability and large improvement over the traditional rule-based approaches (Rao and Tetreault, 2018). However, the performance of the neural network approaches is still limited by the inadequacy of training data: the public parallel corpus for FST training – GYAFC (Rao and Tetreault, 2018) – contains only approximately 100K sentence pairs, which can hardly satiate the neural models with millions of parameters. To tackle the data sparsity problem for FST, we propose to augment parallel data with three specific data augment"
2020.acl-main.294,P19-1482,1,0.722464,"ur experiments. 4 E&M BLEU 69.44 70.09 71.15 70.51 F&R BLEU 74.19 74.52 75.18 74.79 Table 6: Performances of formality discrimination based on different pivot languages: French (Fr), German (De) and Chinese (Zh). celas et al., 2018; Edunov et al., 2018; Li et al., 2019) and Grammatical Error Correction (Kiyono et al., 2019; Grundkiewicz et al., 2019; Zhao et al., 2019; Zhou et al., 2019; Ge et al., 2018a,b; Xie et al., 2018; Yuan et al., 2016; Rei et al., 2017). For text style transfer, however, due to the lack of parallel data, many studies focus on unsupervised approaches (Luo et al., 2019; Wu et al., 2019; Zhang et al., 2018a) and there is little related work concerning data augmentation. As a result, most recent work (Jhamtani et al., 2017; Xu et al., 2012) that models text style transfer as MT suffers from a lack of parallel data for training, which seriously limits the performance of powerful models. To solve this pain point, we propose novel data augmentation methods and study the best way to utilize the augmented data, which not only achieves a success in formality style transfer, but also would be inspiring for other text style transfer tasks. 5 Conclusion In this paper, we propose novel"
2020.acl-main.294,N18-1057,0,0.0191216,"roblem but brings fair improvement because of its largest size. In contrast, the German-based augmented data has relatively high quality and a moderate size, leading to the best result in our experiments. 4 E&M BLEU 69.44 70.09 71.15 70.51 F&R BLEU 74.19 74.52 75.18 74.79 Table 6: Performances of formality discrimination based on different pivot languages: French (Fr), German (De) and Chinese (Zh). celas et al., 2018; Edunov et al., 2018; Li et al., 2019) and Grammatical Error Correction (Kiyono et al., 2019; Grundkiewicz et al., 2019; Zhao et al., 2019; Zhou et al., 2019; Ge et al., 2018a,b; Xie et al., 2018; Yuan et al., 2016; Rei et al., 2017). For text style transfer, however, due to the lack of parallel data, many studies focus on unsupervised approaches (Luo et al., 2019; Wu et al., 2019; Zhang et al., 2018a) and there is little related work concerning data augmentation. As a result, most recent work (Jhamtani et al., 2017; Xu et al., 2012) that models text style transfer as MT suffers from a lack of parallel data for training, which seriously limits the performance of powerful models. To solve this pain point, we propose novel data augmentation methods and study the best way to utilize the"
2020.acl-main.294,W13-1703,0,\N,Missing
2020.acl-main.406,P11-1056,1,0.741829,". Note that a returned sentence is denoted as c , and its context is denoted as T (c). 3.2 Extraction as Textual Entailment Given a sentence c within its context T (c) returned by claim search for q, we need to identify the sources in T (c) that are talking about a statement related to q. This is actually an IE task. Typically, IE is a sequential tagging problem: it needs to learn linguistic patterns from annotated data using syntactic and semantic features, which can express the targeted semantic relations. Most of the solutions in the literature (Surdeanu et al., 2012; Schmitz et al., 2012; Chan and Roth, 2011; Li and Ji, 2014) focus on extracting relationships between two named entities or two nominals. However, in our problem, the relationship of interest is between a nominal/an entity and a statement. The statement can be written either explicitly or implicitly in the given context, and what we only know is that the statement is about q. Therefore, annotation is hard, and existing IE solutions can not be used in this case. Furthermore, the source and the statement may appear across sentences rather than within a single sentence, therefore, coreference resolution may be necessary. For example, “T"
2020.acl-main.406,D13-1184,1,0.871723,"ce and the claim. In terms of creating the corresponding negative examples, we randomly replace either ARG 0 or ARG 1 with other sources or claims. Then we use those created examples to incrementally fine-tune our TE extraction model, which can lead to a better performance. 3.3 Constructing the Graph After extracting the provenance information, the last process is to construct the provenance graph. The first step thereof, is to link the same sources detected in the text with the same statement. Since the sources can be a url or a mention of an entity, we do wikification (Ratinov et al., 2011; Cheng and Roth, 2013) for the extracted sources. Specifically, to wikify a source mention, we first adapt a redirectbased wikification method (RedW) (Shnayderman et al., 2019), which is efficient and context free. Besides Wikipedia redirects, we also include the value of the attribute website as a candidate mention of the entity if it exists, for example nytimes.com for The New York Times. Then we compute the text similarity between the source mention and the other mentions that have already been linked, and eventually map the source mention to the entity in Wikipedia with a similarity score higher than a threshol"
2020.acl-main.406,H05-1045,0,0.150393,"imental Evaluation We evaluate (1) the solutions to infer the provenance graph, and (2) the effectiveness of the claim evidence graph on claim verification, which is adapted from the inferred provenance graph. For each goal, we first elaborate the experimental settings, and then describe the results and analysis 3 . 5.1 Claim Search and Source Extraction To evaluate the methods inferring the provenance, we focus on the performance of claim search and source extraction by looking at if the method can extract the sources accurately and exhaustively. DataSet In this experiment, we use MPQA 2.04 (Choi et al., 2005) as the corpus to train and test our models. The dataset consists of 535 documents that have been manually annotated with opinion related information including sources. For example, given a piece of text “... According to Malan, the dollarization process is irreversible ... ”, “Pedro Malan” is annotated that it has an opinion on “the dollarization process is irreversible”. Note that a single claim can be annotated with multiple sources including the writer of the text, and each source except the writer is a span of text in the given text. MPQA dataset is originally developed for identifying so"
2020.acl-main.406,E17-1044,0,0.0188096,"ng articles as sources is too coarse-grained for claim verification, and thus it is very likely to be biased. The evidence graph provides the models with evidence from more sources (All-Src) and sources that are more likely to be independent (Prov-Src), thus improves the performance. 6 Related Work To the best of our knowledge, our work is the first to formally define and propose a framework to infer the provenance graph of given claims made in natural language. One line of the related work includes identifying sources of opinions in opinion analysis (Choi et al., 2005) and quote attribution (Muzny et al., 2017; Pavllo et al., 2018), which is related to one of the components we use to infer the provenance graph. Earlier work performs information extraction via sequential tagging in a given text and collects paired sources and opinions or quotes and speakers. We do not detect all quotes or opinions stated in the text, but rather detect the sources generating statements related to the given claim, whether it is described implicitly or explicitly in the text. Furthermore, we also construct a graph that depicts the history of how a claim has disseminated over time, a task that was not addressed in earli"
2020.acl-main.406,Q17-1008,0,0.019617,"written either explicitly or implicitly in the given context, and what we only know is that the statement is about q. Therefore, annotation is hard, and existing IE solutions can not be used in this case. Furthermore, the source and the statement may appear across sentences rather than within a single sentence, therefore, coreference resolution may be necessary. For example, “The website Hoax Slayer said the message dates back to 2012 and has recently resurfaced ... it also noted Facebook has no plans to start charging users for normal access...” requires a cross-sentence relation extraction (Peng et al., 2017). Rather than tackling the problem as a sequential tagging problem, we model it as a textual entailment (TE) problem (Dagan et al., 2013). Similar to QA-SRL (He et al., 2015), TE-IE task formulation has the advantages of (1) easier annotation (2) being able to capture implicit statements and implicit sources which requires coreference resolution. TE Modeling We use the dataset (Choi et al., 2005) that contains a set of annotated articles. For each article, it annotates “who” has an opinion on “what”. Formally, given a corpus D, for each article d ∈ D, our training data comes in the form of pai"
2020.acl-main.406,N18-1202,0,0.0161118,"h for the given claim, we need to solve the three problems outlined in Section 2. Here, we propose a pipelined solution, and elaborate them one by one. 3.1 Searching for the Context As we described in Section 2, accurately locating the previous statements about the claim is a very challenging problem. Therefore, instead of directly searching for a possible previous statement, we search for related context, where the source are describing a statement related to the claim. Specifically, we rank sentences in the given corpus, by computing the cosine similarity to the given claim with their ELMo (Peters et al., 2018) representations. Then, we choose sentences that are most similar and fetch their context in a window size w, which means we consider w sentences before and after the returned sentence together as the context, from which we will extract the sources. Note that a returned sentence is denoted as c , and its context is denoted as T (c). 3.2 Extraction as Textual Entailment Given a sentence c within its context T (c) returned by claim search for q, we need to identify the sources in T (c) that are talking about a statement related to q. This is actually an IE task. Typically, IE is a sequential tag"
2020.acl-main.406,N13-1092,0,0.0531826,"Missing"
2020.acl-main.406,P18-2058,0,0.0363513,"of positive and negative examples in the annotated data. 4420 For training, we use a loss function L combining both cross-entropy loss for binary prediction and the margin ranking loss to maximize the difference between positive and negative examples to finetune the language model. That is: L = λLcross + (1 − λ)Lpair (5) − where Lpair = L+ pair + Lpair , and λ is the parameter to trade off different objectives. Candidate Generation. The next question is how to generate source candidate list sc(cdi ) for cdi given T (cdi ). Here, we leverage an off-the-shelf semantic role labeling (SRL) tool (He et al., 2018) that can parse the sentences T (cdi ) to tell us “who did what to whom” in the appropriate sentences. We then take all “who”, i.e., the span of the text with tag ARG 0 detected as a candidate source of cdi . Even though only the “who” followed by a verb such as “say” or “claim” can be the source theoretically, we included all of them as candidates, and leave the identification made by our TE model. Note that here we only use SRL to generate candidate sources. Considering (1) the noisy relationship produced by SRL parser, (2) the crosssentence relationship between the source and the claim, and"
2020.acl-main.406,D15-1076,0,0.0427243,"Missing"
2020.acl-main.406,W09-2415,0,0.0269117,"Missing"
2020.acl-main.406,P14-1038,0,0.0121967,"d sentence is denoted as c , and its context is denoted as T (c). 3.2 Extraction as Textual Entailment Given a sentence c within its context T (c) returned by claim search for q, we need to identify the sources in T (c) that are talking about a statement related to q. This is actually an IE task. Typically, IE is a sequential tagging problem: it needs to learn linguistic patterns from annotated data using syntactic and semantic features, which can express the targeted semantic relations. Most of the solutions in the literature (Surdeanu et al., 2012; Schmitz et al., 2012; Chan and Roth, 2011; Li and Ji, 2014) focus on extracting relationships between two named entities or two nominals. However, in our problem, the relationship of interest is between a nominal/an entity and a statement. The statement can be written either explicitly or implicitly in the given context, and what we only know is that the statement is about q. Therefore, annotation is hard, and existing IE solutions can not be used in this case. Furthermore, the source and the statement may appear across sentences rather than within a single sentence, therefore, coreference resolution may be necessary. For example, “The website Hoax Sl"
2020.acl-main.406,P11-1138,1,0.505964,"ve example of the source and the claim. In terms of creating the corresponding negative examples, we randomly replace either ARG 0 or ARG 1 with other sources or claims. Then we use those created examples to incrementally fine-tune our TE extraction model, which can lead to a better performance. 3.3 Constructing the Graph After extracting the provenance information, the last process is to construct the provenance graph. The first step thereof, is to link the same sources detected in the text with the same statement. Since the sources can be a url or a mention of an entity, we do wikification (Ratinov et al., 2011; Cheng and Roth, 2013) for the extracted sources. Specifically, to wikify a source mention, we first adapt a redirectbased wikification method (RedW) (Shnayderman et al., 2019), which is efficient and context free. Besides Wikipedia redirects, we also include the value of the attribute website as a candidate mention of the entity if it exists, for example nytimes.com for The New York Times. Then we compute the text similarity between the source mention and the other mentions that have already been linked, and eventually map the source mention to the entity in Wikipedia with a similarity score"
2020.acl-main.406,D12-1048,0,0.0241947,"ll extract the sources. Note that a returned sentence is denoted as c , and its context is denoted as T (c). 3.2 Extraction as Textual Entailment Given a sentence c within its context T (c) returned by claim search for q, we need to identify the sources in T (c) that are talking about a statement related to q. This is actually an IE task. Typically, IE is a sequential tagging problem: it needs to learn linguistic patterns from annotated data using syntactic and semantic features, which can express the targeted semantic relations. Most of the solutions in the literature (Surdeanu et al., 2012; Schmitz et al., 2012; Chan and Roth, 2011; Li and Ji, 2014) focus on extracting relationships between two named entities or two nominals. However, in our problem, the relationship of interest is between a nominal/an entity and a statement. The statement can be written either explicitly or implicitly in the given context, and what we only know is that the statement is about q. Therefore, annotation is hard, and existing IE solutions can not be used in this case. Furthermore, the source and the statement may appear across sentences rather than within a single sentence, therefore, coreference resolution may be neces"
2020.acl-main.406,D12-1042,0,0.024584,"ntext, from which we will extract the sources. Note that a returned sentence is denoted as c , and its context is denoted as T (c). 3.2 Extraction as Textual Entailment Given a sentence c within its context T (c) returned by claim search for q, we need to identify the sources in T (c) that are talking about a statement related to q. This is actually an IE task. Typically, IE is a sequential tagging problem: it needs to learn linguistic patterns from annotated data using syntactic and semantic features, which can express the targeted semantic relations. Most of the solutions in the literature (Surdeanu et al., 2012; Schmitz et al., 2012; Chan and Roth, 2011; Li and Ji, 2014) focus on extracting relationships between two named entities or two nominals. However, in our problem, the relationship of interest is between a nominal/an entity and a statement. The statement can be written either explicitly or implicitly in the given context, and what we only know is that the statement is about q. Therefore, annotation is hard, and existing IE solutions can not be used in this case. Furthermore, the source and the statement may appear across sentences rather than within a single sentence, therefore, coreference r"
2020.acl-main.406,C18-1283,0,0.0251117,"the provenance graph. Earlier work performs information extraction via sequential tagging in a given text and collects paired sources and opinions or quotes and speakers. We do not detect all quotes or opinions stated in the text, but rather detect the sources generating statements related to the given claim, whether it is described implicitly or explicitly in the text. Furthermore, we also construct a graph that depicts the history of how a claim has disseminated over time, a task that was not addressed in earlier work. Another line of related work includes factchecking (Thorne et al., 2018; Thorne and Vlachos, 2018; Zhang et al., 2019) and claim verification (Popat et al., 2017, 2018). However, those works focus only on capturing discriminative linguistic features of misinformation, while we argue that determining the provenance of claims is essential for addressing the root of the problem, understanding claims and sources. 7 Conclusion and Future Work We introduce a formal definition and a computational framework for the provenance of a natural language claim given a corpus. We argue that this notion of provenance is essential if we are to understand how claims evolve over time, and what sources contri"
2020.acl-main.406,N18-1074,0,0.0918064,"Missing"
2020.acl-main.406,P19-1040,1,0.560778,"ier work performs information extraction via sequential tagging in a given text and collects paired sources and opinions or quotes and speakers. We do not detect all quotes or opinions stated in the text, but rather detect the sources generating statements related to the given claim, whether it is described implicitly or explicitly in the text. Furthermore, we also construct a graph that depicts the history of how a claim has disseminated over time, a task that was not addressed in earlier work. Another line of related work includes factchecking (Thorne et al., 2018; Thorne and Vlachos, 2018; Zhang et al., 2019) and claim verification (Popat et al., 2017, 2018). However, those works focus only on capturing discriminative linguistic features of misinformation, while we argue that determining the provenance of claims is essential for addressing the root of the problem, understanding claims and sources. 7 Conclusion and Future Work We introduce a formal definition and a computational framework for the provenance of a natural language claim given a corpus. We argue that this notion of provenance is essential if we are to understand how claims evolve over time, and what sources contributed to earlier vers"
2020.findings-emnlp.25,N19-1423,0,0.174366,"dge Representation from Pretrained Language Models 1 Zhiyuan Zhang1 , Xiaoqian Liu1, 2 , Yi Zhang1 , Qi Su1, 2 , Xu Sun1 and Bin He3 MOE Key Laboratory of Computational Linguistic, School of EECS, Peking University 2 School of Foreign Languages, Peking University 3 Huawei Noah’s Ark Lab {zzy1210,liuxiaoqian,zhangyi16,sukia,xusun}@pku.edu.cn hebin.nlp@huawei.com Abstract sparse and noisy dataset annotations. It leads to performance degradation, especially on the lowresource problem. To address this issue, we propose to enrich knowledge representation via pretrained language models (i.e., BERT (Devlin et al., 2019)) given a semantic description of entities and relations. We propose to incorporate world knowledge from BERT to the entity and the relation representation. Although simply fine-tuning BERT can enrich the knowledge representation, it suffers from learning inadequate structure information observed in training triplets, which we have demonstrated when we analyze the rationality of the KGE-training phase. Conventional knowledge graph embedding (KGE) often suffers from limited knowledge representation, leading to performance degradation especially on the low-resource problem. To remedy this, we pr"
2020.findings-emnlp.25,P17-1021,0,0.02994,"ing world knowledge from pretrained models. Specifically, we present a universal training framework named PretrainKGE consisting of three phases: semanticbased fine-tuning phase, knowledge extracting phase and KGE training phase. Extensive experiments show that our proposed PretrainKGE can improve results over KGE models, especially on solving the low-resource problem. 1 Introduction Knowledge graphs (KGs) constitute an effective access to world knowledge for a wide variety of NLP tasks, such as entity linking (Luo et al., 2017), information retrieval (Xiong et al., 2017), question answering (Hao et al., 2017) and recommendation system (Zhang et al., 2016). A typical KG such as Freebase (Bollacker et al., 2008) and WordNet (Miller, 1995), consists of a set of triplets in the form of (h, r, t) with the head entity h and the tail entity t as nodes and relation r as edges in the graph. A triplet represents the relation between two entities, e.g., (Steve Jobs, founded, Apple Inc.). To learn effective representation of entities and relations in the graph, knowledge graph embedding (KGE) models are one of prominent approaches (Bordes et al., 2013; Ji et al., 2015; Lin et al., 2015; Sun et al., 2019; Nick"
2020.findings-emnlp.25,P15-1067,0,0.262513,"iong et al., 2017), question answering (Hao et al., 2017) and recommendation system (Zhang et al., 2016). A typical KG such as Freebase (Bollacker et al., 2008) and WordNet (Miller, 1995), consists of a set of triplets in the form of (h, r, t) with the head entity h and the tail entity t as nodes and relation r as edges in the graph. A triplet represents the relation between two entities, e.g., (Steve Jobs, founded, Apple Inc.). To learn effective representation of entities and relations in the graph, knowledge graph embedding (KGE) models are one of prominent approaches (Bordes et al., 2013; Ji et al., 2015; Lin et al., 2015; Sun et al., 2019; Nickel et al., 2011; Yang et al., 2015; Kazemi and Poole, 2018; Trouillon et al., 2016; Zhang et al., 2019). However, traditional KGE models often suffer from limited knowledge representation due to the We propose a model-agnostic training framework for learning knowledge graph embedding consisting of three phases: semantic-based fine-tuning phase, knowledge extracting phase and KGE training phase (see Fig. 1). During the semantic-based fine-tuning phase, we learn knowledge representation via BERT given the semantic description of entities and relations as"
2020.findings-emnlp.25,N18-1202,0,0.0350897,"h entity and relation are provided as the semantic description of entities and relations. Recent works also leverage semantic description to enrich knowledge representation but ignore contextual information of the semantic description (Socher et al., 2013a; Li et al., 2016; Speer and Havasi, 2012; Xu et al., 2017; Xiao et al., 2017; Xie et al., 2016; An et al., 2018). Instead, our method exploits world information via pretrained models. Recent approaches to modeling language representations offer significant improvements over embeddings, such as pretrained deep contextualized language models (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; Raffel et al., 2019). KG-Bert (Yao et al., 2019) first utilizes BERT (Devlin et al., 2019) for knowledge graph completion, which treats triplets in knowledge graphs as textual sequences. However, KG-Bert does not extract knowledge representations from Bert and thus cannot provide entity or relation embeddings. In this work, we leverage world knowledge from BERT to learn better knowledge representation of entities and relations given semantic description. 3 3.1 Method Training Framework An overview of Pretrain-KGE is shown in Fig. 1. The framework co"
2020.findings-emnlp.338,W19-4828,0,0.0241648,"assess the linguistic knowledge learned by pre-trained LMs, probing task methodology suggest training supervised models on top of the word representations (Ettinger et al., 2016; Hupkes et al., 2018; Belinkov and Glass, 2019; Hewitt and Liang, 2019). Investigated linguistic aspects span across morphology (Shi et al., 2016; Belinkov et al., 2017; Liu et al., 2019a), syntax (Tenney et al., 2019; Hewitt and Manning, 2019), and semantics (Conneau et al., 2018; Liu et al., 2019a). Another line of research inspects internal states of pre-trained LMs such as attention weights (Kovaleva et al., 2019; Clark et al., 2019) or intermediate word representations (Coenen et al., 2019; Ethayarajh, 2019) to facilitate our understanding of how pre-trained LMs work. In particular, Voita et al. (2019) studies the evolution of representations from the bottom to top layers and finds that, for MLM, the token identity tends to be recreated at the top layer. A close work to us is Khandelwal et al. (2018), they conduct context analysis on LSTM language models to learn how much context is used and how nearby and long-range context is represented differently. Our work complements prior efforts by analyzing how models pre-traine"
2020.findings-emnlp.338,N19-1112,0,0.10654,"our findings in contextual analysis. 1 Introduction Pre-trained masked language models (MLM) such as BERT (Devlin et al., 2019) and ALBERT (Lan et al., 2019) have set state-of-the-art performance on a broad range of NLP tasks. The success is often attributed to their ability to capture complex syntactic and semantic characteristics of word use across diverse linguistic contexts (Peters et al., 2018). Yet, how these pre-trained MLMs make use of the context remains largely unanswered. Recent studies have started to inspect the linguistic knowledge learned by pre-trained LMs such as word sense (Liu et al., 2019a) , syntactic parse trees (Hewitt and Manning, 2019), and semantic relations (Tenney et al., 2019). Others directly analyze model’s intermediate representations and attention weights to understand how they work (Kovaleva et al., 2019; Voita et al., 2019). While previous works either assume access to model’s internal states or take advantage of model’s special structures such as self-attention maps, these analysis are difficult to generalize as the architectures evolve. In this paper, our work complements these previous efforts and provides a richer understanding of how pre-trained MLMs levera"
2020.findings-emnlp.338,P19-1012,0,0.0988216,"er perturbation causes a notable prediction change. Being model agnostic, our approach looks into the contextualization in the MLM task itself, and quantify them only on the output layer. We refrain from inspecting internal representations since new architectures might not have a clear notion of ”layer” with inter-leaving jump connections such as those in Guo et al. (2019) and Yao et al. (2020). 3789 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3789–3804 c November 16 - 20, 2020. 2020 Association for Computational Linguistics The second approach is adapted from Falenska and Kuhn (2019) and estimates the impact of an input subword to the target word probability via the norm of the gradients. We study pre-trained MLMs based on two different architectures: Transformer and BiLSTM. The former is essentially BERT and the latter resembles ELMo (Peters et al., 2018). Although the scope in this work is limited to the comparison between two popular architectures, the same novel methodology can be readily applied to multilingual models as well as other Transformerbased models pre-trained with MLM. From our analysis, when encoding words using sentence-level inputs, we find that BERT is"
2020.findings-emnlp.338,Q19-1019,0,0.0273949,"vely mask words that have the least change to the target word probability until the probability deviates too much from the start. At this point, the remaining words are relevant to and used by the MLM to represent the target word, since further perturbation causes a notable prediction change. Being model agnostic, our approach looks into the contextualization in the MLM task itself, and quantify them only on the output layer. We refrain from inspecting internal representations since new architectures might not have a clear notion of ”layer” with inter-leaving jump connections such as those in Guo et al. (2019) and Yao et al. (2020). 3789 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3789–3804 c November 16 - 20, 2020. 2020 Association for Computational Linguistics The second approach is adapted from Falenska and Kuhn (2019) and estimates the impact of an input subword to the target word probability via the norm of the gradients. We study pre-trained MLMs based on two different architectures: Transformer and BiLSTM. The former is essentially BERT and the latter resembles ELMo (Peters et al., 2018). Although the scope in this work is limited to the comparison between tw"
2020.findings-emnlp.338,D19-1445,0,0.104343,"he success is often attributed to their ability to capture complex syntactic and semantic characteristics of word use across diverse linguistic contexts (Peters et al., 2018). Yet, how these pre-trained MLMs make use of the context remains largely unanswered. Recent studies have started to inspect the linguistic knowledge learned by pre-trained LMs such as word sense (Liu et al., 2019a) , syntactic parse trees (Hewitt and Manning, 2019), and semantic relations (Tenney et al., 2019). Others directly analyze model’s intermediate representations and attention weights to understand how they work (Kovaleva et al., 2019; Voita et al., 2019). While previous works either assume access to model’s internal states or take advantage of model’s special structures such as self-attention maps, these analysis are difficult to generalize as the architectures evolve. In this paper, our work complements these previous efforts and provides a richer understanding of how pre-trained MLMs leverage context without assumptions on architectures. We aim to answer following questions: (i) How much context is relevant to and used by pre-trained MLMs when composing representations? (ii) How far do MLMs look when leveraging context?"
2020.findings-emnlp.338,silveira-etal-2014-gold,0,0.0239132,"Missing"
2020.lrec-1.215,W19-5906,0,0.0159006,"ive statistics that summarizes a data collection by a group of unsupervised measures such as mean or median for central tendency, variance or minimum-maximum for dispersion, skewness for symmetry, and kurtosis for heavy-tailed analysis. In recent years, text classification, a category of Natural Language Processing (NLP) tasks, has drawn much attention (Zhang et al., 2015; Joulin et al., 2016; Howard and Ruder, 2018) for its wide-ranging real-world applications such as fake news detection (Shu et al., 2017), document classification (Yang et al., 2016), and spoken language understanding (SLU) (Gupta et al., 2019a; Gupta et al., 2019b; Zhang et al., 2018), a core task of conversational assistants like Amazon Alexa or Google Assistant. However, there are still insufficient characteristic metrics to describe a collection of texts. Unlike numeric or categorical data, simple descriptive statistics alone such as word counts and vocabulary size are difficult to capture the syntactic and semantic properties of a text collection. In this work, we propose a set of characteristic metrics: diversity, density, and homogeneity to quantitatively summarize a collection of texts where the unit of texts could be a phr"
2020.lrec-1.215,D19-1127,1,0.856168,"Missing"
2020.lrec-1.215,P18-1031,0,0.0639551,"ribe or summarize the properties of a data collection. These metrics generally do not use groundtruth labels and only measure the intrinsic characteristics of data. The most prominent example is descriptive statistics that summarizes a data collection by a group of unsupervised measures such as mean or median for central tendency, variance or minimum-maximum for dispersion, skewness for symmetry, and kurtosis for heavy-tailed analysis. In recent years, text classification, a category of Natural Language Processing (NLP) tasks, has drawn much attention (Zhang et al., 2015; Joulin et al., 2016; Howard and Ruder, 2018) for its wide-ranging real-world applications such as fake news detection (Shu et al., 2017), document classification (Yang et al., 2016), and spoken language understanding (SLU) (Gupta et al., 2019a; Gupta et al., 2019b; Zhang et al., 2018), a core task of conversational assistants like Amazon Alexa or Google Assistant. However, there are still insufficient characteristic metrics to describe a collection of texts. Unlike numeric or categorical data, simple descriptive statistics alone such as word counts and vocabulary size are difficult to capture the syntactic and semantic properties of a t"
2020.lrec-1.215,P17-4015,0,0.0201776,"ignificantly improving many NLP tasks. Among the most popular approaches are ULMFiT (Howard and Ruder, 2018), ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2018). In this work, we adopt BERT, a transformer-based technique for NLP pretraining, as the backbone to embed a sentence or a paragraph into a representation vector. Another stream of related works is the evaluation metrics for cluster analysis. As measuring property or quality of outputs from a clustering algorithm is difficult, human judgment with cluster visualization tools (Kwon et al., 2017; Kessler, 2017) are often used. There are unsupervised metrics to measure the quality of a clustering result such as the Calinski-Harabasz score (Cali´nski and Harabasz, 1974), the Davies-Bouldin index (Davies and Bouldin, 1979), and the Silhouette coefficients (Rousseeuw, 1987). Complementary to these works that model cross-cluster similarities or relationships, our proposed diversity, density and homogeneity metrics focus on the characteristics of each single cluster, i.e., intra cluster rather than inter cluster relationships. 3. Proposed Characteristic Metrics We introduce our proposed diversity, density"
2020.lrec-1.215,D14-1162,0,0.0847869,"Missing"
2020.lrec-1.215,N18-1202,0,0.00924372,"hought vectors (Kiros et al., 2015), and self-attentive sentence encoders (Lin et al., 2017). 1739 More recently, there is a paradigm shift from noncontextualized word embeddings to self-supervised language model (LM) pretraining. Language encoders are pretrained on a large text corpus using a LM-based objective and then re-used for other NLP tasks in a transfer learning manner. These methods can produce contextualized word representations, which have proven to be effective for significantly improving many NLP tasks. Among the most popular approaches are ULMFiT (Howard and Ruder, 2018), ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2018). In this work, we adopt BERT, a transformer-based technique for NLP pretraining, as the backbone to embed a sentence or a paragraph into a representation vector. Another stream of related works is the evaluation metrics for cluster analysis. As measuring property or quality of outputs from a clustering algorithm is difficult, human judgment with cluster visualization tools (Kwon et al., 2017; Kessler, 2017) are often used. There are unsupervised metrics to measure the quality of a clustering result such as the Calinski-Harabas"
2020.lrec-1.215,P13-1045,0,0.0105301,"here ei ∈ RH . A text collection {x1 , ..., xm }, i.e., a set of token sequences, is then transformed into a group of Hdimensional vectors {e1 , ..., em }. 4 https://gluon-nlp.mxnet.io/model_zoo/ bert/index.html We compute each metric as described previously, using three BERT layers L1, L6, and L12 as the embedding space, respectively. The calculated metric values are averaged over layers for each class and averaged over classes weighted by class size as the final value for a dataset. 5.2. Experimental Setup In the first task, we use the SST-2 (Stanford Sentiment Treebank, version 2) dataset (Socher et al., 2013) to conduct sentiment analysis experiments. SST-2 is a sentence binary classification dataset with train/dev/test splits provided and two types of sentence labels, i.e., positive and negative. The second task involves two essential problems in SLU, which are intent classification (IC) and slot labeling (SL). In IC, the model needs to detect the intention of a text input (i.e., utterance, conveys). For example, for an input of I want to book a flight to Seattle, the intention is to book a flight ticket, hence the intent class is bookFlight. In SL, the model needs to extract the semantic entitie"
2020.lrec-1.215,N16-1174,0,0.0461291,"aracteristics of data. The most prominent example is descriptive statistics that summarizes a data collection by a group of unsupervised measures such as mean or median for central tendency, variance or minimum-maximum for dispersion, skewness for symmetry, and kurtosis for heavy-tailed analysis. In recent years, text classification, a category of Natural Language Processing (NLP) tasks, has drawn much attention (Zhang et al., 2015; Joulin et al., 2016; Howard and Ruder, 2018) for its wide-ranging real-world applications such as fake news detection (Shu et al., 2017), document classification (Yang et al., 2016), and spoken language understanding (SLU) (Gupta et al., 2019a; Gupta et al., 2019b; Zhang et al., 2018), a core task of conversational assistants like Amazon Alexa or Google Assistant. However, there are still insufficient characteristic metrics to describe a collection of texts. Unlike numeric or categorical data, simple descriptive statistics alone such as word counts and vocabulary size are difficult to capture the syntactic and semantic properties of a text collection. In this work, we propose a set of characteristic metrics: diversity, density, and homogeneity to quantitatively summarize"
2020.lrec-1.215,P19-1519,0,0.02281,"Missing"
2020.nlp4convai-1.12,N18-2118,0,0.0613971,"e models, ELMo and BERT in our case, are complementary to these few-shot learning methods and yield further gains. 1 Slot Label O O O AddToPlaylist:artist AddToPlaylist:artist O AddToPlaylist:playlist owner O AddToPlaylist:playlist AddToPlaylist:playlist AddToPlaylist:playlist AddToPlaylist:playlist Figure 1: Tokens and corresponding slot labels for an utterance from the AddToPlaylist intent class in the S NIPS dataset prefixed by intent class name. As of late, most state-of-the-art IC/SF models are based on feed-forward, convolutional, or recurrent neural networks (Hakkani-T¨ur et al., 2016; Goo et al., 2018; Gupta et al., 2019). These neural models offer substantial gains in performance, but they often require a large number of labeled examples (on the order of hundreds) per intent class and slot-label to achieve these gains. The relative scarcity of large-scale datasets annotated with intents and slots prohibits the use of neural IC/SF models in many promising domains, such as medical consultation, where it is difficult to obtain large quantities of annotated dialogues. Accordingly, we propose the task of few-shot IC/SF, catering to domain adaptation in low resource scenarios, where there are o"
2020.nlp4convai-1.12,D14-1162,0,0.0839494,"ork Architecture We evaluate the network architectures depicted in Figure 2. These networks consist of an embedding layer, a sequence encoder, and two output layers for slots and intents, respectively. We greedily predict the slot-label for each token in the input sequence, according to the maximum output logit at each position. We plan to explore alternate search algorithms (e.g., beam search) in future work. Each architecture uses a different pre-trained embedding layer type, which are either non-contextual or contextual. We experiment with one noncontextual embedding, G LOV E word vectors (Pennington et al., 2014), as well as two contextual embeddings, G LOV E concatenated with ELM O embeddings (Peters et al., 2018), and BERT embeddings (Devlin et al., 2018). The sequence encoder is a bi-directional LSTM (Hochreiter and Schmidhuber, 1997) with a 512-dimensional hidden state. Output layers are fully connected and take concatenated forward and backward LSTM hidden states as input. Pre-trained embeddings are kept frozen for training and adaptation. Attempts to fine-tune BERT led to inferior results. We refer to each architecture by its embedding type, namely G LOV E, ELM O, or BERT. Baseline We compare th"
2020.nlp4convai-1.12,H90-1021,0,\N,Missing
2020.nlp4convai-1.12,W18-2501,0,\N,Missing
2020.nlp4convai-1.12,D18-1300,0,\N,Missing
2020.nlp4convai-1.12,D18-1514,0,\N,Missing
2020.nlp4convai-1.12,N18-3018,0,\N,Missing
2021.acl-long.458,N18-1022,0,0.026421,"icle that should be recommended. However, the meaning of the “reference” is different in these two problems. When recommending citations for a paper, the system is to look for previous works that are related to the arguments in the given paper. The argument was created by the author, and the criteria of the recommendation is the relatedness. While inferring provenance is to do reverse engineering to the given article, so that we can find the articles whose information or claims were actually used when the author was writing. Technically, there are two types of citation recommendation systems (Bhagavatula et al., 2018). One is called local (Huang et al., 2012, 2015), that is, a system takes a few sentences (and an optional placeholder for the candidate citation) as input and recommends citations based on the context of the input sentences. Another one is called global (Kataria et al., 2010; Ren et al., 2014; Bhagavatula et al., 2018), that is, a system takes the entire article (and its meta-data which is optional) as input and recommends cita5901 tions for the paper. Our solution is more related to local recommendation systems, while we do not assume we can access all of the articles that can be cited and h"
2021.acl-long.458,D13-1184,1,0.803314,"s the sources, the titles of the articles are also very likely to be related. In the same example, some of them are all talking about the interviews done by Anthony Fauci at different time, and some of them are talking about the white house’s Coronavirus Task Force in Press Briefing. Therefore, we propose an algorithmic inference framework that can take advantage of those relations between the source articles to determine the correct source articles of identified sentences jointly. 4.3.2 ILP-based Inference We formulate the inference as an Integer Linear Program (ILP) (Roth and tau Yih, 2004; Cheng and Roth, 2013), that allows us to jointly determine the best candidate for each identified sentence. Formally, we introduce two types of Boolean variables: xki , which represents if the k th candidate kl , is the source article of the ith sentence, and zij which represents if the source article of the ith sentence and the source article of the j th sentence are related, which means either they come from related source websites or provide related content. To infer the value of the Boolean variables, our 5898 objective is to assign the best candidate to each identified sentence that can (1) maximize the overa"
2021.acl-long.458,N19-1423,0,0.00572243,"oal to find the best assignments Γd of candidates for the identified sentences is as follows: Γd = argmaxΓ XX i s.t. k ωik xki + XX i,j kl kl  kl τij + γij zij (2) k,l kl xki ∈ {0, 1}, zij ∈ {0, 1} X ∀i, xki = 1 (3) k kl 2zij ≤ xki + xlj P Here, k xki = 1 means only one candidate will finally be chosen as the source article of the ith kl ≤ xk + xl means only if the k th sentence, and 2zij i j candidate of the ith sentence and the lth candidate of the j th sentence have been chosen, we need to consider the relations between them. In our experiments, we use the last hidden layer of BERT-large (Devlin et al., 2019) as the representation for titles and source domains, and use cosine similarity to compute the similarity score. The ILP problem is solved using an off-the-shelf high-performance package 3 . 5 RQ4 Given the identified sentences, can we use the query we generated to find candidates, and successfully use them to improve the inference of source articles? Among those questions, RQ1-RQ3 are to evaluate a specific component of our solution, and RQ4 is to evaluate the joint performance of candidate generation and source article inference. In the following part, we will elaborate the answers to those"
2021.acl-long.458,2020.acl-main.761,0,0.0211339,"ing articles from www.politifa ct.com; our experimental results show that our solution leads to a significant improvement over baselines. 1 Figure 1: An example of a claim (in the red box) with its article. Sentence 1 and sentence 2 (blue boxes) show examples from the article. Each sentence refers to external information: source article 1 and 2, respectively, with accompanying urls. Introduction Misinformation is on the rise, and people are fighting it with fact checking. However, most of the work in the current literature (Thorne et al., 2018; Zhang et al., 2019; Barr´on-Cedeno et al., 2020; Hidey et al., 2020) focuses on automating factchecking for a single claim. In reality, a claim can be complex, and proposed as a conclusion of an article. Therefore, understanding what information supports the article, especially information 1 The data and the code will be available at http://co gcomp.org/page/publication view/944 that was not originated within the same article, and where it originates from, are very important for readers who want to determine whether they can believe the claim. Figure 1 shows an example of such a claim, “Marco Rubio says Anthony Fauci lies about masks. Fauci didn’t.”2 with its"
2021.acl-long.458,2021.ccl-1.108,0,0.0568591,"Missing"
2021.acl-long.458,N19-4014,0,0.0176045,"d by our query generator are helpful. When k = 5, the mean recall can achieve around 0.21, which is much better than 0.15, the best performance achieved by searching the identified sentence directly. However, as what we can observe in the figure, there is still a gap to the performance of MS-UB in Figure 6. This may result from the insufficiency of the query generation, which implies Fact-checking Fact-checking is related to our problem, since there is usually a document retrieval step to find articles that may provide evidence in most of the solutions (Wang et al., 2018; Thorne et al., 2018; Nadeem et al., 2019). Typically, the input of fact-checking is a single claim instead of an article, therefore it is hard to directly extend their solutions to our problem. Even though fact-checking may find various evidentiary articles for the claim, the source articles we are looking for are those that have been used by the author, which is actually a specific subset of the articles that fact-checking targets to, and the size is also much smaller. Furthermore, we try to extract the metadata of the source articles from the text to support a better search, which is not considered in the document retrieval step of"
2021.acl-long.458,W04-2401,1,0.436364,"Missing"
2021.acl-long.458,N18-1074,0,0.0571488,"Missing"
2021.acl-long.458,P19-1040,1,0.751774,"tion dataset 1 , Politi-Prov, based on fact-checking articles from www.politifa ct.com; our experimental results show that our solution leads to a significant improvement over baselines. 1 Figure 1: An example of a claim (in the red box) with its article. Sentence 1 and sentence 2 (blue boxes) show examples from the article. Each sentence refers to external information: source article 1 and 2, respectively, with accompanying urls. Introduction Misinformation is on the rise, and people are fighting it with fact checking. However, most of the work in the current literature (Thorne et al., 2018; Zhang et al., 2019; Barr´on-Cedeno et al., 2020; Hidey et al., 2020) focuses on automating factchecking for a single claim. In reality, a claim can be complex, and proposed as a conclusion of an article. Therefore, understanding what information supports the article, especially information 1 The data and the code will be available at http://co gcomp.org/page/publication view/944 that was not originated within the same article, and where it originates from, are very important for readers who want to determine whether they can believe the claim. Figure 1 shows an example of such a claim, “Marco Rubio says Anthony"
2021.acl-long.458,2020.acl-main.406,1,0.90662,"utational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5894–5903 August 1–6, 2021. ©2021 Association for Computational Linguistics used when they were writing. Furthermore, the problem we address is critical also to authors who want to give credit to those who have contributed to their article, and it enables a recursive analysis that can trace back to the starting points of an article. This motivates the study of provenance for natural language claims, which describes where a specific claim may have come from and how it has spread. Early work (Zhang et al., 2020) proposed a formulation to model, and a solution to infer, the provenance graph for the given claim. However, that model is insufficient to capture the provenance of an article, because (1) an article consists of multiple claims, and it leverages information from other sources, therefore the provenance of all claims should be included in the article’s provenance; (2) the inference solution they proposed can only extract domain-level provenance information, e.g., cbsnews.com, while it can not directly link the claim to its source article, e.g., https://www.cbsnews.com/news/preventingcoronavirus"
2021.acl-long.458,2020.acl-main.703,0,0.0149295,"the figure) is “... On March 29, President ... ”. The source domain of the article it refers to (source article 2 in the figure) is white house, the title of the article is coronavirus task force press briefing, and the published date is March 29, 2020. It is obvious that most of those information has been somehow mentioned in the context or at least can be very easily associated with. Therefore, we treat this problem as a text generation problem, where we feed the identified sentence with its context, and try to generate its metadata. As a baseline, we train this model via fine-tuning BART (Lewis et al., 2020), a pretrained text generation model. 4.2.2 Integrating Search Engine Signals Besides the metadata to generate, the content of the identified sentence itself should be useful for searching, when there is an overlap between the sentence and the content of the target article. In this case, if we search for the identified sentence on a search engine, the results returned can be related articles, and their metadata may provide additional useful information that can tell the model what should be included in the target output. In our running example mentioned in the last section, if we search that s"
2021.acl-long.515,2020.emnlp-main.233,0,0.0403574,"ransfer Learning, Lifelong Learning and Concept Drifting Pre-training a model on large corpora and finetuning on downstream tasks has emerged as a standard paradigm in NLP (Devlin et al., 2019; Lan et al., 2020; CONNEAU and Lample, 2019; Raffel et al., 2020; Brown et al., 2020; Clark et al., 2020). Our work follows this transfer learning paradigm but our main focus is to investigate the regression phenomenon when updating backbone pre-trained models. Another related stream of research is lifelong learning (Lopez-Paz and Ranzato, 2017; Yoon et al., 2018; Delange et al., 2021; Sun et al., 2019; Chuang et al., 2020), incremental learning (Rebuffi et al., 2017; Chaudhry et al., 2018; Prabhu et al., 2020), or concept drifting (Schlimmer and Granger, 1986; Tsymbal, 2004; Klinkenberg, 2005; Žliobait˙e I., 2016) which aims to accumulate knowledge learned either in previous tasks or from data with changing distribution. The model update regression problem differs in that models are trained on the same task and dataset, but we update from one model to another. 6.3 Behavioral Testing of NLP Models To analyze whether a fine-tuned model can handle linguistic phenomena for a specific end task, perturbation techniqu"
2021.acl-long.515,2020.findings-emnlp.372,0,0.0531802,"Missing"
2021.acl-long.515,D13-1170,0,0.0115558,"Missing"
2021.acl-long.515,2020.tacl-1.48,0,0.0195538,"g (Schlimmer and Granger, 1986; Tsymbal, 2004; Klinkenberg, 2005; Žliobait˙e I., 2016) which aims to accumulate knowledge learned either in previous tasks or from data with changing distribution. The model update regression problem differs in that models are trained on the same task and dataset, but we update from one model to another. 6.3 Behavioral Testing of NLP Models To analyze whether a fine-tuned model can handle linguistic phenomena for a specific end task, perturbation techniques are often used (Belinkov and Bisk, 2018; Ribeiro et al., 2018; Prabhakaran et al., 2019; Wu et al., 2019; Talmor et al., 2020). In particular, C HECK L IST (Ribeiro et al., 2020) leverages and expands those techniques to efficiently evaluate a wide range of linguistic behavioral capabilities of NLP models. Our work applies C HECK L IST to inspect where the model update regressions come from and on which linguistic phenomena our proposed solutions help to reduce regressions. 7 Conclusion In this work, we investigated the regression in NLP model updates on classification tasks and show that it has a prevalent presence across tasks and models. We formulated the regression-free model update problem as a constrained optim"
2021.acl-long.515,W18-5446,0,0.0578331,"Missing"
2021.acl-long.515,Q19-1040,0,0.0416638,"Missing"
2021.acl-long.515,N18-1101,0,0.0342265,"Missing"
2021.acl-long.515,P19-1073,0,0.0277959,"r concept drifting (Schlimmer and Granger, 1986; Tsymbal, 2004; Klinkenberg, 2005; Žliobait˙e I., 2016) which aims to accumulate knowledge learned either in previous tasks or from data with changing distribution. The model update regression problem differs in that models are trained on the same task and dataset, but we update from one model to another. 6.3 Behavioral Testing of NLP Models To analyze whether a fine-tuned model can handle linguistic phenomena for a specific end task, perturbation techniques are often used (Belinkov and Bisk, 2018; Ribeiro et al., 2018; Prabhakaran et al., 2019; Wu et al., 2019; Talmor et al., 2020). In particular, C HECK L IST (Ribeiro et al., 2020) leverages and expands those techniques to efficiently evaluate a wide range of linguistic behavioral capabilities of NLP models. Our work applies C HECK L IST to inspect where the model update regressions come from and on which linguistic phenomena our proposed solutions help to reduce regressions. 7 Conclusion In this work, we investigated the regression in NLP model updates on classification tasks and show that it has a prevalent presence across tasks and models. We formulated the regression-free model update problem"
2021.findings-emnlp.316,2020.nlp4convai-1.5,0,0.0235164,"Missing"
2021.findings-emnlp.316,N19-1423,0,0.011949,"Missing"
2021.findings-emnlp.316,N16-1061,0,0.0109807,"al classifier should clas- we have only learned features for “it is red"" (for sify incoming data to the correct existing classes cherries) and “it is yellow"" (for bananas) for a fruit that appeared in training and detect those examples classification task. The problem we are trying to that do not belong to any existing classes. Such overcome manifests when the model is exposed to classifier is thus described as open set recogni- a blueberry during testing. Since it has not seen the tion (Scheirer et al., 2013) or open world classifi- class during training, it does not possess a proper cation (Fei and Liu, 2016). method to extract features for “blue”. Ideally, we The existing research to achieve this capability want a representation learning approach that can in natural language processing (NLP) and computer compute such a representation instead of using the vision (CV) mainly focuses on decision boundary representation of “red” or “yellow”. The straightfinding. Schölkopf et al. (2001); Tax and Duin forward solution is to explore some examples with (2004); Fei et al. (2016) use SVM to detect the “blue” during model training, although a blueberry negative classified examples. Scheirer et al. (2013) do"
2021.findings-emnlp.316,D19-1131,0,0.0414741,"Missing"
2021.findings-emnlp.316,2020.acl-main.703,0,0.0107336,"ber 7–11, 2021. ©2021 Association for Computational Linguistics ple distributionally shifted data creation method for NLP. And then, we train a classifier on in-domain training examples and distributionally shifted examples. Such a classifier can work with existing decision boundary finding methods for further open space risk reduction. Related Works: Besides the works(Shu et al., 2018; Xu et al., 2019) in open-world learning, our work is also related to data augmentation. In CV, Chen et al. (2020) propose a simple image pretraining method based on data augmentation. In NLP, Wu et al. (2020); Lewis et al. (2020a) pretrain language model by contrastive learning on augmented data. Wu et al. (2020) propose word/span deletion, word reorder, word replacement and Lewis et al. (2020a) use paraphrasing method to augment examples. Differently from what we explore in this paper, these works focus on similar instances instead of OOD examples. In this work, we take advantage of the recent success of pretrained language models. We use the sequence-to-sequence language model BART (Lewis et al., 2020b) for distributionally shifted example creation. BART can fill the masked sentences by generation. Furthermore, we"
2021.findings-emnlp.316,W15-1509,0,0.0767386,"Missing"
2021.findings-emnlp.316,D17-1314,1,0.839416,"d method can create out-ofdomain instances from the in-domain training examples with the help of a pre-trained language model. Experimental results show that ODIST performs better than state-of-the-art decision boundary finding method. 1 Introduction train a multi-class classifier and take the outputs of the penultimate layer to fit Weibull distribution. Hendrycks and Gimpel (2017) reject the low confidence samples with the threshold based on the probability of softmax distribution. Liang et al. (2018) add a temperature scaling on the softmax function to get a calibrated softmax score.In NLP, Shu et al. (2017) adopt the sigmoid function to learn the one-vs-all classifier and calculate the confidence threshold by fitting training data to Gaussian statistics. Zhang et al. (2021) propose to learn the adaptive decision boundary (ADB). ADB performs best among all the above methods on the open text classification. In the supervised learning setting, it is generally assumed that test set data points will be organized along the same classes observed during training. Besides adjusting the decision boundary on the This assumption, however, proves unreliable in feature space learned from in-domain data, a goo"
2021.findings-emnlp.316,N18-1101,0,0.0103917,"rastive learning on augmented data. Wu et al. (2020) propose word/span deletion, word reorder, word replacement and Lewis et al. (2020a) use paraphrasing method to augment examples. Differently from what we explore in this paper, these works focus on similar instances instead of OOD examples. In this work, we take advantage of the recent success of pretrained language models. We use the sequence-to-sequence language model BART (Lewis et al., 2020b) for distributionally shifted example creation. BART can fill the masked sentences by generation. Furthermore, we use the finetuned BART 1 on MNLI (Williams et al., 2018) for predicting the relationship between the original text and augmented examples for filtering. 2 Methodology Problem Definition: We define a training data set as D = {(x1 , y1 ), (x2 , y2 ), . . . , (xn , yn )} composed of n examples where the i-th document xi is associated with one of the m seen classes yi ∈ {l1 , l2 , . . . , lm }. In the canonical open-world classification setting, a model learns from the training data and either classifies the test instance to one of the m seen classes or reject it as unseen (denoted by l0 ), i.e., it does not belong to any of the seen classes. Therefore"
2021.findings-emnlp.329,2020.acl-main.421,0,0.0304306,"(Pfeiffer et al., 2020b) for XQuAD which leverages language and task adapters for efficient cross lin9 Model mBERT XLM AMBER mBERT† L2† MAD-XmBERT † WordOT (Ours) Evaluation We evaluate our proposed method for two tasks provided by XTREME benchmarks (Hu et al., 2020): XNLI for textual entailment where the task is to classify the entailment relationship between a given pair of sentences into entailment/neutral/contradiction (Conneau et al., 2018; Williams et al., 2018); XQuAD for question answering where the task is to identify the answer to a question as a span in the corresponding paragraph (Artetxe et al., 2020; Rajpurkar et al., 2016).10 These tasks evaluate zero shot transferability and hence we train all tasks using English labelled data with cross-entropy loss and test on the target languages. More details about the task settings can be found in Appendix B. To measure the improvements, we use F1 score for textual entailment; F1 and EM (Exact Match) scores for question answering which reflect the partial and exact matches between the prediction and ground truth, respectively. 5.4 gual transfer.12 We also compare how our model performs with respect to current state-of-the-art model i.e. XLMR (Conn"
2021.findings-emnlp.329,Q19-1038,0,0.0241294,"al., 2019; Grave et al., 2018). For instance, Zhang et al. (2017) use OT to induce translations for a source word from the target language in bilingual lexicon induction whereas Xu et al. (2018) use OT and back-translation losses to align traditional monolingual word embeddings that do not leverage context (i.e. word type level). 3 Related Work level, word level, or both which allows simultaneous learning from different languages, as opposed to rotation based approaches. Studies that depend on sentence level alignment achieve significantly high performance on bitext sentence retrieval tasks (Artetxe and Schwenk, 2019; Zweigenbaum et al., 2017), and by design they are not applicable to word based applications. For instance, LASER (Artetxe and Schwenk, 2019) learns massively multi-lingual encoder using a huge parallel corpus whereas Feng et al. (2020) trains a bi-directional dual encoder with an additive softmax margin loss to perform translation ranking among in-batch examples. Similar to this line of work, we rely on only parallel sentences as external sources to fine-tune the model, but we define word alignment objective instead. Other studies use word alignment objective to align parallel word pairs and"
2021.findings-emnlp.329,2021.acl-long.265,0,0.143757,"d aligners which are often sub-optimal to generate the parallel word pairs (e.g. FastAlign (Dyer et al., 2013b) or optimal transport (Grave et al., 2018)) and use these pairs as weak form of supervision. Our work is most similar to these methods in that we use word level alignment objective; however, we learn the aligned word pairs implicitly during optimization rather than obtaining them beforehand using external aligners and applying heuristics to keep only one-to-one mappings. Alignment as a post-processing technique on distributional embedding spaces provides an effective soMore recently, Chi et al. (2021) developed an lution to improve cross-lingual downstream appli- end-to-end model that first aligns both source and cations. For non-contextualized embeddings, align- target words with OT and then use the alignments ment based techniques for word embeddings have as self-labels to fine-tune the contextualized LM. been thoroughly surveyed in Ruder et al. (2019). They use three objective functions for fine-tuning: For contextualized embeddings, one direction of Masked Language Modeling (MLM) (Devlin et al., efforts to improve cross-lingual word representa- 2019), Translation Language Modeling (TLM"
2021.findings-emnlp.329,2020.acl-main.747,0,0.243825,"rformance in different NLP source languages (Aldarmaki et al., 2018; Schuster tasks (Peters et al., 2018; Howard and Ruder, 2018; et al., 2019; Wang et al., 2019; Cao et al., 2020). Devlin et al., 2019). Similar advancements have The objective is to align source and target language been made for languages other than English using representations into the same embedding space, models that learn cross-lingual word representa- for instance by encouraging similar words to be tions leveraging monolingual and/or parallel data closer to each other (e.g. cat in English and Katze (Devlin et al., 2019; Conneau et al., 2020; Artetxe in German) with least cost in terms of data and et al., 2020). Such cross-lingual ability helps in computational resources. Such methods require mitigating the lack of abundant data (labelled or some form of cross-lingual signal, such as alignunlabelled) and computational resources for lan- ment in non-contextualized embeddings, mainly guages other than English, with lesser cost. Yet, utilize bilingual/multilingual lexicon that have been there exists a challenge for improving multilingual learned with unsupervised or supervised techniques representations and cross-lingual transfer le"
2021.findings-emnlp.329,D18-1269,0,0.0259594,"uses L2 based alignment objective, 3. AMBER (Hu et al., 2021a) for XNLI which uses a combination of MLM, TLM, word alignment and sentence alignment objectives,11 4. MAD-X (Pfeiffer et al., 2020b) for XQuAD which leverages language and task adapters for efficient cross lin9 Model mBERT XLM AMBER mBERT† L2† MAD-XmBERT † WordOT (Ours) Evaluation We evaluate our proposed method for two tasks provided by XTREME benchmarks (Hu et al., 2020): XNLI for textual entailment where the task is to classify the entailment relationship between a given pair of sentences into entailment/neutral/contradiction (Conneau et al., 2018; Williams et al., 2018); XQuAD for question answering where the task is to identify the answer to a question as a span in the corresponding paragraph (Artetxe et al., 2020; Rajpurkar et al., 2016).10 These tasks evaluate zero shot transferability and hence we train all tasks using English labelled data with cross-entropy loss and test on the target languages. More details about the task settings can be found in Appendix B. To measure the improvements, we use F1 score for textual entailment; F1 and EM (Exact Match) scores for question answering which reflect the partial and exact matches betwe"
2021.findings-emnlp.329,N19-1423,0,0.0110586,"anguage modeling (Lample and Conneau, 2019), integrating language and task adapters (Pfeiffer et al., 2020b), and applying alignment techniques in the embedding spaces (Cao et al., 2020; Wu and Dredze, 2020). Previous studies concerning alignment in the embedding space show promising directions to Contextualized word embeddings have advanced improve cross-lingual transfer abilities for low rethe state-of-the-art performance in different NLP source languages (Aldarmaki et al., 2018; Schuster tasks (Peters et al., 2018; Howard and Ruder, 2018; et al., 2019; Wang et al., 2019; Cao et al., 2020). Devlin et al., 2019). Similar advancements have The objective is to align source and target language been made for languages other than English using representations into the same embedding space, models that learn cross-lingual word representa- for instance by encouraging similar words to be tions leveraging monolingual and/or parallel data closer to each other (e.g. cat in English and Katze (Devlin et al., 2019; Conneau et al., 2020; Artetxe in German) with least cost in terms of data and et al., 2020). Such cross-lingual ability helps in computational resources. Such methods require mitigating the lack of abun"
2021.findings-emnlp.329,2021.eacl-main.181,0,0.032334,"ingual embedding space in another corresponding alignments obtained from OT. Simi(Wang et al., 2019; Schuster et al., 2019). How- lar to their work, we use OT based signals to fineever, this generates a transformation matrix for tune the contextualized LM, but we instead use each language pair which can be inconvenient to the average cost of OT alignments for fine-tuning. apply in downstream tasks. Another direction is There are other studies that attempt to combine to use explicit alignment objective at the sentence various objectives for learning cross-lingual super3906 vision. For example, Dou and Neubig (2021); Hu et al. (2021b) incorporate the following objectives on cross-lingual data: MLM, TLM, sentence level alignment (e.g. parallel sentence identification objective), and word level alignment. In this paper, we do not investigate combined objective functions similar to these works. We believe that adding more objectives can further boost the performance and we leave it for future work. 4 Method Figure 1 shows the overall fine-tuning process. As input, we require parallel sentences (i.e. pairs of aligned sentences in source and target languages) and contextualized multilingual LM. We use English"
2021.findings-emnlp.329,N13-1073,0,0.309425,"s to fine-tune the model, but we define word alignment objective instead. Other studies use word alignment objective to align parallel word pairs and fine-tune the contextualized multi-lingual LM (Cao et al., 2020; Wu and Dredze, 2020; Nagata et al., 2020). Cao et al. (2020) use regularized L2 based alignment objective to align parallel word pairs. Wu and Dredze (2020) use contrastive learning to align parallel word pairs relative to negative pairs in the batch. These approaches rely on unsupervised word aligners which are often sub-optimal to generate the parallel word pairs (e.g. FastAlign (Dyer et al., 2013b) or optimal transport (Grave et al., 2018)) and use these pairs as weak form of supervision. Our work is most similar to these methods in that we use word level alignment objective; however, we learn the aligned word pairs implicitly during optimization rather than obtaining them beforehand using external aligners and applying heuristics to keep only one-to-one mappings. Alignment as a post-processing technique on distributional embedding spaces provides an effective soMore recently, Chi et al. (2021) developed an lution to improve cross-lingual downstream appli- end-to-end model that first"
2021.findings-emnlp.329,eisele-chen-2010-multiun,0,0.0502341,"t) represents the initial representation with the un-tuned contextualized language model. We then back-propagate the resultant regularized loss (as shown in Equation (2)), summed over all K P K languages, i.e., L(c) = l(c; P i ) to fine-tune i=1 the contextualized word representations. 5 5.1 Experimental Setup Data Pre-processing Following previous studies (Lample and Conneau, 2019; Cao et al., 2020), we use parallel data (approximately 32M sentence pairs) from a variety of corpora to cover different language pairs and domains as shown in Appendix A - Europarl corpora (Koehn, 2005), MultiUN (Eisele and Chen, 2010), IIT Bombay (Kunchukuttan et al., 2018), Tanzil and GlobalVoices (Tiedemann, 2012), and OpenSubtitles (Lison and Tiedemann, 2016). In all cases, we use English (en) as the target language and the tokenizer in Koehn et al. (2007). We use 250K sentences for training, upsampling from language pairs where this much data is not available. We shuffled the data to break their chronological order if any. For our main model, we consider the following five languages: Bulgarian (bg), German (de), Greek (el), Spanish (es), and French (fr), similar to Cao et al. (2020). For our larger model, we additional"
2021.findings-emnlp.329,2005.mtsummit-papers.11,0,0.187917,"ng tuned whereas c0 (j, t) represents the initial representation with the un-tuned contextualized language model. We then back-propagate the resultant regularized loss (as shown in Equation (2)), summed over all K P K languages, i.e., L(c) = l(c; P i ) to fine-tune i=1 the contextualized word representations. 5 5.1 Experimental Setup Data Pre-processing Following previous studies (Lample and Conneau, 2019; Cao et al., 2020), we use parallel data (approximately 32M sentence pairs) from a variety of corpora to cover different language pairs and domains as shown in Appendix A - Europarl corpora (Koehn, 2005), MultiUN (Eisele and Chen, 2010), IIT Bombay (Kunchukuttan et al., 2018), Tanzil and GlobalVoices (Tiedemann, 2012), and OpenSubtitles (Lison and Tiedemann, 2016). In all cases, we use English (en) as the target language and the tokenizer in Koehn et al. (2007). We use 250K sentences for training, upsampling from language pairs where this much data is not available. We shuffled the data to break their chronological order if any. For our main model, we consider the following five languages: Bulgarian (bg), German (de), Greek (el), Spanish (es), and French (fr), similar to Cao et al. (2020). Fo"
2021.findings-emnlp.329,L16-1147,0,0.0314212,"t regularized loss (as shown in Equation (2)), summed over all K P K languages, i.e., L(c) = l(c; P i ) to fine-tune i=1 the contextualized word representations. 5 5.1 Experimental Setup Data Pre-processing Following previous studies (Lample and Conneau, 2019; Cao et al., 2020), we use parallel data (approximately 32M sentence pairs) from a variety of corpora to cover different language pairs and domains as shown in Appendix A - Europarl corpora (Koehn, 2005), MultiUN (Eisele and Chen, 2010), IIT Bombay (Kunchukuttan et al., 2018), Tanzil and GlobalVoices (Tiedemann, 2012), and OpenSubtitles (Lison and Tiedemann, 2016). In all cases, we use English (en) as the target language and the tokenizer in Koehn et al. (2007). We use 250K sentences for training, upsampling from language pairs where this much data is not available. We shuffled the data to break their chronological order if any. For our main model, we consider the following five languages: Bulgarian (bg), German (de), Greek (el), Spanish (es), and French (fr), similar to Cao et al. (2020). For our larger model, we additionally used the following languages: Russian (ru), Arabic (ar), Mandarin (zh), Hindi (hi), Thai (th), Turkish (tr), Urdu (ur), Swahili"
2021.findings-emnlp.329,N19-1162,0,0.0163059,"lized LM. been thoroughly surveyed in Ruder et al. (2019). They use three objective functions for fine-tuning: For contextualized embeddings, one direction of Masked Language Modeling (MLM) (Devlin et al., efforts to improve cross-lingual word representa- 2019), Translation Language Modeling (TLM) tions is to use the Procrustes objective to project (Lample and Conneau, 2019), and the cross enthe monolingual embeddings from one language tropy between predicted masked words and their to the monolingual embedding space in another corresponding alignments obtained from OT. Simi(Wang et al., 2019; Schuster et al., 2019). How- lar to their work, we use OT based signals to fineever, this generates a transformation matrix for tune the contextualized LM, but we instead use each language pair which can be inconvenient to the average cost of OT alignments for fine-tuning. apply in downstream tasks. Another direction is There are other studies that attempt to combine to use explicit alignment objective at the sentence various objectives for learning cross-lingual super3906 vision. For example, Dou and Neubig (2021); Hu et al. (2021b) incorporate the following objectives on cross-lingual data: MLM, TLM, sentence lev"
2021.findings-emnlp.329,D19-1575,0,0.286964,"al transport (OT) as the loss value. language modeling (Lample and Conneau, 2019), integrating language and task adapters (Pfeiffer et al., 2020b), and applying alignment techniques in the embedding spaces (Cao et al., 2020; Wu and Dredze, 2020). Previous studies concerning alignment in the embedding space show promising directions to Contextualized word embeddings have advanced improve cross-lingual transfer abilities for low rethe state-of-the-art performance in different NLP source languages (Aldarmaki et al., 2018; Schuster tasks (Peters et al., 2018; Howard and Ruder, 2018; et al., 2019; Wang et al., 2019; Cao et al., 2020). Devlin et al., 2019). Similar advancements have The objective is to align source and target language been made for languages other than English using representations into the same embedding space, models that learn cross-lingual word representa- for instance by encouraging similar words to be tions leveraging monolingual and/or parallel data closer to each other (e.g. cat in English and Katze (Devlin et al., 2019; Conneau et al., 2020; Artetxe in German) with least cost in terms of data and et al., 2020). Such cross-lingual ability helps in computational resources. Such me"
2021.naacl-main.162,2020.emnlp-main.242,0,0.0917603,"rt early exit methods. 2 Related Work Large-scale pre-trained language models (Devlin et al., 2019; Liu et al., 2019) based on the Transformer (Vaswani et al., 2017) architecture demonstrate superior performance in various NLP tasks. However, the impressive performance is on the basis of massive parameters, leading to large memory requirement and computational cost during inference. To overcome this bottleneck, increasing studies work on improving the efficiency of overparameterized pre-trained language models. Knowledge distillation (Hinton et al., 2015; Turc et al., 2019; Jiao et al., 2019; Li et al., 2020a) compacts the model architecture to obtain a smaller model that remains static for all instances at the inference stage. Sanh et al. (2019) focus on reducing the number of layers since their investigation reveals variations on hidden size dimension have a smaller impact on computation efficiency. Sun et al. (2019) learn from multiple intermediate layers of the teacher model for incremental knowledge extraction instead of only learning from the last hidden representations. Further, Wang et al. (2020) design elaborate techniques to drive the student model • We propose a set of global strategie"
2021.naacl-main.162,2021.findings-emnlp.43,1,0.866096,"Missing"
2021.naacl-main.162,N19-1423,0,0.66365,"framework. They inevitably lose valusive experiments demonstrate that our method able features that are captured by passed layers outperforms previous early exit methods by a but are ignored for prediction, leading to less relilarge margin, yielding better and robust perforable prediction results. Moreover, these methods mance1 . abandon the potentially useful features captured by 1 Introduction the future layers that have not been passed, which may hurt the performance of the instances requirPre-trained language models (PLMs), e.g., ing high-level features embedded in the deep layers. BERT (Devlin et al., 2019), RoBERTa (Liu Consequently, their performance dramatically deet al., 2019) and XLNet (Yang et al., 2019), have clines when the inference exits earlier for a higher obtained remarkable success in a wide range of speed-up ratio. NLP tasks. Despite their impressive performance, These two major drawbacks hinder the progress PLMs are usually associated with large memory of early exit research and motivate us to develop requirement and high computational cost. Such a new mechanism using the hierarchical linguisdrawbacks slow down the inference and further tic information embedded in all layers (Jaw"
2021.naacl-main.162,P19-1356,0,0.0221997,"rence. sp = G(s1:i ) (1) where G(·) refers to one of the state incorporation strategies. 3.2 Imitation of Future States Existing work for early exit stops inference at an intermediate layer and ignores the underlying valuable features captured by the future layers. 3.1 Incorporation of Past States Such treatment is partly rationalized by the recent Existing work (Xin et al., 2020) focuses on making claim (Kaya et al., 2019) that shallow layers are exit decision based on a single branch classifier. adequate to make a correct prediction. However, The consequent unreliable result motivates the reJawahar et al. (2019) reveal that the pre-trained cent advance (Zhou et al., 2020) that uses conseculanguage models capture a hierarchy of linguistive states to improve the accuracy and robustness. tic information from the lower to the upper layers, However, the model prediction is still limited to e.g., the lower layers learn the surface or syntactic use several local states. In contrast, we investigate features while the upper layers capture high-level how to incorporate all the past states from a global information like the semantic features. We hypothperspective. The existing strategy using consecuesize that s"
2021.naacl-main.162,2021.ccl-1.108,0,0.0612049,"Missing"
2021.naacl-main.162,2020.acl-main.593,0,0.0192238,"they have to distill a model from scratch global predictions. to meet the varying speed-up ratio requirements. 2014 To meet different constraints for acceleration, another line of work studies instance-adaptive methods to adjust the number of executed layers for different instances. Li et al. (2020b) select models in different sizes depending on the difficulty of input instance. Besides, early exit is a practical method to adaptively accelerate inference and is first proposed for computer vision tasks (Kaya et al., 2019; Teerapittayanon et al., 2016). Elbayad et al. (2020); Xin et al. (2020); Schwartz et al. (2020) follow the essential idea and leverage the method in NLP tasks. To prevent the error from one single classifier, Zhou et al. (2020) make the model stop inference when a cross-layer consistent prediction is achieved. However, researches on the subject has been mostly restricted to only use the local states around the exit layer. 3 • Attn-Pooling: The attentive-pooling takes the weighted summation of all available states as the integrated state. The attention weights are computed with the last state as the query. • Concatenation: All available states are concatenated and then fed into a linear"
2021.naacl-main.162,D19-1441,0,0.0448329,"Missing"
2021.naacl-main.162,2020.acl-main.204,0,0.224127,"ehensive Moreover, they have to distill a model from scratch global predictions. to meet the varying speed-up ratio requirements. 2014 To meet different constraints for acceleration, another line of work studies instance-adaptive methods to adjust the number of executed layers for different instances. Li et al. (2020b) select models in different sizes depending on the difficulty of input instance. Besides, early exit is a practical method to adaptively accelerate inference and is first proposed for computer vision tasks (Kaya et al., 2019; Teerapittayanon et al., 2016). Elbayad et al. (2020); Xin et al. (2020); Schwartz et al. (2020) follow the essential idea and leverage the method in NLP tasks. To prevent the error from one single classifier, Zhou et al. (2020) make the model stop inference when a cross-layer consistent prediction is achieved. However, researches on the subject has been mostly restricted to only use the local states around the exit layer. 3 • Attn-Pooling: The attentive-pooling takes the weighted summation of all available states as the integrated state. The attention weights are computed with the last state as the query. • Concatenation: All available states are concatenated and"
2021.naacl-main.162,2020.emnlp-main.633,0,0.0122821,"ence stage. Sanh et al. (2019) focus on reducing the number of layers since their investigation reveals variations on hidden size dimension have a smaller impact on computation efficiency. Sun et al. (2019) learn from multiple intermediate layers of the teacher model for incremental knowledge extraction instead of only learning from the last hidden representations. Further, Wang et al. (2020) design elaborate techniques to drive the student model • We propose a set of global strategies which to mimic the self-attention module of teacher modeffectively incorporate all available states and els. Xu et al. (2020) compress model by progresthey achieve better performance compared to sive module replacing, showing a new perspective the existing naive global strategies. of model compression. However, these static model • Our early exit method first utilizes the future compression methods treat the instances requiring states which are originally inaccessible at the different computational cost without distinction. inference stage, enabling more comprehensive Moreover, they have to distill a model from scratch global predictions. to meet the varying speed-up ratio requirements. 2014 To meet different constr"
2021.naacl-main.217,D12-1062,1,0.682723,"has impact beyond intelligent task management. For example, learning to decompose complex natural language expressions could have impact on complex question answering (Chali et al., 2009; Luo et al., 2018), where question decomposition, multi-hop reasoning, information synthesis, and implicit knowledge all play an important role. More generally, the ability to model mappings between short text fragments and elements in multiple documents could benefit research in areas such as topic-focused multidocument summarization (Wan et al., 2007) and event timeline extraction of evolving news stories (Do et al., 2012). as the temporal dependencies between them; and (iii) extending a neural text generator by injecting signals for relevance, abstraction and consensus, thereby making it more capable at tackling task decomposition. 2 Problem Definition We begin by defining some key concepts. We refer to a task as a text fragment that represents a goal people want to track, remind themselves of, or learn how to do; for example, “buy a Christmas present”, “eat healthier” or “change a tire”. In order to disambiguate the intent of tasks (consider the fragment “Harry Potter”, which could equally refer to “read [the"
2021.naacl-main.217,2020.acl-main.703,0,0.0266042,"Missing"
2021.naacl-main.217,D18-1242,0,0.0150561,"ur augmented neural text generator, as well as predict dependency edges between the sub-tasks it generates. In experiments, we demonstrate that our optimal solution – which encodes relevance, abstraction and consensus – yields significant improvements over a state-of-the-art text generator on both subtask generation and dependency prediction. The focus of this paper is on Complex Tasks; however, our research has impact beyond intelligent task management. For example, learning to decompose complex natural language expressions could have impact on complex question answering (Chali et al., 2009; Luo et al., 2018), where question decomposition, multi-hop reasoning, information synthesis, and implicit knowledge all play an important role. More generally, the ability to model mappings between short text fragments and elements in multiple documents could benefit research in areas such as topic-focused multidocument summarization (Wan et al., 2007) and event timeline extraction of evolving news stories (Do et al., 2012). as the temporal dependencies between them; and (iii) extending a neural text generator by injecting signals for relevance, abstraction and consensus, thereby making it more capable at tack"
2021.naacl-main.217,2020.acl-main.767,1,0.773501,"Missing"
2021.naacl-main.62,D18-1547,0,0.252579,"les. We introduce a set of newly designed bench-marking descriptions and reveal the model robustness on both homogeneous and heterogeneous description styles in training and evaluation. 1 Introduction From early frame-driven dialog system GUS (Bobrow et al., 1977) to virtual assistants (Alexa, Siri, and Google Assistant et al.), frame-based dialog state tracking has long been studied to meet various challenges. In particular, how to support an everincreasing number of services and APIs spanning multiple domains has been a focal point in recent years, evidenced by multi-domain dialog modeling (Budzianowski et al., 2018; Byrne et al., 2019; Shah et al., 2018a) and transferable dialog state tracking to unseen intent/slots (Mrkši´c et al., 2017; Wu et al., 2019; Hosseini-Asl et al., 2020). Recently, Rastogi et al. (2019) proposed a new paradigm called schema-guided dialog for transferable dialog state tracking by using natural language description to define a dynamic set of service schemata. As shown in Figure 1, the primary motivation is that these descriptions can offer effective ∗ Work done when Jie Cao was an intern at Amazon Figure 1: An example dialog from Restaurant_1 service, along with its service/int"
2021.naacl-main.62,D19-1459,0,0.0143035,"newly designed bench-marking descriptions and reveal the model robustness on both homogeneous and heterogeneous description styles in training and evaluation. 1 Introduction From early frame-driven dialog system GUS (Bobrow et al., 1977) to virtual assistants (Alexa, Siri, and Google Assistant et al.), frame-based dialog state tracking has long been studied to meet various challenges. In particular, how to support an everincreasing number of services and APIs spanning multiple domains has been a focal point in recent years, evidenced by multi-domain dialog modeling (Budzianowski et al., 2018; Byrne et al., 2019; Shah et al., 2018a) and transferable dialog state tracking to unseen intent/slots (Mrkši´c et al., 2017; Wu et al., 2019; Hosseini-Asl et al., 2020). Recently, Rastogi et al. (2019) proposed a new paradigm called schema-guided dialog for transferable dialog state tracking by using natural language description to define a dynamic set of service schemata. As shown in Figure 1, the primary motivation is that these descriptions can offer effective ∗ Work done when Jie Cao was an intern at Amazon Figure 1: An example dialog from Restaurant_1 service, along with its service/intent/slot description"
2021.naacl-main.62,W14-4340,0,0.0335655,"plified strategy does impact our model performance negatively in comparison to a well-designed dialog state tracking model on seen domains. However, it helps reduce the complexity of matching extra slot descriptions for cross-service carryover. We leave the further discussion for future work. Transferable Dialog State Tracking. Another line of research focuses on how to build a transferable dialog system that is easily scalable to newly added intents and slots. This covers diverse topics including e.g., resolving lexical/morphological variabilities by symbolic de-lexicalization-based methods (Henderson et al., 2014; Williams et al., 2016), neural belief tracking (Mrkši´c et al., 2017), generative dialog state tracking (Peng et al., 2020; Hosseini-Asl et al., 2020), modeling DST as a question answering task (Zhang et al., 2019; Lee et al., 2019; Gao et al., 2020, 2019). Our work is similar with the last class. However, we further investigate whether the DST can benefit from NLP tasks other than question answering. Furthermore, without rich description for the service/intent/slot in the schema, previous works mainly focus on simple format on question answering scenarios, such as domain-slottype compounded"
2021.naacl-main.62,E17-3017,0,0.0568371,"Missing"
2021.naacl-main.62,N18-3006,0,0.183186,"-marking descriptions and reveal the model robustness on both homogeneous and heterogeneous description styles in training and evaluation. 1 Introduction From early frame-driven dialog system GUS (Bobrow et al., 1977) to virtual assistants (Alexa, Siri, and Google Assistant et al.), frame-based dialog state tracking has long been studied to meet various challenges. In particular, how to support an everincreasing number of services and APIs spanning multiple domains has been a focal point in recent years, evidenced by multi-domain dialog modeling (Budzianowski et al., 2018; Byrne et al., 2019; Shah et al., 2018a) and transferable dialog state tracking to unseen intent/slots (Mrkši´c et al., 2017; Wu et al., 2019; Hosseini-Asl et al., 2020). Recently, Rastogi et al. (2019) proposed a new paradigm called schema-guided dialog for transferable dialog state tracking by using natural language description to define a dynamic set of service schemata. As shown in Figure 1, the primary motivation is that these descriptions can offer effective ∗ Work done when Jie Cao was an intern at Amazon Figure 1: An example dialog from Restaurant_1 service, along with its service/intent/slot descriptions and dialog state"
ben-gera-etal-2010-semantic,copestake-flickinger-2000-open,0,\N,Missing
ben-gera-etal-2010-semantic,A00-2018,0,\N,Missing
ben-gera-etal-2010-semantic,J93-2004,0,\N,Missing
ben-gera-etal-2010-semantic,P97-1003,0,\N,Missing
ben-gera-etal-2010-semantic,W07-2207,1,\N,Missing
ben-gera-etal-2010-semantic,C02-2025,0,\N,Missing
ben-gera-etal-2010-semantic,J05-1003,0,\N,Missing
ben-gera-etal-2010-semantic,J01-2004,0,\N,Missing
ben-gera-etal-2010-semantic,P08-1067,0,\N,Missing
ben-gera-etal-2010-semantic,P02-1035,0,\N,Missing
ben-gera-etal-2010-semantic,P95-1037,0,\N,Missing
ben-gera-etal-2010-semantic,I05-1015,0,\N,Missing
ben-gera-etal-2010-semantic,W07-1204,0,\N,Missing
C10-1026,W08-1705,0,0.0305405,"Missing"
C10-1026,J07-4004,0,0.0398607,"e Cluster of Multimodal Computing & Interaction. A second problem with deep parsers is their relatively low efficiency. For online applications, it is impermissible to wait for longer than a minute before the system responds. Apart from studies that were aimed at increasing the efficiency of deep parsers by using smarter algorithms (e.g. using left-corner relations (Van Noord, 1997)), several studies in recent years have suggested that search space restriction can offer a beneficial balance between speed and accuracy as well. Techniques that have been proposed are, among others, supertagging (Clark and Curran, 2007), CFG filtering (Matsuzaki et al., 2007) and beam thresholding (Ninomiya et al., 2005). A potential disadvantage of the latter technique is that the unifications have taken place by the time the value of the resulting chart item is investigated. One strategy that tries to prevent execution of unlikely tasks altogether is presented by van Noord (2009). In this method, the parser learns from an unannotated corpus which parse steps contributed to the solution as preferred by the disambiguation model (as opposed to a certain gold standard). Hence, this approach is selflearning. Another study that"
C10-1026,W09-2605,1,0.8393,"ss is achieved by adding radically overgenerating rules to the grammar, which could cover all sentences, given an disproportionate amount of time and memory. By strongly restricting the search space, however, the computation requirements remains within bounds. Because the robustness rules are strongly dispreferred by both the priority model and the disambiguation model, all sentences that would be covered by the ‘restricted’ grammar remain highprecision, but sentences that are not covered will get an additional push from the robustness rules. 1.1 An HPSG grammar for German The grammar we use (Cramer and Zhang, 2009) is the combination of a hand-written, constraintbased grammar in the framework of HPSG and an open word class lexicon extracted from the Tiger treebank (Brants et al., 2002) in a deep lexical acquisition step. One of the aims of this grammar is to be precision-oriented: it tries to give detailed analyses of the German language, and reject ungrammatical sentences as much as possible. However, this precision comes at the cost of lower coverage, as we will see later in this paper. Along with the grammar, a treebank has been developed by re-parsing the Tiger treebank, and including those sentence"
C10-1026,P99-1061,0,0.0185138,"e two pairs of robustness rules, each pair consisting of a rule with MN-DTR first and RB-DTR second, and one rule in the other order: +V The robust daughter is a verb, which is still allowed to have valence, but cannot have any features in NONLOCAL. +NV The robust daughter is anything but a verb, cannot have any non-empty valence list, and cannot have any features in NONLOCAL. 3.2 Fragment parsing As a baseline for comparison, we investigate the existing partial parsing algorithms that pick fragmented analyses from the parse forest as a fallback strategy when there is no full parse available. Kiefer et al. (1999) took a shortest-path approach to find a sequence of fragment analysis that minimizes a heuristics-based cost function. Another variation of the algorithm (Riezler et al., 2001) is to pick fewest chunks that connect the entire sentence. While these early approaches are based on simple heuristics, more sophisticated parse selection methods also use the statistical models to rank the partial analyses. For example, Zhang et al. (2007a) proposed several ways of integrating discriminative parse ranking scores with the partial parse selection algorithm. In this experiment, we first use the shortest"
C10-1026,W05-1511,0,0.0236641,"their relatively low efficiency. For online applications, it is impermissible to wait for longer than a minute before the system responds. Apart from studies that were aimed at increasing the efficiency of deep parsers by using smarter algorithms (e.g. using left-corner relations (Van Noord, 1997)), several studies in recent years have suggested that search space restriction can offer a beneficial balance between speed and accuracy as well. Techniques that have been proposed are, among others, supertagging (Clark and Curran, 2007), CFG filtering (Matsuzaki et al., 2007) and beam thresholding (Ninomiya et al., 2005). A potential disadvantage of the latter technique is that the unifications have taken place by the time the value of the resulting chart item is investigated. One strategy that tries to prevent execution of unlikely tasks altogether is presented by van Noord (2009). In this method, the parser learns from an unannotated corpus which parse steps contributed to the solution as preferred by the disambiguation model (as opposed to a certain gold standard). Hence, this approach is selflearning. Another study that is close to our approach 223 Proceedings of the 23rd International Conference on Compu"
C10-1026,A00-2022,0,0.0315131,"The treebank’s size is just over 25k sentences (only selected from the first 45k sentences, so they don’t overlap with either the development or test set), and contains the correct HPSG derivation trees. These (projective) derivation trees will function as the training set for the statistical models we develop in this study. 2 Restriction of the search space 2.1 The PET parser The parser we employ, the PET parser (Callmeier, 2000), is an agenda-driven, bottom-up, unification-based parser. In order to reduce computational demands, state-of-the-art techniques such as subsumption-based packing (Oepen and Carroll, 2000) and the quasi-destructive unification operator (Tomabechi, 1991) have been implemented. A central component in the parser is the agenda, implemented as a priority queue of parsing tasks (unifications). Tasks are popped from the agenda, until no task is left, after which all passive items spanning the complete sentence are compared with the root conditions as specified by the grammar writer. The best parse is extracted from the parse forest by a Maximum Entropy parse disambiguation model (Toutanova et al., 2002), using selective unpacking (Zhang et al., 2007b). Two different types of items are"
C10-1026,P91-1041,0,0.212754,"irst 45k sentences, so they don’t overlap with either the development or test set), and contains the correct HPSG derivation trees. These (projective) derivation trees will function as the training set for the statistical models we develop in this study. 2 Restriction of the search space 2.1 The PET parser The parser we employ, the PET parser (Callmeier, 2000), is an agenda-driven, bottom-up, unification-based parser. In order to reduce computational demands, state-of-the-art techniques such as subsumption-based packing (Oepen and Carroll, 2000) and the quasi-destructive unification operator (Tomabechi, 1991) have been implemented. A central component in the parser is the agenda, implemented as a priority queue of parsing tasks (unifications). Tasks are popped from the agenda, until no task is left, after which all passive items spanning the complete sentence are compared with the root conditions as specified by the grammar writer. The best parse is extracted from the parse forest by a Maximum Entropy parse disambiguation model (Toutanova et al., 2002), using selective unpacking (Zhang et al., 2007b). Two different types of items are identified: passive items and active items. Passive items are ‘n"
C10-1026,J97-3004,0,0.124971,"Missing"
C10-1026,E09-1093,0,0.0220999,"Missing"
C10-1026,W07-1217,1,0.931082,"as subsumption-based packing (Oepen and Carroll, 2000) and the quasi-destructive unification operator (Tomabechi, 1991) have been implemented. A central component in the parser is the agenda, implemented as a priority queue of parsing tasks (unifications). Tasks are popped from the agenda, until no task is left, after which all passive items spanning the complete sentence are compared with the root conditions as specified by the grammar writer. The best parse is extracted from the parse forest by a Maximum Entropy parse disambiguation model (Toutanova et al., 2002), using selective unpacking (Zhang et al., 2007b). Two different types of items are identified: passive items and active items. Passive items are ‘normal’ chart items, in the sense that they can freely combine with other items. Active items still need to combine with a passive item to be complete. Hence, the parser knows two types of tasks as well (see figure 1): rule+passive and active+passive. Each time a task succeeds, the following happens: 224 • For each inserted passive item, add (rule+passive) tasks that combine the passive item with each of the rules, and add (active+passive) tasks that combine with each of the neighbouring active"
C10-1026,W07-2207,1,0.936201,"as subsumption-based packing (Oepen and Carroll, 2000) and the quasi-destructive unification operator (Tomabechi, 1991) have been implemented. A central component in the parser is the agenda, implemented as a priority queue of parsing tasks (unifications). Tasks are popped from the agenda, until no task is left, after which all passive items spanning the complete sentence are compared with the root conditions as specified by the grammar writer. The best parse is extracted from the parse forest by a Maximum Entropy parse disambiguation model (Toutanova et al., 2002), using selective unpacking (Zhang et al., 2007b). Two different types of items are identified: passive items and active items. Passive items are ‘normal’ chart items, in the sense that they can freely combine with other items. Active items still need to combine with a passive item to be complete. Hence, the parser knows two types of tasks as well (see figure 1): rule+passive and active+passive. Each time a task succeeds, the following happens: 224 • For each inserted passive item, add (rule+passive) tasks that combine the passive item with each of the rules, and add (active+passive) tasks that combine with each of the neighbouring active"
C10-1026,1991.iwpt-1.19,0,\N,Missing
C10-1026,P02-1035,0,\N,Missing
C10-2079,C08-1059,0,0.0153461,"Missing"
C10-2079,C08-1070,0,0.0134161,"because context doesn’t matter much given the user has already chosen a restaurant. As the first step towards using the information 3 We sample 300 reviews for “Time” and “Companion” evaluation. Due to the extremely low probability of occurrence of Occasion context, we futher sample 200 reviews containing Occasion-related expressions and only evaluate extraction accuracy on these samples extraction techniques to help contextual recommendation, the techniques used in this paper are far from optimal. In the future, we will research more effective text mining techniques for contextual extraction(Mazur and Dale, 2008; McCallum et al., 2000; Lafferty et al., 2001) at the same time increasing the amount of annotated review data for better classifier performance through actively learning (Laws and Sch¨utze, 2008). We also plan to work towards a better understanding of contextual information in recommender systems, and explore other types of contextual information in different types of recommendation tasks besides restaurant recommendations. 7 Acknowledgements Part of this research is funded by National Science Foundation IIS-0713111 and the Institute of Education Science. Any opinions, findings, conclusions"
C10-2166,W03-2401,0,0.0682874,"Missing"
C10-2166,W09-3032,1,0.901765,"Missing"
C10-2166,J93-2004,0,0.0351072,"Missing"
C10-2166,C02-2025,0,0.254126,"Missing"
C10-2166,W97-1502,0,\N,Missing
C10-2166,J96-2004,0,\N,Missing
C12-2127,W11-2832,0,0.0132891,"ich set of annotation as input and is tightened to specific frameworks or internal representation, making the reuse of other natural language processing components difficult. Inspired by the successful application of statistical methods in natural language analysis, researchers shifted towards using standardized linguistic annotations to learn generation models. In particular, the now ever-so-popular dependency representation for syntacto-semantic structures has made its way into the sentence realization task, as evident by the recent Generation Challenge 2011 Surface Realization Shared Task (Belz et al., 2011). Given the full-connectedness of the input structure, the task of surface realization concerns mainly about the linearization process1 , which shall determine the ordering of words in the dependency structures. While the earlier work like Langkilde and Knight (1998) showed that the N -gram language models can work well on the tree linearization, more recent study shows that improvements can be achieved by combining the language model outputs with discriminative classifiers (Filippova and Strube, 2009; Bohnet et al., 2010). On the other hand, we see that relatively few results have been report"
C12-2127,D12-1085,0,0.0229219,"Missing"
C12-2127,C10-1012,0,0.0634205,"by the recent Generation Challenge 2011 Surface Realization Shared Task (Belz et al., 2011). Given the full-connectedness of the input structure, the task of surface realization concerns mainly about the linearization process1 , which shall determine the ordering of words in the dependency structures. While the earlier work like Langkilde and Knight (1998) showed that the N -gram language models can work well on the tree linearization, more recent study shows that improvements can be achieved by combining the language model outputs with discriminative classifiers (Filippova and Strube, 2009; Bohnet et al., 2010). On the other hand, we see that relatively few results have been reported on a grammar-based approach, where linearization rules are used instead to determine the word order within a given structured input. In this paper, we use tree linearization grammars to specify the local linearization constraints in bilexical single-headed dependency trees. Unlexicalized linearization rules and their probabilities can be learned easily from the treebank. By using a dependency parser, we expand the grammar extraction to automatically parsed dependency structures as well. The linearization model is fully"
C12-2127,I05-1015,0,0.0855798,"Missing"
C12-2127,P08-1022,0,0.0578704,"Missing"
C12-2127,N09-2057,0,0.0763653,"realization task, as evident by the recent Generation Challenge 2011 Surface Realization Shared Task (Belz et al., 2011). Given the full-connectedness of the input structure, the task of surface realization concerns mainly about the linearization process1 , which shall determine the ordering of words in the dependency structures. While the earlier work like Langkilde and Knight (1998) showed that the N -gram language models can work well on the tree linearization, more recent study shows that improvements can be achieved by combining the language model outputs with discriminative classifiers (Filippova and Strube, 2009; Bohnet et al., 2010). On the other hand, we see that relatively few results have been reported on a grammar-based approach, where linearization rules are used instead to determine the word order within a given structured input. In this paper, we use tree linearization grammars to specify the local linearization constraints in bilexical single-headed dependency trees. Unlexicalized linearization rules and their probabilities can be learned easily from the treebank. By using a dependency parser, we expand the grammar extraction to automatically parsed dependency structures as well. The lineari"
C12-2127,W09-1201,1,0.864841,"Missing"
C12-2127,P98-1116,0,0.0694121,"nalysis, researchers shifted towards using standardized linguistic annotations to learn generation models. In particular, the now ever-so-popular dependency representation for syntacto-semantic structures has made its way into the sentence realization task, as evident by the recent Generation Challenge 2011 Surface Realization Shared Task (Belz et al., 2011). Given the full-connectedness of the input structure, the task of surface realization concerns mainly about the linearization process1 , which shall determine the ordering of words in the dependency structures. While the earlier work like Langkilde and Knight (1998) showed that the N -gram language models can work well on the tree linearization, more recent study shows that improvements can be achieved by combining the language model outputs with discriminative classifiers (Filippova and Strube, 2009; Bohnet et al., 2010). On the other hand, we see that relatively few results have been reported on a grammar-based approach, where linearization rules are used instead to determine the word order within a given structured input. In this paper, we use tree linearization grammars to specify the local linearization constraints in bilexical single-headed depende"
C12-2127,N06-1020,0,0.104081,"Missing"
C12-2127,H05-1066,0,0.0611513,"points suggests that a better ranking model can potentially achieve further improvements on the linearization. This will be discussed in Section 4.3. 4.2 Experiments with Automatically Parsed Data Self-training has been shown to be effective for parser training (McClosky et al., 2006). It expands the training observations on new texts with hypothesized annotation produced by a base model. In our case, we can obtain further linearization observations from unannotated sentences, and rely on a parser to produce the dependency structures7 . We use a state-of-the-art dependency parser, MSTParser (McDonald et al., 2005), and train it with the same data with gold-standard dependency annotations using the second order features and a projective decoder. For the additional data, we use a fragment of the NANC corpus (765670 sequences 〈n|sub j, ad v|mod, v|hd, n1 |o b j〉 and 〈n2 |sub j, v|hd, n1 |o b j, ad v|mod〉. On top of such instances from all the configurations, we train a tri-gram model. 5 The features we use include token features, lemma and part-of-speech, and the dependency relation. We differentiate parent and children nodes by adding different prefixes. 6 In the CoNLL data, the coarse-grained POS is the"
C12-2127,P02-1040,0,0.0835036,"find all such possibilities. With such lazy expansion of the search frontier, only the immediate candidates are added to the local agenda of each node. 4 Experiments For the evaluation, we use the dependency treebanks for multiple languages from the CoNLL-shared task 20092 (Hajiˇc et al., 2009). Additional unlabeled English texts from L.A. Times & Washington Post of the North American News Text (NANC) (Supplement)3 are used for training the English models. Testing results are reported on the development sets of the CoNLL dependency treebanks. In addition to the automatic metrics such as BLEU (Papineni et al., 2002) and Ulam’s distance (Birch et al., 2010), we also manually evaluate the quality of the system outputs (Section 4.3). 4.1 Basic Models For the basic models, we compare our grammar-based approaches with three baselines, Random, N-Gram, and Rank. The first baseline simply produces a random order of the words; the second model can be viewed as a simplified version of (Guo et al., 2011)’s basic model4 ; and the third model 2 3 4 http://ufal.mff.cuni.cz/conll2009-st/index.html http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC98T30 Instead of using grammatical functions derived from le"
C12-2127,C04-1097,0,0.024745,"e lifting of non-projective edges in a dependency tree can be combined with the use of linearization rules in our approach in the future. Note that although we test our models on the same data source as the surface realization shared task8 , subtle differences in the preprocessing of the data and/or the evaluation scripts make the direct comparison to previously reported results difficult. Some comparison of different approaches and reported results will be discussed in the next section. 5 Discussion and Future Work Several works on statistical surface realization have been reported recently. Ringger et al. (2004) proposed several models, and achieved 83.6 BLEU score on the same data source.They also tested their approaches on French and German data, but with predicate-argument structures as input. One of the interesting features of our approach is the generative nature of the model. Unlike the previous work of (Filippova and Strube, 2009; Bohnet et al., 2010) who relied on discriminative modeling for the selection of the realization, our approach actually produces the realization probabilities, and does not rely on ad hoc pruning of the search space. Filippova and Strube (2009) (and their previous pap"
C12-2127,C08-1038,0,\N,Missing
C12-2127,C98-1112,0,\N,Missing
C18-1061,W02-2004,0,0.229065,"Missing"
C18-1061,J81-4005,0,0.680931,"Missing"
C18-1061,W03-0425,0,0.0874044,"Missing"
C18-1061,N01-1025,0,0.457278,"Missing"
C18-1061,C16-1087,0,0.0263549,"Missing"
C18-1061,N16-1030,0,0.248482,"TM-CRF (Senna) (Huang et al., 2015) Edge-based CRF (Ma and Sun, 2016) Encoder-decoder-pointer framework(Zhai et al., 2017) BiLSTM (our implementation) MO-BiLSTM (this work) F1 93.91 94.30 94.29 94.01 94.34 94.32 94.52 94.46 94.80 94.72 93.89 95.01 Table 4: All-Chunking: Comparison with state-of-the-art models. English-NER Combination of HMM, Maxent etc. (Florian et al., 2003) Semi-supervised model combination (Ando and Zhang, 2005) Conv-CRF (Senna + Gazetteer) (Collobert et al., 2011) CRF with Lexicon Infused Embeddings (Passos et al., 2014) BiLSTM-CRF (Senna) (Huang et al., 2015) BiLSTM-CRF (Lample et al., 2016) BiLSTM-CNNs-CRF (Ma and Hovy, 2016) Iterated Dilated CNNs (Strubell et al., 2017) CNN-CNN-LSTM (Shen et al., 2018) BiLSTM (our implementation) MO-BiLSTM (this work) F1 88.76 89.31 89.59 90.90 90.10 90.94 91.21 90.65 90.89 88.23 90.70 Table 5: English-NER: Comparison with state-of-the-art models. Dutch-NER AdaBoost (decision trees) (Carreras et al., 2002) Semi-structured resources (Nothman et al., 2013) Variant of Seq2Seq (Gillick et al., 2015) Character-Level Stacked BiLSTM (Kuru et al., 2016) BiLSTM-CRF (Lample et al., 2016) Special Decoder + Attention (Martins and Kreutzer, 2017) BiLSTM (ou"
C18-1061,P16-1101,0,0.145232,"re, it is natural to take the tag dependencies into consideration when making a prediction in such sequence labeling tasks. Recently, methods have been proposed to capture tag dependencies for neural networks. Collobert et al. (2011) proposed a method based on convolutional neural networks, which can use dynamic programming in training and testing stage (like a CRF layer) to capture tag dependencies. Furthermore, Huang et al. (2015) proposed LSTM-CRF by combining LSTM and CRF for structured learning. They use a transition matrix to model the tag dependencies. A similar structure is adopted by Ma and Hovy (2016). Their model also involves an external layer to extract some character level features. However, it is not explicit how to model the dependencies of more tags or use the dependency information in these lines of work. We then propose a solution to capture long distance tag dependencies and use them for dependency-aware prediction of tags. For clarity, we first give some detailed explanations of the related terms in our work. “order” means the number of tags that a prediction involves in a model. An order-2 tag is a bigram which contains the previous tag and the current tag at a certain time ste"
C18-1061,D17-1036,0,0.0200174,"tates to get higher order states. Soltani and Jiang (2016) propose a model called higher order recurrent neural networks (HORNNs). They proposed to use more memory units to keep track of more preceding RNN states, which are all recurrently fed to the hidden layers as feedback. These structures of Soltani’s work are also termed “higher order” models, but the definition is different from ours. There are several other neural networks that use new techniques to improve sequence labeling. Ling et al. (2015) and Yang et al. (2016) used BiSLTM to compose character embeddings to words representation. Martins and Kreutzer (2017) used an attention mechanism to decide what is the “best” word to focus on next in sequence labeling tasks. Zhai et al. (2017) proposed to separate the segmenting and labeling in chunking. Segmentation is done by a pointer network and a decoder LSTM is used for labeling. Shen et al. (2018) used active learning to strategically choose most useful examples in NER datasets. 6 Conclusions In this paper, we focus on extending LSTM to higher order models in order to capture more tag dependencies for segmenting and labeling sequence data. We introduce a single order model, which is supposed 731 to ca"
C18-1061,H05-1124,0,0.0852536,"Missing"
C18-1061,D13-1032,0,0.0820191,"Missing"
C18-1061,W14-1609,0,0.054704,"Missing"
C18-1061,W00-0726,0,0.16383,"e accuracy and the time cost. Details of the experiments can be found in Section 4. Algorithm 1 shows the detailed process of multi-order decoding with pruning in the order-n case. 4 Experiments 4.1 Datasets Chunking and named entity recognition are sequence labeling tasks that are sensitive to tag dependencies. The tags inside a segment have internal dependencies. The tags in consecutive segments may have dependencies, too. Thus, we conduct experiments on the chunking and NER tasks to evaluate the proposed method. The test metric is F1-score. The chunking data is from CoNLL-2000 shared task (Sang and Buchholz, 2000), where we need to identify constituent parts of sentences (nouns, verbs, adjectives, etc.). To distinguish it from NP-chunking, it is referred to as the all-phrase chunking. We use the English NER data from the CoNLL-2003 shared task (Sang and Meulder, 2003). There are four types of entities to be recognized: PERSON, LOCATION, ORGANIZATION, and MISC. The other NER dataset is the Dutch-NER dataset from the shared task of CoNLL-2002. The types of entities are the same as the English NER dataset. 4.2 Experimental Details Our model uses a single layer for the forward and backward LSTMs whose dime"
C18-1061,W03-0419,0,0.0223126,"abeling tasks that are sensitive to tag dependencies. The tags inside a segment have internal dependencies. The tags in consecutive segments may have dependencies, too. Thus, we conduct experiments on the chunking and NER tasks to evaluate the proposed method. The test metric is F1-score. The chunking data is from CoNLL-2000 shared task (Sang and Buchholz, 2000), where we need to identify constituent parts of sentences (nouns, verbs, adjectives, etc.). To distinguish it from NP-chunking, it is referred to as the all-phrase chunking. We use the English NER data from the CoNLL-2003 shared task (Sang and Meulder, 2003). There are four types of entities to be recognized: PERSON, LOCATION, ORGANIZATION, and MISC. The other NER dataset is the Dutch-NER dataset from the shared task of CoNLL-2002. The types of entities are the same as the English NER dataset. 4.2 Experimental Details Our model uses a single layer for the forward and backward LSTMs whose dimensions are set to 200. We use the Adam learning method (Kingma and Ba, 2014) with the default hyper parameters. We set the dropout (Srivastava et al., 2014) rate to 0.5. Following previous work (Huang et al., 2015), we extract some spelling features and conte"
C18-1061,N03-1028,0,0.509901,"Missing"
C18-1061,C08-1106,1,0.810995,"Missing"
C18-1061,J14-3004,1,0.905087,"Missing"
D07-1110,W02-2001,1,0.844682,"Missing"
D07-1110,baldwin-etal-2004-road,0,0.0343885,"Missing"
D07-1110,A97-1052,0,0.0539433,"Missing"
D07-1110,C02-2025,0,0.0124496,"wrong head words. However, the lexical type predictor of Zhang and Kordoni (2006) that we used in our experiments did not generate interesting new entries for them in the subsequent steps, and they were thus discarded, as discussed below. With the 30 MWE candidates, we extracted a sub-corpus from the BNC with 674 sentences which included at least one of these MWEs. The lexical acquisition technique described in Zhang and Kordoni (2006) was used with this subcorpus in order to acquire new lexical entries for the head words. The lexical acquisition model was trained with the Redwoods treebank (Oepen et al., 2002), following Zhang et al. (2006). The lexical prediction model predicted for each occurrence of the head words a most plausible lexical type in that context. Only those predictions that occurred 5 times or more were taken into consideration for the generation of the new lexical entries. As a result, we obtained 21 new lexical entries. These new lexical entries were later merged into the ERG lexicon. To evaluate the grammar performance with and without these new lexical entries, we 1. parsed the sub-corpus with/without new lexical entries and compared the grammar coverage; 2. inspected the parse"
D07-1110,pearce-2002-comparative,0,0.705957,"ring Aline Villavicencio♣♠ , Valia Kordoni♦ , Yi Zhang♦ , Marco Idiart♥ and Carlos Ramisch♣ ♣ Institute of Informatics, Federal University of Rio Grande do Sul (Brazil) ♠ Department of Computer Sciences, Bath University (UK) ♦ Department of Computational Linguistics, Saarland University, and DFKI GmbH (Germany) ♥ Institute of Physics, Federal University of Rio Grande do Sul (Brazil) avillavicencio@inf.ufrgs.br, {yzhang,kordoni}@coli.uni-sb.de idiart@if.ufrgs.br, ceramisch@inf.ufrgs.br Abstract Another difficulty for work on MWE identification is that of the evaluation of the results obtained (Pearce, 2002; Evert and Krenn, 2005), starting from the lack of consensus about a precise definition for MWEs (Villavicencio et al., 2005). This paper focuses on the evaluation of methods for the automatic acquisition of Multiword Expressions (MWEs) for robust grammar engineering. First we investigate the hypothesis that MWEs can be detected by the distinct statistical properties of their component words, regardless of their type, comparing 3 statistical measures: mutual information (MI), χ2 and permutation entropy (PE). Our overall conclusion is that at least two measures, MI and PE, seem to differentiat"
D07-1110,zhang-kordoni-2006-automated,1,0.854157,"candidates are used in 2 The combination of the “word with space” approach of Zhang et al. (2006) with the constructional approach we propose here is an interesting topic that we want to investigate in future research. 1040 this experiment. We used simple heuristics in order to extract the head words from these MWEs: • the n-grams are POS-tagged with an automatic tagger; • finite verbs in the n-grams are extracted as head words; • nouns are also extracted if there is no verb in the n-gram. Occasionally, the tagger errors might introduce wrong head words. However, the lexical type predictor of Zhang and Kordoni (2006) that we used in our experiments did not generate interesting new entries for them in the subsequent steps, and they were thus discarded, as discussed below. With the 30 MWE candidates, we extracted a sub-corpus from the BNC with 674 sentences which included at least one of these MWEs. The lexical acquisition technique described in Zhang and Kordoni (2006) was used with this subcorpus in order to acquire new lexical entries for the head words. The lexical acquisition model was trained with the Redwoods treebank (Oepen et al., 2002), following Zhang et al. (2006). The lexical prediction model p"
D07-1110,W06-1206,1,0.248015,"se corpora are probably compensated by their size. Finally, we show a qualitative evaluation of the results of automatically adding extracted MWEs to existing linguistic resources. We argue that such a process improves qualitatively, if a more compositional approach to grammar/lexicon automated extension is adopted. 1 Introduction The task of automatically identifying Multiword Expressions (MWEs) like phrasal verbs (break down) and compound nouns (coffee machine) using statistical measures has been the focus of considerable investigative effort, (e.g. Pearce (2002), Evert and Krenn (2005) and Zhang et al. (2006)). Given the heterogeneousness of the different phenomena that are considered to be MWEs, there is no consensus about which method is best suited for which type of MWE, and if there is a single method that can be successfully used for any kind of MWE. In this paper we investigate some of the issues involved in the evaluation of automatically extracted MWEs, from their extraction to their subsequent use in an NLP task. In order to do that, we present a discussion of different statistical measures, and the influence that the size and quality of different data sources have. We then perform a comp"
D07-1110,P04-1057,0,\N,Missing
D07-1110,W06-1208,0,\N,Missing
D09-1082,P08-1118,0,0.0895901,"Missing"
D09-1082,P98-1013,0,0.00670965,"ilarity between T and H, this way of splitting is more reasonable. However, RTE systems using semantic role labelers has not shown very promising results, although SRL has been successfully used in many other NLP tasks, e.g. information extraction, question answering, etc. According to our analysis of the data, there are mainly three reasons: a) the limited coverage of the verb frames or predicates; b) the undetermined relationships between two frames or predicates; and c) the unsatisfying performance of an automatic SRL system. For instance, Burchardt et al. (2007) attempted to use FrameNet (Baker et al., 1998) for the RTE-3 challenge, but did not show substantial improvement. With the recent CoNLL challenges, more and more robust and accurate SRL systems are ready for use, especially for the PAS identification. For the lexical semantics, we also discover that, if we relax the matching criteria (from similarity to relatedness), heterougeous resources can contribute to the coverage differently and then the effectiveness of PAS will be shown as well. Related Work Although the term of Textual Relatedness has not been widely used by the community (as far as we know), many researchers have already incorp"
D09-1082,W07-1402,0,0.0122446,"approaches based on overlapping information or similarity between T and H, this way of splitting is more reasonable. However, RTE systems using semantic role labelers has not shown very promising results, although SRL has been successfully used in many other NLP tasks, e.g. information extraction, question answering, etc. According to our analysis of the data, there are mainly three reasons: a) the limited coverage of the verb frames or predicates; b) the undetermined relationships between two frames or predicates; and c) the unsatisfying performance of an automatic SRL system. For instance, Burchardt et al. (2007) attempted to use FrameNet (Baker et al., 1998) for the RTE-3 challenge, but did not show substantial improvement. With the recent CoNLL challenges, more and more robust and accurate SRL systems are ready for use, especially for the PAS identification. For the lexical semantics, we also discover that, if we relax the matching criteria (from similarity to relatedness), heterougeous resources can contribute to the coverage differently and then the effectiveness of PAS will be shown as well. Related Work Although the term of Textual Relatedness has not been widely used by the community (as far as"
D09-1082,P06-1114,0,0.0448584,"f being classified as a Non-entailment (N) case (=C∪U) against E case in the traditional two-way annotation. Furthermore, many state-of-the-art RTE approaches which are based on overlapping information or similarity functions between T and H, in fact over-cover the E cases, and sometimes, cover the C cases as well. Therefore, in this paper, we Recognizing Textual Entailment (RTE) (Dagan et al., 2006) is a task to detect whether one Hypothesis (H) can be inferred (or entailed) by a Text (T). Being a challenging task, it has been shown that it is helpful to applications like question answering (Harabagiu and Hickl, 2006). The recent research on RTE extends the two-way annotation into three-way1 2 , making it even more difficult, but more linguistic-motivated. The straightforward strategy is to treat it as a three-way classification task, but the performance suffers a significant drop even when using the same classifier and the same feature model. In fact, it can also be dealt with as an extension to the traditional two-way classification, e.g., by identi1 3 http://nlp.stanford.edu/RTE3-pilot/ http://www.nist.gov/tac/tracks/2008/ rte/rte.08.guidelines.html See more details about the annotation guideline at htt"
D09-1082,D08-1021,0,0.0126294,"work, we would like to see whether the PAS can help the second-stage classification as well, e.g. the semantic dependency of negation (AM-NEG) could be helpful for the contraction recognition. Furthermore, since the PAS is usually a bag of unconnected graphs, we could find a way to joint them together, in order to consider both inter- and intra- sentential inferences based on it. In addition, this approach has the potential to be integrated with other RTE modules. For instance, for the predicate alignment, we may consider to use DIRT rules (Lin and Pantel, 2001) or other paraphrase resources (Callison-Burch, 2008), and for the argument alignment, external named-entity recognizer and anaphora resolver would be very helpful. Even more, we also plan to compare/combine it with other methods which are not based on overlapping information between T and H. Impact of the Lexical Resources We did an ablation test of the lexical resources used in our alignment module. Recall that we have applied three lexical resources, VerbOcean for the predicate relatedness function, WordNet for the argument relatedness function, and Normalized Google Distance for both. Table 3 shows the performances of the system without each"
D09-1082,W04-3205,0,0.0232268,")=1  Other Otherwise Now, the only missing components in our definition is the relatedness functions between predicates, arguments, and semantic dependencies. Fortunately, many people have done research on 787 semantic relatedness in lexical semantics that we could use. Therefore, these functions can be realized by different string matching algorithms and/or lexical resources. Since the meaning of relevance is rather wide, apart from the string matching of the lemmas, we also incorporate various resources, from distributionally collected ones to hand-crafted ontologies. We choose VerbOcean (Chklovski and Pantel, 2004) to obtain the relatedness between predicates (after using WordNet (Fellbaum, 1998) to change all the nominal predicates into verbs) and use WordNet for the argument alignment. For the verb relations in VerbOcean, we consider all of them as related; and for WordNet, we not only use the synonyms, hyponyms, and hypernyms, but antonyms as well. Consequently, we simplify these basic relatedness functions into a binary decision. If the corresponding strings are matched or the relations mentioned above exist, the two predicates, arguments, or dependencies are related; otherwise, not. In addition, th"
D09-1082,D08-1084,0,0.123612,"Missing"
D09-1082,H05-1066,0,0.011741,"d produces as outputs the semantic dependencies. The head words of the arguments (including modifiers) are annotated as a direct dependent of the corresponding predicate words, labeled with the type of the semantic relation (Arg0, Arg1 . . . , and various ArgMs). Note that for the application of SRL in RTE task, the PropBank and NomBank notation appears to be more accessible and robust than the the FrameNet notation (with much more detailed roles or frame elements bond to specific verb frames). As input, the SRL system requires syntactic dependency analysis. We use the open source MST Parser (McDonald et al., 2005), trained also on the Wall Street Journal Sections of the Penn Treebank, using a projective decoder with secondorder features. Then the SRL system goes through a pipeline of 4-stage processing: predicate identification (PI) identifies words that evokes a semantic predicate; argument identification (AI) identifies the arguments of the predicates; argument classification (AC) labels the argument with the semantic relations (roles); and predicate classification (PC) further differentiate different use of the predicate word. All components are built as maximal entropy based classifiers, with their"
D09-1082,W08-2121,0,0.036144,"Missing"
D09-1082,W08-2126,1,0.910985,"the K cases instead of E cases is more effective. While in lexical semantics, semantic relatedness is a weaker concept than semantic similarity, there is no counterpart at the sentence or text level. Therefore, in this paper, we propose a Recognizing Textual Relatedness (RTR) task as a subtask or the first step of RTE. By doing so, we choose predicate-argument structure (PAS) as the feature representation, which has already been shown quite useful in the previous RTE challenges (Wang and Neumann, 2007). In order to obtain the PAS, we utilize a Semantic Role Labeling (SRL) system developed by Zhang et al. (2008). Although SRL has been shown to be effective for many tasks, e.g. information extraction, question answering, etc., it has not been successfully used for RTE, mainly due to the low coverage of the verb frame or semantic role resources or the low performance of the automatic SRL systems. The recent CoNLL shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009) have been focusing on semantic dependency parsing along with the traditional syntactic dependency parsing. The PAS from the system output is almost ready for use to build applications based on it. Therefore, another focus of this paper"
D09-1082,C98-1013,0,\N,Missing
D09-1082,W07-1401,0,\N,Missing
D09-1082,W09-1201,1,\N,Missing
D09-1162,H01-1030,0,0.0147921,"ents/sentences and retrieves only those with “novel” information. This paper focuses on applying document/sentencelevel novelty mining on Chinese. In this task, we need to identify all novel Chinese text given groups of relevant documents/sentences. Novelty mining has been performed at three different levels: event level, sentence level and document level (Li and Croft, 2005). Works on novelty mining at the event level originated from research on Topic Detection and Tracking (TDT), which is concerned with online new event detection/first story detection (Allan et al., 1998; Yang et al., 2002; Stokes and Carthy, 2001; Franz et al., 2001; Brants et al., 2003). Research on document and sentence-level novelty mining aims to find relevant and novel documents/sentences given a stream of documents/sentences. Previous studies on document and sentence-level novelty mining tend to apply some promising content-oriented techniques (Li and Croft, 2005; Allan et al., 1998; Yang et al., 1998; Zhang and Tsai, 2009). Similarity metrics that can be used for detecting novel text are word overlap, cosine similarity (Yang et al., 1998), new word count (Brants et al., 2003), etc. Other works utilize ontological knowledge, esp"
D09-1162,J05-4005,0,0.0330304,"ithm (Porter, 1997) for English word stemming. This algorithm removes the commoner morphological and inflexional endings from the words in English. The entire preprocessing steps in English novelty mining can be seen in Figure 1. 3.2 Chinese In Chinese, the word is the smallest independent meaningful element. There is no obvious boundary between words so that Chinese lexical analysis, such as Chinese word segmentation, is the prerequisite for novelty mining. Unlike English, Chinese word segmentation is a very challenging problem because of the difficulties in defining what constitutes a word (Gao et al., 2005). While each criteria provides valuable insights into “word-hood” in Chinese, they do not consistently lead us to the same conclusions. Moreover, there is no white space between Chinese words or expressions and there are many ambiguities in the Chinese language, such as: ‘主 板 和 服 务 器’ (means ‘mainboard and server’ in English) might be ‘主 板/和/服务器’ (means ‘mainboard/and/server’ in English) or ‘主 板/和 服/务/器’ (means ‘mainboard/kimono/task/utensil’ in English). This ambiguity is a great challenge for Chinese word segmentation. In addition, there is no obvious inflected or derived words in Chinese so"
D09-1162,P06-1054,0,0.0530606,"Missing"
D09-1162,W03-1509,0,0.0194634,"alay languages (Kwee et al., 2009; Tang et al., 2009; Tang and Tsai, 2009). Novelty mining studies on the Chinese language have been performed on topic de1561 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1561–1570, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP tection and tracking, which identifies and collects relevant stories on certain topics from information stream (Zheng et al., 2008; Hong et al., 2008). Also many works have discussed the issues, such as word segmentation, POS tagging etc, between English and Chinese (Wang et al., 2006; Wu et al., 2003). However, to the best of our knowledge, no studies have been reported on discussing preprocessing techniques on Chinese document and sentence-level novelty mining, which is the focus of our paper. The rest of this paper is organized as follows. Section 2 gives a brief overview of related work on detecting novel documents and sentences on English and Chinese. Section 3 introduces the details of preprocessing steps for English and Chinese. A general novelty mining algorithm is described in Section 4. Section 5 reports experimental results. Section 6 summarizes the research findings and discusse"
D11-1037,P06-4020,0,0.051008,"Missing"
D11-1037,W08-1705,0,0.023705,"Missing"
D11-1037,cer-etal-2010-parsing,0,0.0398474,"Missing"
D11-1037,P05-1022,0,0.0267785,"ngs from the realm of natural language database interfaces directly with semantic representations in lambda calculus. These were hand-written on the basis of database query statements distributed with the original datasets. English factored model which combines the preferences of unlexicalized PCFG phrase structures and of lexical dependencies, trained on sections 02–21 of the WSJ portion of the PTB. We chose Stanford Parser from among the state-of-the-art PTB-derived parsers for its support for grammatical relations as an alternate interface representation. Charniak&Johnson Reranking Parser (Charniak and Johnson, 2005) is a two-stage PCFG parser with a lexicalized generative model for the firststage, and a discriminative MaxEnt reranker for the second-stage. The models we evaluate are also trained on sections 02–21 of the WSJ. Top-50 readings were used for the reranking stage. The output constituent trees were then converted into Stanford Dependencies. According to Cer et al. (2010), this combination gives the best parsing accuracy in terms of Stanford dependencies on the PTB. Enju (Miyao et al., 2004) is a probabilistic HPSG parser, combining a hand-crafted core grammar with automatically acquired lexical"
D11-1037,J07-4004,0,0.0417408,". (2010), this combination gives the best parsing accuracy in terms of Stanford dependencies on the PTB. Enju (Miyao et al., 2004) is a probabilistic HPSG parser, combining a hand-crafted core grammar with automatically acquired lexical types from the PTB.3 The model we evaluate is trained on the same material from the WSJ sections of the PTB, but the treebank is first semi-automatically converted into HPSG derivations, and the annotation is enriched with typed feature structures for each constituent. In addition to HPSG derivation trees, Enju also produces predicate argument structures. C&C (Clark and Curran, 2007) is a statistical CCG parser. Abstractly similar to the approach of Enju, the grammar and lexicon are automatically induced from CCGBank (Hockenmaier and Steedman, 2007), a largely automatic projection of (the WSJ portion of) PTB trees into the CCG framework. In addition to CCG derivations, the C&C parser can directly output a variant of grammatical relations. RASP (Briscoe et al., 2006) is an unlexicalized robust parsing system, with a hand-crafted “tag sequence” grammar at its core. The parser thus analyses a lattice of PoS tags, building a parse forest from which the most probable syntactic"
D11-1037,W01-0713,0,0.0205191,"ive examples along these dimensions, as these help to position the parsers we subsequently evaluate.1 2.1 Approaches to parsing Source of linguistic knowledge At one end of this dimension, we find systems whose linguistic knowledge is encoded in hand-crafted rules and lexical entries; for English, the ParGram XLE system (Riezler et al., 2002) and DELPH - IN English Resource Grammar (ERG; Flickinger (2000))—each reflecting decades of continuous development—achieve broad coverage of open-domain running text, for example. At the other end of this dimension, we find fully unsupervised approaches (Clark, 2001; Klein and Manning, 2004), where the primary source of linguistic knowledge is co-occurrence patterns of words in unannotated text. As Haghighi and Klein (2006) show, augmenting this knowledge with handcrafted prototype “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e.g. the Penn Treebank (PTB), which encodes “surface grammatical analysis” (Marcus et al., 1993). Such approaches include those that directly (and exclusively) use the information in the treebank (e.g. Charniak"
D11-1037,flickinger-etal-2010-wikiwoods,1,0.827502,"omparable to what they report for the Brown Corpus (but not the WSJ portion of the PTB). 4.2 Annotation format We annotated up to two dependency triples per phenomenon instance, identifying the heads and dependents by the surface form of the head words in the sentence suffixed with a number indicating word position (see Table 2).6 Some strings contain more than one instance of the phenomenon they illustrate; in these cases, multiple sets of dependencies are We processed 900 million tokens of Wikipedia text using the October 2010 release of the ERG, following the work of the WikiWoods project (Flickinger et al., 2010). Using the top-ranked ERG deriva6 tion trees as annotations over this corpus and simAs the parsers differ in tokenization strategies, our evaluaple patterns using names of ERG-specific construc- tion script treats these position IDs as approximate indicators. 402 Item ID 1011079100200 1011079100200 1011079100200 Phenomenon absol absol absol Polarity 1 1 1 Dependency having-2|been-3|passed-4 ARG act-1 withdrew-9 MOD having-2|been-3|passed-4 carried+on-12 MOD having-2|been-3|passed-4 Table 2: Sample annotations for sentence # 1011079100200: The-0 act-1 having-2 been-3 passed-4 in-5 that-6 year-"
D11-1037,P06-1111,0,0.012084,"c knowledge At one end of this dimension, we find systems whose linguistic knowledge is encoded in hand-crafted rules and lexical entries; for English, the ParGram XLE system (Riezler et al., 2002) and DELPH - IN English Resource Grammar (ERG; Flickinger (2000))—each reflecting decades of continuous development—achieve broad coverage of open-domain running text, for example. At the other end of this dimension, we find fully unsupervised approaches (Clark, 2001; Klein and Manning, 2004), where the primary source of linguistic knowledge is co-occurrence patterns of words in unannotated text. As Haghighi and Klein (2006) show, augmenting this knowledge with handcrafted prototype “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e.g. the Penn Treebank (PTB), which encodes “surface grammatical analysis” (Marcus et al., 1993). Such approaches include those that directly (and exclusively) use the information in the treebank (e.g. Charniak (1997), Collins (1999), Petrov et al. (2006), inter alios) as well as those that complement treebank structures with some amount of hand-coded linguistic knowled"
D11-1037,J07-3004,0,0.221976,"type “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e.g. the Penn Treebank (PTB), which encodes “surface grammatical analysis” (Marcus et al., 1993). Such approaches include those that directly (and exclusively) use the information in the treebank (e.g. Charniak (1997), Collins (1999), Petrov et al. (2006), inter alios) as well as those that complement treebank structures with some amount of hand-coded linguistic knowledge (e.g. O’Donovan et al. (2004), Miyao et al. (2004), Hockenmaier and Steedman (2007), inter alios). Another hybrid in terms of its acquisition of linguistic knowledge is the RASP system of Briscoe et al. (2006), combining a hand-coded grammar over PoS tag sequences with a probabilistic tagger and statistical syntactic disambiguation. Design of representations Approaches to parsing also differ fundamentally in the style of representation assigned to strings. These vary both in their 1 Additional sources of variation among extant parsing technologies include (a) the behavior with respect to ungrammatical inputs and (b) the relationship between probabilistic and symbolic knowled"
D11-1037,W07-2416,0,0.109783,"with a hand-coded grammar at their core typically also incorporate an automatically trained probabilistic disambiguation component. 398 formal nature and the “granularity” of linguistic information (i.e. the number of distinctions assumed), encompassing variants of constituent structure, syntactic dependencies, or logical-form representations of semantics. Parser interface representations range between the relatively simple (e.g. phrase structure trees with a limited vocabulary of node labels as in the PTB, or syntactic dependency structures with a limited vocabulary of relation labels as in Johansson and Nugues (2007)) and the relatively complex, as for example elaborate syntactico-semantic analyses produced by the ParGram or DELPH - IN grammars. There tends to be a correlation between the methodology used in the acquisition of linguistic knowledge and the complexity of representations: in the creation of a mostly hand-crafted treebank like the PTB, representations have to be simple enough for human annotators to reliably manipulate. Deriving more complex representations typically presupposes further computational support, often involving some hand-crafted linguistic knowledge—which can take the form of ma"
D11-1037,N04-1013,0,0.0162081,"Missing"
D11-1037,P04-1061,0,0.0139015,"along these dimensions, as these help to position the parsers we subsequently evaluate.1 2.1 Approaches to parsing Source of linguistic knowledge At one end of this dimension, we find systems whose linguistic knowledge is encoded in hand-crafted rules and lexical entries; for English, the ParGram XLE system (Riezler et al., 2002) and DELPH - IN English Resource Grammar (ERG; Flickinger (2000))—each reflecting decades of continuous development—achieve broad coverage of open-domain running text, for example. At the other end of this dimension, we find fully unsupervised approaches (Clark, 2001; Klein and Manning, 2004), where the primary source of linguistic knowledge is co-occurrence patterns of words in unannotated text. As Haghighi and Klein (2006) show, augmenting this knowledge with handcrafted prototype “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e.g. the Penn Treebank (PTB), which encodes “surface grammatical analysis” (Marcus et al., 1993). Such approaches include those that directly (and exclusively) use the information in the treebank (e.g. Charniak (1997), Collins (1999), Pe"
D11-1037,J93-2004,0,0.0463107,"verage of open-domain running text, for example. At the other end of this dimension, we find fully unsupervised approaches (Clark, 2001; Klein and Manning, 2004), where the primary source of linguistic knowledge is co-occurrence patterns of words in unannotated text. As Haghighi and Klein (2006) show, augmenting this knowledge with handcrafted prototype “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e.g. the Penn Treebank (PTB), which encodes “surface grammatical analysis” (Marcus et al., 1993). Such approaches include those that directly (and exclusively) use the information in the treebank (e.g. Charniak (1997), Collins (1999), Petrov et al. (2006), inter alios) as well as those that complement treebank structures with some amount of hand-coded linguistic knowledge (e.g. O’Donovan et al. (2004), Miyao et al. (2004), Hockenmaier and Steedman (2007), inter alios). Another hybrid in terms of its acquisition of linguistic knowledge is the RASP system of Briscoe et al. (2006), combining a hand-coded grammar over PoS tag sequences with a probabilistic tagger and statistical syntactic di"
D11-1037,H05-1066,0,0.0471313,"equence” grammar at its core. The parser thus analyses a lattice of PoS tags, building a parse forest from which the most probable syntactic trees and sets of corresponding grammatical relations can be extracted. Unlike other parsers in our mix, RASP did not build on PTB data in either its PoS tagging 3 This hand-crafted grammar is distinct from the ERG, despite sharing the general framework of HPSG. The ERG is not included in our evaluation, since it was used in the extraction of the original examples and thus cannot be fairly evaluated. 399 or syntactic disambiguation components. MSTParser (McDonald et al., 2005) is a datadriven dependency parser. The parser uses an edgefactored model and searches for a maximal spanning tree that connects all the words in a sentence into a dependency tree. The model we evaluate is the second-order projective model trained on the same WSJ corpus, where the original PTB phrase structure annotations were first converted into dependencies, as established in the CoNLL shared task 2009 (Johansson and Nugues, 2007). XLE/ParGram (Riezler et al., 2002, see also Cahill et al., 2008) applies a hand-built Lexical Functional Grammar for English and a stochastic parse selection mod"
D11-1037,C10-1094,0,0.115638,"Missing"
D11-1037,P04-1047,0,0.0766051,"Missing"
D11-1037,P06-1055,0,0.00651761,"4), where the primary source of linguistic knowledge is co-occurrence patterns of words in unannotated text. As Haghighi and Klein (2006) show, augmenting this knowledge with handcrafted prototype “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e.g. the Penn Treebank (PTB), which encodes “surface grammatical analysis” (Marcus et al., 1993). Such approaches include those that directly (and exclusively) use the information in the treebank (e.g. Charniak (1997), Collins (1999), Petrov et al. (2006), inter alios) as well as those that complement treebank structures with some amount of hand-coded linguistic knowledge (e.g. O’Donovan et al. (2004), Miyao et al. (2004), Hockenmaier and Steedman (2007), inter alios). Another hybrid in terms of its acquisition of linguistic knowledge is the RASP system of Briscoe et al. (2006), combining a hand-coded grammar over PoS tag sequences with a probabilistic tagger and statistical syntactic disambiguation. Design of representations Approaches to parsing also differ fundamentally in the style of representation assigned to strings. These vary both in"
D11-1037,P02-1035,0,0.037496,"anguage Processing, pages 397–408, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics unlabeled training data. A related dimension of variation is the type of representations manipulated by the parser. We briefly review some representative examples along these dimensions, as these help to position the parsers we subsequently evaluate.1 2.1 Approaches to parsing Source of linguistic knowledge At one end of this dimension, we find systems whose linguistic knowledge is encoded in hand-crafted rules and lexical entries; for English, the ParGram XLE system (Riezler et al., 2002) and DELPH - IN English Resource Grammar (ERG; Flickinger (2000))—each reflecting decades of continuous development—achieve broad coverage of open-domain running text, for example. At the other end of this dimension, we find fully unsupervised approaches (Clark, 2001; Klein and Manning, 2004), where the primary source of linguistic knowledge is co-occurrence patterns of words in unannotated text. As Haghighi and Klein (2006) show, augmenting this knowledge with handcrafted prototype “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all"
D11-1037,D09-1085,0,0.273521,"Missing"
D11-1037,P08-1039,0,0.028581,"Missing"
D11-1037,N10-1034,0,\N,Missing
D11-1037,J03-4003,0,\N,Missing
D18-1138,P82-1020,0,0.710299,"Missing"
D18-1138,D14-1181,0,0.0108579,"Missing"
D18-1138,P18-2027,1,0.87641,"ng. Fu et al. (2017) implement a multi-decoder auto-encoder (Bengio et al., 2009; Dai and Le, 2015) where the encoder is used to capture the content and the sentiment-specific decoders are used to generate target sentence. Hu et al. (2017) augment the unstructured variables z in vanilla VAE with a set of structured variables c each of which targets a salient and independent semantic feature of sentences, to control sentence sentiment. However, all of these work attempt to implicitly separate the non-emotional content from the emotional information in a dense sentence representation. Xu et al. (2018) explicitly filter out emotional words. They use two sentiment-specific decoders to attach sentiments to non-emotional context. The decoders bear all the burdens to generate sentiments. In our model, we use sentiment memories to assist generating sentiments with only one decoder, which results in fewer parameters. Proposed Model Emotional Words Detection Model We first find the emotional words that have the most discriminative power for sentiment polarity. This work is done by training a sentiment classifier with a simple self-attention mechanism. Here the sequence of inputs {h1 , ..., hT } ar"
D18-1138,P18-1090,1,0.737107,"ng. Fu et al. (2017) implement a multi-decoder auto-encoder (Bengio et al., 2009; Dai and Le, 2015) where the encoder is used to capture the content and the sentiment-specific decoders are used to generate target sentence. Hu et al. (2017) augment the unstructured variables z in vanilla VAE with a set of structured variables c each of which targets a salient and independent semantic feature of sentences, to control sentence sentiment. However, all of these work attempt to implicitly separate the non-emotional content from the emotional information in a dense sentence representation. Xu et al. (2018) explicitly filter out emotional words. They use two sentiment-specific decoders to attach sentiments to non-emotional context. The decoders bear all the burdens to generate sentiments. In our model, we use sentiment memories to assist generating sentiments with only one decoder, which results in fewer parameters. Proposed Model Emotional Words Detection Model We first find the emotional words that have the most discriminative power for sentiment polarity. This work is done by training a sentiment classifier with a simple self-attention mechanism. Here the sequence of inputs {h1 , ..., hT } ar"
D18-1138,P18-2115,1,0.912831,"ng. Fu et al. (2017) implement a multi-decoder auto-encoder (Bengio et al., 2009; Dai and Le, 2015) where the encoder is used to capture the content and the sentiment-specific decoders are used to generate target sentence. Hu et al. (2017) augment the unstructured variables z in vanilla VAE with a set of structured variables c each of which targets a salient and independent semantic feature of sentences, to control sentence sentiment. However, all of these work attempt to implicitly separate the non-emotional content from the emotional information in a dense sentence representation. Xu et al. (2018) explicitly filter out emotional words. They use two sentiment-specific decoders to attach sentiments to non-emotional context. The decoders bear all the burdens to generate sentiments. In our model, we use sentiment memories to assist generating sentiments with only one decoder, which results in fewer parameters. Proposed Model Emotional Words Detection Model We first find the emotional words that have the most discriminative power for sentiment polarity. This work is done by training a sentiment classifier with a simple self-attention mechanism. Here the sequence of inputs {h1 , ..., hT } ar"
D18-1138,P02-1040,0,0.105027,"atent content space across different sentiments and leverages refined alignment of latent representations to perform sentiment modification. Multi-decoder Auto-Encoder (MAE): This system is proposed by Fu et al. (2017). They use a multi-decoder seq2seq model (Bengio et al., 2009; Dai and Le, 2015) where the encoder captures content information by adversarial learning (Goodfellow et al., 2014) and the sentiment-specific decoders are used to generate target sentences. 4.4 Results and Discussions We use ACC to denote the transformation accuracy. Following Gan et al. (2017), we also compute BLEU (Papineni et al., 2002) between the Table 3: Examples generated by the proposed method and baselines. In comparison, our model changes the sentiment of inputs with higher semantic relevance. output and the source text to evaluate the content preservation degree. A high BLEU score primarily indicates that the system can correctly preserve content by retaining the same words from the source sentence. The experimental results of our proposed model and the baselines are shown in Table 1. Both baseline models have low BLEU score but high accuracy, which indicates that they may be trapped in a situation that they simply o"
D18-1138,D17-1020,0,0.0176799,"ining process and the testing process, respectively. The process with a negative input is in a similar way. mance, especially improves content preservation degree by a large margin. Our contributions are concluded as follows: • We propose a method that uses sentiment memories to accomplish sentiment modification without any help of the parallel data. The proposed sentiment-memory based autoencoder (Bengio et al., 2009; Ma et al., 2018b) learns the idea of memory network (Weston et al., 2014; Sukhbaatar et al., 2015) but simplifies the process. Our work is also related to the generation tasks (Wang et al., 2017; Liu et al., 2018; Ma et al., 2018a; Lin et al., 2018). These tasks usually generate texts that preserve main information of input texts. 3 We first use a variant of self-attention(Lin et al., 2017; Kim et al., 2017) mechanism to distinguish the emotional and non-emotional words. Then the positive words and negative words are used to update the corresponding memory modules. Finally, the decoder uses the target sentiment information extracted from the memory and the content representation to perform decoding. 3.1 • The proposed method improves the content preservation degree by a large margin"
D18-1462,N18-1204,0,0.227753,"harles et al., 2001; Huang et al., 2016), and so on. Different from these studies, we get rid of external materials and consider the complete story generation task (McIntyre and Lapata, 2009). For this task, the widely used models are based on Seq2Seq models. However, although they can generate a fluent sentence (Xu et al., 2018a), these models still perform badly on generating inter-related sentences, which are necessary for a coherent story. To address this problem, there are several models that build the mid-level sentence semantic representation to simplify the dependency among sentences. Clark et al. (2018) extract the entities in sentences, and combine the entity context and text context together when generating a target sentence. Cao et al. (2018) encode the words with specific pre-defined dependency labels to a midlevel sentence representation. Martin et al. (2018) use additional knowledge bases to get a generalized sentence representation. Ma et al. (2018b) use the bag-of-words which occur in all references as a representation of the correct translation. Luo et al. (2018) propose to use two auto-encoders to learn the semantic representation of utterance in dialogue. However, although these m"
D18-1462,D13-1155,0,0.0159139,"model should generate a coherent story based on a given sentence. We build a new dataset for this task by splitting the data into two parts. In each story, we take the first sentence as the input text, and the following sentences as the target text. The processed dataset contains 40153, 4990, and 5054 stories for training, validation, and testing, respectively. The maximum number of sentences in each story is 6. In total, the number of training sentences is over 20K and the number of training words is over 2M. To pre-train the skeleton extraction module, we use a sentence compression dataset (Filippova and Altun, 2013). In this dataset, every compression is a subsequence of tokens from the input. The dataset contains 16999, 1000, and 1998 pairs for training, validation, and testing, respectively. 4.2 Baselines We compare our proposed model with the following the state-of-the-art models. Entity-Enhanced Seq2Seq Model (EESeq2Seq) (Clark et al., 2018). It regards entities as important context needed for coherent stories. When decoding a sentence, it combines entity context and text context together to reduce dependency sparsity. Dependency-Tree Enhanced Seq2Seq Model (DE-Seq2Seq) (Cao et al., 2018). It defines"
D18-1462,P15-1107,0,0.0771,"Missing"
D18-1462,D16-1127,0,0.0777727,"Missing"
D18-1462,D18-1075,1,0.844791,"re several models that build the mid-level sentence semantic representation to simplify the dependency among sentences. Clark et al. (2018) extract the entities in sentences, and combine the entity context and text context together when generating a target sentence. Cao et al. (2018) encode the words with specific pre-defined dependency labels to a midlevel sentence representation. Martin et al. (2018) use additional knowledge bases to get a generalized sentence representation. Ma et al. (2018b) use the bag-of-words which occur in all references as a representation of the correct translation. Luo et al. (2018) propose to use two auto-encoders to learn the semantic representation of utterance in dialogue. However, although these models reduce the dependency sparsity to some extent, the unified rules are non-flexible and tend to generate oversimplified representations, resulting in the loss of key information. Different from these models, we use a reinforcement learning method to automatically extract sentence skeletons for simplifying the dependency of sentences, rather than manual rules. Therefore, our proposed skeleton-based model is more flexible and can adaptively determine the appropriate granu"
D18-1462,P18-2115,1,0.913551,"e story generation. Introduction We focus on the problem of narrative story generation, a special kind of story generation (Li et al., 2013). It requires systems to generate a narrative story based on a short description of a scene or an event, as shown in Table 1. In general, a narrative story is described with several inter-related scenes. Different from traditional text generation tasks, this task is more challenging because it demands the generated sentences with tight semantic connections. Currently, most state-of-the-art approaches (Jain et al., 2017; Liu et al., 2017; Fan et al., 2018; Ma et al., 2018a; Xu et al., 2018b) are largely based on Sequenceto-Sequence (Seq2Seq) models (Sutskever et al., 2014), which generate a sentence at a stroke in a left-to-right manner. However, we find it hard for these approaches to model the semantic dependency among sentences, 1 The code is available at https://github.com/ lancopku/Skeleton-Based-Generation-Model which causes low-quality generated stories where the scenes are irrelevant. In fact, as shown in Figure 1, we observe that the connection among sentences is mainly reflected through key phrases, such as predicates, subjects, objects and so on. In"
D18-1462,P18-2053,1,0.92804,"e story generation. Introduction We focus on the problem of narrative story generation, a special kind of story generation (Li et al., 2013). It requires systems to generate a narrative story based on a short description of a scene or an event, as shown in Table 1. In general, a narrative story is described with several inter-related scenes. Different from traditional text generation tasks, this task is more challenging because it demands the generated sentences with tight semantic connections. Currently, most state-of-the-art approaches (Jain et al., 2017; Liu et al., 2017; Fan et al., 2018; Ma et al., 2018a; Xu et al., 2018b) are largely based on Sequenceto-Sequence (Seq2Seq) models (Sutskever et al., 2014), which generate a sentence at a stroke in a left-to-right manner. However, we find it hard for these approaches to model the semantic dependency among sentences, 1 The code is available at https://github.com/ lancopku/Skeleton-Based-Generation-Model which causes low-quality generated stories where the scenes are irrelevant. In fact, as shown in Figure 1, we observe that the connection among sentences is mainly reflected through key phrases, such as predicates, subjects, objects and so on. In"
D18-1462,P09-1025,0,0.0721962,"Missing"
D18-1462,P02-1040,0,0.101185,"Missing"
D18-1462,D18-1428,1,0.902082,". Introduction We focus on the problem of narrative story generation, a special kind of story generation (Li et al., 2013). It requires systems to generate a narrative story based on a short description of a scene or an event, as shown in Table 1. In general, a narrative story is described with several inter-related scenes. Different from traditional text generation tasks, this task is more challenging because it demands the generated sentences with tight semantic connections. Currently, most state-of-the-art approaches (Jain et al., 2017; Liu et al., 2017; Fan et al., 2018; Ma et al., 2018a; Xu et al., 2018b) are largely based on Sequenceto-Sequence (Seq2Seq) models (Sutskever et al., 2014), which generate a sentence at a stroke in a left-to-right manner. However, we find it hard for these approaches to model the semantic dependency among sentences, 1 The code is available at https://github.com/ lancopku/Skeleton-Based-Generation-Model which causes low-quality generated stories where the scenes are irrelevant. In fact, as shown in Figure 1, we observe that the connection among sentences is mainly reflected through key phrases, such as predicates, subjects, objects and so on. In this work, we reg"
D18-1462,P18-1090,1,0.907641,". Introduction We focus on the problem of narrative story generation, a special kind of story generation (Li et al., 2013). It requires systems to generate a narrative story based on a short description of a scene or an event, as shown in Table 1. In general, a narrative story is described with several inter-related scenes. Different from traditional text generation tasks, this task is more challenging because it demands the generated sentences with tight semantic connections. Currently, most state-of-the-art approaches (Jain et al., 2017; Liu et al., 2017; Fan et al., 2018; Ma et al., 2018a; Xu et al., 2018b) are largely based on Sequenceto-Sequence (Seq2Seq) models (Sutskever et al., 2014), which generate a sentence at a stroke in a left-to-right manner. However, we find it hard for these approaches to model the semantic dependency among sentences, 1 The code is available at https://github.com/ lancopku/Skeleton-Based-Generation-Model which causes low-quality generated stories where the scenes are irrelevant. In fact, as shown in Figure 1, we observe that the connection among sentences is mainly reflected through key phrases, such as predicates, subjects, objects and so on. In this work, we reg"
D19-1460,W17-5526,0,0.0950737,"Missing"
D19-1460,D18-1547,0,0.0768374,"Missing"
D19-1460,P15-1166,0,0.0132237,"roader context. Only by seeing previous utterances, such as requests to book a flight on a specific day to a specific destination, can this task be performed. Additionally, a single intent can be phrased in multiple ways depending on context; “book my flight”, “finalize my reservation”, “Yes, the 6 pm one” may all be referring to a flight-booking intent. Hence, entire conversations, rather than independent utterances, must be collected. Such data is even more pertinent to modeling NLU and related tasks as they require large, varied, and ideally human-generated datasets. Moreover, recent work (Dong et al., 2015; Devlin et al., 2018) has shown the benefit of applying joint-training and transfer learning techniques to natural language processing tasks. However, these approaches have yet to become widely used in dialogue tasks, due to a lack of largescale datasets. Furthermore, the latest state of the art end-to-end neural approaches benefit from such training data even more so than past work on goal-oriented dialogues structured around slot filling (Lemon et al., 2006; Wang and Lemon, 2013). One way to simulate data—and not risk releasing personally identifying information—for a domain is to use a Wiz"
D19-1460,H90-1021,0,0.30112,"tiple existing goal-oriented dialogue collections generated by humans through Wizardof-Oz techniques. The Dialog State Tracking Challenge, aka Dialog Systems Technology Challenge, (DSTC) spans 8 iterations and entails the domains of bus timetables, restaurant reservations, and hotel bookings, travel, alarms, movies, etc. (Williams et al., 2016). Frames (Asri et al., 2017) has 1369 dialogues about vacation packages. MultiWOZ contains 10,438 dialogues about Cambridge hotels and restaurants (Budzianowski et al., 2018). There are several dialogue datasets that specialize in a single domain. ATIS (Hemphill et al., 1990) comprises speech data about airlines structured around formal airline flight tables. Similarly, the Google Airlines dataset purportedly contains 400,000 templated dialogues about airline reservations (Wei 4527 et al., 2018).2 The Ubuntu Dialogue Corpus has over a million dialogues about Ubuntu technical support (Lowe et al., 2015). On the other hand, Chit-chat style dialogues without goals have been popular since ELIZA and have been investigated with neural techniques (Weizenbaum, 1966; Li et al., 2016, 2017). However, these datasets cannot be used for modeling goal-oriented tasks. Related di"
D19-1460,P18-1031,0,0.015712,". Recall that Fastfood had the most diverse dialogues (biases) as per Table 4 and the lowest IAA as per Table 6. sification naturally lends itself to joint training, given agent DAs are shared among all domains. To explore the benefits of multi-domain training, we jointly train an agent DA classification model on all domains and report test results for each domain separately. These results are provided in Table 8. This straightforward technique leads to a consistent but less than one point improvement in F1 scores. We expect that more sophisticated transfer learning methods (Liu et al., 2017; Howard and Ruder, 2018) could generate larger improvements for these domains. Overall, our results demonstrate that there is still headroom for performance improvement, especially for the SL task, across all domains. Consequently, MultiDoGO should be a relevant benchmark for developing new state-of-theart NLU models for the foreseeable future. Sentence vs. Turn Level Annotation Units. Regarding the performance of the LSTM and ELM O models on sentence vs. turn level annotation units, our results suggest that turn level annotations increase the difficulty of the DA classification task. This finding is evidenced by DA"
D19-1460,E06-2009,0,0.0494424,"pertinent to modeling NLU and related tasks as they require large, varied, and ideally human-generated datasets. Moreover, recent work (Dong et al., 2015; Devlin et al., 2018) has shown the benefit of applying joint-training and transfer learning techniques to natural language processing tasks. However, these approaches have yet to become widely used in dialogue tasks, due to a lack of largescale datasets. Furthermore, the latest state of the art end-to-end neural approaches benefit from such training data even more so than past work on goal-oriented dialogues structured around slot filling (Lemon et al., 2006; Wang and Lemon, 2013). One way to simulate data—and not risk releasing personally identifying information—for a domain is to use a Wizard-of-Oz data gathering technique, which requires that participants in a conver4526 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4526–4536, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Role Turn A Hey there! Good morning. You’re connected to LMT Airways. How may I help you? C Hi, I wonder if you c"
D19-1460,D16-1127,0,0.039641,"2018). There are several dialogue datasets that specialize in a single domain. ATIS (Hemphill et al., 1990) comprises speech data about airlines structured around formal airline flight tables. Similarly, the Google Airlines dataset purportedly contains 400,000 templated dialogues about airline reservations (Wei 4527 et al., 2018).2 The Ubuntu Dialogue Corpus has over a million dialogues about Ubuntu technical support (Lowe et al., 2015). On the other hand, Chit-chat style dialogues without goals have been popular since ELIZA and have been investigated with neural techniques (Weizenbaum, 1966; Li et al., 2016, 2017). However, these datasets cannot be used for modeling goal-oriented tasks. Related dialogue dataset collections used for Sequential Question Answering rely on dialogue to answer questions, but the task is notably different from our use case of modeling goal oriented conversational AI, hence leading to different evaluation considerations (Reddy et al., 2019; Choi et al., 2018). 3 3.1 MultiDoGO Dataset Curation Data Collection Procedure We employ both internal data associates, who we train, and crowd-sourced workers from Mechanical Turk (MTurkers) to generate conversational data using a W"
D19-1460,D17-1230,0,0.0714893,"Missing"
D19-1460,P17-1001,0,0.017317,"absolute F1 scores. Recall that Fastfood had the most diverse dialogues (biases) as per Table 4 and the lowest IAA as per Table 6. sification naturally lends itself to joint training, given agent DAs are shared among all domains. To explore the benefits of multi-domain training, we jointly train an agent DA classification model on all domains and report test results for each domain separately. These results are provided in Table 8. This straightforward technique leads to a consistent but less than one point improvement in F1 scores. We expect that more sophisticated transfer learning methods (Liu et al., 2017; Howard and Ruder, 2018) could generate larger improvements for these domains. Overall, our results demonstrate that there is still headroom for performance improvement, especially for the SL task, across all domains. Consequently, MultiDoGO should be a relevant benchmark for developing new state-of-theart NLU models for the foreseeable future. Sentence vs. Turn Level Annotation Units. Regarding the performance of the LSTM and ELM O models on sentence vs. turn level annotation units, our results suggest that turn level annotations increase the difficulty of the DA classification task. This fi"
D19-1460,W15-4640,0,0.206555,"l., 2016). Frames (Asri et al., 2017) has 1369 dialogues about vacation packages. MultiWOZ contains 10,438 dialogues about Cambridge hotels and restaurants (Budzianowski et al., 2018). There are several dialogue datasets that specialize in a single domain. ATIS (Hemphill et al., 1990) comprises speech data about airlines structured around formal airline flight tables. Similarly, the Google Airlines dataset purportedly contains 400,000 templated dialogues about airline reservations (Wei 4527 et al., 2018).2 The Ubuntu Dialogue Corpus has over a million dialogues about Ubuntu technical support (Lowe et al., 2015). On the other hand, Chit-chat style dialogues without goals have been popular since ELIZA and have been investigated with neural techniques (Weizenbaum, 1966; Li et al., 2016, 2017). However, these datasets cannot be used for modeling goal-oriented tasks. Related dialogue dataset collections used for Sequential Question Answering rely on dialogue to answer questions, but the task is notably different from our use case of modeling goal oriented conversational AI, hence leading to different evaluation considerations (Reddy et al., 2019; Choi et al., 2018). 3 3.1 MultiDoGO Dataset Curation Data"
D19-1460,W17-5506,0,0.0272455,"etween the customer and agent roles creates training data for a bot that explicitly simulates agents. Annotation Unit Granularity: Sentence vs. Turn Level An important decision, which is often under discussed, is the proper semantic DA 0.701 ISAA IC 0.728 SL 0.695 Table 2: Dialogue act (DA), Intent class (IC), and slot labeling (SL) Inter Source Annotation Agreement (ISAA) scores quantifying the agreement of crowd sourced and professional annotations. unit of text to annotate in a dialogue. Commonly, datasets provide annotations at the turn level (Budzianowski et al., 2018; Asri et al., 2017; Mihail et al., 2017). However, turn level annotations can introduce confusion for IC datasets, given multiple intents may be present in different sentences of a single turn. For instance, consider the turn “I would like to book a flight to San Francisco. Also, I want to cancel a flight to Austin.” Here, the first sentence has the BookFlight intent and the second sentence has the CancelFlight intent. An turn level annotation of this utterance would yield the multi-class intent (BookFlight, CancelFlight). In contrast, a sentence level annotation of this utterance identifies that the first sentence corresponds to Bo"
D19-1460,L18-1460,0,0.0291419,"K raw conversations of which 54,818 conversations are annotated at the turn level. We investigate multiple levels of annotation granularity. We annotate a subset of the data on both turn and sentence levels. A turn is defined as a sequence of one or more speech/text sentences by a participant in a conversation. A sentence is a period delimited sequence of words in a turn. A turn may comprise one or more sentences. We do use the term utterance to refer to a unit (turn or sentence, spoken or written by a participant).1 1 We acknowledge that the term utterance is controversial in the literature (Pareti and Lando, 2018) In our devised annotation strategy, we distinguish between dialogue speech acts for agents vs. customers. In MultiDoGO, the agents’ speech acts [DA] are annotated with generic class labels common across all domains, while customer speech acts are labeled with intent classes [IC]. Moreover, we annotate customer utterances with the appropriate slot labels [SL], which consist of the SL span and corresponding tokens with that SL tag. We present the strategies we use to curate and annotate such data given its contextual setting. We furthermore illustrate the efficacy of our devised approaches and"
D19-1460,N18-1202,0,0.0355317,"ularity. Red highlight denotes the strategy with the highest DA F1 score across annotation granularities. split. However, our conversation level splits result in imbalanced intent and slot label distributions. Models: We evaluate the performance of two neural models on each domain. The first is a bi-directional LSTM (Hochreiter and Schmidhuber, 1997) with GloVe word embeddings, a hidden state of size 512, and two fully connected output layers for slot labels and intent classes respectively. The second model, ELM O, is similar to the LSTM architecture but it addiitonally uses pretrained ELM O (Peters et al., 2018) embeddings in addition to GloVe word embeddings, which are kept frozen during training. We combine these ELM O and GloVe embeddings via concatenation. As a sanity check, we also include a most frequent class (MFC) baseline. The MFC baseline assigns the most frequent class label in the training split to every utterance u0 in the test split for both DA and IC tasks. To adapt the MFC baseline to SL, we compute the most frequent slot label MFC(w) for each word type w in the training set. Then given a test utterance u0 , we assign the pre-computed, most frequent slot MFC(w0 ) to each word w0 ∈ u0"
D19-1460,W13-4067,0,0.0130692,"ng NLU and related tasks as they require large, varied, and ideally human-generated datasets. Moreover, recent work (Dong et al., 2015; Devlin et al., 2018) has shown the benefit of applying joint-training and transfer learning techniques to natural language processing tasks. However, these approaches have yet to become widely used in dialogue tasks, due to a lack of largescale datasets. Furthermore, the latest state of the art end-to-end neural approaches benefit from such training data even more so than past work on goal-oriented dialogues structured around slot filling (Lemon et al., 2006; Wang and Lemon, 2013). One way to simulate data—and not risk releasing personally identifying information—for a domain is to use a Wizard-of-Oz data gathering technique, which requires that participants in a conver4526 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4526–4536, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Role Turn A Hey there! Good morning. You’re connected to LMT Airways. How may I help you? C Hi, I wonder if you can confirm my seat assi"
D19-1460,D18-1419,0,0.0922691,"Missing"
fokkens-etal-2012-climb,W05-1522,0,\N,Missing
fokkens-etal-2012-climb,W02-1502,0,\N,Missing
fokkens-etal-2012-climb,C94-2144,0,\N,Missing
fokkens-etal-2012-climb,P98-1033,0,\N,Missing
fokkens-etal-2012-climb,C98-1033,0,\N,Missing
fokkens-etal-2012-climb,P11-1107,1,\N,Missing
fokkens-etal-2012-climb,Y11-1025,1,\N,Missing
fokkens-etal-2012-climb,W11-3404,1,\N,Missing
H05-1074,N04-1025,0,0.181113,"filtering system performance, and handle various data missing situations naturally. 1 Introduction An adaptive personal information filtering system is an autonomous agent that delivers information to the user in a dynamic environment over a period of time. A common filtering approach is adapting existing text classification/retrieval algorithms to classify incoming documents as either relevant or non relevant using user profiles learned from explicit user feedback on documents the user has seen. However, there are other important criteria for the user besides relevance, such as readability (Collins-Thompson and Callan, 2004), novelty (Harman, 2003), and authority (Kleinberg, 1998). Besides, much information about the user and the document can be collected by a filtering system. These suggest a way to improve the current filtering system: going beyond relevance and using multiple forms of evidence. ∗ This research was done while at the Language Technologies Institute, Carnegie Mellon University. On the other hand, researchers have identified three major advantages of graphical modeling approach: 1) it provides inference tools to naturally handle situations of missing data entry because of the conditional dependenc"
I11-1086,J99-2004,0,0.0686756,"rd Prediction Methods Kostadin Cholakov† , Gertjan van Noord† , Valia Kordoni‡ , Yi Zhang‡ † University of Groningen, The Netherlands ‡ Saarland University and DFKI GmbH, Germany {k.cholakov,g.j.m.van.noord}@rug.nl {kordoni,yzhang}@coli.uni-sb.de Abstract first type is based on the concept of supertagging while the second one performs LA. Generally, supertagging refers to the process of applying a sequential tagger to assign lexical descriptions associated with each word in an input string, relative to a given grammar. It was introduced as a means to reduce parsing ambiguity of LTAG grammars (Bangalore and Joshi, 1999), and has since been applied within CCG (Clark, 2002; Clark and Curran, 2004) and HPSG (Dridan et al., 2008; Zhang et al., 2010) grammars. Supertagging has also been employed for dealing with unknown words. However, in such methods, the tagger is used to assign lexical descriptions only to the unknown tokens in a given sentence. It is important to note here that henceforth, we will use the term suppertagging in this narrow sense of tagging unknown words only. Supertagging methods often work online. The unknown words are assigned lexical entries when they are encountered in the input during par"
I11-1086,P98-1014,0,0.0567531,"ncreases parsing accuracy compared to the baseline. This difference in quality might not always be crucial since less accurate parses produced by the grammar can still be used successfully in many NLP applications. In such cases, the less complex supertagging methods might be the preferred choice. However, through a small sentence realisation experiment, we give an example of an application where high-quality LA is a prerequisite. Other kinds of LA techniques have also been proposed. Cussens and Pulman (2000) used a symbolic approach employing inductive logic programming, while Erbach (1990), Barg and Walther (1998) and Fouvry (2003) followed a unificationbased approach. However, it is doubtful if those methods are scalable since they have not been applied to large-scale grammars and no meaningful evaluation has been provided. The remainder of the paper is organised as follows. Section 2 describes the resources we employ. Section 3 gives an overview of the supertagging methods previously applied with the GG. Section 4 describes the adaptation of the C& V N method to the GG. Section 5 gives details on the training procedure for the ME-based classifier used in the C& V N technique. Section 6 evaluates the"
I11-1086,W00-0740,0,0.0124041,"of having lower accuracy than the baseline. The application of the adapted C& V N method, on the other hand, increases parsing accuracy compared to the baseline. This difference in quality might not always be crucial since less accurate parses produced by the grammar can still be used successfully in many NLP applications. In such cases, the less complex supertagging methods might be the preferred choice. However, through a small sentence realisation experiment, we give an example of an application where high-quality LA is a prerequisite. Other kinds of LA techniques have also been proposed. Cussens and Pulman (2000) used a symbolic approach employing inductive logic programming, while Erbach (1990), Barg and Walther (1998) and Fouvry (2003) followed a unificationbased approach. However, it is doubtful if those methods are scalable since they have not been applied to large-scale grammars and no meaningful evaluation has been provided. The remainder of the paper is organised as follows. Section 2 describes the resources we employ. Section 3 gives an overview of the supertagging methods previously applied with the GG. Section 4 describes the adaptation of the C& V N method to the GG. Section 5 gives details"
I11-1086,P08-1070,1,0.93808,"en, The Netherlands ‡ Saarland University and DFKI GmbH, Germany {k.cholakov,g.j.m.van.noord}@rug.nl {kordoni,yzhang}@coli.uni-sb.de Abstract first type is based on the concept of supertagging while the second one performs LA. Generally, supertagging refers to the process of applying a sequential tagger to assign lexical descriptions associated with each word in an input string, relative to a given grammar. It was introduced as a means to reduce parsing ambiguity of LTAG grammars (Bangalore and Joshi, 1999), and has since been applied within CCG (Clark, 2002; Clark and Curran, 2004) and HPSG (Dridan et al., 2008; Zhang et al., 2010) grammars. Supertagging has also been employed for dealing with unknown words. However, in such methods, the tagger is used to assign lexical descriptions only to the unknown tokens in a given sentence. It is important to note here that henceforth, we will use the term suppertagging in this narrow sense of tagging unknown words only. Supertagging methods often work online. The unknown words are assigned lexical entries when they are encountered in the input during parsing. Therefore, the focus is primarily on improving the parsing coverage and accuracy of the grammar for a"
I11-1086,W06-1620,0,0.0154973,"s applied with the GG and compares the results to the results reported previously for the suppertagging methods for this corpus. Section 7 explores the possibility of using newly acquired lexical entries in a small sentence realisation task. Section 8 concludes the paper. evaluated in terms of type precision and type recall. For a given LA method, type precision indicates the proportion of correctly predicted lexical entries and type recall indicates how many of the correct lexical entries for a given word are actually found. Both supertagging and LA have been successfully used. For instance, Blunsom and Baldwin (2006) employ a conditional random fieldbased tagger to predict lexical entries for largescale HPSG grammars of English (ERG; (Copestake and Flickinger, 2000)) and Japanese (JACY; (Siegel and Bender, 2002)). Zhang and Kordoni (2006) and Dridan et al. (2008) have developed a maximum entropy-based (ME) tagger and investigate the effect its application has on the parsing performance of the ERG and the GG. Other methods which apply the same tagger to the GG include Nicholson et al. (2008) and Cholakov et al. (2008). The ERG, the GG and JACY are part of the DELPH - IN collaboration1 and as such, they sha"
I11-1086,A00-1031,0,0.0596308,"Missing"
I11-1086,I05-1015,0,0.0592628,"Missing"
I11-1086,R09-1012,1,0.894147,"Missing"
I11-1086,C10-2018,1,0.799003,"Missing"
I11-1086,W08-1708,1,0.695363,"e actually found. Both supertagging and LA have been successfully used. For instance, Blunsom and Baldwin (2006) employ a conditional random fieldbased tagger to predict lexical entries for largescale HPSG grammars of English (ERG; (Copestake and Flickinger, 2000)) and Japanese (JACY; (Siegel and Bender, 2002)). Zhang and Kordoni (2006) and Dridan et al. (2008) have developed a maximum entropy-based (ME) tagger and investigate the effect its application has on the parsing performance of the ERG and the GG. Other methods which apply the same tagger to the GG include Nicholson et al. (2008) and Cholakov et al. (2008). The ERG, the GG and JACY are part of the DELPH - IN collaboration1 and as such, they share the same grammar design and parsing architecture which facilitates the application of the same tagger. Baldwin (2005) presents a LA approach where various secondary resources (POS taggers, chunkers, etc.) are used to create an abstraction of words unknown to the ERG and then binary classifiers are employed to learn lexical entries for those words. However, learning is done based on incomplete information obtained by the various resources used. Further, no evaluation of the effect the method has on the"
I11-1086,C04-1041,0,0.034145,"Yi Zhang‡ † University of Groningen, The Netherlands ‡ Saarland University and DFKI GmbH, Germany {k.cholakov,g.j.m.van.noord}@rug.nl {kordoni,yzhang}@coli.uni-sb.de Abstract first type is based on the concept of supertagging while the second one performs LA. Generally, supertagging refers to the process of applying a sequential tagger to assign lexical descriptions associated with each word in an input string, relative to a given grammar. It was introduced as a means to reduce parsing ambiguity of LTAG grammars (Bangalore and Joshi, 1999), and has since been applied within CCG (Clark, 2002; Clark and Curran, 2004) and HPSG (Dridan et al., 2008; Zhang et al., 2010) grammars. Supertagging has also been employed for dealing with unknown words. However, in such methods, the tagger is used to assign lexical descriptions only to the unknown tokens in a given sentence. It is important to note here that henceforth, we will use the term suppertagging in this narrow sense of tagging unknown words only. Supertagging methods often work online. The unknown words are assigned lexical entries when they are encountered in the input during parsing. Therefore, the focus is primarily on improving the parsing coverage and"
I11-1086,copestake-flickinger-2000-open,0,0.0436578,"the possibility of using newly acquired lexical entries in a small sentence realisation task. Section 8 concludes the paper. evaluated in terms of type precision and type recall. For a given LA method, type precision indicates the proportion of correctly predicted lexical entries and type recall indicates how many of the correct lexical entries for a given word are actually found. Both supertagging and LA have been successfully used. For instance, Blunsom and Baldwin (2006) employ a conditional random fieldbased tagger to predict lexical entries for largescale HPSG grammars of English (ERG; (Copestake and Flickinger, 2000)) and Japanese (JACY; (Siegel and Bender, 2002)). Zhang and Kordoni (2006) and Dridan et al. (2008) have developed a maximum entropy-based (ME) tagger and investigate the effect its application has on the parsing performance of the ERG and the GG. Other methods which apply the same tagger to the GG include Nicholson et al. (2008) and Cholakov et al. (2008). The ERG, the GG and JACY are part of the DELPH - IN collaboration1 and as such, they share the same grammar design and parsing architecture which facilitates the application of the same tagger. Baldwin (2005) presents a LA approach where va"
I11-1086,E03-1041,0,0.0230605,"mpared to the baseline. This difference in quality might not always be crucial since less accurate parses produced by the grammar can still be used successfully in many NLP applications. In such cases, the less complex supertagging methods might be the preferred choice. However, through a small sentence realisation experiment, we give an example of an application where high-quality LA is a prerequisite. Other kinds of LA techniques have also been proposed. Cussens and Pulman (2000) used a symbolic approach employing inductive logic programming, while Erbach (1990), Barg and Walther (1998) and Fouvry (2003) followed a unificationbased approach. However, it is doubtful if those methods are scalable since they have not been applied to large-scale grammars and no meaningful evaluation has been provided. The remainder of the paper is organised as follows. Section 2 describes the resources we employ. Section 3 gives an overview of the supertagging methods previously applied with the GG. Section 4 describes the adaptation of the C& V N method to the GG. Section 5 gives details on the training procedure for the ME-based classifier used in the C& V N technique. Section 6 evaluates the parsing coverage a"
I11-1086,J03-3001,0,0.0370393,"Missing"
I11-1086,W02-2018,0,0.0211703,"of search hits Yahoo! returns for each form in a given paradigm is combined with some simple heuristics to disambiguate the output of the morphology and to determine the correct paradigm(s). We also apply heuristics to guess the gender for words with generated noun paradigms and to determine if a word which is assigned a verb paradigm starts with a separable particle. (1) p(t|c) = P Θi fi (t,c)) P exp( i P ′ t′ ∈T exp( i Θi fi (t ,c)) where fi (t, c) may encode arbitrary features from the context and < Θ1 , Θ2 , ... > can be evaluated by maximising the pseudo-likelihood on a training corpus (Malouf, 2002). Table 2 shows the features for Aufgabe. Since the stem of the unknown word is added to the lexicon, we also experimented with prefix and suffix features extracted from the stem. We assumed that those could allow for a better generalization of morphological properties but they proved to be less informative for LA. One could argue that there is a simpler approach for mapping the various forms of the unknown word to its stem. For instance, the TreeTagger provides both POS and stem information with high accuracy. However, the generation of the paradigms allows us to consider contexts in which ot"
I11-1086,nicholson-etal-2008-evaluating,1,0.870798,"entries for a given word are actually found. Both supertagging and LA have been successfully used. For instance, Blunsom and Baldwin (2006) employ a conditional random fieldbased tagger to predict lexical entries for largescale HPSG grammars of English (ERG; (Copestake and Flickinger, 2000)) and Japanese (JACY; (Siegel and Bender, 2002)). Zhang and Kordoni (2006) and Dridan et al. (2008) have developed a maximum entropy-based (ME) tagger and investigate the effect its application has on the parsing performance of the ERG and the GG. Other methods which apply the same tagger to the GG include Nicholson et al. (2008) and Cholakov et al. (2008). The ERG, the GG and JACY are part of the DELPH - IN collaboration1 and as such, they share the same grammar design and parsing architecture which facilitates the application of the same tagger. Baldwin (2005) presents a LA approach where various secondary resources (POS taggers, chunkers, etc.) are used to create an abstraction of words unknown to the ERG and then binary classifiers are employed to learn lexical entries for those words. However, learning is done based on incomplete information obtained by the various resources used. Further, no evaluation of the ef"
I11-1086,W02-1210,0,0.0115253,"es in a small sentence realisation task. Section 8 concludes the paper. evaluated in terms of type precision and type recall. For a given LA method, type precision indicates the proportion of correctly predicted lexical entries and type recall indicates how many of the correct lexical entries for a given word are actually found. Both supertagging and LA have been successfully used. For instance, Blunsom and Baldwin (2006) employ a conditional random fieldbased tagger to predict lexical entries for largescale HPSG grammars of English (ERG; (Copestake and Flickinger, 2000)) and Japanese (JACY; (Siegel and Bender, 2002)). Zhang and Kordoni (2006) and Dridan et al. (2008) have developed a maximum entropy-based (ME) tagger and investigate the effect its application has on the parsing performance of the ERG and the GG. Other methods which apply the same tagger to the GG include Nicholson et al. (2008) and Cholakov et al. (2008). The ERG, the GG and JACY are part of the DELPH - IN collaboration1 and as such, they share the same grammar design and parsing architecture which facilitates the application of the same tagger. Baldwin (2005) presents a LA approach where various secondary resources (POS taggers, chunker"
I11-1086,2006.jeptalnrecital-invite.2,1,0.833811,"Missing"
I11-1086,zhang-kordoni-2006-automated,1,0.955308,"isation task. Section 8 concludes the paper. evaluated in terms of type precision and type recall. For a given LA method, type precision indicates the proportion of correctly predicted lexical entries and type recall indicates how many of the correct lexical entries for a given word are actually found. Both supertagging and LA have been successfully used. For instance, Blunsom and Baldwin (2006) employ a conditional random fieldbased tagger to predict lexical entries for largescale HPSG grammars of English (ERG; (Copestake and Flickinger, 2000)) and Japanese (JACY; (Siegel and Bender, 2002)). Zhang and Kordoni (2006) and Dridan et al. (2008) have developed a maximum entropy-based (ME) tagger and investigate the effect its application has on the parsing performance of the ERG and the GG. Other methods which apply the same tagger to the GG include Nicholson et al. (2008) and Cholakov et al. (2008). The ERG, the GG and JACY are part of the DELPH - IN collaboration1 and as such, they share the same grammar design and parsing architecture which facilitates the application of the same tagger. Baldwin (2005) presents a LA approach where various secondary resources (POS taggers, chunkers, etc.) are used to create"
I11-1086,N10-1090,0,0.0140834,"Saarland University and DFKI GmbH, Germany {k.cholakov,g.j.m.van.noord}@rug.nl {kordoni,yzhang}@coli.uni-sb.de Abstract first type is based on the concept of supertagging while the second one performs LA. Generally, supertagging refers to the process of applying a sequential tagger to assign lexical descriptions associated with each word in an input string, relative to a given grammar. It was introduced as a means to reduce parsing ambiguity of LTAG grammars (Bangalore and Joshi, 1999), and has since been applied within CCG (Clark, 2002; Clark and Curran, 2004) and HPSG (Dridan et al., 2008; Zhang et al., 2010) grammars. Supertagging has also been employed for dealing with unknown words. However, in such methods, the tagger is used to assign lexical descriptions only to the unknown tokens in a given sentence. It is important to note here that henceforth, we will use the term suppertagging in this narrow sense of tagging unknown words only. Supertagging methods often work online. The unknown words are assigned lexical entries when they are encountered in the input during parsing. Therefore, the focus is primarily on improving the parsing coverage and accuracy of the grammar for a particular input. Th"
I11-1086,W05-1008,0,\N,Missing
I11-1086,C98-1014,0,\N,Missing
K19-1075,N15-1020,0,0.0702975,"Missing"
K19-1075,P16-1230,0,0.0317633,"te that our model can generate higher-quality, more diverse and goalfocused dialogues. In addition, we leverage goaloriented dialogue generation as data augmentation for task-oriented dialogue systems, with better performance achieved. 2 Task-Oriented Dialogue Systems. Conventional task-oriented dialog systems entails a sophisticated pipeline (Raux et al., 2005; Young et al., 2013) with components including spoken language understanding (Chen et al., 2016; Mesnil et al., 2015; Gupta et al., 2019), dialog state tracking (Henderson et al., 2014; Mrksic et al., 2017), and dialog policy learning (Su et al., 2016; Gaˇsi´c and Young, 2014). Building a task-oriented dialogue agent via end-to-end approaches has been explored recently (Li et al., 2017b; Wen et al., 2017). Although several conversational datasets are published recently (Gopalakrishnan et al., 2019; Henderson et al., 2019), the scarcity of annotated conversational data remains a key problem when developing a dialog system. This motivates us to model task-oriented dialogues with goal information in order to achieve controlled dialogue generation for data augmentation. Related Work Dialogues are sequences of utterances, which are sequences of"
K19-1075,E17-1042,0,0.0859893,"gmentation for task-oriented dialogue systems, with better performance achieved. 2 Task-Oriented Dialogue Systems. Conventional task-oriented dialog systems entails a sophisticated pipeline (Raux et al., 2005; Young et al., 2013) with components including spoken language understanding (Chen et al., 2016; Mesnil et al., 2015; Gupta et al., 2019), dialog state tracking (Henderson et al., 2014; Mrksic et al., 2017), and dialog policy learning (Su et al., 2016; Gaˇsi´c and Young, 2014). Building a task-oriented dialogue agent via end-to-end approaches has been explored recently (Li et al., 2017b; Wen et al., 2017). Although several conversational datasets are published recently (Gopalakrishnan et al., 2019; Henderson et al., 2019), the scarcity of annotated conversational data remains a key problem when developing a dialog system. This motivates us to model task-oriented dialogues with goal information in order to achieve controlled dialogue generation for data augmentation. Related Work Dialogues are sequences of utterances, which are sequences of words. For modeling or generating dialogues, hierarchical architectures are usually used to capture their conversational nature. Traditionally, language mod"
K19-1075,D18-1432,0,0.0347481,"Missing"
K19-1075,P17-1061,0,0.0387875,"Missing"
K19-1075,P02-1040,0,\N,Missing
K19-1075,W14-4340,0,\N,Missing
K19-1075,D16-1127,0,\N,Missing
K19-1075,D17-1230,0,\N,Missing
K19-1075,N18-1162,0,\N,Missing
K19-1075,P17-1163,0,\N,Missing
K19-2013,hajic-etal-2012-announcing,0,0.148422,"Missing"
K19-2013,P13-1023,0,0.143795,"Missing"
K19-2013,S19-2001,0,0.0752617,"ture parsing for one of the phrasalanchoring MRs, UCCA. Our system submission ranked 1st in the AMR subtask, and later improvements shows promising results on other frameworks as well. 1 Introduction The design and implementation of broad-coverage and linguistically motivated meaning representation frameworks for natural language is attracting growing attention in recent years. With the advent of deep neural network-based machine learning techniques, we have made significant progress to automatically parse sentences intro structured meaning representation (Oepen et al., 2014, 2015; May, 2016; Hershcovich et al., 2019). Moreover, the differences between various representation frameworks has a significant impact on the design and performance of the parsing systems. Due to the abstract nature of semantics, there is a diverse set of meaning representation frameworks in the literature (Abend and Rappoport, 2017). In some application scenario, tasks-specific formal representations such as database queries and arithmetic formula have also been proposed. However, primarily the study in computational semantics focuses on frameworks that are theoretically grounded on formal semantic theories, and ∗ 2 Anchoring in Me"
K19-2013,P17-1008,0,0.0163058,"ation frameworks for natural language is attracting growing attention in recent years. With the advent of deep neural network-based machine learning techniques, we have made significant progress to automatically parse sentences intro structured meaning representation (Oepen et al., 2014, 2015; May, 2016; Hershcovich et al., 2019). Moreover, the differences between various representation frameworks has a significant impact on the design and performance of the parsing systems. Due to the abstract nature of semantics, there is a diverse set of meaning representation frameworks in the literature (Abend and Rappoport, 2017). In some application scenario, tasks-specific formal representations such as database queries and arithmetic formula have also been proposed. However, primarily the study in computational semantics focuses on frameworks that are theoretically grounded on formal semantic theories, and ∗ 2 Anchoring in Meaning Representation The 2019 Conference on Computational Language Learning (CoNLL) hosted a shared task on 1 The code is available online at https://github.com/ utahnlp/lapa-mrp Work done when Jie Cao was an intern at AWS AI 138 Proceedings of the Shared Task on Cross-Framework Meaning Represe"
K19-2013,K19-2002,0,0.298565,"eakdown of AMR, DM, PSD and UCCA respectively. Each column in the table shows the F1 score of each subcomponent in a graph: top nodes, node lables, node properties, node anchors, edge labels, and overall F1 score. No anchors for AMR, and no node label and propertis for UCCA. We show the results of MRP metric on two datasets. “all” denotes all the examples for that specific MR, while lpps are a set of 100 sentences from The Little Prince, and annotated in all five meaning representations. To better understand the performance, we also reported the official results from two baseline models TUPA (Hershcovich and Arviv, 2019) and ERG (Oepen and Flickinger, 2019). Results At the time of official evaluation, we submitted three lexical anchoring parser, and then we submitted another phrasal-anchoring model for UCCA parsing during post-evaluation stage, and we leave EDS parsing as future work. The following sections are the official results and error breakdowns for lexical-anchoring and phrasalanchoring respectively. Official Results on Lexical Anchoring Table 2 shows the official results for our lexical-anchoring models on AMR, DM, PSD. By using our latent alignment based AMR parser, our system ranked top 1 in the AM"
K19-2013,W13-2322,0,0.234128,"Missing"
K19-2013,W12-3602,0,0.444902,"Missing"
K19-2013,J93-2003,0,0.0765145,"to abstract the meaning representation away from the surface token. The absense of explicit anchoring can present difficulties for parsing. In this section, by extensive analysis on previous work AMR alignments, we show that AMR nodes can be implicitly aligned to the leixical tokens in a sentence. AMR-to-String Alignments A straightforward solution to find the missing anchoring in an AMR Graph is to align it with a sentence; We denote it as AMR-to-String alignment. ISI alignments (Pourdamghani et al., 2014) first linearizes the AMR graph into a sequence, and then use IBM word alignment model (Brown et al., 1993) to align the lin139 _almost_a_1 23:29 ARG1 comp 2:9 _impossible_a_for 30:40 ARG1 ARG1 _similar_a_to 2:9 _a_q 0:1 ARG1 BV _apply_v_to 44:49 ARG2 _technique_n_1 10:19 udef_q 53:100 ARG3 _other_a_1 53:58 BV ARG1 _crop_n_1 59:65 _such+as_p 66:73 ARG1 udef_q 74:100 ARG2 udef_q 74:81 BV _cotton_n_1 74:81 BV implicit_conj 82:100 udef_q 82:100 L-INDEX R-INDEX BV udef_q 82:90 BV _and_c 91:94 L-INDEX _soybean_n_unknown 82:90 udef_q 95:100 R-INDEX BV _rice_n_1 95:100 Figure 1: Phrasal-anchoring in EDS[wsj#0209013], for the sentence &quot;A similar technique is almost impossible to apply to other crops, such"
K19-2013,K19-2007,0,0.175555,"cial results on DM and PSD shows that there is still around 2.5 points performance gap between our model and the top 1 model. data all lpps all lpps all Ours(1) lpps all Top 2 lpps TUPA single TUPA multi tops 63.95 71.96 61.30 72.63 65.92 72.00 78.15 83.00 labels 57.20 55.52 39.80 50.11 82.86 78.71 82.51 76.24 prop 22.31 26.42 27.70 20.25 77.26 58.93 71.33 51.79 edges 36.41 36.38 27.35 33.12 63.57 63.96 63.21 60.43 all 44.73 47.04 33.75 43.38 73.38 71.11 72.94 69.03 Table 4: Our parser on AMR ranked 1st. This table shows the error breakdown when comparing to the baseline TUPA model and top 2 (Che et al., 2019) in official results Official Results on Phrasal Anchoring Table 3 shows that our span-based CKY model for UCCA 145 data all ERG lpps all Top 1 lpps all Ours(7) lpps tops 91.83 95.00 93.23 96.48 70.95 84.00 labels 98.22 97.32 94.14 91.85 93.96 90.55 prop 95.25 97.75 94.83 94.36 92.13 91.91 anchors 98.82 99.46 98.40 99.04 97.25 97.96 edges 90.76 92.71 91.55 93.28 86.45 87.24 all 95.65 97.03 94.76 94.64 92.14 91.82 data all TUPA single lpps all TUPA multi lpps all (Che et al., 2019) lpps all Ours(*5) lpps all Ours + ELMo lpps Table 5: Our parser on DM ranked 7th. This table shows the error break"
K19-2013,S19-2002,0,0.118323,"Hence, we designed a copy mechanism (Luong et al., 2014) in our neural network architecture to decide whether to copying deterministic label given a word or estimate a classification probability from a fixed label set. 3.1.3 3.2 Let us now see our phrasal-anchoring parser for UCCA. We introduce the transformation we used to reduce UCCA parsing into a consituent parsing task, and finally introduce the detailed CKY model for the constituent parsing. 3.2.1 Graph-to-CT Transformation We propose to transform a graph into a constituent tree structure for parsing, which is also used in recent work (Jiang et al., 2019). Figure 3 shows an example of transforming a UCCA graph into a constituent tree. The primary transformation assigns the original label of an edge to its child node. Then to make it compatible with parsers for standard PennTree Bank format, we add some auxiliary nodes such as special non-terminal nodes, TOP, HEAD, and special terminal nodes TOKEN and MWE. We remove all the “remote” annotation in UCCA since the constituent tree structure does not support reentrance. A fully compatible transformation should support both graph-to-tree and tree-to-graph transformation. In our case, due to time con"
K19-2013,E17-1053,0,0.0133697,"chool of Computing, University of Utah ‡ AWS AI, Amazon {jcao, svivek}@cs.utah.edu, {yizhngn, adel}@amazon.com Abstract sometimes also with assumptions on underlying syntactic structures. Anchoring is crucial in graph-based meaning representation parsing. Training a statistical parser typically starts with a conjectured alignment between tokens/spans and the semantic graph nodes to help to factorize the supervision of graph structure into nodes and edges. In our paper, with evidence from previous research on AMR alignments (Pourdamghani et al., 2014; Flanigan et al., 2014; Wang and Xue, 2017; Chen and Palmer, 2017; Szubert et al., 2018; Lyu and Titov, 2018), we propose a uniform handling of three meaning representations from Flavor-0 (DM, PSD) and Flavor-2 (AMR) into a new group referred to as the lexical-anchoring MRs. It supports both explicit and implicit anchoring of semantic concepts to tokens. The other two meaning representations from Flavor-1 (EDS, UCCA) is referred to the group of phrasal-anchoring MRs where the semantic concepts are anchored to phrases as well. To support the simplified taxonomy, we named our parser as LAPA (Lexical-Anchoring and Phrasal-Anchoring)1 . We proposed a graph-base"
K19-2013,P18-1249,0,0.119926,"ontinuous tokens to its nearest continuous parent nodes, we force every sub span are continuous in the transformed trees. We leave the postprocessing to recover those discontinuous as future work. For inference, given an input sentence, we first use the trained constituent tree parsing model to parse it into a tree, and then we transform a tree back into a directed graph by assigning the edge label as its child’s node label, and deleting those auxiliary labels, adding anchors to every remaining node. we use 8-layers with 8 heads transformer encoder, which shows better performance than LSTM in Kitaev and Klein (2018). Tree Factorization In the graph-to-tree transformation, we move the edge label to its child node. By assuming the labels for each node are independent, we factorize the tree structure prediction as independent span-label prediction as Equation 4. However, this assumption does not hold for UCCA. Please see more error analysis in §4.4 T ∗ = arg maxs(T ) T X s(i, j, l) s(T ) = (4) (i,j,l)∈T CKY Parsing By assuming the label prediction is independent of the splitting point, we can further factorize the whole tree as the following dynamic programming in Equation 5. sbest (i, i + 1) = maxs(i, i +"
K19-2013,P17-1014,0,0.0838003,"Missing"
K19-2013,K19-2004,0,0.281217,"Missing"
K19-2013,P14-1134,0,0.601467,"hang‡ , Adel Youssef‡ , Vivek Srikumar† † School of Computing, University of Utah ‡ AWS AI, Amazon {jcao, svivek}@cs.utah.edu, {yizhngn, adel}@amazon.com Abstract sometimes also with assumptions on underlying syntactic structures. Anchoring is crucial in graph-based meaning representation parsing. Training a statistical parser typically starts with a conjectured alignment between tokens/spans and the semantic graph nodes to help to factorize the supervision of graph structure into nodes and edges. In our paper, with evidence from previous research on AMR alignments (Pourdamghani et al., 2014; Flanigan et al., 2014; Wang and Xue, 2017; Chen and Palmer, 2017; Szubert et al., 2018; Lyu and Titov, 2018), we propose a uniform handling of three meaning representations from Flavor-0 (DM, PSD) and Flavor-2 (AMR) into a new group referred to as the lexical-anchoring MRs. It supports both explicit and implicit anchoring of semantic concepts to tokens. The other two meaning representations from Flavor-1 (EDS, UCCA) is referred to the group of phrasal-anchoring MRs where the semantic concepts are anchored to phrases as well. To support the simplified taxonomy, we named our parser as LAPA (Lexical-Anchoring and Phr"
K19-2013,P18-1037,0,0.231257,"AI, Amazon {jcao, svivek}@cs.utah.edu, {yizhngn, adel}@amazon.com Abstract sometimes also with assumptions on underlying syntactic structures. Anchoring is crucial in graph-based meaning representation parsing. Training a statistical parser typically starts with a conjectured alignment between tokens/spans and the semantic graph nodes to help to factorize the supervision of graph structure into nodes and edges. In our paper, with evidence from previous research on AMR alignments (Pourdamghani et al., 2014; Flanigan et al., 2014; Wang and Xue, 2017; Chen and Palmer, 2017; Szubert et al., 2018; Lyu and Titov, 2018), we propose a uniform handling of three meaning representations from Flavor-0 (DM, PSD) and Flavor-2 (AMR) into a new group referred to as the lexical-anchoring MRs. It supports both explicit and implicit anchoring of semantic concepts to tokens. The other two meaning representations from Flavor-1 (EDS, UCCA) is referred to the group of phrasal-anchoring MRs where the semantic concepts are anchored to phrases as well. To support the simplified taxonomy, we named our parser as LAPA (Lexical-Anchoring and Phrasal-Anchoring)1 . We proposed a graph-based parsing framework with a latent-alignment"
K19-2013,S16-1166,0,0.170913,"hrase-structure parsing for one of the phrasalanchoring MRs, UCCA. Our system submission ranked 1st in the AMR subtask, and later improvements shows promising results on other frameworks as well. 1 Introduction The design and implementation of broad-coverage and linguistically motivated meaning representation frameworks for natural language is attracting growing attention in recent years. With the advent of deep neural network-based machine learning techniques, we have made significant progress to automatically parse sentences intro structured meaning representation (Oepen et al., 2014, 2015; May, 2016; Hershcovich et al., 2019). Moreover, the differences between various representation frameworks has a significant impact on the design and performance of the parsing systems. Due to the abstract nature of semantics, there is a diverse set of meaning representation frameworks in the literature (Abend and Rappoport, 2017). In some application scenario, tasks-specific formal representations such as database queries and arithmetic formula have also been proposed. However, primarily the study in computational semantics focuses on frameworks that are theoretically grounded on formal semantic theori"
K19-2013,N18-1202,0,0.0213396,"Adam (Kingma and Ba, 2014), using a batch size 64 for a graph-based model, and 250 for CKY-based model. Hyperparameters were tuned on the development set, based on labeled F1 between two graphs. We exploit early-stopping to avoid over-fitting. 4.3 MR AMR(1) PSD(6) DM(7) Ours (P/R/F1) 75/71/73.38 89/89/88.75 93/92/92.14 Top 1/3/5 (F1) 73.38/71.97/71.72 90.76/89.91/88.77 94.76/94.32/93.74 Table 2: Official results overview on unified MRP metric, we selected the performance from top 1/3/5 system(s) for comparison can achieve 74.00 F1 score on official test set, and ranked 5th. When adding ELMo (Peters et al., 2018) into our model, it can further improve almost 3 points on it. MR UCCA(5) EDS Ours (P/R/F1) 80.83/73.42/76.94 N/A Top 1/3/5 (F1) 81.67/77.80/73.22 94.47/90.75/89.10 Table 3: Official results overview on unified MRP metric, we selected the performance from top 1/3/5 system(s) for comparison. It shows our UCCA model for post-evluation can rank 5th 4.4 Error Breakdown Table 4, 5, 6 and 7 shows the detailed error breakdown of AMR, DM, PSD and UCCA respectively. Each column in the table shows the F1 score of each subcomponent in a graph: top nodes, node lables, node properties, node anchors, edge l"
K19-2013,D14-1048,0,0.63661,"l Anchoring Jie Cao†∗, Yi Zhang‡ , Adel Youssef‡ , Vivek Srikumar† † School of Computing, University of Utah ‡ AWS AI, Amazon {jcao, svivek}@cs.utah.edu, {yizhngn, adel}@amazon.com Abstract sometimes also with assumptions on underlying syntactic structures. Anchoring is crucial in graph-based meaning representation parsing. Training a statistical parser typically starts with a conjectured alignment between tokens/spans and the semantic graph nodes to help to factorize the supervision of graph structure into nodes and edges. In our paper, with evidence from previous research on AMR alignments (Pourdamghani et al., 2014; Flanigan et al., 2014; Wang and Xue, 2017; Chen and Palmer, 2017; Szubert et al., 2018; Lyu and Titov, 2018), we propose a uniform handling of three meaning representations from Flavor-0 (DM, PSD) and Flavor-2 (AMR) into a new group referred to as the lexical-anchoring MRs. It supports both explicit and implicit anchoring of semantic concepts to tokens. The other two meaning representations from Flavor-1 (EDS, UCCA) is referred to the group of phrasal-anchoring MRs where the semantic concepts are anchored to phrases as well. To support the simplified taxonomy, we named our parser as LAPA (Le"
K19-2013,S14-2056,0,0.368437,"Missing"
K19-2013,D15-1136,0,0.0535062,"Missing"
K19-2013,K19-2001,0,0.128815,"Missing"
K19-2013,N18-1106,0,0.0280505,"Missing"
K19-2013,D17-1129,0,0.298612,"Vivek Srikumar† † School of Computing, University of Utah ‡ AWS AI, Amazon {jcao, svivek}@cs.utah.edu, {yizhngn, adel}@amazon.com Abstract sometimes also with assumptions on underlying syntactic structures. Anchoring is crucial in graph-based meaning representation parsing. Training a statistical parser typically starts with a conjectured alignment between tokens/spans and the semantic graph nodes to help to factorize the supervision of graph structure into nodes and edges. In our paper, with evidence from previous research on AMR alignments (Pourdamghani et al., 2014; Flanigan et al., 2014; Wang and Xue, 2017; Chen and Palmer, 2017; Szubert et al., 2018; Lyu and Titov, 2018), we propose a uniform handling of three meaning representations from Flavor-0 (DM, PSD) and Flavor-2 (AMR) into a new group referred to as the lexical-anchoring MRs. It supports both explicit and implicit anchoring of semantic concepts to tokens. The other two meaning representations from Flavor-1 (EDS, UCCA) is referred to the group of phrasal-anchoring MRs where the semantic concepts are anchored to phrases as well. To support the simplified taxonomy, we named our parser as LAPA (Lexical-Anchoring and Phrasal-Anchoring)1 . W"
K19-2013,K19-2003,0,0.104484,"ctively. Each column in the table shows the F1 score of each subcomponent in a graph: top nodes, node lables, node properties, node anchors, edge labels, and overall F1 score. No anchors for AMR, and no node label and propertis for UCCA. We show the results of MRP metric on two datasets. “all” denotes all the examples for that specific MR, while lpps are a set of 100 sentences from The Little Prince, and annotated in all five meaning representations. To better understand the performance, we also reported the official results from two baseline models TUPA (Hershcovich and Arviv, 2019) and ERG (Oepen and Flickinger, 2019). Results At the time of official evaluation, we submitted three lexical anchoring parser, and then we submitted another phrasal-anchoring model for UCCA parsing during post-evaluation stage, and we leave EDS parsing as future work. The following sections are the official results and error breakdowns for lexical-anchoring and phrasalanchoring respectively. Official Results on Lexical Anchoring Table 2 shows the official results for our lexical-anchoring models on AMR, DM, PSD. By using our latent alignment based AMR parser, our system ranked top 1 in the AMR subtask, and outperformed the top 5"
K19-2013,P15-2141,0,0.147739,"Missing"
K19-2013,S15-2153,0,0.353314,"Missing"
K19-2013,P19-1009,0,0.115507,"Missing"
K19-2013,S14-2008,1,0.896776,"Missing"
K19-2013,K15-1004,0,0.127265,"Missing"
kordoni-zhang-2010-disambiguating,A00-2021,0,\N,Missing
kordoni-zhang-2010-disambiguating,J93-2004,0,\N,Missing
kordoni-zhang-2010-disambiguating,W07-2201,0,\N,Missing
kordoni-zhang-2010-disambiguating,C02-2025,0,\N,Missing
kordoni-zhang-2010-disambiguating,P08-1039,0,\N,Missing
kordoni-zhang-2010-disambiguating,P07-1031,0,\N,Missing
krieger-etal-2014-information,P03-1054,0,\N,Missing
krieger-etal-2014-information,P07-1074,1,\N,Missing
krieger-etal-2014-information,P99-1052,0,\N,Missing
L18-1325,W02-1001,0,0.0865208,"n tag and pos tag can Evaluation Evaluation Metrics Simple Heuristic Baseline System The simple heuristic system means always choosing initial characters of words in the segmented full form. This is because the most natural abbreviating heuristic is to produce the first character of each word in the original full form. This is just the simplest baseline. 4.3. Evaluation To study the performance of other machine learning models, we also implement other well known sequential labeling models, including maximum entropy Markov models (MEMMs) (McCallum et al., 2000) and averaged perceptrons (Perc) (Collins, 2002). Besides these traditional models, we also implement a bidirectional LSTM(BLSTM) to evaluate the performance of neural networks on this task. The experimental results are shown in Table 3. In the table, the overall accuracy is most important and it means the final accuracy achieved by the systems in generalized abbreviation prediction with NFFs. For the completeness of experimental information, we also show the discriminate accuracy. The CRF model outperforms the MEMM 2068 and averaged perceptron models. The CRF model achieves best overall character accuracy. BLSTM outperforms other models in"
L18-1325,P15-1033,0,0.0118428,": The number of correct labels (i.e., a classification on a character) generated by the system divided by the total number of characters in the test set. ft = σ(Wf · xt + Uf · ht−1 + bf ) it = σ(Wi · xt + Ui · ht−1 + bi) C˜t = tanh(WC · xt + UC · ht−1 + bC ) Ct = ft ⊗ Ct−1 + it ⊗ C˜t 4.2. (4) ot = σ(Wo · xt + Uo · ht−1 + bo ) ht = ot ⊗ tanh(Ct ) LSTM can solve the long-distance dependencies problem to some extent. However, the LSTM’s hidden state ht takes information only from the past, knowing nothing about the future. An elegant solution whose effectiveness has been proven by previous work (Dyer et al., 2015) is bidirectional LSTM(BLSTM). The basic idea is to present each sequence forwards and backwards to two separate hidden states to capture past and future information, respectively. Then the two hidden states are concatenated to form the final output. In this paper, we employ a bi-directional LSTM, which could capture the contextual information of the current input, to predict the abbreviations of full terms. Since we give a specific segmentation tag and a pos tag for every character, each segmentation tag and pos tag can Evaluation Evaluation Metrics Simple Heuristic Baseline System The simple"
L18-1325,W01-0516,0,0.134562,"an estimate abbreviations of a query, because successful abbreviation prediction may improve the recall of IR systems as Sun et al. (2013a) showed. In addition, Yang et al. (2012) showed that Chinese abbreviation prediction can improve voice-based search quality. 珠穆朗玛峰 (Mount Qomolangma) 奥林匹克运动会 (Olympic Games) 北京大学 (Peking University ) 清华大学 (Tsinghua University ) 黄金市场 (gold market) 珠峰 奥运会 北大 清华 金市 Figure 1: Different cases of generating abbreviations English abbreviations are usually formed as acronyms. Studies for English abbreviation proposed various heuristics for abbreviation prediction (Park and Byrd, 2001; Wren et al., 2002; Schwartz and Hearst, 2002). For example, use of initials, capital letters, syllable boundaries, stop words, etc. These studies performed well for English abbreviations. While Chinese abbreviations are quite different from English ones. Yang et al. (2012) showed that Chinese abbreviations are usually generated by three methods, reduction, elimination, and generalization. Characters are selected from the expanded full name to form the abbreviation. However, there are no general rules to convert a complete term into an abbreviation. As shown in Figure 1, an abbreviation may b"
L18-1325,P09-1102,1,0.862455,"ation expansion, abbreviation recognition, and abbreviation prediction. Expanding the short form of an expression to its full form is called abbreviation expansion. Extracting the short form and full form pairs from the context is called abbreviation recognition. Abbreviation prediction refers to predicting the short form of an expression according to its full form. In this paper, we focus on the last task, i.e., abbreviation prediction. Abbreviation prediction plays an important role in various language processing tasks, because accurate abbreviation prediction will help improve performance. Sun et al. (2009) shows that better abbreviation prediction will help improve the performance of abbreviation recognition. Abbreviation prediction also benefits other tasks. For example, in an information retrieval (IR) system, a large number of the web pages contain only abbreviations. It will be helpful if we can estimate abbreviations of a query, because successful abbreviation prediction may improve the recall of IR systems as Sun et al. (2013a) showed. In addition, Yang et al. (2012) showed that Chinese abbreviation prediction can improve voice-based search quality. 珠穆朗玛峰 (Mount Qomolangma) 奥林匹克运动会 (Olymp"
L18-1325,I13-1073,1,0.806872,"ction. Abbreviation prediction plays an important role in various language processing tasks, because accurate abbreviation prediction will help improve performance. Sun et al. (2009) shows that better abbreviation prediction will help improve the performance of abbreviation recognition. Abbreviation prediction also benefits other tasks. For example, in an information retrieval (IR) system, a large number of the web pages contain only abbreviations. It will be helpful if we can estimate abbreviations of a query, because successful abbreviation prediction may improve the recall of IR systems as Sun et al. (2013a) showed. In addition, Yang et al. (2012) showed that Chinese abbreviation prediction can improve voice-based search quality. 珠穆朗玛峰 (Mount Qomolangma) 奥林匹克运动会 (Olympic Games) 北京大学 (Peking University ) 清华大学 (Tsinghua University ) 黄金市场 (gold market) 珠峰 奥运会 北大 清华 金市 Figure 1: Different cases of generating abbreviations English abbreviations are usually formed as acronyms. Studies for English abbreviation proposed various heuristics for abbreviation prediction (Park and Byrd, 2001; Wren et al., 2002; Schwartz and Hearst, 2002). For example, use of initials, capital letters, syllable boundaries, s"
L18-1325,W05-1304,0,0.036375,"Missing"
L18-1325,N09-2069,0,0.0262024,"abbreviation at all. We usually recognize abbreviations or make abbreviation predictions in the text. Unfortunately, NFFs take up a large portion of Chinese words or phrases in the real world. With the strong noise, distinguishing the full forms with valid abbreviations is more difficult. This undoubtedly increases the difficulty of abbreviation prediction. Many approaches have been proposed in the post studies. Sun et al. (2008) employed Support Vector Regression (SVR) for scoring abbreviation candidates. This method 2065 outperforms the hidden Markov model (HMM) in abbreviation prediction. Yang et al. (2009) proposed to formulate abbreviation generation as a character tagging problem and the conditional random field (CRF) then can be used as the tagging model. Sun et al. (2009) combined latent variable model and global information to predict abbreviations. Zhang et al. (2016) used a recurrent neural networks to predict abbreviations for Chinese named entities. However, most studies of abbreviation prediction focus on positive full form, which means a word has a valid abbreviation. Apparently, this implicit lab assumption is not practical. Nonetheless, we barely see studies that consider NFFs. One"
L18-1325,D14-1147,1,0.846314,"Missing"
L18-1325,D14-1202,1,0.872642,"Missing"
L18-1325,D16-1069,0,0.0123234,"is more difficult. This undoubtedly increases the difficulty of abbreviation prediction. Many approaches have been proposed in the post studies. Sun et al. (2008) employed Support Vector Regression (SVR) for scoring abbreviation candidates. This method 2065 outperforms the hidden Markov model (HMM) in abbreviation prediction. Yang et al. (2009) proposed to formulate abbreviation generation as a character tagging problem and the conditional random field (CRF) then can be used as the tagging model. Sun et al. (2009) combined latent variable model and global information to predict abbreviations. Zhang et al. (2016) used a recurrent neural networks to predict abbreviations for Chinese named entities. However, most studies of abbreviation prediction focus on positive full form, which means a word has a valid abbreviation. Apparently, this implicit lab assumption is not practical. Nonetheless, we barely see studies that consider NFFs. One of the main reasons is the shortage of abbreviation prediction data with NFFs, which is one of the main issues this work tries to solve. Apart from the annotation of a dataset with NFFs, we also conduct a few preprocessing steps to facilitate the usage of the dataset. Chi"
N10-1002,I05-1015,0,0.0145244,"000), but this might also potentially lead to an inconsistent packed parse forest that does not unpack successfully. For chart mining, this means that not all passive edges are directly accessible from the chart. Some of them are packed into others, and the derivatives of the packed edges are not generated. Because of the ambiguity packing, zero or more local analyses may exist for each passive edge on the chart, and the cross-combination of the packed daughter edges is not guaranteed to be compatible. As a result, expensive unification operations must be reapplied during the unpacking phase. Carroll and Oepen (2005) and Zhang et al. (2007b) have proposed efficient k-best unpacking algorithms that can selectively extract the most probable readings from the packed parse forest according to a discriminative parse disambiguation model, by minimising the number of potential unifications. The algorithm can be applied to unpack any passive edges. Because of the dynamic programming used in the algorithm and the hierarchical structure of the edges, the cost of the unpacking routine is empirically linear in the number of desired readings, and O(1) when invoked more than once on the same edge. The other challenge c"
N10-1002,W09-2609,0,0.0339964,"Missing"
N10-1002,P99-1069,0,0.0532038,"ired readings, and O(1) when invoked more than once on the same edge. The other challenge concerns the selection of informative and representative pieces of knowledge from the massive sea of partial analyses in the parsing chart. How to effectively extract the indicative features for a specific language phenomenon is a very task-specific question, as we will show in the context of the VPC extraction task in Section 3.2. However, general strategies can be applied to generate parse ranking scores on each passive edge. The most widely used parse ranking model is the loglinear model (Abney, 1997; Johnson et al., 1999; Toutanova et al., 2002). When the model does not use non-local features, the accumulated score on a sub-tree under a certain (unpacked) passive edge can be used to approximate the probability of the partial analysis conditioned on the sub-string within that span.3 3.2 The Application: Acquiring Features for VPC Extraction As stated above, the target task we use to illustrate the capabilities of our chart mining method is VPC extraction. The grammar we apply our chart mining method to in this paper is the English Resource Grammar (ERG, Flickinger (2002)), a large-scale precision HPSG for Engl"
N10-1002,P99-1061,0,0.00941937,"merican Chapter of the ACL, pages 10–18, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics extract a list of non-compositional VPCs optionally with valence information. For comparison, we parse the same sentence set using a state-of-the-art statistical parser, and extract the VPCs from the parser output. Our results show that our chart mining method produces a model which is superior to the treebank parser. To our knowledge, the only other work that has looked at partial parsing results of precision grammars as a means of linguistic error analysis is that of Kiefer et al. (1999) and Zhang et al. (2007a), where partial parsing models were proposed to select a set of passive edges that together cover the input sequence. Compared to these approaches, our proposed chart mining technique is more general and can be adapted to specific tasks and domains. While we experiment exclusively with an HPSG grammar in this paper, it is important to note that the proposed method can be applied to any grammar formalism which is compatible with chart parsing, and where it is possible to describe an unlexicalised lexical entry for the different categories of lexical item that are to be"
N10-1002,W09-1410,1,0.834536,"tures mined from a partial parsing chart. The proposed technique is shown to outperform a state-of-the-art parser over the target task, despite being based on relatively simplistic features. 1 Introduction Parsing with precision grammars is increasingly achieving broad coverage over open-domain texts for a range of constraint-based frameworks (e.g., TAG, LFG, HPSG and CCG), and is being used in real-world applications including information extraction, question answering, grammar checking and machine translation (Uszkoreit, 2002; Oepen et al., 2004; Frank et al., 2006; Zhang and Kordoni, 2008; MacKinlay et al., 2009). In this context, a “precision grammar” is a grammar which has been engineered to model grammaticality, and contrasts with a treebank-induced grammar, for example. Inevitably, however, such applications demand complete parsing outputs, based on the assumption that the text under investigation will be completely analysable by the grammar. As precision grammars generally make strong assumptions about complete lexical coverage and grammaticality of the input, their utility is limited over noisy or domain-specific data. This lack of complete coverage can make parsing with precision grammars less"
N10-1002,W02-2018,0,0.0128608,"candidate VPC v - le:4, v np le:3, v p le:1, v p-np le:2 LE:M AX C ONS LE:M AX CR ANK PARTICLE off Table 1: Chart mining features used for VPC extraction S3−subjh(.875) S1−subjh(.125) S2−subjh(.925) VP5−hcomp VP1−hadj VP2−hadj(.325) VP3−hcomp v_−_le v_np_le v_p_le VP4−hcomp PP−hcomp v_p−np_le PRTL PREP NP1 NP2 DUMMY−V shows the boy 0 off 2 3 his new toys 4 7 Figure 1: Example of a parsing chart in chart-mining for VPC extraction with the ERG category classification: non-VPC, transitive VPC, or intransitive VPC. For the parameter estimation of the ME model, we use the TADM open source toolkit (Malouf, 2002). The token-level predictions are then combined with a simple majority voting to derive the type-level prediction for the VPC candidate. In the case of a tie, the method backs off to the na¨ıve baseline model described in Section 4.2, which relies on the combined probability of the verb and particle forming a VPC. We have also experimented with other ways of deriving type-level predictions from token-level classification results. For instance, we trained a separate classifier that takes the token-level prediction as input in order to determine the type-level VPC predic14 tion. Our results indi"
N10-1002,A00-2022,0,0.0296662,"que. First, there is potentially a huge number of parsing edges in the chart. For instance, when parsing with a large precision grammar like the HPSG English Resource Grammar (ERG, Flickinger (2002)), it is not unusual for a 20-word sentence to receive over 10,000 passive edges. In order to achieve high efficiency in parsing (as well as generation), ambiguity packing is usually used to reduce the number of productive passive edges on the parsing chart (Tomita, 1985). For constraint-based grammar frameworks like LFG and HPSG, subsumption-based packing is used to achieve a higher packing ratio (Oepen and Carroll, 2000), but this might also potentially lead to an inconsistent packed parse forest that does not unpack successfully. For chart mining, this means that not all passive edges are directly accessible from the chart. Some of them are packed into others, and the derivatives of the packed edges are not generated. Because of the ambiguity packing, zero or more local analyses may exist for each passive edge on the chart, and the cross-combination of the packed daughter edges is not guaranteed to be compatible. As a result, expensive unification operations must be reapplied during the unpacking phase. Carr"
N10-1002,2004.tmi-1.2,0,0.0187101,"Missing"
N10-1002,J93-1007,0,0.167573,"ston and Pullum, 2002; Baldwin and Kim, 2009); for the purposes of our dataset, we assume that all particles are prepositional—by far the most common and productive of the three types—and further restrict our attention to single-particle VPCs (i.e., we ignore VPCs such as get along together). 11 One aspect of VPCs that makes them a particularly challenging target for lexical acquisition is that the verb and particle can be non-contiguous (for instance, hand the paper in and battle right on). This sets them apart from conventional collocations and terminology (cf., Manning and Sch¨utze (1999), Smadja (1993) and McKeown and Radev (2000)) in that they cannot be captured effectively using ngrams, due to their variability in the number and type of words potentially interceding between the verb and the particle. Also, while conventional collocations generally take the form of compound nouns or adjective–noun combinations with relatively simple syntactic structure, VPCs occur with a range of valences. Furthermore, VPCs are highly productive in English and vary in use across domains, making them a prime target for lexical acquisition (Deh´e, 2002; Baldwin, 2005; Baldwin and Kim, 2009). In the VPC datas"
N10-1002,P04-1057,0,0.0524824,"Missing"
N10-1002,zhang-kordoni-2006-automated,1,0.860652,"Determine whether each verb–preposition combination is a VPC or not, and further predict its valence(s) (i.e. unknown if VPC, and unknown valence(s)) VPC Determine whether each verb–preposition combination is a VPC or not ignoring valence (i.e. unknown if VPC, and don’t care about valence) Table 2: Definitions of the three DLA tasks 2001). We use a slightly modified version of the ERG in our experiments, based on the nov-06 release. The modifications include 4 newly-added dummy lexical entries for the verb DUMMY- V and the corresponding inflectional rules, and a lexical type prediction model (Zhang and Kordoni, 2006) trained on the LOGON Treebank (Oepen et al., 2004) for unknown word handling. The parse disambiguation model we use is also trained on the LOGON Treebank. Since the parser has no access to any of the verbs under investigation (due to the DUMMYV substitution), those VPC types attested in the LOGON Treebank do not directly impact on the model’s performance. The chart mining feature extraction process took over 10 CPU days, and collected a total of 44K events for 4,090 candidate VPC triples.4 5-fold cross validation is used to train/test the model. As stated above (Section 2), the VPC triples at"
N10-1002,zhang-kordoni-2008-robust,1,0.826589,"es over unlexicalised features mined from a partial parsing chart. The proposed technique is shown to outperform a state-of-the-art parser over the target task, despite being based on relatively simplistic features. 1 Introduction Parsing with precision grammars is increasingly achieving broad coverage over open-domain texts for a range of constraint-based frameworks (e.g., TAG, LFG, HPSG and CCG), and is being used in real-world applications including information extraction, question answering, grammar checking and machine translation (Uszkoreit, 2002; Oepen et al., 2004; Frank et al., 2006; Zhang and Kordoni, 2008; MacKinlay et al., 2009). In this context, a “precision grammar” is a grammar which has been engineered to model grammaticality, and contrasts with a treebank-induced grammar, for example. Inevitably, however, such applications demand complete parsing outputs, based on the assumption that the text under investigation will be completely analysable by the grammar. As precision grammars generally make strong assumptions about complete lexical coverage and grammaticality of the input, their utility is limited over noisy or domain-specific data. This lack of complete coverage can make parsing with"
N10-1002,W07-1217,1,0.887544,"Missing"
N10-1002,W07-2207,1,0.950451,"L, pages 10–18, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics extract a list of non-compositional VPCs optionally with valence information. For comparison, we parse the same sentence set using a state-of-the-art statistical parser, and extract the VPCs from the parser output. Our results show that our chart mining method produces a model which is superior to the treebank parser. To our knowledge, the only other work that has looked at partial parsing results of precision grammars as a means of linguistic error analysis is that of Kiefer et al. (1999) and Zhang et al. (2007a), where partial parsing models were proposed to select a set of passive edges that together cover the input sequence. Compared to these approaches, our proposed chart mining technique is more general and can be adapted to specific tasks and domains. While we experiment exclusively with an HPSG grammar in this paper, it is important to note that the proposed method can be applied to any grammar formalism which is compatible with chart parsing, and where it is possible to describe an unlexicalised lexical entry for the different categories of lexical item that are to be extracted (see Section"
N10-1002,A00-2018,0,\N,Missing
N10-1002,J97-4005,0,\N,Missing
N10-1002,P03-1059,1,\N,Missing
nicholson-etal-2008-evaluating,copestake-flickinger-2000-open,0,\N,Missing
nicholson-etal-2008-evaluating,zhang-kordoni-2006-automated,1,\N,Missing
nicholson-etal-2008-evaluating,W07-1220,1,\N,Missing
nicholson-etal-2008-evaluating,W02-1210,0,\N,Missing
nicholson-etal-2008-evaluating,W06-1206,1,\N,Missing
nicholson-etal-2008-evaluating,P04-1057,0,\N,Missing
nicholson-etal-2008-evaluating,W05-1008,1,\N,Missing
nicholson-etal-2008-evaluating,baldwin-etal-2004-road,1,\N,Missing
P08-2048,P98-1013,0,0.130398,"nformed structures. As a result, we have seen an emerging interest in parser evaluation based on more theoryneutral and semantically informed representations, such as dependency structures. Some approaches have even tried to acquire semantic representations without full syntactic analyses. The so-called shallow semantic parsers build basic predicate-argument structures or label semantic roles that reveal the partial meaning of sentences (Carreras and M`arquez, 2005). Manually annotated lexical semantic resources like PropBank (Palmer et al., 2005), VerbNet (Kipper-Schuler, 2005), or FrameNet (Baker et al., 1998) are usually used as gold standards for training and evaluation of such systems. In the meantime, various existing parsing systems are also adapted to provide semantic information in their outputs. The obvious advantage in such an approach is that one can derive more fine-grained representations which are not typically available from shallow semantic parsers (e.g., modality and negation, quantifiers and scopes, etc.). To this effect, various semantic representations have been proposed and used in different parsing systems. Generally speaking, such semantic representations should be capable of"
P08-2048,W05-0620,0,0.0602398,"Missing"
P08-2048,2005.mtsummit-papers.22,0,0.15407,"le relations with associated arguments (Copestake et al., 2006). In this paper, the MRS structures are created with the English Resource Grammar (ERG), a HPSG-based broad coverage precision grammar for English. The seman189 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 189–192, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics tic predicates and their linguistic behaviour (including the set of semantic roles, indication of optional arguments, and their possible value constraints are specified by the grammar as its semantic interface (SEM-I) (Flickinger et al., 2005). 3 Relating MRS structures to lexical semantic resources 3.1 Feature extraction from linguistic resources The first set of features used to find corresponding lexical semantic roles for the MRS predicate arguments are taken from Robust MRS (RMRS) structures (Copestake, 2006). The general idea of the process is to traverse the bag of elementary predications looking for the verbs in the parsed sentence. When a verb is found, then its arguments are taken from the rarg tags and alternatively from the in-g conjunctions related to the verb. So, given the sentence: (1) Yields on money-market mutual"
P08-2048,W07-1204,0,0.0967429,"oduce errors in the inference phase. In addition, it is reasonable to use the VerbNet class information in the learning and inference phases, which in fact improves slightly the results. The outcomes also show that the use of the SEM algorithm improves accuracy slightly, meaning that the conditional dependency assumptions were reasonable, but still not perfect. The model can be slightly modified for verb class inference, by adding conditional dependencies between the VerbNet class and SEM-I features, which can potentially improve the parse disambiguation task, in a similar way of thinking to (Fujita et al., 2007). For instance, for the following sentence, we derive an incorrect mapping for the verb stay to the VerbNet class EXIST-47.1-1 with the (falsely) favored parse where the PP “in one place” is treated as an adjunct/modifier. For the correct reading where the PP is a complement to stay, the mapping to the correct VerbNet class LODGE -46 is derived, and the correct LOCATION role is identified for the PP. (3) Regardless of whether [T heme you] hike from lodge to lodge or stayLODGE -46 [Location in one place] and take day trips, there are plenty of choices. 5 Conclusions and Future Work In this pape"
P08-2048,J05-1004,0,0.105749,"shift of focus from pure syntactic analyses to more semantically informed structures. As a result, we have seen an emerging interest in parser evaluation based on more theoryneutral and semantically informed representations, such as dependency structures. Some approaches have even tried to acquire semantic representations without full syntactic analyses. The so-called shallow semantic parsers build basic predicate-argument structures or label semantic roles that reveal the partial meaning of sentences (Carreras and M`arquez, 2005). Manually annotated lexical semantic resources like PropBank (Palmer et al., 2005), VerbNet (Kipper-Schuler, 2005), or FrameNet (Baker et al., 1998) are usually used as gold standards for training and evaluation of such systems. In the meantime, various existing parsing systems are also adapted to provide semantic information in their outputs. The obvious advantage in such an approach is that one can derive more fine-grained representations which are not typically available from shallow semantic parsers (e.g., modality and negation, quantifiers and scopes, etc.). To this effect, various semantic representations have been proposed and used in different parsing systems. Gener"
P08-2048,N04-3012,0,0.0216275,"RMRS feature extraction was applied and a new verb dependency trees dataset was created. To obtain a correspondence between the SEM-I role labels and the PropBank (or VerbNet) role labels, a procedure which maps these labellings for 190 SEM-I roles Mapped roles Features ARG1 ARG2 Experiencer Theme manager n of propositional m rel Table 2: Alignment instance obtained for the verb expect Since the use of fine-grained features can make the learning process very complex, the WordNet semantic network (Fellbaum, 1998) was also employed to obtain generalisations of nouns. The algorithm described in (Pedersen et al., 2004) was used to disambiguate the sense, given the heads of the verb arguments and the verb itself (by using the mapping from VerbNet senses to WordNet verb senses (Kipper-Schuler, 2005)). Alternatively, a naive model has also been proposed, in which these features are simply generalized as nouns. For prepositions, the ontology provided by the SEM-I was used. Other words like adjectives or verbs in arguments were simply generalised as their corresponding type (e.g., adjectival rel or verbal rel). 3.2 Inference of semantic roles with Bayesian Networks The inference of semantic roles is based on tra"
P08-2048,C98-1013,0,\N,Missing
P09-1028,P07-1056,0,0.463816,"ements a co-clustering assumption closely related to Singular Value Decomposition (see also (Dhillon, 2001; Zha et al., 2001) for more on this perspective) while our model is based on Non-negative Matrix Factorization. In another recent paper (Sandler et al., 2008), standard regularization models are constrained using graphs of word co-occurences. These are very recently proposed competing methodologies, and we have not been able to address empirical comparisons with them in this paper. Finally, recent efforts have also looked at transfer learning mechanisms for sentiment analysis, e.g., see (Blitzer et al., 2007). While our focus is on single-domain learning in this paper, we note that cross-domain variants of our model can also be orthogonally developed. i belonging to the k classes, G is an n × k nonnegative matrix representing knowledge in document space, i.e., the i-th row of G represents the posterior probability of document i belonging to the k classes, and S is an k × k nonnegative matrix providing a condensed view of X. The matrix factorization model is similar to the probabilistic latent semantic indexing (PLSI) model (Hofmann, 1999). In PLSI, X is treated as the joint distribution between wo"
P09-1028,P04-1035,0,0.134027,"lts demonstrate the effectiveness and generality of our approach. The rest of the paper is organized as follows. We begin by discussing related work in Section 2. Section 3 gives a quick background on Nonnegative Matrix Tri-factorization models. In Section 4, we present a constrained model and computational algorithm for incorporating lexical knowledge in sentiment analysis. In Section 5, we enhance this model by introducing document labels as additional constraints. Section 6 presents an empirical study on four datasets. Finally, Section 7 concludes this paper. al., 2002). A two-tier scheme (Pang and Lee, 2004) where sentences are first classified as subjective versus objective, and then applying the sentiment classifier on only the subjective sentences further improves performance. Results in these papers also suggest that using more sophisticated linguistic models, incorporating parts-of-speech and n-gram language models, do not improve over the simple unigram bag-of-words representation. In keeping with these findings, we also adopt a unigram text model. A subjectivity classification phase before our models are applied may further improve the results reported in this paper, but our focus is on dr"
P09-1028,W02-1011,0,0.0438588,"er, sentiment is often conveyed with subtle linguistic mechanisms such as the use of sarcasm and highly domain-specific contextual cues. This makes manual annotation of sentiment time consuming and error-prone, presenting a bottleneck in learning high quality models. Moreover, products and services of current focus, and associated community of bloggers with their idiosyncratic expressions, may rapidly evolve over time causing models to potentially lose performance and become stale. This motivates the problem of learning robust sentiment models from minimal supervision. In their seminal work, (Pang et al., 2002) demonstrated that supervised learning significantly outperformed a competing body of work where hand-crafted dictionaries are used to assign sentiment labels based on relative frequencies of positive and negative terms. As observed by (Ng et al., 2006), most semi-automated dictionary-based approaches yield unsatisfactory lexicons, with either high coverage and low precision or vice versa. However, the treatment of such dictionaries as forms of prior knowledge that can be incorporated in machine learning models is a relatively less explored topic; even lesser so in conjunction with semi-superv"
P09-1028,W03-1201,0,0.00569789,"he m × n word-document semantic matrix, X = W SD, W is the word classconditional probability, and D is the document class-conditional probability and S is the class probability distribution. PLSI provides a simultaneous solution for the word and document class conditional distribution. Our model provides simultaneous solution for clustering the rows and the columns of X. To avoid ambiguity, the orthogonality conditions 3 Background 3.1 k 4 Incorporating Lexical Knowledge We used a sentiment lexicon generated by the IBM India Research Labs that was developed for other text mining applications (Ramakrishnan et al., 2003). It contains 2,968 words that have been human-labeled as expressing positive or negative sentiment. In total, there are 1,267 positive (e.g. “great”) and 1,701 negative (e.g., “bad”) unique (1) where F is an m × k non-negative matrix representing knowledge in the word space, i.e., i-th row of F represents the posterior probability of word 246 terms after stemming. We eliminated terms that were ambiguous and dependent on context, such as “dear” and “fine”. It should be noted, that this list was constructed without a specific domain in mind; which is further motivation for using training exampl"
P09-1028,W06-3808,0,0.13597,"from machine learning to bear on this problem: semi-supervised learning and learning from labeled features. The goal of the former theme is to learn from few labeled examples by making use of unlabeled data, while the goal of the latter theme is to utilize weak prior knowledge about term-class affinities (e.g., the term “awful” indicates negative sentiment and therefore may be considered as a negatively labeled feature). Empirical results in this paper demonstrate that simultaneously attempting both these goals in a single model leads to improvements over models that focus on a single goal. (Goldberg and Zhu, 2006) adapt semi-supervised graph-based methods for sentiment analysis but do not incorporate lexical prior knowledge in the form of labeled features. Most work in machine learning literature on utilizing labeled features has focused on using them to generate weakly labeled examples that are then used for standard supervised learning: (Schapire et al., 2002) propose one such framework for boosting logistic regression; (Wu and Srihari, 2004) build a modified SVM and (Liu et al., 2004) use a combination of clustering and EM based methods to instantiate similar frameworks. By contrast, we incorporate"
P09-1028,P02-1053,0,0.00973319,"ed Work We point the reader to a recent book (Pang and Lee, 2008) for an in-depth survey of literature on sentiment analysis. In this section, we briskly cover related work to position our contributions appropriately in the sentiment analysis and machine learning literature. Methods focussing on the use and generation of dictionaries capturing the sentiment of words have ranged from manual approaches of developing domain-dependent lexicons (Das and Chen, 2001) to semi-automated approaches (Hu and Liu, 2004; Zhuang et al., 2006; Kim and Hovy, 2004), and even an almost fully automated approach (Turney, 2002). Most semi-automated approaches have met with limited success (Ng et al., 2006) and supervised learning models have tended to outperform dictionary-based classification schemes (Pang et 245 unlabeled words. The matrix tri-factorization models explored in this paper are closely related to the models proposed recently in (Li et al., 2008; Sindhwani et al., 2008). Though, their techniques for proving algorithm convergence and correctness can be readily adapted for our models, (Li et al., 2008) do not incorporate dual supervision as we do. On the other hand, while (Sindhwani et al., 2008) do inco"
P09-1028,C04-1200,0,0.0915769,"ed and unlabeled documents in conjunction with labeled and 2 Related Work We point the reader to a recent book (Pang and Lee, 2008) for an in-depth survey of literature on sentiment analysis. In this section, we briskly cover related work to position our contributions appropriately in the sentiment analysis and machine learning literature. Methods focussing on the use and generation of dictionaries capturing the sentiment of words have ranged from manual approaches of developing domain-dependent lexicons (Das and Chen, 2001) to semi-automated approaches (Hu and Liu, 2004; Zhuang et al., 2006; Kim and Hovy, 2004), and even an almost fully automated approach (Turney, 2002). Most semi-automated approaches have met with limited success (Ng et al., 2006) and supervised learning models have tended to outperform dictionary-based classification schemes (Pang et 245 unlabeled words. The matrix tri-factorization models explored in this paper are closely related to the models proposed recently in (Li et al., 2008; Sindhwani et al., 2008). Though, their techniques for proving algorithm convergence and correctness can be readily adapted for our models, (Li et al., 2008) do not incorporate dual supervision as we d"
P09-1028,P06-2079,0,\N,Missing
P09-1043,P08-1108,0,0.462384,"f the previous work have been focusing on constituent-based parsing, while the domain adaptation of the dependency parsing has not been fully explored. the graph-based approach where the best parse tree is acquired by searching for a spanning tree which maximizes the score on either a partially or a fully connected graph with all words in the sentence as nodes (Eisner, 1996; McDonald et al., 2005b). As reported in various evaluation competitions, the two systems achieved comparable performances. More recently, approaches of combining these two parsers achieved even better dependency accuracy (Nivre and McDonald, 2008). Granted for the differences between their approaches, both systems heavily rely on machine learning methods to estimate the parsing model from an annotated corpus as training set. Due to the heavy cost of developing high quality large scale syntactically annotated corpora, even for a resource-rich language like English, only very few of them meets the criteria for training a general purpose statistical parsing model. For instance, the text style of WSJ is newswire, and most of the sentences are statements. Being lack of non-statements in the training data could cause problems, when the testi"
P09-1043,W06-2920,0,0.0208099,"out-domain tests.† 1 Introduction Syntactic dependency parsing is attracting more and more research focus in recent years, partially due to its theory-neutral representation, but also thanks to its wide deployment in various NLP tasks (machine translation, textual entailment recognition, question answering, information extraction, etc.). In combination with machine learning methods, several statistical dependency parsing models have reached comparable high parsing accuracy (McDonald et al., 2005b; Nivre et al., 2007b). In the meantime, successful continuation of CoNLL Shared Tasks since 2006 (Buchholz and Marsi, 2006; Nivre et al., 2007a; Surdeanu et al., 2008) have witnessed how easy it has become to train a statistical syntactic dependency parser provided that there is annotated treebank. While the dissemination continues towards various languages, several issues arise with such purely data-driven approaches. One common observation is that statistical parser performance drops significantly when tested on a dataset different from the training set. For instance, when using 2 Parser Domain Adaptation In recent years, two statistical dependency parsing systems, MaltParser (Nivre et al., 2007b) and MSTParser"
P09-1043,P07-1032,0,0.048683,"ecursively embeds smaller feature structures for lower level phrases or words. For the purpose of dependency backbone extraction, we only look at the derivation tree which corresponds to the constituent tree of an HPSG analysis, with all non-terminal nodes labeled by the names of the grammar rules applied. Figure 2 shows an example. Note that all grammar rules in ERG are either unary or binary, giving us relatively deep trees when compared with annotations such as Penn Treebank. Conceptually, this conversion is similar to the conversions from deeper structures to GR reprsentations reported by Clark and Curran (2007) and Miyao et al. (2007). 380 3.3 the CoNLL shared task dependency structures, minor systematic differences still exist for some phenomena. For example, the possessive “’s” is annotated to be governed by its preceding word in CoNLL dependency; while in HPSG, it is treated as the head of a “specifier-head” construction, hence governing the preceding word in the dependency backbone. With several simple tree rewriting rules, we are able to fix the most frequent inconsistencies. With the rule-based backbone extraction and repair, we can finally turn our HPSG parser outputs into dependency structur"
P09-1043,C96-1058,0,0.476231,"ACL and AFNLP ported on approaches of incorporating linguistic features to make the parser less dependent on the nature of training and testing dataset, without resorting to huge amount of unlabeled out-domain data. In addition, most of the previous work have been focusing on constituent-based parsing, while the domain adaptation of the dependency parsing has not been fully explored. the graph-based approach where the best parse tree is acquired by searching for a spanning tree which maximizes the score on either a partially or a fully connected graph with all words in the sentence as nodes (Eisner, 1996; McDonald et al., 2005b). As reported in various evaluation competitions, the two systems achieved comparable performances. More recently, approaches of combining these two parsers achieved even better dependency accuracy (Nivre and McDonald, 2008). Granted for the differences between their approaches, both systems heavily rely on machine learning methods to estimate the parsing model from an annotated corpus as training set. Due to the heavy cost of developing high quality large scale syntactically annotated corpora, even for a resource-rich language like English, only very few of them meets"
P09-1043,2004.tmi-1.2,0,0.0608373,"Missing"
P09-1043,W01-0521,0,0.3282,", 1994) has been successfully applied in several parsing systems for more than a dozen of languages. Some of these grammars, such as the English Resource Grammar (ERG; Flickinger (2002)), have undergone over decades of continuous development, and provide precise linguistic analyses for a broad range of phenomena. These linguistic knowledge are encoded in highly generalized form according to linguists’ reflection for the target languages, and tend to be largely independent from any specific domain. There has been a substantial amount of work on parser adaptation, especially from WSJ to B ROWN. Gildea (2001) compared results from different combinations of the training and testing data to demonstrate that the size of the feature model can be reduced via excluding “domain-dependent” features, while the performance could still be preserved. Furthermore, he also pointed out that if the additional training data is heterogeneous from the original one, the parser will not obtain a substantially better performance. Bacchiani et al. (2006) generalized the previous approaches using a maximum a posteriori (MAP) framework and proposed both supervised and unsupervised adaptation of statistical parsers. McClos"
P09-1043,P07-1079,0,0.033636,"ebanks to help overcome the insufficient data problem for deep parse selection models. locative expression cannot be easily predicted at the pure syntactic level. This also suggests a joint learning of syntactic and semantic dependencies, as proposed in the CoNLL 2008 Shared Task. Instances of wrong HPSG analyses have also been observed as one source of errors. For most of the cases, a correct reading exists, but not picked by our parse selection model. This happens more often with the WSJ test set, partially contributing to the low performance. 5 Conclusion & Future Work Similar to our work, Sagae et al. (2007) also considered the combination of dependency parsing with an HPSG parser, although their work was to use statistical dependency parser outputs as soft constraints to improve the HPSG parsing. Nevertheless, a similar backbone extraction algorithm was used to map between different representations. Similar work also exists in the constituentbased approaches, where CFG backbones were used to improve the efficiency and robustness of HPSG parsers (Matsuzaki et al., 2007; Zhang and Kordoni, 2008). In this paper, we restricted our investigation on the syntactic evaluation using labeled/unlabeled att"
P09-1043,J93-2004,0,0.0360867,"Missing"
P09-1043,W08-2121,0,0.0455886,"Missing"
P09-1043,P06-1043,0,0.0617612,"(2001) compared results from different combinations of the training and testing data to demonstrate that the size of the feature model can be reduced via excluding “domain-dependent” features, while the performance could still be preserved. Furthermore, he also pointed out that if the additional training data is heterogeneous from the original one, the parser will not obtain a substantially better performance. Bacchiani et al. (2006) generalized the previous approaches using a maximum a posteriori (MAP) framework and proposed both supervised and unsupervised adaptation of statistical parsers. McClosky et al. (2006) and McClosky et al. (2008) have shown that out-domain parser performance can be improved with selftraining on a large amount of unlabeled data. Most of these approaches focused on the machine learning perspective instead of the linguistic knowledge embraced in the parsers. Little study has been reThe main issue of parsing with precision grammars is that broad coverage and high precision on linguistic phenomena do not directly guarantee robustness of the parser with noisy real world texts. Also, the detailed linguistic analysis is not always of the highest interest to all NLP applications. It"
P09-1043,zhang-kordoni-2008-robust,1,0.888165,"Missing"
P09-1043,C08-1071,0,0.0164343,"m different combinations of the training and testing data to demonstrate that the size of the feature model can be reduced via excluding “domain-dependent” features, while the performance could still be preserved. Furthermore, he also pointed out that if the additional training data is heterogeneous from the original one, the parser will not obtain a substantially better performance. Bacchiani et al. (2006) generalized the previous approaches using a maximum a posteriori (MAP) framework and proposed both supervised and unsupervised adaptation of statistical parsers. McClosky et al. (2006) and McClosky et al. (2008) have shown that out-domain parser performance can be improved with selftraining on a large amount of unlabeled data. Most of these approaches focused on the machine learning perspective instead of the linguistic knowledge embraced in the parsers. Little study has been reThe main issue of parsing with precision grammars is that broad coverage and high precision on linguistic phenomena do not directly guarantee robustness of the parser with noisy real world texts. Also, the detailed linguistic analysis is not always of the highest interest to all NLP applications. It is not always straightforwa"
P09-1043,W08-2126,1,0.872839,"Missing"
P09-1043,P05-1012,0,0.0568089,"o the features models of statistical parsers. As mordern grammar-based parsers has achieved high runtime efficency (with our HPSG parser parsing at an average speed of ∼3 sentences per second), this adds up to an acceptable overhead. 3.3.1 Feature Model with MSTParser As mentioned before, MSTParser is a graphbased statistical dependency parser, whose learning procedure can be viewed as the assignment of different weights to all kinds of dependency arcs. Therefore, the feature model focuses on each kind of head-child pair in the dependency tree, and mainly contains four categories of features (Mcdonald et al., 2005a): basic uni-gram features, basic bi-gram features, in-between POS features, and surrounding POS features. It is emphasized by the authors that the last two categories contribute a large improvement to the performance and bring the parser to the state-of-the-art accuracy. Therefore, we extend this feature set by adding four more feature categories, which are similar to the original ones, but the dependency relation was replaced by the dependency backbone of the HPSG outputs. The extended feature set is shown in Table 1. Robust Parsing with HPSG As mentioned in Section 2, one pitfall of using"
P09-1043,H05-1066,0,0.769228,"s which, when combined with state-of-the-art statistical dependency parsing models, achieves performance improvements on out-domain tests.† 1 Introduction Syntactic dependency parsing is attracting more and more research focus in recent years, partially due to its theory-neutral representation, but also thanks to its wide deployment in various NLP tasks (machine translation, textual entailment recognition, question answering, information extraction, etc.). In combination with machine learning methods, several statistical dependency parsing models have reached comparable high parsing accuracy (McDonald et al., 2005b; Nivre et al., 2007b). In the meantime, successful continuation of CoNLL Shared Tasks since 2006 (Buchholz and Marsi, 2006; Nivre et al., 2007a; Surdeanu et al., 2008) have witnessed how easy it has become to train a statistical syntactic dependency parser provided that there is annotated treebank. While the dissemination continues towards various languages, several issues arise with such purely data-driven approaches. One common observation is that statistical parser performance drops significantly when tested on a dataset different from the training set. For instance, when using 2 Parser D"
P09-1043,P99-1052,0,\N,Missing
P09-1043,D07-1096,0,\N,Missing
P19-1040,N19-1053,1,0.875952,"Missing"
P19-1040,N13-1132,0,0.054772,"et al., 2018). However, this pipeline is local in that it applies to a given claim. The missing step here is to assess the trustworthiness of the sources producing the claims and evidence. This is a global step that, in principle, accounts for all claims made by a source and all sources making a claim. Previous work has studied how to estimate the trustworthiness or credibility of information sources for fact-finding (Vydiswaran et al.; Pasternack and Roth, 2013), truth discovery (Dong et al.; Pochampally et al., 2014; Dong et al., 2015; Li et al., 2016) and crowdsourcing (Sabou et al., 2012; Hovy et al., 2013; Gao et al., 2015). Usually, given a list of conflicting facts, e.g. “source s asserts claim c”, or “annotator x labels data item t by label y”, we detect the true claims or correct labels for the data item by resolving conflicts, and then compute the trustworthiness of sources. However, many sources do not directly assert claims, but rather generate articles as evidence, expecting readers to infer claims from this evidence. In practice, given a claim of interest, people may search for related articles from multiple sources and collect evidence for the claim; they can then determine the verac"
P19-1040,P14-1095,0,0.114532,". However, many sources do not directly assert claims, but rather generate articles as evidence, expecting readers to infer claims from this evidence. In practice, given a claim of interest, people may search for related articles from multiple sources and collect evidence for the claim; they can then determine the veracity of the claim by deciding whether the evidence found supports or refutes the claim. However, most existing work that attempted to study trustworthiness of sources assumed that sources make assertions directly. Even when intermediate text was accounted for (Vydiswaran et al.; Nakashole and Mitchell, 2014), it was assumed that clean evidence and clear connections between evidence and conflicting claims are provided, disregarding the fact that NLP systems attempting to support these tasks are noisy. This paper considers two situations when evalThe information revolution brought with it information pollution. Information retrieval and extraction help us cope with abundant information from diverse sources. But some sources are of anonymous authorship, and some are of uncertain accuracy, so how can we determine what we should actually believe? Not all information sources are equally trustworthy, an"
P19-1040,D18-1010,1,0.911386,"Missing"
P19-1040,D16-1244,0,0.0874226,"Missing"
P19-1040,D14-1162,0,0.0828048,"φw (e, m, c) evaluates the reliability of an entailment result given by the entailment model. As we described in Section 2.3, φw (e, m, c) is a sigmoid function of a linear combination of feature values, and we include following features: Entailment Score. For each prediction of the given entailment model, the model will predict a label, i.e. entailment, contradiction or neutral as well as a score to support its conclusion. Text Similarity. This feature is computed by the cosine similarity between numerical representations of the evidence and the claim. In this work, we use tf-idf and Glove (Pennington et al., 2014) to represent sentences respectively. To represent a sentence, we use the pre-trained Glove 1 with a simple method proposed in (Arora et al., 2017). Entity Similarity. We identify entities for each pair of evidence and claim, and compute the overlap of entities by jaccard similarity and entity similarity by NESim (Do et al., 2009) as two features. bs,e,ym e ws,e we,m ηi N (8) where φ = φw (e, m, c) for abbreviation. Means while, since Hs ∼ RsR+Q , we model it by mins imizing their KL divergence. Therefore, we also minimize: X Hs Hs ]= Hs log EHs [log Ps Ps s (9) X Hs (Rs + Qs ) = Hs log Rs s"
P19-1040,W18-5523,0,0.0226992,"Missing"
P19-1040,N18-1202,0,0.0697172,"Missing"
P19-1040,N18-1074,0,0.113273,"Missing"
P19-1040,P17-2067,0,0.0830313,"Missing"
Q14-1013,D09-1062,0,0.0259905,"tically motivated rules and assume the existence of independent keywords in sentences which indicate comparative relations. Therefore, these methods fall short of extracting comparative relations based on domain dependent information. Both Johansson and Moschitti (2011) and Wu et al. (2011) formulate fine-grained sentiment analysis as a learning problem with structured outputs. However, they focus only on polarity classification of expressions and require annotation of sentimentbearing expressions for training as well. While ILP has been previously applied for inference in sentiment analysis (Choi and Cardie, 2009; Somasundaran and Wiebe, 2009; Wu et al., 2011), our task requires a complete ILP reformulation due to 1) the absence of annotated sentiment expressions and 2) the constraints imposed by the joint extraction of both sentiment polarity and comparative relations. 3 System Overview This section gives an overview of the whole system for extracting sentiment-oriented relation instances. Prior to presenting the system architecture, we introduce the essential concepts and the definitions of two kinds of directed hypergraphs as the representation of correlated relation instances extracted from senten"
Q14-1013,P10-2050,0,0.0187524,", showing that S ENTI -LSSVM model can effectively learn from a training corpus without explicitly annotated subjective expressions and that its performance significantly outperforms state-of-the-art systems. 2 Related Work There are ample works on analyzing sentiment polarities and entity comparisons, but the majority of them studied the two tasks in isolation. Most prior approaches for fine-grained sentiment analysis focus on polarity classification. Supervised approaches on expression-level analysis require the annotation of sentiment-bearing expressions as training data (Jin et al., 2009; Choi and Cardie, 2010; Johansson and Moschitti, 2011; Yessenalina and Cardie, 2011; Wei and Gulla, 2010). However, the corresponding annotation process is time-consuming. Although sentence-level annotations are easier to obtain, the analysis at this level cannot cope with sentences conveying relations of multiple types (McDonald et al., 2007; Täckström and McDonald, 2011; Socher et al., 2012). Lexiconbased approaches require no training data (Ku et al., 2006; Kim and Hovy, 2006; Godbole et al., 2007; Ding et al., 2008; Popescu and Etzioni, 2005; Liu et al., 2005) but suffer from inferior performance (Wilson et al."
Q14-1013,W06-1651,0,0.0376951,"VM based model can learn from training corpora that do not contain explicit annotations of sentiment-bearing expressions, and it can simultaneously recognize instances of both binary (polarity) and ternary (comparative) relations with regard to entity mentions of interest. The empirical evaluation shows that our approach significantly outperforms stateof-the-art systems across domains (cameras and movies) and across genres (reviews and forum posts). The gold standard corpus that we built will also be a valuable resource for the community. 1 Introduction Sentiment-oriented relation extraction (Choi et al., 2006) is concerned with recognizing sentiment polarities and comparative relations between entities from natural language text. Identifying such relations often requires syntactic and semantic analysis at both sentence and phrase level. Most prior work on sentiment analysis consider either i) subjective sentence detection (Yu and Kübler, 2011), ii) polarity classification (Johansson and Moschitti, 2011; Wilson et al., 2005), or iii) comparative relation identification (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008). In practice, however, differTo the best of our knowledge, the only existing s"
Q14-1013,C08-1031,0,0.179373,"aluable resource for the community. 1 Introduction Sentiment-oriented relation extraction (Choi et al., 2006) is concerned with recognizing sentiment polarities and comparative relations between entities from natural language text. Identifying such relations often requires syntactic and semantic analysis at both sentence and phrase level. Most prior work on sentiment analysis consider either i) subjective sentence detection (Yu and Kübler, 2011), ii) polarity classification (Johansson and Moschitti, 2011; Wilson et al., 2005), or iii) comparative relation identification (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008). In practice, however, differTo the best of our knowledge, the only existing system capable of extracting both comparisons and sentiment polarities is a rule-based system proposed by Ding et al. (2009). We argue that it is better to tackle the task by using a unified model with structured outputs. It allows us to consider a set of correlated relation instances jointly and characterize their interaction through a set of soft and hard constraints. For example, we can encode constraints to discourage an attribute to participate in a polarity relation and a comparative relation at the same time."
Q14-1013,P11-2018,0,0.108016,"cameras and movies) and across genres (reviews and forum posts). The gold standard corpus that we built will also be a valuable resource for the community. 1 Introduction Sentiment-oriented relation extraction (Choi et al., 2006) is concerned with recognizing sentiment polarities and comparative relations between entities from natural language text. Identifying such relations often requires syntactic and semantic analysis at both sentence and phrase level. Most prior work on sentiment analysis consider either i) subjective sentence detection (Yu and Kübler, 2011), ii) polarity classification (Johansson and Moschitti, 2011; Wilson et al., 2005), or iii) comparative relation identification (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008). In practice, however, differTo the best of our knowledge, the only existing system capable of extracting both comparisons and sentiment polarities is a rule-based system proposed by Ding et al. (2009). We argue that it is better to tackle the task by using a unified model with structured outputs. It allows us to consider a set of correlated relation instances jointly and characterize their interaction through a set of soft and hard constraints. For example, we can encode c"
Q14-1013,W06-0301,0,0.0142575,"sed approaches on expression-level analysis require the annotation of sentiment-bearing expressions as training data (Jin et al., 2009; Choi and Cardie, 2010; Johansson and Moschitti, 2011; Yessenalina and Cardie, 2011; Wei and Gulla, 2010). However, the corresponding annotation process is time-consuming. Although sentence-level annotations are easier to obtain, the analysis at this level cannot cope with sentences conveying relations of multiple types (McDonald et al., 2007; Täckström and McDonald, 2011; Socher et al., 2012). Lexiconbased approaches require no training data (Ku et al., 2006; Kim and Hovy, 2006; Godbole et al., 2007; Ding et al., 2008; Popescu and Etzioni, 2005; Liu et al., 2005) but suffer from inferior performance (Wilson et al., 2005; Qu et al., 2012). In contrast, our method requires no annotation of sentiment-bearing expressions for training and can predict both sentiment polarities and comparative relations. Sentiment-oriented comparative relations have been studied in the context of user-generated discourse (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008). Approaches rely on linguistically motivated rules and assume the existence of independent keywords in sentences whic"
Q14-1013,P03-1054,0,0.0042962,"X Structural Inference In order to find the best eMRG for a given sentence with a well trained model, we need to determine the most likely relation type for each relation candidate and support the corresponding assertions with proper textual evidences. We formulate this task as an Integer Linear Programming (ILP). Instead of considering all constituents of a sentence, we empirically select a subset as textual evidences for each relation candidate. 6.1 Textual Evidence Candidates Selection Textual evidences are selected based on the constituent trees of sentences parsed by the Stanford parser (Klein and Manning, 2003). For each mention in a sentence, we first locate a constituent in the tree with the maximal overlap by Jaccard similarity. Starting from this constituent, we consider two types of candidates: type I candidates are constituents at the highest level which contain neither any word of another mention nor any contrast conjunctions such as “but”; type II candidates are constituents at the highest level which cover exactly two mentions of an edge and do not overlap with any other mentions. For a binary edge connecting an entity mention and an attribute mention, we consider a type I candidate startin"
Q14-1013,P09-1039,0,0.1849,"xtracts sets 158 S ENTI -LSSVM Model The task of sentiment-oriented relation extraction is to determine the most likely sSoR in a sentence. Since sSoRs are derived from the corresponding MRG s as described in Section 3, the task is reduced to find the most likely MRG for each sentence. Since an MRG is created by assigning relation types to a subset of all relation candidates, which are possible tuples of mentions with unknown relation types, the number of MRGs can be extremely high. To tackle the task, one solution is to employ an edge-factored linear model in the framework of structural SVM (Martins et al., 2009; Tsochantaridis et al., 2004). The model suggests that a bag of features should be specified for each relation candidate, and then the model predicts the most likely candidate sets along with their relation types to form the optimal MRGs. As we observed, for a relation candidate, the most informative features are the words near its entity mentions in the original text. However, if we represent a candidate by all these words, it is very likely that the instances of different relation types share overly similar features, because a mention is often involved in more than one relation candidate, a"
Q14-1013,P07-1055,0,0.0322612,"f them studied the two tasks in isolation. Most prior approaches for fine-grained sentiment analysis focus on polarity classification. Supervised approaches on expression-level analysis require the annotation of sentiment-bearing expressions as training data (Jin et al., 2009; Choi and Cardie, 2010; Johansson and Moschitti, 2011; Yessenalina and Cardie, 2011; Wei and Gulla, 2010). However, the corresponding annotation process is time-consuming. Although sentence-level annotations are easier to obtain, the analysis at this level cannot cope with sentences conveying relations of multiple types (McDonald et al., 2007; Täckström and McDonald, 2011; Socher et al., 2012). Lexiconbased approaches require no training data (Ku et al., 2006; Kim and Hovy, 2006; Godbole et al., 2007; Ding et al., 2008; Popescu and Etzioni, 2005; Liu et al., 2005) but suffer from inferior performance (Wilson et al., 2005; Qu et al., 2012). In contrast, our method requires no annotation of sentiment-bearing expressions for training and can predict both sentiment polarities and comparative relations. Sentiment-oriented comparative relations have been studied in the context of user-generated discourse (Jindal and Liu, 2006; Ganapathi"
Q14-1013,H05-1043,0,0.0415718,"ation of sentiment-bearing expressions as training data (Jin et al., 2009; Choi and Cardie, 2010; Johansson and Moschitti, 2011; Yessenalina and Cardie, 2011; Wei and Gulla, 2010). However, the corresponding annotation process is time-consuming. Although sentence-level annotations are easier to obtain, the analysis at this level cannot cope with sentences conveying relations of multiple types (McDonald et al., 2007; Täckström and McDonald, 2011; Socher et al., 2012). Lexiconbased approaches require no training data (Ku et al., 2006; Kim and Hovy, 2006; Godbole et al., 2007; Ding et al., 2008; Popescu and Etzioni, 2005; Liu et al., 2005) but suffer from inferior performance (Wilson et al., 2005; Qu et al., 2012). In contrast, our method requires no annotation of sentiment-bearing expressions for training and can predict both sentiment polarities and comparative relations. Sentiment-oriented comparative relations have been studied in the context of user-generated discourse (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008). Approaches rely on linguistically motivated rules and assume the existence of independent keywords in sentences which indicate comparative relations. Therefore, these methods fall shor"
Q14-1013,C10-1103,1,0.854867,"ng to entity A. Co-occurrence: We have mentioned the cooccurrence feature in Equation 2, indicated by Φc (a, a0 ). It captures the co-occurrence of two labeled edges incident to the same entity mention. Note that the co-occurrence feature function is considered only if there is a contrast conjunction such as “but” between the non-shared entity mentions incident to the two labeled edges. Senti-predictors: Following the idea of (Qu et al., 2012), we encode the prediction results from the rule-based phrase-level multi-relation predictor (Ding et al., 2009) and from the bag-of-opinions predictor (Qu et al., 2010) as features based on the textual evidence. The output of the first predictor is an integer value, while the output of the second predictor is a sentiment relation, such as “positive”, “negative”, “better” or “worse”. We map the relational outputs into integer values and then encode the outputs from both predictors as senti-predictor features. Others: The commonly used part-of-speech tags are also included as features. Moreover, for an edge candidate, a set of binary features are used to denote the types of the edge and its entity mentions. For instance, a binary feature indicates whether an e"
Q14-1013,D12-1014,1,0.913576,"son and Moschitti, 2011; Yessenalina and Cardie, 2011; Wei and Gulla, 2010). However, the corresponding annotation process is time-consuming. Although sentence-level annotations are easier to obtain, the analysis at this level cannot cope with sentences conveying relations of multiple types (McDonald et al., 2007; Täckström and McDonald, 2011; Socher et al., 2012). Lexiconbased approaches require no training data (Ku et al., 2006; Kim and Hovy, 2006; Godbole et al., 2007; Ding et al., 2008; Popescu and Etzioni, 2005; Liu et al., 2005) but suffer from inferior performance (Wilson et al., 2005; Qu et al., 2012). In contrast, our method requires no annotation of sentiment-bearing expressions for training and can predict both sentiment polarities and comparative relations. Sentiment-oriented comparative relations have been studied in the context of user-generated discourse (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008). Approaches rely on linguistically motivated rules and assume the existence of independent keywords in sentences which indicate comparative relations. Therefore, these methods fall short of extracting comparative relations based on domain dependent information. Both Johansson and"
Q14-1013,D12-1110,0,0.0421273,"approaches for fine-grained sentiment analysis focus on polarity classification. Supervised approaches on expression-level analysis require the annotation of sentiment-bearing expressions as training data (Jin et al., 2009; Choi and Cardie, 2010; Johansson and Moschitti, 2011; Yessenalina and Cardie, 2011; Wei and Gulla, 2010). However, the corresponding annotation process is time-consuming. Although sentence-level annotations are easier to obtain, the analysis at this level cannot cope with sentences conveying relations of multiple types (McDonald et al., 2007; Täckström and McDonald, 2011; Socher et al., 2012). Lexiconbased approaches require no training data (Ku et al., 2006; Kim and Hovy, 2006; Godbole et al., 2007; Ding et al., 2008; Popescu and Etzioni, 2005; Liu et al., 2005) but suffer from inferior performance (Wilson et al., 2005; Qu et al., 2012). In contrast, our method requires no annotation of sentiment-bearing expressions for training and can predict both sentiment polarities and comparative relations. Sentiment-oriented comparative relations have been studied in the context of user-generated discourse (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008). Approaches rely on linguistic"
Q14-1013,P09-1026,0,0.0139075,"and assume the existence of independent keywords in sentences which indicate comparative relations. Therefore, these methods fall short of extracting comparative relations based on domain dependent information. Both Johansson and Moschitti (2011) and Wu et al. (2011) formulate fine-grained sentiment analysis as a learning problem with structured outputs. However, they focus only on polarity classification of expressions and require annotation of sentimentbearing expressions for training as well. While ILP has been previously applied for inference in sentiment analysis (Choi and Cardie, 2009; Somasundaran and Wiebe, 2009; Wu et al., 2011), our task requires a complete ILP reformulation due to 1) the absence of annotated sentiment expressions and 2) the constraints imposed by the joint extraction of both sentiment polarity and comparative relations. 3 System Overview This section gives an overview of the whole system for extracting sentiment-oriented relation instances. Prior to presenting the system architecture, we introduce the essential concepts and the definitions of two kinds of directed hypergraphs as the representation of correlated relation instances extracted from sentences. 3.1 Concepts and Definiti"
Q14-1013,J11-2001,0,0.0385546,"Missing"
Q14-1013,P10-1059,0,0.0197331,"anon 7D, textitprice). However, constructing a fully annotated training corpus for this task is labor-intensive and requires strong linguistic background. We minimize this overhead by applying a simplified annotation scheme, in which annotators mark mentions of entities and attributes, disambiguate the entities, and label instances of relations for each sentence. Based on the new scheme, we have created a small Sentiment Relation Graph (SRG) corpus for the domains of cameras and movies, which significantly differs from the corpora used in prior work (Wei and Gulla, 2010; Kessler et al., 2010; Toprak et al., 2010; Wiebe et al., 2005; Hu and Liu, 2004) in the following ways: i) both sentiment polarities and comparative relations are annotated; ii) all mentioned entities are disambiguated; and iii) no subjective expressions are annotated, unless they are part of entity mentions. The new annotation scheme raises a new challenge for learning algorithms in that they need to automatically find textual evidences for each annotated relation during training. For example, with the sentence “I like the Rebel a little better, but that is another price jump”, simply assigning a sentimentbearing expression to the n"
Q14-1013,P10-1042,0,0.129447,"l Linguistics. and preferred(Nikon D7000, Canon 7D, textitprice). However, constructing a fully annotated training corpus for this task is labor-intensive and requires strong linguistic background. We minimize this overhead by applying a simplified annotation scheme, in which annotators mark mentions of entities and attributes, disambiguate the entities, and label instances of relations for each sentence. Based on the new scheme, we have created a small Sentiment Relation Graph (SRG) corpus for the domains of cameras and movies, which significantly differs from the corpora used in prior work (Wei and Gulla, 2010; Kessler et al., 2010; Toprak et al., 2010; Wiebe et al., 2005; Hu and Liu, 2004) in the following ways: i) both sentiment polarities and comparative relations are annotated; ii) all mentioned entities are disambiguated; and iii) no subjective expressions are annotated, unless they are part of entity mentions. The new annotation scheme raises a new challenge for learning algorithms in that they need to automatically find textual evidences for each annotated relation during training. For example, with the sentence “I like the Rebel a little better, but that is another price jump”, simply assig"
Q14-1013,H05-1044,0,0.180832,"genres (reviews and forum posts). The gold standard corpus that we built will also be a valuable resource for the community. 1 Introduction Sentiment-oriented relation extraction (Choi et al., 2006) is concerned with recognizing sentiment polarities and comparative relations between entities from natural language text. Identifying such relations often requires syntactic and semantic analysis at both sentence and phrase level. Most prior work on sentiment analysis consider either i) subjective sentence detection (Yu and Kübler, 2011), ii) polarity classification (Johansson and Moschitti, 2011; Wilson et al., 2005), or iii) comparative relation identification (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008). In practice, however, differTo the best of our knowledge, the only existing system capable of extracting both comparisons and sentiment polarities is a rule-based system proposed by Ding et al. (2009). We argue that it is better to tackle the task by using a unified model with structured outputs. It allows us to consider a set of correlated relation instances jointly and characterize their interaction through a set of soft and hard constraints. For example, we can encode constraints to discoura"
Q14-1013,D11-1123,0,0.0218512,"hod requires no annotation of sentiment-bearing expressions for training and can predict both sentiment polarities and comparative relations. Sentiment-oriented comparative relations have been studied in the context of user-generated discourse (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008). Approaches rely on linguistically motivated rules and assume the existence of independent keywords in sentences which indicate comparative relations. Therefore, these methods fall short of extracting comparative relations based on domain dependent information. Both Johansson and Moschitti (2011) and Wu et al. (2011) formulate fine-grained sentiment analysis as a learning problem with structured outputs. However, they focus only on polarity classification of expressions and require annotation of sentimentbearing expressions for training as well. While ILP has been previously applied for inference in sentiment analysis (Choi and Cardie, 2009; Somasundaran and Wiebe, 2009; Wu et al., 2011), our task requires a complete ILP reformulation due to 1) the absence of annotated sentiment expressions and 2) the constraints imposed by the joint extraction of both sentiment polarity and comparative relations. 3 Syste"
Q14-1013,D11-1016,0,0.0189865,"rn from a training corpus without explicitly annotated subjective expressions and that its performance significantly outperforms state-of-the-art systems. 2 Related Work There are ample works on analyzing sentiment polarities and entity comparisons, but the majority of them studied the two tasks in isolation. Most prior approaches for fine-grained sentiment analysis focus on polarity classification. Supervised approaches on expression-level analysis require the annotation of sentiment-bearing expressions as training data (Jin et al., 2009; Choi and Cardie, 2010; Johansson and Moschitti, 2011; Yessenalina and Cardie, 2011; Wei and Gulla, 2010). However, the corresponding annotation process is time-consuming. Although sentence-level annotations are easier to obtain, the analysis at this level cannot cope with sentences conveying relations of multiple types (McDonald et al., 2007; Täckström and McDonald, 2011; Socher et al., 2012). Lexiconbased approaches require no training data (Ku et al., 2006; Kim and Hovy, 2006; Godbole et al., 2007; Ding et al., 2008; Popescu and Etzioni, 2005; Liu et al., 2005) but suffer from inferior performance (Wilson et al., 2005; Qu et al., 2012). In contrast, our method requires no"
Q14-1013,W11-0323,0,0.0179712,"utperforms stateof-the-art systems across domains (cameras and movies) and across genres (reviews and forum posts). The gold standard corpus that we built will also be a valuable resource for the community. 1 Introduction Sentiment-oriented relation extraction (Choi et al., 2006) is concerned with recognizing sentiment polarities and comparative relations between entities from natural language text. Identifying such relations often requires syntactic and semantic analysis at both sentence and phrase level. Most prior work on sentiment analysis consider either i) subjective sentence detection (Yu and Kübler, 2011), ii) polarity classification (Johansson and Moschitti, 2011; Wilson et al., 2005), or iii) comparative relation identification (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008). In practice, however, differTo the best of our knowledge, the only existing system capable of extracting both comparisons and sentiment polarities is a rule-based system proposed by Ding et al. (2009). We argue that it is better to tackle the task by using a unified model with structured outputs. It allows us to consider a set of correlated relation instances jointly and characterize their interaction through a se"
Q14-1013,H05-2017,0,\N,Missing
R11-1049,P98-1014,0,0.123822,"Missing"
R11-1049,I05-1015,0,0.0147384,"igate if the acquired lexical entries affect sentence realisation. The GG adopts Minimal Recursion Semantics (MRS, Copestake et al. (2005)) as semantic representation. This, together with the fine-grained linguistic information in the GG lexical types, allows for finding the textual realisations for a given input semantic representation. Sentence realisation with the GG is performed within the LKB grammar engineering platform which provides an efficient generation engine. This engine is essentially a chart-based generator (Kay, 1996) with various optimisations for MRS and packed parse forest (Carroll and Oepen, 2005). As there are less ordering constraints in the semantic representation (comparing to the word sequence in parsing inputs), the computation is intrinsically more expensive. While in parsing the ambiguity in the less constrained lexical entries acquired with LA dissolves quickly in its context, there is a potential risk of overgeneration in sentence realisation. We conduct an indicative experiment with 14 unknown words from the test set used in Section 4.1. These words have been assigned verb types by the classifier. The focus of the experiment is on verbs because of the large number of possibl"
R11-1049,R09-1012,1,0.892635,"Missing"
R11-1049,C10-2018,1,0.7426,"Missing"
R11-1049,D10-1088,1,0.807892,"Missing"
R11-1049,W08-1708,1,0.874614,"Missing"
R11-1049,copestake-flickinger-2000-open,0,0.148867,"Missing"
R11-1049,W00-0740,0,0.0329213,"Missing"
R11-1049,E03-1041,0,0.0367437,"Missing"
R11-1049,P96-1027,0,0.0164094,"luation, extending the evaluation methodology of C& V N, we also investigate if the acquired lexical entries affect sentence realisation. The GG adopts Minimal Recursion Semantics (MRS, Copestake et al. (2005)) as semantic representation. This, together with the fine-grained linguistic information in the GG lexical types, allows for finding the textual realisations for a given input semantic representation. Sentence realisation with the GG is performed within the LKB grammar engineering platform which provides an efficient generation engine. This engine is essentially a chart-based generator (Kay, 1996) with various optimisations for MRS and packed parse forest (Carroll and Oepen, 2005). As there are less ordering constraints in the semantic representation (comparing to the word sequence in parsing inputs), the computation is intrinsically more expensive. While in parsing the ambiguity in the less constrained lexical entries acquired with LA dissolves quickly in its context, there is a potential risk of overgeneration in sentence realisation. We conduct an indicative experiment with 14 unknown words from the test set used in Section 4.1. These words have been assigned verb types by the class"
R11-1049,J03-3001,0,0.0093745,"alization of morphological properties but they proved to be less informative for the classifier. Further, the paradigm generation method outputs a single paradigm for Abfahrten indicating that this word is a singular feminine noun. This information is explicitly used as a feature in the classifier which is shown in row (v) of Table 1. entries in the lexicon mapped onto it and it is assigned to at least 15 distinct words occurring in large corpora parsed with PET and the GG. The parsed corpus we use consists of roughly 2.5M sentences randomly selected from the German part of the Wacky project (Kilgarriff and Grefenstette, 2003). The Wacky project aims at the creation of large corpora for different languages, including German, from various web sources, such as online newspapers and magazines, legal texts, internet fora, etc. Following these criteria, we have selected 39 open-class types out of the 411 lexical types defined in the GG. As described in Section 2.2, we re-defined the type definitions of the 39 types which resulted in the creation of 68 expanded types. This number is smaller than the 611 types used in the experiments with Alpino because the GG does not have a full form lexicon. Table 2 gives more details"
R11-1049,W02-2018,0,0.00948395,"the word. All other morphological forms are derived by applying various morphological rules defined in the GG to the word stem. For this reason, we employ the paradigm not only as a source of features for the classifier but also as a way to map the unknown word to its stem. The stem for nouns is the singular nominative noun form, for adjectives it is the base nonin(1) p(t|c) = P Θi fi (t,c)) P exp( i P ′ t′ ∈T exp( i Θi fi (t ,c)) where fi (t, c) may encode arbitrary characteristics of the context and &lt; Θ1 , Θ2 , ... > can be evaluated by maximising the pseudo-likelihood on a training corpus (Malouf, 2002). Table 1 shows the features for Abfahrten. Row (i) contains 4 separate features derived from the prefix of the word and 4 other suffix features are 2 357 TADM; http://tadm.sourceforge.net/ given in row (ii). The two features in rows (iii) and (iv) indicate whether the word starts with a separable particle and if it contains a hyphen, respectively. Since it is the stem of the unknown word we add to the lexicon, we also experimented with prefix and suffix features extracted from the stem. We assumed that those could allow for a better generalization of morphological properties but they proved t"
R11-1049,2006.jeptalnrecital-invite.2,1,0.887295,"Missing"
R11-1049,zhang-kordoni-2006-automated,1,0.84341,"Missing"
R11-1049,W05-1008,0,\N,Missing
R11-1049,C98-1014,0,\N,Missing
S10-1061,H05-1066,0,0.038749,"he meaning of the input text. In particular, after tokenization and POS tagging, we did dependency parsing and semantic role labeling. In addition, HPSG parsing is a filter for ungrammatical hypotheses. Tokenization and POS Tagging We use the Penn Treebank style tokenization throughout the various processing stages. TnT, an HMM-based POS tagger trained with Wall Street Journal sections of the PTB, was used to automatically predict the part-of-speech of each token in the texts and hypotheses. Dependency Parsing For obtaining the syntactic dependencies, we use two dependency parsers, MSTParser (McDonald et al., 2005) and MaltParser (Nivre et al., 2007). MSTParser is a graphbased dependency parser where the best parse tree is acquired by searching for a spanning tree 272 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 272–275, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics Preprocessing Dependency Path Extraction T H HPSG Parsing Dependency Parsing Semantic Role Labeling Path Extraction Feature-based Classification Dependency Triple Extraction Feature Extraction SVM-based Classification Yes/No No Figure 1: Workflow of the System 2.2"
S10-1061,S10-1009,0,0.0434839,"Missing"
S10-1061,W08-2126,1,\N,Missing
S10-1061,W07-1401,0,\N,Missing
S14-2008,D12-1133,0,0.0149346,"ained or otherwise derived from WSJ Section 21. This restriction implies that typical off-the-shelf syntactic parsers had to be re-trained, as many datadriven parsers for English include this section of the PTB in their default training data. To simplify participation in the open track, the organizers prepared ready-to-use ‘companion’ syntactic analyses, sentence- and token-aligned to the SDP data, in two formats, viz. PTB-style phrase structure trees obtained from the parser of Petrov et al. (2006) and Stanford Basic syntactic dependencies (de Marneffe et al., 2006) produced by the parser of Bohnet and Nivre (2012). 6 Submissions and Results From 36 teams who had registered for the task, test runs were submitted for nine systems. Each team submitted one or two test runs per track. In total, there were ten runs submitted to the closed track and nine runs to the open track. Three teams submitted to both the closed and the open track. The main results are summarized and ranked in Table 4. The ranking is based on the average LF score across all three target representations, which is given in the LF column. In cases where a team submitted two runs to a track, only the highestranked score is included in the t"
S14-2008,W06-2920,0,0.101128,"em in comparison to other sub-tasks in computational language analysis, introduce the semantic dependency target representations used, reflect on high-level commonalities and differences between these representations, and summarize the task setup, participating systems, and main results. 1 Background and Motivation Syntactic dependency parsing has seen great advances in the past decade, in part owing to relatively broad consensus on target representations, and in part reflecting the successful execution of a series of shared tasks at the annual Conference for Natural Language Learning (CoNLL; Buchholz & Marsi, 2006; Nivre et al., 2007; inter alios). From this very active research area accurate and efficient syntactic parsers have developed for a wide range of natural languages. However, the predominant data structure in dependency parsing to date are trees, in the formal sense that every node in the dependency graph is reachable from a distinguished root node by exactly one directed path. (1) A similar technique is almost impossible to apply to other crops, such as cotton, soybeans, and rice. Semantically, technique arguably is dependent on the determiner (the quantificational locus), the modifier simil"
S14-2008,de-marneffe-etal-2006-generating,0,0.0702807,"Missing"
S14-2008,oepen-lonning-2006-discriminant,1,0.758648,"antic dependency graphs originate in a manual re-annotation of Sections 00– 21 of the WSJ Corpus with syntactico-semantic analyses derived from the LinGO English Resource Grammar (ERG; Flickinger, 2000). Among other layers of linguistic annotation, this resource— dubbed DeepBank by Flickinger et al. (2012)— includes underspecified logical-form meaning representations in the framework of Minimal Recursion Semantics (MRS; Copestake et al., 2005). Our DM target representations are derived through a two-step ‘lossy’ conversion of MRSs, first to variable-free Elementary Dependency Structures (EDS; Oepen & Lønning, 2006), then to ‘pure’ bi-lexical form—projecting some construction semantics onto word-to-word dependencies (Ivanova et al., 2012). In preparing our gold-standard DM graphs from DeepBank, the same conversion pipeline was used as in the system submission of Miyao et al. (2014). For this target representation, top nodes designate the highest-scoping (nonquantifier) predicate in the graph, e.g. the (scopal) degree adverb almost in Figure 1.2 NNP NNP VBZ NNP . − − + − − + − + − − arg1 arg2 _ compound _ _ _ _ ARG1 _ ARG2 _ Table 1: Tabular SDP data format (showing DM). texts from the PTB, and their Czec"
S14-2008,W09-1201,1,0.855031,"Missing"
S14-2008,J05-1004,0,0.129381,"ependency graphs for Example (1). uous preposition marking the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node reentrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. In addition to its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea & Jurafsky, 2002). In much previous work, however, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena— for example negation and other scopal embedding, comparatives, possessives, various types of modification, and even conjunction—typically remain unanalyzed in SRL. Thus, its target representations are partial to a degree that can prohibit semantic downstream processing, for example inferencebased techniques. In contrast, we require parsers to identify all semantic dependencies, i.e. compute a representation that integrates all content words in o"
S14-2008,P06-1055,0,0.0124328,"e of the gold-standard syntactic or semantic analyses of the SDP 2014 test data, i.e. were directly or indirectly trained or otherwise derived from WSJ Section 21. This restriction implies that typical off-the-shelf syntactic parsers had to be re-trained, as many datadriven parsers for English include this section of the PTB in their default training data. To simplify participation in the open track, the organizers prepared ready-to-use ‘companion’ syntactic analyses, sentence- and token-aligned to the SDP data, in two formats, viz. PTB-style phrase structure trees obtained from the parser of Petrov et al. (2006) and Stanford Basic syntactic dependencies (de Marneffe et al., 2006) produced by the parser of Bohnet and Nivre (2012). 6 Submissions and Results From 36 teams who had registered for the task, test runs were submitted for nine systems. Each team submitted one or two test runs per track. In total, there were ten runs submitted to the closed track and nine runs to the open track. Three teams submitted to both the closed and the open track. The main results are summarized and ranked in Table 4. The ranking is based on the average LF score across all three target representations, which is given i"
S14-2008,hajic-etal-2012-announcing,1,0.772691,"Missing"
S14-2008,W12-3602,1,0.885947,"yses derived from the LinGO English Resource Grammar (ERG; Flickinger, 2000). Among other layers of linguistic annotation, this resource— dubbed DeepBank by Flickinger et al. (2012)— includes underspecified logical-form meaning representations in the framework of Minimal Recursion Semantics (MRS; Copestake et al., 2005). Our DM target representations are derived through a two-step ‘lossy’ conversion of MRSs, first to variable-free Elementary Dependency Structures (EDS; Oepen & Lønning, 2006), then to ‘pure’ bi-lexical form—projecting some construction semantics onto word-to-word dependencies (Ivanova et al., 2012). In preparing our gold-standard DM graphs from DeepBank, the same conversion pipeline was used as in the system submission of Miyao et al. (2014). For this target representation, top nodes designate the highest-scoping (nonquantifier) predicate in the graph, e.g. the (scopal) degree adverb almost in Figure 1.2 NNP NNP VBZ NNP . − − + − − + − + − − arg1 arg2 _ compound _ _ _ _ ARG1 _ ARG2 _ Table 1: Tabular SDP data format (showing DM). texts from the PTB, and their Czech translations. Similarly to other treebanks in the Prague family, there are two layers of syntactic annotation: analytical ("
S14-2008,C08-1095,0,0.478753,".27 to 75.89 and the corresponding scores across systems are 88.64 for PAS, 84.95 for DM, and 67.52 for PCEDT. While these scores are consistently higher than in the closed track, the differences are small. In fact, for each of the three teams that submitted to both tracks (Alpage, Potsdam, and Priberam) improvements due to the use of additional resources in the open track do not exceed two points LF. 7 dencies), while the others apply post-processing to recover non-tree structures. The second strategy is to use a parsing algorithm that can directly generate graph structures (in the spirit of Sagae & Tsujii, 2008; Titov et al., 2009). In many cases such algorithms generate restricted types of graph structures, but these restrictions appear feasible for our target representations. The last approach is more machine learning–oriented; they apply classifiers or scoring methods (e.g. edge-factored scores), and find the highest-scoring structures by some decoding method. It is difficult to tell which approach is the best; actually, the top three systems in the closed and open tracks selected very different approaches. A possible conclusion is that exploiting existing systems or techniques for dependency par"
S14-2008,P10-5006,0,0.0693315,"ple inferencebased techniques. In contrast, we require parsers to identify all semantic dependencies, i.e. compute a representation that integrates all content words in one structure. Another difference to common interpretations of SRL is that the SDP 2014 task definition does not encompass predicate disambiguation, a design decision in part owed to our goal to focus on parsing-oriented, i.e. structural, analysis, and in part to lacking consensus on sense inventories for all content words. Finally, a third closely related area of much current interest is often dubbed ‘semantic parsing’, which Kate and Wong (2010) define as “the task of mapping natural language sentences into complete formal meaning representations which a computer can execute for some domain-specific application.” In contrast to most work in this tradition, our SDP target representations aim to be task- and domainindependent, though at least part of this generality comes at the expense of ‘completeness’ in the above sense; i.e. there are aspects of sentence meaning that arguably remain implicit. 2 Target Representations We use three distinct target representations for semantic dependencies. As is evident in our running example (Figure"
S14-2008,J93-2004,0,0.0590496,"t of multiple predicates (i.e. have more than one incoming arc), and it will often be desirable to leave nodes corresponding to semantically vacuous word classes unattached (with no incoming arcs). Thus, Task 8 at SemEval 2014, Broad-Coverage Semantic Dependency Parsing (SDP 2014),1 seeks to stimulate the dependency parsing community to move towards more general graph processing, to thus enable a more direct analysis of Who did What to Whom? For English, there exist several independent annotations of sentence meaning over the venerable Wall Street Journal (WSJ) text of the Penn Treebank (PTB; Marcus et al., 1993). These resources constitute parallel semantic annotations over the same common text, but to date they have not been related to each other and, in fact, have hardly been applied for training and testing of datadriven parsers. In this task, we have used three different such target representations for bi-lexical semantic dependencies, as demonstrated in Figure 1 below for the WSJ sentence: Task 8 at SemEval 2014 defines BroadCoverage Semantic Dependency Parsing (SDP) as the problem of recovering sentence-internal predicate–argument relationships for all content words, i.e. the semantic structure"
S14-2008,P07-1031,0,0.0115637,"ersely, in PCEDT the last coordinating conjunction takes all conjuncts as its arguments (in case there is no overt conjunction, a punctuation mark is used instead); additional conjunctions or punctuation marks are not connected to the graph.7 A linguistic difference between our representations that highlights variable granularities of analysis and, relatedly, diverging views on the scope of the problem can be observed in Figure 2. Much noun phrase–internal structure is not made explicit in the PTB, and the Enju Treebank from which our PAS representation derives predates the bracketing work of Vadas and Curran (2007). In the four-way nominal compounding example of Figure 2, thus, PAS arrives at a strictly left-branching tree, and there is no attempt at interpreting semantic roles among the members of the compound either; PCEDT, on the other hand, annotates both the actual compound-internal bracketing and the assignment of roles, e.g. making stock the PAT(ient) of investment. In this spirit, the PCEDT annotations could be directly paraphrased along the lines of plans by employees for investment in stocks. In a middle position between the other two, DM disambiguates the bracketing but, by design, merely ass"
S14-2008,meyers-etal-2004-annotating,0,0.0465249,"Example (1). uous preposition marking the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node reentrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. In addition to its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea & Jurafsky, 2002). In much previous work, however, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena— for example negation and other scopal embedding, comparatives, possessives, various types of modification, and even conjunction—typically remain unanalyzed in SRL. Thus, its target representations are partial to a degree that can prohibit semantic downstream processing, for example inferencebased techniques. In contrast, we require parsers to identify all semantic dependencies, i.e. compute a representation that integrates all content words in one structure. Another"
S14-2008,S14-2056,1,0.893424,"pBank by Flickinger et al. (2012)— includes underspecified logical-form meaning representations in the framework of Minimal Recursion Semantics (MRS; Copestake et al., 2005). Our DM target representations are derived through a two-step ‘lossy’ conversion of MRSs, first to variable-free Elementary Dependency Structures (EDS; Oepen & Lønning, 2006), then to ‘pure’ bi-lexical form—projecting some construction semantics onto word-to-word dependencies (Ivanova et al., 2012). In preparing our gold-standard DM graphs from DeepBank, the same conversion pipeline was used as in the system submission of Miyao et al. (2014). For this target representation, top nodes designate the highest-scoping (nonquantifier) predicate in the graph, e.g. the (scopal) degree adverb almost in Figure 1.2 NNP NNP VBZ NNP . − − + − − + − + − − arg1 arg2 _ compound _ _ _ _ ARG1 _ ARG2 _ Table 1: Tabular SDP data format (showing DM). texts from the PTB, and their Czech translations. Similarly to other treebanks in the Prague family, there are two layers of syntactic annotation: analytical (a-trees) and tectogrammatical (t-trees). PCEDT bi-lexical dependencies in this task have been extracted from the t-trees. The specifics of the PCE"
S14-2008,C10-1011,0,\N,Missing
S14-2008,S14-2080,0,\N,Missing
S14-2008,S14-2082,0,\N,Missing
S14-2008,J02-3001,0,\N,Missing
S14-2008,W15-0128,1,\N,Missing
S14-2008,D07-1096,0,\N,Missing
S14-2008,cinkova-2006-propbank,0,\N,Missing
U05-1006,W05-1008,0,0.0954397,"Missing"
U05-1006,J99-2004,0,0.138881,"1997) reported an empirical approach towards unknown lexical analysis using morphological and syntactic information. The approach is similar to ours in spirit. However, the experiments were done for a shallow parser with a very limited number of word classes. The applicability to lexicalist deep grammars with lots of lexical types is unknown. In (Malouf and van Noord, 2004), the maximum entropy models were used for wide coverage parsing with the Alpino Dutch grammar (Bouma et al., 2001). But the focus was on parse selection, not unknown words processing. Another related work is supertagging (Bangalore and Joshi, 1999). In supertagging, the lexical items are assigned with rich descriptions (supertags) that impose complex constraints in a local context. Some statistical techniques of assigning supertags to unknown words have been reported. For example, (Bangalore and Joshi, 1999) used a simple method of combining a probability estimate for unknown words P (U KN |Ti ) with a probability estimate based on word features (capitalization, hyphenation, ending of words) by: P (Wi |Ti ) = P (U N K|Ti ) ∗ P (w f eat(Wi )|Ti ) (2) where U N K is a token associated with each supertag and its count NU N K is estimated b"
U05-1006,P98-1014,0,0.0332015,"east) one lexical entry for the unknown, so that the deep processing does not halt at the very beginning. A more important difference is that, while (Baldwin, 2005) focuses on generalizing the method of deriving DLA models on various secondary language resources, our work focuses more on how to utilize the deep grammar itself as a source for enhancing robustness. The Redwoods Treebank is by nature the output of the deep grammar. And the parsing, as well as the disambiguation models are also part of the grammar that has eventually contributed to the unknown word type prediction. (Erbach, 1990; Barg and Walther, 1998; Fouvry, 2003) followed a different approach towards unknown words processing for unification based grammars. The basic idea was to use the underspecified lexical entries, namely TFSs with fewer constraints, in order to generate full parses for the sentences, and then extract the sub-TFS from the parses as a new lexical entry. However, lexical entries generated in this way might be both too general and too specific. And underspecified lexical entries with fewer constraints allow more grammar rules to be applied while parsing. It gets even worse when 5 We used a text set named rondane for trai"
U05-1006,callmeier-etal-2004-deepthought,0,0.0341559,"Missing"
U05-1006,copestake-flickinger-2000-open,0,0.274171,"Missing"
U05-1006,copestake-etal-2004-lexicon,0,0.0133003,"ludes email correspondence, travel planning dialogs, etc. The 5th growth of Redwoods contains about 16.5K sentences and 122K tokens3 . In all our experiments, we have done a 10fold cross validation on the Redwoods treebank. For each fold, words that do not occur in the training partition are assumed to be unknown. A modified version of the efficient HPSG parser PET (Callmeier, 2000; Callmeier, 2001) has been used to generate the derivation tree fragments of the partial parses. 3 Sentences without a full analysis are neither counted here nor used in experiments. 28 We have also modified LexDB (Copestake et al., 2004) in order to be able to add temporal lexical entries that are only active for specific sentence. The parse disambiguation model we have used is a maximum entropy based model that uses non-lexicalized features with 2 levels of grandparnets (see (Toutanova et al., 2002) for detailed discussion about parse disambiguation models for HPSG grammars). For maximum entropy parameter estimation, we have used (Malouf, 2002)’s MaxEnt package. 5.2 Results For comparison, we have built a baseline system that always assigns a majority type to each unknown according to the POS tag. More specificically, we tag"
U05-1006,E03-1041,0,0.0170058,"for the unknown, so that the deep processing does not halt at the very beginning. A more important difference is that, while (Baldwin, 2005) focuses on generalizing the method of deriving DLA models on various secondary language resources, our work focuses more on how to utilize the deep grammar itself as a source for enhancing robustness. The Redwoods Treebank is by nature the output of the deep grammar. And the parsing, as well as the disambiguation models are also part of the grammar that has eventually contributed to the unknown word type prediction. (Erbach, 1990; Barg and Walther, 1998; Fouvry, 2003) followed a different approach towards unknown words processing for unification based grammars. The basic idea was to use the underspecified lexical entries, namely TFSs with fewer constraints, in order to generate full parses for the sentences, and then extract the sub-TFS from the parses as a new lexical entry. However, lexical entries generated in this way might be both too general and too specific. And underspecified lexical entries with fewer constraints allow more grammar rules to be applied while parsing. It gets even worse when 5 We used a text set named rondane for training and hike f"
U05-1006,W02-2018,0,0.271539,"of a Maximum entropy model lie in the general feature representation and in no independence assumptions between features. A maximum entropy model can also easily handle thousands of features and large numbers of possible outputs. For our prediction model, the probability of a lexical type t given an unknown word and its context c is: p(t, c) = P exp( i θi fi (t, c)) P 0 0 t ∈T exp( i θi fi (t , c)) P (1) where feature fi (t, c) may encode arbitrary characteristics of the context. The parameters < θ1 , θ2 , . . . &gt; can be evaluated by maximizing the pseudo-likelihood on a training corpus (see (Malouf, 2002)). The basic feature templates used in our MEbased model include the prefix and suffix of the unknown word, the context words within a window size of 5, and their corresponding lexical types. Using Partial Parsing Results as Features Each lexical type is essentially a set of constraints on linguistic objects. If a word has a specific lexical type, it must conform to all the constraints demanded by the type, and hence it can only appear in some specific linguistic context. The constraints concern various linguistic aspects, among which syntactic constraints are predominant. One advantage of usi"
U05-1006,C02-2025,0,0.0499545,"Missing"
U05-1006,W97-0124,0,0.0312127,"written English about tourism in the norwegian mountain area, with an average sentence length of 16 words; hike contains 320 sentences about outdoor hiking in Norway with an average sentence length of 14.3 words. Both contain a lot of unknowns like location names, transliterations, etc. two unknown words occur next to each other, which might allow almost any constituent to be constructed. Also, the underspecified lexical entry significantly increases computational complexity. (van Schagen and Knott, 2004) took a similar approach of interactive unknown word acquisition in a dialogue context. (Thede and Harper, 1997) reported an empirical approach towards unknown lexical analysis using morphological and syntactic information. The approach is similar to ours in spirit. However, the experiments were done for a shallow parser with a very limited number of word classes. The applicability to lexicalist deep grammars with lots of lexical types is unknown. In (Malouf and van Noord, 2004), the maximum entropy models were used for wide coverage parsing with the Alpino Dutch grammar (Bouma et al., 2001). But the focus was on parse selection, not unknown words processing. Another related work is supertagging (Bangal"
U05-1006,U04-1018,0,0.0871669,"Missing"
U05-1006,baldwin-etal-2004-road,0,\N,Missing
W06-1206,baldwin-etal-2004-road,0,0.134886,"Missing"
W06-1206,W05-1008,0,0.0425203,"Missing"
W06-1206,P04-1057,0,0.0200567,"Missing"
W06-1206,1999.tc-1.8,0,0.0293775,"of van Noord (2004), the word sequences are used, mainly because the cost to compute and count the word sequences is minimum. The parsability of a sequence wi . . . wj is defined as: R(wi . . . wj ) = C(wi . . . wj , OK) C(wi . . . wj ) them. The output of the error mining phase proposes a set of n-grams, which also contain MWEs. Therefore, the task is to distinguish the MWEs from the other cases. To do this, first we propose to use the World Wide Web as a very large corpus from which we collect evidence that enables us to rule out noisy cases (due to spelling errors, for instance), following Grefenstette (1999), Keller et al. (2002), Kilgarriff and Grefenstette (2003) and Villavicencio (2005). The candidates that are kept can be semi-automatically included in the grammar, by employing a lexical type predictor, whose output we use in order to add lexical entries to the lexicon, with a possible manual check by a grammar writer. This procedure significantly speeds up the process of grammar development, relieving the grammar developer of some of the burden by automatically detecting parse failures and providing semi-automatic means for handling them. The paper starts with a discussion of MWEs and of som"
W06-1206,zhang-kordoni-2006-automated,1,0.83552,"rther subtypes. We will call the maximum lexical types after extension the atomic lexical types. Then the lexicon will be a multi-valued mapping from the word stems to the atomic lexical types. Needless to underline here that all we have exp( i θi fi (t, c)) P p(t|c) = P 0 t0 ∈T exp( i θi fi (t , c)) P (3) where feature fi (t, c) may encode arbitrary characteristics of the context. The parameters < θ1 , θ2 , . . . > can be evaluated by maximising the pseudo-likelihood on a training corpus (Malouf, 2002). The detailed design and feature selection for the lexical type predictor are described in Zhang and Kordoni (2006). 5 Lexical ambiguity is not considered here for the unknowns. In principle, this constraint can be relaxed by allowing the classifier to return more than one results by, setting a confidence threshold, for example. 42 In the experiment described here, we have used the latest version of the Redwoods Treebank in order to train the lexical type predictor with morphological features and context words/POS tags features 6 . We have then extracted from the BNC 6248 sentences, which contain at least one of the 311 MWE candidates verified with World Wide Web in the way described in the previous sectio"
W06-1206,W02-1030,0,0.0377286,"the word sequences are used, mainly because the cost to compute and count the word sequences is minimum. The parsability of a sequence wi . . . wj is defined as: R(wi . . . wj ) = C(wi . . . wj , OK) C(wi . . . wj ) them. The output of the error mining phase proposes a set of n-grams, which also contain MWEs. Therefore, the task is to distinguish the MWEs from the other cases. To do this, first we propose to use the World Wide Web as a very large corpus from which we collect evidence that enables us to rule out noisy cases (due to spelling errors, for instance), following Grefenstette (1999), Keller et al. (2002), Kilgarriff and Grefenstette (2003) and Villavicencio (2005). The candidates that are kept can be semi-automatically included in the grammar, by employing a lexical type predictor, whose output we use in order to add lexical entries to the lexicon, with a possible manual check by a grammar writer. This procedure significantly speeds up the process of grammar development, relieving the grammar developer of some of the burden by automatically detecting parse failures and providing semi-automatic means for handling them. The paper starts with a discussion of MWEs and of some of the characteristi"
W06-1206,J03-3001,0,0.157918,"used, mainly because the cost to compute and count the word sequences is minimum. The parsability of a sequence wi . . . wj is defined as: R(wi . . . wj ) = C(wi . . . wj , OK) C(wi . . . wj ) them. The output of the error mining phase proposes a set of n-grams, which also contain MWEs. Therefore, the task is to distinguish the MWEs from the other cases. To do this, first we propose to use the World Wide Web as a very large corpus from which we collect evidence that enables us to rule out noisy cases (due to spelling errors, for instance), following Grefenstette (1999), Keller et al. (2002), Kilgarriff and Grefenstette (2003) and Villavicencio (2005). The candidates that are kept can be semi-automatically included in the grammar, by employing a lexical type predictor, whose output we use in order to add lexical entries to the lexicon, with a possible manual check by a grammar writer. This procedure significantly speeds up the process of grammar development, relieving the grammar developer of some of the burden by automatically detecting parse failures and providing semi-automatic means for handling them. The paper starts with a discussion of MWEs and of some of the characteristics that make them so challenging for"
W06-1206,W02-2018,0,0.0498277,"hat corresponds to the occurrence of the word in the given context5 . We use a single classifier to predict the atomic lexical type. There are normally hundreds of atomic lexical types for a large grammar. So the classification model should be able to handle a large number of output classes. We choose the Maximum Entropy-based model because it can easily handle thousands of features and a large number of possible outputs. It also has the advantages of general feature representation and no independence assumption between features. With the efficient parameter estimation algorithms discussed by Malouf (2002), the training of the model is now very fast. For our prediction model, the probability of a lexical type t given an unknown word and its context c is: Atomic Lexical Types Lexicalist grammars are normally composed of a limited number of rules and a lexicon with rich linguistic features attached to each entry. Some grammar formalisms have a type inheriting system to encode various constraints, and a flat structure of the lexicon with each entry mapped onto one type in the inheritance hierarchy. The following discussion is based on Head-driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1"
W06-1206,A00-2022,0,0.072191,"Missing"
W07-1217,baldwin-etal-2004-road,0,0.1165,"Missing"
W07-1217,P06-2006,0,0.0472637,"Missing"
W07-1217,P06-4020,0,0.0562837,"Missing"
W07-1217,I05-1015,0,0.0570435,"he local ambiguity packing poses an efficiency and accuracy challenge, as not all the intermediate parsing results are directly available as passive edges on the chart. Without unpacking the ambiguity readings, interesting partial analyses might be lost.2 But exhaustively unpacking all the readings will pay back the efficiency gain by ambiguity packing, and eventually lead to computational intractable results. To efficiently recover the ambiguous readings from packed representations, the selective unpacking algorithm has been recently implemented as an extension to the algorithm described in (Carroll and Oepen, 2005). It is able to recover the top-n best readings of a given passive parser edge based on the score assigned by a maximum entropy parse ranking model. This neat feature largely facilitates the efficient searching for best partial parses described in later sections. 3 Partial Parse Selection A partial parse is a set of partial analyses licensed by the grammar which cover the entire input without overlapping. As shown in the previous section, there are usually more than one possible partial parses for a given input. For deep linguistic processing, a high level of local ambiguity means there are ev"
W07-1217,W06-1106,0,0.173979,"Missing"
W07-1217,P04-1005,0,0.285593,"ation is reasonable performance considering the types of noise in the speech transcript input. As a further step to show the competence of partial parsing, we briefly investigated its application in capturing disfluent regions in speech texts. The state of the art approach in identifying disfluent re3 Lexical prediction was not used here to avoid obfuscating the quality of partial parsing by introducing lexical type prediction errors. 4 The repetition error of “it” is interpreted as a topicalization. 134 gions and potentially capturing meaningful text is a shallow parsing method described in (Johnson and Charniak, 2004), which searches the text string for approximately repeated constituents. We ran their system on our random sample of the Fisher data, and compared its results to the partial parse output of the nine well-segmented partial parses analyses (every utterance of which contained some speaker-induced disfluency) to see how well partial parsing could potentially fare as an approach for identifying disfluent regions of speech text. Often the (Johnson and Charniak, 2004) method identified disfluent regions overlapped with identified fragments found in the partial parse, the removal of which would yield"
W07-1217,P99-1069,0,0.0324247,"ation Functions Generally speaking, the weights of the edges in the shortest path approach represent the quality of the local analyses and their likelihood of appearing in the analysis of the entire input. This is an interesting parallel to the parse selection models for the full analyses, where a goodness score is usually assigned to the full analysis. For example, the parse disambiguation model described in (Toutanova et al., 2002) uses a maximum entropy approach to model the conditional probability of a parse for a given input sequence P (t|w). A similar approach has also been reported in (Johnson et al., 1999; Riezler et al., 2002; Malouf and van Noord, 2004). For a given partial parse Φ = {t1 , . . . , tk }, Ω = 131 k Y P (ti |wi ) (2) i=1 Therefore, the log-probability will be log P (Φ|w) ≈ log P (Ω|w) + k X log P (ti |wi ) (3) i=1 Equation 3 indicates that the log-probability of a partial parse for a given input is the sum of the logprobability of local analyses for the sub-strings, with an additional component − log P (Ω|w) representing the conditional log-probability of the segmentation. If we use − log P (ti |wi ) as the weight for each local analysis, then the DAG shortest path algorithm wi"
W07-1217,W02-2018,0,0.0388681,"n. If we use − log P (ti |wi ) as the weight for each local analysis, then the DAG shortest path algorithm will quickly find the partial parse that maximizes log P (Φ|w) − log P (Ω|w). The probability P (ti |wi ) can be modeled in a similar way to the maximum entropy based full parse selection models: P exp nj=1 λj fj (ti , wi ) Pn P (ti |wi ) = P ′ j=1 λj fj (t , wi ) t′ ∈T exp (4) where T is the set of all possible structures that can be assigned to wi , f1 . . . fn are the features and λ1 . . . λn are the parameters. The parameters can be efficiently estimated from a treebank, as shown by (Malouf, 2002). The only difference from the full parse selection model is that here intermediate results are used to generate events for training the model (i.e. the intermediate nodes are used as positive events if it occurs on one of the active tree, or as negative events if not). Since there is a huge number of intermediate results availalbe, we only randomly select a part of them as training data. This is essentially similar to the approach in (Osborne, 2000), where there is an infeasibly large number of training events, only part of which is used in the estimation step. The exact features used in the"
W07-1217,J93-2004,0,0.0292787,"they are very much specific to the annotation guidelines. Also, the deep grammars we are working with are not automatically extracted from annotated corpora. Therefore, unless there are partial treebanks built specifically for the deep grammars, there is simply no ‘gold’ standard for non-golden partial analyses. Instead, in this paper, we evaluate the partial analyses results on the basis of multiple metrics, from both the syntactic and semantic point of views. Empirical evaluation has been done with the ERG on a small set of texts from the Wall Street Journal Section 22 of the Penn Treebank (Marcus et al., 1993). A pilot study of applying partial parsing in spontaneous speech text processing is also carried out. The remainder of the paper is organized as follow. Section 2 provides background knowledge about partial analysis. Section 3 presents various partial parse selection models. Section 4 describes the evaluation setup and results. Section 5 concludes the paper. tures (attribute value pairs) and a type inheritance system. Therefore, each passive edge on the parsing chart corresponds to a TFS. A relatively small set of highly generalized rules are used to check the compatibility among smaller TFSe"
W07-1217,A00-2022,0,0.165579,"3 Local Ambiguity Packing There is one more complication concerning the partial parses when the local ambiguity packing is used in the parser. Due to the inherent ambiguity of natural language, the same sequence of input may be analyzed as the same linguistic object in different ways. Such intermediate analyses must be recorded during the processing and recovered in later stages. Without any efficient processing technique, parsing becomes computationally intractable with the combinatory explosion of such local ambiguities. In PET, the subsumption-based ambiguity packing algorithm proposed in (Oepen and Carroll, 2000) is used. This separates the parsing into two phases: forest creation phase and read-out/unpacking phase. In relation to the work on partial parsing in this paper, the local ambiguity packing poses an efficiency and accuracy challenge, as not all the intermediate parsing results are directly available as passive edges on the chart. Without unpacking the ambiguity readings, interesting partial analyses might be lost.2 But exhaustively unpacking all the readings will pay back the efficiency gain by ambiguity packing, and eventually lead to computational intractable results. To efficiently recove"
W07-1217,C02-2025,0,0.0215999,"rses. For full parsers, there are generally two ways of evaluation. For parsers that are trained on a treebank using an automatically extracted grammar, an unseen set of manually annotated data is used as the test set. The parser output on the test set is compared to the gold standard annotation, either with the widely used PARSEVAL measurement, or with more annotation-neutral dependency relations. For parsers based on manually compiled grammars, more human judgment is involved in the evaluation. With the evolution of the grammar, the treebank as the output from the grammar changes over time (Oepen et al., 2002). The grammar writer inspects the parses generated by the grammar and either “accepts” or “rejects” the analysis. In partial parsing for manually compiled grammars, the criterion for acceptable analyses is less evident. Most current treebanking tools are not designed for annotating partial analyses. Large-scale manually annotated treebanks do have the annotation for sentences that deep grammars are not able to fully analyze. And the annotation difference in other language resources makes the comparison less straightforward. More complication is involved with the platform and resources used in"
W07-1217,C00-1085,0,0.0178667,"to wi , f1 . . . fn are the features and λ1 . . . λn are the parameters. The parameters can be efficiently estimated from a treebank, as shown by (Malouf, 2002). The only difference from the full parse selection model is that here intermediate results are used to generate events for training the model (i.e. the intermediate nodes are used as positive events if it occurs on one of the active tree, or as negative events if not). Since there is a huge number of intermediate results availalbe, we only randomly select a part of them as training data. This is essentially similar to the approach in (Osborne, 2000), where there is an infeasibly large number of training events, only part of which is used in the estimation step. The exact features used in the log-linear model can significantly influence the disambiguation accuracy. In this experiment we used the same features as those used in the PCFG-S model in (Toutanova et al., 2002) (i.e., depth-1 derivation trees). The estimation of P (Ω|w) is more difficult. In a sense it is similar to a segmentation or chunking model, where the task is to segment the input into fragments. However, it is difficult to collect training data to directly train such a mo"
W07-1217,P02-1035,0,0.0603517,"lly speaking, the weights of the edges in the shortest path approach represent the quality of the local analyses and their likelihood of appearing in the analysis of the entire input. This is an interesting parallel to the parse selection models for the full analyses, where a goodness score is usually assigned to the full analysis. For example, the parse disambiguation model described in (Toutanova et al., 2002) uses a maximum entropy approach to model the conditional probability of a parse for a given input sequence P (t|w). A similar approach has also been reported in (Johnson et al., 1999; Riezler et al., 2002; Malouf and van Noord, 2004). For a given partial parse Φ = {t1 , . . . , tk }, Ω = 131 k Y P (ti |wi ) (2) i=1 Therefore, the log-probability will be log P (Φ|w) ≈ log P (Ω|w) + k X log P (ti |wi ) (3) i=1 Equation 3 indicates that the log-probability of a partial parse for a given input is the sum of the logprobability of local analyses for the sub-strings, with an additional component − log P (Ω|w) representing the conditional log-probability of the segmentation. If we use − log P (ti |wi ) as the weight for each local analysis, then the DAG shortest path algorithm will quickly find the pa"
W07-1217,P99-1052,0,\N,Missing
W07-1220,baldwin-etal-2004-road,1,0.94279,"s within NLP tasks, to arrive at a detailed (=deep) syntactic and semantic analysis of the data. It is conventionally driven by deep grammars, which encode linguistically-motivated predictions of language behaviour, are usually capable of both parsing and generation, and generate a highlevel semantic abstraction of the input data. While enjoying a resurgence of interest due to advances in parsing algorithms and stochastic parse pruning/ranking, deep grammars remain an underutilised resource predominantly because of their lack of coverage/robustness in parsing tasks. As noted in previous work (Baldwin et al., 2004), a significant cause It is often the case that the different measures lead to significantly different assessments of the quality of DLA, even for a given DLA approach. Additionally, it is far from clear how the numbers generated by these evaluation metrics correlate with actual parsing performance when the output of a given DLA method is used. This makes standardised comparison among the various different approaches to DLA very difficult, if not impossible. It is far from clear which evaluation metrics are more indicative of the true “goodness” of the lexicon. The aim of this research, theref"
W07-1220,W05-1008,1,0.791921,"y may be either too general or too specific. Underspecified lexical entries with fewer constraints allow more grammar rules to be applied while parsing, and fullyunderspecified lexical entries are computationally intractable. The whole procedure gets even more complicated when two unknown words occur next to each other, potentially allowing almost any constituent to be constructed. The evaluation of these proposals has tended to be small-scale and somewhat brittle. No concrete results have been presented relating to the improvement in grammar performance, either for parsing or for generation. Baldwin (2005) took a statistical approach to automated lexical acquisition for deep grammars. Focused on generalising the method of deriving DLA models on various secondary language resources, Baldwin used a large set of binary classifiers to predict whether a given unknown word is of a particular lexical type. This data-driven approach is grammar independent and can be scaled up for large grammars. Evaluation was via type precision, type recall, type F-measure and token accuracy, resulting in different interpretations of the data depending on the evaluation metric used. Zhang and Kordoni (2006) tackled th"
W07-1220,P98-1014,0,0.0260799,"over a carefully designed test suite and inspecting the outputs. This procedure becomes less reliable as the grammar gets larger. Also we can never expect to attain complete lexical coverage, due to language evolution and the effects of domain/genre. A static, manually compiled lexicon, therefore, becomes inevitably insufficient when faced with open domain text. In recent years, some approaches have been developed to (semi-)automatically detect and/or repair the lexical errors in linguistic grammars. Such approaches can be broadly categorised as either symbolic or statistical. Erbach (1990), Barg and Walther (1998) and Fouvry (2003) followed a unification-based symbolic approach to unknown word processing for constraint-based grammars. The basic idea is to use underspecified lexical entries, namely entries with fewer constraints, to parse whole sentences, and generate the “real” lexical entries afterwards by collecting information from the full parses. However, lexical entries generated in this way may be either too general or too specific. Underspecified lexical entries with fewer constraints allow more grammar rules to be applied while parsing, and fullyunderspecified lexical entries are computational"
W07-1220,E03-1041,0,0.0135372,"est suite and inspecting the outputs. This procedure becomes less reliable as the grammar gets larger. Also we can never expect to attain complete lexical coverage, due to language evolution and the effects of domain/genre. A static, manually compiled lexicon, therefore, becomes inevitably insufficient when faced with open domain text. In recent years, some approaches have been developed to (semi-)automatically detect and/or repair the lexical errors in linguistic grammars. Such approaches can be broadly categorised as either symbolic or statistical. Erbach (1990), Barg and Walther (1998) and Fouvry (2003) followed a unification-based symbolic approach to unknown word processing for constraint-based grammars. The basic idea is to use underspecified lexical entries, namely entries with fewer constraints, to parse whole sentences, and generate the “real” lexical entries afterwards by collecting information from the full parses. However, lexical entries generated in this way may be either too general or too specific. Underspecified lexical entries with fewer constraints allow more grammar rules to be applied while parsing, and fullyunderspecified lexical entries are computationally intractable. Th"
W07-1220,P06-1064,0,0.0613453,"Missing"
W07-1220,C02-2025,0,0.0585456,"Missing"
W07-1220,W02-1210,0,0.77711,"disation in evaluation, with commonly-used evaluation metrics including: This paper is concerned with the standardisation of evaluation metrics for lexical acquisition over precision grammars, which are attuned to actual parser performance. Specifically, we investigate the impact that lexicons at varying levels of lexical item precision and recall have on the performance of pre-existing broad-coverage precision grammars in parsing, i.e., on their coverage and accuracy. The grammars used for the experiments reported here are the LinGO English Resource Grammar (ERG; Flickinger (2000)) and JACY (Siegel and Bender, 2002), precision grammars of English and Japanese, respectively. Our results show convincingly that traditional Fscore-based evaluation of lexical acquisition does not correlate with actual parsing performance. What we argue for, therefore, is a recall-heavy interpretation of F-score in designing and optimising automated lexical acquisition algorithms. • Type precision: the proportion of correctly hypothesised lexical entries • Type recall: the proportion of gold-standard lexical entries that are correctly hypothesised • Type F-measure: the harmonic mean of the type precision and type recall • Toke"
W07-1220,P04-1057,0,0.265235,"Missing"
W07-1220,2006.jeptalnrecital-invite.2,0,0.0410557,"Missing"
W07-1220,zhang-kordoni-2006-automated,1,0.930026,"and the Lexicon: Standardising Deep Lexical Acquisition Evaluation Yi Zhang† and Timothy Baldwin‡ and Valia Kordoni† † Dept of Computational Linguistics, Saarland University and DFKI GmbH, Germany ‡ Dept of Computer Science and Software Engineering, University of Melbourne, Australia {yzhang,kordoni}@coli.uni-sb.de tim@csse.unimelb.edu.au Abstract of diminished coverage is the lack of lexical coverage. Various attempts have been made to ameliorate the deficiencies of hand-crafted lexicons. More recently, there has been an explosion of interest in deep lexical acquisition (DLA; (Baldwin, 2005; Zhang and Kordoni, 2006; van de Cruys, 2006)) for broad-coverage deep grammars, either by exploiting the linguistic information encoded in the grammar itself (in vivo), or by using secondary language resources (in vitro). Such approaches provide (semi-)automatic ways of extending the lexicon with minimal (or no) human interference. One stumbling block in DLA research has been the lack of standardisation in evaluation, with commonly-used evaluation metrics including: This paper is concerned with the standardisation of evaluation metrics for lexical acquisition over precision grammars, which are attuned to actual pars"
W07-1220,A00-2018,0,\N,Missing
W07-1220,C98-1014,0,\N,Missing
W07-2207,J97-4005,0,0.122736,"Missing"
W07-2207,P89-1018,0,0.22569,"4), who use an HPSG grammar comparable to the ERG and GG, non-local ME features, and a two-phase parse forest creation and unpacking approach. However, their unpacking phase uses a beam search to find a good (single) candidate for the best parse; in contrast— for ME models containing the types of non-local features that are most important for accurate parse selection—we avoid an approximative search and efficiently identify exactly the n-best parses. When parsing with context free grammars, a (single) parse can be retrieved from a parse forest in time linear in the length of the input string (Billot & Lang, 1989). However, as discussed in Section 2, when parsing with a unification-based grammar and packing under feature structure subsumption, the cross-product of some local ambiguities may not be globally consistent. This means that additional unifications are required at unpacking time. In principle, when parsing with a pathological grammar with a high rate of failure, extracting a single consistent parse from the forest could take exponential time (see Lang (1994) for a discussion of this issue with respect to Indexed Grammars). In the case of GG, a high rate of unification failure in unpacking is d"
W07-2207,N03-1016,0,0.0171498,"properties. This lack of monotonicity in the scores associated with sub-trees, on the one hand, is beneficial, in that performing a greedy best-first search becomes practical: in contrast, with PCFGs and their monotonically decreasing probabilities on larger sub-trees, once the parser finds the first full tree the chart necessarily has been instantiated almost completely. On the other hand, the same property prohibits the application of exact best-first techniques like A∗ search, because there is no reliable future cost estimate; in this respect, our set-up differs fundamentally from that of Klein & Manning (2003) and related PCFG parsing work. Using the unnormalized sum of ME 51 weights on a partial solution as its agenda score, effectively, means that sub-trees with low scores ‘sink’ to the bottom of the agenda; highly-ranked partial constituents, in turn, instigate the immediate creation of larger structures, and ideally the bottom-up agenda-driven search will greedily steer the parser towards full analyses with high scores. Given its heuristic nature, this procedure cannot guarantee that its n-best list of results corresponds to the globally correct rank order, but it may in practice come reasonabl"
W07-2207,J98-2004,0,0.0238923,"ple packed forest: given two ways of decomposing 6 , there will be three candidate ways of instantiating 2 and six for 4 , respectively, for a total of nine full trees. crafted grammars and inputs of average complexity the approach can perform reasonably well. Another mode of operation is to organize the parser’s search according to an agenda (i.e. priority queue) that assigns numeric scores to parsing moves (Erbach, 1991). Each such move is an application of the fundamental rule of chart parsing, combining an active and a passive edge, and the scores represent the expected ‘figure of merit’ (Caraballo & Charniak, 1998) of the resulting structure. Assuming a parse selection model of the type sketched in Section 2, we can determine the agenda priority for a parsing move according to the (unnormalized) ME score of the derivation (sub-)tree that would result from its successful execution. Note that, unlike in probabilistic context-free grammars (PCFGs), ME scores of partial trees do not necessarily decrease as the tree size increases; instead, the distribution of feature weights is in the range (−∞, +∞), centered around 0, where negative weights intuitively correspond to dis-preferred properties. This lack of m"
W07-2207,I05-1015,1,0.317703,"n-best list of results; and (c) a two-phase approach, where a complete packed for48 Proceedings of the 10th Conference on Parsing Technologies, pages 48–59, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics est is created and combined with a specialized graph search procedure to selectively enumerate results in (globally) correct rank order. Although conceptually simple, the second technique has not previously been evaluated for HPSG parsing (to the best of our knowledge). The last of these techniques, which we call selective unpacking, was first proposed by Carroll & Oepen (2005) in the context of chart-based generation. However, they only provide an account of the algorithm for local ME properties and assert that the technique should generalize to larger contexts straightforwardly. This paper describes this generalization of selective unpacking, in its application to parsing, and demonstrates that the move from features that resemble a context-free domain of locality to features of, in principle, arbitrary context size can indeed be based on the same algorithm, but the required extensions are non-trivial. The structure of the paper is as follows. Section 2 summarizes"
W07-2207,P04-1014,0,0.0216039,"algorithm empirically exhibits a linear relationship between processing time and the number of analyses unpacked at all degrees of ME feature nonlocality; in addition, compared with agendadriven best-first parsing and exhaustive parsing with post-hoc parse selection it leads to improved parsing speed, coverage, and accuracy.† 1 Background—Motivation Technology for natural language analysis using linguistically precise grammars has matured to a level of coverage and efficiency that enables parsing of large amounts of running text. Research groups working within grammatical frameworks like CCG (Clark & Curran, 2004), LFG (Riezler et al., 2002), and HPSG (Malouf & van Noord, 2004; Oepen, Flickinger, Toutanova, & Manning, 2004; Miyao, Ninomiya, & Tsujii, 2005) have successfully integrated broad-coverage computational grammars with sophisticated statistical parse selection models. The former delineate the space of possible analyses, while the latter provide a probability distribu† The first author warmly acknowledges the guidance of his PhD advisors, Valia Kordoni and Hans Uszkoreit. We are grateful to Ulrich Callmeier, Berthold Crysmann, Dan Flickinger, and Erik Velldal for many discussions and their suppo"
W07-2207,P02-1036,0,0.0182332,"a parse forest. The algorithm of Carroll & Oepen (2005) and the final one of Huang & Chiang (2005) are essentially equivalent, and turn out to be reformulations of an approach originally described by Jim´enez & Marzal (2000) (although expressed there only for grammars in Chomsky Normal Form). In this paper we have considered ME properties that extend beyond immediate dominance relations, extending up to 4 levels of grandparenting. Previous work has either assumed properties that are restricted to the minimal parse fragments (i.e. subtrees of depth one) that make up the packed representation (Geman & Johnson, 2002), or has taken a more relaxed approach by allowing non-local propConfiguration greedy best-first selective, no caching selective, with cache Unifications (#) 5980 5535 4915 Copies (#) 1447 1523 1522 Hypotheses (#) – 1245 382 Space (kbyte) 9202 27188 27176 Unpack (ms) – 70 10 Total (ms) 400 410 350 Table 5: Efficiency effects of the instantiation failure caching and propagation with GG, without grandparenting. All statistics are averages over the 1941 items that complete within the resource bounds in all three configurations. Unification, Copies, Unpack, and Total have the same interpretation a"
W07-2207,W05-1506,0,0.00687881,"zation of hyper-parameters for individual configurations would moderately improve model performance, especially for higher-order grandparenting levels with large numbers of features. 57 8 Discussion The approach to n-best parsing described in this paper takes as its point of departure recent work of Carroll & Oepen (2005), which describes an efficient algorithm for unpacking n-best trees from a forest produced by a chart-based sentence generator and containing local ME properties with associated weights. In an almost contemporaneous study, but in the context of parsing with treebank grammars, Huang & Chiang (2005) develop a series of increasingly efficient algorithms for unpacking n-best results from a weighted hypergraph representing a parse forest. The algorithm of Carroll & Oepen (2005) and the final one of Huang & Chiang (2005) are essentially equivalent, and turn out to be reformulations of an approach originally described by Jim´enez & Marzal (2000) (although expressed there only for grammars in Chomsky Normal Form). In this paper we have considered ME properties that extend beyond immediate dominance relations, extending up to 4 levels of grandparenting. Previous work has either assumed properti"
W07-2207,A00-2023,0,0.0153214,"number of hypotheses that need to be considered is doubled; as an immediate consequence, there can be up to eight distinct lexicalized variants for the decomposition 1 → h 4 3 i further up in the tree. It may look as if combinatorics will cross-multiply throughout the tree—in the worst case returning us to an exponential number of hypotheses—but this is fortunately not the case: regarding the external bi-grams of 1 , node 6 no longer participates in its left- or rightmost periphery, so variation internal to 6 is not a multiplicative factor at this level. This is essentially the observation of Langkilde (2000), and her bottom-up factoring of n-gram computation is easily incorporated into our top-down selective unpacking control structure. At the point where hypothesizeedge() invokes itself recursively (line 23 in Figure 3), its return value is now a set of lexicalized alternates, and hypothesis creation (in line 26) can take into account the local cross-product of all such alternation. 54 Including additional properties from non-local subtrees (for example higher-order n-grams and head lexicalization) is a straightforward extension of this scheme, replacing our per-edge left- and rightmost peripher"
W07-2207,W02-2018,0,0.0722633,"in “space” between exhaustive and selective unpacking. Also, the difference in “unifications” and “copies” indicates that with our selective unpacking algorithm, these expensive operations on typed feature structures are significantly reduced. In return for increased processing time (and marginal loss in coverage) when using grandparenting features, Table 3 shows some large improvements in parse selection accuracy (although the picture is less clear-cut at higher-order levels of grandparenting5 ). A balance point between efficiency 5 The models were trained using the open-source TADM package (Malouf, 2002), using default hyper-parameters for all configurations, viz. a convergence threshold of 10−8 , variance of the prior of 10−4 , and frequency cut-off of 5. It is likely that ≤ 15 words Configuration GP greedy best-first exhaustive unpacking 0 0 0 1 2 3 4 0 0 0 1 2 3 4 selective unpacking greedy best-first exhaustive unpacking &gt; 15 words selective unpacking Unifications (#) 1845 2287 1912 1913 1914 1914 1914 25233 39095 17489 17493 17493 17495 17495 Copies (#) 527 795 589 589 589 589 589 5602 15685 4422 4421 4421 4422 4422 Space (kbyte) 2328 8907 8109 8109 8109 8110 8110 24646 80832 33326 33318"
W07-2207,P02-1035,0,0.0871292,"ts a linear relationship between processing time and the number of analyses unpacked at all degrees of ME feature nonlocality; in addition, compared with agendadriven best-first parsing and exhaustive parsing with post-hoc parse selection it leads to improved parsing speed, coverage, and accuracy.† 1 Background—Motivation Technology for natural language analysis using linguistically precise grammars has matured to a level of coverage and efficiency that enables parsing of large amounts of running text. Research groups working within grammatical frameworks like CCG (Clark & Curran, 2004), LFG (Riezler et al., 2002), and HPSG (Malouf & van Noord, 2004; Oepen, Flickinger, Toutanova, & Manning, 2004; Miyao, Ninomiya, & Tsujii, 2005) have successfully integrated broad-coverage computational grammars with sophisticated statistical parse selection models. The former delineate the space of possible analyses, while the latter provide a probability distribu† The first author warmly acknowledges the guidance of his PhD advisors, Valia Kordoni and Hans Uszkoreit. We are grateful to Ulrich Callmeier, Berthold Crysmann, Dan Flickinger, and Erik Velldal for many discussions and their support. We thank Ron Kaplan, Mar"
W07-2207,P99-1069,0,0.140483,"t be globally consistent. Assume for example that, in Figure 2, edges 6 and 8 subsume 7 and 9 , respectively; combining 7 and 9 into the same tree during unpacking can in principle fail. Thus, unpacking effectively needs to deterministically replay unifications, but this extra expense in our experience is negligible when compared to the decreased cost of constructing the forest under subsumption. In Section 3 we argue that this very property, in addition to increasing parsing efficiency, interacts beneficially with parse selection and on-demand enumeration of results in rank order. Following (Johnson et al., 1999), a conditional ME model of the probabilities of trees {t1 . . . tn } for a string s, and assuming a set of feature functions {f1 . . . fm } with corresponding weights {λ1 . . . λm }, is defined as: P exp j λj fj (ti ) P p(ti |s) = Pn (1) k=1 exp j λj fj (tk ) 2 This property of parse forests is not a prerequisite of the chart parsing framework. The basic CKY procedure (Kasami, 1965), for example, as well as many unification-based adaptations (e.g. the Core Language Engine; Moore & Alshawi, 1992) merely record the local category of each edge, which is sufficient for the recognition task and si"
W07-2207,J08-1002,0,\N,Missing
W07-2207,J07-4004,0,\N,Missing
W07-2207,P08-1067,0,\N,Missing
W07-2207,W01-1812,0,\N,Missing
W08-1708,J03-3001,0,0.0459102,"archy. The grammar originates from (M¨uller and Kasper, 2000), but continued to improve after the end of the Verbmobil project (Wahlster, 2000) and it currently consists of 5K types, 115 rules and the lexicon contains approximately 35K entries. These entries belong to 386 distinct lexical types. In the experiments we report here two corpora of different kind and size have been used. The first one has been extracted from the Frankfurter Rundschau newspaper and contains about 614K sentences that have between 5 and 20 tokens. The second corpus is a subset of the German part of the Wacky project (Kilgarriff and Grefenstette, 2003). The Wacky project aims at the creation of large corpora for different languages, including German, from various web sources, such as online newspapers and magazines, legal texts, internet fora, university and science web sites, etc. The German part, named deWaC (Web as Corpus), contains about 93M sentences and 1.65 billion tokens. The subset used in our experiments is extracted by randomly selecting 2.57M sentences that have between 4 and 30 tokens. These corpora have been chosen because it is interesting to observe the grammar performance on a relatively balanced newspaper corpus that does"
W08-1708,P04-1057,0,0.297812,"Missing"
W08-1708,zhang-kordoni-2006-automated,1,0.941759,"and Re-Usability of Lexicalised Grammars Kostadin Cholakov† , Valia Kordoni†‡ , Yi Zhang†‡ † Department of Computational Linguistics, Saarland University, Germany ‡ LT-Lab, DFKI GmbH, Germany {kostadin,kordoni,yzhang}@coli.uni-sb.de Abstract In recent years, various techniques and resources have been developed in order to improve robustness of deep grammars for real-life applications in various domains. Nevertheless, low coverage of such grammars remains the main hindrance to their employment in open domain natural language processing. (Baldwin et al., 2004), as well as (van Noord, 2004) and (Zhang and Kordoni, 2006) have clearly shown that the majority of parsing failures with large-scale deep grammars are caused by missing or wrong entries in the lexicons accompanying grammars like the aforementioned ones. Based on these findings, it has become clear that it is crucial to explore and develop efficient methods for automated (Deep) Lexical Acquisition (henceforward (D)LA), the process of automatically recovering missing entries in the lexicons of deep grammars. Recently, various high-quality DLA approaches have been proposed. (Baldwin, 2005), as well as (Zhang and Kordoni, 2006), (van de Cruys, 2006) and"
W08-1708,baldwin-etal-2004-road,0,0.0130114,"pendent Deep Linguistic Processing: Ensuring Portability and Re-Usability of Lexicalised Grammars Kostadin Cholakov† , Valia Kordoni†‡ , Yi Zhang†‡ † Department of Computational Linguistics, Saarland University, Germany ‡ LT-Lab, DFKI GmbH, Germany {kostadin,kordoni,yzhang}@coli.uni-sb.de Abstract In recent years, various techniques and resources have been developed in order to improve robustness of deep grammars for real-life applications in various domains. Nevertheless, low coverage of such grammars remains the main hindrance to their employment in open domain natural language processing. (Baldwin et al., 2004), as well as (van Noord, 2004) and (Zhang and Kordoni, 2006) have clearly shown that the majority of parsing failures with large-scale deep grammars are caused by missing or wrong entries in the lexicons accompanying grammars like the aforementioned ones. Based on these findings, it has become clear that it is crucial to explore and develop efficient methods for automated (Deep) Lexical Acquisition (henceforward (D)LA), the process of automatically recovering missing entries in the lexicons of deep grammars. Recently, various high-quality DLA approaches have been proposed. (Baldwin, 2005), as"
W08-1708,W05-1008,0,0.31703,"ldwin et al., 2004), as well as (van Noord, 2004) and (Zhang and Kordoni, 2006) have clearly shown that the majority of parsing failures with large-scale deep grammars are caused by missing or wrong entries in the lexicons accompanying grammars like the aforementioned ones. Based on these findings, it has become clear that it is crucial to explore and develop efficient methods for automated (Deep) Lexical Acquisition (henceforward (D)LA), the process of automatically recovering missing entries in the lexicons of deep grammars. Recently, various high-quality DLA approaches have been proposed. (Baldwin, 2005), as well as (Zhang and Kordoni, 2006), (van de Cruys, 2006) and (Nicholson et al., 2008) describe efficient methods towards the task of lexicon acquisition for large-scale deep grammars for English, Dutch and German. They treat DLA as a classification task and make use of various robust and efficient machine learning techniques to perform the acquisition process. However, it is our claim that to achieve better and more practically useful results, apart from good learning algorithms, we also need to incorporate into the learning process fine-grained linguistic information which deep grammars i"
W08-1708,A00-1031,0,0.0885319,"Missing"
W08-1708,nicholson-etal-2008-evaluating,1,\N,Missing
W08-1708,copestake-flickinger-2000-open,0,\N,Missing
W08-2126,C04-1186,0,0.0550371,"ypes (POSes marked as predicates for at least 50 times in the training set). This helps to significantly improve the system efficiency in both training and prediction time without sacrificing prediction accuracy. It should be noted that the prediction of nominal predicates are generally much more difficult (based on CoNLL 2008 shared task annotation). The PI model achieved 96.32 F-score on WSJ with verbal predicates, but only 84.74 on nominal ones. Argument Identification After PI, the arguments to the predicted predicates are identified with the AI component. Similar to the approach taken in Hacioglu (2004), we use a statistical classifier to select from a set of candidate nodes in a dependency tree. However, instead of selecting from a set of neighboring nodes from the predicate word 2 , we define the concept of argument path as a chain of dependency relations from the predicate to the argument in the dependency tree. For instance, an argument path [ |] indicates that if the predicate is syntactically depending as  on a node which has a  child, then the  node 2 Hacioglu (2004) defines a tree-structured family of a predicate as a measure of locality. It is a set of dependency rela"
W08-2126,H05-1066,0,0.0287923,"predictions. In particular, the second part can be further divided into four stages: predicate identification (PI), argument identification (AI), argument classification (AC), and predicate classification (PC). Maximum entropy-based machine learning techniques are used in both components which we will see in detail in the following sections. 3 • Root attachments: the number of tokens attached to the ROOT node by the parser in one sentence Syntactic Dependency Parsing For obtaining syntactic dependencies, we have combined the results of two state-of-the-art dependency parsers: the MST parser (McDonald et al., 2005) and the MaltParser (Nivre et al., 2007). The MST parser formalizes dependency parsing as searching for maximum spanning trees (MSTs) in directed graphs. A major advantage of their framework is the ability to naturally and efficiently model both projective and non-projective parses. To learn these structures they used online largemargin learning that empirically provides state-ofthe-art performance. The MaltParser is a transition-based incremental dependency parser, which is language-independent and data-driven. It contains a deterministic algorithm, which can be viewed as a variant of the bas"
W08-2126,W08-2121,0,0.168141,"Missing"
W08-2126,U05-1006,1,0.88562,"Missing"
W08-2126,W07-1217,1,0.85909,"Missing"
W09-0405,W07-0726,1,0.843237,"ata is included in this year’s With the wealth of machine translation systems available nowadays (many of them online and for free), it makes increasing sense to investigate clever ways of combining them. Obviously, the main objective lies in finding out how to integrate the respective advantages of different approaches: Statistical machine translation (SMT) and rulebased machine translation (RBMT) systems often have complementary characteristics. Previous work on building hybrid systems includes, among others, approaches using reranking, regeneration with an SMT decoder (Eisele et al., 2008; Chen et al., 2007), and confusion networks (Matusov et al., 2006; Rosti et al., 2007; He et al., 2008). The approach by (Eisele et al., 2008) aimed specifically at filling lexical gaps in an SMT system with information from a number of RBMT systems. The output of the RBMT engines was word-aligned with the input, yielding a total of seven phrase tables which where simply concatenated to expand the phrase table constructed from the training corpus. This approach differs from the confusion network approaches mainly in that the final hypotheses do not necessarily follow any of the input translations as the skeleton"
W09-0405,W08-0328,1,0.915116,"ction 4 its application in a number of experiments. Finally, Section 5 concludes this paper with a summary and some thoughts on future work. We present a simple method for generating translations with the Moses toolkit (Koehn et al., 2007) from existing hypotheses produced by other translation engines. As the structures underlying these translation engines are not known, an evaluationbased strategy is applied to select systems for combination. The experiments show promising improvements in terms of BLEU. 1 2 Introduction Integrating Multiple Systems of Unknown Type and Quality When comparing (Eisele et al., 2008) to the present work, our proposal is more general in a way that the requirement for knowledge about the systems is minimum. The types and the identities of the participated systems are assumed unknown. Accordingly, we are not able to restrict ourselves to a certain class of systems as (Eisele et al., 2008) did. We rely on a standard phrase-based SMT framework to extract the valuable pieces from the system outputs. These extracted segments are also used to improve an existing SMT system that we have access to. While (Eisele et al., 2008) included translations from all of a fixed number of RBMT"
W09-0405,P07-1040,0,0.046861,"ion systems available nowadays (many of them online and for free), it makes increasing sense to investigate clever ways of combining them. Obviously, the main objective lies in finding out how to integrate the respective advantages of different approaches: Statistical machine translation (SMT) and rulebased machine translation (RBMT) systems often have complementary characteristics. Previous work on building hybrid systems includes, among others, approaches using reranking, regeneration with an SMT decoder (Eisele et al., 2008; Chen et al., 2007), and confusion networks (Matusov et al., 2006; Rosti et al., 2007; He et al., 2008). The approach by (Eisele et al., 2008) aimed specifically at filling lexical gaps in an SMT system with information from a number of RBMT systems. The output of the RBMT engines was word-aligned with the input, yielding a total of seven phrase tables which where simply concatenated to expand the phrase table constructed from the training corpus. This approach differs from the confusion network approaches mainly in that the final hypotheses do not necessarily follow any of the input translations as the skeleton. On the other hand, it emphasizes that the additional translation"
W09-0405,W08-0332,0,0.037797,"h directions. The phrases are extracted from the intersection of the alignments with the “grow” heuristics. In addition, we also generate a reordering model with the default configuration as included in the Moses toolkit. This “hypothesis” translation model can already be used by the 3.3 System evaluation Since both the system translations and the reference translations are available for the tuning 43 set, we first compare each output to the reference translation using BLEU (Papineni et al., 2001) and METEOR (Banerjee and Lavie, 2005) and a combined scoring scheme provided by the ULC toolkit (Gimenez and Marquez, 2008). In our experiments, we selected a subset of 5 systems for the combination, in most cases, based on BLEU. On the other hand, some systems may be designed in a way that they deliver interesting unique translation segments. Therefore, we also measure the similarity among system outputs as shown in Table 2 in a given collection by calculating average similarity scores across every pair of outputs. Num. Median Range Top 5 Median Range de-en 20 19.87 16.37 de-en 22.26 4.31 fr-en 23 26.55 17.06 fr-en 27.93 4.76 es-en 28 22.50 9.74 es-en 26.43 5.71 en-de 15 13.78 4.75 en-de 15.21 1.71 en-fr 16 24.76"
W09-0405,D08-1011,0,0.0384327,"e nowadays (many of them online and for free), it makes increasing sense to investigate clever ways of combining them. Obviously, the main objective lies in finding out how to integrate the respective advantages of different approaches: Statistical machine translation (SMT) and rulebased machine translation (RBMT) systems often have complementary characteristics. Previous work on building hybrid systems includes, among others, approaches using reranking, regeneration with an SMT decoder (Eisele et al., 2008; Chen et al., 2007), and confusion networks (Matusov et al., 2006; Rosti et al., 2007; He et al., 2008). The approach by (Eisele et al., 2008) aimed specifically at filling lexical gaps in an SMT system with information from a number of RBMT systems. The output of the RBMT engines was word-aligned with the input, yielding a total of seven phrase tables which where simply concatenated to expand the phrase table constructed from the training corpus. This approach differs from the confusion network approaches mainly in that the final hypotheses do not necessarily follow any of the input translations as the skeleton. On the other hand, it emphasizes that the additional translations should be produc"
W09-0405,W07-0733,0,0.0224942,"ur hope is that the additional translation hypotheses could bring in new phrases or, more generally, new information that was not contained in the Europarl model. In order to facilitate comparisons, we use in-domain LMs for all setups. We investigate two alternative ways of integrating the additional phrases into the existing SMT system: One is to take the hypothesis translation model described in Section 3.1, the other is to construct system-specific models constructed with only translations from one system at a time. Although the Moses decoder is able to work with two phrase tables at once (Koehn and Schroeder, 2007), it is difficult to use this method when there is more than one additional model. The method requires tuning on at least six more features, which expands the search space for the translation task unnecessarily. We instead integrate the translation models from multiple sources by extending the phrase table. In contrast to the prior approach presented in (Chen et al., 2007) and (Eisele et al., 2008) which concatenates the phrase tables and adds new features as system markers, our extension method avoids duplicate entries in the final combined table. Given a set of hypothesis translation models"
W09-0405,2005.mtsummit-papers.11,0,0.132271,"ing sentence set. task. In this paper, we use the Moses decoder to construct translations from the given system outputs. We mainly propose two slightly different ways: One is to construct translation models solely from the given translations and the other is to extend an existing translation model with these additional translations. 3 3.2 Sometimes, the goal of system combination is not only to produce a translation but also to improve one of the systems. In this paper, we aim at incorporating the additional system outputs to improve an out-of-domain SMT system trained on the Europarl corpus (Koehn, 2005). Our hope is that the additional translation hypotheses could bring in new phrases or, more generally, new information that was not contained in the Europarl model. In order to facilitate comparisons, we use in-domain LMs for all setups. We investigate two alternative ways of integrating the additional phrases into the existing SMT system: One is to take the hypothesis translation model described in Section 3.1, the other is to construct system-specific models constructed with only translations from one system at a time. Although the Moses decoder is able to work with two phrase tables at onc"
W09-0405,E06-1005,0,0.0367832,"th of machine translation systems available nowadays (many of them online and for free), it makes increasing sense to investigate clever ways of combining them. Obviously, the main objective lies in finding out how to integrate the respective advantages of different approaches: Statistical machine translation (SMT) and rulebased machine translation (RBMT) systems often have complementary characteristics. Previous work on building hybrid systems includes, among others, approaches using reranking, regeneration with an SMT decoder (Eisele et al., 2008; Chen et al., 2007), and confusion networks (Matusov et al., 2006; Rosti et al., 2007; He et al., 2008). The approach by (Eisele et al., 2008) aimed specifically at filling lexical gaps in an SMT system with information from a number of RBMT systems. The output of the RBMT engines was word-aligned with the input, yielding a total of seven phrase tables which where simply concatenated to expand the phrase table constructed from the training corpus. This approach differs from the confusion network approaches mainly in that the final hypotheses do not necessarily follow any of the input translations as the skeleton. On the other hand, it emphasizes that the ad"
W09-0405,J03-1002,0,0.0039123,"parseness problem). However, in the system combination task, this is no longer an issue as the system only needs to translate sentences within the data set. When more translation engines are available, the size of this set becomes larger. Hence, we collect translations from all available systems and pair them with the corresponding input text, thus forming a medium-sized “hypothesis” corpus. Our system starts processing this corpus with a standard phrase-based SMT setup, using the Moses toolkit (Koehn et al., 2007). The hypothesis corpus is first tokenized and lowercased. Then, we run GIZA++ (Och and Ney, 2003) on the corpus to obtain word alignments in both directions. The phrases are extracted from the intersection of the alignments with the “grow” heuristics. In addition, we also generate a reordering model with the default configuration as included in the Moses toolkit. This “hypothesis” translation model can already be used by the 3.3 System evaluation Since both the system translations and the reference translations are available for the tuning 43 set, we first compare each output to the reference translation using BLEU (Papineni et al., 2001) and METEOR (Banerjee and Lavie, 2005) and a combin"
W09-0405,P03-1021,0,0.0175625,"15.21 1.71 en-fr 16 24.76 11.05 en-fr 26.62 0.68 input texts. Moreover, we also generate “hypothesis” LMs solely based on the given system outputs, that is, LMs that model how the candidate systems convey information in the target language. These LMs do not require any additional training data. Therefore, we do not require any training data other than the given system outputs by using the “hypothesis” language model and the “hypothesis” translation model. 3.5 After building the models, it is essential to tune the SMT system to optimize the feature weights. We use Minimal Error Rate Training (Och, 2003) to maximize BLEU on the complete development data. Unlike the standard tuning procedure, we do not tune the final system directly. Instead, we obtain the weights using models built from the tuning portion of the system outputs. For each combination variant, we first train models on the provided outputs corresponding to the tuning set. This system, called the tuning system, is also tuned on the tuning set. The initial weights of any additional features not included in the standard setting are set to 0. We then adapt the weights to the system built with translations corresponding to the test se"
W09-0405,P02-1040,0,\N,Missing
W09-0405,W05-0909,0,\N,Missing
W09-0405,P07-2045,0,\N,Missing
W09-1201,burchardt-etal-2006-salsa,1,0.483589,"Missing"
W09-1201,D07-1101,0,0.391748,"Missing"
W09-1201,W09-1202,0,0.0278745,"order syntactic parsing and a particular setting for Catalan 16 and Spanish. (Gesmundo et al., 2009) use an incremental parsing model with synchronous syntactic and semantic derivations and a joint probability model for syntactic and semantic dependency structures. The system uses a single input queue but two separate stacks and synchronizes syntactic and semantic derivations at every word. The synchronous derivations are modeled with an Incremental Sigmoid Belief Network that has latent variables for both syntactic and semantic states and connections from syntax to semantics and vice versa. (Dai et al., 2009) designed an iterative system to exploit the inter-connections between the different subtasks of the CoNLL shared task. The idea is to decompose the joint learning problem into four subtasks – syntactic dependency identification, syntactic dependency labeling, semantic dependency identification and semantic dependency labeling. The initial step is to use a pipeline approach to use the input of one subtask as input to the next, in the order specified. The iterative steps then use additional features that are not available in the initial step to improve the accuracy of the overall system. For ex"
W09-1201,W09-1205,0,0.222475,"al token; and (c), the existence of an edge between each pair of tokens. Subsequently, they combine the (possibly conflicting) output of the three classifiers by a ranking approach to determine the most likely structure that meets all well-formedness constraints. (Llu´ıs et al., 2009) present a joint approach based on an extension of Eisner’s parser to accommodate also semantic dependency labels. This architecture is similar to the one presented by the same authors in the past edition, with the extension to a second-order syntactic parsing and a particular setting for Catalan 16 and Spanish. (Gesmundo et al., 2009) use an incremental parsing model with synchronous syntactic and semantic derivations and a joint probability model for syntactic and semantic dependency structures. The system uses a single input queue but two separate stacks and synchronizes syntactic and semantic derivations at every word. The synchronous derivations are modeled with an Incremental Sigmoid Belief Network that has latent variables for both syntactic and semantic states and connections from syntax to semantics and vice versa. (Dai et al., 2009) designed an iterative system to exploit the inter-connections between the differen"
W09-1201,W09-1212,1,0.83271,"Missing"
W09-1201,S07-1008,1,0.697359,"Missing"
W09-1201,H05-1066,1,0.168004,"Missing"
W09-1201,W04-2705,1,0.527791,"Missing"
W09-1201,W09-1219,0,0.0294123,"Missing"
W09-1201,H05-1108,1,0.506508,"y, adding further manual labels where necessary. Then, we used frequency and grammatical realization information to map the remaining roles onto higher-numbered Arg roles. We considerably simplified the annotations provided by SALSA, which use a rather complex annotation scheme. In particular, we removed annotation for multi-word expressions (which may be non-contiguous), annotations involving multiple frames for the same predicate (metaphors, underspecification), and inter-sentence roles. The out-of-domain dataset was taken from a study on the multi-lingual projection of FrameNet annotation (Pado and Lapata, 2005). It is sampled from the EUROPARL corpus and was chosen to maximize the lexical coverage, i.e., it contains of a large number of infrequent predicates. Both syntactic and semantic structure were annotated manually, in the TIGER and SALSA format, respectively. Since it uses a simplified annotation schemes, we did not have to discard any annotation. For both datasets, we converted the syntactic TIGER (Brants et al., 2002) representations into dependencies with a similar set of head-finding rules used for the preparation of the CoNLL-X shared task German dataset. Minor modifications (for the con1"
W09-1201,C08-1085,1,0.175934,"Missing"
W09-1201,J05-1004,0,0.213522,"nnotation of the Wall Street Journal corpus (Weischedel and Brunstein, 2005) takes the form of SGML inline markup of text, tokenized to be completely compatible with the Penn Treebank annotation. For the CoNLL-2008 shared task evaluation, this corpus was extended by the task organizers to cover the subset of the Brown corpus used as a secondary testing dataset. From this corpus we only used NE boundaries to derive NAME dependencies between NE tokens, e.g., we create a NAME dependency from Mary to Smith given the NE mention Mary Smith. • Proposition Bank I (PropBank) – The PropBank annotation (Palmer et al., 2005) classifies the arguments of all the main verbs in the Penn Treebank corpus, other than be. Arguments are numbered (Arg0, Arg1, . . .) based on lexical entries or frame files. Different sets of arguments are assumed for different rolesets. Dependent constituents that fall into categories independent of the lexical entries are classified as various types of adjuncts (ArgM-TMP, -ADV, etc.). • NomBank – NomBank annotation (Meyers et al., 2004) uses essentially the same framework as PropBank to annotate arguments of nouns. Differences between PropBank and NomBank stem from differences between noun"
W09-1201,E09-1087,1,0.646818,"Missing"
W09-1201,W08-2121,1,0.597132,"Missing"
W09-1201,taule-etal-2008-ancora,0,0.543017,"Missing"
W09-1201,cmejrek-etal-2004-prague,1,0.63993,"Missing"
W09-1201,kawahara-etal-2002-construction,1,\N,Missing
W09-1201,J93-2004,0,\N,Missing
W09-1201,D07-1096,1,\N,Missing
W09-1204,adolphs-etal-2008-fine,1,0.829188,"e, rejects the conventional assumptions underlying the PTB (and derived tools). It opts for an analysis of punctuation akin to affixation (rather than as standalone tokens), does not break up contracted negated auxiliaries, and splits hyphenated words like illadvised into two tokens (the hyphen being part of the first component). Thus, a string like Don’t you! in the CoNLL data is tokenized as the four-element sequence hdo, n’t, you, !i,2 whereas the ERG analysis has only two leaf nodes: hdon’t, you!i. Fortunately, the DELPH-IN toolchain recently incorporated a mechanism called chart mapping (Adolphs et al., 2008), which allows one to map flexibly from ‘external’ input to grammar-internal assumptions, while keeping track of external token identities and their contributions to the final analysis. The February 2009 release of the ERG already had this machinery in place (with the goal of supporting extant, PTB-trained PoS taggers in pre-processing input to the deep parser), and we found that only a tiny number of additional chart mapping rules was required to ‘fix up’ CoNLL-specific deviations from the PTB tradition. With the help of the original developers, we created new chart mapping configurations for"
W09-1204,burchardt-etal-2006-salsa,0,0.127541,"Missing"
W09-1204,W09-1201,1,0.865296,"Missing"
W09-1204,kawahara-etal-2002-construction,0,0.0194058,"Missing"
W09-1204,W02-2018,0,0.00915684,"mprovement observed after using the HPSG features. Therefore, we did not include it in the final submission. 5 Semantic Role Labeling The semantic role labeling component used in the submitted system is similar to the one described by Zhang et al. (2008). Since predicates are indicated in the data, the predicate identification module is removed from this year’s system. Argument identification, argument classification and predicate classification are the three sub-components in the pipeline. All of them are MaxEnt-based classifiers. For parameter estimation, we use the open source TADM system (Malouf, 2002). The active features used in various steps of SRL are fine tuned separately for different languages using development datasets. The significance of feature types varies across languages and datasets. SYN SRL Closed ood Closed ood Open ood ca 82.67 67.34 - zh 73.63 73.20 - cs 75.58 71.29 78.28 77.78 - en 87.90 81.50 77.85 67.07 78.13 (↑0.28) 68.11 (↑1.04) de 84.57 75.06 62.95 54.87 64.31 (↑1.36) 58.42 (↑3.55) ja 91.47 64.71 65.95 (↑1.24) - es 82.69 67.81 68.24 (↑0.43) - Table 2: Summary of System Performance on Multiple Languages In the open challenge, two groups of extra features from HPSG pa"
W09-1204,J05-1004,0,0.0711747,"Missing"
W09-1204,W02-1210,0,0.017282,"atively detailed, hand-coded linguistic knowledge— including lexical argument structure and the linking of syntactic functions to thematic arguments—and are intended as general-purpose resources, applicable to both parsing and generation. Semantics in DELPH-IN is cast in the Minimal Recursion Semantics framework (MRS; Copestake, Flickinger, Pollard, & Sag, 2005), essentially predicate – argument structures with provision for underspecified scopal relations. For the 2009 ‘open’ task, we used the DELPH-IN grammars for English (ERG; Flickinger, 2000), German (GG; Crysmann, 2005), Japanese (JaCY; Siegel & Bender, 2002), and Spanish (SRG; Marimon, Bel, & Seghezzi, 2007). The grammars vary in their stage of development: the ERG comprises some 15 years of continuous development, whereas work on the SRG only started about five years ago, with GG and JaCY ranging somewhere inbetween. 3.1 Overall Setup We applied the DELPH-IN grammars to the CoNLL data using the PET parser (Callmeier, 2002) running 1 See http://www.delph-in.net for background. 32 it through the [incr tsdb()] environment (Oepen & Carroll, 2000), for parallelization and distribution. Also, [incr tsdb()] provides facilities for (re-)training the Max"
W09-1204,W08-2121,0,0.100771,"Missing"
W09-1204,taule-etal-2008-ancora,0,0.0657859,"Missing"
W09-1204,W07-2207,1,0.835309,"with the TADM software, using tenfold cross-validation and exact match ranking accuracy (against the binarized training distribution) to optimize estimation hyper-parameters 3.3 Deep Parsing Features HPSG parsing coverage and average cpu time per input for the four languages with DELPH-IN grammars are summarized in Table 1. The PoS-based unknown word mechanism was active for all grammars but no other robustness measures (which tend to lower the quality of results) were used, i.e. only complete spanning HPSG analyses were accepted. Parse times are for 1-best parsing, using selective unpacking (Zhang, Oepen, & Carroll, 2007). HPSG parsing outputs are available in several different forms. We investigated two types of structures: syntactic derivations and MRS meaningrepresentations. Representative features were extracted from both structures and selectively used in the statistical syntactic dependency parsing and semantic role labeling modules for the ‘open’ challenge. 3 We also experimented with using DA scores directly as empirical probabilities in the training distribution (or some function of DA, to make it fall off more sharply), but none of these methods seemed to further improve parse selection performance."
W09-1204,W08-2126,1,0.789115,"Cluster of Multimodal Computing and Interaction for the support of the work. The second author is funded by the PIRE PhD scholarship program. Participation of the third author in this work was supported by the University of Oslo, as part of its research partnership with the Center for the Study of Language and Information at Stanford University. Our deep parsing experimentation was executed on the TITAN HPC facilities at the University of Oslo. 31 performance. This makes the task a nice testbed for the cross-fertilization of various language processing techniques. As an example of such work, Zhang et al. (2008) have shown in the past that deep linguistic parsing outputs can be integrated to help improve the performance of the English semantic role labeling task. But several questions remain unanswered. First, the integration only experimented with the semantic role labeling part of the task. It is not clear whether syntactic dependency parsing can also benefit from grammar-based parsing results. Second, the English grammar used to achieve the improvement is one of the largest and most mature hand-crafted linguistic grammars. It is not clear whether similar improvements can be achieved with less deve"
W09-2605,P06-1064,0,0.266817,"s in the right bracket. The unary rule slash-subj moves the required subject towards the SLASH value, so that it can be discharged in the Vorfeld by the head-filler schema. ‘m¨ussen’ is an example of an argument attraction verb, because it pulls the valence feature (containing SUBJ, SUBCAT etc; not visible in the diagram) to itself. The head-cluster rule assures that the VAL value then percolates upwards. Because ‘Amerikaner’ does not have a specifier, a separate unary rule (no-det) takes care of discharging the SPEC feature, before it can be combined with the filler-head rule. As opposed to (Hockenmaier, 2006), this study wird ausschlafen’ (‘He will sleep in’). In such verbs, the word ‘zu’ (which translates to the English ‘to’ in ‘to sleep’) can be infixed as well: ‘er versucht auszuschlafen’ (‘He tries to sleep in’). These characteristics make German a comparatively complex language to parse with CFGs: more variants of the same lemma have to be memorised, and the expansion of production rules will be more diverse, with a less peaked statistical distribution. Efforts have been made to adapt existing CFG models to German (Dubey and Keller, 2003), but the results still don’t compare to state-of-thear"
W09-2605,W02-1503,0,0.0756529,"Missing"
W09-2605,P04-1041,0,0.101632,"Missing"
W09-2605,P95-1037,0,0.120179,"t as reasonably possible. employs a core lexicon for words that have marked semantic behaviour. These are usually closed word classes, and include items such as raising and auxiliary verbs, possessives, reflexives, articles, complementisers etc. The size of this core lexicon is around 550 words. Note that, because the core lexicon only contains function words, its coverage is negligible without additional entries. 3 3.1 3.2 Preprocessing A number of changes had to be applied to the treebank to facilitate the read-off procedure: • A heuristic head-finding procedure is applied in the spirit of (Magerman, 1995). We use priority lists to find the NP’s head, determiner, appositions and modifiers. PPs and CPs are also split into a head and its dependent. Derivation of the lexicon The Tiger treebank The Tiger treebank (Brants et al., 2002) is a treebank that embraces the concept of constituency, but can have crossing branches, i.e. the tree might be non-projective. This allowed the annotators to capture the German free word order. Around onethird of the sentences received a non-projective analysis. An example can be found in figure 2. Additionally, it annotates each branch with a syntactic function. The"
W09-2605,A00-2022,0,0.265175,"Missing"
W09-2605,P05-1022,0,0.0723825,"Missing"
W09-2605,P03-1013,0,0.0114871,"t can be combined with the filler-head rule. As opposed to (Hockenmaier, 2006), this study wird ausschlafen’ (‘He will sleep in’). In such verbs, the word ‘zu’ (which translates to the English ‘to’ in ‘to sleep’) can be infixed as well: ‘er versucht auszuschlafen’ (‘He tries to sleep in’). These characteristics make German a comparatively complex language to parse with CFGs: more variants of the same lemma have to be memorised, and the expansion of production rules will be more diverse, with a less peaked statistical distribution. Efforts have been made to adapt existing CFG models to German (Dubey and Keller, 2003), but the results still don’t compare to state-of-theart parsing of English. 2.3 Structure of the core grammar The grammar uses the main tenets from Headdriven Phrase Structure Grammar (Pollard and Sag, 1994). However, different from earlier deep grammar extraction studies, more sophisticated structures are added. M¨uller (2002) proposes a new schema (head-cluster) to account for verb clusters in the right bracket, which includes the possibility to merge subcategorisation frames of e.g. object-control verbs and its dependent verb. Separate rules for determinerless NPs, genitive modification, c"
W09-2605,W07-2207,1,0.910214,"Missing"
W09-2605,W09-1204,1,0.883129,"Missing"
W09-2605,hockenmaier-steedman-2002-acquiring,0,0.172457,"Missing"
W09-2605,A00-1031,0,\N,Missing
W09-2605,P02-1035,0,\N,Missing
W09-3032,J93-2004,0,0.0319203,"Missing"
W09-3032,C02-2025,0,0.797141,"completely independent annotation of the WSJ texts built without conversion from the original PTB trees. Another popular alternative way to aid treebank development is to use automatic parsing outputs as guidance. Many state-of-the-art parsers are able to efficiently produce large amount of annotated syntactic structures with relatively high accuracy. This approach has changed the role of human annotation from a labour-intensive task of drawing trees from scratch to a more intelligencedemanding task of correcting parsing errors, or eliminating unwanted ambiguities (cf., the Redwoods Treebank (Oepen et al., 2002)). It is our aim in this on-going project to build a HPSG treebank for the WSJ sections of the PTB based on the hand-written ERG for English. 3 our project for parsing the WSJ sections of the PTB, and [incr tsdb()] (Oepen, 2001), the grammar performance profiling system we are using, which comes with a complete set of GUI-based tools for treebanking. Version control system also plays an important role in this project. 3.2 The sentences from the Wall Street Journal Sections of the Penn Treebank are extracted with their original tokenization, with each word paired with a part-of-speech tag. Each"
W09-3032,zhang-kordoni-2008-robust,1,0.897023,"Missing"
W09-3032,adolphs-etal-2008-fine,0,\N,Missing
W09-3032,2000.iwpt-1.19,0,\N,Missing
W09-3103,P03-1050,0,0.015428,". The result is a system capable of allowing a user to construct readability software for languages like Indonesian, for example, even if that user does not speak Indonesian — this is possible due to the large parallel English-Indonesian corpus on Wikipedia. sentence alignment via Pointwise Mutual Information (PMI). Their approach provides broad coverage of cross-linguistic morphology, which has implications for dictionary expansion tasks; problems encountered in dealing with the agglutinative morphology of Inuktitut are suggestive of the myriad issues arising from cross-language comparisons. Rogati et al. (2003) present an unsupervised learning approach to building an Arabic stemmer, modeled on statistical machine translation. The authors use an English stemmer and a small parallel corpus as training resources, with no parallel text necessary after the training phase. Additional monolingual texts can be incorporated to improve the stemmer by allowing it to adapt to a specific domain. 7 Conclusion We have proposed a general framework to quickly construct a standalone readability classifier for an arbitrary (and possibly unfamiliar) language using statistical language models based both on monolingual a"
W09-3103,H01-1035,0,0.0212871,"CHRC Chinese documents is 2.48. This number compares favorably against both of the baseline algorithms. Recall that the actual RL-tagging procedure has been treated as a document retrieval task, using Vector Space Cosine similarity. As such, RLs are not simply “picked out” for each document: each document receives a cosine similarity score for each RL, calculated on the basis of its similarity to 6 Related Work The method described above builds on recent work that has exploited the web and parallel corpora to develop language technologies for minority languages (Trosterud (2002), inter alia). Yarowsky et al. (2001) describe a system and a set of algorithms for automatically deriving autonomous monolingual POS-taggers, base nounphrase bracketers, named-entity taggers, and morphological analyzers for an arbitrary target language. Bilingual text corpora are treated with existing text analysis tools for English, and their output is projected onto the target language via statistically derived word alignments. Their approach is especially interesting insofar as the system does not require hand-annotation of targetlanguage training data or virtually any targetlanguage-specific knowledge or resources. Martin et"
W09-3103,N04-1025,0,0.231543,"h a typical user does not need to have access to sets of bilingual documents for the system to run successfully, we circumvented both the lack of off-the-shelf Chinese readability labeling software and the lack of labeled Chinese documents for the evaluation of the results of our system by using a high quality translated parallel document set. Since RLs are rough measures of semantic and structural complexity, we assume they should be approximately if not exactly the same for a given document and its translation in a different language, an extension of the ideas in Collins-Thompson and Callan (2004). Based on this assumption, we can accurately compare the RLs of the translated CCHRC Chinese medical documents to the RLs of the original English documents, which we call the “true RLs” of the testing documents. Then, we concatenated the English OHSU87 documents in each RL group. The tf*idf formula was used to select the K English words most representative of each RL group. Next, we automatically selected a set of Chinese words for each RL class to create a corresponding Chinese readability model by passing each English word through the CLIR system, EX CLAIM , to retrieve the most relevant En"
W09-3103,W03-0320,0,0.0250238,"l. (2001) describe a system and a set of algorithms for automatically deriving autonomous monolingual POS-taggers, base nounphrase bracketers, named-entity taggers, and morphological analyzers for an arbitrary target language. Bilingual text corpora are treated with existing text analysis tools for English, and their output is projected onto the target language via statistically derived word alignments. Their approach is especially interesting insofar as the system does not require hand-annotation of targetlanguage training data or virtually any targetlanguage-specific knowledge or resources. Martin et al. (2003) present an English-Inuktitut aligned parallel corpus, demonstrating superior 16 language-specific readability classification based on training data drawn from the same language as the testing data (Collins-Thompson and Callan, 2004), we have constructed a radically extensible tool that can easily create readability classifiers for an arbitrary target language using training data from a source language such as English. The result is a system capable of allowing a user to construct readability software for languages like Indonesian, for example, even if that user does not speak Indonesian — thi"
W09-3836,W09-3032,1,0.834151,"national Conference on Parsing Technologies (IWPT), pages 226–229, c Paris, October 2009. 2009 Association for Computational Linguistics D1 D2 D3 D4 D5 D6 D7 D8 SUBJH HSPEC FRAG _ NP HSPEC NOUN _ N _ CMPND ... PLUR _ NOUN _ ORULE v_-_le n_-_mc_le of parse disambiguation. The decisions record the fine-grained human judgements in the manual disambiguation process. This is different from the traditional use of treebanks to build parse selection models, where a marked gold tree is picked from the parse forest without concerning detailed selection steps. Recent study on double annotated treebanks (Kordoni and Zhang, 2009) shows that annotators tend to start with the decisions with the most certainty, and delay the “hard” decisions as much as possible. As the decision process goes, many of the “hard” discriminants will receive an inferred value from the certain decisions. This greedy approach helps to guarantee high interannotator agreement. Concerning the statistical parse selection models, the discriminative nature of these treebanking decisions suggests that they are highly effective features, and if properly used, they will contribute to an efficient disambiguation model. the dog ||barks the ||dog barks the"
W09-3836,C02-2025,0,0.016751,"parse forest produced by the parser. While most of the hand-annotated treebanks contain only gold trees, treebanks constructed from parser outputs include both preferred and non-preferred analyses. Some treebanking environments (such as the SRI Cambridge TreeBanker (Carter, 1997) or [incr tsdb()] (Oepen, 2001)) even record the treebanking decisions (see section 2) that the annotators take during manual annotation. These treebanking decisions are, usually, stored in the database/log files and used later for dynamic propagation if a newer version of the grammar on the same corpus is available (Oepen et al., 2002). But until now, to our best knowledge, no research has been reported on exploiting these decisions for building a parse disambiguation model. 2 Treebanking decisions One of the defining characteristics of Redwoodsstyle treebanks1 (Oepen et al., 2002) is that the candidate trees are constructed automatically by 1 More details available in http://redwoods.stanford.edu. 226 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 226–229, c Paris, October 2009. 2009 Association for Computational Linguistics D1 D2 D3 D4 D5 D6 D7 D8 SUBJH HSPEC FRAG _ NP HSPEC NOUN _"
W09-3836,2004.tmi-1.2,0,0.0279029,"tences. So, for example, if a feature (of a particular feature type) is observed 100 times, then these 100 occurrences are added to the total FHC. • Feature Type Hit Count (FTHC): it is the total number of distinct features (of the corresponding feature type) observed inside the syntactic analyses of all the test sentences. While exact and 5-best match measures show relative informativeness and robustness of the feature types, FHC and FTHC provide a more comprehensive picture of relative efficiencies. Experiment 4.1 Data 4.3 We use a collection of 8593 English sentences from the LOGON corpus (Oepen et al., 2004) for our experiment. 874 of them are kept as test items and the remaining 7719 items are used for training. The sentences have an average length of 14.68 and average number of 203.26 readings per sentence. The out-of-domain data are a set of 531 English Wikipedia sentences from WeScience corpus (Ytrestøl et al., 2009). Previous studies (Toutanova et al., 2005; Osborne and Baldridge, 2004) have reported relatively high exact match accuracy with earlier versions of ERG (Flickinger, 2000) on datasets with very short sentences. With much higher structural ambiguities in LOGON and WeScience sentenc"
W09-3836,N04-1012,0,0.0161998,"rmativeness and robustness of the feature types, FHC and FTHC provide a more comprehensive picture of relative efficiencies. Experiment 4.1 Data 4.3 We use a collection of 8593 English sentences from the LOGON corpus (Oepen et al., 2004) for our experiment. 874 of them are kept as test items and the remaining 7719 items are used for training. The sentences have an average length of 14.68 and average number of 203.26 readings per sentence. The out-of-domain data are a set of 531 English Wikipedia sentences from WeScience corpus (Ytrestøl et al., 2009). Previous studies (Toutanova et al., 2005; Osborne and Baldridge, 2004) have reported relatively high exact match accuracy with earlier versions of ERG (Flickinger, 2000) on datasets with very short sentences. With much higher structural ambiguities in LOGON and WeScience sentences, the overall disambiguation accuracy drops significantly. Results and discussion As we can see in Table 1, local configurations achieve highest accuracy among the traditional feature types. They also use higher number of features (almost 2.7 millions). TDFC do better than both n-grams and active edges, even with a lower number of features. Though, local configurations gain more accurac"
W09-3836,W97-1502,0,0.269613,"ls are tested on out-of-domain data. We show that, features extracted using treebanking decisions are more efficient, informative and robust compared to traditional features. 1 Introduction State-of-the-art parse disambiguation models are trained on treebanks, which are either fully handannotated or manually disambiguated from the parse forest produced by the parser. While most of the hand-annotated treebanks contain only gold trees, treebanks constructed from parser outputs include both preferred and non-preferred analyses. Some treebanking environments (such as the SRI Cambridge TreeBanker (Carter, 1997) or [incr tsdb()] (Oepen, 2001)) even record the treebanking decisions (see section 2) that the annotators take during manual annotation. These treebanking decisions are, usually, stored in the database/log files and used later for dynamic propagation if a newer version of the grammar on the same corpus is available (Oepen et al., 2002). But until now, to our best knowledge, no research has been reported on exploiting these decisions for building a parse disambiguation model. 2 Treebanking decisions One of the defining characteristics of Redwoodsstyle treebanks1 (Oepen et al., 2002) is that th"
W09-3836,A00-2018,0,\N,Missing
W09-3836,P99-1069,0,\N,Missing
W09-4101,W02-1502,0,0.0457508,"Missing"
W09-4101,P06-4014,0,0.0447989,"Missing"
W09-4101,I05-2035,0,0.0463399,"Missing"
W09-4101,P01-1019,0,0.0964314,"Missing"
W09-4101,P08-1111,0,0.0311775,"Missing"
W09-4101,boguslavsky-etal-2002-development,0,0.075896,"Missing"
W09-4101,C02-2025,0,\N,Missing
W09-4101,apresjan-etal-2006-syntactically,0,\N,Missing
W09-4107,W05-1008,0,0.0136693,"yment in open domain natural language processing. [2], as well as [6] and [7] have clearly shown that the majority of parsing failures with large-scale deep grammars are caused by missing or wrong entries in the lexica accompanying grammars like the aforementioned ones. Based on these findings, it has become clear that it is crucial to explore and come up with efficient methods for automated (Deep) Lexical Acquisition (henceforward (D)LA), the process of automatically recovering missing entries in the lexicons of deep grammars. Recently, various high-quality DLA approaches have been proposed. [1], as well as [7] and [5] describe efficient methods towards the task of lexicon acquisition for large-scale deep grammars for English and Dutch. They treat DLA as a classification task and make use of various robust and efficient machine learning techniques to perform the acquisition process. We use the ERG and GG grammars for the work we present in this talk, for the ERG is one of the biggest deep linguistic resources to date which has been being used in many (industrial) applications, and GG is to our knowledge one of the most thorough deep grammars of German, a language with rich morphology"
W09-4107,baldwin-etal-2004-road,0,0.0159027,"g robustness and ensuring maintainability and re-usability for two largescale “deep” grammars, one of English (ERG; [3]) and one of German (GG; [4]), developed in the framework of Head-driven Phrase Structure Grammar (HPSG). Specifically, we show that the incorporation of detailed Main Focus Points In recent years, various techniques and resources have been developed in order to improve robustness of deep grammars for real-life applications in various domains. Nevertheless, low coverage of such grammars remains the main hindrance to their employment in open domain natural language processing. [2], as well as [6] and [7] have clearly shown that the majority of parsing failures with large-scale deep grammars are caused by missing or wrong entries in the lexica accompanying grammars like the aforementioned ones. Based on these findings, it has become clear that it is crucial to explore and come up with efficient methods for automated (Deep) Lexical Acquisition (henceforward (D)LA), the process of automatically recovering missing entries in the lexicons of deep grammars. Recently, various high-quality DLA approaches have been proposed. [1], as well as [7] and [5] describe efficient method"
W09-4107,copestake-flickinger-2000-open,0,0.0381181,"ency deficiency of large-scale lexicalised grammars, ensuring this way their portability and re-usability and aiming at domain-independent linguistic processing. In particular, we illustrate and underline the importance of making detailed linguistic information a central part of the process of automatic acquisition of large-scale lexicons as a means for enhancing robustness and ensuring maintainability and re-usability of lexicalised grammars. To this effect, we focus on enhancing robustness and ensuring maintainability and re-usability for two largescale “deep” grammars, one of English (ERG; [3]) and one of German (GG; [4]), developed in the framework of Head-driven Phrase Structure Grammar (HPSG). Specifically, we show that the incorporation of detailed Main Focus Points In recent years, various techniques and resources have been developed in order to improve robustness of deep grammars for real-life applications in various domains. Nevertheless, low coverage of such grammars remains the main hindrance to their employment in open domain natural language processing. [2], as well as [6] and [7] have clearly shown that the majority of parsing failures with large-scale deep grammars are"
W09-4107,P04-1057,0,0.0372151,"ensuring maintainability and re-usability for two largescale “deep” grammars, one of English (ERG; [3]) and one of German (GG; [4]), developed in the framework of Head-driven Phrase Structure Grammar (HPSG). Specifically, we show that the incorporation of detailed Main Focus Points In recent years, various techniques and resources have been developed in order to improve robustness of deep grammars for real-life applications in various domains. Nevertheless, low coverage of such grammars remains the main hindrance to their employment in open domain natural language processing. [2], as well as [6] and [7] have clearly shown that the majority of parsing failures with large-scale deep grammars are caused by missing or wrong entries in the lexica accompanying grammars like the aforementioned ones. Based on these findings, it has become clear that it is crucial to explore and come up with efficient methods for automated (Deep) Lexical Acquisition (henceforward (D)LA), the process of automatically recovering missing entries in the lexicons of deep grammars. Recently, various high-quality DLA approaches have been proposed. [1], as well as [7] and [5] describe efficient methods towards the ta"
W09-4107,zhang-kordoni-2006-automated,1,0.802444,"g maintainability and re-usability for two largescale “deep” grammars, one of English (ERG; [3]) and one of German (GG; [4]), developed in the framework of Head-driven Phrase Structure Grammar (HPSG). Specifically, we show that the incorporation of detailed Main Focus Points In recent years, various techniques and resources have been developed in order to improve robustness of deep grammars for real-life applications in various domains. Nevertheless, low coverage of such grammars remains the main hindrance to their employment in open domain natural language processing. [2], as well as [6] and [7] have clearly shown that the majority of parsing failures with large-scale deep grammars are caused by missing or wrong entries in the lexica accompanying grammars like the aforementioned ones. Based on these findings, it has become clear that it is crucial to explore and come up with efficient methods for automated (Deep) Lexical Acquisition (henceforward (D)LA), the process of automatically recovering missing entries in the lexicons of deep grammars. Recently, various high-quality DLA approaches have been proposed. [1], as well as [7] and [5] describe efficient methods towards the task of le"
W09-4107,P09-1043,1,0.881966,"Missing"
W10-4144,P05-1022,0,0.225381,"Missing"
W10-4144,P02-1034,0,0.0384164,"best parse tree given by the Parse Reranker will be the result for Task 2.2; and the final output of the system will be the result for Task 2.1. Since we have already mentioned the Berkeley Parser in the related work, we will focus on the other two modules in the rest of this section. 3.1 Parse Reranker We follow Collins and Koo (2005)’s discriminative reranking model to score possible parse trees of each sentence given by the Berkeley Parser. Previous research on English shows that structured perceptron (Collins, 2002) is one of the strongest machine learning algorithms for parse reranking (Collins and Duffy, 2002; Gao et al., 2007). In our system, we use the averaged perceptron algorithm to do parameter estimation. Algorithm 1 illustrates the learning procedure. The parameter vector w is initialized to (0, ..., 0). The learner processes all the instances (t is from 1 to n) in each iteration (i). If current hypothesis (w) fails to predict xt , the learner update w through calculating the difference between Φ(xt , yt∗ ) and Φ(xt , yt ). At the end of each iteration, the learner save the current model as w + i, and finally all these models will be added up to get aw. 3.2 Features We use an example to sho"
W10-4144,J05-1003,0,0.0744491,"e of the same sentence. For example, we can check whether the boundary predictions given by the TCT parser are agreed by the PCTB parser. Since the PCTB parser is trained on a different treebank from TCT, our reranking model can be seen as a method to use a heterogenous resource. The best parse tree given by the Parse Reranker will be the result for Task 2.2; and the final output of the system will be the result for Task 2.1. Since we have already mentioned the Berkeley Parser in the related work, we will focus on the other two modules in the rest of this section. 3.1 Parse Reranker We follow Collins and Koo (2005)’s discriminative reranking model to score possible parse trees of each sentence given by the Berkeley Parser. Previous research on English shows that structured perceptron (Collins, 2002) is one of the strongest machine learning algorithms for parse reranking (Collins and Duffy, 2002; Gao et al., 2007). In our system, we use the averaged perceptron algorithm to do parameter estimation. Algorithm 1 illustrates the learning procedure. The parameter vector w is initialized to (0, ..., 0). The learner processes all the instances (t is from 1 to n) in each iteration (i). If current hypothesis (w)"
W10-4144,W02-1001,0,0.300956,"rom TCT, our reranking model can be seen as a method to use a heterogenous resource. The best parse tree given by the Parse Reranker will be the result for Task 2.2; and the final output of the system will be the result for Task 2.1. Since we have already mentioned the Berkeley Parser in the related work, we will focus on the other two modules in the rest of this section. 3.1 Parse Reranker We follow Collins and Koo (2005)’s discriminative reranking model to score possible parse trees of each sentence given by the Berkeley Parser. Previous research on English shows that structured perceptron (Collins, 2002) is one of the strongest machine learning algorithms for parse reranking (Collins and Duffy, 2002; Gao et al., 2007). In our system, we use the averaged perceptron algorithm to do parameter estimation. Algorithm 1 illustrates the learning procedure. The parameter vector w is initialized to (0, ..., 0). The learner processes all the instances (t is from 1 to n) in each iteration (i). If current hypothesis (w) fails to predict xt , the learner update w through calculating the difference between Φ(xt , yt∗ ) and Φ(xt , yt ). At the end of each iteration, the learner save the current model as w +"
W10-4144,P07-1104,0,0.0134447,"the Parse Reranker will be the result for Task 2.2; and the final output of the system will be the result for Task 2.1. Since we have already mentioned the Berkeley Parser in the related work, we will focus on the other two modules in the rest of this section. 3.1 Parse Reranker We follow Collins and Koo (2005)’s discriminative reranking model to score possible parse trees of each sentence given by the Berkeley Parser. Previous research on English shows that structured perceptron (Collins, 2002) is one of the strongest machine learning algorithms for parse reranking (Collins and Duffy, 2002; Gao et al., 2007). In our system, we use the averaged perceptron algorithm to do parameter estimation. Algorithm 1 illustrates the learning procedure. The parameter vector w is initialized to (0, ..., 0). The learner processes all the instances (t is from 1 to n) in each iteration (i). If current hypothesis (w) fails to predict xt , the learner update w through calculating the difference between Φ(xt , yt∗ ) and Φ(xt , yt ). At the end of each iteration, the learner save the current model as w + i, and finally all these models will be added up to get aw. 3.2 Features We use an example to show the features we e"
W10-4144,N07-1051,0,0.0495543,"Missing"
W10-4144,P06-1055,0,0.202281,"Missing"
W11-2915,P05-1022,0,0.149875,"Missing"
W11-2915,W08-1301,0,0.0169911,"do et al., 2003; Greenwood and Stevenson, 2006)). The IE framework extended in this paper utilizes minimally supervised learning of extraction rules for the detection of relation instances (Xu et al., 2007). Since the minimally supervised learning starts its bootstrapping from a few semantic examples, no treebanking or any other annotation is required for new domains. In addition to this inherently domain-adaptable rule-learning component, the framework also employs two language analysis modules: a namedentity (NE) recognizer (Drozdzynski et al., 2004) and a parser (Lin, 1998; de Marneffe and Manning, 2008). NE recognizers are adapted to new domains–if needed–by adding rules for new NE types and extending the gazetteers. The employed generic data-driven dependency parsers or deeplinguistic handcrafted parsers have not yet been adapted to IE domains and tasks. The new work presented here concerns the adaptation of a generic parser to a given relation extraction (RE) task and domain without actually changing the parser itself. For the experiments a generic deep-linguistic parser was used together with a hand-crafted HPSG (Pollard and Sag, 1994) grammar for English (ERG) (Flickinger, 2000). The out"
W11-2915,D10-1068,0,0.0167683,"rd making deep linguistic grammars useful for relation extraction, whereas up to now most minimally supervised approaches to RE have employed shallower robust parsers. The hope behind these attempts is to improve precision without losing too much recall. After reclaiming recall through our parse reranking, next steps in this line of research will be dedicated to balancing off the deficits in coverage by data-driven lexicon extension in the spirit of (Zhang et al., 2010) and by exploiting the chart for partial parses involving the relevant types of named entities. Furthermore, the approach of (Dridan and Baldwin, 2010) to learning a parse selection model in an unsupervised way by utilizing the constraints of HSPG grammars might also be interesting for domain adaptive parse selection for relation extraction. At some point we may then be in a position to conduct a fair empirical comparison between deep-linguistic parsing with hand-crafted grammars on the one hand and purely statistical parsing on the other. An error analysis may then indicate the chances for hybrid approaches. However, before targeting these medium-term goals we plan to investigate whether our approach can also be applied to other parsers wit"
W11-2915,P10-1074,0,0.0388319,"Missing"
W11-2915,P06-2034,0,0.0499667,"Missing"
W11-2915,W06-0204,0,0.0712755,"Missing"
W11-2915,C02-2025,0,0.0824744,"uistic handcrafted parsers have not yet been adapted to IE domains and tasks. The new work presented here concerns the adaptation of a generic parser to a given relation extraction (RE) task and domain without actually changing the parser itself. For the experiments a generic deep-linguistic parser was used together with a hand-crafted HPSG (Pollard and Sag, 1994) grammar for English (ERG) (Flickinger, 2000). The output of this parser is a list of n best parses selected and ranked by a MaxEnt parse-ranking component (Toutanova et al., 2005b), which had been trained on a generic HPSG treebank (Oepen et al., 2002). The parse ranking had attracted our The paper demonstrates how the generic parser of a minimally supervised information extraction framework can be adapted to a given task and domain for relation extraction (RE). For the experiments a generic deep-linguistic parser was employed that works with a largely hand-crafted headdriven phrase structure grammar (HPSG) for English. The output of this parser is a list of n best parses selected and ranked by a MaxEnt parse-ranking component, which had been trained on a more or less generic HPSG treebank. It will be shown how the estimated confidence of R"
W11-2915,C96-1079,0,0.059077,"erformance but from an application-driven selection of parses that are best suited for the RE task. Since the readings best suited for successful rule extraction and instance extraction are often not the readings favored by a regular parser evaluation, generic parsing accuracy actually decreases. The novel method for taskspecific parse reranking does not require any annotated data beyond the semantic seed, which is needed anyway for the RE task. 1 Introduction Domain adaptation is a central research topic for many language technologies including information extraction (IE) and parsing (e.g., (Grishman and Sundheim, 1996; Muslea, 1999; Hara et al., 2005; McClosky et al., 2010; Miwa et al., 2010)). The largest challenge is to develop methods that 118 Proceedings of the 12th International Conference on Parsing Technologies, pages 118–128, c 2011 Association for Computational Linguistics October 5-7, 2011, Dublin City University. lease of the grammar we use is accompanied by a maximum-entropy-based parse disambiguation model trained on the Redwoods Treebank (Oepen et al., 2002), a treebank of ∼20K sentences with mixed genre texts (dialogs, tourist information, emails, etc). The discriminative log-linear disambig"
W11-2915,W10-2105,0,0.0515468,"Missing"
W11-2915,I05-1018,0,0.0300548,"election of parses that are best suited for the RE task. Since the readings best suited for successful rule extraction and instance extraction are often not the readings favored by a regular parser evaluation, generic parsing accuracy actually decreases. The novel method for taskspecific parse reranking does not require any annotated data beyond the semantic seed, which is needed anyway for the RE task. 1 Introduction Domain adaptation is a central research topic for many language technologies including information extraction (IE) and parsing (e.g., (Grishman and Sundheim, 1996; Muslea, 1999; Hara et al., 2005; McClosky et al., 2010; Miwa et al., 2010)). The largest challenge is to develop methods that 118 Proceedings of the 12th International Conference on Parsing Technologies, pages 118–128, c 2011 Association for Computational Linguistics October 5-7, 2011, Dublin City University. lease of the grammar we use is accompanied by a maximum-entropy-based parse disambiguation model trained on the Redwoods Treebank (Oepen et al., 2002), a treebank of ∼20K sentences with mixed genre texts (dialogs, tourist information, emails, etc). The discriminative log-linear disambiguation model scores each parse by"
W11-2915,W09-2205,0,0.0249408,"e relation extraction in contrast to text understanding does not need the entire and correct syntactic structure for the detection of relation instances. The ease and consistency of rule extraction and rule application counts more than the linguistically correct analysis. The gained new insight that the consistency of parse selection is more relevant Related Work Various attempts have been made to improve the cross-domain performance of statistical parsing models. McClosky et al. (2006) uses self-training to improve Charniak’s parser by feeding large amount of unannotated texts to the parser. Plank (2009) utilize structural-correspondence learning to improve the accuracy of the Dutch Alpino parser on the Wikipedia texts. Rimell and Clark (2008) show that a small set of annotated indomain data can significantly improve the CCG parser’s performance. Hara et al. (2007) improves the Enju HPSG parser performance in the biomedical domain by a low-cost retraining of the lexical disambiguation model. Nearly all approaches evaluate the parsing quality against a “gold-standard” treebank. Miwa et al. (2010) compares five parsers for bio-molecular event extraction to investigate the correlation between th"
W11-2915,W07-2202,0,0.0200102,"ct analysis. The gained new insight that the consistency of parse selection is more relevant Related Work Various attempts have been made to improve the cross-domain performance of statistical parsing models. McClosky et al. (2006) uses self-training to improve Charniak’s parser by feeding large amount of unannotated texts to the parser. Plank (2009) utilize structural-correspondence learning to improve the accuracy of the Dutch Alpino parser on the Wikipedia texts. Rimell and Clark (2008) show that a small set of annotated indomain data can significantly improve the CCG parser’s performance. Hara et al. (2007) improves the Enju HPSG parser performance in the biomedical domain by a low-cost retraining of the lexical disambiguation model. Nearly all approaches evaluate the parsing quality against a “gold-standard” treebank. Miwa et al. (2010) compares five parsers for bio-molecular event extraction to investigate the correlation between the performance on a gold-stand treebank and the usefulness in real-world applications. All four domain-adapted parsers achieve similar IE perfor125 References than parsing accuracy, we consider worth sharing. The presented results may also be viewed as a step forward"
W11-2915,D08-1050,0,0.0271645,"of relation instances. The ease and consistency of rule extraction and rule application counts more than the linguistically correct analysis. The gained new insight that the consistency of parse selection is more relevant Related Work Various attempts have been made to improve the cross-domain performance of statistical parsing models. McClosky et al. (2006) uses self-training to improve Charniak’s parser by feeding large amount of unannotated texts to the parser. Plank (2009) utilize structural-correspondence learning to improve the accuracy of the Dutch Alpino parser on the Wikipedia texts. Rimell and Clark (2008) show that a small set of annotated indomain data can significantly improve the CCG parser’s performance. Hara et al. (2007) improves the Enju HPSG parser performance in the biomedical domain by a low-cost retraining of the lexical disambiguation model. Nearly all approaches evaluate the parsing quality against a “gold-standard” treebank. Miwa et al. (2010) compares five parsers for bio-molecular event extraction to investigate the correlation between the performance on a gold-stand treebank and the usefulness in real-world applications. All four domain-adapted parsers achieve similar IE perfo"
W11-2915,P06-1043,0,0.025829,"not necessarily correspond to a better parse ranking for other purposes or for generic parsing. This should not be surprising since relation extraction in contrast to text understanding does not need the entire and correct syntactic structure for the detection of relation instances. The ease and consistency of rule extraction and rule application counts more than the linguistically correct analysis. The gained new insight that the consistency of parse selection is more relevant Related Work Various attempts have been made to improve the cross-domain performance of statistical parsing models. McClosky et al. (2006) uses self-training to improve Charniak’s parser by feeding large amount of unannotated texts to the parser. Plank (2009) utilize structural-correspondence learning to improve the accuracy of the Dutch Alpino parser on the Wikipedia texts. Rimell and Clark (2008) show that a small set of annotated indomain data can significantly improve the CCG parser’s performance. Hara et al. (2007) improves the Enju HPSG parser performance in the biomedical domain by a low-cost retraining of the lexical disambiguation model. Nearly all approaches evaluate the parsing quality against a “gold-standard” treeba"
W11-2915,P03-1029,0,0.0867986,"Missing"
W11-2915,N10-1004,0,0.026464,"that are best suited for the RE task. Since the readings best suited for successful rule extraction and instance extraction are often not the readings favored by a regular parser evaluation, generic parsing accuracy actually decreases. The novel method for taskspecific parse reranking does not require any annotated data beyond the semantic seed, which is needed anyway for the RE task. 1 Introduction Domain adaptation is a central research topic for many language technologies including information extraction (IE) and parsing (e.g., (Grishman and Sundheim, 1996; Muslea, 1999; Hara et al., 2005; McClosky et al., 2010; Miwa et al., 2010)). The largest challenge is to develop methods that 118 Proceedings of the 12th International Conference on Parsing Technologies, pages 118–128, c 2011 Association for Computational Linguistics October 5-7, 2011, Dublin City University. lease of the grammar we use is accompanied by a maximum-entropy-based parse disambiguation model trained on the Redwoods Treebank (Oepen et al., 2002), a treebank of ∼20K sentences with mixed genre texts (dialogs, tourist information, emails, etc). The discriminative log-linear disambiguation model scores each parse by the following (Toutano"
W11-2915,P05-1073,0,0.167111,"e gazetteers. The employed generic data-driven dependency parsers or deeplinguistic handcrafted parsers have not yet been adapted to IE domains and tasks. The new work presented here concerns the adaptation of a generic parser to a given relation extraction (RE) task and domain without actually changing the parser itself. For the experiments a generic deep-linguistic parser was used together with a hand-crafted HPSG (Pollard and Sag, 1994) grammar for English (ERG) (Flickinger, 2000). The output of this parser is a list of n best parses selected and ranked by a MaxEnt parse-ranking component (Toutanova et al., 2005b), which had been trained on a generic HPSG treebank (Oepen et al., 2002). The parse ranking had attracted our The paper demonstrates how the generic parser of a minimally supervised information extraction framework can be adapted to a given task and domain for relation extraction (RE). For the experiments a generic deep-linguistic parser was employed that works with a largely hand-crafted headdriven phrase structure grammar (HPSG) for English. The output of this parser is a list of n best parses selected and ranked by a MaxEnt parse-ranking component, which had been trained on a more or less"
W11-2915,W10-1905,0,0.143777,"r the RE task. Since the readings best suited for successful rule extraction and instance extraction are often not the readings favored by a regular parser evaluation, generic parsing accuracy actually decreases. The novel method for taskspecific parse reranking does not require any annotated data beyond the semantic seed, which is needed anyway for the RE task. 1 Introduction Domain adaptation is a central research topic for many language technologies including information extraction (IE) and parsing (e.g., (Grishman and Sundheim, 1996; Muslea, 1999; Hara et al., 2005; McClosky et al., 2010; Miwa et al., 2010)). The largest challenge is to develop methods that 118 Proceedings of the 12th International Conference on Parsing Technologies, pages 118–128, c 2011 Association for Computational Linguistics October 5-7, 2011, Dublin City University. lease of the grammar we use is accompanied by a maximum-entropy-based parse disambiguation model trained on the Redwoods Treebank (Oepen et al., 2002), a treebank of ∼20K sentences with mixed genre texts (dialogs, tourist information, emails, etc). The discriminative log-linear disambiguation model scores each parse by the following (Toutanova et al., 2005b), P"
W11-2915,P07-1074,1,0.945583,"it, Sebastian Krause DFKI, LT-Lab, Germany {feiyu,lihong,Yi.Zhang,uszkoreit,sebastian.krause}@dfki.de Abstract exploit domain knowledge with minimal human effort. Many IE systems benefit from combining generic NLP components with task-specific extraction methods. Various machine learning approaches have been employed for adapting the IE methods to new domains and extraction tasks (e.g., (Yangarber, 2001; Sudo et al., 2003; Greenwood and Stevenson, 2006)). The IE framework extended in this paper utilizes minimally supervised learning of extraction rules for the detection of relation instances (Xu et al., 2007). Since the minimally supervised learning starts its bootstrapping from a few semantic examples, no treebanking or any other annotation is required for new domains. In addition to this inherently domain-adaptable rule-learning component, the framework also employs two language analysis modules: a namedentity (NE) recognizer (Drozdzynski et al., 2004) and a parser (Lin, 1998; de Marneffe and Manning, 2008). NE recognizers are adapted to new domains–if needed–by adding rules for new NE types and extending the gazetteers. The employed generic data-driven dependency parsers or deeplinguistic handc"
W11-2915,C10-2155,1,0.922211,"Therefore, the confidence values can be utilized as feedback to the parser to help it to rerank its readings. Figure 1: DARE core architecture Relying entirely on semantic seeds as domain knowledge, DARE can accommodate new relation types and domains with minimal effort. Since we had already reported on experiments applying the framework to different relation types and corpora including MUC-6 data in the cited papers, including comparisons with other ML approaches to RE (Xu, 2007; Uszkoreit et al., 2009), we omit a comparative discussion here. For confidence estimation, the method proposed by Xu et al. (2010) is adopted.1 Actually, in (2) we propose an extended version of the rule scoring, since the rule scoring in (Xu et al., 2010) did not consider the case when a learned rule does not extract any new instances. Thus, given the scoring of instances, the confidence value of a rule is the average score of all instances (Iextracted ) extracted by this rule or the average score of seed instances (Irule ) from which they are learned. Through the factor δ we reduce the score of rules that have not proven yet their potential for extracting instances. 4.2 Reranking Architecture and Method Figure 2 depict"
W11-2915,P09-1043,1,0.850491,"and t is the HPSG reading; T (w) is the set of all possible readings for a given sentence w licensed by the grammar; hf1 , . . . , fn i and hλ1 , . . . , λn i are feature functions and their corresponding weights. In practice, the effective features are defined on the HPSG derivation trees (without details from the feature structures), and the best readings are decoded efficiently from a packed parse forest with dynamic programming (Zhang et al., 2007). Although there are indications that parsers with hand-written grammars usually suffer less from the shift of domain than statistical parsers (Zhang and Wang, 2009; Plank and van Noord, 2010), the effect can still be observed, say in the preference of lexical selection. The issue is not that the correct analysis would be ruled out by the constraints in the treebank-induced grammar, but rather that it is not favored by the statistical ranking model, since the statistical distribution of the syntactic structures in the training corpus is different from the target application domain. This issue is recently acknowledged in most parsing systems and known as the domain adaptation task. 3 DARE and Confidence Estimation DARE (Xu et al., 2007; Xu, 2007) is a min"
W11-2915,W07-2207,1,0.836424,"m. Section 6 discusses related work. Finally, Section 7 summarizes the results and suggests directions for further research. 2 where w is the given input sentence and t is the HPSG reading; T (w) is the set of all possible readings for a given sentence w licensed by the grammar; hf1 , . . . , fn i and hλ1 , . . . , λn i are feature functions and their corresponding weights. In practice, the effective features are defined on the HPSG derivation trees (without details from the feature structures), and the best readings are decoded efficiently from a packed parse forest with dynamic programming (Zhang et al., 2007). Although there are indications that parsers with hand-written grammars usually suffer less from the shift of domain than statistical parsers (Zhang and Wang, 2009; Plank and van Noord, 2010), the effect can still be observed, say in the preference of lexical selection. The issue is not that the correct analysis would be ruled out by the constraints in the treebank-induced grammar, but rather that it is not favored by the statistical ranking model, since the statistical distribution of the syntactic structures in the training corpus is different from the target application domain. This issue"
W11-2915,N10-1002,1,0.846736,"similar IE perfor125 References than parsing accuracy, we consider worth sharing. The presented results may also be viewed as a step forward toward making deep linguistic grammars useful for relation extraction, whereas up to now most minimally supervised approaches to RE have employed shallower robust parsers. The hope behind these attempts is to improve precision without losing too much recall. After reclaiming recall through our parse reranking, next steps in this line of research will be dedicated to balancing off the deficits in coverage by data-driven lexicon extension in the spirit of (Zhang et al., 2010) and by exploiting the chart for partial parses involving the relevant types of named entities. Furthermore, the approach of (Dridan and Baldwin, 2010) to learning a parse selection model in an unsupervised way by utilizing the constraints of HSPG grammars might also be interesting for domain adaptive parse selection for relation extraction. At some point we may then be in a position to conduct a fair empirical comparison between deep-linguistic parsing with hand-crafted grammars on the one hand and purely statistical parsing on the other. An error analysis may then indicate the chances for hy"
W11-2915,P02-1062,0,\N,Missing
W11-2923,P99-1061,1,0.689206,"al large-scale HPSG-based NLP parsing systems have been built in the past decade. Among them are the Enju English & Chinese parser (Miyao et al., 2004; Yu et al., 2010), the Alpino parser for Dutch (van Noord, 2006), and the LKB & PET (Copestake, 2002; Callmeier, 2000) for English, German, Japanese and a dozen more other DELPH-IN2 grammars of various languages. These systems are successful showcases of where modern grammar engineering contributes to the state-of-the-art parsing systems. With the modern processing techniques such as quasi-destructive unification (Tomabechi, 1991), quick check (Kiefer et al., 1999), ambiguity packing (Oepen and Carroll, 2000) and selective unpacking (Zhang et al., 2007), the practical parsing efficiency has been greatly improved. But none of these changes the underlying formalism, therefore the parser still can run into exponential parsing time in theory. Another disadvantage of deep grammar lies in the difficulty of proper statistical modeling of the richer representation. For example, Abney (1997) shows that na¨ıve MLE is not consistent for unification-based grammars, and proposes random fields as an alternative. In practice, we see most HPSG parsing systems opt for a"
W11-2923,J97-4005,0,0.104221,"ring contributes to the state-of-the-art parsing systems. With the modern processing techniques such as quasi-destructive unification (Tomabechi, 1991), quick check (Kiefer et al., 1999), ambiguity packing (Oepen and Carroll, 2000) and selective unpacking (Zhang et al., 2007), the practical parsing efficiency has been greatly improved. But none of these changes the underlying formalism, therefore the parser still can run into exponential parsing time in theory. Another disadvantage of deep grammar lies in the difficulty of proper statistical modeling of the richer representation. For example, Abney (1997) shows that na¨ıve MLE is not consistent for unification-based grammars, and proposes random fields as an alternative. In practice, we see most HPSG parsing systems opt for a discriminative Maximum Entropy Model (MEM) for parse ranking on top of the hypothesis space licensed by 2 3 Related Work Previous work in the direction of HPSG approximation has seen two major approaches: grammarbased approach and the corpus-driven approach. The grammar-based approach (Kiefer and Krieger, 2004) tries to compile out a huge set of categories by flattening the TFSes into atomic symbols. This approach can in"
W11-2923,C02-1075,1,0.7765,"tion. Also, the symbols in CFG only carry partial information from a small set of featurepaths used in quick check (Kiefer et al., 1999), i.e., the frequently failed feature-paths in unification, hence the most discriminating ones. Both approaches are symbolic in the sense that there is no probabilistic model produced to disambiguate the CFG parses. In the case of corpusbased approach, one can also acquire frequency counts for each CFG rule. But since not all passive edges occur in a full parse tree, and not all parses are correct, the statistics obtained is not suitable for the parsing task. Kiefer et al. (2002) propose to use a PCFG in a http://www.delph-in.net/ 199 we perform several normalizing transformations on the original derivations. First, in order to acquire an unlexicalized grammar, we replace the lexical entry names on the preterminal with their corresponding lexical type defined in the ERG lexicon. Second, we collapse the unary chain of morphological rules together with the preterminal lexical types to form the so-called “supertag”. As shown in Figure 1, these unary rules always occur above the preterminals and below any syntactic constructional rules. Practice shows that this helps impr"
W11-2923,P04-1041,0,0.125171,"Missing"
W11-2923,P03-1054,0,0.466697,"JOHN BUY V1 John bought crashed. Figure 1: Example of an original ERG derivation tree ROOT ROOT STRICT SB-HD MC C SP-HD N C d - the le The HD OPTCMP C HDN-AJ RC C n - c le&N SG ILR computer W PERIOD PLR CL RC FIN NWH C v pp unacc le&V PST OLR PUNCT PERIOD SB-HD NMC C crashed HDN BNP PN C HD XCOMP C n - pn-msc le&N SG ILR v np le&V PST OLR John bought Figure 2: Example of a normalized derivation tree 201 . mation coming from the HPSG sign.3 1 2 3 4 5 6 For the external annotation, we mark each nonterminal node with up to n grandparents. This is an effective technique used in both PCFG parsing (Klein and Manning, 2003)4 and HPSG parse disambiguation (Toutanova et al., 2005). As ERG rules are either unary or binary, we do not annotate nodes with sibling information, though for a grammar with flat rules this could potentially help, as shown by Klein and Manning (2003) (socalled horizontal markovization). We choose not to annotate the preterminal supertags with grandparents, for the overly fine-grained tagset hurts the parsing coverage. Feature-Path SYNSEM.LOCAL.CAT.HEAD SYNSEM.LOCAL.CONJ SYNSEM.LOCAL.AGR.PNG.PN SYNSEM.LOCAL.CAT.VAL.COMPS SYNSEM.LOCAL.CAT.HEAD.MOD SYNSEM.LOCAL.CAT.VAL.COMPS.FIRST.OPT Table 1:"
W11-2923,flickinger-etal-2010-wikiwoods,0,0.0359086,"al rules, and 50 lex203 full parse is not found, a partial parsing model tries to recover fragmented analysis according to the Viterbi probabilities of the constituents. With careful design of the PCFG and sufficient training data, the parser normally delivers close to full parsing coverage even without the fragmented partial parsing mode. keep “ws12” for development and “ws13” for the final testing. Sections “ws01” to “ws11” contain a total of 7,636 “gold” trees, which are used for training. Apart from the WeScience, we also use the large-scale automatically disambiguated WikiWoods Treebank (Flickinger et al., 2010). Currently, WikiWoods contains about 55M English sentences extracted from the English Wikipedia articles. The corpus is parsed with the 1010 version of the ERG, and automatically disambiguated with a Maximum Entropy model trained with the manually disambiguated trees. Only 1 top-ranked reading is preserved. Since the correctness of the parse is unchecked, the dataset is potentially noisy. The total amount of trees available for training is about 48M. 5.2 5.3 Evaluation & Results For the evaluation of our approximating PCFGs, we compare the top-1 parses produced by the PCFG with the manually d"
W11-2923,N06-1020,0,0.100295,"Missing"
W11-2923,2006.jeptalnrecital-invite.2,0,0.116676,"Missing"
W11-2923,A00-2022,0,0.669076,"tems have been built in the past decade. Among them are the Enju English & Chinese parser (Miyao et al., 2004; Yu et al., 2010), the Alpino parser for Dutch (van Noord, 2006), and the LKB & PET (Copestake, 2002; Callmeier, 2000) for English, German, Japanese and a dozen more other DELPH-IN2 grammars of various languages. These systems are successful showcases of where modern grammar engineering contributes to the state-of-the-art parsing systems. With the modern processing techniques such as quasi-destructive unification (Tomabechi, 1991), quick check (Kiefer et al., 1999), ambiguity packing (Oepen and Carroll, 2000) and selective unpacking (Zhang et al., 2007), the practical parsing efficiency has been greatly improved. But none of these changes the underlying formalism, therefore the parser still can run into exponential parsing time in theory. Another disadvantage of deep grammar lies in the difficulty of proper statistical modeling of the richer representation. For example, Abney (1997) shows that na¨ıve MLE is not consistent for unification-based grammars, and proposes random fields as an alternative. In practice, we see most HPSG parsing systems opt for a discriminative Maximum Entropy Model (MEM) f"
W11-2923,C10-2162,0,0.0496766,"d-Driven Phrase Structure Grammars Head-Driven Phrase Structure Grammar (Pollard and Sag, 1994) is a constraint-based highly lexicalized non-derivational generative grammar framework. Based on a typed feature structures formalism, the HPSG theory describes a small set of highly generalized linguistic principles, with which the rich information derived from the detailed lexical types interacts to produce precise linguistic interpretations. Several large-scale HPSG-based NLP parsing systems have been built in the past decade. Among them are the Enju English & Chinese parser (Miyao et al., 2004; Yu et al., 2010), the Alpino parser for Dutch (van Noord, 2006), and the LKB & PET (Copestake, 2002; Callmeier, 2000) for English, German, Japanese and a dozen more other DELPH-IN2 grammars of various languages. These systems are successful showcases of where modern grammar engineering contributes to the state-of-the-art parsing systems. With the modern processing techniques such as quasi-destructive unification (Tomabechi, 1991), quick check (Kiefer et al., 1999), ambiguity packing (Oepen and Carroll, 2000) and selective unpacking (Zhang et al., 2007), the practical parsing efficiency has been greatly improv"
W11-2923,C02-2025,0,0.0931694,"Missing"
W11-2923,W07-2207,1,0.819257,"m are the Enju English & Chinese parser (Miyao et al., 2004; Yu et al., 2010), the Alpino parser for Dutch (van Noord, 2006), and the LKB & PET (Copestake, 2002; Callmeier, 2000) for English, German, Japanese and a dozen more other DELPH-IN2 grammars of various languages. These systems are successful showcases of where modern grammar engineering contributes to the state-of-the-art parsing systems. With the modern processing techniques such as quasi-destructive unification (Tomabechi, 1991), quick check (Kiefer et al., 1999), ambiguity packing (Oepen and Carroll, 2000) and selective unpacking (Zhang et al., 2007), the practical parsing efficiency has been greatly improved. But none of these changes the underlying formalism, therefore the parser still can run into exponential parsing time in theory. Another disadvantage of deep grammar lies in the difficulty of proper statistical modeling of the richer representation. For example, Abney (1997) shows that na¨ıve MLE is not consistent for unification-based grammars, and proposes random fields as an alternative. In practice, we see most HPSG parsing systems opt for a discriminative Maximum Entropy Model (MEM) for parse ranking on top of the hypothesis spa"
W11-2923,P06-1055,0,0.132635,"t annotations (with unification operations involved), the procedure marches through thousands of trees per minute. This allows us to scale up the extraction with millions of trees. 3 Our notion of internal and external annotation is slightly different to that of (Klein and Manning, 2003). In our notion, internal annotation refers to the information from the local HPSG sign. 4 This technique is called vertical markovization in (Klein and Manning, 2003). 4.5 Hierarchically Split-Merge PCFG For comparison we also trained a hierarchically split-merge latent-variable PCFG with the Berkeley parser (Petrov et al., 2006). The latent-variable 202 ROOT ROOT STRICT ˆROOT [verb full] SB-HD MC C ˆROOT STRICT [verb full] SP-HD N C HD OPTCMP C ˆSB-HD M C [noun] ˆSB-HD MC C [verb full] d - the le HDN-AJ RC C W PERIOD PLR The ˆSP-HD N C [noun] ˆHD OPTCMP C [verb full] n - c le&N SG ILR CL RC FIN NWH C computer ˆHDN-AJ RC C [verb full] v pp unacc le&V PST OLR PUNCT PERIOD . crashed SB-HD NMC C ˆCL RC-FIN-NWH C [verb full] HDN BNP PN C HD XCOMP C ˆSB-HD NMC C [noun] ˆSB-HD NMC C [verb full] n - pn-msc le&N SG ILR v np le&V PST OLR John bought Figure 3: Example tree with 1-level grandparent and HEAD feature-path annotati"
W11-2923,C04-1024,0,0.0298692,"pretokenized but not tagged. All comparisons are done on the original derivations. Several accuracy measures are used, including the ParsEval labeled bracketing precision, recall, F1 and exact match ratio. Since the ParsEval scores ignore the preterminal nodes, we also report the (lexical type) tagging accuracy. Several different training sets are used. The Parser The approximating PCFG tends to grow huge when rich annotations and large corpora are used. For efficient application of the resulting grammar, we implemented a CKY-style parser with bitvector-based algorithm as the one proposed by Schmid (2004). The algorithm shows its strength in extensibility for grammars with millions of rules and hundreds of thousands of non-terminal symbols. A slight deviation of our implementation from the BitPar algorithm is that, after constructing the bit-vector-based recognition chart, we do not apply the top-down filtering routine before calculating the Viterbi probabilities. Practice shows that in our case the recognition chart is normally sparse, and the filtering routine itself costs more time than what it saves from the additional calculations in the Viterbi step. For correctness checking, we reproduc"
W11-2923,P91-1041,0,\N,Missing
W11-2923,1991.iwpt-1.19,0,\N,Missing
W11-3216,J93-2003,0,0.0192027,"d Ch-En transliterations, there was a discussion on whether to use the intermediate phonemic interpretation, i.e., Pinyin. Li et al. (2004) showed empirically that by skipping the intermediate phonemic interpretation (denoted as grapheme-based methods), the transliteration error rate was reduced significantly, since the mapping between Pinyin and Chinese characters was not trivial. Oh et al. (2009) had a more generalized version of Li et al. (2004)’s system as well as other 2.1 Phrase-based SMT The basic architecture of a phrase-based SMT system is an instance of the noisy-channel approaches (Brown et al., 1993). In the context of transliteration, the term “phrase” in phrase-based SMT would refer to a sequence of characters chosen by its statistical rather then any grammatical properties. The transliteration of a name s in the source language into a name t in the target language is modeled as: 101 Proceedings of the 2011 Named Entities Workshop, IJCNLP 2011, pages 101–105, Chiang Mai, Thailand, November 12, 2011. have the transliteration probability defined as: arg max P (t|s) = arg max(P (t)P (s|t)); t P (s, t, α) = t K X k=1 (1) where < e, c &gt;k is the k th aligned pair of translation units. Therefo"
W11-3216,N03-1017,0,0.00791569,"SRILM toolkit (Stolcke, 2002). The translation model is built from the character alignments given the M2MJSC model and we did not construct any distortion models. M2MJSC is first applied to the training set to divide each source name in parallel with the corresponding target name into the same number of segments. These segments are then considered as words that are one-to-one aligned. The PBSMT system takes multiple segments, namely phrases, as translation units. The phrase extraction follows the heuristic that starts with the given word alignment and expands to the adjacent alignment points (Koehn et al., 2003). The translation probabilities of the extracted phrases are estimated accordingly. As the last step, we split all the segments in the translation model into characters to allow more straightforward integration into the original PBSMT system that relies on character based inputs. 3.2.2 Moses decoder We used the open-source SMT decoder Moses (Koehn et al., 2007). Moses allows a log-linear model to combine various models and implements an efficient beam search algorithm that quickly finds the best translation among the large number of hypotheses. In order to adapt the SMT decoder to the translit"
W11-3216,P04-1021,0,0.604184,"flexibility of the PBSMT system. Introduction Machine transliteration has drawn a lot of attention in the previous years. In particular, the previous two shared tasks (Li et al., 2009; Li et al., 2010) attracted more than 30 participants. This year’s task only focuses on the transliteration generation task. As our first attempt in this area, we participated in English-to-Chinese transliteration (En-Ch) and Chinese-to-English back transliteration (Ch-En) tasks. For En-Ch and Ch-En transliterations, there was a discussion on whether to use the intermediate phonemic interpretation, i.e., Pinyin. Li et al. (2004) showed empirically that by skipping the intermediate phonemic interpretation (denoted as grapheme-based methods), the transliteration error rate was reduced significantly, since the mapping between Pinyin and Chinese characters was not trivial. Oh et al. (2009) had a more generalized version of Li et al. (2004)’s system as well as other 2.1 Phrase-based SMT The basic architecture of a phrase-based SMT system is an instance of the noisy-channel approaches (Brown et al., 1993). In the context of transliteration, the term “phrase” in phrase-based SMT would refer to a sequence of characters chose"
W11-3216,P07-1016,0,0.0413608,"Missing"
W11-3216,W09-3501,0,0.156905,"systems do not consider the key features of the transliteration task, which, on the other hand, have been emphasized by the joint source channel models. Our primary system is a standard phrase-based statistical machine translation (PBSMT) system with a modification based on the Multi-to-Multi Joint Source Channel model. We hope the combination could benefit from the simplicity of a joint source channel model without losing the flexibility of the PBSMT system. Introduction Machine transliteration has drawn a lot of attention in the previous years. In particular, the previous two shared tasks (Li et al., 2009; Li et al., 2010) attracted more than 30 participants. This year’s task only focuses on the transliteration generation task. As our first attempt in this area, we participated in English-to-Chinese transliteration (En-Ch) and Chinese-to-English back transliteration (Ch-En) tasks. For En-Ch and Ch-En transliterations, there was a discussion on whether to use the intermediate phonemic interpretation, i.e., Pinyin. Li et al. (2004) showed empirically that by skipping the intermediate phonemic interpretation (denoted as grapheme-based methods), the transliteration error rate was reduced significa"
W11-3216,W10-2401,0,0.278245,"onsider the key features of the transliteration task, which, on the other hand, have been emphasized by the joint source channel models. Our primary system is a standard phrase-based statistical machine translation (PBSMT) system with a modification based on the Multi-to-Multi Joint Source Channel model. We hope the combination could benefit from the simplicity of a joint source channel model without losing the flexibility of the PBSMT system. Introduction Machine transliteration has drawn a lot of attention in the previous years. In particular, the previous two shared tasks (Li et al., 2009; Li et al., 2010) attracted more than 30 participants. This year’s task only focuses on the transliteration generation task. As our first attempt in this area, we participated in English-to-Chinese transliteration (En-Ch) and Chinese-to-English back transliteration (Ch-En) tasks. For En-Ch and Ch-En transliterations, there was a discussion on whether to use the intermediate phonemic interpretation, i.e., Pinyin. Li et al. (2004) showed empirically that by skipping the intermediate phonemic interpretation (denoted as grapheme-based methods), the transliteration error rate was reduced significantly, since the ma"
W11-3216,J04-4002,0,0.0681232,"can be used in the transliterated western names. Accordingly, for each source name, there are only a limited set of candidate transliterations, unlike the infinite target set for the general translation task. It is critical to take into account these characteristics mentioned above when utilizing an SMT system for transliteration. First, the distortion model, one of the major components in a standard PBSMT system, is redundant for transliteration. Including the unnecessary model expands the search space and makes it more difficult to find the good candidates. Second, the word alignment model (Och and Ney, 2004) in a PBSMT system also assumes flexible ordering of correspondence to some extent. This could introduce additional noise to the translation models if applied directly to transliteration tasks without any modifications. 2.2 P (< e, c &gt;k |< e, c &gt;k−1 k−n+1 ) t = argmax P (s, t, α) (2) s = argmax P (s, t, α) (3) s,α t,α The alignment statistics can be obtained with an Expectation-Maximization procedure over the training corpus. For English-Chinese bidirectional transliteration, Li et al. (2004) assumed that each Chinese character aligns with a sequence of one or more letters in English. This ass"
W11-3216,P03-1021,0,0.00488314,"t to 0. 103 Tasks English-to-Chinese English-to-Chinese Chinese-to-English Chinese-to-English System M2MJC+PBSMT M2MJC M2MJC+PBSMT M2MJC ACC 0.320 0.260 0.133 0.117 Mean F 0.674 0.638 0.746 0.731 MRR 0.397 0.340 0.210 0.177 Map ref 0.308 0.251 0.133 0.117 Table 2: Official results 3.2.3 5 Parameter tuning The system integrates all the models into a more complex discriminative model in a log linear formulation. The weights for the individual models can be optimized on development data so that the system outputs are as close as possible to correct candidates. Minimum error rate training (MERT) (Och, 2003) is one of the common method for balancing between features on different bases. We used Z-MERT (Zaidan, 2009) to search for the set of feature weights that maximizes the official f-score evaluation metric on the development set. Moreover, we extracted a small development set of 500 names randomly from the official development set. The rest of the official development set served as a development test set, so we could run additional experiments on the provided data set apart from our submission. The feature weights we used for our submission are obtained from the complete development set. 4 Conc"
W11-3216,D09-1069,0,0.0185288,"ocuses on the transliteration generation task. As our first attempt in this area, we participated in English-to-Chinese transliteration (En-Ch) and Chinese-to-English back transliteration (Ch-En) tasks. For En-Ch and Ch-En transliterations, there was a discussion on whether to use the intermediate phonemic interpretation, i.e., Pinyin. Li et al. (2004) showed empirically that by skipping the intermediate phonemic interpretation (denoted as grapheme-based methods), the transliteration error rate was reduced significantly, since the mapping between Pinyin and Chinese characters was not trivial. Oh et al. (2009) had a more generalized version of Li et al. (2004)’s system as well as other 2.1 Phrase-based SMT The basic architecture of a phrase-based SMT system is an instance of the noisy-channel approaches (Brown et al., 1993). In the context of transliteration, the term “phrase” in phrase-based SMT would refer to a sequence of characters chosen by its statistical rather then any grammatical properties. The transliteration of a name s in the source language into a name t in the target language is modeled as: 101 Proceedings of the 2011 Named Entities Workshop, IJCNLP 2011, pages 101–105, Chiang Mai, T"
W11-3216,P07-2045,0,\N,Missing
W11-3216,J98-4003,0,\N,Missing
W11-3404,I05-2035,0,0.0775161,"Missing"
W11-3404,P08-1111,0,0.0461164,"Missing"
W11-3404,C10-1122,0,0.252129,"er to its head. Unlike Yu et al. (2010) who separate complement list into LCOMPS and RCOMPS, we keep all complements on the same complement list (hCOMPS),iand use an additional boolean feature RC ± to indicate whether the complement is to the right or to the left of the head. The grammar currently contains about 20 rule schemata. It should be noted that most of these rule schemata are very general. They are be used to handle multiple types of constructions, some of which will be illustrated below. 2.2 HEAD Yu et al. (2010) introduce an extra valence feature (TOPIC) for the topic construction. Tse and Curran (2010) distinguish two types of topics,gap or non-gap. Both solutions are rather similar to ours nonetheless. 2.4 Numeral-classifier structures are analyzed as a phrase with rule SPEC - HEAD, and they together serve as a specifier to the head noun. A feature “ CL” in the HEAD type of noun identifies the suitable groups of classifiers. Demonstratives are also treated as specifiers to nouns (similar to the double specifier account in (Ng, 1997)), though specific word order constraints are further enforced for the correct NP structure. Both specifiers of nouns are optional. The numeral before the class"
W11-3404,Y09-2048,0,0.107667,"ice sentences in Chinese. Similar to the analysis of BA , we use a specialized unary rule to promote the complement of the verb into the subject list, and "" # HEAD noun change the original subject into a Locative phrases serve as both pre-verbal and post-verbal modifiers, and generally take the form of zai + NP + Loc, e.g. 在 桌子 上 (on the table), 在 房子 东面 (to the east of the house), etc. Locative phrases can always serve as pre-verbal modifiers. But only certain verbs can take postverbal locatives with the HEAD - ADJ rule. The treatment of locative phrases as normal prepositional phrases as in (Wang et al., 2009) may lead to massive over-generation. The analysis of temporal phrases is similar to the locative phrases. 2.7 INDEX There are various discussions on BA in the literature. Bender (2000) considered it as a verb, Gao (2000) and Wang et al. (2009) treated it as a casemarker, and Yu et al. (2010) as a preposition. We categorize BA as a special coverb. This makes it similar to prepositions. But it will be subcategorized by (instead of modifying) the verb phrase. • non-gapping D E where neither of the above two cases applies 2.6 1 moves the direct object of a verb 20 2.10 Resultative verb compound o"
W11-3404,C10-2162,0,0.199621,"no special treatment involved • Temporal or location topics are treated as modifiers with ADJ - HEAD • A special rule SUBJ 2- HEAD is used to fill topics headed by noun or verb into the SPR valence of the main sentence. This is also referred to as the “double subject” constructions An HPSG Analysis of Mandarin Design of sign & schemata The design of the HPSG sign in MCG is compatible with the design in the LinGO Grammar Matrix. Four valcence features were employed: SUBJ for subjects, COMPS for complements, SPR for specifiers, and SPEC for back-reference from the specifier to its head. Unlike Yu et al. (2010) who separate complement list into LCOMPS and RCOMPS, we keep all complements on the same complement list (hCOMPS),iand use an additional boolean feature RC ± to indicate whether the complement is to the right or to the left of the head. The grammar currently contains about 20 rule schemata. It should be noted that most of these rule schemata are very general. They are be used to handle multiple types of constructions, some of which will be illustrated below. 2.2 HEAD Yu et al. (2010) introduce an extra valence feature (TOPIC) for the topic construction. Tse and Curran (2010) distinguish two t"
W11-3404,C94-2144,0,0.645808,"Missing"
W13-4403,C02-1126,0,0.0866221,"Missing"
W13-4403,A00-2018,0,0.0149568,"Missing"
W13-4403,D09-1127,0,0.077969,"Missing"
W13-4403,J93-2004,0,0.0424905,"Missing"
W13-4403,P03-1056,0,0.085125,"Missing"
W13-4403,W12-6332,1,0.808052,"Missing"
W13-4403,W09-3825,0,0.0151052,"tion Penn Treebank (PTB) was built based on the idea of context-free PSG (Marcus et al., 1993). It is now a common practice to develop data-driven English parsers using PTB annotation and encouraging performances have been reported (Collins, 2000; Charniak, 2000). Following the success of PTB, Xue et al. 2000 built Penn Chinese Treebank (CTB). CTB is also based on context-free PSG. Since CTB provides training data for Chinese parsing, researchers attempted to train Chinese parsing with CTB (Bikel and Chiang, 2000; Chiang and Bikel, 2002; Levy and Manning, 2003; Bikel, 2004; Wang et al., 2006; Zhang and Clark, 2009; 1a. Students process data 1b. Data processing system 1c. Data was processed 2a. 学生 处理 数据 Student process data Students process data 2b. 数据 处理 系统 Data process system Data processing system 11 Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing (SIGHAN-7), pages 11–19, Nagoya, Japan, 14 October 2013. 2c. 数据 处理 了 Data process le Data was processed 4a. 鸟儿 飞 向 南方 Bird fly towards south Birds fly towards the south 4b. *鸟儿 吃 向 南方 Bird eat towards south Birds eat towards the south 4c. *鸟儿 喜欢 向 南方 Bird like towards south Birds like towards the south 5a. Agent Direction V 5b. Age"
W13-4403,W00-1201,0,\N,Missing
W13-4403,P06-1054,0,\N,Missing
wang-zhang-2010-hybrid,N07-1051,0,\N,Missing
wang-zhang-2010-hybrid,W09-1204,1,\N,Missing
wang-zhang-2010-hybrid,C96-1058,0,\N,Missing
wang-zhang-2010-hybrid,H05-1066,0,\N,Missing
wang-zhang-2010-hybrid,W00-1205,0,\N,Missing
wang-zhang-2010-hybrid,W04-3224,0,\N,Missing
Y11-1025,W02-1502,1,0.874761,"eaning, but also those that exist only to express generalizations over their subtypes. We use these techniques to explore the degree of redundancy in a range of DELPH - IN1 grammars, including the two grammars of Wambaya (Bender, 2010), the BURGER grammar of Bulgarian (Osenova, 2010), the ManGO grammar2 of Mandarin Chinese, all built with the LinGO 1 2 Copyright 2011 by Antske Fokkens, Yi Zhang and Emily M. Bender http://www.delph-in.net/ http://wiki.delph-in.net/moin/MandarinGrammarOnline 25th Pacific Asia Conference on Language, Information and Computation, pages 236–244 236 Grammar Matrix (Bender et al., 2002; Bender et al., 2010), and two much larger grammars, the English Resource Grammar (Flickinger, 2000) and German Grammar (M¨uller and Kasper, 2000; Crysmann, 2005). This paper is structured as follows: First, we describe the overall structure of the grammars under consideration. This section is followed by an overview of the first approach under examination: removing superfluous types from the grammar. Section 4 provides the details of our second investigation of relevant types: maximally reduced computationally equivalent grammars. The next section presents our quantitative results and their"
zhang-etal-2012-joint,C10-2166,1,\N,Missing
zhang-etal-2012-joint,C10-1122,0,\N,Missing
zhang-etal-2012-joint,W97-1502,0,\N,Missing
zhang-etal-2012-joint,C10-2162,0,\N,Missing
zhang-etal-2012-joint,W09-3032,1,\N,Missing
zhang-etal-2012-joint,W09-2605,1,\N,Missing
zhang-etal-2012-joint,C02-2025,0,\N,Missing
zhang-etal-2012-joint,W02-1502,0,\N,Missing
zhang-etal-2012-joint,Y09-2048,0,\N,Missing
zhang-etal-2012-joint,P05-1041,0,\N,Missing
zhang-etal-2012-joint,W11-3404,1,\N,Missing
zhang-kordoni-2006-automated,copestake-flickinger-2000-open,0,\N,Missing
zhang-kordoni-2006-automated,W96-0213,0,\N,Missing
zhang-kordoni-2006-automated,E03-1041,0,\N,Missing
zhang-kordoni-2006-automated,C02-2025,0,\N,Missing
zhang-kordoni-2006-automated,W00-0740,0,\N,Missing
zhang-kordoni-2006-automated,P04-1057,0,\N,Missing
zhang-kordoni-2006-automated,W05-1008,0,\N,Missing
zhang-kordoni-2006-automated,baldwin-etal-2004-road,0,\N,Missing
zhang-kordoni-2006-automated,W02-2018,0,\N,Missing
zhang-kordoni-2008-robust,W03-2401,0,\N,Missing
zhang-kordoni-2008-robust,callmeier-etal-2004-deepthought,0,\N,Missing
zhang-kordoni-2008-robust,J97-4005,0,\N,Missing
zhang-kordoni-2008-robust,zhang-kordoni-2006-automated,1,\N,Missing
zhang-kordoni-2008-robust,W07-2207,1,\N,Missing
zhang-kordoni-2008-robust,C02-2025,0,\N,Missing
zhang-kordoni-2008-robust,W07-1217,1,\N,Missing
zhang-kordoni-2008-robust,W05-1008,0,\N,Missing
zhang-kordoni-2008-robust,P06-4020,0,\N,Missing
zhang-kordoni-2008-robust,W06-1106,0,\N,Missing
zhang-kordoni-2008-robust,P99-1052,0,\N,Missing
zhang-kordoni-2008-robust,I05-1015,0,\N,Missing
zhang-kordoni-2008-robust,baldwin-etal-2004-road,0,\N,Missing
