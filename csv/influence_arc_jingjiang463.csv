2020.aacl-srw.2,N19-4009,0,0.0503265,"Missing"
2020.aacl-srw.2,W18-6301,0,0.150906,"Computational Linguistics the first implementations of co-attention on transformer model (Lu et al., 2019; Tan and Bansal, 2019) and we are the first to apply transformerbased co-attention on machine translation tasks. In particular, we apply our model on the popular WMT machine translation tasks where two input channels are in one same domain. Our code also leverages half-precision floating-point format (FP16) training and synchronous distributed training for inter-GPU communication (we do not discard gradients calculated by ”stragglers”) which dramatically accelerate our training procedure (Ott et al., 2018; Micikevicius et al., 2018). Rd → R is a pairwise function (“×” is Cartesian product), g : Rd → Rd is a unary function and C : Rd × Rn×d → R calculates a normalizer, but dispense with the assumption that V = K = Q. However, if we assume f (qi , kj ) = T Q T K T e(qi W )·(kj W ) P , g(vi ) = viT W V , the normalizer C(qi , K) = j f (qi , kj ) and V = K = Q, then the non-local operation degrades to the multihead self-attention as is described in (Vaswani et al., 2017) (formula 2 describes only one attention head): 2 Considering two input channels, denoted as ‘left’ and ‘right’, we present the f"
2020.aacl-srw.2,P19-1295,0,0.0247103,"Missing"
2020.aacl-srw.2,D18-1475,0,0.0182484,"le 2: This table evaluates if the models selected by the Dev set are also better than others on the test set. Here we provide the percentage of selected models that rank TOP 1, TOP 3, TOP 5 or TOP 10 among all models derived from all training epochs. Figure 6: Loss vs Epoch for THM-big and Transformerbig on EN-FI ers (Vaswani et al., 2017), recent work points out that it may tend to overlook neighboring information (Yang et al., 2019a; Xu et al., 2019). It is found that applying an adaptive attention span could be conducive to character level language modeling tasks (Sukhbaatar et al., 2019). Yang et al. (2018a) propose to model localness for selfattention, which would be conducive to capturing local information by learning a Gaussian bias predicting the region of local attention. Other work indicates that adding convolution layers would ameliorate the aforementioned issue (Yang et al., 2018b, 2019b). Multi-head attention can also be used in multi-modal scenarios when V, K and Q gates take in data from different domains. Helcl et al. (2018) adds an attention layer on top of the encoder-decoder layer with K and V being CNNextracted image features. Lu et al. (2019) and Tan and Bansal (2019) use co-at"
2020.aacl-srw.2,P19-1032,0,0.0451967,"Missing"
2020.aacl-srw.2,N19-1407,0,0.045674,"Missing"
2020.aacl-srw.2,D19-1514,0,0.130994,"ls but do not assume that the two channels of input must be disparate. Our coattention mechanism is designed in a ”Transformer” style, and to the best of our knowledge, our proposed Crossed Co-Attention Network is one of 8 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop, pages 8–15 c December 4 - 7, 2020. 2020 Association for Computational Linguistics the first implementations of co-attention on transformer model (Lu et al., 2019; Tan and Bansal, 2019) and we are the first to apply transformerbased co-attention on machine translation tasks. In particular, we apply our model on the popular WMT machine translation tasks where two input channels are in one same domain. Our code also leverages half-precision floating-point format (FP16) training and synchronous distributed training for inter-GPU communication (we do not discard gradients calculated by ”stragglers”) which dramatically accelerate our training procedure (Ott et al., 2018; Micikevicius et al., 2018). Rd → R is a pairwise function (“×” is Cartesian product), g : Rd → Rd is a unary f"
2020.aacl-srw.2,Q16-1019,0,0.0936098,"Missing"
2020.acl-main.306,W15-4319,0,0.0960996,"Missing"
2020.acl-main.306,C02-1025,0,0.289138,"including entity linking (Derczynski et al., 2015), opinion mining (Maynard et al., 2012), and event detection (Ritter et al., 2012), named 3349 entity recognition (NER) has attracted much attention in the research community in the past two decades (Li et al., 2018). Methods for NER: In the literature, various supervised learning approaches have been proposed for NER. Traditional approaches typically focus on designing various effective NER features, followed by feeding them to different linear classifiers such as maximum entropy, conditional random fields (CRFs), and support vector machines (Chieu and Ng, 2002; Florian et al., 2003; Finkel et al., 2005; Ratinov and Roth, 2009; Lin and Wu, 2009; Passos et al., 2014; Luo et al., 2015). To reduce the feature engineering efforts, a number of recent studies proposed to couple different neural network architectures with a CRF layer (Lafferty et al., 2001) for word-level predictions, including convolutional neural networks (Collobert et al., 2011), recurrent neural networks (Chiu and Nichols, 2016; Lample et al., 2016), and their hierarchical combinations (Ma and Hovy, 2016). These neural approaches have been shown to achieve the state-of-the-art performa"
2020.acl-main.306,Q16-1026,0,0.0686536,"ctive NER features, followed by feeding them to different linear classifiers such as maximum entropy, conditional random fields (CRFs), and support vector machines (Chieu and Ng, 2002; Florian et al., 2003; Finkel et al., 2005; Ratinov and Roth, 2009; Lin and Wu, 2009; Passos et al., 2014; Luo et al., 2015). To reduce the feature engineering efforts, a number of recent studies proposed to couple different neural network architectures with a CRF layer (Lafferty et al., 2001) for word-level predictions, including convolutional neural networks (Collobert et al., 2011), recurrent neural networks (Chiu and Nichols, 2016; Lample et al., 2016), and their hierarchical combinations (Ma and Hovy, 2016). These neural approaches have been shown to achieve the state-of-the-art performance on different benchmark datasets based on formal text (Yang et al., 2018). However, when applying these approaches to social media text, most of them fail to achieve satisfactory results. To address this issue, many studies proposed to exploit external resources (e.g., shallow parser, Freebase dictionary, and orthographic characteristics) to incorporate a set of tweet-specific features into both traditional approaches (Ritter et al."
2020.acl-main.306,N16-1030,0,0.764588,"f automatically extracting important information from these massive unstructured contents. As a crucial component of many information extraction tasks, named entity recognition (NER) aims to discover named entities in free text and classify them into pre-defined types, such as person (PER), location (LOC) and organization (ORG). Given its importance, NER has attracted much attention in the research community (Yadav and Bethard, 2018). Although many methods coupled with either discrete shallow features (Zhou and Su, 2002; Finkel et al., 2005; Torisawa et al., 2007) or continuous deep features (Lample et al., 2016; Ma and Hovy, ∗ Corresponding author. (b). Vote for [King of the Jungle MISC] — [Kian PER] or [David PER] ? 2016) have shown success in identifying entities in formal newswire text, most of them perform poorly on informal social media text (e.g., tweets) due to its short length and noisiness. To adapt existing NER models to social media, various methods have been proposed to incorporate many tweet-specific features (Ritter et al., 2011; Li et al., 2012, 2014; Limsopatham and Collier, 2016). More recently, as social media posts become increasingly multimodal, several studies proposed to exploi"
2020.acl-main.306,W16-3920,0,0.0876071,"crete shallow features (Zhou and Su, 2002; Finkel et al., 2005; Torisawa et al., 2007) or continuous deep features (Lample et al., 2016; Ma and Hovy, ∗ Corresponding author. (b). Vote for [King of the Jungle MISC] — [Kian PER] or [David PER] ? 2016) have shown success in identifying entities in formal newswire text, most of them perform poorly on informal social media text (e.g., tweets) due to its short length and noisiness. To adapt existing NER models to social media, various methods have been proposed to incorporate many tweet-specific features (Ritter et al., 2011; Li et al., 2012, 2014; Limsopatham and Collier, 2016). More recently, as social media posts become increasingly multimodal, several studies proposed to exploit useful visual information to improve the performance of NER (Moon et al., 2018; Zhang et al., 2018; Lu et al., 2018). In this work, following the recent trend, we focus on multimodal named entity recognition (MNER) for social media posts, where the goal is to detect named entities and identify their entity types given a {sentence, image} pair. For example, in Fig. 1.a, it is expected to recognize that Kevin Durant, Oracle Arena, and Jordan belong to the category of person names (i.e., PER"
2020.acl-main.306,W17-4421,0,0.0163641,"e been shown to achieve the state-of-the-art performance on different benchmark datasets based on formal text (Yang et al., 2018). However, when applying these approaches to social media text, most of them fail to achieve satisfactory results. To address this issue, many studies proposed to exploit external resources (e.g., shallow parser, Freebase dictionary, and orthographic characteristics) to incorporate a set of tweet-specific features into both traditional approaches (Ritter et al., 2011; Li et al., 2014; Baldwin et al., 2015) and recent neural approaches (Limsopatham and Collier, 2016; Lin et al., 2017), which can obtain much better performance on social media text. Methods for Multimodal NER (MNER): As multimodal data become increasingly popular on social media platforms, several recent studies focus on the MNER task, where the goal is to leverage the associate images to better identify the named entities contained in the text. Specifically, Moon et al. (2018) proposed a multimodal NER network with modality attention to fuse the textual and visual information. To model the inter-modal interactions and filter out the noise in the visual context, Zhang et al. (2018) and Lu et al. (2018) respe"
2020.acl-main.306,P09-1116,0,0.0290948,"2), and event detection (Ritter et al., 2012), named 3349 entity recognition (NER) has attracted much attention in the research community in the past two decades (Li et al., 2018). Methods for NER: In the literature, various supervised learning approaches have been proposed for NER. Traditional approaches typically focus on designing various effective NER features, followed by feeding them to different linear classifiers such as maximum entropy, conditional random fields (CRFs), and support vector machines (Chieu and Ng, 2002; Florian et al., 2003; Finkel et al., 2005; Ratinov and Roth, 2009; Lin and Wu, 2009; Passos et al., 2014; Luo et al., 2015). To reduce the feature engineering efforts, a number of recent studies proposed to couple different neural network architectures with a CRF layer (Lafferty et al., 2001) for word-level predictions, including convolutional neural networks (Collobert et al., 2011), recurrent neural networks (Chiu and Nichols, 2016; Lample et al., 2016), and their hierarchical combinations (Ma and Hovy, 2016). These neural approaches have been shown to achieve the state-of-the-art performance on different benchmark datasets based on formal text (Yang et al., 2018). However"
2020.acl-main.306,P05-1045,0,0.161816,"formation, the flourish of social media also solicits the emerging need of automatically extracting important information from these massive unstructured contents. As a crucial component of many information extraction tasks, named entity recognition (NER) aims to discover named entities in free text and classify them into pre-defined types, such as person (PER), location (LOC) and organization (ORG). Given its importance, NER has attracted much attention in the research community (Yadav and Bethard, 2018). Although many methods coupled with either discrete shallow features (Zhou and Su, 2002; Finkel et al., 2005; Torisawa et al., 2007) or continuous deep features (Lample et al., 2016; Ma and Hovy, ∗ Corresponding author. (b). Vote for [King of the Jungle MISC] — [Kian PER] or [David PER] ? 2016) have shown success in identifying entities in formal newswire text, most of them perform poorly on informal social media text (e.g., tweets) due to its short length and noisiness. To adapt existing NER models to social media, various methods have been proposed to incorporate many tweet-specific features (Ritter et al., 2011; Li et al., 2012, 2014; Limsopatham and Collier, 2016). More recently, as social media"
2020.acl-main.306,W03-0425,0,0.0280736,"king (Derczynski et al., 2015), opinion mining (Maynard et al., 2012), and event detection (Ritter et al., 2012), named 3349 entity recognition (NER) has attracted much attention in the research community in the past two decades (Li et al., 2018). Methods for NER: In the literature, various supervised learning approaches have been proposed for NER. Traditional approaches typically focus on designing various effective NER features, followed by feeding them to different linear classifiers such as maximum entropy, conditional random fields (CRFs), and support vector machines (Chieu and Ng, 2002; Florian et al., 2003; Finkel et al., 2005; Ratinov and Roth, 2009; Lin and Wu, 2009; Passos et al., 2014; Luo et al., 2015). To reduce the feature engineering efforts, a number of recent studies proposed to couple different neural network architectures with a CRF layer (Lafferty et al., 2001) for word-level predictions, including convolutional neural networks (Collobert et al., 2011), recurrent neural networks (Chiu and Nichols, 2016; Lample et al., 2016), and their hierarchical combinations (Ma and Hovy, 2016). These neural approaches have been shown to achieve the state-of-the-art performance on different bench"
2020.acl-main.306,P18-1185,0,0.428682,"Missing"
2020.acl-main.306,D15-1104,0,0.0208117,"2012), named 3349 entity recognition (NER) has attracted much attention in the research community in the past two decades (Li et al., 2018). Methods for NER: In the literature, various supervised learning approaches have been proposed for NER. Traditional approaches typically focus on designing various effective NER features, followed by feeding them to different linear classifiers such as maximum entropy, conditional random fields (CRFs), and support vector machines (Chieu and Ng, 2002; Florian et al., 2003; Finkel et al., 2005; Ratinov and Roth, 2009; Lin and Wu, 2009; Passos et al., 2014; Luo et al., 2015). To reduce the feature engineering efforts, a number of recent studies proposed to couple different neural network architectures with a CRF layer (Lafferty et al., 2001) for word-level predictions, including convolutional neural networks (Collobert et al., 2011), recurrent neural networks (Chiu and Nichols, 2016; Lample et al., 2016), and their hierarchical combinations (Ma and Hovy, 2016). These neural approaches have been shown to achieve the state-of-the-art performance on different benchmark datasets based on formal text (Yang et al., 2018). However, when applying these approaches to soci"
2020.acl-main.306,P16-1101,0,0.17414,"slightly different from the one used in their experiments, as they later remove a small portion of tweets for privacy issues. 4 https://download.pytorch.org/models/resnet152b121ed2d.pth. Compared Systems To demonstrate the effect of our Unified Multimodal Transformer (UMT) model, we first consider a number of representative text-based approaches for NER: (1) BiLSTM-CRF (Huang et al., 2015), a pioneering study which eliminates the heavy reliance on hand-crafted features, and simply employs a bidirectional LSTM model followed by a CRF layer for each word’s final prediction; (2) CNN-BiLSTM-CRF (Ma and Hovy, 2016), a widely adopted neural network model for NER, which is an improvement of BiLSTM-CRF by replacing each word’s word embedding with the concatenation of its word embedding and CNNbased character-level word representations; (3) HBiLSTM-CRF (Lample et al., 2016), an end-toend hierarchical LSTM architectures, which replaces the bottom CNN layer in CNN-BiLSTMCRF with an LSTM layer to obtain the characterlevel word representations; (4) BERT (Devlin et al., 2018), a multi-layer bidirectional Transformer encoder, which gives contextualized representations for each word, followed by stacking a softmax"
2020.acl-main.306,N18-1078,0,0.508186,"he Jungle MISC] — [Kian PER] or [David PER] ? 2016) have shown success in identifying entities in formal newswire text, most of them perform poorly on informal social media text (e.g., tweets) due to its short length and noisiness. To adapt existing NER models to social media, various methods have been proposed to incorporate many tweet-specific features (Ritter et al., 2011; Li et al., 2012, 2014; Limsopatham and Collier, 2016). More recently, as social media posts become increasingly multimodal, several studies proposed to exploit useful visual information to improve the performance of NER (Moon et al., 2018; Zhang et al., 2018; Lu et al., 2018). In this work, following the recent trend, we focus on multimodal named entity recognition (MNER) for social media posts, where the goal is to detect named entities and identify their entity types given a {sentence, image} pair. For example, in Fig. 1.a, it is expected to recognize that Kevin Durant, Oracle Arena, and Jordan belong to the category of person names (i.e., PER), place names (i.e., LOC), and other names (i.e., MISC), respectively. While previous work has shown success of fusing visual information into NER (Moon et al., 2018; Zhang et al., 201"
2020.acl-main.306,W14-1609,0,0.0230089,"ction (Ritter et al., 2012), named 3349 entity recognition (NER) has attracted much attention in the research community in the past two decades (Li et al., 2018). Methods for NER: In the literature, various supervised learning approaches have been proposed for NER. Traditional approaches typically focus on designing various effective NER features, followed by feeding them to different linear classifiers such as maximum entropy, conditional random fields (CRFs), and support vector machines (Chieu and Ng, 2002; Florian et al., 2003; Finkel et al., 2005; Ratinov and Roth, 2009; Lin and Wu, 2009; Passos et al., 2014; Luo et al., 2015). To reduce the feature engineering efforts, a number of recent studies proposed to couple different neural network architectures with a CRF layer (Lafferty et al., 2001) for word-level predictions, including convolutional neural networks (Collobert et al., 2011), recurrent neural networks (Chiu and Nichols, 2016; Lample et al., 2016), and their hierarchical combinations (Ma and Hovy, 2016). These neural approaches have been shown to achieve the state-of-the-art performance on different benchmark datasets based on formal text (Yang et al., 2018). However, when applying these"
2020.acl-main.306,W09-1119,0,0.0459286,"ing (Maynard et al., 2012), and event detection (Ritter et al., 2012), named 3349 entity recognition (NER) has attracted much attention in the research community in the past two decades (Li et al., 2018). Methods for NER: In the literature, various supervised learning approaches have been proposed for NER. Traditional approaches typically focus on designing various effective NER features, followed by feeding them to different linear classifiers such as maximum entropy, conditional random fields (CRFs), and support vector machines (Chieu and Ng, 2002; Florian et al., 2003; Finkel et al., 2005; Ratinov and Roth, 2009; Lin and Wu, 2009; Passos et al., 2014; Luo et al., 2015). To reduce the feature engineering efforts, a number of recent studies proposed to couple different neural network architectures with a CRF layer (Lafferty et al., 2001) for word-level predictions, including convolutional neural networks (Collobert et al., 2011), recurrent neural networks (Chiu and Nichols, 2016; Lample et al., 2016), and their hierarchical combinations (Ma and Hovy, 2016). These neural approaches have been shown to achieve the state-of-the-art performance on different benchmark datasets based on formal text (Yang et a"
2020.acl-main.306,D11-1141,0,0.470217,"lthough many methods coupled with either discrete shallow features (Zhou and Su, 2002; Finkel et al., 2005; Torisawa et al., 2007) or continuous deep features (Lample et al., 2016; Ma and Hovy, ∗ Corresponding author. (b). Vote for [King of the Jungle MISC] — [Kian PER] or [David PER] ? 2016) have shown success in identifying entities in formal newswire text, most of them perform poorly on informal social media text (e.g., tweets) due to its short length and noisiness. To adapt existing NER models to social media, various methods have been proposed to incorporate many tweet-specific features (Ritter et al., 2011; Li et al., 2012, 2014; Limsopatham and Collier, 2016). More recently, as social media posts become increasingly multimodal, several studies proposed to exploit useful visual information to improve the performance of NER (Moon et al., 2018; Zhang et al., 2018; Lu et al., 2018). In this work, following the recent trend, we focus on multimodal named entity recognition (MNER) for social media posts, where the goal is to detect named entities and identify their entity types given a {sentence, image} pair. For example, in Fig. 1.a, it is expected to recognize that Kevin Durant, Oracle Arena, and J"
2020.acl-main.306,E99-1023,0,0.117497,"give an overview of our method. We then delve into the details of each component in our model. Task Formulation: Given a sentence S and its associated image V as input, the goal of MNER is to extract a set of entities from S, and classify each extracted entity into one of the pre-defined types. As with most existing work on MNER, we formulate the task as a sequence labeling problem. Let S = (s1 , s2 , . . . , sn ) denote a sequence of input words, and y = (y1 , y2 , . . . , yn ) be the corresponding label sequence, where yi ∈ Y and Y is the pre-defined label set with the BIO2 tagging schema (Sang and Veenstra, 1999). 2.1 Overall Architecture Fig. 2.a illustrates the overall architecture of our Unified Multimodal Transformer, which contains three main components: (1) representation learning for unimodal input; (2) a Multimodal Transformer for MNER; and (3) a unified architecture with auxiliary entity span detection (ESD) module. 3343 Multimodal Transformer for MNER Auxiliary Entity Span Detection Module t0 B I F1 F2 t1 t2 O ...... h0 F3 ...... Fn t3 ...... tn c0 K c1 I-PER O ...... B-MISC E1 E2 E3 ...... En h1 h2 h3 ...... hn EKevin a0 tn+1 r0 r1 r3 ...... rn r2 Transformer Layer with Self-Attention V Q c"
2020.acl-main.306,D07-1073,0,0.135269,"Missing"
2020.acl-main.306,P19-1656,0,0.027843,"block is often closely related to several input words, incorporating the visual block representation can potentially make the prediction of its related words more accurately. Inspired by these observations, we propose a multimodal interaction (MMI) module to learn an image-aware word representation and a word-aware visual representation for each word. 2.3.1 Image-Aware Word Representation Cross-Modal Transformer (CMT) Layer: As shown on the left of Fig. 2.b, to learn better word representations with the guidance of associated images, we first employ an m-head cross-modal attention mechanism (Tsai et al., 2019), by treating V ∈ Rd×49 as queries, and R ∈ Rd×(n+1) as keys and values: CAi (V, R) = softmax( MH-CA(V, R) = W0 [CA1 (V, R), . . . , CAm (V, R)]&gt; , where CAi refers to the i-th head of cross-modal attention, {Wqi , Wki , Wvi } ∈ Rd/m×d , and W0 ∈ Rd×d denote the weight matrices for the query, key, value, and multi-head attention, respectively. Next, we stack another three sub-layers on top: 1 (1) (2) Bias terms are omitted to avoid confusion in this paper. 2.3.2 Word-Aware Visual Representation To obtain a visual representation for each word, it is necessary to align each word with its closely"
2020.acl-main.306,C18-1182,0,0.0135433,"e growth of user-generated contents on social media platforms such as Twitter. While empowering users with rich information, the flourish of social media also solicits the emerging need of automatically extracting important information from these massive unstructured contents. As a crucial component of many information extraction tasks, named entity recognition (NER) aims to discover named entities in free text and classify them into pre-defined types, such as person (PER), location (LOC) and organization (ORG). Given its importance, NER has attracted much attention in the research community (Yadav and Bethard, 2018). Although many methods coupled with either discrete shallow features (Zhou and Su, 2002; Finkel et al., 2005; Torisawa et al., 2007) or continuous deep features (Lample et al., 2016; Ma and Hovy, ∗ Corresponding author. (b). Vote for [King of the Jungle MISC] — [Kian PER] or [David PER] ? 2016) have shown success in identifying entities in formal newswire text, most of them perform poorly on informal social media text (e.g., tweets) due to its short length and noisiness. To adapt existing NER models to social media, various methods have been proposed to incorporate many tweet-specific feature"
2020.acl-main.306,C18-1327,0,0.0294224,"oth, 2009; Lin and Wu, 2009; Passos et al., 2014; Luo et al., 2015). To reduce the feature engineering efforts, a number of recent studies proposed to couple different neural network architectures with a CRF layer (Lafferty et al., 2001) for word-level predictions, including convolutional neural networks (Collobert et al., 2011), recurrent neural networks (Chiu and Nichols, 2016; Lample et al., 2016), and their hierarchical combinations (Ma and Hovy, 2016). These neural approaches have been shown to achieve the state-of-the-art performance on different benchmark datasets based on formal text (Yang et al., 2018). However, when applying these approaches to social media text, most of them fail to achieve satisfactory results. To address this issue, many studies proposed to exploit external resources (e.g., shallow parser, Freebase dictionary, and orthographic characteristics) to incorporate a set of tweet-specific features into both traditional approaches (Ritter et al., 2011; Li et al., 2014; Baldwin et al., 2015) and recent neural approaches (Limsopatham and Collier, 2016; Lin et al., 2017), which can obtain much better performance on social media text. Methods for Multimodal NER (MNER): As multimoda"
2020.acl-main.306,P02-1060,0,0.0655541,"users with rich information, the flourish of social media also solicits the emerging need of automatically extracting important information from these massive unstructured contents. As a crucial component of many information extraction tasks, named entity recognition (NER) aims to discover named entities in free text and classify them into pre-defined types, such as person (PER), location (LOC) and organization (ORG). Given its importance, NER has attracted much attention in the research community (Yadav and Bethard, 2018). Although many methods coupled with either discrete shallow features (Zhou and Su, 2002; Finkel et al., 2005; Torisawa et al., 2007) or continuous deep features (Lample et al., 2016; Ma and Hovy, ∗ Corresponding author. (b). Vote for [King of the Jungle MISC] — [Kian PER] or [David PER] ? 2016) have shown success in identifying entities in formal newswire text, most of them perform poorly on informal social media text (e.g., tweets) due to its short length and noisiness. To adapt existing NER models to social media, various methods have been proposed to incorporate many tweet-specific features (Ritter et al., 2011; Li et al., 2012, 2014; Limsopatham and Collier, 2016). More rece"
2020.acl-main.91,C16-1236,0,0.576229,"nd recently some studies looked into complex KBQA. Two different types of complexity have been studied: (1) Single-relation questions with constraints. For example, in the question “Who was the first president of the U.S.?” there is a single relation “president of” between the answer entity and the entity “U.S.,” but we also have the constraint “first” that needs to be satisfied. For this type of complex questions, a staged query graph generation method has been proposed, which first identifies a single-hop relation path and then adds constraints to it to form a query graph (Yih et al., 2015; Bao et al., 2016; Luo et al., 2018). (2) Questions with multiple hops of relations. For example, Jing Jiang Singapore Management University jingjiang@smu.edu.sg for the question “Who is the wife of the founder of Facebook?” the answer is related to “Facebook” through two hops of relations, namely, “wife of” and “founder of.” To answer this type of multihop questions, we need to consider longer relation paths in order to reach the correct answers. The main challenge here is how to restrict the search space, i.e., to reduce the number of multi-hop relation paths to be considered, because the search space grows"
2020.acl-main.91,P17-1003,0,0.107972,"plexity at the same time. Motivated by the observation that early incorporation of constraints into query graphs can more effectively prune the search space, we propose a modified staged query graph generation method with more flexible ways to generate query graphs. Our experiments clearly show that our method achieves the state of the art on three benchmark KBQA datasets. 1 Introduction Knowledge base question answering (KBQA) aims at answering factoid questions from a knowledge base (KB). It has attracted much attention in recent years (Bordes et al., 2014; Xu et al., 2016; Yu et al., 2017; Liang et al., 2017; Hu et al., 2018; Petrochuk and Zettlemoyer, 2018). Early work on KBQA focused on simple questions containing a single relation (Yih et al., 2014; Bordes et al., 2015; Dong et al., 2015; Hao et al., 2017). However, real questions are often more complex and recently some studies looked into complex KBQA. Two different types of complexity have been studied: (1) Single-relation questions with constraints. For example, in the question “Who was the first president of the U.S.?” there is a single relation “president of” between the answer entity and the entity “U.S.,” but we also have the constrain"
2020.acl-main.91,D14-1067,0,0.0504709,"f relations. In this paper, we handle both types of complexity at the same time. Motivated by the observation that early incorporation of constraints into query graphs can more effectively prune the search space, we propose a modified staged query graph generation method with more flexible ways to generate query graphs. Our experiments clearly show that our method achieves the state of the art on three benchmark KBQA datasets. 1 Introduction Knowledge base question answering (KBQA) aims at answering factoid questions from a knowledge base (KB). It has attracted much attention in recent years (Bordes et al., 2014; Xu et al., 2016; Yu et al., 2017; Liang et al., 2017; Hu et al., 2018; Petrochuk and Zettlemoyer, 2018). Early work on KBQA focused on simple questions containing a single relation (Yih et al., 2014; Bordes et al., 2015; Dong et al., 2015; Hao et al., 2017). However, real questions are often more complex and recently some studies looked into complex KBQA. Two different types of complexity have been studied: (1) Single-relation questions with constraints. For example, in the question “Who was the first president of the U.S.?” there is a single relation “president of” between the answer entity"
2020.acl-main.91,N19-1031,0,0.451761,"ngapore Management University jingjiang@smu.edu.sg for the question “Who is the wife of the founder of Facebook?” the answer is related to “Facebook” through two hops of relations, namely, “wife of” and “founder of.” To answer this type of multihop questions, we need to consider longer relation paths in order to reach the correct answers. The main challenge here is how to restrict the search space, i.e., to reduce the number of multi-hop relation paths to be considered, because the search space grows exponentially with the length of relation paths. One idea is to use beam search. For example, Chen et al. (2019) and Lan et al. (2019b) proposed to consider only the best matching relation instead of all relations when extending a relation path. However, little work has been done to deal with both types of complexity together. In this paper, we handle both constraints and multi-hop relations together for complex KBQA. We propose to modify the staged query graph generation method by allowing longer relation paths. However, instead of adding constraints only after relation paths have been constructed, we propose to incorporate constraints and extend relation paths at the same time. This allows us to more"
2020.acl-main.91,N19-1423,0,0.00966622,"a linking model. For temporal expressions and superlative linking, we simply use regular expressions and a superlative word list. The superlative words are manually mapped to two aggregation functions: argmax and argmin. We initialize the BERT module in the ranker with the BERT base model9 . Other parameters are initialized randomly. For the hyper-parameters in BERT model, we set the dropout ratio as 0.1, the hidden size as 768. The number of layers and the with the special token [CLS] separating them, as how BERT is used typically to handle two sequences. We then use the standard BERT model (Devlin et al., 2019) to process the entire sequence and derive a score at the top layer. Note that we fine-tune the pre-trained BERT parameters during learning. 8 The tool can be found at https://developers. google.com/knowledge-graph. 9 The pre-trained BERT base model could be found at https://github.com/huggingface/ pytorch-transformers. number of multi-attention heads are set as 6 and 12, respectively. we use the latest dump of Freebase10 as our KB for all the datasets. For beam search, we set the beam size K to be 3. 3.2 Datasets We use three datasets to evaluate our method: ComplexWebQuestons (CWQ) (Talmor a"
2020.acl-main.91,P15-1026,0,0.0258929,"query graph generation method with more flexible ways to generate query graphs. Our experiments clearly show that our method achieves the state of the art on three benchmark KBQA datasets. 1 Introduction Knowledge base question answering (KBQA) aims at answering factoid questions from a knowledge base (KB). It has attracted much attention in recent years (Bordes et al., 2014; Xu et al., 2016; Yu et al., 2017; Liang et al., 2017; Hu et al., 2018; Petrochuk and Zettlemoyer, 2018). Early work on KBQA focused on simple questions containing a single relation (Yih et al., 2014; Bordes et al., 2015; Dong et al., 2015; Hao et al., 2017). However, real questions are often more complex and recently some studies looked into complex KBQA. Two different types of complexity have been studied: (1) Single-relation questions with constraints. For example, in the question “Who was the first president of the U.S.?” there is a single relation “president of” between the answer entity and the entity “U.S.,” but we also have the constraint “first” that needs to be satisfied. For this type of complex questions, a staged query graph generation method has been proposed, which first identifies a single-hop relation path and"
2020.acl-main.91,P17-1021,0,0.0146897,"ion method with more flexible ways to generate query graphs. Our experiments clearly show that our method achieves the state of the art on three benchmark KBQA datasets. 1 Introduction Knowledge base question answering (KBQA) aims at answering factoid questions from a knowledge base (KB). It has attracted much attention in recent years (Bordes et al., 2014; Xu et al., 2016; Yu et al., 2017; Liang et al., 2017; Hu et al., 2018; Petrochuk and Zettlemoyer, 2018). Early work on KBQA focused on simple questions containing a single relation (Yih et al., 2014; Bordes et al., 2015; Dong et al., 2015; Hao et al., 2017). However, real questions are often more complex and recently some studies looked into complex KBQA. Two different types of complexity have been studied: (1) Single-relation questions with constraints. For example, in the question “Who was the first president of the U.S.?” there is a single relation “president of” between the answer entity and the entity “U.S.,” but we also have the constraint “first” that needs to be satisfied. For this type of complex questions, a staged query graph generation method has been proposed, which first identifies a single-hop relation path and then adds constrain"
2020.acl-main.91,D18-1234,0,0.0250022,"Missing"
2020.acl-main.91,D18-1242,0,0.810156,"tudies looked into complex KBQA. Two different types of complexity have been studied: (1) Single-relation questions with constraints. For example, in the question “Who was the first president of the U.S.?” there is a single relation “president of” between the answer entity and the entity “U.S.,” but we also have the constraint “first” that needs to be satisfied. For this type of complex questions, a staged query graph generation method has been proposed, which first identifies a single-hop relation path and then adds constraints to it to form a query graph (Yih et al., 2015; Bao et al., 2016; Luo et al., 2018). (2) Questions with multiple hops of relations. For example, Jing Jiang Singapore Management University jingjiang@smu.edu.sg for the question “Who is the wife of the founder of Facebook?” the answer is related to “Facebook” through two hops of relations, namely, “wife of” and “founder of.” To answer this type of multihop questions, we need to consider longer relation paths in order to reach the correct answers. The main challenge here is how to restrict the search space, i.e., to reduce the number of multi-hop relation paths to be considered, because the search space grows exponentially with"
2020.acl-main.91,D19-1242,0,0.395877,"Missing"
2020.acl-main.91,N18-1059,0,0.0826724,"Missing"
2020.acl-main.91,P16-1220,0,0.0123392,"paper, we handle both types of complexity at the same time. Motivated by the observation that early incorporation of constraints into query graphs can more effectively prune the search space, we propose a modified staged query graph generation method with more flexible ways to generate query graphs. Our experiments clearly show that our method achieves the state of the art on three benchmark KBQA datasets. 1 Introduction Knowledge base question answering (KBQA) aims at answering factoid questions from a knowledge base (KB). It has attracted much attention in recent years (Bordes et al., 2014; Xu et al., 2016; Yu et al., 2017; Liang et al., 2017; Hu et al., 2018; Petrochuk and Zettlemoyer, 2018). Early work on KBQA focused on simple questions containing a single relation (Yih et al., 2014; Bordes et al., 2015; Dong et al., 2015; Hao et al., 2017). However, real questions are often more complex and recently some studies looked into complex KBQA. Two different types of complexity have been studied: (1) Single-relation questions with constraints. For example, in the question “Who was the first president of the U.S.?” there is a single relation “president of” between the answer entity and the entity “"
2020.acl-main.91,P15-1128,0,0.449189,"ten more complex and recently some studies looked into complex KBQA. Two different types of complexity have been studied: (1) Single-relation questions with constraints. For example, in the question “Who was the first president of the U.S.?” there is a single relation “president of” between the answer entity and the entity “U.S.,” but we also have the constraint “first” that needs to be satisfied. For this type of complex questions, a staged query graph generation method has been proposed, which first identifies a single-hop relation path and then adds constraints to it to form a query graph (Yih et al., 2015; Bao et al., 2016; Luo et al., 2018). (2) Questions with multiple hops of relations. For example, Jing Jiang Singapore Management University jingjiang@smu.edu.sg for the question “Who is the wife of the founder of Facebook?” the answer is related to “Facebook” through two hops of relations, namely, “wife of” and “founder of.” To answer this type of multihop questions, we need to consider longer relation paths in order to reach the correct answers. The main challenge here is how to restrict the search space, i.e., to reduce the number of multi-hop relation paths to be considered, because the s"
2020.acl-main.91,P14-2105,0,0.0342413,"ch space, we propose a modified staged query graph generation method with more flexible ways to generate query graphs. Our experiments clearly show that our method achieves the state of the art on three benchmark KBQA datasets. 1 Introduction Knowledge base question answering (KBQA) aims at answering factoid questions from a knowledge base (KB). It has attracted much attention in recent years (Bordes et al., 2014; Xu et al., 2016; Yu et al., 2017; Liang et al., 2017; Hu et al., 2018; Petrochuk and Zettlemoyer, 2018). Early work on KBQA focused on simple questions containing a single relation (Yih et al., 2014; Bordes et al., 2015; Dong et al., 2015; Hao et al., 2017). However, real questions are often more complex and recently some studies looked into complex KBQA. Two different types of complexity have been studied: (1) Single-relation questions with constraints. For example, in the question “Who was the first president of the U.S.?” there is a single relation “president of” between the answer entity and the entity “U.S.,” but we also have the constraint “first” that needs to be satisfied. For this type of complex questions, a staged query graph generation method has been proposed, which first id"
2020.coling-main.113,D15-1075,0,0.0225352,"reading comprehension datasets like CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (CBT) (Hill et al., 2015) and RACE (Lai et al., 2017). These datasets have inspired the design of various neural-based models (Hermann et al., 2015; Chen et al., 2016) and some become benchmarks for machine reading comprehension. The dataset ChID used in this paper is also a large scale cloze-style dataset but focuses on Chinese idiom prediction. 2.2 Pre-trained Language Models Language model pre-training has been proven to be effective over a list of natural language tasks at both sentence level (Bowman et al., 2015) and token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016). Existing strategies of using pre-trained language models include feature-based methods like ELMO (Peters et al., 2018) and fine-tuning methods such as OpenAI GPT (Radford et al., 2018) and BERT (Devlin et al., 2019). BERT-based fine-tuning strategy and its extensions (Cui et al., 2019; Yang et al., 2019; Liu et al., 2019a) are pushing performance of neural models to near-human or super-human level. In this paper, we use pre-trained Chinese BERT with Whole Word Masking (Cui et al., 2019) as text sequence processor."
2020.coling-main.113,P16-1223,0,0.107394,"ormation to make correct predictions. Some case studies show that indeed our method makes use of more global contextual information to make predictions. 1313 2 2.1 Related Work Cloze-style Reading Comprehension Cloze-style reading comprehension is an important form in assessing machine reading abilities. Researchers created many large-scale cloze-style reading comprehension datasets like CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (CBT) (Hill et al., 2015) and RACE (Lai et al., 2017). These datasets have inspired the design of various neural-based models (Hermann et al., 2015; Chen et al., 2016) and some become benchmarks for machine reading comprehension. The dataset ChID used in this paper is also a large scale cloze-style dataset but focuses on Chinese idiom prediction. 2.2 Pre-trained Language Models Language model pre-training has been proven to be effective over a list of natural language tasks at both sentence level (Bowman et al., 2015) and token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016). Existing strategies of using pre-trained language models include feature-based methods like ELMO (Peters et al., 2018) and fine-tuning methods such as OpenAI GPT (R"
2020.coling-main.113,N19-1423,0,0.178669,"machine reading comprehension. The dataset ChID used in this paper is also a large scale cloze-style dataset but focuses on Chinese idiom prediction. 2.2 Pre-trained Language Models Language model pre-training has been proven to be effective over a list of natural language tasks at both sentence level (Bowman et al., 2015) and token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016). Existing strategies of using pre-trained language models include feature-based methods like ELMO (Peters et al., 2018) and fine-tuning methods such as OpenAI GPT (Radford et al., 2018) and BERT (Devlin et al., 2019). BERT-based fine-tuning strategy and its extensions (Cui et al., 2019; Yang et al., 2019; Liu et al., 2019a) are pushing performance of neural models to near-human or super-human level. In this paper, we use pre-trained Chinese BERT with Whole Word Masking (Cui et al., 2019) as text sequence processor. 2.3 Modelling Figurative Language Figurative (or non-literal) language is different from literal language where words or characters in literal language act in accordance with conventionally accepted meanings or denotation. In figurative language, meaning can be detached from the words or charac"
2020.coling-main.113,P16-1018,0,0.0612561,"Missing"
2020.coling-main.113,W18-0516,0,0.0811773,"y interpreted as “it must be not easy” in modern Chinese. However, the idiom is constructed from grammars and word senses of ancient Chinese. Its idiomatic meaning is “once decided, never change”, which is not even close to the literal meaning. As a result, the usage of Chinese idioms poses a challenge on language understanding not only for humans but also for artificial intelligence. Due to their pervasive usage, Chinese idiom prediction is an important task in Chinese language understanding. There have been several studies focusing on representing Chinese idioms using neural network models (Jiang et al., 2018; Liu et al., 2019b), but they were limited by the amount of data available for training. Recently, Zheng et al. (2019) released a large-scale Chinese IDiom Dataset (ChID) to facilitate machine comprehension of Chinese idioms. The ChID dataset contains more than 500K passages and 600K blanks, making it possible for researchers to train deep neural network models. The dataset is in cloze test style that target Chinese idioms in passages are replaced by blanks. For each blank, a set of candidate Chinese idioms is provided and the task is to pick the correct one based on the context. Table 1 This"
2020.coling-main.113,D17-1082,0,0.0198596,"ct further analysis using a gradient-based attribution method to check if our model can indeed capture global information to make correct predictions. Some case studies show that indeed our method makes use of more global contextual information to make predictions. 1313 2 2.1 Related Work Cloze-style Reading Comprehension Cloze-style reading comprehension is an important form in assessing machine reading abilities. Researchers created many large-scale cloze-style reading comprehension datasets like CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (CBT) (Hill et al., 2015) and RACE (Lai et al., 2017). These datasets have inspired the design of various neural-based models (Hermann et al., 2015; Chen et al., 2016) and some become benchmarks for machine reading comprehension. The dataset ChID used in this paper is also a large scale cloze-style dataset but focuses on Chinese idiom prediction. 2.2 Pre-trained Language Models Language model pre-training has been proven to be effective over a list of natural language tasks at both sentence level (Bowman et al., 2015) and token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016). Existing strategies of using pre-trained language"
2020.coling-main.113,D17-1124,0,0.0144773,"delling Figurative Language Figurative (or non-literal) language is different from literal language where words or characters in literal language act in accordance with conventionally accepted meanings or denotation. In figurative language, meaning can be detached from the words or characters while a more complicated meaning or heightened effect is reattached. As a special type of figurative language, idioms have been actively researched in tasks like Idiom Identification (Muzny and Zettlemoyer, 2013), Idiom Recommendation (Liu et al., 2019b) and Idiom Representation (Guti´errez et al., 2016; Liu et al., 2017; Jiang et al., 2018; Zheng et al., 2019). In this paper, we will focus on the representations of Chinese idioms using a BERT-based approach. 3 3.1 Method Task Definition and Dataset We formally define the Chinese idiom prediction task as follows. Given a passage P , represented as a sequence of tokens (p1 , p2 , . . . , pn ), where each token is either a Chinese character or the special “blank” token [MASK], and a set of K candidate Chinese idioms denoted as A = {a1 , a2 , . . . , aK }, our goal is to select an idiom a∗ ∈ A that best fits the blank in P . See the example in Table 1. We assume"
2020.coling-main.113,P19-1552,0,0.141385,"must be not easy” in modern Chinese. However, the idiom is constructed from grammars and word senses of ancient Chinese. Its idiomatic meaning is “once decided, never change”, which is not even close to the literal meaning. As a result, the usage of Chinese idioms poses a challenge on language understanding not only for humans but also for artificial intelligence. Due to their pervasive usage, Chinese idiom prediction is an important task in Chinese language understanding. There have been several studies focusing on representing Chinese idioms using neural network models (Jiang et al., 2018; Liu et al., 2019b), but they were limited by the amount of data available for training. Recently, Zheng et al. (2019) released a large-scale Chinese IDiom Dataset (ChID) to facilitate machine comprehension of Chinese idioms. The ChID dataset contains more than 500K passages and 600K blanks, making it possible for researchers to train deep neural network models. The dataset is in cloze test style that target Chinese idioms in passages are replaced by blanks. For each blank, a set of candidate Chinese idioms is provided and the task is to pick the correct one based on the context. Table 1 This work is licensed"
2020.coling-main.113,D13-1145,0,0.0194644,"this paper, we use pre-trained Chinese BERT with Whole Word Masking (Cui et al., 2019) as text sequence processor. 2.3 Modelling Figurative Language Figurative (or non-literal) language is different from literal language where words or characters in literal language act in accordance with conventionally accepted meanings or denotation. In figurative language, meaning can be detached from the words or characters while a more complicated meaning or heightened effect is reattached. As a special type of figurative language, idioms have been actively researched in tasks like Idiom Identification (Muzny and Zettlemoyer, 2013), Idiom Recommendation (Liu et al., 2019b) and Idiom Representation (Guti´errez et al., 2016; Liu et al., 2017; Jiang et al., 2018; Zheng et al., 2019). In this paper, we will focus on the representations of Chinese idioms using a BERT-based approach. 3 3.1 Method Task Definition and Dataset We formally define the Chinese idiom prediction task as follows. Given a passage P , represented as a sequence of tokens (p1 , p2 , . . . , pn ), where each token is either a Chinese character or the special “blank” token [MASK], and a set of K candidate Chinese idioms denoted as A = {a1 , a2 , . . . , aK"
2020.coling-main.113,N18-1202,0,0.0381596,"ious neural-based models (Hermann et al., 2015; Chen et al., 2016) and some become benchmarks for machine reading comprehension. The dataset ChID used in this paper is also a large scale cloze-style dataset but focuses on Chinese idiom prediction. 2.2 Pre-trained Language Models Language model pre-training has been proven to be effective over a list of natural language tasks at both sentence level (Bowman et al., 2015) and token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016). Existing strategies of using pre-trained language models include feature-based methods like ELMO (Peters et al., 2018) and fine-tuning methods such as OpenAI GPT (Radford et al., 2018) and BERT (Devlin et al., 2019). BERT-based fine-tuning strategy and its extensions (Cui et al., 2019; Yang et al., 2019; Liu et al., 2019a) are pushing performance of neural models to near-human or super-human level. In this paper, we use pre-trained Chinese BERT with Whole Word Masking (Cui et al., 2019) as text sequence processor. 2.3 Modelling Figurative Language Figurative (or non-literal) language is different from literal language where words or characters in literal language act in accordance with conventionally accepted"
2020.coling-main.113,radev-etal-2002-evaluating,0,0.0110454,"eng et al., 2019) 71.8 72.7 71.7 - 71.5 72.4 71.5 - 80.7 82.0 80.0 - 65.6 66.2 64.9 - 61.5 62.9 61.7 - BL-CharSeq BL-IdmEmb (w/o EC) BL-IdmEmb 79.33 73.59 80.24 0.017 0.433 79.42 73.31 79.76 0.017 0.429 88.84 81.05 91.87 0.017 0.429 72.93 68.13 71.93 0.017 0.429 73.11 63.82 72.17 0.012 0.332 Ours-CP Ours-Full (CP+DE) 82.03 82.58 0.436 0.450 81.86 82.40 0.434 0.447 92.46 92.73 0.434 0.447 74.71 75.02 0.434 0.447 74.82 75.73 0.328 0.354 Table 3: The experiment results on ChID. We only compute MRR for methods that have idiom embeddings. setting, we use Mean Reciprocal Rank (MRR) (Voorhees, 1999; Radev et al., 2002), a well-established metric for ranking problems, as the evaluation metric. Other Settings: We use pre-trained BERT for Chinese with Whole Word Masking (WWM) (Cui et al., 2019)2 . To reduce computational cost, we choose 128 as the maximum length for the input sequence, and we truncate passages longer than this limit by keeping only the 128 characters surrounding [MASK], with [MASK] in the middle. We use 4 Nvidia 1080Ti GPU cards and a batch size of 10 per card with a total 5 training epochs. The initial learning rate is set to 5e−5 with 1000 warm-up steps. We use the optimizer AdamW in accorda"
2020.coling-main.113,D16-1264,0,0.0393334,"Children’s Book Test (CBT) (Hill et al., 2015) and RACE (Lai et al., 2017). These datasets have inspired the design of various neural-based models (Hermann et al., 2015; Chen et al., 2016) and some become benchmarks for machine reading comprehension. The dataset ChID used in this paper is also a large scale cloze-style dataset but focuses on Chinese idiom prediction. 2.2 Pre-trained Language Models Language model pre-training has been proven to be effective over a list of natural language tasks at both sentence level (Bowman et al., 2015) and token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016). Existing strategies of using pre-trained language models include feature-based methods like ELMO (Peters et al., 2018) and fine-tuning methods such as OpenAI GPT (Radford et al., 2018) and BERT (Devlin et al., 2019). BERT-based fine-tuning strategy and its extensions (Cui et al., 2019; Yang et al., 2019; Liu et al., 2019a) are pushing performance of neural models to near-human or super-human level. In this paper, we use pre-trained Chinese BERT with Whole Word Masking (Cui et al., 2019) as text sequence processor. 2.3 Modelling Figurative Language Figurative (or non-literal) language is diff"
2020.coling-main.113,W10-3703,0,0.0291452,"ontext pooling. We further propose to use two separate idiom embeddings for the two kinds of matching. Experiments on a recently released Chinese idiom cloze test dataset show that our proposed method performs better than the existing state of the art. Ablation experiments also show that both context pooling and dual embedding contribute to the improvement of performance. 1 Introduction In this paper, we study Chinese idiom prediction, a language understanding problem that has not been extensively explored before in computational linguistics. Chinese idioms, mainly Chengyu (成语) (set phrases) (Wang and Yu, 2010; Wang, 2019), have fixed forms in structure; the component characters (mostly four) cannot be changed. Chinese idioms are characterized by rich contents, concise forms and frequent use (Wang, 2019) with properties of structural regularity, semantic fusion, and functional integrity (Shao, 2018; Wang, 2019). Chinese idioms are commonly used in both written and spoken Chinese, and understanding Chinese idioms is important for learning Chinese as a second language. The meaning of each Chinese idiom may not be literally understood through the composition of its characters, especially for those whi"
2020.coling-main.113,P19-1075,0,0.104547,"ses of ancient Chinese. Its idiomatic meaning is “once decided, never change”, which is not even close to the literal meaning. As a result, the usage of Chinese idioms poses a challenge on language understanding not only for humans but also for artificial intelligence. Due to their pervasive usage, Chinese idiom prediction is an important task in Chinese language understanding. There have been several studies focusing on representing Chinese idioms using neural network models (Jiang et al., 2018; Liu et al., 2019b), but they were limited by the amount of data available for training. Recently, Zheng et al. (2019) released a large-scale Chinese IDiom Dataset (ChID) to facilitate machine comprehension of Chinese idioms. The ChID dataset contains more than 500K passages and 600K blanks, making it possible for researchers to train deep neural network models. The dataset is in cloze test style that target Chinese idioms in passages are replaced by blanks. For each blank, a set of candidate Chinese idioms is provided and the task is to pick the correct one based on the context. Table 1 This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/."
2020.coling-main.113,P16-2034,0,0.024364,"Table 2. Methods Compared: We compare the following different methods. Performance of the first three baselines are directly taken from (Zheng et al., 2019). It is worth noting that the three baselines use BiLSTM as their backbones while our methods use BERT (Transformer) as our backbones. Although BiLSTM with attention can also capture the global contextual information in the passages, our experiments below will show that empirically our BERT-based methods are more effective. Language Model (LM): This method is based on standard bidirectional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997; Zhou et al., 2016). It uses BiLSTM to encode the given passage and obtain the hidden state of the blank. Then it compares the blank state with the embedding vector of each candidate idiom to choose the best idiom. Attentive Reader (AR): This method also uses BiLSTM but augments it with attention mechanism. It is based on the Attentive Reader model by (Hermann et al., 2015). Standard Attentive Reader (SAR): This is an altered version of Attentive Reader, where attention weights are computed using a bilinear matrix (Chen et al., 2016). BL-CharSeq: This is the first BERT baseline treating idioms as character seque"
2020.coling-main.145,D18-1245,0,0.0587233,"ocess for a more accurate bag-level representation. In the experiments on the popular benchmark dataset NYT, the proposed CoRA improves the prior state-of-the-art performance by a large margin in terms of Precision@N, AUC and Hits@K. Further analyses verify its superior capability in handling long-tail relations in contrast to the competitors. 1 Introduction Relation extraction, as a fundamental task in natural language processing (NLP), aims to discriminate the relation between two given entities in a plain text. As recent data-driven algorithms, e.g., deep neural networks (Han et al., 2018; Du et al., 2018; Ye and Ling, 2019), have shown their capabilities in tackling NLP tasks, labor-intensive annotation and scarce training data become the major obstacles of achieving promising performance on relation extraction. Instead of costly manual labeling, distant supervision method (Mintz et al., 2009) is proposed to autolabel the training data for relation extraction. It labels a sentence containing a pair of entities with the relation between them in a knowledge graph, e.g. Freebase (Bollacker et al., 2008). A strong assumption behind this is that a sentence containing two entities only expresses th"
2020.coling-main.145,P05-1053,0,0.183885,"ttention could be invalid when sibling relations have totally distinct meanings, and posts negative effects on relation extraction. For example, /people/person/children and /people/person/profession refer to opposite meanings. 3) Since a sentence embedding is augmented by multiple semantically-related relation embeddings, relation ambiguity problem deteriorates to post errors. For example, it is hard to distinguish /people/deceased person/place of death and /people/deceased person/place of burial. 4 Related Work Relation Extraction. Supervised relation extraction models (Zelenko et al., 2003; GuoDong et al., 2005) require large amounts of annotated data, which is time consuming and labor intensive. To obtain a large amount of labeled data, Mintz et al. (2009) propose distant supervision method to automatically annotate data. However, it inevitably leads to the wrong labeling problem due to the strong assumption. To reduce the effect of wrong labeling problem, multi-instance learning paradigm (Riedel et al., 2010; Hoffmann et al., 2011) is proposed. To introduce the merits of deep learning into relation extraction, Zeng et 1661 al. (2014; 2015) specifically design the position embedding and piecewise co"
2020.coling-main.145,D18-1247,0,0.217585,"ationaugmenting process for a more accurate bag-level representation. In the experiments on the popular benchmark dataset NYT, the proposed CoRA improves the prior state-of-the-art performance by a large margin in terms of Precision@N, AUC and Hits@K. Further analyses verify its superior capability in handling long-tail relations in contrast to the competitors. 1 Introduction Relation extraction, as a fundamental task in natural language processing (NLP), aims to discriminate the relation between two given entities in a plain text. As recent data-driven algorithms, e.g., deep neural networks (Han et al., 2018; Du et al., 2018; Ye and Ling, 2019), have shown their capabilities in tackling NLP tasks, labor-intensive annotation and scarce training data become the major obstacles of achieving promising performance on relation extraction. Instead of costly manual labeling, distant supervision method (Mintz et al., 2009) is proposed to autolabel the training data for relation extraction. It labels a sentence containing a pair of entities with the relation between them in a knowledge graph, e.g. Freebase (Bollacker et al., 2008). A strong assumption behind this is that a sentence containing two entities"
2020.coling-main.145,P11-1055,0,0.0406088,"ceased person/place of death and /people/deceased person/place of burial. 4 Related Work Relation Extraction. Supervised relation extraction models (Zelenko et al., 2003; GuoDong et al., 2005) require large amounts of annotated data, which is time consuming and labor intensive. To obtain a large amount of labeled data, Mintz et al. (2009) propose distant supervision method to automatically annotate data. However, it inevitably leads to the wrong labeling problem due to the strong assumption. To reduce the effect of wrong labeling problem, multi-instance learning paradigm (Riedel et al., 2010; Hoffmann et al., 2011) is proposed. To introduce the merits of deep learning into relation extraction, Zeng et 1661 al. (2014; 2015) specifically design the position embedding and piecewise convolutional neural network to better extract the features of each sentence. To further alleviate the effect of wrong labeling problem, Lin et al. (2016) propose the selective attention framework under multi-instance learning paradigm. Recently, many works (Liu et al., 2016; Du et al., 2018; Li et al., 2020) are built upon the selective attention (Lin et al., 2016) framework to handle wrong labeling problem in distant supervisi"
2020.coling-main.145,D19-1395,0,0.0367765,"ecewise convolutional neural network to better extract the features of each sentence. To further alleviate the effect of wrong labeling problem, Lin et al. (2016) propose the selective attention framework under multi-instance learning paradigm. Recently, many works (Liu et al., 2016; Du et al., 2018; Li et al., 2020) are built upon the selective attention (Lin et al., 2016) framework to handle wrong labeling problem in distant supervision relation extraction. For example, Ye and Ling (2019) propose bag-level selective attention to share training information among the bags with the same label. Hu et al. (2019) propose a multi-layer attention-based model with joint label embedding. Li et al. (2020) propose to replace the attention with a gate mechanism especially for one-sentence bags. Hierarchical Relation Extraction. More related to our work, to alleviate long-tail problem posted by distant supervision, it is natural to utilize relation hierarchies for knowledge transfer crossing relations. There are two existing works falling into this paradigm. Besides using the embedding of fine-grained relation as a query of selective attention, Han et al. (2018) also use embeddings of coarse-grained relations"
2020.coling-main.145,D14-1181,0,0.00312326,"X = [x1 , . . . , xn ] ∈ Rdx ×n is the resulting sequence of word embeddings specially for relation extraction. Piecewise Convolutional Neural Network. As a common practice in distantly supervised relation extraction, piecewise convolutional neural network (PCNN) (Zeng et al., 2015) is used to generate contextualized representations over an input sequence of word embeddings. Compared to the typical 1D-CNN with max-pooling (Zeng et al., 2014), piecewise max-pooling has the capability to capture the structure information between two entities by considering their positions. Specifically, 1D-CNN (Kim, 2014) is first invoked over the input sequence for contextualized representations. Then a piecewise max-pooling performs over the output sequence to obtain sentence-level embedding. These steps are written as H = [h1 , . . . , hn ] = 1D-CNN(X; W (c) , b(c) ) ∈ Rdc ×n , (4) s = tanh([Pool(H (1) ); Pool(H (2) ); Pool(H (3) )]), (5) where W (c) ∈ Rdc ×Q×dx is a conv kernel with window size of Q. H (1) , H (2) and H (3) are three consecutive parts of H, obtained by dividing H w.r.t. indices of head e(h) and tail e(t) entities. Consequently, s ∈ Rdh , where dh = 3dc , is the resulting sentence-level rep"
2020.coling-main.145,P16-1200,0,0.124114,"sive annotation and scarce training data become the major obstacles of achieving promising performance on relation extraction. Instead of costly manual labeling, distant supervision method (Mintz et al., 2009) is proposed to autolabel the training data for relation extraction. It labels a sentence containing a pair of entities with the relation between them in a knowledge graph, e.g. Freebase (Bollacker et al., 2008). A strong assumption behind this is that a sentence containing two entities only expresses the relation existing in the knowledge graph, but this assumption will not always hold (Lin et al., 2016). Hence, it leads to two main problems regrading the training data. First, when the assumption is invalid, wrong labeling problem appears and degrades algorithms by introducing noisy supervision signals. This problem has been well-studied by recent works (Lin et al., 2016; Han et al., 2018; Ji et al., 2017; Li et al., 2020) operating at bag level for multi-instance learning (Hoffmann et al., 2011), where the “bag” denotes a set of sentences with the same entity pair. Most of them use selective attention to avoid wrongly-labeled sentences. Second, the long-tail problem is caused by using a know"
2020.coling-main.145,P09-1113,0,0.87082,"ng long-tail relations in contrast to the competitors. 1 Introduction Relation extraction, as a fundamental task in natural language processing (NLP), aims to discriminate the relation between two given entities in a plain text. As recent data-driven algorithms, e.g., deep neural networks (Han et al., 2018; Du et al., 2018; Ye and Ling, 2019), have shown their capabilities in tackling NLP tasks, labor-intensive annotation and scarce training data become the major obstacles of achieving promising performance on relation extraction. Instead of costly manual labeling, distant supervision method (Mintz et al., 2009) is proposed to autolabel the training data for relation extraction. It labels a sentence containing a pair of entities with the relation between them in a knowledge graph, e.g. Freebase (Bollacker et al., 2008). A strong assumption behind this is that a sentence containing two entities only expresses the relation existing in the knowledge graph, but this assumption will not always hold (Lin et al., 2016). Hence, it leads to two main problems regrading the training data. First, when the assumption is invalid, wrong labeling problem appears and degrades algorithms by introducing noisy supervisi"
2020.coling-main.145,N19-1288,0,0.269139,"accurate bag-level representation. In the experiments on the popular benchmark dataset NYT, the proposed CoRA improves the prior state-of-the-art performance by a large margin in terms of Precision@N, AUC and Hits@K. Further analyses verify its superior capability in handling long-tail relations in contrast to the competitors. 1 Introduction Relation extraction, as a fundamental task in natural language processing (NLP), aims to discriminate the relation between two given entities in a plain text. As recent data-driven algorithms, e.g., deep neural networks (Han et al., 2018; Du et al., 2018; Ye and Ling, 2019), have shown their capabilities in tackling NLP tasks, labor-intensive annotation and scarce training data become the major obstacles of achieving promising performance on relation extraction. Instead of costly manual labeling, distant supervision method (Mintz et al., 2009) is proposed to autolabel the training data for relation extraction. It labels a sentence containing a pair of entities with the relation between them in a knowledge graph, e.g. Freebase (Bollacker et al., 2008). A strong assumption behind this is that a sentence containing two entities only expresses the relation existing"
2020.coling-main.145,C14-1220,0,0.249615,"2020). The integration of them has been proven crucial and effective to relation extraction by previous work (Li et al., 2020). In the following, we omit the index of a sentence, j, for a clear elaboration. Basically, a sentence s is first tokenized into a sequence of n words, s = [w1 , . . . , wn ], then a word2vec method (Mikolov et al., 2013) is used to transform the discrete tokens into low-dimensional, real-valued vector embeddings, i.e., V = [v1 , . . . , vn ] ∈ Rdw ×n . Word Embedding. On the one hand, position-aware embedding offers rich positional information for downstream modules (Zeng et al., 2014). For i-th word, the relative position is represented as the distances from the word to head e(h) and tail e(t) entities respectively. Two integer distances are then (ph) (pt) transformed into low-dimensional vectors, xi and xi ∈ Rdp , by a learnable weight matrix. Conse(p) (p) quently, a sequence of position-aware embeddings is denoted as X (p) = [x1 , . . . , xn ] ∈ R(dw +2dp )×n (p) (ph) (pt) where xi = [vi , xi ; xi ]. [; ] denotes the operation of vector concatenation. On the other hand, entity-aware embedding is also crucial since the goal of relation extraction is to discriminate the re"
2020.coling-main.145,D15-1203,0,0.326319,"is to predict the relation label rˆ(0) of an entity pair based on the corresponding sentence bag when the pair is not included in the knowledge graph. As following the hierarchical setting (Han et al., 2018; Zhang et al., 2019), labels of coarse-grained relations, [r(1) , . . . , r(M ) ], can be used to share knowledge across relations. 2.2 Sentence-Level Representation To embed each sentence sj in a bag B = {s1 , . . . , sm } into latent semantic space, we derive a sentence representation from three kinds of features, including word embedding (Mikolov et al., 2013), 1655 position embedding (Zeng et al., 2015) and entity embedding (Li et al., 2020). The integration of them has been proven crucial and effective to relation extraction by previous work (Li et al., 2020). In the following, we omit the index of a sentence, j, for a clear elaboration. Basically, a sentence s is first tokenized into a sequence of n words, s = [w1 , . . . , wn ], then a word2vec method (Mikolov et al., 2013) is used to transform the discrete tokens into low-dimensional, real-valued vector embeddings, i.e., V = [v1 , . . . , vn ] ∈ Rdw ×n . Word Embedding. On the one hand, position-aware embedding offers rich positional inf"
2020.coling-main.145,N19-1306,0,0.451074,"sually stored in the relation hierarchies of a knowledge graph, e.g., Freebase in Figure 1 (middle). Specifically, these approaches extend selective attention (Lin et al., 2016) by introducing the embeddings of high-level (i.e., coarse-grained) relations as complements to original low-level (i.e., fine-grained) relations. As such, the high-level relation embeddings are used as queries of selective attention to derive extra bag-level features. To learn the relation embeddings, Han et al. (2018) randomly initialize them followed by supervised learning in end-to-end fashion, whereas Zhang et al. (2019) combine the embeddings from both TransE (Bordes et al., 2013) pre-trained on Freebase and graph convolutional network (Defferrard et al., 2016) applied to the relation’s hierarchies. Despite proven to improve overall and long-tail performance, they also post two issues: 1) Limited by selective attention framework, the relation embeddings are only used as the attention’s queries and thus not well-exploited to share knowledge. 2) Despite the capability in mitigating the long-tail problem, graph embeddings pre-trained on large-scale knowledge graph are time-consuming and not always offthe-shelte"
2020.coling-main.48,D19-1522,0,0.0604235,"-adversarial sampling and type-constraint training technique (i.e., Local-cognitive w/o self-adv loss) contribute to performance 563 Negative Sampling Method Uniform Self-adversarial (Sun et al., 2019) Local-cognitive w/o self-adv loss Local-cognitive (ours) MR 3180 3114 3094 2860 MRR .471 .480 .479 .488 WN18RR Hits@10 .564 .576 .577 .590 @3 .478 .481 .489 .506 @1 .428 .433 .434 .441 MR 224 177 180 172 MRR .320 .339 .340 .344 FB15k-237 Hits@10 .525 .536 .538 .541 @3 .374 .375 .374 .382 @1 .220 .244 .241 .261 Table 8: Performance of RatE with different negative sampling methods. Method TuckER (Balazevic et al., 2019) RatE (ours) MR 2860 MRR .470 .488 WN18RR Hits@10 .526 .590 @3 .482 .506 @1 .443 .441 MR 172 MRR .358 .344 FB15k-237 Hits@10 @3 .544 .394 .541 .382 @1 .266 .261 #θ d2e dr 8|R| Table 9: Performance comparison between TuckER and RatE on WN18RR/FB15k-237. “#θ” denotes the number of learnable parameters only for scoring, where de and dr are the embedding sizes of entity and relation respectively. improvement. The results also emphasize the effectiveness of our proposed local-cognitive negative sampling method, a non-trivial integration of the both above, in structured knowledge learning. 3.7 Analy"
2020.coling-main.48,N18-1133,0,0.0781879,"dding size by one or two orders of magnitude. Moreover, besides handling the four relation patterns (i.e., symmetry, antisymmetry, inversion and composition), the proposed RatE also reduces the effect of embedding ambiguity (detailed at the end of this section). It is also noteworthy that although the integration above is based on RotatE, the proposed weighted product is compatible with any complex or hypercomplex embedding approach (e.g., QuatE). 2.4 Negative Sampling and Optimization The way to conduct negative sampling can significantly affect the performance of a graph embedding approach (Cai and Wang, 2018; Sun et al., 2019) because contrasting a challenging negative sample against the corresponding positive one is more effective for learning structured knowledge. Formally, given an arbitrary correct triple x = (h, r, t) ∈ G (tr) , negative sampling aims at corrupting its either head or tail entity to get a wrong triple x0 = (h0 , r, t) or (h, r, t0 ), where x0 ∈ / G (tr) . G (tr) denotes the knowledge graph to train an embedding model. Note, we only exhibit tail corruption for a clear elaboration in the following, and head corruption is also considered in our implementation. We first introduce"
2020.coling-main.48,P17-1021,0,0.0131356,"y RatE achieves state-of-the-art performance on four link prediction benchmarks. 1 Introduction A knowledge graph refers to a collection of interlinked entities, which is usually formatted as a set of triples. A triple is represented as a head entity linked to a tail entity by a relation, which is written as (head, relation, tail) or (h, r, t). Large-scale knowledge graphs, such as Freebase (Bollacker et al., 2008) and WordNet (Miller, 1995), containing structured information, have been leveraged to support a broad spectrum of natural language processing (NLP) tasks, e.g., question answering (Hao et al., 2017), recommender system (Zhang et al., 2016), relation extraction (Min et al., 2013), etc. Nonetheless, the human-curated, real-world knowledge graphs often suffer from incompleteness or sparseness problem (Toutanova et al., 2015), which inevitably hurts the performance of downstream tasks. Hence, how to auto-complete knowledge graphs becomes a popular problem in both research and industry communities. For this purpose, many light-weight graph embedding approaches (Bordes et al., 2013; Yang et al., 2015; Sun et al., 2019) have been proposed. Unlike costly graph neural networks (GNNs) (Schlichtkru"
2020.coling-main.48,P15-1067,0,0.0373462,"graph embedding approaches (Nickel et al., 2011; Dettmers et al., 2017; Balazevic et al., 2019; Zhang et al., 2019) require additional overheads to score a triple, this work is in line with trans-based graph embedding approaches that employ an efficient translation function defined in a latent space. TransE (Bordes et al., 2013) is the most representative trans-based approach, which embeds entities/relations in real vector space and utilizes the relations as translations. It optimizes score function towards “h+r = t”. Several recent trans-based approaches (Wang et al., 2014; Lin et al., 2015; Ji et al., 2015; Ebisu and Ichise, 2018) can be viewed as extensions of TransE. More recently, RotatE (Sun et al., 2019), as a state-of-the-art trans-based approach, represents the entities/relations in complex vector space and formulates the translating process as a rotation in complex space. Also related to this work, many negative sampling methods (Cai and Wang, 2018; Sun et al., 2019) are proposed to effectively learn structured knowledge. KBGAN (Cai and Wang, 2018) uses knowledge graph embedding model as a negative sample generator to fool the main embedding model (i.e., the discriminator in GANs). In c"
2020.coling-main.48,N13-1095,0,0.021679,"1 Introduction A knowledge graph refers to a collection of interlinked entities, which is usually formatted as a set of triples. A triple is represented as a head entity linked to a tail entity by a relation, which is written as (head, relation, tail) or (h, r, t). Large-scale knowledge graphs, such as Freebase (Bollacker et al., 2008) and WordNet (Miller, 1995), containing structured information, have been leveraged to support a broad spectrum of natural language processing (NLP) tasks, e.g., question answering (Hao et al., 2017), recommender system (Zhang et al., 2016), relation extraction (Min et al., 2013), etc. Nonetheless, the human-curated, real-world knowledge graphs often suffer from incompleteness or sparseness problem (Toutanova et al., 2015), which inevitably hurts the performance of downstream tasks. Hence, how to auto-complete knowledge graphs becomes a popular problem in both research and industry communities. For this purpose, many light-weight graph embedding approaches (Bordes et al., 2013; Yang et al., 2015; Sun et al., 2019) have been proposed. Unlike costly graph neural networks (GNNs) (Schlichtkrull et al., 2018), these approaches use low-dimensional embeddings to represent th"
2020.coling-main.48,D15-1174,0,0.0310067,"ation with projective transformation and changes the distance between the tail entities according to spatial positions of the head entities. Consequently, besides increasing the distance between the tail entities to disambiguate entity embeddings, RatE could also decrease the distance for better support of many-to-one relations. A rigorous proof of these properties is provided in Appendix A. 3 Experiments 3.1 Experimental Setting Dataset. We employ four widely-used link prediction benchmark, WN18, FB15K, WN18RR and FB15K-237, whose statistics are summarized in Table 2. Note, Toutanova et al., (2015) find that both WN18 and FB15K suffer from direct link problem caused by most test triples (e1 , r1 , e2 ) can be found in the training or valid set with another relation, e.g., (e1 , r2 , e2 ) or (e2 , r2 , e1 ). • WN18 (Bordes et al., 2013) is extracted from WordNet (Miller, 1995), a knowledge graph composed of English phrases and lexical relations between them. • FB15k (Bordes et al., 2013) is extracted from Freebase (Bollacker et al., 2008), which is a largescale knowledge graph consisting real-world named entities and their relationships. • FB15k-237 (Toutanova et al., 2015) is a subset o"
2020.emnlp-main.108,aker-etal-2017-simple,0,0.0127898,"two benchmarks, respectively. 2 Related Work Stance Classification: Although stance classification has been well studied in different contexts such as online forums (Hasan and Ng, 2013; Lukasik et al., 2016; Ferreira and Vlachos, 2016; Mohammad et al., 2016), a recent trend is to study stance classification towards rumors in different social media platforms (Mendoza et al., 2010; Qazvinian et al., 2011). These studies can be roughly categorized into two groups. One line of work aims to design different features to capture the sequential property of conversation threads (Zubiaga et al., 2016; Aker et al., 2017; Pamungkas et al., 2018; Zubiaga et al., 2018b; Giasemidis et al., 2018). Another line of work attempts to apply recent deep learning models to automatically capture effective stance features (Kochkina et al., 2017; Veyseh et al., 2017). Our work extends the latter line of work by proposing a hierarchical Transformer based on the recent pre-trained BERT for this task. Moreover, we notice that our BERT-based hierarchical Transformer is similar to the model proposed in (Pappagari et al., 2019), but we want to point out that our model design in the input and output layers is specific to stance c"
2020.emnlp-main.108,N19-1423,0,0.492668,"ions with separate stance-specific layers and rumor-specific layers in the high level. Although these MTL approaches have shown the usefulness of stance signals to rumor verification, they still suffer from the following shortcomings: (1) The first obstacle lies in their single-task models for SC or RV, whose randomly initialized text encoders such as LSTM tend to overfit existing small annotated corpora. With the recent trend of pre-training, many pre-trained text encoders such as BERT have been shown to overcome the overfitting problem and achieve significant improvements in many NLP tasks (Devlin et al., 2019). However, unlike previous sentence-level tasks, our SC and RV tasks require the language understanding over conversation threads in social media. Since BERT is unable to process arbitrarily long sequences due to its maximum length constraint in the pre-training stage, it remains an open question how to extend BERT to our SC and RV tasks. (2) Another important limitation of previous studies lies in their multi-task learning framework. First, the MTL2 framework used in existing methods fails to explicitly model the inter-task interactions between the stance-specific and rumor-specific layers. S"
2020.emnlp-main.108,N16-1138,0,0.0387081,"Missing"
2020.emnlp-main.108,I13-1191,0,0.0711848,"Missing"
2020.emnlp-main.108,2020.acl-main.623,0,0.0105306,"hin each subthread, followed by modeling the global interactions among all the posts in the whole thread. social media is naturally labor-intensive and timeconsuming, it is crucial to develop an automatic rumor verification approach to mitigate their harmful effect. Rumor verification is typically defined as a task of determining whether the source claim in a conversation thread is false rumor, true rumor, or unverified rumor (Zubiaga et al., 2018a). In the literature, much work has been done for rumor verification (Liu et al., 2015; Ma et al., 2016; Ruchansky et al., 2017; Chen et al., 2018; Kochkina and Liakata, 2020). Among them, one appealing line of work focuses on exploiting stance signals to enhance rumor verification (Zubiaga et al., 2016), since it is observed that people’s stances in reply posts usually provide important clues to rumor verification (e.g., in Fig. 1, if the source claim is denied or queried by most replies, it is highly probable that the source claim contains misinformation and is false rumor). This line of work has attracted increasing attention in recent years. A number of multi-task learning (MTL) methods have been proposed to jointly perform stance classification (SC) and rumor"
2020.emnlp-main.108,S17-2083,0,0.0207529,"rreira and Vlachos, 2016; Mohammad et al., 2016), a recent trend is to study stance classification towards rumors in different social media platforms (Mendoza et al., 2010; Qazvinian et al., 2011). These studies can be roughly categorized into two groups. One line of work aims to design different features to capture the sequential property of conversation threads (Zubiaga et al., 2016; Aker et al., 2017; Pamungkas et al., 2018; Zubiaga et al., 2018b; Giasemidis et al., 2018). Another line of work attempts to apply recent deep learning models to automatically capture effective stance features (Kochkina et al., 2017; Veyseh et al., 2017). Our work extends the latter line of work by proposing a hierarchical Transformer based on the recent pre-trained BERT for this task. Moreover, we notice that our BERT-based hierarchical Transformer is similar to the model proposed in (Pappagari et al., 2019), but we want to point out that our model design in the input and output layers is specific to stance classification, which is different from their work. Rumor Verification: Due to the negative impact 1393 of various rumors spreading on social media, rumor verification has attracted increasing attention in recent yea"
2020.emnlp-main.108,C18-1288,0,0.107905,"number of multi-task learning (MTL) methods have been proposed to jointly perform stance classification (SC) and rumor verification (RV) over conversation threads, including Sequential LSTM-based methods (Li et al., 2019), Tree LSTM-based methods (Kumar and Carley, 1392 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1392–1401, c November 16–20, 2020. 2020 Association for Computational Linguistics 2019), and Graph Convolutional Network-based methods (Wei et al., 2019). These MTL approaches are mainly constructed upon the MTL2 framework proposed in Kochkina et al. (2018), which aims to first learn shared representations with shared layers in the low level, followed by learning task-specific representations with separate stance-specific layers and rumor-specific layers in the high level. Although these MTL approaches have shown the usefulness of stance signals to rumor verification, they still suffer from the following shortcomings: (1) The first obstacle lies in their single-task models for SC or RV, whose randomly initialized text encoders such as LSTM tend to overfit existing small annotated corpora. With the recent trend of pre-training, many pre-trained t"
2020.emnlp-main.108,P19-1498,0,0.0179053,"erent from these studies, the goal in this paper is to leverage stance classification to improve rumor verification with a multi-task learning architecture. Stance-Aware Rumor Verification: The recent advance in rumor verification is to exploit stance information to enhance rumor verification with different multi-task learning approaches. Specifically, Ma et al. (2018a) and Kochkina et al. (2018) respectively proposed two multi-task learning architectures to jointly optimize stance classification and rumor verification based on two different variants of RNN, i.e., GRU and LSTM. More recently, Kumar and Carley (2019) proposed another multi-task LSTM model based on tree structures for stanceaware rumor verification. Our work bears the same intuition to these previous studies, and aims to explore the potential of the pre-trained BERT to this multi-task learning task. 3 Methodology In this section, we first formulate the task of stance classification (SC) and rumor verification (RV). We then describe our single-task model for SC and RV, followed by introducing our multi-task learning framework for stance-aware rumor verification. 3.1 Task Formulation Given a Twitter corpus, let us first use D = {C1 , C2 , ."
2020.emnlp-main.108,P19-1113,0,0.0831316,"ce rumor verification (Zubiaga et al., 2016), since it is observed that people’s stances in reply posts usually provide important clues to rumor verification (e.g., in Fig. 1, if the source claim is denied or queried by most replies, it is highly probable that the source claim contains misinformation and is false rumor). This line of work has attracted increasing attention in recent years. A number of multi-task learning (MTL) methods have been proposed to jointly perform stance classification (SC) and rumor verification (RV) over conversation threads, including Sequential LSTM-based methods (Li et al., 2019), Tree LSTM-based methods (Kumar and Carley, 1392 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1392–1401, c November 16–20, 2020. 2020 Association for Computational Linguistics 2019), and Graph Convolutional Network-based methods (Wei et al., 2019). These MTL approaches are mainly constructed upon the MTL2 framework proposed in Kochkina et al. (2018), which aims to first learn shared representations with shared layers in the low level, followed by learning task-specific representations with separate stance-specific layers and rumor-specific laye"
2020.emnlp-main.108,P16-2064,0,0.0258956,"Missing"
2020.emnlp-main.108,P17-1066,0,0.0129743,"act 1393 of various rumors spreading on social media, rumor verification has attracted increasing attention in recent years. Existing approaches to single-task rumor verification generally belong to two groups. The first line of work focuses on either employing a myriad of hand-crafted features (Qazvinian et al., 2011; Yang et al., 2012; Kwon et al., 2013; Ma et al., 2015) including post contents, user profiles, information credibility features (Castillo et al., 2011), and propagation patterns, or resorting to various kinds of kernels to model the event propagation structure (Wu et al., 2015; Ma et al., 2017). The second line of work applies variants of several neural network models to automatically capture important features among all the propagated posts (Ma et al., 2016; Ruchansky et al., 2017; Chen et al., 2018). Different from these studies, the goal in this paper is to leverage stance classification to improve rumor verification with a multi-task learning architecture. Stance-Aware Rumor Verification: The recent advance in rumor verification is to exploit stance information to enhance rumor verification with different multi-task learning approaches. Specifically, Ma et al. (2018a) and Kochki"
2020.emnlp-main.108,P18-1184,0,0.33239,"al., 2015; Ma et al., 2017). The second line of work applies variants of several neural network models to automatically capture important features among all the propagated posts (Ma et al., 2016; Ruchansky et al., 2017; Chen et al., 2018). Different from these studies, the goal in this paper is to leverage stance classification to improve rumor verification with a multi-task learning architecture. Stance-Aware Rumor Verification: The recent advance in rumor verification is to exploit stance information to enhance rumor verification with different multi-task learning approaches. Specifically, Ma et al. (2018a) and Kochkina et al. (2018) respectively proposed two multi-task learning architectures to jointly optimize stance classification and rumor verification based on two different variants of RNN, i.e., GRU and LSTM. More recently, Kumar and Carley (2019) proposed another multi-task LSTM model based on tree structures for stanceaware rumor verification. Our work bears the same intuition to these previous studies, and aims to explore the potential of the pre-trained BERT to this multi-task learning task. 3 Methodology In this section, we first formulate the task of stance classification (SC) and"
2020.emnlp-main.108,S16-1003,0,0.166972,"Missing"
2020.emnlp-main.108,D19-1485,0,0.162175,"ation and is false rumor). This line of work has attracted increasing attention in recent years. A number of multi-task learning (MTL) methods have been proposed to jointly perform stance classification (SC) and rumor verification (RV) over conversation threads, including Sequential LSTM-based methods (Li et al., 2019), Tree LSTM-based methods (Kumar and Carley, 1392 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1392–1401, c November 16–20, 2020. 2020 Association for Computational Linguistics 2019), and Graph Convolutional Network-based methods (Wei et al., 2019). These MTL approaches are mainly constructed upon the MTL2 framework proposed in Kochkina et al. (2018), which aims to first learn shared representations with shared layers in the low level, followed by learning task-specific representations with separate stance-specific layers and rumor-specific layers in the high level. Although these MTL approaches have shown the usefulness of stance signals to rumor verification, they still suffer from the following shortcomings: (1) The first obstacle lies in their single-task models for SC or RV, whose randomly initialized text encoders such as LSTM ten"
2020.emnlp-main.108,N16-1174,0,0.0453078,"ent years have witnessed a profound revolution in social media, as many individuals gradually turn to different social platforms to share the latest news and voice personal opinions. Meanwhile, the flourish of social media also enables rapid dissemination of unverified information (i.e., rumors) on a massive scale, which may cause serious harm to our society (e.g., impacting presidential election decisions (Allcott and Gentzkow, 2017)). Since manually checking a sheer quantity of rumors on 1 Veracity Label: False Rumor Note that the concept of hierarchy in this paper is different from that in Yang et al. (2016), as we use hierarchy to refer to a neural structure that first models the local interactions among posts within each subthread, followed by modeling the global interactions among all the posts in the whole thread. social media is naturally labor-intensive and timeconsuming, it is crucial to develop an automatic rumor verification approach to mitigate their harmful effect. Rumor verification is typically defined as a task of determining whether the source claim in a conversation thread is false rumor, true rumor, or unverified rumor (Zubiaga et al., 2018a). In the literature, much work has bee"
2020.emnlp-main.108,D11-1147,0,0.0594817,"that our BERT-based hierarchical Transformer is similar to the model proposed in (Pappagari et al., 2019), but we want to point out that our model design in the input and output layers is specific to stance classification, which is different from their work. Rumor Verification: Due to the negative impact 1393 of various rumors spreading on social media, rumor verification has attracted increasing attention in recent years. Existing approaches to single-task rumor verification generally belong to two groups. The first line of work focuses on either employing a myriad of hand-crafted features (Qazvinian et al., 2011; Yang et al., 2012; Kwon et al., 2013; Ma et al., 2015) including post contents, user profiles, information credibility features (Castillo et al., 2011), and propagation patterns, or resorting to various kinds of kernels to model the event propagation structure (Wu et al., 2015; Ma et al., 2017). The second line of work applies variants of several neural network models to automatically capture important features among all the propagated posts (Ma et al., 2016; Ruchansky et al., 2017; Chen et al., 2018). Different from these studies, the goal in this paper is to leverage stance classification"
2020.emnlp-main.30,D15-1075,0,0.203337,"Missing"
2020.emnlp-main.30,P19-1259,0,0.0263533,"Missing"
2020.emnlp-main.30,N16-1162,0,0.0575373,"et al. (2016) proposes Long Short-Term Memory-Networks (LSTMN) for inferring the relation between sentences, and Lin et al. (2017) combines LSTM and self-attention mechanism to improve sentence embeddings. Multi-task learning (Subramanian et al., 2018; Cer et al., 2018) has also been applied for training better sentence embeddings. Recently, in additional to supervised learning, models pre-trained with unsupervised methods begin to dominate the field. Pre-training Several methods have been proposed to directly pre-train sentence embedding, such as Skip-thought (Kiros et al., 2015), FastSent (Hill et al., 2016), and Inverse Cloze Task (Lee et al., 2019). Although these methods can obtain better sentence embeddings in an unsupervised way, they cannot achieve state-of-the-art performance in downstream tasks even with further finetuning. More recently, Peters et al. (2018) proposes to pre-train LSTM with language modeling (LM) task, and Radford et al. (2018) pre-trains Transformer also with LM. Instead of sequentially generating words in a single direction, Devlin et al. (2019) proposes the masked language modeling task to pre-train bidirectional Transformer. Most recently, Guu et al. (2020); Lewis et"
2020.emnlp-main.30,D18-2029,0,0.0337848,"ings. Huang et al. (2013) proposes deep structured semantic encoders for web search. Tan et al. (2015) uses LSTM as the encoder for non-factoid answer selection, and Tai et al. (2015) proposes treeLSTM to compute semantic relatedness between sentences. Mou et al. (2016) also uses tree-based CNN as the encoder for textual entailment tasks. Cheng et al. (2016) proposes Long Short-Term Memory-Networks (LSTMN) for inferring the relation between sentences, and Lin et al. (2017) combines LSTM and self-attention mechanism to improve sentence embeddings. Multi-task learning (Subramanian et al., 2018; Cer et al., 2018) has also been applied for training better sentence embeddings. Recently, in additional to supervised learning, models pre-trained with unsupervised methods begin to dominate the field. Pre-training Several methods have been proposed to directly pre-train sentence embedding, such as Skip-thought (Kiros et al., 2015), FastSent (Hill et al., 2016), and Inverse Cloze Task (Lee et al., 2019). Although these methods can obtain better sentence embeddings in an unsupervised way, they cannot achieve state-of-the-art performance in downstream tasks even with further finetuning. More recently, Peters et"
2020.emnlp-main.30,P19-1612,0,0.143375,"y accelerate inference speed. For example, when used in question answering (QA), it can significantly shorten inference time with all the embeddings of candidate paragraphs pre-cached into memory and only matched with the question embedding during inference. There have been several models specifically designed to pre-train sentence encoders with largescale unlabeled corpus. For example, Skipthought (Kiros et al., 2015) uses encoded sentence 1 Our code will be released at https://github.com/ shuohangwang/Cross-Thought. embeddings to generate the next sentence (Figure 2(a)). Inverse Cloze Task (Lee et al., 2019) defines some pseudo labels to pre-train a sentence encoder (Figure 2(b)). However, pseudo labels may bear low accuracy, and rich linguistic information that can be well learned in generic language modeling is often lost in these unsupervised methods. In this paper, we propose a novel unsupervised approach that fully exploits the strength of language modeling for sentence encoder pre-training. Popular pre-training tasks such as language modeling (Peters et al., 2018; Radford et al., 2018), masked language modeling (Devlin et al., 2019; Liu et al., 2019) and sequence generation (Dong et al., 20"
2020.emnlp-main.30,D16-1053,0,0.015148,"d to downstream tasks (e.g., in QA tasks, similarity scores between question and candidate answers can be ranked by their respective sentence embeddings). 413 2 Related Work Sequence Encoder Many studies have explored different ways to improve sequence embeddings. Huang et al. (2013) proposes deep structured semantic encoders for web search. Tan et al. (2015) uses LSTM as the encoder for non-factoid answer selection, and Tai et al. (2015) proposes treeLSTM to compute semantic relatedness between sentences. Mou et al. (2016) also uses tree-based CNN as the encoder for textual entailment tasks. Cheng et al. (2016) proposes Long Short-Term Memory-Networks (LSTMN) for inferring the relation between sentences, and Lin et al. (2017) combines LSTM and self-attention mechanism to improve sentence embeddings. Multi-task learning (Subramanian et al., 2018; Cer et al., 2018) has also been applied for training better sentence embeddings. Recently, in additional to supervised learning, models pre-trained with unsupervised methods begin to dominate the field. Pre-training Several methods have been proposed to directly pre-train sentence embedding, such as Skip-thought (Kiros et al., 2015), FastSent (Hill et al., 2"
2020.emnlp-main.30,N19-1423,0,0.583802,"generate the next sentence (Figure 2(a)). Inverse Cloze Task (Lee et al., 2019) defines some pseudo labels to pre-train a sentence encoder (Figure 2(b)). However, pseudo labels may bear low accuracy, and rich linguistic information that can be well learned in generic language modeling is often lost in these unsupervised methods. In this paper, we propose a novel unsupervised approach that fully exploits the strength of language modeling for sentence encoder pre-training. Popular pre-training tasks such as language modeling (Peters et al., 2018; Radford et al., 2018), masked language modeling (Devlin et al., 2019; Liu et al., 2019) and sequence generation (Dong et al., 2019; Lewis et al., 2019) are not directly applicable to sentence encoder training, because only the hidden state of the first token (a special token) (Reimers and Gurevych, 2019; Devlin et al., 2019) can be used as the sentence embedding, but no loss or gradient is specifically designed for the first special token, which renders sentence embeddings learned in such settings contain limited useful information. Another limitation in existing masked language modeling methods (Devlin et al., 2019; Liu et al., 2019) is that they focus on lon"
2020.emnlp-main.30,2020.acl-main.703,0,0.0748172,"Missing"
2020.emnlp-main.30,2021.ccl-1.108,0,0.213811,"Missing"
2020.emnlp-main.30,P16-2022,0,0.0210474,"raining, the attention weights in the cross-sequence Transformers can be directly applied to downstream tasks (e.g., in QA tasks, similarity scores between question and candidate answers can be ranked by their respective sentence embeddings). 413 2 Related Work Sequence Encoder Many studies have explored different ways to improve sequence embeddings. Huang et al. (2013) proposes deep structured semantic encoders for web search. Tan et al. (2015) uses LSTM as the encoder for non-factoid answer selection, and Tai et al. (2015) proposes treeLSTM to compute semantic relatedness between sentences. Mou et al. (2016) also uses tree-based CNN as the encoder for textual entailment tasks. Cheng et al. (2016) proposes Long Short-Term Memory-Networks (LSTMN) for inferring the relation between sentences, and Lin et al. (2017) combines LSTM and self-attention mechanism to improve sentence embeddings. Multi-task learning (Subramanian et al., 2018; Cer et al., 2018) has also been applied for training better sentence embeddings. Recently, in additional to supervised learning, models pre-trained with unsupervised methods begin to dominate the field. Pre-training Several methods have been proposed to directly pre-tra"
2020.emnlp-main.30,D19-1258,0,0.0544261,"Missing"
2020.emnlp-main.30,N18-1202,0,0.294889,"ased at https://github.com/ shuohangwang/Cross-Thought. embeddings to generate the next sentence (Figure 2(a)). Inverse Cloze Task (Lee et al., 2019) defines some pseudo labels to pre-train a sentence encoder (Figure 2(b)). However, pseudo labels may bear low accuracy, and rich linguistic information that can be well learned in generic language modeling is often lost in these unsupervised methods. In this paper, we propose a novel unsupervised approach that fully exploits the strength of language modeling for sentence encoder pre-training. Popular pre-training tasks such as language modeling (Peters et al., 2018; Radford et al., 2018), masked language modeling (Devlin et al., 2019; Liu et al., 2019) and sequence generation (Dong et al., 2019; Lewis et al., 2019) are not directly applicable to sentence encoder training, because only the hidden state of the first token (a special token) (Reimers and Gurevych, 2019; Devlin et al., 2019) can be used as the sentence embedding, but no loss or gradient is specifically designed for the first special token, which renders sentence embeddings learned in such settings contain limited useful information. Another limitation in existing masked language modeling met"
2020.emnlp-main.30,D19-1410,0,0.120872,"ords. Experiments on question answering and textual entailment tasks demonstrate that our pre-trained encoder can outperform state-of-the-art encoders trained with continuous sentence signals as well as traditional masked language modeling baselines. Our proposed approach also achieves new state of the art on HotpotQA (full-wiki setting) by improving intermediate information retrieval performance.1 1 Figure 1: Example of short sequences that can leverage each other for pre-training sentence encoder. Introduction Encoding sentences into embeddings (Kiros et al., 2015; Subramanian et al., 2018; Reimers and Gurevych, 2019) is a critical step in many Natural Language Processing (NLP) tasks. The benefit of using sentence embeddings is that the representations of all the encoded sentences can be reused on a chunk level (compared to word-level embeddings), which can significantly accelerate inference speed. For example, when used in question answering (QA), it can significantly shorten inference time with all the embeddings of candidate paragraphs pre-cached into memory and only matched with the question embedding during inference. There have been several models specifically designed to pre-train sentence encoders"
2020.emnlp-main.30,P15-1150,0,0.0408522,"ntly, to retrieve relevant sequence embeddings for masked words prediction. After pre-training, the attention weights in the cross-sequence Transformers can be directly applied to downstream tasks (e.g., in QA tasks, similarity scores between question and candidate answers can be ranked by their respective sentence embeddings). 413 2 Related Work Sequence Encoder Many studies have explored different ways to improve sequence embeddings. Huang et al. (2013) proposes deep structured semantic encoders for web search. Tan et al. (2015) uses LSTM as the encoder for non-factoid answer selection, and Tai et al. (2015) proposes treeLSTM to compute semantic relatedness between sentences. Mou et al. (2016) also uses tree-based CNN as the encoder for textual entailment tasks. Cheng et al. (2016) proposes Long Short-Term Memory-Networks (LSTMN) for inferring the relation between sentences, and Lin et al. (2017) combines LSTM and self-attention mechanism to improve sentence embeddings. Multi-task learning (Subramanian et al., 2018; Cer et al., 2018) has also been applied for training better sentence embeddings. Recently, in additional to supervised learning, models pre-trained with unsupervised methods begin to"
2020.emnlp-main.30,W18-5446,0,0.0520957,"Missing"
2020.emnlp-main.30,N18-1101,0,0.0924233,"Missing"
2020.emnlp-main.30,D18-1259,0,0.12361,"Missing"
2021.acl-long.255,D13-1160,0,0.156954,"Missing"
2021.acl-long.255,D14-1067,0,0.0582279,"Missing"
2021.acl-long.255,P19-1082,0,0.0191438,"et al., 2017; Reddy et al., 2019) or modeling the flow of the conversation along with a passage (Huang et al., 2019; Gao et al., 2019, 2020). Our work also intends to capture the flow of the conversation but we specifically model the transitions of focal entities. Regarding conversational KBQA, Saha et al. (2018) proposed a model consisting of a hierarchical encoder, a key-value memory network and a decoder. Guo et al. (2018) and Shen et al. (2019) employed a seq2seq model to encode the conversation history then output a sequence of actions to form an executable command. Some follow-up work (Guo et al., 2019; Shen et al., 2020) focused on the meta-learning setting or the effective search strategy under weak supervision, which is beyond the focus of this paper. Christmann et al. (2019) detected frontier nodes by expanding a subgraph, which are potential answer entities to the current question. 3295 Their motivation is relevant to ours but we target at modeling the focal entities in the conversation. 6 Michel Galley. 2018. A knowledge-grounded neural conversation model. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 5110– 5117. Conclusion In this paper, we present a method"
2021.acl-long.255,2020.acl-main.91,1,0.78023,"es of questions also lead to incorrectness of the final answers, because if the entity linker links the question to a wrong entity, it is unlikely to answer the question correctly. This is a general challenge for KBQA. 5 Related Work Single-turn KBQA task has been studied for decades. Traditional methods tried to retrieve the correct answers from the KB via either embeddingbased methods (Bordes et al., 2014; Xu et al., 2019; Sun et al., 2018, 2019; Qiu et al., 2020; He et al., 2021) or semantic parsing-based methods (Berant et al., 2013; Yih et al., 2015; Luo et al., 2018; Zhang et al., 2019; Lan and Jiang, 2020). Conversational KBQA is a relatively new direction that builds on top of single-turn KBQA. 8 Since the original D2A and MaSP codes leverage the ground truth topic entities and relations to pre-train the entity linker and relation predictor but we do not, we skip the pretraining procedure in our re-implementation. Conversational KBQA is related to dialogue systems and conversational QA in general, which require techniques to sequentially generate responses based on the interactions with users (Ghazvininejad et al., 2018; Rajendran et al., 2018; Das et al., 2017). A conversation history can be"
2021.acl-long.255,P19-1480,0,0.0188693,"ain the entity linker and relation predictor but we do not, we skip the pretraining procedure in our re-implementation. Conversational KBQA is related to dialogue systems and conversational QA in general, which require techniques to sequentially generate responses based on the interactions with users (Ghazvininejad et al., 2018; Rajendran et al., 2018; Das et al., 2017). A conversation history can be encoded via different techniques such as a hierarchical neural network (Serban et al., 2017; Reddy et al., 2019) or modeling the flow of the conversation along with a passage (Huang et al., 2019; Gao et al., 2019, 2020). Our work also intends to capture the flow of the conversation but we specifically model the transitions of focal entities. Regarding conversational KBQA, Saha et al. (2018) proposed a model consisting of a hierarchical encoder, a key-value memory network and a decoder. Guo et al. (2018) and Shen et al. (2019) employed a seq2seq model to encode the conversation history then output a sequence of actions to form an executable command. Some follow-up work (Guo et al., 2019; Shen et al., 2020) focused on the meta-learning setting or the effective search strategy under weak supervision, whi"
2021.acl-long.255,2020.acl-main.88,0,0.0742073,"Missing"
2021.acl-long.255,D17-1018,0,0.0662095,"Missing"
2021.acl-long.255,D18-1242,0,0.118719,"ned, such as the focal entities we introduced earlier. Starting from these entities, the Query Generator generates a set of candidate query graphs (Yih et al., 2016) from K, which lead to some candidate answers to the question. The second component of a single-turn KBQA system, the Answer Predictor, is a neural-network-based ranker that takes in the question as well as the generated query graphs as input and outputs a predicted answer a ˆ. For conversational KBQA, the initial question q0 in a conversation c can be answered directly using an existing single-turn KBQA approach (Yu et al., 2017; Luo et al., 2018; Yih et al., 2016; Lan et al., 2019). When the single-turn KBQA system is used for answering follow-up questions, we make the following modifications: First, we assume that a focal entity distribution (which is the core of our method and will be presented in detail below) is derived from the conversation history. Then each focal entity is considered relevant to the current question and will be used to generate candidate query graphs by the Query Generator. Meanwhile, the probabilities of these focal entities (i.e., their focal scores) will be used by the Answer Predictor when it ranks the can"
2021.acl-long.255,D18-1418,0,0.0283466,"et al., 2015; Luo et al., 2018; Zhang et al., 2019; Lan and Jiang, 2020). Conversational KBQA is a relatively new direction that builds on top of single-turn KBQA. 8 Since the original D2A and MaSP codes leverage the ground truth topic entities and relations to pre-train the entity linker and relation predictor but we do not, we skip the pretraining procedure in our re-implementation. Conversational KBQA is related to dialogue systems and conversational QA in general, which require techniques to sequentially generate responses based on the interactions with users (Ghazvininejad et al., 2018; Rajendran et al., 2018; Das et al., 2017). A conversation history can be encoded via different techniques such as a hierarchical neural network (Serban et al., 2017; Reddy et al., 2019) or modeling the flow of the conversation along with a passage (Huang et al., 2019; Gao et al., 2019, 2020). Our work also intends to capture the flow of the conversation but we specifically model the transitions of focal entities. Regarding conversational KBQA, Saha et al. (2018) proposed a model consisting of a hierarchical encoder, a key-value memory network and a decoder. Guo et al. (2018) and Shen et al. (2019) employed a seq2se"
2021.acl-long.255,Q19-1016,0,0.020837,"Since the original D2A and MaSP codes leverage the ground truth topic entities and relations to pre-train the entity linker and relation predictor but we do not, we skip the pretraining procedure in our re-implementation. Conversational KBQA is related to dialogue systems and conversational QA in general, which require techniques to sequentially generate responses based on the interactions with users (Ghazvininejad et al., 2018; Rajendran et al., 2018; Das et al., 2017). A conversation history can be encoded via different techniques such as a hierarchical neural network (Serban et al., 2017; Reddy et al., 2019) or modeling the flow of the conversation along with a passage (Huang et al., 2019; Gao et al., 2019, 2020). Our work also intends to capture the flow of the conversation but we specifically model the transitions of focal entities. Regarding conversational KBQA, Saha et al. (2018) proposed a model consisting of a hierarchical encoder, a key-value memory network and a decoder. Guo et al. (2018) and Shen et al. (2019) employed a seq2seq model to encode the conversation history then output a sequence of actions to form an executable command. Some follow-up work (Guo et al., 2019; Shen et al., 202"
2021.acl-long.255,D18-1455,0,0.0443582,"Missing"
2021.acl-long.255,N19-1301,0,0.0417314,"Missing"
2021.acl-long.255,P15-1128,0,0.0286177,"rrors caused by wrong identification of the topic entities of questions also lead to incorrectness of the final answers, because if the entity linker links the question to a wrong entity, it is unlikely to answer the question correctly. This is a general challenge for KBQA. 5 Related Work Single-turn KBQA task has been studied for decades. Traditional methods tried to retrieve the correct answers from the KB via either embeddingbased methods (Bordes et al., 2014; Xu et al., 2019; Sun et al., 2018, 2019; Qiu et al., 2020; He et al., 2021) or semantic parsing-based methods (Berant et al., 2013; Yih et al., 2015; Luo et al., 2018; Zhang et al., 2019; Lan and Jiang, 2020). Conversational KBQA is a relatively new direction that builds on top of single-turn KBQA. 8 Since the original D2A and MaSP codes leverage the ground truth topic entities and relations to pre-train the entity linker and relation predictor but we do not, we skip the pretraining procedure in our re-implementation. Conversational KBQA is related to dialogue systems and conversational QA in general, which require techniques to sequentially generate responses based on the interactions with users (Ghazvininejad et al., 2018; Rajendran et"
2021.acl-long.255,P16-2033,0,0.0199498,"ions (q1 , q2 , ..., qt−1 ) in the same conversation. 2.2 Pipeline for Single-turn KBQA A standard single-turn KBQA includes two main components: a Query Generator and an Answer Predictor. The Query Generator generates a set of candidate query graphs C for a given q. Specifically, we first assume that some entities relevant to q are first identified. These can be entities directly mentioned in q or other entities relevant to q but implicitly mentioned, such as the focal entities we introduced earlier. Starting from these entities, the Query Generator generates a set of candidate query graphs (Yih et al., 2016) from K, which lead to some candidate answers to the question. The second component of a single-turn KBQA system, the Answer Predictor, is a neural-network-based ranker that takes in the question as well as the generated query graphs as input and outputs a predicted answer a ˆ. For conversational KBQA, the initial question q0 in a conversation c can be answered directly using an existing single-turn KBQA approach (Yu et al., 2017; Luo et al., 2018; Yih et al., 2016; Lan et al., 2019). When the single-turn KBQA system is used for answering follow-up questions, we make the following modification"
2021.acl-long.255,P17-1053,0,0.0183576,"implicitly mentioned, such as the focal entities we introduced earlier. Starting from these entities, the Query Generator generates a set of candidate query graphs (Yih et al., 2016) from K, which lead to some candidate answers to the question. The second component of a single-turn KBQA system, the Answer Predictor, is a neural-network-based ranker that takes in the question as well as the generated query graphs as input and outputs a predicted answer a ˆ. For conversational KBQA, the initial question q0 in a conversation c can be answered directly using an existing single-turn KBQA approach (Yu et al., 2017; Luo et al., 2018; Yih et al., 2016; Lan et al., 2019). When the single-turn KBQA system is used for answering follow-up questions, we make the following modifications: First, we assume that a focal entity distribution (which is the core of our method and will be presented in detail below) is derived from the conversation history. Then each focal entity is considered relevant to the current question and will be used to generate candidate query graphs by the Query Generator. Meanwhile, the probabilities of these focal entities (i.e., their focal scores) will be used by the Answer Predictor whe"
2021.acl-long.255,P19-1440,0,0.0222283,"of the topic entities of questions also lead to incorrectness of the final answers, because if the entity linker links the question to a wrong entity, it is unlikely to answer the question correctly. This is a general challenge for KBQA. 5 Related Work Single-turn KBQA task has been studied for decades. Traditional methods tried to retrieve the correct answers from the KB via either embeddingbased methods (Bordes et al., 2014; Xu et al., 2019; Sun et al., 2018, 2019; Qiu et al., 2020; He et al., 2021) or semantic parsing-based methods (Berant et al., 2013; Yih et al., 2015; Luo et al., 2018; Zhang et al., 2019; Lan and Jiang, 2020). Conversational KBQA is a relatively new direction that builds on top of single-turn KBQA. 8 Since the original D2A and MaSP codes leverage the ground truth topic entities and relations to pre-train the entity linker and relation predictor but we do not, we skip the pretraining procedure in our re-implementation. Conversational KBQA is related to dialogue systems and conversational QA in general, which require techniques to sequentially generate responses based on the interactions with users (Ghazvininejad et al., 2018; Rajendran et al., 2018; Das et al., 2017). A conver"
2021.acl-long.255,D19-1248,0,0.201448,"h Dakota ??: What is the name of the author? The Great Gatsby ??: F. Scott Fitzgerald ??: What’s his first novel? F. Scott Fitzgerald R?: This Side of Paradise ??: Who was his child? F. Scott Fitzgerald R?: Frances Scott Fitzgerald Figure 1: An example conversation in conversational KBQA. The entities shown in blue are what we call the focal entities, which are implicit but important for answering the questions. Introduction Recently, conversational Knowledge Base Question Answering (KBQA) has started to attract people’s attention (Saha et al., 2018; Christmann et al., 2019; Guo et al., 2018; Shen et al., 2019). Motivated by real-world conversational applications, particularly personal assistants such as Apple Siri and Amazon Alexa, the task aims to answer questions over KBs in a conversational manner. Figure 1 shows an example of conversational KBQA. As we can see, the conversation can be roughly divided into two parts: Q1, Q2 and Q3 revolve around the book “The Great Gatsby,” while Q4 and Q5 revolve around its author, “F. Scott Fitzgerald”. Although these entities are not explicitly mentioned in the questions, they are implied by the conversation history, and they are critical for answering the qu"
2021.acl-long.255,D19-1242,0,0.0411005,"Missing"
2021.acl-long.48,D19-1607,0,0.0281088,"ntax; and 3) extensive experiments on three benchmarks demonstrate the effectiveness of our method for cross-lingual tasks. 2 Related Work Cross-lingual Transfer. Large-scale pre-trained language models (Devlin et al., 2019; Liu et al., 2019) have achieved sequential success in various natural language processing tasks. Recent studies (Lample and Conneau, 2019; Conneau et al., 2020a) extend the pre-trained language models to multilingual tasks and demonstrate their prominent capability on cross-lingual knowledge transfer, even under zero-shot scenario (Wu and Dredze, 2019; Pires et al., 2019; Hsu et al., 2019). Motivated by the success of multilingual language models on cross-lingual transfer, several works explore how these models work and what their bottleneck is. On the one hand, some studies ﬁnd that the shared sub-words (Wu and Dredze, 2019; Dufter and Sch¨utze, 2020) and the parameters of top layers (Conneau et al., 2020b) are crucial for cross-lingual transfer. On the other hand, the bottleneck is attributed to two issues: (i) catastrophic forgetting (Keung et al., 2020; Liu et al., 2020), where knowledge learned in the pre-training stage is forgotten in downstream ﬁne-tuning; (ii) lack of l"
2021.acl-long.48,2020.acl-main.654,1,0.826781,"Missing"
2021.acl-long.48,2020.acl-main.87,0,0.0275965,"(1) XMAML-one (Nooralahzadeh et al., 2020) borrows the idea from meta-learning. Speciﬁcally, XMAML-one utilizes an auxiliary language development data in training, e.g., using the development set of Spanish in training to assist German on MLQA. XMAML-One reports the results based on the most beneﬁcial auxiliary language. (2) STILT (Phang et al., 2020) augments intermediate task training before ﬁne-tuning on the target task, e.g., adding training of HellaSwag (Zellers et al., 2019) before training on the NLI task. STILT also reports results with the most beneﬁcial intermediate task. (3) LAKM (Yuan et al., 2020) ﬁrst mines knowledge phrases along with passages from the Web. Then these Web data are used to enhance the phrase boundaries through a masked language model objective. Note that LAKM is only evaluated on three languages of MLQA. On the one hand, we observe that COSY surpasses the compared SOTA methods over all evaluation metrics. Although meta-learning methods (Finn et al., 2017; Gu et al., 2018; Sun et al., 2019) advance the state-of-the-art performance for few-shot learning, our COSY still outperforms the meta-learning-based method, i.e., XMAML-One, with 1.1 percentage points in the few-sho"
2021.acl-long.48,P19-1472,0,0.0280583,". Comparison with the State of the Art. We ﬁrst outline the SOTA zero-shot (few-shot) crosslingual methods we compared with as follows: (1) XMAML-one (Nooralahzadeh et al., 2020) borrows the idea from meta-learning. Speciﬁcally, XMAML-one utilizes an auxiliary language development data in training, e.g., using the development set of Spanish in training to assist German on MLQA. XMAML-One reports the results based on the most beneﬁcial auxiliary language. (2) STILT (Phang et al., 2020) augments intermediate task training before ﬁne-tuning on the target task, e.g., adding training of HellaSwag (Zellers et al., 2019) before training on the NLI task. STILT also reports results with the most beneﬁcial intermediate task. (3) LAKM (Yuan et al., 2020) ﬁrst mines knowledge phrases along with passages from the Web. Then these Web data are used to enhance the phrase boundaries through a masked language model objective. Note that LAKM is only evaluated on three languages of MLQA. On the one hand, we observe that COSY surpasses the compared SOTA methods over all evaluation metrics. Although meta-learning methods (Finn et al., 2017; Gu et al., 2018; Sun et al., 2019) advance the state-of-the-art performance for few-"
2021.acl-long.48,P19-1336,1,0.88758,"Missing"
2021.acl-long.48,2020.emnlp-main.276,0,0.0162897,"teel, 2004). Recently, counterfactual reasoning has motivated studies in applications. In the community of computer vision, counterfactual analysis has been successfully applied in explanation (Goyal et al., 2019a,b), long-tailed classiﬁcation (Tang et al., 2020a), scene graph generation (Tang et al., 2020b), and visual question answering (Chen et al., 2020; Niu et al., 2020; Abbasnejad et al., 2020). In the community of natural language processing, counterfactual methods are also emerging recently in text classiﬁcation (Choi et al., 2020), story generation (Qin et al., 2019), dialog systems (Zhu et al., 2020), gender bias (Vig et al., 2020; Shin et al., 2020), question answering (Yu et al., 2020), and sentiment bias (Huang et al., 2020). To the best of our knowledge, we are the ﬁrst to conduct counterfactual analysis in cross-lingual understanding. Different from previous works (Zhu et al., 2020; Qin et al., 2019) that generate word-level or sentence-level counterfactual samples, our counterfactual analysis dives into syntax level that is more controllable than text and free from complex language generation module. 3 COSY: COunterfactual SYntax COSY aims to leverage the syntactic information, e.g."
2021.eacl-main.131,D19-1418,0,0.027756,"in (Kumar and Carley, 2019), we select five breaking events from PHEME and split them into two sets. Four events are chosen for training and in-domain testing, and the remaining one is used as out-of-domain testing set. 4.3 Baselines and Our Methods We consider a state-of-the-art model and some baselines that are also addressing cross-domain issues. StA-HiTPLAN: Replicating (Khoo et al., 2020), we train a hierarchical transformer model which is a state-of-the art model and can be viewed as a feature extractor in the following experiments. Ensemble-based model (EM): Following (He et al., 2019; Clark et al., 2019), we take topical words as bias features and introduce an auxiliary bias only model fb taking bias priori features as input. Then using this bias only model to train a robust model through an ensemble model. We firstly obtain the class distribution pb (y|x) using this biased model. Then we train an ensemble model that combines the former biased model with ˆ = a robust model through this function: p(y|x) T (p(y|x) + pb (y|x)). In the testing stage, only the robust model p(y|x) is used for prediction. Adversary-based model (AM): This is a common way to learn domain-invariant features. We impleme"
2021.eacl-main.131,D19-6115,0,0.0200616,"wing the setting in (Kumar and Carley, 2019), we select five breaking events from PHEME and split them into two sets. Four events are chosen for training and in-domain testing, and the remaining one is used as out-of-domain testing set. 4.3 Baselines and Our Methods We consider a state-of-the-art model and some baselines that are also addressing cross-domain issues. StA-HiTPLAN: Replicating (Khoo et al., 2020), we train a hierarchical transformer model which is a state-of-the art model and can be viewed as a feature extractor in the following experiments. Ensemble-based model (EM): Following (He et al., 2019; Clark et al., 2019), we take topical words as bias features and introduce an auxiliary bias only model fb taking bias priori features as input. Then using this bias only model to train a robust model through an ensemble model. We firstly obtain the class distribution pb (y|x) using this biased model. Then we train an ensemble model that combines the former biased model with ˆ = a robust model through this function: p(y|x) T (p(y|x) + pb (y|x)). In the testing stage, only the robust model p(y|x) is used for prediction. Adversary-based model (AM): This is a common way to learn domain-invariant"
2021.eacl-main.131,P19-1498,0,0.0167215,"t BERT (Devlin et al., 2018) did not improve results and was time-consuming, we apply GLOVE-300d (Pennington et al., 2014) to embed each token in a post. The initial learning rate was set as 0.01 with 0.3 dropout and we used the ADAM optimizer with 6000 warm start-up steps. Batch size is set as 256 for all cross-validation tasks. 4.2 Dataset We use the public PHEME dataset (Zubiaga et al., 2016) for our evaluation. PHEME was collected based on 9 breaking news stories and can be categorised into four classes: true rumor, false rumour, unverified rumour and non-rumour. Following the setting in (Kumar and Carley, 2019), we select five breaking events from PHEME and split them into two sets. Four events are chosen for training and in-domain testing, and the remaining one is used as out-of-domain testing set. 4.3 Baselines and Our Methods We consider a state-of-the-art model and some baselines that are also addressing cross-domain issues. StA-HiTPLAN: Replicating (Khoo et al., 2020), we train a hierarchical transformer model which is a state-of-the art model and can be viewed as a feature extractor in the following experiments. Ensemble-based model (EM): Following (He et al., 2019; Clark et al., 2019), we tak"
2021.eacl-main.131,P19-1244,0,0.103212,"his topic mixture is combined with the vector representation of the instance itself to make rumor predictions. Our experiments show that our proposed method can outperform two baseline debiasing methods in a cross-topic setting. In a synthetic setting when we removed topic-specific words, our method also works better than the baselines, showing that our method does not rely on superficial features. 1 Introduction Recently, there has been much interest in detecting online false information such as rumors and fake news. Existing work has explored different features including network structures (Ma et al., 2019a), propagation paths (Liu and Wu, 2018), user credibility (Castillo et al., 2011) and the fusion of heterogeneous data such as image and text (Wang et al., 2018). However, these proposed algorithms still cannot be easily deployed for real-world applications, and one of the key reasons is that, just like many other NLP problems, rumor or fake news detection models may easily overfit the training data and thus cannot perform well on new data. The ∗ Corresponding author problem can be more serious with deep learning solutions, because deep neural networks tend to learn superficial patterns that"
2021.eacl-main.131,D14-1162,0,0.0865678,"During test time, since our instance x does not have a t associated with it, we use a topic classification model trained on the training data where each example has its correct topic labeled to estimate p(t|x). 4 4.1 Experiments Implementation Details We follow the model architecture StA-PLAN in (Khoo et al., 2020) as our backbone. StA-PLAN is a hierarchical transformer which contains 12 post-level multi-head attention layer (MHA) and 2 token-level MHA layers. As claimed in (Khoo et al., 2020) that BERT (Devlin et al., 2018) did not improve results and was time-consuming, we apply GLOVE-300d (Pennington et al., 2014) to embed each token in a post. The initial learning rate was set as 0.01 with 0.3 dropout and we used the ADAM optimizer with 6000 warm start-up steps. Batch size is set as 256 for all cross-validation tasks. 4.2 Dataset We use the public PHEME dataset (Zubiaga et al., 2016) for our evaluation. PHEME was collected based on 9 breaking news stories and can be categorised into four classes: true rumor, false rumour, unverified rumour and non-rumour. Following the setting in (Kumar and Carley, 2019), we select five breaking events from PHEME and split them into two sets. Four events are chosen fo"
2021.findings-emnlp.350,D17-1088,0,0.0692608,"Missing"
2021.findings-emnlp.350,2020.tacl-1.13,0,0.0551051,"Missing"
2021.findings-emnlp.350,2020.emnlp-main.579,0,0.013485,"0.6 0.5 0.4 7 0.3 ??1 ??2 ??3 ??4 Conversation Flow ??5 ??≥6 Figure 6: Performance changes in conversation flow on the test set (zh). Related Work Math Word Problems. In the past few years, there has been a growing number of datasets (Wang et al., 2017a; Miao et al., 2020; Patel et al., 2021) and methods that have been proposed for MWPs, including statistical machine learning methods (Mitra and Baral, 2016; Roy and Roth, 2018), semantic parsing methods (Liang et al., 2018), and deep learning methods (Wang et al., 2017b, 2018b,a, 2019; Xie and Sun, 2019b; Zhang et al., 2020; Qin et al., 2020; Wu et al., 2020), emerging in the field of solving MWPs. tic similarities. Our model can achieve 49.21 GEM points and 69.83 DAGsim scores for the English test set and 59.32 GEM points and 79.56 DAGsim scores for the Chinese test set. It also suggests that there is still a lot of room for improvement in generating reliable reasoning graph. Analysis of Different Answer Types. As shown Question Answering Datasets. This work mainly in Table 6, we perform a break-down analysis of refers to conversational QA datasets (Reddy et al., different answer types to various methods on the Chinese test set. GTS obtains the b"
2021.findings-emnlp.350,D18-1259,0,0.208518,"conversation QA1:(t−1) and the next question Qt , the task is to return a textual and 2WikiMultiHopQA (Ho et al., 2020) solve answer At to the next question Qt and generate a this problem to some extent by introducing a set ˆ rt . Next we will introduce the reasoning graph G of triplets or reasoning path, which is not suitable detailed notations. in the scenario where the reasoning process is Textual Answer. Each sample in our dataset concomplicated. Taking the example in Figure 1 for 4148 Dataset Conversational Cross-lingual Mathematics Expression Evidence CoQA (Reddy et al., 2019) HotpotQA (Yang et al., 2018) R4 C (Inoue et al., 2020) 2WikiMultiHopQA (Ho et al., 2020) DropQA (Dua et al., 2019) Dream (Sun et al., 2019) MathQA (Amini et al., 2019) Math23K (Wang et al., 2017a) ✓ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✓ ✓ ✓ ✓ ✗ ✗ ✗ ✗ ✗ ✗ ✓ ✓ ✓ ✓ ✓ ✓ ✗ ✗ ✗ ✗ Rationale (text span) Evidence (set of supporting facts) Derivation (set of triplets) Reasoning Path (chain of triplets) Operation Programs - Explanation Annotations N OAHQA ✓ ✓ ✓ ✓ ✓ Reasoning Graph Table 1: Comparison of N OAHQA with existing datasets. One may argue that expression equals to evidence which is not always the case, e.g., both of “tw"
2021.findings-emnlp.350,2020.acl-main.362,1,0.760864,"&Edge RGNet w/o Pre&GCN NumNet+ 0.7 EM 0.6 0.5 0.4 7 0.3 ??1 ??2 ??3 ??4 Conversation Flow ??5 ??≥6 Figure 6: Performance changes in conversation flow on the test set (zh). Related Work Math Word Problems. In the past few years, there has been a growing number of datasets (Wang et al., 2017a; Miao et al., 2020; Patel et al., 2021) and methods that have been proposed for MWPs, including statistical machine learning methods (Mitra and Baral, 2016; Roy and Roth, 2018), semantic parsing methods (Liang et al., 2018), and deep learning methods (Wang et al., 2017b, 2018b,a, 2019; Xie and Sun, 2019b; Zhang et al., 2020; Qin et al., 2020; Wu et al., 2020), emerging in the field of solving MWPs. tic similarities. Our model can achieve 49.21 GEM points and 69.83 DAGsim scores for the English test set and 59.32 GEM points and 79.56 DAGsim scores for the Chinese test set. It also suggests that there is still a lot of room for improvement in generating reliable reasoning graph. Analysis of Different Answer Types. As shown Question Answering Datasets. This work mainly in Table 6, we perform a break-down analysis of refers to conversational QA datasets (Reddy et al., different answer types to various methods on the"
C12-1061,W11-3705,0,0.0255577,"er reputation, whereas (Lu et al., 2010) proposed a linear regression model with various social contexts 1 2 http://www.pmo.gov.sg/ http://www.whitehouse.gov/ 997 for review quality prediction. They combined textual and social context information to evaluate the quality of individual reviewers and to assess the quality of the reviews. We do not consider these factors as we want to focus on textual cues first. These additional features can be factored in as an independent step. Similar line of work can be seen by (Chen et al., 2011; Liu et al., 2007; Bian et al., 2009). Our work is similar to (Amgoud et al., 2011) where they introduced argument analysis together with opinion. In their task, properties of a person or product (honesty, rigor, friendliness, etc.) are treated as arguments. The task is oriented towards aggregating features related to the product and supporting arguments to detect polarity. The task we address in this paper is quite different from their work in two main aspects. Firstly, for sociopolitical issues, the policy makers look for insightful reasoning text to understand the public sentiment in which case, properties are insufficient. Secondly, we study the attentiveness of the comm"
C12-1061,W06-1651,0,0.0331903,"ember 2012. 995 1 Introduction In recent years sentiment analysis and opinion mining has been extensively studied in natural language processing (Pang and Lee, 2008), largely because of the availability of a huge amount of opinionated text in online product reviews, blogs, social networking sites, forums, etc. Most work on opinion mining is about mining reviews of products and services, and the focus of these studies has been on a few important subtasks such as sentiment classification (Pang et al., 2002; Cui et al., 2006; Jiang et al., 2011) and opinion extraction (Popescu and Etzioni, 2005; Choi et al., 2006; Wu et al., 2009). When we go beyond product review mining and consider the general problem of opinion mining from social media, many other subtasks and challenges arise. One of them is how to assess the quality of online comments and select high quality ones for further analysis and summarization. Consider the problem of mining the comments found in online social media towards a political speech such as Obama’s State of the Union address. By restricting the search space to politically active blogs and forums and by using queries such as “State of the Union,” likely we are able to retrieve hi"
C12-1061,P11-1016,0,0.0248685,"COLING 2012: Technical Papers, pages 995–1010, COLING 2012, Mumbai, December 2012. 995 1 Introduction In recent years sentiment analysis and opinion mining has been extensively studied in natural language processing (Pang and Lee, 2008), largely because of the availability of a huge amount of opinionated text in online product reviews, blogs, social networking sites, forums, etc. Most work on opinion mining is about mining reviews of products and services, and the focus of these studies has been on a few important subtasks such as sentiment classification (Pang et al., 2002; Cui et al., 2006; Jiang et al., 2011) and opinion extraction (Popescu and Etzioni, 2005; Choi et al., 2006; Wu et al., 2009). When we go beyond product review mining and consider the general problem of opinion mining from social media, many other subtasks and challenges arise. One of them is how to assess the quality of online comments and select high quality ones for further analysis and summarization. Consider the problem of mining the comments found in online social media towards a political speech such as Obama’s State of the Union address. By restricting the search space to politically active blogs and forums and by using qu"
C12-1061,W06-1650,0,0.257836,"a justification or an argument to the issue(s) in the target document. It is particularly important for sociopolitical opinion mining because of the complexity of sociopolitical issues. Intuitively, finding thoughtful comments is related to measuring text quality. There has been a large body of previous work on text quality prediction, but the methods are usually applied to student essays (Attali and Burstein, 2006) and news articles (Tang et al., 2003), (TREC novelty track 2003 and 2004). In social media mining, there have also been a number of studies on finding high quality reviews (e.g. (Kim et al., 2006; Agichtein et al., 2008)), but the focus in not on finding thoughtful comments, which requires us to look for reasoning in text. Presumably, a thoughtful comment should be logically well organized and coherent. We therefore hypothesize that discourse relations such as comparison, expansion and contingency will play an important role in finding thoughtful comments. A well organized comment is not always thoughtful. Comments such as, “He is a great speaker as he writes the speech by himself and also delivers it very confidently” are justified but are not relevant to the issues discussed in the"
C12-1061,D07-1035,0,0.0245537,"ing framework to simultaneously predict content quality and user reputation, whereas (Lu et al., 2010) proposed a linear regression model with various social contexts 1 2 http://www.pmo.gov.sg/ http://www.whitehouse.gov/ 997 for review quality prediction. They combined textual and social context information to evaluate the quality of individual reviewers and to assess the quality of the reviews. We do not consider these factors as we want to focus on textual cues first. These additional features can be factored in as an independent step. Similar line of work can be seen by (Chen et al., 2011; Liu et al., 2007; Bian et al., 2009). Our work is similar to (Amgoud et al., 2011) where they introduced argument analysis together with opinion. In their task, properties of a person or product (honesty, rigor, friendliness, etc.) are treated as arguments. The task is oriented towards aggregating features related to the product and supporting arguments to detect polarity. The task we address in this paper is quite different from their work in two main aspects. Firstly, for sociopolitical issues, the policy makers look for insightful reasoning text to understand the public sentiment in which case, properties"
C12-1061,W02-1011,0,0.0114291,"Text Classification. Proceedings of COLING 2012: Technical Papers, pages 995–1010, COLING 2012, Mumbai, December 2012. 995 1 Introduction In recent years sentiment analysis and opinion mining has been extensively studied in natural language processing (Pang and Lee, 2008), largely because of the availability of a huge amount of opinionated text in online product reviews, blogs, social networking sites, forums, etc. Most work on opinion mining is about mining reviews of products and services, and the focus of these studies has been on a few important subtasks such as sentiment classification (Pang et al., 2002; Cui et al., 2006; Jiang et al., 2011) and opinion extraction (Popescu and Etzioni, 2005; Choi et al., 2006; Wu et al., 2009). When we go beyond product review mining and consider the general problem of opinion mining from social media, many other subtasks and challenges arise. One of them is how to assess the quality of online comments and select high quality ones for further analysis and summarization. Consider the problem of mining the comments found in online social media towards a political speech such as Obama’s State of the Union address. By restricting the search space to politically"
C12-1061,D08-1020,0,0.454005,"i ) by a feature vector x(di , ci ) (which we refer to as xi ). We can then use standard classification algorithms to learn a classifier from {(xi , yi )}Ni=1 . This classifier can be used to predict y for any unseen pair of d and c. In the following sections, we will explain in detail the features we consider and the classification algorithm we use. 4 Features There have been many studies on measuring text quality and many features have been proposed to capture text quality. As mentioned previously our methodology is based on existing work on this topic. In particular, we follow the work by (Pitler and Nenkova, 2008). They 998 conducted a systematic study on text quality using various linguistic features and Wall Street Journal articles. Based on the major findings of their study, we take the following features as our starting point. 4.1 Structural Feature Structural features are generated from the comment structure. (Pitler and Nenkova, 2008) tested various structural features including the average number of characters per word, the average number of words per sentence, the maximum number of words per sentence, and article length. According to their findings, article length was the only significant facto"
C12-1061,C08-2022,0,0.0588852,"Missing"
C12-1061,H05-1043,0,0.0814204,"0, COLING 2012, Mumbai, December 2012. 995 1 Introduction In recent years sentiment analysis and opinion mining has been extensively studied in natural language processing (Pang and Lee, 2008), largely because of the availability of a huge amount of opinionated text in online product reviews, blogs, social networking sites, forums, etc. Most work on opinion mining is about mining reviews of products and services, and the focus of these studies has been on a few important subtasks such as sentiment classification (Pang et al., 2002; Cui et al., 2006; Jiang et al., 2011) and opinion extraction (Popescu and Etzioni, 2005; Choi et al., 2006; Wu et al., 2009). When we go beyond product review mining and consider the general problem of opinion mining from social media, many other subtasks and challenges arise. One of them is how to assess the quality of online comments and select high quality ones for further analysis and summarization. Consider the problem of mining the comments found in online social media towards a political speech such as Obama’s State of the Union address. By restricting the search space to politically active blogs and forums and by using queries such as “State of the Union,” likely we are"
C12-1061,N06-2034,0,0.0760384,"Missing"
C12-1061,N03-2033,0,0.0365176,"oblem of finding thoughtful comments from social media is what we study in this paper. Formally, a thoughtful comment is relevant to the target document and has a justification or an argument to the issue(s) in the target document. It is particularly important for sociopolitical opinion mining because of the complexity of sociopolitical issues. Intuitively, finding thoughtful comments is related to measuring text quality. There has been a large body of previous work on text quality prediction, but the methods are usually applied to student essays (Attali and Burstein, 2006) and news articles (Tang et al., 2003), (TREC novelty track 2003 and 2004). In social media mining, there have also been a number of studies on finding high quality reviews (e.g. (Kim et al., 2006; Agichtein et al., 2008)), but the focus in not on finding thoughtful comments, which requires us to look for reasoning in text. Presumably, a thoughtful comment should be logically well organized and coherent. We therefore hypothesize that discourse relations such as comparison, expansion and contingency will play an important role in finding thoughtful comments. A well organized comment is not always thoughtful. Comments such as, “He i"
C12-1061,D09-1159,0,0.0168583,"ntroduction In recent years sentiment analysis and opinion mining has been extensively studied in natural language processing (Pang and Lee, 2008), largely because of the availability of a huge amount of opinionated text in online product reviews, blogs, social networking sites, forums, etc. Most work on opinion mining is about mining reviews of products and services, and the focus of these studies has been on a few important subtasks such as sentiment classification (Pang et al., 2002; Cui et al., 2006; Jiang et al., 2011) and opinion extraction (Popescu and Etzioni, 2005; Choi et al., 2006; Wu et al., 2009). When we go beyond product review mining and consider the general problem of opinion mining from social media, many other subtasks and challenges arise. One of them is how to assess the quality of online comments and select high quality ones for further analysis and summarization. Consider the problem of mining the comments found in online social media towards a political speech such as Obama’s State of the Union address. By restricting the search space to politically active blogs and forums and by using queries such as “State of the Union,” likely we are able to retrieve highly relevant comm"
C12-1061,H05-2017,0,\N,Missing
C12-1061,prasad-etal-2008-penn,0,\N,Missing
C12-2042,W06-0301,0,0.0473729,"Missing"
C12-2042,D07-1035,0,0.0237007,"iment extraction and classification tasks (Turney, 2002; Pang et al., 2002). However, according to (Hu and Liu, 2004), fine-grained opinion mining and analysis is highly effective like feature identification by (Popescu and Etzioni, 2005), linking opinions to features by (Lin and He, 2009), and polarity classification by (Liu et al., 2005). Assessing the usefulness and quality of text has been well studied in natural language processing as quality plays a key role in online content (Agichtein et al., 2008) like helpfulness of reviews (Ghose and Ipeirotis, 2011), detecting low quality reviews (Liu et al., 2007) and detecting spam reviews (Lim et al., 2010). Actionable content: (Zhang et al., 2009) attempted to discover the diagnostic knowledge and defined diagnostic data mining as, “a task to understand the data and/or to find causes of problems and actionable knowledge in order to solve the problems”. Their work is more focussed towards manufacturing applications in which the problems are identified to aid the designers in the product design improvements. (Simm et al., 2010) analysed actionable knowledge in on-line social media conversation and the concept of actionability is defined as request or"
C12-2042,C10-2090,0,0.0376973,"Missing"
C12-2042,W02-1011,0,0.0174418,"Missing"
C12-2042,H05-1043,0,0.0409922,"malization results our Q4. We further analyzed the results for complete link. For article, A4 and the normalized entity Ionescu, the actionable comments have entity mentions like asshole, dog etc., which could not be normalized due to non-distinctive feature set. 7 Related Work Opinion Mining: Opinion mining is a well studied research for the past ten years mainly focussing on the sentiment extraction and classification tasks (Turney, 2002; Pang et al., 2002). However, according to (Hu and Liu, 2004), fine-grained opinion mining and analysis is highly effective like feature identification by (Popescu and Etzioni, 2005), linking opinions to features by (Lin and He, 2009), and polarity classification by (Liu et al., 2005). Assessing the usefulness and quality of text has been well studied in natural language processing as quality plays a key role in online content (Agichtein et al., 2008) like helpfulness of reviews (Ghose and Ipeirotis, 2011), detecting low quality reviews (Liu et al., 2007) and detecting spam reviews (Lim et al., 2010). Actionable content: (Zhang et al., 2009) attempted to discover the diagnostic knowledge and defined diagnostic data mining as, “a task to understand the data and/or to find"
C12-2042,D10-1048,0,0.0271891,"dependencies.shtml 424 The output of this task is S = {ei , ai }, a set of entity-action pairs. The next task is to normalize S which is described below. 4.2 Entity-action normalization Given S = {ei , ai }, a set of entity-action pairs, the goal is to generate N S = {nei , nai }, a set of normalized entity-action pairs. 4.2.1 Entity normalization We use agglomerative clustering which is a hierarchical clustering method which works bottomup (Olson, 1995) together with expanding the entity with the features from Google and Semantic-Similarity Sieves adopted from Stanford coreference algorithm (Raghunathan et al., 2010). Features: Two types of features are used to expand an entity mention: first from Google and second from the parse tree structure. The representative of a cluster, ne is chosen to be the entity mention which has the largest average similarity distance from the other entity mentions in the cluster. a. Alias features: This sieve addresses name aliases, which are detected as follows: Given an entity mention, it is first expanded with the title of the news article and this query is fed to the Google API. Google outputs the ranked matching outputs. One option is to use the entire snippet as the fe"
C12-2042,W95-0107,0,0.0266619,"In this section, we first describe our solution for entity-action extraction based on CRF model (Lafferty et al., 2001) and then our normalization model based on the clustering techniques for entity and action normalization. 4.1 Entity-action extraction The entity-action extraction problem can be treated as a sequence labeling task. Let x = (x 1 , x 2 , . . . , x n ) denote a comment sentence where each x i is a single token. We need to assign a sequence of labels or tags y = ( y1 , y2 , . . . , yn ) to x. We define our tag set as {BE, IE, BA, IA, O}, following the commonly used BIO notation (Ramshaw and Marcus, 1995), where E stands for entity and A stands for action. Features: To develop features, we consider three main properties of actionable comments. First, the entities of the actionable pairs are mostly nouns or pronouns. Second, the entities display the positional properties with respect to the keywords. Third, the entities should be grammatically related to the actions. For example the verb in the action phrase is related to the subject which is an entity of the actionable comment. a. Parts-of-speech features: To capture the first property, we classify each word x i into one of the POS tags using"
C12-2042,P02-1053,0,0.00507956,"Missing"
C12-2042,H05-2017,0,\N,Missing
C14-1158,P12-1061,0,0.012958,"work on summarizing answers is mainly based on query focused multi-document summarization techniques to summarize multiple answer documents given a single question. Liu et al. (2008) proposed a CQA question taxonomy to classify questions in CQA and question-type oriented answer summarization for better reuse of answers. Tomasoni and Huang (2010) proposed two conceptscoring functions to combine quality, coverage, relevance and novelty measures for answer summary in response to a question and showed that their summarized answers constitute a solid complement to best answers voted by CQA users. Chan et al. (2012) presented an answer summarization method for complex multi-sentence questions. For our work, we study a new problem of summarizing multiple threads to automatically generate city travel guides based on known template structure from well-written travel guides, which is different from the setting of single Q&A thread summarization in the previous related studies. 7 Conclusion and Future Work In this paper we proposed a summarization framework to generate well structured supplementary travel guides from social media based on a latent variable model and integer linear programming. The latent vari"
C14-1158,W04-3247,0,0.317815,"ion method as well as the baselines are also applied to these 30 threads per section per city for fair comparison. We randomly select 4 cities for human annotation, giving us 8 × 4 = 32 section-specific summarization tasks. For each task, we ask four annotators to read all 30 threads and write a summary as model summaries in our experiments2 . We use the following baseline algorithms for comparison: (1) Random, which randomly picks summary sentences. (2) Centroid (Radev et al., 2004), which selects sentences according to several features like tfidf, cluster centroid and position. (3) LexRank (Erkan and Radev, 2004b)., which applies a graphbased algorithm . (4) DivRank (Mei et al., 2010), which employs a time-variant random walk to enhance diversity. (5) GMDS (Wan, 2008), which incorporates the document-level information and the sentenceto-document relationship into the ranking process. (6) ILP-BL, which is the method proposed by Gillick and Favre (2009). We empirically set Dirichlet hyperparameters α = 0.5, β = 0.01, γ = 0.01, β 0 = 0.1. We run JCSM with 400 iterations of Gibbs sampling. For the weight parameters in the ILP model, we empirically set λ1 = 0.7, λ2 = 0.1, ρ = 0.7 after we conduct multiple"
C14-1158,W09-1802,0,0.227245,"hods to forum and CQA threads may not produce good travel guides for the following reasons: (1) Summaries produced by standard summarization methods are not structured, but travel guides usually follow a template structure. (2) Travel guides put much emphasis on points of interest, which are usually location entities, but standard text summarization methods are not entity-oriented. To illustrate our points, in Table 1 we show (i) the overall structure of a travel guide for Sydney from Lonely Planet, (ii) an excerpt from a summary generated by a state-of-the-art ILP-based summarization method (Gillick and Favre, 2009) from a set of threads related to Sydney, and (iii) excerpts of a structured summary generated by our proposed method. The comparison shows that the summary generated by the standard ILP method mixes information on different topics together and does not mention many * Corresponding author. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1670 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics:"
C14-1158,N09-1041,0,0.0338583,"ed framework was introduced as a global inference algorithm for multi-document summarization by McDonald (2007), which considers information and redundancy at the sentence level. Gillick and Favre (2009) studied information and redundancy at a sub-sentence, “concept” level, modeling the value of a summary as a function of the concepts it covers. In our work we also model concept level coverage of the summaries. Li et al. (2013) proposed a regression model to estimate the frequency of bigrams in the reference summary and analyzed the impact of bigram selection, weight estimation and ILP setup. Haghighi and Vanderwende (2009) constructed a sequence of generative probabilistic models for multi-document summarization, exhibiting ROUGE gains along the way. Sauper and Barzilay (2009) investigated an approach for creating a comprehensive textual overview of subject composed of information drawn from the Internet and applied ILP to optimize both local fit of information into each topic and global coherence across the entire overview. Li et al. (2011) developed an entity-aspect LDA model to cluster sentences into aspects and then extend LexRank algorithm to rank sentences. Hu and Wan (2013) proposed to use SVR model and"
C14-1158,D11-1105,1,0.828112,"regression model to estimate the frequency of bigrams in the reference summary and analyzed the impact of bigram selection, weight estimation and ILP setup. Haghighi and Vanderwende (2009) constructed a sequence of generative probabilistic models for multi-document summarization, exhibiting ROUGE gains along the way. Sauper and Barzilay (2009) investigated an approach for creating a comprehensive textual overview of subject composed of information drawn from the Internet and applied ILP to optimize both local fit of information into each topic and global coherence across the entire overview. Li et al. (2011) developed an entity-aspect LDA model to cluster sentences into aspects and then extend LexRank algorithm to rank sentences. Hu and Wan (2013) proposed to use SVR model and ILP method to generate presentation slides for academic papers. Our work is different from standard ILP-based multi-document summarization. We designed a latent variable model to first separate the threads to be summarized into sections based on model gravel guides. 1679 We also emphasized the inclusion of potential points of interest in formulating the ILP optimization problem. Our work is also closely related to previous"
C14-1158,P13-1099,0,0.0175244,"d DivRank (Mei et al., 2010), which introduces a time-variant matrix into a reinforced random walk to balance prestige and diversity. More recently, Integer Linear Programming (ILP) based framework was introduced as a global inference algorithm for multi-document summarization by McDonald (2007), which considers information and redundancy at the sentence level. Gillick and Favre (2009) studied information and redundancy at a sub-sentence, “concept” level, modeling the value of a summary as a function of the concepts it covers. In our work we also model concept level coverage of the summaries. Li et al. (2013) proposed a regression model to estimate the frequency of bigrams in the reference summary and analyzed the impact of bigram selection, weight estimation and ILP setup. Haghighi and Vanderwende (2009) constructed a sequence of generative probabilistic models for multi-document summarization, exhibiting ROUGE gains along the way. Sauper and Barzilay (2009) investigated an approach for creating a comprehensive textual overview of subject composed of information drawn from the Internet and applied ILP to optimize both local fit of information into each topic and global coherence across the entire"
C14-1158,N03-1020,0,0.127497,"e (2009). We empirically set Dirichlet hyperparameters α = 0.5, β = 0.01, γ = 0.01, β 0 = 0.1. We run JCSM with 400 iterations of Gibbs sampling. For the weight parameters in the ILP model, we empirically set λ1 = 0.7, λ2 = 0.1, ρ = 0.7 after we conduct multiple experiments to determine the best values of them from 0.1 to 0.9. 5.2 Summarization Results To compare the summaries generated by our method with those generated by the baselines, we first compute their ROUGE scores against the human generated model summaries. ROUGE scores have been widely used for evaluation of summarization systems (Lin and Hovy, 2003). We use the ROUGE toolkit3 , which provides multiple kinds of ROUGE metrics including ROUGE-N, ROUGE-L, ROUGEW and ROUGE-SU4. In the experiment results we report three ROUGE F-measure scores, namely, ROUGE-1, ROUGE-2 and ROUGE-SU4. The higher the ROUGE scores, the better a summary is. In Table 2 we show the summarization results of our method (with the optimal parameter setting) and the baseline methods. For each city, the scores we show are averaged over the 8 sections. The overall average scores on the right hand side are averaged over the 4 cities. We have the following findings from the t"
C14-1158,C08-1063,0,0.0194222,"ers. Our work is different from standard ILP-based multi-document summarization. We designed a latent variable model to first separate the threads to be summarized into sections based on model gravel guides. 1679 We also emphasized the inclusion of potential points of interest in formulating the ILP optimization problem. Our work is also closely related to previous work on answer summarization in community-based QA sites. Previous work on summarizing answers is mainly based on query focused multi-document summarization techniques to summarize multiple answer documents given a single question. Liu et al. (2008) proposed a CQA question taxonomy to classify questions in CQA and question-type oriented answer summarization for better reuse of answers. Tomasoni and Huang (2010) proposed two conceptscoring functions to combine quality, coverage, relevance and novelty measures for answer summary in response to a question and showed that their summarized answers constitute a solid complement to best answers voted by CQA users. Chan et al. (2012) presented an answer summarization method for complex multi-sentence questions. For our work, we study a new problem of summarizing multiple threads to automatically"
C14-1158,C10-2106,0,0.0268529,"elect sentences, different features and ranking strategies have been studied. Early work focuses on finding good features to select summary sentences. Radev et al. (2004) proposed a centroid-based summarizer which combines several pre-defined features like tfidf, cluster centroid and position to score sentences. Lin and Hovy (2002) built the NeATS multi-document summarization system using term frequency, sentence position, stigma words and simplified Maximal Marginal Relecvance (MMR). Nenkova et al. (2006) proved that high-frequency words were significant in reflecting the focus of documents. Ouyang et al. (2010) studied the influence of different word positions in summarization. Later, graphbased ranking algorithms have been successfully applied to summarization. LexPageRank (Erkan and Radev, 2004a) is a representative one based on the PageRank algorithm (Page et al., 1999). Later extensions include ToPageRank (Pei et al., 2012), which incorporates topic information into the propagation mechanism, the manifold-ranking based method for topic-focused summarization (Wan et al., 2007) and DivRank (Mei et al., 2010), which introduces a time-variant matrix into a reinforced random walk to balance prestige"
C14-1158,P09-1024,0,0.0323347,"sentence level. Gillick and Favre (2009) studied information and redundancy at a sub-sentence, “concept” level, modeling the value of a summary as a function of the concepts it covers. In our work we also model concept level coverage of the summaries. Li et al. (2013) proposed a regression model to estimate the frequency of bigrams in the reference summary and analyzed the impact of bigram selection, weight estimation and ILP setup. Haghighi and Vanderwende (2009) constructed a sequence of generative probabilistic models for multi-document summarization, exhibiting ROUGE gains along the way. Sauper and Barzilay (2009) investigated an approach for creating a comprehensive textual overview of subject composed of information drawn from the Internet and applied ILP to optimize both local fit of information into each topic and global coherence across the entire overview. Li et al. (2011) developed an entity-aspect LDA model to cluster sentences into aspects and then extend LexRank algorithm to rank sentences. Hu and Wan (2013) proposed to use SVR model and ILP method to generate presentation slides for academic papers. Our work is different from standard ILP-based multi-document summarization. We designed a lat"
C14-1158,P10-1078,0,0.0966485,"es are between 0 and 1. We solve the above optimization problem using the IBM ILOG CPLEX Optimizer1 . 5 Experiments 5.1 Data and Experimental Setup We use real data from Yahoo! Answers and Lonely Planet for evaluation. We first crawl the travel guides for 10 cities from Lonely Planet, where each travel guide has 8 sections. We then crawl the top 60000 Q&A threads ranked by number of posts related to these 10 cities (6000 for each city) from Yahoo! Answers under the “travel” category where all questions have been grouped by cities. We filter out trivial factoid questions using features used by Tomasoni and Huang (2010). We then use the Stanford 1 http://www-01.ibm.com/software/commerce/optimization/cplex-optimizer/ 1675 Method Random Centroid LexRank DivRank GMDS ILP-BL Our Method R-1 0.4091 0.4029 0.4396 0.4534 0.3918 0.4635 0.4723 Singapore R-2 RSU4 R-1 Sydney R-2 RSU4 R-1 New York City R-2 RSU4 0.1046 0.0993 0.1451 0.1504 0.0890 0.1650 0.1655 0.1576 0.1484 0.1891 0.1888 0.1415 0.2000 0.2035 0.4496 0.4228 0.4406 0.4473 0.4339 0.4948 0.5078 0.1100 0.1100 0.1296 0.1161 0.1066 0.1731 0.1787 0.1925 0.1764 0.1955 0.1925 0.1784 0.2333 0.2397 0.4442 0.4235 0.4304 0.4391 0.4064 0.4691 0.4716 0.1192 0.1192 0.1397"
C14-1158,D08-1079,0,0.0446804,"Missing"
C14-1158,P02-1058,0,\N,Missing
C16-1223,H05-1091,0,0.461181,"n free text. For example, a relation classification system should be able to capture the Cause-Effect relation between the entities pressure and burst in the sentence “The burst has been caused by water hammer pressure.” Like any classification task, a key research question of relation classification is the identification of a good feature representation for each relation instance. Traditional approaches focus on either combining many manually designed features (Zhou et al., 2005; Jiang and Zhai, 2007; Li and Ji, 2014) or leveraging various kernels to implicitly explore a large feature space (Bunescu and Mooney, 2005; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009), but both approaches suffer from their poor generalization ability on unseen words, and fail to achieve very satisfactory performance (Nguyen et al., 2015). Recently, with the advances of deep learning in NLP, neural networks (NNs) have exhibited their advantages in dealing with unseen words through pre-trained word embeddings and capturing meaningful hidden representations. Different NN architectures, including Convolutional Neural Network (CNN) (Zeng et al., 2014), Recursive Neural Network (ReNN) (Socher et al., 2012) and Recurren"
C16-1223,W06-1670,0,0.0477704,"8 65.6 56.4 44.6 47.6 57.2 50.4 55.2 56.8 Table 9: Evaluation of our combined CNN model together with the mirror instance method on ACE2005. The results of the state-of-the-art systems are taken from Nguyen and Grishman (2016). We can also see that without utilizing any AF, our Comb+RMI method can push the F1 score to the state-of-the-art, 85.0. Furthermore, we also consider adding two kinds of lexical features to our model, namely, Named Entity type (NER) and WordNet hypernyms. We first obtain the NER features of all words and Wordnet hypernyms of the two entities using the tool developed by Ciaramita and Altun (2006). Then, we represent each token by concatenating its word embedding, position embedding and entity embedding. Finally, following the practice by Zeng et al. (2014), we also concatenate the Wordnet hypernyms of the two entities with the combined hidden vector. As we can see from the last line of Table 8, our method can achieve the state-of-the-art F1 score, 85.7. ACE-2005: In Table 9, it is easy for us to observe that on all three test domains, our proposed Comb+MI method can outperform the state-of-the-art single system FCM with a large margin, which combines traditional linguistic features wi"
C16-1223,P15-1061,0,0.232774,"p learning in NLP, neural networks (NNs) have exhibited their advantages in dealing with unseen words through pre-trained word embeddings and capturing meaningful hidden representations. Different NN architectures, including Convolutional Neural Network (CNN) (Zeng et al., 2014), Recursive Neural Network (ReNN) (Socher et al., 2012) and Recurrent Neural Network (RNN) (Xu et al., 2015b), have been applied to relation classification. However, most existing NN-based approaches only exploit one of the following structures to represent relation instances: raw word sequences (Zeng et al., 2014; dos Santos et al., 2015), constituency parse trees (Socher et al., 2012; Hashimoto et al., 2013) and dependency parse trees (Xu et al., 2015a; Xu et al., 2015b; Miwa and Bansal, 2016). For the models based on raw sequence, despite maintaining all the information in relation instances, they cannot well handle long-distance relations. For the models based on constituency parse trees, one of the bottlenecks is handling long-distance relations (Ebrahimi and Dou, 2015). For the dependency tree-based models, although they focus on the condensed information This work is licenced under a Creative Commons Attribution 4.0 Inte"
C16-1223,N15-1133,0,0.016764,"er, most existing NN-based approaches only exploit one of the following structures to represent relation instances: raw word sequences (Zeng et al., 2014; dos Santos et al., 2015), constituency parse trees (Socher et al., 2012; Hashimoto et al., 2013) and dependency parse trees (Xu et al., 2015a; Xu et al., 2015b; Miwa and Bansal, 2016). For the models based on raw sequence, despite maintaining all the information in relation instances, they cannot well handle long-distance relations. For the models based on constituency parse trees, one of the bottlenecks is handling long-distance relations (Ebrahimi and Dou, 2015). For the dependency tree-based models, although they focus on the condensed information This work is licenced under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 2366 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2366–2377, Osaka, Japan, December 11-17 2016. captured by the shortest dependency path between the two entities and thus are good at capturing longdistance relations, they lose some supplementary information in the original instance (Liu et a"
C16-1223,D15-1205,0,0.0657344,". Prec Rec F1 85.33 86.63 87.76 83.20 84.23∗ 84.96∗ Table 5: Evaluation of our combined CNN model together with the mirror instance method. RMI stands for reduced mirror instances. instances as the development set. Following all previous work, we use the macro-averaged F1 score to evaluate our model based on the SemEval-2010 Task 8 official scorer. ACE-2005: This dataset consists of 6 domains: broadcast news (bn), newswire (nw), broadcast conversation (bc), telephone conversation (cts), weblogs (wl) and usenet (un). Following some previous work (Plank and Moschitti, 2013; Nguyen et al., 2015; Gormley et al., 2015), we consider a domain adaptation setting for coarse-grained relation extraction. Specifically, we take the union of bn and nw as the training set, half of bc as the development set, and the remainder (i.e., cts and wl as well as the other half of bc) as the test set. Following Plank and Moschitti (2013), we use the micro-averaged F1 score to evaluate our model. 3.2 Experiment Settings We use the pre-trained word embeddings from word2vec1 to initialize the lookup table We , and set the dimension d1 to 300. For unknown words and directed dependency labels, we randomly initialize their 300-dimen"
C16-1223,D13-1137,0,0.0183269,"ges in dealing with unseen words through pre-trained word embeddings and capturing meaningful hidden representations. Different NN architectures, including Convolutional Neural Network (CNN) (Zeng et al., 2014), Recursive Neural Network (ReNN) (Socher et al., 2012) and Recurrent Neural Network (RNN) (Xu et al., 2015b), have been applied to relation classification. However, most existing NN-based approaches only exploit one of the following structures to represent relation instances: raw word sequences (Zeng et al., 2014; dos Santos et al., 2015), constituency parse trees (Socher et al., 2012; Hashimoto et al., 2013) and dependency parse trees (Xu et al., 2015a; Xu et al., 2015b; Miwa and Bansal, 2016). For the models based on raw sequence, despite maintaining all the information in relation instances, they cannot well handle long-distance relations. For the models based on constituency parse trees, one of the bottlenecks is handling long-distance relations (Ebrahimi and Dou, 2015). For the dependency tree-based models, although they focus on the condensed information This work is licenced under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details:"
C16-1223,N07-1015,1,0.898182,"he goal of relation classification is to automatically identify the semantic relation between a pair of entities in free text. For example, a relation classification system should be able to capture the Cause-Effect relation between the entities pressure and burst in the sentence “The burst has been caused by water hammer pressure.” Like any classification task, a key research question of relation classification is the identification of a good feature representation for each relation instance. Traditional approaches focus on either combining many manually designed features (Zhou et al., 2005; Jiang and Zhai, 2007; Li and Ji, 2014) or leveraging various kernels to implicitly explore a large feature space (Bunescu and Mooney, 2005; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009), but both approaches suffer from their poor generalization ability on unseen words, and fail to achieve very satisfactory performance (Nguyen et al., 2015). Recently, with the advances of deep learning in NLP, neural networks (NNs) have exhibited their advantages in dealing with unseen words through pre-trained word embeddings and capturing meaningful hidden representations. Different NN architectures, including Conv"
C16-1223,D14-1181,0,0.092459,"the mirror instances. The loss function is given as follows: Jf = − N X n=1 where σ(ωy(n) ) =   x(n) ; Θ0 ) , log σ(ωy(n) )p(y (n) |x(n) ; Θ) + (1 − σ(ωy(n) ))p(rev(y (n) )|¯ 1 −ω (n) 1+e y is a tradeoff weight between the probability of x(n) being y (n) and the probability of x ¯(n) being rev(y (n) ). Finally, we minimize Jc + Jf as our overall objective function. Since the overall objective function consists of two components and each component is related to the other, we propose to jointly optimize them via stochastic gradient descent with shuffled mini-batches, based on the practice by Kim (2014). In our implementation, the learning rate of each parameter is scheduled by Adadelta (Zeiler, 2012) ( = 10−1 , ρ = 0.95 for ω, and  = 10−6 , ρ = 0.95 for Θ and Θ0 ). Testing: After training with pairs of original and mirror instances, during the testing stage, how to predict the label of a relation instance becomes straightforward. For a test instance xt , we should again generate its mirror instance x ¯t . Thereafter, we can obtain two class distributions by using the trained t model, one from x and the other from x ¯t . Let us use c(xt ) to denote the former and c(¯ xt ) the latter. t t t"
C16-1223,D07-1114,0,0.0239364,"pendency path representations of relation instances and uses mirror instances to perform pairwise relation classification. We evaluate our proposed models on two widely used datasets: SemEval-2010 Task 8 and ACE-2005. The empirical results show that our combined model together with mirror instances achieves the state-of-the-art results on both datasets. 1 Introduction Relation classification is a very important task for many Natural Language Processing (NLP) applications including question answering (Yao and Van Durme, 2014), knowledge base population (Socher et al., 2013) and opinion mining (Kobayashi et al., 2007). The goal of relation classification is to automatically identify the semantic relation between a pair of entities in free text. For example, a relation classification system should be able to capture the Cause-Effect relation between the entities pressure and burst in the sentence “The burst has been caused by water hammer pressure.” Like any classification task, a key research question of relation classification is the identification of a good feature representation for each relation instance. Traditional approaches focus on either combining many manually designed features (Zhou et al., 200"
C16-1223,P14-1038,0,0.111827,"assification is to automatically identify the semantic relation between a pair of entities in free text. For example, a relation classification system should be able to capture the Cause-Effect relation between the entities pressure and burst in the sentence “The burst has been caused by water hammer pressure.” Like any classification task, a key research question of relation classification is the identification of a good feature representation for each relation instance. Traditional approaches focus on either combining many manually designed features (Zhou et al., 2005; Jiang and Zhai, 2007; Li and Ji, 2014) or leveraging various kernels to implicitly explore a large feature space (Bunescu and Mooney, 2005; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009), but both approaches suffer from their poor generalization ability on unseen words, and fail to achieve very satisfactory performance (Nguyen et al., 2015). Recently, with the advances of deep learning in NLP, neural networks (NNs) have exhibited their advantages in dealing with unseen words through pre-trained word embeddings and capturing meaningful hidden representations. Different NN architectures, including Convolutional Neural N"
C16-1223,P15-2047,0,0.0670968,"u, 2015). For the dependency tree-based models, although they focus on the condensed information This work is licenced under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 2366 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2366–2377, Osaka, Japan, December 11-17 2016. captured by the shortest dependency path between the two entities and thus are good at capturing longdistance relations, they lose some supplementary information in the original instance (Liu et al., 2015). Observing that the raw sequence and the dependency path representations highly complement each other, we expect a combination of the two structures to be more effective in capturing long-distance relations without losing any information. Moreover, another important issue with the feature representation of relation instances is regarding the asymmetry of relation types. Most relation types are asymmetric. Take the Cause-Effect relation in the SemEval dataset as an example. Cause-Effect(e1 , e2 ) indicates that e1 is the cause and e2 is the effect. If their roles are reversed, we need to repre"
C16-1223,P16-1105,0,0.217602,"ngful hidden representations. Different NN architectures, including Convolutional Neural Network (CNN) (Zeng et al., 2014), Recursive Neural Network (ReNN) (Socher et al., 2012) and Recurrent Neural Network (RNN) (Xu et al., 2015b), have been applied to relation classification. However, most existing NN-based approaches only exploit one of the following structures to represent relation instances: raw word sequences (Zeng et al., 2014; dos Santos et al., 2015), constituency parse trees (Socher et al., 2012; Hashimoto et al., 2013) and dependency parse trees (Xu et al., 2015a; Xu et al., 2015b; Miwa and Bansal, 2016). For the models based on raw sequence, despite maintaining all the information in relation instances, they cannot well handle long-distance relations. For the models based on constituency parse trees, one of the bottlenecks is handling long-distance relations (Ebrahimi and Dou, 2015). For the dependency tree-based models, although they focus on the condensed information This work is licenced under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 2366 Proceedings of COLING 2016, the 26th International Conference on Computat"
C16-1223,W15-1506,0,0.0151804,"entity type embeddings, whose dimension is set to 50, and represent each token by concatenating its word embedding, position embedding and entity embedding. We want to compare our combined CNN model with models that use either RSeq or SDP alone, so we consider three experiment settings: SDP refers to a CNN model that uses only SDP representation of a relation instance, RSeq refers to a CNN model that uses only the RSeq, and Comb refers to our combined model. For each setting, we use the development set to tune the window size n and the dimension of the hidden states h. In a previous study by Nguyen and Grishman (2015), it was found that using multiple window sizes in CNN can bring significant improvements for the RSeq representation. We therefore also experiment with combining multiple window sizes for RSeq and SDP. In the end, we find that for RSeq, the optimal setting is to use a combination of windows with sizes 2, 3, 4 and 5 and to set h to 150. For SDP, the optimal setting is to use a single window of size 5 and to set h to 400. For Comb, we use the same window sizes and hidden sizes h as RSeq and SDP. For the other parameters in Θ and Θ0 , we adopt the settings reported by Nguyen and Grishman (2015)."
C16-1223,D09-1143,0,0.138829,"be able to capture the Cause-Effect relation between the entities pressure and burst in the sentence “The burst has been caused by water hammer pressure.” Like any classification task, a key research question of relation classification is the identification of a good feature representation for each relation instance. Traditional approaches focus on either combining many manually designed features (Zhou et al., 2005; Jiang and Zhai, 2007; Li and Ji, 2014) or leveraging various kernels to implicitly explore a large feature space (Bunescu and Mooney, 2005; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009), but both approaches suffer from their poor generalization ability on unseen words, and fail to achieve very satisfactory performance (Nguyen et al., 2015). Recently, with the advances of deep learning in NLP, neural networks (NNs) have exhibited their advantages in dealing with unseen words through pre-trained word embeddings and capturing meaningful hidden representations. Different NN architectures, including Convolutional Neural Network (CNN) (Zeng et al., 2014), Recursive Neural Network (ReNN) (Socher et al., 2012) and Recurrent Neural Network (RNN) (Xu et al., 2015b), have been applied"
C16-1223,P15-1062,0,0.0461678,"Like any classification task, a key research question of relation classification is the identification of a good feature representation for each relation instance. Traditional approaches focus on either combining many manually designed features (Zhou et al., 2005; Jiang and Zhai, 2007; Li and Ji, 2014) or leveraging various kernels to implicitly explore a large feature space (Bunescu and Mooney, 2005; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009), but both approaches suffer from their poor generalization ability on unseen words, and fail to achieve very satisfactory performance (Nguyen et al., 2015). Recently, with the advances of deep learning in NLP, neural networks (NNs) have exhibited their advantages in dealing with unseen words through pre-trained word embeddings and capturing meaningful hidden representations. Different NN architectures, including Convolutional Neural Network (CNN) (Zeng et al., 2014), Recursive Neural Network (ReNN) (Socher et al., 2012) and Recurrent Neural Network (RNN) (Xu et al., 2015b), have been applied to relation classification. However, most existing NN-based approaches only exploit one of the following structures to represent relation instances: raw wor"
C16-1223,P13-1147,0,0.0225923,"e models using either RSeq or SDP representation. Prec Rec F1 85.33 86.63 87.76 83.20 84.23∗ 84.96∗ Table 5: Evaluation of our combined CNN model together with the mirror instance method. RMI stands for reduced mirror instances. instances as the development set. Following all previous work, we use the macro-averaged F1 score to evaluate our model based on the SemEval-2010 Task 8 official scorer. ACE-2005: This dataset consists of 6 domains: broadcast news (bn), newswire (nw), broadcast conversation (bc), telephone conversation (cts), weblogs (wl) and usenet (un). Following some previous work (Plank and Moschitti, 2013; Nguyen et al., 2015; Gormley et al., 2015), we consider a domain adaptation setting for coarse-grained relation extraction. Specifically, we take the union of bn and nw as the training set, half of bc as the development set, and the remainder (i.e., cts and wl as well as the other half of bc) as the test set. Following Plank and Moschitti (2013), we use the micro-averaged F1 score to evaluate our model. 3.2 Experiment Settings We use the pre-trained word embeddings from word2vec1 to initialize the lookup table We , and set the dimension d1 to 300. For unknown words and directed dependency la"
C16-1223,C08-1088,0,0.0274772,"ation system should be able to capture the Cause-Effect relation between the entities pressure and burst in the sentence “The burst has been caused by water hammer pressure.” Like any classification task, a key research question of relation classification is the identification of a good feature representation for each relation instance. Traditional approaches focus on either combining many manually designed features (Zhou et al., 2005; Jiang and Zhai, 2007; Li and Ji, 2014) or leveraging various kernels to implicitly explore a large feature space (Bunescu and Mooney, 2005; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009), but both approaches suffer from their poor generalization ability on unseen words, and fail to achieve very satisfactory performance (Nguyen et al., 2015). Recently, with the advances of deep learning in NLP, neural networks (NNs) have exhibited their advantages in dealing with unseen words through pre-trained word embeddings and capturing meaningful hidden representations. Different NN architectures, including Convolutional Neural Network (CNN) (Zeng et al., 2014), Recursive Neural Network (ReNN) (Socher et al., 2012) and Recurrent Neural Network (RNN) (Xu et al., 2015"
C16-1223,S10-1057,0,0.0280649,"Missing"
C16-1223,D12-1110,0,0.312505,"e space (Bunescu and Mooney, 2005; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009), but both approaches suffer from their poor generalization ability on unseen words, and fail to achieve very satisfactory performance (Nguyen et al., 2015). Recently, with the advances of deep learning in NLP, neural networks (NNs) have exhibited their advantages in dealing with unseen words through pre-trained word embeddings and capturing meaningful hidden representations. Different NN architectures, including Convolutional Neural Network (CNN) (Zeng et al., 2014), Recursive Neural Network (ReNN) (Socher et al., 2012) and Recurrent Neural Network (RNN) (Xu et al., 2015b), have been applied to relation classification. However, most existing NN-based approaches only exploit one of the following structures to represent relation instances: raw word sequences (Zeng et al., 2014; dos Santos et al., 2015), constituency parse trees (Socher et al., 2012; Hashimoto et al., 2013) and dependency parse trees (Xu et al., 2015a; Xu et al., 2015b; Miwa and Bansal, 2016). For the models based on raw sequence, despite maintaining all the information in relation instances, they cannot well handle long-distance relations. For"
C16-1223,N16-1065,0,0.0585874,"Missing"
C16-1223,D15-1062,0,0.326097,"an et al., 2008; Nguyen et al., 2009), but both approaches suffer from their poor generalization ability on unseen words, and fail to achieve very satisfactory performance (Nguyen et al., 2015). Recently, with the advances of deep learning in NLP, neural networks (NNs) have exhibited their advantages in dealing with unseen words through pre-trained word embeddings and capturing meaningful hidden representations. Different NN architectures, including Convolutional Neural Network (CNN) (Zeng et al., 2014), Recursive Neural Network (ReNN) (Socher et al., 2012) and Recurrent Neural Network (RNN) (Xu et al., 2015b), have been applied to relation classification. However, most existing NN-based approaches only exploit one of the following structures to represent relation instances: raw word sequences (Zeng et al., 2014; dos Santos et al., 2015), constituency parse trees (Socher et al., 2012; Hashimoto et al., 2013) and dependency parse trees (Xu et al., 2015a; Xu et al., 2015b; Miwa and Bansal, 2016). For the models based on raw sequence, despite maintaining all the information in relation instances, they cannot well handle long-distance relations. For the models based on constituency parse trees, one o"
C16-1223,D15-1206,0,0.351931,"an et al., 2008; Nguyen et al., 2009), but both approaches suffer from their poor generalization ability on unseen words, and fail to achieve very satisfactory performance (Nguyen et al., 2015). Recently, with the advances of deep learning in NLP, neural networks (NNs) have exhibited their advantages in dealing with unseen words through pre-trained word embeddings and capturing meaningful hidden representations. Different NN architectures, including Convolutional Neural Network (CNN) (Zeng et al., 2014), Recursive Neural Network (ReNN) (Socher et al., 2012) and Recurrent Neural Network (RNN) (Xu et al., 2015b), have been applied to relation classification. However, most existing NN-based approaches only exploit one of the following structures to represent relation instances: raw word sequences (Zeng et al., 2014; dos Santos et al., 2015), constituency parse trees (Socher et al., 2012; Hashimoto et al., 2013) and dependency parse trees (Xu et al., 2015a; Xu et al., 2015b; Miwa and Bansal, 2016). For the models based on raw sequence, despite maintaining all the information in relation instances, they cannot well handle long-distance relations. For the models based on constituency parse trees, one o"
C16-1223,P14-1090,0,0.0843684,"Missing"
C16-1223,C14-1220,0,0.325619,"various kernels to implicitly explore a large feature space (Bunescu and Mooney, 2005; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009), but both approaches suffer from their poor generalization ability on unseen words, and fail to achieve very satisfactory performance (Nguyen et al., 2015). Recently, with the advances of deep learning in NLP, neural networks (NNs) have exhibited their advantages in dealing with unseen words through pre-trained word embeddings and capturing meaningful hidden representations. Different NN architectures, including Convolutional Neural Network (CNN) (Zeng et al., 2014), Recursive Neural Network (ReNN) (Socher et al., 2012) and Recurrent Neural Network (RNN) (Xu et al., 2015b), have been applied to relation classification. However, most existing NN-based approaches only exploit one of the following structures to represent relation instances: raw word sequences (Zeng et al., 2014; dos Santos et al., 2015), constituency parse trees (Socher et al., 2012; Hashimoto et al., 2013) and dependency parse trees (Xu et al., 2015a; Xu et al., 2015b; Miwa and Bansal, 2016). For the models based on raw sequence, despite maintaining all the information in relation instance"
C16-1223,P06-1104,0,0.0473858,"a relation classification system should be able to capture the Cause-Effect relation between the entities pressure and burst in the sentence “The burst has been caused by water hammer pressure.” Like any classification task, a key research question of relation classification is the identification of a good feature representation for each relation instance. Traditional approaches focus on either combining many manually designed features (Zhou et al., 2005; Jiang and Zhai, 2007; Li and Ji, 2014) or leveraging various kernels to implicitly explore a large feature space (Bunescu and Mooney, 2005; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009), but both approaches suffer from their poor generalization ability on unseen words, and fail to achieve very satisfactory performance (Nguyen et al., 2015). Recently, with the advances of deep learning in NLP, neural networks (NNs) have exhibited their advantages in dealing with unseen words through pre-trained word embeddings and capturing meaningful hidden representations. Different NN architectures, including Convolutional Neural Network (CNN) (Zeng et al., 2014), Recursive Neural Network (ReNN) (Socher et al., 2012) and Recurrent Neural Network (RN"
C16-1223,P05-1053,0,0.424202,"hi et al., 2007). The goal of relation classification is to automatically identify the semantic relation between a pair of entities in free text. For example, a relation classification system should be able to capture the Cause-Effect relation between the entities pressure and burst in the sentence “The burst has been caused by water hammer pressure.” Like any classification task, a key research question of relation classification is the identification of a good feature representation for each relation instance. Traditional approaches focus on either combining many manually designed features (Zhou et al., 2005; Jiang and Zhai, 2007; Li and Ji, 2014) or leveraging various kernels to implicitly explore a large feature space (Bunescu and Mooney, 2005; Zhang et al., 2006; Qian et al., 2008; Nguyen et al., 2009), but both approaches suffer from their poor generalization ability on unseen words, and fail to achieve very satisfactory performance (Nguyen et al., 2015). Recently, with the advances of deep learning in NLP, neural networks (NNs) have exhibited their advantages in dealing with unseen words through pre-trained word embeddings and capturing meaningful hidden representations. Different NN archite"
C16-1284,D14-1179,0,0.00676876,"Missing"
C16-1284,C10-2028,0,0.0199908,"Missing"
C16-1284,C12-2027,0,0.0732102,"he contents of the corpus at a very high level. Given a collection of microblog posts, LDA is able to learn a sparse topic representation for each post. The topic representation is viewed as a kind of global semantic information of a post, which we can utilize to learn the interactions between each words and the whole microblog post. 3 The Approach In this section, we will present our proposed model for hashtag recommendation. We formulate the hashtag recommendation task as a multi-class classification problem. It has been observed that hashtags indicate the primary topics of microblog posts (Ding et al., 2012; Godin et al., 2013). To incorporate the topics of microblogs, we take into account the attention mechanism and develop a novel Topical Attention-Based LSTM model, or TAB-LSTM for short. The basic idea of TAB-LSTM is to combine local hidden representations with global topic vectors through an attention mechanism. We believe that in this way our model can capture the importance of different local words according to the global topics of a microblog post. Our overall model is illustrated in Figure 2. The model mainly consists of three parts, namely, LSTM based sequence encoder, topic modeling, a"
C16-1284,D15-1046,0,0.390199,"s and the baseline methods, we use the validation data to tune the hyperparameters, we report the results of the test data in the same setting of hyperparameters. Furthermore, the word embeddings used in all methods are pre-trained from the original twitter data released by (Yang and Leskovec, 2011) with the word2vec toolkit (Mikolov et al., 2013). We use hashtags annotated by users as the golden set. To evaluate the performance, we use precision (P ), recall (R), and F1-score (F ) as the evaluation metrics. The same settings are adopted by previous work (Ding et al., 2012; Ding et al., 2013; Gong et al., 2015). 4.3 Comparison to Other Methods In Table 2, we compare the results of our method and the state-of-the-art discriminative and generative methods on the dataset. TAB-LSTM denotes our proposed model. We have the following observations. (1) First of all, SVM performs much better than LDA, showing that the embedding features capture more semantic information than bag-of-words (BoW). (2) Both TAB-LSTM and the degenerate models significantly outperform the baseline methods LDA, SVM and TTM. The results demonstrate that the neural network can achieve better performance on this task. (3) If we compar"
C16-1284,D11-1146,0,0.0120099,"ent each candidate hashtag as a feature vector and use pairwise learning to rank method to find the top ranked hashtags from the candidate set. Mazzia and Juett (2009) apply a Naive Bayes model to estimate the maximum a posteriori probability of each hashtag class given the words of the tweet. Furthermore, Godin et al. (2013) propose to incorporate topic models to learn the underlying topic assignment of language classified tweets, and suggest hashtags to a tweet based on the topic distribution. Under the assumption “hashtags and tweets are parallel description of a resource” that proposed by Liu et al. (2011), Ding et al. try to integrate latent topical information into translation model. The model uses topic-specific word trigger to bridge the vocabulary gap between the words in tweets and hashtags (Ding et al., 2012; Ding et al., 2013). Most of the works mentioned above are based on textual information. There have also been some attempts that combine text with other types of data. Kywe et al. propose a collaborative filtering model to incorporate user preferences in hashtag recommendation (Kywe et al., 2012). Besides that, Zhang et al. (2014) and Ma et al. (2014) try to incorporate temporal info"
C16-1284,D15-1166,0,0.0318503,"ic distribution of the post. Using these words, we can predict its hashtag #travel. necessary information of the input post has to be compressed into a fixed-length vector. This may make it difficult to cope with long sentences (Bahdanau et al., 2015). One possible solution is to perform an average pooling operation over the hidden vectors of LSTM (Boureau et al., 2011), but not all words in a microblog post contribute equally for hashtag recommendation. Inspired by the success of attention mechanism in computer vision and natural language processing (Mnih et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), we investigate the use of attention mechanism to automatically capture the most relevant words in a microblog to the recommendation task. Furthermore, it has been observed that most hashtags indicate the topics of a microblog (Ding et al., 2012; Godin et al., 2013), as illustrated in Figure 1. To this end, we propose a novel attention-based LSTM model which incorporates LDA topics of microblogs into the LSTM architecture through an attention mechanism. By modeling the interactions between the words and the global topics, our model can learn effective representations of microblogs for hashtag"
C16-1284,D15-1044,0,0.14038,"llaborative filtering to probabilistic models such as naive Bayes and topic models. Most of these methods depend on sparse lexical features including bag-of-word (BoW) models and exquisitely designed patterns. However, feature engineering is labor-intensive and the sparse and discrete features cannot effectively encode semantic and syntactic information of words. On the other hand, neural models recently have shown great potential for learning effective representations and delivered state-of-the-art performance on various natural language processing tasks (Cho et al., 2014; Tang et al., 2015; Rush et al., 2015). Among these methods, the long short-term memory (LSTM), a variant of recurrent neural network (RNN), is widely adopted due to its capability of capturing long-term dependencies in learning sequential representations (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Palangi et al., 2016). In this work, we model the hashtag recommendation task as a multi-class classification problem. A typical approach is to adopt LSTM to learn the representation of a microblog post and then perform text classification based on this representation. However, a potential issue with this approach is that all"
C16-1284,D15-1167,1,0.119957,"Missing"
C16-1284,N16-1170,1,0.573558,"on mechanism. Experiments on data from a real microblogging service show that our model achieves significantly better performance than various state-of-the-art methods. 2 Background Before going to the details of our method, we provide some background on two topics relevant to our work: the attention mechanism and Latent Dirichlet Allocation (LDA). 2.1 Attention Mechanism Attention-based models have demonstrated success in a wide range of NLP tasks including sentence summarization (Rush et al., 2015), reading comprehension (Hermann et al., 2015) and text entailment (Rockt¨aschel et al., 2016; Wang and Jiang, 2016). The basic idea of the attention mechanism is that it assigns a weight to each position in a lower-level of the neural network when computing an upper-level 3020 representation (Bahdanau et al., 2015; Luong et al., 2015). Bahdanau et al. (2015) made the first attempt to use an attention-based neural machine translation (NMT) approach to jointly translate and align words. The model is based on the basic encoder-decoder model (Cho et al., 2014). Differently, it encodes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively through the attention mechanism"
C16-1284,C14-1021,0,0.0746909,"re parallel description of a resource” that proposed by Liu et al. (2011), Ding et al. try to integrate latent topical information into translation model. The model uses topic-specific word trigger to bridge the vocabulary gap between the words in tweets and hashtags (Ding et al., 2012; Ding et al., 2013). Most of the works mentioned above are based on textual information. There have also been some attempts that combine text with other types of data. Kywe et al. propose a collaborative filtering model to incorporate user preferences in hashtag recommendation (Kywe et al., 2012). Besides that, Zhang et al. (2014) and Ma et al. (2014) try to incorporate temporal information. Gong et al. (2015) propose to model type of hashtag as a hidden variable into their DPMM (Dirichlet Process Mixture Models) based method. 3026 More recently, Gong and Zhang (2016) propose an attention-based convolutional neural network, which incorporates a local attention channel and global channel for hashtag recommendation. However, to the best of our knowledge, there is no work yet on employing both topic models and deep neural networks for this task. 6 Conclusion In this paper, we investigated a novel topical attention-based L"
C18-1023,S13-1002,0,0.0323101,"ddings (Faruqui et al., 2015; Chen et al., 2015), they have not investigated the usage of these enhanced word embeddings for RTE. Textual entailment with lexical knowledge: Many studies have attempted to inject lexical knowledge into the RTE task. Balkir et al. (2015) proposed compositional distributional models to extend representations from words to sentences. By mapping from a premise to a hypothesis by dependency trees or proposition extraction and then comparing lexical entailment between nodes or words, Herrera et al. (2005) and Ofoghi and Yearwood (2009) tried to solve RTE via WordNet. Beltagy et al. (2013), Beltagy et al. (2016) and Martinez-Gomez et al. (2017) first changed sentences to logical forms and then leveraged logic inference engines combined with lexical entailment axioms to make entailment judgments. But we are not aware of any work on how to apply lexical knowledge to textual entailment based on neural networks. 5 Conclusions In this paper, we propose to learn entailment vectors that encode lexical entailment knowledge from WordNet. We incorporate such entailment vectors into a decomposable attention model for RTE. Our empirical evaluation shows that the entailment vectors can inde"
C18-1023,J16-4007,0,0.222911,"hat pot is a direct hypernym of kettle and therefore kettle should entail pot. If this kind of prior knowledge can be injected into a trained RTE model, then for sentence pairs with words that do not occur in the training data but can be found in WordNet, the RTE predictions could potentially be made easier. Premise: Someone is stirring chili in a kettle. Hypothesis: Someone is stirring chili in a pot. Table 1: An example of a pair of premise and hypothesis from SICK. Indeed, WordNet (Miller, 1995) knowledge has been used to help RTE in a number of previous studies (Corley and Mihalcea, 2005; Beltagy et al., 2016; Martinez-Gomez et al., 2017). However, these This work is licensed under a Creative Commons Attribution 4.0 International License. http://creativecommons.org/licenses/by/4.0/. License details: 270 Proceedings of the 27th International Conference on Computational Linguistics, pages 270–281 Santa Fe, New Mexico, USA, August 20-26, 2018. Figure 1: An overview of our approach. previous studies were not based on neural network models, and thus their base models without using WordNet may not represent the state of the art or may require much human effort. For example, our baseline model based on a"
C18-1023,E12-1004,0,0.0175379,"nt entailment entailment contradiction neutral neutral (⟨lawn, field⟩, @) neutral Table 5: Examples of successful and erroneous predictions. considered to be different. The last example shows that for paraphrases not at the lexical level, our entailment vectors cannot help. 4 Related Work Lexical entailment: Our work is related to recognizing lexical entailment. Many existing methods for lexical entailment are based on the distributional inclusion hypothesis and leverage co-occurrence information of words from a large corpus (Weeds et al., 2004; Geffet and Dagan, 2005; Kotlerman et al., 2010; Bernardi et al., 2012). In contrast, our work uses labeled word pairs derived from WordNet to learn entailment vectors to encode the lexical entailment relations. The semantic relations we consider are also designed specifically for natural language inference. Word embeddings: Recently, neural-network-based approaches have been developed to learn vector representations of words (Mikolov et al., 2013; Pennington et al., 2014). These word embeddings are trained from an unlabeled large corpus and for general usage. In contrast, our entailment word vectors are not meant to be used as general-purpose word embeddings. Th"
C18-1023,D15-1075,0,0.204823,"ove the performance of textual entailment compared with a baseline that uses only standard word2vec vectors. The final performance of our model is close to or above the state of the art, but our method does not rely on any manually-crafted rules or extensive syntactic features. 1 Introduction Recognizing textual entailment (RTE) is the task of determining whether a hypothesis sentence can be inferred from a given premise sentence. The task has been well studied since it was first introduced by Dagan et al. (2006). Recently, there has been much interest in applying deep learning models to RTE (Bowman et al., 2015; Rocktaschel et al., 2016; Wang and Jiang, 2016; Parikh et al., 2016; Sha et al., 2016). These models usually do not perform any linguistic analysis or require any feature engineering but have been shown to perform very well on this task. Intuitively, lexical-level entailment relations should help sentence-level RTE. For example, Table 1 shows a premise and a hypothesis taken from the test data of the SICK dataset (Marelli et al., 2014). We can see that in this example the premise entails the hypothesis, and in order to correctly identify this relation, one has to know that the word kettle en"
C18-1023,P15-1011,0,0.0209484,"rd embeddings: Recently, neural-network-based approaches have been developed to learn vector representations of words (Mikolov et al., 2013; Pennington et al., 2014). These word embeddings are trained from an unlabeled large corpus and for general usage. In contrast, our entailment word vectors are not meant to be used as general-purpose word embeddings. They are designed specifically for incorporating external knowledge from WordNet into neural network models for RTE. Although there has been some work attempting to inject external lexical knowledge into word embeddings (Faruqui et al., 2015; Chen et al., 2015), they have not investigated the usage of these enhanced word embeddings for RTE. Textual entailment with lexical knowledge: Many studies have attempted to inject lexical knowledge into the RTE task. Balkir et al. (2015) proposed compositional distributional models to extend representations from words to sentences. By mapping from a premise to a hypothesis by dependency trees or proposition extraction and then comparing lexical entailment between nodes or words, Herrera et al. (2005) and Ofoghi and Yearwood (2009) tried to solve RTE via WordNet. Beltagy et al. (2013), Beltagy et al. (2016) and"
C18-1023,W05-1203,0,0.0934361,"et we can easily find out that pot is a direct hypernym of kettle and therefore kettle should entail pot. If this kind of prior knowledge can be injected into a trained RTE model, then for sentence pairs with words that do not occur in the training data but can be found in WordNet, the RTE predictions could potentially be made easier. Premise: Someone is stirring chili in a kettle. Hypothesis: Someone is stirring chili in a pot. Table 1: An example of a pair of premise and hypothesis from SICK. Indeed, WordNet (Miller, 1995) knowledge has been used to help RTE in a number of previous studies (Corley and Mihalcea, 2005; Beltagy et al., 2016; Martinez-Gomez et al., 2017). However, these This work is licensed under a Creative Commons Attribution 4.0 International License. http://creativecommons.org/licenses/by/4.0/. License details: 270 Proceedings of the 27th International Conference on Computational Linguistics, pages 270–281 Santa Fe, New Mexico, USA, August 20-26, 2018. Figure 1: An overview of our approach. previous studies were not based on neural network models, and thus their base models without using WordNet may not represent the state of the art or may require much human effort. For example, our bas"
C18-1023,N15-1184,0,0.0246984,"language inference. Word embeddings: Recently, neural-network-based approaches have been developed to learn vector representations of words (Mikolov et al., 2013; Pennington et al., 2014). These word embeddings are trained from an unlabeled large corpus and for general usage. In contrast, our entailment word vectors are not meant to be used as general-purpose word embeddings. They are designed specifically for incorporating external knowledge from WordNet into neural network models for RTE. Although there has been some work attempting to inject external lexical knowledge into word embeddings (Faruqui et al., 2015; Chen et al., 2015), they have not investigated the usage of these enhanced word embeddings for RTE. Textual entailment with lexical knowledge: Many studies have attempted to inject lexical knowledge into the RTE task. Balkir et al. (2015) proposed compositional distributional models to extend representations from words to sentences. By mapping from a premise to a hypothesis by dependency trees or proposition extraction and then comparing lexical entailment between nodes or words, Herrera et al. (2005) and Ofoghi and Yearwood (2009) tried to solve RTE via WordNet. Beltagy et al. (2013), Belta"
C18-1023,P05-1014,0,0.0672188,"adiction (⟨inside, outside⟩, ∣) neutral entailment entailment entailment contradiction neutral neutral (⟨lawn, field⟩, @) neutral Table 5: Examples of successful and erroneous predictions. considered to be different. The last example shows that for paraphrases not at the lexical level, our entailment vectors cannot help. 4 Related Work Lexical entailment: Our work is related to recognizing lexical entailment. Many existing methods for lexical entailment are based on the distributional inclusion hypothesis and leverage co-occurrence information of words from a large corpus (Weeds et al., 2004; Geffet and Dagan, 2005; Kotlerman et al., 2010; Bernardi et al., 2012). In contrast, our work uses labeled word pairs derived from WordNet to learn entailment vectors to encode the lexical entailment relations. The semantic relations we consider are also designed specifically for natural language inference. Word embeddings: Recently, neural-network-based approaches have been developed to learn vector representations of words (Mikolov et al., 2013; Pennington et al., 2014). These word embeddings are trained from an unlabeled large corpus and for general usage. In contrast, our entailment word vectors are not meant t"
C18-1023,S13-1001,0,0.0285735,"and b′ ∈ Rk denote a weight matrix and a bias vector, and k = ∣R∣. Given all the labeled word pairs in L, we can use the cross entropy loss as the objective function to learn the entailment vector for each w ∈ V as well as the various parameters above. 272 Set-Theoretic Model Although the standard neural network model above is straightforward, it is hard to explain how the learned word vectors can encode the lexical entailment relations. Inspired both by the set-theoretic definitions of the basic semantic relations shown in Table 2a and by some recent work on formal distributional semantics (Grefenstette, 2013; Rocktaschel et al., 2014), we hypothesize that a good entailment vector for a word essentially encodes which elements in D are members of the set representing this word. We now present a novel set-theoretic model to learn the entailment vectors. To illustrate our idea, we first show that in the extreme case when word vectors we try to learn are binary vectors precisely representing set memberships, the semantic relation between two words can be determined by comparing the two vectors element-wise. Specifically, let D denote the size of the universe D. Since a word x can be regarded as a subs"
C18-1023,N15-1098,0,0.0285353,"is the original decomposable attention model. The numbers shown in the table are what was reported in these papers. ** and * indicate statistical significance (p ≤ 0.01) and (p ≤ 0.05) compared with w2v by McNemars test, respectively. We show the results in terms of accuracy and F1 in Table 3a. We can observe the following from the table. (1) The entailment vectors give better accuracies for predicting the entailment relations compared with using word2vec embeddings. (2) Between the standard neural network model and our set-theoretic model, the set-theoretic model achieves better performance. Levy et al. (2015) previously showed that for some models using word embedding vectors to identify hypernyms, the model is essentially memorizing “prototypical” hypernyms. In order to check whether this happens to our model, we create a special set of labeled word pairs in which we introduce some “confusing” word pairs. We use the following rule to generate these word pairs: If we know (⟨w2 , w1 ⟩, @ ) and (⟨w1 , w3 ⟩, ∣), then we can infer that (⟨w2 , w3 ⟩, ∣). We obtain 719 labeled word pairs in this way. For example, (⟨staple gun, musical instrument⟩, ∣) is one in this set. We denote this dataset as confusio"
C18-1023,W09-3714,0,0.031932,"entailment (strict) reverse entailment equivalence alternation negation cover independence Symbol x1 @ x2 x1 A x2 x1 ≡ x2 x1 ∣ x2 x1 ∧ x2 x1 ⌣ x2 x1 #x2 Set-theoretic definition x1 ⊂ x2 x1 ⊃ x2 x1 = x2 x1 ∩ x2 = ∅ ∧ x1 ∪ x2 ≠ D x1 ∩ x2 = ∅ ∧ x1 ∪ x2 = D x1 ∩ x2 ≠ ∅ ∧ x1 ∪ x2 = D (else) Example woman, person person, woman couch, sofa woman, man able, unable person, non-woman woman, doctor (a) x1 @ x2 x1 A x2 x1 ≡ x2 x1 ∣ x2 x1 ∧ x2 x1 ⌣ x2 x1 #x2 c0,0 − − − >0 =0 =0 >0 c0,1 >0 =0 =0 − − − − c1,0 =0 >0 =0 − − − − c1,1 − − − =0 =0 >0 >0 (b) Table 2: (a) Seven basic semantic relations defined by MacCartney and Manning (2009). Here D is the universe that contains all entities. Each x1 (or x2 ) is a language unit that represents a subset of D. (b) Criteria for different basic semantic relations based on the counters c0,0 , c0,1 , c1,0 and c1,1 . two different neural network models used to learn the entailment vectors from L. We defer the description of how we derive labeled word pairs L from WordNet until Section 2.3. Lexical Entailment Relations Recall that eventually the entailment vectors will be used for textual entailment. In standard textual entailment datasets such as SICK and SNLI, there are three sentence-"
C18-1023,marelli-etal-2014-sick,0,0.0197399,"task has been well studied since it was first introduced by Dagan et al. (2006). Recently, there has been much interest in applying deep learning models to RTE (Bowman et al., 2015; Rocktaschel et al., 2016; Wang and Jiang, 2016; Parikh et al., 2016; Sha et al., 2016). These models usually do not perform any linguistic analysis or require any feature engineering but have been shown to perform very well on this task. Intuitively, lexical-level entailment relations should help sentence-level RTE. For example, Table 1 shows a premise and a hypothesis taken from the test data of the SICK dataset (Marelli et al., 2014). We can see that in this example the premise entails the hypothesis, and in order to correctly identify this relation, one has to know that the word kettle entails the word pot. However, if we train a neural network model on a set of labeled sentence pairs, and if the training dataset does not contain the word pair kettle and pot anywhere, it would be hard for the learned model to know that kettle entails pot and subsequently predict the relation between the premise and the hypothesis to be entailment. On the other hand, from WordNet we can easily find out that pot is a direct hypernym of ket"
C18-1023,E17-1067,0,0.0987379,"pernym of kettle and therefore kettle should entail pot. If this kind of prior knowledge can be injected into a trained RTE model, then for sentence pairs with words that do not occur in the training data but can be found in WordNet, the RTE predictions could potentially be made easier. Premise: Someone is stirring chili in a kettle. Hypothesis: Someone is stirring chili in a pot. Table 1: An example of a pair of premise and hypothesis from SICK. Indeed, WordNet (Miller, 1995) knowledge has been used to help RTE in a number of previous studies (Corley and Mihalcea, 2005; Beltagy et al., 2016; Martinez-Gomez et al., 2017). However, these This work is licensed under a Creative Commons Attribution 4.0 International License. http://creativecommons.org/licenses/by/4.0/. License details: 270 Proceedings of the 27th International Conference on Computational Linguistics, pages 270–281 Santa Fe, New Mexico, USA, August 20-26, 2018. Figure 1: An overview of our approach. previous studies were not based on neural network models, and thus their base models without using WordNet may not represent the state of the art or may require much human effort. For example, our baseline model based on a neural network model without"
C18-1023,U09-1018,0,0.0193781,"empting to inject external lexical knowledge into word embeddings (Faruqui et al., 2015; Chen et al., 2015), they have not investigated the usage of these enhanced word embeddings for RTE. Textual entailment with lexical knowledge: Many studies have attempted to inject lexical knowledge into the RTE task. Balkir et al. (2015) proposed compositional distributional models to extend representations from words to sentences. By mapping from a premise to a hypothesis by dependency trees or proposition extraction and then comparing lexical entailment between nodes or words, Herrera et al. (2005) and Ofoghi and Yearwood (2009) tried to solve RTE via WordNet. Beltagy et al. (2013), Beltagy et al. (2016) and Martinez-Gomez et al. (2017) first changed sentences to logical forms and then leveraged logic inference engines combined with lexical entailment axioms to make entailment judgments. But we are not aware of any work on how to apply lexical knowledge to textual entailment based on neural networks. 5 Conclusions In this paper, we propose to learn entailment vectors that encode lexical entailment knowledge from WordNet. We incorporate such entailment vectors into a decomposable attention model for RTE. Our empirical"
C18-1023,D16-1244,0,0.343106,"at uses only standard word2vec vectors. The final performance of our model is close to or above the state of the art, but our method does not rely on any manually-crafted rules or extensive syntactic features. 1 Introduction Recognizing textual entailment (RTE) is the task of determining whether a hypothesis sentence can be inferred from a given premise sentence. The task has been well studied since it was first introduced by Dagan et al. (2006). Recently, there has been much interest in applying deep learning models to RTE (Bowman et al., 2015; Rocktaschel et al., 2016; Wang and Jiang, 2016; Parikh et al., 2016; Sha et al., 2016). These models usually do not perform any linguistic analysis or require any feature engineering but have been shown to perform very well on this task. Intuitively, lexical-level entailment relations should help sentence-level RTE. For example, Table 1 shows a premise and a hypothesis taken from the test data of the SICK dataset (Marelli et al., 2014). We can see that in this example the premise entails the hypothesis, and in order to correctly identify this relation, one has to know that the word kettle entails the word pot. However, if we train a neural network model on a"
C18-1023,D14-1162,0,0.10028,"lment are based on the distributional inclusion hypothesis and leverage co-occurrence information of words from a large corpus (Weeds et al., 2004; Geffet and Dagan, 2005; Kotlerman et al., 2010; Bernardi et al., 2012). In contrast, our work uses labeled word pairs derived from WordNet to learn entailment vectors to encode the lexical entailment relations. The semantic relations we consider are also designed specifically for natural language inference. Word embeddings: Recently, neural-network-based approaches have been developed to learn vector representations of words (Mikolov et al., 2013; Pennington et al., 2014). These word embeddings are trained from an unlabeled large corpus and for general usage. In contrast, our entailment word vectors are not meant to be used as general-purpose word embeddings. They are designed specifically for incorporating external knowledge from WordNet into neural network models for RTE. Although there has been some work attempting to inject external lexical knowledge into word embeddings (Faruqui et al., 2015; Chen et al., 2015), they have not investigated the usage of these enhanced word embeddings for RTE. Textual entailment with lexical knowledge: Many studies have atte"
C18-1023,W14-2409,0,0.0218127,"a weight matrix and a bias vector, and k = ∣R∣. Given all the labeled word pairs in L, we can use the cross entropy loss as the objective function to learn the entailment vector for each w ∈ V as well as the various parameters above. 272 Set-Theoretic Model Although the standard neural network model above is straightforward, it is hard to explain how the learned word vectors can encode the lexical entailment relations. Inspired both by the set-theoretic definitions of the basic semantic relations shown in Table 2a and by some recent work on formal distributional semantics (Grefenstette, 2013; Rocktaschel et al., 2014), we hypothesize that a good entailment vector for a word essentially encodes which elements in D are members of the set representing this word. We now present a novel set-theoretic model to learn the entailment vectors. To illustrate our idea, we first show that in the extreme case when word vectors we try to learn are binary vectors precisely representing set memberships, the semantic relation between two words can be determined by comparing the two vectors element-wise. Specifically, let D denote the size of the universe D. Since a word x can be regarded as a subset of D, we assume that the"
C18-1023,C16-1270,0,0.0196994,"word2vec vectors. The final performance of our model is close to or above the state of the art, but our method does not rely on any manually-crafted rules or extensive syntactic features. 1 Introduction Recognizing textual entailment (RTE) is the task of determining whether a hypothesis sentence can be inferred from a given premise sentence. The task has been well studied since it was first introduced by Dagan et al. (2006). Recently, there has been much interest in applying deep learning models to RTE (Bowman et al., 2015; Rocktaschel et al., 2016; Wang and Jiang, 2016; Parikh et al., 2016; Sha et al., 2016). These models usually do not perform any linguistic analysis or require any feature engineering but have been shown to perform very well on this task. Intuitively, lexical-level entailment relations should help sentence-level RTE. For example, Table 1 shows a premise and a hypothesis taken from the test data of the SICK dataset (Marelli et al., 2014). We can see that in this example the premise entails the hypothesis, and in order to correctly identify this relation, one has to know that the word kettle entails the word pot. However, if we train a neural network model on a set of labeled sent"
C18-1023,N16-1170,1,0.849295,"red with a baseline that uses only standard word2vec vectors. The final performance of our model is close to or above the state of the art, but our method does not rely on any manually-crafted rules or extensive syntactic features. 1 Introduction Recognizing textual entailment (RTE) is the task of determining whether a hypothesis sentence can be inferred from a given premise sentence. The task has been well studied since it was first introduced by Dagan et al. (2006). Recently, there has been much interest in applying deep learning models to RTE (Bowman et al., 2015; Rocktaschel et al., 2016; Wang and Jiang, 2016; Parikh et al., 2016; Sha et al., 2016). These models usually do not perform any linguistic analysis or require any feature engineering but have been shown to perform very well on this task. Intuitively, lexical-level entailment relations should help sentence-level RTE. For example, Table 1 shows a premise and a hypothesis taken from the test data of the SICK dataset (Marelli et al., 2014). We can see that in this example the premise entails the hypothesis, and in order to correctly identify this relation, one has to know that the word kettle entails the word pot. However, if we train a neura"
C18-1023,C04-1146,0,0.0632226,"eutral neutral contradiction (⟨inside, outside⟩, ∣) neutral entailment entailment entailment contradiction neutral neutral (⟨lawn, field⟩, @) neutral Table 5: Examples of successful and erroneous predictions. considered to be different. The last example shows that for paraphrases not at the lexical level, our entailment vectors cannot help. 4 Related Work Lexical entailment: Our work is related to recognizing lexical entailment. Many existing methods for lexical entailment are based on the distributional inclusion hypothesis and leverage co-occurrence information of words from a large corpus (Weeds et al., 2004; Geffet and Dagan, 2005; Kotlerman et al., 2010; Bernardi et al., 2012). In contrast, our work uses labeled word pairs derived from WordNet to learn entailment vectors to encode the lexical entailment relations. The semantic relations we consider are also designed specifically for natural language inference. Word embeddings: Recently, neural-network-based approaches have been developed to learn vector representations of words (Mikolov et al., 2013; Pennington et al., 2014). These word embeddings are trained from an unlabeled large corpus and for general usage. In contrast, our entailment word"
D10-1006,N10-1122,0,0.400963,"ed extraction of opinion expressions and their targets (Wu et al., 2009). In particular, a general framework of summarizing reviews of a certain product is to first identify different aspects (a.k.a. features) of the given product and then extract specific opinion expressions for each aspect. For example, aspects of a restaurant may include food, staff, ambience and price, and opinion expressions for staff may include friendly, rude, etc. Because of the practicality of this structured summary format, it has been adopted in several previous studies (Hu and Liu, 2004; Popescu and Etzioni, 2005; Brody and Elhadad, 2010) as well as some commercial systems, e.g. the “scorecard” feature at Bing shopping1 . Different approaches have been proposed to identify aspect words and phrases from reviews. Previous methods using frequent itemset mining (Hu and Liu, 2004) or supervised learning (Jin and Ho, 2009; Jin et al., 2009; Wu et al., 2009) have the limitation that they do not group semantically related aspect expressions together. Supervised learning also suffers from its heavy dependence on training data. In contrast, unsupervised, knowledge-lean topic modeling approach has been shown to be effective in automatica"
D10-1006,W02-1011,0,0.0200894,"monstrate the domain adaptability of our model. 1 Introduction With the dramatic growth of opinionated usergenerated content, consumers often turn to online product reviews to seek advice while companies see reviews as a valuable source of consumer feedback. How to automatically understand, extract and summarize the opinions expressed in online reviews has therefore become an important research topic and gained much attention in recent years (Pang and Lee, 2008). A wide spectrum of tasks have been studied under review mining, ranging from coarse-grained document-level polarity classification (Pang et al., 2002) to fine-grained extraction of opinion expressions and their targets (Wu et al., 2009). In particular, a general framework of summarizing reviews of a certain product is to first identify different aspects (a.k.a. features) of the given product and then extract specific opinion expressions for each aspect. For example, aspects of a restaurant may include food, staff, ambience and price, and opinion expressions for staff may include friendly, rude, etc. Because of the practicality of this structured summary format, it has been adopted in several previous studies (Hu and Liu, 2004; Popescu and E"
D10-1006,H05-1043,0,0.566923,"et al., 2002) to fine-grained extraction of opinion expressions and their targets (Wu et al., 2009). In particular, a general framework of summarizing reviews of a certain product is to first identify different aspects (a.k.a. features) of the given product and then extract specific opinion expressions for each aspect. For example, aspects of a restaurant may include food, staff, ambience and price, and opinion expressions for staff may include friendly, rude, etc. Because of the practicality of this structured summary format, it has been adopted in several previous studies (Hu and Liu, 2004; Popescu and Etzioni, 2005; Brody and Elhadad, 2010) as well as some commercial systems, e.g. the “scorecard” feature at Bing shopping1 . Different approaches have been proposed to identify aspect words and phrases from reviews. Previous methods using frequent itemset mining (Hu and Liu, 2004) or supervised learning (Jin and Ho, 2009; Jin et al., 2009; Wu et al., 2009) have the limitation that they do not group semantically related aspect expressions together. Supervised learning also suffers from its heavy dependence on training data. In contrast, unsupervised, knowledge-lean topic modeling approach has been shown to"
D10-1006,D09-1159,0,0.778755,"of opinionated usergenerated content, consumers often turn to online product reviews to seek advice while companies see reviews as a valuable source of consumer feedback. How to automatically understand, extract and summarize the opinions expressed in online reviews has therefore become an important research topic and gained much attention in recent years (Pang and Lee, 2008). A wide spectrum of tasks have been studied under review mining, ranging from coarse-grained document-level polarity classification (Pang et al., 2002) to fine-grained extraction of opinion expressions and their targets (Wu et al., 2009). In particular, a general framework of summarizing reviews of a certain product is to first identify different aspects (a.k.a. features) of the given product and then extract specific opinion expressions for each aspect. For example, aspects of a restaurant may include food, staff, ambience and price, and opinion expressions for staff may include friendly, rude, etc. Because of the practicality of this structured summary format, it has been adopted in several previous studies (Hu and Liu, 2004; Popescu and Etzioni, 2005; Brody and Elhadad, 2010) as well as some commercial systems, e.g. the “s"
D10-1006,H05-2017,0,\N,Missing
D11-1074,E06-1002,0,0.678858,", i.e. θQ Q observe that all the curves are stable and 75 &lt;= σ &lt;= 100 appears to work well. We set σ=100 for all our experiments. Moreover, after 100, the graph becomes stable, which indicates that proximity has less impact on the method from this point on. This implies that an equal weighing scheme actually would work the same for these experiments. Part of the reason may be that by using only named entities in the context rather than all words, we have effectively picked the most useful contextual terms. Therefore, positional feedback models do have exhibit much benefit for our problem. 812 Bunescu and Pasca (2006) and Cucerzan (2007) explored the entity linking task using Vector Space Models for ranking. They took a classification approach together with the novel idea of exploiting Wikipedia knowledge. In their pioneering work, they used Wikipedia’s category information for entity disambiguation. They show that using different background knowledge, we can find efficient approaches for disambiguation. In their work, they took an assumption that every entity has a KB entry and thus the NIL entries are not handled. Similar to other researchers, Zhang et al. (2010) took an approach of classification and us"
D11-1074,D07-1074,0,0.57826,"the curves are stable and 75 &lt;= σ &lt;= 100 appears to work well. We set σ=100 for all our experiments. Moreover, after 100, the graph becomes stable, which indicates that proximity has less impact on the method from this point on. This implies that an equal weighing scheme actually would work the same for these experiments. Part of the reason may be that by using only named entities in the context rather than all words, we have effectively picked the most useful contextual terms. Therefore, positional feedback models do have exhibit much benefit for our problem. 812 Bunescu and Pasca (2006) and Cucerzan (2007) explored the entity linking task using Vector Space Models for ranking. They took a classification approach together with the novel idea of exploiting Wikipedia knowledge. In their pioneering work, they used Wikipedia’s category information for entity disambiguation. They show that using different background knowledge, we can find efficient approaches for disambiguation. In their work, they took an assumption that every entity has a KB entry and thus the NIL entries are not handled. Similar to other researchers, Zhang et al. (2010) took an approach of classification and used a twostage approa"
D11-1074,C10-1032,0,0.492051,"ing knowledge base is called entity linking and has been proposed and studied in the Knowledge Base Population (KBP) track of the Text Analysis Conference (TAC) (McNamee and Dang, 2009). Besides improving an online surfer’s browsing experience, entity linking also has potential usIntuitively, to disambiguate a polysemous entity name, we should make use of the context in which the name occurs, and to address synonymy, external world knowledge is usually needed to expand acronyms or find other name variations. Indeed both strategies have been explored in existing literature (Zhang et al., 2010; Dredze et al., 2010; Zheng et al., 2010). However, most existing work uses supervised learning approaches that require careful feature engineering and a large amount of training data. In this paper, we take a simpler unsupervised approach using statistical language model-based information retrieval. We use the KL-divergence retrieval model (Zhai and Lafferty, 2001) and expand the query language models by considering both the local contexts within the query documents and global world knowledge obtained from the Web. 804 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages"
D11-1074,C10-1145,0,0.553662,"entries in an existing knowledge base is called entity linking and has been proposed and studied in the Knowledge Base Population (KBP) track of the Text Analysis Conference (TAC) (McNamee and Dang, 2009). Besides improving an online surfer’s browsing experience, entity linking also has potential usIntuitively, to disambiguate a polysemous entity name, we should make use of the context in which the name occurs, and to address synonymy, external world knowledge is usually needed to expand acronyms or find other name variations. Indeed both strategies have been explored in existing literature (Zhang et al., 2010; Dredze et al., 2010; Zheng et al., 2010). However, most existing work uses supervised learning approaches that require careful feature engineering and a large amount of training data. In this paper, we take a simpler unsupervised approach using statistical language model-based information retrieval. We use the KL-divergence retrieval model (Zhai and Lafferty, 2001) and expand the query language models by considering both the local contexts within the query documents and global world knowledge obtained from the Web. 804 Proceedings of the 2011 Conference on Empirical Methods in Natural Langua"
D11-1074,N10-1072,0,0.744711,"called entity linking and has been proposed and studied in the Knowledge Base Population (KBP) track of the Text Analysis Conference (TAC) (McNamee and Dang, 2009). Besides improving an online surfer’s browsing experience, entity linking also has potential usIntuitively, to disambiguate a polysemous entity name, we should make use of the context in which the name occurs, and to address synonymy, external world knowledge is usually needed to expand acronyms or find other name variations. Indeed both strategies have been explored in existing literature (Zhang et al., 2010; Dredze et al., 2010; Zheng et al., 2010). However, most existing work uses supervised learning approaches that require careful feature engineering and a large amount of training data. In this paper, we take a simpler unsupervised approach using statistical language model-based information retrieval. We use the KL-divergence retrieval model (Zhai and Lafferty, 2001) and expand the query language models by considering both the local contexts within the query documents and global world knowledge obtained from the Web. 804 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 804–813, c Edinburgh,"
D11-1075,N10-1083,0,0.0706436,"Missing"
D11-1075,de-marneffe-etal-2006-generating,0,0.0232361,"Missing"
D11-1075,P06-2027,0,0.143574,"main experts, and annotated documents are often created to facilitate supervised learning approaches to IE. However, both manual template construction and data annotation are labor-intensive. More importantly, templates and annotated data usually cannot be re-used in new domains due to domain dependency. It is therefore natural to consider the problem of unsupervised template induction and information extraction. This is the topic of this paper. There have been a few previous attempts to address the unsupervised IE problem (Shinyama and Sekine, 2006; Sekine, 2006; Rosenfeld and Feldman, 2006; Filatova et al., 2006). These approaches have a commonality: they try to cluster candidate slot fillers, which are often nouns and noun phrases, into slots of the template to be constructed. However, most of them have neglected the following important observation: a single document or text segment tends to cover different slots rather than redundantly fill the same slot. In other words, during clustering, candidates within the same text segment should be more likely to be distributed into different clusters. In this paper, we propose a generative model that incorporates this distributional prior knowledge. We defin"
D11-1075,P05-1045,0,0.0113538,"yi i ∑ j 1(yi,j = k)xi,j ∑ , y i αi,y i j 1(yi,j = k) ∑ ∑ αi,yi (10) 819 We use three data sets for evaluating our unsupervised IE task. Note that to speed up computation, we only include documents or text segments containing no more than 10 candidates in our experiments. The first data set contains a set of seminar announcements (Freitag and McCallum, 1999), annotated with four slot labels, namely stime (start time), etime (end time), speaker and location. We used as candidates all strings labeled in the annotated data as well as all named entities found by the Stanford NER tagger for CoNLL (Finkel et al., 2005). There are 309 seminar announcements with 2262 candidates in this data set. The second data set is a collection of paragraphs describing aviation incidents, taken from the Wikipedia article on “List of accidents and incidents involving commercial aircraft” (Wikipedia, 2009). Each paragraph in the article contains one to a few sentences describing an incident. In this domain, we take each paragraph as a separate document, and all hyperlinked phrases in the original Wikipedia article as candidates. For evaluation, we manually annotated the paragraphs of incidents from 2006 to 2009 with five slo"
D11-1075,P04-1053,0,0.0456621,"erform unsupervised relation extraction. T EXT RUNNER (Banko et al., 2007), for example, automatically extracts all possible relations between pairs of noun phrases from a given corpus. The main difference between open domain IE and our work is that open domain IE does not aim to induce domain templates, whereas we focus on a single do815 main with the goal of inducing a template that describes salient information structure of that domain. Furthermore, T EXT RUNNER and related studies on unsupervised relation extraction often rely on highly redundant information on the Web or in large corpus (Hasegawa et al., 2004; Rosenfeld and Feldman, 2006; Yan et al., 2009), which is not assumed in our study. We propose a generative model with a distributional prior for the unsupervised IE task, where slot fillers correspond to observations in the model, and their labels correspond to hidden variables we want to learn. In the machine learning literature, researchers have explored the use of similar prior knowledge in the form of constraints through model expectation. For example, Grac¸a et al. (2007) proposed to place constraints on the posterior probabilities of hidden variables in a generative model, while Druck"
D11-1075,P06-2086,0,0.190238,"s usually done manually by domain experts, and annotated documents are often created to facilitate supervised learning approaches to IE. However, both manual template construction and data annotation are labor-intensive. More importantly, templates and annotated data usually cannot be re-used in new domains due to domain dependency. It is therefore natural to consider the problem of unsupervised template induction and information extraction. This is the topic of this paper. There have been a few previous attempts to address the unsupervised IE problem (Shinyama and Sekine, 2006; Sekine, 2006; Rosenfeld and Feldman, 2006; Filatova et al., 2006). These approaches have a commonality: they try to cluster candidate slot fillers, which are often nouns and noun phrases, into slots of the template to be constructed. However, most of them have neglected the following important observation: a single document or text segment tends to cover different slots rather than redundantly fill the same slot. In other words, during clustering, candidates within the same text segment should be more likely to be distributed into different clusters. In this paper, we propose a generative model that incorporates this distributional p"
D11-1075,P06-2094,0,0.139818,"tes. Template construction is usually done manually by domain experts, and annotated documents are often created to facilitate supervised learning approaches to IE. However, both manual template construction and data annotation are labor-intensive. More importantly, templates and annotated data usually cannot be re-used in new domains due to domain dependency. It is therefore natural to consider the problem of unsupervised template induction and information extraction. This is the topic of this paper. There have been a few previous attempts to address the unsupervised IE problem (Shinyama and Sekine, 2006; Sekine, 2006; Rosenfeld and Feldman, 2006; Filatova et al., 2006). These approaches have a commonality: they try to cluster candidate slot fillers, which are often nouns and noun phrases, into slots of the template to be constructed. However, most of them have neglected the following important observation: a single document or text segment tends to cover different slots rather than redundantly fill the same slot. In other words, during clustering, candidates within the same text segment should be more likely to be distributed into different clusters. In this paper, we propose a generative mo"
D11-1075,N06-1039,0,0.020869,"ion of templates. Template construction is usually done manually by domain experts, and annotated documents are often created to facilitate supervised learning approaches to IE. However, both manual template construction and data annotation are labor-intensive. More importantly, templates and annotated data usually cannot be re-used in new domains due to domain dependency. It is therefore natural to consider the problem of unsupervised template induction and information extraction. This is the topic of this paper. There have been a few previous attempts to address the unsupervised IE problem (Shinyama and Sekine, 2006; Sekine, 2006; Rosenfeld and Feldman, 2006; Filatova et al., 2006). These approaches have a commonality: they try to cluster candidate slot fillers, which are often nouns and noun phrases, into slots of the template to be constructed. However, most of them have neglected the following important observation: a single document or text segment tends to cover different slots rather than redundantly fill the same slot. In other words, during clustering, candidates within the same text segment should be more likely to be distributed into different clusters. In this paper, we propose a generative mo"
D11-1075,P03-1029,0,0.0382942,"data set and find that the proposed prior will have little effect if there are no good discriminative features to begin with. In summary, we find that 814 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 814–824, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics our Poisson-based label assignment prior is effective when coupled with good discriminative features. 2 Related Work One common approach to unsupervised IE is based on automatic IE pattern acquisition on a cluster of similar documents. For instance, Sudo et al. (2003) and Sekine (2006) proposed different methods for automatic IE pattern acquisition for a given domain based on frequent subtree discovery in dependency parse trees. These methods leveraged heavily on the entity types of candidates when assigning them to template slots. As a consequence, potentially different semantic roles of candidates having the same entity type could become indistinguishable (Sudo et al., 2003; Sekine, 2006). This problem is alleviated in our work by exploiting distributional prior knowledge about template slots, which is shown effective when coupled with discriminative fea"
D11-1075,P09-1115,0,0.0166285,"R (Banko et al., 2007), for example, automatically extracts all possible relations between pairs of noun phrases from a given corpus. The main difference between open domain IE and our work is that open domain IE does not aim to induce domain templates, whereas we focus on a single do815 main with the goal of inducing a template that describes salient information structure of that domain. Furthermore, T EXT RUNNER and related studies on unsupervised relation extraction often rely on highly redundant information on the Web or in large corpus (Hasegawa et al., 2004; Rosenfeld and Feldman, 2006; Yan et al., 2009), which is not assumed in our study. We propose a generative model with a distributional prior for the unsupervised IE task, where slot fillers correspond to observations in the model, and their labels correspond to hidden variables we want to learn. In the machine learning literature, researchers have explored the use of similar prior knowledge in the form of constraints through model expectation. For example, Grac¸a et al. (2007) proposed to place constraints on the posterior probabilities of hidden variables in a generative model, while Druck et al. (2008) studied a similar problem in a dis"
D11-1105,P06-1039,0,0.145204,"Missing"
D11-1105,W08-1105,0,0.010264,"paragraph within a document is likely to be about the same aspect. The way we separate words into stop words, background words, document words and aspect words bears similarity to that used in (Daum´e III and Marcu, 2006; Haghighi and Vanderwende, 2009). Paul and Girju (2010) proposed a topic-aspect model for simultaneously finding topics and aspects. The most related extension is entityaspect model proposed by Li et al. (2010). The main difference between event-aspect model and entityaspect model is our model further consider aspect granularity and add a layer to model topic-related events. Filippova and Strube (2008) proposed a dependency tree based sentence compression algorithm. Their approach need a large corpus to build language model for compression, whereas we prune dependency tree using grammatical rules. Paul et al. (2010) proposed to modify LexRank algorithm using their topic-aspect model. But their task is to summarize contrastive viewpoints in opinionated text. Furthermore, they use a simple greedy approach for constructing summary. McDonald (2007) proposed to use Integer Linear Programming framework in multi-document summarization. And Sauper and Barzilay (2009) use integer linear programming"
D11-1105,W09-1802,0,0.0423992,"o score representative sentences in each cluster (See Section 3). Sentence Compression: In this step, we aim to improve the linguistic quality of the summaries by simplifying the sentence expressions. We prune sentences using grammatical relations defined on dependency trees for recognizing important clauses and removing redundant subtrees (See Section 4). Sentence Selection: Finally, we select one compressed version of the sentences from each aspect cluster. We use Integer Linear Programming (ILP) algorithm, which optimizes a global objective function, for sentence selection (McDonald, 2007; Gillick and Favre, 2009; Sauper and Barzilay, 2009) (See Section 5). We evaluate our method using TAC2010 Guided Summarization task data sets1 (Section 6). Our evaluation shows that our method obtains better ROUGE recall score compared with four baseline methods, and it also achieve reasonably high-quality aspect clusters in terms of purity. 2 Sentence Clustering In this step, our goal is to discover event aspects contained in a document set and cluster sentences into aspects. Here we substantially extend the entityaspect model in Li et al. (2010) and refer to it as event-aspect model. The main difference between ou"
D11-1105,N09-1041,0,0.57074,"hat summarizes the subject of articles before the table of contents and other elaborate sections. Second, opinionated text often contains multiple viewpoints about an issue generated by different people. Summarizing these multiple opinions can help people easily digest them. Furthermore, combined with search engines and question&answering systems, we can better organize the summary content based on aspects to improve user experience. Despite its usefulness, the problem of modeling domain specific aspects for multi-document summarization has not been well studied. The most relevant work is by (Haghighi and Vanderwende, 2009) on exploring content models for multi-document summarization. They proposed a HIERSUM model for finding the subtopics or aspects which are combined by using KL-divergence criterion for selecting relevant sentences. They introduced a general content distribution and several specific content distributions to discover the topic and aspects for a single document collection. However, the aspects may be shared not only across documents in a single collection, but also across documents in different topic-related collections. Their model is conceptually inadequate for simultaneously summarizing multi"
D11-1105,P10-1066,1,0.87905,"Natural Language Processing, pages 1137–1146, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics limitations. We hypothesize that comparatively summarizing topics across similar collections can improve the effectiveness of aspect-oriented multidocument summarization. We propose a novel extraction-based approach which consists of four main steps listed below: Sentence Clustering: Our goal in this step is to automatically identify the different aspects and cluster sentences into aspects (See Section 2). We substantially extend the entity-aspect model in (Li et al., 2010) for generating general sentence clusters. Sentence Ranking: In this step, we use an extension of LexRank algorithm proposed by (Paul et al., 2010) to score representative sentences in each cluster (See Section 3). Sentence Compression: In this step, we aim to improve the linguistic quality of the summaries by simplifying the sentence expressions. We prune sentences using grammatical relations defined on dependency trees for recognizing important clauses and removing redundant subtrees (See Section 4). Sentence Selection: Finally, we select one compressed version of the sentences from each asp"
D11-1105,N03-1020,0,0.141258,"en the main verb of a clause and other sentential elements, such as a sentential parenthetical, colon, or semicolon Compressed When rescue workers arrived, only one of his limbs was visible. l=1 4 Sjl = 1 ∀j ∈ {1 . . . K} http://lpsolve.sourceforge.net/5.5/ Redundancy Constraints 6.2 Quality of summary We also want to prevent redundancy across different aspects. If sentence-similarity sim(sjl , sj ′ l′ ) between sentence sjl and sj ′ l′ is above 0.5, then we drop the pair and choose one sentence ranked higher from the pair otherwise. This constraint is formulated as follows: We use the ROUGE (Lin and Hovy, 2003) metric for measuring the summarization system performance. Ideally, a summarization criterion should be more recall oriented. So the average recall of ROUGE1, ROUGE-2, ROUGE-SU4, ROUGE-W-1.2 and ROUGE-L were computed by running ROUGE1.5.5 with stemming but no removal of stop words. We compare our method with the following four baseline methods. ′ (Sjl + Sj ′ l′ ) · sim(sjl , sj ′ l′ ) ≤ 0.5 ∀j, j ∈ {1 . . . K}∀l ∈ {1 . . . Rj }∀l′ ∈ {1 . . . Rj ′ } Length Constraints Baseline 1 We add this constraint to ensure that the length of the final summary is limited to L words. In this baseline, we tr"
D11-1105,D10-1007,0,0.435048,"ions. We hypothesize that comparatively summarizing topics across similar collections can improve the effectiveness of aspect-oriented multidocument summarization. We propose a novel extraction-based approach which consists of four main steps listed below: Sentence Clustering: Our goal in this step is to automatically identify the different aspects and cluster sentences into aspects (See Section 2). We substantially extend the entity-aspect model in (Li et al., 2010) for generating general sentence clusters. Sentence Ranking: In this step, we use an extension of LexRank algorithm proposed by (Paul et al., 2010) to score representative sentences in each cluster (See Section 3). Sentence Compression: In this step, we aim to improve the linguistic quality of the summaries by simplifying the sentence expressions. We prune sentences using grammatical relations defined on dependency trees for recognizing important clauses and removing redundant subtrees (See Section 4). Sentence Selection: Finally, we select one compressed version of the sentences from each aspect cluster. We use Integer Linear Programming (ILP) algorithm, which optimizes a global objective function, for sentence selection (McDonald, 2007"
D11-1105,P09-1024,0,0.0568093,"ntences in each cluster (See Section 3). Sentence Compression: In this step, we aim to improve the linguistic quality of the summaries by simplifying the sentence expressions. We prune sentences using grammatical relations defined on dependency trees for recognizing important clauses and removing redundant subtrees (See Section 4). Sentence Selection: Finally, we select one compressed version of the sentences from each aspect cluster. We use Integer Linear Programming (ILP) algorithm, which optimizes a global objective function, for sentence selection (McDonald, 2007; Gillick and Favre, 2009; Sauper and Barzilay, 2009) (See Section 5). We evaluate our method using TAC2010 Guided Summarization task data sets1 (Section 6). Our evaluation shows that our method obtains better ROUGE recall score compared with four baseline methods, and it also achieve reasonably high-quality aspect clusters in terms of purity. 2 Sentence Clustering In this step, our goal is to discover event aspects contained in a document set and cluster sentences into aspects. Here we substantially extend the entityaspect model in Li et al. (2010) and refer to it as event-aspect model. The main difference between our event-aspect model and ent"
D12-1114,D08-1031,0,0.595918,"irwise classification model and the mention clustering model. This idea has been explored to some extent by McCallum and Wellner (2005) using conditional undirected graphical models and by Finley and Joachims (2005) using an SVM-based supervised clustering method. Introduction The task of noun phrase coreference resolution is to determine which mentions in a text refer to the same real-world entity. Many methods have been proposed for this problem. Among them the mentionpair model (McCarthy and Lehnert, 1995) is one of the most influential ones and can achieve the stateof-the-art performance (Bengtson and Roth, 2008). The mention-pair model splits the task into three parts: mention detection, pairwise classification and mention clustering. Mention detection aims to identify anaphoric noun phrases, including proper nouns, common noun phrases and pronouns. Pairwise classification takes a pair of detected anaphoric noun In this paper, we study how to use a different learning framework, Markov logic (Richardson and Domingos, 2006), to learn a joint model for both pairwise classification and mention clustering under the mention-pair model. We choose Markov logic because of its appealing properties. Markov logi"
D12-1114,W11-1904,0,0.0357831,".20 51.46 57.50 54.53 - MUC P 64.05 64.10 68.52 68.40 59.10 62.25 - F 60.89 60.90 59.26 58.73 58.30 58.13 55.8 R 67.11 67.12 60.85 59.79 71.00 63.72 - B-cube P 73.88 74.13 80.15 81.69 69.20 73.83 - F 70.33 70.45 69.18 69.04 70.10 68.40 69.29 R 47.6 47.70 51.6 53.03 48.10 47.20 - CEAF P 41.92 41.96 37.05 37.84 46.50 40.01 - F 44.58 44.65 43.13 44.17 47.30 43.31 43.96 Avg F 58.60 58.67 57.19 57.31 58.60 56.61 56.35 Table 4: Comparisons with state-of-the-art systems on the development dataset. MaxEnt+BF and MaxEnt+Trans. They also outperform the learning-based systems of Sapena et al. (2011) and Chang et al. (2011), and perform competitively with Lee’s system (Lee et al., 2011). Note that Lee’s system is purely rule-based, while our methods are developed in a theoretically sound way, i.e., Markov logic framework. 5 Related Work Supervised noun phrase coreference resolution has been extensively studied. Besides the mention-pair model, two other commonly used models are the entity-mention model (Luo et al., 2004; Yang et al., 2008) and ranking models (Denis and Baldridge, 2008; Rahman and Ng, 2009). Interested readers can refer to the literature review by Ng (2010). Under the mention-pair model, Klenner ("
D12-1114,C10-1019,0,0.0216608,"graphical models. Finley and Joachims (2005) proposed a general SVM-based framework for supervised clustering that learns item-pair similarity measures, and applied the framework to noun phrase 1252 coreference resolution. In our work, we take a different approach and apply Markov logic. As we have shown in Section 3, given the flexibility of Markov logic, it is straightforward to perform joint learning of pairwise classification and mention clustering. In recent years, Markov logic has been widely used in natural language processing problems (Poon and Domingos, 2009; Yoshikawa et al., 2009; Che and Liu, 2010). For coreference resolution, the most notable one is unsupervised coreference resolution by Poon and Domingos (2008). Poon and Domingos (2008) followed the entity-mention model while we follow the mention-pair model, which are quite different approaches. To seek good performance in an unsupervised way, Poon and Domingos (2008) highly rely on two important strong indicators: appositives and predicate nominatives. However, OntoNotes corpus (state-of-art NLP data collection) on coreference layer for CoNLL-2011 has excluded these two conditions of annotations (appositives and predicate nominative"
D12-1114,H05-1013,0,0.0639415,"Missing"
D12-1114,N07-1030,0,0.114892,"m joint learning, at the inference stage, they still make pairwise coreference decisions and cluster mentions sequentially. Unlike their method, We formulate the two steps into a single framework. Besides combining pairwise classification and mention clustering, there has also been some work that jointly performs mention detection and coreference resolution. Daum´e and Marcu (2005) developed such a model based on the Learning as Search Optimization (LaSO) framework. Rahman and Ng (2009) proposed to learn a cluster-ranker for discourse-new mention detection jointly with coreference resolution. Denis and Baldridge (2007) adopted an Integer Linear Programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task. 6 Conclusion In this paper we present a joint learning method with Markov logic which naturally combines pairwise classification and mention clustering. Experimental results show that the joint learning method significantly outperforms baseline methods. Our method is also better than all the learning-based systems in CoNLL-2011 and reaches the same level of performance with the best system. In the future we will try to design more global constraints and"
D12-1114,D08-1069,0,0.0395601,"tems on the development dataset. MaxEnt+BF and MaxEnt+Trans. They also outperform the learning-based systems of Sapena et al. (2011) and Chang et al. (2011), and perform competitively with Lee’s system (Lee et al., 2011). Note that Lee’s system is purely rule-based, while our methods are developed in a theoretically sound way, i.e., Markov logic framework. 5 Related Work Supervised noun phrase coreference resolution has been extensively studied. Besides the mention-pair model, two other commonly used models are the entity-mention model (Luo et al., 2004; Yang et al., 2008) and ranking models (Denis and Baldridge, 2008; Rahman and Ng, 2009). Interested readers can refer to the literature review by Ng (2010). Under the mention-pair model, Klenner (2007) and Finkel and Manning (2008) applied Integer Linear Programming (ILP) to enforce transitivity on the pairwise classification results. Chang et al. (2011) used the same ILP technique to incorporate best-first clustering and generate the mention clusters. In all these studies, however, mention clustering is combined with pairwise classification only at the inference stage but not at the learning stage. To perform joint learning of pairwise classification and m"
D12-1114,P08-2012,0,0.439324,"s used for unsupervised coreference resolution and the method was based on a different model, the entity-mention model. More specifically, to combine mention clustering with pairwise classification, we adopt the commonly used strategies (such as best-first clustering and transitivity constraint), and formulate them as first-order logic formulas under the Markov logic framework. Best-first clustering has been previously studied by Ng and Cardie (2002) and Bengtson and Roth (2008) and found to be effective. Transitivity constraint has been applied to coreference resolution by Klenner (2007) and Finkel and Manning (2008), and also achieved good performance. We evaluate Markov logic-based method on the dataset from CoNLL-2011 shared task. Our experiment results demonstrate the advantage of joint learning of pairwise classification and mention clustering over independent learning. We examine best-first clustering and transitivity constraint in our methods, and find that both are very useful for coreference resolution. Compared with the state of the art, our method outperforms a baseline that represents a typical system using the mention-pair model. Our method is also better than all learning systems from the Co"
D12-1114,W11-1902,0,0.149477,"Missing"
D12-1114,P04-1018,0,0.0629881,"6.61 56.35 Table 4: Comparisons with state-of-the-art systems on the development dataset. MaxEnt+BF and MaxEnt+Trans. They also outperform the learning-based systems of Sapena et al. (2011) and Chang et al. (2011), and perform competitively with Lee’s system (Lee et al., 2011). Note that Lee’s system is purely rule-based, while our methods are developed in a theoretically sound way, i.e., Markov logic framework. 5 Related Work Supervised noun phrase coreference resolution has been extensively studied. Besides the mention-pair model, two other commonly used models are the entity-mention model (Luo et al., 2004; Yang et al., 2008) and ranking models (Denis and Baldridge, 2008; Rahman and Ng, 2009). Interested readers can refer to the literature review by Ng (2010). Under the mention-pair model, Klenner (2007) and Finkel and Manning (2008) applied Integer Linear Programming (ILP) to enforce transitivity on the pairwise classification results. Chang et al. (2011) used the same ILP technique to incorporate best-first clustering and generate the mention clusters. In all these studies, however, mention clustering is combined with pairwise classification only at the inference stage but not at the learning"
D12-1114,H05-1004,0,0.235132,"rom the CoNLL-2011 shared task, “Modeling Unrestricted Coreference in OntoNotes” (Pradhan et al., 2011)2 . It uses the English portion of the OntoNotes v4.0 corpus. There are three important differences between OntoNotes 2 4.2 Evaluation Metrics We use the same evaluation metrics as used in CoNLL-2011. Specifically, for mention detection, we use precision, recall and the F-measure. A mention is considered to be correct only if it matches the exact same span of characters in the annotation key. For coreference resolution, MUC (Vilain et al., 1995), B-CUBED (Bagga and Baldwin, 1998) and CEAF-E (Luo, 2005) are used for evaluation. The unweighted average F score of them is used to compare different systems. 4.3 The Effect of Joint Learning In this section, we will first describe the dataset and evaluation metrics we use. We will then present the effect of our joint learning method, and finally discuss the comparison with the state of the art. 4.1 and another well-known coreference dataset from ACE. First, OntoNotes does not label any singleton entity cluster, which has only one reference in the text. Second, only identity coreference is tagged in OntoNotes, but not appositives or predicate nomin"
D12-1114,P02-1014,0,0.826267,"ng methods as an isolated inference procedure at the end. We propose a joint learning model which combines pairwise classification and mention clustering with Markov logic. Experimental results show that our joint learning system outperforms independent learning systems. Our system gives a better performance than all the learning-based systems from the CoNLL-2011 shared task on the same dataset. Compared with the best system from CoNLL2011, which employs a rule-based method, our system shows competitive performance. 1 Much work has been done following the mentionpair model (Soon et al., 2001; Ng and Cardie, 2002). In most work, pairwise classification and mention clustering are done sequentially. A major weakness of this approach is that pairwise classification considers only local information, which may not be sufficient to make correct decisions. One way to address this weakness is to jointly learn the pairwise classification model and the mention clustering model. This idea has been explored to some extent by McCallum and Wellner (2005) using conditional undirected graphical models and by Finley and Joachims (2005) using an SVM-based supervised clustering method. Introduction The task of noun phras"
D12-1114,P10-1142,0,0.0271294,"ms of Sapena et al. (2011) and Chang et al. (2011), and perform competitively with Lee’s system (Lee et al., 2011). Note that Lee’s system is purely rule-based, while our methods are developed in a theoretically sound way, i.e., Markov logic framework. 5 Related Work Supervised noun phrase coreference resolution has been extensively studied. Besides the mention-pair model, two other commonly used models are the entity-mention model (Luo et al., 2004; Yang et al., 2008) and ranking models (Denis and Baldridge, 2008; Rahman and Ng, 2009). Interested readers can refer to the literature review by Ng (2010). Under the mention-pair model, Klenner (2007) and Finkel and Manning (2008) applied Integer Linear Programming (ILP) to enforce transitivity on the pairwise classification results. Chang et al. (2011) used the same ILP technique to incorporate best-first clustering and generate the mention clusters. In all these studies, however, mention clustering is combined with pairwise classification only at the inference stage but not at the learning stage. To perform joint learning of pairwise classification and mention clustering, in (McCallum and Wellner, 2005), each mention pair corresponds to a bin"
D12-1114,D08-1068,0,0.455659,"how to use a different learning framework, Markov logic (Richardson and Domingos, 2006), to learn a joint model for both pairwise classification and mention clustering under the mention-pair model. We choose Markov logic because of its appealing properties. Markov logic is based on first-order logic, which makes the learned models readily interpretable by humans. Moreover, joint learning is natural under the Markov logic framework, with local pairwise classification and global mention clustering both formulated as weighted first-order clauses. In fact, Markov logic has been previously used by Poon and Domingos (2008) for coreference resolution and achieved good 1245 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1245–1254, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics results, but it was used for unsupervised coreference resolution and the method was based on a different model, the entity-mention model. More specifically, to combine mention clustering with pairwise classification, we adopt the commonly used strategies (such as best-first clustering and transitivity const"
D12-1114,D09-1001,0,0.030231,"se variables is modeled by conditional undirected graphical models. Finley and Joachims (2005) proposed a general SVM-based framework for supervised clustering that learns item-pair similarity measures, and applied the framework to noun phrase 1252 coreference resolution. In our work, we take a different approach and apply Markov logic. As we have shown in Section 3, given the flexibility of Markov logic, it is straightforward to perform joint learning of pairwise classification and mention clustering. In recent years, Markov logic has been widely used in natural language processing problems (Poon and Domingos, 2009; Yoshikawa et al., 2009; Che and Liu, 2010). For coreference resolution, the most notable one is unsupervised coreference resolution by Poon and Domingos (2008). Poon and Domingos (2008) followed the entity-mention model while we follow the mention-pair model, which are quite different approaches. To seek good performance in an unsupervised way, Poon and Domingos (2008) highly rely on two important strong indicators: appositives and predicate nominatives. However, OntoNotes corpus (state-of-art NLP data collection) on coreference layer for CoNLL-2011 has excluded these two conditions of annot"
D12-1114,W11-1901,0,0.309737,"tion y ′ (i.e. the one with the highest score s(y ′ , xi ), equivalent to yˆ in Section 3.3)) is at least as big as the loss L(yi , y ′ ), while changing wt−1 as little as possible. The number of false ground atoms of coref predicate is selected as loss function in our experiments. Hard global constraints (i.e. best-first clustering or transitivity constraint) must be satisfied when inferring the best y ′ in each iteration, which can make learned weights more effective. 4 Experiments Data Set We use the dataset from the CoNLL-2011 shared task, “Modeling Unrestricted Coreference in OntoNotes” (Pradhan et al., 2011)2 . It uses the English portion of the OntoNotes v4.0 corpus. There are three important differences between OntoNotes 2 4.2 Evaluation Metrics We use the same evaluation metrics as used in CoNLL-2011. Specifically, for mention detection, we use precision, recall and the F-measure. A mention is considered to be correct only if it matches the exact same span of characters in the annotation key. For coreference resolution, MUC (Vilain et al., 1995), B-CUBED (Bagga and Baldwin, 1998) and CEAF-E (Luo, 2005) are used for evaluation. The unweighted average F score of them is used to compare different"
D12-1114,D09-1101,0,0.271277,"aset. MaxEnt+BF and MaxEnt+Trans. They also outperform the learning-based systems of Sapena et al. (2011) and Chang et al. (2011), and perform competitively with Lee’s system (Lee et al., 2011). Note that Lee’s system is purely rule-based, while our methods are developed in a theoretically sound way, i.e., Markov logic framework. 5 Related Work Supervised noun phrase coreference resolution has been extensively studied. Besides the mention-pair model, two other commonly used models are the entity-mention model (Luo et al., 2004; Yang et al., 2008) and ranking models (Denis and Baldridge, 2008; Rahman and Ng, 2009). Interested readers can refer to the literature review by Ng (2010). Under the mention-pair model, Klenner (2007) and Finkel and Manning (2008) applied Integer Linear Programming (ILP) to enforce transitivity on the pairwise classification results. Chang et al. (2011) used the same ILP technique to incorporate best-first clustering and generate the mention clusters. In all these studies, however, mention clustering is combined with pairwise classification only at the inference stage but not at the learning stage. To perform joint learning of pairwise classification and mention clustering, in"
D12-1114,W11-1903,0,0.0338164,"Missing"
D12-1114,J01-4004,0,0.681245,"the pairwise learning methods as an isolated inference procedure at the end. We propose a joint learning model which combines pairwise classification and mention clustering with Markov logic. Experimental results show that our joint learning system outperforms independent learning systems. Our system gives a better performance than all the learning-based systems from the CoNLL-2011 shared task on the same dataset. Compared with the best system from CoNLL2011, which employs a rule-based method, our system shows competitive performance. 1 Much work has been done following the mentionpair model (Soon et al., 2001; Ng and Cardie, 2002). In most work, pairwise classification and mention clustering are done sequentially. A major weakness of this approach is that pairwise classification considers only local information, which may not be sufficient to make correct decisions. One way to address this weakness is to jointly learn the pairwise classification model and the mention clustering model. This idea has been explored to some extent by McCallum and Wellner (2005) using conditional undirected graphical models and by Finley and Joachims (2005) using an SVM-based supervised clustering method. Introduction"
D12-1114,W11-1908,0,0.0127817,"ion and mention clustering. 2.1 Mention Detection For mention detection, traditional methods include learning-based and rule-based methods. Which kind of method to choose depends on specific dataset. In 1246 this paper, we first consider all the noun phrases in the given text as candidate mentions. Without gold standard mention boundaries, we use a well-known preprocessing tool from Stanford’s NLP group1 to extract noun phrases. After obtaining all the extracted noun phrases, we also use a rule-based method to remove some erroneous candidates based on previous studies (e.g. Lee et al. (2011), Uryupina et al. (2011)). Some examples of these erroneous candidates include stop words (e.g. uh, hmm), web addresses (e.g. http://www.google.com), numbers (e.g. $9,000) and pleonastic “it” pronouns. 2.2 Pairwise Classification For pairwise classification, traditional learningbased methods usually adopt a classification model such as maximum entropy models and support vector machines. Training instances (i.e. positive and negative mention pairs) are constructed from known coreference chains, and features are defined to represent these instances. In this paper, we build a baseline system that uses maximum entropy mo"
D12-1114,M95-1005,0,0.527813,"weights more effective. 4 Experiments Data Set We use the dataset from the CoNLL-2011 shared task, “Modeling Unrestricted Coreference in OntoNotes” (Pradhan et al., 2011)2 . It uses the English portion of the OntoNotes v4.0 corpus. There are three important differences between OntoNotes 2 4.2 Evaluation Metrics We use the same evaluation metrics as used in CoNLL-2011. Specifically, for mention detection, we use precision, recall and the F-measure. A mention is considered to be correct only if it matches the exact same span of characters in the annotation key. For coreference resolution, MUC (Vilain et al., 1995), B-CUBED (Bagga and Baldwin, 1998) and CEAF-E (Luo, 2005) are used for evaluation. The unweighted average F score of them is used to compare different systems. 4.3 The Effect of Joint Learning In this section, we will first describe the dataset and evaluation metrics we use. We will then present the effect of our joint learning method, and finally discuss the comparison with the state of the art. 4.1 and another well-known coreference dataset from ACE. First, OntoNotes does not label any singleton entity cluster, which has only one reference in the text. Second, only identity coreference is t"
D12-1114,P08-1096,0,0.0425499,": Comparisons with state-of-the-art systems on the development dataset. MaxEnt+BF and MaxEnt+Trans. They also outperform the learning-based systems of Sapena et al. (2011) and Chang et al. (2011), and perform competitively with Lee’s system (Lee et al., 2011). Note that Lee’s system is purely rule-based, while our methods are developed in a theoretically sound way, i.e., Markov logic framework. 5 Related Work Supervised noun phrase coreference resolution has been extensively studied. Besides the mention-pair model, two other commonly used models are the entity-mention model (Luo et al., 2004; Yang et al., 2008) and ranking models (Denis and Baldridge, 2008; Rahman and Ng, 2009). Interested readers can refer to the literature review by Ng (2010). Under the mention-pair model, Klenner (2007) and Finkel and Manning (2008) applied Integer Linear Programming (ILP) to enforce transitivity on the pairwise classification results. Chang et al. (2011) used the same ILP technique to incorporate best-first clustering and generate the mention clusters. In all these studies, however, mention clustering is combined with pairwise classification only at the inference stage but not at the learning stage. To perform j"
D12-1114,P09-1046,0,0.0740571,"y conditional undirected graphical models. Finley and Joachims (2005) proposed a general SVM-based framework for supervised clustering that learns item-pair similarity measures, and applied the framework to noun phrase 1252 coreference resolution. In our work, we take a different approach and apply Markov logic. As we have shown in Section 3, given the flexibility of Markov logic, it is straightforward to perform joint learning of pairwise classification and mention clustering. In recent years, Markov logic has been widely used in natural language processing problems (Poon and Domingos, 2009; Yoshikawa et al., 2009; Che and Liu, 2010). For coreference resolution, the most notable one is unsupervised coreference resolution by Poon and Domingos (2008). Poon and Domingos (2008) followed the entity-mention model while we follow the mention-pair model, which are quite different approaches. To seek good performance in an unsupervised way, Poon and Domingos (2008) highly rely on two important strong indicators: appositives and predicate nominatives. However, OntoNotes corpus (state-of-art NLP data collection) on coreference layer for CoNLL-2011 has excluded these two conditions of annotations (appositives and"
D12-1114,D08-1067,0,\N,Missing
D12-1134,D11-1145,0,0.0272091,"urst detection such as the χ2 test based method (Swan and Allan, 2000), the parameter-free method (Fung et al., 2005) and moving average method (Vlachos et al., 2004). Our work is related to the applications of these burst detection algorithms for event detection (He et al., 2007; Fung et al., 2007b; Shan et al., 2012; Zhao et al., 2012). 11 All experiments are tested in a Mac PC, 2.4GHz Intel Core 2 Duo. Some recent work try to identify hot trends (Mathioudakis and Koudas, 2010; Zubiaga et al., 2011; Budak et al., 2011; Naaman et al., 2011) or make use of the burstiness (Sakaki et al., 2010; Aramki et al., 2011; Marcus et al., 2011) in social media. However, few of these methods consider modeling the local smoothness of one state sequence in a systematic way and often use a fixed window length of 2. Little work considers making use of different types of social media activities for burst detection. (Yao et al., 2010; Kotov et al., 2011; Wang et al., 2007; Wang et al., 2009) conducted some preliminary studies of mining correlated bursty patterns from multiple sources. However, they either highly relies on high-quality training datasets or require a considerable amount of training time. Online social a"
D12-1134,P12-2009,1,0.817171,"sults are shown in Figure 4, and we can see M Burst+DISJUNCT is consistently better than single-best. 5 Related Work Our work is related to burst detection from text streams. Pioneered by the automaton model proposed in (Kleinberg, 2003), many techniques have been proposed for burst detection such as the χ2 test based method (Swan and Allan, 2000), the parameter-free method (Fung et al., 2005) and moving average method (Vlachos et al., 2004). Our work is related to the applications of these burst detection algorithms for event detection (He et al., 2007; Fung et al., 2007b; Shan et al., 2012; Zhao et al., 2012). 11 All experiments are tested in a Mac PC, 2.4GHz Intel Core 2 Duo. Some recent work try to identify hot trends (Mathioudakis and Koudas, 2010; Zubiaga et al., 2011; Budak et al., 2011; Naaman et al., 2011) or make use of the burstiness (Sakaki et al., 2010; Aramki et al., 2011; Marcus et al., 2011) in social media. However, few of these methods consider modeling the local smoothness of one state sequence in a systematic way and often use a fixed window length of 2. Little work considers making use of different types of social media activities for burst detection. (Yao et al., 2010; Kotov et"
D13-1191,D10-1111,0,0.11784,"Missing"
D13-1191,W11-1701,0,0.0183132,"Missing"
D13-1191,P12-2041,0,0.0657852,"Missing"
D13-1191,P12-2013,0,0.0241432,"Missing"
D13-1191,P05-1045,0,0.0024014,"late elements such as navigation and advertisments) using Boilerpipe (Kohlsch¨utter et al., 2010).5 We tokenized the text and filtered stopwords.6 We considered both unigrams and bigrams in our model, keeping all unigrams and removing bigram types that appeared less than 5 times in the corpus. Although our modeling approach ultimately treats texts as bags of terms (unigrams and bigrams), one important preprocessing step was taken to further improve the interpretability of the inferred representation: named entity mentions of persons. We identified these mentions of persons using Stanford NER (Finkel et al., 2005) and treated each person mention as a single token. In our qualitative analysis of the model (§4.2), we will show how this special treatment of person mentions enables the association of well-known individuals with debate topics. Though not part of our experimental evaluation in this paper, such associations are, we believe, an interesting direction for future applications of the model. 3 Model Our model defines a probability distribution over terms7 that are observed in the corpus. Each term occurs in a context defined by the tuple hd, q, s, ai (respectively, a debate, a question within the d"
D13-1191,C08-1031,0,0.014975,"Missing"
D13-1191,N09-1057,0,0.058725,"Missing"
D13-1191,W02-1011,0,0.0141142,"Missing"
D13-1191,D10-1007,0,0.01089,"Missing"
D13-1191,D13-1010,1,0.351102,"Missing"
D13-1191,P09-1026,0,0.102041,"Missing"
D13-1191,H05-2018,0,0.0077349,"o a position. b. ∀ questions q in d: i. Draw topic mixture proportions θ d,q ∼ Dirichlet(α). ii. ∀ arguments a under question q and term positions n in a: A. Draw topic label zd,q,s,a ∼ Multinomial(θ d,q ). B. Draw functional term type yd,q,s,a ∼ Multinomial(µ). C. Draw term wd,q,s,a ∼ Multinomial (φyd,q,s,a |id,1 , id,2 , zd,q,s,a ). Figure 2: Generative story for our model of Debatepedia. apply empirical Bayesian techniques to estimate the hyperparameters. Motivated by past efforts to exploit prior knowledge (Zhao et al., 2010; Lin and He, 2009), we use the OpinionFinder sentiment lexicon9 (Wilson et al., 2005) to construct η i and η o . Specifically, terms w in the lexicon were given pai = η o = 0.01, and other terms were rameters ηw w i o = 0.001, capturing our prior belief given ηw = ηw that opinion-expressing terms are likely to be used in expressing positions. 5,451 types were given a “boost” through this prior. Information retrieval has long exploited the observation that a term’s document frequency (i.e., the number of documents a term occurs in) is inversely related its usefulness in retrieval (Jones, 1972). We encode this in η b , the prior over the background term distribution, by setting"
D13-1191,D09-1159,0,0.0174039,"Missing"
D13-1191,D10-1006,1,0.196152,"debates d: a. Draw id,1 , id,2 ∼ Multinomial(ι), assigning each of the two sides to a position. b. ∀ questions q in d: i. Draw topic mixture proportions θ d,q ∼ Dirichlet(α). ii. ∀ arguments a under question q and term positions n in a: A. Draw topic label zd,q,s,a ∼ Multinomial(θ d,q ). B. Draw functional term type yd,q,s,a ∼ Multinomial(µ). C. Draw term wd,q,s,a ∼ Multinomial (φyd,q,s,a |id,1 , id,2 , zd,q,s,a ). Figure 2: Generative story for our model of Debatepedia. apply empirical Bayesian techniques to estimate the hyperparameters. Motivated by past efforts to exploit prior knowledge (Zhao et al., 2010; Lin and He, 2009), we use the OpinionFinder sentiment lexicon9 (Wilson et al., 2005) to construct η i and η o . Specifically, terms w in the lexicon were given pai = η o = 0.01, and other terms were rameters ηw w i o = 0.001, capturing our prior belief given ηw = ηw that opinion-expressing terms are likely to be used in expressing positions. 5,451 types were given a “boost” through this prior. Information retrieval has long exploited the observation that a term’s document frequency (i.e., the number of documents a term occurs in) is inversely related its usefulness in retrieval (Jones, 1972)"
D13-1191,P12-1042,0,\N,Missing
D13-1192,P12-1056,1,0.527626,"larization helps find more meaningful events; the event-topic affinity vectors improve an event recommendation task and helps produce a meaningful organization of events by topics. 2 Related Work Study of topics, events and users on Twitter is related to several branches of work. We review the most interesting and relevant work below. Event detection on Twitter: There have been quite a few studies in this direction in recent years, including both online detection (Sakaki et al., 2010; 1870 Petrovi´c et al., 2010; Weng and Lee, 2011; Becker et al., 2011; Li et al., 2012) and offline detection (Diao et al., 2012). Online detection is mostly concerned with early detection of major events, so efficiency of the algorithms is the main focus. These algorithms do not aim to identify all relevant tweets, nor do they analyze the association of events with topics. In comparison, our work focuses on modeling topics, events and users as well as their relation. Recently, Petrovi´c et al. (2013) pointed out that Twitter stream does not lead news stream for major news events, but Twitter stream covers a much wider range of events than news stream. Our work helps better understand these additional events on Twitter"
D13-1192,C12-1093,0,0.0489588,"Missing"
D13-1192,N10-1021,0,0.181123,"Missing"
D16-1023,W06-1615,0,0.595274,"are learned sequentially, we propose to jointly learn the hidden feature representation together with the sentiment classification model itself, and we show that joint learning works better than sequential learning. We conduct experiments on a number of different source and target domains for sentence-level sentiment classification. We show that our proposed method is able to achieve the best performance compared with a number of baselines for most of these domain pairs. 2 Related Work Domain Adaptation: Domain adaptation is a general problem in NLP and has been well studied in recent years (Blitzer et al., 2006; Daum´e III, 2007; Jiang and Zhai, 2007; Dredze and Crammer, 237 2008; Titov, 2011; Yu and Jiang, 2015). For sentiment classification, most existing domain adaptation methods are based on traditional discrete feature representations and linear classifiers. One line of work focuses on inducing a general lowdimensional cross-domain representation based on the co-occurrences of domain-specific and domainindependent features (Blitzer et al., 2007; Pan et al., 2010; Pan et al., 2011). Another line of work tries to derive domain-specific sentiment words (Bollegala et al., 2011; Li et al., 2012). Ou"
D16-1023,P07-1056,0,0.995403,"fei Yu School of Information Systems Singapore Management University jfyu.2014@phdis.smu.edu.sg Abstract et al., 2007; Pan et al., 2010; Bollegala et al., 2011; Ponomareva and Thelwall, 2012; Bollegala et al., 2016). Among them, an appealing method is the Structural Correspondence Learning (SCL) method (Blitzer et al., 2007), which uses pivot feature prediction tasks to induce a projected feature space that works well for both the source and the target domains. The intuition behind is that these pivot prediction tasks are highly correlated with the original task. For sentiment classification, Blitzer et al. (2007) first chose pivot words which have high mutual information with the sentiment labels, and then set up the pivot prediction tasks to be the predictions of each of these pivot words using the other words. In this paper, we study cross-domain sentiment classification with neural network architectures. We borrow the idea from Structural Correspondence Learning and use two auxiliary tasks to help induce a sentence embedding that supposedly works well across domains for sentiment classification. We also propose to jointly learn this sentence embedding together with the sentiment classifier itself."
D16-1023,P11-1014,0,0.057858,"tudied in recent years (Blitzer et al., 2006; Daum´e III, 2007; Jiang and Zhai, 2007; Dredze and Crammer, 237 2008; Titov, 2011; Yu and Jiang, 2015). For sentiment classification, most existing domain adaptation methods are based on traditional discrete feature representations and linear classifiers. One line of work focuses on inducing a general lowdimensional cross-domain representation based on the co-occurrences of domain-specific and domainindependent features (Blitzer et al., 2007; Pan et al., 2010; Pan et al., 2011). Another line of work tries to derive domain-specific sentiment words (Bollegala et al., 2011; Li et al., 2012). Our proposed method is similar to the first line of work in that we also aim to learn a general, cross-domain representation (sentence embeddings in our case). Neural Networks for Sentiment Classification: A recent trend of deep learning enhances various kinds of neural network models for sentiment classification, including Convolutional Neural Networks (CNNs), Recursive Neural Network (ReNNs) and Recurrent Neural Network (RNNs), which have been shown to achieve competitive results across different benchmarks (Socher et al., 2013; Dong et al., 2014a; Dong et al., 2014b; Kim"
D16-1023,P15-1071,0,0.0870346,"; Yang and Eisenstein, 2014). They use Stacked Denoising Auto-encoders (SDA) to induce a hidden representation that presumably works well across domains. However, SDA is fully unsupervised and does not consider the end task we need to solve, i.e., the sentiment classification task. In contrast, the idea behind SCL is to use carefullychosen auxiliary tasks that correlate with the end task to induce a hidden representation. Another line of work aims to learn a low dimensional representation for each feature in both domains based on predicting its neighboring features (Yang and Eisenstein, 2015; Bollegala et al., 2015). Different from these methods, we aim to directly learn sentence embeddings that work well across domains. In this paper, we aim to extend the main idea behind SCL to neural network-based solutions to sentiment classification to address the domain adaptation problem. Specifically, we borrow the idea of using pivot prediction tasks from SCL. But instead of learning thousands of pivot predictors and performing singular value decomposition on the learned weights, which all relies on linear transformations, we introduce only two auxiliary binary prediction tasks and directly learn a non-linear tr"
D16-1023,D08-1083,0,0.0182276,"jointly learn this sentence embedding together with the sentiment classifier itself. Experiment results demonstrate that our proposed joint model outperforms several state-of-theart methods on five benchmark datasets. 1 Jing Jiang School of Information Systems Singapore Management University jingjiang@smu.edu.sg Introduction With the growing need of correctly identifying the sentiments expressed in subjective texts such as product reviews, sentiment classification has received continuous attention in the NLP community for over a decade (Pang et al., 2002; Pang and Lee, 2004; Hu and Liu, 2004; Choi and Cardie, 2008; Nakagawa et al., 2010). One of the big challenges of sentiment classification is how to adapt a sentiment classifier trained on one domain to a different new domain. This is because sentiments are often expressed with domain-specific words and expressions. For example, in the Movie domain, words such as moving and engaging are usually positive, but they may not be relevant in the Restaurant domain. Since labeled data is expensive to obtain, it would be very useful if we could adapt a model trained on a source domain to a target domain. Much work has been done in sentiment analysis to address"
D16-1023,P07-1033,0,0.0371925,"Missing"
D16-1023,P14-2009,0,0.134188,"vant in the Restaurant domain. Since labeled data is expensive to obtain, it would be very useful if we could adapt a model trained on a source domain to a target domain. Much work has been done in sentiment analysis to address this domain adaptation problem (Blitzer However, the original SCL method is based on traditional discrete feature representations and linear classifiers. In recent years, with the advances of deep learning in NLP, multi-layer neural network models such as RNNs and CNNs have been widely used in sentiment classification and achieved good performance (Socher et al., 2013; Dong et al., 2014a; Dong et al., 2014b; Kim, 2014; Tang et al., 2015). In these models, dense, real-valued feature vectors and non-linear classification functions are used. By using real-valued word embeddings pre-trained from a large corpus, these models can take advantage of the embedding space that presumably better captures the syntactic and semantic similarities between words. And by using non-linear functions through multi-layer neural networks, these models represent a more expressive hypothesis space. Therefore, it would be interesting to explore how these neural network models could be extended for cr"
D16-1023,D08-1072,0,0.0602374,"Missing"
D16-1023,P07-1034,1,0.892054,"jointly learn the hidden feature representation together with the sentiment classification model itself, and we show that joint learning works better than sequential learning. We conduct experiments on a number of different source and target domains for sentence-level sentiment classification. We show that our proposed method is able to achieve the best performance compared with a number of baselines for most of these domain pairs. 2 Related Work Domain Adaptation: Domain adaptation is a general problem in NLP and has been well studied in recent years (Blitzer et al., 2006; Daum´e III, 2007; Jiang and Zhai, 2007; Dredze and Crammer, 237 2008; Titov, 2011; Yu and Jiang, 2015). For sentiment classification, most existing domain adaptation methods are based on traditional discrete feature representations and linear classifiers. One line of work focuses on inducing a general lowdimensional cross-domain representation based on the co-occurrences of domain-specific and domainindependent features (Blitzer et al., 2007; Pan et al., 2010; Pan et al., 2011). Another line of work tries to derive domain-specific sentiment words (Bollegala et al., 2011; Li et al., 2012). Our proposed method is similar to the firs"
D16-1023,D14-1181,0,0.0972511,"eled data is expensive to obtain, it would be very useful if we could adapt a model trained on a source domain to a target domain. Much work has been done in sentiment analysis to address this domain adaptation problem (Blitzer However, the original SCL method is based on traditional discrete feature representations and linear classifiers. In recent years, with the advances of deep learning in NLP, multi-layer neural network models such as RNNs and CNNs have been widely used in sentiment classification and achieved good performance (Socher et al., 2013; Dong et al., 2014a; Dong et al., 2014b; Kim, 2014; Tang et al., 2015). In these models, dense, real-valued feature vectors and non-linear classification functions are used. By using real-valued word embeddings pre-trained from a large corpus, these models can take advantage of the embedding space that presumably better captures the syntactic and semantic similarities between words. And by using non-linear functions through multi-layer neural networks, these models represent a more expressive hypothesis space. Therefore, it would be interesting to explore how these neural network models could be extended for cross-domain sentiment classificat"
D16-1023,P09-1078,0,0.0472196,"ion Recall that the two auxiliary tasks depend on two domain-independent sentiment word lists, i.e., pivot word lists. Different from Blitzer et al. (2007), we employ weighted log-likelihood ratio (WLLR) to select the most positive and negative words in both domains as pivots. The reason is that in our preliminary experiments we observe that mutual information (used by Blitzer et al. (2007)) is biased towards low frequency words. Some high frequency words including good and great are scored low. In comparison, WLLR does not have this issue. The same observation was also reported previously by Li et al. (2009). More specifically, we first tokenize the sentences in Ds and Dt and perform part-of-speech tagging using the NLTK toolkit. Next, we extract only adjectives, adverbs and verbs with a frequency of at least 3 in the source domain and at least 3 in the target domain. We also remove negation words such as not and stop words using a stop word list. We then measure each remaining candidate word’s relevance to the positive and the negative classes based on Ds 240 r(w, y) = p˜(w|y) log p˜(w|y) , p˜(w|¯ y) where w is a word, y ∈ {+, −} is a sentiment label, y¯ is the opposite label of y, and p˜(w|y) i"
D16-1023,P12-1043,0,0.0596108,"Blitzer et al., 2006; Daum´e III, 2007; Jiang and Zhai, 2007; Dredze and Crammer, 237 2008; Titov, 2011; Yu and Jiang, 2015). For sentiment classification, most existing domain adaptation methods are based on traditional discrete feature representations and linear classifiers. One line of work focuses on inducing a general lowdimensional cross-domain representation based on the co-occurrences of domain-specific and domainindependent features (Blitzer et al., 2007; Pan et al., 2010; Pan et al., 2011). Another line of work tries to derive domain-specific sentiment words (Bollegala et al., 2011; Li et al., 2012). Our proposed method is similar to the first line of work in that we also aim to learn a general, cross-domain representation (sentence embeddings in our case). Neural Networks for Sentiment Classification: A recent trend of deep learning enhances various kinds of neural network models for sentiment classification, including Convolutional Neural Networks (CNNs), Recursive Neural Network (ReNNs) and Recurrent Neural Network (RNNs), which have been shown to achieve competitive results across different benchmarks (Socher et al., 2013; Dong et al., 2014a; Dong et al., 2014b; Kim, 2014; Tang et al"
D16-1023,N10-1120,0,0.021368,"ence embedding together with the sentiment classifier itself. Experiment results demonstrate that our proposed joint model outperforms several state-of-theart methods on five benchmark datasets. 1 Jing Jiang School of Information Systems Singapore Management University jingjiang@smu.edu.sg Introduction With the growing need of correctly identifying the sentiments expressed in subjective texts such as product reviews, sentiment classification has received continuous attention in the NLP community for over a decade (Pang et al., 2002; Pang and Lee, 2004; Hu and Liu, 2004; Choi and Cardie, 2008; Nakagawa et al., 2010). One of the big challenges of sentiment classification is how to adapt a sentiment classifier trained on one domain to a different new domain. This is because sentiments are often expressed with domain-specific words and expressions. For example, in the Movie domain, words such as moving and engaging are usually positive, but they may not be relevant in the Restaurant domain. Since labeled data is expensive to obtain, it would be very useful if we could adapt a model trained on a source domain to a target domain. Much work has been done in sentiment analysis to address this domain adaptation"
D16-1023,P04-1035,0,0.0409361,"nt classification. We also propose to jointly learn this sentence embedding together with the sentiment classifier itself. Experiment results demonstrate that our proposed joint model outperforms several state-of-theart methods on five benchmark datasets. 1 Jing Jiang School of Information Systems Singapore Management University jingjiang@smu.edu.sg Introduction With the growing need of correctly identifying the sentiments expressed in subjective texts such as product reviews, sentiment classification has received continuous attention in the NLP community for over a decade (Pang et al., 2002; Pang and Lee, 2004; Hu and Liu, 2004; Choi and Cardie, 2008; Nakagawa et al., 2010). One of the big challenges of sentiment classification is how to adapt a sentiment classifier trained on one domain to a different new domain. This is because sentiments are often expressed with domain-specific words and expressions. For example, in the Movie domain, words such as moving and engaging are usually positive, but they may not be relevant in the Restaurant domain. Since labeled data is expensive to obtain, it would be very useful if we could adapt a model trained on a source domain to a target domain. Much work has b"
D16-1023,P05-1015,0,0.132662,"earn the projection function. (4) We perform joint learning of the auxiliary tasks and the end task, i.e., sentiment classification, while SCL performs the learning in a sequential manner. 4 Experiments 4.1 Data Sets and Experiment Settings Data Set Movie1(MV1) Movie2(MV2) Camera(CR) Laptop(LT) Restaurant(RT) # Sentences # Words 10662 9613 3770 1907 1572 18765 16186 5340 2837 2930 Table 1: Statistics of our data sets. To evaluate our proposed method, we conduct experiments using five benchmark data sets. The data sets are summarized in Table 1. Movie12 and Movie23 are movie reviews labeled by Pang and Lee (2005) and Socher et al. (2013), respectively. Camera4 are reviews of digital products such as MP3 players and cameras (Hu and Liu, 2004). Laptop and Restaurant5 are laptop and restaurant reviews taken 2 https://www.cs.cornell.edu/people/pabo/ movie-review-data/ 3 http://nlp.stanford.edu/sentiment/ 4 http://www.cs.uic.edu/˜liub/FBS/ sentiment-analysis.html 5 Note that the original data set is for aspect-level sentiment analysis. We remove sentences with opposite polarities towards different aspects, and use the consistent polarity as the sentencelevel sentiment of each remaining sentence. 241 from S"
D16-1023,W02-1011,0,0.0497124,"domains for sentiment classification. We also propose to jointly learn this sentence embedding together with the sentiment classifier itself. Experiment results demonstrate that our proposed joint model outperforms several state-of-theart methods on five benchmark datasets. 1 Jing Jiang School of Information Systems Singapore Management University jingjiang@smu.edu.sg Introduction With the growing need of correctly identifying the sentiments expressed in subjective texts such as product reviews, sentiment classification has received continuous attention in the NLP community for over a decade (Pang et al., 2002; Pang and Lee, 2004; Hu and Liu, 2004; Choi and Cardie, 2008; Nakagawa et al., 2010). One of the big challenges of sentiment classification is how to adapt a sentiment classifier trained on one domain to a different new domain. This is because sentiments are often expressed with domain-specific words and expressions. For example, in the Movie domain, words such as moving and engaging are usually positive, but they may not be relevant in the Restaurant domain. Since labeled data is expensive to obtain, it would be very useful if we could adapt a model trained on a source domain to a target dom"
D16-1023,D12-1060,0,0.0334819,"Missing"
D16-1023,D13-1170,0,0.278776,"they may not be relevant in the Restaurant domain. Since labeled data is expensive to obtain, it would be very useful if we could adapt a model trained on a source domain to a target domain. Much work has been done in sentiment analysis to address this domain adaptation problem (Blitzer However, the original SCL method is based on traditional discrete feature representations and linear classifiers. In recent years, with the advances of deep learning in NLP, multi-layer neural network models such as RNNs and CNNs have been widely used in sentiment classification and achieved good performance (Socher et al., 2013; Dong et al., 2014a; Dong et al., 2014b; Kim, 2014; Tang et al., 2015). In these models, dense, real-valued feature vectors and non-linear classification functions are used. By using real-valued word embeddings pre-trained from a large corpus, these models can take advantage of the embedding space that presumably better captures the syntactic and semantic similarities between words. And by using non-linear functions through multi-layer neural networks, these models represent a more expressive hypothesis space. Therefore, it would be interesting to explore how these neural network models could"
D16-1023,D15-1167,0,0.0688839,"s expensive to obtain, it would be very useful if we could adapt a model trained on a source domain to a target domain. Much work has been done in sentiment analysis to address this domain adaptation problem (Blitzer However, the original SCL method is based on traditional discrete feature representations and linear classifiers. In recent years, with the advances of deep learning in NLP, multi-layer neural network models such as RNNs and CNNs have been widely used in sentiment classification and achieved good performance (Socher et al., 2013; Dong et al., 2014a; Dong et al., 2014b; Kim, 2014; Tang et al., 2015). In these models, dense, real-valued feature vectors and non-linear classification functions are used. By using real-valued word embeddings pre-trained from a large corpus, these models can take advantage of the embedding space that presumably better captures the syntactic and semantic similarities between words. And by using non-linear functions through multi-layer neural networks, these models represent a more expressive hypothesis space. Therefore, it would be interesting to explore how these neural network models could be extended for cross-domain sentiment classification. 236 Proceedings"
D16-1023,P11-1007,0,0.194069,"ether with the sentiment classification model itself, and we show that joint learning works better than sequential learning. We conduct experiments on a number of different source and target domains for sentence-level sentiment classification. We show that our proposed method is able to achieve the best performance compared with a number of baselines for most of these domain pairs. 2 Related Work Domain Adaptation: Domain adaptation is a general problem in NLP and has been well studied in recent years (Blitzer et al., 2006; Daum´e III, 2007; Jiang and Zhai, 2007; Dredze and Crammer, 237 2008; Titov, 2011; Yu and Jiang, 2015). For sentiment classification, most existing domain adaptation methods are based on traditional discrete feature representations and linear classifiers. One line of work focuses on inducing a general lowdimensional cross-domain representation based on the co-occurrences of domain-specific and domainindependent features (Blitzer et al., 2007; Pan et al., 2010; Pan et al., 2011). Another line of work tries to derive domain-specific sentiment words (Bollegala et al., 2011; Li et al., 2012). Our proposed method is similar to the first line of work in that we also aim to learn"
D16-1023,P14-2088,0,0.651997,"rities between words. And by using non-linear functions through multi-layer neural networks, these models represent a more expressive hypothesis space. Therefore, it would be interesting to explore how these neural network models could be extended for cross-domain sentiment classification. 236 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 236–246, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics There has been some recent studies on neural network-based domain adaptation (Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2014). They use Stacked Denoising Auto-encoders (SDA) to induce a hidden representation that presumably works well across domains. However, SDA is fully unsupervised and does not consider the end task we need to solve, i.e., the sentiment classification task. In contrast, the idea behind SCL is to use carefullychosen auxiliary tasks that correlate with the end task to induce a hidden representation. Another line of work aims to learn a low dimensional representation for each feature in both domains based on predicting its neighboring features (Yang and Eisenstein, 2015; Bollegala et al., 2015). Dif"
D16-1023,N15-1069,0,0.0539191,"l., 2011; Chen et al., 2012; Yang and Eisenstein, 2014). They use Stacked Denoising Auto-encoders (SDA) to induce a hidden representation that presumably works well across domains. However, SDA is fully unsupervised and does not consider the end task we need to solve, i.e., the sentiment classification task. In contrast, the idea behind SCL is to use carefullychosen auxiliary tasks that correlate with the end task to induce a hidden representation. Another line of work aims to learn a low dimensional representation for each feature in both domains based on predicting its neighboring features (Yang and Eisenstein, 2015; Bollegala et al., 2015). Different from these methods, we aim to directly learn sentence embeddings that work well across domains. In this paper, we aim to extend the main idea behind SCL to neural network-based solutions to sentiment classification to address the domain adaptation problem. Specifically, we borrow the idea of using pivot prediction tasks from SCL. But instead of learning thousands of pivot predictors and performing singular value decomposition on the learned weights, which all relies on linear transformations, we introduce only two auxiliary binary prediction tasks and direc"
D16-1023,P15-2028,1,0.850694,"e sentiment classification model itself, and we show that joint learning works better than sequential learning. We conduct experiments on a number of different source and target domains for sentence-level sentiment classification. We show that our proposed method is able to achieve the best performance compared with a number of baselines for most of these domain pairs. 2 Related Work Domain Adaptation: Domain adaptation is a general problem in NLP and has been well studied in recent years (Blitzer et al., 2006; Daum´e III, 2007; Jiang and Zhai, 2007; Dredze and Crammer, 237 2008; Titov, 2011; Yu and Jiang, 2015). For sentiment classification, most existing domain adaptation methods are based on traditional discrete feature representations and linear classifiers. One line of work focuses on inducing a general lowdimensional cross-domain representation based on the co-occurrences of domain-specific and domainindependent features (Blitzer et al., 2007; Pan et al., 2010; Pan et al., 2011). Another line of work tries to derive domain-specific sentiment words (Bollegala et al., 2011; Li et al., 2012). Our proposed method is similar to the first line of work in that we also aim to learn a general, cross-dom"
D18-1137,P17-1067,0,0.0815788,"fear, joy, love, optimism, pessimism, sadness, surprise and trust. Table 1 shows three example sentences along with their emotion labels. Traditional approaches to emotion detection include lexicon-based methods (Wang and Pal, 2015), graphical model-based methods (Li et al., 2015b) and linear classifier-based methods (Quan et al., 2015; Li et al., 2015a). Given the recent success of deep learning models, various neural network models and advanced attention mechanisms have been proposed for this task and have achieved highly competitive results on several benchmark datasets (Wang et al., 2016; Abdul-Mageed and Ungar, 2017; Felbo et al., 2017; Baziotis et al., 2018; He and Xia, 2018; Kim et al., 2018). However, these deep models must overcome a heavy reliance on large amounts of annotated data in order to learn a robust feature representation for multi-label emotion classification. In reality, large-scale datasets are usually not readily available and costly to obtain, partly due to the ambiguity of many informal expressions in user-generated comments. Conversely, it is easier to find datasets (especially in English) associated with another closely related task: sentiment classification, which aims to classify"
D18-1137,S18-1037,0,0.0570783,"Missing"
D18-1137,D17-1169,0,0.0364397,"simism, sadness, surprise and trust. Table 1 shows three example sentences along with their emotion labels. Traditional approaches to emotion detection include lexicon-based methods (Wang and Pal, 2015), graphical model-based methods (Li et al., 2015b) and linear classifier-based methods (Quan et al., 2015; Li et al., 2015a). Given the recent success of deep learning models, various neural network models and advanced attention mechanisms have been proposed for this task and have achieved highly competitive results on several benchmark datasets (Wang et al., 2016; Abdul-Mageed and Ungar, 2017; Felbo et al., 2017; Baziotis et al., 2018; He and Xia, 2018; Kim et al., 2018). However, these deep models must overcome a heavy reliance on large amounts of annotated data in order to learn a robust feature representation for multi-label emotion classification. In reality, large-scale datasets are usually not readily available and costly to obtain, partly due to the ambiguity of many informal expressions in user-generated comments. Conversely, it is easier to find datasets (especially in English) associated with another closely related task: sentiment classification, which aims to classify the sentiment polari"
D18-1137,S18-1019,0,0.0647235,"Missing"
D18-1137,D15-1099,0,0.0193699,"velop effective emotion detection models to automatically identify emotions from these online posts. In the literature, emotion detection is typically modeled as a supervised multi-label classification problem, because each sentence may contain one or more emotions from a standard emotion set containing anger, anticipation, disgust, fear, joy, love, optimism, pessimism, sadness, surprise and trust. Table 1 shows three example sentences along with their emotion labels. Traditional approaches to emotion detection include lexicon-based methods (Wang and Pal, 2015), graphical model-based methods (Li et al., 2015b) and linear classifier-based methods (Quan et al., 2015; Li et al., 2015a). Given the recent success of deep learning models, various neural network models and advanced attention mechanisms have been proposed for this task and have achieved highly competitive results on several benchmark datasets (Wang et al., 2016; Abdul-Mageed and Ungar, 2017; Felbo et al., 2017; Baziotis et al., 2018; He and Xia, 2018; Kim et al., 2018). However, these deep models must overcome a heavy reliance on large amounts of annotated data in order to learn a robust feature representation for multi-label emotion cla"
D18-1137,P15-1101,0,0.110789,"velop effective emotion detection models to automatically identify emotions from these online posts. In the literature, emotion detection is typically modeled as a supervised multi-label classification problem, because each sentence may contain one or more emotions from a standard emotion set containing anger, anticipation, disgust, fear, joy, love, optimism, pessimism, sadness, surprise and trust. Table 1 shows three example sentences along with their emotion labels. Traditional approaches to emotion detection include lexicon-based methods (Wang and Pal, 2015), graphical model-based methods (Li et al., 2015b) and linear classifier-based methods (Quan et al., 2015; Li et al., 2015a). Given the recent success of deep learning models, various neural network models and advanced attention mechanisms have been proposed for this task and have achieved highly competitive results on several benchmark datasets (Wang et al., 2016; Abdul-Mageed and Ungar, 2017; Felbo et al., 2017; Baziotis et al., 2018; He and Xia, 2018; Kim et al., 2018). However, these deep models must overcome a heavy reliance on large amounts of annotated data in order to learn a robust feature representation for multi-label emotion cla"
D18-1137,P17-1001,0,0.180943,"erence on Empirical Methods in Natural Language Processing, pages 1097–1102 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics ys yt ys yt ys yt ys yt Sentence Embedding Attention weights α? α? α? α? α? α? α? α? Sentence Encoding Layer L S T M L S T M L S T M L S T M L S T M L S T M L S T M L S T M xs xt xs xt xs xt a. Fully-Shared (FS) xs xt b. Private-Shared-Private (PSP) c. Shared-Private (SP) d. Dual Attention Transfer Network Figure 1: Overview of Different Transfer Learning Models. shared feature space and two task-specific feature spaces (Liu et al., 2017; Yu et al., 2018), as demonstrated by Fig 1.a and Fig 1.b. However, when applying these TL approaches to our scenario, the former approach may lead the learnt sentence representation to pay more attention to general sentiment words such as good but less attention to the other sentiment-ambiguous words like shock that are also integral to emotion classification. The latter approach can capture both the sentiment and the emotion-specific words. However, some sentiment words only occur in the source sentiment classification task. These words tend to receive more attention in the source-specific"
D18-1137,S18-1001,0,0.128825,"Missing"
D18-1137,S16-1001,0,0.019917,"flow. Evaluation Metrics: We take the official code from SemEval-18 Task 1C and use accuracy and Macro F1 score as main metrics. For E2, we follow (Zhou et al., 2018a) to use average precision (AP) and one error (OE) as secondary metrics. 3 3.2 M 1 X J =− log p(ym |Hc ) + L M m=1 +λ N X cos sim(αis , αit ), i=1 where λ is a hyperparameter used to control the effect of the similarity loss. 2.2.3 3.1 Model Details Experiments Experiment Settings Datasets: We conduct experiments on both English and Chinese languages. For English, we employ a widely used Twitter dataset from SemEval 2016 Task 4A (Nakov et al., 2016) as our source sentiment classification task. For our target emotion classification task, we use the Twitter dataset recently released by SemEval 2018 Task 1C (Mohammad et al., 2018), which contains 11 emotions as shown in the top of Fig. 2. To tokenize the tweets in our dataset, we follow (Owoputi et al., 2013) by adopting most of Results To better evaluate our proposed methods, we employed the following systems for comparison: 1) Base, training our base model in Section 2.1 only on De ; 2) FT (Fine-Tuning), using Ds to pretrain the whole model, followed by using De to Fine Tune the model par"
D18-1137,N13-1039,0,0.0165365,"Missing"
D18-1137,N18-1052,0,0.196138,"ness Shitty is the worst feeling ever #depressed #anxiety I am back lol. #revenge joy, optimism T2 T3 fear, sadness joy, anger Table 1: Example Tweets from SemEval-18 Task 1. Introduction In recent years, the number of user-generated comments on social media platforms has grown exponentially. In particular, social platforms such as Twitter allow users to easily share their personal opinions, attitudes and emotions about any topic through short posts. Understanding people’s emotions expressed in these short posts can facilitate many important downstream applications such as emotional chatbots (Zhou et al., 2018b), personalized recommendations, stock market prediction, policy studies, etc. Therefore, it is crucial to develop effective emotion detection models to automatically identify emotions from these online posts. In the literature, emotion detection is typically modeled as a supervised multi-label classification problem, because each sentence may contain one or more emotions from a standard emotion set containing anger, anticipation, disgust, fear, joy, love, optimism, pessimism, sadness, surprise and trust. Table 1 shows three example sentences along with their emotion labels. Traditional appro"
D18-1137,D16-1061,0,0.147579,"444 0.641 0.680 0.725 0.732 0.523 0.455 0.425 0.415 Table 3: The results of different transfer learning methods by averaging ten runs (top) and the comparison between our best model and the state-of-the-art systems (bottom). DATN2∗ indicates the ensemble results of ten runs. Base† and DATN-2† denotes the average results of conducting ten-fold cross validation on the whole dataset for fair comparison, and here for the source and target tasks in DATN-2† , we use the same training data. For E1, Rank1 and Rank2 are the top two systems from the official leadboard; For E2, Rank1 and Rank2 are from (Zhou et al., 2016, 2018a). sarial losses by (Liu et al., 2017) as shown in Fig 1.b; 5) SP, DATN-1 and DATN-2, the SharedPrivate model and two variants of our Dual Attention Transfer Network as shown in Fig 1.c and Fig 1.d. In Table 3, we report the comparison results between our method and the baseline systems. It can be easily observed that 1) for transfer learning, although the performance of SP is similar to or even lower than some baseline systems, our proposed dual attention models, i.e., DATN-1 and DATN2, can generally boost SP to achieve the best results. To investigate the significance of the improveme"
D18-1137,D16-1046,0,0.162423,"to find datasets (especially in English) associated with another closely related task: sentiment classification, which aims to classify the sentiment polarity of a given piece of text (i.e., positive, negative and neutral). We expect that these resources may allow us to improve sentiment-sensitive representations and thus more accurately identify emotions in social media posts. To achieve these goals, we propose an effective transfer learning (TL) approach in this paper. Most existing TL methods either 1) assume that both the source and the target tasks share the same sentence representation (Mou et al., 2016) or 2) divide the representation of each sentence into a 1097 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1097–1102 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics ys yt ys yt ys yt ys yt Sentence Embedding Attention weights α? α? α? α? α? α? α? α? Sentence Encoding Layer L S T M L S T M L S T M L S T M L S T M L S T M L S T M L S T M xs xt xs xt xs xt a. Fully-Shared (FS) xs xt b. Private-Shared-Private (PSP) c. Shared-Private (SP) d. Dual Attention Transfer Network Figure 1: Overview of Diffe"
I11-1044,P10-1013,0,0.105213,"tors with Conditional Random Fields Yaliang Li† , Jing Jiang† , Hai Leong Chieu‡ , Kian Ming A. Chai‡ † School of Information Systems, Singapore Management University, Singapore ‡ DSO National Laboratories, Singapore {ylli,jingjiang}@smu.edu.sg, {chaileon,ckianmin}@dso.org.sg Abstract types (Hasegawa et al., 2004; Rosenfeld and Feldman, 2006; Shinyama and Sekine, 2006). More recently, open relation extraction has also been proposed where there is no fixed domain or predefined relation type, and the goal is to identify all possible relations from an open-domain corpus (Banko and Etzioni, 2008; Wu and Weld, 2010; Hoffmann et al., 2010). These different relation extraction settings suit different applications. In this paper, we focus on another setting where the relation types are defined at a general level but a more specific relation description is desired. For example, in the widely used ACE1 data sets, relation types are defined at a fairly coarse granularity. Take for instance the “employment” relation, which is a major relation type defined in ACE. In ACE evaluation, extraction of this relation only involves deciding whether a person entity is employed by an organization entity. In practice, how"
I11-1044,H05-1091,0,0.579718,"two data sets we have annotated, we evaluate our methods and find that both modifications to linear-chain CRFs can significantly improve the performance for our task. 1 Introduction Relation extraction is the task of identifying and characterizing the semantic relations between entities in text. Depending on the application and the resources available, relation extraction has been studied in a number of different settings. When relation types are well defined and labeled relation mention instances are available, supervised learning is usually applied (Zelenko et al., 2003; Zhou et al., 2005; Bunescu and Mooney, 2005; Zhang et al., 2006). When relation types are known but little training data is available, bootstrapping has been used to iteratively expand the set of seed examples and relation patterns (Agichtein and Gravano, 2000). When no relation type is pre-defined but there is a focused corpus of interest, unsupervised relation discovery tries to cluster entity pairs in order to identify interesting relation 1 Automatic Content Extraction http://www.itl. nist.gov/iad/mig/tests/ace/ 392 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 392–400, c Chiang Mai, Th"
I11-1044,N06-1038,0,0.119757,"relation descriptor, respectively. and January 2010. After sentence segmentation and tokenization, we used the Stanford NER tagger (Finkel et al., 2005) to identify PER and ORG named entities from each sentence. For named entities that contain multiple tokens we concatenated them into a single token. We then took each pair of (PER, ORG) entities that occur in the same sentence as a single candidate relation instance, where the PER entity is treated as ARG-1 and the ORG entity is treated as ARG-2. The second data set comes from a Wikipedia personal/social relation data set previously used in (Culotta et al., 2006). The original data set does not contain annotations of relation descriptors such as “sister” or “friend” between the two PER arguments. We therefore also manually annotated this data set. Similarly, we performed sentence segmentation, tokenization and NER tagging, and took each pair of (PER, PER) entities occurring in the same sentence as a candidate relation instance. Because both arguments involved in the “personal/social” relation are PER entities, we always treat the first PER entity as ARG-1 and the second PER entity as ARG-2.3 We go through each candidate relation instance to find wheth"
I11-1044,N06-1037,0,0.121171,"otated, we evaluate our methods and find that both modifications to linear-chain CRFs can significantly improve the performance for our task. 1 Introduction Relation extraction is the task of identifying and characterizing the semantic relations between entities in text. Depending on the application and the resources available, relation extraction has been studied in a number of different settings. When relation types are well defined and labeled relation mention instances are available, supervised learning is usually applied (Zelenko et al., 2003; Zhou et al., 2005; Bunescu and Mooney, 2005; Zhang et al., 2006). When relation types are known but little training data is available, bootstrapping has been used to iteratively expand the set of seed examples and relation patterns (Agichtein and Gravano, 2000). When no relation type is pre-defined but there is a focused corpus of interest, unsupervised relation discovery tries to cluster entity pairs in order to identify interesting relation 1 Automatic Content Extraction http://www.itl. nist.gov/iad/mig/tests/ace/ 392 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 392–400, c Chiang Mai, Thailand, November 8 –"
I11-1044,P05-1045,0,0.0157437,", REL REL PREP Path-based Features word or POS tag sequence between ARG1 and relation descriptor word or POS tag sequence between ARG2 and relation descriptor word or POS tag sequence containing ARG1, ARG2 and relation descriptor ARG1 is REL REL PREP ARG2 ARG2 ’s REL , ARG1 Phrase Boundary Feature whether relation descriptor violates phrase boundaries 1 or 0 Table 3: Long-range feature templates. r and s are the indices of the first word and the last word of the relation descriptor, respectively. and January 2010. After sentence segmentation and tokenization, we used the Stanford NER tagger (Finkel et al., 2005) to identify PER and ORG named entities from each sentence. For named entities that contain multiple tokens we concatenated them into a single token. We then took each pair of (PER, ORG) entities that occur in the same sentence as a single candidate relation instance, where the PER entity is treated as ARG-1 and the ORG entity is treated as ARG-2. The second data set comes from a Wikipedia personal/social relation data set previously used in (Culotta et al., 2006). The original data set does not contain annotations of relation descriptors such as “sister” or “friend” between the two PER argume"
I11-1044,P05-1053,0,0.290997,"of our task. Using two data sets we have annotated, we evaluate our methods and find that both modifications to linear-chain CRFs can significantly improve the performance for our task. 1 Introduction Relation extraction is the task of identifying and characterizing the semantic relations between entities in text. Depending on the application and the resources available, relation extraction has been studied in a number of different settings. When relation types are well defined and labeled relation mention instances are available, supervised learning is usually applied (Zelenko et al., 2003; Zhou et al., 2005; Bunescu and Mooney, 2005; Zhang et al., 2006). When relation types are known but little training data is available, bootstrapping has been used to iteratively expand the set of seed examples and relation patterns (Agichtein and Gravano, 2000). When no relation type is pre-defined but there is a focused corpus of interest, unsupervised relation discovery tries to cluster entity pairs in order to identify interesting relation 1 Automatic Content Extraction http://www.itl. nist.gov/iad/mig/tests/ace/ 392 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages"
I11-1044,P04-1053,0,0.0856321,"Missing"
I11-1044,P10-1030,0,0.0221025,"th Conditional Random Fields Yaliang Li† , Jing Jiang† , Hai Leong Chieu‡ , Kian Ming A. Chai‡ † School of Information Systems, Singapore Management University, Singapore ‡ DSO National Laboratories, Singapore {ylli,jingjiang}@smu.edu.sg, {chaileon,ckianmin}@dso.org.sg Abstract types (Hasegawa et al., 2004; Rosenfeld and Feldman, 2006; Shinyama and Sekine, 2006). More recently, open relation extraction has also been proposed where there is no fixed domain or predefined relation type, and the goal is to identify all possible relations from an open-domain corpus (Banko and Etzioni, 2008; Wu and Weld, 2010; Hoffmann et al., 2010). These different relation extraction settings suit different applications. In this paper, we focus on another setting where the relation types are defined at a general level but a more specific relation description is desired. For example, in the widely used ACE1 data sets, relation types are defined at a fairly coarse granularity. Take for instance the “employment” relation, which is a major relation type defined in ACE. In ACE evaluation, extraction of this relation only involves deciding whether a person entity is employed by an organization entity. In practice, how"
I11-1044,W95-0107,0,0.250392,"of valid label sequences, i.e., those that have either one or no relation descriptor sequence. We ˆ as follows: then choose the best sequence y The relation descriptor extraction task can be treated as a sequence labeling problem. Let x = (x1 , x2 , . . . , xn ) denote the sequence of observations in a relation instance, where xi is wi augmented with additional information such as the POS tag of wi , and the phrase boundary information. Each observation xi is associated with a label yi ∈ Y which indicates whether wi is part of the relation descriptor. Following the commonly used BIO notation (Ramshaw and Marcus, 1995) in sequence labeling, we define Y = {B-REL, I-REL, O}. Let y = (y1 , y2 , . . . , yn ) denote the sequence of labels for x. Our task can ˆ be reduced to finding the best label sequence y among all the possible label sequences for x. 4.2 k j log p(y ∗j |xj , Λ) We note that while we can directly apply linearchain CRFs to extract relation descriptors, there are some special properties of our task that allow us to modify standard linear-chain CRFs to better suit our needs. Method 4.1 P − X ˜ Z(x, Λ) = (2) X ˜ y 0 ∈Y k 394 exp “XX i k ” 0 λk fk (yi−1 , yi0 , x) . (5) Linear-chain features The dif"
I11-1044,P06-2086,0,0.0736506,"Missing"
I11-1044,N06-1039,0,0.0608245,"Missing"
I11-1044,P08-1004,0,\N,Missing
I17-1066,K15-1006,0,0.0132261,"Kitchen domain. Similarly, words such as sharp and clean, 654 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 654–663, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP sentiment classification, most existing domain adaptation methods focus on inducing shared representations across domains. One line of work tries to leverage the co-occurrences of domainspecific and domain-independent features to learn a general low-dimensional cross-domain representation (Blitzer et al., 2007; Pan et al., 2010; He et al., 2011; Bollegala et al., 2015; Bhatt et al., 2015). Another line of work is based on a purely unsupervised learning method, denoising autoencoders, where the hidden layers in multi-layer neural networks are believed to be robust against domain shift (Glorot et al., 2011; Chen et al., 2012; Zhou et al., 2016). However, all these methods are still based on traditional discrete representations, and the shared representations are learned separately from the final classifier and therefore not directly related to sentiment classification. More recently, we proposed a unified neural model to jointly learn the shared sentence embeddings and the final"
I17-1066,P07-1056,0,0.828544,"ositive in the Kitchen domain, can rarely be seen in the Book domain. Due to the high cost of obtaining labeled data, it would be very attractive if we can adapt a model trained on a source domain to a target domain. A number of different models have been proposed for cross-domain sentiment classification, and the core idea of them is to learn a shared latent representation that is general across domains. Most of these studies can be categorized into two lines. The first line of work focuses on carefully designing some auxiliary prediction tasks to induce a robust cross-domain representation (Blitzer et al., 2007; Pan et al., 2010; Bollegala et al., 2015, 2016). With the trend of deep learning, another line of work centers on employing denoising autoencoders to learn hidden representations across domains in a purely unsupervised learning manner (Glorot et al., 2011; Chen et al., 2012; Zhou et al., 2016). However, most of the two lines of research are based on traditional discrete feature representations, and the induced shared representations are not necessarily specific to sentiment classification. In our recent work, we designed two simple auxiliary tasks, which are closely related to the actual end"
I17-1066,W06-1615,0,0.754216,"aluation on a widely used dataset about product reviews from four different domains shows that our methods can significantly outperform a number of baselines and are able to achieve comparable or even better results compared with a strong baseline proposed by us. 2 Related Work Domain Adaptation: Domain adaptation has been extensively studied in recent years (Pan and Yang, 2010). In NLP, it has also attracted much attention, where most domain adaptation methods can be categorized into two groups: instance reweighting (Jiang and Zhai, 2007; Xia et al., 2014) and shared representation learning (Blitzer et al., 2006; Daum´e III, 2007; Titov, 2011). In this work, we follow the latter line of work, and focus on inducing a domain-independent feature space based on a recently proposed NN architecture. Neural Networks for Sentiment Classification: With the recent trend of deep learning, a large amount of NN models, including Convolutional Neural Network (Kim, 2014), Recursive Neural Network (Irsoy and Cardie, 2014) and Recurrent Neural Network (Tai et al., 2015), have been proposed for sentiment classification. Although these models have achieved highly competitive results on different benchmarks, most of the"
I17-1066,P15-1071,0,0.0890631,"y be seen in the Book domain. Due to the high cost of obtaining labeled data, it would be very attractive if we can adapt a model trained on a source domain to a target domain. A number of different models have been proposed for cross-domain sentiment classification, and the core idea of them is to learn a shared latent representation that is general across domains. Most of these studies can be categorized into two lines. The first line of work focuses on carefully designing some auxiliary prediction tasks to induce a robust cross-domain representation (Blitzer et al., 2007; Pan et al., 2010; Bollegala et al., 2015, 2016). With the trend of deep learning, another line of work centers on employing denoising autoencoders to learn hidden representations across domains in a purely unsupervised learning manner (Glorot et al., 2011; Chen et al., 2012; Zhou et al., 2016). However, most of the two lines of research are based on traditional discrete feature representations, and the induced shared representations are not necessarily specific to sentiment classification. In our recent work, we designed two simple auxiliary tasks, which are closely related to the actual end task, for sentence-level cross-domain sen"
I17-1066,D08-1083,0,0.0448782,"ce document embeddings and sentence embeddings that work well for different domains. When these document and sentence embeddings are used for sentiment classification, we find that with both pseudo and external sentiment lexicons, our proposed methods can perform similarly to or better than several highly competitive domain adaptation methods on a benchmark dataset of product reviews. 1 Jing Jiang School of Information Systems Singapore Management University jingjiang@smu.edu.sg Introduction Sentiment classification is a fundamental task in opinion mining (Pang et al., 2002; Hu and Liu, 2004; Choi and Cardie, 2008; Nakagawa et al., 2010). Recently, with the advances of deep learning techniques for many NLP applications, various kinds of neural network (NN)-based models have been proposed for this task (Socher et al., 2013; Lei et al., 2015; Yang et al., 2016). As with any supervised learning method, the NN-based models also suffer from the domain adaptation problem, where training data and test data come from different domains. The reason for this is that sentiments are often expressed with domain-specific words and expressions. For example, in the Book domain, expressions like an insider’s look and a"
I17-1066,W02-1011,0,0.0266746,"rk architectures to respectively induce document embeddings and sentence embeddings that work well for different domains. When these document and sentence embeddings are used for sentiment classification, we find that with both pseudo and external sentiment lexicons, our proposed methods can perform similarly to or better than several highly competitive domain adaptation methods on a benchmark dataset of product reviews. 1 Jing Jiang School of Information Systems Singapore Management University jingjiang@smu.edu.sg Introduction Sentiment classification is a fundamental task in opinion mining (Pang et al., 2002; Hu and Liu, 2004; Choi and Cardie, 2008; Nakagawa et al., 2010). Recently, with the advances of deep learning techniques for many NLP applications, various kinds of neural network (NN)-based models have been proposed for this task (Socher et al., 2013; Lei et al., 2015; Yang et al., 2016). As with any supervised learning method, the NN-based models also suffer from the domain adaptation problem, where training data and test data come from different domains. The reason for this is that sentiments are often expressed with domain-specific words and expressions. For example, in the Book domain,"
I17-1066,P07-1033,0,0.16348,"Missing"
I17-1066,D13-1170,0,0.0116689,"timent lexicons, our proposed methods can perform similarly to or better than several highly competitive domain adaptation methods on a benchmark dataset of product reviews. 1 Jing Jiang School of Information Systems Singapore Management University jingjiang@smu.edu.sg Introduction Sentiment classification is a fundamental task in opinion mining (Pang et al., 2002; Hu and Liu, 2004; Choi and Cardie, 2008; Nakagawa et al., 2010). Recently, with the advances of deep learning techniques for many NLP applications, various kinds of neural network (NN)-based models have been proposed for this task (Socher et al., 2013; Lei et al., 2015; Yang et al., 2016). As with any supervised learning method, the NN-based models also suffer from the domain adaptation problem, where training data and test data come from different domains. The reason for this is that sentiments are often expressed with domain-specific words and expressions. For example, in the Book domain, expressions like an insider’s look and a must read are usually positive, but they may not be useful for the Kitchen domain. Similarly, words such as sharp and clean, 654 Proceedings of the The 8th International Joint Conference on Natural Language Proce"
I17-1066,P15-1150,0,0.0750471,"Missing"
I17-1066,P11-1013,0,0.0205062,"itive, but they may not be useful for the Kitchen domain. Similarly, words such as sharp and clean, 654 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 654–663, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP sentiment classification, most existing domain adaptation methods focus on inducing shared representations across domains. One line of work tries to leverage the co-occurrences of domainspecific and domain-independent features to learn a general low-dimensional cross-domain representation (Blitzer et al., 2007; Pan et al., 2010; He et al., 2011; Bollegala et al., 2015; Bhatt et al., 2015). Another line of work is based on a purely unsupervised learning method, denoising autoencoders, where the hidden layers in multi-layer neural networks are believed to be robust against domain shift (Glorot et al., 2011; Chen et al., 2012; Zhou et al., 2016). However, all these methods are still based on traditional discrete representations, and the shared representations are learned separately from the final classifier and therefore not directly related to sentiment classification. More recently, we proposed a unified neural model to jointly learn"
I17-1066,D15-1167,0,0.530341,"tecture. Neural Networks for Sentiment Classification: With the recent trend of deep learning, a large amount of NN models, including Convolutional Neural Network (Kim, 2014), Recursive Neural Network (Irsoy and Cardie, 2014) and Recurrent Neural Network (Tai et al., 2015), have been proposed for sentiment classification. Although these models have achieved highly competitive results on different benchmarks, most of them are targeted at sentence-level sentiment classification. Considering that the relations between sentences are important for predicting the sentiment polarity of any document, Tang et al. (2015) proposed a hieararchical NN model to encode the relations between sentences for document-level sentiment classification. Since it has been shown to significantly outperform standard non-hierarchical models on several benchmarks, we try to apply this model to domain adaptation settings in this work. Cross-Domain Sentiment Classification: For 3 Methodology In this section we present our domain adaptation method for document-level sentiment classification. 3.1 Problem Definition and Notation Our task is sentiment classification at the document level. We assume that each input d is a document con"
I17-1066,P11-1007,0,0.0179271,"product reviews from four different domains shows that our methods can significantly outperform a number of baselines and are able to achieve comparable or even better results compared with a strong baseline proposed by us. 2 Related Work Domain Adaptation: Domain adaptation has been extensively studied in recent years (Pan and Yang, 2010). In NLP, it has also attracted much attention, where most domain adaptation methods can be categorized into two groups: instance reweighting (Jiang and Zhai, 2007; Xia et al., 2014) and shared representation learning (Blitzer et al., 2006; Daum´e III, 2007; Titov, 2011). In this work, we follow the latter line of work, and focus on inducing a domain-independent feature space based on a recently proposed NN architecture. Neural Networks for Sentiment Classification: With the recent trend of deep learning, a large amount of NN models, including Convolutional Neural Network (Kim, 2014), Recursive Neural Network (Irsoy and Cardie, 2014) and Recurrent Neural Network (Tai et al., 2015), have been proposed for sentiment classification. Although these models have achieved highly competitive results on different benchmarks, most of them are targeted at sentence-level"
I17-1066,P07-1034,1,0.873591,"mains and a shared sentence embedding for each sentence in all documents. Evaluation on a widely used dataset about product reviews from four different domains shows that our methods can significantly outperform a number of baselines and are able to achieve comparable or even better results compared with a strong baseline proposed by us. 2 Related Work Domain Adaptation: Domain adaptation has been extensively studied in recent years (Pan and Yang, 2010). In NLP, it has also attracted much attention, where most domain adaptation methods can be categorized into two groups: instance reweighting (Jiang and Zhai, 2007; Xia et al., 2014) and shared representation learning (Blitzer et al., 2006; Daum´e III, 2007; Titov, 2011). In this work, we follow the latter line of work, and focus on inducing a domain-independent feature space based on a recently proposed NN architecture. Neural Networks for Sentiment Classification: With the recent trend of deep learning, a large amount of NN models, including Convolutional Neural Network (Kim, 2014), Recursive Neural Network (Irsoy and Cardie, 2014) and Recurrent Neural Network (Tai et al., 2015), have been proposed for sentiment classification. Although these models h"
I17-1066,P14-2088,0,0.013007,"SRE, SSR and SSRW are based on our second NN architecture to induce sentence-level shared representations, as introduced in 3.5. Hyperparameters: For Naive, we train linear classifiers with LibLinear5 by using unigrams and bigrams with a frequency of at least 5 as features. For SCL and mDA, we use mutual information to select pivot features, and the number of chosen pivots is tuned from {500, 1000, 1500, 2000} on the development set. In SCL, we tune the number of induced features K in {25, 50, 100}, and also use normalization and rescaling. In mDA, we employ the dropout noise strategy used by Yang and Eisenstein (2014) without any parameter. In 5 Results 6 http://www.csie.ntu.edu.tw/cjlin/liblinear/ 660 https://code.google.com/p/word2vec/ Task In-D HNN E2D B2D K2D E2B D2B K2B B2E D2E K2E E2K B2K D2K AVG 0.845 0.843 0.858 0.883 0.857 Compared Methods (Cross-Domain) Proposed Methods (Cross-Domain) Naive SCL mDA HNN H-WN H-SCL H-mDA DSRE SSRE DSR SSR DSRW SSRW 0.680 0.773 0.698 0.693 0.751 0.690 0.701 0.706 0.799 0.828 0.724 0.716 0.700 0.771 0.721 0.704 0.780 0.740 0.746 0.743 0.818 0.829 0.763 0.758 0.727 0.806 0.741 0.728 0.802 0.725 0.753 0.746 0.830 0.833 0.754 0.742 0.805 0.814 0.791 0.786 0.805 0.766 0."
I17-1066,D14-1181,0,0.12425,"(Pan and Yang, 2010). In NLP, it has also attracted much attention, where most domain adaptation methods can be categorized into two groups: instance reweighting (Jiang and Zhai, 2007; Xia et al., 2014) and shared representation learning (Blitzer et al., 2006; Daum´e III, 2007; Titov, 2011). In this work, we follow the latter line of work, and focus on inducing a domain-independent feature space based on a recently proposed NN architecture. Neural Networks for Sentiment Classification: With the recent trend of deep learning, a large amount of NN models, including Convolutional Neural Network (Kim, 2014), Recursive Neural Network (Irsoy and Cardie, 2014) and Recurrent Neural Network (Tai et al., 2015), have been proposed for sentiment classification. Although these models have achieved highly competitive results on different benchmarks, most of them are targeted at sentence-level sentiment classification. Considering that the relations between sentences are important for predicting the sentiment polarity of any document, Tang et al. (2015) proposed a hieararchical NN model to encode the relations between sentences for document-level sentiment classification. Since it has been shown to signifi"
I17-1066,N16-1174,0,0.0447476,"an perform similarly to or better than several highly competitive domain adaptation methods on a benchmark dataset of product reviews. 1 Jing Jiang School of Information Systems Singapore Management University jingjiang@smu.edu.sg Introduction Sentiment classification is a fundamental task in opinion mining (Pang et al., 2002; Hu and Liu, 2004; Choi and Cardie, 2008; Nakagawa et al., 2010). Recently, with the advances of deep learning techniques for many NLP applications, various kinds of neural network (NN)-based models have been proposed for this task (Socher et al., 2013; Lei et al., 2015; Yang et al., 2016). As with any supervised learning method, the NN-based models also suffer from the domain adaptation problem, where training data and test data come from different domains. The reason for this is that sentiments are often expressed with domain-specific words and expressions. For example, in the Book domain, expressions like an insider’s look and a must read are usually positive, but they may not be useful for the Kitchen domain. Similarly, words such as sharp and clean, 654 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 654–663, c Taipei, Taiwan"
I17-1066,D15-1180,0,0.0190626,"proposed methods can perform similarly to or better than several highly competitive domain adaptation methods on a benchmark dataset of product reviews. 1 Jing Jiang School of Information Systems Singapore Management University jingjiang@smu.edu.sg Introduction Sentiment classification is a fundamental task in opinion mining (Pang et al., 2002; Hu and Liu, 2004; Choi and Cardie, 2008; Nakagawa et al., 2010). Recently, with the advances of deep learning techniques for many NLP applications, various kinds of neural network (NN)-based models have been proposed for this task (Socher et al., 2013; Lei et al., 2015; Yang et al., 2016). As with any supervised learning method, the NN-based models also suffer from the domain adaptation problem, where training data and test data come from different domains. The reason for this is that sentiments are often expressed with domain-specific words and expressions. For example, in the Book domain, expressions like an insider’s look and a must read are usually positive, but they may not be useful for the Kitchen domain. Similarly, words such as sharp and clean, 654 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 654–6"
I17-1066,D16-1023,1,0.630619,"of deep learning, another line of work centers on employing denoising autoencoders to learn hidden representations across domains in a purely unsupervised learning manner (Glorot et al., 2011; Chen et al., 2012; Zhou et al., 2016). However, most of the two lines of research are based on traditional discrete feature representations, and the induced shared representations are not necessarily specific to sentiment classification. In our recent work, we designed two simple auxiliary tasks, which are closely related to the actual end task, for sentence-level cross-domain sentiment classification (Yu and Jiang, 2016). Furthermore, we proposed to jointly learn domainindependent sentence embeddings based on the two auxiliary tasks together with the classifier for the end task in a unified NN framework. Although our joint learning model has been shown to outperform previous domain adaptation methods in sentence-level sentiment classification, it is unclear how to extend this to document-level sentiment classification since the two auxiliary tasks will become much less useful for documents. In this paper, we aim to propose a domain adaptation method for document-level sentiment clasIn this paper, we study dom"
I17-1066,N10-1120,0,0.0318603,"and sentence embeddings that work well for different domains. When these document and sentence embeddings are used for sentiment classification, we find that with both pseudo and external sentiment lexicons, our proposed methods can perform similarly to or better than several highly competitive domain adaptation methods on a benchmark dataset of product reviews. 1 Jing Jiang School of Information Systems Singapore Management University jingjiang@smu.edu.sg Introduction Sentiment classification is a fundamental task in opinion mining (Pang et al., 2002; Hu and Liu, 2004; Choi and Cardie, 2008; Nakagawa et al., 2010). Recently, with the advances of deep learning techniques for many NLP applications, various kinds of neural network (NN)-based models have been proposed for this task (Socher et al., 2013; Lei et al., 2015; Yang et al., 2016). As with any supervised learning method, the NN-based models also suffer from the domain adaptation problem, where training data and test data come from different domains. The reason for this is that sentiments are often expressed with domain-specific words and expressions. For example, in the Book domain, expressions like an insider’s look and a must read are usually po"
I17-1066,P16-1031,0,0.123818,"fication, and the core idea of them is to learn a shared latent representation that is general across domains. Most of these studies can be categorized into two lines. The first line of work focuses on carefully designing some auxiliary prediction tasks to induce a robust cross-domain representation (Blitzer et al., 2007; Pan et al., 2010; Bollegala et al., 2015, 2016). With the trend of deep learning, another line of work centers on employing denoising autoencoders to learn hidden representations across domains in a purely unsupervised learning manner (Glorot et al., 2011; Chen et al., 2012; Zhou et al., 2016). However, most of the two lines of research are based on traditional discrete feature representations, and the induced shared representations are not necessarily specific to sentiment classification. In our recent work, we designed two simple auxiliary tasks, which are closely related to the actual end task, for sentence-level cross-domain sentiment classification (Yu and Jiang, 2016). Furthermore, we proposed to jointly learn domainindependent sentence embeddings based on the two auxiliary tasks together with the classifier for the end task in a unified NN framework. Although our joint learn"
N06-1010,W03-0420,0,0.214476,"at logistic regression has a close relation with maximum entropy models. Indeed, when the features in a maximum entropy model are defined as conjunctions of a feature on observations only and a Kronecker delta of a class label, which is a common practice in NER, the maximum entropy model is equivalent to a logistic regression model (Finkel et al., 2005). Thus the logistic regression method we use for NER is essentially the same as the maximum entropy models used for NER in previous work. To avoid overfitting, a zero mean Gaussian prior on the weights is usually used (Chen and Rosenfeld, 1999; Bender et al., 2003), and a maximum a posterior (MAP) estimator is used to maximize the posterior probability: β N "" j=1 p(yj |xj , β), (4) where yj is the true class label for xj , N is the number of training examples, and p(β) = |F | "" 1 β2 # exp(− i 2 ). 2σi 2πσi2 i=1 We need to find a transformation function h : {1, 2, . . . , |F |} → R+ so that we can set σi2 = h(rgen (fi )), where rgen (fi ) is the rank of feature fi in the generalizability-based ranked feature list, as defined in Section 2. We choose the following h function because it has the desired properties as described above: a h(r) = 1/b , (6) r whe"
N06-1010,W99-0613,0,0.0973406,"erence for top-ranked features. Figure 2, Figure 3 and Figure 4 show the performance of different feature ranking methods in the three sets of experiments as the parameter b for the rank-based prior changes. As we pointed out in Section 4, b is proportional to the logarithm of the number of “effective features”. 80 6 Related Work The NER problem has been extensively studied in the NLP community. Most existing work has focused on supervised learning approaches, employing models such as HMMs (Zhou and Su, 2002), MEMMs (Bender et al., 2003; Finkel et al., 2005), and CRFs (McCallum and Li, 2003). Collins and Singer (1999) proposed an unsupervised method for named entity classification based on the idea of cotraining. Ando and Zhang (2005) proposed a semisupervised learning method to exploit unlabeled data for building more robust NER systems. In all these studies, the evaluation is conducted on unlabeled data similar to the labeled data. Recently there have been some studies on adapting NER systems to new domains employing techniques such as active learning and semi-supervised learning (Shen et al., 2004; Mohit and Hwa, 2005), or incorporating external lexical knowledge (Ciaramita and Altun, 2005). However, th"
N06-1010,W03-0425,0,0.046428,"e certain types of named entities (NEs), such as persons, organizations and locations in news articles, and genes, proteins and chemicals in biomedical literature. NER is a fundamental task in many natural language processing applications, such as question answering, machine translation, text mining, and information retrieval (Srihari and Li, 1999; Huang and Vogel, 2002). Existing approaches to NER are mostly based on supervised learning. They can often achieve high accuracy provided that a large annotated training set similar to the test data is available (Borthwick, 1999; Zhou and Su, 2002; Florian et al., 2003; Klein et al., 2003; Finkel et al., 2005). Unfortunately, when the test data has some difference from the training data, these approaches tend to not perform well. For example, Ciaramita and Altun (2005) reported a performance degradation of a named entity recognizer trained on CoNLL 2003 Reuters corpus, where the F1 measure dropped from 0.908 when tested on a similar Reuters set to 0.643 when tested on a Wall Street Journal set. The degradation can be expected to be worse if the training data and the test data are more different. The performance degradation indicates that existing approaches"
N06-1010,W03-0428,0,0.0111407,"ed entities (NEs), such as persons, organizations and locations in news articles, and genes, proteins and chemicals in biomedical literature. NER is a fundamental task in many natural language processing applications, such as question answering, machine translation, text mining, and information retrieval (Srihari and Li, 1999; Huang and Vogel, 2002). Existing approaches to NER are mostly based on supervised learning. They can often achieve high accuracy provided that a large annotated training set similar to the test data is available (Borthwick, 1999; Zhou and Su, 2002; Florian et al., 2003; Klein et al., 2003; Finkel et al., 2005). Unfortunately, when the test data has some difference from the training data, these approaches tend to not perform well. For example, Ciaramita and Altun (2005) reported a performance degradation of a named entity recognizer trained on CoNLL 2003 Reuters corpus, where the F1 measure dropped from 0.908 when tested on a similar Reuters set to 0.643 when tested on a Wall Street Journal set. The degradation can be expected to be worse if the training data and the test data are more different. The performance degradation indicates that existing approaches adapt poorly to new"
N06-1010,W03-0430,0,0.0245501,"to incorporate the preference for top-ranked features. Figure 2, Figure 3 and Figure 4 show the performance of different feature ranking methods in the three sets of experiments as the parameter b for the rank-based prior changes. As we pointed out in Section 4, b is proportional to the logarithm of the number of “effective features”. 80 6 Related Work The NER problem has been extensively studied in the NLP community. Most existing work has focused on supervised learning approaches, employing models such as HMMs (Zhou and Su, 2002), MEMMs (Bender et al., 2003; Finkel et al., 2005), and CRFs (McCallum and Li, 2003). Collins and Singer (1999) proposed an unsupervised method for named entity classification based on the idea of cotraining. Ando and Zhang (2005) proposed a semisupervised learning method to exploit unlabeled data for building more robust NER systems. In all these studies, the evaluation is conducted on unlabeled data similar to the labeled data. Recently there have been some studies on adapting NER systems to new domains employing techniques such as active learning and semi-supervised learning (Shen et al., 2004; Mohit and Hwa, 2005), or incorporating external lexical knowledge (Ciaramita an"
N06-1010,P05-3015,0,0.063803,"Ms (Bender et al., 2003; Finkel et al., 2005), and CRFs (McCallum and Li, 2003). Collins and Singer (1999) proposed an unsupervised method for named entity classification based on the idea of cotraining. Ando and Zhang (2005) proposed a semisupervised learning method to exploit unlabeled data for building more robust NER systems. In all these studies, the evaluation is conducted on unlabeled data similar to the labeled data. Recently there have been some studies on adapting NER systems to new domains employing techniques such as active learning and semi-supervised learning (Shen et al., 2004; Mohit and Hwa, 2005), or incorporating external lexical knowledge (Ciaramita and Altun, 2005). However, there has not been any study on exploiting the domain structure contained in the training examples themselves to build generalizable NER systems. We focus on the domain structure in the training data to build a classifier that relies more on features generalizable across different domains to avoid overfitting the training domains. As our method is orthogonal to most of the aforementioned work, they can be combined to further improve the performance. Stanley F. Chen and Ronald Rosenfeld. 1999. A Gaussian prior f"
N06-1010,P04-1075,0,0.0128592,"and Su, 2002), MEMMs (Bender et al., 2003; Finkel et al., 2005), and CRFs (McCallum and Li, 2003). Collins and Singer (1999) proposed an unsupervised method for named entity classification based on the idea of cotraining. Ando and Zhang (2005) proposed a semisupervised learning method to exploit unlabeled data for building more robust NER systems. In all these studies, the evaluation is conducted on unlabeled data similar to the labeled data. Recently there have been some studies on adapting NER systems to new domains employing techniques such as active learning and semi-supervised learning (Shen et al., 2004; Mohit and Hwa, 2005), or incorporating external lexical knowledge (Ciaramita and Altun, 2005). However, there has not been any study on exploiting the domain structure contained in the training examples themselves to build generalizable NER systems. We focus on the domain structure in the training data to build a classifier that relies more on features generalizable across different domains to avoid overfitting the training domains. As our method is orthogonal to most of the aforementioned work, they can be combined to further improve the performance. Stanley F. Chen and Ronald Rosenfeld. 19"
N06-1010,P02-1060,0,0.234922,"phrases that denote certain types of named entities (NEs), such as persons, organizations and locations in news articles, and genes, proteins and chemicals in biomedical literature. NER is a fundamental task in many natural language processing applications, such as question answering, machine translation, text mining, and information retrieval (Srihari and Li, 1999; Huang and Vogel, 2002). Existing approaches to NER are mostly based on supervised learning. They can often achieve high accuracy provided that a large annotated training set similar to the test data is available (Borthwick, 1999; Zhou and Su, 2002; Florian et al., 2003; Klein et al., 2003; Finkel et al., 2005). Unfortunately, when the test data has some difference from the training data, these approaches tend to not perform well. For example, Ciaramita and Altun (2005) reported a performance degradation of a named entity recognizer trained on CoNLL 2003 Reuters corpus, where the F1 measure dropped from 0.908 when tested on a similar Reuters set to 0.643 when tested on a Wall Street Journal set. The degradation can be expected to be worse if the training data and the test data are more different. The performance degradation indicates th"
N06-1010,P05-1001,0,\N,Missing
N07-1015,H05-1091,0,0.935193,"statistical machine learning approach to this problem. There are generally two lines of work following this approach. The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1 . The second line of work designs kernel functions on some structured representation (sequences or trees) of the relation instances to capture the similarity between two relation instances (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006a; Zhang et al., 2006b). Of particular interest among the various kernels proposed are the convolution kernels (Bunescu and Mooney, 2005b; Zhang et al., 2006a), because they can efficiently compute the similarity between two instances in a huge feature space due to their recursive nature. Apart from their computational efficiency, convolution kernels also implicitly correspond to some feature space. Therefore, both lines of work rely on an appropriately de1 Although Zhao and Grishman (2005) defined a number of kernels for relation extraction, the"
N07-1015,P04-1054,0,0.869901,"f discriminative model-based statistical machine learning approach to this problem. There are generally two lines of work following this approach. The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1 . The second line of work designs kernel functions on some structured representation (sequences or trees) of the relation instances to capture the similarity between two relation instances (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006a; Zhang et al., 2006b). Of particular interest among the various kernels proposed are the convolution kernels (Bunescu and Mooney, 2005b; Zhang et al., 2006a), because they can efficiently compute the similarity between two instances in a huge feature space due to their recursive nature. Apart from their computational efficiency, convolution kernels also implicitly correspond to some feature space. Therefore, both lines of work rely on an appropriately de1 Although Zhao and Grishman (2005) defined a number of kernels fo"
N07-1015,P04-1053,0,0.0467648,"Missing"
N07-1015,P03-1005,0,0.00720066,"vely strict matching criteria. Bunescu and Mooney (2005a) proposed a dependency path kernel for relation extraction. This kernel also suffers from low recall for the same reason. Bunescu and Mooney (2005b) and Zhang et. al. (2006a; 2006b) applied convolution string kernels and tree kernels, respectively, to relation extraction. The convolution tree kernels achieved stateof-the-art performance. Since convolution kernels correspond to some explicit large feature spaces, the feature selection problem still remains. General structural representations of natural language data have been studied in (Suzuki et al., 2003; Cumby and Roth, 2003), but these models were not designed specifically for relation extraction. Our feature definition is similar to these models, but more specifically designed for relation extraction and systematic exploration of the feature space. Compared with (Cumby and Roth, 2003), our feature space is more compact and provides more guidance on selecting meaningful subspaces. 3 Task Definition Given a small piece of text that contains two entity mentions, the task of relation extraction is to decide whether the text states some semantic relation between the two entities, and if so, cla"
N07-1015,N06-1037,0,0.609729,". There are generally two lines of work following this approach. The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1 . The second line of work designs kernel functions on some structured representation (sequences or trees) of the relation instances to capture the similarity between two relation instances (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006a; Zhang et al., 2006b). Of particular interest among the various kernels proposed are the convolution kernels (Bunescu and Mooney, 2005b; Zhang et al., 2006a), because they can efficiently compute the similarity between two instances in a huge feature space due to their recursive nature. Apart from their computational efficiency, convolution kernels also implicitly correspond to some feature space. Therefore, both lines of work rely on an appropriately de1 Although Zhao and Grishman (2005) defined a number of kernels for relation extraction, the method is essentially similar to feature-based"
N07-1015,P06-1104,0,0.632563,". There are generally two lines of work following this approach. The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1 . The second line of work designs kernel functions on some structured representation (sequences or trees) of the relation instances to capture the similarity between two relation instances (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006a; Zhang et al., 2006b). Of particular interest among the various kernels proposed are the convolution kernels (Bunescu and Mooney, 2005b; Zhang et al., 2006a), because they can efficiently compute the similarity between two instances in a huge feature space due to their recursive nature. Apart from their computational efficiency, convolution kernels also implicitly correspond to some feature space. Therefore, both lines of work rely on an appropriately de1 Although Zhao and Grishman (2005) defined a number of kernels for relation extraction, the method is essentially similar to feature-based"
N07-1015,P05-1052,0,0.857074,"“the square”. Relation extraction has applications in many domains, including finding affiliation relations from web pages and finding protein-protein interactions from biomedical literature. Recent studies on relation extraction have shown the advantages of discriminative model-based statistical machine learning approach to this problem. There are generally two lines of work following this approach. The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1 . The second line of work designs kernel functions on some structured representation (sequences or trees) of the relation instances to capture the similarity between two relation instances (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006a; Zhang et al., 2006b). Of particular interest among the various kernels proposed are the convolution kernels (Bunescu and Mooney, 2005b; Zhang et al., 2006a), because they can efficiently compute the similarity between two instances in a huge feature space due to"
N07-1015,P05-1053,0,0.935768,"traction has applications in many domains, including finding affiliation relations from web pages and finding protein-protein interactions from biomedical literature. Recent studies on relation extraction have shown the advantages of discriminative model-based statistical machine learning approach to this problem. There are generally two lines of work following this approach. The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1 . The second line of work designs kernel functions on some structured representation (sequences or trees) of the relation instances to capture the similarity between two relation instances (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006a; Zhang et al., 2006b). Of particular interest among the various kernels proposed are the convolution kernels (Bunescu and Mooney, 2005b; Zhang et al., 2006a), because they can efficiently compute the similarity between two instances in a huge feature space due to their recursive natu"
N07-1015,A00-2030,0,\N,Missing
N13-1041,P12-1042,0,0.584872,"ons with other users or comments on topic aspects. The second is to use the latent vectors to group users based on viewpoints. We find that the latent factor representation can produce good prediction results for the first task and improve the clustering results of the second task compared with a number of baselines, showing the effectiveness of collaborative filtering for mining social relations from online discussions. 2 Related Work Our work is closely related to recent studies on detecting subgroups from online discussions (AbuJbara et al., 2012; Dasigi et al., 2012; Hassan et al., 2012). Abu-Jbara et al. (2012) proposed to build discussant attitude profiles (DAP) from online posts and use these profiles to cluster users into subgroups. A DAP is a vector that contains the attitudes of a discussant towards other discussants and a set of opinion targets. We also extract opinions of users towards other users and opinion targets from posts, which are similar to DAPs. The difference is that we further apply probabilistic matrix factorization to derive a low-rank representation from the raw opinion scores. Our comparison with DAP-based clustering shows that probabilistic matrix factorization can improve su"
N13-1041,baccianella-etal-2010-sentiwordnet,0,0.0119207,"ese aspects, some additional information such as Wikipedia entries and Google snippets may be considered. We will study this problem in our future work. 4.2 Opinion Expression Identification Our next step is to identify candidate opinion expressions. This problem has been studied in Hu and Liu (2004), Popescu and Etzioni (2005), and Hassan and Radev (2010). Based on previous work, we do the following. We first combine three popular sentiment lexicons to form a single sentiment lexicon: the lexicon used in Hu and Liu (2004), MPQA Subjectivity Lexicon by Wilson et al. (2005) and SentiWordNet by Baccianella et al. (2010). Our final sentiment lexicon contains 15,322 negative expressions and 10,144 positive expressions. We then identify candidate opinion expressions by searching for occurrences of words in this lexicon in the posts. 4.3 Opinion Relation Extraction Given a post that contains an aspect and an opinion expression, we still need to determine whether the opinion expression is used to describe the aspect. This is a relation extraction problem. We use a supervised learning approach based on dependency 1 2 http://opennlp.apache.org/ http://nlp.stanford.edu/ner/index.shtml 100 80 60 40 20 O BA M A B AM U"
N13-1041,W06-1651,0,0.0322435,"social network information with rating data using the PMF framework to perform social recommendation. Our model bears similarity to SocRec in that we also consider two types of interactions, i.e. user-user interactions and useraspect interactions. However, different from Ma et al. (2008), we predict both the user-user and useraspect scores from textual posts using sentiment analysis, and the user-user opinion polarity scores are symmetric. Part of our method uses sentiment analysis to extract opinions from text. This is built on top of a large body of existing work on opinion extraction, e.g. Choi et al. (2006) and Wu et al. (2009). As the sentiment analysis component is not our main contribution, we do not review existing work along this direction in detail here. Interested readers can refer to Pang and Lee (2008). The idea of incorporating sentiment analysis into collaborative filtering algorithms has been explored by Kawamae (2011), Moshfeghi et al. (2011) and Leung et al. (2011). While their work also combines sentiment analysis with collaborative filtering, the purpose is to improve the accuracy of item recommendation. In contrast, we borrow the idea and technique of collaborative filtering to"
N13-1041,P12-2013,0,0.0214548,"textual exchanges but from their interactions with other users or comments on topic aspects. The second is to use the latent vectors to group users based on viewpoints. We find that the latent factor representation can produce good prediction results for the first task and improve the clustering results of the second task compared with a number of baselines, showing the effectiveness of collaborative filtering for mining social relations from online discussions. 2 Related Work Our work is closely related to recent studies on detecting subgroups from online discussions (AbuJbara et al., 2012; Dasigi et al., 2012; Hassan et al., 2012). Abu-Jbara et al. (2012) proposed to build discussant attitude profiles (DAP) from online posts and use these profiles to cluster users into subgroups. A DAP is a vector that contains the attitudes of a discussant towards other discussants and a set of opinion targets. We also extract opinions of users towards other users and opinion targets from posts, which are similar to DAPs. The difference is that we further apply probabilistic matrix factorization to derive a low-rank representation from the raw opinion scores. Our comparison with DAP-based clustering shows that pr"
N13-1041,P10-1041,0,0.0223413,"pressions from at least M users. We set M to 2 in our experiments. Figure 1 shows the top salient aspects for the thread on “Will you vote for Obama?” We acknowledge there are still duplicate aspects in the results like “Republican Party” and “GOP”. To normalize these aspects, some additional information such as Wikipedia entries and Google snippets may be considered. We will study this problem in our future work. 4.2 Opinion Expression Identification Our next step is to identify candidate opinion expressions. This problem has been studied in Hu and Liu (2004), Popescu and Etzioni (2005), and Hassan and Radev (2010). Based on previous work, we do the following. We first combine three popular sentiment lexicons to form a single sentiment lexicon: the lexicon used in Hu and Liu (2004), MPQA Subjectivity Lexicon by Wilson et al. (2005) and SentiWordNet by Baccianella et al. (2010). Our final sentiment lexicon contains 15,322 negative expressions and 10,144 positive expressions. We then identify candidate opinion expressions by searching for occurrences of words in this lexicon in the posts. 4.3 Opinion Relation Extraction Given a post that contains an aspect and an opinion expression, we still need to deter"
N13-1041,D12-1006,0,0.169921,"t from their interactions with other users or comments on topic aspects. The second is to use the latent vectors to group users based on viewpoints. We find that the latent factor representation can produce good prediction results for the first task and improve the clustering results of the second task compared with a number of baselines, showing the effectiveness of collaborative filtering for mining social relations from online discussions. 2 Related Work Our work is closely related to recent studies on detecting subgroups from online discussions (AbuJbara et al., 2012; Dasigi et al., 2012; Hassan et al., 2012). Abu-Jbara et al. (2012) proposed to build discussant attitude profiles (DAP) from online posts and use these profiles to cluster users into subgroups. A DAP is a vector that contains the attitudes of a discussant towards other discussants and a set of opinion targets. We also extract opinions of users towards other users and opinion targets from posts, which are similar to DAPs. The difference is that we further apply probabilistic matrix factorization to derive a low-rank representation from the raw opinion scores. Our comparison with DAP-based clustering shows that probabilistic matrix fac"
N13-1041,P03-1054,0,0.00342836,"f money. PAKISTAN is increasing terrorist threat. OBAMA was a top scorer for occidental college. OBAMA is smarter than people. Table 1: Examples of frequent dependency path rules in our training data. OP and TR refer to the opinion and the target. The opinion words are in italic and the aspect words are in uppercase. paths. Previous work by Mintz et al. (2009), and Qiu et al. (2009) has shown that the shortest path between a candidate opinion aspect and a candidate opinion expression in the dependency parse tree can be effective in extracting opinion relations. We use the Stanford Parser from Klein and Manning (2003) to obtain the dependency parse trees for each sentence in the posts and then get the dependency paths between each pair of candidate aspect and opinion expression. We use dependency relations and POS tags of nodes along the path to represent a dependency path. Given a set of training sentences (we use the one from Wu et al. (2009)), we can get a set of dependency path rules based on their frequencies in the training data. Table 1 shows the frequent dependency path rules in our training data. When a pair of aspect and opinion expression is identified to be related, we use the polarity of the o"
N13-1041,P09-1113,0,0.0485699,"→ nsubj → NT R ADVOP ← advmod ← V → nsubj → NT R I simply point out how terrible REPUBLICAN PARTY is. BUSH is even more reasonable for tax hike than Obama. I would never support OBAMA. I’ll vote for OBAMA. DEMOCRATIC PARTY are ultimately corrupted by love of money. PAKISTAN is increasing terrorist threat. OBAMA was a top scorer for occidental college. OBAMA is smarter than people. Table 1: Examples of frequent dependency path rules in our training data. OP and TR refer to the opinion and the target. The opinion words are in italic and the aspect words are in uppercase. paths. Previous work by Mintz et al. (2009), and Qiu et al. (2009) has shown that the shortest path between a candidate opinion aspect and a candidate opinion expression in the dependency parse tree can be effective in extracting opinion relations. We use the Stanford Parser from Klein and Manning (2003) to obtain the dependency parse trees for each sentence in the posts and then get the dependency paths between each pair of candidate aspect and opinion expression. We use dependency relations and POS tags of nodes along the path to represent a dependency path. Given a set of training sentences (we use the one from Wu et al. (2009)), we"
N13-1041,H05-1043,0,0.0497182,"te aspects which have opinion expressions from at least M users. We set M to 2 in our experiments. Figure 1 shows the top salient aspects for the thread on “Will you vote for Obama?” We acknowledge there are still duplicate aspects in the results like “Republican Party” and “GOP”. To normalize these aspects, some additional information such as Wikipedia entries and Google snippets may be considered. We will study this problem in our future work. 4.2 Opinion Expression Identification Our next step is to identify candidate opinion expressions. This problem has been studied in Hu and Liu (2004), Popescu and Etzioni (2005), and Hassan and Radev (2010). Based on previous work, we do the following. We first combine three popular sentiment lexicons to form a single sentiment lexicon: the lexicon used in Hu and Liu (2004), MPQA Subjectivity Lexicon by Wilson et al. (2005) and SentiWordNet by Baccianella et al. (2010). Our final sentiment lexicon contains 15,322 negative expressions and 10,144 positive expressions. We then identify candidate opinion expressions by searching for occurrences of words in this lexicon in the posts. 4.3 Opinion Relation Extraction Given a post that contains an aspect and an opinion expre"
N13-1041,H05-1044,0,0.0436736,"ublican Party” and “GOP”. To normalize these aspects, some additional information such as Wikipedia entries and Google snippets may be considered. We will study this problem in our future work. 4.2 Opinion Expression Identification Our next step is to identify candidate opinion expressions. This problem has been studied in Hu and Liu (2004), Popescu and Etzioni (2005), and Hassan and Radev (2010). Based on previous work, we do the following. We first combine three popular sentiment lexicons to form a single sentiment lexicon: the lexicon used in Hu and Liu (2004), MPQA Subjectivity Lexicon by Wilson et al. (2005) and SentiWordNet by Baccianella et al. (2010). Our final sentiment lexicon contains 15,322 negative expressions and 10,144 positive expressions. We then identify candidate opinion expressions by searching for occurrences of words in this lexicon in the posts. 4.3 Opinion Relation Extraction Given a post that contains an aspect and an opinion expression, we still need to determine whether the opinion expression is used to describe the aspect. This is a relation extraction problem. We use a supervised learning approach based on dependency 1 2 http://opennlp.apache.org/ http://nlp.stanford.edu/n"
N13-1041,D09-1159,0,0.10161,"ion with rating data using the PMF framework to perform social recommendation. Our model bears similarity to SocRec in that we also consider two types of interactions, i.e. user-user interactions and useraspect interactions. However, different from Ma et al. (2008), we predict both the user-user and useraspect scores from textual posts using sentiment analysis, and the user-user opinion polarity scores are symmetric. Part of our method uses sentiment analysis to extract opinions from text. This is built on top of a large body of existing work on opinion extraction, e.g. Choi et al. (2006) and Wu et al. (2009). As the sentiment analysis component is not our main contribution, we do not review existing work along this direction in detail here. Interested readers can refer to Pang and Lee (2008). The idea of incorporating sentiment analysis into collaborative filtering algorithms has been explored by Kawamae (2011), Moshfeghi et al. (2011) and Leung et al. (2011). While their work also combines sentiment analysis with collaborative filtering, the purpose is to improve the accuracy of item recommendation. In contrast, we borrow the idea and technique of collaborative filtering to improve user relation"
N13-1041,H05-2017,0,\N,Missing
N13-1123,W11-0702,0,0.0319093,"Missing"
N13-1123,P12-3023,0,0.0286336,"xicon-based approach that does not need training data. Recently, Mukherjee and Liu (2012) proposed an unsupervised model to extract different types of expressions including agreement/disagreement expressions. However, our focus is not to detect agreement/disagreement expressions but to model the interplay between agreement/disagreement expressions and viewpoints. The work by Mukherjee and Liu (2012) can potentially be combined with our model. Another line of related work is subgroup detection, which aims to separate users holding different viewpoints. This problem has recently been studied by Abu-Jbara and Radev (2012), Dasigi et al. (2012), Abu-Jbara et al. (2012) and Hassan et al. (2012), where a clustering based approach is used. Lu et al. (2012) studied both textual content and social interactions to find opposing network from online forums. In our experiments we show that our model can also be used for subgroup detection, but meanwhile we also directly identify viewpoints, which is not the goal of existing work on subgroup finding or opposing network extraction. 3 3.1 clearly have a viewpoint. In our model, we assume that there is a user-level viewpoint distribution. For each post by a user, its viewpo"
N13-1123,P12-1042,0,0.0756007,"ata. Recently, Mukherjee and Liu (2012) proposed an unsupervised model to extract different types of expressions including agreement/disagreement expressions. However, our focus is not to detect agreement/disagreement expressions but to model the interplay between agreement/disagreement expressions and viewpoints. The work by Mukherjee and Liu (2012) can potentially be combined with our model. Another line of related work is subgroup detection, which aims to separate users holding different viewpoints. This problem has recently been studied by Abu-Jbara and Radev (2012), Dasigi et al. (2012), Abu-Jbara et al. (2012) and Hassan et al. (2012), where a clustering based approach is used. Lu et al. (2012) studied both textual content and social interactions to find opposing network from online forums. In our experiments we show that our model can also be used for subgroup detection, but meanwhile we also directly identify viewpoints, which is not the goal of existing work on subgroup finding or opposing network extraction. 3 3.1 clearly have a viewpoint. In our model, we assume that there is a user-level viewpoint distribution. For each post by a user, its viewpoint is drawn from the corresponding viewpoint d"
N13-1123,andreas-etal-2012-annotating,0,0.0194879,") , Z¬(u,n,l) , Y, W, γ, η, β, β 0 C¬(u,n,l) +γ (·) C¬(u,n,l) + 2γ 4 B ) B . (6) CB,¬(u,n,l) + V β B Here again the Cs are counters defined in similar 1 ways as before. For example, C¬(u,n,l) is the number of times we observe 1 assigned to an x variable, excluding xu,n,l . 3.4 Models for Comparison ) wu,n,l +η u,n,l ,¬(u,n,l) (·) Cy + u,n,l ,¬(u,n,l) B Interaction polarity prediction The problem of detecting agreement and disagreement from forum posts is relatively new. One possible solution is to use supervised learning, which requires training data (Galley et al., 2004; Abbott et al., 2011; Andreas et al., 2012). However, training 1035 In our experiments, we compare our model, Joint Viewpoint-Topic Model with User Interaction (JVTM-UI), with the following baseline models. JVTM: The model is shown in Figure 3(a), a variant of JVTM-UI that does not consider user interaction. Through comparison with it, we can check the effect of modeling user interactions. JVTM-G: We consider JVTM-G in Figure 3(b), a variant of JVTM which assumes a global viewpoint distribution. Comparison with it allows us to check the usefulness of user identity in the task. UIM: The third model we consider is a User Interaction Mode"
N13-1123,baccianella-etal-2010-sentiwordnet,0,0.00322632,"ict the polarity of interaction expressions. Specifically, we first identify interaction sentences following the strategies from Hassan et al. (2012). We assume sentences containing mentions of the recipient of a post are interaction sentences. Next, we consider words within a text window of 8 words surrounding these mentions. We then use a subjectivity lexicon to label these words. To form an English lexicon, we combine three popular lexicons: the sentiment lexicon used by Hu and Liu (2004), Multi-Perspective Question Answering Subjectivity Lexicon by Wilson et al. (2005) and SentiWordNet by Baccianella et al. (2010). Since we also work with a Chinese data set, to form the Chinese sentiment lexicon, we use opinion words from HowNet2 and NTUSD by Ku et al. (2007). To predict the polarity of an interaction expression, we simply check whether there are more positive sentiment words or more negative sentiment words in the expression, and label the interaction expression accordingly. We would like to stress that since this interaction classification step is independent of the latent variable model, we can always apply a more accurate method, but this is not the focus of this work. p(S|yu,n = k, Y¬(u,n) , δ) ="
N13-1123,P12-2013,0,0.0161082,"es not need training data. Recently, Mukherjee and Liu (2012) proposed an unsupervised model to extract different types of expressions including agreement/disagreement expressions. However, our focus is not to detect agreement/disagreement expressions but to model the interplay between agreement/disagreement expressions and viewpoints. The work by Mukherjee and Liu (2012) can potentially be combined with our model. Another line of related work is subgroup detection, which aims to separate users holding different viewpoints. This problem has recently been studied by Abu-Jbara and Radev (2012), Dasigi et al. (2012), Abu-Jbara et al. (2012) and Hassan et al. (2012), where a clustering based approach is used. Lu et al. (2012) studied both textual content and social interactions to find opposing network from online forums. In our experiments we show that our model can also be used for subgroup detection, but meanwhile we also directly identify viewpoints, which is not the goal of existing work on subgroup finding or opposing network extraction. 3 3.1 clearly have a viewpoint. In our model, we assume that there is a user-level viewpoint distribution. For each post by a user, its viewpoint is drawn from the"
N13-1123,P04-1085,0,0.0605762,"010) studied how to identify stances in online debates. They used a supervised approach for classifying stances in ideological debates. In comparison, our model is an unsupervised method. The same authors proposed an unsupervised method which relies on associations of aspects with topics indicative of stances mined from the Web for the task (Somasundaran and Wiebe, 2009). In contrast, our model is also an unsupervised one but we do not rely on any external knowledge. Part of our work is related to detecting agreement/disagreement from text. For this task, normally supervised methods are used (Galley et al., 2004; Abbott et al., 2011), which require sufficient labeled training data. In our work, since we deal with different languages, we use a lexicon-based approach that does not need training data. Recently, Mukherjee and Liu (2012) proposed an unsupervised model to extract different types of expressions including agreement/disagreement expressions. However, our focus is not to detect agreement/disagreement expressions but to model the interplay between agreement/disagreement expressions and viewpoints. The work by Mukherjee and Liu (2012) can potentially be combined with our model. Another line of r"
N13-1123,D12-1006,0,0.29198,"Liu (2012) proposed an unsupervised model to extract different types of expressions including agreement/disagreement expressions. However, our focus is not to detect agreement/disagreement expressions but to model the interplay between agreement/disagreement expressions and viewpoints. The work by Mukherjee and Liu (2012) can potentially be combined with our model. Another line of related work is subgroup detection, which aims to separate users holding different viewpoints. This problem has recently been studied by Abu-Jbara and Radev (2012), Dasigi et al. (2012), Abu-Jbara et al. (2012) and Hassan et al. (2012), where a clustering based approach is used. Lu et al. (2012) studied both textual content and social interactions to find opposing network from online forums. In our experiments we show that our model can also be used for subgroup detection, but meanwhile we also directly identify viewpoints, which is not the goal of existing work on subgroup finding or opposing network extraction. 3 3.1 clearly have a viewpoint. In our model, we assume that there is a user-level viewpoint distribution. For each post by a user, its viewpoint is drawn from the corresponding viewpoint distribution. User interac"
N13-1123,P12-1034,0,0.0949826,"upervised method which relies on associations of aspects with topics indicative of stances mined from the Web for the task (Somasundaran and Wiebe, 2009). In contrast, our model is also an unsupervised one but we do not rely on any external knowledge. Part of our work is related to detecting agreement/disagreement from text. For this task, normally supervised methods are used (Galley et al., 2004; Abbott et al., 2011), which require sufficient labeled training data. In our work, since we deal with different languages, we use a lexicon-based approach that does not need training data. Recently, Mukherjee and Liu (2012) proposed an unsupervised model to extract different types of expressions including agreement/disagreement expressions. However, our focus is not to detect agreement/disagreement expressions but to model the interplay between agreement/disagreement expressions and viewpoints. The work by Mukherjee and Liu (2012) can potentially be combined with our model. Another line of related work is subgroup detection, which aims to separate users holding different viewpoints. This problem has recently been studied by Abu-Jbara and Radev (2012), Dasigi et al. (2012), Abu-Jbara et al. (2012) and Hassan et a"
N13-1123,D10-1007,0,0.301896,"discussion threads, we often observe heated debates over a controversial issue, with different sides defending their viewpoints with different arguments. For example, after the presidential debate between Barack Obama and Mitt Romney, there were heated discussions in online forums like CreateDebate1 where some people expressed their support for Obama while some 1 Jing Jiang School of Information Systems Singapore Management University Singapore jingjiang@smu.edu.sg http://www.createdebate.com/ Recently there has been some work on finding contrastive viewpoints from text. The model proposed by Paul et al. (2010) assumes viewpoints and topics are orthogonal dimensions. Another model proposed by Fang et al. (2012) assumes that documents are already grouped by viewpoints and it focus on identifying contrastive viewpoint words under the same topic. However, these existing studies are not based on interdependent documents like threaded forum posts. As a result, at least two important characteristics of threaded forum data are not considered in these models. (1) User identity: The user or publisher of each forum post is known, and a user may publish several posts in the same thread. Since the same user’s o"
N13-1123,P09-1026,0,0.0235514,"sume that documents are already grouped by viewpoints, which is not the case for forum posts. Therefore, their model cannot be directly applied to forum posts. There has also been some work on finding viewpoints from social media. Somasundaran and Wiebe (2010) studied how to identify stances in online debates. They used a supervised approach for classifying stances in ideological debates. In comparison, our model is an unsupervised method. The same authors proposed an unsupervised method which relies on associations of aspects with topics indicative of stances mined from the Web for the task (Somasundaran and Wiebe, 2009). In contrast, our model is also an unsupervised one but we do not rely on any external knowledge. Part of our work is related to detecting agreement/disagreement from text. For this task, normally supervised methods are used (Galley et al., 2004; Abbott et al., 2011), which require sufficient labeled training data. In our work, since we deal with different languages, we use a lexicon-based approach that does not need training data. Recently, Mukherjee and Liu (2012) proposed an unsupervised model to extract different types of expressions including agreement/disagreement expressions. However,"
N13-1123,W10-0214,0,0.00427947,"nsider user identity. In comparison, our model has the notion of topics and viewpoints, but we explicitly model the dependency of topics on viewpoints, i.e. we assume each viewpoint has a topic distribution. We also consider author identities as an important factor of our model. Fang et al. (2012) proposed a model that also combines topics and viewpoints. But they assume that documents are already grouped by viewpoints, which is not the case for forum posts. Therefore, their model cannot be directly applied to forum posts. There has also been some work on finding viewpoints from social media. Somasundaran and Wiebe (2010) studied how to identify stances in online debates. They used a supervised approach for classifying stances in ideological debates. In comparison, our model is an unsupervised method. The same authors proposed an unsupervised method which relies on associations of aspects with topics indicative of stances mined from the Web for the task (Somasundaran and Wiebe, 2009). In contrast, our model is also an unsupervised one but we do not rely on any external knowledge. Part of our work is related to detecting agreement/disagreement from text. For this task, normally supervised methods are used (Gall"
N13-1123,H05-1044,0,0.00234591,"con together with some heuristics to predict the polarity of interaction expressions. Specifically, we first identify interaction sentences following the strategies from Hassan et al. (2012). We assume sentences containing mentions of the recipient of a post are interaction sentences. Next, we consider words within a text window of 8 words surrounding these mentions. We then use a subjectivity lexicon to label these words. To form an English lexicon, we combine three popular lexicons: the sentiment lexicon used by Hu and Liu (2004), Multi-Perspective Question Answering Subjectivity Lexicon by Wilson et al. (2005) and SentiWordNet by Baccianella et al. (2010). Since we also work with a Chinese data set, to form the Chinese sentiment lexicon, we use opinion words from HowNet2 and NTUSD by Ku et al. (2007). To predict the polarity of an interaction expression, we simply check whether there are more positive sentiment words or more negative sentiment words in the expression, and label the interaction expression accordingly. We would like to stress that since this interaction classification step is independent of the latent variable model, we can always apply a more accurate method, but this is not the foc"
N16-1170,D15-1075,0,0.573279,"by Rockt¨aschel et al. (2016) in a different way but the underlying model is the same. Our hak is their rt , our Hs (all of hsj ) is their Y, our htk is their ht , and our αk is their αt . Our presentation is close to the one by Bahdanau et al. (2015), with our attention vectors a corresponding to the context vectors c in their paper. 1444 whole premise, together with htN , which can be approximately regarded as an aggregated representation of the hypothesis2 , to predict the label y. 2.2 Our Model Although the neural attention model by Rockt¨aschel et al. (2016) achieved better results than Bowman et al. (2015), we see two limitations. First, the model still uses a single vector representation of the premise, namely haN , to match the entire hypothesis. We speculate that if we instead use each of the attention-weighted representations of the premise for matching, i.e., use ak at position k to match the hidden state htk of the hypothesis while we go through the hypothesis, we could achieve better matching results. This can be done using an RNN which at each position takes in both ak and htk as its input and determines how well the overall matching of the two sentences is up to the current position. I"
N16-1170,N13-1092,0,0.0160799,"Missing"
N16-1170,D08-1084,0,0.0234122,"Missing"
N16-1170,marelli-etal-2014-sick,0,0.0175344,"Missing"
N16-1170,D14-1162,0,0.120813,"k + Vmo hm k−1 + b ), Implementation Details Besides the difference of the LSTM architecture, we also introduce a few other changes from the model by Rockt¨aschel et al. (2016). First, we insert a special word NULL to the premise, and we allow words in the hypothesis to be aligned with this NULL. This is inspired by common practice in machine translation. Specifically, we introduce a vector hs0 , which is fixed to be a vector of 0s of dimension d. This hs0 represents NULL and is used with other hsj to derive the attention vectors {ak }N k=1 . Second, we use word embeddings trained from GloVe (Pennington et al., 2014) instead of word2vec vectors. The main reason is that GloVe covers more words in the SNLI corpus than word2vec3 . Third, for words which do not have pre-trained word embeddings, we take the average of the embeddings of all the words (in GloVe) surrounding the unseen word within a window size of 9 (4 on the left and 4 on the right) as an approximation of the embedding of this unseen word. Then we do not update any word embedding when learning our model. Although this is a very crude approximation, it reduces 3 The SNLI corpus contains 37K unique tokens. Around 12.1K of them cannot be found in w"
N16-1170,D15-1044,0,0.047435,"Missing"
N19-1094,D15-1075,0,0.0312843,"onent of natural language understanding, commonsense reasoning has been included in an increasing number of tasks for evaluation: COPA (Roemmele et al., 2011) assesses commonsense causal reasoning by selecting an alternative, which has a more plausible causal relation with the given premise. Story Cloze Test (ROCStories, Mostafazadeh et al. 2016) evaluates story understanding, story generation, and script learning by choosing the most sensible ending to a short story. JOCI (Zhang et al., 2017) generalizes the natural language inference (NLI) framework (Cooper et al., 1996; Dagan et al., 2006; Bowman et al., 2015; Williams et al., 2018) and evaluates commonsense inference by predicting the ordinal likelihood of a hypothesis given a context. Event2Mind (Rashkin et al., 2018b) models stereotypical intents and reactions of people, described in short free-form text. SWAG (Zellers et al., 2018) frames commonsense inference as multiple-choice questions for follow-up events given some context. ReCoRD (Zhang et al., 2018) 1 We don’t believe it is possible to construct such a knowledge base given that the world is changing constantly. 2 DSSMs can be applied to a wide range of tasks depending on the definition"
N19-1094,D16-1245,0,0.0172502,"ith candidates. To make a fair comparison to Trinh and Le (2018)’s work, we also train our single model on the corpus of Gutenberg only. We can see that both of our methods get significant improvement on the PDP dataset, and our UDSSM-II can achieve much better performance on the WSC dataset. We also report our ensemble model (nine models with different hyperparameters) trained with both corpus of Gutenberg and 1 Billion Word, and it also achieve better performance than Google Language Model trained with the same corpus. Finally, we also compare to the pre-trained Coreference Resolution Tool (Clark and Manning, 2016a,b)12 , and we can see that it doesn’t adapt to our commonsense reasoning tasks and can’t tell 4.4 Analysis WSC 1: Ours 1: WSC 2: Ours 2: Paul tried to call George on the phone, but he wasn’t successful. He tried to call 911 using her cell phone but that he could n’t get the phone to work. Paul tried to call George on the phone, but he was n’t available . He tried twice to call her but she did not answer the phone . Table 3: Comparison of the data from WSC and our training data. Our sentences are retrieved from the UDSSM-II training dataset based on the BM25 value for analysis. The pseudo lab"
N19-1094,P16-1061,0,0.0223517,"ith candidates. To make a fair comparison to Trinh and Le (2018)’s work, we also train our single model on the corpus of Gutenberg only. We can see that both of our methods get significant improvement on the PDP dataset, and our UDSSM-II can achieve much better performance on the WSC dataset. We also report our ensemble model (nine models with different hyperparameters) trained with both corpus of Gutenberg and 1 Billion Word, and it also achieve better performance than Google Language Model trained with the same corpus. Finally, we also compare to the pre-trained Coreference Resolution Tool (Clark and Manning, 2016a,b)12 , and we can see that it doesn’t adapt to our commonsense reasoning tasks and can’t tell 4.4 Analysis WSC 1: Ours 1: WSC 2: Ours 2: Paul tried to call George on the phone, but he wasn’t successful. He tried to call 911 using her cell phone but that he could n’t get the phone to work. Paul tried to call George on the phone, but he was n’t available . He tried twice to call her but she did not answer the phone . Table 3: Comparison of the data from WSC and our training data. Our sentences are retrieved from the UDSSM-II training dataset based on the BM25 value for analysis. The pseudo lab"
N19-1094,P18-1043,0,0.170671,"(Miller, 1995) for external supervision to train word embeddings and solve the WSC challenge. Recently, Trinh and Le (2018) first used raw text from books/news to train a neural Language Model (LM), and then emIntroduction Commonsense reasoning is concerned with simulating the human ability to make presumptions about the type and essence of ordinary situations they encounter every day (Davis and Marcus, 2015). It is one of the key challenges in natural language understanding, and has drawn increasing attention in recent years (Levesque et al., 2011; Roemmele et al., 2011; Zhang et al., 2017; Rashkin et al., 2018a,b; Zellers et al., 2018; Trinh and Le, 2018). However, due to the lack of labeled training data or comprehensive hand-crafted knowledge bases, commonsense reasoning tasks such as Winograd Schema Challenge (Levesque et al., 2011) are still far from being solved. In this work, we propose two effective unsupervised models for commonsense reasoning, and evaluate them on two classic commonsense reasoning tasks: Winograd Schema Challenge (WSC) and Pronoun Disambiguation Problems (PDP). Compared to other commonsense reasoning tasks, ∗ B. The demonstraWork done when the author was at Microsoft 882 P"
N19-1094,N16-1098,0,0.0368582,"different types of pairwise training data, thus the model structures are different, as illustrated in Figure 1(b) and 1(c), respectively. Experiments demonstrated that our methods outperform stat-of-the-art performance on the tasks of WSC and PDP. 2 Related Work As a key component of natural language understanding, commonsense reasoning has been included in an increasing number of tasks for evaluation: COPA (Roemmele et al., 2011) assesses commonsense causal reasoning by selecting an alternative, which has a more plausible causal relation with the given premise. Story Cloze Test (ROCStories, Mostafazadeh et al. 2016) evaluates story understanding, story generation, and script learning by choosing the most sensible ending to a short story. JOCI (Zhang et al., 2017) generalizes the natural language inference (NLI) framework (Cooper et al., 1996; Dagan et al., 2006; Bowman et al., 2015; Williams et al., 2018) and evaluates commonsense inference by predicting the ordinal likelihood of a hypothesis given a context. Event2Mind (Rashkin et al., 2018b) models stereotypical intents and reactions of people, described in short free-form text. SWAG (Zellers et al., 2018) frames commonsense inference as multiple-choic"
N19-1094,J01-4004,0,0.563139,"In the following sections, we will use uppercase symbols in bold, e.g., Sx , to represent matrices. Lowercase symbols in bold, e.g., hx , represent vectors. A regular uppercase symbol, e.g., S x , represents a lexical sequence. A regular lowercase symbol, e.g., xi or y, represents a token. Among all these commonsense reasoning tasks, the Winograd Schema Challenge (WSC) and Pronoun Disambiguation Problems (PDP) (Levesque et al., 2011) are known as the most challenging tasks for commonsense reasoning. Although both tasks are based on pronoun disambiguation, a subtask of coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Peng et al., 2016), PDP and WSC differ from normal pronoun disambiguation due to their unique properties, which are based on commonsense, selecting the most likely antecedent from both candidates in the directly preceding context. Previous efforts on solving the Winograd Schema Challenge and Pronoun Disambiguation Problems mostly rely on human-labeled data, sophisticated rules, hand-crafted features, or external knowledge bases (Peng et al., 2015; Bailey et al., 2015; Sch¨uller, 2014). Rahman and Ng (2012) hired workers to annotate supervised training data and designed 7"
N19-1094,P02-1014,0,0.464835,"ections, we will use uppercase symbols in bold, e.g., Sx , to represent matrices. Lowercase symbols in bold, e.g., hx , represent vectors. A regular uppercase symbol, e.g., S x , represents a lexical sequence. A regular lowercase symbol, e.g., xi or y, represents a token. Among all these commonsense reasoning tasks, the Winograd Schema Challenge (WSC) and Pronoun Disambiguation Problems (PDP) (Levesque et al., 2011) are known as the most challenging tasks for commonsense reasoning. Although both tasks are based on pronoun disambiguation, a subtask of coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Peng et al., 2016), PDP and WSC differ from normal pronoun disambiguation due to their unique properties, which are based on commonsense, selecting the most likely antecedent from both candidates in the directly preceding context. Previous efforts on solving the Winograd Schema Challenge and Pronoun Disambiguation Problems mostly rely on human-labeled data, sophisticated rules, hand-crafted features, or external knowledge bases (Peng et al., 2015; Bailey et al., 2015; Sch¨uller, 2014). Rahman and Ng (2012) hired workers to annotate supervised training data and designed 70K hand-crafted featu"
N19-1094,N15-1082,0,0.0589557,"tasks for commonsense reasoning. Although both tasks are based on pronoun disambiguation, a subtask of coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Peng et al., 2016), PDP and WSC differ from normal pronoun disambiguation due to their unique properties, which are based on commonsense, selecting the most likely antecedent from both candidates in the directly preceding context. Previous efforts on solving the Winograd Schema Challenge and Pronoun Disambiguation Problems mostly rely on human-labeled data, sophisticated rules, hand-crafted features, or external knowledge bases (Peng et al., 2015; Bailey et al., 2015; Sch¨uller, 2014). Rahman and Ng (2012) hired workers to annotate supervised training data and designed 70K hand-crafted features. Sharma et al. (2015); Sch¨uller (2014); Bailey et al. (2015); Liu et al. (2017) utilized expensive knowledge bases in their reasoning processes. Recently, Trinh and Le 2018 applied neural language models trained with a massive amount of unlabeled data to the Winograd Schema Challenge and improved the performance by a large margin. In contrast, our unsupervised method based on DSSM significantly outperforms the previous state-of-theart method,"
N19-1094,N18-1101,0,0.0235238,"uage understanding, commonsense reasoning has been included in an increasing number of tasks for evaluation: COPA (Roemmele et al., 2011) assesses commonsense causal reasoning by selecting an alternative, which has a more plausible causal relation with the given premise. Story Cloze Test (ROCStories, Mostafazadeh et al. 2016) evaluates story understanding, story generation, and script learning by choosing the most sensible ending to a short story. JOCI (Zhang et al., 2017) generalizes the natural language inference (NLI) framework (Cooper et al., 1996; Dagan et al., 2006; Bowman et al., 2015; Williams et al., 2018) and evaluates commonsense inference by predicting the ordinal likelihood of a hypothesis given a context. Event2Mind (Rashkin et al., 2018b) models stereotypical intents and reactions of people, described in short free-form text. SWAG (Zellers et al., 2018) frames commonsense inference as multiple-choice questions for follow-up events given some context. ReCoRD (Zhang et al., 2018) 1 We don’t believe it is possible to construct such a knowledge base given that the world is changing constantly. 2 DSSMs can be applied to a wide range of tasks depending on the definition of (x, y). For example,"
N19-1094,D16-1038,0,0.0539894,"uppercase symbols in bold, e.g., Sx , to represent matrices. Lowercase symbols in bold, e.g., hx , represent vectors. A regular uppercase symbol, e.g., S x , represents a lexical sequence. A regular lowercase symbol, e.g., xi or y, represents a token. Among all these commonsense reasoning tasks, the Winograd Schema Challenge (WSC) and Pronoun Disambiguation Problems (PDP) (Levesque et al., 2011) are known as the most challenging tasks for commonsense reasoning. Although both tasks are based on pronoun disambiguation, a subtask of coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Peng et al., 2016), PDP and WSC differ from normal pronoun disambiguation due to their unique properties, which are based on commonsense, selecting the most likely antecedent from both candidates in the directly preceding context. Previous efforts on solving the Winograd Schema Challenge and Pronoun Disambiguation Problems mostly rely on human-labeled data, sophisticated rules, hand-crafted features, or external knowledge bases (Peng et al., 2015; Bailey et al., 2015; Sch¨uller, 2014). Rahman and Ng (2012) hired workers to annotate supervised training data and designed 70K hand-crafted features. Sharma et al. ("
N19-1094,D18-1009,0,0.130991,"nal supervision to train word embeddings and solve the WSC challenge. Recently, Trinh and Le (2018) first used raw text from books/news to train a neural Language Model (LM), and then emIntroduction Commonsense reasoning is concerned with simulating the human ability to make presumptions about the type and essence of ordinary situations they encounter every day (Davis and Marcus, 2015). It is one of the key challenges in natural language understanding, and has drawn increasing attention in recent years (Levesque et al., 2011; Roemmele et al., 2011; Zhang et al., 2017; Rashkin et al., 2018a,b; Zellers et al., 2018; Trinh and Le, 2018). However, due to the lack of labeled training data or comprehensive hand-crafted knowledge bases, commonsense reasoning tasks such as Winograd Schema Challenge (Levesque et al., 2011) are still far from being solved. In this work, we propose two effective unsupervised models for commonsense reasoning, and evaluate them on two classic commonsense reasoning tasks: Winograd Schema Challenge (WSC) and Pronoun Disambiguation Problems (PDP). Compared to other commonsense reasoning tasks, ∗ B. The demonstraWork done when the author was at Microsoft 882 Proceedings of NAACL-HLT 2"
N19-1094,N18-1202,0,0.0325233,"lassification model based on whether the word pairs appear in the knowledge base. One limitation of these methods is that they rely heavily on the external knowledge bases. Another limitation is that they just linearly aggregate the embeddings of the words in the context, and that’s hard to integrate the word order information. Instead, our model with LSTM can better represent the contextual information. Besides, our model don’t need any external knowledge bases, and achieve a significant improvement on both of the datasets. We further compare our models with the unsupervised baselines, ELMo (Peters et al., 2018) which selects the candidate based on the cosine similarity of the hidden states of noun and pronoun. Another unsupervised baseline, Google Language Model for commonsense reasoning (Trinh and Le, 2018), which compares the perplexities of the new sentences by replacing the pronoun with candidates. To make a fair comparison to Trinh and Le (2018)’s work, we also train our single model on the corpus of Gutenberg only. We can see that both of our methods get significant improvement on the PDP dataset, and our UDSSM-II can achieve much better performance on the WSC dataset. We also report our ensem"
N19-1094,D12-1071,0,0.431322,"ased on pronoun disambiguation, a subtask of coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Peng et al., 2016), PDP and WSC differ from normal pronoun disambiguation due to their unique properties, which are based on commonsense, selecting the most likely antecedent from both candidates in the directly preceding context. Previous efforts on solving the Winograd Schema Challenge and Pronoun Disambiguation Problems mostly rely on human-labeled data, sophisticated rules, hand-crafted features, or external knowledge bases (Peng et al., 2015; Bailey et al., 2015; Sch¨uller, 2014). Rahman and Ng (2012) hired workers to annotate supervised training data and designed 70K hand-crafted features. Sharma et al. (2015); Sch¨uller (2014); Bailey et al. (2015); Liu et al. (2017) utilized expensive knowledge bases in their reasoning processes. Recently, Trinh and Le 2018 applied neural language models trained with a massive amount of unlabeled data to the Winograd Schema Challenge and improved the performance by a large margin. In contrast, our unsupervised method based on DSSM significantly outperforms the previous state-of-theart method, with the advantage of capturing more contextual information i"
N19-1094,P18-1213,0,0.0882579,"(Miller, 1995) for external supervision to train word embeddings and solve the WSC challenge. Recently, Trinh and Le (2018) first used raw text from books/news to train a neural Language Model (LM), and then emIntroduction Commonsense reasoning is concerned with simulating the human ability to make presumptions about the type and essence of ordinary situations they encounter every day (Davis and Marcus, 2015). It is one of the key challenges in natural language understanding, and has drawn increasing attention in recent years (Levesque et al., 2011; Roemmele et al., 2011; Zhang et al., 2017; Rashkin et al., 2018a,b; Zellers et al., 2018; Trinh and Le, 2018). However, due to the lack of labeled training data or comprehensive hand-crafted knowledge bases, commonsense reasoning tasks such as Winograd Schema Challenge (Levesque et al., 2011) are still far from being solved. In this work, we propose two effective unsupervised models for commonsense reasoning, and evaluate them on two classic commonsense reasoning tasks: Winograd Schema Challenge (WSC) and Pronoun Disambiguation Problems (PDP). Compared to other commonsense reasoning tasks, ∗ B. The demonstraWork done when the author was at Microsoft 882 P"
N19-1094,Q17-1027,1,\N,Missing
N19-1127,D15-1075,0,0.274918,"build CNN/RNN-free neural networks based on MTSA for sentence embedding and sequence tagging tasks, including natural language inference, semantic role labeling, sentiment analysis, question-type classification, machine translation, etc. The results demonstrate that MTSA achieves state-of-the-art or competitive performance on nine benchmark datasets. To summarize the comparison of MTSA with recently popular models, we show the memory consumption and time cost vs. sequence length respectively in Figure 1(a) and 1(b) on synthetic data (batch size of 64 and feature channels of 300). On the SNLI (Bowman et al., 2015), a public dataset for language inference, as shown in Figure 1(c), MTSA achieves the best result but is as fast and as memory-efficient as the CNNs (all baselines and the benchmark are detailed in Section 4). Notations: 1) lowercase denotes a vector; 2) bold lowercase denotes a sequence of vectors (stored as a matrix); and 3) uppercase denotes a matrix or tensor. 2 Background 2.1 Attention Mechanism Given an input sequence of token embeddings or memory slots x = [x1 , . . . , xn ] ∈ Rde ×n , and a vector representation of a query q ∈ Rdq , attention mechanism (Bahdanau et al., 2015; Luong et"
N19-1127,P16-1139,0,0.0203347,"self-attention with intra-/interblock self-attention, aiming to reduce the time and space complexities of multi-dim self-attention by using hierarchical structure. 4.1 Natural Language Inference Natural language inference (NLI) aims at speculating on the relationship between a premise and a corresponding hypothesis, where the relationship could be entailment, neutral or contradiction. In experiments, we first compare MTSA with other baselines on the Stanford Natural Language Inference (Bowman et al., 2015) (SNLI) dataset. Following the method of applying sentenceencoding model to NLI given by Bowman et al. (2016), two parameter-tied sentenceencoding models are used to generate embeddings for premise and hypothesis, resulting in sp and sh respectively. The concatenation of sp , sh , sp − sh and sp sh representing the relationship is passed into a 3-way neural classifier for final prediction. The experimental results of the models from the official leaderboard, baselines, and MTSA are shown in Table 1. MTSA achieves state-of-theart performance with less time and memory cost. Compared to the methods from the leaderboard, MTSA outperforms RNN-based encoders (e.g., Residual stacked enc.), RNN+attention enc"
N19-1127,D17-1151,0,0.0956213,"ing (Tan et al., 2017). Self-attention mechanisms can be categorized into two classes according to the type of dependency each aims to model. The first category is token2token self-attention (Hu et al., 2017; Vaswani et al., 2017; Shen et al., 2018a) that captures syntactic dependency between every two tokens in a sequence. An efficient dot-product compatibility function is usually deployed to measure this pairwise dependency (Vaswani et al., 2017). In contrast, additive compatibility function captures the dependency by multi-layer perceptron (MLP), and can usually achieve better performance (Britz et al., 2017). Its expressive power can be further improved if expanded to multiple dimensions (Shen et al., 2018a). This multi-dim self-attention empirically surpasses dot-product one, but suffers from expensive computation and memory, which grow linearly with the number of features and quadratically with the sequence length. Hence, it is not scalable to long sequences in practice. The second category is source2token selfattention (Liu et al., 2016; Lin et al., 2017; Shen et al., 2018a) aiming to capture global dependency, i.e., the importance of each token to the entire sequence for a specific task. Its"
N19-1127,C18-1154,0,0.0231878,"Missing"
N19-1127,W17-5307,0,0.0304956,"Missing"
N19-1127,P18-2058,0,0.0325152,"Missing"
N19-1127,P17-1044,0,0.0603524,"Missing"
N19-1127,P14-1062,0,0.0248017,"SUBJ a TREC cBoW 79.9 86.4 91.3 87.3 Skip-thoughtb 81.3 87.5 93.6 92.2 DCNNc / / / 93.0 SRUd 84.8(1.3)89.7(1.1)93.4(0.8)93.9(0.6) CNNsd 82.2(.2) 88.8(1.2)92.9(0.7)93.2(0.5) SST-5 / / 48.5 / / Bi-LSTM Multi-head DiSA Bi-BloSA 84.6(1.6)90.2(0.9)94.7(0.7)94.4(0.3)49.9(0.8) 82.6(1.9)89.8(1.2)94.0(0.8)93.4(0.4)48.2(0.6) 84.8(2.0)90.1(0.4)94.2(0.6)94.2(0.1)51.0(0.7) 84.8(0.9)90.4(0.8)94.5(0.5)94.8(0.2)50.6(0.5) MTSA 84.9(2.4)90.5(0.6)94.5(0.6)95.3(0.3)51.3(0.7) Table 5: Experimental results on five sentence classification benchmarks. References: a (Mikolov et al., 2013), b (Kiros et al., 2015), c (Kalchbrenner et al., 2014), d (Lei and Zhang, 2017). 1) assigning either a semantic argument or nonargument to a given predicate and 2) labeling a specific semantic role for the identified argument. We follow the experimental setup in Tan et al. (2017), where the SRL task is treated as a BIO tagging problem. Tan et al. (2017) designed a deep attentive neural net by stacking multi-head self-attention, named as deepatt, to perform context fusion, whose output is then passed to a neural classifier to make the final decision. The results achieved by previous methods, baselines, and MTSA are shown in Table 4, which demonstr"
N19-1127,D14-1181,0,0.241791,"ssing a sentence embedding problem, a multidim source2token self-attention is applied on the top of context fusion module to produce the sequence embedding. Codes are implemented in Python with Tensorflow and executed on a single NVIDIA GTX 1080Ti graphics card. In addition, data for both time cost and memory consumption are collected under Tensorflow-1.7 with CUDA9 and cuDNN7. The context fusion baselines include 1) BiLSTM (Graves et al., 2013): 600D bi-directional LSTM consisting of 300D forward plus 300D backward LSTMs, 2) Bi-GRU (Chung et al., 2014): 600D bi-directional GRU, 3) Multi-CNN (Kim, 2014): three CNNs with 200D kernels to model 3/4/5-grams respectively, 4) Hrchy-CNN (Gehring et al., 2017): 3-layer 300D stacked CNN with kernel size 5, gated linear units (Dauphin et al., 2016) and residual connections (He et al., 2016), 5) Multi-head (Vaswani et al., 2017): 600D multi-head self-attention with 8 heads (751 Codes for Experiments are released at https:// github.com/taoshen58/mtsa. dim subspace per head) and positional embedding used by Vaswani et al. (2017), 6) DiSA (Shen et al., 2018a): 600D directional selfattention mechanism consisting of 300D forward and 300D backward masked sel"
N19-1127,C02-1150,0,0.106866,"lfattention baselines. 4.3 Sentence Classifications The goal of sentence classification is to predict the correct label for a sentence in various scenarios. We evaluate the models on five sentence classification benchmarks for different NLP tasks, which include 1) CR (Hu and Liu, 2004): customer reviews of various products to predict whether the review is positive or negative, 2) MPQA (Wiebe et al., 2005): an opinion polarity detection subtask of the MPQA dataset, 3) SUBJ (Pang and Lee, 2004): subjectivity dataset where a label indicates whether a sentence is subjective or objective, 4) TREC (Li and Roth, 2002): question-type classification dataset which classifies the question sentences into six classes, 5) SST-5 (Socher et al., 2013): the Stanford Sentiment Treebank dataset with five sentiment labels. The reported accuracies for CR, MPQA, and SUBJ are the mean of 10-fold cross validation. The accuracies for TREC are the mean of five runs on the dev set, and the accuracies for SST-5 are the mean of five runs on the test set. All standard deviations are shown in parentheses. The prediction accuracies achieved on these five benchmarks are shown in Table 5. MTSA achieves the best prediction accuracy o"
N19-1127,D15-1166,0,0.0372003,"l., 2015), a public dataset for language inference, as shown in Figure 1(c), MTSA achieves the best result but is as fast and as memory-efficient as the CNNs (all baselines and the benchmark are detailed in Section 4). Notations: 1) lowercase denotes a vector; 2) bold lowercase denotes a sequence of vectors (stored as a matrix); and 3) uppercase denotes a matrix or tensor. 2 Background 2.1 Attention Mechanism Given an input sequence of token embeddings or memory slots x = [x1 , . . . , xn ] ∈ Rde ×n , and a vector representation of a query q ∈ Rdq , attention mechanism (Bahdanau et al., 2015; Luong et al., 2015) computes an alignment score between each token xi and q by a compatibility function f (xi , q), which aims to measure the dependency/relevance between xi and q, or the attention of q to xi , w.r.t. a given task. The scores are transformed into probabilities through a softmax function. These probabilities are then used as weights to sum all the tokens and generate a contextual embedding for q, i.e., p(z|x, q) = softmax(a), a = [f (xi , q)]ni=1 , n X s= p(z = i|x, q) · xi = Ei∼p(z|x,q) [xi ], (1) i=1 where a ∈ Rn denotes the vector of n alignment scores, p(z|x, q) is the categorical distributio"
N19-1127,W17-5308,0,0.0419708,"Missing"
N19-1127,P04-1035,0,0.0131211,"chieves new state-of-the-art performance on the CoNLL-05 dataset by costing similar training time as CNN and multi-head selfattention baselines. 4.3 Sentence Classifications The goal of sentence classification is to predict the correct label for a sentence in various scenarios. We evaluate the models on five sentence classification benchmarks for different NLP tasks, which include 1) CR (Hu and Liu, 2004): customer reviews of various products to predict whether the review is positive or negative, 2) MPQA (Wiebe et al., 2005): an opinion polarity detection subtask of the MPQA dataset, 3) SUBJ (Pang and Lee, 2004): subjectivity dataset where a label indicates whether a sentence is subjective or objective, 4) TREC (Li and Roth, 2002): question-type classification dataset which classifies the question sentences into six classes, 5) SST-5 (Socher et al., 2013): the Stanford Sentiment Treebank dataset with five sentiment labels. The reported accuracies for CR, MPQA, and SUBJ are the mean of 10-fold cross validation. The accuracies for TREC are the mean of five runs on the dev set, and the accuracies for SST-5 are the mean of five runs on the test set. All standard deviations are shown in parentheses. The p"
N19-1127,D13-1170,0,0.00315113,"sentence in various scenarios. We evaluate the models on five sentence classification benchmarks for different NLP tasks, which include 1) CR (Hu and Liu, 2004): customer reviews of various products to predict whether the review is positive or negative, 2) MPQA (Wiebe et al., 2005): an opinion polarity detection subtask of the MPQA dataset, 3) SUBJ (Pang and Lee, 2004): subjectivity dataset where a label indicates whether a sentence is subjective or objective, 4) TREC (Li and Roth, 2002): question-type classification dataset which classifies the question sentences into six classes, 5) SST-5 (Socher et al., 2013): the Stanford Sentiment Treebank dataset with five sentiment labels. The reported accuracies for CR, MPQA, and SUBJ are the mean of 10-fold cross validation. The accuracies for TREC are the mean of five runs on the dev set, and the accuracies for SST-5 are the mean of five runs on the test set. All standard deviations are shown in parentheses. The prediction accuracies achieved on these five benchmarks are shown in Table 5. MTSA achieves the best prediction accuracy on CR, MPQA, TREC and SST-5 benchmarks with better time efficiency and a lower memory load. 4.4 Machine Translation We also eval"
N19-1127,D18-1548,0,0.031027,"Missing"
N19-1127,Q15-1003,0,0.0616257,"Missing"
N19-1127,P15-1109,0,0.0468197,"Missing"
P07-1034,W06-1615,0,0.896471,"1 Introduction Many natural language processing (NLP) problems such as part-of-speech (POS) tagging, named entity (NE) recognition, relation extraction, and semantic role labeling, are currently solved by supervised learning from manually labeled data. A bottleneck problem with this supervised learning approach is the lack of annotated data. As a special case, we often face the situation where we have a sufficient amount of labeled data in one domain, but have little or no labeled data in another related domain which we are interested in. We thus face the domain adaptation problem. Following (Blitzer et al., 2006), we 264 call the first the source domain, and the second the target domain. The domain adaptation problem is commonly encountered in NLP. For example, in POS tagging, the source domain may be tagged WSJ articles, and the target domain may be scientific literature that contains scientific terminology. In NE recognition, the source domain may be annotated news articles, and the target domain may be personal blogs. Another example is personalized spam filtering, where we may have many labeled spam and ham emails from publicly available sources, but we need to adapt the learned spam filter to an"
P07-1034,W04-3237,0,0.0543529,"tribution of emails and notion of spams. Despite the importance of domain adaptation in NLP, currently there are no standard methods for solving this problem. An immediate possible solution is semi-supervised learning, where we simply treat the target instances as unlabeled data but do not distinguish the two domains. However, given that the source data and the target data are from different distributions, we should expect to do better by exploiting the domain difference. Recently there have been some studies addressing domain adaptation from different perspectives (Roark and Bacchiani, 2003; Chelba and Acero, 2004; Florian et al., 2004; Daum´e III and Marcu, 2006; Blitzer et al., 2006). However, there have not been many studies that focus on the difference between the instance distributions in the two domains. A detailed discussion on related work is given in Section 5. In this paper, we study the domain adaptation problem from the instance weighting perspective. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 264–271, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics In general, the domain adaptation problem arises when th"
P07-1034,N04-1001,0,0.0128528,"notion of spams. Despite the importance of domain adaptation in NLP, currently there are no standard methods for solving this problem. An immediate possible solution is semi-supervised learning, where we simply treat the target instances as unlabeled data but do not distinguish the two domains. However, given that the source data and the target data are from different distributions, we should expect to do better by exploiting the domain difference. Recently there have been some studies addressing domain adaptation from different perspectives (Roark and Bacchiani, 2003; Chelba and Acero, 2004; Florian et al., 2004; Daum´e III and Marcu, 2006; Blitzer et al., 2006). However, there have not been many studies that focus on the difference between the instance distributions in the two domains. A detailed discussion on related work is given in Section 5. In this paper, we study the domain adaptation problem from the instance weighting perspective. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 264–271, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics In general, the domain adaptation problem arises when the source instances and"
P07-1034,N03-1027,0,0.0118429,"sumably very different, distribution of emails and notion of spams. Despite the importance of domain adaptation in NLP, currently there are no standard methods for solving this problem. An immediate possible solution is semi-supervised learning, where we simply treat the target instances as unlabeled data but do not distinguish the two domains. However, given that the source data and the target data are from different distributions, we should expect to do better by exploiting the domain difference. Recently there have been some studies addressing domain adaptation from different perspectives (Roark and Bacchiani, 2003; Chelba and Acero, 2004; Florian et al., 2004; Daum´e III and Marcu, 2006; Blitzer et al., 2006). However, there have not been many studies that focus on the difference between the instance distributions in the two domains. A detailed discussion on related work is given in Section 5. In this paper, we study the domain adaptation problem from the instance weighting perspective. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 264–271, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics In general, the domain adaptatio"
P09-1114,P08-1029,0,0.0213354,"Missing"
P09-1114,P08-1004,0,0.0417207,"al., 2006; Qian et al., 2008). However, in both feature-based and kernel-based studies, availability of sufficient labeled training data is always assumed. Chen et al. (2006) explored semi-supervised learning for relation extraction using label propagation, which makes use of unlabeled data. Zhou et al. (2008) proposed a hierarchical learning strategy to address the data sparseness problem in relation extraction. They also considered the commonality among different relation types, but compared with our work, they had a different problem setting and a different way of modeling the commonality. Banko and Etzioni (2008) studied open domain relation extraction, for which they manually identified several common relation patterns. In contrast, our method obtains common patterns through statistical learning. Xu et al. (2008) studied the problem of adapting a rule-based relation extraction system to new domains, but the types of relations to be extracted remain the same. Transfer learning aims at transferring knowledge learned from one or a number of old tasks to a new task. Domain adaptation is a special case of transfer learning where the learning task remains the same but the distribution of data changes. Ther"
P09-1114,W06-1615,0,0.747922,"elation, and thus have to create training data for the new relation type. However, is the old training data really useless? Inspired by recent work on transfer learning and domain adaptation, in this paper, we study how we can leverage labeled data of some old relation types to help the extraction of a new relation type in a weakly-supervised setting, where only a few seed instances of the new relation type are available. While transfer learning was proposed more than a decade ago (Thrun, 1996; Caruana, 1997), its application in natural language processing is still a relatively new territory (Blitzer et al., 2006; Daume III, 2007; Jiang and Zhai, 2007a; Arnold et al., 2008; Dredze and Crammer, 2008), and its application in relation extraction is still unexplored. Our idea of performing transfer learning is motivated by the observation that different relation types share certain common syntactic structures, which can possibly be transferred from the old types to the new type. We therefore propose to use a general multi-task learning framework in which classification models for a number of related tasks are forced to share a common model component and trained together. By treating classification of diff"
P09-1114,H05-1091,0,0.598098,"red to the target relation type, and allows easy control of the tradeoff between precision and recall. Empirical evaluation on the ACE 2004 data set shows that the proposed method substantially improves over two baseline methods. 1 Introduction Relation extraction is the task of detecting and characterizing semantic relations between entities from free text. Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008). However, supervised learning heavily relies on a sufficient amount of labeled data for training, which is not always available in practice due to the labor-intensive nature of human annotation. This problem is especially serious for relation extraction because the types of relations to be extracted are highly dependent on the application domain. For example, when working in the financial domain we may be interested in the employment relation, but when moving to the terrorism domain we now may be interested in the ethnic and ideology affiliation relation, and thus have to"
P09-1114,P06-1017,0,0.00925886,"elation extraction. We systematically explored the feature space for relation extraction (Jiang and Zhai, 2007b) . Kernel methods allow a large set of features to be used without being explicitly extracted. A number of relation extraction kernels have been proposed, including dependency tree kernels (Culotta and Sorensen, 2004), shortest dependency path kernels (Bunescu and Mooney, 2005) and more recently convolution tree kernels (Zhang et al., 2006; Qian et al., 2008). However, in both feature-based and kernel-based studies, availability of sufficient labeled training data is always assumed. Chen et al. (2006) explored semi-supervised learning for relation extraction using label propagation, which makes use of unlabeled data. Zhou et al. (2008) proposed a hierarchical learning strategy to address the data sparseness problem in relation extraction. They also considered the commonality among different relation types, but compared with our work, they had a different problem setting and a different way of modeling the commonality. Banko and Etzioni (2008) studied open domain relation extraction, for which they manually identified several common relation patterns. In contrast, our method obtains common"
P09-1114,C08-1088,0,0.175986,"type, and allows easy control of the tradeoff between precision and recall. Empirical evaluation on the ACE 2004 data set shows that the proposed method substantially improves over two baseline methods. 1 Introduction Relation extraction is the task of detecting and characterizing semantic relations between entities from free text. Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008). However, supervised learning heavily relies on a sufficient amount of labeled data for training, which is not always available in practice due to the labor-intensive nature of human annotation. This problem is especially serious for relation extraction because the types of relations to be extracted are highly dependent on the application domain. For example, when working in the financial domain we may be interested in the employment relation, but when moving to the terrorism domain we now may be interested in the ethnic and ideology affiliation relation, and thus have to create training data"
P09-1114,xu-etal-2008-adaptation,0,0.0684872,"relation extraction using label propagation, which makes use of unlabeled data. Zhou et al. (2008) proposed a hierarchical learning strategy to address the data sparseness problem in relation extraction. They also considered the commonality among different relation types, but compared with our work, they had a different problem setting and a different way of modeling the commonality. Banko and Etzioni (2008) studied open domain relation extraction, for which they manually identified several common relation patterns. In contrast, our method obtains common patterns through statistical learning. Xu et al. (2008) studied the problem of adapting a rule-based relation extraction system to new domains, but the types of relations to be extracted remain the same. Transfer learning aims at transferring knowledge learned from one or a number of old tasks to a new task. Domain adaptation is a special case of transfer learning where the learning task remains the same but the distribution of data changes. There has been an increasing amount of work on transfer learning and domain adaptation in natural language processing recently. Blitzer et al. (2006) proposed a structural correspondence learning method for do"
P09-1114,N06-1037,0,0.0155625,"y feature-based and kernel-based supervised learning methods. Zhou et al. (2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction. We systematically explored the feature space for relation extraction (Jiang and Zhai, 2007b) . Kernel methods allow a large set of features to be used without being explicitly extracted. A number of relation extraction kernels have been proposed, including dependency tree kernels (Culotta and Sorensen, 2004), shortest dependency path kernels (Bunescu and Mooney, 2005) and more recently convolution tree kernels (Zhang et al., 2006; Qian et al., 2008). However, in both feature-based and kernel-based studies, availability of sufficient labeled training data is always assumed. Chen et al. (2006) explored semi-supervised learning for relation extraction using label propagation, which makes use of unlabeled data. Zhou et al. (2008) proposed a hierarchical learning strategy to address the data sparseness problem in relation extraction. They also considered the commonality among different relation types, but compared with our work, they had a different problem setting and a different way of modeling the commonality. Banko and"
P09-1114,P05-1052,0,0.0696039,"ity type constraints on the relation arguments, which can usually be derived from the definition of a relation type. Imposing these constraints further improves the precision of the final relation extractor. Empirical evaluation on the ACE 2004 data set shows that our proposed method largely outperforms two baseline methods, improving the average F1 measure from 0.1532 to 0.4132 when only 10 seed instances of the new relation type are used. 2 Related work Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods. Zhou et al. (2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction. We systematically explored the feature space for relation extraction (Jiang and Zhai, 2007b) . Kernel methods allow a large set of features to be used without being explicitly extracted. A number of relation extraction kernels have been proposed, including dependency tree kernels (Culotta and Sorensen, 2004), shortest dependency path kernels (Bunescu and Mooney, 2005) and more recently convolution tree kernels (Zhang et al., 2006; Qian et al., 2008). However, in both feature-based and kernel-based studies, availability"
P09-1114,P04-1054,0,0.519488,"rom the auxiliary relation types to be transferred to the target relation type, and allows easy control of the tradeoff between precision and recall. Empirical evaluation on the ACE 2004 data set shows that the proposed method substantially improves over two baseline methods. 1 Introduction Relation extraction is the task of detecting and characterizing semantic relations between entities from free text. Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008). However, supervised learning heavily relies on a sufficient amount of labeled data for training, which is not always available in practice due to the labor-intensive nature of human annotation. This problem is especially serious for relation extraction because the types of relations to be extracted are highly dependent on the application domain. For example, when working in the financial domain we may be interested in the employment relation, but when moving to the terrorism domain we now may be interested in the ethnic and ide"
P09-1114,P05-1053,0,0.536856,"ypes to be transferred to the target relation type, and allows easy control of the tradeoff between precision and recall. Empirical evaluation on the ACE 2004 data set shows that the proposed method substantially improves over two baseline methods. 1 Introduction Relation extraction is the task of detecting and characterizing semantic relations between entities from free text. Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008). However, supervised learning heavily relies on a sufficient amount of labeled data for training, which is not always available in practice due to the labor-intensive nature of human annotation. This problem is especially serious for relation extraction because the types of relations to be extracted are highly dependent on the application domain. For example, when working in the financial domain we may be interested in the employment relation, but when moving to the terrorism domain we now may be interested in the ethnic and ideology affiliation r"
P09-1114,P07-1033,0,0.181822,"Missing"
P09-1114,D08-1072,0,0.041203,"is the old training data really useless? Inspired by recent work on transfer learning and domain adaptation, in this paper, we study how we can leverage labeled data of some old relation types to help the extraction of a new relation type in a weakly-supervised setting, where only a few seed instances of the new relation type are available. While transfer learning was proposed more than a decade ago (Thrun, 1996; Caruana, 1997), its application in natural language processing is still a relatively new territory (Blitzer et al., 2006; Daume III, 2007; Jiang and Zhai, 2007a; Arnold et al., 2008; Dredze and Crammer, 2008), and its application in relation extraction is still unexplored. Our idea of performing transfer learning is motivated by the observation that different relation types share certain common syntactic structures, which can possibly be transferred from the old types to the new type. We therefore propose to use a general multi-task learning framework in which classification models for a number of related tasks are forced to share a common model component and trained together. By treating classification of different relation types as related tasks, the learning framework can naturally model the co"
P09-1114,P07-1034,1,0.920149,"ng data for the new relation type. However, is the old training data really useless? Inspired by recent work on transfer learning and domain adaptation, in this paper, we study how we can leverage labeled data of some old relation types to help the extraction of a new relation type in a weakly-supervised setting, where only a few seed instances of the new relation type are available. While transfer learning was proposed more than a decade ago (Thrun, 1996; Caruana, 1997), its application in natural language processing is still a relatively new territory (Blitzer et al., 2006; Daume III, 2007; Jiang and Zhai, 2007a; Arnold et al., 2008; Dredze and Crammer, 2008), and its application in relation extraction is still unexplored. Our idea of performing transfer learning is motivated by the observation that different relation types share certain common syntactic structures, which can possibly be transferred from the old types to the new type. We therefore propose to use a general multi-task learning framework in which classification models for a number of related tasks are forced to share a common model component and trained together. By treating classification of different relation types as related tasks,"
P09-1114,N07-1015,1,0.948616,"ng data for the new relation type. However, is the old training data really useless? Inspired by recent work on transfer learning and domain adaptation, in this paper, we study how we can leverage labeled data of some old relation types to help the extraction of a new relation type in a weakly-supervised setting, where only a few seed instances of the new relation type are available. While transfer learning was proposed more than a decade ago (Thrun, 1996; Caruana, 1997), its application in natural language processing is still a relatively new territory (Blitzer et al., 2006; Daume III, 2007; Jiang and Zhai, 2007a; Arnold et al., 2008; Dredze and Crammer, 2008), and its application in relation extraction is still unexplored. Our idea of performing transfer learning is motivated by the observation that different relation types share certain common syntactic structures, which can possibly be transferred from the old types to the new type. We therefore propose to use a general multi-task learning framework in which classification models for a number of related tasks are forced to share a common model component and trained together. By treating classification of different relation types as related tasks,"
P09-2050,P05-1074,0,0.0434392,"Missing"
P09-2050,P01-1008,0,0.351192,"s. The duplicate bug reports are manually tagged and associated to the original bug report by either the system manager or software developers. These families of duplicate bug reports form a semi-parallel 1 However, bug reports have characteristics that raise many new challenges. Different from many other parallel corpora, bug reports are noisy. We observe at least three types of noise common in bug reports. First, many bug reports have many spelling, grammatical and sentence structure errors. To address this we extend a suitable stateof-the-art technique that is robust to such corpora, i.e. (Barzilay and McKeown, 2001). Second, many duplicate bug report families contain sentences that are not truly parallel. An example is shown in Table 1 (middle). We handle this by considering lexical similarity between duplicate bug reports. Third, even if the bug reports are parallel, we find many cases of context-peculiar paraphrases, i.e., a pair of phrases that have the same meaning in a very narrow context. An example is shown in Table 1 (bottom). To address this, we introduce two notions of global context-based score and co-occurrence based score which take into account all good and bad occurrences of the phrases in"
P09-2050,W03-1608,0,0.0108541,"een used in many tasks, e.g., (Wang et al., 2008; Tan et al., 2007). However, most paraphrases used are obtained manually. A recent study using synonyms from WordNet highlights the fact that these are not effective in software engineering tasks due to domain specificity (Sridhara et al., 2008). Therefore, an automatic way to derive technical paraphrases specific to software engineering is desired. Paraphrases can be extracted from non-parallel corpora using contextual similarity (Lin, 1998). They can also be obtained from parallel corpora if such data is available (Barzilay and McKeown, 2001; Ibrahim et al., 2003). Recently, there are also a number of studies that extract paraphrases from multilingual corpora (Bannard and CallisonBurch, 2005; Zhao et al., 2008). The approach in (Barzilay and McKeown, 2001) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours. Due to this reason, we build our technique on top of theirs. The following provides a summary of their technique. Two types of paraphrase patterns are defined: (1) Syntactic patterns which consist of the POS tags of the phrases. For example, the paraphrases “a VGA monitor” and “a monitor” are represented as “D"
P09-2050,P08-1089,0,\N,Missing
P09-2050,P98-2127,0,\N,Missing
P09-2050,C98-2122,0,\N,Missing
P10-1066,P06-1039,0,0.135887,"Missing"
P10-1066,P06-2027,0,0.150849,", automatic template generation provides a solution to induction of infobox structures, which are still highly incomplete in Wikipedia (Wu and Weld, 2007). A template can also serve as a starting point for human editors to create new summary articles. Furthermore, with summary templates, we can potentially apply information retrieval and extraction techniques to construct summaries for new entities automatically on the fly, improving the user experience for search engine and question answering systems. Despite its usefulness, the problem has not been well studied. The most relevant work is by Filatova et al. (2006) on automatic creation of domain templates, where the defintion of a domain is similar to our notion of an entity category. Filatova et al. (2006) first identify the important verbs for a domain using corpus statistics, and then find frequent parse tree patterns from sentences containing these verbs to construct a domain template. There are two major limitations of their approach. First, the focus on verbs restricts the template patterns that can be found. Second, redundant or related patterns using different verbs to express the same or similar facts cannot be grouped together. For example, “"
P10-1066,N09-1041,0,0.0174802,"c topics. Our background and document language models are similar to theirs. However, they still treat documents as bags of words rather than sets of sentences as in our model. Titov and McDonald (2008) exploited the idea that a short paragraph within a document is likely to be about the same aspect. Our one-aspect-per-sentence assumption is a stricter than theirs, but it is required in our model for the purpose of mining sentence patterns. The way we separate words into stop words, background words, document words and aspect words bears similarity to that used in (Daum´e III and Marcu, 2006; Haghighi and Vanderwende, 2009), but their task is multi-document summarization while ours is to induce summary templates. Related Work The most related existing work is on domain template generation by Filatova et al. (2006). There are several differences between our work and theirs. First, their template patterns must contain a non-auxiliary verb whereas ours do not have this restriction. Second, their verb-centered patterns are independent of each other, whereas we group semantically related patterns into aspects, giving more meaningful templates. Third, in their work, named entities, numbers and general nouns are treate"
P10-1066,P09-1024,0,0.2856,"ture. For example, biographies of physicists usually contain facts about the nationality, educational background, affiliation and major contributions of the physicist, whereas introductions of companies usually list information such 640 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 640–649, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics Aspect cluding unsupervised IE pattern discovery (Sudo et al., 2003; Shinyama and Sekine, 2006; Sekine, 2006; Yan et al., 2009) and automatic generation of Wikipedia articles (Sauper and Barzilay, 2009). We discuss the differences of our work from existing related work in Section 6. In this paper we propose a novel approach to the task of automatically generating entity summary templates. We first develop an entity-aspect model that extends standard LDA to identify clusters of words that can represent different aspects of facts that are salient in a given summary collection (Section 3). For example, the words “received,” “award,” “won” and “Nobel” may be clustered together from biographies of physicists to represent one aspect, even though they may appear in different sentences from differen"
P10-1066,P06-2094,0,0.0228437,"ries we consider. Summaries of entities from the same category usually share some common structure. For example, biographies of physicists usually contain facts about the nationality, educational background, affiliation and major contributions of the physicist, whereas introductions of companies usually list information such 640 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 640–649, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics Aspect cluding unsupervised IE pattern discovery (Sudo et al., 2003; Shinyama and Sekine, 2006; Sekine, 2006; Yan et al., 2009) and automatic generation of Wikipedia articles (Sauper and Barzilay, 2009). We discuss the differences of our work from existing related work in Section 6. In this paper we propose a novel approach to the task of automatically generating entity summary templates. We first develop an entity-aspect model that extends standard LDA to identify clusters of words that can represent different aspects of facts that are salient in a given summary collection (Section 3). For example, the words “received,” “award,” “won” and “Nobel” may be clustered together from biograp"
P10-1066,N06-1039,0,0.00654777,"entity summaries we consider. Summaries of entities from the same category usually share some common structure. For example, biographies of physicists usually contain facts about the nationality, educational background, affiliation and major contributions of the physicist, whereas introductions of companies usually list information such 640 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 640–649, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics Aspect cluding unsupervised IE pattern discovery (Sudo et al., 2003; Shinyama and Sekine, 2006; Sekine, 2006; Yan et al., 2009) and automatic generation of Wikipedia articles (Sauper and Barzilay, 2009). We discuss the differences of our work from existing related work in Section 6. In this paper we propose a novel approach to the task of automatically generating entity summary templates. We first develop an entity-aspect model that extends standard LDA to identify clusters of words that can represent different aspects of facts that are salient in a given summary collection (Section 3). For example, the words “received,” “award,” “won” and “Nobel” may be clustered together from biograp"
P10-1066,P03-1029,0,0.035354,"ons are examples of entity summaries we consider. Summaries of entities from the same category usually share some common structure. For example, biographies of physicists usually contain facts about the nationality, educational background, affiliation and major contributions of the physicist, whereas introductions of companies usually list information such 640 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 640–649, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics Aspect cluding unsupervised IE pattern discovery (Sudo et al., 2003; Shinyama and Sekine, 2006; Sekine, 2006; Yan et al., 2009) and automatic generation of Wikipedia articles (Sauper and Barzilay, 2009). We discuss the differences of our work from existing related work in Section 6. In this paper we propose a novel approach to the task of automatically generating entity summary templates. We first develop an entity-aspect model that extends standard LDA to identify clusters of words that can represent different aspects of facts that are salient in a given summary collection (Section 3). For example, the words “received,” “award,” “won” and “Nobel” may be clus"
P10-1066,P09-1115,0,0.0246524,"of entities from the same category usually share some common structure. For example, biographies of physicists usually contain facts about the nationality, educational background, affiliation and major contributions of the physicist, whereas introductions of companies usually list information such 640 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 640–649, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics Aspect cluding unsupervised IE pattern discovery (Sudo et al., 2003; Shinyama and Sekine, 2006; Sekine, 2006; Yan et al., 2009) and automatic generation of Wikipedia articles (Sauper and Barzilay, 2009). We discuss the differences of our work from existing related work in Section 6. In this paper we propose a novel approach to the task of automatically generating entity summary templates. We first develop an entity-aspect model that extends standard LDA to identify clusters of words that can represent different aspects of facts that are salient in a given summary collection (Section 3). For example, the words “received,” “award,” “won” and “Nobel” may be clustered together from biographies of physicists to represent o"
P11-1039,W08-1404,0,0.142117,"Missing"
P11-1039,D10-1036,0,0.79695,"udy the novel problem of extracting topical keyphrases for summarizing and analyzing Twitter content. In other words, we extract and organize keyphrases by topics learnt from Twitter. In our work, we follow the standard three steps of keyphrase extraction, namely, keyword ranking, candidate keyphrase generation 379 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 379–388, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics and keyphrase ranking. For keyword ranking, we modify the Topical PageRank method proposed by Liu et al. (2010) by introducing topic-sensitive score propagation. We find that topic-sensitive propagation can largely help boost the performance. For keyphrase ranking, we propose a principled probabilistic phrase ranking method, which can be flexibly combined with any keyword ranking method and candidate keyphrase generation method. Experiments on a large Twitter data set show that our proposed methods are very effective in topical keyphrase extraction from Twitter. Interestingly, our proposed keyphrase ranking method can incorporate users’ interests by modeling the retweet behavior. We further examine wha"
P11-1039,W03-1805,0,0.355712,"ditionally, keyphrases are defined as a short list of terms to summarize the topics of a document (Turney, 2000). So far there is little work on keyword or keyphrase extraction from Twitter. Wu et al. (2010) proposed to automatically generate personalized tags for Twitter users. However, user-level tags may not be suitable to summarize the overall Twitter content within a certain period and/or from a certain group of people such as people in the same region. Existing work on keyphrase extraction identifies keyphrases from either individual documents or an entire text collection (Turney, 2000; Tomokiyo and Hurst, 2003). These approaches are not immediately applicable to Twitter because it does not make sense to extract keyphrases from a single tweet, and if we extract keyphrases from a whole tweet collection we will mix a diverse range of topics together, which makes it difficult for users to follow the extracted keyphrases. Therefore, in this paper, we propose to study the novel problem of extracting topical keyphrases for summarizing and analyzing Twitter content. In other words, we extract and organize keyphrases by topics learnt from Twitter. In our work, we follow the standard three steps of keyphrase"
P11-1039,N10-1101,0,0.0984392,"ediction power of Twitter (Tumasjan et al., 2010), etc. However, current explorations are still in an early stage and our understanding of Twitter content still remains limited. How to automatically understand, extract and summarize useful Twitter content has therefore become an important and emergent research topic. In this paper, we propose to extract keyphrases as a way to summarize Twitter content. Traditionally, keyphrases are defined as a short list of terms to summarize the topics of a document (Turney, 2000). So far there is little work on keyword or keyphrase extraction from Twitter. Wu et al. (2010) proposed to automatically generate personalized tags for Twitter users. However, user-level tags may not be suitable to summarize the overall Twitter content within a certain period and/or from a certain group of people such as people in the same region. Existing work on keyphrase extraction identifies keyphrases from either individual documents or an entire text collection (Turney, 2000; Tomokiyo and Hurst, 2003). These approaches are not immediately applicable to Twitter because it does not make sense to extract keyphrases from a single tweet, and if we extract keyphrases from a whole tweet"
P11-1039,W04-3252,0,\N,Missing
P15-2028,W10-2608,0,0.167917,"Missing"
P15-2028,D08-1072,0,0.1417,"a small amount of labeled data from the target domain. The problem arises when the target domain has a different data distribution from the source domain, which is often the case. In NLP, domain adaptation has been well studied in recent years. Existing work has proposed both techniques designed for specific NLP tasks (Chan and Ng, 2007; Daume III and Jagarlamudi, 2011; Yang et al., 2012; Plank and Moschitti, 2013; Hu et al., 2014; Nguyen et al., 2014; Nguyen and Grishman, 2014) and general approaches applicable to different tasks (Blitzer et al., 2006; Daum´e III, 2007; Jiang and Zhai, 2007; Dredze and Crammer, 2008; Titov, 2011). With the recent trend of applying deep learning in NLP, deep learning-based domain adaptation methods (Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2014) have also been adopted for NLP tasks (Yang and Eisenstein, 2015). 168 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 168–173, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Eisenstein, 2015), our method is much easier to implement. In summar"
P15-2028,N15-1069,0,0.0333869,"sting work has proposed both techniques designed for specific NLP tasks (Chan and Ng, 2007; Daume III and Jagarlamudi, 2011; Yang et al., 2012; Plank and Moschitti, 2013; Hu et al., 2014; Nguyen et al., 2014; Nguyen and Grishman, 2014) and general approaches applicable to different tasks (Blitzer et al., 2006; Daum´e III, 2007; Jiang and Zhai, 2007; Dredze and Crammer, 2008; Titov, 2011). With the recent trend of applying deep learning in NLP, deep learning-based domain adaptation methods (Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2014) have also been adopted for NLP tasks (Yang and Eisenstein, 2015). 168 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 168–173, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Eisenstein, 2015), our method is much easier to implement. In summary, our main contribution is a simple, effective and theoretically justifiable unsupervised domain adaptation method for NLP problems. 2 source domain instance is transformed into a Kdimensional vector by Equation 1, we can append this vector to the o"
P15-2028,P12-2053,0,0.0216571,"ng Jiang School of Information Systems Singapore Management University jingjiang@smu.edu.sg Introduction Domain adaptation aims to use labeled data from a source domain to help build a system for a target domain, possibly with a small amount of labeled data from the target domain. The problem arises when the target domain has a different data distribution from the source domain, which is often the case. In NLP, domain adaptation has been well studied in recent years. Existing work has proposed both techniques designed for specific NLP tasks (Chan and Ng, 2007; Daume III and Jagarlamudi, 2011; Yang et al., 2012; Plank and Moschitti, 2013; Hu et al., 2014; Nguyen et al., 2014; Nguyen and Grishman, 2014) and general approaches applicable to different tasks (Blitzer et al., 2006; Daum´e III, 2007; Jiang and Zhai, 2007; Dredze and Crammer, 2008; Titov, 2011). With the recent trend of applying deep learning in NLP, deep learning-based domain adaptation methods (Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2014) have also been adopted for NLP tasks (Yang and Eisenstein, 2015). 168 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Internatio"
P15-2028,P14-1110,0,0.0300334,"re Management University jingjiang@smu.edu.sg Introduction Domain adaptation aims to use labeled data from a source domain to help build a system for a target domain, possibly with a small amount of labeled data from the target domain. The problem arises when the target domain has a different data distribution from the source domain, which is often the case. In NLP, domain adaptation has been well studied in recent years. Existing work has proposed both techniques designed for specific NLP tasks (Chan and Ng, 2007; Daume III and Jagarlamudi, 2011; Yang et al., 2012; Plank and Moschitti, 2013; Hu et al., 2014; Nguyen et al., 2014; Nguyen and Grishman, 2014) and general approaches applicable to different tasks (Blitzer et al., 2006; Daum´e III, 2007; Jiang and Zhai, 2007; Dredze and Crammer, 2008; Titov, 2011). With the recent trend of applying deep learning in NLP, deep learning-based domain adaptation methods (Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2014) have also been adopted for NLP tasks (Yang and Eisenstein, 2015). 168 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Pro"
P15-2028,P07-1034,1,0.63997,"domain, possibly with a small amount of labeled data from the target domain. The problem arises when the target domain has a different data distribution from the source domain, which is often the case. In NLP, domain adaptation has been well studied in recent years. Existing work has proposed both techniques designed for specific NLP tasks (Chan and Ng, 2007; Daume III and Jagarlamudi, 2011; Yang et al., 2012; Plank and Moschitti, 2013; Hu et al., 2014; Nguyen et al., 2014; Nguyen and Grishman, 2014) and general approaches applicable to different tasks (Blitzer et al., 2006; Daum´e III, 2007; Jiang and Zhai, 2007; Dredze and Crammer, 2008; Titov, 2011). With the recent trend of applying deep learning in NLP, deep learning-based domain adaptation methods (Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2014) have also been adopted for NLP tasks (Yang and Eisenstein, 2015). 168 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 168–173, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Eisenstein, 2015), our method is much easi"
P15-2028,P14-2012,0,0.0141867,".edu.sg Introduction Domain adaptation aims to use labeled data from a source domain to help build a system for a target domain, possibly with a small amount of labeled data from the target domain. The problem arises when the target domain has a different data distribution from the source domain, which is often the case. In NLP, domain adaptation has been well studied in recent years. Existing work has proposed both techniques designed for specific NLP tasks (Chan and Ng, 2007; Daume III and Jagarlamudi, 2011; Yang et al., 2012; Plank and Moschitti, 2013; Hu et al., 2014; Nguyen et al., 2014; Nguyen and Grishman, 2014) and general approaches applicable to different tasks (Blitzer et al., 2006; Daum´e III, 2007; Jiang and Zhai, 2007; Dredze and Crammer, 2008; Titov, 2011). With the recent trend of applying deep learning in NLP, deep learning-based domain adaptation methods (Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2014) have also been adopted for NLP tasks (Yang and Eisenstein, 2015). 168 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 168–173, c Beijing,"
P15-2028,P14-1076,0,0.0329743,"Missing"
P15-2028,P13-1147,0,0.0191045,"Information Systems Singapore Management University jingjiang@smu.edu.sg Introduction Domain adaptation aims to use labeled data from a source domain to help build a system for a target domain, possibly with a small amount of labeled data from the target domain. The problem arises when the target domain has a different data distribution from the source domain, which is often the case. In NLP, domain adaptation has been well studied in recent years. Existing work has proposed both techniques designed for specific NLP tasks (Chan and Ng, 2007; Daume III and Jagarlamudi, 2011; Yang et al., 2012; Plank and Moschitti, 2013; Hu et al., 2014; Nguyen et al., 2014; Nguyen and Grishman, 2014) and general approaches applicable to different tasks (Blitzer et al., 2006; Daum´e III, 2007; Jiang and Zhai, 2007; Dredze and Crammer, 2008; Titov, 2011). With the recent trend of applying deep learning in NLP, deep learning-based domain adaptation methods (Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2014) have also been adopted for NLP tasks (Yang and Eisenstein, 2015). 168 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Nat"
P15-2028,P11-1007,0,0.0592675,"data from the target domain. The problem arises when the target domain has a different data distribution from the source domain, which is often the case. In NLP, domain adaptation has been well studied in recent years. Existing work has proposed both techniques designed for specific NLP tasks (Chan and Ng, 2007; Daume III and Jagarlamudi, 2011; Yang et al., 2012; Plank and Moschitti, 2013; Hu et al., 2014; Nguyen et al., 2014; Nguyen and Grishman, 2014) and general approaches applicable to different tasks (Blitzer et al., 2006; Daum´e III, 2007; Jiang and Zhai, 2007; Dredze and Crammer, 2008; Titov, 2011). With the recent trend of applying deep learning in NLP, deep learning-based domain adaptation methods (Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2014) have also been adopted for NLP tasks (Yang and Eisenstein, 2015). 168 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 168–173, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Eisenstein, 2015), our method is much easier to implement. In summary, our main co"
P15-2028,P07-1007,0,\N,Missing
P15-2028,W06-1615,0,\N,Missing
P15-2028,P11-2071,0,\N,Missing
P15-2028,P14-2088,0,\N,Missing
P15-2028,P07-1033,0,\N,Missing
P17-1127,W08-1105,0,0.375484,"to graphbased dependency parsing. Clarke and Lapata (2008) first used an ILP framework for sentence compression. In the paper, the author put forward three models. The first model is a language model reformulated by ILP. As the first model treats all the words equally, the second model uses a corpus to learn an importance score for each word and then incorporates it in the ILP model. The Last model, which is based on (McDonald, 2006), replaces the decoder with an ILP model and adds many linguistic constraints such as dependency parsing compared with the previous two ILP models. Filippova and Strube (2008) represented sentences with dependency parse trees and an ILPbased method was used to decide whether the dependencies were preserved or not. Different from most previous work that treats sentence extraction and sentence compression separately, BergKirkpatrick et al. (2011) jointly model the two processes in one ILP problem. Bigrams and subtrees are represented by some features, and feature are learned on some training data. The ILP problem maximizes the coverage of weighted bigrams and deleted subtrees of the summary. In recent years, neural network models, especially sequence-to-sequence mode"
P17-1127,N07-1023,0,0.0124465,"s in the compressed sentence all come from the source sentence. In this paper, we focus on deletion-based sentence compression, which is a spacial case of extractive sentence compression. An early work on sentence compression was done by Jing (2000), who proposed to use several resources to decide whether a phrase in a sentence should be removed or not. Knight and Marcu (2000) proposed to apply a noisy-channel model from machine translation to the sentence compression task, but their model encountered the problem that many SCFG rules have unreliable probability estimates with inadequate data. Galley and McKeown (2007) tried to solve this problem by utilizing parent annotation, Markovization and lexicalization, which have all been shown to improve the quality of the rule probability estimates. Cohn and Lapata (2007) formulated sentence compression as a tree-to-tree rewrite problem. They utilized a synchronous tree substitution grammar (STSG) to license the space of all possible rewrites. Each rule has a weight learned from the training data. For prediction, an algorithm was used to search for the best scoring compression using the gram1391 Although dynamic oracles are widely used in dependency parsing and a"
P17-1127,A00-1043,0,0.843703,"cope. Compressed (by human): she helped other wives cope. Compressed (by machine): wives and flexible wives Introduction Sentence compression is the task of compressing long, verbose sentences into short, concise ones. It can be used as a component of a text summarization system. Figure 1 shows two example input sentences and the compressed sentences written by human. The task has been studied for almost two decades. Early work on this task mostly relies on syntactic information such as constituency-based parse trees to help decide what to prune from a sentence or how to re-write a sentence (Jing, 2000; Knight and Marcu, 2000). Recently, there has been much interest in applying neural network models to solve the problem, where little or no linguistic analysis is performed except for tokenization (Filippova et al., 2015; Rush et al., 2015; Chopra et al., 2016). Although neural network-based models have achieved good performance on this task recently, they tend to suffer from two problems: (1) They require a large amount of data for training. For example, Filippova et al. (2015) used close to two Figure 1: Examples of in-domain and out-ofdomain results by a standard abstractive sequenceto-seq"
P17-1127,P11-1049,0,0.102054,"Missing"
P17-1127,N16-1012,0,0.0717079,"a text summarization system. Figure 1 shows two example input sentences and the compressed sentences written by human. The task has been studied for almost two decades. Early work on this task mostly relies on syntactic information such as constituency-based parse trees to help decide what to prune from a sentence or how to re-write a sentence (Jing, 2000; Knight and Marcu, 2000). Recently, there has been much interest in applying neural network models to solve the problem, where little or no linguistic analysis is performed except for tokenization (Filippova et al., 2015; Rush et al., 2015; Chopra et al., 2016). Although neural network-based models have achieved good performance on this task recently, they tend to suffer from two problems: (1) They require a large amount of data for training. For example, Filippova et al. (2015) used close to two Figure 1: Examples of in-domain and out-ofdomain results by a standard abstractive sequenceto-sequence model trained on the Gigaword corpus. The first input sentence comes from the Gigaword corpus while the second input sentence comes from the written news corpus used by Clarke and Lapata (2008). million sentence pairs to train an LSTM-based sentence compre"
P17-1127,E06-1038,0,0.0756324,"g the structure , we encode the sentence recursively in a bottom-up fashion to produce a vector representation and decode it . Table 2: Some input sentences from the Research Papers dataset and the automatically compressed sentences using different methods. T: Traditional ILP method. S: LSTM+. B: BiLSTM+SynFeat+ILP. Traditional ILP LSTM+ BiLSTM+SynFeat+ILP readability informativeness 3.94 3.69 4.29 3.33 3.07 3.46 Table 3: Manual evaluation. mar rules. Besides, Cohn and Lapata (2008) extended this model to abstractive sentence compression, which includes substitution, reordering and insertion. McDonald (2006) proposed a graphbased sentence compression method. The general idea is that each word pair in the original sentence has a score. The task then becomes how to find a compressed sentence with a length limit according word pair scores. Their method is similar to graphbased dependency parsing. Clarke and Lapata (2008) first used an ILP framework for sentence compression. In the paper, the author put forward three models. The first model is a language model reformulated by ILP. As the first model treats all the words equally, the second model uses a corpus to learn an importance score for each wor"
P17-1127,D07-1008,0,0.0180442,"sentence compression was done by Jing (2000), who proposed to use several resources to decide whether a phrase in a sentence should be removed or not. Knight and Marcu (2000) proposed to apply a noisy-channel model from machine translation to the sentence compression task, but their model encountered the problem that many SCFG rules have unreliable probability estimates with inadequate data. Galley and McKeown (2007) tried to solve this problem by utilizing parent annotation, Markovization and lexicalization, which have all been shown to improve the quality of the rule probability estimates. Cohn and Lapata (2007) formulated sentence compression as a tree-to-tree rewrite problem. They utilized a synchronous tree substitution grammar (STSG) to license the space of all possible rewrites. Each rule has a weight learned from the training data. For prediction, an algorithm was used to search for the best scoring compression using the gram1391 Although dynamic oracles are widely used in dependency parsing and available for most standard transition systems , no dynamic oracle parsing model has yet been proposed for phrase structure grammars T: Although are used for transition systems model has been proposed f"
P17-1127,C08-1018,0,0.0500835,"ords with output . S: Following the structure of a source sentence encode the sentence recursively in a bottom-up fashion . B: Following the structure , we encode the sentence recursively in a bottom-up fashion to produce a vector representation and decode it . Table 2: Some input sentences from the Research Papers dataset and the automatically compressed sentences using different methods. T: Traditional ILP method. S: LSTM+. B: BiLSTM+SynFeat+ILP. Traditional ILP LSTM+ BiLSTM+SynFeat+ILP readability informativeness 3.94 3.69 4.29 3.33 3.07 3.46 Table 3: Manual evaluation. mar rules. Besides, Cohn and Lapata (2008) extended this model to abstractive sentence compression, which includes substitution, reordering and insertion. McDonald (2006) proposed a graphbased sentence compression method. The general idea is that each word pair in the original sentence has a score. The task then becomes how to find a compressed sentence with a length limit according word pair scores. Their method is similar to graphbased dependency parsing. Clarke and Lapata (2008) first used an ILP framework for sentence compression. In the paper, the author put forward three models. The first model is a language model reformulated b"
P17-1127,W12-3018,0,0.00756965,"Missing"
P17-1127,D14-1162,0,0.119562,"ion-data.json. 3 Available at http://jamesclarke.net/ research/resources/. pendency embeddings as input to the bi-LSTM model and use the predictions of y from the biLSTM model. BiLSTM+SynFeat+ILP: In this setting, on top of BiLSTM+SynFeat, we solve the ILP problem as described in Section 2.4 to predict the final label sequence y. In the experiments, our model was trained using the Adam (Kingma and Ba, 2015) algorithm with a learning rate initialized at 0.001. The dimension of the hidden layers of bi-LSTM is 100. Word embeddings are initialized from GloVe 100dimensional pre-trained embeddings (Pennington et al., 2014). POS and dependency embeddings are randomly initialized with 40-dimensional vectors. The embeddings are all updated during training. Dropping probability for dropout layers between stacked LSTM layers is 0.5. The batch size is set as 30. For the ILP part, λ is set to 0.5, β and γ are turned by the validation data and finally they are set to 0.7 and 0.2, respectively. We utilize an open source ILP solver4 in our method. We compare our methods with a few baselines: LSTM: This is the basic LSTM-based deletion method proposed by (Filippova et al., 2015). We report both the performance they achiev"
P17-1127,D15-1044,0,0.493482,"d as a component of a text summarization system. Figure 1 shows two example input sentences and the compressed sentences written by human. The task has been studied for almost two decades. Early work on this task mostly relies on syntactic information such as constituency-based parse trees to help decide what to prune from a sentence or how to re-write a sentence (Jing, 2000; Knight and Marcu, 2000). Recently, there has been much interest in applying neural network models to solve the problem, where little or no linguistic analysis is performed except for tokenization (Filippova et al., 2015; Rush et al., 2015; Chopra et al., 2016). Although neural network-based models have achieved good performance on this task recently, they tend to suffer from two problems: (1) They require a large amount of data for training. For example, Filippova et al. (2015) used close to two Figure 1: Examples of in-domain and out-ofdomain results by a standard abstractive sequenceto-sequence model trained on the Gigaword corpus. The first input sentence comes from the Gigaword corpus while the second input sentence comes from the written news corpus used by Clarke and Lapata (2008). million sentence pairs to train an LSTM"
P17-1127,D15-1042,0,0.539632,"Missing"
P17-1127,D13-1155,0,0.113131,"Missing"
P18-2118,P16-1223,0,0.0515277,"sentence-level aggregation of the co-matching states. All these representations will be further integrated by another Bi-LSTM to get the final triplet matching representation. Hs = [hs1 ; hs2 ; . . . ; hsN ], ht = MaxPooling (Bi-LSTM (Hs )) , (5) 748 RACE-M RACE-H RACE Random Sliding Window Stanford AR GA ElimiNet HAF MUSIC 24.6 37.3 44.2 43.7 45.3 51.5 25.0 30.4 43.0 44.2 47.9 45.7 24.9 32.2 43.3 44.1 44.7 47.2 47.4 Hier-Co-Matching - Hier-Aggregation - Co-Matching 55.8∗ 54.2 50.7 48.2∗ 46.2 45.6 50.4∗ 48.5 46.4 Turkers Ceiling 85.1 95.4 69.4 94.2 73.3 94.5 • Stanford Attentive Reader (AR) (Chen et al., 2016) first builds a question-related passage representation through attention mechanism and then compares it with each candidate answer representation to get the answer probabilities. • GA (Dhingra et al., 2017) uses gated attention mechanism with multiple hops to extract the question-related information of the passage and compares it with candidate answers. • ElimiNet (Soham et al., 2017) tries to first eliminate the most irrelevant choices and then select the best answer. • HAF (Zhou et al., 2018) considers not only the matching between the three sequences, namely, passage, question and candidat"
P18-2118,P17-1168,0,0.0804797,"sN ], ht = MaxPooling (Bi-LSTM (Hs )) , (5) 748 RACE-M RACE-H RACE Random Sliding Window Stanford AR GA ElimiNet HAF MUSIC 24.6 37.3 44.2 43.7 45.3 51.5 25.0 30.4 43.0 44.2 47.9 45.7 24.9 32.2 43.3 44.1 44.7 47.2 47.4 Hier-Co-Matching - Hier-Aggregation - Co-Matching 55.8∗ 54.2 50.7 48.2∗ 46.2 45.6 50.4∗ 48.5 46.4 Turkers Ceiling 85.1 95.4 69.4 94.2 73.3 94.5 • Stanford Attentive Reader (AR) (Chen et al., 2016) first builds a question-related passage representation through attention mechanism and then compares it with each candidate answer representation to get the answer probabilities. • GA (Dhingra et al., 2017) uses gated attention mechanism with multiple hops to extract the question-related information of the passage and compares it with candidate answers. • ElimiNet (Soham et al., 2017) tries to first eliminate the most irrelevant choices and then select the best answer. • HAF (Zhou et al., 2018) considers not only the matching between the three sequences, namely, passage, question and candidate answer, but also the matching between the candidate answers. • MUSIC (Xu et al., 2017) integrates different sequence matching strategies into the model and also adds a unit of multi-step reasoning for sele"
P18-2118,P82-1020,0,0.828018,"Missing"
P18-2118,D17-1082,0,0.37953,"poses a new co-matching approach to this problem, which jointly models whether a passage can match both a question and a candidate answer. Experimental results on the RACE dataset demonstrate that our approach achieves state-of-the-art performance. 1 Introduction Enabling machines to understand natural language text is arguably the ultimate goal of natural language processing, and the task of machine reading comprehension is an intermediate step towards this ultimate goal (Richardson et al., 2013; Hermann et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016; Nguyen et al., 2016). Recently, Lai et al. (2017) released a new multi-choice machine comprehension dataset called RACE that was extracted from middle and high school English examinations in China. Figure 1 shows an example passage and two related questions from RACE. The key difference between RACE and previously released machine comprehension datasets (e.g., the CNN/Daily Mail dataset (Hermann et al., 2015) and SQuAD (Rajpurkar et al., 2016)) is that the answers in RACE often cannot be directly extracted from the given passages, as illustrated by the two example questions (Q1 & Q2) in Figure 1. Thus, answering these questions is more chall"
P18-2118,D16-1264,0,0.0501101,"een a passage and a question-answer pair. This paper proposes a new co-matching approach to this problem, which jointly models whether a passage can match both a question and a candidate answer. Experimental results on the RACE dataset demonstrate that our approach achieves state-of-the-art performance. 1 Introduction Enabling machines to understand natural language text is arguably the ultimate goal of natural language processing, and the task of machine reading comprehension is an intermediate step towards this ultimate goal (Richardson et al., 2013; Hermann et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016; Nguyen et al., 2016). Recently, Lai et al. (2017) released a new multi-choice machine comprehension dataset called RACE that was extracted from middle and high school English examinations in China. Figure 1 shows an example passage and two related questions from RACE. The key difference between RACE and previously released machine comprehension datasets (e.g., the CNN/Daily Mail dataset (Hermann et al., 2015) and SQuAD (Rajpurkar et al., 2016)) is that the answers in RACE often cannot be directly extracted from the given passages, as illustrated by the two example questions (Q1 & Q2) in Figu"
P18-2118,D13-1020,0,0.133603,"prehension is a challenging task, which involves the matching between a passage and a question-answer pair. This paper proposes a new co-matching approach to this problem, which jointly models whether a passage can match both a question and a candidate answer. Experimental results on the RACE dataset demonstrate that our approach achieves state-of-the-art performance. 1 Introduction Enabling machines to understand natural language text is arguably the ultimate goal of natural language processing, and the task of machine reading comprehension is an intermediate step towards this ultimate goal (Richardson et al., 2013; Hermann et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016; Nguyen et al., 2016). Recently, Lai et al. (2017) released a new multi-choice machine comprehension dataset called RACE that was extracted from middle and high school English examinations in China. Figure 1 shows an example passage and two related questions from RACE. The key difference between RACE and previously released machine comprehension datasets (e.g., the CNN/Daily Mail dataset (Hermann et al., 2015) and SQuAD (Rajpurkar et al., 2016)) is that the answers in RACE often cannot be directly extracted from the given passag"
P18-2118,P15-1150,0,0.0139012,"nces and we use P1 , P2 , . . . , PN to represent these sentences, where N is the number of sentences in the passage. For each triplet {Pn , Q, A}, n ∈ [1, N ], we can get the co-matching states Cn through Eqn. (1-3). Then we build a bi-directional LSTM followed by max pooling on top of the comatching states of each sentence as follows: where Wg ∈Rl×2l and bg ∈ Rl are the parame· ters to learn. is the column-wise concatenation · of two matrices, and · · and · ⊗ · are the elementwise subtraction and multiplication between two matrices, which are used to build better matching representations (Tai et al., 2015; Wang and Jiang, 2017). Mq ∈ Rl×P represents the matching between the hidden states of the passage and the corresponding attention-weighted representations of the question. Similarly, we match the passage with the candidate answer and represent the matching results using Ma ∈ Rl×P . Finally C ∈ R2l×P is the concatenation of Mq ∈ Rl×P hsn = MaxPooling (Bi-LSTM (Cn )) , (4) where the function MaxPooling(·) is the row-wise max pooling operation. hsn ∈ Rl , n ∈ [1, N ] is the sentence-level aggregation of the co-matching states. All these representations will be further integrated by another Bi-L"
P18-2118,D15-1167,0,0.261894,"r made his living by driving. b. The author’s wife supported to buy the island. c. Blue and red are the main colors of his national flag. d. People can travel around the island free of charge. Q2: How did the author get the island? a. It was a present from Blandy. b. The king sold it to him. c. He bought it from Blandy. d. He inherited from his father. Table 1: An example passage and two related multi-choice questions. The ground-truth answers are in bold. process. Finally in Section 2.3 we present the objective function. An overview of our co-matching model is shown in Figure 2. chical LSTM (Tang et al., 2015) over the sequence of co-matching states at different positions of the passage. Information is aggregated from wordlevel to sentence-level and then from sentencelevel to document-level. In this way, our model can better deal with the questions that require evidence scattered in different sentences in the passage. Our model improves the state-of-the-art model by 3 percentage on the RACE dataset. Our code will be released under https://github. com/shuohangwang/comatch. 2 2.1 Co-matching The co-matching part of our model aims to match the passage with the question and the candidate answer at the"
P18-2118,P16-1041,0,0.0113818,"the passage. Information is aggregated from wordlevel to sentence-level and then from sentencelevel to document-level. In this way, our model can better deal with the questions that require evidence scattered in different sentences in the passage. Our model improves the state-of-the-art model by 3 percentage on the RACE dataset. Our code will be released under https://github. com/shuohangwang/comatch. 2 2.1 Co-matching The co-matching part of our model aims to match the passage with the question and the candidate answer at the word-level. Inspired by some previous work (Wang and Jiang, 2016; Trischler et al., 2016), we first use bi-directional LSTMs (Hochreiter and Schmidhuber, 1997) to pre-process the sequences as follows: Model Hp = Bi-LSTM(P), Hq = Bi-LSTM(Q), Ha = Bi-LSTM(A), For the task of multi-choice reading comprehension, the machine is given a passage, a question and a set of candidate answers. The goal is to select the correct answer from the candidates. Let us use P ∈ Rd×P , Q ∈ Rd×Q and A ∈ Rd×A to represent the passage, the question and a candidate answer, respectively, where each word in each sequence is represented by an embedding vector. d is the dimensionality of the embeddings, and P"
P18-2118,N16-1170,1,0.781208,"different positions of the passage. Information is aggregated from wordlevel to sentence-level and then from sentencelevel to document-level. In this way, our model can better deal with the questions that require evidence scattered in different sentences in the passage. Our model improves the state-of-the-art model by 3 percentage on the RACE dataset. Our code will be released under https://github. com/shuohangwang/comatch. 2 2.1 Co-matching The co-matching part of our model aims to match the passage with the question and the candidate answer at the word-level. Inspired by some previous work (Wang and Jiang, 2016; Trischler et al., 2016), we first use bi-directional LSTMs (Hochreiter and Schmidhuber, 1997) to pre-process the sequences as follows: Model Hp = Bi-LSTM(P), Hq = Bi-LSTM(Q), Ha = Bi-LSTM(A), For the task of multi-choice reading comprehension, the machine is given a passage, a question and a set of candidate answers. The goal is to select the correct answer from the candidates. Let us use P ∈ Rd×P , Q ∈ Rd×Q and A ∈ Rd×A to represent the passage, the question and a candidate answer, respectively, where each word in each sequence is represented by an embedding vector. d is the dimensionality"
P18-2118,W16-0103,0,0.0296719,"Missing"
R15-1019,P12-1036,0,0.0235274,"d probabilistic models like LDA. Wang et al. (2011a) modeled review documents using LDA and treated ratings as a linear combination of topic-word-specific sentiment scores. Sauper et al. (2011) modeled word sentiment under different topics with a topic-sentiment word distribution. While these studies simultaneously model review documents and associated ratings, they do not consider user identity and item identity, which makes them unable to discover user preference and item quality. There have been many studies on the extraction of product aspects (Qiu et al., 2011; Titov and McDonald, 2008b; Mukherjee and Liu, 2012). These studies use either linguistic patterns or a topic modeling approach, or a combination of both, to identify product features or aspects. However, they do not distinguish between aspects and properties. More recent work has started paying attention to taking user and product identity into consideration. McAuley and Leskovec (2013) used a principled model similar to that of Wang and Blei (2011) to map each latent factor to a topic learned by LDA from review documents. Two variations of this α φp y z ψa vUu ru,i w σ vIi θi A I β Figure 1: Plate notation of our PAR model. Circles in gray in"
R15-1019,J11-1002,0,0.0160191,"re attention to modeling texts with principled probabilistic models like LDA. Wang et al. (2011a) modeled review documents using LDA and treated ratings as a linear combination of topic-word-specific sentiment scores. Sauper et al. (2011) modeled word sentiment under different topics with a topic-sentiment word distribution. While these studies simultaneously model review documents and associated ratings, they do not consider user identity and item identity, which makes them unable to discover user preference and item quality. There have been many studies on the extraction of product aspects (Qiu et al., 2011; Titov and McDonald, 2008b; Mukherjee and Liu, 2012). These studies use either linguistic patterns or a topic modeling approach, or a combination of both, to identify product features or aspects. However, they do not distinguish between aspects and properties. More recent work has started paying attention to taking user and product identity into consideration. McAuley and Leskovec (2013) used a principled model similar to that of Wang and Blei (2011) to map each latent factor to a topic learned by LDA from review documents. Two variations of this α φp y z ψa vUu ru,i w σ vIi θi A I β Figure 1"
R15-1019,P11-1036,0,0.0180733,"operties, aspects and ratings. γ U P ρu,i Related Work πu Research on modeling review texts and the associated ratings or sentiments has attracted much attention. In the pioneering work by Hu and Liu (2004), the authors extracted product aspects and predicted sentiment orientations. While this work was mainly based on frequent pattern mining, recent work in this direction pays more attention to modeling texts with principled probabilistic models like LDA. Wang et al. (2011a) modeled review documents using LDA and treated ratings as a linear combination of topic-word-specific sentiment scores. Sauper et al. (2011) modeled word sentiment under different topics with a topic-sentiment word distribution. While these studies simultaneously model review documents and associated ratings, they do not consider user identity and item identity, which makes them unable to discover user preference and item quality. There have been many studies on the extraction of product aspects (Qiu et al., 2011; Titov and McDonald, 2008b; Mukherjee and Liu, 2012). These studies use either linguistic patterns or a topic modeling approach, or a combination of both, to identify product features or aspects. However, they do not dist"
R15-1019,P08-1036,0,0.207728,"deling texts with principled probabilistic models like LDA. Wang et al. (2011a) modeled review documents using LDA and treated ratings as a linear combination of topic-word-specific sentiment scores. Sauper et al. (2011) modeled word sentiment under different topics with a topic-sentiment word distribution. While these studies simultaneously model review documents and associated ratings, they do not consider user identity and item identity, which makes them unable to discover user preference and item quality. There have been many studies on the extraction of product aspects (Qiu et al., 2011; Titov and McDonald, 2008b; Mukherjee and Liu, 2012). These studies use either linguistic patterns or a topic modeling approach, or a combination of both, to identify product features or aspects. However, they do not distinguish between aspects and properties. More recent work has started paying attention to taking user and product identity into consideration. McAuley and Leskovec (2013) used a principled model similar to that of Wang and Blei (2011) to map each latent factor to a topic learned by LDA from review documents. Two variations of this α φp y z ψa vUu ru,i w σ vIi θi A I β Figure 1: Plate notation of our PA"
R15-1020,P11-1052,0,0.0315814,"ng on the job ? His face look like wks.. this is becoming the PAP ’s official logo Sleep and Dream Table 1: Summary sentences selected by the ILP-based method (Gillick and Favre, 2009) from a thread on criticizing parliament ministers sleeping in a meeting. Existing approaches include centroid-based methods (Radev et al., 2004), learning-based methods (Kupiec et al., 1995; Wong et al., 2008) and graph-based methods (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Mei et al., 2010). More recently, Lin and Bilmes have done a series of work modeling text summarization with submodular functions (Lin and Bilmes, 2011; Lin and Bilmes, 2010). To globally infer an optimal set of sentences as a summary, ILP-based document summarization has been used. It was first proposed by McDonald (2007) and Gillick and Favre (2009) proposed a scalable version. Opinion summarization: Much work on opinion summarization is for product reviews (Hu and Liu, 2004; Popescu et al., 2005; Ganesan et al., 2012). As we have pointed out, summarizing opinions from online forums, where the topics can be social issues, is quite different from summarizing product reviews. For general opinion summarization, in 2008 the Text Analysis Confe"
R15-1020,W14-1820,0,0.0337741,"Missing"
R15-1020,W04-3247,0,0.0544342,"sters / MPs must take parliament session very seriously . becos in parliament , very important topics are being discussed and debated . must pay attention and stay awake ! ! sleeping on the job ? His face look like wks.. this is becoming the PAP ’s official logo Sleep and Dream Table 1: Summary sentences selected by the ILP-based method (Gillick and Favre, 2009) from a thread on criticizing parliament ministers sleeping in a meeting. Existing approaches include centroid-based methods (Radev et al., 2004), learning-based methods (Kupiec et al., 1995; Wong et al., 2008) and graph-based methods (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Mei et al., 2010). More recently, Lin and Bilmes have done a series of work modeling text summarization with submodular functions (Lin and Bilmes, 2011; Lin and Bilmes, 2010). To globally infer an optimal set of sentences as a summary, ILP-based document summarization has been used. It was first proposed by McDonald (2007) and Gillick and Favre (2009) proposed a scalable version. Opinion summarization: Much work on opinion summarization is for product reviews (Hu and Liu, 2004; Popescu et al., 2005; Ganesan et al., 2012). As we have pointed out, summarizing opinions"
R15-1020,C10-2032,0,0.0149614,"nces. Percent of capitalized words. We also observe that in the threads we have collected, some sentences contain many capitalized words such as “HaHa” and “LOL.” We hypothesize that the more capitalized words a sentence contains, the less likely it is a good summary sentence. Average word length. This is the average length of a word in a sentence in terms of characters. With this feature, we would like to check whether good summary sentences tend to contain longer words. 4.2.2 4.2.3 POS based Features Part-of-speech based grammatical features have been widely used in text quality prediction (Feng et al., 2010; Dell’Orletta et al., 2014). They can capture the linguistic and syntactic structure of sentence, which may affect its readability. In this work, we calculate the percentage of nouns, verbs, adjectives and adverbs in each sentence. 4.2.4 Parse Tree Height The height of the parse tree of a sentence has been used in previous work to assess text quality (Dell’Orletta et al., 2014; Pitler and Nenkova, 2008; Schwarm and Ostendorf, 2005). Here we use Stanford PCFG Parser to extract this feature. We hypothesize that as summary sentences tend to be more informative and more well-written, they may be"
R15-1020,D08-1020,0,0.257871,"tures and subsequently improved our summarization method. 4 4.2 Text Quality We hypothesize that text quality is especially important for summarizing forum posts because usergenerated content tends to be of lower quality compared with traditional corpora. Typical characteristics of user-generated content that affect its text quality include use of Internet slang words, misspelling, grammatical errors, excess use of punctuation marks, etc. We hypothesize that low quality sentences are less likely to be chosen as summary sentences. While many features have been proposed to measure text quality (Pitler and Nenkova, 2008), based on our observation with our data, here we consider the following features: Sentence Features In this section, we identify a number of sentence features which we hypothesize to have correlations with whether a sentence is a good summary sentence for forum opinion summarization. While a large number of features have been examined in previous studies on standard summarization (Kupiec et al., 1995; Wong et al., 2008), in this work we hypothesize that for our problem the following characteristics of a sentence are the most important: (1) representativeness with respect to the entire thread,"
R15-1020,W09-1802,0,0.405843,"l an under-explored area. A vast body of work has been done on summarizing online product reviews, but because of the special properties of product reviews, opinion summarization of product reviews tends to focus on product aspect identification and sentiment polarity classification. When it comes to summarizing general online discussions, particularly discussions on controversial topics such as a policy or a social issue, the challenges we face can be very different from summarizing product reviews. Table 1 shows a set of summary sentences selected by a state-of-the-art summarization method (Gillick and Favre, 2009) from a forum thread on criticizing parliament ministers sleeping in a meeting. We can see that the summary contains low-quality sentences and some sentences do not express opinions. This result shows that traditonal text summarization techniques, which only consider text representativeness, may not be able to summarize opinions from online forums very well. In particular, we hypothesize that two important factors should be considered for summarizing online discussions. First, because forum posts are often noisy, with misspelling, broken sentences and online jargon, text quality should be cons"
R15-1020,H05-2017,0,0.088754,"s (Kupiec et al., 1995; Wong et al., 2008) and graph-based methods (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Mei et al., 2010). More recently, Lin and Bilmes have done a series of work modeling text summarization with submodular functions (Lin and Bilmes, 2011; Lin and Bilmes, 2010). To globally infer an optimal set of sentences as a summary, ILP-based document summarization has been used. It was first proposed by McDonald (2007) and Gillick and Favre (2009) proposed a scalable version. Opinion summarization: Much work on opinion summarization is for product reviews (Hu and Liu, 2004; Popescu et al., 2005; Ganesan et al., 2012). As we have pointed out, summarizing opinions from online forums, where the topics can be social issues, is quite different from summarizing product reviews. For general opinion summarization, in 2008 the Text Analysis Conference (TAC) organized an opinion summarization task. But their task is different from the one we study here. Their task is a query-oriented summarization problem where a target topic is given together with some specific questions. The corpus they use is a large set of blogs. Our task is not query-oriented, and we aim to summarize the opinions found i"
R15-1020,W14-1504,0,0.0393937,"Missing"
R15-1020,P05-1065,0,0.0396546,"ood summary sentences tend to contain longer words. 4.2.2 4.2.3 POS based Features Part-of-speech based grammatical features have been widely used in text quality prediction (Feng et al., 2010; Dell’Orletta et al., 2014). They can capture the linguistic and syntactic structure of sentence, which may affect its readability. In this work, we calculate the percentage of nouns, verbs, adjectives and adverbs in each sentence. 4.2.4 Parse Tree Height The height of the parse tree of a sentence has been used in previous work to assess text quality (Dell’Orletta et al., 2014; Pitler and Nenkova, 2008; Schwarm and Ostendorf, 2005). Here we use Stanford PCFG Parser to extract this feature. We hypothesize that as summary sentences tend to be more informative and more well-written, they may be more complicated in terms of syntactic structure and their parse tree height are probably larger than non-summary sentences. 4.3 Subjectivity Although online forums mostly contain opinions, people sometimes also share facts or perceived facts in forums. Since our problem is opinion summarization, the summary sentences presumably should be subjective. We therefore use the following feature to test our hypothesis. Number of sentiment"
R15-1020,N10-1134,0,0.237013,"ce look like wks.. this is becoming the PAP ’s official logo Sleep and Dream Table 1: Summary sentences selected by the ILP-based method (Gillick and Favre, 2009) from a thread on criticizing parliament ministers sleeping in a meeting. Existing approaches include centroid-based methods (Radev et al., 2004), learning-based methods (Kupiec et al., 1995; Wong et al., 2008) and graph-based methods (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Mei et al., 2010). More recently, Lin and Bilmes have done a series of work modeling text summarization with submodular functions (Lin and Bilmes, 2011; Lin and Bilmes, 2010). To globally infer an optimal set of sentences as a summary, ILP-based document summarization has been used. It was first proposed by McDonald (2007) and Gillick and Favre (2009) proposed a scalable version. Opinion summarization: Much work on opinion summarization is for product reviews (Hu and Liu, 2004; Popescu et al., 2005; Ganesan et al., 2012). As we have pointed out, summarizing opinions from online forums, where the topics can be social issues, is quite different from summarizing product reviews. For general opinion summarization, in 2008 the Text Analysis Conference (TAC) organized a"
R15-1020,H05-1044,0,0.0620153,"Missing"
R15-1020,C08-1124,0,0.16262,"and snoozing means thinking very hard . ministers / MPs must take parliament session very seriously . becos in parliament , very important topics are being discussed and debated . must pay attention and stay awake ! ! sleeping on the job ? His face look like wks.. this is becoming the PAP ’s official logo Sleep and Dream Table 1: Summary sentences selected by the ILP-based method (Gillick and Favre, 2009) from a thread on criticizing parliament ministers sleeping in a meeting. Existing approaches include centroid-based methods (Radev et al., 2004), learning-based methods (Kupiec et al., 1995; Wong et al., 2008) and graph-based methods (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Mei et al., 2010). More recently, Lin and Bilmes have done a series of work modeling text summarization with submodular functions (Lin and Bilmes, 2011; Lin and Bilmes, 2010). To globally infer an optimal set of sentences as a summary, ILP-based document summarization has been used. It was first proposed by McDonald (2007) and Gillick and Favre (2009) proposed a scalable version. Opinion summarization: Much work on opinion summarization is for product reviews (Hu and Liu, 2004; Popescu et al., 2005; Ganesan et al., 2012"
R15-1020,W04-3252,0,\N,Missing
W11-1922,W11-1901,0,0.0833179,"Missing"
W11-1922,M95-1005,0,0.254413,"and PRP-PRP link. Different link types may have different feature preferences. So we train the classification based pre-cluster pair model for each link type separately and use different models to predicate the results. With the predicating results for pre-cluster pairs, we use closest-first clustering to link them and form the final cluster results. 4 Experimental Results We present our evaluation results on development dataset for CoNLL-2011 shared Task in Table 3, Table 4 and Table 5. Official test results are given in Table 6 and Table 7. Three different evaluation metrics were used: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998) and CEAF (Luo, 2005). Finally, the average scores of these three metrics are used to rank the participating systems. The difference between Table 3 and Table 4 is whether gold standard mention boundaries are given. Here ”mention boundaries” means a more broad concept than the mention definition we gave earlier. We should also detect real mentions from them. From the tables, we can see that the scores can be improved litttle by using gold standard mention boundaries. Also the results from Table 5 tell us that combining different link-type based classification mod"
W11-1922,H05-1004,0,0.200238,"Missing"
W11-1922,D08-1067,0,0.116733,"Missing"
W11-1922,D09-1101,0,0.375705,"Missing"
W11-1922,P10-1142,0,0.0354776,"Missing"
W11-1922,P06-1005,0,0.0649394,"Missing"
W14-2705,D10-1007,0,0.034017,"f word distributions. We also assume an author-level distribution over writing styles. It is worth pointing out that although our model bears similarity to a number of other LDA extensions, our objectives are different from existing work. E.g., the author topic model (Rosen-Zvi et al., 2004) also assumes an authorlevel distribution over topics, but the author-level distribution is meant to capture an author’s topical interests. In contrast, our user-level distribution is over writing styles and is meant to identify serious versus unserious users. Similar to the models by Mei et al. (2007) and Paul et al. (2010) , we also use switch variables to alternate between different types of word distributions, but our goal is to identify words associated with writing styles instead of sentiment words or perspective words. Table 2: Sample posts of two example users. the same writing style in most of his posts. For example, Table 2 shows two example users, each with two sample posts. We can see that their writing styles are consistent in the two posts. If we treat each writing style as a latent factor associated with a word distribution, we can associate observed words with the underlying writing styles. Howeve"
W14-2705,D08-1020,0,0.0320538,"a meaningful slang lexicon. Moreover, we find that topics discovered by our topic-style model are more distinctive from each other than topics produced by standard LDA. Our contributions in this paper can be summarized as follows: 1) We propose a principled topicstyle model to jointly model topics and writing styles at the same time in online forums. 2) An extensive set of experiments shows that our model is effective in separating the more serious posts and unserious posts and identifying slang words. 2 Another body of related research is around studying text quality, formality and sarcasm. Pitler and Nenkova (2008) investigated different features for text readability judgement and empirically demonstrated that discourse relations are highly correlated with perceived readability. Brooke et al. (2010) applied Latent Semantic Analysis to determine the formality level of lexical items. Agichtein et al. (2008) presented a general classification framework incorporating community feedback to identify high quality content in social media. Davidov et al. (2010) proposed the first robust algorithm for recognition of sarcasm. Gonz´alezIb´an˜ ez et al. (2011) took a closer look at sarcasm in Twitter messages and fo"
W14-2705,C10-2011,0,0.0192871,"is paper can be summarized as follows: 1) We propose a principled topicstyle model to jointly model topics and writing styles at the same time in online forums. 2) An extensive set of experiments shows that our model is effective in separating the more serious posts and unserious posts and identifying slang words. 2 Another body of related research is around studying text quality, formality and sarcasm. Pitler and Nenkova (2008) investigated different features for text readability judgement and empirically demonstrated that discourse relations are highly correlated with perceived readability. Brooke et al. (2010) applied Latent Semantic Analysis to determine the formality level of lexical items. Agichtein et al. (2008) presented a general classification framework incorporating community feedback to identify high quality content in social media. Davidov et al. (2010) proposed the first robust algorithm for recognition of sarcasm. Gonz´alezIb´an˜ ez et al. (2011) took a closer look at sarcasm in Twitter messages and found that automatic classification can be as good as human classification. All these studies mainly rely on supervised techniques and human annotation needs to be done, which is very time c"
W14-2705,W10-2914,0,0.0373503,"ous posts and unserious posts and identifying slang words. 2 Another body of related research is around studying text quality, formality and sarcasm. Pitler and Nenkova (2008) investigated different features for text readability judgement and empirically demonstrated that discourse relations are highly correlated with perceived readability. Brooke et al. (2010) applied Latent Semantic Analysis to determine the formality level of lexical items. Agichtein et al. (2008) presented a general classification framework incorporating community feedback to identify high quality content in social media. Davidov et al. (2010) proposed the first robust algorithm for recognition of sarcasm. Gonz´alezIb´an˜ ez et al. (2011) took a closer look at sarcasm in Twitter messages and found that automatic classification can be as good as human classification. All these studies mainly rely on supervised techniques and human annotation needs to be done, which is very time consuming. Our method is fully unsupervised, which can automatically uncover different styles and separate serious posts from unserious posts. Our work is also related to spam/spammer detection in social media, which has been studied over different platforms"
W14-2705,I13-1014,0,0.0304502,"nd separate serious posts from unserious posts. Our work is also related to spam/spammer detection in social media, which has been studied over different platforms for a few years. Jindal and Liu (2008) first studied opinion spam in online reviews and proposed a classification method for opinion spam detection. Bhattarai et al. (2009) Related Work Latent Dirichlet Allocation (LDA) (Blei et al., 2003) has been shown to be useful for many ap34 investigated different content attributes of comment spam in the Blogsphere and built a detection system with good performance based on these attributes. Ding et al. (2013) proposed to utilize both content and social features to detect spams in online question answer website. Existing work on spam detection need annotated data to learn the spam features but our model does not as it is fully unsupervised. 3 A Topic-Style Model Writing styles can be reflected in many different ways. Besides choices of words or expressions, many other linguistic features such as sentence length, sentence complexity and use of punctuation marks may all be associated with one’s writing style. In this work, however, we try to take an approach that does not rely on heavy linguistic ana"
W14-2705,P11-2102,0,0.0223464,"d studying text quality, formality and sarcasm. Pitler and Nenkova (2008) investigated different features for text readability judgement and empirically demonstrated that discourse relations are highly correlated with perceived readability. Brooke et al. (2010) applied Latent Semantic Analysis to determine the formality level of lexical items. Agichtein et al. (2008) presented a general classification framework incorporating community feedback to identify high quality content in social media. Davidov et al. (2010) proposed the first robust algorithm for recognition of sarcasm. Gonz´alezIb´an˜ ez et al. (2011) took a closer look at sarcasm in Twitter messages and found that automatic classification can be as good as human classification. All these studies mainly rely on supervised techniques and human annotation needs to be done, which is very time consuming. Our method is fully unsupervised, which can automatically uncover different styles and separate serious posts from unserious posts. Our work is also related to spam/spammer detection in social media, which has been studied over different platforms for a few years. Jindal and Liu (2008) first studied opinion spam in online reviews and proposed"
W14-2705,C94-2174,0,0.0524021,"means the styles are mostly distinct from each other when the number is 2 and their difference decreases when there are more styles. To get a better understanding of the differences of using different numbers of styles, we compare the top words in each style when the number of styles is set to be 2, 3 and 4. The results are shown in Table 5 where all uppercase words represent emoticons. From the top words of the first row, we Probability 0.2 0.15 0.1 0.05 0 0 2 4 6 8 10 12 14 16 18 Word Length Figure 3: Word length distribution One previous work uses word length as an indicator of formality (Karlgren and Cutting, 1994). Here, we borrow this idea and compare the word length of Style 1 and Style 2. We calculate the distributions of word length and show the results in Figure 3. It shows that the majority of words in Style 1 are longer compared with those in Style 2. To have a quantitative view of the difference between the word lengths of these two styles, we heuristically extract words labeled with Style 1 37 and Style 2 in our dataset in the final iteration of Gibbs sampling and apply Mann-Whitney U test on these two word length populations. The null hypothesis that the two input populations are the same is"
