1984.bcs-1.13,J85-2001,1,0.826318,"Missing"
1989.mtsummit-1.1,1989.mtsummit-1.18,1,0.881553,"onjunction with the conference, fourteen companies presented exhibitions of MT systems and related technology, enabling the participants to obtain heightened recognition of the present MT systems. Items clarified at the conference included the following. (1) Although the present MT systems are imperfect, they are economical, depending on the application, when compared with human translation. (2) To use MT systems practically, it is important effectively to execute pre-editing and postediting. When a mass of documents are translated, division of labor in such editing is particularly effective. (3) The compilation of a technical-term dictionary has been under way, but additional term dictionaries are required in many specialized fields. Also, there are often cases where the party ordering the translation specifies the translation words to be used, so that the system must be applicable in such a situation. (4) We should develop a system whereby the user can input the experiences of machine translation post-editing, a system that could be improved not only by makers but also by users. 102 (5) The experiences of the users of MT systems should be studied carefully. Case studies must be cond"
1992.tmi-1.11,J85-2001,1,0.86393,"Missing"
1992.tmi-1.11,P84-1069,1,0.786529,"the former needs very many rules than the latter. Linguists also prefer to represent collocational phenomena of several far apart elements easier in many-to-one merging rules than two-to-one rules. 2 To represent complex linguistic phenomena 2.1 We introduced the grammar formalism of tree transformation into our machine translation system which was built by the Japanese Governmental Project, called Mu Project, which was conducted during 1982-1986[l]. We developed a software system called GRADE which accepted a tree composed of any number of grammatical terms and converted it into another tree[2]. Each term could have attribute-value pairs, and semantic restriction could be expressed by using this information. Not only the attributevalue pair information could be inherited, but also some synthesized information from the attribute-value pairs could be given to a term in the transformed tree. This tree transformation grammar formalism was used not only for the analysis but also for the transfer and generation of sentences in our machine translation system. Case 129 frame information was given by this formalism, which generally included three to five grammatical components. In the transf"
1992.tmi-1.11,C92-1029,1,0.810113,"ures when they form a parallel structure. It is not the simple similarity of head words in two componential word strings, the similarity of total phrasal structures of both componential word strings. If we apply the ordinary parsing strategy by using ordinary grammar rules to find out this kind of parallel structures we will fall into the same mistakes of wrong parsing or parsing failure, into which the parsing systems of current machine translation systems fall. We introduced here a very new idea to find out similarity between two arbitrary word strings by using the dynamic programming method[3]. The method is shown conceptually in Fig. 1. An example is given in Fig. 2. We place a sentence on a diagonal edge of a triangle by word unit (in case of Japanese, by bunsetsu unit which is composed of a content word and suffix words). Then we find out in these units suffix words or special symbols which indicate possible existence of parallel structures on both sides of them. ""to(and)"", ""matawa(or)"", "",'' etc. are such typical words and symbols, and hereafter we call them keys for parallel structures. In Fig. 2 keys are indicated by > such as a>, b> and c>. In each crossing position of two w"
1992.tmi-1.11,J94-4001,1,0.877889,"Missing"
1993.iwpt-1.11,C90-2049,0,0.0653994,"Missing"
1995.iwpt-1.22,H91-1060,0,0.0833004,"Missing"
C86-1029,C86-1021,1,\N,Missing
C86-1029,C82-1040,1,\N,Missing
C86-1029,P84-1057,1,\N,Missing
C86-1029,P84-1069,1,\N,Missing
C86-1029,P84-1086,1,\N,Missing
C86-1029,J85-2001,1,\N,Missing
C88-2098,C80-1081,0,0.0248638,"Missing"
C88-2098,C82-1036,0,0.0669798,"Missing"
C88-2098,J87-3001,0,\N,Missing
C88-2141,P84-1057,1,\N,Missing
C88-2142,1987.mtsummit-1.23,1,0.745733,"Missing"
C88-2142,C86-1021,1,\N,Missing
C88-2142,C86-1149,0,\N,Missing
C88-2142,J85-2001,1,\N,Missing
C90-3044,1988.tmi-1.13,0,0.46655,"Missing"
C92-2088,P91-1027,0,0.0420531,"Missing"
C92-2088,P91-1017,0,0.087783,"and word mf~anings (such as English and Japanese), and to c(nnt~are analyzed results from each language, h| many (:asc~, the two languagcs }Lave different types of syntactic ambiguities, anti comparison of syntactic structures of both bmguagcs helps to resolve the ambiguities. Also, a pair of bilingually equivalent snrface words helps to a~&apos;4ociate tile words with conceptual l&apos;~oc. OF COL]NG-92. NANTES.AUG.23-28, 1992 words helps to associate the words with conceptual items, because the intersection of conceptual items that each surface word has could be considered as one conceptual item[ll] [2]. [&quot;or example, in tire case of the translation example given in Example 1, both syntactic and semantic ambiguities are resolved. Example 1 E: J: I hung my coat on the hook. ~:L(I) ;~ (topic) ~2~ (coat) ~ (ca.se-m~trker) ~&apos;5&quot; (hook) lZ (case-marker) zi&apos;$~&apos;f: (hung)o 1. S y n t a c t i c d i s u m b i g u a t i o n The English sentence in Example 1 is syntactically ambiguous because the prepositional phrase &quot;on the hook&quot; can modify both the verb &quot;hung&quot; aad tim noun phrase &quot;my coat&quot; using grammatical knowledge only. On the other band, in the Japanese sentence, the phrase &quot;7)~~&apos;, Is_&quot; can modify"
C92-2088,P90-1034,0,0.0411537,"ion. One ~nch approach is to extract hierarclfical relations or it thesanrtm of conceptual items froln hunLall dictionaries in an automatic way. q)surrnnaru et el. studied to construct a t}LeSaLLrlIsof nominal concepts from noun detinitions[t3], q b m i a r a et al. also extracted snperordinatc-subordmatc relation between verbs from the defining sentences in IPAL[12]. l i e sidcs these rcseasches, there are other several research activitics tbr lexical knowledge acquisition, which syntactically anMyze the sentences m large corpora and attcmpt to extract lcxical knowledge from statistical data [3] [1]. Most of the works undertake shallow analysis of texts and they extract only superticial lexical information. For the development of tile techniques of knowledge acquisition from natural language texts, it is very important to improve the httter approach of cornpiling semantic dictionaries by comimter l)rograuL~. Ilowever, there are at least two basic difficulties in this at)preach 1. Tire i~robh~m (ff s y n t a c t i c a m b i g u i t i e s When analyzing a sentence., syntactic ambiguities often remain. So i~ is not easy to obtain correct parsed results automatically. 2. The, probh~rrr o"
C92-2088,C90-3044,1,0.839076,"COLING-92, NANTES,AUG. 23-28, 1992 cable to sew:ral otller problenrs as well. One of t h e m is to acquire features of nominal concepts. We are at the m o m e n t looking at some specitie nominal expression &quot;A q) B&quot; in Japanese, corresponding literally to &quot;I1 of A&quot; in English. T h a t expression specifies a variety of relationships of noun phrases, which are often stated in different expressions in English. T h e y will help to acquire typical attributes of nominal concepts fl&apos;om bilingual corpora. Our ntethod is also useful to collect parsed traamlation examples tbr example-based translation [9] attd to acquire translation p a t t e r n s between two languages. &apos;Fable 3: Acquired Case Frames for &quot;~-[ &lt; (wr;le)&quot; (7~Lse Frame 15 (on) l~t . :6¢ (sub3) ~ ) J3.. ~ ( [subj,passive]) &quot;~&quot; (with) -e I PRO IIUM REL, QUA, LIN (i,,) Ca.~e Frlmm 2 V- (to) HUM ~;t • fie (subj) HUM l;~ &quot; ~ (obj) t:]: • fit ( LIN [subj,p,,ssive]) ~ (with) -e (i,,) the e x t r a c t e d cm~e slots, ttle systenr ~sks the h u m a n instructor a b o u t the pcx~sibi[ities of tile co-occurrence of the case slots that do not cc.occur in the trans lation examples by composing saml)le phr,&apos;~ses. T h e questions and answers"
C92-2088,H91-1067,0,\N,Missing
C92-2088,P86-1038,0,\N,Missing
C94-2169,1993.iwpt-1.11,1,0.734317,"roposes a novel example retrieval method for avoiding ftfll retrieval of examples. The proposed method has the following three features, 1) it generates retrieval queries from similarities, 2) efficient example retrieval through the tree structure of a thesaurus, 3) binary search along subsumption ordering of retrieval queries. Example retrieval time drastically decreases with the method. 1 2 Introduction Since a nmdel of machine translation (MT) called Translation by Analogy was first proposed in Nagao (1984), nmch work has been undertaken in exampleba~sed NLP (e.g. Sato and Nagao (1990) and Kurohashi and Nagao (1993)). The basic idea of examplebased approach to NLP is to accomplish some task in NLP by imitating a similar previous example, instead of using rules written by h u m a n writers. Major processing steps of example-based approach are: 1) collect examples and the results of performing the task in a database, 2) given an input, retrieve similar examples from the database, 3) adapt the results of tile similar examples to the current input and obtain the output. Compared with the traditional rule-based approach, example-based approach has advantages like: 1) it is easier to m a i n t a i n the implem"
C94-2169,C90-3044,1,0.740067,"the database. This paper proposes a novel example retrieval method for avoiding ftfll retrieval of examples. The proposed method has the following three features, 1) it generates retrieval queries from similarities, 2) efficient example retrieval through the tree structure of a thesaurus, 3) binary search along subsumption ordering of retrieval queries. Example retrieval time drastically decreases with the method. 1 2 Introduction Since a nmdel of machine translation (MT) called Translation by Analogy was first proposed in Nagao (1984), nmch work has been undertaken in exampleba~sed NLP (e.g. Sato and Nagao (1990) and Kurohashi and Nagao (1993)). The basic idea of examplebased approach to NLP is to accomplish some task in NLP by imitating a similar previous example, instead of using rules written by h u m a n writers. Major processing steps of example-based approach are: 1) collect examples and the results of performing the task in a database, 2) given an input, retrieve similar examples from the database, 3) adapt the results of tile similar examples to the current input and obtain the output. Compared with the traditional rule-based approach, example-based approach has advantages like: 1) it is easie"
C94-2169,1988.tmi-1.13,0,0.0817155,"Missing"
C94-2175,J90-2002,0,0.292042,"Missing"
C94-2175,J93-2003,0,0.0512019,"Missing"
C94-2175,J93-1004,0,0.225334,"echniques are apt plied to estimate word correspondences not included in bilingual dictionaries. Estimated word correspondences are useful for improving both sentence alignment and structural matching. Introduction Bilingnal (or parallel) texts are useful as resources of linguistic knowledge as well as in applications such as machine translation. One of the major approaches to analyzing bilingual texts is the statistical approach. The statistical approach involves the following: alignment of bilingual texts at the sentence level nsing statistical techniques (e.g. Brown, Lai and Mercer (1991), Gale and Church (1993), Chen (1993), and Kay and RSscheisen (1993)), statistical machine translation models (e.g. Brown, Cooke, Pietra, Pietra et al. (1990)), finding character-level / word-level / phrase-level correspondences from bilingual texts (e.g. Gale and Church (1991), Church (1993), and Kupiec (1993)), and word sense disambiguation for MT (e.g. Dagan, Itai and Schwall (1991)). In general, the statistical approach does not use existing hand-written bilingual dictionaries, and depends solely upon statistics. For example, sentence alignment of bilingual texts are performed just by measuring sentence lengths i"
C94-2175,C90-3101,0,0.017792,"91; Gale and Church, 1993), or by statistically estimating word level correspondences (Chen, 1993; Kay and RSscheisen, 1993). The statistical approach analyzes unstructured sentences in bilingual texts, and it is claimed that the results are useful enough in real applications such as machine translation and word sense disambiguation. However, structured bilingual sentences are undoubtedly more informative and important for future natural language researches. Structured bilingual or multilingual corpora serve as richer sonrces for extracting linguistic knowledge (Klavans and Tzonkermann, 1990; Sadler and Vendelmans, 1990; Kaji, Kida attd Morimoto, 1992; Utsuro, Matsnmoto and Nagao, 1992; Matsumoto, l.shimoto and Utsuro, 1993; Ut1076 Makoto Nagao ~ tGraduate School of Information Science Abstract 1 Yuji Matsumoto t suro, Matsumoto and Nagao, 1993). Compared with the statistical approach, those works are quite different in that they use word correspondence information available in hand-written bilingual dictionaries and try to extract structured linguistic knowledge such as structured translation patterns and case frames of verbs. For example, in Matsunloto et al. (1993), we proposed a method for finding struct"
C94-2175,C92-2088,1,0.843099,"proach, those works are quite different in that they use word correspondence information available in hand-written bilingual dictionaries and try to extract structured linguistic knowledge such as structured translation patterns and case frames of verbs. For example, in Matsunloto et al. (1993), we proposed a method for finding structural matching of parallel sentences, making use of word level similarities calculated from a bilingual dictionary and a thesaurus. Then, those structurally matched parallel sentences are used as a source for acquiring lexical knowledge snch as verbal case frames (Utsuro et al., 1992; Utsuro et al., 1993). With the aim of acquiring those structnred linguistic knowledge, this paper describes a unilied framework for bilingual text matching by combining existing hand-written bilingual dictionaries and statistical techniques. The process of bilingual text matchin 9 consists of two major steps: sentence alignment and structural matching of bilingual sentences. In those two steps, we use word correspondence information, which is available in hand-written bilingual dictionaries, or not included in bilingual dictionaries but estimated with statistical techniques. The reasons why"
C94-2175,C92-2101,0,0.0750694,"Missing"
C94-2175,C90-3031,0,0.0660578,"Missing"
C94-2175,P93-1003,0,0.0605224,"Missing"
C94-2175,P93-1001,0,\N,Missing
C94-2175,P91-1022,0,\N,Missing
C94-2175,P93-1004,1,\N,Missing
C94-2175,P91-1017,0,\N,Missing
C94-2175,J93-1006,0,\N,Missing
C94-2175,H91-1026,0,\N,Missing
C94-2175,P93-1002,0,\N,Missing
C94-2183,J91-2003,0,\N,Missing
C94-2183,C92-1029,1,\N,Missing
C94-2183,P84-1085,0,\N,Missing
C94-2183,J86-3001,0,\N,Missing
C94-2183,P84-1055,0,\N,Missing
C94-2183,P84-1076,0,\N,Missing
C96-2134,C94-2172,0,\N,Missing
C96-2202,H92-1030,0,\N,Missing
C96-2202,H92-1001,0,\N,Missing
C96-2202,E95-1020,0,\N,Missing
C98-2143,P97-1003,0,0.0406129,"in almost all of the recent practical applications in that it describes only relations between sequential elements. Some linguistic phenomena, however, are better described by assuming relations between separated elements. And modeling this kind of phenomena, the accuracies of various application axe generally augmented. As for English, there have been researches in which a stochastic context-free grammar (SCFG) (Fujisaki et al., 1989) is used for model description. Recently some researchers have pointed out the importa~lce of the lexicon and proposed lexicMized models (Jelinek et al., 1994; Collins, 1997). In these models, every headword is propagated up through the derivation tree such that every parent receives a headword from tile head-child. This kind of speciaLization may, however, be excessive if tile criterion is predictive power of the model. Research aimed at estimating the best specialization level for 2-grarn model (Mori et al., 1997) shows a class-based model is more predictive than a word-based 2-gram model, a completely lexicaiized model, comparing cross entropy of a POS-based 2-grain model, a word-based 2-gram model aztd a class-based 2-gram model, estimated from information the"
C98-2143,W89-0209,0,0.447453,"s of the alphabet (n-gram) on a corpus containing a large number of sentences of a language. This is the same model as 0 This work is done when the auther was at Kyoto Univ. 898 used in almost all of the recent practical applications in that it describes only relations between sequential elements. Some linguistic phenomena, however, are better described by assuming relations between separated elements. And modeling this kind of phenomena, the accuracies of various application axe generally augmented. As for English, there have been researches in which a stochastic context-free grammar (SCFG) (Fujisaki et al., 1989) is used for model description. Recently some researchers have pointed out the importa~lce of the lexicon and proposed lexicMized models (Jelinek et al., 1994; Collins, 1997). In these models, every headword is propagated up through the derivation tree such that every parent receives a headword from tile head-child. This kind of speciaLization may, however, be excessive if tile criterion is predictive power of the model. Research aimed at estimating the best specialization level for 2-grarn model (Mori et al., 1997) shows a class-based model is more predictive than a word-based 2-gram model, a"
C98-2143,W97-1003,0,0.024236,"Missing"
C98-2143,H94-1052,0,0.0137414,"t Kyoto Univ. 898 used in almost all of the recent practical applications in that it describes only relations between sequential elements. Some linguistic phenomena, however, are better described by assuming relations between separated elements. And modeling this kind of phenomena, the accuracies of various application axe generally augmented. As for English, there have been researches in which a stochastic context-free grammar (SCFG) (Fujisaki et al., 1989) is used for model description. Recently some researchers have pointed out the importa~lce of the lexicon and proposed lexicMized models (Jelinek et al., 1994; Collins, 1997). In these models, every headword is propagated up through the derivation tree such that every parent receives a headword from tile head-child. This kind of speciaLization may, however, be excessive if tile criterion is predictive power of the model. Research aimed at estimating the best specialization level for 2-grarn model (Mori et al., 1997) shows a class-based model is more predictive than a word-based 2-gram model, a completely lexicaiized model, comparing cross entropy of a POS-based 2-grain model, a word-based 2-gram model aztd a class-based 2-gram model, estimated from"
C98-2143,J92-4003,0,\N,Missing
C98-2145,1993.tmi-1.18,1,0.825132,"o &quot;OJIISAN (old man)&quot; in the following sentences have the salne referent, the second &quot;OJIISAN (old man)&quot; should be prononfinalized in the translation into English. For languages that have articles, like English, we can use articles (&quot;the&quot;, &quot;a&quot;, and so on) to decide whether a noun phrase has an antecedent or not. In contrast, for languages that have no articles, like Japanese, it is dimcult to decide whether a noun phrase has all autecedent. We previously estimated tile referential properties of noun phrases that correspond to articles for the translation of Japanese noun phrases into English (Murata and Nagao 1993). By using these referential properties, our system determines the referents of noun p h r ~ e s in Japanese sentences. Noun phrases are classified by referential property into generic noun phrases, definite noun phrases, and indefinite noun phrases. When tile referential property of a noun phrase is a definite noun phrase, the noun phrase can refer to the entity denoted by a noun phrase that has already appeared. When the referential property of a noun phrase is an indefinite noun phrase or a generic noun phrase, the uoun phrase cannot refer to the entity denoted by a noun phrase that has alr"
J85-2001,P84-1069,1,0.586282,"Missing"
J85-2001,P84-1011,0,0.447574,"Missing"
J85-2001,P84-1057,1,0.817703,"Missing"
J85-2001,P84-1086,1,\N,Missing
J94-4001,J83-2002,0,0.041279,"cate) are called renyoh chuushi-ho (see example sentence (iv) of Table 1). A renyoh chuushi-ho appears in an embedded sentence to modify nouns and is also used to connect two or more sentences. This form is used frequently in Japanese and is a major cause of structural ambiguity. Many major sentential components are omitted in the posterior part of renyoh chuushi expressions, thus complicating the analysis. For the successful analysis of long sentences, these conjunctive phrases and clauses, including renyoh chuushi-ho, must be recognized correctly. Nevertheless, most work in this area (e.g., Dahl and McCord 1983; Fong and Berwick 1985; Hirschman 1986; Kaplan and Maxwell 1988; Sag et al. 1985; Sedogbo 1985; Steedman 1990; Woods 1973) has concerned the problem of creating candidate conjunctive structures or explaining correct conjunctive structures, and not the method for selecting correct structures among many candidates. A method proposed by some researchers (Agarwal and Boggess 1992; Nagao et al. 1983) for selecting the correct structure is, in outline, that the two most similar components to the left side and to the right side of a conjunction are detected as two conjoinedheads in a conjunctive str"
J94-4001,P85-1014,0,0.061048,"h chuushi-ho (see example sentence (iv) of Table 1). A renyoh chuushi-ho appears in an embedded sentence to modify nouns and is also used to connect two or more sentences. This form is used frequently in Japanese and is a major cause of structural ambiguity. Many major sentential components are omitted in the posterior part of renyoh chuushi expressions, thus complicating the analysis. For the successful analysis of long sentences, these conjunctive phrases and clauses, including renyoh chuushi-ho, must be recognized correctly. Nevertheless, most work in this area (e.g., Dahl and McCord 1983; Fong and Berwick 1985; Hirschman 1986; Kaplan and Maxwell 1988; Sag et al. 1985; Sedogbo 1985; Steedman 1990; Woods 1973) has concerned the problem of creating candidate conjunctive structures or explaining correct conjunctive structures, and not the method for selecting correct structures among many candidates. A method proposed by some researchers (Agarwal and Boggess 1992; Nagao et al. 1983) for selecting the correct structure is, in outline, that the two most similar components to the left side and to the right side of a conjunction are detected as two conjoinedheads in a conjunctive structure. For example, in"
J94-4001,C88-1061,0,0.025976,"of Table 1). A renyoh chuushi-ho appears in an embedded sentence to modify nouns and is also used to connect two or more sentences. This form is used frequently in Japanese and is a major cause of structural ambiguity. Many major sentential components are omitted in the posterior part of renyoh chuushi expressions, thus complicating the analysis. For the successful analysis of long sentences, these conjunctive phrases and clauses, including renyoh chuushi-ho, must be recognized correctly. Nevertheless, most work in this area (e.g., Dahl and McCord 1983; Fong and Berwick 1985; Hirschman 1986; Kaplan and Maxwell 1988; Sag et al. 1985; Sedogbo 1985; Steedman 1990; Woods 1973) has concerned the problem of creating candidate conjunctive structures or explaining correct conjunctive structures, and not the method for selecting correct structures among many candidates. A method proposed by some researchers (Agarwal and Boggess 1992; Nagao et al. 1983) for selecting the correct structure is, in outline, that the two most similar components to the left side and to the right side of a conjunction are detected as two conjoinedheads in a conjunctive structure. For example, in &quot;John enjoyed the book and liked the pla"
J94-4001,P92-1003,0,\N,Missing
P84-1057,C82-1062,1,\N,Missing
P84-1057,P84-1069,1,\N,Missing
P84-1057,P84-1086,1,\N,Missing
P84-1057,P84-1011,0,\N,Missing
P84-1069,C82-1004,0,\N,Missing
P84-1069,P84-1057,1,\N,Missing
P84-1069,P84-1086,1,\N,Missing
P84-1069,P84-1011,0,\N,Missing
P84-1086,P84-1069,1,0.782474,"Missing"
P84-1086,P84-1057,1,0.553084,"Missing"
P84-1086,P84-1011,0,0.513078,"Missing"
P84-1086,C82-1062,1,0.78735,"Missing"
P98-2148,J92-4003,0,0.0297894,"in the present paper attest, word-class relation is dependent on language model. In this paper, taking Japanese as the object language, we propose two complete stochastic language models using dependency between bugsetsu, a sequence of one or more content words followed by zero, one or more function words, and evaluate their predictive power by cross entropy. Since the number of sorts of b u n s e t s u is enormous, considering it as a symbol to be predicted would surely invoke the datasparseness problem. To cope with this problem we use the concept of class proposed for a word n-gram model (Brown et al., 1992). Each bunsetsu is represented by the class calculated from the POS of its last content word and that of its last function word. The relation between bunsetsu, called dependency, is described by a stochastic context-free grammar (Fu, 1974) on the classes. From the class of a bunsetsu, the content word sequence and the function word sequence are independently predicted by word n-gram models equipped with unknown word models (Mori and Yamaji, 1997). The above model assumes that the syntactic behavior of each bunsetsu depends only on POS. The POS system invented by grammarians may not always be t"
P98-2148,P97-1003,0,0.0403373,"d in almost all of the recent practicM applications in that it describes only relations between sequential elements. Some linguistic phenomena, however, axe better described by assuming relations between sepaxated elements. And modeling this kind of phenomena, the accuracies of various application axe generally augmented. As for English, there have been researches in which a stochastic context-free grammar (SCFG) (Fujisaki et ~1., 1989) is used for model description. Recently some researchers have pointed out the importance of the lexicon and proposed lexicalized models (Jelinek et al., 1994; Collins, 1997). In these models, every headword is propagated up through the derivation tree such that every parent receives a headword from the head-child. This kind of specialization may, however, be excessive if the criterion is predictive power of the model. Research ~ m e d at estimating the best specialization level for 2-gram model (Mori et aL, 1997) shows a class-based model is more predictive than a word-based 2-gram model, a completely lexicalized model, comparing cross entropy of a POS-based 2-graxa model, a word-based 2-gram model and a class-based 2-graxa model, estimated from information theor"
P98-2148,W89-0209,0,0.595279,"Missing"
P98-2148,W97-1003,0,0.0235135,"Missing"
P98-2148,H94-1052,0,0.0149636,"Sakyo Kyoto, Japan used in almost all of the recent practicM applications in that it describes only relations between sequential elements. Some linguistic phenomena, however, axe better described by assuming relations between sepaxated elements. And modeling this kind of phenomena, the accuracies of various application axe generally augmented. As for English, there have been researches in which a stochastic context-free grammar (SCFG) (Fujisaki et ~1., 1989) is used for model description. Recently some researchers have pointed out the importance of the lexicon and proposed lexicalized models (Jelinek et al., 1994; Collins, 1997). In these models, every headword is propagated up through the derivation tree such that every parent receives a headword from the head-child. This kind of specialization may, however, be excessive if the criterion is predictive power of the model. Research ~ m e d at estimating the best specialization level for 2-gram model (Mori et aL, 1997) shows a class-based model is more predictive than a word-based 2-gram model, a completely lexicalized model, comparing cross entropy of a POS-based 2-graxa model, a word-based 2-gram model and a class-based 2-graxa model, estimated from i"
P98-2150,1993.tmi-1.18,1,0.694109,"e referential property of a noun phrase here means how the noun phrase denotes the referent. If the system can recognize that the second &quot;OJIISAN (old man)&quot; has the referential property of the definite noun phrase, indicating that the noun phrase refers to the contextually non-ambiguous entity, it will be able to judge that the second &quot;OJIISAN (old man)&quot; refers to the entity denoted by the first &quot;OJIISAN (old man). The referential property plays an important role in clarifying the anaphoric relation. We previously classified noun phrases by referential property into the following three types (Murata and Nagao 1993). NP g e n e r i c NP { n o n g e n e r i c NP d e f i n i t e NP i n d e f i n i t e NP G e n e r i c n o u n p h r a s e A noun phrase is classified as generic when it denotes all members of the class described by the noun phrase or the class itself of the noun phrase. For example, &quot;INU(dog)&quot; in the following sentence is a generic noun phrase. INU-WA YAKUNI-TATSU. (dog) (useful) (Dogs are useful.) (3) A generic noun phrase cannot refer to the entity denoted by an indefinite or definite noun phrase. Two generic noun phrases can have the same referent. D e f i n i t e n o u n p h r a s e A nou"
W97-0109,C94-2195,0,0.710845,"Missing"
W97-0109,P93-1035,0,0.0221687,"Missing"
W97-0109,H94-1047,0,0.0349976,"Missing"
W97-0109,W95-0103,0,0.37782,"Missing"
W97-0109,H94-1046,0,0.0797553,"Missing"
W97-0109,W95-0105,0,0.0661747,"Missing"
W97-0109,H91-1037,0,0.0639452,"Missing"
W97-0109,H93-1052,0,0.107194,"Missing"
W97-0109,P95-1026,0,0.0915626,"Missing"
W98-0605,W97-0109,1,\N,Missing
W98-0701,C96-1005,0,0.0763487,"Missing"
W98-0701,J96-1002,0,0.00360777,"(lo)g i = 2--, h~ L j / L represents the score of the head word sense j based on the matrices Q calculated in the step 1., i.e.: (ll) Lj = ~ maz(zi(j, u)) i=I I where x i ( j , u ) E Qi and m a x ( x i ( j , u ) ) highest score in the line of the matrix Qi corresponds to the head word sense j. n number of modifiers of the head word h current tree level, and is the which is the at the k i Lj = j~l Lj where k is the number of senses of the head word h. I i I I The reason why gj (I0) is calculated as a sum of the best scores (ll), rather than by using the traditional maximum likelihood estimate (Berger et al., 1996)(Gah eta[., 1993), is to minimise the effectof the sparse data problem. Imagine, for example, the phrase VP-- VB NP PP, where the head verb V B is in the object relation with the head of the noun phrase NP and also in the modifying relation with the head of the prepositional p h r ~ e PP. Let us also assume that the correct sense of the verb VB is a. Even if the verb-object relation provided a strong selectional support for the sense a, if there was no example in the training set for the second relation (between VB and PP) which would score a hit for the sense a, multiplying the scores of that"
W98-0701,P96-1025,0,0.0785509,"Missing"
W98-0701,H94-1052,0,0.018234,"Missing"
W98-0701,W96-0104,0,0.022059,"Missing"
W98-0701,P95-1037,0,0.0581033,"Missing"
W98-0701,J93-2004,0,0.028952,"Missing"
W98-0701,H93-1061,0,0.112636,"e # of senses 5.4 10.5 5.5 3.? 5.8 The Task Specification For our work, we used the word sense definitions as given in WordNet (Miller, 1990), which is comparable to a good printed dictionary in its coverage and distinction of senses. Since WordNet only provides definitions for content words (nouns, verbs, adjectives and adverbs), we are only concerned with identifying the correct senses of the content words. Both for the training and for the testing of our algorithm, we used the syntactically analysed sentences of the Brown Corpus (Marcus, 1993), which have been manually semantically tagged (Miller et al., 1993) into semantic concordance files (SemCor). These files combine 103 passages of the Brown Corpus with the WordNet lexical database in such a way that every content word in the text carries both a syntactic tag and a semantic tag pointing to the appropriate sense of that word in WordNet. Passages in the Brown Corpus are approximately 2,000 words long, and each contains approximately 1,000 content words. The percentages of the nouns, verbs, adjectives and adverbs in the semantically tagged corpus, together with theiraverage number of Word Net senses, are given in Table I. Although most of the wor"
W98-0701,H93-1054,0,0.0420515,"Missing"
W98-0701,W95-0105,0,0.0306191,"Missing"
W98-0701,P95-1026,0,0.445582,"Missing"
W98-0701,W97-0109,1,\N,Missing
W98-0701,W96-0208,0,\N,Missing
W98-0701,H94-1046,0,\N,Missing
W98-0701,H93-1052,0,\N,Missing
W98-0701,P94-1020,0,\N,Missing
W98-0701,P92-1032,0,\N,Missing
W98-0701,P91-1034,0,\N,Missing
W98-0701,P91-1017,0,\N,Missing
W98-0701,P95-1025,0,\N,Missing
W99-0205,1993.tmi-1.18,1,0.887395,"an example of the form &quot;Noun X no Noun Y&quot; (Y of X), when noun Y is a 33 Table 5: Case frame of verb &quot;mukad&apos; (go to) Surface case ga-case (subject) n/-case (object) Semantic constraint concrete place a focus is defined as a word which is stressed by the speaker (or the writer). But we cannot detect topics and foci correctly. Therefore we approximated them as shown in Table 3 and Table 4. The distance D is the number of the topics (foci) between the anaphor and a possible antecedent which is a topic (focus). The value P is given by the score of the definiteness in referential property analysis (Murata and Nagao, 1993). This is because it is easier for a definite noun phrase to have an antecedent than for an indefinite noun phrase to have one. The value S is the semantic similarity between a possible antecedent and Noun X of &quot;Noun X no Noun Y.&quot; Semantic similarity is shown by level in Bunrui Goi Hyou (NLRI, 1964). in meaning. ojiisan-wa ooyorokobi-wo-shite ie-ni kaerimashita. (the old man) (in great joy) (house) (returned) [The old man returned home (house) in great joy,] okotta koto-wo hitobito-ni hanashimashita (happened to him) (all things) (everybody) (told) (and told everybody all that had happened to"
W99-0205,J94-2003,0,\N,Missing
W99-0206,P86-1031,0,0.018776,"Missing"
W99-0206,J94-2003,0,\N,Missing
W99-0206,C94-2188,0,\N,Missing
W99-0206,C96-2134,1,\N,Missing
