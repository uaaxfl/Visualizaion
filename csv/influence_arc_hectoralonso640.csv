2016.gwc-1.30,A00-2006,0,0.217945,"Missing"
2016.gwc-1.30,alvez-etal-2008-complete,0,0.0258999,"bers of the current inventory, which we postulate by identifying semantically coherent groups of synsets. We cover the expansion of the already-established supernsense inventory for nouns and verbs, the addition of coarse supersenses for adjectives in absence of a canonical supersense inventory, and supersenses for verbal satellites. We evaluate the viability of the new senses examining the annotation agreement, frequency and co-ocurrence patterns. 1 Introduction Coarse word-sense disambiguation is a well established discipline (Segond et al., 1997; Peters et al., 1998; Lapata and Brew, 2004; Alvez et al., 2008; Izquierdo et al., 2009) that has acquired more momentum in the latter years under the name of supersense tagging (SST). SST uses a coarse sense inventory to label spans of variable word length (Ciaramita and Johnson, 2003; Ciaramita and Altun, 2006; Johannsen et al., 2014). This coarse sense inventory is obtained from the list of WordNet first beginners, i.e. the names of the lexicographer files that hold the synsets. However, lexicographer files were devised for practical reasons, namely as an organization method for the development of WordNet (Miller, 1990; Gross and Miller, 1990; Fellbaum"
2016.gwc-1.30,J12-3005,0,0.0564994,"Missing"
2016.gwc-1.30,W06-1670,0,0.0468962,"tives in absence of a canonical supersense inventory, and supersenses for verbal satellites. We evaluate the viability of the new senses examining the annotation agreement, frequency and co-ocurrence patterns. 1 Introduction Coarse word-sense disambiguation is a well established discipline (Segond et al., 1997; Peters et al., 1998; Lapata and Brew, 2004; Alvez et al., 2008; Izquierdo et al., 2009) that has acquired more momentum in the latter years under the name of supersense tagging (SST). SST uses a coarse sense inventory to label spans of variable word length (Ciaramita and Johnson, 2003; Ciaramita and Altun, 2006; Johannsen et al., 2014). This coarse sense inventory is obtained from the list of WordNet first beginners, i.e. the names of the lexicographer files that hold the synsets. However, lexicographer files were devised for practical reasons, namely as an organization method for the development of WordNet (Miller, 1990; Gross and Miller, 1990; Fellbaum, 1990), and not as final target categories to annotate with or disambiguate from. Nevertheless, the organization of lexicographer files is semantically motivated, and supersenses have proven useful for natural language processing such as metaphor de"
2016.gwc-1.30,W03-1022,0,0.206469,"coarse supersenses for adjectives in absence of a canonical supersense inventory, and supersenses for verbal satellites. We evaluate the viability of the new senses examining the annotation agreement, frequency and co-ocurrence patterns. 1 Introduction Coarse word-sense disambiguation is a well established discipline (Segond et al., 1997; Peters et al., 1998; Lapata and Brew, 2004; Alvez et al., 2008; Izquierdo et al., 2009) that has acquired more momentum in the latter years under the name of supersense tagging (SST). SST uses a coarse sense inventory to label spans of variable word length (Ciaramita and Johnson, 2003; Ciaramita and Altun, 2006; Johannsen et al., 2014). This coarse sense inventory is obtained from the list of WordNet first beginners, i.e. the names of the lexicographer files that hold the synsets. However, lexicographer files were devised for practical reasons, namely as an organization method for the development of WordNet (Miller, 1990; Gross and Miller, 1990; Fellbaum, 1990), and not as final target categories to annotate with or disambiguate from. Nevertheless, the organization of lexicographer files is semantically motivated, and supersenses have proven useful for natural language pro"
2016.gwc-1.30,W97-0802,0,0.0931647,"Missing"
2016.gwc-1.30,W14-0107,0,0.020603,"iented towards conveying sense denotation that connotation. Hence, we suggest a new supersense A . FUNCTION to give account for function-related senses, what in the terminology of Pustejovsky (1991) would be the telic role. We observe that the ALLGEMEIN (‘general’) category of GermaNet and Tsvetkov et al’s MISCEL LANEOUS hold similar senses. 5.4 Satellites When annotating nouns in Section 3, we annotate continuous NER-like spans. But verb-headed multiwords pose a challenge because they are not necessarily continuous, and pose attested challenges for their annotation and automatic recognition (Hoppermann and Hinrichs, 2014; Baldwin, 2005b; Baldwin, 2005a). We use three satellite tags; S . COLLOCATION, S . PARTICLE and S . REFLPRON (for reflexive pronouns). While the particle distinction is more relevant for satellite-framed languages (Talmy, 1985) like Germanic languages, light-verb constructions are pervasive in many languages, also characteristically verb-framed languages like Spanish or French, where we find verb-headed multiwords like llevar a cabo (lit. ‘take to ending’, ‘carry out’) or avoir l’air (lit. ‘to have the air’, ‘seem’), respectively. A similar approach has been used by Schneider and Smith (2015"
2016.gwc-1.30,E09-1045,0,0.0514391,"Missing"
2016.gwc-1.30,S14-1001,1,0.83293,"Missing"
2016.gwc-1.30,kipper-etal-2006-extending,0,0.0477719,"h. This metric aims at justifying having document as an NER label, where span identification is as relevant as proper labeling. We believe the frequency of document-name named entities makes a good case for considering the N . DOCUMENT class as an addition to the SSI and to NER. However, we do not find enough support to recommend a N . LANGUAGE supersense and prefer using the original N . COMMUNICATION instead. 5.2 Verbs Verbs are central to the theory of lexical semantics, yet their semantic characterization has been closer to the syntax-semantics interface (Levin, 1993; Kipper et al., 2000; Kipper et al., 2006). In this aspect, the wordnet SSI for verbs is very different, e.g. verbs like jump or displace are of the V. MOTION, even though their argument structures are very different. Nevertheless, verbal sense alternations are often associated with different argument structures (Grimshaw, 1990). The V. CHANGE supersense is populated with semantically disparate categories and is very difficult to annotate, even though it is a very frequent sense, both in terms of annotated words and of synsets adscribed to it. According to Fellbaum (1990), ‘the concept of change is flexible enough to accomodate verbs"
2016.gwc-1.30,J04-1003,0,0.0595355,"s are extensions of members of the current inventory, which we postulate by identifying semantically coherent groups of synsets. We cover the expansion of the already-established supernsense inventory for nouns and verbs, the addition of coarse supersenses for adjectives in absence of a canonical supersense inventory, and supersenses for verbal satellites. We evaluate the viability of the new senses examining the annotation agreement, frequency and co-ocurrence patterns. 1 Introduction Coarse word-sense disambiguation is a well established discipline (Segond et al., 1997; Peters et al., 1998; Lapata and Brew, 2004; Alvez et al., 2008; Izquierdo et al., 2009) that has acquired more momentum in the latter years under the name of supersense tagging (SST). SST uses a coarse sense inventory to label spans of variable word length (Ciaramita and Johnson, 2003; Ciaramita and Altun, 2006; Johannsen et al., 2014). This coarse sense inventory is obtained from the list of WordNet first beginners, i.e. the names of the lexicographer files that hold the synsets. However, lexicographer files were devised for practical reasons, namely as an organization method for the development of WordNet (Miller, 1990; Gross and Mi"
2016.gwc-1.30,N01-1009,0,0.176692,"Missing"
2016.gwc-1.30,W15-0114,0,0.0665215,"Missing"
2016.gwc-1.30,W97-0811,0,0.148135,"e supersense inventory. All new supersenses are extensions of members of the current inventory, which we postulate by identifying semantically coherent groups of synsets. We cover the expansion of the already-established supernsense inventory for nouns and verbs, the addition of coarse supersenses for adjectives in absence of a canonical supersense inventory, and supersenses for verbal satellites. We evaluate the viability of the new senses examining the annotation agreement, frequency and co-ocurrence patterns. 1 Introduction Coarse word-sense disambiguation is a well established discipline (Segond et al., 1997; Peters et al., 1998; Lapata and Brew, 2004; Alvez et al., 2008; Izquierdo et al., 2009) that has acquired more momentum in the latter years under the name of supersense tagging (SST). SST uses a coarse sense inventory to label spans of variable word length (Ciaramita and Johnson, 2003; Ciaramita and Altun, 2006; Johannsen et al., 2014). This coarse sense inventory is obtained from the list of WordNet first beginners, i.e. the names of the lexicographer files that hold the synsets. However, lexicographer files were devised for practical reasons, namely as an organization method for the develo"
2016.gwc-1.30,W15-2005,1,0.604712,"ry genres that are used for information extraction, without sacrificing its adequacy for more usual domains. Generally speaking, another corpus choice would yield a different supersense expansion. Metrics This section describes the metrics applied to the supersense-annotated corpus in order to assess the distribution of the new supersenses. 4.1 Sense-wise agreement variation Inter-annotator agreement is a source of information on the reliability of semantic categories (Lopez de Lacalle and Agirre, 2015). In this section, we examine the variation in agreement for noun and verb supersenses. Cf. Olsen et al. (2015) for a more detailed account. Figures 1 and 2 portray the variation of agreement across noun and verb supersenses. Each cell in the matrix indicates the probability of a token being annotated with a row-column tuple of supersenses (ri , c j ) by the two annotators. The matrix is normalized row-wise, and each row describes the probability distribution of a certain supersense ri to be annotated with any other supersense c j . When ri and c j have the same value, annotators agree. Rows are sorted in descending order of agreement, i.e. the size of the ri = c j box on the diagonal. The larger the b"
2016.gwc-1.30,W09-2402,0,0.0258818,"rdnets is irregular. If, as stated in Section 1, NER compatibility is a favorable side effect of SST, we consider improved NER compatiblity of the new SSI as a plus. Even though NER inventories are application dependent (cf. Nadeau and Sekine (2007) for a survey), our reference is the de facto standard CONLL inventory (Tjong Kim Sang and De Meulder, 2003), with the labels P ERSON, L OCATION and O RGANIZATION, as well as a M ISCELLA NEOUS label, needed for full coverage but not present in e.g. the 7-label inventory of MUC-7 (Chinchor and Robinson, 1997). Concrete meaning is easier to annotate (Passonneau et al., 2009) and can be the easiest to extend with new senses. As a matter of fact, the concrete N . ARTIFACT supersense is the one that yields more new supersenses in our analysis, namely N . BUILDING , N . CONTAINER and N . VEHICLE . In particular, N . BUILDING extends N . ARTIFACT because artifactual locations, already noted as a semantic type the SIMPLE ontology (Lenci et al., 2000), like houses and highways are very often predicated as locations (following locative prepositions, etc.) instead of having the typical distribution of artifacts, i.e. with the verb use or the preposition with. Moreover, N"
2016.gwc-1.30,P14-1024,0,0.0618987,"ained from the list of WordNet first beginners, i.e. the names of the lexicographer files that hold the synsets. However, lexicographer files were devised for practical reasons, namely as an organization method for the development of WordNet (Miller, 1990; Gross and Miller, 1990; Fellbaum, 1990), and not as final target categories to annotate with or disambiguate from. Nevertheless, the organization of lexicographer files is semantically motivated, and supersenses have proven useful for natural language processing such as metaphor detection or relation extraction (Ciaramita and Johnson, 2003; Tsvetkov et al., 2014a; Søgaard et al., 2015). According to Ciaramita and Altun (2006), supersenses extend the named entity recognition (NER) inventory so that the predictions of an SST model subsume the output of NER. Schneider et al. (2015) provide a full SSI for prepositions. The current supersense inventory (henceforth SSI) enjoys de facto standardness, but in spite of its potential usefulness, it is used acritically. The current SSI provides 26 noun supersenes and 15 verb supersenses. Adjective and adverb lexicographer files are disregarded. We provide a revision of the SSI by an extension of its supersenses"
2016.gwc-1.30,tsvetkov-etal-2014-augmenting-english,0,0.0911162,"ained from the list of WordNet first beginners, i.e. the names of the lexicographer files that hold the synsets. However, lexicographer files were devised for practical reasons, namely as an organization method for the development of WordNet (Miller, 1990; Gross and Miller, 1990; Fellbaum, 1990), and not as final target categories to annotate with or disambiguate from. Nevertheless, the organization of lexicographer files is semantically motivated, and supersenses have proven useful for natural language processing such as metaphor detection or relation extraction (Ciaramita and Johnson, 2003; Tsvetkov et al., 2014a; Søgaard et al., 2015). According to Ciaramita and Altun (2006), supersenses extend the named entity recognition (NER) inventory so that the predictions of an SST model subsume the output of NER. Schneider et al. (2015) provide a full SSI for prepositions. The current supersense inventory (henceforth SSI) enjoys de facto standardness, but in spite of its potential usefulness, it is used acritically. The current SSI provides 26 noun supersenes and 15 verb supersenses. Adjective and adverb lexicographer files are disregarded. We provide a revision of the SSI by an extension of its supersenses"
2016.gwc-1.30,J91-4003,0,0.622463,"icle reviewed all the N . COMMUNICATION spans and classified them in three categories, two of them mapped from the EWN top ontology, N . DOCUMENT and N . LANGUAGE , and a third back-off category for N . COMMUNICATION. Notice how, in spite of having spawned three senses (N . CONTAINER, N . VEHICLE and N . BUILDING ), N . ARTIFACT is still a very frequent supersense. The document-language distinction is a highlevel type in the SIMPLE ontology (Lenci et al., 2000). Note that these two new communication subsenses do not solve the artifact-information ambiguity commonly found in lexical semantics (Pustejovsky, 1991). While N . LANGUAGE has more often an eventual reading (e.g. conversation, remark), N . DOCUMENT refers more often to works and other entities with a non-temporal denotation. We also use N . LANGUAGE for the metalinguistic usage of words (e.g. ‘The word drizzle sounds funny’). This re-annotation produces examples like the following: N . INSTITUTION , H. C. Andersen er jo verdensberømt , fordi hans forfatterskab/N . DOCUMENT er blevet oversat til alle sprog/N . LANGUAGE . H. C. Andersen is world famous, because his writing has been translated to all languages. Out of the 1513 N . COMMUNICATION"
2016.gwc-1.30,N15-1177,0,0.0134478,"ermann and Hinrichs, 2014; Baldwin, 2005b; Baldwin, 2005a). We use three satellite tags; S . COLLOCATION, S . PARTICLE and S . REFLPRON (for reflexive pronouns). While the particle distinction is more relevant for satellite-framed languages (Talmy, 1985) like Germanic languages, light-verb constructions are pervasive in many languages, also characteristically verb-framed languages like Spanish or French, where we find verb-headed multiwords like llevar a cabo (lit. ‘take to ending’, ‘carry out’) or avoir l’air (lit. ‘to have the air’, ‘seem’), respectively. A similar approach has been used by Schneider and Smith (2015). The intention of these tags is to help isolate the head of a verb-headed multiword. We assign the sense label to the syntactic head, even though a light verb construction would be arguably best headed by its introduced noun. In this manner, gøre grin af (‘make fun of’) would be labeled as gøre/V. COMMUNICATION grin/S . COLLOCATION af /S . COLLOCATION’, and we thus avoid giving gøre (‘make’) the V. CREATION sense. 6 Conclusions and further work We suggest an extension of the SSI for the three main lexical parts of speech. We obtain new supersenses using a mapping from ontological types, and e"
2016.gwc-1.30,W15-1612,0,0.0152077,"development of WordNet (Miller, 1990; Gross and Miller, 1990; Fellbaum, 1990), and not as final target categories to annotate with or disambiguate from. Nevertheless, the organization of lexicographer files is semantically motivated, and supersenses have proven useful for natural language processing such as metaphor detection or relation extraction (Ciaramita and Johnson, 2003; Tsvetkov et al., 2014a; Søgaard et al., 2015). According to Ciaramita and Altun (2006), supersenses extend the named entity recognition (NER) inventory so that the predictions of an SST model subsume the output of NER. Schneider et al. (2015) provide a full SSI for prepositions. The current supersense inventory (henceforth SSI) enjoys de facto standardness, but in spite of its potential usefulness, it is used acritically. The current SSI provides 26 noun supersenes and 15 verb supersenses. Adjective and adverb lexicographer files are disregarded. We provide a revision of the SSI by an extension of its supersenses using the Danish wordnet as starting point. This revision is empirically backed by four evaluation criteria, namely inter-annotator agreement, sense frequency after adjucation, sense coocurrence, and NER compliance (whene"
2016.jeptalnrecital-poster.5,W04-1013,0,0.057255,"Missing"
2016.jeptalnrecital-poster.5,P15-2138,1,0.895009,"Missing"
2020.emnlp-main.651,W13-2322,0,0.0428121,"language variations. Neural classification models (Mrkˇsi´c et al., 2017; Mrkˇsi´c and Vuli´c, 2018) alleviate the problem by learning distributed representations of user utterances. However, they still lack scalability to large unbounded output space (Xu and Hu, 2018; Lee et al., 2019) and structured representations. To address the limitations, some recent work treats slot filling as a sequence generation task (Ren et al., 2019; Wu et al., 2019). On the other hand, single-turn semantic parsers have long used structured meaning representations to address compositionality (Liang et al., 2013; Banarescu et al., 2013; Kollar et al., 2018; Yu et al., 2018; Gupta et al., 2018). Solutions range 8108 from chart-based constituency parsers (Berant et al., 2013) to more recent neural sequence-to-sequence models (Jia and Liang, 2016). The general challenge of scaling semantic parsing to DST is that dialog state, as an accumulation of conversation history, requires expensive context-dependent annotation. It is also unclear how utterance semantics can be aggregated and maintained in a structured way. In this work we provide a solution to unify DST with semantic parsing. Data Collection The most straightforward appr"
2020.emnlp-main.651,D13-1160,0,0.0605655,"uted representations of user utterances. However, they still lack scalability to large unbounded output space (Xu and Hu, 2018; Lee et al., 2019) and structured representations. To address the limitations, some recent work treats slot filling as a sequence generation task (Ren et al., 2019; Wu et al., 2019). On the other hand, single-turn semantic parsers have long used structured meaning representations to address compositionality (Liang et al., 2013; Banarescu et al., 2013; Kollar et al., 2018; Yu et al., 2018; Gupta et al., 2018). Solutions range 8108 from chart-based constituency parsers (Berant et al., 2013) to more recent neural sequence-to-sequence models (Jia and Liang, 2016). The general challenge of scaling semantic parsing to DST is that dialog state, as an accumulation of conversation history, requires expensive context-dependent annotation. It is also unclear how utterance semantics can be aggregated and maintained in a structured way. In this work we provide a solution to unify DST with semantic parsing. Data Collection The most straightforward approach to building datasets for task-oriented dialog is to directly annotate human-system conversations (Williams et al., 2016). A limitation i"
2020.emnlp-main.651,D18-1547,0,0.129775,"so unclear how utterance semantics can be aggregated and maintained in a structured way. In this work we provide a solution to unify DST with semantic parsing. Data Collection The most straightforward approach to building datasets for task-oriented dialog is to directly annotate human-system conversations (Williams et al., 2016). A limitation is that the approach requires a working system at hand, which causes a classic chicken-and-egg problem for improving user experience. The issue can be avoided with Wizard-of-Oz (WoZ) experiments to collect human-human conversations (El Asri et al., 2017; Budzianowski et al., 2018; Peskov et al., 2019; Byrne et al., 2019; Radlinski et al., 2019). However, dialog state annotation remains challenging and costly in WoZ, and the resulting distribution could be different from that of human-machine conversations (Budzianowski et al., 2018). One approach that avoids direct meaning annotation is to use a dialog simulator (Schatzmann et al., 2007; Li et al., 2016). Recently, Shah et al. (2018) and Rastogi et al. (2019) generate synthetic conversations which are subsequently paraphrased by crowdsourcing. This approach has been proven to provide a better coverage while reducing t"
2020.emnlp-main.651,D19-1459,0,0.0133942,"ated and maintained in a structured way. In this work we provide a solution to unify DST with semantic parsing. Data Collection The most straightforward approach to building datasets for task-oriented dialog is to directly annotate human-system conversations (Williams et al., 2016). A limitation is that the approach requires a working system at hand, which causes a classic chicken-and-egg problem for improving user experience. The issue can be avoided with Wizard-of-Oz (WoZ) experiments to collect human-human conversations (El Asri et al., 2017; Budzianowski et al., 2018; Peskov et al., 2019; Byrne et al., 2019; Radlinski et al., 2019). However, dialog state annotation remains challenging and costly in WoZ, and the resulting distribution could be different from that of human-machine conversations (Budzianowski et al., 2018). One approach that avoids direct meaning annotation is to use a dialog simulator (Schatzmann et al., 2007; Li et al., 2016). Recently, Shah et al. (2018) and Rastogi et al. (2019) generate synthetic conversations which are subsequently paraphrased by crowdsourcing. This approach has been proven to provide a better coverage while reducing the error and cost of dialog state annotat"
2020.emnlp-main.651,W10-4336,0,0.0309509,"a dotted tree and its full drawing can be found in Appendix A. by Shah et al. (2018) and Rastogi et al. (2019), we have collected a large dataset of task-oriented dialogs annotated with hierarchical meaning representations. Each dialog was generated through a two-step process. First, a generative dialog simulator produces a meaningful conversational flow and a template-based utterance for each turn in the conversation. Then the utterances are paraphrased by human annotators Related Work Modeling Traditional DST models apply discriminative classifiers over the space of slot-value combinations (Crook and Lemon, 2010; Henderson et al., 2014b; Williams et al., 2016). These models require feature extraction from user utterances based on manually constructed semantic dictionaries, making them vulnerable to language variations. Neural classification models (Mrkˇsi´c et al., 2017; Mrkˇsi´c and Vuli´c, 2018) alleviate the problem by learning distributed representations of user utterances. However, they still lack scalability to large unbounded output space (Xu and Hu, 2018; Lee et al., 2019) and structured representations. To address the limitations, some recent work treats slot filling as a sequence generation"
2020.emnlp-main.651,W17-5526,0,0.0485685,"Missing"
2020.emnlp-main.651,P16-1154,0,0.0278687,"s defined as follows: ai,j = attn(gi , H) wi,j = softmax(ai,j ) n X ¯ hi = wi,j hj (3) j=1 where attn represents the feed-forward attention defined in Bahdanau et al. (2015) and the softmax is taken over index j. By applying the attention mechanism to all three sources, we get three 8112 Figure 1: An overview of the T ED encoder-decoder architecture. ¯x , h ¯ s , and h ¯ u . The vectors are attention vectors h i i i concatenated together with the state gi to form a feature vector fi , which is used to compute the probability of the next token though a mixture of generation and copy mechanism (Gu et al., 2016): λ = σ(Wi fi + bi ) Pgen = softmax(Wv fi + bv ) Pcopy = softmax(ai , ci , ei ) (4) u ) = λPgen + (1 − λ)Pcopy P (yt,i where W and b are all model parameters. λ is a soft gate controlling the proportion of generation and copy. Pgen is computed with a softmax over the generation vocabulary. a, c and e denote attention logits computed for the three encoders. Since there are three input sources, we concatenate all logits and normalize them to compute the copy distribution Pcopy . The model is optimised on the u ). An log-likelihood of output distribution P (yt,i overview of the model is shown in"
2020.emnlp-main.651,D18-1300,0,0.0199022,"t al., 2017; Mrkˇsi´c and Vuli´c, 2018) alleviate the problem by learning distributed representations of user utterances. However, they still lack scalability to large unbounded output space (Xu and Hu, 2018; Lee et al., 2019) and structured representations. To address the limitations, some recent work treats slot filling as a sequence generation task (Ren et al., 2019; Wu et al., 2019). On the other hand, single-turn semantic parsers have long used structured meaning representations to address compositionality (Liang et al., 2013; Banarescu et al., 2013; Kollar et al., 2018; Yu et al., 2018; Gupta et al., 2018). Solutions range 8108 from chart-based constituency parsers (Berant et al., 2013) to more recent neural sequence-to-sequence models (Jia and Liang, 2016). The general challenge of scaling semantic parsing to DST is that dialog state, as an accumulation of conversation history, requires expensive context-dependent annotation. It is also unclear how utterance semantics can be aggregated and maintained in a structured way. In this work we provide a solution to unify DST with semantic parsing. Data Collection The most straightforward approach to building datasets for task-oriented dialog is to di"
2020.emnlp-main.651,W14-4337,1,0.714351,"Missing"
2020.emnlp-main.651,W14-4340,0,0.114654,"ull drawing can be found in Appendix A. by Shah et al. (2018) and Rastogi et al. (2019), we have collected a large dataset of task-oriented dialogs annotated with hierarchical meaning representations. Each dialog was generated through a two-step process. First, a generative dialog simulator produces a meaningful conversational flow and a template-based utterance for each turn in the conversation. Then the utterances are paraphrased by human annotators Related Work Modeling Traditional DST models apply discriminative classifiers over the space of slot-value combinations (Crook and Lemon, 2010; Henderson et al., 2014b; Williams et al., 2016). These models require feature extraction from user utterances based on manually constructed semantic dictionaries, making them vulnerable to language variations. Neural classification models (Mrkˇsi´c et al., 2017; Mrkˇsi´c and Vuli´c, 2018) alleviate the problem by learning distributed representations of user utterances. However, they still lack scalability to large unbounded output space (Xu and Hu, 2018; Lee et al., 2019) and structured representations. To address the limitations, some recent work treats slot filling as a sequence generation task (Ren et al., 2019;"
2020.emnlp-main.651,P16-1002,0,0.0200054,"ility to large unbounded output space (Xu and Hu, 2018; Lee et al., 2019) and structured representations. To address the limitations, some recent work treats slot filling as a sequence generation task (Ren et al., 2019; Wu et al., 2019). On the other hand, single-turn semantic parsers have long used structured meaning representations to address compositionality (Liang et al., 2013; Banarescu et al., 2013; Kollar et al., 2018; Yu et al., 2018; Gupta et al., 2018). Solutions range 8108 from chart-based constituency parsers (Berant et al., 2013) to more recent neural sequence-to-sequence models (Jia and Liang, 2016). The general challenge of scaling semantic parsing to DST is that dialog state, as an accumulation of conversation history, requires expensive context-dependent annotation. It is also unclear how utterance semantics can be aggregated and maintained in a structured way. In this work we provide a solution to unify DST with semantic parsing. Data Collection The most straightforward approach to building datasets for task-oriented dialog is to directly annotate human-system conversations (Williams et al., 2016). A limitation is that the approach requires a working system at hand, which causes a cl"
2020.emnlp-main.651,N18-3022,0,0.0189093,"ural classification models (Mrkˇsi´c et al., 2017; Mrkˇsi´c and Vuli´c, 2018) alleviate the problem by learning distributed representations of user utterances. However, they still lack scalability to large unbounded output space (Xu and Hu, 2018; Lee et al., 2019) and structured representations. To address the limitations, some recent work treats slot filling as a sequence generation task (Ren et al., 2019; Wu et al., 2019). On the other hand, single-turn semantic parsers have long used structured meaning representations to address compositionality (Liang et al., 2013; Banarescu et al., 2013; Kollar et al., 2018; Yu et al., 2018; Gupta et al., 2018). Solutions range 8108 from chart-based constituency parsers (Berant et al., 2013) to more recent neural sequence-to-sequence models (Jia and Liang, 2016). The general challenge of scaling semantic parsing to DST is that dialog state, as an accumulation of conversation history, requires expensive context-dependent annotation. It is also unclear how utterance semantics can be aggregated and maintained in a structured way. In this work we provide a solution to unify DST with semantic parsing. Data Collection The most straightforward approach to building data"
2020.emnlp-main.651,P19-1546,0,0.0106513,"ated Work Modeling Traditional DST models apply discriminative classifiers over the space of slot-value combinations (Crook and Lemon, 2010; Henderson et al., 2014b; Williams et al., 2016). These models require feature extraction from user utterances based on manually constructed semantic dictionaries, making them vulnerable to language variations. Neural classification models (Mrkˇsi´c et al., 2017; Mrkˇsi´c and Vuli´c, 2018) alleviate the problem by learning distributed representations of user utterances. However, they still lack scalability to large unbounded output space (Xu and Hu, 2018; Lee et al., 2019) and structured representations. To address the limitations, some recent work treats slot filling as a sequence generation task (Ren et al., 2019; Wu et al., 2019). On the other hand, single-turn semantic parsers have long used structured meaning representations to address compositionality (Liang et al., 2013; Banarescu et al., 2013; Kollar et al., 2018; Yu et al., 2018; Gupta et al., 2018). Solutions range 8108 from chart-based constituency parsers (Berant et al., 2013) to more recent neural sequence-to-sequence models (Jia and Liang, 2016). The general challenge of scaling semantic parsing t"
2020.emnlp-main.651,W16-3602,0,0.0434381,"Missing"
2020.emnlp-main.651,J13-2005,0,0.0301533,"g them vulnerable to language variations. Neural classification models (Mrkˇsi´c et al., 2017; Mrkˇsi´c and Vuli´c, 2018) alleviate the problem by learning distributed representations of user utterances. However, they still lack scalability to large unbounded output space (Xu and Hu, 2018; Lee et al., 2019) and structured representations. To address the limitations, some recent work treats slot filling as a sequence generation task (Ren et al., 2019; Wu et al., 2019). On the other hand, single-turn semantic parsers have long used structured meaning representations to address compositionality (Liang et al., 2013; Banarescu et al., 2013; Kollar et al., 2018; Yu et al., 2018; Gupta et al., 2018). Solutions range 8108 from chart-based constituency parsers (Berant et al., 2013) to more recent neural sequence-to-sequence models (Jia and Liang, 2016). The general challenge of scaling semantic parsing to DST is that dialog state, as an accumulation of conversation history, requires expensive context-dependent annotation. It is also unclear how utterance semantics can be aggregated and maintained in a structured way. In this work we provide a solution to unify DST with semantic parsing. Data Collection The m"
2020.emnlp-main.651,P17-1163,1,0.892484,"Missing"
2020.emnlp-main.651,P18-2018,0,0.0220161,"Missing"
2020.emnlp-main.651,D19-1460,0,0.0199216,"mantics can be aggregated and maintained in a structured way. In this work we provide a solution to unify DST with semantic parsing. Data Collection The most straightforward approach to building datasets for task-oriented dialog is to directly annotate human-system conversations (Williams et al., 2016). A limitation is that the approach requires a working system at hand, which causes a classic chicken-and-egg problem for improving user experience. The issue can be avoided with Wizard-of-Oz (WoZ) experiments to collect human-human conversations (El Asri et al., 2017; Budzianowski et al., 2018; Peskov et al., 2019; Byrne et al., 2019; Radlinski et al., 2019). However, dialog state annotation remains challenging and costly in WoZ, and the resulting distribution could be different from that of human-machine conversations (Budzianowski et al., 2018). One approach that avoids direct meaning annotation is to use a dialog simulator (Schatzmann et al., 2007; Li et al., 2016). Recently, Shah et al. (2018) and Rastogi et al. (2019) generate synthetic conversations which are subsequently paraphrased by crowdsourcing. This approach has been proven to provide a better coverage while reducing the error and cost of"
2020.emnlp-main.651,W19-5941,0,0.0272833,"in a structured way. In this work we provide a solution to unify DST with semantic parsing. Data Collection The most straightforward approach to building datasets for task-oriented dialog is to directly annotate human-system conversations (Williams et al., 2016). A limitation is that the approach requires a working system at hand, which causes a classic chicken-and-egg problem for improving user experience. The issue can be avoided with Wizard-of-Oz (WoZ) experiments to collect human-human conversations (El Asri et al., 2017; Budzianowski et al., 2018; Peskov et al., 2019; Byrne et al., 2019; Radlinski et al., 2019). However, dialog state annotation remains challenging and costly in WoZ, and the resulting distribution could be different from that of human-machine conversations (Budzianowski et al., 2018). One approach that avoids direct meaning annotation is to use a dialog simulator (Schatzmann et al., 2007; Li et al., 2016). Recently, Shah et al. (2018) and Rastogi et al. (2019) generate synthetic conversations which are subsequently paraphrased by crowdsourcing. This approach has been proven to provide a better coverage while reducing the error and cost of dialog state annotation (Rastogi et al., 2019"
2020.emnlp-main.651,D19-1196,0,0.0226953,"Missing"
2020.emnlp-main.651,N07-2038,0,0.291371,"requires a working system at hand, which causes a classic chicken-and-egg problem for improving user experience. The issue can be avoided with Wizard-of-Oz (WoZ) experiments to collect human-human conversations (El Asri et al., 2017; Budzianowski et al., 2018; Peskov et al., 2019; Byrne et al., 2019; Radlinski et al., 2019). However, dialog state annotation remains challenging and costly in WoZ, and the resulting distribution could be different from that of human-machine conversations (Budzianowski et al., 2018). One approach that avoids direct meaning annotation is to use a dialog simulator (Schatzmann et al., 2007; Li et al., 2016). Recently, Shah et al. (2018) and Rastogi et al. (2019) generate synthetic conversations which are subsequently paraphrased by crowdsourcing. This approach has been proven to provide a better coverage while reducing the error and cost of dialog state annotation (Rastogi et al., 2019). We adopt a similar approach in our work, but focusing on a structured meaning space. 3 3.1 Setup Problem Statement We use the following notions throughout the paper. A conversation X has representation Y grounded to an ontology K: at turn t, every user utterance xut is annotated with a user dia"
2020.emnlp-main.651,D19-1206,0,0.0129632,"darEvent.create . $newEvent Simulator The most common approach of simulating a conversation flow is agenda-based (Schatzmann et al., 2007; Li et al., 2016; Shah et al., 2018; Rastogi et al., 2019). At the beginning of this approach, a new goal is defined in the form of slot-value pairs describing user’s requests and constraints; and an agenda is constructed by decomposing the user goal into a sequence of user actions. Although the approach ensures the user behaves in a goal-oriented manner, it constrains the output space with pre-defined agendas, which is hard to craft for complex user goals (Shi et al., 2019). Arguably, a more natural solution to dialog simulation for complex output space is a fully generative method. It complies with the behavior that a real user may only have an initial goal at the start of conversation, while the final dialog state cannot be foreseen in advance. The whole conversation can be defined generatively as follows: P (Y ) = P (y0u ) n X t=0 P (yts |ytu ) n X s u P (ytu |y<t , y<t ) t=1 (1) $newEvent → object.equals . $attendees . $location This example generates a calendar event creation intent that contains two slots of attendees and location. In a statistical approac"
2020.emnlp-main.651,E17-1042,0,0.028351,"Missing"
2020.emnlp-main.651,P19-1078,0,0.171086,"; Williams et al., 2016). These models require feature extraction from user utterances based on manually constructed semantic dictionaries, making them vulnerable to language variations. Neural classification models (Mrkˇsi´c et al., 2017; Mrkˇsi´c and Vuli´c, 2018) alleviate the problem by learning distributed representations of user utterances. However, they still lack scalability to large unbounded output space (Xu and Hu, 2018; Lee et al., 2019) and structured representations. To address the limitations, some recent work treats slot filling as a sequence generation task (Ren et al., 2019; Wu et al., 2019). On the other hand, single-turn semantic parsers have long used structured meaning representations to address compositionality (Liang et al., 2013; Banarescu et al., 2013; Kollar et al., 2018; Yu et al., 2018; Gupta et al., 2018). Solutions range 8108 from chart-based constituency parsers (Berant et al., 2013) to more recent neural sequence-to-sequence models (Jia and Liang, 2016). The general challenge of scaling semantic parsing to DST is that dialog state, as an accumulation of conversation history, requires expensive context-dependent annotation. It is also unclear how utterance semantics"
2020.emnlp-main.651,P18-1134,0,0.0112164,"an annotators Related Work Modeling Traditional DST models apply discriminative classifiers over the space of slot-value combinations (Crook and Lemon, 2010; Henderson et al., 2014b; Williams et al., 2016). These models require feature extraction from user utterances based on manually constructed semantic dictionaries, making them vulnerable to language variations. Neural classification models (Mrkˇsi´c et al., 2017; Mrkˇsi´c and Vuli´c, 2018) alleviate the problem by learning distributed representations of user utterances. However, they still lack scalability to large unbounded output space (Xu and Hu, 2018; Lee et al., 2019) and structured representations. To address the limitations, some recent work treats slot filling as a sequence generation task (Ren et al., 2019; Wu et al., 2019). On the other hand, single-turn semantic parsers have long used structured meaning representations to address compositionality (Liang et al., 2013; Banarescu et al., 2013; Kollar et al., 2018; Yu et al., 2018; Gupta et al., 2018). Solutions range 8108 from chart-based constituency parsers (Berant et al., 2013) to more recent neural sequence-to-sequence models (Jia and Liang, 2016). The general challenge of scaling"
2020.emnlp-main.651,D18-1425,0,0.0221038,"odels (Mrkˇsi´c et al., 2017; Mrkˇsi´c and Vuli´c, 2018) alleviate the problem by learning distributed representations of user utterances. However, they still lack scalability to large unbounded output space (Xu and Hu, 2018; Lee et al., 2019) and structured representations. To address the limitations, some recent work treats slot filling as a sequence generation task (Ren et al., 2019; Wu et al., 2019). On the other hand, single-turn semantic parsers have long used structured meaning representations to address compositionality (Liang et al., 2013; Banarescu et al., 2013; Kollar et al., 2018; Yu et al., 2018; Gupta et al., 2018). Solutions range 8108 from chart-based constituency parsers (Berant et al., 2013) to more recent neural sequence-to-sequence models (Jia and Liang, 2016). The general challenge of scaling semantic parsing to DST is that dialog state, as an accumulation of conversation history, requires expensive context-dependent annotation. It is also unclear how utterance semantics can be aggregated and maintained in a structured way. In this work we provide a solution to unify DST with semantic parsing. Data Collection The most straightforward approach to building datasets for task-ori"
alonso-etal-2012-voting,W99-0512,0,\N,Missing
alonso-etal-2012-voting,markert-nissim-2002-towards,0,\N,Missing
alonso-etal-2012-voting,bel-etal-2000-simple,1,\N,Missing
alonso-etal-2012-voting,N01-1010,0,\N,Missing
alonso-etal-2012-voting,N07-2002,1,\N,Missing
alonso-etal-2012-voting,E09-1005,0,\N,Missing
alonso-etal-2012-voting,E09-1045,0,\N,Missing
alonso-etal-2012-voting,W04-1908,0,\N,Missing
alonso-etal-2012-voting,C04-1133,0,\N,Missing
alonso-etal-2012-voting,W09-3716,0,\N,Missing
alonso-etal-2012-voting,J03-2004,0,\N,Missing
alonso-etal-2012-voting,H92-1045,0,\N,Missing
alonso-etal-2012-voting,S10-1005,0,\N,Missing
alonso-etal-2012-voting,pustejovsky-etal-2006-towards,0,\N,Missing
alonso-etal-2012-voting,jezek-quochi-2010-capturing,0,\N,Missing
alonso-romeo-2014-crowdsourcing,markert-nissim-2002-towards,0,\N,Missing
alonso-romeo-2014-crowdsourcing,W11-1304,0,\N,Missing
alonso-romeo-2014-crowdsourcing,D08-1027,0,\N,Missing
alonso-romeo-2014-crowdsourcing,J03-2004,0,\N,Missing
alonso-romeo-2014-crowdsourcing,P13-1053,0,\N,Missing
alonso-romeo-2014-crowdsourcing,P93-1012,0,\N,Missing
alonso-romeo-2014-crowdsourcing,J08-4004,0,\N,Missing
alonso-romeo-2014-crowdsourcing,D09-1030,0,\N,Missing
alonso-romeo-2014-crowdsourcing,N13-1062,0,\N,Missing
alonso-romeo-2014-crowdsourcing,P13-2127,1,\N,Missing
alonso-romeo-2014-crowdsourcing,N13-1132,0,\N,Missing
alonso-romeo-2014-crowdsourcing,W13-2323,0,\N,Missing
alonso-romeo-2014-crowdsourcing,W13-2305,0,\N,Missing
D15-1245,acs-2014-pivot,0,0.0535652,"Missing"
D15-1245,P14-1136,0,0.113418,"Missing"
D15-1245,W07-2416,0,0.0467962,"Missing"
D15-1245,D07-1002,0,0.0208932,"e-semantic parsing is the task of automatically finding semantically salient targets in text, disambiguating the targets by assigning a sense (frame) to them, identifying their arguments, and labeling these arguments with appropriate roles. The F RAME N ET 1.5 lexicon1 provides a fixed repository of semantic frames and roles, which we use in the experiments below. Several learning and parsing algorithms have been developed for frame-semantic analysis (Johansson and Nugues, 2007; Das et al., 2014; Täckström et al., 2015), and frame semantics has been successfully applied to question-answering (Shen and Lapata, 2007), information extraction (Surdeanu et al., 2003) and knowledge extraction (Søgaard et al., 2015b). 1 https://framenet.icsi.berkeley.edu/ Contributions This paper makes the following three contributions. We present a new multilingual frame-annotated corpus covering five topics, two domains (Wikipedia and Twitter), and nine languages. We implement a simplified version of the frame-semantic parser introduced in Das et al. (2014). Finally, we show how to modify this parser to learn any-language frame-semantic parsing models using inter-lingual word embeddings (Søgaard et al., 2015a). 2 Data annota"
D15-1245,P03-1002,0,0.0606614,"finding semantically salient targets in text, disambiguating the targets by assigning a sense (frame) to them, identifying their arguments, and labeling these arguments with appropriate roles. The F RAME N ET 1.5 lexicon1 provides a fixed repository of semantic frames and roles, which we use in the experiments below. Several learning and parsing algorithms have been developed for frame-semantic analysis (Johansson and Nugues, 2007; Das et al., 2014; Täckström et al., 2015), and frame semantics has been successfully applied to question-answering (Shen and Lapata, 2007), information extraction (Surdeanu et al., 2003) and knowledge extraction (Søgaard et al., 2015b). 1 https://framenet.icsi.berkeley.edu/ Contributions This paper makes the following three contributions. We present a new multilingual frame-annotated corpus covering five topics, two domains (Wikipedia and Twitter), and nine languages. We implement a simplified version of the frame-semantic parser introduced in Das et al. (2014). Finally, we show how to modify this parser to learn any-language frame-semantic parsing models using inter-lingual word embeddings (Søgaard et al., 2015a). 2 Data annotation Figure 1 depicts a F RAME N ET 1.5 frame-se"
D15-1245,Q15-1003,0,0.0331545,"Missing"
D15-1245,P12-1068,0,0.0523384,"Missing"
D15-1245,J14-1002,0,\N,Missing
D15-1245,P15-1165,1,\N,Missing
E17-1005,P98-1013,0,0.0583186,"), the size of the label inventory counting Blabels and I-labels as different (|Y |), and the proportion of out-of-span labels, which we refer to as O labels. The table also provides some of the information-theoretical measures we describe in Section 2.4. Note that D EP R ELS and POS are the only datasets without any O labels, while F RAMES and S EM T RAITS are the two tasks with O labels but no B/I-span notation, as tokens are annotated individually. Main tasks We use the following main tasks, aimed to represent a variety of semantic sequence labeling tasks. F RAMES: We use the FrameNet 1.5 (Baker et al., 1998) annotated corpus for a joint frame detection and frame identification tasks where a word can receive a predicate label like Arson or Personal success. We use the data splits from (Das et al., 2014; Hermann et al., 2014). While frame identification is normally treated as single classification, we keep the sequence-prediction paradigm so all main tasks rely on the same architecture. S UPERSENSES: We use the supersense version of SemCor (Miller et al., 1993) from (Ciaramita and Altun, 2006), with coarse-grained semantic labels like noun.person or verb.change. NER: The CONLL2003 shared-task data"
E17-1005,D15-1041,0,0.00538846,"rk is trained jointly such that the hidden representation captures the important information from the sequence for the prediction task. A bi-directional recurrent neural network (Graves and Schmidhuber, 2005) is an extension of an RNN that reads the input sequence twice, from left to right and right to left, and the encodings are concatenated. An LSTM (Long Short-Term Memory) is an extension of an RNN with more stable gradients (Hochreiter and Schmidhuber, 1997). Bi-LSTM have recently successfully been used for a variety of tasks (Collobert et al., 2011; Huang et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Liu et al., 2015; Plank et al., 2016). For further details, cf. Goldberg (2015) and Cho (2015). We use an off-the-shelf bidirectional LSTM model (Plank et al., 2016).2 The model is illustrated in Figure 1. It is a context bi-LSTM taking as input word embeddings w. ~ Character embeddings ~c are incorporated via a hierarchical biLSTM using a sequence bi-LSTM at the lower level (Ballesteros et al., 2015; Plank et al., 2016). The character representation is concatenated with the (learned) word embeddings w ~ to form the input to the context bi-LSTM at the upper la"
E17-1005,C16-1333,1,0.672525,"e experiment with different data sources to control for label inventory size and corpus source for the auxiliary task. Introduction The recent success of recurrent neural networks (RNNs) for sequence prediction has raised a great deal of interest, which has lead researchers to propose competing architectures for several language-processing tasks. These architectures often rely on multitask learning (Caruana, 1997). Multitask learning (MTL) has been applied with success to a variety of sequence-prediction tasks including chunking and tagging (Collobert et al., 2011; Søgaard and Goldberg, 2016; Bjerva et al., 2016; Plank, 2016), name error detection (Cheng et al., 2015) and machine translation (Luong et al., 2016). However, little is known about MTL for tasks which are more semantic in nature, i.e., tasks that aim at labeling some aspect of the meaning of words (Cruse, 1986), instead their morphosyntactic behavior. In fact, results on semantic tasks are either mixed (Collobert et al., 2011) or, due to the file drawer bias (Rosenthal, 1979), simply not reported. There is no prior study—to From our empirical study we observe the MTL architecture’s sensitivity to label distribution properties, and its pre"
E17-1005,C16-1179,1,0.0804739,"or N ER. BL (w + c) nition (Cheng et al., 2015), tagging and chunking (Collobert et al., 2011; Plank et al., 2016), entity and relation extraction (Gupta et al., 2016), machine translation (Luong et al., 2016) and machine translation quality estimation including modeling annotator bias (Cohn and Specia, 2013; Shah and Specia, 2016). Most earlier work had in common that it assumed jointly labeled data (same corpus annotated with multiple labels). In contrast, in this paper we evaluate multitask training from distinct sources to address data paucity, like done recently (Kshirsagar et al., 2015; Braud et al., 2016; Plank, 2016). Sutton et al. (2007) demonstrate improvements for POS tagging by training a joint CRF model for both POS tagging and noun-phrase chunking. However, it is not clear under what conditions multi-task learning works. In fact, Collobert et al. (2011) train a joint feedforward neural network for POS, chunks and NER, and observe only improvements in chunking (similar to our findings, cf. Section 4.2), however, did not investigate data properties of these tasks. To the best of our knowledge, this is the first extensive evaluation of the effect of data properties and main-auxiliary task"
E17-1005,W06-1670,0,0.0684416,"g main tasks, aimed to represent a variety of semantic sequence labeling tasks. F RAMES: We use the FrameNet 1.5 (Baker et al., 1998) annotated corpus for a joint frame detection and frame identification tasks where a word can receive a predicate label like Arson or Personal success. We use the data splits from (Das et al., 2014; Hermann et al., 2014). While frame identification is normally treated as single classification, we keep the sequence-prediction paradigm so all main tasks rely on the same architecture. S UPERSENSES: We use the supersense version of SemCor (Miller et al., 1993) from (Ciaramita and Altun, 2006), with coarse-grained semantic labels like noun.person or verb.change. NER: The CONLL2003 shared-task data for named entity recognition for labels Person, Loc, etc. (Tjong Kim Sang and De Meulder, 2003). S EM T RAITS: We have used the EurWordNet list of ontological types for senses (Vossen et al., 1998) to convert the S UPERSENSES into coarser semantic traits like Animate or UnboundedEvent.1 MPQA: The Multi-Perspective Question Answering (MPQA) corpus (Deng and Wiebe, 2015), which contains sentiment information among others. We use the annotation corresponding to the 2.4 Information-theoretic"
E17-1005,P13-1004,0,0.0137168,"ngs are informative for N ER, because they approximate the well-known capitalization features in traditional models. Character features are not informative for tasks that are more dependent on word identity (like F RAMES), but are indeed useful for tasks where parts of the word can be informative, such as P OS or N ER. BL (w + c) nition (Cheng et al., 2015), tagging and chunking (Collobert et al., 2011; Plank et al., 2016), entity and relation extraction (Gupta et al., 2016), machine translation (Luong et al., 2016) and machine translation quality estimation including modeling annotator bias (Cohn and Specia, 2013; Shah and Specia, 2016). Most earlier work had in common that it assumed jointly labeled data (same corpus annotated with multiple labels). In contrast, in this paper we evaluate multitask training from distinct sources to address data paucity, like done recently (Kshirsagar et al., 2015; Braud et al., 2016; Plank, 2016). Sutton et al. (2007) demonstrate improvements for POS tagging by training a joint CRF model for both POS tagging and noun-phrase chunking. However, it is not clear under what conditions multi-task learning works. In fact, Collobert et al. (2011) train a joint feedforward neu"
E17-1005,P16-1101,0,0.0120729,"ine). All reported systems degrade around 0.50 points with regards to the baseline, except S UPERSENSES which improves slightly form 96.27 to 96.44. The high precision obtained for the also very difficult F RAMES tasks suggests that this architecture, while not suitable for frame disambiguation, can be used for frame-target identification. Disregarding F REQ B IN, the only low-level tasks that seems to aid prediction is POS. An interesting observation from the BIO task analysis is that while the standard bi-LSTM model used here does not have a Viterbi-style decoding like more complex systems (Ma and Hovy, 2016; Lample et al., 2016), we have found very few invalid BIO sequences. For N ER, there are only ten I-labels after an O-label, out of the 27K predicted by the bi-LSTM. For S UPERSENSES there are 59, out of 1,5K predicted I-labels. The amount of invalid predicted sequences is lower than expected, indicating that an additional decoding layer plays a smaller role in prediction quality than label distribution and corpus size, e.g. N ER is a large dataset with few labels, and the system has little difficulty in learning label precedences. For larger label sets or smaller data sizes, Results This sec"
E17-1005,J93-2004,0,0.0912746,"Missing"
E17-1005,N15-1146,0,0.00520123,"sks rely on the same architecture. S UPERSENSES: We use the supersense version of SemCor (Miller et al., 1993) from (Ciaramita and Altun, 2006), with coarse-grained semantic labels like noun.person or verb.change. NER: The CONLL2003 shared-task data for named entity recognition for labels Person, Loc, etc. (Tjong Kim Sang and De Meulder, 2003). S EM T RAITS: We have used the EurWordNet list of ontological types for senses (Vossen et al., 1998) to convert the S UPERSENSES into coarser semantic traits like Animate or UnboundedEvent.1 MPQA: The Multi-Perspective Question Answering (MPQA) corpus (Deng and Wiebe, 2015), which contains sentiment information among others. We use the annotation corresponding to the 2.4 Information-theoretic measures In order to quantify the properties of the different label distributions, we calculate three informationtheoretical quantities based on two metrics, kurtosis and entropy. Entropy is the best-known informationtheoretical metric. It indicates the amount of uncertainty in a distribution. We calculate two variants of entropy, one taking all labels in consideration H(Yf ull ), and another one H(Y−O ) where we discard the O label and only measure the entropy for the name"
E17-1005,H93-1061,0,0.117633,"n tasks We use the following main tasks, aimed to represent a variety of semantic sequence labeling tasks. F RAMES: We use the FrameNet 1.5 (Baker et al., 1998) annotated corpus for a joint frame detection and frame identification tasks where a word can receive a predicate label like Arson or Personal success. We use the data splits from (Das et al., 2014; Hermann et al., 2014). While frame identification is normally treated as single classification, we keep the sequence-prediction paradigm so all main tasks rely on the same architecture. S UPERSENSES: We use the supersense version of SemCor (Miller et al., 1993) from (Ciaramita and Altun, 2006), with coarse-grained semantic labels like noun.person or verb.change. NER: The CONLL2003 shared-task data for named entity recognition for labels Person, Loc, etc. (Tjong Kim Sang and De Meulder, 2003). S EM T RAITS: We have used the EurWordNet list of ontological types for senses (Vossen et al., 1998) to convert the S UPERSENSES into coarser semantic traits like Animate or UnboundedEvent.1 MPQA: The Multi-Perspective Question Answering (MPQA) corpus (Deng and Wiebe, 2015), which contains sentiment information among others. We use the annotation corresponding"
E17-1005,P15-1033,0,0.0135916,"s. The entire network is trained jointly such that the hidden representation captures the important information from the sequence for the prediction task. A bi-directional recurrent neural network (Graves and Schmidhuber, 2005) is an extension of an RNN that reads the input sequence twice, from left to right and right to left, and the encodings are concatenated. An LSTM (Long Short-Term Memory) is an extension of an RNN with more stable gradients (Hochreiter and Schmidhuber, 1997). Bi-LSTM have recently successfully been used for a variety of tasks (Collobert et al., 2011; Huang et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Liu et al., 2015; Plank et al., 2016). For further details, cf. Goldberg (2015) and Cho (2015). We use an off-the-shelf bidirectional LSTM model (Plank et al., 2016).2 The model is illustrated in Figure 1. It is a context bi-LSTM taking as input word embeddings w. ~ Character embeddings ~c are incorporated via a hierarchical biLSTM using a sequence bi-LSTM at the lower level (Ballesteros et al., 2015; Plank et al., 2016). The character representation is concatenated with the (learned) word embeddings w ~ to form the input to the conte"
E17-1005,P16-2067,1,0.929504,", we take the index of the k-quantilized cumulative frequency for a word w. We use this parametric version of F REQ B IN with the median number of labels produced by the previous variants to examine the importance of the label distribution being skewed. For k=5, this variant maximizes the entropy of a F REQ B IN five-label distribution. Note that this method still places all hapaxes and outof-vocabulary words of the test data in the same frequency bin. F REQ B IN variants Recently, a simple auxiliary task has been proposed with success for POS tagging: predicting the log frequency of a token (Plank et al., 2016). The intuition behind this model is that the auxiliary loss, predicting word frequency, helps differentiate rare and common words, thus providing better predictions for frequency-sensitive labels. They refer to this auxiliary task as F REQ B IN, however, focus on POS only. Plank et al. (2016) used the discretized log frequency of the current word to build the F REQ B IN auxiliary task to aid POS Even though we could have used a reference corpus to have the same F REQ B IN for all the data, we prefer to use the main-task corpus for F RE Q B IN . Using an external corpus would otherwise lead to"
E17-1005,C16-1059,1,0.936533,"ferent data sources to control for label inventory size and corpus source for the auxiliary task. Introduction The recent success of recurrent neural networks (RNNs) for sequence prediction has raised a great deal of interest, which has lead researchers to propose competing architectures for several language-processing tasks. These architectures often rely on multitask learning (Caruana, 1997). Multitask learning (MTL) has been applied with success to a variety of sequence-prediction tasks including chunking and tagging (Collobert et al., 2011; Søgaard and Goldberg, 2016; Bjerva et al., 2016; Plank, 2016), name error detection (Cheng et al., 2015) and machine translation (Luong et al., 2016). However, little is known about MTL for tasks which are more semantic in nature, i.e., tasks that aim at labeling some aspect of the meaning of words (Cruse, 1986), instead their morphosyntactic behavior. In fact, results on semantic tasks are either mixed (Collobert et al., 2011) or, due to the file drawer bias (Rosenthal, 1979), simply not reported. There is no prior study—to From our empirical study we observe the MTL architecture’s sensitivity to label distribution properties, and its preference for co"
E17-1005,C16-1239,0,0.00762229,"about 2.5 points, namely M PQA and F RAMES. For the other two tasks we observe drops up to a maximum of 8-points for N ER. Character embeddings are informative for N ER, because they approximate the well-known capitalization features in traditional models. Character features are not informative for tasks that are more dependent on word identity (like F RAMES), but are indeed useful for tasks where parts of the word can be informative, such as P OS or N ER. BL (w + c) nition (Cheng et al., 2015), tagging and chunking (Collobert et al., 2011; Plank et al., 2016), entity and relation extraction (Gupta et al., 2016), machine translation (Luong et al., 2016) and machine translation quality estimation including modeling annotator bias (Cohn and Specia, 2013; Shah and Specia, 2016). Most earlier work had in common that it assumed jointly labeled data (same corpus annotated with multiple labels). In contrast, in this paper we evaluate multitask training from distinct sources to address data paucity, like done recently (Kshirsagar et al., 2015; Braud et al., 2016; Plank, 2016). Sutton et al. (2007) demonstrate improvements for POS tagging by training a joint CRF model for both POS tagging and noun-phrase chun"
E17-1005,P14-1136,0,0.00482107,"measures we describe in Section 2.4. Note that D EP R ELS and POS are the only datasets without any O labels, while F RAMES and S EM T RAITS are the two tasks with O labels but no B/I-span notation, as tokens are annotated individually. Main tasks We use the following main tasks, aimed to represent a variety of semantic sequence labeling tasks. F RAMES: We use the FrameNet 1.5 (Baker et al., 1998) annotated corpus for a joint frame detection and frame identification tasks where a word can receive a predicate label like Arson or Personal success. We use the data splits from (Das et al., 2014; Hermann et al., 2014). While frame identification is normally treated as single classification, we keep the sequence-prediction paradigm so all main tasks rely on the same architecture. S UPERSENSES: We use the supersense version of SemCor (Miller et al., 1993) from (Ciaramita and Altun, 2006), with coarse-grained semantic labels like noun.person or verb.change. NER: The CONLL2003 shared-task data for named entity recognition for labels Person, Loc, etc. (Tjong Kim Sang and De Meulder, 2003). S EM T RAITS: We have used the EurWordNet list of ontological types for senses (Vossen et al., 1998) to convert the S UPERS"
E17-1005,N16-1069,0,0.0210238,"N ER, because they approximate the well-known capitalization features in traditional models. Character features are not informative for tasks that are more dependent on word identity (like F RAMES), but are indeed useful for tasks where parts of the word can be informative, such as P OS or N ER. BL (w + c) nition (Cheng et al., 2015), tagging and chunking (Collobert et al., 2011; Plank et al., 2016), entity and relation extraction (Gupta et al., 2016), machine translation (Luong et al., 2016) and machine translation quality estimation including modeling annotator bias (Cohn and Specia, 2013; Shah and Specia, 2016). Most earlier work had in common that it assumed jointly labeled data (same corpus annotated with multiple labels). In contrast, in this paper we evaluate multitask training from distinct sources to address data paucity, like done recently (Kshirsagar et al., 2015; Braud et al., 2016; Plank, 2016). Sutton et al. (2007) demonstrate improvements for POS tagging by training a joint CRF model for both POS tagging and noun-phrase chunking. However, it is not clear under what conditions multi-task learning works. In fact, Collobert et al. (2011) train a joint feedforward neural network for POS, chu"
E17-1005,P16-2038,0,0.625312,"ibution, and iii) for P OS we experiment with different data sources to control for label inventory size and corpus source for the auxiliary task. Introduction The recent success of recurrent neural networks (RNNs) for sequence prediction has raised a great deal of interest, which has lead researchers to propose competing architectures for several language-processing tasks. These architectures often rely on multitask learning (Caruana, 1997). Multitask learning (MTL) has been applied with success to a variety of sequence-prediction tasks including chunking and tagging (Collobert et al., 2011; Søgaard and Goldberg, 2016; Bjerva et al., 2016; Plank, 2016), name error detection (Cheng et al., 2015) and machine translation (Luong et al., 2016). However, little is known about MTL for tasks which are more semantic in nature, i.e., tasks that aim at labeling some aspect of the meaning of words (Cruse, 1986), instead their morphosyntactic behavior. In fact, results on semantic tasks are either mixed (Collobert et al., 2011) or, due to the file drawer bias (Rosenthal, 1979), simply not reported. There is no prior study—to From our empirical study we observe the MTL architecture’s sensitivity to label distribution pr"
E17-1005,Q16-1023,0,0.00584593,"that the hidden representation captures the important information from the sequence for the prediction task. A bi-directional recurrent neural network (Graves and Schmidhuber, 2005) is an extension of an RNN that reads the input sequence twice, from left to right and right to left, and the encodings are concatenated. An LSTM (Long Short-Term Memory) is an extension of an RNN with more stable gradients (Hochreiter and Schmidhuber, 1997). Bi-LSTM have recently successfully been used for a variety of tasks (Collobert et al., 2011; Huang et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Liu et al., 2015; Plank et al., 2016). For further details, cf. Goldberg (2015) and Cho (2015). We use an off-the-shelf bidirectional LSTM model (Plank et al., 2016).2 The model is illustrated in Figure 1. It is a context bi-LSTM taking as input word embeddings w. ~ Character embeddings ~c are incorporated via a hierarchical biLSTM using a sequence bi-LSTM at the lower level (Ballesteros et al., 2015; Plank et al., 2016). The character representation is concatenated with the (learned) word embeddings w ~ to form the input to the context bi-LSTM at the upper layers. For hyperparameter setting"
E17-1005,P15-2036,0,0.0197467,"nformative, such as P OS or N ER. BL (w + c) nition (Cheng et al., 2015), tagging and chunking (Collobert et al., 2011; Plank et al., 2016), entity and relation extraction (Gupta et al., 2016), machine translation (Luong et al., 2016) and machine translation quality estimation including modeling annotator bias (Cohn and Specia, 2013; Shah and Specia, 2016). Most earlier work had in common that it assumed jointly labeled data (same corpus annotated with multiple labels). In contrast, in this paper we evaluate multitask training from distinct sources to address data paucity, like done recently (Kshirsagar et al., 2015; Braud et al., 2016; Plank, 2016). Sutton et al. (2007) demonstrate improvements for POS tagging by training a joint CRF model for both POS tagging and noun-phrase chunking. However, it is not clear under what conditions multi-task learning works. In fact, Collobert et al. (2011) train a joint feedforward neural network for POS, chunks and NER, and observe only improvements in chunking (similar to our findings, cf. Section 4.2), however, did not investigate data properties of these tasks. To the best of our knowledge, this is the first extensive evaluation of the effect of data properties and"
E17-1005,N16-1030,0,0.00432161,"systems degrade around 0.50 points with regards to the baseline, except S UPERSENSES which improves slightly form 96.27 to 96.44. The high precision obtained for the also very difficult F RAMES tasks suggests that this architecture, while not suitable for frame disambiguation, can be used for frame-target identification. Disregarding F REQ B IN, the only low-level tasks that seems to aid prediction is POS. An interesting observation from the BIO task analysis is that while the standard bi-LSTM model used here does not have a Viterbi-style decoding like more complex systems (Ma and Hovy, 2016; Lample et al., 2016), we have found very few invalid BIO sequences. For N ER, there are only ten I-labels after an O-label, out of the 27K predicted by the bi-LSTM. For S UPERSENSES there are 59, out of 1,5K predicted I-labels. The amount of invalid predicted sequences is lower than expected, indicating that an additional decoding layer plays a smaller role in prediction quality than label distribution and corpus size, e.g. N ER is a large dataset with few labels, and the system has little difficulty in learning label precedences. For larger label sets or smaller data sizes, Results This section describes the res"
E17-1005,D15-1168,0,0.00719371,"captures the important information from the sequence for the prediction task. A bi-directional recurrent neural network (Graves and Schmidhuber, 2005) is an extension of an RNN that reads the input sequence twice, from left to right and right to left, and the encodings are concatenated. An LSTM (Long Short-Term Memory) is an extension of an RNN with more stable gradients (Hochreiter and Schmidhuber, 1997). Bi-LSTM have recently successfully been used for a variety of tasks (Collobert et al., 2011; Huang et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Liu et al., 2015; Plank et al., 2016). For further details, cf. Goldberg (2015) and Cho (2015). We use an off-the-shelf bidirectional LSTM model (Plank et al., 2016).2 The model is illustrated in Figure 1. It is a context bi-LSTM taking as input word embeddings w. ~ Character embeddings ~c are incorporated via a hierarchical biLSTM using a sequence bi-LSTM at the lower level (Ballesteros et al., 2015; Plank et al., 2016). The character representation is concatenated with the (learned) word embeddings w ~ to form the input to the context bi-LSTM at the upper layers. For hyperparameter settings, see Section 3.1"
E17-1022,W15-5301,1,0.894616,"Missing"
E17-1022,P15-2044,1,0.884906,"Missing"
E17-1022,W09-0106,0,0.0296839,"t such a formalism lends itself more naturally to a simple and linguistically sound rulebased approach to cross-lingual parsing. In this paper we present such an approach. Our system is a dependency parser that requires no training, and relies solely on explicit part-ofspeech (POS) constraints that UD imposes. In particular, UD prescribes that trees are single-rooted, and that function words like adpositions, auxiliaries, and determiners are always dependents of content words, while other formalisms might treat them as heads (De Marneffe et al., 2014). We ascribe our work to the viewpoints of Bender (2009) about the incorporation of linguistic knowledge in language-independent systems. We propose UDP, the first training-free parser for Universal Dependencies (UD). Our algorithm is based on PageRank and a small set of head attachment rules. It features two-step decoding to guarantee that function words are attached as leaf nodes. The parser requires no training, and it is competitive with a delexicalized transfer system. UDP offers a linguistically sound unsupervised alternative to cross-lingual parsing for UD, which can be used as a baseline for such systems. The parser has very few parameters"
E17-1022,A00-1031,0,0.0296335,"ord forms. If there is more than one treebank per language, we use the treebank that has the 5.2 The resulting trees always pass the validation script in github.com/UniversalDependencies/tools. They also had a special connection to some extremists They - also had - • • • • a special connection - some extremists • • - to • • • • • - - Evaluation setup Our system relies solely on POS tags. To estimate the quality degradation of our system under non-gold POS scenarios, we evaluate UDP on two alternative scenarios. The first is predicted POS (UDPP ), where we tag the respective test set with TnT (Brants, 2000) trained on each language’s training set. The second is a naive typeconstrained two-POS tag scenario (UDPN ), and approximates a lower bound. We give each word either CONTENT or FUNCTION tag, depending on the word’s frequency. The 100 most frequent words of the input test section receive the FUNC TION tag. 4 −→ Baseline • • • • • - Table 4: Matrix representation of the directed graph for the words in the sentence. 234 Finally, we compare our parser UDP to a supervised cross-lingual system (MSD). It is a multisource delexicalized transfer parser, referred to as multi-dir in the original paper b"
E17-1022,P11-1061,0,0.0305356,"ing, 2004; Spitkovsky et al., 2010a; Spitkovsky et al., 2010b). Still, the performance of these parsers falls far behind the approaches involving any sort of supervision. Our work builds on the line of research on ruleaided unsupervised dependency parsing by Gillenwater et al. (2010) and Naseem et al. (2010), and also relates to Søgaard’s (2012a; 2012b) work. Our parser, however, features two key differences: Related work Cross-lingual learning Recent years have seen exciting developments in cross-lingual linguistic structure prediction based on transfer or projection of POS and dependencies (Das and Petrov, 2011; McDonald et al., 2011). These works mainly use supervised learning and domain adaptation techniques for the target language. The first group of approaches deals with annotation projection (Yarowsky et al., 2001), whereby parallel corpora are used to transfer annotations between resource-rich source languages and lowresource target languages. Projection relies on the availability and quality of parallel corpora, sourceside taggers and parsers, but also tokenizers, sentence aligners, and word aligners for sources and targets. Hwa et al. (2005) were the first to project syntactic dependencies,"
E17-1022,de-marneffe-etal-2014-universal,0,0.0875731,"Missing"
E17-1022,W12-1909,0,0.0167797,"head attachment rules. It features two-step decoding to guarantee that function words are attached as leaf nodes. The parser requires no training, and it is competitive with a delexicalized transfer system. UDP offers a linguistically sound unsupervised alternative to cross-lingual parsing for UD, which can be used as a baseline for such systems. The parser has very few parameters and is distinctly robust to domain change across languages. 1 Introduction Grammar induction and unsupervised dependency parsing are active fields of research in natural language processing (Klein and Manning, 2004; Gelling et al., 2012). However, many data-driven approaches struggle with learning relations that match the conventions of the test data, e.g., Klein and Manning reported the tendency of their DMV parser to make determiners the heads of German nouns, which would not be an error if the test data used a DP analysis (Abney, 1987). Even supervised transfer approaches (McDonald et al., 2011) suffer from target adaptation problems when facing word order differences. The Universal Dependencies (UD) project (Nivre et al., 2015; Nivre et al., 2016) offers a dependency formalism that aims at providing a consistent represent"
E17-1022,P10-2036,0,0.0744508,"roach that takes a fresh angle on both aspects. Specifically, we propose a parser that i) requires no training data, and in contrast ii) critically relies on exploiting the UD constraints. These two characteristics make our parser unsupervised. Data-driven unsupervised dependency parsing is now a well-established discipline (Klein and Manning, 2004; Spitkovsky et al., 2010a; Spitkovsky et al., 2010b). Still, the performance of these parsers falls far behind the approaches involving any sort of supervision. Our work builds on the line of research on ruleaided unsupervised dependency parsing by Gillenwater et al. (2010) and Naseem et al. (2010), and also relates to Søgaard’s (2012a; 2012b) work. Our parser, however, features two key differences: Related work Cross-lingual learning Recent years have seen exciting developments in cross-lingual linguistic structure prediction based on transfer or projection of POS and dependencies (Das and Petrov, 2011; McDonald et al., 2011). These works mainly use supervised learning and domain adaptation techniques for the target language. The first group of approaches deals with annotation projection (Yarowsky et al., 2001), whereby parallel corpora are used to transfer ann"
E17-1022,P16-2091,1,0.781898,"Missing"
E17-1022,P06-1063,0,0.117258,"Missing"
E17-1022,P04-1061,0,0.494345,"eRank and a small set of head attachment rules. It features two-step decoding to guarantee that function words are attached as leaf nodes. The parser requires no training, and it is competitive with a delexicalized transfer system. UDP offers a linguistically sound unsupervised alternative to cross-lingual parsing for UD, which can be used as a baseline for such systems. The parser has very few parameters and is distinctly robust to domain change across languages. 1 Introduction Grammar induction and unsupervised dependency parsing are active fields of research in natural language processing (Klein and Manning, 2004; Gelling et al., 2012). However, many data-driven approaches struggle with learning relations that match the conventions of the test data, e.g., Klein and Manning reported the tendency of their DMV parser to make determiners the heads of German nouns, which would not be an error if the test data used a DP analysis (Abney, 1987). Even supervised transfer approaches (McDonald et al., 2011) suffer from target adaptation problems when facing word order differences. The Universal Dependencies (UD) project (Nivre et al., 2015; Nivre et al., 2016) offers a dependency formalism that aims at providing"
E17-1022,P14-1126,0,0.0366931,"et al., 2001), whereby parallel corpora are used to transfer annotations between resource-rich source languages and lowresource target languages. Projection relies on the availability and quality of parallel corpora, sourceside taggers and parsers, but also tokenizers, sentence aligners, and word aligners for sources and targets. Hwa et al. (2005) were the first to project syntactic dependencies, and Tiedemann et al. (2014; 2016) improved on their projection algorithm. Current state of the art in cross-lingual dependency parsing involves leveraging parallel corpora for annotation projection (Ma and Xia, 2014; Rasooli and Collins, 2015). The second group of approaches deals with transferring source parsing models to target languages. Zeman and Resnik (2008) were the first to introduce the idea of delexicalization: removing lexical features by training and cross-lingually applying parsers solely on POS sequences. Søgaard (2011) and McDonald et al. (2011) independently extended the approach by using multiple sources, requiring uniform POS and dependency representations (McDonald et al., 2013). Both model transfer and annotation projection rely on a large number of presumptions to derive their compet"
E17-1022,P13-2109,0,0.0770734,"Missing"
E17-1022,D11-1006,0,0.500127,"y few parameters and is distinctly robust to domain change across languages. 1 Introduction Grammar induction and unsupervised dependency parsing are active fields of research in natural language processing (Klein and Manning, 2004; Gelling et al., 2012). However, many data-driven approaches struggle with learning relations that match the conventions of the test data, e.g., Klein and Manning reported the tendency of their DMV parser to make determiners the heads of German nouns, which would not be an error if the test data used a DP analysis (Abney, 1987). Even supervised transfer approaches (McDonald et al., 2011) suffer from target adaptation problems when facing word order differences. The Universal Dependencies (UD) project (Nivre et al., 2015; Nivre et al., 2016) offers a dependency formalism that aims at providing a consistent representation across languages, while enforcing a few hard constraints. The arrival of such treebanks, expanded and improved on a regular basis, provides a new milestone for crosslingual dependency parsing research (McDonald et al., 2013). Contributions We introduce, to the best of our knowledge, the first unsupervised rule-based dependency parser for Universal Dependencies"
E17-1022,W10-2105,1,0.869888,"Missing"
E17-1022,D15-1039,0,0.0523361,"ereby parallel corpora are used to transfer annotations between resource-rich source languages and lowresource target languages. Projection relies on the availability and quality of parallel corpora, sourceside taggers and parsers, but also tokenizers, sentence aligners, and word aligners for sources and targets. Hwa et al. (2005) were the first to project syntactic dependencies, and Tiedemann et al. (2014; 2016) improved on their projection algorithm. Current state of the art in cross-lingual dependency parsing involves leveraging parallel corpora for annotation projection (Ma and Xia, 2014; Rasooli and Collins, 2015). The second group of approaches deals with transferring source parsing models to target languages. Zeman and Resnik (2008) were the first to introduce the idea of delexicalization: removing lexical features by training and cross-lingually applying parsers solely on POS sequences. Søgaard (2011) and McDonald et al. (2011) independently extended the approach by using multiple sources, requiring uniform POS and dependency representations (McDonald et al., 2013). Both model transfer and annotation projection rely on a large number of presumptions to derive their competitive parsing models. By and"
E17-1022,N10-1116,0,0.0314682,"hey typically do not exploit constraints placed on linguistic structures through a formalism, and they do so by design. With the emergence of UD as the practical standard for multilingual POS and syntactic dependency annotation, we argue for an approach that takes a fresh angle on both aspects. Specifically, we propose a parser that i) requires no training data, and in contrast ii) critically relies on exploiting the UD constraints. These two characteristics make our parser unsupervised. Data-driven unsupervised dependency parsing is now a well-established discipline (Klein and Manning, 2004; Spitkovsky et al., 2010a; Spitkovsky et al., 2010b). Still, the performance of these parsers falls far behind the approaches involving any sort of supervision. Our work builds on the line of research on ruleaided unsupervised dependency parsing by Gillenwater et al. (2010) and Naseem et al. (2010), and also relates to Søgaard’s (2012a; 2012b) work. Our parser, however, features two key differences: Related work Cross-lingual learning Recent years have seen exciting developments in cross-lingual linguistic structure prediction based on transfer or projection of POS and dependencies (Das and Petrov, 2011; McDonald et"
E17-1022,D10-1120,0,0.0928531,"on both aspects. Specifically, we propose a parser that i) requires no training data, and in contrast ii) critically relies on exploiting the UD constraints. These two characteristics make our parser unsupervised. Data-driven unsupervised dependency parsing is now a well-established discipline (Klein and Manning, 2004; Spitkovsky et al., 2010a; Spitkovsky et al., 2010b). Still, the performance of these parsers falls far behind the approaches involving any sort of supervision. Our work builds on the line of research on ruleaided unsupervised dependency parsing by Gillenwater et al. (2010) and Naseem et al. (2010), and also relates to Søgaard’s (2012a; 2012b) work. Our parser, however, features two key differences: Related work Cross-lingual learning Recent years have seen exciting developments in cross-lingual linguistic structure prediction based on transfer or projection of POS and dependencies (Das and Petrov, 2011; McDonald et al., 2011). These works mainly use supervised learning and domain adaptation techniques for the target language. The first group of approaches deals with annotation projection (Yarowsky et al., 2001), whereby parallel corpora are used to transfer annotations between resource"
E17-1022,W10-2902,0,0.029288,"hey typically do not exploit constraints placed on linguistic structures through a formalism, and they do so by design. With the emergence of UD as the practical standard for multilingual POS and syntactic dependency annotation, we argue for an approach that takes a fresh angle on both aspects. Specifically, we propose a parser that i) requires no training data, and in contrast ii) critically relies on exploiting the UD constraints. These two characteristics make our parser unsupervised. Data-driven unsupervised dependency parsing is now a well-established discipline (Klein and Manning, 2004; Spitkovsky et al., 2010a; Spitkovsky et al., 2010b). Still, the performance of these parsers falls far behind the approaches involving any sort of supervision. Our work builds on the line of research on ruleaided unsupervised dependency parsing by Gillenwater et al. (2010) and Naseem et al. (2010), and also relates to Søgaard’s (2012a; 2012b) work. Our parser, however, features two key differences: Related work Cross-lingual learning Recent years have seen exciting developments in cross-lingual linguistic structure prediction based on transfer or projection of POS and dependencies (Das and Petrov, 2011; McDonald et"
E17-1022,W12-1910,1,0.850332,"ed on the fly at runtime. We refer henceforth to our UD parser as UDP. 231 3.1 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: PageRank setup Our system uses the PageRank (PR) algorithm (Page et al., 1999) to estimate the relevance of the content words of a sentence. PR uses a random walk to estimate which nodes in the graph are more likely to be visited often, and thus, it gives higher rank to nodes with more incoming edges, as well as to nodes connected to those. Using PR to score word relevance requires an effective graphbuilding strategy. We have experimented with the strategies by Søgaard (2012b), such as words being connected to adjacent words, but our system fares best strictly using the dependency rules in Table 1 to build the graph. UD trees are often very flat, and a highly connected graph yields a PR distribution that is closer to uniform, thereby removing some of the difference of word relevance. We build a multigraph of all words in the sentence covered by the head-dependent rules in Table 1, giving each word an incoming edge for each eligible dependent, i.e., ADV depends on ADJ and VERB . This strategy does not always yield connected graphs, and we use a teleport probabilit"
E17-1022,C14-1175,0,0.0824867,"Missing"
E17-1022,H01-1035,0,0.0694204,"on ruleaided unsupervised dependency parsing by Gillenwater et al. (2010) and Naseem et al. (2010), and also relates to Søgaard’s (2012a; 2012b) work. Our parser, however, features two key differences: Related work Cross-lingual learning Recent years have seen exciting developments in cross-lingual linguistic structure prediction based on transfer or projection of POS and dependencies (Das and Petrov, 2011; McDonald et al., 2011). These works mainly use supervised learning and domain adaptation techniques for the target language. The first group of approaches deals with annotation projection (Yarowsky et al., 2001), whereby parallel corpora are used to transfer annotations between resource-rich source languages and lowresource target languages. Projection relies on the availability and quality of parallel corpora, sourceside taggers and parsers, but also tokenizers, sentence aligners, and word aligners for sources and targets. Hwa et al. (2005) were the first to project syntactic dependencies, and Tiedemann et al. (2014; 2016) improved on their projection algorithm. Current state of the art in cross-lingual dependency parsing involves leveraging parallel corpora for annotation projection (Ma and Xia, 20"
E17-1022,I08-3008,0,0.275845,"s. Projection relies on the availability and quality of parallel corpora, sourceside taggers and parsers, but also tokenizers, sentence aligners, and word aligners for sources and targets. Hwa et al. (2005) were the first to project syntactic dependencies, and Tiedemann et al. (2014; 2016) improved on their projection algorithm. Current state of the art in cross-lingual dependency parsing involves leveraging parallel corpora for annotation projection (Ma and Xia, 2014; Rasooli and Collins, 2015). The second group of approaches deals with transferring source parsing models to target languages. Zeman and Resnik (2008) were the first to introduce the idea of delexicalization: removing lexical features by training and cross-lingually applying parsers solely on POS sequences. Søgaard (2011) and McDonald et al. (2011) independently extended the approach by using multiple sources, requiring uniform POS and dependency representations (McDonald et al., 2013). Both model transfer and annotation projection rely on a large number of presumptions to derive their competitive parsing models. By and large, these presumptions are unrealistic and exclusive to a group of very closely related, resource-rich IndoEuropean lan"
E17-1022,Q16-1022,1,\N,Missing
K15-1033,E06-1040,0,0.0477065,"ng et al., 2013), we see human judgments as an important supplement to extrinsic evaluation. To the best of our knowledge, no prior study has analyzed the correlation between dependency parsing metrics and human judgments. For a range of other NLP tasks, metrics have been evaluated by how well they correlate with human judgments. For instance, the standard automatic metrics for certain tasks—such as BLEU in machine translation, or ROUGE-N and NIST in summarization or natural language generation—were evaluated, reaching correlation coefficients well above .80 (Papineni et al., 2002; Lin, 2004; Belz and Reiter, 2006; Callison-Burch et al., 2007). We find that correlations between evaluation metrics and human judgments are weaker for dependency parsing than other NLP tasks—our correlation coefficients are typically between .35 and .55—and that inter-annotator agreement is sometimes higher than human-metric agreement. Moreover, our analysis (§5) reveals that humans have a preference for attachment over labeling decisions, and that attachments closer to the root are more important. Our findings suggest that the currently employed metrics are not fully adequate. Using automatic measures such as labeled and u"
K15-1033,P02-1040,0,0.100561,"appropriate downstream tasks (Elming et al., 2013), we see human judgments as an important supplement to extrinsic evaluation. To the best of our knowledge, no prior study has analyzed the correlation between dependency parsing metrics and human judgments. For a range of other NLP tasks, metrics have been evaluated by how well they correlate with human judgments. For instance, the standard automatic metrics for certain tasks—such as BLEU in machine translation, or ROUGE-N and NIST in summarization or natural language generation—were evaluated, reaching correlation coefficients well above .80 (Papineni et al., 2002; Lin, 2004; Belz and Reiter, 2006; Callison-Burch et al., 2007). We find that correlations between evaluation metrics and human judgments are weaker for dependency parsing than other NLP tasks—our correlation coefficients are typically between .35 and .55—and that inter-annotator agreement is sometimes higher than human-metric agreement. Moreover, our analysis (§5) reveals that humans have a preference for attachment over labeling decisions, and that attachments closer to the root are more important. Our findings suggest that the currently employed metrics are not fully adequate. Using automa"
K15-1033,W06-2920,0,0.0844944,"0 sentences for each of the 5 languages) annotated with human judgments for the preferred automatically parsed dependency tree, enabling further research in this direction. 2 UCP = LCP = We evaluate seven dependency parsing metrics, described in this section. Given a labeled gold tree G = hV, EG , lG (·)i and a labeled predicted tree P = hV, EP , lP (·)i, let E ⊂ V × V be the set of directed edges from dependents to heads, and let l : V × V → L be the edge labeling function, with L the set of dependency labels. The three most commonly used metrics are those from the CoNLL 2006–7 shared tasks (Buchholz and Marsi, 2006): unlabeled attachment score (UAS), label accuracy (LA), both introduced by Eisner (1996), and labeled attachment score (LAS), the pivotal dependency parsing metric introduced by Nivre et al. (2004). LAS = 3 Experiment In our analysis, we compare the metrics with human judgments. We examine how well the automatic metrics correlate with each other, as well as with human judgments, and whether interannotator agreement exceeds annotator-metric agreement. |{e |e ∈ EG ∩ EP }| |V | |{e |lG (e) = lP (e), e ∈ EG ∩ EP }| |V | LA = |{v |Vverb , cG (v) = cP (v) ∧ lG (v, ·) = lP (v, ·)}| |Vverb | For the"
K15-1033,W07-0718,0,0.188994,"e human judgments as an important supplement to extrinsic evaluation. To the best of our knowledge, no prior study has analyzed the correlation between dependency parsing metrics and human judgments. For a range of other NLP tasks, metrics have been evaluated by how well they correlate with human judgments. For instance, the standard automatic metrics for certain tasks—such as BLEU in machine translation, or ROUGE-N and NIST in summarization or natural language generation—were evaluated, reaching correlation coefficients well above .80 (Papineni et al., 2002; Lin, 2004; Belz and Reiter, 2006; Callison-Burch et al., 2007). We find that correlations between evaluation metrics and human judgments are weaker for dependency parsing than other NLP tasks—our correlation coefficients are typically between .35 and .55—and that inter-annotator agreement is sometimes higher than human-metric agreement. Moreover, our analysis (§5) reveals that humans have a preference for attachment over labeling decisions, and that attachments closer to the root are more important. Our findings suggest that the currently employed metrics are not fully adequate. Using automatic measures such as labeled and unlabeled attachment scores is"
K15-1033,P11-1067,0,0.0697005,"Missing"
K15-1033,C96-1058,0,0.12707,"ly parsed dependency tree, enabling further research in this direction. 2 UCP = LCP = We evaluate seven dependency parsing metrics, described in this section. Given a labeled gold tree G = hV, EG , lG (·)i and a labeled predicted tree P = hV, EP , lP (·)i, let E ⊂ V × V be the set of directed edges from dependents to heads, and let l : V × V → L be the edge labeling function, with L the set of dependency labels. The three most commonly used metrics are those from the CoNLL 2006–7 shared tasks (Buchholz and Marsi, 2006): unlabeled attachment score (UAS), label accuracy (LA), both introduced by Eisner (1996), and labeled attachment score (LAS), the pivotal dependency parsing metric introduced by Nivre et al. (2004). LAS = 3 Experiment In our analysis, we compare the metrics with human judgments. We examine how well the automatic metrics correlate with each other, as well as with human judgments, and whether interannotator agreement exceeds annotator-metric agreement. |{e |e ∈ EG ∩ EP }| |V | |{e |lG (e) = lP (e), e ∈ EG ∩ EP }| |V | LA = |{v |Vverb , cG (v) = cP (v) ∧ lG (v, ·) = lP (v, ·)}| |Vverb | For the final figure of seven different parsing metrics, on top of the previous five, in our expe"
K15-1033,C12-1147,0,0.117595,"r NLP tasks. Also, inter-annotator agreement is sometimes higher than the agreement between judgments and metrics, indicating that the standard metrics fail to capture certain aspects of parse quality, such as the relevance of root attachment or the relative importance of the different parts of speech. 1 Introduction In dependency parser evaluation, the standard accuracy metrics—labeled and unlabeled attachment scores—are defined simply as averages over correct attachment decisions. Several authors have pointed out problems with these metrics; they are both sensitive to annotation guidelines (Schwartz et al., 2012; Tsarfaty et al., 2011), and they fail to say anything about how parsers fare on rare, but important linguistic constructions (Nivre et al., 2010). Both criticisms rely on the intuition that some parsing errors are more important than others, and that our metrics should somehow reflect that. There are sentences that are hard to annotate because they are ambiguous, or because they contain phenomena peripheral to linguistic theory, such as punctuation, clitics, or fragments. Manning (2011) discusses similar issues for part-ofspeech tagging. 315 Proceedings of the 19th Conference on Computationa"
K15-1033,N13-1070,1,0.846825,"ges: Croatian, Danish, English, German, and Spanish. For the human judgments, we asked professional linguists with dependency annotation experience to judge which of two parsers produced the better parse. Our stance here is that, insofar experts are able to annotate dependency trees, they are also able to determine the quality of a predicted syntactic structure, which we can in turn use to evaluate parser evaluation metrics. Even though downstream evaluation is critical in assessing the usefulness of parses, it also presents non-trivial challenges in choosing the appropriate downstream tasks (Elming et al., 2013), we see human judgments as an important supplement to extrinsic evaluation. To the best of our knowledge, no prior study has analyzed the correlation between dependency parsing metrics and human judgments. For a range of other NLP tasks, metrics have been evaluated by how well they correlate with human judgments. For instance, the standard automatic metrics for certain tasks—such as BLEU in machine translation, or ROUGE-N and NIST in summarization or natural language generation—were evaluated, reaching correlation coefficients well above .80 (Papineni et al., 2002; Lin, 2004; Belz and Reiter,"
K15-1033,D11-1036,0,0.220614,"r-annotator agreement is sometimes higher than the agreement between judgments and metrics, indicating that the standard metrics fail to capture certain aspects of parse quality, such as the relevance of root attachment or the relative importance of the different parts of speech. 1 Introduction In dependency parser evaluation, the standard accuracy metrics—labeled and unlabeled attachment scores—are defined simply as averages over correct attachment decisions. Several authors have pointed out problems with these metrics; they are both sensitive to annotation guidelines (Schwartz et al., 2012; Tsarfaty et al., 2011), and they fail to say anything about how parsers fare on rare, but important linguistic constructions (Nivre et al., 2010). Both criticisms rely on the intuition that some parsing errors are more important than others, and that our metrics should somehow reflect that. There are sentences that are hard to annotate because they are ambiguous, or because they contain phenomena peripheral to linguistic theory, such as punctuation, clitics, or fragments. Manning (2011) discusses similar issues for part-ofspeech tagging. 315 Proceedings of the 19th Conference on Computational Language Learning, pag"
K15-1033,N13-1132,0,0.022444,"78 .437 .250* .469 .404 .230* .195* .297 .331 .232 .318 .323 .171 .223 .466 .453 .310 .501 .331 .120* .190* .540 .397 .467 .446 .405* .120* .143* .457 .425 .324* .448 .361* .126* .195* Table 4: Correlations between human judgments and metrics (micro avg). * means significantly different from LAS ρ using Fisher’s z-transform. Bold: highest correlation per language. correlated, e.g., LAS and LA, and UAS and NED, but some exhibit very low correlation coefficients. Next we study correlations with human judgments (Table 4). In order to aggregate over the annotations, we use an item-response model (Hovy et al., 2013). The correlations are relatively weak compared to similar findings for other NLP tasks. For instance, ROUGE-1 (Lin, 2004) correlates strongly with perceived summary quality, with a coefficient of 0.99. The same holds for BLEU and human judgments of machine translation quality (Papineni et al., 2002). We find that, overall, LAS is the metric that correlates best with human judgments. It is closely followed by UAS, which does not differ significantly from LAS, albeit the correlations for UAS are slightly lower on average. NED is in turn highly correlated with UAS. The correlations for the predi"
K15-1033,E12-1006,0,0.0277816,"Missing"
K15-1033,W04-1013,0,0.191111,"tasks (Elming et al., 2013), we see human judgments as an important supplement to extrinsic evaluation. To the best of our knowledge, no prior study has analyzed the correlation between dependency parsing metrics and human judgments. For a range of other NLP tasks, metrics have been evaluated by how well they correlate with human judgments. For instance, the standard automatic metrics for certain tasks—such as BLEU in machine translation, or ROUGE-N and NIST in summarization or natural language generation—were evaluated, reaching correlation coefficients well above .80 (Papineni et al., 2002; Lin, 2004; Belz and Reiter, 2006; Callison-Burch et al., 2007). We find that correlations between evaluation metrics and human judgments are weaker for dependency parsing than other NLP tasks—our correlation coefficients are typically between .35 and .55—and that inter-annotator agreement is sometimes higher than human-metric agreement. Moreover, our analysis (§5) reveals that humans have a preference for attachment over labeling decisions, and that attachments closer to the root are more important. Our findings suggest that the currently employed metrics are not fully adequate. Using automatic measure"
K15-1033,P05-1012,0,0.0845214,". |{v |v ∈ V, lG (v, ·) = lP (v, ·)}| |V | Data In our experiments we use data from five languages: The English (en), German (de) and Spanish (es) treebanks from the Universal Dependencies (UD v1.0) project (Nivre et al., 2015), the Copenhagen Dependency Treebank (da) (BuchKromann, 2003), and the Croatian Dependency Treebank (hr) (Agi´c and Merkler, 2013). We keep the original POS tags for all datasets (17 tags in case of UD, 13 tags for Croatian, and 23 for Danish). Data characteristics are in Table 1. For the parsing systems, we follow McDonald and Nivre (2007) and use the second order MST (McDonald et al., 2005), as well as Malt parser with pseudo-projectivization (Nivre and Nilsson, 2005) and default parameters. For each language, we train the parsers on the canonical training section. We randomly select 200 sentences from the test sections, where our two deWe include two further metrics—namely, labeled (LCP) and unlabeled (UCP) complete predications—to give account for the relevance of correct predicate prediction for parsing quality. LCP is inspired by the complete predicates metric from the SemEval 2015 shared task on semantic parsing (Oepen et al., 2015).2 LCP is triggered by a verb (i.e., set o"
K15-1033,P05-1013,0,0.0676584,"a from five languages: The English (en), German (de) and Spanish (es) treebanks from the Universal Dependencies (UD v1.0) project (Nivre et al., 2015), the Copenhagen Dependency Treebank (da) (BuchKromann, 2003), and the Croatian Dependency Treebank (hr) (Agi´c and Merkler, 2013). We keep the original POS tags for all datasets (17 tags in case of UD, 13 tags for Croatian, and 23 for Danish). Data characteristics are in Table 1. For the parsing systems, we follow McDonald and Nivre (2007) and use the second order MST (McDonald et al., 2005), as well as Malt parser with pseudo-projectivization (Nivre and Nilsson, 2005) and default parameters. For each language, we train the parsers on the canonical training section. We randomly select 200 sentences from the test sections, where our two deWe include two further metrics—namely, labeled (LCP) and unlabeled (UCP) complete predications—to give account for the relevance of correct predicate prediction for parsing quality. LCP is inspired by the complete predicates metric from the SemEval 2015 shared task on semantic parsing (Oepen et al., 2015).2 LCP is triggered by a verb (i.e., set of nodes Vverb ) and checks whether all its core arguments match, i.e., all outg"
K15-1033,W04-2407,0,0.0333814,"dependency parsing metrics, described in this section. Given a labeled gold tree G = hV, EG , lG (·)i and a labeled predicted tree P = hV, EP , lP (·)i, let E ⊂ V × V be the set of directed edges from dependents to heads, and let l : V × V → L be the edge labeling function, with L the set of dependency labels. The three most commonly used metrics are those from the CoNLL 2006–7 shared tasks (Buchholz and Marsi, 2006): unlabeled attachment score (UAS), label accuracy (LA), both introduced by Eisner (1996), and labeled attachment score (LAS), the pivotal dependency parsing metric introduced by Nivre et al. (2004). LAS = 3 Experiment In our analysis, we compare the metrics with human judgments. We examine how well the automatic metrics correlate with each other, as well as with human judgments, and whether interannotator agreement exceeds annotator-metric agreement. |{e |e ∈ EG ∩ EP }| |V | |{e |lG (e) = lP (e), e ∈ EG ∩ EP }| |V | LA = |{v |Vverb , cG (v) = cP (v) ∧ lG (v, ·) = lP (v, ·)}| |Vverb | For the final figure of seven different parsing metrics, on top of the previous five, in our experiments we also include the neutral edge direction metric (NED) (Schwartz et al., 2011), and tree edit distan"
K15-1033,C10-1094,0,0.118629,"cs fail to capture certain aspects of parse quality, such as the relevance of root attachment or the relative importance of the different parts of speech. 1 Introduction In dependency parser evaluation, the standard accuracy metrics—labeled and unlabeled attachment scores—are defined simply as averages over correct attachment decisions. Several authors have pointed out problems with these metrics; they are both sensitive to annotation guidelines (Schwartz et al., 2012; Tsarfaty et al., 2011), and they fail to say anything about how parsers fare on rare, but important linguistic constructions (Nivre et al., 2010). Both criticisms rely on the intuition that some parsing errors are more important than others, and that our metrics should somehow reflect that. There are sentences that are hard to annotate because they are ambiguous, or because they contain phenomena peripheral to linguistic theory, such as punctuation, clitics, or fragments. Manning (2011) discusses similar issues for part-ofspeech tagging. 315 Proceedings of the 19th Conference on Computational Language Learning, pages 315–320, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics Contributions We present i)"
K15-1033,S15-2153,0,0.0389892,"e (2007) and use the second order MST (McDonald et al., 2005), as well as Malt parser with pseudo-projectivization (Nivre and Nilsson, 2005) and default parameters. For each language, we train the parsers on the canonical training section. We randomly select 200 sentences from the test sections, where our two deWe include two further metrics—namely, labeled (LCP) and unlabeled (UCP) complete predications—to give account for the relevance of correct predicate prediction for parsing quality. LCP is inspired by the complete predicates metric from the SemEval 2015 shared task on semantic parsing (Oepen et al., 2015).2 LCP is triggered by a verb (i.e., set of nodes Vverb ) and checks whether all its core arguments match, i.e., all outgoing dependency edges except for punctuation. Since LCP is a very strict metric, we also evaluate UCP, its unlabeled variant. Given a function cX (v) that retrieves the set of child nodes of a node v from a tree X, we first define UCP as follows, and then incorporate the label matching for LCP: 1 The dataset is publicly available at https:// bitbucket.org/lowlands/release 2 http://alt.qcri.org/semeval2015/ 3 316 http://www.tsarfaty.com/unipar/ L ANG PARSER LAS UAS LA NED TED"
K15-1033,S14-2008,0,\N,Missing
K17-3001,K17-3023,0,0.0375672,"Missing"
K17-3001,P16-1231,1,0.301678,"M Table 1: The supporting data overview: the number of words (M = million; K = thousand) for each language. http://commoncrawl.org/ Except for Ancient Greek, which was gathered from the Perseus Digital Library. 3 http://github.com/CLD2Owners/cld2 4 http://unicode.org/reports/tr15/ 3 verted to Unicode character NO-BREAK SPACE (U+00A0).5 The dimensionality of the word embeddings was chosen to be 100 after thorough discussion – more dimensions may yield better results and are commonly used, but even with just 100, the uncompressed word embeddings for the 45 languages take 135 GiB. Also note that Andor et al. (2016) achieved state-of-the-art results with 64 dimensions. The word embeddings were precomputed using word2vec (Mikolov et al., 2013) with the following options: word2vec -min-count 10 -size 100 -window 10 -negative 5 -iter 2 -threads 16 -cbow 0 -binary 0. The precomputed word embeddings are available on-line (Ginter et al., 2017). 2.3 this shared task, i.e., not included in any previous UD release. The PUD treebank consists of 1000 sentences currently in 18 languages (15 K to 27 K words, depending on the language), which were randomly picked from on-line newswire and Wikipedia;7 usually only a fe"
K17-3001,W06-2920,0,0.0145655,"categorization of the different approaches of the participating systems. Introduction Ten years ago, two CoNLL shared tasks were a major milestone for parsing research in general and dependency parsing in particular. For the first time dependency treebanks in more than ten languages were available for learning parsers. Many of them were used in follow-up work, evaluating parsers on multiple languages became standard, and multiple state-of-the-art, open-source parsers became available, facilitating production of dependency structures to be used in downstream applications. While the two tasks (Buchholz and Marsi, 2006; Nivre et al., 2007) were extremely important in setting the scene for the following years, there were also limitations that complicated application of their results: (1) gold-standard to1 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 1–19, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. kenization and part-of-speech tags in the test data moved the tasks away from real-world scenarios, and (2) incompatible annotation schemes made cross-linguistic comparison impossible. CoNLL 2017 has picked"
K17-3001,K17-3017,0,0.147208,"emains with participants, and since open sourcing the software underlying a paper is still the exception rather than the rule. To ensure both, TIRA supplies participants with a virtual machine, offering a range of commonly used operating systems in order not to limit the choice of technology stacks and development environments. Once deployed and tested, the virtual machines are archived to preserve the software within. Many participants agreed to share their code so that we decided to collect the respective projects in a kind of open source proceedings at GitHub.14 4.3 by Straka and Strakov´a (2017) as one of the competing systems. Straka and Strakov´a (2017) describe both these versions in more detail. The baseline models were released together with the UD 2.0 training data, one model for each treebank. Because only training and development data were available during baseline model training, we put aside a part of the training data for hyperparameter tuning, and evaluated the baseline model performance on development data. We called this data split baseline model split. The baseline models, the baseline model split, and also UD 2.0 training data with morphology predicted by 10-fold jack"
K17-3001,K17-3005,0,0.0752704,"Missing"
K17-3001,K17-3026,0,0.0310687,"E 90.88 82.31 82.46 LyS-FASTPARSE 90.88 82.31 79.14 NAIST SATO 90.88 82.31 82.46 Orange – Deski˜n 90.88 38.81 15.38 UALING 90.88 82.31 82.46 UParse 90.88 82.31 82.46 naistCL 90.88 82.31 82.46 Table 5: Universal POS tags, features and lemmas (ordered by UPOS F1 scores). duce suboptimal results when deployed on a machine different from the one where it was trained. Several teams used the library and may have been affected; for the Uppsala team (de Lhoneux et al., 2017) the issue led to official LAS = 65.11 (23rd place) instead of 69.66 (9th place). In the second case, the ParisNLP system (De La Clergerie et al., 2017) used a wrong method of recognizing the input language, which was not supported in the test data (but unfortunately it was possible to get along with it in development and trial data). Simply crashing could mean that the task moderator would show the team their diagnostic output and they would fix the bug; however, the parser was robust enough to switch to a languageagnostic mode and produced results that were not great, but also not so bad to alert the moderator and make him investigate. Thus the official LAS of the system is 60.02 (27th place) while without the bug it could have been 70.35 ("
K17-3001,K17-3021,0,0.0954088,"emains with participants, and since open sourcing the software underlying a paper is still the exception rather than the rule. To ensure both, TIRA supplies participants with a virtual machine, offering a range of commonly used operating systems in order not to limit the choice of technology stacks and development environments. Once deployed and tested, the virtual machines are archived to preserve the software within. Many participants agreed to share their code so that we decided to collect the respective projects in a kind of open source proceedings at GitHub.14 4.3 by Straka and Strakov´a (2017) as one of the competing systems. Straka and Strakov´a (2017) describe both these versions in more detail. The baseline models were released together with the UD 2.0 training data, one model for each treebank. Because only training and development data were available during baseline model training, we put aside a part of the training data for hyperparameter tuning, and evaluated the baseline model performance on development data. We called this data split baseline model split. The baseline models, the baseline model split, and also UD 2.0 training data with morphology predicted by 10-fold jack"
K17-3001,K17-3022,1,0.891655,"Missing"
K17-3001,K17-3025,0,0.0327614,"Missing"
K17-3001,K17-3024,0,0.050508,"Missing"
K17-3001,K17-3027,0,0.0537913,"Missing"
K17-3001,K17-3014,0,0.0756362,"Missing"
K17-3001,K17-3015,0,0.0745209,"Missing"
K17-3001,K17-3007,0,0.0511894,"Missing"
K17-3001,L16-1262,1,0.869327,"Missing"
K17-3001,W14-6111,0,0.0253686,"Missing"
K17-3001,W17-0411,1,0.831758,"ossible when the system run completed; before that, even the task moderator would not see whether the system was really producing output and not just sitting in an endless loop. Especially given the scale of operations this year, this turned out to be a major obstacle for some participants; TIRA needs to be improved by offering more finegrained process monitoring tools, both for organizers and participants. Content-word Labeled Attachment Score (CLAS) has been proposed as an alternative parsing metric that is tailored to the UD annotation style and more suitable for cross-language comparison (Nivre and Fang, 2017). It differs from LAS in that it only considers relations between content words. Attachment of function words is disregarded because it corresponds to morphological features in other languages (and morphology is not evaluated in this shared task). Furthermore, languages with many function words (e.g., English) have longer sentences than morphologically rich languages (e.g., Finnish), hence a single error in Finnish costs the parser significantly more than an error in English. CLAS also disregards attachment of punctuation. As CLAS is still experimental, we have designated full LAS as our main"
K17-3001,K17-3003,0,0.0845341,"Missing"
K17-3001,W17-0412,1,0.869806,"Missing"
K17-3001,L16-1680,1,0.0475333,"Missing"
K17-3001,K17-3009,1,0.104147,"Missing"
K17-3001,tiedemann-2012-parallel,0,0.0126153,"oses (so that follow-up research is not obstructed). We deliberately did not place upper bounds on data sizes (in contrast to e.g. Nivre et al. (2007)), despite the fact that processing large amounts of data may be difficult for some teams. Our primary objective was to determine the capability of current parsers with the data that is currently available. In practice, the task was formally closed, i.e., we listed the approved data resources so that all participants were aware of their options. However, the selection was rather broad, ranging from Wikipedia dumps over the OPUS parallel corpora (Tiedemann, 2012) to morphological transducers. Some of the resources were proposed by the participating teams. 2.2 Supporting Data To enable the induction of custom embeddings and the use of semi-supervised methods in general, the participants were provided with supporting resources primarily consisting of large text corpora for (nearly) all of the languages in the task, as well as embeddings pre-trained on these corpora. 1 Outside CoNLL, there were several other parsing tasks in the meantime, which naturally also explored previously unadressed aspects—for example SANCL (Petrov and McDonald, 2012) or SPMRL (S"
K17-3001,K17-3016,0,0.0605417,"Missing"
K17-3001,K17-3020,0,0.0375614,"Missing"
K17-3001,K17-3013,0,0.0456211,"Missing"
K17-3001,D07-1096,1,\N,Missing
K17-3001,K17-3002,1,\N,Missing
K17-3001,K17-3019,0,\N,Missing
K17-3001,K17-3012,1,\N,Missing
K17-3001,K17-3006,0,\N,Missing
K17-3001,K17-3010,0,\N,Missing
K17-3001,K17-3018,0,\N,Missing
K17-3001,K17-3028,1,\N,Missing
K17-3001,K17-3011,0,\N,Missing
L16-1136,J08-4004,0,0.173758,"an blogs and chat, probably because the language of these text types is intrinsically more complex and contains more abstract concepts (for a detailed study on domain differences in the annotations see Olsen et al. 2015). Figure 2 shows how each noun supersense is represented disagreement-wise in the corpus. The rows in the disagreement plot are sorted after the size of the diagonal value. Rows with many large, spread boxes indicate supersenses with low agreement which need a closer examination or more precise guidelines. For 2 http://wordnet.princeton.edu/wordnet/man/lexnames.5WN.ht ml 3 Cf. Artstein & Poesio 2008 for discussion of agreement scores in computational linguistics. 843 instance, the supersenses n.person and n.institution seem to be hard for the annotators to distinguish from each other, whereas n.disease has proven easy to identify. a set of precise guidelines defining the sense structuring principles. In rough terms, these principles were based on the distinctions between core and subsenses as defined by Cruse (2000:110ff). Among several types of regular relations between senses, Cruse defines four types where the senses might be of the same ontological type (and therefore candidates of c"
L16-1136,brown-etal-2010-number,0,0.026183,"ent words are annotated (so-called lexical sample corpora). As discussed in Ide & Wilks (2007), Kilgarriff (2007) and others, defining appropriate sense inventories for annotation and word sense disambiguation tasks is however a very hard task. The need for coarser and more manageable sense inventories has emerged, partly driven by poor sense annotator agreement scores in the aforementioned annotations. This has resulted in a series of annotation experiments applying manually and automatically clustered senses, as seen in Agirre & Lacalle (2003), Palmer et al. (2007), Passonneau et al. (2012) Brown et al. (2010), de Melo et al. (2012), and others. The need for “light weight” semantic annotations has led researchers to focus also on very coarse word sense annotation applying so-called supersenses that are derived from the list of WordNet’s first beginners or lexicographical files. This approach is becoming a de facto standard in recent years (Ciaramita & Johnson 2003, Qiu et al. 2011, Schneider et al. 2012). In the SemDaX corpus we include both supersense annotations and lexical sample annotations with fine-grained and automatically clustered senses for a selected set of highly ambiguous nouns. All an"
L16-1136,S14-1001,1,0.848737,"Missing"
L16-1136,W15-1831,1,0.907794,"that the coarse-grained supersense scheme is quite manageable to the annotators resulting in an acceptable agreement of 0.62 applying Krippendorff’s α. However, as shown in the dispersion plot in Figure 1 the scheme leaves room for improvements and adjustments; i.e. some particular supersenses prove very hard to agree upon. Further, the considerable information loss in the coarse annotations should be addressed in future extrinsic evaluations; for instance, it can be questioned to which extent we actually capture the practically relevant ambiguities with this coarse scheme; see also Martínez Alonso et al. 2015 for a first attempt of inducing a supersense tagger from our 6 Note that the supersense scheme is not directly comparable to the fine-grained schemes since the annotation tasks differ (all-word vs. lexical sample). 845 supersense annotations. This leads us to the finer-grained dictionary-driven annotations of highly ambiguous nouns that we described in Section 4. Here we can conclude that a clustered annotation scheme based on an ontologically driven collapsing of subsenses performs substantially better than a fully fine-grained scheme (disregarding here the better chance of agreeing on few t"
L16-1136,W15-1806,1,0.857672,"Missing"
L16-1136,2016.gwc-1.30,1,0.534761,"Missing"
L16-1136,W15-2005,1,0.784725,"Missing"
L16-1136,passonneau-etal-2012-masc,0,0.047019,"Missing"
L16-1136,E14-1078,1,0.850484,"flecting formal distinctions or logical relations between senses of the word in question. Not only does doubly annotated data provide valuable feedback regarding the annotation schemes, we also think that it can help us improve our learning algorithms. Our corpus is about one fifth of the size of SemCor. However, as mentioned, a large part of the data has been doubly annotated and later adjudicated. We make available both the final adjucated version and the individual annotations in order to facilitate research that deals with the linguistic information that resides in agreement variation. In Plank et al. (2014) and Martínez Alonso et al. (2015a) we present an algorithm that learns regularizers from small seeds of doubly annotated data. In future work we will apply SemDaX for further experiments along the same lines. Finally, our project includes a pilot study on the compilation of a Danish framenet (similar to the well-known Berkeley FrameNet, cf. https://framenet.icsi.berkeley.edu/fndrupal/). This part of the project has been embarked recently by focusing on the approx. 1/3 of the sentences of our corpus where cognition and communication verbs are present (identified via the previously mentioned su"
L16-1136,P12-2050,0,0.0982508,"ations. This has resulted in a series of annotation experiments applying manually and automatically clustered senses, as seen in Agirre & Lacalle (2003), Palmer et al. (2007), Passonneau et al. (2012) Brown et al. (2010), de Melo et al. (2012), and others. The need for “light weight” semantic annotations has led researchers to focus also on very coarse word sense annotation applying so-called supersenses that are derived from the list of WordNet’s first beginners or lexicographical files. This approach is becoming a de facto standard in recent years (Ciaramita & Johnson 2003, Qiu et al. 2011, Schneider et al. 2012). In the SemDaX corpus we include both supersense annotations and lexical sample annotations with fine-grained and automatically clustered senses for a selected set of highly ambiguous nouns. All annotations in the corpus rely on the combined wordnet and dictionary resources: DanNet (cf. Pedersen 2009 et al.) and a comprehensive monolingual, corpus-based dictionary of modern Danish, Den Danske Ordbog (DDO, Hjorth et al. 2005), which share sense identifies. The aim of the corpus is twofold: i) to assess the reliability of the different sense annotation schemes in terms of different levels of gr"
L16-1136,P13-4001,0,0.0740307,"Missing"
L16-1136,J99-4008,0,0.0763419,"Missing"
L16-1136,W14-0132,0,0.160111,"Missing"
L16-1136,W03-1022,0,\N,Missing
L18-1586,bird-etal-2008-acl,0,0.0367685,"in ACL2. ACL terms Classification 22,980 labels (Complingterm) Figure 1: Flowchart representation of the semantic annotation process. Pekar and Staab, 2003; Ruiz-Casado et al., 2007; Popescu et al., 2008; Ji and Grishman, 2011) in that we aim at the automatic type-based estimation of the semantic class of words or word sequences. Inspired by this family of approaches, we label each term, which is a specialized nominal expression of length one or more, with a semantic class. 3. Data Preparation and Semantic Labeling We work on the ACL Anthology Reference Corpus (ACL ARC) in its first version (Bird et al., 2008). This corpus contains more than 10,000 scholarly articles from the computational linguistics domain that were published between 1965 and 2006. We also use two additional data sets that have been created on the basis of the ACL ARC. In particular, we use a list of technical terms (ACL RD-TEC 1.0, termed ACL1) that was created by means of automatic term extraction (Q. Zadeh and Handschuh, 2014). More specifically, the term list was created with the help of several term extractors, and each term candidate was then manually validated by the main curator of the resource. This process resulted in m"
L18-1586,J92-4003,0,0.266532,"ion. The resulting classifier uses the following features: 1. BoW: Identity of the term headword. If the term is longer than one word, we treat all words from the second as a bag of words (BoW). In this way, we give the headword of the term a special treatment, which makes it easier to identify as a trigger term for a certain class. 2. Length: The length of the term in number of words, and the proportion of capitalized characters. These features help identify multi-word expressions and determine whether they are terminological units, or whether they are acronyms. 3. Brown: The Brown clusters (Brown et al., 1992) from the full ACL corpus for the words in the term. Brown clusters group words in a corpus according to their immediate surrounding bi-grams and provide good features to estimate semantic classes. 4. Embeddings: The average word embedding for all words in the term. We use embeddings from the ACL corpus with 100 dimensions and a word window of 5. Using embeddings allows us to incorporate distributional information of words involved in a term that is larger in scope than the information captured by Brown clusters. 5. WordNet: The number of senses in WordNet (Miller, 1995) for the term headword,"
L18-1586,I11-1001,0,0.0138307,"dly. However, even “dormant” (Menard, 1971) science can regain importance if new data is produced or methods are developed to tackle unresolved research problems. Scientific thought exhibits intricate evolutionary patterns (Fleck, 1980) and paradigm change (Kuhn, 1962) can affect the structure and outline of a whole discipline. The availability of large collections of digitized scientific text enables systematic studies of the processes that drive scientific development. Recent years have seen a notable increase in quantitative studies of scientific text collections, e. g. Hall et al. (2008), Gupta and Manning (2011), Michaelis et al. (2013), Mariani et al. (2014), Babko-Malaya et al. (2015), Schumann and QasemiZadeh (2015b), Asooja et al. (2016), Francopoulo et al. (2016), Schumann (2016), Heyer et al. (2016). The present study is a contribution to this research strand. Our work centers on the use of semantic labeling techniques for the automatic enhancement of a small corpus of manual term and semantic class annotations. The ultimate goal of our work, however, is to use this information as one feature in the profiling of scientific papers, communities, and disciplines. In using semantic class labels as"
L18-1586,D08-1038,0,0.011978,"ts move forward rapidly. However, even “dormant” (Menard, 1971) science can regain importance if new data is produced or methods are developed to tackle unresolved research problems. Scientific thought exhibits intricate evolutionary patterns (Fleck, 1980) and paradigm change (Kuhn, 1962) can affect the structure and outline of a whole discipline. The availability of large collections of digitized scientific text enables systematic studies of the processes that drive scientific development. Recent years have seen a notable increase in quantitative studies of scientific text collections, e. g. Hall et al. (2008), Gupta and Manning (2011), Michaelis et al. (2013), Mariani et al. (2014), Babko-Malaya et al. (2015), Schumann and QasemiZadeh (2015b), Asooja et al. (2016), Francopoulo et al. (2016), Schumann (2016), Heyer et al. (2016). The present study is a contribution to this research strand. Our work centers on the use of semantic labeling techniques for the automatic enhancement of a small corpus of manual term and semantic class annotations. The ultimate goal of our work, however, is to use this information as one feature in the profiling of scientific papers, communities, and disciplines. In using"
L18-1586,P11-1115,0,0.0155803,"om/anetschka/ complingterm. 2 3707 Online analytical processing (Codd et al., 1993). ACL2 Extraction (Section 3.2.) 1,686 instances Aggregation (Section 3.3.) 1,4900 types Type Technologies Tools Language resources Lang. resource products Models Measures Other Training (Section 3.4.) Example parsing parser corpus Brown corpus language model Bleu score residual class Table 1: Semantic classes in ACL2. ACL terms Classification 22,980 labels (Complingterm) Figure 1: Flowchart representation of the semantic annotation process. Pekar and Staab, 2003; Ruiz-Casado et al., 2007; Popescu et al., 2008; Ji and Grishman, 2011) in that we aim at the automatic type-based estimation of the semantic class of words or word sequences. Inspired by this family of approaches, we label each term, which is a specialized nominal expression of length one or more, with a semantic class. 3. Data Preparation and Semantic Labeling We work on the ACL Anthology Reference Corpus (ACL ARC) in its first version (Bird et al., 2008). This corpus contains more than 10,000 scholarly articles from the computational linguistics domain that were published between 1965 and 2006. We also use two additional data sets that have been created on the"
L18-1586,W93-0231,0,0.721915,"this end, we query the data set resulting from our annotation effort on both the term and the semantic class level level. Keywords: semantic labeling, terminology, history of science 1. Introduction Science changes continually: While certain research topics may be in a state of stagnation or decline, other research fronts move forward rapidly. However, even “dormant” (Menard, 1971) science can regain importance if new data is produced or methods are developed to tackle unresolved research problems. Scientific thought exhibits intricate evolutionary patterns (Fleck, 1980) and paradigm change (Kuhn, 1962) can affect the structure and outline of a whole discipline. The availability of large collections of digitized scientific text enables systematic studies of the processes that drive scientific development. Recent years have seen a notable increase in quantitative studies of scientific text collections, e. g. Hall et al. (2008), Gupta and Manning (2011), Michaelis et al. (2013), Mariani et al. (2014), Babko-Malaya et al. (2015), Schumann and QasemiZadeh (2015b), Asooja et al. (2016), Francopoulo et al. (2016), Schumann (2016), Heyer et al. (2016). The present study is a contribution to this re"
L18-1586,mariani-etal-2014-rediscovering,0,0.0242339,"can regain importance if new data is produced or methods are developed to tackle unresolved research problems. Scientific thought exhibits intricate evolutionary patterns (Fleck, 1980) and paradigm change (Kuhn, 1962) can affect the structure and outline of a whole discipline. The availability of large collections of digitized scientific text enables systematic studies of the processes that drive scientific development. Recent years have seen a notable increase in quantitative studies of scientific text collections, e. g. Hall et al. (2008), Gupta and Manning (2011), Michaelis et al. (2013), Mariani et al. (2014), Babko-Malaya et al. (2015), Schumann and QasemiZadeh (2015b), Asooja et al. (2016), Francopoulo et al. (2016), Schumann (2016), Heyer et al. (2016). The present study is a contribution to this research strand. Our work centers on the use of semantic labeling techniques for the automatic enhancement of a small corpus of manual term and semantic class annotations. The ultimate goal of our work, however, is to use this information as one feature in the profiling of scientific papers, communities, and disciplines. In using semantic class labels as one source of information, we take a macro- rath"
L18-1586,E03-1084,0,0.0701266,"ion (Montoyo et al., 2001; Bergamaschi et al., 2007; 1 https://github.com/anetschka/ complingterm. 2 3707 Online analytical processing (Codd et al., 1993). ACL2 Extraction (Section 3.2.) 1,686 instances Aggregation (Section 3.3.) 1,4900 types Type Technologies Tools Language resources Lang. resource products Models Measures Other Training (Section 3.4.) Example parsing parser corpus Brown corpus language model Bleu score residual class Table 1: Semantic classes in ACL2. ACL terms Classification 22,980 labels (Complingterm) Figure 1: Flowchart representation of the semantic annotation process. Pekar and Staab, 2003; Ruiz-Casado et al., 2007; Popescu et al., 2008; Ji and Grishman, 2011) in that we aim at the automatic type-based estimation of the semantic class of words or word sequences. Inspired by this family of approaches, we label each term, which is a specialized nominal expression of length one or more, with a semantic class. 3. Data Preparation and Semantic Labeling We work on the ACL Anthology Reference Corpus (ACL ARC) in its first version (Bird et al., 2008). This corpus contains more than 10,000 scholarly articles from the computational linguistics domain that were published between 1965 and"
L18-1586,W14-4807,0,0.0575881,"Missing"
L18-1586,L16-1294,1,0.87639,"ermed ACL1) that was created by means of automatic term extraction (Q. Zadeh and Handschuh, 2014). More specifically, the term list was created with the help of several term extractors, and each term candidate was then manually validated by the main curator of the resource. This process resulted in more than 20,000 specialized terms that were deemed valid. In our experiments, the ACL1 term list is used to identify all known terms. Moreover, we use a set of in-line, doubleblind term and semantic class annotations (ACL RD-TEC 2.0, termed ACL2) provided on a subset of abstracts from the ACL ARC (QasemiZadeh and Schumann, 2016). These annotations were created by two human annotators in a multi-step process that resulted in both term span and semantic class annotations, following annotations guidelines that differentiate between seven semantic classes, as shown in Table 1. We use these high-quality annotations to train our classification models. Figure 1 provides a graphical representation of our work-flow for data preparation and annotation. Data flows from green input data ellipses to gray output data ellipses are represented with dashed lines. Blue boxes represent major work steps in the process and are explained"
L18-1718,P13-2107,0,0.0454926,"Missing"
L18-1718,W13-4916,0,0.0300012,"Missing"
L18-1718,W09-3821,1,0.740978,"Missing"
L18-1718,F12-2024,1,0.899058,"Missing"
L18-1718,candito-etal-2010-statistical,1,0.865662,"Missing"
L18-1718,C12-1052,0,0.052222,"Missing"
L18-1718,C12-1059,0,0.0416459,"Missing"
L18-1718,J93-2004,0,0.0629965,"Missing"
L18-1718,P09-2010,0,0.0726827,"Missing"
L18-1718,L16-1375,1,0.89553,"Missing"
L18-1718,W13-4917,1,0.856651,"Missing"
L18-1718,W14-6111,1,0.908556,"Missing"
L18-1718,W13-4906,1,0.857223,"Missing"
L18-1718,E17-1034,0,0.0185843,"an iterative fashion, or as new relevant conversion needs are identified. A full manual evaluation of a converted treebank could represent an effort comparable to full re-annotation of a large part of the data. Indeed, few of the UD-conversion papers provide accuracy scores of the conversion on a manually annotated testbench. For instance, The Danish conversion of Johannsen et al. (2015), uses a small set of hand-annotated sentences that reflect specific phenomena and hard cases that is used as held-out section during the iterative development of conversion rules. The Hungarian conversion of Vincze et al. (2017) uses a hand-corrected gold standard of 1,800 sentences. When comparing the quality of the conversion with the gold standard, they consider the accuracy (87.81 UAS and 75.99 LAS) not sufficient to release the resulting treebank. We draw inspiration on their method to develop a handcorrected sample to evaluate the quality of our conversion.One of the authors of the article, an expert in dependency annotation very familiar with the UD formalism, reviewed 100 sentences from the test section and 100 sentences from the dev section manually, correcting edges and labels that were either not properly"
L18-1718,K17-3001,1,0.897083,"Missing"
N13-1070,D11-1037,0,0.010158,"the tasks will serve to show that the choice of conversion scheme has significant impact on down-stream performance. We used the most recent release of the Mate parser first described in Bohnet (2010),5 trained on Sections 2–21 of the Wall Street Journal section of the English Treebank (Marcus et al., 1993). The graphbased parser is similar to, except much faster, and performs slightly better than the MSTParser (McDonald et al., 2005), which is known to perform well on long-distance dependencies often important for down-stream applications (McDonald and Nivre, 2007; Galley and Manning, 2009; Bender et al., 2011). This choice may of course have an effect on what conversion schemes seem superior (Johansson and Nugues, 2007). Sentence splitting was done using splitta,6 , and the sentences were then tokenized using PTB-style tokenization7 and tagged using the in-built Mate POS tagger. Previous work There has been considerable work on down-stream evaluation of syntactic parsers in the literature, but most previous work has focused on evaluating parsing models rather than linguistic theories. No one has, to the best of our knowledge, compared the impact of choice of tree-to-dependency conversion scheme acr"
N13-1070,C10-1011,0,0.00726912,"ot rely4 The LTH conversion scheme can be obtained by running pennconverter.jar available at with the http://nlp.cs.lth.se/software/treebank converter/ ’oldLTH’ flag set. 618 ing on syntactic features, when possible, and to results in the literature, when comparable results exist. Note that negation resolution and SRL are not end applications. It is not easy to generalize across five very different tasks, but the tasks will serve to show that the choice of conversion scheme has significant impact on down-stream performance. We used the most recent release of the Mate parser first described in Bohnet (2010),5 trained on Sections 2–21 of the Wall Street Journal section of the English Treebank (Marcus et al., 1993). The graphbased parser is similar to, except much faster, and performs slightly better than the MSTParser (McDonald et al., 2005), which is known to perform well on long-distance dependencies often important for down-stream applications (McDonald and Nivre, 2007; Galley and Manning, 2009; Bender et al., 2011). This choice may of course have an effect on what conversion schemes seem superior (Johansson and Nugues, 2007). Sentence splitting was done using splitta,6 , and the sentences wer"
N13-1070,P11-2031,0,0.0123088,"sentences of the parallel news data that have been parsed with each of the tree-to-dependency conversion schemes. The reordering models condition reordering on the word forms, POS, and syntactic dependency relations of the words to be reordered, as described in Elming and Haulrich (2011). The paper shows that while reordering by parsing leads to significant improvements in standard metrics such as BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007), improvements are more spelled out with human judgements. All SMT results reported below are averages based on 5 MERT runs following Clark et al. (2011). 2.4 Sentence compression is a restricted form of sentence simplification with numerous usages, including text simplification, summarization and recognizing textual entailment. The most commonly used dataset in the literature is the Ziff-Davis corpus.12 A widely used baseline for sentence compression experiments is Knight and Marcu (2002), who introduce two models: the noisy-channel model and a decision tree-based model. Both are tree-based methods that find the most likely compressed syntactic tree and outputs the yield of this tree. McDonald et al. (2006) instead use syntactic features to d"
N13-1070,P09-1087,0,0.361999,"very different tasks, but the tasks will serve to show that the choice of conversion scheme has significant impact on down-stream performance. We used the most recent release of the Mate parser first described in Bohnet (2010),5 trained on Sections 2–21 of the Wall Street Journal section of the English Treebank (Marcus et al., 1993). The graphbased parser is similar to, except much faster, and performs slightly better than the MSTParser (McDonald et al., 2005), which is known to perform well on long-distance dependencies often important for down-stream applications (McDonald and Nivre, 2007; Galley and Manning, 2009; Bender et al., 2011). This choice may of course have an effect on what conversion schemes seem superior (Johansson and Nugues, 2007). Sentence splitting was done using splitta,6 , and the sentences were then tokenized using PTB-style tokenization7 and tagged using the in-built Mate POS tagger. Previous work There has been considerable work on down-stream evaluation of syntactic parsers in the literature, but most previous work has focused on evaluating parsing models rather than linguistic theories. No one has, to the best of our knowledge, compared the impact of choice of tree-to-dependency"
N13-1070,D11-1138,0,0.0118129,"uate linguistic representations directly, evaluating representations and models jointly. Bender et al. (2011) compare several parsers across linguistic representations on a carefully designed evaluation set of hard, but relatively frequent syntactic constructions. They compare dependency parsers, constituent-based parsers and deep parsers. The authors argue in favor of evaluating parsers on diverse and richly annotated data. Others have discussed various ways of evaluating across annotation guidelines or translating structures to a common format (Schwartz et al., 2011; Tsarfaty et al., 2012). Hall et al. (2011) discuss optimizing parsers for specific down-stream applications, but consider only a single annotation scheme. Yuret et al. (2012) present an overview of the SemEval-2010 Evaluation Exercises on Semantic 619 Evaluation track on recognition textual entailment using dependency parsing. They also compare several parsers using the heuristics of the winning system for inference. While the shared task is an example of down-stream evaluation of dependency parsers, the evaluation examples only cover a subset of the textual entailments relevant for practical applications, and the heuristics used in t"
N13-1070,W12-3602,0,0.00940108,"Missing"
N13-1070,W10-2910,0,0.0245474,"(ewt labels). Finally, Schwartz et al. (2012) compare the above conversion schemes and several combinations thereof in terms of learnability. This is very different from what is done here. While learnability may be a theoretically motivated parameter, our results indicate that learnability and downstream performance do not correlate well. 2 Applications Dependency parsing has proven useful for a wide range of NLP applications, including statistical machine translation (Galley and Manning, 2009; Xu et al., 2009; Elming and Haulrich, 2011) and sentiment analysis (Joshi and Penstein-Rose, 2009; Johansson and Moschitti, 2010). This section describes the applications and experimental set-ups included in this study. In the five applications considered below we use syntactic features in slightly different ways. While our statistical machine translation and sentence compression systems use dependency relations as additional information about words and on a par with POS, our negation resolution system uses dependency paths, conditioning decisions on both dependency arcs and labels. In perspective classification, we use dependency triples (e.g. SUBJ(John, snore)) as features, while the semantic role labeling system cond"
N13-1070,W07-2416,0,0.0543773,"performance. We used the most recent release of the Mate parser first described in Bohnet (2010),5 trained on Sections 2–21 of the Wall Street Journal section of the English Treebank (Marcus et al., 1993). The graphbased parser is similar to, except much faster, and performs slightly better than the MSTParser (McDonald et al., 2005), which is known to perform well on long-distance dependencies often important for down-stream applications (McDonald and Nivre, 2007; Galley and Manning, 2009; Bender et al., 2011). This choice may of course have an effect on what conversion schemes seem superior (Johansson and Nugues, 2007). Sentence splitting was done using splitta,6 , and the sentences were then tokenized using PTB-style tokenization7 and tagged using the in-built Mate POS tagger. Previous work There has been considerable work on down-stream evaluation of syntactic parsers in the literature, but most previous work has focused on evaluating parsing models rather than linguistic theories. No one has, to the best of our knowledge, compared the impact of choice of tree-to-dependency conversion scheme across several NLP tasks. Johansson and Nugues (2007) compare the impact of yamada and lth on semantic role labelin"
N13-1070,W08-2123,0,0.0435471,"cy features at all, the models are tested using gold cues. Table 1 shows F1 scores for scopes, events and full negations, where a true positive correctly assigns both scope tokens and events to the rightful cue. The scores are produced using the evaluation script provided by the *SEM organizers. 2.2 Semantic role labeling Semantic role labeling (SRL) is the attempt to determine semantic predicates in running text and label their arguments with semantic roles. In our experiments we have reproduced the second bestperforming system in the CoNLL 2008 shared task in syntactic and semantic parsing (Johansson and Nugues, 2008).9 The English training data for the CoNLL 2008 shared task were obtained from PropBank and NomBank. For licensing reasons, we used OntoNotes 4.0, which includes PropBank, but not NomBank. This means that our system is only trained to classify verbal predicates. We used the Clearparser conversion tool10 to convert the OntoNotes 4.0 and subsequently supplied syntactic dependency trees using our different conversion schemes. We rely on gold standard argument identification and focus solely on the performance metric semantic labeled F1. 9 http://nlp.cs.lth.se/software/semantic parsing: propbank n"
N13-1070,P09-2079,0,0.0585007,"fixed set of dependency labels (ewt labels). Finally, Schwartz et al. (2012) compare the above conversion schemes and several combinations thereof in terms of learnability. This is very different from what is done here. While learnability may be a theoretically motivated parameter, our results indicate that learnability and downstream performance do not correlate well. 2 Applications Dependency parsing has proven useful for a wide range of NLP applications, including statistical machine translation (Galley and Manning, 2009; Xu et al., 2009; Elming and Haulrich, 2011) and sentiment analysis (Joshi and Penstein-Rose, 2009; Johansson and Moschitti, 2010). This section describes the applications and experimental set-ups included in this study. In the five applications considered below we use syntactic features in slightly different ways. While our statistical machine translation and sentence compression systems use dependency relations as additional information about words and on a par with POS, our negation resolution system uses dependency paths, conditioning decisions on both dependency arcs and labels. In perspective classification, we use dependency triples (e.g. SUBJ(John, snore)) as features, while the se"
N13-1070,S12-1042,1,0.816976,"us (CD),8 was released in conjunction with the *SEM shared task. The annotations in CD extend on cues and scopes by introducing annotations for in-scope events that are negated in factual contexts. The following is an example from the corpus showing the annotations for cues (bold), scopes (underlined) and negated events (italicized): (1) Since we have been so unfortunate as to miss him [. . . ] CD-style scopes can be discontinuous and overlapping. Events are a portion of the scope that is semantically negated, with its truth value reversed by the negation cue. The NR system used in this work (Lapponi et al., 2012), one of the best performing systems in the *SEM shared task, is a CRF model for scope resolution that relies heavily on features extracted from dependency graphs. The feature model contains token distance, direction, n-grams of word forms, lemmas, POS and combinations thereof, as well as the syntactic features presented in Figure 4. The results in our 8 http://www.clips.ua.ac.be/sem2012-st-neg/data.html 620 Syntactic Cue-dependent constituent dependency relation parent head POS grand parent head POS word form+dependency relation POS+dependency relation directed dependency distance bidirection"
N13-1070,W07-0734,0,0.0110385,"best performing system in the WMT 2011 shared task on this dataset. The four experimental systems have reordering models that are trained on the first 25,000 sentences of the parallel news data that have been parsed with each of the tree-to-dependency conversion schemes. The reordering models condition reordering on the word forms, POS, and syntactic dependency relations of the words to be reordered, as described in Elming and Haulrich (2011). The paper shows that while reordering by parsing leads to significant improvements in standard metrics such as BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007), improvements are more spelled out with human judgements. All SMT results reported below are averages based on 5 MERT runs following Clark et al. (2011). 2.4 Sentence compression is a restricted form of sentence simplification with numerous usages, including text simplification, summarization and recognizing textual entailment. The most commonly used dataset in the literature is the Ziff-Davis corpus.12 A widely used baseline for sentence compression experiments is Knight and Marcu (2002), who introduce two models: the noisy-channel model and a decision tree-based model. Both are tree-based m"
N13-1070,J93-2004,0,0.0472942,"http://nlp.cs.lth.se/software/treebank converter/ ’oldLTH’ flag set. 618 ing on syntactic features, when possible, and to results in the literature, when comparable results exist. Note that negation resolution and SRL are not end applications. It is not easy to generalize across five very different tasks, but the tasks will serve to show that the choice of conversion scheme has significant impact on down-stream performance. We used the most recent release of the Mate parser first described in Bohnet (2010),5 trained on Sections 2–21 of the Wall Street Journal section of the English Treebank (Marcus et al., 1993). The graphbased parser is similar to, except much faster, and performs slightly better than the MSTParser (McDonald et al., 2005), which is known to perform well on long-distance dependencies often important for down-stream applications (McDonald and Nivre, 2007; Galley and Manning, 2009; Bender et al., 2011). This choice may of course have an effect on what conversion schemes seem superior (Johansson and Nugues, 2007). Sentence splitting was done using splitta,6 , and the sentences were then tokenized using PTB-style tokenization7 and tagged using the in-built Mate POS tagger. Previous work"
N13-1070,H05-1066,0,0.0310964,"Missing"
N13-1070,E06-1038,0,0.228356,"Missing"
N13-1070,S12-1035,0,0.0116767,"a lot of information, including the word form of the head, the dependent and the argument candidates, the concatenation of the dependency labels of the predicate, and the labeled dependency relations between predicate and its head, its arguments, dependents or siblings. 2.1 Negation resolution Negation resolution (NR) is the task of finding negation cues, e.g. the word not, and determining their scope, i.e. the tokens they affect. NR has recently seen considerable interest in the NLP community (Morante and Sporleder, 2012; Velldal et al., 2012) and was the topic of the 2012 *SEM shared task (Morante and Blanco, 2012). The data set used in this work, the Conan Doyle corpus (CD),8 was released in conjunction with the *SEM shared task. The annotations in CD extend on cues and scopes by introducing annotations for in-scope events that are negated in factual contexts. The following is an example from the corpus showing the annotations for cues (bold), scopes (underlined) and negated events (italicized): (1) Since we have been so unfortunate as to miss him [. . . ] CD-style scopes can be discontinuous and overlapping. Events are a portion of the scope that is semantically negated, with its truth value reversed"
N13-1070,J12-2001,0,0.00610037,"triples (e.g. SUBJ(John, snore)) as features, while the semantic role labeling system conditions on a lot of information, including the word form of the head, the dependent and the argument candidates, the concatenation of the dependency labels of the predicate, and the labeled dependency relations between predicate and its head, its arguments, dependents or siblings. 2.1 Negation resolution Negation resolution (NR) is the task of finding negation cues, e.g. the word not, and determining their scope, i.e. the tokens they affect. NR has recently seen considerable interest in the NLP community (Morante and Sporleder, 2012; Velldal et al., 2012) and was the topic of the 2012 *SEM shared task (Morante and Blanco, 2012). The data set used in this work, the Conan Doyle corpus (CD),8 was released in conjunction with the *SEM shared task. The annotations in CD extend on cues and scopes by introducing annotations for in-scope events that are negated in factual contexts. The following is an example from the corpus showing the annotations for cues (bold), scopes (underlined) and negated events (italicized): (1) Since we have been so unfortunate as to miss him [. . . ] CD-style scopes can be discontinuous and overlappin"
N13-1070,P02-1040,0,0.107319,"aseline system was one of the tied best performing system in the WMT 2011 shared task on this dataset. The four experimental systems have reordering models that are trained on the first 25,000 sentences of the parallel news data that have been parsed with each of the tree-to-dependency conversion schemes. The reordering models condition reordering on the word forms, POS, and syntactic dependency relations of the words to be reordered, as described in Elming and Haulrich (2011). The paper shows that while reordering by parsing leads to significant improvements in standard metrics such as BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007), improvements are more spelled out with human judgements. All SMT results reported below are averages based on 5 MERT runs following Clark et al. (2011). 2.4 Sentence compression is a restricted form of sentence simplification with numerous usages, including text simplification, summarization and recognizing textual entailment. The most commonly used dataset in the literature is the Ziff-Davis corpus.12 A widely used baseline for sentence compression experiments is Knight and Marcu (2002), who introduce two models: the noisy-channel model and a decision tr"
N13-1070,P11-1067,0,0.0598892,"in biomedical event extraction, but do not evaluate linguistic representations directly, evaluating representations and models jointly. Bender et al. (2011) compare several parsers across linguistic representations on a carefully designed evaluation set of hard, but relatively frequent syntactic constructions. They compare dependency parsers, constituent-based parsers and deep parsers. The authors argue in favor of evaluating parsers on diverse and richly annotated data. Others have discussed various ways of evaluating across annotation guidelines or translating structures to a common format (Schwartz et al., 2011; Tsarfaty et al., 2012). Hall et al. (2011) discuss optimizing parsers for specific down-stream applications, but consider only a single annotation scheme. Yuret et al. (2012) present an overview of the SemEval-2010 Evaluation Exercises on Semantic 619 Evaluation track on recognition textual entailment using dependency parsing. They also compare several parsers using the heuristics of the winning system for inference. While the shared task is an example of down-stream evaluation of dependency parsers, the evaluation examples only cover a subset of the textual entailments relevant for practica"
N13-1070,C12-1147,0,0.104984,"r only a single annotation scheme. Yuret et al. (2012) present an overview of the SemEval-2010 Evaluation Exercises on Semantic 619 Evaluation track on recognition textual entailment using dependency parsing. They also compare several parsers using the heuristics of the winning system for inference. While the shared task is an example of down-stream evaluation of dependency parsers, the evaluation examples only cover a subset of the textual entailments relevant for practical applications, and the heuristics used in the experiments assume a fixed set of dependency labels (ewt labels). Finally, Schwartz et al. (2012) compare the above conversion schemes and several combinations thereof in terms of learnability. This is very different from what is done here. While learnability may be a theoretically motivated parameter, our results indicate that learnability and downstream performance do not correlate well. 2 Applications Dependency parsing has proven useful for a wide range of NLP applications, including statistical machine translation (Galley and Manning, 2009; Xu et al., 2009; Elming and Haulrich, 2011) and sentiment analysis (Joshi and Penstein-Rose, 2009; Johansson and Moschitti, 2010). This section d"
N13-1070,E12-1006,0,0.116337,"raction, but do not evaluate linguistic representations directly, evaluating representations and models jointly. Bender et al. (2011) compare several parsers across linguistic representations on a carefully designed evaluation set of hard, but relatively frequent syntactic constructions. They compare dependency parsers, constituent-based parsers and deep parsers. The authors argue in favor of evaluating parsers on diverse and richly annotated data. Others have discussed various ways of evaluating across annotation guidelines or translating structures to a common format (Schwartz et al., 2011; Tsarfaty et al., 2012). Hall et al. (2011) discuss optimizing parsers for specific down-stream applications, but consider only a single annotation scheme. Yuret et al. (2012) present an overview of the SemEval-2010 Evaluation Exercises on Semantic 619 Evaluation track on recognition textual entailment using dependency parsing. They also compare several parsers using the heuristics of the winning system for inference. While the shared task is an example of down-stream evaluation of dependency parsers, the evaluation examples only cover a subset of the textual entailments relevant for practical applications, and the"
N13-1070,J12-2005,0,0.0246868,"e)) as features, while the semantic role labeling system conditions on a lot of information, including the word form of the head, the dependent and the argument candidates, the concatenation of the dependency labels of the predicate, and the labeled dependency relations between predicate and its head, its arguments, dependents or siblings. 2.1 Negation resolution Negation resolution (NR) is the task of finding negation cues, e.g. the word not, and determining their scope, i.e. the tokens they affect. NR has recently seen considerable interest in the NLP community (Morante and Sporleder, 2012; Velldal et al., 2012) and was the topic of the 2012 *SEM shared task (Morante and Blanco, 2012). The data set used in this work, the Conan Doyle corpus (CD),8 was released in conjunction with the *SEM shared task. The annotations in CD extend on cues and scopes by introducing annotations for in-scope events that are negated in factual contexts. The following is an example from the corpus showing the annotations for cues (bold), scopes (underlined) and negated events (italicized): (1) Since we have been so unfortunate as to miss him [. . . ] CD-style scopes can be discontinuous and overlapping. Events are a portion"
N13-1070,N09-1028,0,0.0260543,"al applications, and the heuristics used in the experiments assume a fixed set of dependency labels (ewt labels). Finally, Schwartz et al. (2012) compare the above conversion schemes and several combinations thereof in terms of learnability. This is very different from what is done here. While learnability may be a theoretically motivated parameter, our results indicate that learnability and downstream performance do not correlate well. 2 Applications Dependency parsing has proven useful for a wide range of NLP applications, including statistical machine translation (Galley and Manning, 2009; Xu et al., 2009; Elming and Haulrich, 2011) and sentiment analysis (Joshi and Penstein-Rose, 2009; Johansson and Moschitti, 2010). This section describes the applications and experimental set-ups included in this study. In the five applications considered below we use syntactic features in slightly different ways. While our statistical machine translation and sentence compression systems use dependency relations as additional information about words and on a par with POS, our negation resolution system uses dependency paths, conditioning decisions on both dependency arcs and labels. In perspective classifica"
N13-1070,W02-1001,0,\N,Missing
N13-1070,J03-4003,0,\N,Missing
N13-1070,C10-1088,0,\N,Missing
N15-1135,W06-1615,0,0.437228,"Missing"
N15-1135,J92-4003,0,0.658692,"obin Williams playing an adult Peter and Dustin Hoffman as the dastardly Captain Hook. Rooofiii Oooooo, didn’t think ppl<3the movie as much as me, this movie will always b the peter pan story2me #robin #williams #hook I loved that movie... Uhm... You know, Hook. With Robin Williams, uh. peter pan williams movie Table 1: Examples from source (top row) and target domains (bottom rows) spelling variations with the standard form (youuuuuuuu → you), which reduces the vocabulary size. For languages where no such normalization dictionary is available, we use word clusterings based on Brown clusters (Brown et al., 1992) to generalize tags from unambiguous words to previously unseen words in the same class. C LUSTER 01011110 01011110 01011110 01011110 01011110 01011110 01011110 T OKEN offish alreadyyy finali aleady previously already recently TAG ∈ D ADJ ??? ??? ??? ADV ADV ADV P ROJ . TAG — ADV ADV ADV — — — Figure 1: Example of a Brown cluster with unambiguous tokens, as well as projected tags for new tokens (tokens marked “—” are unchanged in D0 ). In particular, to extend the dictionary D to D0 using clusters, we first run clustering on the unlabeled data T , using Brown clustering.2 We then assign to eac"
N15-1135,P07-1033,0,0.668034,"Missing"
N15-1135,I11-1100,0,0.0495232,"Missing"
N15-1135,P11-2008,0,0.158516,"Missing"
N15-1135,P11-1038,0,0.0629429,"generalize across spelling variations and synonyms. Additionally, we evaluate our approach on Dutch, Portuguese and Spanish Twitter and present tow novel data sets for the latter two languages. 2 Data 2.1 Wiktionary In our experiments, we use the (unigram) tag dictionaries from Wiktionary, as collected by Li et al. (2012).1 The size and quality of our tag dictionaries crucially influence how much unambiguous data we can extract, and for some languages, the number of dictionary entries is small. We can resort to normalization dictionaries to extend Wiktionary’s coverage. We do so for English (Han and Baldwin, 2011). It replaces some 1 https://code.google.com/p/ wikily-supervised-pos-tagger/ 1256 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1256–1261, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics N EWSWIRE T WITTER S POKEN Q UERIES Spielberg took the helm of this big budget live action project with Robin Williams playing an adult Peter and Dustin Hoffman as the dastardly Captain Hook. Rooofiii Oooooo, didn’t think ppl<3the movie as much as me, this movie will always b the peter pan story2me #robin #will"
N15-1135,hovy-etal-2014-pos,1,0.86932,"guese, we use the training section of the Bosque treebank.7 For Spanish, we use the training section of the Cast3LB treebank.8 In order to map between Wiktionary and the treebanks, we need a common coarse tag set. We thus map all data to the universal tag set (Petrov et al., 2012). Dev and test sets Our approach is basically parameter free. However, we did experiment with different ways of extending Wiktionary and hence used an average over three English Twitter dev sections as development set (Ritter et al., 2011; Gimpel et al., 2011; Foster et al., 2011), all mapped and normalized following Hovy et al. (2014). For evaluation, we use three domains: tweets, spoken data and queries. For Twitter, we performed experiments in four languages: English, Portuguese, Spanish and Dutch. The Spanish and Portuguese tweets were annotated in-house, which will be made available.9 For the other languages, we use preexisting datasets for English (Hovy et al., 2014) and Dutch (Avontuur et al., 2012). Table 2 lists the complete statistics for the different language data sets. For the other two domains, we use the manually labeled data from Switchboard section 4 as spoken data test set. For queries, we use manually lab"
N15-1135,I05-1017,0,0.0340948,"from the set of licensed ones. In the second case, we assume the unknown word to be a NOUN, since unknown words mostly tend to be proper names. When added to newswire, this data results in worse models, presumably by introducing too much noise. However, for low-resource languages or domains with longer sentences and no available newswire data, this might be a viable alternative. 6 Related Work Our approach is similar to mining high-precision items. However, previous approaches on this in NLP have mainly focused on well-defined classification tasks, such as PP attachment (Pantel and Lin, 2000; Kawahara and Kurohashi, 2005), or discourse connective disambiguation (Marcu and Echihabi, 2002). In contrast, we mine for sequences of unambiguous tokens in a structured prediction task. While we use the same dictionaries as in Li et al. (2012) and T¨ackstr¨om et al. (2013), our approach differs in several respects. First, we use Wiktionary to mine for training data, rather than as type constraints, and second, we use Brown clusters to extend Wiktionary. We did experiment with different ways of doing this, including using various forms of word embeddings, leading to models similar to the baseline models in Socher et al."
N15-1135,2005.mtsummit-papers.11,0,0.0607748,"r example, is only about 0.012 (or 1 in 84), and the distribution of tags in the Twitter data set is heavily skewed towards nouns, while several other labels are under-represented. Twitter We collect the unlabeled data from the Twitter streaming API.3 We collected 57m tweets for English, 8.2m for Spanish, 4.1m for Portuguese, and 0.5m for Dutch. We do not perform sentence splitting on tweets, but take them as unit sequences. Spoken language We use the Switchboard corpus of transcribed telephone conversations (Godfrey et al., 1992), sections 2 and 3, as well as the English section of EuroParl (Koehn, 2005) and CHILDES (MacWhinney, 1997). We removed all meta-data and inline annotations (gestures, sounds, etc.), as well as dialogue markers. The final joint corpus contains transcriptions of 570k spoken sentences. Search queries For search queries, we use a combination of queries from Yahoo4 and AOL. We only use the search terms and ignore any additional information, such as user ID, time, and linked URLs. The resulting data set contains 10m queries. 3 2 https://github.com/percyliang/ brown-cluster 4 1257 Unlabeled data https://github.com/saffsd/langid.py http://webscope.sandbox.yahoo.com/ 2.3 Labe"
N15-1135,D12-1127,0,0.0753564,"Missing"
N15-1135,P02-1047,0,0.0803701,"n word to be a NOUN, since unknown words mostly tend to be proper names. When added to newswire, this data results in worse models, presumably by introducing too much noise. However, for low-resource languages or domains with longer sentences and no available newswire data, this might be a viable alternative. 6 Related Work Our approach is similar to mining high-precision items. However, previous approaches on this in NLP have mainly focused on well-defined classification tasks, such as PP attachment (Pantel and Lin, 2000; Kawahara and Kurohashi, 2005), or discourse connective disambiguation (Marcu and Echihabi, 2002). In contrast, we mine for sequences of unambiguous tokens in a structured prediction task. While we use the same dictionaries as in Li et al. (2012) and T¨ackstr¨om et al. (2013), our approach differs in several respects. First, we use Wiktionary to mine for training data, rather than as type constraints, and second, we use Brown clusters to extend Wiktionary. We did experiment with different ways of doing this, including using various forms of word embeddings, leading to models similar to the baseline models in Socher et al. (2013), but the approach based on Brown clusters led to the best re"
N15-1135,P09-1113,0,0.130157,"Missing"
N15-1135,N13-1039,0,0.203014,"Missing"
N15-1135,P00-1014,0,0.0692605,"omly at training time from the set of licensed ones. In the second case, we assume the unknown word to be a NOUN, since unknown words mostly tend to be proper names. When added to newswire, this data results in worse models, presumably by introducing too much noise. However, for low-resource languages or domains with longer sentences and no available newswire data, this might be a viable alternative. 6 Related Work Our approach is similar to mining high-precision items. However, previous approaches on this in NLP have mainly focused on well-defined classification tasks, such as PP attachment (Pantel and Lin, 2000; Kawahara and Kurohashi, 2005), or discourse connective disambiguation (Marcu and Echihabi, 2002). In contrast, we mine for sequences of unambiguous tokens in a structured prediction task. While we use the same dictionaries as in Li et al. (2012) and T¨ackstr¨om et al. (2013), our approach differs in several respects. First, we use Wiktionary to mine for training data, rather than as type constraints, and second, we use Brown clusters to extend Wiktionary. We did experiment with different ways of doing this, including using various forms of word embeddings, leading to models similar to the ba"
N15-1135,petrov-etal-2012-universal,0,0.0266651,"ndbox.yahoo.com/ 2.3 Labeled data We train our models on newswire, as well as mined unambiguous instances. For English, we use the OntoNotes release of the WSJ section of the Penn Treebank as training data for Twitter, spoken data, and queries.5 For Dutch, we use the training section of the Alpino treebank from the CoNLL task.6 For Portuguese, we use the training section of the Bosque treebank.7 For Spanish, we use the training section of the Cast3LB treebank.8 In order to map between Wiktionary and the treebanks, we need a common coarse tag set. We thus map all data to the universal tag set (Petrov et al., 2012). Dev and test sets Our approach is basically parameter free. However, we did experiment with different ways of extending Wiktionary and hence used an average over three English Twitter dev sections as development set (Ritter et al., 2011; Gimpel et al., 2011; Foster et al., 2011), all mapped and normalized following Hovy et al. (2014). For evaluation, we use three domains: tweets, spoken data and queries. For Twitter, we performed experiments in four languages: English, Portuguese, Spanish and Dutch. The Spanish and Portuguese tweets were annotated in-house, which will be made available.9 For"
N15-1135,C14-1168,1,0.853249,"Missing"
N15-1135,D11-1141,0,0.0257816,"ries.5 For Dutch, we use the training section of the Alpino treebank from the CoNLL task.6 For Portuguese, we use the training section of the Bosque treebank.7 For Spanish, we use the training section of the Cast3LB treebank.8 In order to map between Wiktionary and the treebanks, we need a common coarse tag set. We thus map all data to the universal tag set (Petrov et al., 2012). Dev and test sets Our approach is basically parameter free. However, we did experiment with different ways of extending Wiktionary and hence used an average over three English Twitter dev sections as development set (Ritter et al., 2011; Gimpel et al., 2011; Foster et al., 2011), all mapped and normalized following Hovy et al. (2014). For evaluation, we use three domains: tweets, spoken data and queries. For Twitter, we performed experiments in four languages: English, Portuguese, Spanish and Dutch. The Spanish and Portuguese tweets were annotated in-house, which will be made available.9 For the other languages, we use preexisting datasets for English (Hovy et al., 2014) and Dutch (Avontuur et al., 2012). Table 2 lists the complete statistics for the different language data sets. For the other two domains, we use the manuall"
N15-1135,Q13-1001,0,0.0669642,"Missing"
N15-1135,P99-1023,0,0.0664404,"floresta_English.html 8 http://www.iula.upf.edu/recurs01_tbk_ uk.htm 9 http://lowlands.ku.dk/results 10 https://code.google.com/p/crfpp/ 6 1258 fault parameters. As baselines we consider a) a CRF model trained only on newswire; b) available off-the-shelf systems (T OOLS); and c) a weakly supervised model (L I 10). For English, the off-theshelf tagger is the Stanford tagger (Toutanova et al., 2003), for the other languages we use TreeTagger (Schmid, 1994) with pre-trained models. The weakly supervised model trained is on the unannotated data. It is a second-order HMM model (Mari et al., 1997; Thede and Harper, 1999) (SOHMM) using logistic regression to estimate the emission probabilities. This method allows us to use feature vectors rather than just word identity, as in standard HMMs. In addition, we constrain the inference space of the tagger using type-level tag constraints derived from Wiktionary. This model, called L I 10 in Table 3, was originally proposed by Li et al. (2012). We extend the model by adding continuous word representations, induced from the unlabeled data using the skip-gram algorithm (Mikolov et al., 2013), to the feature representations. Our logistic regression model thus works over"
N15-1135,N03-1033,0,0.0285312,"10). 3 Experiments 3.1 Model We use a CRF10 model (Lafferty et al., 2001) with the same features as Owoputi et al. (2013) and de5 LDC2011T03. http://www.let.rug.nl/˜vannoord/trees/ 7 http://www.linguateca.pt/floresta/info_ floresta_English.html 8 http://www.iula.upf.edu/recurs01_tbk_ uk.htm 9 http://lowlands.ku.dk/results 10 https://code.google.com/p/crfpp/ 6 1258 fault parameters. As baselines we consider a) a CRF model trained only on newswire; b) available off-the-shelf systems (T OOLS); and c) a weakly supervised model (L I 10). For English, the off-theshelf tagger is the Stanford tagger (Toutanova et al., 2003), for the other languages we use TreeTagger (Schmid, 1994) with pre-trained models. The weakly supervised model trained is on the unannotated data. It is a second-order HMM model (Mari et al., 1997; Thede and Harper, 1999) (SOHMM) using logistic regression to estimate the emission probabilities. This method allows us to use feature vectors rather than just word identity, as in standard HMMs. In addition, we constrain the inference space of the tagger using type-level tag constraints derived from Wiktionary. This model, called L I 10 in Table 3, was originally proposed by Li et al. (2012). We e"
N15-1152,N13-1070,1,0.847311,"Related Work Plank et al. (2014a) propose IAA-weighted costsensitive learning for POS tagging. We extend their line of work to dependency parsing. A single sentence can have more than one plausible dependency annotation. Some researchers have 1360 proposed evaluation metrics that do not penalize disagreements (Schwartz et al., 2011; Tsarfaty et al., 2011), while others have argued that we should instead ensure the consistency of treebanks (Dickinson, 2010; Manning, 2011; McDonald et al., 2013). Others have claimed that because of these ambiguities, only downstream evaluations are meaningful (Elming et al., 2013). Syntactic annotation disagreement has typically been studied in the context of treebank development. Haverinen et al. (2012), for example, analyze annotator disagreement for Finnish dependency syntax, and compare it against parser performance. Skjærholt (2014) use doubly-annotated data to evaluate various agreement metrics. Our paper differs from both lines of research in that we leverage disagreements from doubly-annotated data to obtain more robust models. While we agree that evaluation metrics should probably reflect disagreements, we show that our learning algorithms can indeed benefit f"
N15-1152,C12-1059,0,0.0219341,"l1 i and hh2 , l2 i count as disagreement, iff hj < i < h k . d) H EAD P OS: disagreement on head POS. That is, hh1 , l1 i and hh2 , l2 i count as disagreement, iff POS(h1 )6=POS(h2 ). e) H EAD P OS D, i.e., H EAD P OS, plus direction. That is, hh1 , l1 i and hh2 , l2 i count as disagreement, iff POS(h1 )6=POS(h2 ) or hj < i < hk . train 13.7k/209k 3.6k/70k 4.2k/74k 3.9k/73k 3.1k/79k 9.1k/123k Cost-sensitive updates We use the cost-sensitive perceptron classifier, following Plank et al. (2014a), but extend it to transition-based dependency parsing, where the predicted values are transitions (Goldberg and Nivre, 2012). Given a gold yi and predicted label yˆi (POS tags or transitions), the loss is weighted by γ(ˆ yi , yi ): Lw (ˆ yi , yi ) = γ(ˆ yi , yi ) max(0, −yi w · xi ) Whenever a transition has been wrongly predicted, we retrieve the predicted edge and compare it to the gold dependency to calculate γ. γ(yi , yj ) is then the inverse of the confusion probability estimated from our sample of doubly-annotated data. For example, using the factorization L ABEL, if the parser predicts wi to be S UBJECT and the gold annotation is O B JECT , the confusion probability is the number of times one annotator said"
N15-1152,P05-1013,0,0.133665,"Missing"
N15-1152,E14-1078,1,0.731472,"-annotated data, and involves two steps: i) how to factorize attachment or labeling disagreements; and ii) how to inform the parser of them during learning (§3). 2 Introduction Typically, NLP annotation projects employ guidelines to maximize inter-annotator agreement. Possible inconsistencies are resolved by adjudication, and models are induced assuming there is one single ground truth. However, there exist linguistically hard cases where there is no clear answer (Zeman, 2010; Manning, 2011), and incorporating such disagreements into the training of a model has proven helpful for POS tagging (Plank et al., 2014a; Plank et al., 2014b). Inter-annotator agreement (IAA) is straightforward to calculate for POS, but not for dependency trees. There is no well-established standard for computing agreement on trees (Skjærholt, 2014). Factorizations Assume a sample of sentences annotated by annotators A1 and A2 . With such a sample we can estimate probabilities of the two annotators’ disagreeing on the annotation of a word or span, relative to some dependency tree factorization. We factorize disagreement on dependency tree annotations relative to four properties of the annotated dependency edges: the POS of th"
N15-1152,P14-2083,1,0.854013,"-annotated data, and involves two steps: i) how to factorize attachment or labeling disagreements; and ii) how to inform the parser of them during learning (§3). 2 Introduction Typically, NLP annotation projects employ guidelines to maximize inter-annotator agreement. Possible inconsistencies are resolved by adjudication, and models are induced assuming there is one single ground truth. However, there exist linguistically hard cases where there is no clear answer (Zeman, 2010; Manning, 2011), and incorporating such disagreements into the training of a model has proven helpful for POS tagging (Plank et al., 2014a; Plank et al., 2014b). Inter-annotator agreement (IAA) is straightforward to calculate for POS, but not for dependency trees. There is no well-established standard for computing agreement on trees (Skjærholt, 2014). Factorizations Assume a sample of sentences annotated by annotators A1 and A2 . With such a sample we can estimate probabilities of the two annotators’ disagreeing on the annotation of a word or span, relative to some dependency tree factorization. We factorize disagreement on dependency tree annotations relative to four properties of the annotated dependency edges: the POS of th"
N15-1152,P11-1067,0,0.259652,"Missing"
N15-1152,P14-1088,1,0.930456,"guidelines to maximize inter-annotator agreement. Possible inconsistencies are resolved by adjudication, and models are induced assuming there is one single ground truth. However, there exist linguistically hard cases where there is no clear answer (Zeman, 2010; Manning, 2011), and incorporating such disagreements into the training of a model has proven helpful for POS tagging (Plank et al., 2014a; Plank et al., 2014b). Inter-annotator agreement (IAA) is straightforward to calculate for POS, but not for dependency trees. There is no well-established standard for computing agreement on trees (Skjærholt, 2014). Factorizations Assume a sample of sentences annotated by annotators A1 and A2 . With such a sample we can estimate probabilities of the two annotators’ disagreeing on the annotation of a word or span, relative to some dependency tree factorization. We factorize disagreement on dependency tree annotations relative to four properties of the annotated dependency edges: the POS of the dependent, the POS of the head, the label of the edge and the direction (left or right) of the head with regards to the dependent. This section describes the different factorizations. We present five factorizations"
N15-1152,solberg-etal-2014-norwegian,1,0.900114,"Missing"
N15-1152,D11-1036,0,0.0431991,"Missing"
N15-1152,P10-1075,0,\N,Missing
N15-1152,arias-etal-2014-boosting,0,\N,Missing
P13-2127,J08-4004,0,0.183848,"20 171 298 82 95 140 139 U 7 54 25 22 48 91 83 69 47 V 3 8 0 3 3 53 44 54 40 B 4 48 25 19 45 38 39 15 7 Table 3: Literal, Metonymic and Underspecified sense distributions, and underspecified senses broken down in Voting and Backoff Average observed agreement (Ao ) is the mean across examples for the proportion of matching senses assigned by the annotators. Krippendorff’s alpha is an aggregate measure that takes chance disagreement in consideration and accounts for the replicability of an annotation scheme. There are large differences in α across datasets. The scheme can only provide reliable (Artstein and Poesio, 2008) annotations (α &gt; 0.6) for one dot type2 . This indicates that not all dot types are equally easy to annotate, regardless of the kind of annotator. In spite of the number and type of annotators, the Location/Organization dot type gives fairly high agreement values for a semantic task, and this behavior is consistent across languages. The columns labelled L, M and U in Table 3 provide the sense distributions for each dot type. The preference for the underspecified sense varies greatly, from the very infrequent for English in Animal/Meat to the two Danish datasets where the underspecified sense"
P13-2127,J12-3005,0,0.0920653,"s, the results in terms of inter-encoder agreement, and the sense distributions obtained with two methods: majority voting with a theory-compliant backoff strategy, and MACE, an unsupervised system to choose the most likely sense from all the annotations. 1 ´ Nuria Bel Universitat Pompeu Fabra Barcelona (Spain) nuria.bel@upf.edu Introduction This article shows the annotation task of a corpus in English, Danish and Spanish for regular polysemy. Regular polysemy (Apresjan, 1974; Pustejovsky, 1995; Briscoe et al., 1995; Nunberg, 1995) has received a lot of attention in computational linguistics (Boleda et al., 2012; Rumshisky et al., 2007; Shutova, 2009). The lack of available senseannotated gold standards with underspecification is a limitation for NLP applications that rely on dot types1 (Rumshisky et al., 2007; Poibeau, 2006; Pustejovsky et al., 2009). Our goal is to obtain human-annotated corpus data to study regular polysemy and to detect it in an automatic manner. We have collected a corpus of annotated examples in English, Danish and Spanish to study the alternation between senses and the cases of underspecification, including a contrastive study between languages. Here we describe the annotation"
P13-2127,W09-3716,0,0.0271131,"the annotations. 1 ´ Nuria Bel Universitat Pompeu Fabra Barcelona (Spain) nuria.bel@upf.edu Introduction This article shows the annotation task of a corpus in English, Danish and Spanish for regular polysemy. Regular polysemy (Apresjan, 1974; Pustejovsky, 1995; Briscoe et al., 1995; Nunberg, 1995) has received a lot of attention in computational linguistics (Boleda et al., 2012; Rumshisky et al., 2007; Shutova, 2009). The lack of available senseannotated gold standards with underspecification is a limitation for NLP applications that rely on dot types1 (Rumshisky et al., 2007; Poibeau, 2006; Pustejovsky et al., 2009). Our goal is to obtain human-annotated corpus data to study regular polysemy and to detect it in an automatic manner. We have collected a corpus of annotated examples in English, Danish and Spanish to study the alternation between senses and the cases of underspecification, including a contrastive study between languages. Here we describe the annotation process, its results in terms of inter-encoder agreement, and the sense distributions obtained with two methods: majority voting with a theory-compliant backoff strategy and, MACE an unsupervised system to choose the most likely sense from all"
P13-2127,gonzalez-agirre-etal-2012-multilingual,0,0.0239491,"responding dot type. In spite of a part of the annotation being made with a contrastive study in mind, no parallel text was used to avoid using translated text. For English and Danish we used freely available reference corpora (Ide and Macleod, 2001; Andersen et al., 2002) and, for Spanish, a corpus built from newswire and technical text (Vivaldi, 2009). For most of the English examples we used the words in Rumshisky (2007), except for Location/Organization. For Danish and Spanish we translated the words from English. We expanded the lists using each language’s wordnet (Pedersen et al., 2009; Gonzalez-Agirre et al., 2012) as thesaurus to make the total of occurrences reach 500 after we had removed homonyms and other forms of semantic variation outside of the purview of regular polysemy. For Location/Organization we have used highfrequency names of geopolitical locations from each of the corpora. Many of them are corpusspecific (e.g. Madrid is more frequent in the Spanish corpus) but a set of words is shared: Afghanistan, Africa, America, China, England, Europe,Germany, London. Every dot type has its particularities that we had to deal with. For instance, English has lexical alFigure 1: Screen capture for a Mec"
P13-2127,N13-1132,0,0.0505433,"Missing"
P13-2127,P09-3001,0,0.0176863,"ement, and the sense distributions obtained with two methods: majority voting with a theory-compliant backoff strategy, and MACE, an unsupervised system to choose the most likely sense from all the annotations. 1 ´ Nuria Bel Universitat Pompeu Fabra Barcelona (Spain) nuria.bel@upf.edu Introduction This article shows the annotation task of a corpus in English, Danish and Spanish for regular polysemy. Regular polysemy (Apresjan, 1974; Pustejovsky, 1995; Briscoe et al., 1995; Nunberg, 1995) has received a lot of attention in computational linguistics (Boleda et al., 2012; Rumshisky et al., 2007; Shutova, 2009). The lack of available senseannotated gold standards with underspecification is a limitation for NLP applications that rely on dot types1 (Rumshisky et al., 2007; Poibeau, 2006; Pustejovsky et al., 2009). Our goal is to obtain human-annotated corpus data to study regular polysemy and to detect it in an automatic manner. We have collected a corpus of annotated examples in English, Danish and Spanish to study the alternation between senses and the cases of underspecification, including a contrastive study between languages. Here we describe the annotation process, its results in terms of inter-"
P13-2127,D08-1027,0,0.254133,"Missing"
P13-2127,J03-2004,0,0.0295571,") (ANC): a) Manuel died in exile in 1932 in England. b) England was being kept busy with other concerns c) England was, after all, an important wine market In case a), England refers to the English territory (Location), whereas in b) it refers arguably to England as a political entity (Organization). The third case refers to both. The ability of certain words to switch between semantic types in a predictable manner is referred to as regular polysemy. Unlike other forms of meaning variation caused by metaphor or homonymy, regular polysemy is considered to be caused by metonymy (Apresjan, 1974; Lapata and Lascarides, 2003). Regular polysemy is different from other forms of polysemy in that both senses can be active at the same in a predicate, which we refer to as underspecification. Underspecified instances can be broken down in: 1. Contextually complex: England was, after all, an important wine market 2. Zeugmatic, in which two mutually exclusive readings are coordinated: England is conservative and rainy 3. Vague, in which no contextual element enforces a reading: The case of England is similar We present the result of an annotation task on regular polysemy for a series of semantic classes or dot types in Eng"
P13-2127,markert-nissim-2002-towards,0,0.152885,"locations from each of the corpora. Many of them are corpusspecific (e.g. Madrid is more frequent in the Spanish corpus) but a set of words is shared: Afghanistan, Africa, America, China, England, Europe,Germany, London. Every dot type has its particularities that we had to deal with. For instance, English has lexical alFigure 1: Screen capture for a Mechanical Turk annotation instance or HIT This annotation scheme is designed with the intention of capturing literal, metonymic and underspecified senses, and we use an inventory of three possible answers, instead of using Markert and Nissim’s (Markert and Nissim, 2002; Nissim and Markert, 2005) approach with fine-grained sense distinctions, which are potentially more difficult to annotate and resolve automatically. Markert and Nissim acknowledge a mixed sense they define as being literal and metonymic at the same time. For English we used Amazon Mechanical Turk (AMT) with five annotations per example by turkers certified as Classification Masters. Using AMT provides annotations very quickly, possibly at the expense of reliability, but it has been proven suitable for sense-disambiguation task (Snow et al., 2008). Moreover, it is not possible to obtain annot"
P13-2127,D12-1017,0,0.0680757,"Table 5 breaks down the five annotations that each example received by turkers in literal, metonymic and underspecified. The last two columns show the sense tag provided by voting or MACE. Example d) e) f) g) h) i) j) L 2 3 1 2 2 3 1 M 2 1 2 2 2 0 2 U 1 1 2 1 1 2 2 VOTING U L M U U L M Conclusions MACE L U U M M U U 9 Table 5: Annotation summary and sense tags for the examples in this section Further work After collecting annotated data, the natural next step is to attempt class-based word-sense disambiguation (WSD) to predict the senses in Tables 3 and 4 using a state-of-the-art system like Nastase et al. (2012). We will consider a sense-assignment method (voting or MACE) as more appropriate if it provides the sense tags that are easiest to learn by our WSD system. However, learnability is only one possible parameter for quality, and we also want to develop an expert-annotated gold standard to compare our data against. We also consider the possibility of developing a sense-assignment method that relies both on the theoretical assumption behind the voting scheme and the latent-variable approach used by MACE. Just by looking at the table it is not immediate which method is preferable to assign sense ta"
P15-1165,P14-2131,0,0.0530581,"a mean vector to out-of-vocabulary words. System For our system, we simply augment the delexicalized POS tagger with the I NVERTED distributional representation of the current word. The best parameter setting on Spanish development data was σ = 0.01, δ = 160. 3.3 http://code.google.com/p/uni-dep-tb/ https://code.google.com/p/ wikily-supervised-pos-tagger/ 8 For our embeddings baselines, we augment the feature space by adding embedding vectors for head h and dependent d. We experimented with different versions of combining embedding vectors, from firing separate h and d per-dimension features (Bansal et al., 2014) to combining their information. We found that combining the embeddings of h and d is effective and consistently use the absolute difference between the embedding vectors, since that worked better than addition and multiplication on development data. Delexicalized transfer (D ELEX) uses three (3) iterations over the data in both the single-source and the multi-source set-up, a parameter set on the Spanish development data. The remaining parameters were obtained by averaging over performance with different embeddings on the Spanish development data, obtaining: σ = 0.005, δ = 20, i = 3, and abso"
P15-1165,P14-1023,0,0.0109756,"proaches focus on different kinds of similarity, some more syntactic, some more semantic. The representations are typically either clusters of distributionally similar words, e.g., Brown et al. (1992), or vector representations. In this paper, we focus on vector representations. In vector-based approaches, similar representations are vectors close in some multi-dimensional space. 2.1 Count-based and prediction-based representations There are, briefly put, two approaches to inducing vector-based distributional word representations from large corpora: count-based and predictionbased approaches (Baroni et al., 2014). Countbased approaches represent words by their cooccurrences. Dimensionality reduction is typically performed on a raw or weighted co-occurrence matrix using methods such as singular value decomposition (SVD), a method for maximizing the variance in a dataset in few dimensions. In our inverted indexing, we use raw co-occurrence data. Prediction-based methods use discriminative learning techniques to learn how to predict words from their context, or vice versa. They rely on a neural network architecture, and once the network converges, they use word representations from a middle layer as thei"
P15-1165,C10-1011,1,0.695318,"ing. For compatibility with Xiao and Guo (2014), we also present results on CoNLL 2006 and 2007 treebanks for languages for which we had baseline and system word representations (de, es, sv). Our parameter settings for these experiments were the same as those tuned on the Spanish development data from the Google Universal Treebanks v. 1.0. Baselines The most obvious baseline in our experiments is delexicalized transfer (D ELEX) (McDonald et al., 2011; Søgaard, 2011). This baseline system simply learns models without lexical features. We use a modified version of the first-order Mate 7 parser (Bohnet, 2010) that also takes continuousvalued embeddings as input an disregards features that include lexical items. 3.4 Word alignment Data We use the manually word-aligned EnglishSpanish Europarl data from Graca et al. (2008). The dataset contains 100 sentences. The annotators annotated whether word alignments were certain or possible, and we present results with all word alignments and with only the certain ones. See Graca et al. (2008) for details. Baselines For word alignment, we simply align every aligned word in the gold data, for which we have a word embedding, to its (Euclidean) nearest neighbor"
P15-1165,J92-4003,0,0.127536,"al and sparse models. Also, simple bagof-words models fail to capture the relatedness of words. In many tasks, synonymous words should be treated alike, but their bag-of-words representations are as different as those of dog and therefore. Distributional word representations are supposed to capture distributional similarities between words. Intuitively, we want similar words to have similar representations. Known approaches focus on different kinds of similarity, some more syntactic, some more semantic. The representations are typically either clusters of distributionally similar words, e.g., Brown et al. (1992), or vector representations. In this paper, we focus on vector representations. In vector-based approaches, similar representations are vectors close in some multi-dimensional space. 2.1 Count-based and prediction-based representations There are, briefly put, two approaches to inducing vector-based distributional word representations from large corpora: count-based and predictionbased approaches (Baroni et al., 2014). Countbased approaches represent words by their cooccurrences. Dimensionality reduction is typically performed on a raw or weighted co-occurrence matrix using methods such as sing"
P15-1165,W02-1001,0,0.0575931,"s well as tag dictionaries (Li et al., 2012) needed for the POS tagging experiments. Baselines One baseline method is a typeconstrained structured perceptron with only ortographic features, which are expected to transfer across languages. The type constraints come from Wiktionary, a crowd-sourced tag dictionary.8 Type constraints from Wiktionary were first used by Li et al. (2012), but note that their set-up is unsupervised learning. T¨ackstr¨om et al. (2013) also used type constraints in a supervised set-up. Our learning algorithm is the structured perceptron algorithm originally proposed by Collins (2002). In our POS tagging experiments, we always do 10 passes over the data. We also present two other baselines, where we augment the feature representation with different embeddings for the target word, K LEMENTIEV and C HANDAR. With all the embeddings in POS tagging, we assign a mean vector to out-of-vocabulary words. System For our system, we simply augment the delexicalized POS tagger with the I NVERTED distributional representation of the current word. The best parameter setting on Spanish development data was σ = 0.01, δ = 160. 3.3 http://code.google.com/p/uni-dep-tb/ https://code.google.com"
P15-1165,N15-1157,1,0.757356,"peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackstr¨om et al., 2013; Xiao and Guo, 2014). In cross-lingual POS tagging, mostly annotation projection has been explored (Fossum and Abney, 2005; Das and Petrov, 2011), since all features in POS tagging models are typically lexical. However, using bilingual word representations was recently explored as an alternative to projectionbased approaches (Gouws and Søgaard, 2015). The major drawback of using bi-lexical representations is that it limits us to using a single source language. T¨ackstr¨om et al. (2013) obtained significant improvements using bilingual word clusters over a single source delexicalized transfer model, for example, but even better results were obtained with delexicalized transfer in McDonald et al. (2011) by simply using several source languages. This paper introduces a simple method for obtaining truly inter-lingual word representations in order to train models with lexical features on several source languages at the same time. Briefly put,"
P15-1165,graca-etal-2008-building,0,0.0195628,"tings for these experiments were the same as those tuned on the Spanish development data from the Google Universal Treebanks v. 1.0. Baselines The most obvious baseline in our experiments is delexicalized transfer (D ELEX) (McDonald et al., 2011; Søgaard, 2011). This baseline system simply learns models without lexical features. We use a modified version of the first-order Mate 7 parser (Bohnet, 2010) that also takes continuousvalued embeddings as input an disregards features that include lexical items. 3.4 Word alignment Data We use the manually word-aligned EnglishSpanish Europarl data from Graca et al. (2008). The dataset contains 100 sentences. The annotators annotated whether word alignments were certain or possible, and we present results with all word alignments and with only the certain ones. See Graca et al. (2008) for details. Baselines For word alignment, we simply align every aligned word in the gold data, for which we have a word embedding, to its (Euclidean) nearest neighbor in the target sentence. We evaluate this strategy by its precision (P@1). System We compare I NVERTED with K LEMEN TIEV and C HANDAR . To ensure a fair comparison, we use the subset of words covered by all three emb"
P15-1165,C12-1089,0,0.772355,"th rely on three level architectures with input, output and a middle layer for intermediate target word representations. The major difference is that skip-gram uses the target word as input and the context as output, whereas the CBOW model does it the other way around. Learning goes by back-propagation, and random target words are used as negative examples. Levy and Goldberg (2014) show that prediction-based representations obtained with the skip-gram model can be related to count-based ones obtained with PMI. They argue that which is best, varies across tasks. 2.1.2 Bilingual representations Klementiev et al. (2012) learn distinct embedding models for the source and target languages, but while learning to minimize the sum of the two models’ losses, they jointly learn a regularizing interaction matrix, enforcing word pairs aligned in parallel text to have similar representations. Note that Klementiev et al. (2012) rely on word-aligned parallel text, and thereby on a large-coverage soft mapping of source words to target words. Other approaches rely on small coverage dictionaries with hard 1:1 mappings between words. Klementiev et al. (2012) do not use skip-gram or CBOW, but the language model presented in"
P15-1165,P14-2050,0,0.00763126,"ed Table 1: Three nearest neighbors in the English training data of six words occurring in the Spanish test data, in the embeddings used in our experiments. Only 2/6 words were in the German data. skip-gram model and CBOW. The two models both rely on three level architectures with input, output and a middle layer for intermediate target word representations. The major difference is that skip-gram uses the target word as input and the context as output, whereas the CBOW model does it the other way around. Learning goes by back-propagation, and random target words are used as negative examples. Levy and Goldberg (2014) show that prediction-based representations obtained with the skip-gram model can be related to count-based ones obtained with PMI. They argue that which is best, varies across tasks. 2.1.2 Bilingual representations Klementiev et al. (2012) learn distinct embedding models for the source and target languages, but while learning to minimize the sum of the two models’ losses, they jointly learn a regularizing interaction matrix, enforcing word pairs aligned in parallel text to have similar representations. Note that Klementiev et al. (2012) rely on word-aligned parallel text, and thereby on a lar"
P15-1165,D12-1127,0,0.0369956,"Missing"
P15-1165,D11-1006,0,0.435697,"the availability of English resources and the availability of parallel data for (and translations between) English and most other languages. In cross-lingual syntactic parsing, for example, two approaches to cross-lingual learning have been explored, namely annotation projection and delexicalized transfer. Annotation projection (Hwa et al., 2005) uses word-alignments in human translations to project predicted sourceside analyses to the target language, producing a noisy syntactically annotated resource for the target language. On the other hand, delexicalized transfer (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011) simply removes lexical features from mono-lingual parsing models, but assumes reliable POS tagging for the target language. Delexicalized transfer works particularly well when resources from several source languages are used for training; learning from multiple other languages prevents over-fitting to the peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackstr¨om et al., 2013; Xiao"
P15-1165,D09-1139,0,0.0521645,"Missing"
P15-1165,P10-1114,0,0.0276803,"on and word alignment, we fix the number of dimensions to 40. For both our baselines and systems, we also tune a scaling factor σ ∈ {1.0, 0.1, 0.01, 0.001} for POS tagging and dependency parsing, using the scaling method from Turian et al. (2010), also used in Gouws and Søgaard (2015). We do not scale our embeddings for document classification or word alignment. 3 Experiments The data set characteristics are found in Table 2.3. 3.1 Document classification Data Our first document classification task is topic classification on the cross-lingual multi-domain sentiment analysis dataset A MAZON in Prettenhofer and Stein (2010).4 We represent each document by the average of the representations of those words that we find both in the documents and in our embeddings. Rather than classifying reviews by sentiment, we classify by topic, trying to discriminate between book reviews, music reviews and DVD reviews, as a three-way classification problem, training on English and testing on German. Unlike in the other tasks below, we always 4 use unscaled word representations, since these are our only features. All word representations have 40 dimensions. The other document classification task is a fourway classification proble"
P15-1165,P11-1061,0,0.039505,"the target language. Delexicalized transfer works particularly well when resources from several source languages are used for training; learning from multiple other languages prevents over-fitting to the peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackstr¨om et al., 2013; Xiao and Guo, 2014). In cross-lingual POS tagging, mostly annotation projection has been explored (Fossum and Abney, 2005; Das and Petrov, 2011), since all features in POS tagging models are typically lexical. However, using bilingual word representations was recently explored as an alternative to projectionbased approaches (Gouws and Søgaard, 2015). The major drawback of using bi-lexical representations is that it limits us to using a single source language. T¨ackstr¨om et al. (2013) obtained significant improvements using bilingual word clusters over a single source delexicalized transfer model, for example, but even better results were obtained with delexicalized transfer in McDonald et al. (2011) by simply using several source lan"
P15-1165,P11-2120,1,0.534532,"lish resources and the availability of parallel data for (and translations between) English and most other languages. In cross-lingual syntactic parsing, for example, two approaches to cross-lingual learning have been explored, namely annotation projection and delexicalized transfer. Annotation projection (Hwa et al., 2005) uses word-alignments in human translations to project predicted sourceside analyses to the target language, producing a noisy syntactically annotated resource for the target language. On the other hand, delexicalized transfer (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011) simply removes lexical features from mono-lingual parsing models, but assumes reliable POS tagging for the target language. Delexicalized transfer works particularly well when resources from several source languages are used for training; learning from multiple other languages prevents over-fitting to the peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackstr¨om et al., 2013; Xiao and Guo, 2014)."
P15-1165,I05-1075,0,0.0207395,"reliable POS tagging for the target language. Delexicalized transfer works particularly well when resources from several source languages are used for training; learning from multiple other languages prevents over-fitting to the peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackstr¨om et al., 2013; Xiao and Guo, 2014). In cross-lingual POS tagging, mostly annotation projection has been explored (Fossum and Abney, 2005; Das and Petrov, 2011), since all features in POS tagging models are typically lexical. However, using bilingual word representations was recently explored as an alternative to projectionbased approaches (Gouws and Søgaard, 2015). The major drawback of using bi-lexical representations is that it limits us to using a single source language. T¨ackstr¨om et al. (2013) obtained significant improvements using bilingual word clusters over a single source delexicalized transfer model, for example, but even better results were obtained with delexicalized transfer in McDonald et al. (2011) by simply u"
P15-1165,Q13-1001,0,0.0450202,"Missing"
P15-1165,P10-1040,0,0.0800173,"NLL 07 – D EPENDENCY PARSING en es de sv 18.6 – – – 447k – – – en es – – – – – 206 357 389 – 5.7k 5.7k 5.7k – 0.841 0.616 n/a E UROPARL – W ORD A LIGNMENT 100 100 – – 0.370 0.533 Table 2: Characteristics of the data sets. Embeddings coverage (token-level) for K LEMENTIEV, C HAN DAR and I NVERTED on the test sets. We use the common vocabulary on W ORD A LIGNMENT . sification and word alignment, we fix the number of dimensions to 40. For both our baselines and systems, we also tune a scaling factor σ ∈ {1.0, 0.1, 0.01, 0.001} for POS tagging and dependency parsing, using the scaling method from Turian et al. (2010), also used in Gouws and Søgaard (2015). We do not scale our embeddings for document classification or word alignment. 3 Experiments The data set characteristics are found in Table 2.3. 3.1 Document classification Data Our first document classification task is topic classification on the cross-lingual multi-domain sentiment analysis dataset A MAZON in Prettenhofer and Stein (2010).4 We represent each document by the average of the representations of those words that we find both in the documents and in our embeddings. Rather than classifying reviews by sentiment, we classify by topic, trying t"
P15-1165,W14-1613,0,0.53136,"2011; Søgaard, 2011) simply removes lexical features from mono-lingual parsing models, but assumes reliable POS tagging for the target language. Delexicalized transfer works particularly well when resources from several source languages are used for training; learning from multiple other languages prevents over-fitting to the peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackstr¨om et al., 2013; Xiao and Guo, 2014). In cross-lingual POS tagging, mostly annotation projection has been explored (Fossum and Abney, 2005; Das and Petrov, 2011), since all features in POS tagging models are typically lexical. However, using bilingual word representations was recently explored as an alternative to projectionbased approaches (Gouws and Søgaard, 2015). The major drawback of using bi-lexical representations is that it limits us to using a single source language. T¨ackstr¨om et al. (2013) obtained significant improvements using bilingual word clusters over a single source delexicalized transfer model, for example, b"
P15-1165,I08-3008,0,0.017072,"asons for this; namely, the availability of English resources and the availability of parallel data for (and translations between) English and most other languages. In cross-lingual syntactic parsing, for example, two approaches to cross-lingual learning have been explored, namely annotation projection and delexicalized transfer. Annotation projection (Hwa et al., 2005) uses word-alignments in human translations to project predicted sourceside analyses to the target language, producing a noisy syntactically annotated resource for the target language. On the other hand, delexicalized transfer (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011) simply removes lexical features from mono-lingual parsing models, but assumes reliable POS tagging for the target language. Delexicalized transfer works particularly well when resources from several source languages are used for training; learning from multiple other languages prevents over-fitting to the peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackst"
Q16-1022,A00-1031,0,0.555773,"observe a major advantage in using reverse-mode alignment for POS projection (4-5 accuracy points absolute).10 In addition, we use the IBM1 aligner efmaral11 by ¨ Ostling (2015). The intuition behind using IBM1 is that IBM2 introduces a bias toward more closely related languages, and we confirm this intuition through our experiments. We modify both aligners so that they output the alignment probability for each aligned token pair. Tagging and parsing The source-sides of the two multi-parallel corpora, EBC and WTC, are POStagged by taggers trained on the respective source languages, using TnT (Brants, 2000). We parse the corpora using TurboParser (Martins et al., 2013). The parser is used in simple arc-factored mode with pruning.12 We alter it to output per-sentence arc 7 http://universaldependencies.org/ format.html 8 https://github.com/coastalcph/ ud-conversion-tools. 9 Parameters used: utf, bisent, cautious, realign. 10 Parameters used: d, o, v, r. 11 Also reverse mode, with default settings, see https:// github.com/robertostling/efmaral. 12 Parameters used: basic. weight matrices.13 4 Experiments Outline For each sentence in a target language corpus, we retrieve the aligned sentences in the"
Q16-1022,P11-1061,0,0.159967,"ning data. In our dependency graph projection, we normalize the weights per sentence. For future development, we note that corpus-level normalization might achieve the same balancing effect while still preserving possibly important language-specific signals regarding structural disambiguations. EBC and WTC constitute a (hopefully small) subset of the publicly available multilingual parallel corpora. The outdated EBC texts can be replaced by newer ones, and the EBC itself replaced or aug310 Related work POS tagging While projection annotation of POS labels goes back to Yarowsky’s seminal work, Das and Petrov (2011) recently renewed interest in this problem. Das and Petrov (2011) go beyond our approach to POS annotation by combining annotation projection and unsupervised learning techniques, but they restrict themselves to Indo-European languages and a coarser tagset. Li et al. (2012) introduce an approach that leverages potentially noisy, but sizeable POS tag dictionaries in the form of Wiktionaries for 9 resource-rich languages. Garrette et al. (2013) also consider the problem of learning POS taggers for truly low-resource languages, but suggest crowdsourcing such POS tag dictionaries. Finally, Agi´c e"
Q16-1022,P13-1057,0,0.0330031,"newer ones, and the EBC itself replaced or aug310 Related work POS tagging While projection annotation of POS labels goes back to Yarowsky’s seminal work, Das and Petrov (2011) recently renewed interest in this problem. Das and Petrov (2011) go beyond our approach to POS annotation by combining annotation projection and unsupervised learning techniques, but they restrict themselves to Indo-European languages and a coarser tagset. Li et al. (2012) introduce an approach that leverages potentially noisy, but sizeable POS tag dictionaries in the form of Wiktionaries for 9 resource-rich languages. Garrette et al. (2013) also consider the problem of learning POS taggers for truly low-resource languages, but suggest crowdsourcing such POS tag dictionaries. Finally, Agi´c et al. (2015) were the first to introduce the idea of learning models for more than a dozen truly low-resource languages in one go, and our contribution can be seen as a non-trivial extension of theirs. Parsing With the exception of Zeman and Resnik (2008), initial work on cross-lingual dependency parsing focused on annotation projection (Hwa et al., 2005; Spreyer et al., 2010). McDonald et al. (2011) and Søgaard (2011) simultaneously took up"
Q16-1022,P16-2091,1,0.868444,"Missing"
Q16-1022,D12-1127,0,0.0917574,"Missing"
Q16-1022,C14-1075,0,0.0462189,"dea of delexicalized transfer after Zeman and Resnik (2008), but more importantly, they also introduced the idea of multi-source cross-lingual transfer in the context of dependency parsing. McDonald et al. (2011) were the first to combine annotation projection and multi-source transfer, the approach taken in this paper. Annotation projection has been explored in the context of cross-lingual dependency parsing since Hwa et al. (2005). Notable approaches include the 18 http://www.ohchr.org/EN/UDHR/Pages/ SearchByLang.aspx 19 http://opus.lingfil.uu.se/ soft projection of reliable dependencies by Li et al. (2014), and the work of Ma and Xia (2014), who make use of the source-side distributions through a training objective function. Tiedemann and Agi´c (2016) provide a more detailed overview of model transfer and annotation projection, while introducing a competitive machine translation-based approach to synthesizing dependency treebanks. In their work, we note the IBM4 word alignments favor more closely related languages, and that building machine translation systems requires parallel data in quantities that far surpass EBC and WTC combined. The best results reported to date were presented by Rasooli"
Q16-1022,P14-1126,0,0.169202,"r Zeman and Resnik (2008), but more importantly, they also introduced the idea of multi-source cross-lingual transfer in the context of dependency parsing. McDonald et al. (2011) were the first to combine annotation projection and multi-source transfer, the approach taken in this paper. Annotation projection has been explored in the context of cross-lingual dependency parsing since Hwa et al. (2005). Notable approaches include the 18 http://www.ohchr.org/EN/UDHR/Pages/ SearchByLang.aspx 19 http://opus.lingfil.uu.se/ soft projection of reliable dependencies by Li et al. (2014), and the work of Ma and Xia (2014), who make use of the source-side distributions through a training objective function. Tiedemann and Agi´c (2016) provide a more detailed overview of model transfer and annotation projection, while introducing a competitive machine translation-based approach to synthesizing dependency treebanks. In their work, we note the IBM4 word alignments favor more closely related languages, and that building machine translation systems requires parallel data in quantities that far surpass EBC and WTC combined. The best results reported to date were presented by Rasooli and Collins (2015). They use the in"
Q16-1022,P13-2109,0,0.0643836,"Missing"
Q16-1022,P05-1012,0,0.0672856,"Missing"
Q16-1022,D11-1006,0,0.675369,"es in the arc projection, but we use unit votes in POS voting. The opposite yields the best IBM2 scores: binarizing the alignment scores in dependency projection, while weight-voting the POS tags. We also evaluated a number of different normalization techniques in projection, only to arrive at standardization and softmax as by far the best choices. Baselines and upper bounds We compare our systems to three competitive baselines, as well as three informed upper bounds or oracles. First, we list our baselines. D ELEX -MS: This is the multi-source direct delexicalized parser transfer baseline of McDonald et al. (2011).15 DCA-P ROJ: This is the direct correspondence assumption (DCA)-based approach to projection, i.e., the de facto standard for projecting dependencies. First introduced by Hwa et al. (2005), it was recently elucidated by Tiedemann (2014), whose implementation we follow here. In contrast to our approach, 15 Referred to as multi-dir in the original paper. DCA projects trees on a source-target sentence pair basis, relying on heuristics and spurious nodes or edges to maintain the tree structure. In the setup, we basically plug DCA into our projection-voting pipeline instead of our own method. R E"
Q16-1022,P13-2017,0,0.0731737,"Missing"
Q16-1022,P15-2034,0,0.0166339,"there are more candidates, we select one through POS ranking.8 Alignment We sentence- and word-align all language pairs in both our multi-parallel corpora. We use hunalign (Varga et al., 2005) to perform conservative sentence alignment.9 The selected sentence pairs then enter word alignment. Here, we use two different aligners. The first one is IBM2 fastalign by Dyer et al. (2013), where we adopt the setup of Agi´c et al. (2015) who observe a major advantage in using reverse-mode alignment for POS projection (4-5 accuracy points absolute).10 In addition, we use the IBM1 aligner efmaral11 by ¨ Ostling (2015). The intuition behind using IBM1 is that IBM2 introduces a bias toward more closely related languages, and we confirm this intuition through our experiments. We modify both aligners so that they output the alignment probability for each aligned token pair. Tagging and parsing The source-sides of the two multi-parallel corpora, EBC and WTC, are POStagged by taggers trained on the respective source languages, using TnT (Brants, 2000). We parse the corpora using TurboParser (Martins et al., 2013). The parser is used in simple arc-factored mode with pruning.12 We alter it to output per-sentence a"
Q16-1022,petrov-etal-2012-universal,0,0.0990105,"S tagging Below, we present results with POS taggers based on annotation projection with both IBM1 and IBM2; cf. Table 3. We train TnT with default settings on the projected annotations. Note that we use the resulting POS taggers in our dependency parsing experiments in order not to have our parsers assume the existence of POS-annotated corpora. For a more extensive assessment, we refer to the work by Agi´c et al. (2015) who report baseline and upper bounds. In contrast to their work, we consider two different alignment models and use the UD POS tagset (17 tags), in contrast to the 12 tags of Petrov et al. (2012). This makes our POS tagging problem slightly more challenging, but our parsing models potentially benefit from the extended tagset.14 Dependency parsing We use arc-factored TurboParser for all parsing models, applying the same setup as in preprocessing. There are three sets of models: our systems, baselines, and upper bounds. 13 Our fork of TurboParser is available from https:// github.com/andersjo/TurboParser. 14 For example, the AUX vs. VERB distinction from UD POS does not exist the tagset of Petrov et al. (2012), and neither does NOUN vs. PROPN (proper noun). 307 Our systems are trained o"
Q16-1022,D15-1039,0,0.358424,"(73m), Hausa (50m), and Kurdish (30m). Cross-lingual transfer learning—or simply cross-lingual learning—refers to work on using annotated resources in other (source) languages to induce models for such low-resource (target) languages. Even simple cross-lingual learning techniques outperform unsupervised grammar induction by a large margin. Most work in cross-lingual learning, however, makes assumptions about the availability of linguistic resources that do not hold for the majority of low-resource languages. The best cross-lingual dependency parsing results reported to date were presented by Rasooli and Collins (2015). They use the intersection of languages covered in the Google dependency treebanks project and those contained in the Europarl corpus. Consequently, they only consider closely related Indo-European languages for which high-quality tokenization can be obtained with simple heuristics. In other words, we argue that recent approaches to cross-lingual POS tagging and dependency parsing are biased toward Indo-European languages, in particular the Germanic and Romance families. The bias is not hard to explain: treebanks, as well as large volumes of parallel data, are readily available for many Germa"
Q16-1022,P15-2040,0,0.143668,"Missing"
Q16-1022,N06-2033,0,0.0734653,"entation we follow here. In contrast to our approach, 15 Referred to as multi-dir in the original paper. DCA projects trees on a source-target sentence pair basis, relying on heuristics and spurious nodes or edges to maintain the tree structure. In the setup, we basically plug DCA into our projection-voting pipeline instead of our own method. R EPARSE: For this baseline, we parse a target sentence using multiple single-source delexicalized parsers. Then, we collect the output trees in a graph, unit-voting the individual edge weights, and finally using DMST to compute the best dependency tree (Sagae and Lavie, 2006). Now, we explain the three upper bounds: D ELEX -SB: This result is using the best singlesource delexicalized system for a given target language following McDonald et al. (2013). We parse a target with multiple single-source delexicalized parsers, and select the best-performing one. S ELF -T RAIN: For this result we parse the targetlanguage EBC and WTC data, train parsers on the output predictions, and evaluate the resulting parsers on the evaluation data. Note this result is available only for the source languages. Also, note that while we refer to this as self-training, we do not concatenat"
Q16-1022,spreyer-etal-2010-training,0,0.0207293,"naries in the form of Wiktionaries for 9 resource-rich languages. Garrette et al. (2013) also consider the problem of learning POS taggers for truly low-resource languages, but suggest crowdsourcing such POS tag dictionaries. Finally, Agi´c et al. (2015) were the first to introduce the idea of learning models for more than a dozen truly low-resource languages in one go, and our contribution can be seen as a non-trivial extension of theirs. Parsing With the exception of Zeman and Resnik (2008), initial work on cross-lingual dependency parsing focused on annotation projection (Hwa et al., 2005; Spreyer et al., 2010). McDonald et al. (2011) and Søgaard (2011) simultaneously took up the idea of delexicalized transfer after Zeman and Resnik (2008), but more importantly, they also introduced the idea of multi-source cross-lingual transfer in the context of dependency parsing. McDonald et al. (2011) were the first to combine annotation projection and multi-source transfer, the approach taken in this paper. Annotation projection has been explored in the context of cross-lingual dependency parsing since Hwa et al. (2005). Notable approaches include the 18 http://www.ohchr.org/EN/UDHR/Pages/ SearchByLang.aspx 19"
Q16-1022,P11-2120,1,0.889189,"rich languages. Garrette et al. (2013) also consider the problem of learning POS taggers for truly low-resource languages, but suggest crowdsourcing such POS tag dictionaries. Finally, Agi´c et al. (2015) were the first to introduce the idea of learning models for more than a dozen truly low-resource languages in one go, and our contribution can be seen as a non-trivial extension of theirs. Parsing With the exception of Zeman and Resnik (2008), initial work on cross-lingual dependency parsing focused on annotation projection (Hwa et al., 2005; Spreyer et al., 2010). McDonald et al. (2011) and Søgaard (2011) simultaneously took up the idea of delexicalized transfer after Zeman and Resnik (2008), but more importantly, they also introduced the idea of multi-source cross-lingual transfer in the context of dependency parsing. McDonald et al. (2011) were the first to combine annotation projection and multi-source transfer, the approach taken in this paper. Annotation projection has been explored in the context of cross-lingual dependency parsing since Hwa et al. (2005). Notable approaches include the 18 http://www.ohchr.org/EN/UDHR/Pages/ SearchByLang.aspx 19 http://opus.lingfil.uu.se/ soft projection"
Q16-1022,W14-1614,1,0.84878,"Missing"
Q16-1022,tiedemann-2012-parallel,0,0.0365742,"Missing"
Q16-1022,C14-1175,0,0.211283,"ncy such that (ut , vt ) becomes a dependency edge in the target sentence, making the a dependent of word. Obviously, dependency annotation projection is more challenging than projecting POS, as there is a structural constraint: the projected edges must form a dependency tree on the target side. Hwa et al. (2005) were the first to consider this problem, applying heuristics to ensure well-formed trees on the target side. The heuristics were not perfect, as they have been shown to result in excessive non-projectivity and the introduction of spurious relations and tokens (Tiedemann et al., 2014; Tiedemann, 2014). These design choices all lead to di1 https://bitbucket.org/lowlands/release Figure 1: An outline of dependency annotation projection, voting, and decoding in our method, using two sources i (German) and j (Croatian) and a target t (English). Part 1 represents the multi-parallel corpus preprocessing, while parts 2 and 3 relate to our projection method. The graphs are represented as adjacency matrices with column indices encoding dependency heads. We highlight how the weight of target edge (ut = was, vt = beginning) is computed from the two contributing sources. minished parsing quality. We in"
Q16-1022,H01-1035,0,0.430375,"ts weight matrices from multiple sources, rather than dependency trees or individual dependencies from a single source. (iii) We show that our approach performs significantly better than commonly used heuristics for annotation projection, as well as than delexicalized transfer baselines. Moreover, in comparison to these systems, our approach performs particularly well on truly low-resource non-Indo-European languages. 302 All code and data are made freely available for general use.1 2 Weighted annotation projection Motivation Our approach is based on the general idea of annotation projection (Yarowsky et al., 2001) using parallel sentences. The goal is to augment an unannotated target sentence with syntactic annotations projected from one or more source sentences through word alignments. The principle is illustrated in Figure 1, where the source languages are German and Croatian, and the target is English. The simplest case is projecting POS labels, which are observed in the source sentences but unknown in the target language. In order to induce the grammatical category of the target word beginning, we project POS from the aligned words Anfang and poˇcetku, both of which are correctly annotated as N OUN"
Q16-1022,I08-3008,0,0.081872,"ges and a coarser tagset. Li et al. (2012) introduce an approach that leverages potentially noisy, but sizeable POS tag dictionaries in the form of Wiktionaries for 9 resource-rich languages. Garrette et al. (2013) also consider the problem of learning POS taggers for truly low-resource languages, but suggest crowdsourcing such POS tag dictionaries. Finally, Agi´c et al. (2015) were the first to introduce the idea of learning models for more than a dozen truly low-resource languages in one go, and our contribution can be seen as a non-trivial extension of theirs. Parsing With the exception of Zeman and Resnik (2008), initial work on cross-lingual dependency parsing focused on annotation projection (Hwa et al., 2005; Spreyer et al., 2010). McDonald et al. (2011) and Søgaard (2011) simultaneously took up the idea of delexicalized transfer after Zeman and Resnik (2008), but more importantly, they also introduced the idea of multi-source cross-lingual transfer in the context of dependency parsing. McDonald et al. (2011) were the first to combine annotation projection and multi-source transfer, the approach taken in this paper. Annotation projection has been explored in the context of cross-lingual dependency"
Q16-1022,P15-2044,1,\N,Missing
Q16-1022,N13-1073,0,\N,Missing
S14-1001,P05-3014,0,0.0381676,"se (MFS) of a word has been proven to be a strong baseline. Following this, our MFS baseline simply predicts the supersense of the most frequent WordNet sense for a tuple of a word and a part of speech. We use the part of speech predicted by the L APOS tagger (Tsuruoka et al., 2011). Any word not in WordNet is labeled as noun.person, which is the most frequent sense overall in the training data. After tagging, we run a script to correct the BI tag prefixes, as described above for the annotation ask. We also compare to the performance of existing SST systems. In particular we use SenseLearner (Mihalcea and Csomai, 2005) as a baseline, which produces estimates of the WordNet sense for each word. For these predictions, we retrieve the corresponding supersense. Finally, we use a publicly available reimplementation of Ciaramita and Altun (2006) by Michael Heilman, which reaches comparable performance on goldtagged S EM C OR.3 3.3 4 Results The results are presented in Table 2. We distinguish between three settings with various degrees of supervision: weakly supervised, which uses no domain annotated information, but solely relies on embeddings trained on unlabeled Twitter data; unsupervised domain adaptation (DA"
S14-1001,C12-1028,0,0.0175268,"rds. 8 Yuret and Yatbaz (2010) present a weakly unsupervised approach to this problem, still evaluating on S ENS E VAL -2 and S ENS E VAL -3. They focus only on nouns, relying on gold part-of-speech, but also experiment with a coarse-grained mapping, using only three high level classes. For Twitter, we are aware of little previous work on word sense disambiguation. Gella et al. (2014) present lexical sample word sense disambiguation annotation of 20 target nouns on Twitter, but no experimental results with this data. There has also been related work on disambiguation to Wikipedia for Twitter (Cassidy et al., 2012). In sum, existing work on supersense tagging and coarse-grained word sense disambiguation for English has to the best of our knowledge all focused on newswire and literature. Moreover, they all rely on gold standard POS information, making previous performance estimates rather optimistic. still predicted the correct verb.cognition as supersense. 6 Related Work There has been relatively little previous work on supersense tagging, and to the best of our knowledge, all of it has been limited to English newswire and literature (S EM C OR and S ENS E VAL). The task of supersense tagging was first"
S14-1001,W06-1670,0,0.762458,"aptation (here, from newswire to Twitter). In We present two Twitter datasets annotated with coarse-grained word senses (supersenses), as well as a series of experiments with three learning scenarios for supersense tagging: weakly supervised learning, as well as unsupervised and supervised domain adaptation. We show that (a) off-the-shelf tools perform poorly on Twitter, (b) models augmented with embeddings learned from Twitter data perform much better, and (c) errors can be reduced using type-constrained inference with distant supervision from WordNet. 1 Introduction Supersense tagging (SST, Ciaramita and Altun, 2006) is the task of assigning high-level ontological classes to open-class words (here, nouns and verbs). It is thus a coarse-grained word sense disambiguation task. The labels are based on the lexicographer file names for Princeton WordNet (Fellbaum, 1998). They include 15 senses for verbs and 26 for nouns (see Table 1). While WordNet also provides catch-all supersenses for adjectives and adverbs, these are grammatically, not semantically motivated, and do not provide any higherlevel abstraction (recently, however, Tsvetkov et al. (2014) proposed a semantic taxonomy for adjectives). They will not"
S14-1001,H94-1046,0,0.25003,"simply express the opinions of the author on some subject matter. Supersense tagging is relevant for Twitter, because it can aid e.g. QA and open RE. If someone posts a message saying that some LaTeX module now supports “drawing trees”, it is important to know whether the post is about drawing natural objects such as oaks or pines, or about drawing tree-shaped data representations. This paper is, to the best of our knowledge, the first work to address the problem of SST for Twitter. While there exist corpora of newswire and literary texts that are annotated with supersenses, e.g., S EM C OR (Miller et al., 1994), no data is available for microblogs or related domains. This paper introduces two new data sets. Furthermore, most, if not all, of previous work on SST has relied on gold standard part-of-speech (POS) tags as input. However, in a domain such as Twitter, which has proven to be challenging for POS tagging (Foster et al., 2011; Ritter et al., 2011), results obtained under the assumption of available perfect POS information are almost meaningless for any real-life application. In this paper, we instead use predicted POS tags and investigate experimental settings in which one or more of the follo"
S14-1001,W02-1001,0,0.0447847,"ocial v.stative v.weather Table 1: The 41 noun and verb supersenses in WordNet Finally, we annotated data sets for Twitter, making supervised domain adaptation (SU) experiments possible. For supervised domain adaptation, we use the annotated training data sets from both the newswire and the Twitter domain, as well as WordNet. 2 More or less supervised models This sections covers the varying degree of supervision of our systems as well as the usage of type constraints as distant supervision. For both unsupervised domain adaptation and supervised domain adaptation, we use structured perceptron (Collins, 2002), i.e., a discriminative HMM model, and search-based structured prediction (S EARN) (Daume et al., 2009). We augment both the EM-trained HMM2, discriminative HMMs and S EARN with type constraints and continuous word representations. We also experimented with conditional random fields (Lafferty et al., 2001), but obtained worse or similar results than with the other models. 2.1 Distant supervision Distant supervision in these experiments was implemented by only allowing a system to predict a certain supersense for a given word if that supersense had either been observed in the training data, or"
S14-1001,R13-1026,0,0.0254562,"aramita and Altun (2006).1 S EARN performed slightly better than structured perceptron, so we use it as our inhouse baseline in the experiments below. In this section, we briefly explain the two approaches. 3 Experiments We experiment with weakly supervised learning, unsupervised domain adaptation, as well as supervised domain adaptation, i.e., where our models are induced from hand-annotated newswire and Twitter data. Note that in all our experiments, 1 https://github.com/coastalcph/ rungsted 2 3 http://hunch.net/˜vw/ use for training, development and evaluation, using the splits proposed in Derczynski et al. (2013), and (b) supersense annotations for a sample of 200 tweets, which we use for additional, out-of-sample evaluation. We call these data sets R ITTER{T RAIN ,D EV,E VAL} and I N -H OUSE -E VAL, respectively. The I N -H OUSE -E VAL dataset was downloaded in 2013 and is a sample of tweets that contain links to external homepages but are otherwise unbiased. It was previously used (with partof-speech annotation) in (Plank et al., 2014). Both data sets are made publicly available with this paper. Supersenses are annotated with in spans defined by the BIO (Begin-Inside-Other) notation. To obtain the T"
S14-1001,E14-1078,1,0.832545,"l our experiments, 1 https://github.com/coastalcph/ rungsted 2 3 http://hunch.net/˜vw/ use for training, development and evaluation, using the splits proposed in Derczynski et al. (2013), and (b) supersense annotations for a sample of 200 tweets, which we use for additional, out-of-sample evaluation. We call these data sets R ITTER{T RAIN ,D EV,E VAL} and I N -H OUSE -E VAL, respectively. The I N -H OUSE -E VAL dataset was downloaded in 2013 and is a sample of tweets that contain links to external homepages but are otherwise unbiased. It was previously used (with partof-speech annotation) in (Plank et al., 2014). Both data sets are made publicly available with this paper. Supersenses are annotated with in spans defined by the BIO (Begin-Inside-Other) notation. To obtain the Twitter data sets, we carried out an annotation task. We first pre-annotated all data sets with WordNet’s most frequent senses. If the word was not in WordNet and a noun, we assigned it the sense n.person. All other words were labeled O. Chains of nouns were altered to give every element the sense of the head noun, and the BI tags adjusted, i.e.: we use predicted POS tags as input to the system, in order to produce a realistic est"
S14-1001,I11-1100,0,0.0237767,"Missing"
S14-1001,D11-1141,0,0.1048,"ng tree-shaped data representations. This paper is, to the best of our knowledge, the first work to address the problem of SST for Twitter. While there exist corpora of newswire and literary texts that are annotated with supersenses, e.g., S EM C OR (Miller et al., 1994), no data is available for microblogs or related domains. This paper introduces two new data sets. Furthermore, most, if not all, of previous work on SST has relied on gold standard part-of-speech (POS) tags as input. However, in a domain such as Twitter, which has proven to be challenging for POS tagging (Foster et al., 2011; Ritter et al., 2011), results obtained under the assumption of available perfect POS information are almost meaningless for any real-life application. In this paper, we instead use predicted POS tags and investigate experimental settings in which one or more of the following resources are available to us: • a large corpus of unlabeled Twitter data; • Princeton WordNet (Fellbaum, 1998); • S EM C OR (Miller et al., 1994); and • a small corpus of Twitter data annotated with supersenses. We approach SST of Twitter using various degrees of supervision for both learning and domain adaptation (here, from newswire to Twi"
S14-1001,E14-4042,0,0.0248191,"tiv e .pers on noun .grou p noun .artif noun a ct .com muni catio n noun .even t noun .loca tion noun .time noun Figure 2: Inter-annotator confusion matrix on T WITTER -E VAL. 0.4 0.3 0.2 0.1 0 Figure 3: Sense distribution of OOV words. 8 Yuret and Yatbaz (2010) present a weakly unsupervised approach to this problem, still evaluating on S ENS E VAL -2 and S ENS E VAL -3. They focus only on nouns, relying on gold part-of-speech, but also experiment with a coarse-grained mapping, using only three high level classes. For Twitter, we are aware of little previous work on word sense disambiguation. Gella et al. (2014) present lexical sample word sense disambiguation annotation of 20 target nouns on Twitter, but no experimental results with this data. There has also been related work on disambiguation to Wikipedia for Twitter (Cassidy et al., 2012). In sum, existing work on supersense tagging and coarse-grained word sense disambiguation for English has to the best of our knowledge all focused on newswire and literature. Moreover, they all rely on gold standard POS information, making previous performance estimates rather optimistic. still predicted the correct verb.cognition as supersense. 6 Related Work Th"
S14-1001,P12-2050,0,0.379949,"rovide information similar to higher-level distributional clusters, but more interpretable, and have thus been used as highlevel features in various tasks, such as preposition sense disambiguation, noun compound interpretation, and metaphor detection (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013). Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns, but Tsvetkov et al. (2014) have recently proposed an extension of the taxonomy to cover adjectives. Outside of English, supersenses have been annotated for Arabic Wikipedia articles by Schneider et al. (2012). In addition, a few researchers have tried to solve coarse-grained word sense disambiguation problems that are very similar to supersense tagging. Kohomban and Lee (2005) and Kohomban and Lee (2007) also propose to use lexicographer file identifers from Princeton WordNet senses (supersenses) and, in addition, discuss how to retrieve fine-grained senses from those predictions. They evaluate their model on all-words data from S ENSE E VAL -2 and S ENSE E VAL -3. They use a classification approach rather than structured prediction. 7 Conclusion In this paper, we present two Twitter data sets wit"
S14-1001,P05-1005,0,0.0587046,"tion sense disambiguation, noun compound interpretation, and metaphor detection (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013). Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns, but Tsvetkov et al. (2014) have recently proposed an extension of the taxonomy to cover adjectives. Outside of English, supersenses have been annotated for Arabic Wikipedia articles by Schneider et al. (2012). In addition, a few researchers have tried to solve coarse-grained word sense disambiguation problems that are very similar to supersense tagging. Kohomban and Lee (2005) and Kohomban and Lee (2007) also propose to use lexicographer file identifers from Princeton WordNet senses (supersenses) and, in addition, discuss how to retrieve fine-grained senses from those predictions. They evaluate their model on all-words data from S ENSE E VAL -2 and S ENSE E VAL -3. They use a classification approach rather than structured prediction. 7 Conclusion In this paper, we present two Twitter data sets with manually annotated supersenses, as well as a series of experiments with these data sets. The data is publicly available for download. In this article we have provided, t"
S14-1001,P99-1023,0,0.187848,"Missing"
S14-1001,S10-1049,0,0.0509089,"nd Paaß, 2008; Paaß and Reichartz, 2009) extended this work, using a CRF model as well as LDA topic features. They report an F1 score of 80.2, again relying on gold standard POS features. Our implementation follows their setup and feature model, but we rely on predicted POS features, not gold standard features. Supersenses provide information similar to higher-level distributional clusters, but more interpretable, and have thus been used as highlevel features in various tasks, such as preposition sense disambiguation, noun compound interpretation, and metaphor detection (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013). Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns, but Tsvetkov et al. (2014) have recently proposed an extension of the taxonomy to cover adjectives. Outside of English, supersenses have been annotated for Arabic Wikipedia articles by Schneider et al. (2012). In addition, a few researchers have tried to solve coarse-grained word sense disambiguation problems that are very similar to supersense tagging. Kohomban and Lee (2005) and Kohomban and Lee (2007) also propose to use lexicographer file identifers from Princeton WordNe"
S14-1001,D12-1127,0,0.0563414,"Missing"
S14-1001,W11-0328,0,0.0190122,"er k on development data for using the k-most frequent senses inWordNet as type constraints. Our supervised models are trained on S EM C OR +R ITTER -T RAIN or simply R ITTER -T RAIN, depending on what gave us the best performance on the held-out data. Baselines For most word sense disambiguation studies, predicting the most frequent sense (MFS) of a word has been proven to be a strong baseline. Following this, our MFS baseline simply predicts the supersense of the most frequent WordNet sense for a tuple of a word and a part of speech. We use the part of speech predicted by the L APOS tagger (Tsuruoka et al., 2011). Any word not in WordNet is labeled as noun.person, which is the most frequent sense overall in the training data. After tagging, we run a script to correct the BI tag prefixes, as described above for the annotation ask. We also compare to the performance of existing SST systems. In particular we use SenseLearner (Mihalcea and Csomai, 2005) as a baseline, which produces estimates of the WordNet sense for each word. For these predictions, we retrieve the corresponding supersense. Finally, we use a publicly available reimplementation of Ciaramita and Altun (2006) by Michael Heilman, which reach"
S14-1001,W13-0906,0,0.0402627,"d Reichartz, 2009) extended this work, using a CRF model as well as LDA topic features. They report an F1 score of 80.2, again relying on gold standard POS features. Our implementation follows their setup and feature model, but we rely on predicted POS features, not gold standard features. Supersenses provide information similar to higher-level distributional clusters, but more interpretable, and have thus been used as highlevel features in various tasks, such as preposition sense disambiguation, noun compound interpretation, and metaphor detection (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013). Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns, but Tsvetkov et al. (2014) have recently proposed an extension of the taxonomy to cover adjectives. Outside of English, supersenses have been annotated for Arabic Wikipedia articles by Schneider et al. (2012). In addition, a few researchers have tried to solve coarse-grained word sense disambiguation problems that are very similar to supersense tagging. Kohomban and Lee (2005) and Kohomban and Lee (2007) also propose to use lexicographer file identifers from Princeton WordNet senses (supersenses) a"
S14-1001,tsvetkov-etal-2014-augmenting-english,1,0.22216,"n from WordNet. 1 Introduction Supersense tagging (SST, Ciaramita and Altun, 2006) is the task of assigning high-level ontological classes to open-class words (here, nouns and verbs). It is thus a coarse-grained word sense disambiguation task. The labels are based on the lexicographer file names for Princeton WordNet (Fellbaum, 1998). They include 15 senses for verbs and 26 for nouns (see Table 1). While WordNet also provides catch-all supersenses for adjectives and adverbs, these are grammatically, not semantically motivated, and do not provide any higherlevel abstraction (recently, however, Tsvetkov et al. (2014) proposed a semantic taxonomy for adjectives). They will not be considered in this paper. Coarse-grained categories such as supersenses are useful for downstream tasks such as questionanswering (QA) and open relation extraction (RE). SST is different from NER in that it has a larger set of labels and in the absence of strong orthographic cues (capitalization, quotation marks, etc.). Moreover, supersenses can be applied to any of the lexical parts of speech and not only proper names. Also, while high-coverage gazetteers can be found for named entity recognition, the lexical resources available"
S14-1001,S07-1051,0,0.166033,"and Paaß (Reichartz and Paaß, 2008; Paaß and Reichartz, 2009) extended this work, using a CRF model as well as LDA topic features. They report an F1 score of 80.2, again relying on gold standard POS features. Our implementation follows their setup and feature model, but we rely on predicted POS features, not gold standard features. Supersenses provide information similar to higher-level distributional clusters, but more interpretable, and have thus been used as highlevel features in various tasks, such as preposition sense disambiguation, noun compound interpretation, and metaphor detection (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013). Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns, but Tsvetkov et al. (2014) have recently proposed an extension of the taxonomy to cover adjectives. Outside of English, supersenses have been annotated for Arabic Wikipedia articles by Schneider et al. (2012). In addition, a few researchers have tried to solve coarse-grained word sense disambiguation problems that are very similar to supersense tagging. Kohomban and Lee (2005) and Kohomban and Lee (2007) also propose to use lexicographer file identifers"
S14-1001,J10-1004,0,0.0158174,"ystems (DA) S EARN (Baseline) S EARN S EARN HMM HMM + + + + + - Supervised domain adaptation systems (SU) S EARN (Baseline) S EARN S EARN HMM HMM + + + + + + + + + + Table 2: Weighted F1 average over 41 supersenses. 7 noun .act noun .food noun .attri bute noun .relat ion verb. cogn ition verb. creat ion verb. emot ion verb. moti on verb. perce ption verb. stativ e .pers on noun .grou p noun .artif noun a ct .com muni catio n noun .even t noun .loca tion noun .time noun Figure 2: Inter-annotator confusion matrix on T WITTER -E VAL. 0.4 0.3 0.2 0.1 0 Figure 3: Sense distribution of OOV words. 8 Yuret and Yatbaz (2010) present a weakly unsupervised approach to this problem, still evaluating on S ENS E VAL -2 and S ENS E VAL -3. They focus only on nouns, relying on gold part-of-speech, but also experiment with a coarse-grained mapping, using only three high level classes. For Twitter, we are aware of little previous work on word sense disambiguation. Gella et al. (2014) present lexical sample word sense disambiguation annotation of 20 target nouns on Twitter, but no experimental results with this data. There has also been related work on disambiguation to Wikipedia for Twitter (Cassidy et al., 2012). In sum,"
S14-2034,W09-1206,0,0.114151,"Missing"
S14-2034,C10-1011,0,0.164674,"Missing"
S14-2034,J93-2004,0,0.0469115,"Missing"
S14-2034,D07-1111,0,0.0867606,"Missing"
S14-2034,C08-1095,0,0.112956,"Missing"
S15-2118,N13-1037,0,0.0128538,"ata, label: -1.24; GSA prediction: +5. does not result always in the exact opposite sentiment and therefore it is not as simple as just inverting the scores from a general SA system. Only few studies have attempted SA on figurative language so far (Reyes and Rosso, 2012; Reyes et al., 2013). The prediction of a fine-grained sentiment score (between -5 and 5) for a tweet poses a series of challenges. First of all, accurate language technology on tweets is hard due to sample bias, i.e., collections of tweets are inherently biased towards the particular time (or way, cf. §2) they were collected (Eisenstein, 2013; Hovy et al., 2014). Secondly, the notion of figurativeness (or its complementary notion of literality) does not have a strong definition, let alone do irony, sarcasm, or satire. As pointed out by Reyes and Rosso (2012), “there is not a clear distinction about the boundaries among these terms”. Yet alone attaching a fine-grained score is far from straightforward. In fact, the gold standard consists of the average score assigned by humans through crowdsourcing reflecting an uncertainty in ground truth. 2 Data Analysis The goal of the initial data exploration was to investigate the amount of no"
S15-2118,W14-2602,1,0.832592,"−5.0 −2.5 0.0 2.5 5.0 gold Figure 1: Label Plots for RR predictions. distributions of the gold scores and GSA predictions for the trial data. It shows that the gold distribution is skewed with regards to the number of negative instances to positives, while the GSA predicts more positive sentiment. Support 135 22 29 8 3 92 Table 2: Tweet Label Type and Expression. The Effect of a General Sentiment System The data for this task is very different from data that most lexicon-based or general sentiment-analysis models fare best on. In fact, running a general sentiment classifier (GSA) described in Elming et al. (2014) on the trial data showed that its predictions are actually slightly anti-correlated with the gold standard scores for the Tweets in this task (cosine similarity score of -0.08 and MSE of 18.62). We exploited these anti-correlated results as features for our stacking systems (cf. § 3.2). Figure 2 shows the 700 Figure 2: Distribution of Gold Scores and GSA Predictions for Trial Data. 3 System Description We approach the task (Ghosh et al., 2015) as a regression task (cf. §4.4), combining several systems using stacking (§ 3.2), and relying on features without POS, lemma or explicit use of lexico"
S15-2118,S15-2080,0,0.0227948,"rom data that most lexicon-based or general sentiment-analysis models fare best on. In fact, running a general sentiment classifier (GSA) described in Elming et al. (2014) on the trial data showed that its predictions are actually slightly anti-correlated with the gold standard scores for the Tweets in this task (cosine similarity score of -0.08 and MSE of 18.62). We exploited these anti-correlated results as features for our stacking systems (cf. § 3.2). Figure 2 shows the 700 Figure 2: Distribution of Gold Scores and GSA Predictions for Trial Data. 3 System Description We approach the task (Ghosh et al., 2015) as a regression task (cf. §4.4), combining several systems using stacking (§ 3.2), and relying on features without POS, lemma or explicit use of lexicons, cf. § 3.3. 3.1 Single Systems 3.3 Ridge Regression (RR) A standard supervised ridge regression model with default parameters.3 PCA GMM Ridge Regression (GMM) A ridge regression model trained on the output of unsupervised induced features, i.e., a Gaussian Mixture Models (GMM) trained on PCA of word n-grams. PCA was used to reduce the dimensionality to 100, and GMM under the assumption that the data was sampled from different distributions o"
S15-2118,hovy-etal-2014-pos,1,0.896031,"GSA prediction: +5. does not result always in the exact opposite sentiment and therefore it is not as simple as just inverting the scores from a general SA system. Only few studies have attempted SA on figurative language so far (Reyes and Rosso, 2012; Reyes et al., 2013). The prediction of a fine-grained sentiment score (between -5 and 5) for a tweet poses a series of challenges. First of all, accurate language technology on tweets is hard due to sample bias, i.e., collections of tweets are inherently biased towards the particular time (or way, cf. §2) they were collected (Eisenstein, 2013; Hovy et al., 2014). Secondly, the notion of figurativeness (or its complementary notion of literality) does not have a strong definition, let alone do irony, sarcasm, or satire. As pointed out by Reyes and Rosso (2012), “there is not a clear distinction about the boundaries among these terms”. Yet alone attaching a fine-grained score is far from straightforward. In fact, the gold standard consists of the average score assigned by humans through crowdsourcing reflecting an uncertainty in ground truth. 2 Data Analysis The goal of the initial data exploration was to investigate the amount of non-figurativeness in"
S15-2118,W12-0607,0,0.115298,"Missing"
S16-1160,J92-4003,0,0.135761,"dia, and their ratio. C HAR C OMPLEXITY The feature group contains the character-level unigram and bigram probability of the word based on frequencies in Wikipedia and Simple Wikipedia, as well as the respective ratios. This is motivated by the observation that “words with simple graphemeto-phoneme ratios [are] easier to learn than more phonetically complex words” (Dela Rosa and Eskenazi, 2011). Finally, this group includes the share of vowels in the word. B ROWN This feature group includes the Brown cluster of w, as well as height and depth of the cluster in the hierarchical clustering tree (Brown et al., 1992). We use the default 1000 1030 clusters generated by Percy Liang’s implementation.2 8. E MBS This feature group contains the 300dimensional GloVe embeddings of w, calculated using Word2Vec over a Wikipedia dump.3 9. W ORD N ET This feature reflects the semantic complexity of w as measured by its number of WordNet synsets (Fellbaum, 1998). 5 Our systems System 1: NeuralNet. We train a deep neural network with 2 hidden layers of 150 and 50 units, respectively, using PyCnn.4 At every hidden layer, we perform L2-regularisation. The network has a single output unit that yields a value between 0 and"
S16-1160,P14-2075,0,0.0130778,"nt of training data. However, we show in posthoc experiments how careful network design may lead to performance figures close to those of the task-winning system. We make our revised system publicly available.1 1 https://github.com/jbingel/cwi2016 1028 Proceedings of SemEval-2016, pages 1028–1033, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics 2 Related work The identification of complex words in context has in the past been embedded, often implicitly, into broader (lexical) simplification endeavours (Yatskar et al., 2010; Medero and Ostendorf, 2011; Horn et al., 2014). These models usually employ lexicons or corpus frequencies to determine candidates for lexical substitution. In a corpus study of the standard/Simple English Wikipedia, Medero and Ostendorf (2009) identify a number of word-level features that are indicative of texts with more difficult vocabulary. One of their findings is that words that are typical of simple texts tend to have longer definitions and more user-entered translations in Wiktionary. They also tend to be more ambiguous with respect to word class membership. Research explicitly dedicated to complex word identification in context,"
S16-1160,P14-5010,0,0.00549836,"Missing"
S16-1160,W13-4813,0,0.0155231,"and extensive posttask experiments. Those revealed that despite poor results in the task, our neural network approach is competitive with the systems achieving the best results. The central contribution of this paper is therefore a demonstration of the aptitude of deep neural networks for the task of identifying complex words. 1 Introduction The identification of complex words plays an important role in the development of simplified reading resources. In particular, accurate automatic complex word identification strongly benefits lexical simplification (LS) as a first step in an LS pipeline (Paetzold and Specia, 2013; Shardlow, 2014). As such, accurate complex word identification may also be critical to higher-level tasks in text simplification, e.g. sentence compression, where the overall readability of In this system description paper, we focus specifically on our neural network model. As noted in the task description paper (Paetzold and Specia, 2016), systems based on neural networks generally performed rather weakly in the task, which the organisers speculate to be a consequence of the small amount of training data. However, we show in posthoc experiments how careful network design may lead to perform"
S16-1160,P13-3015,0,0.0502534,"s or corpus frequencies to determine candidates for lexical substitution. In a corpus study of the standard/Simple English Wikipedia, Medero and Ostendorf (2009) identify a number of word-level features that are indicative of texts with more difficult vocabulary. One of their findings is that words that are typical of simple texts tend to have longer definitions and more user-entered translations in Wiktionary. They also tend to be more ambiguous with respect to word class membership. Research explicitly dedicated to complex word identification in context, however, has only appeared recently. Shardlow (2013a) presents experiments based on his complex word dataset mined from edit histories in Simple Wikipedia (Shardlow, 2013b); his classification system uses an SVM over a small number of features, achieving an F -score above 0.8 on the named dataset, where one word per sentence is a positive instance. 3 Task data The data for the task was collected in a survey with 400 non-native speakers of English, such that it may in particular serve the identification of words that pose problems to language learners. The organisers chose to have each of the 200 sentences in the training set annotated by 20 in"
S16-1160,W13-2908,0,0.0224524,"s or corpus frequencies to determine candidates for lexical substitution. In a corpus study of the standard/Simple English Wikipedia, Medero and Ostendorf (2009) identify a number of word-level features that are indicative of texts with more difficult vocabulary. One of their findings is that words that are typical of simple texts tend to have longer definitions and more user-entered translations in Wiktionary. They also tend to be more ambiguous with respect to word class membership. Research explicitly dedicated to complex word identification in context, however, has only appeared recently. Shardlow (2013a) presents experiments based on his complex word dataset mined from edit histories in Simple Wikipedia (Shardlow, 2013b); his classification system uses an SVM over a small number of features, achieving an F -score above 0.8 on the named dataset, where one word per sentence is a positive instance. 3 Task data The data for the task was collected in a survey with 400 non-native speakers of English, such that it may in particular serve the identification of words that pose problems to language learners. The organisers chose to have each of the 200 sentences in the training set annotated by 20 in"
S16-1160,shardlow-2014-open,0,0.0317606,"eriments. Those revealed that despite poor results in the task, our neural network approach is competitive with the systems achieving the best results. The central contribution of this paper is therefore a demonstration of the aptitude of deep neural networks for the task of identifying complex words. 1 Introduction The identification of complex words plays an important role in the development of simplified reading resources. In particular, accurate automatic complex word identification strongly benefits lexical simplification (LS) as a first step in an LS pipeline (Paetzold and Specia, 2013; Shardlow, 2014). As such, accurate complex word identification may also be critical to higher-level tasks in text simplification, e.g. sentence compression, where the overall readability of In this system description paper, we focus specifically on our neural network model. As noted in the task description paper (Paetzold and Specia, 2016), systems based on neural networks generally performed rather weakly in the task, which the organisers speculate to be a consequence of the small amount of training data. However, we show in posthoc experiments how careful network design may lead to performance figures clos"
S16-1160,N10-1056,0,0.0119255,"rs speculate to be a consequence of the small amount of training data. However, we show in posthoc experiments how careful network design may lead to performance figures close to those of the task-winning system. We make our revised system publicly available.1 1 https://github.com/jbingel/cwi2016 1028 Proceedings of SemEval-2016, pages 1028–1033, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics 2 Related work The identification of complex words in context has in the past been embedded, often implicitly, into broader (lexical) simplification endeavours (Yatskar et al., 2010; Medero and Ostendorf, 2011; Horn et al., 2014). These models usually employ lexicons or corpus frequencies to determine candidates for lexical substitution. In a corpus study of the standard/Simple English Wikipedia, Medero and Ostendorf (2009) identify a number of word-level features that are indicative of texts with more difficult vocabulary. One of their findings is that words that are typical of simple texts tend to have longer definitions and more user-entered translations in Wiktionary. They also tend to be more ambiguous with respect to word class membership. Research explicitly dedic"
S16-1209,E09-1005,0,0.0352399,"2.18 18.45 # Attach 367 71 569 # Merge 33 56 31 Table 1: Overview of the datasets, showing the composition of nouns and verbs, merge-action and attach-actions, and the mean number of description sentences per term and number of tokens per description sentence. 4 Features 400 Our systems use both lexical and syntactic features, with distributional features included in the second run. As a first step, we POS-tag and dependency parse the description sentences using the Mate parser in (Bohnet et al., 2013).2 Additionally, we apply the unsupervised word sense disambiguation algorithm described in (Agirre and Soroa, 2009).3 # Instances 300 225 200 100 67 28 0 0 1 13 7 2 3 δ(g, sc (t, g)) 4 Figure 1: Number of instances in the training set such that δ(g, sc (t, g)) corresponds respectively to 0, 1, 2, 3, and 4. As is apparent from the Zipfian-like distribution, sc constitutes an excellent guess for g. Defining wc (t, d, g) for each d ∈ D(t) in similar fashion as the synset minimizing δ(g, wc (t, g)), an analogous observation can be made for each description sentence. The result can be seen in Figure 2 below: # Sentences 600 400 276 200 97 42 0 0 1 25 2 3 δ(g, wc (t, d, g)) 50 4 4.1 Description words are represe"
S16-1209,W13-3520,0,0.013958,"ndidate for sc (t, g). As terms often have multiple description sentences, we therefore have for each term several good guesses for sc (t, g) and therefore g. These observations form the basis of the strategies discussed in Section 5.2. 1338 Constrained Features Embedding Features We extend the features defined above with SkipGram embeddings as discussed in (Mikolov et al., ˇ uˇrek and So2013). We train the Gensim model (Reh˚ jka, 2010) on a corpus consisting of the description sentences and the Wikipedia entries for each term, padded with the English part of the PolyGlot corpus presented in (Al-Rfou et al., 2013). 2 Available at: https://code.google.com/archive/p/matetools/wikis/ParserAndModels.wiki . 3 Available at: http://ixa2.si.ehu.es/ukb/ . Word-level features are extended with embeddings for the word itself and the dependency head. Term-level features are extended with the sum of the embedding vectors of each word in the n-gram. Synset-level features are extended with the mean of the embedding vectors for each lemma. Finally, all pairwise features are extended with cosine distances. 5 Experiments In this section, we explain the strategies we attempted. In all cases, we discount the merge action,"
S16-1209,Q13-1034,0,0.0603888,"Missing"
S16-1209,N15-1169,0,0.0294687,"Missing"
S16-1209,N15-1098,0,0.027484,"ed and unconstrained versions of the ranking-based systems. The submitted runs are marked in bold. Recall is 1.00 for all systems except for the submitted runs, where a software error caused a recall of 0.97. 7 Discussion Investigating the errors made by the voting-based first-sense systems, we see a pattern in which hypernyms of the correct integration mistakenly are chosen. Examples include steroid instead of anabolic steroid, drug instead of opiate, and person instead of woman. Although these errors occur under both conditions, they are slightly more frequent in the constrained system. In (Levy et al., 2015), evidence is presented that distributional embeddings mainly encode hierarchical level rather than hierarchical branch. Our findings seem to support this conclusion. Refining the usage of the embeddings may well resolve this particular source of error. In Table 1, we see how the training dataset not only on the average has more description sentences per term, but also longer description sentences. As the Personalized Pagerank algorithm relies on a context created from these sets of tokens, this may be the source of the poor performance of sense disambiguation compared to most frequent sense o"
S16-1209,P06-1101,0,0.0641555,"ds for taxonomy enrichment can roughly be divided into two categories: relying on alignment between multiple taxonomies, or relying on machine learning-based rating of subgraphs. 1 http://alt.qcri.org/semeval2016/task14/ In (Jurgens and Pilehvar, 2015), Wordnet is extended with technical terms and rare lemmas from Wiktionary. In (Suchanek et al., 2007), relationextraction is used to unify WordNet and Wikipedia. (Navigli and Ponzetto, 2012) further automates the alignment process itself. In (Toral et al., 2008), named entities are brought from Wikipedia to WordNet through pattern matching. In (Snow et al., 2006), the probabilities of taxonomies are evaluated based on evidence vectors associated to edges. A similar approach formulated in terms of factor graphs can be seen in (Bansal et al., 2013). Finally, (Yamada et al., 2011) employ a hybrid strategy, scoring edges by likelihood of appearance in Wikipedia. Our approach also falls in the second category, relying on a machine learning process to rank hypernym-hyponym edges. 3 Data There are three data splits: train, trial, and test, described in Table 1. We use the trial data for model selection. Notice that attach-actions represent the vast majority"
S16-1209,P14-1098,0,\N,Missing
S16-1209,I11-1098,0,\N,Missing
S16-1209,toral-etal-2008-named,0,\N,Missing
W11-4604,E09-1005,0,0.0130194,"which is related to the lack of strategies to capture meaning underspecification. 3 State of the art The computational study of systematic polysemy has been geared to the collapsing of senses (Vossen et al., 1999; Buitelaar, 1998; Tomuro, 2001) prior to Word Sense Disambiguation (WSD). The best performance in WSD is obtained by supervised methods that require a very large amount of annotated learning data. The other main approach is to use a lexical knowledge base such as WordNet and a PageRank algorithm to compute the most likely sense in the sense enumeration of the lexical knowledge base (Agirre and Soroa, 2009). WordNet does not include the Location/Organization alternation in geopolitical locations, so the task at hands falls outside the traditional scope of WSD. The field of Named Entity Recognition (NER) shows two different approaches to regular-polysemy based sense alternations. In their account, Johannessen et al. (2005) differentiate what they call the Form over Function and the Function over Form strategy. Some NER systems assign a constant value to a word type, enforcing what Finkel et al. (2005) call label consistency, namely Form over Function. The Function over Form strategy, however, ass"
W11-4604,P05-1045,0,0.00423742,"ithm to compute the most likely sense in the sense enumeration of the lexical knowledge base (Agirre and Soroa, 2009). WordNet does not include the Location/Organization alternation in geopolitical locations, so the task at hands falls outside the traditional scope of WSD. The field of Named Entity Recognition (NER) shows two different approaches to regular-polysemy based sense alternations. In their account, Johannessen et al. (2005) differentiate what they call the Form over Function and the Function over Form strategy. Some NER systems assign a constant value to a word type, enforcing what Finkel et al. (2005) call label consistency, namely Form over Function. The Function over Form strategy, however, assigns a semantic type to the analyzed word depending on how it behaves in each context and is analogous to the work exposed in this article. A class of nominals that shows regular polysemy and is well studied is the deverbal noun (destruction, examination), which has distinct grammatical features that can help pinpoint its reading as either process or result, as covered in theory by Grimshaw (1990) and computationally acknowledged by Peris et al. (2009). There is also recent work in the identificati"
W11-4604,H92-1045,0,0.0794082,"lgarrif et al, 2004), which has only been used to establish the nominal word space. No other external resources like FrameNet or WordNet have been used, following Markert and Nissim’s (2009) claim that grammatical features tend to be the most discriminating features. For similar remarks, cf. Peris (2009), Rumshisky (2007). The hypotheses that regular polysemy alternations are often determined at subphrasal level can contradict traditional WSD algorithms like Page Rank, which have a larger scope of analysis. Selection of metonymical senses falls outside of the One-sense-per-discourse approach (Gale et al., 1992), since such approach has been phrased reLexical and grammatical features Figure 1: word sketch for ""country"" 21 Hector Martinez Alonso, Nuria Bel and Bolette Sandford Pedersen ´ 5.2 Following Joanis et al. (2006), the occurrences have been characterized in order to assess the amount of semantic information that their distributional data can provide. The total size of the feature space is of 317 binary features, divided as follows: 1. NP-traits (6 features): which describe the internal structure of the NP where t appears. The features indicate the presence of an adjective in the NP, of a commo"
W11-4604,J03-2004,0,0.0527164,"Missing"
W11-4604,S10-1005,0,0.0215225,"ng on how it behaves in each context and is analogous to the work exposed in this article. A class of nominals that shows regular polysemy and is well studied is the deverbal noun (destruction, examination), which has distinct grammatical features that can help pinpoint its reading as either process or result, as covered in theory by Grimshaw (1990) and computationally acknowledged by Peris et al. (2009). There is also recent work in the identification of metonymy (Markert and Nissim, 2009) as well as other Generative-Lexicon based sensedisambiguation works, such as Rumshisky et al. (2007) or Pustejovsky et al. (2010). Disambiguation systems, however, are still coping with the need of a representation and recognition of underspecification (Pustejovsky, 2009). The SIMPLE lexicon (Lenci et al., 2000) is a GL-compliant lexicon for twelve European languages. It describes its lexical items in terms of their position within a type ontology as well as 19 Hector Martinez Alonso, Nuria Bel and Bolette Sandford Pedersen ´ a qualia structure. SIMPLE list the Geopolitical Location class as a class associated to a complex type <Location,Human_Group>, which expresses the dot-type ambiguity of words of this class. Words"
W11-4604,N01-1010,0,0.0157248,"which can be seen a kind of underspecification. In spite of the GL's computational perspective, Natural Language Processing (NLP) implementations that examine the actual computational feasibility of the GL are few. Moreover, there is no overt attempt to identify the possible three behaviors of a dot type, as the dot predication has not been computationally tackled, which is related to the lack of strategies to capture meaning underspecification. 3 State of the art The computational study of systematic polysemy has been geared to the collapsing of senses (Vossen et al., 1999; Buitelaar, 1998; Tomuro, 2001) prior to Word Sense Disambiguation (WSD). The best performance in WSD is obtained by supervised methods that require a very large amount of annotated learning data. The other main approach is to use a lexical knowledge base such as WordNet and a PageRank algorithm to compute the most likely sense in the sense enumeration of the lexical knowledge base (Agirre and Soroa, 2009). WordNet does not include the Location/Organization alternation in geopolitical locations, so the task at hands falls outside the traditional scope of WSD. The field of Named Entity Recognition (NER) shows two different a"
W11-4604,W99-0512,0,0.0491728,"ible senses as most salient, as in k), which can be seen a kind of underspecification. In spite of the GL's computational perspective, Natural Language Processing (NLP) implementations that examine the actual computational feasibility of the GL are few. Moreover, there is no overt attempt to identify the possible three behaviors of a dot type, as the dot predication has not been computationally tackled, which is related to the lack of strategies to capture meaning underspecification. 3 State of the art The computational study of systematic polysemy has been geared to the collapsing of senses (Vossen et al., 1999; Buitelaar, 1998; Tomuro, 2001) prior to Word Sense Disambiguation (WSD). The best performance in WSD is obtained by supervised methods that require a very large amount of annotated learning data. The other main approach is to use a lexical knowledge base such as WordNet and a PageRank algorithm to compute the most likely sense in the sense enumeration of the lexical knowledge base (Agirre and Soroa, 2009). WordNet does not include the Location/Organization alternation in geopolitical locations, so the task at hands falls outside the traditional scope of WSD. The field of Named Entity Recogni"
W11-4604,bel-etal-2000-simple,1,0.673715,"on, examination), which has distinct grammatical features that can help pinpoint its reading as either process or result, as covered in theory by Grimshaw (1990) and computationally acknowledged by Peris et al. (2009). There is also recent work in the identification of metonymy (Markert and Nissim, 2009) as well as other Generative-Lexicon based sensedisambiguation works, such as Rumshisky et al. (2007) or Pustejovsky et al. (2010). Disambiguation systems, however, are still coping with the need of a representation and recognition of underspecification (Pustejovsky, 2009). The SIMPLE lexicon (Lenci et al., 2000) is a GL-compliant lexicon for twelve European languages. It describes its lexical items in terms of their position within a type ontology as well as 19 Hector Martinez Alonso, Nuria Bel and Bolette Sandford Pedersen ´ a qualia structure. SIMPLE list the Geopolitical Location class as a class associated to a complex type <Location,Human_Group>, which expresses the dot-type ambiguity of words of this class. Words that are considered geopolitical locations can be proper (Africa, Boston, China) or common (city, nation, state, etc) nouns. 4 Experiment We propose a classification experiment that id"
W11-4604,C10-1006,1,\N,Missing
W13-5411,J12-3005,0,0.101032,"tion 5) and conclude with final observations and future work (Sections 6 and 7). 2 Related Work Natural Language Processing (NLP) tasks that exploit distributional information are based on the Distributional Hypothesis (Harris, 1954). However, Pustejovsky and Je˘zek (2008) claim that only using distributional data cannot explain the variation of linguistic meaning in language, while Markert and Nissim (2009) refer to the challenges of dealing with regular polysemy as the different senses of polysemous words present obstacles due to varied use in context. Along this line, the empirical work of Boleda et al. (2012) showed that the skewed sense distribution of many words makes it difficult to distinguish evidence of a class from noise, presenting a challenge to model the relations between senses. When their machinelearning experiments reached the upper bound set by the inter-encoder agreement in their gold standard, they concluded that in order to improve the modelling of polysemy there is a need to shift from a type to a token-based (word-in-context) model (Sch¨utze, 1998; Erk and Pad´o, 2008). Hence, we employ a token-based model in our experiments. In our approach, we propose an unsupervised task usin"
W13-5411,W13-0602,0,0.0770537,"was being kept busy with other concerns. (c) England is conservative and rainy. In this example, (1a) shows the literal sense of England as a location, while (1b) demonstrates the metonymic sense of England as an organization. Dot types also allow for both senses to be simultaneously active in a predicate, as in example (1c). ´ Nuria Bel Universitat Pompeu Fabra Roc Boronat, 138 Barcelona (Spain) nuria.bel@upf.edu All proper names representative of geopolitical entities, for instance, demonstrate this type of classwide sense alternation, which is defined as regular polysemy (Apresjan, 1974). Copestake (2013) emphasizes the relevance of distributional evidence in tasks regarding phenomena characteristic to regular polysemy, such as underspecification, because it incorporates frequency effects and is theory-neutral, requiring only that examples cluster in a way that mirrors their senses. Thus far, underspecification in dot types has been formalized in the linguistic theory of lexical semantics, but has not been explicitly studied using WSI. Kilgariff (1997) claims that word senses should be “construed as abstractions over clusters of word usages”. Following this claim, our strategy employs WSI, whi"
W13-5411,D08-1094,0,0.0463486,"Missing"
W13-5411,E12-1060,0,0.088197,"ristic to regular polysemy, such as underspecification, because it incorporates frequency effects and is theory-neutral, requiring only that examples cluster in a way that mirrors their senses. Thus far, underspecification in dot types has been formalized in the linguistic theory of lexical semantics, but has not been explicitly studied using WSI. Kilgariff (1997) claims that word senses should be “construed as abstractions over clusters of word usages”. Following this claim, our strategy employs WSI, which aims to automatically induce senses of words by clustering patterns found in a corpus (Lau et al., 2012; Jurgens, 2012). In this way, we hypothesize that dot-type nominals will generate semantically more consistent (i.e. more homogeneous, cf. Section 5) groupings if clustered into more than two induced senses. This paper is organized as follows: we discuss related work (Section 2); elaborate upon our use of WSI and methodology employed (Section 3 and Section 4), as well as present results obtained; we discuss our results (Section 5) and conclude with final observations and future work (Sections 6 and 7). 2 Related Work Natural Language Processing (NLP) tasks that exploit distributional informat"
W13-5411,S10-1011,0,0.0222412,"nd Markert (2005) and Nastase et al. (2012), but we make use of a much larger amount of data and thus should suffer from less sparsity. The related experiment by Rumshisky et al. (2007) uses verbal arguments as features, while we use only a five-word context window. 2.1 Word Sense Induction As stated above, our main goal is to use WSI to capture the sense alternation of dot types in context. WSI methods, based on the distributional information available in corpus data, employ unsupervised means to induce senses using contexts of indicated target words without relying on handcrafted resources (Manandhar et al., 2010). Distributional Semantic Models (DSM) provide the groundwork for WSI. A DSM, also known as a Word Space Model (Turney and Pantel, 2010), attempts to describe the meaning of words by characterizing their usage over distributional patterns, i.e. their context. Each word is represented by a numeric vector positioned in a space where vectors for words that appear in similar contexts are closer to each other. Sense induction is achieved by building a DSM over a large corpus and clustering the contexts into induced senses. In recent years, WSI has been used with success for different tasks such as:"
W13-5411,P13-2127,1,0.599797,"Missing"
W13-5411,D12-1017,0,0.0562275,"Missing"
W13-5411,W11-1104,0,0.0137045,"r WSI. A DSM, also known as a Word Space Model (Turney and Pantel, 2010), attempts to describe the meaning of words by characterizing their usage over distributional patterns, i.e. their context. Each word is represented by a numeric vector positioned in a space where vectors for words that appear in similar contexts are closer to each other. Sense induction is achieved by building a DSM over a large corpus and clustering the contexts into induced senses. In recent years, WSI has been used with success for different tasks such as: novel sense detection (Lau et al., 2012), community detection (Jurgens, 2011) and graded sense disambiguation (Jurgens, 2012), among others. Jurgens (2011) previously employed WSI to discover overlaps in the distributional behavior of words in order to identify multiple senses with success. However, that work was not inclusive to any specific phenomenon of polysemy. Our objective is to cluster dot-type nominals according to their distributional evidence in context, using WSI to characterize the behavior of these nouns. 3 Method We use WSI to computationally assess the predicational behavior of dot types. To do this, we employ a WSI system to induce senses from a large"
W13-5411,S12-1027,0,0.0505611,"polysemy, such as underspecification, because it incorporates frequency effects and is theory-neutral, requiring only that examples cluster in a way that mirrors their senses. Thus far, underspecification in dot types has been formalized in the linguistic theory of lexical semantics, but has not been explicitly studied using WSI. Kilgariff (1997) claims that word senses should be “construed as abstractions over clusters of word usages”. Following this claim, our strategy employs WSI, which aims to automatically induce senses of words by clustering patterns found in a corpus (Lau et al., 2012; Jurgens, 2012). In this way, we hypothesize that dot-type nominals will generate semantically more consistent (i.e. more homogeneous, cf. Section 5) groupings if clustered into more than two induced senses. This paper is organized as follows: we discuss related work (Section 2); elaborate upon our use of WSI and methodology employed (Section 3 and Section 4), as well as present results obtained; we discuss our results (Section 5) and conclude with final observations and future work (Sections 6 and 7). 2 Related Work Natural Language Processing (NLP) tasks that exploit distributional information are based on"
W13-5411,D07-1043,0,0.0105421,"ample. For each sentence in the test data, we isolated the placeholder to disambiguate and we calculated the representation of the sentence within the corresponding WSI model using the specified 5-word context window. Once the vector for the sentence was obtained, we assigned the sentence to the induced sense representing the highest cosine similarity for each model (cf. Table 2 in Section 4 for evaluation). 4 Results To determine the success of our task for each class, sense representation and k value, we consider the information-theoretic measures of homogeneity, completeness and V-measure (Rosenberg and Hirschberg, 2007). These three measures compare the output of the clustering with a gold standard (as described in Section 3.1) and provide a score that can be interpreted in a manner similar to precision, recall and F1, respectively. Homogeneity determines to which extent each cluster only contains members of a single class, while completeness determines if all members of a given class are assigned to the same cluster. Both the homogeneity and completeness scores are bounded by 0.0 and 1.0, with 1.0 corresponding to the most homogeneous or complete solution, and can be interpreted in a manner similar to preci"
W13-5411,J98-1004,0,0.404918,"Missing"
W13-5411,W11-2214,0,\N,Missing
W14-1601,N13-1070,1,0.862135,"in the absence of bias and with perfect metrics – gives us the level of confidence we expect as a research community, i.e., P P V = 0.95. Significance results would thus be more reliable and reduce type 1 error. 5 newswire corpora. This is also standard practice in the machine learning community (Demsar, 2006). Poor metrics. For tasks such as POS tagging and dependency parsing, our metrics are suboptimal (Manning, 2011; Schwartz et al., 2011; Tsarfaty et al., 2012). System A and System B may perform equally well as measured by some metric, but contribute very differently to downstream tasks. Elming et al. (2013) show how parsers trained on different annotation schemes lead to very different downstream results. This suggests that being wrong with respect to a gold standard, e.g., choosing NP analysis over a “correct” DP analysis, may in some cases lead to better downstream performance. See the discussion in Manning (2011) for POS tagging. One simple approach to this problem is to report results across available metrics. If System A improves over System B wrt. most metrics, we obtain significance against the odds. POS taggers and dependency parsers should also be evaluated by their impact on downstream"
W14-1601,W03-0425,0,0.0235909,"Missing"
W14-1601,W05-0909,0,0.0206194,"01 <0.001 0.2020 0.3965 0.0020 0.2480 0.4497 0.4497 UA (b) 0.0430 0.2566 <0.001 0.0143 <0.001 0.0210 0.0543 0.0024 0.0024 0.0924 SA (b) 0.3788 0.4515 <0.001 <0.001 0.1622 0.1238 0.0585 0.2435 0.2435 0.1111 SA(w) 0.9270 0.9941 <0.001 <0.001 0.0324 0.6602 0.0562 0.9390 0.9390 0.7853 et al., 2012) have been proposed in addition to unlabeled and labeled attachment scores, as well as exact matches. Perhaps more famously, in machine translation and summarization it is common practice to use multiple metrics, and there exists a considerable literature on that topic (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005; Clark et al., 2011; Rankel et al., 2011). Even in POS tagging, some report tagging accuracies, tagging accuracies over unseen words, macro-averages over sentence-level accuracies, or number of exact matches. Table 2: POS tagging p-values across tagging accuracy (TA), accuracy for unseen words (UA) and sentence-level accuracy (SA) with bootstrap (b) and Wilcoxon (w) (p < 0.05 gray-shaded). Answers Emails Newsgroups Reviews Weblogs WSJ LAS 0.020 0.083 0.049 <0.001 <0.001 <0.001 The existence of several metrics is not in itself a problem, but if researchers can cherry-pick their favorite metric"
W14-1601,I11-1100,0,0.0289698,"Missing"
W14-1601,D12-1091,0,0.0568125,"ata sets, such tests seem to be the right choice (Demsar, 2006; Søgaard, 2013). The draw-back of rank-based tests is their relatively weak statistical power. When we reduce scores to ranks, we throw away information, and rank-based tests are therefore relatively conservative, potentially leading to high type 2 error rate ( , i.e., the number of false negatives over trials). An alternative, however, are randomization-based tests such as the bootstrap test (Efron and Tibshirani, 1993) and approximate randomization (Noreen, 1989), which are the de facto standards in NLP. In this paper, we follow Berg-Kirkpatrick et al. (2012) in focusing on the bootstrap test. The bootstrap test is non-parametric and stronger than rank-based testing, i.e., introduces fewer type 2 errors. For small samples, however, it does so at the expense of a 1 In many fields, including NLP, it has become good practice to report actual p-values, but we still need to understand how significance levels relate to the probability that research findings are false, to interpret such values. The fact that we propose a new cut-off level for the ideal case with perfect metrics and no bias does not mean that we do not recommend reporting actual p-values."
W14-1601,W06-1615,0,0.0543626,"Missing"
W14-1601,P07-1034,0,0.0536678,"Missing"
W14-1601,C00-1011,0,0.168794,"Missing"
W14-1601,W04-1013,0,0.0270518,"0.3569 <0.001 <0.001 0.2020 0.3965 0.0020 0.2480 0.4497 0.4497 UA (b) 0.0430 0.2566 <0.001 0.0143 <0.001 0.0210 0.0543 0.0024 0.0024 0.0924 SA (b) 0.3788 0.4515 <0.001 <0.001 0.1622 0.1238 0.0585 0.2435 0.2435 0.1111 SA(w) 0.9270 0.9941 <0.001 <0.001 0.0324 0.6602 0.0562 0.9390 0.9390 0.7853 et al., 2012) have been proposed in addition to unlabeled and labeled attachment scores, as well as exact matches. Perhaps more famously, in machine translation and summarization it is common practice to use multiple metrics, and there exists a considerable literature on that topic (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005; Clark et al., 2011; Rankel et al., 2011). Even in POS tagging, some report tagging accuracies, tagging accuracies over unseen words, macro-averages over sentence-level accuracies, or number of exact matches. Table 2: POS tagging p-values across tagging accuracy (TA), accuracy for unseen words (UA) and sentence-level accuracy (SA) with bootstrap (b) and Wilcoxon (w) (p < 0.05 gray-shaded). Answers Emails Newsgroups Reviews Weblogs WSJ LAS 0.020 0.083 0.049 <0.001 <0.001 <0.001 The existence of several metrics is not in itself a problem, but if researchers can cherry-"
W14-1601,W06-2920,0,0.0650816,"Missing"
W14-1601,C08-1015,0,0.0300507,"Missing"
W14-1601,W03-0423,0,0.0336765,"odels from the associated websites.2 Dependency parsing. Here we compare the pre-trained linear SVM MaltParser model for English (Nivre et al., 2007b) to the compositional vector grammar model for the Stanford parser (Socher et al., 2013). For this task, we use the subset of the POS data sets that comes with Stanfordstyle syntactic dependencies (cf. Table 1), excluding the Twitter data set which we found too small to produce reliable results. NER. We use the publicly available runs of the two best systems from the CoNLL 2003 shared task, namely F LORIAN (Florian et al., 2003) and C HIEU -N G (Chieu and Ng, 2003).3 • • • • • • • Table 1: Evaluation data. 3 Experiments Throughout the rest of the paper, we use four running examples: a synthetic toy example and three standard experimental NLP tasks, namely POS tagging, dependency parsing and NER. The toy example is supposed to illustrate the logic behind our reasoning and is not specific to NLP. It shows how likely we are to obtain a low p-value for the difference in means when sampling from exactly the same (Gaussian) distributions. For the NLP setups (2-4), we use off-the-shelf models or available runs, as described next. 3.2 Standard comparisons POS t"
W14-1601,P11-2031,0,0.00842907,"0020 0.2480 0.4497 0.4497 UA (b) 0.0430 0.2566 <0.001 0.0143 <0.001 0.0210 0.0543 0.0024 0.0024 0.0924 SA (b) 0.3788 0.4515 <0.001 <0.001 0.1622 0.1238 0.0585 0.2435 0.2435 0.1111 SA(w) 0.9270 0.9941 <0.001 <0.001 0.0324 0.6602 0.0562 0.9390 0.9390 0.7853 et al., 2012) have been proposed in addition to unlabeled and labeled attachment scores, as well as exact matches. Perhaps more famously, in machine translation and summarization it is common practice to use multiple metrics, and there exists a considerable literature on that topic (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005; Clark et al., 2011; Rankel et al., 2011). Even in POS tagging, some report tagging accuracies, tagging accuracies over unseen words, macro-averages over sentence-level accuracies, or number of exact matches. Table 2: POS tagging p-values across tagging accuracy (TA), accuracy for unseen words (UA) and sentence-level accuracy (SA) with bootstrap (b) and Wilcoxon (w) (p < 0.05 gray-shaded). Answers Emails Newsgroups Reviews Weblogs WSJ LAS 0.020 0.083 0.049 <0.001 <0.001 <0.001 The existence of several metrics is not in itself a problem, but if researchers can cherry-pick their favorite metric when reporting resu"
W14-1601,P02-1040,0,0.113383,"Twitter TA (b) 0.3445 0.3569 <0.001 <0.001 0.2020 0.3965 0.0020 0.2480 0.4497 0.4497 UA (b) 0.0430 0.2566 <0.001 0.0143 <0.001 0.0210 0.0543 0.0024 0.0024 0.0924 SA (b) 0.3788 0.4515 <0.001 <0.001 0.1622 0.1238 0.0585 0.2435 0.2435 0.1111 SA(w) 0.9270 0.9941 <0.001 <0.001 0.0324 0.6602 0.0562 0.9390 0.9390 0.7853 et al., 2012) have been proposed in addition to unlabeled and labeled attachment scores, as well as exact matches. Perhaps more famously, in machine translation and summarization it is common practice to use multiple metrics, and there exists a considerable literature on that topic (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005; Clark et al., 2011; Rankel et al., 2011). Even in POS tagging, some report tagging accuracies, tagging accuracies over unseen words, macro-averages over sentence-level accuracies, or number of exact matches. Table 2: POS tagging p-values across tagging accuracy (TA), accuracy for unseen words (UA) and sentence-level accuracy (SA) with bootstrap (b) and Wilcoxon (w) (p < 0.05 gray-shaded). Answers Emails Newsgroups Reviews Weblogs WSJ LAS 0.020 0.083 0.049 <0.001 <0.001 <0.001 The existence of several metrics is not in itself a problem, but if researchers"
W14-1601,P07-1033,0,0.0204031,"Missing"
W14-1601,P11-1157,1,0.595908,"Missing"
W14-1601,Y09-1013,0,0.0190285,"acy (TA), accuracy for unseen words (UA) and sentence-level accuracy (SA) with bootstrap (b) and Wilcoxon (w) (p < 0.05 gray-shaded). Answers Emails Newsgroups Reviews Weblogs WSJ LAS 0.020 0.083 0.049 <0.001 <0.001 <0.001 The existence of several metrics is not in itself a problem, but if researchers can cherry-pick their favorite metric when reporting results, this increases the a priori chance of establishing significance. In POS tagging, most papers report significant improvements over tagging accuracy, but some report significant improvements over tagging accuracy of unknown words, e.g., Denis and Sagot (2009) and Umansky-Pesin et al. (2010). This corresponds to the situation in psychology where researchers cherry-pick between several dependent variables (Simmons et al., 2011), which also increases the chance of finding a significant correlation. UAS <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 Table 3: Parsing p-values (M ALT-L IN VS . S TANFORD -RNN) across LAS and UAS (p < 0.05 gray-shaded). than S TANFORD, but in one case it is the other way around. If we do a Wilcoxon test over the results on the 10 data sets, following the methodology in Demsar (2006) and Søgaard (2013), the difference, which is"
W14-1601,D11-1043,0,0.0493232,"Missing"
W14-1601,W05-0908,0,0.132601,"Missing"
W14-1601,D09-1085,0,0.0396616,"Missing"
W14-1601,P11-1067,0,0.0160959,"Figure 7: PPV for different ↵ (horizontal line is PPV for p = 0.05, vertical line is ↵ for PPV=0.95). could propose a p-value cut-off at p < 0.0025. This is the cut-off that – in the absence of bias and with perfect metrics – gives us the level of confidence we expect as a research community, i.e., P P V = 0.95. Significance results would thus be more reliable and reduce type 1 error. 5 newswire corpora. This is also standard practice in the machine learning community (Demsar, 2006). Poor metrics. For tasks such as POS tagging and dependency parsing, our metrics are suboptimal (Manning, 2011; Schwartz et al., 2011; Tsarfaty et al., 2012). System A and System B may perform equally well as measured by some metric, but contribute very differently to downstream tasks. Elming et al. (2013) show how parsers trained on different annotation schemes lead to very different downstream results. This suggests that being wrong with respect to a gold standard, e.g., choosing NP analysis over a “correct” DP analysis, may in some cases lead to better downstream performance. See the discussion in Manning (2011) for POS tagging. One simple approach to this problem is to report results across available metrics. If System"
W14-1601,P13-1045,0,0.00363571,"ASKS Dep. NER al., 2011; Tjong Kim Sang and De Meulder, 2003, LDC99T42; LDC2012T13). POS tagging. We compare the performance of two state-of-the-art newswire taggers across 10 evaluation data sets (see Table 1), namely the L A POS tagger (Tsuruoka et al., 2011) and the S TAN FORD tagger (Toutanova et al., 2003), both trained on WSJ00–18. We use the publicly available pretrained models from the associated websites.2 Dependency parsing. Here we compare the pre-trained linear SVM MaltParser model for English (Nivre et al., 2007b) to the compositional vector grammar model for the Stanford parser (Socher et al., 2013). For this task, we use the subset of the POS data sets that comes with Stanfordstyle syntactic dependencies (cf. Table 1), excluding the Twitter data set which we found too small to produce reliable results. NER. We use the publicly available runs of the two best systems from the CoNLL 2003 shared task, namely F LORIAN (Florian et al., 2003) and C HIEU -N G (Chieu and Ng, 2003).3 • • • • • • • Table 1: Evaluation data. 3 Experiments Throughout the rest of the paper, we use four running examples: a synthetic toy example and three standard experimental NLP tasks, namely POS tagging, dependency"
W14-1601,N13-1068,1,0.72957,"nalysis from previous experiments, and research findings often consist in system comparisons showing that System A is better than System B. Effect size, i.e., one system’s improvements over another, can be seen as a random variable. If the random variable follows a known distribution, e.g., a normal distribution, we can use parametric tests to estimate whether System A is better than System B. If it follows a normal distribution, we can use Student’s t-test, for example. Effect sizes in NLP are generally not normally distributed or follow any of the other wellstudied distributions (Yeh, 2000; Søgaard, 2013). The standard significance testing methods in NLP are therefore rank- or randomization-based nonparametric tests (Yeh, 2000; Riezler and Maxwell, 1 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 1–10, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics Covariates. Sometimes we may bin our results by variables that are actually predictive of the outcome (covariates) (Simmons et al., 2011). In some subfields of NLP, such as machine translation or (unsupervised) syntactic parsing, for example, it is common to report resul"
W14-1601,N03-1033,0,0.00466178,"ndency parsing. D OMAIN #W ORDS C O NLL 2007 Bio Chem POS 4k 5k • • S WITCHBOARD 4 Spoken 162k • E NGLISH W EB T REEBANK Answers Emails Newsgrs Reviews Weblogs WSJ 29k 28k 21k 28k 20k 40k • • • • • • F OSTER Twitter 3k • C O NLL 2003 News 50k Figure 1: Accuracies of L APOS VS . S TANFORD across 10 data sets. TASKS Dep. NER al., 2011; Tjong Kim Sang and De Meulder, 2003, LDC99T42; LDC2012T13). POS tagging. We compare the performance of two state-of-the-art newswire taggers across 10 evaluation data sets (see Table 1), namely the L A POS tagger (Tsuruoka et al., 2011) and the S TAN FORD tagger (Toutanova et al., 2003), both trained on WSJ00–18. We use the publicly available pretrained models from the associated websites.2 Dependency parsing. Here we compare the pre-trained linear SVM MaltParser model for English (Nivre et al., 2007b) to the compositional vector grammar model for the Stanford parser (Socher et al., 2013). For this task, we use the subset of the POS data sets that comes with Stanfordstyle syntactic dependencies (cf. Table 1), excluding the Twitter data set which we found too small to produce reliable results. NER. We use the publicly available runs of the two best systems from the CoNLL 2003"
W14-1601,E12-1006,0,0.0571022,"or a choice between tagging accuracy and sentence-level accuracy, we see a significant improvement in 4/10 cases, i.e., for 4/10 data sets the effect is significance wrt. at least one metric. If we allow for a free choice between all three metrics (TA, UA, and SA), we observe significance in 9/10 cases. This way the existence of multiple metrics almost guarantees significant differences. Note that there are only two data sets (Answers and Spoken), where all metric differences appear significant. Dependency parsing. While there are multiple metrics in dependency parsing (Schwartz et al., 2011; Tsarfaty et al., 2012), we focus on the two standard metrics: labeled (LAS) and unlabeled attachment score (UAS) (Buchholz and Marsi, 2006). If we just consider the results in Table 3, i.e., only the comparison of M ALT-L IN VS . S TANFORD -RNN, we observe significant improvements in all cases, if we allow for a free choice between metrics. Bod (2000) provides a good example of a parsing paper evaluating models using different metrics on different test sets. Chen et al. (2008), similarly, only report UAS. NER. While macro-f1 is fairly standard in NER, we do have several available multiple metrics, including the unl"
W14-1601,W11-0328,0,0.0154577,"across 3 runs for POS and NER and 10 runs for dependency parsing. D OMAIN #W ORDS C O NLL 2007 Bio Chem POS 4k 5k • • S WITCHBOARD 4 Spoken 162k • E NGLISH W EB T REEBANK Answers Emails Newsgrs Reviews Weblogs WSJ 29k 28k 21k 28k 20k 40k • • • • • • F OSTER Twitter 3k • C O NLL 2003 News 50k Figure 1: Accuracies of L APOS VS . S TANFORD across 10 data sets. TASKS Dep. NER al., 2011; Tjong Kim Sang and De Meulder, 2003, LDC99T42; LDC2012T13). POS tagging. We compare the performance of two state-of-the-art newswire taggers across 10 evaluation data sets (see Table 1), namely the L A POS tagger (Tsuruoka et al., 2011) and the S TAN FORD tagger (Toutanova et al., 2003), both trained on WSJ00–18. We use the publicly available pretrained models from the associated websites.2 Dependency parsing. Here we compare the pre-trained linear SVM MaltParser model for English (Nivre et al., 2007b) to the compositional vector grammar model for the Stanford parser (Socher et al., 2013). For this task, we use the subset of the POS data sets that comes with Stanfordstyle syntactic dependencies (cf. Table 1), excluding the Twitter data set which we found too small to produce reliable results. NER. We use the publicly availab"
W14-1601,C10-2146,0,0.0272174,"Missing"
W14-1601,C00-2137,0,0.652215,"as error analysis from previous experiments, and research findings often consist in system comparisons showing that System A is better than System B. Effect size, i.e., one system’s improvements over another, can be seen as a random variable. If the random variable follows a known distribution, e.g., a normal distribution, we can use parametric tests to estimate whether System A is better than System B. If it follows a normal distribution, we can use Student’s t-test, for example. Effect sizes in NLP are generally not normally distributed or follow any of the other wellstudied distributions (Yeh, 2000; Søgaard, 2013). The standard significance testing methods in NLP are therefore rank- or randomization-based nonparametric tests (Yeh, 2000; Riezler and Maxwell, 1 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 1–10, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics Covariates. Sometimes we may bin our results by variables that are actually predictive of the outcome (covariates) (Simmons et al., 2011). In some subfields of NLP, such as machine translation or (unsupervised) syntactic parsing, for example, it is common"
W14-1601,E14-4014,0,\N,Missing
W14-1601,W03-0419,0,\N,Missing
W14-1601,D07-1096,0,\N,Missing
W15-1617,ide-etal-2008-masc,0,0.0739475,"Missing"
W15-1617,D14-1108,0,0.0276301,"istribution of the Wall Street Journal dependency treebank (Bies et al., 2012; Petrov and McDonald, 2012). 2. Answers: The Yahoo! Answers test section from the English Web Treebank (Bies et al., 2012; Petrov and McDonald, 2012). 3. Spoken: The Switchboard corpus section of the MASC corpus (Ide et al., 2008). 4. Fiction: The literature subset of the test section of the Brown test set from CoNLL 2008 (Surdeanu et al., 2008), which encompasses the fiction, mystery, science-fiction, romance and humor categories of the Brown corpus. 5. Twitter: The test section of the Tweebank dependency treebank (Kong et al., 2014). WSJ is the perceived-of-as-canonical dataset. Answers and Twitter are datasets of social media texts from two different social media. We include Switchboard as an example of spoken language (transcriptions of telephone conversations), and Fiction to incorporate carefully edited (i.e., not user-generated) text that is lexically and syntactically different to newswire. From each corpus, we randomly selected 50 sentences and doubly-annotated them. D OMAIN A0 A1 MV WSJ Twitter Answers Spoken Fiction 99 88 92 100 96 76 72 79 86 76 72 56 63 81 78 Table 2: Frequency counts for arguments in the anno"
W15-1617,P05-1012,0,0.0577417,"ce (average per-edge confidence). between sentence length and sentence-wise agreement for all 250 annotated sentences, however, found the correlation to be low (0.1364). Consequently, it seems unlikely that sentence length had a major effect on our annotations. We may speculate that annotation disagreements can be due to rare linguistic phenomena and linguistic outliers. In Table 4 we show the correlation per domain between sentence-wise agreement and dependency parsing confidence. We have obtained this confidence from the edge-wise confidence scores provided by an instance of the MST parser (McDonald et al., 2005) trained on WSJ. The parsing confidence for a sentence is obtained from the average of the edges that have received a label (A0, MV, A1) by the annotators, averaged between the two annotators. The correlation for newswire is high, but not the highest, because despite high parsing confidence, annotation agreement is rather low. On the other end, the lowest correlation between parser confidence and agreement is for Answers, which has the highest inter-annotator agreement. These results, in our view, indicate that what makes annotating social media text hard (at times) is not what makes annotatin"
W15-1617,W08-2121,0,0.122021,"Missing"
W15-1806,W06-1670,0,0.0494196,", and it has mostly been limited to English newswire and literature (namely running on SemCor and SensEval data).9 Nevertheless, the interest in applying word-sense disambiguation techniques to reduced, coarser sense inventories has been a topic since the development of the first wordnets (Peters et al., 1998). Kohomban and Lee (2005) and Kohomban and Lee (2007) also propose to use lexicographer file identifers from Princeton WordNet senses (supersenses) and, in addition, discuss how to retrieve fine-grained senses from those predictions. The task of supersense tagging was first introduced by Ciaramita and Altun (2006), who used a structured perceptron trained and evaluated on S EM C OR via 5-fold cross validation. Johannsen et al. (2014) extend the SST approach to the Twitter domain, and include the usage of word embeddings in their feature representation. Supersenses have been used as features in various tasks, such as preposition sense disambiguation, noun compound interpretation, metaphor detection and relation extraction (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013; Søgaard et al., 2015). Schneider et al. (2012) annotated supersenses on Arabic Wikipedia articles . Princeton WordNe"
W15-1806,W97-0802,0,0.0681552,"embeddings in their feature representation. Supersenses have been used as features in various tasks, such as preposition sense disambiguation, noun compound interpretation, metaphor detection and relation extraction (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013; Søgaard et al., 2015). Schneider et al. (2012) annotated supersenses on Arabic Wikipedia articles . Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns. Tsvetkov et al. (2014) propose an extension for adjectives, along the lines of the adjective sense of the German wordnet(Hamp and Feldweg, 1997). To the best of our knowledge, the current work is the first SST approach to Danish, which also extends to less canonical, characteristically webbased text types like chats or fora. 7 Conclusions We have presented a resource for SST that includes an extension of the English supersense inventory that can be used for any language, plus three additional tags that give account for characteristics of the syntax-semantics interface of a satellite-framing language like Danish. We have conducted an annotation task on 1,500 sentences, reaching 0.63 κ score after refining the annotation guidelines. Aft"
W15-1806,S14-1001,1,0.877501,"Missing"
W15-1806,P05-1005,0,0.0094743,"7.8 39.3 34.3 39.8 37.0 64.7 55.0 28.8 46.2 33.3 32.6 34.2 SAT. COLL SAT. PARTICLE SAT. REFLPRON 37.9 59.4 69.6 7.7 47.9 76.4 Table 11: Performance for extended noun and verb supersenses, and satellites. 6 Related work There has been relatively little previous work on supersense tagging, and it has mostly been limited to English newswire and literature (namely running on SemCor and SensEval data).9 Nevertheless, the interest in applying word-sense disambiguation techniques to reduced, coarser sense inventories has been a topic since the development of the first wordnets (Peters et al., 1998). Kohomban and Lee (2005) and Kohomban and Lee (2007) also propose to use lexicographer file identifers from Princeton WordNet senses (supersenses) and, in addition, discuss how to retrieve fine-grained senses from those predictions. The task of supersense tagging was first introduced by Ciaramita and Altun (2006), who used a structured perceptron trained and evaluated on S EM C OR via 5-fold cross validation. Johannsen et al. (2014) extend the SST approach to the Twitter domain, and include the usage of word embeddings in their feature representation. Supersenses have been used as features in various tasks, such as p"
W15-1806,H94-1046,0,0.0286344,"in that the labels are comprised within spans of one or more tokens. NER, however, only recognizes a handful of entity types 1 The data is available at clarin.dk under Danish Supersense Corpus and does not extend beyond nouns, while supersenses may be defined for all part of speech and permit more granular semantic distinctions. While coarse-grained semantic types find use in a range of applications, such as information retrieval, question answering (QA), and relation extraction, one of the main intended uses of the annotated corpus is building a semantic concordancer in the style of SemCor (Miller et al., 1994). We base our annotation effort on the set of supersenses derived from Princeton Wordnet, which makes our annotations interoperable across many languages through the already existing linkings to Princeton Wordnet. However, we found several cases where the Princeton supersenses made overly broad distinctions that caused large groups of lexemes to be grouped together (e.g. buildings and vehicles falling under the ARTIFACT class). The original sense inventory comprises a total of 41 senses, spread over 26 noun senses, and 15 verb senses, plus a single “catch-all” sense for adjectives, which is gr"
W15-1806,P12-2050,0,0.21141,"redictions. The task of supersense tagging was first introduced by Ciaramita and Altun (2006), who used a structured perceptron trained and evaluated on S EM C OR via 5-fold cross validation. Johannsen et al. (2014) extend the SST approach to the Twitter domain, and include the usage of word embeddings in their feature representation. Supersenses have been used as features in various tasks, such as preposition sense disambiguation, noun compound interpretation, metaphor detection and relation extraction (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013; Søgaard et al., 2015). Schneider et al. (2012) annotated supersenses on Arabic Wikipedia articles . Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns. Tsvetkov et al. (2014) propose an extension for adjectives, along the lines of the adjective sense of the German wordnet(Hamp and Feldweg, 1997). To the best of our knowledge, the current work is the first SST approach to Danish, which also extends to less canonical, characteristically webbased text types like chats or fora. 7 Conclusions We have presented a resource for SST that includes an extension of the English supersense inventory that can b"
W15-1806,N03-1033,0,0.0302794,"kov, 2012), which consists of newspapers, magazines, oral debates, blogs, and social media.3 Table 4 lists the amount of training data (1,500 sentences in total) currently annotated for each domain. We describe each domain in terms of its average sentence length (SL) and proportion of tokens per type, namely the average amount of repetitions for a certain type. The final release will be made up of 600 sentences from all of the domains in Table 4, plus the test section of the Danish Dependency Treebank (Buch-Kromann et al., 2003). All the data has been POS-tagged using the Stanford POS-tagger (Toutanova et al., 2003) trained on the Danish PAROLE corpus.4 Note that we strictly use predicted POS instead of goldstandard to provide a more realistic setup for the evaluation of our system in Section 5. 3.2 Annotation guidelines Sense inventory The guidelines for the supersense annotation comprise the list of supersenses provided with an explanation and examples for each supersense. 3 http://cst.ku.dk/Workshop311012/sprogtekno2012.pdf 4 http://korpus.dsl.dk/e-resurser/paroledoc en.pdf Application rules The second part of the guidelines consists of a set of more specific rules for each part of speech. The rules f"
W15-1806,S10-1049,0,0.102102,"n addition, discuss how to retrieve fine-grained senses from those predictions. The task of supersense tagging was first introduced by Ciaramita and Altun (2006), who used a structured perceptron trained and evaluated on S EM C OR via 5-fold cross validation. Johannsen et al. (2014) extend the SST approach to the Twitter domain, and include the usage of word embeddings in their feature representation. Supersenses have been used as features in various tasks, such as preposition sense disambiguation, noun compound interpretation, metaphor detection and relation extraction (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013; Søgaard et al., 2015). Schneider et al. (2012) annotated supersenses on Arabic Wikipedia articles . Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns. Tsvetkov et al. (2014) propose an extension for adjectives, along the lines of the adjective sense of the German wordnet(Hamp and Feldweg, 1997). To the best of our knowledge, the current work is the first SST approach to Danish, which also extends to less canonical, characteristically webbased text types like chats or fora. 7 Conclusions We have presented a resource for SST th"
W15-1806,W13-0906,0,0.0474527,"w to retrieve fine-grained senses from those predictions. The task of supersense tagging was first introduced by Ciaramita and Altun (2006), who used a structured perceptron trained and evaluated on S EM C OR via 5-fold cross validation. Johannsen et al. (2014) extend the SST approach to the Twitter domain, and include the usage of word embeddings in their feature representation. Supersenses have been used as features in various tasks, such as preposition sense disambiguation, noun compound interpretation, metaphor detection and relation extraction (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013; Søgaard et al., 2015). Schneider et al. (2012) annotated supersenses on Arabic Wikipedia articles . Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns. Tsvetkov et al. (2014) propose an extension for adjectives, along the lines of the adjective sense of the German wordnet(Hamp and Feldweg, 1997). To the best of our knowledge, the current work is the first SST approach to Danish, which also extends to less canonical, characteristically webbased text types like chats or fora. 7 Conclusions We have presented a resource for SST that includes an extensio"
W15-1806,tsvetkov-etal-2014-augmenting-english,0,0.0802991,"d cross validation. Johannsen et al. (2014) extend the SST approach to the Twitter domain, and include the usage of word embeddings in their feature representation. Supersenses have been used as features in various tasks, such as preposition sense disambiguation, noun compound interpretation, metaphor detection and relation extraction (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013; Søgaard et al., 2015). Schneider et al. (2012) annotated supersenses on Arabic Wikipedia articles . Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns. Tsvetkov et al. (2014) propose an extension for adjectives, along the lines of the adjective sense of the German wordnet(Hamp and Feldweg, 1997). To the best of our knowledge, the current work is the first SST approach to Danish, which also extends to less canonical, characteristically webbased text types like chats or fora. 7 Conclusions We have presented a resource for SST that includes an extension of the English supersense inventory that can be used for any language, plus three additional tags that give account for characteristics of the syntax-semantics interface of a satellite-framing language like Danish. We"
W15-1806,S07-1051,0,0.246823,"s (supersenses) and, in addition, discuss how to retrieve fine-grained senses from those predictions. The task of supersense tagging was first introduced by Ciaramita and Altun (2006), who used a structured perceptron trained and evaluated on S EM C OR via 5-fold cross validation. Johannsen et al. (2014) extend the SST approach to the Twitter domain, and include the usage of word embeddings in their feature representation. Supersenses have been used as features in various tasks, such as preposition sense disambiguation, noun compound interpretation, metaphor detection and relation extraction (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013; Søgaard et al., 2015). Schneider et al. (2012) annotated supersenses on Arabic Wikipedia articles . Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns. Tsvetkov et al. (2014) propose an extension for adjectives, along the lines of the adjective sense of the German wordnet(Hamp and Feldweg, 1997). To the best of our knowledge, the current work is the first SST approach to Danish, which also extends to less canonical, characteristically webbased text types like chats or fora. 7 Conclusions We have presented"
W15-1806,P13-4001,0,0.134357,"Missing"
W15-1806,W15-2005,1,0.785087,"etaProceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 22 Domain Blog Chat Forum Magazine Newswire Parliament SL tokens types Sentences 16.44 14.61 20.51 19.45 17.43 31.21 2.95 3.70 3.85 2.95 3.28 5.00 100 200 200 200 600 200 Table 4: Supersense tagging data sets. tion is annotated in the corpus in the following way: han satte(VERB . COMMUNICATION) ham p˚a(COLL) plads(COLL). 3 Annotation process This section the describes the annotation task for supersenses, including detailes on corpus, guidelines and resulting agreement scores. For further information, cf. Olsen et al. (2015). 3.1 Corpus We have chosen to annotate from the Danish CLARIN Reference Corpus (Asmussen and Halskov, 2012), which consists of newspapers, magazines, oral debates, blogs, and social media.3 Table 4 lists the amount of training data (1,500 sentences in total) currently annotated for each domain. We describe each domain in terms of its average sentence length (SL) and proportion of tokens per type, namely the average amount of repetitions for a certain type. The final release will be made up of 600 sentences from all of the domains in Table 4, plus the test section of the Danish Dependency Tree"
W15-1814,W14-1214,0,0.013336,"and complexity. These analyses enable us to evaluate whether sentence-level simplification operations can be meaningfully and directly assessed using eye tracking, which would be of relevance to both manual and automated simplification efforts. Introduction 1.1 Intuitively, the readability of a text should reflect the effort that a reader must put into recognizing the meaning encoded in the text. As a concept, readability thus integrates both content and form. Sentence-level readability assessment is desirable from a computational point of view because Automatic Simplification by Compression Amancio et al. (2014) found that more than one fourth of the transformations observed in sentence pairs from Wikipedia and Single English Wikipedia were compressions. To obtain automatically simplified sentences we therefore train a sentence-compression model. Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 97 With inspiration from McDonald (2006), we train a sentence compression system on a corpus of parallel sentences of manually expert-simplified and original newswire text where all simplifications are compressions. The system is described in detail in section 2. Sentence"
W15-1814,W12-4903,0,0.0670919,"Missing"
W15-1814,P11-2117,0,0.0280778,"S-tags and parsing features are more reliable in the beginning of the sentence. This could be tested in the future by applying the model to text from a domain with different information structure. 5.2 Implications for System Development We found that the very simple compression model presented in this paper was performing extensive simplifications, which is important in light of the fact that humans consider it harder to produce more aggressive simplifications. We trained our model on a relatively small, specialized compression corpus. The Simple English Wikipedia simplification corpus (SEW) (Coster and Kauchak, 2011), which has been used in a range of statistical text simplification systems (Coster and Kauchak, 2011; Zhu et al., 2010; Woodsend and Lapata, 2011), is far bigger, but also noisier. We found fewer than 50 sentence pairs fitting our compression criteria when exploring the possibility of generating a similar training set for English from the SEW. However, in future work, other, smaller simplification corpora could be adapted to the task, providing insight into the robustness of using compression for simplification. 5.3 Implications for Evaluation Methodology In many natural language generation a"
W15-1814,W14-1213,0,0.0424761,"Missing"
W15-1814,C10-2032,0,0.0120436,"token deletions per sentence. 2.2 Compression Model and Features The compression model is a conditional random field (CRF) model trained to make a sequence of categorical decisions, in each determining whether the current word should be left out of the compression output while taking into account the previous decision. We used CRF++ (Lafferty et al., 2001) trained with default parameter settings. Below, we describe the features we implemented. The features focus on surface form, PoStags, dependencies and word frequency information. Our initial choice of features is based on the comparisons in Feng et al. (2010) and Falkenjack and J¨onsson (2014), who both find that parsing 1 The corpus was PoS-tagged and parsed using the Bohnet parser (Bohnet, 2010) trained on the Danish Dependency Treebank (Kromann, 2003) with Universal PoS-tags (Petrov et al., 2011). 2 Note that this dataset did not contribute to training, tuning or choosing the model. Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 98 Figure 1: We extract observed compressions from the simplification corpus and train an automatic compression model. For the eye tracking and subjective evaluation we run the mo"
W15-1814,klerke-sogaard-2012-dsim,1,0.860803,"slation (Stymne et al., 2013). Below, we present the automatic simplification setup, including the parallel data, features and model selection and details on how we select the data for the eye-tracking experiment. The following section details the eye movement recording and subjective evaluation setup. Section 4 presents our results followed by a discussion and our conclusions. 2 Automatic Simplification Setup 2.1 Training and Evaluation Corpus For the sentence compression training and evaluation data we extracted a subset of ordinary and simplified newswire texts from the Danish DSim corpus (Klerke and Søgaard, 2012). In Figure 1 we give a schematic overview of how the data for our experiments was obtained. For model development and selection we extracted all pairs of original and simplified sentences under the following criteria: 1. No sentence pair differs by more than 150 characters excluding punctuation. 2. The simplified sentence must be a strict subset of the original and contain a minimum of four tokens. 3. The original sentence must have at least one additional token compared to the simplified sentence and this difference must be nonpunctuation and of minimum three characters’ length. This results"
W15-1814,C10-1152,0,0.0263809,"he model to text from a domain with different information structure. 5.2 Implications for System Development We found that the very simple compression model presented in this paper was performing extensive simplifications, which is important in light of the fact that humans consider it harder to produce more aggressive simplifications. We trained our model on a relatively small, specialized compression corpus. The Simple English Wikipedia simplification corpus (SEW) (Coster and Kauchak, 2011), which has been used in a range of statistical text simplification systems (Coster and Kauchak, 2011; Zhu et al., 2010; Woodsend and Lapata, 2011), is far bigger, but also noisier. We found fewer than 50 sentence pairs fitting our compression criteria when exploring the possibility of generating a similar training set for English from the SEW. However, in future work, other, smaller simplification corpora could be adapted to the task, providing insight into the robustness of using compression for simplification. 5.3 Implications for Evaluation Methodology In many natural language generation and manipulation setups, it is important that the system is able Proceedings of the 20th Nordic Conference of Computatio"
W15-1814,E06-1038,0,0.0448541,"eaning encoded in the text. As a concept, readability thus integrates both content and form. Sentence-level readability assessment is desirable from a computational point of view because Automatic Simplification by Compression Amancio et al. (2014) found that more than one fourth of the transformations observed in sentence pairs from Wikipedia and Single English Wikipedia were compressions. To obtain automatically simplified sentences we therefore train a sentence-compression model. Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 97 With inspiration from McDonald (2006), we train a sentence compression system on a corpus of parallel sentences of manually expert-simplified and original newswire text where all simplifications are compressions. The system is described in detail in section 2. Sentence compression works by simply dropping parts of a sentence and outputting the shorter sentence with less information content and simpler syntax. This approach allows us to control a number of variables, and in particular, it guarantees that each expert simplification and each system output are true subsets of the original input, providing three highly comparable vers"
W15-1814,petrov-etal-2012-universal,0,0.020957,"Missing"
W15-1814,P05-1065,0,0.0471776,". Bjornsson (1983) and Flesch (1948)) and are by their own definitions unsuitable for automatic application (cf. Benjamin (2012) for an evaluation of readability-formula usability). The standard approach to assessing text readability in natural language processing (NLP) is to ask readers to judge the quality of the output in terms of comprehensibility, grammaticality and meaning preservation (cf. Siddharthan and Katsos (2012)). An alternative is to use existing text collections categorized by readability level for learning models of distinct categories of readability e.g. age or grade levels (Schwarm and Ostendorf, 2005; Vajjala and Meurers, 2014). In this paper we seek to establish whether readers share an intuitive conceptualization of the readability of single sentences, and to what extent this conceptualization is reflected in their reading behavior. We research this by comparing subjective sentence-level readability judgments to recordings of readers’ eye movements and by testing to what extent these measures co-vary across sentences of varying length and complexity. These analyses enable us to evaluate whether sentence-level simplification operations can be meaningfully and directly assessed using eye"
W15-1814,W12-2203,0,0.0299398,"h information into account with each decision. This computer-centric approach is in contrast to traditional human-centric readability metrics which are explicitly constructed for use at text level (cf. Bjornsson (1983) and Flesch (1948)) and are by their own definitions unsuitable for automatic application (cf. Benjamin (2012) for an evaluation of readability-formula usability). The standard approach to assessing text readability in natural language processing (NLP) is to ask readers to judge the quality of the output in terms of comprehensibility, grammaticality and meaning preservation (cf. Siddharthan and Katsos (2012)). An alternative is to use existing text collections categorized by readability level for learning models of distinct categories of readability e.g. age or grade levels (Schwarm and Ostendorf, 2005; Vajjala and Meurers, 2014). In this paper we seek to establish whether readers share an intuitive conceptualization of the readability of single sentences, and to what extent this conceptualization is reflected in their reading behavior. We research this by comparing subjective sentence-level readability judgments to recordings of readers’ eye movements and by testing to what extent these measures"
W15-1814,W13-5634,0,0.0308642,"ntence with less information content and simpler syntax. This approach allows us to control a number of variables, and in particular, it guarantees that each expert simplification and each system output are true subsets of the original input, providing three highly comparable versions of each sentence. Further the system serves as a proof of concept that a relatively small amount of taskspecific data can be sufficient for this task. Sentence compression is, in addition, an important step in several downstream NLP tasks, including summarization (Knight and Marcu, 2000) and machine translation (Stymne et al., 2013). Below, we present the automatic simplification setup, including the parallel data, features and model selection and details on how we select the data for the eye-tracking experiment. The following section details the eye movement recording and subjective evaluation setup. Section 4 presents our results followed by a discussion and our conclusions. 2 Automatic Simplification Setup 2.1 Training and Evaluation Corpus For the sentence compression training and evaluation data we extracted a subset of ordinary and simplified newswire texts from the Danish DSim corpus (Klerke and Søgaard, 2012). In"
W15-1814,W14-1203,0,0.0303104,"(1948)) and are by their own definitions unsuitable for automatic application (cf. Benjamin (2012) for an evaluation of readability-formula usability). The standard approach to assessing text readability in natural language processing (NLP) is to ask readers to judge the quality of the output in terms of comprehensibility, grammaticality and meaning preservation (cf. Siddharthan and Katsos (2012)). An alternative is to use existing text collections categorized by readability level for learning models of distinct categories of readability e.g. age or grade levels (Schwarm and Ostendorf, 2005; Vajjala and Meurers, 2014). In this paper we seek to establish whether readers share an intuitive conceptualization of the readability of single sentences, and to what extent this conceptualization is reflected in their reading behavior. We research this by comparing subjective sentence-level readability judgments to recordings of readers’ eye movements and by testing to what extent these measures co-vary across sentences of varying length and complexity. These analyses enable us to evaluate whether sentence-level simplification operations can be meaningfully and directly assessed using eye tracking, which would be of"
W15-1831,P07-1007,0,0.0315542,"2008; Marcheggiani and Artieres, 2014). Our experiments were simple, and several things can be done to improve results, i.e., by reducing sampling bias. In particular, several techniques have been introduced for improving out-of-domain performance using active learning. Rai et al. (2010) perform target-domain AL with a seed of source-domain data. Among other things, they propose to use the source and target unlabeled data to train a classifier to learn what target domain data points are similar to the source domain, in a way similar to Plank et al. (2014). For more work along these lines, see Chan and Ng (2007) and Xiao and Guo (2013). 7 Conclusions The systems that use the M AX selection strategy have a strong bias for the longest possible sentence, resulting from the low probability values obtained when calculating the prediction confidence of very long sequences. With few exceptions (e.g. an 11-word sentence on the 5th iteration for AM1 ), the systems exhaust the maximum-length sentences, and proceed to choose the longest available, and so forth. We do not take our individual annotator’s bias into consideration, but we believe that such bias plays a minor role in the differences of performance be"
W15-1831,D14-1097,0,0.0143127,"e take a small seed of data points, train a sequential labeling, and iterate over an unlabeled pool of data, selecting the data points our labeler is least confident about. In the AL literature, the selected data points are often those close to a decision boundary or those most likely to decrease overall uncertainty. This obviously leads to biased sampling, which can sometimes be avoided using different techniques, e.g., by exploiting cluster structure in the data. Generally, active learning for sequential labeling has received less attention than for classification (Settles and Craven, 2008; Marcheggiani and Artieres, 2014). Our experiments were simple, and several things can be done to improve results, i.e., by reducing sampling bias. In particular, several techniques have been introduced for improving out-of-domain performance using active learning. Rai et al. (2010) perform target-domain AL with a seed of source-domain data. Among other things, they propose to use the source and target unlabeled data to train a classifier to learn what target domain data points are similar to the source domain, in a way similar to Plank et al. (2014). For more work along these lines, see Chan and Ng (2007) and Xiao and Guo (2"
W15-1831,D14-1104,1,0.838297,"ss attention than for classification (Settles and Craven, 2008; Marcheggiani and Artieres, 2014). Our experiments were simple, and several things can be done to improve results, i.e., by reducing sampling bias. In particular, several techniques have been introduced for improving out-of-domain performance using active learning. Rai et al. (2010) perform target-domain AL with a seed of source-domain data. Among other things, they propose to use the source and target unlabeled data to train a classifier to learn what target domain data points are similar to the source domain, in a way similar to Plank et al. (2014). For more work along these lines, see Chan and Ng (2007) and Xiao and Guo (2013). 7 Conclusions The systems that use the M AX selection strategy have a strong bias for the longest possible sentence, resulting from the low probability values obtained when calculating the prediction confidence of very long sequences. With few exceptions (e.g. an 11-word sentence on the 5th iteration for AM1 ), the systems exhaust the maximum-length sentences, and proceed to choose the longest available, and so forth. We do not take our individual annotator’s bias into consideration, but we believe that such bia"
W15-1831,W10-0104,0,0.0158409,"or those most likely to decrease overall uncertainty. This obviously leads to biased sampling, which can sometimes be avoided using different techniques, e.g., by exploiting cluster structure in the data. Generally, active learning for sequential labeling has received less attention than for classification (Settles and Craven, 2008; Marcheggiani and Artieres, 2014). Our experiments were simple, and several things can be done to improve results, i.e., by reducing sampling bias. In particular, several techniques have been introduced for improving out-of-domain performance using active learning. Rai et al. (2010) perform target-domain AL with a seed of source-domain data. Among other things, they propose to use the source and target unlabeled data to train a classifier to learn what target domain data points are similar to the source domain, in a way similar to Plank et al. (2014). For more work along these lines, see Chan and Ng (2007) and Xiao and Guo (2013). 7 Conclusions The systems that use the M AX selection strategy have a strong bias for the longest possible sentence, resulting from the low probability values obtained when calculating the prediction confidence of very long sequences. With few"
W15-1831,D08-1112,0,0.0559162,"here are very standard. We take a small seed of data points, train a sequential labeling, and iterate over an unlabeled pool of data, selecting the data points our labeler is least confident about. In the AL literature, the selected data points are often those close to a decision boundary or those most likely to decrease overall uncertainty. This obviously leads to biased sampling, which can sometimes be avoided using different techniques, e.g., by exploiting cluster structure in the data. Generally, active learning for sequential labeling has received less attention than for classification (Settles and Craven, 2008; Marcheggiani and Artieres, 2014). Our experiments were simple, and several things can be done to improve results, i.e., by reducing sampling bias. In particular, several techniques have been introduced for improving out-of-domain performance using active learning. Rai et al. (2010) perform target-domain AL with a seed of source-domain data. Among other things, they propose to use the source and target unlabeled data to train a classifier to learn what target domain data points are similar to the source domain, in a way similar to Plank et al. (2014). For more work along these lines, see Chan"
W15-1831,W13-3501,0,0.0135807,"Artieres, 2014). Our experiments were simple, and several things can be done to improve results, i.e., by reducing sampling bias. In particular, several techniques have been introduced for improving out-of-domain performance using active learning. Rai et al. (2010) perform target-domain AL with a seed of source-domain data. Among other things, they propose to use the source and target unlabeled data to train a classifier to learn what target domain data points are similar to the source domain, in a way similar to Plank et al. (2014). For more work along these lines, see Chan and Ng (2007) and Xiao and Guo (2013). 7 Conclusions The systems that use the M AX selection strategy have a strong bias for the longest possible sentence, resulting from the low probability values obtained when calculating the prediction confidence of very long sequences. With few exceptions (e.g. an 11-word sentence on the 5th iteration for AM1 ), the systems exhaust the maximum-length sentences, and proceed to choose the longest available, and so forth. We do not take our individual annotator’s bias into consideration, but we believe that such bias plays a minor role in the differences of performance between M AX and S AMPLE."
W15-2005,W15-2000,0,0.180083,"Missing"
W15-2005,P13-4001,0,0.176245,"Missing"
W15-2005,J08-4004,0,0.223422,"roup n.phenomenon n.cognition n.building n.act n.artifact n.attribute n.communication n.person n.time n.institution n.body Table 4: Inter-annotator agreement κ across domains together with the percentage of double annotated files. This feed was published Tuesday September 21 at 10:00 and is saved under My garden. You can follow all comments to this feed via the RSS 2.0 feed. Figure 3: Disagreement for noun senses in NEWSWIRE . In such cases the annotators reached a consensus on how to tag the blog-specific metadata. Table 4 shows that even if agreement results are generally good for the task (Artstein and Poesio, 2008), not all textual domains are equally easy to annotate. NEWSWIRE and PARLIAMENT show the lowest agreement, which is a somewhat surprising finding, because these texts are the most canonical and elaborate and thus arguably easier to understand and annotate. FORUM has 300 sentences, unlike the other domains, which have double the amount. This difference has an impact in the chance-correction measure of the κ coefficient, making the chance-adjustment more severe. However, NEWSWIRE has more semantic types than e.g. BLOG (see Figure 3, 4 and 5), and the more varied the text, the more difficult it w"
W15-2005,brown-etal-2010-number,0,0.39602,"Missing"
W15-2005,W03-1022,0,0.350322,"Missing"
W15-2005,de-melo-etal-2012-empirical,0,0.0579156,"Missing"
W15-2005,H92-1045,0,0.609943,"the EuroWordNet top ontology as described in Pedersen et al. (2009) and Vossen (1998), have enabled us to automatically the word senses defined for the Danish vocabulary onto the cross-lingual supersenses. These are based on the Princeton Wordnet lexicographical classes1 and have become a popular choice for coarse-grained sense tagging with the advantage of being applicable across languages. 1 Introduction It is commonly observed that word meanings vary substantially across textual domains, so that an appropriate sense inventory for one domain may be inappropriate or insufficient for another (Gale et al., 1992). This essential quality of the lexicon poses a huge challenge to natural language processing and underlines the need for developing systems that are generally less sensitive to domain shifts. The present work is framed within a project that deals with sense inventories of different granularity and across textual domains. The overall goal is to discover what sense inventories and algorithms are manageable for annotation purposes and useful for automatic sense 1 https://wordnet.princeton.edu/man/ lexnames.5WN.html Sussi Olsen, Bolette S. Pedersen, Héctor Martínez Alonso and Anders Johannsen 201"
W15-2005,W15-1806,1,0.848138,"Missing"
W15-2005,W15-1831,1,0.564731,"Missing"
W15-2711,agirre-etal-2006-methodology,1,0.773099,"series of English lexical-sample words from Passonneau et al. (2012), with several annotation rounds. We include the second, third and fourth round of annotation in our experiments. We use on the whole dataset (M ASCEW) pooling all the rounds together, as well as on each round independently, namely MASCE 2, MASCE 3 and M ASCE 4. 6 F NTW The English Twitter FrameNet data of Søgaard et al. (2015). We treat the framename layer as a word-sense layer, and disregard the arguments. 7 E NSST The English supersense-annotated data of Johannsen et al. (2014). 8 E USC The Basque lexical-sample SemCor of Agirre et al. (2006). 9 DASST The Danish supersense-annotated data of Mart´ınez Alonso et al. (2015). Table 1 provides the characteristics of the datasets. The annotation task can be lexical-sample (ls) or all-words (aw). The number of instances is different from the number of sentences for all-words annotation. The type of annotators can be expert (ex) or crowdsourced (cs). The α scores can differ from those reported in the datasets’ documentation given our example-selection criteria. The last two columns describe the target variables of observed agreement (Ao ) and the proportion of low-, midand high-agreement"
W15-2711,J08-4004,0,0.0589102,"Introduction Sense-annotation tasks show less-than-perfect agreement scores. However, variation in agreement is not the result of featureless, white noise in the annotations; Krippendorff (2011) defines disagreement as by chance—caused by unavoidable inconsistencies in annotator behavior—and systematic—caused by properties of the data. Our goal is to predict the agreement of senseannotated examples by examining their linguistic properties. If we can identify properties predictive of low or high agreement, then we can claim that some of the agreement variation in the data is indeed systematic. Artstein and Poesio (2008) provide an interpretation of Kripperdorff’s α coefficient to describe the reliability of a whole annotation task and the way that observed agreement (Ao ) is calculated for each example. Strictly speaking, the value of α only provides an indication of the replicability 2 Related work In their study, Yarowsky and Florian (2002) examine the relation between agreement variation and predictive power of word-sense disambiguation systems, which is later expanded by Lopez de Lacalle and Agirre (2015a). Our work is different in that we do not study the relation between agreement and performance, but"
W15-2711,passonneau-etal-2010-word,0,0.413538,"Missing"
W15-2711,H92-1045,0,0.452662,"f_n c_slength_n c_maxidf_n c_content_proportion_n a_targetfreq_n a_headfreq_n Correlation with Ao Figure 1: Correlation between the numeric-valued features and Ao for MASCC and FNTW properties of the senses such abstractness. Context features can also be expanded by adding information from word sense induction and distributional models. Moreover, if we are to examine agreement variation in full-document (as opposed to sentence-bysentence) annotation, we suggest that documentlevel frequency would help concretize the meaning of a certain word, following the principle of one sense per discourse (Gale et al., 1992). If the numeric prediction of agreement is desirable over classification, a metric like annotation entropy (Lopez de Lacalle and Agirre, 2015a) is worth considering as an alternative measure to Ao , since it an information-theoretical measure that also gives account for distribution skewness. ther all-words or groupings of lexical-sample annotations for different words (e.g. MASC 2 contains examples of fair-j, know-v, land-n, etc.), which means that some of the class- or lemma-dependent features might be swamped by the superposition of features from the other words. Nevertheless, the systems"
W15-2711,passonneau-etal-2012-masc,0,0.0148026,"esearch efforts advocate for models of annotator behavior (Passonneau et al., 2009; Passonneau et al., 2010; Passonneau and Carpenter, 2014; Cohn and Specia, 2013). 3 Data We conduct our study on sense-annotated datasets, keeping only the examples with at least two annotations per item. In the datasets with two annotators and one adjudicator, we disregard adjudications given their potentially different bias. 1 MASCC The English crowdsourced lexicalsample word-sense corpus from Passonneau and Carpenter (2014). 2-5 MASCE * The expert annotations for a series of English lexical-sample words from Passonneau et al. (2012), with several annotation rounds. We include the second, third and fourth round of annotation in our experiments. We use on the whole dataset (M ASCEW) pooling all the rounds together, as well as on each round independently, namely MASCE 2, MASCE 3 and M ASCE 4. 6 F NTW The English Twitter FrameNet data of Søgaard et al. (2015). We treat the framename layer as a word-sense layer, and disregard the arguments. 7 E NSST The English supersense-annotated data of Johannsen et al. (2014). 8 E USC The Basque lexical-sample SemCor of Agirre et al. (2006). 9 DASST The Danish supersense-annotated data of"
W15-2711,S14-1001,1,0.873417,"Missing"
W15-2711,P14-2083,0,0.0814323,"2002) examine the relation between agreement variation and predictive power of word-sense disambiguation systems, which is later expanded by Lopez de Lacalle and Agirre (2015a). Our work is different in that we do not study the relation between agreement and performance, but between example properties and agreement. Mart´ınez Alonso (2013) experiments with prediction of agreement for coarse-sense annotation. Tomuro (2001) uses the disagreement between annotators of two English sense-annotated corpora to provide insights on the relations between synsets, and more recent studies (Jurgens, 2013; Plank et al., 2014; Jurgens, 2014; Lopez de Lacalle and Agirre, 2015b) have empirically tackled 89 Proceedings of the EMNLP 2015 Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 89–94, c Lisboa, Portugal, 18 September 2015. 2015 Association for Computational Linguistics. stance, we obtain features for a word w and its syntactic parent p in a sentence s, organized in feature groups. The word identities of w and p are not included in the features to keep the models more general. Number of features are in parentheses. Frequency(2) We calculate the frequency of w and p, scaling"
W15-2711,N13-1062,0,0.0538415,"y and Florian (2002) examine the relation between agreement variation and predictive power of word-sense disambiguation systems, which is later expanded by Lopez de Lacalle and Agirre (2015a). Our work is different in that we do not study the relation between agreement and performance, but between example properties and agreement. Mart´ınez Alonso (2013) experiments with prediction of agreement for coarse-sense annotation. Tomuro (2001) uses the disagreement between annotators of two English sense-annotated corpora to provide insights on the relations between synsets, and more recent studies (Jurgens, 2013; Plank et al., 2014; Jurgens, 2014; Lopez de Lacalle and Agirre, 2015b) have empirically tackled 89 Proceedings of the EMNLP 2015 Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 89–94, c Lisboa, Portugal, 18 September 2015. 2015 Association for Computational Linguistics. stance, we obtain features for a word w and its syntactic parent p in a sentence s, organized in feature groups. The word identities of w and p are not included in the features to keep the models more general. Number of features are in parentheses. Frequency(2) We calculate the frequency"
W15-2711,jurgens-2014-analysis,0,0.0211186,"lation between agreement variation and predictive power of word-sense disambiguation systems, which is later expanded by Lopez de Lacalle and Agirre (2015a). Our work is different in that we do not study the relation between agreement and performance, but between example properties and agreement. Mart´ınez Alonso (2013) experiments with prediction of agreement for coarse-sense annotation. Tomuro (2001) uses the disagreement between annotators of two English sense-annotated corpora to provide insights on the relations between synsets, and more recent studies (Jurgens, 2013; Plank et al., 2014; Jurgens, 2014; Lopez de Lacalle and Agirre, 2015b) have empirically tackled 89 Proceedings of the EMNLP 2015 Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 89–94, c Lisboa, Portugal, 18 September 2015. 2015 Association for Computational Linguistics. stance, we obtain features for a word w and its syntactic parent p in a sentence s, organized in feature groups. The word identities of w and p are not included in the features to keep the models more general. Number of features are in parentheses. Frequency(2) We calculate the frequency of w and p, scaling by log(rank(x)"
W15-2711,C12-1095,0,0.0256845,"Missing"
W15-2711,W15-0114,1,0.858821,"Missing"
W15-2711,S15-1007,1,0.770663,"Missing"
W15-2711,W15-1806,1,0.706152,"ral annotation rounds. We include the second, third and fourth round of annotation in our experiments. We use on the whole dataset (M ASCEW) pooling all the rounds together, as well as on each round independently, namely MASCE 2, MASCE 3 and M ASCE 4. 6 F NTW The English Twitter FrameNet data of Søgaard et al. (2015). We treat the framename layer as a word-sense layer, and disregard the arguments. 7 E NSST The English supersense-annotated data of Johannsen et al. (2014). 8 E USC The Basque lexical-sample SemCor of Agirre et al. (2006). 9 DASST The Danish supersense-annotated data of Mart´ınez Alonso et al. (2015). Table 1 provides the characteristics of the datasets. The annotation task can be lexical-sample (ls) or all-words (aw). The number of instances is different from the number of sentences for all-words annotation. The type of annotators can be expert (ex) or crowdsourced (cs). The α scores can differ from those reported in the datasets’ documentation given our example-selection criteria. The last two columns describe the target variables of observed agreement (Ao ) and the proportion of low-, midand high-agreement instances, cf. 3.2 for details. 3.1 3.2 Target variable Regression Instance-wise"
W15-2711,D10-1004,0,0.0507306,"Missing"
W15-2711,Q14-1025,0,0.0830639,"Missing"
W15-2711,W09-2402,0,\N,Missing
W15-2711,P13-1004,0,\N,Missing
W16-1706,P13-1004,0,0.0701839,"Missing"
W16-1706,S14-1001,1,0.886972,"Missing"
W16-1706,N13-1062,0,0.0585746,"Missing"
W16-1706,W15-2711,1,0.817393,"Missing"
W16-1706,W15-1806,1,0.891015,"Missing"
W16-1706,N15-1152,1,0.559133,"Missing"
W16-1706,H94-1046,0,0.475658,"Missing"
W16-1706,Q14-1025,0,0.136872,"Missing"
W16-1706,passonneau-etal-2010-word,0,0.0561585,"Missing"
W16-1706,L16-1136,1,0.889351,"Missing"
W16-1706,E14-1078,1,0.909187,"Cognition, University of Groningen, The Netherlands ♠ Univ. Paris Diderot, Sorbonne Paris Cit – Alpage, INRIA, France hector.martinez-alonso@inria.fr,anders@johannsen.com,b.plank@rug.nl Abstract tion in agreement, providing smaller losses when a classifier training decision makes a misclassification that matches with human disagreement. For example, the loss for predicting a particle instead of an adverb is smaller than the loss for predicting a noun instead of an adverb, because the particle/adverb confusion is fairly common among annotators (Sec. 3). In this article, we apply the method of Plank et al. (2014a) to a semantic sequence-prediction task, namely supersense tagging (SST). SST is considered a more difficult task than POS tagging, because the semantic classes are more dependent on world knowledge, and the number of supersenses is higher than the number of POS labels. We experiment with different methods to calculate the label-wise agreement (Sec. 3.1), and apply these methods to datasets in two languages, namely English and Danish (Sec. 3.2). Moreover, we also perform cross-linguistic experiments to assess how much of the annotation variation in one language can be applied to another. Lin"
W16-1706,P14-2083,1,0.885319,"Cognition, University of Groningen, The Netherlands ♠ Univ. Paris Diderot, Sorbonne Paris Cit – Alpage, INRIA, France hector.martinez-alonso@inria.fr,anders@johannsen.com,b.plank@rug.nl Abstract tion in agreement, providing smaller losses when a classifier training decision makes a misclassification that matches with human disagreement. For example, the loss for predicting a particle instead of an adverb is smaller than the loss for predicting a noun instead of an adverb, because the particle/adverb confusion is fairly common among annotators (Sec. 3). In this article, we apply the method of Plank et al. (2014a) to a semantic sequence-prediction task, namely supersense tagging (SST). SST is considered a more difficult task than POS tagging, because the semantic classes are more dependent on world knowledge, and the number of supersenses is higher than the number of POS labels. We experiment with different methods to calculate the label-wise agreement (Sec. 3.1), and apply these methods to datasets in two languages, namely English and Danish (Sec. 3.2). Moreover, we also perform cross-linguistic experiments to assess how much of the annotation variation in one language can be applied to another. Lin"
W16-1706,W96-0213,0,0.590461,"Missing"
W16-1706,W08-1203,0,0.483304,"Missing"
W16-1706,D11-1141,0,0.0139462,"Missing"
W16-1706,N01-1010,0,0.170134,"Missing"
W16-1706,S15-1007,0,\N,Missing
W16-1706,K15-1033,1,\N,Missing
W16-1801,W02-2001,0,0.0506867,"Missing"
W16-1801,W11-1602,0,0.0669186,"Missing"
W16-1801,I05-5001,0,0.0382306,"ets and candidates, skip-grams longest common subsequences, POS tags and proper names. Connor and Roth (2007) develop a global classifier that takes a word v and its context, along with a candidate word u, and determines whether u can replace v in the given context while maintaining the original meaning. Their work focuses on verb paraphrasing. Notions of context include: being either subject or object of the verb, named entities that appear as subject or object, all dependency links connected to the target, all noun phrases in sentences containing the target, or all of the above. The work of Brockett and Dolan (2005) uses annotated datasets and Support Vector Machines (SVMs) to induce larger monolingual paraphrase corpora from a comparable corpus of news clusters found on the World Wide Web. Features in2.2 Multi-word Expression Resources While there are some works on the extraction of multi-word expressions and on investigation of their impact on different NLP applications, as far as we know, there is no single work dedicated 2 on paraphrasing multi-word expressions. Various approaches exist for the extraction of MWEs: Tsvetkov and Wintner (2010) present an approach to extract MWEs from parallel corpora."
W16-1801,S07-1009,0,0.213274,"Missing"
W16-1801,W03-1810,0,0.0513713,"s gradient boosting Figure 1: Paraphrase targets (a) and paraphrase candidates (b). We select targets that have at least five candidates in our combined paraphrase resources. The paraphrase resources (S) for candidates generations are composed of collections from PPDB (Pavlick et al., 2015), WordNet and JoBimText distributional thesaurus (DT – only for single words). For MWE paraphrase targets, we have used different MWE resources. A total of 79,349 MWE are collected from WordNet, STREUSLE (Schneider and Smith, 2015; Schneider et al., 2014)4 , Wiki50 (Vincze et al., 2011) and the MWE project (McCarthy et al., 2003; Baldwin and Villavicencio, 2002)5 . We consider MWEs from this resources to be a paraphrase target when it is possible to generate paraphrase candidates from our paraphrase resources (S). Candidates paraphrases for a target (both single and MWE) are generated as follows. For each paraphrase target, we retrieve candidates from the 2 http://www.anc.org/ https://www.mturk.com/mturk/help? helpPage=overview 4 http://www.cs.cmu.edu/˜ark/LexSem/ 5 http://mwe.stanford.edu 3 6 https://people.cs.umass.edu/˜vdang/ ranklib.html 7 http://sourceforge.net/projects/ lemur/ 4 Figure 2: User-interface for par"
W16-1801,W15-1501,0,0.0140988,"w 4 http://www.cs.cmu.edu/˜ark/LexSem/ 5 http://mwe.stanford.edu 3 6 https://people.cs.umass.edu/˜vdang/ ranklib.html 7 http://sourceforge.net/projects/ lemur/ 4 Figure 2: User-interface for paraphrase selection. to directly optimize learning-to-rank specific cost functions such as Normalized Discounted Cumulative Gain (NDCG) and Mean Average Precision (MAP). 3.4 by averaging the embeddings of their parts. We use the word embeddings of the target (F 1) and the candidate (F 2) phrases. Candidate-Target similarities: The dot product of the target and candidate embeddings (F 3), as described in (Melamud et al., 2015). Target-Sentence similarity: The dot product between a candidate and the sentence, i.e. the average embeddings of all words in the sentence (F 4). The following features use local context information: Target-Close context similarity: The dot product between the candidate and the left and right 3-gram (F 5) and 5-gram embedding (F 6) resp.. Ngram features: A normalized frequency for a 2-5-gram context with the target and candidate phrases (F 7) based on Google Web 1T 5-Grams10 . Language model score: A normalized language model score using a sentence as context with the target and candidate ph"
W16-1801,W11-0805,0,0.0427054,"Missing"
W16-1801,P11-1027,0,0.046514,"roduct between a candidate and the sentence, i.e. the average embeddings of all words in the sentence (F 4). The following features use local context information: Target-Close context similarity: The dot product between the candidate and the left and right 3-gram (F 5) and 5-gram embedding (F 6) resp.. Ngram features: A normalized frequency for a 2-5-gram context with the target and candidate phrases (F 7) based on Google Web 1T 5-Grams10 . Language model score: A normalized language model score using a sentence as context with the target and candidate phrases (F 8). An n-gram language model (Pauls and Klein, 2011) is built using the BNC and Wikipedia corpora. Also, we experimented with features that eventually did not improve results, such as the embeddings of the target’s n = 5 most similar words, length and length ratios between target and candidate, most similar words and number of shared senses among target and candidate phrases based JoBimText DT (Ruppert et al., 2015), and N-gram POS sequences and dependency labels of the tarFeatures We have modeled three types of features: a resource-based feature where feature values are taken from a lexical resource (F 0), four features based on global context"
W16-1801,P15-2070,0,0.144898,"Missing"
W16-1801,P16-1012,1,0.889804,"Missing"
W16-1801,ramisch-etal-2010-mwetoolkit,0,0.0239911,"extraction of MWEs: Tsvetkov and Wintner (2010) present an approach to extract MWEs from parallel corpora. They align the parallel corpus and focus on misalignment, which typically indicates expressions in the source language that are translated to the target in a non-compositional way. Frantzi et al. (2000) present a method to extract multi-word terms from English corpora, which combines linguistic and statistical information. The Multi-word Expression Toolkit (MWEtoolkit) extracts MWE candidates based on flat n-grams or specific morphosyntactic patterns (of surface forms, lemmas, POS tags) (Ramisch et al., 2010) and apply different fillters ranging form simple count thresholds to a more complex cases such as Association Measures (AMs). The tool further supports indexing and searching of MWEs, validation, and annotation facilities. Schneider et al. (2014) developed a sequencetagging-based supervised approach to MWE identification. A rich set of features has been used in a linguistically-driven evaluation of the identification of heterogeneous MWEs. The work by Vincze et al. (2011) constructs a multi-word expression corpus annotated with different types of MWEs such as compound, idiom, verb-particle co"
W16-1801,N06-1058,0,0.0788485,"Missing"
W16-1801,P15-4018,1,0.771204,"of only single words. platform. In the first annotation task, a total of 171 sentences are selected from the British Academic Written English (BAWE) corpus1 (Alsop and Nesi, 2009), with five paraphrase targets. The targets are selected in such a way that a) include MWEs as targets when it is possible (see Subection 3.2 how we select targets), b) the candidates could bear more than one contextual meaning and, c) workers can select up to three paraphrases and have to supply their own paraphrase if none of the candidates match. To satisfy condition b), we have used the JoBimText DT database API (Ruppert et al., 2015) to obtain single word candidates with multiple senses according to automatic sense induction. We conduct this annotation setup twice, both with and without showing the original context (3– 8 sentences). For both setups, a task is assigned to 5 workers. We incorporate control questions with invalid candidate paraphrases in order to reject unreliable workers. In addition to the control questions, JavaScript functions are embedded to ensure that workers select or supply at least one paraphrase. The results are aggregated by summing the number of workers that agreed on candidates, for scores betw"
W16-1801,E14-1057,0,0.0632122,"Missing"
W16-1801,N15-1177,0,0.0253392,"roject7 . In this paper, we present the results obtained using LambdaMART. LambdaMART (Burges, 2010) uses gradient boosting Figure 1: Paraphrase targets (a) and paraphrase candidates (b). We select targets that have at least five candidates in our combined paraphrase resources. The paraphrase resources (S) for candidates generations are composed of collections from PPDB (Pavlick et al., 2015), WordNet and JoBimText distributional thesaurus (DT – only for single words). For MWE paraphrase targets, we have used different MWE resources. A total of 79,349 MWE are collected from WordNet, STREUSLE (Schneider and Smith, 2015; Schneider et al., 2014)4 , Wiki50 (Vincze et al., 2011) and the MWE project (McCarthy et al., 2003; Baldwin and Villavicencio, 2002)5 . We consider MWEs from this resources to be a paraphrase target when it is possible to generate paraphrase candidates from our paraphrase resources (S). Candidates paraphrases for a target (both single and MWE) are generated as follows. For each paraphrase target, we retrieve candidates from the 2 http://www.anc.org/ https://www.mturk.com/mturk/help? helpPage=overview 4 http://www.cs.cmu.edu/˜ark/LexSem/ 5 http://mwe.stanford.edu 3 6 https://people.cs.umass.e"
W16-1801,D13-1198,0,0.0486316,"Missing"
W16-1801,W09-2506,0,0.0368321,"Missing"
W16-1801,C10-2144,0,0.0185508,"ences containing the target, or all of the above. The work of Brockett and Dolan (2005) uses annotated datasets and Support Vector Machines (SVMs) to induce larger monolingual paraphrase corpora from a comparable corpus of news clusters found on the World Wide Web. Features in2.2 Multi-word Expression Resources While there are some works on the extraction of multi-word expressions and on investigation of their impact on different NLP applications, as far as we know, there is no single work dedicated 2 on paraphrasing multi-word expressions. Various approaches exist for the extraction of MWEs: Tsvetkov and Wintner (2010) present an approach to extract MWEs from parallel corpora. They align the parallel corpus and focus on misalignment, which typically indicates expressions in the source language that are translated to the target in a non-compositional way. Frantzi et al. (2000) present a method to extract multi-word terms from English corpora, which combines linguistic and statistical information. The Multi-word Expression Toolkit (MWEtoolkit) extracts MWE candidates based on flat n-grams or specific morphosyntactic patterns (of surface forms, lemmas, POS tags) (Ramisch et al., 2010) and apply different fillt"
W16-1801,R11-1040,0,0.134303,"extracts MWE candidates based on flat n-grams or specific morphosyntactic patterns (of surface forms, lemmas, POS tags) (Ramisch et al., 2010) and apply different fillters ranging form simple count thresholds to a more complex cases such as Association Measures (AMs). The tool further supports indexing and searching of MWEs, validation, and annotation facilities. Schneider et al. (2014) developed a sequencetagging-based supervised approach to MWE identification. A rich set of features has been used in a linguistically-driven evaluation of the identification of heterogeneous MWEs. The work by Vincze et al. (2011) constructs a multi-word expression corpus annotated with different types of MWEs such as compound, idiom, verb-particle constructions, light verb constructions, and others. In our work, we have used a combination of many MWEs resources from different sources for both MWE target detection and candidate generation (see Subsection 3.2). 3 No context Context MWE (ρ) 0.25 0.23 Single (ρ) 0.36 0.32 Table 1: Spearman correlation of human judgment with PPDB2 default rankings. The column MWE shows the result of only MWEs and the column Single shows the result of only single words. platform. In the fir"
W16-1801,Q14-1016,0,\N,Missing
W16-3905,W09-3821,0,0.0643458,"Missing"
W16-3905,C10-2013,0,0.0199812,"had to make bananas the main i i 20 predicate, and treat freeze as a subordinate clause. Regardless of the difficulty of the domain, it appears that a UD dependency analysis lends itself to dependency annotation in an easier way: Since non-leaf relations appear between lexical words, this representation is more robust to missing determiners, prepositions and punctuations, even phrases. Also, if we used other dependency formalisms that for instance place prepositions as heads of nouns, it would be more difficult to annotate as it is the case using the current French Treebank dependency scheme (Candito et al., 2010). Nevertheless, dependency analyses conflate functional and structural information (Silveira and Manning, 2015), and some of the structural information can be lost in cases such as the Example C, discussed above. Annotating dependencies lends itself well to noisy user-generated data. In a strict lexicalist analysis such as UD, where there are no tokens for unobserved words (e.g. dropped subjects, missing main verbs), we must build a structure from the existing words, and not from idealised sentence representations. We finally observe that for UGC, shorter sentences are harder to annotate. Inde"
W16-3905,N13-1037,0,0.0627008,"f the main causes behind the expectable performance drops in tagging and parsing. We therefore conduct a series of automatic and manual inspections to better understand the linguistic phenomena behind UGC linguistic variability. We explore the relation between predicting performance and annotation difficulty, which is seldom explicitly addressed (Plank et al., 2015). 4 A Threefold Categorisation for UGC Idiosyncrasies Even though user-generated content does not constitute a uniform genre, many works have characterised its idiosyncrasies (Foster, 2010; Gimpel et al., 2011; Seddah et al., 2012; Eisenstein, 2013), which can be characterised on three axes, defined by the intentionality or communication needs of the word variants: 1. Encoding simplification: This axis covers ergographic phenomena, i.e.,phenomena aiming at reducing the writing effort, perceived as first glance as genuine misspell errors, and transverse phe4 In fact, in the case of L EAGUE OF L EGENDS in-game data, the normalisation step adds a significant amount of noise. A solution to this problem and more generally to the limitations of deterministic rule-based normalisation lies in the development of non-supervised or semi-supervised"
W16-3905,P11-1118,0,0.0222664,"n. It is far from being the case for UGC data, as shown by Foster (2010). Indeed, in her seminal work on parsing web data, different issues preventing reasonably good parsing performance were highlighted; most of them were tied to lexical differences (coming from either genuine unknown words, typographical divergences, bad segmentation, etc.) or syntactic structures absent from training data (imperative usage, direct discourse, slang, etc.). This suboptimal parsing behavior on web data was in turn confirmed in follow-up works on Twitter and IRC chat (Foster et al., 2011a; Gimpel et al., 2010; Elsner and Charniak, 2011). They were again confirmed during the SANCL shared task, organised by Google, aimed at assessing the performances of parsers on various genres of Web texts (Petrov and McDonald, 2012). Foster (2010) and Foster et al. (2011b) noted that simple lexical and tokenisation convention adaptation to the Wall-Street Journal text genre could increase the parsing performance by a large margin. In addition, Seddah et al. (2012) showed that a certain amount of normalisation brought a large improvement in POS tagger performance of French social media texts. These normalisation steps mostly apply at the lex"
W16-3905,W07-2204,1,0.86197,"Missing"
W16-3905,I11-1100,0,0.0964601,"Missing"
W16-3905,N10-1060,0,0.65778,"a, Japan, December 11 2016. 2 Related Work Before the global availability of social-media feeds, studies on the difficulties of out-of-domain statistical parsing have been focusing mainly on slightly different newspaper texts (Gildea, 2001; McClosky et al., 2006b; McClosky et al., 2006a), biomedical data (Lease and Charniak, 2005; McClosky and Charniak, 2008) or balanced corpora mixing different genres (Foster et al., 2007). For such data, which is as edited as standard data sources, the problem is “simply” a matter of domain adaptation. It is far from being the case for UGC data, as shown by Foster (2010). Indeed, in her seminal work on parsing web data, different issues preventing reasonably good parsing performance were highlighted; most of them were tied to lexical differences (coming from either genuine unknown words, typographical divergences, bad segmentation, etc.) or syntactic structures absent from training data (imperative usage, direct discourse, slang, etc.). This suboptimal parsing behavior on web data was in turn confirmed in follow-up works on Twitter and IRC chat (Foster et al., 2011a; Gimpel et al., 2010; Elsner and Charniak, 2011). They were again confirmed during the SANCL s"
W16-3905,W01-0521,0,0.0192116,"UGC, and (iii) the first corpus obtained from M INECRAFT and L EAGUE OF L EGENDS gaming logs. All corpora and annotations are freely available. This work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 13 Proceedings of the 2nd Workshop on Noisy User-generated Text, pages 13–23, Osaka, Japan, December 11 2016. 2 Related Work Before the global availability of social-media feeds, studies on the difficulties of out-of-domain statistical parsing have been focusing mainly on slightly different newspaper texts (Gildea, 2001; McClosky et al., 2006b; McClosky et al., 2006a), biomedical data (Lease and Charniak, 2005; McClosky and Charniak, 2008) or balanced corpora mixing different genres (Foster et al., 2007). For such data, which is as edited as standard data sources, the problem is “simply” a matter of domain adaptation. It is far from being the case for UGC data, as shown by Foster (2010). Indeed, in her seminal work on parsing web data, different issues preventing reasonably good parsing performance were highlighted; most of them were tied to lexical differences (coming from either genuine unknown words, typo"
W16-3905,P11-2008,0,0.306203,"Missing"
W16-3905,C16-1286,0,0.0226449,"shorter sentences are harder to annotate. Indeed, sentences closer to the lower threshold of 4 tokens we have determined, seem to present more ellipsis, while longer sentences in our data have structures closer to more canonical syntax. 5.7 The Unspoken Costs of Treebank Annotation As we all know, creating annotated data is a rewarding task, extremely useful for evaluation as well as for building feature-rich supervised models. Yet, it is time consuming and as generally said, relatively costly (Schneider, 2015) even though crowd-sourcing solutions through games with a purpose start to emerge (Guillaume et al., 2016). The dataset we presented in that paper are part of process that was initiated 5 years ago when we were confronted to the lack of syntactically-annotated out-of-domain dataset for French. The purely syntactic annotation phase for the LoL and Minecraft data is still ongoing and we expect it to be finished in the first few months of 2017. It is important to consider that such a task was made possible because of the experience we gained along the years and because we relied on a highly trained team of annotators. This training was the most important point in term of costs and had to be extended"
W16-3905,D15-1157,0,0.166833,"Missing"
W16-3905,I05-1006,0,0.0412939,"gaming logs. All corpora and annotations are freely available. This work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 13 Proceedings of the 2nd Workshop on Noisy User-generated Text, pages 13–23, Osaka, Japan, December 11 2016. 2 Related Work Before the global availability of social-media feeds, studies on the difficulties of out-of-domain statistical parsing have been focusing mainly on slightly different newspaper texts (Gildea, 2001; McClosky et al., 2006b; McClosky et al., 2006a), biomedical data (Lease and Charniak, 2005; McClosky and Charniak, 2008) or balanced corpora mixing different genres (Foster et al., 2007). For such data, which is as edited as standard data sources, the problem is “simply” a matter of domain adaptation. It is far from being the case for UGC data, as shown by Foster (2010). Indeed, in her seminal work on parsing web data, different issues preventing reasonably good parsing performance were highlighted; most of them were tied to lexical differences (coming from either genuine unknown words, typographical divergences, bad segmentation, etc.) or syntactic structures absent from training"
W16-3905,P08-2026,0,0.0138217,"and annotations are freely available. This work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 13 Proceedings of the 2nd Workshop on Noisy User-generated Text, pages 13–23, Osaka, Japan, December 11 2016. 2 Related Work Before the global availability of social-media feeds, studies on the difficulties of out-of-domain statistical parsing have been focusing mainly on slightly different newspaper texts (Gildea, 2001; McClosky et al., 2006b; McClosky et al., 2006a), biomedical data (Lease and Charniak, 2005; McClosky and Charniak, 2008) or balanced corpora mixing different genres (Foster et al., 2007). For such data, which is as edited as standard data sources, the problem is “simply” a matter of domain adaptation. It is far from being the case for UGC data, as shown by Foster (2010). Indeed, in her seminal work on parsing web data, different issues preventing reasonably good parsing performance were highlighted; most of them were tied to lexical differences (coming from either genuine unknown words, typographical divergences, bad segmentation, etc.) or syntactic structures absent from training data (imperative usage, direct"
W16-3905,P06-1043,0,0.0310279,"the first corpus obtained from M INECRAFT and L EAGUE OF L EGENDS gaming logs. All corpora and annotations are freely available. This work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 13 Proceedings of the 2nd Workshop on Noisy User-generated Text, pages 13–23, Osaka, Japan, December 11 2016. 2 Related Work Before the global availability of social-media feeds, studies on the difficulties of out-of-domain statistical parsing have been focusing mainly on slightly different newspaper texts (Gildea, 2001; McClosky et al., 2006b; McClosky et al., 2006a), biomedical data (Lease and Charniak, 2005; McClosky and Charniak, 2008) or balanced corpora mixing different genres (Foster et al., 2007). For such data, which is as edited as standard data sources, the problem is “simply” a matter of domain adaptation. It is far from being the case for UGC data, as shown by Foster (2010). Indeed, in her seminal work on parsing web data, different issues preventing reasonably good parsing performance were highlighted; most of them were tied to lexical differences (coming from either genuine unknown words, typographical divergences,"
W16-3905,N06-1020,0,0.0421604,"the first corpus obtained from M INECRAFT and L EAGUE OF L EGENDS gaming logs. All corpora and annotations are freely available. This work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 13 Proceedings of the 2nd Workshop on Noisy User-generated Text, pages 13–23, Osaka, Japan, December 11 2016. 2 Related Work Before the global availability of social-media feeds, studies on the difficulties of out-of-domain statistical parsing have been focusing mainly on slightly different newspaper texts (Gildea, 2001; McClosky et al., 2006b; McClosky et al., 2006a), biomedical data (Lease and Charniak, 2005; McClosky and Charniak, 2008) or balanced corpora mixing different genres (Foster et al., 2007). For such data, which is as edited as standard data sources, the problem is “simply” a matter of domain adaptation. It is far from being the case for UGC data, as shown by Foster (2010). Indeed, in her seminal work on parsing web data, different issues preventing reasonably good parsing performance were highlighted; most of them were tied to lexical differences (coming from either genuine unknown words, typographical divergences,"
W16-3905,C14-1166,0,0.075608,"without normalisation. The tagger (Denis and Sagot, 2012) was trained on the canonical training section of the French Treebank (Abeillé et al., 2003) instance, F TB - UC, from (Candito and Crabbé, 2009). We used an extended version of the rewriting rules used to pre-annotate the French Social Media Bank (Seddah et al., 2012). They work jointly with the tagger to provide internal cleaned versions of a token, or a sequence of, which are tagged separately. Resulting POS tags are finally merged to the original token(s). (e.g.wanna → want/VB to/TO → wanna/VB+TO). UGC tagging (Seddah et al., 2012; Nooralahzadeh et al., 2014): normalisation helps to cover some of the most frequent lexical variations and hence improves substantially the tagging accuracy. However in the case of in-game L EAGUE OF L EGENDS chat session, the normalisation is detrimental to the overall tagging performance as well as for unseen words. One obvious hypothesis is simply that the rules are applied deterministically and assign wrong PoSs while letting the pure tagging model work alone provide reasonable assumption on what would be the correct label for an out-of-domain word. Let us add that the MElt tagger makes a heavy use of features extra"
W16-3905,P14-2083,0,0.0176469,"Google, aimed at assessing the performances of parsers on various genres of Web texts (Petrov and McDonald, 2012). Foster (2010) and Foster et al. (2011b) noted that simple lexical and tokenisation convention adaptation to the Wall-Street Journal text genre could increase the parsing performance by a large margin. In addition, Seddah et al. (2012) showed that a certain amount of normalisation brought a large improvement in POS tagger performance of French social media texts. These normalisation steps mostly apply at the lexical level, at the very definition of what constitutes a minimal unit. Plank et al. (2014) attempt to quantify how much of the domain-specific variation of POS labeling is a result of different interpretations, and how much is arguably just noise. Regarding the study of French UGC, our starting point is the part-of-speech and phrase-structure annotation guidelines by Seddah et al. (2012). However, we conduct our syntactic analysis in terms of dependency structures. 3 Data Collection and Part-of-Speech Annotation Our dataset contains three different sources of user-generated content. Two of them are logs of multiplayer video-game chat sessions, M INECRAFT and L EAGUE OF L EGENDS1 ,"
W16-3905,W15-1617,1,0.836342,"lf-training or other semi-supervised learning techniques. Therefore, the purpose of the present work is not only to potentially provide a new dataset to be used as additional training data for domain adaptation. Rather, we provide a close inspection of the main causes behind the expectable performance drops in tagging and parsing. We therefore conduct a series of automatic and manual inspections to better understand the linguistic phenomena behind UGC linguistic variability. We explore the relation between predicting performance and annotation difficulty, which is seldom explicitly addressed (Plank et al., 2015). 4 A Threefold Categorisation for UGC Idiosyncrasies Even though user-generated content does not constitute a uniform genre, many works have characterised its idiosyncrasies (Foster, 2010; Gimpel et al., 2011; Seddah et al., 2012; Eisenstein, 2013), which can be characterised on three axes, defined by the intentionality or communication needs of the word variants: 1. Encoding simplification: This axis covers ergographic phenomena, i.e.,phenomena aiming at reducing the writing effort, perceived as first glance as genuine misspell errors, and transverse phe4 In fact, in the case of L EAGUE OF L"
W16-3905,W15-1618,0,0.0197729,"e existing words, and not from idealised sentence representations. We finally observe that for UGC, shorter sentences are harder to annotate. Indeed, sentences closer to the lower threshold of 4 tokens we have determined, seem to present more ellipsis, while longer sentences in our data have structures closer to more canonical syntax. 5.7 The Unspoken Costs of Treebank Annotation As we all know, creating annotated data is a rewarding task, extremely useful for evaluation as well as for building feature-rich supervised models. Yet, it is time consuming and as generally said, relatively costly (Schneider, 2015) even though crowd-sourcing solutions through games with a purpose start to emerge (Guillaume et al., 2016). The dataset we presented in that paper are part of process that was initiated 5 years ago when we were confronted to the lack of syntactically-annotated out-of-domain dataset for French. The purely syntactic annotation phase for the LoL and Minecraft data is still ongoing and we expect it to be finished in the first few months of 2017. It is important to consider that such a task was made possible because of the experience we gained along the years and because we relied on a highly trai"
W16-3905,L16-1375,1,0.910238,"g point is the part-of-speech and phrase-structure annotation guidelines by Seddah et al. (2012). However, we conduct our syntactic analysis in terms of dependency structures. 3 Data Collection and Part-of-Speech Annotation Our dataset contains three different sources of user-generated content. Two of them are logs of multiplayer video-game chat sessions, M INECRAFT and L EAGUE OF L EGENDS1 , the last one is made of instant cooking-related web questions from M ARMITON2 , a widely popular French recipe website. This set of questions was collected during the building of the French QuestionBank (Seddah and Candito, 2016) but was not described nor analysed because of its syntactic peculiarity and was thus considered by the authors as a clear outlier. We chose to include this sample in our study because it offers a sharp contrast with video games chat logs in term of domain variation while retaining a live nature: users asks questions related to their immediate needs and expect a quick answer. The L EAGUE OF L EGENDS data set was collected by Lamy (2015) in early 2015 and consists of two types of recorded user interactions: a first part is the record of discussions occurring during an on-going game session whil"
W16-3905,C12-1149,1,0.559586,"urse, slang, etc.). This suboptimal parsing behavior on web data was in turn confirmed in follow-up works on Twitter and IRC chat (Foster et al., 2011a; Gimpel et al., 2010; Elsner and Charniak, 2011). They were again confirmed during the SANCL shared task, organised by Google, aimed at assessing the performances of parsers on various genres of Web texts (Petrov and McDonald, 2012). Foster (2010) and Foster et al. (2011b) noted that simple lexical and tokenisation convention adaptation to the Wall-Street Journal text genre could increase the parsing performance by a large margin. In addition, Seddah et al. (2012) showed that a certain amount of normalisation brought a large improvement in POS tagger performance of French social media texts. These normalisation steps mostly apply at the lexical level, at the very definition of what constitutes a minimal unit. Plank et al. (2014) attempt to quantify how much of the domain-specific variation of POS labeling is a result of different interpretations, and how much is arguably just noise. Regarding the study of French UGC, our starting point is the part-of-speech and phrase-structure annotation guidelines by Seddah et al. (2012). However, we conduct our synt"
W16-3905,W15-2134,0,0.0203327,"e difficulty of the domain, it appears that a UD dependency analysis lends itself to dependency annotation in an easier way: Since non-leaf relations appear between lexical words, this representation is more robust to missing determiners, prepositions and punctuations, even phrases. Also, if we used other dependency formalisms that for instance place prepositions as heads of nouns, it would be more difficult to annotate as it is the case using the current French Treebank dependency scheme (Candito et al., 2010). Nevertheless, dependency analyses conflate functional and structural information (Silveira and Manning, 2015), and some of the structural information can be lost in cases such as the Example C, discussed above. Annotating dependencies lends itself well to noisy user-generated data. In a strict lexicalist analysis such as UD, where there are no tokens for unobserved words (e.g. dropped subjects, missing main verbs), we must build a structure from the existing words, and not from idealised sentence representations. We finally observe that for UGC, shorter sentences are harder to annotate. Indeed, sentences closer to the lower threshold of 4 tokens we have determined, seem to present more ellipsis, whil"
W16-3905,L16-1262,0,\N,Missing
W17-0805,P13-1162,0,0.0779491,"Missing"
W17-0805,1999.tmi-1.13,0,0.312672,"ing, is ellipsis. In the case of an ellipsis, the deleted segment can be reconstructed given a discourse antecedent in the same document, be it observed or idealized (Asher et al., 2001; Merchant, 2016). In the case of omission, a reference and a target version of a statement are involved, the deleted segment in one version having an antecedent in the other version of the statement, in another document, as a result of editorial choices. Our task is similar to the problem of omission detection in translations, but the bilingual setting allows for word-alignment-based approaches (Melamed, 1996; Russell, 1999), which we cannot use in our setup. Omission detection is also related to hedge detection, which can be achieved using specific lexical triggers such as vagueness markers (Szarvas et al., 2012; Vincze, 2013). 3 something substantial, such as time, place, cause, people involved or important event information.” The OMp scheme aims to represent a naive user intuition of the relevance of a difference between statements, akin to the intuition of the users mentioned in Section 1, whereas OMe aims at capturing our intuition that relevant omissions relate to missing key news elements describable in te"
W17-0805,J12-2004,0,0.0429233,"Missing"
W17-0805,P05-1045,0,0.0150831,"appear in r, we generate the following feature sets: 1. Dice (Fa ): Dice coefficient between r and t. 2. Length (Fb ): The length of r, the length of t, and their difference. 3. BoW (Fc ): A bag of words (BoW) of M . 4. DWR (Fd ): A dense word representation is word-vector representation of M built from the average word vector for all words in M . We use the representations from GloVe (Pennington et al., 2014). 5. Stop proportion (Fe ): The proportion of stop words and punctuation in M . 6. Entities (Ff ): The number of entities in M predicted by the 4-class Stanford Named Entity Recognizer (Finkel et al., 2005). Table 3 shows the classification results. We use all exhaustive combinations of these feature sets to train a discriminative classifier, namely a logistic regression classifier, to obtain a best feature combination. We consider a feature combination to be the best when it outperforms the others in both accuracy and F1 for the Omission label. We compare all systems against the most frequent label (MFL) baseline. We evaluate each feature twice, namely using five-cold cross validation (CV-5 OMp ), and in a split scenario where we test on the 100 examples of OE after training with the remaining"
W17-0805,I13-1044,0,0.0146224,"he case of omission, a reference and a target version of a statement are involved, the deleted segment in one version having an antecedent in the other version of the statement, in another document, as a result of editorial choices. Our task is similar to the problem of omission detection in translations, but the bilingual setting allows for word-alignment-based approaches (Melamed, 1996; Russell, 1999), which we cannot use in our setup. Omission detection is also related to hedge detection, which can be achieved using specific lexical triggers such as vagueness markers (Szarvas et al., 2012; Vincze, 2013). 3 something substantial, such as time, place, cause, people involved or important event information.” The OMp scheme aims to represent a naive user intuition of the relevance of a difference between statements, akin to the intuition of the users mentioned in Section 1, whereas OMe aims at capturing our intuition that relevant omissions relate to missing key news elements describable in terms of the 5-W questions (Parton et al., 2009; Das et al., 2012). We ran AMT task twice, once for each scheme. For each scheme, we assigned 5 turkers per instance, and we required that the annotators be Cate"
W17-0805,N13-1132,0,0.0771197,"Missing"
W17-0805,P13-2011,0,0.0332052,"Missing"
W17-0805,C96-2129,0,0.0872199,"different setting, is ellipsis. In the case of an ellipsis, the deleted segment can be reconstructed given a discourse antecedent in the same document, be it observed or idealized (Asher et al., 2001; Merchant, 2016). In the case of omission, a reference and a target version of a statement are involved, the deleted segment in one version having an antecedent in the other version of the statement, in another document, as a result of editorial choices. Our task is similar to the problem of omission detection in translations, but the bilingual setting allows for word-alignment-based approaches (Melamed, 1996; Russell, 1999), which we cannot use in our setup. Omission detection is also related to hedge detection, which can be achieved using specific lexical triggers such as vagueness markers (Szarvas et al., 2012; Vincze, 2013). 3 something substantial, such as time, place, cause, people involved or important event information.” The OMp scheme aims to represent a naive user intuition of the relevance of a difference between statements, akin to the intuition of the users mentioned in Section 1, whereas OMe aims at capturing our intuition that relevant omissions relate to missing key news elements d"
W17-0805,P09-1048,0,0.0494486,"Missing"
W17-0805,Q14-1025,0,0.0211382,"used two different annotation schemes, namely OMp , where the option to mark an omission is “Text B leaves out some substantial information”, and OMe , where it is “Text B leaves out Figure 1: Annotation scheme for OMp Annotation results The first column in Table 2 shows the agreement of the annotation tasks in terms of Krippendorff’s α coefficient. A score of e.g. 0.52 is not a very high value, but is well within what can be expected on crowdsourced semantic annotations. Note, however, the chance correction that the calculation of α applies to a skewed binary distribution is very aggressive (Passonneau and Carpenter, 2014). The conservativeness of the chance-corrected coefficient can be assessed if we compare the raw agreement between experts (0.86) with the α of 0.67. OMe causes agreement to descend slightly, and damages the agreement of Same, while Omission remains largely constant. Moreover, disagreement is not evenly distributed across annotated instances, i.e. some instances show perfect agreement, while other instances have maximal disagreement. We also measured the median annotation time per instance for all three methods; OMe is almost twice as slow as OMp (42s vs. 22s), while 3 The full distribution of"
W17-0805,D14-1162,0,0.0835688,"ined by simple proxy linguistic properties like word overlap or length of the statements and/or lexical properties. Features: For a reference statement r, a target statement t and a set M of the words that only appear in r, we generate the following feature sets: 1. Dice (Fa ): Dice coefficient between r and t. 2. Length (Fb ): The length of r, the length of t, and their difference. 3. BoW (Fc ): A bag of words (BoW) of M . 4. DWR (Fd ): A dense word representation is word-vector representation of M built from the average word vector for all words in M . We use the representations from GloVe (Pennington et al., 2014). 5. Stop proportion (Fe ): The proportion of stop words and punctuation in M . 6. Entities (Ff ): The number of entities in M predicted by the 4-class Stanford Named Entity Recognizer (Finkel et al., 2005). Table 3 shows the classification results. We use all exhaustive combinations of these feature sets to train a discriminative classifier, namely a logistic regression classifier, to obtain a best feature combination. We consider a feature combination to be the best when it outperforms the others in both accuracy and F1 for the Omission label. We compare all systems against the most frequent"
W17-1725,P15-1108,0,0.0189776,"and nominal modifier. Larger treebanks have more stable results and aid the learning system. Indeed, the F ULL transition system, which has more operations will need more data to converge. This argument is supported 6 Related Work MWE processing is an ever growing research topic since Sag et al. (2002), as it has been shown in (Ramisch, 2015). On the side of MWE-aware dependency parsing, the main line of research for joint approaches is to use standard dependency parsers using special arc labels and flat structures for MWEs (Nivre and Nilsson, 2004; Eryi˘git et al., 2011; Seddah et al., 2013; Nasr et al., 2015). Vincze et al. (2013) and Candito and Constant (2014) integrate richer arc labels and nonflat structures to predict internal MWE structure. Truly joint approaches incorporating special parsing mechanisms to handle MWE recognition is a recent line of research (Constant and Nivre, 2016). On the side of UD, Silveira and Manning (2015) explore whether the UD treebank formalism needs an additional representation to improve parsing. Salehi et al. (2016) identify MWE in a surprise target language with no prior knowledge of MWE 184 patterns, using training on a UD MWE-aware treebank of a source langu"
W17-1725,W04-0308,0,0.0221154,"artial system that predicts the syntactic layer only. For this, the lexical stack is deactivated as well as the MergeN and Complete transitions. This partial system is equivalent to the one in Nivre (2014). versely, lexical nodes are not necessarily syntactic ones: the light verb construction make decision is an MWE node, but is not part of the syntactic tree; only its components make and decision are. The fixed MWE at least is a syntactic node, but its child nodes at and least are not. 2.1 A transition-based system The proposed transition system is a mild extension of an arc-standard system (Nivre, 2004), as schematized in Figure 1. It iteratively builds a graph over lexical nodes by applying a sequence of actions (namely transitions) from an initial parser state (namely Initial configuration) to a terminal state (namely Terminal configuration). Every parsing state is a 5-uple made of two stacks (a lexical stack σl and a syntactic stack σs ), one buffer (β), a set of already predicted syntactic arcs (A) and a set of already predicted lexical trees (L). We use only one buffer in order to synchronize the prediction of the two layers, as they share elements, namely syntactic nodes. Each element"
W17-1725,P09-1040,0,0.0353584,", A ∪ {(x, k, y)}, L) (σl |y, σs , β, A ∪ {(y, k, x)}, L) (σl |t(x, y), σs |t(x, y), β, A, L) (σl , σs |t(x, y), β, A, L) (σl , σs , β, A, L ∪ {x}) (σl , σs |y, x|β, A, L) Figure 1: Transition system for joint syntactic and lexical analysis handling non-projectivity. This schema simply extends (Constant and Nivre, 2016) system by adding a Swap transition. (2016) only works for predicting projective syntactic trees. A classical way to deal with nonprojectivity is to add a Swap transition, permuting the two top elements on the syntactic stack, the second element being pushed back to the buffer (Nivre, 2009). This simple integration in our joint system is not as straightforward as in a classical arc-standard system. One needs to add more conditions to apply the Shift transition. If the buffer is not empty, the first element x is moved onto the syntactic stack. It is also pushed onto the lexical stack if the following condition holds: x must not have been already pushed on the lexical stack in order to avoid it being processed multiple times during lexical analysis. In our experiments, we also used a partial system that predicts the syntactic layer only. For this, the lexical stack is deactivated"
W17-1725,P14-1070,1,0.885407,"Missing"
W17-1725,P16-1016,1,0.868018,"ay be non-projective, we have improved the parsing algorithm to account for nonprojective trees, which the original work in CN16 could not provide. In the setup of CN16, only projective sentences could be used for training. This article evaluates the extension of a dependency parser that performs joint syntactic analysis and multiword expression identification. We show that, given sufficient training data, the parser benefits from explicit multiword information and improves overall labeled accuracy score in eight of the ten evaluation cases. 1 Introduction In this paper, we expand the work of Constant and Nivre (2016) —henceforth CN16— by evaluating their system more extensively, representing Multiword Expressions (MWEs) in different ways that are linguistically motivated. Their transitionbased system jointly performs lexical analysis and syntactic dependency parsing, using special transitions for MWE identification. In particular, these special transitions generate new lexical nodes for MWEs, that can also serve as nodes of the syntactic dependency trees. Their system is based on the classical split between fixed and free MWEs. Fixed MWEs defined by Sag et al. (2002) are contiguous. They are considered sy"
W17-1725,C16-1046,0,0.0491791,"Missing"
W17-1725,W15-2134,0,0.0316327,"Missing"
W17-1725,I13-1024,0,0.0245543,"Missing"
W17-1725,L16-1262,0,\N,Missing
W17-6304,W13-3520,0,0.0410365,"over the UD1.3 corpora. “MA” stands for morphological-analyser-based lexicon. Lexicons based on Apertium and Giellatekno data are in their coarse version unless full is indicated. Other lexicons have been adapted from available resources.1 We also provide the type-token ratio of the corpus (TTR) and whether there were available Polyglot embeddings (PG) to initialize w. ~ in which labels are the concatenation of the Universal PoS and Universal Features. Pre-computed embeddings Whenever available and following Plank et al. (2016), we performed experiments using Polyglot pre-computed embeddings (Al-Rfou et al., 2013). Languages for which Polyglot embeddings are available are indicated in Table 1. We also took advantage of other existing lexicons. For space reasons, we are not able to describe here the language-specific transformations we applied to some of these lexicons. See Table 1 and its caption for more information. We determine the best performing lexicon for each language based on tagging accuracy on the development set. In the remainder of this paper, all information about the lexicons (Table 1) and accuracy results are restricted to these best performing lexicons. We trained our tagger with and w"
W17-6304,erjavec-2010-multext,0,0.0545621,"e-art bi-LSTM PoS tagger bilty, a freely available6 and “significantly refactored version of the code originally used” by Plank et al. (2016). We use its standard configuration, with one bi-LSTM layer, characterbased embeddings size of 100, word embedding size of 64 (same as Polyglot embeddings), no multitask learning,7 and 20 iterations for training. We extended bilty for enabling integration of lexical morphosyntactic information, in the way described in the previous section. 5 Bouma et al., 2000; Oliver and Tadi´c, 2004; Heslin, 2007; Borin et al., 2008; Molinero et al., 2009; Sagot, 2010; Erjavec, 2010; Sagot and Walther, 2010; Mˇechura, 2014; Sagot, 2014. 6 https://github.com/bplank/bilstm-aux 7 Plank et al.’s (2016) secondary task—predicting the frequency class of each word—results in better OOV scores but virtually identical overall scores when averaged over all tested languages/corpora. 8 Note that we discarded alternative UD 1.3 corpora (e.g. nl lassysmall vs. nl), as well as corpora for languages for which we had neither a lexicon nor Polyglot embeddings (Old Church Slavonic, Hungarian, Gothic, Tamil). 28 w(P ~ ) +~ c + ~l lang a lexicon are those with smallest datasets. ∆ w.r.t.w(P ~"
W17-6304,D15-1041,0,0.0274164,"information. First, morphosyntactic lexicons provide a large inventory of (word, PoS) pairs. Such lexical information can be used in the form of constraints at tagging time (Kim et al., 1999; Hajiˇc, 2000) or during the training process as additional features combined with standard features extracted from the training corpus (Chrupała et al., 2008; Goldberg et al., 2009; Denis and Sagot, 2012). Second, lexical information encoded in vector representations, known as word embeddings, have emerged more recently (Bengio et al., 2003; Collobert and Weston, 2008; Chrupała, 2013; Ling et al., 2015; Ballesteros et al., 2015; M¨uller and Sch¨utze, 2015). Such representations, often extracted from large amounts of raw text, have proved very useful for numerous tasks including PoS tagging, in particular when used in recurrent neural networks (RNNs) and more specifically in mono- or bi-directional, word-level or characterlevel long short-term memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997; Ling et al., 2015; Ballesteros et al., 2015; Plank et al., 2016). Character-level embeddings are of particular interest for PoS tagging as they generate vector representations that result from the internal characterleve"
W17-6304,E09-1038,0,0.0301809,"ce {benoit.sagot,hector.martinez-alonso}@inria.fr Abstract curacy levels, improving over the state of the art in a number of settings (Plank et al., 2016). As a complement to annotated training corpora, external lexicons can be a valuable source of information. First, morphosyntactic lexicons provide a large inventory of (word, PoS) pairs. Such lexical information can be used in the form of constraints at tagging time (Kim et al., 1999; Hajiˇc, 2000) or during the training process as additional features combined with standard features extracted from the training corpus (Chrupała et al., 2008; Goldberg et al., 2009; Denis and Sagot, 2012). Second, lexical information encoded in vector representations, known as word embeddings, have emerged more recently (Bengio et al., 2003; Collobert and Weston, 2008; Chrupała, 2013; Ling et al., 2015; Ballesteros et al., 2015; M¨uller and Sch¨utze, 2015). Such representations, often extracted from large amounts of raw text, have proved very useful for numerous tasks including PoS tagging, in particular when used in recurrent neural networks (RNNs) and more specifically in mono- or bi-directional, word-level or characterlevel long short-term memory networks (LSTMs) (Ho"
W17-6304,A00-2013,0,0.296612,"Missing"
W17-6304,W99-0615,0,0.0414295,"Missing"
W17-6304,D15-1176,0,0.102226,"Missing"
W17-6304,A00-1031,0,0.670113,"Missing"
W17-6304,P95-1037,0,0.0868754,"to associate each “word” with a morphosyntactic tag, whose granularity can range from a simple morphosyntactic category, or part-of-speech (hereafter PoS), to finer categories enriched with morphological features (gender, number, case, tense, mood, person, etc.). The use of machine learning algorithms trained on manually annotated corpora has long become the standard way to develop PoS taggers. A large variety of algorithms have been used, such as (in approximative chronological order) bigram and trigram hidden Markov models (Merialdo, 1994; Brants, 1996, 2000), decision trees (Schmid, 1994; Magerman, 1995), maximum entropy Markov models (MEMMs) (Ratnaparkhi, 1996) and Conditional Random Fields (CRFs) (Lafferty et al., 2001; Constant and Tellier, 2012). Recently, neural approaches have reached very competitive ac25 Proceedings of the 15th International Conference on Parsing Technologies, pages 25–31, c Pisa, Italy; September 20–22, 2017. 2017 Association for Computational Linguistics vantages of using character-level embeddings and external lexical information is an interesting idea to follow. However, the inclusion of morphosyntactic information from lexicons into neural PoS tagging architectur"
W17-6304,chrupala-etal-2008-learning,0,0.0677445,"Missing"
W17-6304,J94-2001,0,0.377092,"ng is now a classic task in natural language processing. Its aim is to associate each “word” with a morphosyntactic tag, whose granularity can range from a simple morphosyntactic category, or part-of-speech (hereafter PoS), to finer categories enriched with morphological features (gender, number, case, tense, mood, person, etc.). The use of machine learning algorithms trained on manually annotated corpora has long become the standard way to develop PoS taggers. A large variety of algorithms have been used, such as (in approximative chronological order) bigram and trigram hidden Markov models (Merialdo, 1994; Brants, 1996, 2000), decision trees (Schmid, 1994; Magerman, 1995), maximum entropy Markov models (MEMMs) (Ratnaparkhi, 1996) and Conditional Random Fields (CRFs) (Lafferty et al., 2001; Constant and Tellier, 2012). Recently, neural approaches have reached very competitive ac25 Proceedings of the 15th International Conference on Parsing Technologies, pages 25–31, c Pisa, Italy; September 20–22, 2017. 2017 Association for Computational Linguistics vantages of using character-level embeddings and external lexical information is an interesting idea to follow. However, the inclusion of morphosyn"
W17-6304,R09-1049,1,0.847833,"We use as a baseline the state-of-the-art bi-LSTM PoS tagger bilty, a freely available6 and “significantly refactored version of the code originally used” by Plank et al. (2016). We use its standard configuration, with one bi-LSTM layer, characterbased embeddings size of 100, word embedding size of 64 (same as Polyglot embeddings), no multitask learning,7 and 20 iterations for training. We extended bilty for enabling integration of lexical morphosyntactic information, in the way described in the previous section. 5 Bouma et al., 2000; Oliver and Tadi´c, 2004; Heslin, 2007; Borin et al., 2008; Molinero et al., 2009; Sagot, 2010; Erjavec, 2010; Sagot and Walther, 2010; Mˇechura, 2014; Sagot, 2014. 6 https://github.com/bplank/bilstm-aux 7 Plank et al.’s (2016) secondary task—predicting the frequency class of each word—results in better OOV scores but virtually identical overall scores when averaged over all tested languages/corpora. 8 Note that we discarded alternative UD 1.3 corpora (e.g. nl lassysmall vs. nl), as well as corpora for languages for which we had neither a lexicon nor Polyglot embeddings (Old Church Slavonic, Hungarian, Gothic, Tamil). 28 w(P ~ ) +~ c + ~l lang a lexicon are those with smal"
W17-6304,constant-tellier-2012-evaluating,0,0.0203217,"ech (hereafter PoS), to finer categories enriched with morphological features (gender, number, case, tense, mood, person, etc.). The use of machine learning algorithms trained on manually annotated corpora has long become the standard way to develop PoS taggers. A large variety of algorithms have been used, such as (in approximative chronological order) bigram and trigram hidden Markov models (Merialdo, 1994; Brants, 1996, 2000), decision trees (Schmid, 1994; Magerman, 1995), maximum entropy Markov models (MEMMs) (Ratnaparkhi, 1996) and Conditional Random Fields (CRFs) (Lafferty et al., 2001; Constant and Tellier, 2012). Recently, neural approaches have reached very competitive ac25 Proceedings of the 15th International Conference on Parsing Technologies, pages 25–31, c Pisa, Italy; September 20–22, 2017. 2017 Association for Computational Linguistics vantages of using character-level embeddings and external lexical information is an interesting idea to follow. However, the inclusion of morphosyntactic information from lexicons into neural PoS tagging architecture, as a replacement or complement to character-based or pre-computed word embeddings, remains to be investigated. In this paper, we describe how suc"
W17-6304,N15-1055,0,0.040058,"Missing"
W17-6304,W96-0213,0,0.749519,"ose granularity can range from a simple morphosyntactic category, or part-of-speech (hereafter PoS), to finer categories enriched with morphological features (gender, number, case, tense, mood, person, etc.). The use of machine learning algorithms trained on manually annotated corpora has long become the standard way to develop PoS taggers. A large variety of algorithms have been used, such as (in approximative chronological order) bigram and trigram hidden Markov models (Merialdo, 1994; Brants, 1996, 2000), decision trees (Schmid, 1994; Magerman, 1995), maximum entropy Markov models (MEMMs) (Ratnaparkhi, 1996) and Conditional Random Fields (CRFs) (Lafferty et al., 2001; Constant and Tellier, 2012). Recently, neural approaches have reached very competitive ac25 Proceedings of the 15th International Conference on Parsing Technologies, pages 25–31, c Pisa, Italy; September 20–22, 2017. 2017 Association for Computational Linguistics vantages of using character-level embeddings and external lexical information is an interesting idea to follow. However, the inclusion of morphosyntactic information from lexicons into neural PoS tagging architecture, as a replacement or complement to character-based or pre"
W17-6304,W14-4607,0,0.614965,"Missing"
W17-6304,sagot-2010-lefff,1,0.904736,"e state-of-the-art bi-LSTM PoS tagger bilty, a freely available6 and “significantly refactored version of the code originally used” by Plank et al. (2016). We use its standard configuration, with one bi-LSTM layer, characterbased embeddings size of 100, word embedding size of 64 (same as Polyglot embeddings), no multitask learning,7 and 20 iterations for training. We extended bilty for enabling integration of lexical morphosyntactic information, in the way described in the previous section. 5 Bouma et al., 2000; Oliver and Tadi´c, 2004; Heslin, 2007; Borin et al., 2008; Molinero et al., 2009; Sagot, 2010; Erjavec, 2010; Sagot and Walther, 2010; Mˇechura, 2014; Sagot, 2014. 6 https://github.com/bplank/bilstm-aux 7 Plank et al.’s (2016) secondary task—predicting the frequency class of each word—results in better OOV scores but virtually identical overall scores when averaged over all tested languages/corpora. 8 Note that we discarded alternative UD 1.3 corpora (e.g. nl lassysmall vs. nl), as well as corpora for languages for which we had neither a lexicon nor Polyglot embeddings (Old Church Slavonic, Hungarian, Gothic, Tamil). 28 w(P ~ ) +~ c + ~l lang a lexicon are those with smallest datasets"
W17-6304,sagot-2014-delex,1,0.845311,"“significantly refactored version of the code originally used” by Plank et al. (2016). We use its standard configuration, with one bi-LSTM layer, characterbased embeddings size of 100, word embedding size of 64 (same as Polyglot embeddings), no multitask learning,7 and 20 iterations for training. We extended bilty for enabling integration of lexical morphosyntactic information, in the way described in the previous section. 5 Bouma et al., 2000; Oliver and Tadi´c, 2004; Heslin, 2007; Borin et al., 2008; Molinero et al., 2009; Sagot, 2010; Erjavec, 2010; Sagot and Walther, 2010; Mˇechura, 2014; Sagot, 2014. 6 https://github.com/bplank/bilstm-aux 7 Plank et al.’s (2016) secondary task—predicting the frequency class of each word—results in better OOV scores but virtually identical overall scores when averaged over all tested languages/corpora. 8 Note that we discarded alternative UD 1.3 corpora (e.g. nl lassysmall vs. nl), as well as corpora for languages for which we had neither a lexicon nor Polyglot embeddings (Old Church Slavonic, Hungarian, Gothic, Tamil). 28 w(P ~ ) +~ c + ~l lang a lexicon are those with smallest datasets. ∆ w.r.t.w(P ~ ) +~ c OOTC OOTC in Lex. OOTC OOTC in Lex. ar bg ca c"
W17-6304,sagot-walther-2010-morphological,1,0.897189,"oS tagger bilty, a freely available6 and “significantly refactored version of the code originally used” by Plank et al. (2016). We use its standard configuration, with one bi-LSTM layer, characterbased embeddings size of 100, word embedding size of 64 (same as Polyglot embeddings), no multitask learning,7 and 20 iterations for training. We extended bilty for enabling integration of lexical morphosyntactic information, in the way described in the previous section. 5 Bouma et al., 2000; Oliver and Tadi´c, 2004; Heslin, 2007; Borin et al., 2008; Molinero et al., 2009; Sagot, 2010; Erjavec, 2010; Sagot and Walther, 2010; Mˇechura, 2014; Sagot, 2014. 6 https://github.com/bplank/bilstm-aux 7 Plank et al.’s (2016) secondary task—predicting the frequency class of each word—results in better OOV scores but virtually identical overall scores when averaged over all tested languages/corpora. 8 Note that we discarded alternative UD 1.3 corpora (e.g. nl lassysmall vs. nl), as well as corpora for languages for which we had neither a lexicon nor Polyglot embeddings (Old Church Slavonic, Hungarian, Gothic, Tamil). 28 w(P ~ ) +~ c + ~l lang a lexicon are those with smallest datasets. ∆ w.r.t.w(P ~ ) +~ c OOTC OOTC in Lex."
W17-6304,oliver-tadic-2004-enlarging,0,0.60392,"Missing"
W17-6304,P16-2067,0,0.379799,"sentations, known as word embeddings, have emerged more recently (Bengio et al., 2003; Collobert and Weston, 2008; Chrupała, 2013; Ling et al., 2015; Ballesteros et al., 2015; M¨uller and Sch¨utze, 2015). Such representations, often extracted from large amounts of raw text, have proved very useful for numerous tasks including PoS tagging, in particular when used in recurrent neural networks (RNNs) and more specifically in mono- or bi-directional, word-level or characterlevel long short-term memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997; Ling et al., 2015; Ballesteros et al., 2015; Plank et al., 2016). Character-level embeddings are of particular interest for PoS tagging as they generate vector representations that result from the internal characterlevel make-up of each word. It can generalise over relevant sub-parts such as prefixes or suffixes, thus directly addressing the problem of unknown words. However, unknown words do not always follow such generalisations. In such cases, character-level models cannot bring any advantage. This is a difference with external lexicons, which provides information about any word it contains, yet without any quantitative distinction between relevant and"
W18-0523,W02-0109,0,0.217306,"or each user 1. User, session and client: Non-linguistic data, but also potential sources of error. • country country codes from which this user has done exercises 2. Task format: Whether a given data point belongs to the listen, reverse tap or reverse translate task format. Each task has a different error prior (Table 1). • client - the student’s device platform (one of: android, ios, or web) • session - the session type (one of: lesson, practice, or test; explanation below) 3. Word properties (base): Basic word properties, i.e., the word form and its stem. We use the NLTK Snowball stemmers (Loper and Bird, 2002) for the three languages at hand. We add the word’s log frequency calculated from Universal Dependencies (UD) 2.1 (Nivre et al., 2016). There were three tracks for learners of English, Spanish, and French. In particular, en-es consists of English learners (who already speak Spanish), es-en are Spanish learners (who already speak English), and fr-en are French learners (who already speak English). We participated in all three. An overview of the data for the three tracks, including number of users, tokens and average error rate is given in Table 1. The distribution of four attributes of the tex"
W18-0523,W18-0506,0,0.0297354,"the data before detailing each group of features and proceeding to describe the model and results. We present our submission to the 2018 Duolingo Shared Task on Second Language Acquisition Modeling (SLAM). We focus on evaluating a range of features for the task, including user-derived measures, while examining how far we can get with a simple linear classifier. Our analysis reveals that errors differ per exercise format, which motivates our final and best-performing system: a task-wise (per exercise-format) model. 1 Introduction The shared task on Second Language Acquisition Modeling (SLAM) (Settles et al., 2018) consisted of an error prediction task, i.e., determining whether a language learner (user) made a tokenlevel mistake.1 Exploring if and how errors can be predicted can provide insights into the learning process and help pinpoint specific constructs that challenge learners of different languages. The design of each exercise and the time spent on a particular task and language course, which can be expected to influence the performance, are included in the data. The learning context and the learners’ background language skills, which would also influence performance, are not known or controlled"
