2015.mtsummit-users.20,E14-2007,0,0.013062,"lingual lexicon. 1 Introduction The problem of language translation has been in focus for many decades and has seen contributions from both linguistic and computer science communities. Linguistic contribution (Streiter (1996)) has come in the form of several language resources comprising of dictionaries, grammar and studies on units of translation. Computer science community has contributed in coming up with formal machine translation (MT) models (Vogel et al. (2003)) that leverage corpus statistics along with linguistic features and resources. There is a body of work (Federico et al. (2014); Alabau et al. (2014)) that studies the complementary contributions of humans and MT models and present “machine-centric” translation systems that leverage human input. These systems, referred to as computer aided translation (CAT) systems, typically employ a statistical MT model to translate text and provide a post-editing tooling to enable humans to correct the resulting translations. Human feedback and corrections are used to adapt and retrain the translation model. What constitutes the right unit of translation and how can the human feedback be incorporated in the underlying translation model, pose interesting"
2015.mtsummit-users.20,aziz-etal-2012-pet,0,0.0136619,"r a growing number of language pairs along with the necessary tools and a translation engine. However, these systems typically involve a complex pipeline and statistical tools, making it diﬃcult to track and correct errors. Many researchers in the past have claimed and suggested that we cannot remove humans completely from the translation pipeline (Kay (1980)). In order to cater to applications requiring a high-quality translation, the output of MT systems is often revised by a post-editing phase. Several computer-aided translation (CAT) tools exist that are either desktop-based (Carl (2012); Aziz et al. (2012)), iOmegaT1 or web-based (Federico et al. (2014); Denkowski and Lavie (2012); Roturier et al. (2013)). As an alternative to pure post-editing systems, interactive machine translation (IMT) (Toselli et al. (2011)) combines a MT engine with human, in an interactive setup, where, the MT engine continuously exploits human feedback and attempts to improve future translations. Daniel Ortiz-Martınez (2011); Ortiz-Martínez et al. (2010), for instance, talk about online learning in the machine translation pipeline, where, human feedback on translations is used to re-estimate the parameters of a statis"
2015.mtsummit-users.20,2013.mtsummit-papers.5,0,0.0149835,"o et al. (2014); Denkowski and Lavie (2012); Roturier et al. (2013)). As an alternative to pure post-editing systems, interactive machine translation (IMT) (Toselli et al. (2011)) combines a MT engine with human, in an interactive setup, where, the MT engine continuously exploits human feedback and attempts to improve future translations. Daniel Ortiz-Martınez (2011); Ortiz-Martínez et al. (2010), for instance, talk about online learning in the machine translation pipeline, where, human feedback on translations is used to re-estimate the parameters of a statistical machine translation model. Bertoldi et al. (2013) address the problem of dynamically adapting a phrase-based SMT model from user post-editing by means of a caching mechanism. Their cache-based model combines a large global static model with a small local and dynamic model estimated from recent items in the input stream. Lavie (2014) incorporate human feedback and propose three online methods for improving an underlying MT engine based on translation grammar, Bayesian language model and parameter optimization. Anusaarka (Bharati et al. (2003)), a hybrid machine translation system for English to Hindi, also involves interaction but is restrict"
2015.mtsummit-users.20,carl-2012-translog,0,0.0155861,"istic data for a growing number of language pairs along with the necessary tools and a translation engine. However, these systems typically involve a complex pipeline and statistical tools, making it diﬃcult to track and correct errors. Many researchers in the past have claimed and suggested that we cannot remove humans completely from the translation pipeline (Kay (1980)). In order to cater to applications requiring a high-quality translation, the output of MT systems is often revised by a post-editing phase. Several computer-aided translation (CAT) tools exist that are either desktop-based (Carl (2012); Aziz et al. (2012)), iOmegaT1 or web-based (Federico et al. (2014); Denkowski and Lavie (2012); Roturier et al. (2013)). As an alternative to pure post-editing systems, interactive machine translation (IMT) (Toselli et al. (2011)) combines a MT engine with human, in an interactive setup, where, the MT engine continuously exploits human feedback and attempts to improve future translations. Daniel Ortiz-Martınez (2011); Ortiz-Martínez et al. (2010), for instance, talk about online learning in the machine translation pipeline, where, human feedback on translations is used to re-estimate the pa"
2015.mtsummit-users.20,2011.mtsummit-wpt.7,0,0.0419479,"Missing"
2015.mtsummit-users.20,P11-4012,0,0.0214168,"Missing"
2015.mtsummit-users.20,C14-2028,0,0.0820188,"rating a high quality bilingual lexicon. 1 Introduction The problem of language translation has been in focus for many decades and has seen contributions from both linguistic and computer science communities. Linguistic contribution (Streiter (1996)) has come in the form of several language resources comprising of dictionaries, grammar and studies on units of translation. Computer science community has contributed in coming up with formal machine translation (MT) models (Vogel et al. (2003)) that leverage corpus statistics along with linguistic features and resources. There is a body of work (Federico et al. (2014); Alabau et al. (2014)) that studies the complementary contributions of humans and MT models and present “machine-centric” translation systems that leverage human input. These systems, referred to as computer aided translation (CAT) systems, typically employ a statistical MT model to translate text and provide a post-editing tooling to enable humans to correct the resulting translations. Human feedback and corrections are used to adapt and retrain the translation model. What constitutes the right unit of translation and how can the human feedback be incorporated in the underlying translation m"
2015.mtsummit-users.20,P07-2045,0,0.00286799,"y prompts the human on what to translate. This interactive human-machine dialog produces a translation system that aims to achieve high precision in-domain translations and might ﬁnd application in several technical domains including medical, education, legal etc.The system is available for demo at http://mtdemo.hostzi.com. 2 Related Work There has been a lot of research on automated statistical machine translation (SMT) and several systems (Wang and Waibel (1998); Vogel et al. (2003); Och and Ney (2000); Proceedings of MT Summit XV, vol. 2: MT Users&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 260 Koehn et al. (2007)) have been proposed. While they are all typically based on a combination of a translation model and the target language model, the diﬀerence lies in their units of translation (word-based, phrase-based etc.) and translation decoding. The statistical approach to MT itself falls under the general category of example-based MT (EBMT) (Somers (1999)) or memory-based MT (Sato and Nagao (1990)). These approaches rely on the availability of a corpus or a database of already translated examples, and involve a process of matching a new input against this database to extract suitable examples and then d"
2015.mtsummit-users.20,E14-1042,0,0.0143841,"and attempts to improve future translations. Daniel Ortiz-Martınez (2011); Ortiz-Martínez et al. (2010), for instance, talk about online learning in the machine translation pipeline, where, human feedback on translations is used to re-estimate the parameters of a statistical machine translation model. Bertoldi et al. (2013) address the problem of dynamically adapting a phrase-based SMT model from user post-editing by means of a caching mechanism. Their cache-based model combines a large global static model with a small local and dynamic model estimated from recent items in the input stream. Lavie (2014) incorporate human feedback and propose three online methods for improving an underlying MT engine based on translation grammar, Bayesian language model and parameter optimization. Anusaarka (Bharati et al. (2003)), a hybrid machine translation system for English to Hindi, also involves interaction but is restricted to authoring rules for word sense disambiguation. Ranta had proposed Grammatical Framework (GF) (Ranta (2004)) which is a grammar formalism and a programming language for multilingual grammar applications. One good example of applications using GF2 is Molto (Cristina Espa˜na-Bonet"
2015.mtsummit-users.20,P00-1056,0,0.0331795,"-centric”, in that, it heavily relies on manually curated linguistic resources, while the machine continuously prompts the human on what to translate. This interactive human-machine dialog produces a translation system that aims to achieve high precision in-domain translations and might ﬁnd application in several technical domains including medical, education, legal etc.The system is available for demo at http://mtdemo.hostzi.com. 2 Related Work There has been a lot of research on automated statistical machine translation (SMT) and several systems (Wang and Waibel (1998); Vogel et al. (2003); Och and Ney (2000); Proceedings of MT Summit XV, vol. 2: MT Users&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 260 Koehn et al. (2007)) have been proposed. While they are all typically based on a combination of a translation model and the target language model, the diﬀerence lies in their units of translation (word-based, phrase-based etc.) and translation decoding. The statistical approach to MT itself falls under the general category of example-based MT (EBMT) (Somers (1999)) or memory-based MT (Sato and Nagao (1990)). These approaches rely on the availability of a corpus or a database of already translated example"
2015.mtsummit-users.20,2013.mtsummit-wptp.14,0,0.0273974,"Missing"
2015.mtsummit-users.20,C90-3044,0,0.805946,"tems, referred to as computer aided translation (CAT) systems, typically employ a statistical MT model to translate text and provide a post-editing tooling to enable humans to correct the resulting translations. Human feedback and corrections are used to adapt and retrain the translation model. What constitutes the right unit of translation and how can the human feedback be incorporated in the underlying translation model, pose interesting research challenges. A domain corpus is often replete with redundancy arising due to the choice of vocabulary and syntax. Translation memory-based systems (Sato and Nagao (1990)) exploit this redundancy and store recurring phrases and their translations. We are furProceedings of MT Summit XV, vol. 2: MT Users&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 259 Figure 1: An example illustrating the principle of compositionality and higher order patterns in a domain corpus ther motivated by Frege’s principle of compositionality (Pelletier (1994)), which states that the meaning of a compound expression is a function of the meaning of its parts and of the syntactic rule by which they are combined. Figure 1 shows an example, taken from legal domain, of a compound expression and it"
2015.mtsummit-users.20,2007.mtsummit-wpt.4,0,0.0330983,"pression and its constituent expressions. Some of these expressions comprise of categories that generalize over several tokens, thus, forming higher order recurring patterns in the corpus. Extraction of these patterns and using them as the unit of translation might enable us to better capture the structure and semantics of the domain. An in-domain (especially technical, legal) corpus often adheres to a certain lexical and syntactic structure and is often less amenable to creative or “free” translation. These domains, therefore, might be good candidates for translation using rule-based systems Terumasa (2007), comprising of source and target language dictionaries, grammars and translation rules. Grammatical Framework (GF) (Ranta (2004)) provides the necessary formalism to theorize rule-based translations and also provides a system to author abstract and concrete language syntax. We present an approach and a system that builds on these ideas to extract meaningful patterns from a domain corpus, gather human feedback on their translation and learn a rule-based translation system using the GF formalism. The system is “human-centric”, in that, it heavily relies on manually curated linguistic resources,"
2015.mtsummit-users.20,2003.mtsummit-papers.53,0,0.294553,"teractive system leverages these patterns in multiple iterations of translation and post-editing, thereby progressively generating a high quality bilingual lexicon. 1 Introduction The problem of language translation has been in focus for many decades and has seen contributions from both linguistic and computer science communities. Linguistic contribution (Streiter (1996)) has come in the form of several language resources comprising of dictionaries, grammar and studies on units of translation. Computer science community has contributed in coming up with formal machine translation (MT) models (Vogel et al. (2003)) that leverage corpus statistics along with linguistic features and resources. There is a body of work (Federico et al. (2014); Alabau et al. (2014)) that studies the complementary contributions of humans and MT models and present “machine-centric” translation systems that leverage human input. These systems, referred to as computer aided translation (CAT) systems, typically employ a statistical MT model to translate text and provide a post-editing tooling to enable humans to correct the resulting translations. Human feedback and corrections are used to adapt and retrain the translation model"
2020.aacl-main.78,P18-1177,0,0.0303221,"Missing"
2020.aacl-main.78,P17-1123,0,0.0946496,"a simple majority are sent to the mayor ( the President of Warsaw ) , who may sign them into law . If the mayor vetoes a bill , the Council has 30 days to override the veto by a two-thirds majority vote . Introduction and Related work Automatic question generation (QG) from text aims to generate meaningful, relevant, and answerable questions from a given textual input. Owing to its applicability in conversational systems such as Cortana, Siri, chatbots, and automated tutoring systems, QG has attracted considerable interest in both academia and industry. Recent neural network-based approaches (Du et al., 2017; Kumar et al., 2018a,b; Du and Cardie, 2018; Zhao et al., 2018; Song et al., 2018; Subramanian et al., 2018; Tang et al., 2017; Wang et al., 2017) represent the state-of-the-art in question generation. Most of these techniques learn to generate questions from short text, i.e., one or two sentences (Du et al., 2017; Kumar et al., 2018a,b; Du and Cardie, 2018). On the other hand, the ability to generate high-quality questions from longer text such as from multiple Human Generated: Our Model: How many members are on the Warsaw City Counil ? How many members are in the Warsaw City Council ? Human"
2020.aacl-main.78,P16-1154,0,0.0374451,"ncoded paragraph Figure 2: Overall architecture of our paragraph-level question generation model. 2 Paragraph encoder Problem Formulation & Approach Given a paragraph ‘P ’ and answer ‘A’, a question generation model iteratively samples question word qt ∈ V Q at every time step ‘t’ from the probability distribution given by: |Q| Y Pr(Q|P,A;θ) = Pr(qt |P,A;θ) (1) t=1 Where V Q is the question vocabulary, θ is the set of parameters, and A is the answer. 782 Dynamic, shared dictionary In the traditional approach, a new/unknown word is typically replaced with the “&lt;unk&gt;” token. The copy mechanism (Gu et al., 2016) then unfortunately learns to copy this “&lt;unk&gt;” token instead of the actual (unknown) word from the source paragraph. Instead, we use a separate dynamic dictionary unique to each source paragraph, which includes all and only words that occur in the paragraph. This allows our model to copy source words that may not be in the target dictionary into the target (question). Using a dynamic dictionary consisting of the preprocessed vocabulary instead of a static one enables the copy mechanism to copy the exact words directly into the question, even if they are rare and unknown. Given a source paragr"
2020.aacl-main.78,P02-1040,0,0.107814,"al., 2018) L2A (Du et al., 2017) NQGdd [w/o copy attention] NQGdd [with copy attention] BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE-L 45.07 42.54 55.32 61.84 29.58 25.33 32.39 41.73 21.60 16.98 20.12 30.19 16.38 11.86 12.86 22.62 20.25 16.28 17.00 21.93 44.48 39.37 42.77 48.60 Table 1: Results on the test set on automatic evaluation metrics. Best results for each metric (column) are bolded. 3 Experimental Setup We report the experimental result of our model (referred to as NQGdd ) and compare it with the current state of the art MPGSN (Zhao et al., 2018). We employ the widely-used metrics BLEU (Papineni et al., 2002), ROUGE-L and METEOR for automatic evaluation. We use evaluation script provided by (Chen et al., 2015). Similar to (Kumar et al., 2018a) we also report qualitative assessment on the syntax, semantics and relevance of the questions generated by our model. All experiments are performed on the SQuAD dataset (Rajpurkar et al., 2016), where complete paragraphs are taken as input instead of just one or two sentences. We reformat the SQuAD dataset such that during training time, each source instance is a (paragraph, question) pair annotated with the gold answers, and the target is a question. Follow"
2020.aacl-main.78,D16-1264,0,0.0466924,"on metrics. Best results for each metric (column) are bolded. 3 Experimental Setup We report the experimental result of our model (referred to as NQGdd ) and compare it with the current state of the art MPGSN (Zhao et al., 2018). We employ the widely-used metrics BLEU (Papineni et al., 2002), ROUGE-L and METEOR for automatic evaluation. We use evaluation script provided by (Chen et al., 2015). Similar to (Kumar et al., 2018a) we also report qualitative assessment on the syntax, semantics and relevance of the questions generated by our model. All experiments are performed on the SQuAD dataset (Rajpurkar et al., 2016), where complete paragraphs are taken as input instead of just one or two sentences. We reformat the SQuAD dataset such that during training time, each source instance is a (paragraph, question) pair annotated with the gold answers, and the target is a question. Following the exact setup from MPGSN (Zhao et al., 2018), we split the SQuAD train set into train and validation set containing 77,526 and 9,995 instances respectively, and take the separate SQuAD dev set containing 10,556 instances as our test set. 4 parameters. We averaged responses received by all three evaluators to compute the fin"
2020.aacl-main.78,W18-2609,0,0.0173716,"If the mayor vetoes a bill , the Council has 30 days to override the veto by a two-thirds majority vote . Introduction and Related work Automatic question generation (QG) from text aims to generate meaningful, relevant, and answerable questions from a given textual input. Owing to its applicability in conversational systems such as Cortana, Siri, chatbots, and automated tutoring systems, QG has attracted considerable interest in both academia and industry. Recent neural network-based approaches (Du et al., 2017; Kumar et al., 2018a,b; Du and Cardie, 2018; Zhao et al., 2018; Song et al., 2018; Subramanian et al., 2018; Tang et al., 2017; Wang et al., 2017) represent the state-of-the-art in question generation. Most of these techniques learn to generate questions from short text, i.e., one or two sentences (Du et al., 2017; Kumar et al., 2018a,b; Du and Cardie, 2018). On the other hand, the ability to generate high-quality questions from longer text such as from multiple Human Generated: Our Model: How many members are on the Warsaw City Counil ? How many members are in the Warsaw City Council ? Human Generated: Our Model: How often are elections for the counsel held ? How often are the Rada Miasta elected"
2020.aacl-main.78,D17-1090,0,0.0130452,", the Council has 30 days to override the veto by a two-thirds majority vote . Introduction and Related work Automatic question generation (QG) from text aims to generate meaningful, relevant, and answerable questions from a given textual input. Owing to its applicability in conversational systems such as Cortana, Siri, chatbots, and automated tutoring systems, QG has attracted considerable interest in both academia and industry. Recent neural network-based approaches (Du et al., 2017; Kumar et al., 2018a,b; Du and Cardie, 2018; Zhao et al., 2018; Song et al., 2018; Subramanian et al., 2018; Tang et al., 2017; Wang et al., 2017) represent the state-of-the-art in question generation. Most of these techniques learn to generate questions from short text, i.e., one or two sentences (Du et al., 2017; Kumar et al., 2018a,b; Du and Cardie, 2018). On the other hand, the ability to generate high-quality questions from longer text such as from multiple Human Generated: Our Model: How many members are on the Warsaw City Counil ? How many members are in the Warsaw City Council ? Human Generated: Our Model: How often are elections for the counsel held ? How often are the Rada Miasta elected ? Human Generated:"
2020.aacl-main.78,D18-1424,0,0.391576,"rsaw ) , who may sign them into law . If the mayor vetoes a bill , the Council has 30 days to override the veto by a two-thirds majority vote . Introduction and Related work Automatic question generation (QG) from text aims to generate meaningful, relevant, and answerable questions from a given textual input. Owing to its applicability in conversational systems such as Cortana, Siri, chatbots, and automated tutoring systems, QG has attracted considerable interest in both academia and industry. Recent neural network-based approaches (Du et al., 2017; Kumar et al., 2018a,b; Du and Cardie, 2018; Zhao et al., 2018; Song et al., 2018; Subramanian et al., 2018; Tang et al., 2017; Wang et al., 2017) represent the state-of-the-art in question generation. Most of these techniques learn to generate questions from short text, i.e., one or two sentences (Du et al., 2017; Kumar et al., 2018a,b; Du and Cardie, 2018). On the other hand, the ability to generate high-quality questions from longer text such as from multiple Human Generated: Our Model: How many members are on the Warsaw City Counil ? How many members are in the Warsaw City Council ? Human Generated: Our Model: How often are elections for the counsel"
2021.eacl-main.247,P15-1054,1,0.810089,"ings learnt using the joint optimisation approach correlate better with the ground truth than other alternatives. 2 Related Work Several conventional classification methods are capable of handling classification in multi-label settings. However, relatively fewer of these are designed to incorporate the possibly hierarchical organisation of the class labels. These include both traditional methods (Gopal and Yang, 2013; Lewis et al., 2004) as well as deep learning methods (Johnson and Zhang, 2015; Peng et al., 2018) across varied domains such as news articles, web content, etc. Some approaches (Bairi et al., 2015, 2016) have also attempted to identify a subset of class labels from the classification hierarchy that effectively represents most instances from the training dataset. Traditional or flat classification approaches typically perform prediction assuming that all the classes are independent of each other, ignoring the class hierarchy. Whereas ‘local’ classification approaches (Koller and Sahami, 1997; Cesa-Bianchi et al., 2006) train a set of classifiers at each level of the hierarchy. However, it has also been argued (Cerri et al., 2011) that it is impractical to train separate classifiers at e"
2021.eacl-main.247,N15-1011,0,0.027236,"state-of-the-art hierarchical multi-label classifiers that have complete access to the true label hierarchy (c) label embeddings learnt using the joint optimisation approach correlate better with the ground truth than other alternatives. 2 Related Work Several conventional classification methods are capable of handling classification in multi-label settings. However, relatively fewer of these are designed to incorporate the possibly hierarchical organisation of the class labels. These include both traditional methods (Gopal and Yang, 2013; Lewis et al., 2004) as well as deep learning methods (Johnson and Zhang, 2015; Peng et al., 2018) across varied domains such as news articles, web content, etc. Some approaches (Bairi et al., 2015, 2016) have also attempted to identify a subset of class labels from the classification hierarchy that effectively represents most instances from the training dataset. Traditional or flat classification approaches typically perform prediction assuming that all the classes are independent of each other, ignoring the class hierarchy. Whereas ‘local’ classification approaches (Koller and Sahami, 1997; Cesa-Bianchi et al., 2006) train a set of classifiers at each level of the hie"
2021.eacl-main.247,D14-1181,0,0.00277338,"er that jointly learns the classifier parameters as well as the label embeddings. 2831 4.2 Our Model: H IDDE N Our proposed model H IDDE N has two key components: one for representing the documents that may lead to well-generalizing classifiers and the other for embedding the labels in a hyperbolic space. Recall that hyperbolic spaces have shown to be wellsuited for data satisfying hierarchical relations. Document Model Fw accepts as input a document, D, and outputs a n-dimensional representation of it, Fw (D) ∈ Rn . Here, w is the set of parameters to be learnt. In this work, we use TextCNN (Kim, 2014) as the document model. But our approach remains valid irrespective of the chosen document model. Label Embedding Model GΘ accepts as input a label l and outputs a finite dimensional representation, GΘ (l). Here, Θ is the set of parameters to be learnt. In this work, following Nickel and Kiela (2018), we employ the simple look-up based model defined by GΘ (l) ≡ Θ ∗ y l = Θl , where Θ ∈ Rn×L and Θl is the lth column of Θ. These Euclidean embeddings Θl are then projected onto the Poincare manifold using the transformation Π(x) = √ x . In summary, the hyperbolic 2 1+ 1+kxk2 embedding of label, l,"
2021.eacl-main.247,D19-1042,0,0.185831,"ribution Times) corpus “Voice Recognition Is Improving, but Don’t Stop the Elocution Lessons” for which labels are “Top/News/Technology”. Here, labels are arranged in a hierarchy, hereafter referred as label hierarchy. We undertake the task of labelling documents with classes that are hierarchically organised; this problem is popularly known as hierarchical multi-label text classification (HMC). HMC methods have found several applications in online advertising systems (Agrawal et al., 2013), bio-informatics (Peng et al., 2016; Triguero and Vens, 2016), text classification (Rousu et al., 2006; Mao et al., 2019). The main challenge in HMC is in modelling classification of the document into a large, imbalanced and structured output space. In HMC, the label taxonomy is a partially ordered set (L, ≺) where L is a finite set of all class labels. Relation ≺ refers to is-a relationship between labels, which is asymmetric, anti-reflexive and transitive (Silla and Freitas, 2011). Hierarchical structures can provide important insights for learning and classification tasks. However, explicit knowledge of hierarchy is not available in several domains, for instance, extreme classification datasets (Bhatia et al."
2021.eacl-main.247,D14-1162,0,0.091022,"Missing"
2021.eacl-main.314,2020.acl-main.747,0,0.128303,"Missing"
2021.eacl-main.314,D18-1398,0,0.10296,"er learning in a variety of NLP applications such as question answering, neural machine translation, etc. (McCann et al., 2018; Hashimoto et al., 2017; Chen et al., 2018; Kiperwasser and Ballesteros, 2018). Some multi-task learning approaches (Jawanpuria et al., 2015) have attempted to identify clusters (or groups) of related tasks based on end-to-end convex optimization formulations. Meta-learning algorithms (Nichol et al., 2018) are highly effective for fast adaptation and have recently been shown to be beneficial for several machine learning tasks (Santoro et al., 2016; Finn et al., 2017). Gu et al. (2018) use a meta-learning algorithm for machine translation to leverage information from high-resource languages. Dou et al. (2019) investigate multiple model agnostic meta-learning algorithms for low-resource natural language understanding on the GLUE (Wang et al., 2018) benchmark. Data sampling strategies for multi-task learning and meta-learning form the third thread of related work. A good sampling strategy has to account for the imbalance in dataset sizes across tasks/languages and the similarity between tasks/languages. A simple heuristic-based solution to address the issue of data imbalance"
2021.eacl-main.314,D17-1206,0,0.0243548,"ask on target languages that never appear during training and show its superiority compared to competitive zero-shot baselines. 2 Related Work We summarize three threads of related research that look at the transferability in models across different tasks and different languages: multi-task learning, meta-learning and data sampling strategies for both multi-task learning and meta-learning. Multitask learning (Caruana, 1993) has proven to be highly effective for transfer learning in a variety of NLP applications such as question answering, neural machine translation, etc. (McCann et al., 2018; Hashimoto et al., 2017; Chen et al., 2018; Kiperwasser and Ballesteros, 2018). Some multi-task learning approaches (Jawanpuria et al., 2015) have attempted to identify clusters (or groups) of related tasks based on end-to-end convex optimization formulations. Meta-learning algorithms (Nichol et al., 2018) are highly effective for fast adaptation and have recently been shown to be beneficial for several machine learning tasks (Santoro et al., 2016; Finn et al., 2017). Gu et al. (2018) use a meta-learning algorithm for machine translation to leverage information from high-resource languages. Dou et al. (2019) investi"
2021.eacl-main.314,2020.acl-main.754,0,0.101613,"et al., 2018) benchmark. Data sampling strategies for multi-task learning and meta-learning form the third thread of related work. A good sampling strategy has to account for the imbalance in dataset sizes across tasks/languages and the similarity between tasks/languages. A simple heuristic-based solution to address the issue of data imbalance is to assign more weight to low-resource tasks or lan3601 guages (Aharoni et al., 2019). Arivazhagan et al. (2019) define a temperature parameter which controls how often one samples from low-resource tasks/languages. The MultiDDS algorithm, proposed by Wang et al. (2020b), actively learns a different set of parameters for sampling batches given a set of tasks such that the performance on a held-out set is maximized. We use a variant of MultiDDS as a sampling strategy in our meta-learned model. Nooralahzadeh et al. (2020) is most similar in spirit to our work in that they study a crosslingual and cross-task meta-learning architecture but only focus on zero-shot and few-shot transfer for two natural language understanding tasks, NLI and QA. In contrast, we study many tasks in many languages, in conjunction with sampling strategies, and offer concrete insights"
2021.eacl-main.314,N18-1101,0,0.0466985,"ssociated datasets in multiple languages. The sources of POS and NER datasets are Universal Dependency v2.5 treebank (Nivre et al., 2020) and WikiAnn (Pan et al., 2017) respectively, with ground-truth labels available for each language. Large-scale datasets for QA, NLI and PA were originally available only for English. The PAWS-X (Yang et al., 2019) dataset contains machine-translated training pairs and human-translated evaluation pairs for PA. The authors of XTREME train a custom-built translation system to obtain translated datasets for QA and NLI. For the NLI task, we train using MultiNLI (Williams et al., 2018) and evaluate on XNLI (Conneau et al., 2018). For the QA task, SQuAD 1.1 (Rajpurkar et al., 2016) was used for training and MLQA (Lewis et al., 2019) for evaluation. Regarding evaluation metrics, for QA we report F1 scores and for the other four tasks (PA, NLI, POS, NER) we report accuracy scores. 4.2 Implementation Details BERT (Devlin et al., 2019) models yield state-ofthe-art performance for many NLP tasks. Since we are dealing with datasets in multiple languages, we build our meta learning models on mBERT (Pires et al., 2019; Wu and Dredze, 2019) base architecture, implemented by Wolf et a"
2021.findings-acl.408,D12-1012,1,0.717088,"es feature information and LFs to learn the parameters jointly. Hu et al. (2016) proposed a student-teacher model that transfers rule information by assigning linear weight to each rule based on an agreement objective. The model we propose in this paper jointly learns parameters over features and rules in a semi-supervised manner rather than just weighing their outputs and can therefore be more expressive. Other works such as Jawanpuria et al. (2011); Dembczy´nski et al. (2008) discovers simple and conjuctive rules from input features and assign weight to each rules for better generalization. Nagesh et al. (2012) induces rules from query language to build NER extractor while Kulkarni et al. (2018) uses active learning to derive consensus among labelers to label data. Semi-Supervised Data Programming: The only work which, to our knowledge, combines rules with supervised learning in a joint framework is the work by Awasthi et al. (2020). They leverage both rules and labelled data by associating each rule with exemplars of correct firings (i.e., instantiations) of that rule. Their joint training algorithms denoise over-generalized rules and train a classification model. Our approach differs from their wo"
2021.findings-acl.436,A00-2032,0,0.175524,"gs of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4923–4932 August 1–6, 2021. ©2021 Association for Computational Linguistics Rule Set e(i1,j1) l(i1,j1) VP -> NP sat VPZ -> NPZ sat NP -> the cat NP -> the VP VP -> cat sat NPZ -> the cat The cat sat Figure 1: For the input ‘The cat sat’, DIORA computes e(i1 , j1 ) compatibility score for each pair of neighboring constituents. l(i1 , j1 ) is computed using triggered rules for each span and it interacts with the compatibility score in our loss function as explained in Section 3.1. constituency trees (Brill et al., 1990; Ando and Lee, 2000) using dependency parsers (Klein and Manning, 2004) and inside-outside parsing algorithm (Drozdov et al., 2019). Recently, (Drozdov et al., 2019) proposed an unsupervised latent chart tree parsing algorithm, viz., DIORA, that uses the inside-outside algorithm for parsing and has an autoencoder-based neural network trained to reconstruct the input sentence. DIORA is trained end to end using masked language model via word prediction. As of date, DIORA is the state-of-the-art approach to unsupervised constituency parsing. Exploiting additional semantic and syntactic information that acts as a sou"
2021.findings-acl.436,H90-1055,0,0.758467,"rning of 4923 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4923–4932 August 1–6, 2021. ©2021 Association for Computational Linguistics Rule Set e(i1,j1) l(i1,j1) VP -> NP sat VPZ -> NPZ sat NP -> the cat NP -> the VP VP -> cat sat NPZ -> the cat The cat sat Figure 1: For the input ‘The cat sat’, DIORA computes e(i1 , j1 ) compatibility score for each pair of neighboring constituents. l(i1 , j1 ) is computed using triggered rules for each span and it interacts with the compatibility score in our loss function as explained in Section 3.1. constituency trees (Brill et al., 1990; Ando and Lee, 2000) using dependency parsers (Klein and Manning, 2004) and inside-outside parsing algorithm (Drozdov et al., 2019). Recently, (Drozdov et al., 2019) proposed an unsupervised latent chart tree parsing algorithm, viz., DIORA, that uses the inside-outside algorithm for parsing and has an autoencoder-based neural network trained to reconstruct the input sentence. DIORA is trained end to end using masked language model via word prediction. As of date, DIORA is the state-of-the-art approach to unsupervised constituency parsing. Exploiting additional semantic and syntactic informati"
2021.findings-acl.436,P17-2012,0,0.0276203,"anism ensures that the learnt model leverages the wellunderstood language grammar. We propose an approach that utilizes very generic linguistic knowledge of the language present in the form of syntactic rules, thus inducing better syntactic structures. We introduce a novel formulation that takes advantage of the syntactic grammar rules and is independent of the base system. We achieve new state-of-the-art results on two benchmarks datasets, MNLI and WSJ.1 1 Introduction Syntactic parse trees have demonstrated their importance in several downstream NLP applications such as machine translation (Eriguchi et al., 2017; Zaremoodi and Haffari, 2018), natural language inference (NLI) (Choi et al., 2018), relation extraction (Gamallo et al., 2012) and text classification (Tai et al., 2015). Based on linguistic theories that have promoted the usefulness of tree-based representation of natural language text, tree-based models such as Tree-LSTM have been proposed to learn sentence representations (Socher et al., 2011). Inspired by the Tree-LSTM based models, many approaches were proposed do not require parse tree supervision (Yogatama et al., 2017; Choi et al., 2018; Maillard et al., 2019; Drozdov et al., 2019)."
2021.findings-acl.436,W12-0702,0,0.0474963,"Missing"
2021.findings-acl.436,P19-1228,0,0.0325149,"Missing"
2021.findings-acl.436,P04-1061,0,0.100511,"stics: ACL-IJCNLP 2021, pages 4923–4932 August 1–6, 2021. ©2021 Association for Computational Linguistics Rule Set e(i1,j1) l(i1,j1) VP -> NP sat VPZ -> NPZ sat NP -> the cat NP -> the VP VP -> cat sat NPZ -> the cat The cat sat Figure 1: For the input ‘The cat sat’, DIORA computes e(i1 , j1 ) compatibility score for each pair of neighboring constituents. l(i1 , j1 ) is computed using triggered rules for each span and it interacts with the compatibility score in our loss function as explained in Section 3.1. constituency trees (Brill et al., 1990; Ando and Lee, 2000) using dependency parsers (Klein and Manning, 2004) and inside-outside parsing algorithm (Drozdov et al., 2019). Recently, (Drozdov et al., 2019) proposed an unsupervised latent chart tree parsing algorithm, viz., DIORA, that uses the inside-outside algorithm for parsing and has an autoencoder-based neural network trained to reconstruct the input sentence. DIORA is trained end to end using masked language model via word prediction. As of date, DIORA is the state-of-the-art approach to unsupervised constituency parsing. Exploiting additional semantic and syntactic information that acts as a source of additional guidance rather than the primary"
2021.findings-acl.436,P14-5010,0,0.00968022,"tively. RL and CE are rule loss and cross entropy loss respectively. (δ) is the average tree height. PP refers to post-processing heuristic. † indicates scores reported by (Williams et al., 2018) for a fully supervised model. ∗ are reported by (Kim et al., 2019a). vised parsing, unsupervised segment recall, and phrase similarity. 4.1 Rule Set Data We evaluate our model on two data sets: The Wall Street Journal (WSJ) and MultiNLI. WSJ is an extraction of PennTree Bank (Marcus et al., 2002) containing human-annotated constituency parse trees. MultiNLI consists of Stanford generated parse trees (Manning et al., 2014) as the ground truth. MultiNLI is originally designed for evaluating NLI tasks, but is often also utilized to evaluate constituency parse trees. We train on the complete NLI dataset, which is a composition of the MultiNLI and SNLI train sets. We evaluate model performance on the MultiNLI dev set and WSJ test set (split 23) following the experimental setting and evaluation metrics in (Drozdov et al., 2019). Further details are provided in the appendix. We initialize our model with the trained weights of DIORA and evaluate on unsupervised constituency parsing and segment recall. We also perform"
2021.findings-acl.436,W12-4501,0,0.0298975,"es the models capability to learn meaningful representation for spans of the text. Generally, most models focus more on generating the tokens representation and then use some ad-hoc arithmetic operations to generate representation for the larger spans of text thus losing the essence of the context that ties the words of the span. To evaluate on the phrase similarity task we consider two data sets of labeled phrases: 1) CoNLL 2000 (Tjong Kim Sang and Buchholz, 2000), which is a shallow parsed dataset and contains spans of verb phrases, noun phrases, preposition phrases etc., and 2) CoNLL 2012 (Pradhan et al., 2012) which is a named entity dataset containing 19 different entity types. For the evaluation routine, we first generated the phrase representation of labeled spans whose length is greater than one. Cosine similarity is then used to obtain the similarity score of it with respect to all other labeled spans. We then calculate if the label for that query span matches the labels for each of the K most similar other spans in the dataset.In Table 4 we report precision@K for both datasets and various values of K. The baseline numbers are reported using the weights of DIORA provided by the authors. 5 Conc"
2021.findings-acl.436,P15-1150,0,0.0377443,"nt in the form of syntactic rules, thus inducing better syntactic structures. We introduce a novel formulation that takes advantage of the syntactic grammar rules and is independent of the base system. We achieve new state-of-the-art results on two benchmarks datasets, MNLI and WSJ.1 1 Introduction Syntactic parse trees have demonstrated their importance in several downstream NLP applications such as machine translation (Eriguchi et al., 2017; Zaremoodi and Haffari, 2018), natural language inference (NLI) (Choi et al., 2018), relation extraction (Gamallo et al., 2012) and text classification (Tai et al., 2015). Based on linguistic theories that have promoted the usefulness of tree-based representation of natural language text, tree-based models such as Tree-LSTM have been proposed to learn sentence representations (Socher et al., 2011). Inspired by the Tree-LSTM based models, many approaches were proposed do not require parse tree supervision (Yogatama et al., 2017; Choi et al., 2018; Maillard et al., 2019; Drozdov et al., 2019). However, (Williams et al., 2018; Sahay et al., 2021) have shown that these methods cannot learn meaningful semantics (not even simple grammar), though they perform well on"
2021.findings-acl.436,W00-0726,0,0.578821,"CoNLL 2012 tasks. 4.5 Phrase Similarity We also employed the phrase similarity strategy followed by (Drozdov et al., 2019). Phrase Similarity scores measures the models capability to learn meaningful representation for spans of the text. Generally, most models focus more on generating the tokens representation and then use some ad-hoc arithmetic operations to generate representation for the larger spans of text thus losing the essence of the context that ties the words of the span. To evaluate on the phrase similarity task we consider two data sets of labeled phrases: 1) CoNLL 2000 (Tjong Kim Sang and Buchholz, 2000), which is a shallow parsed dataset and contains spans of verb phrases, noun phrases, preposition phrases etc., and 2) CoNLL 2012 (Pradhan et al., 2012) which is a named entity dataset containing 19 different entity types. For the evaluation routine, we first generated the phrase representation of labeled spans whose length is greater than one. Cosine similarity is then used to obtain the similarity score of it with respect to all other labeled spans. We then calculate if the label for that query span matches the labels for each of the K most similar other spans in the dataset.In Table 4 we re"
2021.findings-acl.436,Q18-1019,0,0.169007,"i and Haffari, 2018), natural language inference (NLI) (Choi et al., 2018), relation extraction (Gamallo et al., 2012) and text classification (Tai et al., 2015). Based on linguistic theories that have promoted the usefulness of tree-based representation of natural language text, tree-based models such as Tree-LSTM have been proposed to learn sentence representations (Socher et al., 2011). Inspired by the Tree-LSTM based models, many approaches were proposed do not require parse tree supervision (Yogatama et al., 2017; Choi et al., 2018; Maillard et al., 2019; Drozdov et al., 2019). However, (Williams et al., 2018; Sahay et al., 2021) have shown that these methods cannot learn meaningful semantics (not even simple grammar), though they perform well on NLI tasks. Recently, there has been surge in approaches using weak supervision in the form of rules for various ∗ Equal contribution The source code of the paper is available at https://github.com/anshuln/Diora with rules. 1 tasks such as sequence classification (Safranchik et al., 2020), text classification (Chatterjee et al., 2020; Maheshwari et al., 2020), etc. These approaches have demonstrated the importance of external knowledge in both unsupervised"
2021.findings-acl.436,C18-1120,0,0.0155306,"learnt model leverages the wellunderstood language grammar. We propose an approach that utilizes very generic linguistic knowledge of the language present in the form of syntactic rules, thus inducing better syntactic structures. We introduce a novel formulation that takes advantage of the syntactic grammar rules and is independent of the base system. We achieve new state-of-the-art results on two benchmarks datasets, MNLI and WSJ.1 1 Introduction Syntactic parse trees have demonstrated their importance in several downstream NLP applications such as machine translation (Eriguchi et al., 2017; Zaremoodi and Haffari, 2018), natural language inference (NLI) (Choi et al., 2018), relation extraction (Gamallo et al., 2012) and text classification (Tai et al., 2015). Based on linguistic theories that have promoted the usefulness of tree-based representation of natural language text, tree-based models such as Tree-LSTM have been proposed to learn sentence representations (Socher et al., 2011). Inspired by the Tree-LSTM based models, many approaches were proposed do not require parse tree supervision (Yogatama et al., 2017; Choi et al., 2018; Maillard et al., 2019; Drozdov et al., 2019). However, (Williams et al., 201"
2021.findings-acl.447,D18-1276,1,0.786097,"s compared to ASR systems that use native scripts.1 1 Introduction Sanskrit is a language with fairly advanced disciplines of phonetics (Śiksā),  prosody (Chandas), and grammar (Vyākaran a). The language has a rich oral tradition and it tends to follow a phonemic-orthography resulting in systematic ∗ Joint first author Dataset and code can be accessed from www.cse.iitb.ac.in/~asr and https://github. com/cyfer0618/Vaksanca.git. 1 grapheme-phoneme correspondences. Connected speech leads to phonemic transformations in utteracnes, and in Sanskrit this is faithfully preserved in writing as well (Krishna et al., 2018). This is called as Sandhi and is defined as the euphonic assimilation of sounds, i.e., modification and fusion of sounds, at or across the boundaries of grammatical units (Matthews, 2007, p. 353). Phonemic orthography is beneficial for a language, when it comes to designing automatic speech recognition Systems (ASR), specifically for unit selection at both the Acoustic Model (AM) and Language Model (LM) levels. Regardless of the aforementioned commonalities preserved in both the speech and text in Sanskrit, designing a large scale ASR system raises several challenges. The Unicode encoding for"
2021.findings-acl.447,P19-1111,1,0.830009,"then output += ci ; else if ci+1 is C and ci+2 is V and ci+2 is first vowel of the word then output += ci ; else if ci+1 is C and ci+2 is V then output += ci + “ ”; BPE-based Word Segmentation Byte pair encoding (BPE) is a simple data compression algorithm that iteratively replaces the frequently occurring subword units with a single unused byte (Gage, 1994). This technique was first adopted to model rare words using subword units in neural machine translation (Sennrich et al., 2016). Interestingly, BPE has been explored for learning new vocabulary for poetry to prose conversion in Sanskrit (Krishna et al., 2019). We consider the benefits of using BPE as a subword unit for Sanskrit ASR. While BPE is a purely data-driven segmentation strategy, we next present a linguistically motivated segmentation approach that might be aligned with finding syllable units for ASR that are more phonetically compliant. We refer to this technique as vowel segmentation. 3.2.2 Vowel Segmentation Splitting the tokens based on vowels and adjacent consonants is inspired by the identification of metres in Sanskrit prosody, where the metre of a verse is identified by using syllable segmentation, followed by identification of sy"
2021.findings-acl.447,2020.lrec-1.874,1,0.673818,"re possible in utterance of long text sequences where multiple lexical items are fused together via Sandhi. These segmentations are accompanied with the corresponding Sandhi based transformations, resulting in a new phonetic sequence different from the original sequence. Finally, Sanskrit might be one of those rare natural languages where the number of non-native proficient speakers are 5039 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 5039–5050 August 1–6, 2021. ©2021 Association for Computational Linguistics manifold in comparison to the native speakers (Krishna et al., 2020). This makes the ASR task further challenging, as the speakers are prone to carry their influence from their corresponding mother tongues into the Sanskrit utterances as well. While there exist several computational models for processing Sanskrit texts (Kulkarni, 2013; Kumar et al., 2010; Shukla et al., 2010; Kulkarni et al., 2010a; Goyal et al., 2012; Kulkarni et al., 2010c; Mishra et al., 2013; Saluja et al., 2017; Anoop and Ramakrishnan, 2019; Krishna et al., 2021), large scale systems for processing of speech in Sanskrit, are almost non-existent. First, we present a new dataset, with 78 ho"
2021.findings-acl.447,W13-3718,0,0.0212995,"ally, Sanskrit might be one of those rare natural languages where the number of non-native proficient speakers are 5039 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 5039–5050 August 1–6, 2021. ©2021 Association for Computational Linguistics manifold in comparison to the native speakers (Krishna et al., 2020). This makes the ASR task further challenging, as the speakers are prone to carry their influence from their corresponding mother tongues into the Sanskrit utterances as well. While there exist several computational models for processing Sanskrit texts (Kulkarni, 2013; Kumar et al., 2010; Shukla et al., 2010; Kulkarni et al., 2010a; Goyal et al., 2012; Kulkarni et al., 2010c; Mishra et al., 2013; Saluja et al., 2017; Anoop and Ramakrishnan, 2019; Krishna et al., 2021), large scale systems for processing of speech in Sanskrit, are almost non-existent. First, we present a new dataset, with 78 hours of speech covering about 46,000 sentences, for ASR in Sanskrit. Keeping the rich and long cultural heritage the language carries, we prepare our dataset to be diverse both chronologically and in terms of the domain coverage. Further, the dataset contains utterance"
2021.findings-acl.447,D13-1019,0,0.0131228,"”. In prior work involving Indian languages for TTS, Kishore et al. (2002) proposed various syllabification rules for words. Herein (with a few exceptions), if a vowel is followed by 3 or more consonants, only the first following vowel is grouped with the preceding vowel to form the subword unit. Our proposed algorithm for vowel segmentation (VS) is outlined in Algorithm 1. We propose segmenting words at vowel boundaries to extract the units for which alignment with speech is learnt within the ASR system. For acoustic models, an effective unit of a word for ASR would arguably be the syllable (Lee et al., 2013). Representing a word in terms of syllables demands the mapping of a word from graphemes to phonemes. To create syllable units, phonemes are then combined together based on the sonority sequencing principle (Clements, 1990). Absence of accurate syllabifiers for Indian languages restricts the use of syllables as units for learning alignment. Our approach produces units which can be viewed as a rough approximation to a syllable. A syllable is composed of three parts viz., onset, nucleus and coda, where nucleus has the highest sonority and is always a vowel. In our approach, the onset is always o"
2021.findings-acl.447,P16-1162,0,0.00864625,"= ci ; else if ci+1 is C and ci+2 is V then output += ci + “ ”; else if ci+1 is V then output += ci ; else if ci+1 is C and ci+2 is C then output += ci ; else if ci+1 is C and ci+2 is V and ci+2 is first vowel of the word then output += ci ; else if ci+1 is C and ci+2 is V then output += ci + “ ”; BPE-based Word Segmentation Byte pair encoding (BPE) is a simple data compression algorithm that iteratively replaces the frequently occurring subword units with a single unused byte (Gage, 1994). This technique was first adopted to model rare words using subword units in neural machine translation (Sennrich et al., 2016). Interestingly, BPE has been explored for learning new vocabulary for poetry to prose conversion in Sanskrit (Krishna et al., 2019). We consider the benefits of using BPE as a subword unit for Sanskrit ASR. While BPE is a purely data-driven segmentation strategy, we next present a linguistically motivated segmentation approach that might be aligned with finding syllable units for ASR that are more phonetically compliant. We refer to this technique as vowel segmentation. 3.2.2 Vowel Segmentation Splitting the tokens based on vowels and adjacent consonants is inspired by the identification of m"
bellare-etal-2004-generic,J98-3005,0,\N,Missing
bellare-etal-2004-generic,W97-0703,0,\N,Missing
bellare-etal-2004-generic,P00-1038,0,\N,Missing
bellare-etal-2004-generic,C96-2166,0,\N,Missing
bellare-etal-2004-generic,W01-0100,0,\N,Missing
D12-1012,X98-1004,0,0.080891,"s on SystemT and AQL (Section 3) and define the target language for our induction algorithm and the notion of rule complexity (Section 4). We then present our approach for inducing CD and CR rules, and discuss induction biases that would favor interpretability (Section 5), and discuss the results of an empirical evaluation (Section 6). We conclude with avenues for improvement in the future (Section 7). 2 Related Work Existing approaches to rule induction for IE focus on rule-based systems based on the cascading grammar formalism exemplified by the Common Pattern Specification Language (CPSL) (Appelt and Onyshkevych, 1998), where rules are specified as a sequence of basic features that describe an entity, with limited predicates in the context of an entity mention. Patel et al. (2009) and Soderland (1999) elaborate on top-down techniques for induction of IE rules, whereas (Califf and Mooney, 1997; Califf and Mooney, 1999) discuss a bottom-up IE rule induction system that uses the relative least general generalization (RLGG) of examples1 . However, in all these systems, the expressivity of the rulerepresentation language is restricted to that of capturing sequence information. As discussed in Section 3, contextu"
D12-1012,P10-1014,1,0.836172,"s that the induced rules have good accuracy and low complexity according to our complexity measure. SystemT Dataset ACE2002 ACE 2005 CoNLL 2003 Enron Generic 57.8 57.32 64.15 76.53 Fβ=1 Customized 82.2 88.95 91.77 85.29 Table 1: Quality of generic vs. customized rules. 1 Introduction Named-entity recognition (NER) is the task of identifying mentions of rigid designators from text belonging to named-entity types such as persons, organizations and locations (Nadeau and Sekine, 2007). Generic NER rules have been shown to work reasonably well-out-of-the-box, and with further domain customization (Chiticariu et al., 2010b), achieve quality surpassing state-of-the-art results. Table 1 summarizes the quality of NER rules out-of-the-box and after domain customization in the GATE (Cunningham et al., 2011) and SystemT (Chiticariu et al., 2010a) systems, as reported in (Maynard et al., 2003) and (Chiticariu et al., 2010b) respectively. Rule-based systems are widely used in enterprise settings due to their explainability. Rules are transparent, which leads to better explainability of errors. One can easily identify the cause of a false positive or negative, and refine the rules without affecting other correct result"
D12-1012,D10-1098,1,0.920912,"Missing"
D12-1012,D08-1003,1,0.939845,"items in the WHERE clause), or the size of the head of the query (e.g., items in the SELECT clause). As such, our notion of complexity is rather coarse, and we shall discuss its shortcomings in detail in Section 6.2. However, we shall show that the complexity score significantly reduces the search space of our induction techniques leading to Phase name Basic Features AQL statements Phase 1 (Clustering and RLGG) Phase 2 (Propositional Rule Learning) Consolidation select extract select, union all, minus consolidate, union all Prescription Off-the-shelf, Learning using prior work (Riloff, 1993; Li et al., 2008) Bottom-up learning (LGG), Top-down refinement RIPPER, Lightweight Rule Induction Rule Type Basic Features Definition Manually identified consolidation rules, based on domain knowledge Consolidation rules Development of Candidate Rules Candidate Rules Filtering Table 2: Phases in induction, the language constructs invoked in each phase, the prescriptions for inducing rules in the phase and the corresponding type of rule in manual rule development. simpler and smaller extractors, and therefore constitutes a good basis for more comprehensive measures of interpretability in the future. 5 Inductio"
D12-1012,W03-0419,0,0.261988,"Missing"
D14-1208,P11-1055,0,0.582331,"set of relations or none of them. Consequently, a number of models have been proposed in literature to provide better heuristics for the mapping between the entity pair in the database and its mentions in the sentences of the corpus. Riedel et al. (2010) tightens the assumption of distant supervision in the following manner: “Given a pair of entities and their mentions in sentences from a corpus, at least one of the mentions express the relation given in the database”. In other words, it models the problem as that of multi-instance (mentions) single-label (relation) learning. Following this, Hoffmann et al. (2011) and Surdeanu et al. (2012) propose models that consider the mapping as that of multi-instance multi-label learning. The instances are the mentions of the entity pair in the sentences of the corpus and the entity pair can participate in more than one relation. Although, these models work very well in practice, they have a number of shortcomings. One of them is the possibility that during the alignment, a fact in the database might not have an instantiation in the corpus. For instance, if our corpus only contains documents from the years 2000 to 2005, the fact presidentOf(Barack Obama, United S"
D14-1208,P09-1113,0,0.702117,"inference procedures in training as integer linear programming (ILP) problems and implement the relaxation to the “at least one ” heuristic via a soft constraint in this formulation. Empirically, we demonstrate that this simple strategy leads to a better performance under certain settings over the existing approaches. 1 Introduction Although supervised approaches to relation extraction (GuoDong et al., 2005; Surdeanu and Ciaramita, 2007) achieve very high accuracies, they do not scale as they are data intensive and the cost of creating annotated data is quite high. To alleviate this problem, Mintz et al. (2009) proposed relation extraction in the paradigm of distant supervision. In this approach, given a database of facts (e.g. Freebase1 ) and an unannotated document collection, the goal is to heuristically align the facts in the database to the sentences in the corpus which contain the entities mentioned in the fact. This is done to create weakly labeled training data to train a classifier for relation extraction. The underlying assumption is that all mentions of 1 Ganesh Ramakrishnan 3 Dept. of CSE, IIT Bombay ganesh@cse.iitb.ac.in an entity pair2 (i.e. sentences containing the entity pair) in the"
D14-1208,W04-2401,0,0.259456,"Missing"
D14-1208,D12-1042,0,0.582717,"f them. Consequently, a number of models have been proposed in literature to provide better heuristics for the mapping between the entity pair in the database and its mentions in the sentences of the corpus. Riedel et al. (2010) tightens the assumption of distant supervision in the following manner: “Given a pair of entities and their mentions in sentences from a corpus, at least one of the mentions express the relation given in the database”. In other words, it models the problem as that of multi-instance (mentions) single-label (relation) learning. Following this, Hoffmann et al. (2011) and Surdeanu et al. (2012) propose models that consider the mapping as that of multi-instance multi-label learning. The instances are the mentions of the entity pair in the sentences of the corpus and the entity pair can participate in more than one relation. Although, these models work very well in practice, they have a number of shortcomings. One of them is the possibility that during the alignment, a fact in the database might not have an instantiation in the corpus. For instance, if our corpus only contains documents from the years 2000 to 2005, the fact presidentOf(Barack Obama, United States) will not be present"
D14-1208,P05-1053,0,0.116053,"n the corpus. In this paper, we discuss and critically analyse a popular alignment strategy called the “at least one” heuristic. We provide a simple, yet effective relaxation to this strategy. We formulate the inference procedures in training as integer linear programming (ILP) problems and implement the relaxation to the “at least one ” heuristic via a soft constraint in this formulation. Empirically, we demonstrate that this simple strategy leads to a better performance under certain settings over the existing approaches. 1 Introduction Although supervised approaches to relation extraction (GuoDong et al., 2005; Surdeanu and Ciaramita, 2007) achieve very high accuracies, they do not scale as they are data intensive and the cost of creating annotated data is quite high. To alleviate this problem, Mintz et al. (2009) proposed relation extraction in the paradigm of distant supervision. In this approach, given a database of facts (e.g. Freebase1 ) and an unannotated document collection, the goal is to heuristically align the facts in the database to the sentences in the corpus which contain the entities mentioned in the fact. This is done to create weakly labeled training data to train a classifier for"
D19-3030,W00-1316,0,0.259299,"s organized as follows. We discuss related work in Section 2. In Section 3, We describe the architecture of ParaQG. This is followed by details of the demonstration in Section 4 and the implementation in Section 5. Conclusion is discussed in Section 6. 2 Related Work Automatically generating questions and answers from text is a challenging task. This task can be traced back to 1976 when Wolfe (1976) presented their system AUTOQUEST, which examined the generation of Wh-questions from single sentences. This was followed by several pattern matching (Hirschman et al., 1999) and linear regression (Ng et al., 2000) based models. These approaches are heavily dependent on either rules 3 System Architecture ParaQG generates questions from sentences and paragraphs following a four-stage interactive procedure: (a) paragraph review, (b) answer selection, (c) question generation with associated confi176 3.3 dence score, and (d) question filtering and grouping based on answer facets. Given a paragraph, ParaQG first reviews the content automatically and then flags any unprocessable characters (e.g. Unicode characters) and URLs, which the user are prompted to edit or remove (Section 3.1). Next, the user is provid"
D19-3030,P18-2124,0,0.0309571,"quence-tosequence model augmented with dynamic dictionary, copy mechanism and global sparse-max attention. Our question generation module consists of a paragraph encoder and a question decoder. The encoder represents the paragraph input as a single fixed-length continuous vector. This vector representation of the paragraph is passed to the decoder with reusable copy mechanism and sparsemax attention to generate questions. 3.4 BERT-based Question Filtering We use the BERTbase (Devlin et al., 2018) model to filter out unanswerable questions generated by our model. we fine-tune BERT on SQuAD 2.0(Rajpurkar et al., 2018). SQuAD 2.0 extends SQuAD with over 50000 unanswerable questions. The unanswerable questions are flagged with the attribute is impossible. We represent input question (question generated by our QG model) and the passage in a single packed sequence of tokens, while using a special token [SEP] to separate the question from the passage. Similar to (Devlin et al., 2018) we use a special classification token [CLS] at the start of every sequence. Let us denote the final hidden representation of the [CLS] token by C and the final hidden representation for the ith input token by Ti . For each unanswer"
D19-3030,D16-1264,0,0.0614964,"aph word “1909”. Similarly, the question word “disease” is generated by attending over the word “disease” in the paragraph. tion answer pairs he/she may edit it. The system stores all version of questions and answers. Users can download the final generated set of questions and answers in JSON or text format at the end. 5 Implementation Details ParaQG1 comprises the frontend user interface, the backend question generator and a BERT-based question filtering module. The question generator model is implemented using the PyTorch2 framework. We trained the question generator model on the SQuAD 1.0 (Rajpurkar et al., 2016) dataset. We use pre-trained GloVe word vectors of 300 dimension and fix them during training. We employ a 2-layer Bi-LSTM encoder and a single-layer BiLSTM decoder of hidden size 600. For optimization we use SGD with annealing. We set the initial learning rate to 0.1. We train the model for 20 epochs with batch size 64. Dropout of 0.3 was applied between vertical Bi-LSTM stacks. Our question generator module provide a REST API to which we can send requests and receive responses from in JSON format. The embedded Javascript is used as the template rendering engine to render the front-end of the"
D19-3030,W14-4012,0,0.0873191,"Missing"
D19-3030,N18-2090,0,0.105028,"in a wide variety of areas such as FAQ generation, intelligent tutoring systems, automating reading comprehension, and virtual assistants/chatbots. For a QG system, the task is to generate syntactically coherent, semantically correct and natural questions from text. Additionally, it is highly desirable that the questions are relevant to the text and are pivoted on answers present in the text. Distinct from other natural lanNeural network-based sequence-to-sequence (Seq2Seq) models represent the state-of-the-art in question generation. Most of these models (Du et al., 2017; Kumar et al., 2018; Song et al., 2018; Kumar et al., 2019c,b) take single sentence as input, thus limiting their usefulness in real-world settings. Some recent techniques tackle the problem of question generation from paragraphs (Zhao et al., 2018). However, none of the above works is publicly available as an online service. In this work we present ParaQG, an interactive Web-based question generation system to generate correct, meaningful and relevant questions 175 Proceedings of the 2019 EMNLP and the 9th IJCNLP (System Demonstrations), pages 175–180 c Hong Kong, China, November 3 – 7, 2019. 2019 Association for Computational Li"
D19-3030,P17-1123,0,0.211123,"y. A QG system has many applications in a wide variety of areas such as FAQ generation, intelligent tutoring systems, automating reading comprehension, and virtual assistants/chatbots. For a QG system, the task is to generate syntactically coherent, semantically correct and natural questions from text. Additionally, it is highly desirable that the questions are relevant to the text and are pivoted on answers present in the text. Distinct from other natural lanNeural network-based sequence-to-sequence (Seq2Seq) models represent the state-of-the-art in question generation. Most of these models (Du et al., 2017; Kumar et al., 2018; Song et al., 2018; Kumar et al., 2019c,b) take single sentence as input, thus limiting their usefulness in real-world settings. Some recent techniques tackle the problem of question generation from paragraphs (Zhao et al., 2018). However, none of the above works is publicly available as an online service. In this work we present ParaQG, an interactive Web-based question generation system to generate correct, meaningful and relevant questions 175 Proceedings of the 2019 EMNLP and the 9th IJCNLP (System Demonstrations), pages 175–180 c Hong Kong, China, November 3 – 7, 2019"
D19-3030,P16-1154,0,0.0399573,"rocedure: (1) text review, (2) pivotal answer selection (3) automatic question generation pertaining to the selected answer, and (4) filtering and grouping questions based on confidence scores and different facets of the selected answer. from sentences, and paragraphs. Given a passage of text as input, users can manually select a set of answer spans to ask questions about (i.e. choose answers) from an automatically curated set of noun phrases and named entities. Questions are then generated by a combination of a (novel) sequence-to-sequence model with dynamic dictionaries, the copy mechanism (Gu et al., 2016) and the global sparse-max attention (Martins and Astudillo, 2016). ParaQG incorporates the following main features. 1. An interactive, user-configurable Web application to automatically generate questions from a sentence, or a paragraph based on user selected answers, with visual insights on the generated questions. 2. A technique to create faceted views of the generated questions having overlapping or similar answers. Given an input passage, the same answer may appear multiple times in different spans, from which similar questions can be generated. ParaQG detects and presents the generated q"
D19-3030,P99-1042,0,0.371159,"wers selected by users. The rest of the paper is organized as follows. We discuss related work in Section 2. In Section 3, We describe the architecture of ParaQG. This is followed by details of the demonstration in Section 4 and the implementation in Section 5. Conclusion is discussed in Section 6. 2 Related Work Automatically generating questions and answers from text is a challenging task. This task can be traced back to 1976 when Wolfe (1976) presented their system AUTOQUEST, which examined the generation of Wh-questions from single sentences. This was followed by several pattern matching (Hirschman et al., 1999) and linear regression (Ng et al., 2000) based models. These approaches are heavily dependent on either rules 3 System Architecture ParaQG generates questions from sentences and paragraphs following a four-stage interactive procedure: (a) paragraph review, (b) answer selection, (c) question generation with associated confi176 3.3 dence score, and (d) question filtering and grouping based on answer facets. Given a paragraph, ParaQG first reviews the content automatically and then flags any unprocessable characters (e.g. Unicode characters) and URLs, which the user are prompted to edit or remove"
D19-3030,D18-1424,0,0.0531884,"t, semantically correct and natural questions from text. Additionally, it is highly desirable that the questions are relevant to the text and are pivoted on answers present in the text. Distinct from other natural lanNeural network-based sequence-to-sequence (Seq2Seq) models represent the state-of-the-art in question generation. Most of these models (Du et al., 2017; Kumar et al., 2018; Song et al., 2018; Kumar et al., 2019c,b) take single sentence as input, thus limiting their usefulness in real-world settings. Some recent techniques tackle the problem of question generation from paragraphs (Zhao et al., 2018). However, none of the above works is publicly available as an online service. In this work we present ParaQG, an interactive Web-based question generation system to generate correct, meaningful and relevant questions 175 Proceedings of the 2019 EMNLP and the 9th IJCNLP (System Demonstrations), pages 175–180 c Hong Kong, China, November 3 – 7, 2019. 2019 Association for Computational Linguistics or question templates, and require deep linguistic knowledge, yet are not exhaustive enough. Recent successes in neural machine translation (Sutskever et al., 2014; Cho et al., 2014) have helped addres"
D19-3030,P19-1481,1,0.912557,"f areas such as FAQ generation, intelligent tutoring systems, automating reading comprehension, and virtual assistants/chatbots. For a QG system, the task is to generate syntactically coherent, semantically correct and natural questions from text. Additionally, it is highly desirable that the questions are relevant to the text and are pivoted on answers present in the text. Distinct from other natural lanNeural network-based sequence-to-sequence (Seq2Seq) models represent the state-of-the-art in question generation. Most of these models (Du et al., 2017; Kumar et al., 2018; Song et al., 2018; Kumar et al., 2019c,b) take single sentence as input, thus limiting their usefulness in real-world settings. Some recent techniques tackle the problem of question generation from paragraphs (Zhao et al., 2018). However, none of the above works is publicly available as an online service. In this work we present ParaQG, an interactive Web-based question generation system to generate correct, meaningful and relevant questions 175 Proceedings of the 2019 EMNLP and the 9th IJCNLP (System Demonstrations), pages 175–180 c Hong Kong, China, November 3 – 7, 2019. 2019 Association for Computational Linguistics or questio"
gavankar-etal-2014-efficient,P10-1014,0,\N,Missing
gavankar-etal-2014-efficient,D11-1142,0,\N,Missing
gavankar-etal-2014-efficient,castilho-etal-2012-corpus,0,\N,Missing
gavankar-etal-2014-efficient,D08-1003,0,\N,Missing
I08-2118,H92-1022,0,0.120506,"P-Hard and cannot be approximated within any reasonable factors. We then propose some heuristic algorithms and conduct exhaustive experiments to evaluate their performance. In our experiments we also observe performance improvement over an existing decision list learning algorithm, by merely re-ordering the rules output by it. 1 Ganesh Ramakrishnan IBM India Research Lab ganramkr@in.ibm.com Introduction Rule-based systems have been extensively used for several problems in text mining. Some problems in text mining where rule-based systems have been successfully used are part of speech tagging (Brill, 1992), named entity annotation (Grishman, 1997; Appelt et al., 1995), information extraction (Maynard et al., 2001), question answering (Riloff and Thelen, 2000) and classification (Han et al., 2003; Li and Yamanishi, 1999; Sasaki and Kita, 1998). Several studies have been conducted that compare the performance of rule-based systems and other machine learning techniques with mixed results. While there is no clear winner between the two approaches in terms of performance, the rule-based approach is clearly preferred in operational settings (Borthwick, 1999; Varadarajan et al., 2002). Rule-based syst"
I08-2118,W00-0603,0,0.0381317,"evaluate their performance. In our experiments we also observe performance improvement over an existing decision list learning algorithm, by merely re-ordering the rules output by it. 1 Ganesh Ramakrishnan IBM India Research Lab ganramkr@in.ibm.com Introduction Rule-based systems have been extensively used for several problems in text mining. Some problems in text mining where rule-based systems have been successfully used are part of speech tagging (Brill, 1992), named entity annotation (Grishman, 1997; Appelt et al., 1995), information extraction (Maynard et al., 2001), question answering (Riloff and Thelen, 2000) and classification (Han et al., 2003; Li and Yamanishi, 1999; Sasaki and Kita, 1998). Several studies have been conducted that compare the performance of rule-based systems and other machine learning techniques with mixed results. While there is no clear winner between the two approaches in terms of performance, the rule-based approach is clearly preferred in operational settings (Borthwick, 1999; Varadarajan et al., 2002). Rule-based systems are human comprehensible and can be improved over time. Therefore, it is imperative to develop methods that assist in building rule-based systems. In ge"
I13-1131,R09-1020,0,0.0235619,"inting to a category same as the query category are picked Introduction The ruling paradigm for Information retrieval (IR) (Manning et al., 2009) is Pseudo Relevance feedback (PRF). In PRF, an assumption is made that the top retrieved documents are relevant to the query for picking expansion terms. Zhai and Lafferty (2001) show that using pseudo relevance feedback on monolingual retrieval improves the 982 International Joint Conference on Natural Language Processing, pages 982–986, Nagoya, Japan, 14-18 October 2013. The more the relevance of D, the less is DKL (θQ |θD ). as expansion terms in Ganesh and Verma (2009). This work exploits the structure only in the form of anchor texts and category information. Techniques to disambiguate query terms based on disambiguation pages of Wikipedia are proposed in (Xu et al., 2009; Lin et al., 2010). Once disambiguated, the page is considered for picking expansion terms. Other literatures that deal with PRF based IR are (Milne. et al., 2007; Lin and Wu, 2008; Lv and Zhai, 2010; Jiang, 2011). 3 3.2 This model picks expansion terms that get combined with the query. Choosing expansion terms involves selecting a set of relevant documents and identifying terms that uniq"
K19-1076,N18-1150,0,0.0263957,"Missing"
K19-1076,P17-1123,0,0.297174,"2,3 , Ganesh Ramakrishnan2 , and Yuan-Fang Li3 1 IITB-Monash Research Academy, Mumbai, India 2 IIT Bombay, Mumbai, India 3 Monash University, Melbourne, Australia Abstract et al., 2010). More recently, neural network based techniques such as sequence-to-sequence (Seq2Seq) learning have achieved remarkable success on various NLP tasks, including question generation. A recent deep learning approach to question generation (Serban et al., 2016) investigates a simpler task of generating questions only from a triplet of subject, relation and object. Learning to ask (referred to as L2A hereinafter) (Du et al., 2017) proposes a Seq2Seq model with attention for question generation from text. (Song et al., 2018) (in an approach referred to as NQGLC hereafter) encoded groundtruth answers and employed bi-directional LSTMs in a Seq2Seq setting. In addition, they use the copy mechanism (See et al., 2017) and context matching to capture interactions between the given ground-truth answer and its context within the passage. In the context of QG from paragraphs, (Zhao et al., 2018) proposed maxout pointer network to keep track of word coverage. Our previous work (Kumar et al., 2018) (referred to as AutoQG hereinaft"
K19-1076,C18-1150,0,0.0196342,"ion 2.2.1 DAS will give high reward even though the generated question has low BLEU score. Thus, the performance of the Analyzing Choice of Reward Function BLEU(Papineni et al., 2002) measures precision and ROUGE(Lin, 2004) measures recall, we believe that cross-entropy loss was already account819 copy from the input into the question, leaving remaining keywords for the answer. This also requires the development of a specific probabilistic generative model. (Yao et al., 2018) proposed generative adversarial network (GAN) framework with modified discriminator to predict question type. Recently Fan et al (2018) proposed a bidiscriminator framework for visual question generation. They formulate the task of visual question generation as a language generation task with some linguistic and content specific attributes. model on automatic evaluation metrics does not improve with DAS as the reward function, though the quality of questions certainly improves. Further, ROUGE in conjunction with the cross entropy loss improves on recall as well as precision whereas every other combination overly focuses only on precision. Error analysis of our best model reveals that most errors can be attributed to intra-sen"
K19-1076,P17-1099,0,0.287711,"achieved remarkable success on various NLP tasks, including question generation. A recent deep learning approach to question generation (Serban et al., 2016) investigates a simpler task of generating questions only from a triplet of subject, relation and object. Learning to ask (referred to as L2A hereinafter) (Du et al., 2017) proposes a Seq2Seq model with attention for question generation from text. (Song et al., 2018) (in an approach referred to as NQGLC hereafter) encoded groundtruth answers and employed bi-directional LSTMs in a Seq2Seq setting. In addition, they use the copy mechanism (See et al., 2017) and context matching to capture interactions between the given ground-truth answer and its context within the passage. In the context of QG from paragraphs, (Zhao et al., 2018) proposed maxout pointer network to keep track of word coverage. Our previous work (Kumar et al., 2018) (referred to as AutoQG hereinafter) generates candidate answers from text using Pointer Networks (Vinyals et al., 2015) and encodes the answer in the question decoder for improved performance. We first present a framework in which a generator mechanism (the horse) that is employed for generating a question-answer pair"
K19-1076,P16-1154,0,0.0665114,"bility which determines whether to copy a word from source text or sample it from vocabulary distribution. (policy) that maximizes the expected return: LossRL (θ) = −EPθ (Y0:T |X) T X rt (Yt ; X, Y0:t−1 ) t=0 eti = v t tanh(Weh hi + Wsh st + batt ) (1) where X is the current input and Y0:t−1 is the predicted sequence until time t − 1. This supervised learning framework allows us to directly optimize task-specific evaluation metrics (rt ) such as BLEU. The generator is a sequence-to-sequence model, augmented with (i) an encoding for the potentially best pivotal answer, (ii) the copy mechanism (Gu et al., 2016) to help generate contextually important words, and (iii) the coverage mechanism (Tu et al., 2016) to discourage word repetitions. The evaluator provides rewards to fine-tune the generator. The reward function can be chosen to be a combination of one or more metrics. The high-level architecture of our question generation framework is presented in Figure 1. Here v t , Weh , Wsh and batt are model parameters to be learned, and hi is the concatenation of forward and backward hidden states of the encoder. We use this attention ati to generate sum of enthe context vector c∗t as a weighted P t ∗ cod"
K19-1076,N18-2090,0,0.210044,"2 IIT Bombay, Mumbai, India 3 Monash University, Melbourne, Australia Abstract et al., 2010). More recently, neural network based techniques such as sequence-to-sequence (Seq2Seq) learning have achieved remarkable success on various NLP tasks, including question generation. A recent deep learning approach to question generation (Serban et al., 2016) investigates a simpler task of generating questions only from a triplet of subject, relation and object. Learning to ask (referred to as L2A hereinafter) (Du et al., 2017) proposes a Seq2Seq model with attention for question generation from text. (Song et al., 2018) (in an approach referred to as NQGLC hereafter) encoded groundtruth answers and employed bi-directional LSTMs in a Seq2Seq setting. In addition, they use the copy mechanism (See et al., 2017) and context matching to capture interactions between the given ground-truth answer and its context within the passage. In the context of QG from paragraphs, (Zhao et al., 2018) proposed maxout pointer network to keep track of word coverage. Our previous work (Kumar et al., 2018) (referred to as AutoQG hereinafter) generates candidate answers from text using Pointer Networks (Vinyals et al., 2015) and enc"
K19-1076,P19-1481,1,0.839107,"we (2018) proposed to augment each word with linguistic features and encode the most relevant pivotal answer to the text while generating questions. Similarly, Song et al (2018) encode ground-truth answers (given in the training data), use the copy mechanism and additionally employ context matching to capture interactions between the answer and its context within the passage. They encode ground truth answer for generating questions which might not be available for test set in contrast we train a Pointer Network based model to predict the pivotal answer to generate question about. In our work (Kumar et al., 2019a) we proposed a transformer based architecture to automatically generate complex multihop questions from knowledge graphs. In (Kumar et al., 2019b) we proposed a cross lingual training method for automatically generating questions from text in low resource languages. Very recently deep reinforcement learning has been successfully applied to natural language generation tasks such as abstractive summarization (Paulus et al., 2018; Celikyilmaz et al., 2018) and dialogue generation (Li et al., 2016). In summarization, one generates and paraphrases sentences that capture salient points of the text"
K19-1076,P16-1008,0,0.0231904,"ibution. (policy) that maximizes the expected return: LossRL (θ) = −EPθ (Y0:T |X) T X rt (Yt ; X, Y0:t−1 ) t=0 eti = v t tanh(Weh hi + Wsh st + batt ) (1) where X is the current input and Y0:t−1 is the predicted sequence until time t − 1. This supervised learning framework allows us to directly optimize task-specific evaluation metrics (rt ) such as BLEU. The generator is a sequence-to-sequence model, augmented with (i) an encoding for the potentially best pivotal answer, (ii) the copy mechanism (Gu et al., 2016) to help generate contextually important words, and (iii) the coverage mechanism (Tu et al., 2016) to discourage word repetitions. The evaluator provides rewards to fine-tune the generator. The reward function can be chosen to be a combination of one or more metrics. The high-level architecture of our question generation framework is presented in Figure 1. Here v t , Weh , Wsh and batt are model parameters to be learned, and hi is the concatenation of forward and backward hidden states of the encoder. We use this attention ati to generate sum of enthe context vector c∗t as a weighted P t ∗ coder hidden states: ct = i ai hi . We further use the c∗t vector to obtain a probability distributio"
K19-1076,D16-1127,0,0.0357849,"Missing"
K19-1076,P02-1040,0,0.104328,"in association with the name of the city. We however note that in row 3, the word ‘new’ has been erroneously repeated twice, since an encoder-decoder based model could generate questions with meaningless repetitions. We introduce a mechanism for discouraging such repetitions in our generator by quantitatively emphasizing the coverage of sentence words while decoding. Row 4 shows the improved and relevant question generated by our model trained by incorporating both the copy and coverage mechanisms. The standard metrics for evaluating the performance of question generation models such as BLEU (Papineni et al., 2002), GLEU, and ROUGE-L (Lin, 2004) are based on degree of ngram overlaps between a generated question and the ground-truth question. It would be desirable to be able to directly optimize these taskspecific metrics. However, these n-gram based metrics do not decompose over individual words and are therefore hard to optimize. We explicitly employ an evaluator that rewards each generated question based on its conformance to one (or more than one using decomposable attention) questions in the ground-truth set using these possibly non-decomposable reward functions. We find such learning to be a natura"
K19-1076,D18-1424,0,0.0133327,"mpler task of generating questions only from a triplet of subject, relation and object. Learning to ask (referred to as L2A hereinafter) (Du et al., 2017) proposes a Seq2Seq model with attention for question generation from text. (Song et al., 2018) (in an approach referred to as NQGLC hereafter) encoded groundtruth answers and employed bi-directional LSTMs in a Seq2Seq setting. In addition, they use the copy mechanism (See et al., 2017) and context matching to capture interactions between the given ground-truth answer and its context within the passage. In the context of QG from paragraphs, (Zhao et al., 2018) proposed maxout pointer network to keep track of word coverage. Our previous work (Kumar et al., 2018) (referred to as AutoQG hereinafter) generates candidate answers from text using Pointer Networks (Vinyals et al., 2015) and encodes the answer in the question decoder for improved performance. We first present a framework in which a generator mechanism (the horse) that is employed for generating a question-answer pair invokes or pulls the evaluator mechanism (the cart) that is employed for evaluating the generated pair. Our clearly delineated generator-evaluator framework lets us (a) easily"
K19-1076,D16-1244,0,0.0964046,"Missing"
K19-1076,D16-1264,0,0.201549,"? how much did the economic loss run in sichuan ? what is the largest cities in sichuan ? Table 2: Sample text and questions generated using different reward functions, with and without our new QGspecific rewards QSS+ANSS. • Novel reward functions that ensure that the generated question is relevant to the text and conforms to the encoded answer. tions generated using combination of standard reward functions with reward functions specific to QG quality (QSS+ANSS) exhibit higher quality. Contributions We summarize our main contributions as follows: When evaluated on the benchmark SQuAD dataset (Rajpurkar et al., 2016), our system considerably outperforms state-of-the-art question generation models (Du et al., 2017; Kumar et al., 2018; Song et al., 2018) in automatic and human evaluation. • A comprehensive, end-to-end generatorevaluator framework naturally suited for automated question generation. Whereas earlier approaches employ some mechanism (the horse) for generating the question, intertwined with an evaluation mechanism (the cart), we show that these approaches can benefit from a much clearer separation of the generator of the question from its evaluator. 2 Our Approach Our framework for question gene"
N15-1090,P11-1055,0,0.570134,"t hand labeled, the facts in the database act as “weak” or “distant” labels, hence the learning scenario is termed as distantly supervised. Prior work casts this problem as a multi-instance multi-label learning problem (Hoffmann et al., 2011; Surdeanu et al., 2012). It is multi-instance since for a given entity-pair, only the label of the bag of sentences containing both entities (aka mentions) is given. It is multi-label since a bag of mentions can have multiple labels. The inter-dependencies between relation labels and (hidden) mention labels are modeled by a Markov Random Field (Figure 1) (Hoffmann et al., 2011). The learning algorithms used in the literature for this problem optimize the (conditional) likelihood, but the evaluation measure is commonly the F -score. Formally, the training data is D := {(xi , yi )}N i=1 where xi ∈ X is the entity-pair, yi ∈ Y denotes the relation labels, and hi ∈ H denotes the hidden mention labels. The possible relation labels for the entity pair are observed from a given knowledgebase. If there are L candidate relation labels in the knowledge-base, then yi ∈ {0, 1}L , (e.g. yi,` is 1 if the relation ` is licensed by the knowledge-base for the entity-pair) and hi ∈ {"
N15-1090,P09-1113,0,0.326401,"Missing"
N15-1090,Q13-1030,0,0.177481,"e analysis on the benefits of using this search strategy vis-a-vis the exhaustive search in the Experiments section. 4 Experiments Dataset: We use the challenging benchmark dataset created by Riedel et al. (2010) for distant supervision of relation extraction models. It is created by aligning relations from Freebase4 with the sentences in New York Times corpus (Sandhaus, 2008). The labels for the datapoints come from the Freebase 3 For a given (f p, f n), we set y0 by picking the sorted unary terms that maximize the score according to y. 4 www.freebase.com database but Freebase is incomplete (Ritter et al., 2013). So a data point is labeled nil when either no relation exists or the relation is absent in Freebase. To avoid this ambiguity we train and evaluate the baseline and our algorithms on a subset of this dataset which consists of only non-nil relation labeled datapoints (termed as positive dataset). For the sake of completeness, we do report the accuracies of the various approaches on the entire evaluation dataset. Systems and Baseline: Hoffmann et al. (2011) describe a state-of-the-art approach for this task. They use a perceptron-style parameter update scheme adapted to handle latent variables;"
N15-1090,D12-1042,0,0.0797486,"Obama, United States 2 2.1 Preliminaries Distant Supervision for Relation Extraction Our framework is motivated by distant supervision for learning relation extraction models (Mintz et al., 2009). The goal is to learn relation extraction models by aligning facts in a database to sentences in a large unlabeled corpus. Since the individual sentences are not hand labeled, the facts in the database act as “weak” or “distant” labels, hence the learning scenario is termed as distantly supervised. Prior work casts this problem as a multi-instance multi-label learning problem (Hoffmann et al., 2011; Surdeanu et al., 2012). It is multi-instance since for a given entity-pair, only the label of the bag of sentences containing both entities (aka mentions) is given. It is multi-label since a bag of mentions can have multiple labels. The inter-dependencies between relation labels and (hidden) mention labels are modeled by a Markov Random Field (Figure 1) (Hoffmann et al., 2011). The learning algorithms used in the literature for this problem optimize the (conditional) likelihood, but the evaluation measure is commonly the F -score. Formally, the training data is D := {(xi , yi )}N i=1 where xi ∈ X is the entity-pair"
N15-1090,P13-2117,0,0.435481,"Missing"
N15-1090,D14-1166,0,\N,Missing
N18-5010,W15-1701,0,0.0307538,"ies. (Wang et al., 2012) propose a hybrid human-machine approach to determine the most relevant, matching entity pairs. Instead of directly asking users to resolve entities, the system adopts a two-step approach; the system estimates the most likely mentions for an entity. This information is presented to the users to verify matching pairs of entities. (Inkpen et al., 2017) proposed a set of heuristic rules to disambiguate location names in Twitter5 messages. Their heuristics rely on geographical (latitude-longitude, geographic hierarchy) and demographic information (population of a region). (Awamura et al., 2015) used spatial proximity and temporal consistency clues to disambiguation location names. Our approach jointly resolves entity and disambiguate location names using publicly available web data. 3 Our Approach We propose a novel technique to address the problem of entity resolution and location disambiguation. To extract the basic location and video data related to each temple, we use the Google Maps6 and YouTube API7 respectively. We disambiguate the name and location of each temple using publicly available data on the Web and leverage Google Maps to assign videos to the correct temple. The tem"
N18-5010,N03-1033,0,0.0160052,"m 48 q and snippet s is computed as: sim(q,s)=qT W s (1) For our CNN model, we use the short text ranking system proposed by Severyn (Severyn and Moschitti, 2015). The convolution filter width is set to 5, the feature map size to 150, and the batch size to 50. We set the dropout parameter to 0.5. We initialized word vectors using pre-trained word embeddings. Before passing our input to the CNN, we pre-process the text and exclude plural nouns, cardinal numbers and foreign words from the snippets. Pre-processing helps us handle out of vocabulary words. We use the Stanford Part of Speech (POS) (Toutanova et al., 2003) tagger to annotate each word with its POS tag. As an example, consider the following input snippet: Temple of Lord Somnath one of Jyotirlinga temple of Lord Shiva is situated near the town of Veraval in Western part of Gujarat whose present structure is built in 1951. The PoS tagger annotates words like Lord , Somnath, Shiva, Veraval, Gujarat as proper nouns. We provide the query-temple pair as an input to the CNN which outputs the associated similarity score. The highest score represents the most relevant snippet for the temple. and recall numbers, despite the association of a single temple"
P15-1054,E06-1002,0,0.0509993,"ore than 1M categories (topics) arranged hierarchically. In fact, they form a cyclic graph (Zesch and Gurevych, 2007). However, we can convert it to a DAG by eliminating the cycles as described in the supplementary material. YAGO (Suchanek et al., 2007) and Freebase (Bollacker et al., 2008) are other instances of massive topic hierarchies. The association of the documents with the existing topic hierarchy is also well studied. Systems such as WikipediaMiner (Milne, 2009), TAGME (Ferragina and Scaiella, 2010) and several annotation systems such as (Dill et al., 2003; Mihalcea and Csomai, 2007; Bunescu and Pasca, 2006) attach topics from Wikipedia (and other catalogs) to the documents by establishing the hard or soft links mentioned above. Our goal is the following: Given a (ground set) collection V of topics organized in a pre-existing hierarchical DAG structure, and a collection D of documents, chose a size K ∈ Z+ representative subset of topics. Our approach is distinct from earlier work (e.g., (Kanungo et al., 2002; Blei et al., 2003)) where typically only a set of documents is classified and categorized in some way. We next provide a few definitions needed later in the paper. Definition 1: Transitive C"
P15-1054,D14-1014,1,0.849433,"t of features U , define mu (S) as the score associated with the set of categories S for feature u ∈ U . The feature set could represent, for example, the documents, in which case mu (S) represents the number of times document u is covered by the set S. U could also represent more complicated features. For example, in the context of Wikipedia disambiguation, U could represent TFIDF features over the documents. P Feature based are then defined as f (S) = u∈U ψ(mu (S)), where ψ is a concave (e.g., the square root) function. This function class has been successfully used in several applications (Kirchhoff and Bilmes, 2014; Wei et al., 2014a; Wei et al., 2014b). 557 3.1.2 Similarity based Functions Similarity functions are defined through a similarity matrix S = {sij }i,j∈V . Given categories i, j ∈ V , similarity sij in our case can be defined as sij = |Γ(i)∩Γ(j)|, i.e the number of documents commonly covered by both i and j. Facility Location: TheP facility location function, defined as f (S) = i∈V maxj∈S sij , is a natural model for k-medoids and exemplar based clustering, and has been used in several summarization problems (Tschiatschek et al., 2014; Wei et al., 2014a). Penalty based diversity: A similarity"
P15-1054,N10-1134,1,0.798191,"⊆ V  {v}, where V represents the ground set of elements, f (A ∪ {v}) − f (A) ≥ f (B ∪ {v}) − f (B). This is called the diminishing returns property and states, informally, that adding an element to a smaller set increases the function value more than adding that element to a larger set. Submodular functions naturally model notions of coverage and diversity in applications, and therefore, a number of machine learning problems can be modeled as forms of submodular optimization (Kempe et al., 2003; Krause and Guestrin, 2005; Narasimhan and Bilmes, 2004; Iyer et al., 2013; Lin and Bilmes, 2012; Lin and Bilmes, 2010). In this paper, we investigate structured prediction methods for learn554 ing weighted mixtures of submodular functions to summarize topics for a collection of objects using DAG-structured topic hierarchies. Throughout this paper we use the terms “topic” and “category” interchangeably. 1.1 Related Work To the best of our knowledge, the specific problem we consider here is new. Previous work on identifying topics can be broadly categorized into one of the following types: a) cluster the objects and then identify names for the clusters; or b) dynamically identify topics (including hierarchical)"
P15-1054,P11-1052,1,0.773739,"ilarity sij in our case can be defined as sij = |Γ(i)∩Γ(j)|, i.e the number of documents commonly covered by both i and j. Facility Location: TheP facility location function, defined as f (S) = i∈V maxj∈S sij , is a natural model for k-medoids and exemplar based clustering, and has been used in several summarization problems (Tschiatschek et al., 2014; Wei et al., 2014a). Penalty based diversity: A similarity matrix may be used to express a form of coverage of a set S but that is then penalized with a redundancyPterm, as in the P following P difference: f (S) = s − λ i∈V,j∈S ij i∈S j∈S, si,j (Lin and Bilmes, 2011)). Here λ ∈ [0, 1]. This function is submodular, but is not in general monotone, and has been used in document summarization (Lin and Bilmes, 2011), as a dispersion function (Borodin et al., 2012), and in image summarization (Tschiatschek et al., 2014). 3.1.3 QC Functions As Barrier Modular Mixtures: We introduce a modular function for every QC function as follows  α fspecificity (s) = JΓ(t)&gt;0K if the height of topic s is at least α otherwise  1 0 if the clarity of topic s is at least β otherwise for every possible value of α. This creates a submodular mixture with as many components as the"
P15-1054,W07-0201,0,0.030466,"tions that we introduce in the next section ensure these properties are satisfied. Formally, we solve the following discrete optimization problem: X S ∗ ∈ argmax wi fi (S) (1) S⊆V :|S|≤K i where, fi are monotone submodular mixture components and wi ≥ 0 are the weights associated with those mixture components. Set S ∗ is the summary topics scored best. It is easy to find massive (i.e., size in the order of million) DAG structured topic hierarchies in practice. Wikipedia’s category hierarchy consists of more than 1M categories (topics) arranged hierarchically. In fact, they form a cyclic graph (Zesch and Gurevych, 2007). However, we can convert it to a DAG by eliminating the cycles as described in the supplementary material. YAGO (Suchanek et al., 2007) and Freebase (Bollacker et al., 2008) are other instances of massive topic hierarchies. The association of the documents with the existing topic hierarchy is also well studied. Systems such as WikipediaMiner (Milne, 2009), TAGME (Ferragina and Scaiella, 2010) and several annotation systems such as (Dill et al., 2003; Mihalcea and Csomai, 2007; Bunescu and Pasca, 2006) attach topics from Wikipedia (and other catalogs) to the documents by establishing the hard"
P19-1481,D18-1549,0,0.0887451,"i et al., 2017) model that has shown great success compared to recurrent neural network-based models in neural machine translation. Encoders and decoders consist of a stack of four identical layers, of which two layers are independently trained and two are trained in a shared manner. Each layer of the transformer consists of a multi-headed selfattention model followed by a position-wise fully connected feed-forward network. 3.1 Unsupervised Pretraining We use monolingual corpora available in the primary (Hindi/Chinese) and secondary (English) languages for unsupervised pretraining. Similar to Artetxe et al. (2018), we use denoising autoencoders along with back-translation (described in Section 3.1.1) for pretraining the language models in both the primary and secondary languages. Specifically, we first train the model to reconstruct their inputs, which will expose the model to the grammar and vocabulary specific to each language while enforcing a shared latent-space with the help Back translation In addition to denoising autoencoders, we utilize back-translation (Sennrich et al., 2016a). This further aids in enforcing the shared latent space assumption by generating a pseudo-parallel corpus (Imankulova"
P19-1481,P17-1123,0,0.370709,"d rules. Manually curating a large number of rules for a new language is a tedious and challenging task. More recently, Zheng et al. (2018) propose a template-based technique to construct questions from Chinese text, where they rank generated questions using a neural model and select the topranked question as the final output. Neural Network Based: Neural network based approaches do not rely on hand-crafted rules, but instead use an encoder-decoder architecture which can be trained in an end-to-end fashion to automatically generate questions from text. Several neural network based approaches (Du et al., 2017; Kumar et al., 2018a,b) have been proposed for automatic question generation from text. Du et al. (2017) propose a sequence to sequence model for automatic question generation from English text. Kumar et al. (2018a) use a rich set of linguistic features and encode pivotal answers predicted using a pointer network based model to automatically generate a question for the encoded Denoising Autoencoding WEpri, WDpri WEpri WDpri WEshared WDshared WEsec WEsec, WDsec Back Translation WEpri, WDsec WEsec, WDpri Supervised Training WDsec WEpri, WDpri WEsec, WDsec All Training Phases WEshared, WDshared"
P19-1481,L18-1548,0,0.0248095,"esults are reported on the test set. 4.1.2 Other Datasets 4.2 We briefly describe all the remaining datasets used in our experiments. (The relevant primary or secondary language is mentioned in parenthesis, alongside the name of the datasets.) IITB Hindi Monolingual Corpus (Primary language: Hindi) We extracted 93,000 sentences from the IITB Hindi monolingual corpus4 , where each sentence has between 4 and 25 tokens. These sentences were used for unsupervised pretraining. IITB Parallel Corpus (Primary language: Hindi) We selected 100,000 English-Hindi sentence pairs from IITB parallel corpus (Kunchukuttan et al., 2018) where the number of tokens in the sentence was greater than 10 for both languages. We used this dataset to further fine-tune the weights of the encoder and decoder layers after unsupervised pretraining. DuReader (He et al., 2018) Chinese Dataset: (Primary language: Chinese) This dataset consists of question-answer pairs along with the question type. We preprocessed and used “DESCRIPTION” type questions for our experiments, resulting in a total of 8000 instances. From this subset, we created a 6000/1000/1000 split to construct train, development and test sets for our experiments. We also prepr"
P19-1481,P02-1040,0,0.103759,"allel BLEU-1 28.414 41.059 41.034 42.281 25.52 30.38 30.69 30.30 BLEU-2 18.493 29.294 29.792 32.074 9.22 14.01 14.51 13.93 BLEU-3 12.356 21.403 22.038 25.182 5.14 8.37 8.82 8.43 BLEU-4 8.644 16.047 16.598 20.242 3.25 5.18 5.39 5.51 METEOR 23.803 28.159 27.581 29.143 7.64 10.46 10.44 10.26 ROUGE-L 29.893 39.395 39.852 40.643 27.40 32.71 31.82 31.58 Table 2: BLEU, METEOR and ROUGE-L scores on the test set for Hindi and Chinese question generation. Best results for each metric (column) are highlighted in bold. 4.3 Evaluation Methods We evaluate our systems and report results on widely used BLEU (Papineni et al., 2002), ROUGE-L and METEOR metrics. We also performed a human evaluation study to evaluate the quality of the questions generated. Following Kumar et al. (2018a), we measure the quality of questions in terms of syntactic correctness, semantic correctness and relevance. Syntactic correctness measures the grammatical correctness of a generated question, semantic correctness measures naturalness of the question, and relevance measures how relevant the question is to the text and answerability of the question from the sentence. 5 Results We present our automatic evaluation results in Table 2, where the"
P19-1481,D16-1264,0,0.532136,"first describe all the datasets we used in our experiments, starting with a detailed description of our new Hindi question answering dataset, “HiQuAD”. We will then describe various implementation-specific details relevant to training our models. We conclude this section with a description of our evaluation methods. 4.1 4.1.1 Datasets HiQuAD HiQuAD (Hindi Question Answering dataset) is a new question answering dataset in Hindi that we developed for this work. This dataset contains 6555 question-answer pairs from 1334 paragraphs in a series of books called Dharampal Books. 3 Similar to SQuAD (Rajpurkar et al., 2016), an English question answering dataset that we describe further in Section 4.1.2, HiQuAD also consists of a paragraph, a list of questions answerable from the paragraph and answers to those questions. To construct sentence-question pairs, for a given question, we identified the first word of the answer in the paragraph and extracted the corresponding sentence to be paired along with the question. We curated a total of 6555 sentencequestion pairs. We tokenize the sentence-question pairs to remove any extra white spaces. For our experiments, we randomly split the HiQuAD dataset into train, 2 Us"
P19-1481,N19-1380,0,0.0300498,") propose a joint model to address QG and the question answering problem together. All prior work on QG assumed access to a sufficiently large number of training instances for a language. We relax this assumption in our work as we only have access to a small question answering dataset in the primary language. We show how we can improve QG performance on the primary language by leveraging a larger question answering dataset in a secondary language. (Similarly in spirit, cross-lingual transfer learning based approaches have been recently proposed for other NLP tasks such as machine translation (Schuster et al., 2019; Lample and Conneau, 2019).) 3 Our Approach We propose a shared encoder-decoder architecture that is trained in two phases. The first, is an unsupervised pretraining phase, consisting of denoising autoencoding and back-translation. This pretraining phase only requires sentences in both the primary and secondary languages. This is followed by a supervised question generation training phase that uses sentence-question pairs in both languages to fine-tune the pretrained weights. 4864 1 of the shared encoder and decoder layers. To prevent the model from simply learning to copy every word, we rand"
P19-1481,P16-1009,0,0.159255,"ora available in the primary (Hindi/Chinese) and secondary (English) languages for unsupervised pretraining. Similar to Artetxe et al. (2018), we use denoising autoencoders along with back-translation (described in Section 3.1.1) for pretraining the language models in both the primary and secondary languages. Specifically, we first train the model to reconstruct their inputs, which will expose the model to the grammar and vocabulary specific to each language while enforcing a shared latent-space with the help Back translation In addition to denoising autoencoders, we utilize back-translation (Sennrich et al., 2016a). This further aids in enforcing the shared latent space assumption by generating a pseudo-parallel corpus (Imankulova et al., 2017).1 Back translation has been demonstrated to be very important for unsupervised NMT (Yang et al., 2018; Lample et al., 2018). Given a sentence in the secondary language xs , we generate a translated sentence in the primary language, x˜p . We then use the translated sentence x˜p to generate the original xs back, while updating the weights WEsec , WEshared , WDshared and WDpri as shown in Figure 2. Note that we utilize denoising autoencoding and back-translation f"
P19-1481,P16-1162,0,0.0840284,"ora available in the primary (Hindi/Chinese) and secondary (English) languages for unsupervised pretraining. Similar to Artetxe et al. (2018), we use denoising autoencoders along with back-translation (described in Section 3.1.1) for pretraining the language models in both the primary and secondary languages. Specifically, we first train the model to reconstruct their inputs, which will expose the model to the grammar and vocabulary specific to each language while enforcing a shared latent-space with the help Back translation In addition to denoising autoencoders, we utilize back-translation (Sennrich et al., 2016a). This further aids in enforcing the shared latent space assumption by generating a pseudo-parallel corpus (Imankulova et al., 2017).1 Back translation has been demonstrated to be very important for unsupervised NMT (Yang et al., 2018; Lample et al., 2018). Given a sentence in the secondary language xs , we generate a translated sentence in the primary language, x˜p . We then use the translated sentence x˜p to generate the original xs back, while updating the weights WEsec , WEshared , WDshared and WDpri as shown in Figure 2. Note that we utilize denoising autoencoding and back-translation f"
P19-1481,W18-2605,0,0.0328584,"s.) IITB Hindi Monolingual Corpus (Primary language: Hindi) We extracted 93,000 sentences from the IITB Hindi monolingual corpus4 , where each sentence has between 4 and 25 tokens. These sentences were used for unsupervised pretraining. IITB Parallel Corpus (Primary language: Hindi) We selected 100,000 English-Hindi sentence pairs from IITB parallel corpus (Kunchukuttan et al., 2018) where the number of tokens in the sentence was greater than 10 for both languages. We used this dataset to further fine-tune the weights of the encoder and decoder layers after unsupervised pretraining. DuReader (He et al., 2018) Chinese Dataset: (Primary language: Chinese) This dataset consists of question-answer pairs along with the question type. We preprocessed and used “DESCRIPTION” type questions for our experiments, resulting in a total of 8000 instances. From this subset, we created a 6000/1000/1000 split to construct train, development and test sets for our experiments. We also preprocessed and randomly extracted 100,000 descriptions to be used as a Chinese monolingual corpus for the unsupervised pretraining stage. News Commentary Dataset: (Primary language: Chinese) This is a parallel corpus of 4 http://www."
P19-1481,D17-1090,0,0.0264375,"nding decoder layers. WEshared and WDshared refer to weights of the encoder and decoder layers shared across both languages, respectively. Weights updated in each training phase are explicitly listed. answer. All existing models optimize a crossentropy based loss function, that suffers from exposure bias (Ranzato et al., 2016). Further, existing methods do not directly address the problem of handling important rare words and word repetition in QG. Kumar et al. (2018b) propose a reinforcement learning based framework which addresses the problem of exposure bias, word repetition and rare words. Tang et al. (2017) and Wang et al. (2017) propose a joint model to address QG and the question answering problem together. All prior work on QG assumed access to a sufficiently large number of training instances for a language. We relax this assumption in our work as we only have access to a small question answering dataset in the primary language. We show how we can improve QG performance on the primary language by leveraging a larger question answering dataset in a secondary language. (Similarly in spirit, cross-lingual transfer learning based approaches have been recently proposed for other NLP tasks such as"
P19-1481,W17-5704,0,0.0142899,"al. (2018), we use denoising autoencoders along with back-translation (described in Section 3.1.1) for pretraining the language models in both the primary and secondary languages. Specifically, we first train the model to reconstruct their inputs, which will expose the model to the grammar and vocabulary specific to each language while enforcing a shared latent-space with the help Back translation In addition to denoising autoencoders, we utilize back-translation (Sennrich et al., 2016a). This further aids in enforcing the shared latent space assumption by generating a pseudo-parallel corpus (Imankulova et al., 2017).1 Back translation has been demonstrated to be very important for unsupervised NMT (Yang et al., 2018; Lample et al., 2018). Given a sentence in the secondary language xs , we generate a translated sentence in the primary language, x˜p . We then use the translated sentence x˜p to generate the original xs back, while updating the weights WEsec , WEshared , WDshared and WDpri as shown in Figure 2. Note that we utilize denoising autoencoding and back-translation for both languages in each step of training. 3.2 Supervised Question Generation We formulate the QG problem as a sequence to sequence m"
P19-1481,P18-1005,0,0.275158,"along with the questions predicted by our best model. We also experimented with Chinese as a primary language. This choice was informed by our desire to use a language that was very different from Hindi. We use the same secondary language – English – with both choices of our primary language. Drawing inspiration from recent work on unsupervised neural machine translation (Artetxe et al., 4863 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4863–4872 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2018; Yang et al., 2018), we propose a crosslingual model to leverage resources available in a secondary language while learning to automatically generate questions from a primary language. We first train models for alignment between the primary and secondary languages in an unsupervised manner using monolingual text in both languages. We then use the relatively larger QG dataset in a secondary language to improve QG on the primary language. Our main contributions can be summarized as follows: • We present a cross-lingual model that effectively exploits resources in a secondary language to improve QG for a primary la"
S07-1099,W04-0807,0,\N,Missing
S13-2037,P11-1052,0,0.10666,"Missing"
S13-2037,J13-3008,0,0.0469736,"Missing"
S13-2037,S13-2035,0,0.0305766,"Missing"
W03-1201,W96-0213,0,\N,Missing
W12-5020,W10-3604,1,\N,Missing
W12-5807,P09-1113,0,0.0152756,"rocessing Tools in Education, pages 51–60, COLING 2012, Mumbai, December 2012. 51 1 Introduction Ontologies and knowledge bases play an important role in semantic web. This has led to an independent and distributed effort of developing several domain ontologies and public knowledge bases. In this context, ontology mapping enables interlinking of different ontologies and their population by exploiting similarities between predicates. Prior research discusses several approaches to ontology population ranging from automated to semi-supervised. Bootstrapping approach (Agichtein and Gravano, 2000; Mintz et al., 2009) to ontology population often makes use of a small number of seed examples for each predicate in an ontology. Generating these seeds could benefit from availability of a mapping between a domain ontology and a knowledge base like DBpedia. Using an academic ontology as our target, we study this approach to ontology population and propose a query based formulation to map nodes from the academic ontology to those of the DBpedia ontology. For e.g., Journal node from the academic ontology could be mapped to the Academic Journal concept in DBpedia. Similarly Software, Person, Programming Language et"
W15-5934,E06-1002,0,0.0680407,". In a typical system architecture (Cucerzan, 2007; Dill et al., 2003; Kulkarni et al., 2009; Milne and Witten, 2008) a spotter first identifies short token segments or “spots” as potential mentions of entities from its catalog. For our purposes, a catalog consists of a directed graph of categories, to which entity nodes are attached. Many entities may qualify for a given text segment, e.g., both Kernel trick and Linux Kernel might qualify for the text segment “...Kernel...”. In the second stage, a disambiguator assigns zero or more entities 219 to Prior Work Earlier works (Dill et al., 2003; Bunescu and Pasca, 2006; Mihalcea and Csomai, 2007) on entity annotation focused on per-mention disambiguation. This involves selecting the best entity to assign to a mention, independent of the assignments to other mentions in the document. Wikify! (Mihalcea and Csomai, 2007) for instance, uses context overlap for disambiguation and combines it with a classifier model that exploits local and topical features. Cucerzan (Cucerzan, 2007) introduced the notion of agreement on categories of entities in addition to the local context overlap, in which the entity context comprised 1 http://web-ngram.research.microsoft. com"
W15-5934,N06-1016,0,0.0398337,".57 .68 .80 .62 .70 .84 .54 .66 .69 .52 .59 .69 .57 .62 .85 .54 .66 .44 .42 .43 .85 .56 .68 .79 .63 .70 Table 2: Effect of edge features: two-fold cross validation on W ikicur . Edge features that showed improvement over node features are shown in bold. 5.4.3 Does training help? We sampled 50 documents from the W ikicur dataset, 5 at a time and used them for training, applying both passive (PL) and active learning (AL). The F1 measure evaluated on an independent test set of 30 documents is shown in the plot (Refer to figure 4). The F1 on training set seems to fluctuate, more so for Train-AL (Chen et al., 2006), but the F1 on test set does show a steady improvement. Figure 2: Effect of data curation Annotator AIDA Wikify! TagMe Wikipedia Miner Illionis Wikifier Our Model (Node+I) Our Model (Node+I+O+F) IIT Bpart F P R .07 .66 .04 .37 .55 .28 .44 .45 .42 .52 .57 .48 .44 .58 .36 .67 .76 .60 .65 .69 .61 Figure 3: Effect of varying C: C0 and C1 AQUAINT F P R .21 .35 .15 .34 .29 .42 .51 .46 .57 .47 .38 .63 .34 .29 .42 .78 .81 .74 .79 .82 .75 MSNBC F P R .47 .75 .35 .41 .34 .51 .52 .48 .55 .46 .55 .36 .41 .34 .51 .67 .68 .66 .66 .63 .69 Table 3: Comparison with publicly available systems (as reported by ("
W15-5934,D07-1074,0,0.788222,"atedness features. Also, unlike most of the prior work, we jointly learn the local and global feature weights. Our Markov network-based model, along with suitable assumptions, makes efficient learning possible. The model links mentions to zero or more entities, thus offering a natural solution to the problem of NAs and multiple attachments. Introduction 2 Search systems proposed today (Chakrabarti et al., 2006; Cheng et al., 2007; Kasneci et al., 2008; Li et al., 2010) are greatly enriched by recognizing and exploiting entities embedded in unstructured pages. In a typical system architecture (Cucerzan, 2007; Dill et al., 2003; Kulkarni et al., 2009; Milne and Witten, 2008) a spotter first identifies short token segments or “spots” as potential mentions of entities from its catalog. For our purposes, a catalog consists of a directed graph of categories, to which entity nodes are attached. Many entities may qualify for a given text segment, e.g., both Kernel trick and Linux Kernel might qualify for the text segment “...Kernel...”. In the second stage, a disambiguator assigns zero or more entities 219 to Prior Work Earlier works (Dill et al., 2003; Bunescu and Pasca, 2006; Mihalcea and Csomai, 2007"
W15-5934,C12-1050,0,0.0187807,"ir in-link overlap. Relatedness, in conjunction with the prior probability of occurrence of an entity, was then used to train a classifier model. Han et al. (Han and Zhao, 2009) leveraged the semantic information in Wikipedia to build a large-scale semantic network and developed a similarity measure to be used for disambiguation. Kulkarni et al. (Kulkarni et al., 2009) were the first to propose a general collective disambiguation approach, giving formulations for trade-off between mention-entity compatibility and coherence between entities. Several graphbased approaches (Hoffart et al., 2011; Fahrni and Strube, 2012) followed that cast the disambiguation problem as a problem of dense subgraph selection from a graph of mentions and candidate entities, making use of collective signals. Most of these systems seem to prefer tagging conservatively. Some of them (Cucerzan, 2007; Hoffart et al., 2011) restrict their tagging to named entities, while others use a subset of entities from a background taxonomy such as TAP (Dill et al., 2003) or Wikipedia (Milne and Witten, 2008; Mihalcea and Csomai, 2007). Others (Kataria et al., 2011; Bhattacharya and Getoor, 2006) have proposed LDA-based generative models but focu"
W15-5934,H92-1045,0,0.714099,"Missing"
W15-5934,D12-1010,0,0.0312131,"Missing"
W15-5934,D11-1072,0,0.0825544,"Missing"
W15-5934,W12-3016,0,0.0169326,"the local and global feature weights. In this work we present a collective disambiguation model, that, under suitable assumptions makes efficient implementation of exact MAP inference possible. We also present an efficient approach to train the local and global features of this model and implement it in an interactive entity linking system. The system receives human feedback on a document collection and progressively trains the underlying disambiguation model. selected mentions, based on mention-entity coherence, as well as entity-entity similarity. Some of the recent work (Zhou et al., 2010; Lin et al., 2012) shows that several mentions may have no associated sense in the catalog. This is referred to as the no-attachment (NA) problem (or NIL in the TAC-KBP challenge (McNamee, 2009)). The other, relatively lesser addressed challenge is that of multiple attachments (Kulkarni et al., 2014), where a mention might link to more than one entities from the catalog. This might often be a result of insufficient context and has been acknowledged by some of the recent entity disambiguation challenges1 . We present an approach to collective disambiguation of several mentions by combining various mention-entity"
W15-5934,P11-1138,0,0.116486,"2006) have proposed LDA-based generative models but focus only on person names. Some of the more recent systems (Kulkarni et al., 2009; Han et al., 2011) do perform aggressive spotting, aided by the anchor dictionary of Wikipedia entities and study the recall-precision tradeoff. (Kulkarni et al., 2014) propose a joint disambiguation model based on a Markov network of entities as nodes and edges for their relatedness. Disambiguation is achieved by performing a MAP inference on this graph and it naturally handles the NA and multiple attachment cases. Unlike other approaches (Han and Sun, 2012; Ratinov et al., 2011), their graph models candidate entities with binary labels, instead of mentions with multiple labels. A suitable assumption on cliques and their potentials makes efficient computation of exact inference possible. However, it is not clear as to how the node and edge feature weights are set. To the best of our knowledge, none of the graphbased models above have attempted to jointly 220 learn the node and edge feature weights. While there is prior work (Wellner et al., 2004; Wick et al., 2009) that applied graphical models to the problem of information extraction and coreference resolution, exact"
W15-5934,C10-1150,0,0.0178016,"and jointly train the local and global feature weights. In this work we present a collective disambiguation model, that, under suitable assumptions makes efficient implementation of exact MAP inference possible. We also present an efficient approach to train the local and global features of this model and implement it in an interactive entity linking system. The system receives human feedback on a document collection and progressively trains the underlying disambiguation model. selected mentions, based on mention-entity coherence, as well as entity-entity similarity. Some of the recent work (Zhou et al., 2010; Lin et al., 2012) shows that several mentions may have no associated sense in the catalog. This is referred to as the no-attachment (NA) problem (or NIL in the TAC-KBP challenge (McNamee, 2009)). The other, relatively lesser addressed challenge is that of multiple attachments (Kulkarni et al., 2014), where a mention might link to more than one entities from the catalog. This might often be a result of insufficient context and has been acknowledged by some of the recent entity disambiguation challenges1 . We present an approach to collective disambiguation of several mentions by combining var"
