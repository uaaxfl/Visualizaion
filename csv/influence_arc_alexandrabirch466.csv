2006.amta-papers.2,N03-1017,0,0.241887,"acent words, and, because word alignments inadequately represent the real dependencies between translations. Also, by heuristically creating phrasal alignments from the Viterbi word-level alignments, we throw away the probabilities that were estimated when learning word alignment parameters and we can introduce errors. In contrast, the Joint Model can search areas of the alignment space in order to learn a distribution of possible phrasal alignments that better handles the uncertainty inherent in the translation process. Models Standard Phrase-based Model Most phrase-based models (Och, 2003b; Koehn et al., 2003; Vogel et al., 2003) rely on a pre-existing set of word-based alignments from which they induce their parameters. In this project we use the model described by Koehn et al. (2003) which extracts its phrase alignments from a corpus that has been word aligned. From now on we refer to this phrase-based model as the Standard Model. The Standard Model decomposes the foreign input sentence F into a sequence of I phrases f 1 , . . . , f I . Each foreign phrase fi is translated to an English phrase ei using the probability distribution θ(f i |ei ). English phrases may be reordered using a relative di"
2006.amta-papers.2,koen-2004-pharaoh,0,0.0349132,"which is about a factor of eight. Experiments The experiments were run using the GermanEnglish Europarl corpus (Koehn, 2005). Europarl contains proceedings from the European Parliament covering the years 1996-2003. The test set consisted of the standard Europarl test set of 1755 sentences which ranged from 5 to 15 words in length. This makes results directly comparable to Koehn et al. (2003). For the language model we used the SRI Language Modelling Toolkit (Stolcke, 2002) to train a trigram model on the English section of the Europarl corpus. To perform the translations we used the Pharaoh (Koehn, 2004) beam search decoder version 1.2.8, with all the standard settings. Our evaluation metric was Bleu (Papineni et al., 2002) which compares the output sentences with human translated sentences using 4-gram precision. The translation models are included within a loglinear model (Och and Ney, 2002) which allows a weighted combination of features functions. Only Figure 3. Bleu scores using 5,000 sentences training data The effect of fast hill-climbing on the quality of translations can be seen in Figure 3. The default method slightly outperforms fast hill-climbing for 15 Corpus Size Standard Model"
2006.amta-papers.2,2005.mtsummit-papers.11,0,0.0426336,"tively slow even for the smallest data sets, so the first experiment explores the gains to be made by using fast hill-climbing on a training corpus of 5000 sentences. Figure 2. Time taken for EM training in minutes per iteration for 5,000 sentences on a machine with 2Gb RAM and a 2.4GHz CPU In Figure 2 we can see that fast hill-climbing is much faster than the normal hill-climbing. We have reduced the time taken to perform the first iteration from nearly 5 hours to about 40 minutes, which is about a factor of eight. Experiments The experiments were run using the GermanEnglish Europarl corpus (Koehn, 2005). Europarl contains proceedings from the European Parliament covering the years 1996-2003. The test set consisted of the standard Europarl test set of 1755 sentences which ranged from 5 to 15 words in length. This makes results directly comparable to Koehn et al. (2003). For the language model we used the SRI Language Modelling Toolkit (Stolcke, 2002) to train a trigram model on the English section of the Europarl corpus. To perform the translations we used the Pharaoh (Koehn, 2004) beam search decoder version 1.2.8, with all the standard settings. Our evaluation metric was Bleu (Papineni et a"
2006.amta-papers.2,W02-1018,0,0.489907,"irch Chris Callison-Burch Miles Osborne School of Informatics University of Edinburgh 2 Buccleuch Place Edinburgh, EH8 9LW, UK a.c.birch-mayne@sms.ed.ac.uk Abstract word-based models because phrases provide local context which leads to better lexical choice and more reliable local reordering. However, most phrasebased models extract their phrase pairs from previously word-aligned corpora using ad-hoc heuristics. These models perform no search for optimal phrasal alignments. Even though this is an efficient strategy, it is a departure from the rigourous statistical framework of the IBM Models. Marcu and Wong (2002) proposed a Joint Probability Model which directly estimates phrase translation probabilities from the corpus. This model neither relies on potentially sub-optimal word alignments nor on heuristics for phrase extraction. Instead, it searches the phrasal alignment space, simultaneously learning translation lexicons for both words and phrases. The Joint Model has been shown to outperform standard models on restricted data sets such as the small data track for Chinese-English in the 2004 NIST MT Evaluation (Przybocki, 2004). However, considering all possible phrases and all their possible alignme"
2006.amta-papers.2,P02-1038,0,0.0294569,"s which ranged from 5 to 15 words in length. This makes results directly comparable to Koehn et al. (2003). For the language model we used the SRI Language Modelling Toolkit (Stolcke, 2002) to train a trigram model on the English section of the Europarl corpus. To perform the translations we used the Pharaoh (Koehn, 2004) beam search decoder version 1.2.8, with all the standard settings. Our evaluation metric was Bleu (Papineni et al., 2002) which compares the output sentences with human translated sentences using 4-gram precision. The translation models are included within a loglinear model (Och and Ney, 2002) which allows a weighted combination of features functions. Only Figure 3. Bleu scores using 5,000 sentences training data The effect of fast hill-climbing on the quality of translations can be seen in Figure 3. The default method slightly outperforms fast hill-climbing for 15 Corpus Size Standard Model Joint Model + IBM + IBM + Lex the first few iterations, but then fast hill-climbing overtakes it. The difference in performance between the two methods is small and we apply fast hillclimbing in the remaining experiments. 5.2 IBM Constraints 10,000 21.69 19.93 22.13 22.79 20,000 23.61 23.08 24."
2006.amta-papers.2,J04-4002,0,0.0123472,"hout compromising the quality of the resulting translations. 2 2.1 for optimal phrase pairs. Instead, it extracts phrase pairs (f i , ei ) in the following manner. First, it uses the IBM Models to learn the Viterbi alignments for English to Foreign and Foreign to English. It then uses a heuristic to reconcile the two alignments, starting from the points of high confidence in the intersection of the two Viterbi alignments and growing towards the points in the union. Points from the union are selected if they are adjacent to points from the intersection and their words are previously unaligned. Och and Ney (2004) discusses and compares variations on this strategy. Phrases are then extracted by selecting phrase pairs which are ‘consistent’ with the symmetrised alignment. Here ‘consistent’ means that all words within the source language phrase are only aligned to the words of the target language phrase and vice versa. Finally the phrase translation probability distribution is estimated using the relative frequencies of the extracted phrase pairs. This approach to phrase extraction means that phrasal alignments are locked into the symmetrised alignment. This is problematic because the symmetrisation proc"
2006.amta-papers.2,P03-1021,0,0.509867,"oduction Machine translation is a hard problem because of the highly complex, irregular and diverse nature of natural languages. It is impossible to accurately model all the linguistic rules that shape the translation process, and therefore a principled approach uses statistical methods to make optimal decisions given incomplete data. The original IBM Models (Brown et al., 1993) learned only word-to-word alignment probabilities which made it computationally feasible to estimate model parameters from large amounts of training data. Phrase-based SMT models, such as the Alignment Template Model (Och, 2003b), improve on 10 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 10-18, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas space of the Joint Model to areas where most of the unpromising phrasal alignments are eliminated and yet as many potentially useful alignments as possible are still explored. The Joint Model is constrained to phrasal alignments which do not contradict a set high confidence word alignments for each sentence. These high confidence alignments can incorporate information from both statisti"
2006.amta-papers.2,P02-1040,0,0.108735,"(Koehn, 2005). Europarl contains proceedings from the European Parliament covering the years 1996-2003. The test set consisted of the standard Europarl test set of 1755 sentences which ranged from 5 to 15 words in length. This makes results directly comparable to Koehn et al. (2003). For the language model we used the SRI Language Modelling Toolkit (Stolcke, 2002) to train a trigram model on the English section of the Europarl corpus. To perform the translations we used the Pharaoh (Koehn, 2004) beam search decoder version 1.2.8, with all the standard settings. Our evaluation metric was Bleu (Papineni et al., 2002) which compares the output sentences with human translated sentences using 4-gram precision. The translation models are included within a loglinear model (Och and Ney, 2002) which allows a weighted combination of features functions. Only Figure 3. Bleu scores using 5,000 sentences training data The effect of fast hill-climbing on the quality of translations can be seen in Figure 3. The default method slightly outperforms fast hill-climbing for 15 Corpus Size Standard Model Joint Model + IBM + IBM + Lex the first few iterations, but then fast hill-climbing overtakes it. The difference in perfor"
2006.amta-papers.2,P92-1017,0,0.0866123,"18288740923 4.4145633531e+32 2.7340255177e+83 Table 1. The number of possible phrasal alignments for sentence pairs calculated using Stirling numbers of the second kind. Table 1 shows just how many phrasal alignments are possible between sentences of different length. Even for medium length sentences that are 20 words in lengths, the total number of alignments is huge. Apart from being intractable, when one has a very large parameter estimation space the EM algorithm struggles to discover good parameters. One approach to dealing with this problem is to constrain the search space. For example, Pereira and Schabes (1992) proposed a method for dealing with this problem for PCFG estimation from treebanks. They encouraged the probabilities into good regions of the parameter space by constraining the search to only consider parses that did not cross Penn-Treebank nodes. We adopt a similar approach for constraining the joint model, by only considering alignments that do not contradict high probability word alignments. During EM a very small proportion of the possible alignments are searched and many good alignments are likely to be missed. Normally alignments 2.2.2 Expectation Maximisation After initialising the t"
2006.amta-papers.2,J93-2003,0,0.0103816,"and linguistically motivated word alignments. This method reduces the complexity and size of the Joint Model and allows it to display performance superior to the standard phrase-based models for small amounts of training material. 1 Introduction Machine translation is a hard problem because of the highly complex, irregular and diverse nature of natural languages. It is impossible to accurately model all the linguistic rules that shape the translation process, and therefore a principled approach uses statistical methods to make optimal decisions given incomplete data. The original IBM Models (Brown et al., 1993) learned only word-to-word alignment probabilities which made it computationally feasible to estimate model parameters from large amounts of training data. Phrase-based SMT models, such as the Alignment Template Model (Och, 2003b), improve on 10 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 10-18, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas space of the Joint Model to areas where most of the unpromising phrasal alignments are eliminated and yet as many potentially useful alignments as possible are s"
2006.amta-papers.2,P04-1023,1,0.129086,"al Viterbi alignments. Concepts which contain many f c(e, f ) = (1 − λ)p(e, f |E, F ) + λpc(e, f ) The fractional count for each concept in each sentence is calculated by interpolating the joint probability of the concept, based on the Stirling numbers, and the prior count, which reflects the probability of the phrasal alignment given the high confidence word alignments. The use of the weight to balance the two contributions allows us to adjust for differences in scale and our confidence in each of the two measures. After testing various settings for λ the value 0.5 gave the best Bleu scores. Callison-Burch et al. (2004) used a similar technique for combining word and sentence aligned data. However, they inserted data from labelled word alignments which meant that they did not need to sum over all possible alignments for a sentence pair. 4.2 Fast Hill-climbing The constraints on the Joint Model reduce its size by restricting the initialisation phase of the training. This is one of the two major drawbacks of the 14 three features were used for both the Joint and the Standard Model: p(e|f ), p(f |e) and the language model, and they were given equal weights. model discussed by Marcu and Wong (2002). The other ma"
2006.amta-papers.2,2003.mtsummit-papers.53,0,0.0230242,"cause word alignments inadequately represent the real dependencies between translations. Also, by heuristically creating phrasal alignments from the Viterbi word-level alignments, we throw away the probabilities that were estimated when learning word alignment parameters and we can introduce errors. In contrast, the Joint Model can search areas of the alignment space in order to learn a distribution of possible phrasal alignments that better handles the uncertainty inherent in the translation process. Models Standard Phrase-based Model Most phrase-based models (Och, 2003b; Koehn et al., 2003; Vogel et al., 2003) rely on a pre-existing set of word-based alignments from which they induce their parameters. In this project we use the model described by Koehn et al. (2003) which extracts its phrase alignments from a corpus that has been word aligned. From now on we refer to this phrase-based model as the Standard Model. The Standard Model decomposes the foreign input sentence F into a sequence of I phrases f 1 , . . . , f I . Each foreign phrase fi is translated to an English phrase ei using the probability distribution θ(f i |ei ). English phrases may be reordered using a relative distortion probability"
2006.amta-papers.2,W06-3105,0,0.415441,"ature functions were optimized using minimum error rate training (Och, 2003a). Joint + IBM Standard Model BLEU 26.17 28.35 Pruning eliminates many phrase pairs, but further investigation indicates that this has little impact on BLEU scores. The fact that only a small proportion of the alignment space is searched is very likely to be hampering the Joint Model’s performance. The small number of alignments visited leads to data sparseness and over-fitting. Another factor could be efficiency trade-offs like the fast but not optimal competitive linking search for phrasal alignments. 6 Related Work DeNero et al. (2006) argue that training a translation model at the phrase level results in inferior parameters to the standard, heuristic phrase-based models. They suggest that the reason for this is that EM optimizes by selecting different segmentations and loses important phrase translation ambiguity. They say that the model results in a very peaked distribution and entropy drops too low. However, their argument only holds for conditional models. In a conditional model, there is competition for the probability mass of the conditioned word, and instead of spreading that mass between different translations, diff"
2006.amta-papers.2,W06-3114,0,0.00807513,"no such competition and the resulting phrase table’s entropy is in fact higher than that of the Standard Model. Size 2.28 19.04 Table 6. Bleu scores and model size in millions of phrase pairs for Spanish-English The results in Table 6 show that the Joint Model is capable of training on larger data sets, with a reasonable performance. On smaller data sets, as shown in sections 5.2 and 5.3 the Joint Model shows performance superior or comparable to the Standard Model. However, here it seems that the Standard Model has an advantage which is statistically significant according to the sign method (Koehn and Monz, 2006). This is almost certainly related to the fact that the Joint Model results in a much smaller phrase table. The size of the resulting Joint Model is in fact comparable to the size of the model in previous experiments when training with just 20,000 sentences. This is because the model must be kept in memory for collecting fractional counts in EM and even though the corpus is bigger, the memory available remains the same (the Standard Model phrase table is created on disk). To keep the Joint Model within memory, pruning is necessary after initialization because this is where most phrase pairs ar"
2006.amta-papers.2,P97-1063,0,\N,Missing
2006.amta-papers.2,P05-1066,0,\N,Missing
2006.amta-papers.2,2005.iwslt-1.8,1,\N,Missing
2006.amta-papers.2,P00-1056,0,\N,Missing
2009.mtsummit-papers.7,W08-0309,1,0.714167,"cal machine 1 2 http://eur-lex.europa.eu/ http://europa.eu/eurovoc/ translation system, and very little additional processing is needed. It is hard to quantify how much training data is needed to achieve a minimum level of performance. This depends on the expansiveness of the domain and the language pair. Typically, tens of millions of words give decent performance: For instance, systems trained on the 30–40 million word Europarl corpus are competitive with commercial systems, typically better on this domain and even close in performance when translating related material such as news stories (Callison-Burch et al., 2008). The JRC-Acquis corpus is large enough to expect decent translation performance within its domain, but on the other hand, the domain is also very specific. Translation models trained on such legal texts do not necessarily perform well on other domains. 3.2 Tuning and Test Sets Since we develop machine translation systems for 462 language pairs, we wanted to have a common tuning and testing environment. Hence, we extracted from part of the corpus subset where sentences are aligned one-to-one across all languages. First, we identified all documents that exist for all languages. This is a set of"
2009.mtsummit-papers.7,D08-1089,0,0.0147934,"present when choosing candidate translation phrases. We have also included corpus size as a factor as the amount of Acquis data per language pair can vary by a factor of four. The following characteristic form part of our analysis: 5 Reordering We measure word order differences between languages by assuming that reordering is a binary process between two blocks that are adjacent in the source and whose order is reversed in the target. Word alignments are extracted using GIZA++ and then merged using the grow-finaldiag algorithm. Reorderings are then extracted using the shift-reduce algorithm (Galley and Manning, 2008). These reorderings are used to extract a sentence level metric, RQuantity (Birch et al., 2008), which is the sum of the widths of all the reorderings on the source side, normalized by the length of the source sentence. This measure is averaged over a random sample of 2000 training sentences to get the corpus RQuantity. Analysis The Acquis corpus comprises of a very large number and variety of language pairs. The breadth of data conditions make this corpus ideal for performing experiments which investigate language pair characteristics and the effect they have on translation. This allows us to"
2009.mtsummit-papers.7,W09-0431,0,0.0369666,"tion performance more often than not. This is not the case for other languages. See Table 7 for summary statistics for English and French as pivot. When using English as pivot, we find not much difference (BLEU diverges by up to 2 points) for about a third of language pairs, for another third there are significant gains (2-5 points) and for another third even larger gains (5-10 points). However, using French as pivot generally decreases performance, only for a sixth of language pairs there is not much difference. English as pivot has also shown to be beneficial for Arabic–Chinese translation (Habash and Hu, 2009). We find it hard to claim that this is due to linguistic reasons, but rather an artifact of the data set we are using. It is likely that most of the text was originally authored in English. 6.2 Multi-Pivot Translation While pivoting through any language but English does generally lead to worse translations, it does constitute an alternative translation path. A recent trend in statistical machine translation is to combine the output of different MT systems in form of a consensus translation. In multi-pivot translation, we combine the direct translation system with several pivot systems, a nove"
2009.mtsummit-papers.7,P07-2045,1,0.0121715,"age. This gave us a set of 12,322 sentences aligned across all 22 languages of the corpus. We split this set into three parts, a tuning set for parameter optimization, a development test set for experimentation and a final test set to report translation performance. Since these sets contain many short and a few very long sentences, we reduced the tuning set further, by requiring that all sentences are between 8 and 60 words long. This left us with a tuning set of 1944 sentences per language. 3.3 Training For the development of the translation system, we used the defaults of the Moses toolkit (Koehn et al., 2007) with the following additional settings: maximum sentence length 80 words, bi-directional msd reordering model, 5-gram language model. 4 Performance A thorough evaluation of the translation quality of translation systems for 462 different language pairs would be a daunting task, so we rely on automatic metrics. The most commonly used metric in statistical machine translation is the BLEU score (Papineni et al., 2002). Table 3 shows the scores for all the 462 translation systems. Performance varies widely for the different language pairs. For instance, French–English translation (64.0) is better"
2009.mtsummit-papers.7,P02-1040,0,0.101453,"nd 60 words long. This left us with a tuning set of 1944 sentences per language. 3.3 Training For the development of the translation system, we used the defaults of the Moses toolkit (Koehn et al., 2007) with the following additional settings: maximum sentence length 80 words, bi-directional msd reordering model, 5-gram language model. 4 Performance A thorough evaluation of the translation quality of translation systems for 462 different language pairs would be a daunting task, so we rely on automatic metrics. The most commonly used metric in statistical machine translation is the BLEU score (Papineni et al., 2002). Table 3 shows the scores for all the 462 translation systems. Performance varies widely for the different language pairs. For instance, French–English translation (64.0) is better than Bulgarian–Hungarian (24.7). French Input LE CONSEIL DE LA COMMUNAUTE´ ´ ´ ECONOMIQUE EUROPEENNE, consid´erant que l’instauration d’une politique commune des transports comporte entre autres l’´etablissement de r`egles communes applicables aux transports internationaux de marchandises par route, ex´ecut´es au d´epart ou a` destination du territoire d’un e´ tat membre, ou traversant le territoire d’un ou plusieu"
2009.mtsummit-papers.7,N07-1029,0,0.0423619,"Missing"
2009.mtsummit-papers.7,D08-1078,1,0.937108,"s the amount of Acquis data per language pair can vary by a factor of four. The following characteristic form part of our analysis: 5 Reordering We measure word order differences between languages by assuming that reordering is a binary process between two blocks that are adjacent in the source and whose order is reversed in the target. Word alignments are extracted using GIZA++ and then merged using the grow-finaldiag algorithm. Reorderings are then extracted using the shift-reduce algorithm (Galley and Manning, 2008). These reorderings are used to extract a sentence level metric, RQuantity (Birch et al., 2008), which is the sum of the widths of all the reorderings on the source side, normalized by the length of the source sentence. This measure is averaged over a random sample of 2000 training sentences to get the corpus RQuantity. Analysis The Acquis corpus comprises of a very large number and variety of language pairs. The breadth of data conditions make this corpus ideal for performing experiments which investigate language pair characteristics and the effect they have on translation. This allows us to provide a wide perspective on the challenges facing machine translation and provide strong mot"
2009.mtsummit-papers.7,steinberger-etal-2006-jrc,1,0.296887,"Missing"
2009.mtsummit-papers.7,P07-1108,0,\N,Missing
2013.iwslt-evaluation.22,2013.iwslt-evaluation.11,1,0.781008,"Missing"
2013.iwslt-evaluation.22,P10-2041,0,0.0114973,"64K words imposed by the version of HDecode in use here. The vocabulary size was 62,522. Initialisms included in the vocabulary were treated as single words for LM purposes, e.g. “u.s.” (with the dots retained to distinguish them from words such as “us”). Once the vocabulary had been defined, out-of-vocabulary initialisms were broken into single letters, e.g. “m. f. n.”, so as to be modelled as sequences of in-vocabulary words (letter names) rather than treated as OOV. In view of the mismatch in content and style between the target domain (TED talks) and the OOD data, a data selection process [3, 4] was applied to the OOD corpora to obtain an appropriate subset of data for LM training. The set of out-ofdomain data DS was chosen by computing a cross-entropy difference (CED) score for each sentence s: DS = {s|HI (s) − HO (s) &lt; τ } Language model TED 3-gram OOD (312MW / 751MW) 3-gram TED+OOD (312MW / 751MW) 3-gram TED 4-gram OOD (312MW / 751MW) 4-gram TED+OOD (312MW / 751MW) 4-gram Perplexity 183.2 133.5 / 138.3 125.1 / 124.9 179.9 123.9 / 126.4 114.9 / 113.4 Table 1: Perplexities of N-gram language models on TED development set. Corpus TED Europarl News Commentary News Crawl Gigaword OOD t"
2013.iwslt-evaluation.22,2012.iwslt-evaluation.4,1,\N,Missing
2013.iwslt-evaluation.22,2014.iwslt-evaluation.6,1,\N,Missing
2013.iwslt-evaluation.22,2013.iwslt-evaluation.3,1,\N,Missing
2013.iwslt-evaluation.3,W13-2212,1,0.917473,"ersity of Edinburgh Scotland, United Kingdom a.birch@ed.ac.uk {dnadir,pkoehn}@inf.ed.ac.uk Abstract This paper gives a description of the University of Edinburgh’s (UEDIN) systems for IWSLT 2013. We participated in all the MT tracks and the German-to-English and Englishto-French SLT tracks. Our SLT submissions experimented with including ASR uncertainty into the decoding process via confusion networks, and looked at different ways of punctuating ASR output. Our MT submissions are mainly based on a system used in the recent evaluation campaign at the Workshop on Statistical Machine Translation [1]. We additionally explored the use of generalized representations (Brown clusters, POS and morphological tags) translating out of English into European languages. 1. Spoken Language Translation We submit two systems to the Spoken Language Translation track: English-French and German-English. These systems were built to take maximum advantage of Edinburgh’s English [2] and German [3] 2013 IWSLT speech recognition systems. We explored different strategies for minimizing the mismatch between unpunctuated ASR output and SMT models, which are typically trained on punctuated text. We wanted to exami"
2013.iwslt-evaluation.3,2013.iwslt-evaluation.11,0,0.22948,"Missing"
2013.iwslt-evaluation.3,2006.iwslt-papers.1,0,0.157669,"Missing"
2013.iwslt-evaluation.3,P07-2045,1,0.0218946,"ilities over the 3000 context-dependent states of a HMM. Language modelling was done with a 4-gram LM which was trained on approximately 30 million words, selected from a text corpus of 994 million words, according to maximal cross-entropy with the TED domain. The lexicon was restricted to 300,000 words, striking a balance between adequate word coverage and low perplexity on the TED domain. The lattices were first generated with a heavily pruned version of this LM, and then rescored with the full model. For details, see [3]. 1.2. Experimental design We trained a phrase-based model using Moses [8] on the parallel corpora described in Table 1. These are large parallel corpora, with only TED talks [9] consisting of in-domain data. Europarl v7 [10], News Commentary corpus and Multi United Nations corpus [11], Gigaword corpus (French Gigaword Second Edition, English Gigaword Fifth Edition) and Common Crawl [12] consist of parallel data which contain some noise, and a large number of examples which are likely irrelevant for the target TED domain. We therefore used a domain filtering technique [13] which was applied successfully in last year’s Edinburgh submission [14]. This uses bilingual c"
2013.iwslt-evaluation.3,2012.eamt-1.60,0,0.140449,"Missing"
2013.iwslt-evaluation.3,W12-3102,1,0.854746,"Missing"
2013.iwslt-evaluation.3,P13-1135,1,0.795193,"Missing"
2013.iwslt-evaluation.3,D11-1033,0,0.0649187,"Missing"
2013.iwslt-evaluation.3,2012.iwslt-papers.17,1,0.710035,"Missing"
2013.iwslt-evaluation.3,E03-1076,1,0.788801,"Missing"
2013.iwslt-evaluation.3,P05-1066,1,0.842596,"Missing"
2013.iwslt-evaluation.3,N03-1017,1,0.0193068,"e submissions did slightly better. 2. Machine Translation Systems Our machine translation systems are based on our setup [1] that has been proven successful at the recent evaluation campaign at the Workshop on Statistical Machine Translation [20]. Language Arabic Chinese Dutch Farsi French German Italian Polish Portuguese Romanian Russian Slovenian Spanish Turkish Into English 24.8 11.8 32.8 14.5 33.3 30.5 29.7 17.7 36.0 31.7 19.1 24.7 39.5 13.5 From English 7.6 9.8 26.5 8.0 33.2 22.9 23.7 9.7 30.8 21.1 13.1 18.0 33.9 7.2 2.1. Baseline The system uses the baseline Moses [8] phrase-based model [21] (as given in the example files for the experimental management system), with the following additions: • limitation of phrase length to 5 • sparse domain indicator, lexical, phrase length, and count bin features [22] • factored models for German–English and English– German • source-side German compound splitting [23] • cube pruning with pop limit 1000 for tuning, 5000 for testing [24] • operation sequence model (OSM) with 4 additional supportive features: 2 gap based penalties, 1 distance based feature and 1 deletion penalty [25] • batch k-best MIRA tuning [26] • interpolated 5-gram KenLM lang"
2013.iwslt-evaluation.3,N09-1025,0,0.063499,"tical Machine Translation [20]. Language Arabic Chinese Dutch Farsi French German Italian Polish Portuguese Romanian Russian Slovenian Spanish Turkish Into English 24.8 11.8 32.8 14.5 33.3 30.5 29.7 17.7 36.0 31.7 19.1 24.7 39.5 13.5 From English 7.6 9.8 26.5 8.0 33.2 22.9 23.7 9.7 30.8 21.1 13.1 18.0 33.9 7.2 2.1. Baseline The system uses the baseline Moses [8] phrase-based model [21] (as given in the example files for the experimental management system), with the following additions: • limitation of phrase length to 5 • sparse domain indicator, lexical, phrase length, and count bin features [22] • factored models for German–English and English– German • source-side German compound splitting [23] • cube pruning with pop limit 1000 for tuning, 5000 for testing [24] • operation sequence model (OSM) with 4 additional supportive features: 2 gap based penalties, 1 distance based feature and 1 deletion penalty [25] • batch k-best MIRA tuning [26] • interpolated 5-gram KenLM language models [27] • minimum Bayes risk decoding [28] • no-reordering-over-punctuation heuristic [29] In the IWSLT systems, we also used: • compact phrase tables [30] • filter out phrase translations with conditional p"
2013.iwslt-evaluation.3,P07-1019,0,0.235981,".8 32.8 14.5 33.3 30.5 29.7 17.7 36.0 31.7 19.1 24.7 39.5 13.5 From English 7.6 9.8 26.5 8.0 33.2 22.9 23.7 9.7 30.8 21.1 13.1 18.0 33.9 7.2 2.1. Baseline The system uses the baseline Moses [8] phrase-based model [21] (as given in the example files for the experimental management system), with the following additions: • limitation of phrase length to 5 • sparse domain indicator, lexical, phrase length, and count bin features [22] • factored models for German–English and English– German • source-side German compound splitting [23] • cube pruning with pop limit 1000 for tuning, 5000 for testing [24] • operation sequence model (OSM) with 4 additional supportive features: 2 gap based penalties, 1 distance based feature and 1 deletion penalty [25] • batch k-best MIRA tuning [26] • interpolated 5-gram KenLM language models [27] • minimum Bayes risk decoding [28] • no-reordering-over-punctuation heuristic [29] In the IWSLT systems, we also used: • compact phrase tables [30] • filter out phrase translations with conditional probability of less than 0.0001 • hierarchical lexicalized reordering (mslr) [31] • MADA tokenizer for source-side Arabic [32] • Stanford Chinese segmenter [33] We also tri"
2013.iwslt-evaluation.3,P11-1105,1,0.799042,"seline The system uses the baseline Moses [8] phrase-based model [21] (as given in the example files for the experimental management system), with the following additions: • limitation of phrase length to 5 • sparse domain indicator, lexical, phrase length, and count bin features [22] • factored models for German–English and English– German • source-side German compound splitting [23] • cube pruning with pop limit 1000 for tuning, 5000 for testing [24] • operation sequence model (OSM) with 4 additional supportive features: 2 gap based penalties, 1 distance based feature and 1 deletion penalty [25] • batch k-best MIRA tuning [26] • interpolated 5-gram KenLM language models [27] • minimum Bayes risk decoding [28] • no-reordering-over-punctuation heuristic [29] In the IWSLT systems, we also used: • compact phrase tables [30] • filter out phrase translations with conditional probability of less than 0.0001 • hierarchical lexicalized reordering (mslr) [31] • MADA tokenizer for source-side Arabic [32] • Stanford Chinese segmenter [33] We also tried hierarchical phrase-based models for Chinese, but did not achieve better results. In addition to the data provided directly from the IWSLT organi"
2013.iwslt-evaluation.3,N12-1047,0,0.0421976,"ine Moses [8] phrase-based model [21] (as given in the example files for the experimental management system), with the following additions: • limitation of phrase length to 5 • sparse domain indicator, lexical, phrase length, and count bin features [22] • factored models for German–English and English– German • source-side German compound splitting [23] • cube pruning with pop limit 1000 for tuning, 5000 for testing [24] • operation sequence model (OSM) with 4 additional supportive features: 2 gap based penalties, 1 distance based feature and 1 deletion penalty [25] • batch k-best MIRA tuning [26] • interpolated 5-gram KenLM language models [27] • minimum Bayes risk decoding [28] • no-reordering-over-punctuation heuristic [29] In the IWSLT systems, we also used: • compact phrase tables [30] • filter out phrase translations with conditional probability of less than 0.0001 • hierarchical lexicalized reordering (mslr) [31] • MADA tokenizer for source-side Arabic [32] • Stanford Chinese segmenter [33] We also tried hierarchical phrase-based models for Chinese, but did not achieve better results. In addition to the data provided directly from the IWSLT organizers, we also included whenever"
2013.iwslt-evaluation.3,W11-2123,0,0.0555168,"n the example files for the experimental management system), with the following additions: • limitation of phrase length to 5 • sparse domain indicator, lexical, phrase length, and count bin features [22] • factored models for German–English and English– German • source-side German compound splitting [23] • cube pruning with pop limit 1000 for tuning, 5000 for testing [24] • operation sequence model (OSM) with 4 additional supportive features: 2 gap based penalties, 1 distance based feature and 1 deletion penalty [25] • batch k-best MIRA tuning [26] • interpolated 5-gram KenLM language models [27] • minimum Bayes risk decoding [28] • no-reordering-over-punctuation heuristic [29] In the IWSLT systems, we also used: • compact phrase tables [30] • filter out phrase translations with conditional probability of less than 0.0001 • hierarchical lexicalized reordering (mslr) [31] • MADA tokenizer for source-side Arabic [32] • Stanford Chinese segmenter [33] We also tried hierarchical phrase-based models for Chinese, but did not achieve better results. In addition to the data provided directly from the IWSLT organizers, we also included whenever applicable: • Common Crawl parallel corpus, as pr"
2013.iwslt-evaluation.3,N04-1022,0,0.439255,"ental management system), with the following additions: • limitation of phrase length to 5 • sparse domain indicator, lexical, phrase length, and count bin features [22] • factored models for German–English and English– German • source-side German compound splitting [23] • cube pruning with pop limit 1000 for tuning, 5000 for testing [24] • operation sequence model (OSM) with 4 additional supportive features: 2 gap based penalties, 1 distance based feature and 1 deletion penalty [25] • batch k-best MIRA tuning [26] • interpolated 5-gram KenLM language models [27] • minimum Bayes risk decoding [28] • no-reordering-over-punctuation heuristic [29] In the IWSLT systems, we also used: • compact phrase tables [30] • filter out phrase translations with conditional probability of less than 0.0001 • hierarchical lexicalized reordering (mslr) [31] • MADA tokenizer for source-side Arabic [32] • Stanford Chinese segmenter [33] We also tried hierarchical phrase-based models for Chinese, but did not achieve better results. In addition to the data provided directly from the IWSLT organizers, we also included whenever applicable: • Common Crawl parallel corpus, as provided by WMT 2013 [34] • Europarl"
2013.iwslt-evaluation.3,W09-0429,1,0.841827,"itions: • limitation of phrase length to 5 • sparse domain indicator, lexical, phrase length, and count bin features [22] • factored models for German–English and English– German • source-side German compound splitting [23] • cube pruning with pop limit 1000 for tuning, 5000 for testing [24] • operation sequence model (OSM) with 4 additional supportive features: 2 gap based penalties, 1 distance based feature and 1 deletion penalty [25] • batch k-best MIRA tuning [26] • interpolated 5-gram KenLM language models [27] • minimum Bayes risk decoding [28] • no-reordering-over-punctuation heuristic [29] In the IWSLT systems, we also used: • compact phrase tables [30] • filter out phrase translations with conditional probability of less than 0.0001 • hierarchical lexicalized reordering (mslr) [31] • MADA tokenizer for source-side Arabic [32] • Stanford Chinese segmenter [33] We also tried hierarchical phrase-based models for Chinese, but did not achieve better results. In addition to the data provided directly from the IWSLT organizers, we also included whenever applicable: • Common Crawl parallel corpus, as provided by WMT 2013 [34] • Europarl version 7 parallel corpus1 [35] • news commentar"
2013.iwslt-evaluation.3,D08-1089,0,0.1348,"an compound splitting [23] • cube pruning with pop limit 1000 for tuning, 5000 for testing [24] • operation sequence model (OSM) with 4 additional supportive features: 2 gap based penalties, 1 distance based feature and 1 deletion penalty [25] • batch k-best MIRA tuning [26] • interpolated 5-gram KenLM language models [27] • minimum Bayes risk decoding [28] • no-reordering-over-punctuation heuristic [29] In the IWSLT systems, we also used: • compact phrase tables [30] • filter out phrase translations with conditional probability of less than 0.0001 • hierarchical lexicalized reordering (mslr) [31] • MADA tokenizer for source-side Arabic [32] • Stanford Chinese segmenter [33] We also tried hierarchical phrase-based models for Chinese, but did not achieve better results. In addition to the data provided directly from the IWSLT organizers, we also included whenever applicable: • Common Crawl parallel corpus, as provided by WMT 2013 [34] • Europarl version 7 parallel corpus1 [35] • news commentary parallel corpus, as provided by WMT 2013 1 http://www.statmt.org/europarl/ Table 7: Baseline system performance for machine translation systems (Section 2.1): Cased BLEU scores on test2010 using"
2013.iwslt-evaluation.3,N06-2013,0,0.0781791,"h pop limit 1000 for tuning, 5000 for testing [24] • operation sequence model (OSM) with 4 additional supportive features: 2 gap based penalties, 1 distance based feature and 1 deletion penalty [25] • batch k-best MIRA tuning [26] • interpolated 5-gram KenLM language models [27] • minimum Bayes risk decoding [28] • no-reordering-over-punctuation heuristic [29] In the IWSLT systems, we also used: • compact phrase tables [30] • filter out phrase translations with conditional probability of less than 0.0001 • hierarchical lexicalized reordering (mslr) [31] • MADA tokenizer for source-side Arabic [32] • Stanford Chinese segmenter [33] We also tried hierarchical phrase-based models for Chinese, but did not achieve better results. In addition to the data provided directly from the IWSLT organizers, we also included whenever applicable: • Common Crawl parallel corpus, as provided by WMT 2013 [34] • Europarl version 7 parallel corpus1 [35] • news commentary parallel corpus, as provided by WMT 2013 1 http://www.statmt.org/europarl/ Table 7: Baseline system performance for machine translation systems (Section 2.1): Cased BLEU scores on test2010 using NIST’s mteval-v13a. Test on tune for Slovenia"
2013.iwslt-evaluation.3,W08-0336,0,0.0381555,"for testing [24] • operation sequence model (OSM) with 4 additional supportive features: 2 gap based penalties, 1 distance based feature and 1 deletion penalty [25] • batch k-best MIRA tuning [26] • interpolated 5-gram KenLM language models [27] • minimum Bayes risk decoding [28] • no-reordering-over-punctuation heuristic [29] In the IWSLT systems, we also used: • compact phrase tables [30] • filter out phrase translations with conditional probability of less than 0.0001 • hierarchical lexicalized reordering (mslr) [31] • MADA tokenizer for source-side Arabic [32] • Stanford Chinese segmenter [33] We also tried hierarchical phrase-based models for Chinese, but did not achieve better results. In addition to the data provided directly from the IWSLT organizers, we also included whenever applicable: • Common Crawl parallel corpus, as provided by WMT 2013 [34] • Europarl version 7 parallel corpus1 [35] • news commentary parallel corpus, as provided by WMT 2013 1 http://www.statmt.org/europarl/ Table 7: Baseline system performance for machine translation systems (Section 2.1): Cased BLEU scores on test2010 using NIST’s mteval-v13a. Test on tune for Slovenian. Moses multi-bleu.perl for Chine"
2013.iwslt-evaluation.3,2005.mtsummit-papers.11,1,0.0568235,"uation heuristic [29] In the IWSLT systems, we also used: • compact phrase tables [30] • filter out phrase translations with conditional probability of less than 0.0001 • hierarchical lexicalized reordering (mslr) [31] • MADA tokenizer for source-side Arabic [32] • Stanford Chinese segmenter [33] We also tried hierarchical phrase-based models for Chinese, but did not achieve better results. In addition to the data provided directly from the IWSLT organizers, we also included whenever applicable: • Common Crawl parallel corpus, as provided by WMT 2013 [34] • Europarl version 7 parallel corpus1 [35] • news commentary parallel corpus, as provided by WMT 2013 1 http://www.statmt.org/europarl/ Table 7: Baseline system performance for machine translation systems (Section 2.1): Cased BLEU scores on test2010 using NIST’s mteval-v13a. Test on tune for Slovenian. Moses multi-bleu.perl for Chinese target. • news language model data provided by WMT 2013 • LDC Gigaword for French, Spanish, and English as output language We built systems for all language pairs of the IWSLT evaluation campaign. The quality scores (BLEU) of the resulting systems as measured on the development test set is given in Tabl"
2013.iwslt-evaluation.3,W13-2205,0,0.0250289,"013 1 http://www.statmt.org/europarl/ Table 7: Baseline system performance for machine translation systems (Section 2.1): Cased BLEU scores on test2010 using NIST’s mteval-v13a. Test on tune for Slovenian. Moses multi-bleu.perl for Chinese target. • news language model data provided by WMT 2013 • LDC Gigaword for French, Spanish, and English as output language We built systems for all language pairs of the IWSLT evaluation campaign. The quality scores (BLEU) of the resulting systems as measured on the development test set is given in Table 7. 2.2. Brown Cluster Language Models As suggested by [36], we explored the use of Brown clusters [37]. We computed the clusters with GIZA++’s mkcls [38] on the target side of the parallel training corpus. Brown clusters are word classes that are optimized to reduce n-gram perplexity. By generating the Brown cluster identifier for each output word, we are able to add an n-gram model over these identifiers as an additional scoring function. The inclusion of such an additional factor is trivial given the factored model implementation [39] of Moses. The n-gram model is trained on the target side of the TED corpus made available by the IWSLT organizers."
2013.iwslt-evaluation.3,J92-4003,0,0.213359,"7: Baseline system performance for machine translation systems (Section 2.1): Cased BLEU scores on test2010 using NIST’s mteval-v13a. Test on tune for Slovenian. Moses multi-bleu.perl for Chinese target. • news language model data provided by WMT 2013 • LDC Gigaword for French, Spanish, and English as output language We built systems for all language pairs of the IWSLT evaluation campaign. The quality scores (BLEU) of the resulting systems as measured on the development test set is given in Table 7. 2.2. Brown Cluster Language Models As suggested by [36], we explored the use of Brown clusters [37]. We computed the clusters with GIZA++’s mkcls [38] on the target side of the parallel training corpus. Brown clusters are word classes that are optimized to reduce n-gram perplexity. By generating the Brown cluster identifier for each output word, we are able to add an n-gram model over these identifiers as an additional scoring function. The inclusion of such an additional factor is trivial given the factored model implementation [39] of Moses. The n-gram model is trained on the target side of the TED corpus made available by the IWSLT organizers. The motivation for using Brown clusters stem"
2013.iwslt-evaluation.3,E99-1010,0,0.159661,"ion systems (Section 2.1): Cased BLEU scores on test2010 using NIST’s mteval-v13a. Test on tune for Slovenian. Moses multi-bleu.perl for Chinese target. • news language model data provided by WMT 2013 • LDC Gigaword for French, Spanish, and English as output language We built systems for all language pairs of the IWSLT evaluation campaign. The quality scores (BLEU) of the resulting systems as measured on the development test set is given in Table 7. 2.2. Brown Cluster Language Models As suggested by [36], we explored the use of Brown clusters [37]. We computed the clusters with GIZA++’s mkcls [38] on the target side of the parallel training corpus. Brown clusters are word classes that are optimized to reduce n-gram perplexity. By generating the Brown cluster identifier for each output word, we are able to add an n-gram model over these identifiers as an additional scoring function. The inclusion of such an additional factor is trivial given the factored model implementation [39] of Moses. The n-gram model is trained on the target side of the TED corpus made available by the IWSLT organizers. The motivation for using Brown clusters stems from the success of using n-gram models over part"
2013.iwslt-evaluation.3,D07-1091,1,0.895243,"ems as measured on the development test set is given in Table 7. 2.2. Brown Cluster Language Models As suggested by [36], we explored the use of Brown clusters [37]. We computed the clusters with GIZA++’s mkcls [38] on the target side of the parallel training corpus. Brown clusters are word classes that are optimized to reduce n-gram perplexity. By generating the Brown cluster identifier for each output word, we are able to add an n-gram model over these identifiers as an additional scoring function. The inclusion of such an additional factor is trivial given the factored model implementation [39] of Moses. The n-gram model is trained on the target side of the TED corpus made available by the IWSLT organizers. The motivation for using Brown clusters stems from the success of using n-gram models over part-of-speech and morphological tags and the lack of the required taggers and analyzers for many language pairs. Brown clustering induces word classes that are similar to part-of-speech tags (for instance, placing adjectives with the same inflection into one class), with some additional semantic grouping (for instance, grouping all color adjectives). Results are shown in Table 8. While the"
2013.iwslt-evaluation.3,N13-1001,1,0.70383,"del alone did not give much improvement. We also tried using OSM models over different numbers of clusters simultaneously for English-to-{French, Spanish and Dutch} pairs. Small gain was observed in the case of English-to-Spanish as the best system improved from 34.7 to 35.0. No further gains were observed in the case of other two pairs. For each system, our official submission is the system with the best performance on the development test set. 2.3. Operation Sequence Models over Generalized Representations 2.3.2. POS and Morph Tags The integration of the OSM model into phrase-based decoding [40, 41] addresses the problem of phrasal independence assumption since the model considers context beyond phrasal boundaries. However, due to data sparsity the model often falls back to very small context sizes. We investigated the use of generalized representations (pos, morphological analysis and word clusters) in the OSM model. The expectation is that given the sparse training data for many of the language pairs, defining this model over the more general word classes would lead to a model that is able to consider wider context and learn richer lexical and reordering patterns. We also tried using t"
2013.iwslt-evaluation.3,P13-2071,1,0.726323,"del alone did not give much improvement. We also tried using OSM models over different numbers of clusters simultaneously for English-to-{French, Spanish and Dutch} pairs. Small gain was observed in the case of English-to-Spanish as the best system improved from 34.7 to 35.0. No further gains were observed in the case of other two pairs. For each system, our official submission is the system with the best performance on the development test set. 2.3. Operation Sequence Models over Generalized Representations 2.3.2. POS and Morph Tags The integration of the OSM model into phrase-based decoding [40, 41] addresses the problem of phrasal independence assumption since the model considers context beyond phrasal boundaries. However, due to data sparsity the model often falls back to very small context sizes. We investigated the use of generalized representations (pos, morphological analysis and word clusters) in the OSM model. The expectation is that given the sparse training data for many of the language pairs, defining this model over the more general word classes would lead to a model that is able to consider wider context and learn richer lexical and reordering patterns. We also tried using t"
2013.iwslt-evaluation.3,2012.iwslt-evaluation.4,1,\N,Missing
2013.iwslt-evaluation.3,N03-1031,0,\N,Missing
2013.iwslt-evaluation.3,2013.iwslt-evaluation.22,1,\N,Missing
2013.iwslt-evaluation.3,W13-2201,1,\N,Missing
2013.iwslt-evaluation.3,P13-3000,0,\N,Missing
2013.iwslt-evaluation.3,P13-4000,0,\N,Missing
2013.iwslt-evaluation.3,P13-5000,0,\N,Missing
2013.iwslt-evaluation.3,P13-1000,0,\N,Missing
2014.iwslt-evaluation.6,P07-2045,1,0.0218346,"English→French, Arabic↔English, Farsi→English, Hebrew→English, Spanish↔English, and Portuguese-Brazil↔English tasks. For our SLT submissions, we experimented with comparing operation sequence models with bilingual neural network language models. For our MT submissions, we explored using unsupervised transliteration for languages which have a different script than English, in particular for Arabic, Farsi, and Hebrew. We also investigated syntax-based translation and system combination. 1. Introduction The University of Edinburgh’s translation engines are based on the open source Moses toolkit [1]. We set up phrase-based systems [2] for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [3, 4] for the English→German MT task. The setups for our phrase-based systems have evolved from the configurations of the engines we built for last year’s IWSLT [5] and for this year’s Workshop on Statistical Machine Translation (WMT) [6]. The notable features of these systems are: • Phrase translation scores in both directions, smoothed with Good-Turing discounting • Lexical translation scores in both directions • Word and phrase penalties • Six simple co"
2014.iwslt-evaluation.6,N03-1017,1,0.0403051,"si→English, Hebrew→English, Spanish↔English, and Portuguese-Brazil↔English tasks. For our SLT submissions, we experimented with comparing operation sequence models with bilingual neural network language models. For our MT submissions, we explored using unsupervised transliteration for languages which have a different script than English, in particular for Arabic, Farsi, and Hebrew. We also investigated syntax-based translation and system combination. 1. Introduction The University of Edinburgh’s translation engines are based on the open source Moses toolkit [1]. We set up phrase-based systems [2] for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [3, 4] for the English→German MT task. The setups for our phrase-based systems have evolved from the configurations of the engines we built for last year’s IWSLT [5] and for this year’s Workshop on Statistical Machine Translation (WMT) [6]. The notable features of these systems are: • Phrase translation scores in both directions, smoothed with Good-Turing discounting • Lexical translation scores in both directions • Word and phrase penalties • Six simple count-based binary features • Phrase l"
2014.iwslt-evaluation.6,N04-1035,0,0.251325,"we experimented with comparing operation sequence models with bilingual neural network language models. For our MT submissions, we explored using unsupervised transliteration for languages which have a different script than English, in particular for Arabic, Farsi, and Hebrew. We also investigated syntax-based translation and system combination. 1. Introduction The University of Edinburgh’s translation engines are based on the open source Moses toolkit [1]. We set up phrase-based systems [2] for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [3, 4] for the English→German MT task. The setups for our phrase-based systems have evolved from the configurations of the engines we built for last year’s IWSLT [5] and for this year’s Workshop on Statistical Machine Translation (WMT) [6]. The notable features of these systems are: • Phrase translation scores in both directions, smoothed with Good-Turing discounting • Lexical translation scores in both directions • Word and phrase penalties • Six simple count-based binary features • Phrase length features • Distance-based distortion cost • A hierarchical lexicalized reordering model [7] • Sparse le"
2014.iwslt-evaluation.6,D08-1089,0,0.662228,"d system [3, 4] for the English→German MT task. The setups for our phrase-based systems have evolved from the configurations of the engines we built for last year’s IWSLT [5] and for this year’s Workshop on Statistical Machine Translation (WMT) [6]. The notable features of these systems are: • Phrase translation scores in both directions, smoothed with Good-Turing discounting • Lexical translation scores in both directions • Word and phrase penalties • Six simple count-based binary features • Phrase length features • Distance-based distortion cost • A hierarchical lexicalized reordering model [7] • Sparse lexical and domain indicator features [8] • Operation sequence models (OSMs) over different word representations [9, 10] • A 5-gram language model (LM) over words We typically train factored phrase-based translation models [11, 12] and also incorporate higher order n-gram LMs over word representations given by the factors. Factors can for instance be lemma, part-of-speech (POS) tag, morphological tag, or automatically learnt word classes in the manner of Brown clusters [13]. Edinburgh’s syntax-based systems have recently yielded state-of-the-art performance on English→German news tra"
2014.iwslt-evaluation.6,2012.iwslt-papers.17,1,0.922891,"setups for our phrase-based systems have evolved from the configurations of the engines we built for last year’s IWSLT [5] and for this year’s Workshop on Statistical Machine Translation (WMT) [6]. The notable features of these systems are: • Phrase translation scores in both directions, smoothed with Good-Turing discounting • Lexical translation scores in both directions • Word and phrase penalties • Six simple count-based binary features • Phrase length features • Distance-based distortion cost • A hierarchical lexicalized reordering model [7] • Sparse lexical and domain indicator features [8] • Operation sequence models (OSMs) over different word representations [9, 10] • A 5-gram language model (LM) over words We typically train factored phrase-based translation models [11, 12] and also incorporate higher order n-gram LMs over word representations given by the factors. Factors can for instance be lemma, part-of-speech (POS) tag, morphological tag, or automatically learnt word classes in the manner of Brown clusters [13]. Edinburgh’s syntax-based systems have recently yielded state-of-the-art performance on English→German news translation tasks [14, 15] but have not been applied i"
2014.iwslt-evaluation.6,P11-1105,1,0.920037,"f the engines we built for last year’s IWSLT [5] and for this year’s Workshop on Statistical Machine Translation (WMT) [6]. The notable features of these systems are: • Phrase translation scores in both directions, smoothed with Good-Turing discounting • Lexical translation scores in both directions • Word and phrase penalties • Six simple count-based binary features • Phrase length features • Distance-based distortion cost • A hierarchical lexicalized reordering model [7] • Sparse lexical and domain indicator features [8] • Operation sequence models (OSMs) over different word representations [9, 10] • A 5-gram language model (LM) over words We typically train factored phrase-based translation models [11, 12] and also incorporate higher order n-gram LMs over word representations given by the factors. Factors can for instance be lemma, part-of-speech (POS) tag, morphological tag, or automatically learnt word classes in the manner of Brown clusters [13]. Edinburgh’s syntax-based systems have recently yielded state-of-the-art performance on English→German news translation tasks [14, 15] but have not been applied in an IWSLT-style setting before. Standard features of our stringto-tree syntax-"
2014.iwslt-evaluation.6,C14-1041,1,0.826647,"f the engines we built for last year’s IWSLT [5] and for this year’s Workshop on Statistical Machine Translation (WMT) [6]. The notable features of these systems are: • Phrase translation scores in both directions, smoothed with Good-Turing discounting • Lexical translation scores in both directions • Word and phrase penalties • Six simple count-based binary features • Phrase length features • Distance-based distortion cost • A hierarchical lexicalized reordering model [7] • Sparse lexical and domain indicator features [8] • Operation sequence models (OSMs) over different word representations [9, 10] • A 5-gram language model (LM) over words We typically train factored phrase-based translation models [11, 12] and also incorporate higher order n-gram LMs over word representations given by the factors. Factors can for instance be lemma, part-of-speech (POS) tag, morphological tag, or automatically learnt word classes in the manner of Brown clusters [13]. Edinburgh’s syntax-based systems have recently yielded state-of-the-art performance on English→German news translation tasks [14, 15] but have not been applied in an IWSLT-style setting before. Standard features of our stringto-tree syntax-"
2014.iwslt-evaluation.6,D07-1091,1,0.88178,"on (WMT) [6]. The notable features of these systems are: • Phrase translation scores in both directions, smoothed with Good-Turing discounting • Lexical translation scores in both directions • Word and phrase penalties • Six simple count-based binary features • Phrase length features • Distance-based distortion cost • A hierarchical lexicalized reordering model [7] • Sparse lexical and domain indicator features [8] • Operation sequence models (OSMs) over different word representations [9, 10] • A 5-gram language model (LM) over words We typically train factored phrase-based translation models [11, 12] and also incorporate higher order n-gram LMs over word representations given by the factors. Factors can for instance be lemma, part-of-speech (POS) tag, morphological tag, or automatically learnt word classes in the manner of Brown clusters [13]. Edinburgh’s syntax-based systems have recently yielded state-of-the-art performance on English→German news translation tasks [14, 15] but have not been applied in an IWSLT-style setting before. Standard features of our stringto-tree syntax-based systems are: • Rule translation scores in both directions, smoothed with Good-Turing discounting • Lexica"
2014.iwslt-evaluation.6,2012.amta-papers.9,1,0.923939,"on (WMT) [6]. The notable features of these systems are: • Phrase translation scores in both directions, smoothed with Good-Turing discounting • Lexical translation scores in both directions • Word and phrase penalties • Six simple count-based binary features • Phrase length features • Distance-based distortion cost • A hierarchical lexicalized reordering model [7] • Sparse lexical and domain indicator features [8] • Operation sequence models (OSMs) over different word representations [9, 10] • A 5-gram language model (LM) over words We typically train factored phrase-based translation models [11, 12] and also incorporate higher order n-gram LMs over word representations given by the factors. Factors can for instance be lemma, part-of-speech (POS) tag, morphological tag, or automatically learnt word classes in the manner of Brown clusters [13]. Edinburgh’s syntax-based systems have recently yielded state-of-the-art performance on English→German news translation tasks [14, 15] but have not been applied in an IWSLT-style setting before. Standard features of our stringto-tree syntax-based systems are: • Rule translation scores in both directions, smoothed with Good-Turing discounting • Lexica"
2014.iwslt-evaluation.6,E99-1010,0,0.726319,"features • Phrase length features • Distance-based distortion cost • A hierarchical lexicalized reordering model [7] • Sparse lexical and domain indicator features [8] • Operation sequence models (OSMs) over different word representations [9, 10] • A 5-gram language model (LM) over words We typically train factored phrase-based translation models [11, 12] and also incorporate higher order n-gram LMs over word representations given by the factors. Factors can for instance be lemma, part-of-speech (POS) tag, morphological tag, or automatically learnt word classes in the manner of Brown clusters [13]. Edinburgh’s syntax-based systems have recently yielded state-of-the-art performance on English→German news translation tasks [14, 15] but have not been applied in an IWSLT-style setting before. Standard features of our stringto-tree syntax-based systems are: • Rule translation scores in both directions, smoothed with Good-Turing discounting • Lexical translation scores in both directions • Word and rule penalties • A rule rareness penalty • The monolingual PCFG probability of the tree fragment from which the rule was extracted • A 5-gram LM over words For our Spanish↔English and PortugueseBr"
2014.iwslt-evaluation.6,W12-3150,1,0.921133,"we experimented with comparing operation sequence models with bilingual neural network language models. For our MT submissions, we explored using unsupervised transliteration for languages which have a different script than English, in particular for Arabic, Farsi, and Hebrew. We also investigated syntax-based translation and system combination. 1. Introduction The University of Edinburgh’s translation engines are based on the open source Moses toolkit [1]. We set up phrase-based systems [2] for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [3, 4] for the English→German MT task. The setups for our phrase-based systems have evolved from the configurations of the engines we built for last year’s IWSLT [5] and for this year’s Workshop on Statistical Machine Translation (WMT) [6]. The notable features of these systems are: • Phrase translation scores in both directions, smoothed with Good-Turing discounting • Lexical translation scores in both directions • Word and phrase penalties • Six simple count-based binary features • Phrase length features • Distance-based distortion cost • A hierarchical lexicalized reordering model [7] • Sparse le"
2014.iwslt-evaluation.6,W13-2221,1,0.91182,"al and domain indicator features [8] • Operation sequence models (OSMs) over different word representations [9, 10] • A 5-gram language model (LM) over words We typically train factored phrase-based translation models [11, 12] and also incorporate higher order n-gram LMs over word representations given by the factors. Factors can for instance be lemma, part-of-speech (POS) tag, morphological tag, or automatically learnt word classes in the manner of Brown clusters [13]. Edinburgh’s syntax-based systems have recently yielded state-of-the-art performance on English→German news translation tasks [14, 15] but have not been applied in an IWSLT-style setting before. Standard features of our stringto-tree syntax-based systems are: • Rule translation scores in both directions, smoothed with Good-Turing discounting • Lexical translation scores in both directions • Word and rule penalties • A rule rareness penalty • The monolingual PCFG probability of the tree fragment from which the rule was extracted • A 5-gram LM over words For our Spanish↔English and PortugueseBrazil↔English submissions, we ran the engines as described in last year’s system description paper [5]. In the following, we focus on de"
2014.iwslt-evaluation.6,2013.iwslt-evaluation.3,1,0.887168,"ansliteration for languages which have a different script than English, in particular for Arabic, Farsi, and Hebrew. We also investigated syntax-based translation and system combination. 1. Introduction The University of Edinburgh’s translation engines are based on the open source Moses toolkit [1]. We set up phrase-based systems [2] for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [3, 4] for the English→German MT task. The setups for our phrase-based systems have evolved from the configurations of the engines we built for last year’s IWSLT [5] and for this year’s Workshop on Statistical Machine Translation (WMT) [6]. The notable features of these systems are: • Phrase translation scores in both directions, smoothed with Good-Turing discounting • Lexical translation scores in both directions • Word and phrase penalties • Six simple count-based binary features • Phrase length features • Distance-based distortion cost • A hierarchical lexicalized reordering model [7] • Sparse lexical and domain indicator features [8] • Operation sequence models (OSMs) over different word representations [9, 10] • A 5-gram language model (LM) over word"
2014.iwslt-evaluation.6,W14-3324,1,0.869051,"al and domain indicator features [8] • Operation sequence models (OSMs) over different word representations [9, 10] • A 5-gram language model (LM) over words We typically train factored phrase-based translation models [11, 12] and also incorporate higher order n-gram LMs over word representations given by the factors. Factors can for instance be lemma, part-of-speech (POS) tag, morphological tag, or automatically learnt word classes in the manner of Brown clusters [13]. Edinburgh’s syntax-based systems have recently yielded state-of-the-art performance on English→German news translation tasks [14, 15] but have not been applied in an IWSLT-style setting before. Standard features of our stringto-tree syntax-based systems are: • Rule translation scores in both directions, smoothed with Good-Turing discounting • Lexical translation scores in both directions • Word and rule penalties • A rule rareness penalty • The monolingual PCFG probability of the tree fragment from which the rule was extracted • A 5-gram LM over words For our Spanish↔English and PortugueseBrazil↔English submissions, we ran the engines as described in last year’s system description paper [5]. In the following, we focus on de"
2014.iwslt-evaluation.6,W14-3309,1,0.886786,"particular for Arabic, Farsi, and Hebrew. We also investigated syntax-based translation and system combination. 1. Introduction The University of Edinburgh’s translation engines are based on the open source Moses toolkit [1]. We set up phrase-based systems [2] for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [3, 4] for the English→German MT task. The setups for our phrase-based systems have evolved from the configurations of the engines we built for last year’s IWSLT [5] and for this year’s Workshop on Statistical Machine Translation (WMT) [6]. The notable features of these systems are: • Phrase translation scores in both directions, smoothed with Good-Turing discounting • Lexical translation scores in both directions • Word and phrase penalties • Six simple count-based binary features • Phrase length features • Distance-based distortion cost • A hierarchical lexicalized reordering model [7] • Sparse lexical and domain indicator features [8] • Operation sequence models (OSMs) over different word representations [9, 10] • A 5-gram language model (LM) over words We typically train factored phrase-based translation models [11, 12] and"
2014.iwslt-evaluation.6,2012.eamt-1.60,0,0.0588662,"rections, smoothed with Good-Turing discounting • Lexical translation scores in both directions • Word and rule penalties • A rule rareness penalty • The monolingual PCFG probability of the tree fragment from which the rule was extracted • A 5-gram LM over words For our Spanish↔English and PortugueseBrazil↔English submissions, we ran the engines as described in last year’s system description paper [5]. In the following, we focus on describing the new systems which were developed for the rest of the tasks. Our this year’s IWSLT systems were trained using monolingual and parallel data from WIT3 [16], Europarl [17], MultiUN [18], the Gigaword corpora as provided by the Linguistic Data Consortium [19], the German Political Speeches Corpus [20], and the corpora provided for the WMT shared translation task [21]. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [22] and symmetrizing the two alignments with the grow-diagfinal-and heuristic [23, 2]. Word alignments for the SLT track systems were created using fast align [24]. The SRILM toolkit [25] was employed to train 5-gram language models (LMs) with modified Kneser-Ney smoothing [26]"
2014.iwslt-evaluation.6,2005.mtsummit-papers.11,1,0.143449,"hed with Good-Turing discounting • Lexical translation scores in both directions • Word and rule penalties • A rule rareness penalty • The monolingual PCFG probability of the tree fragment from which the rule was extracted • A 5-gram LM over words For our Spanish↔English and PortugueseBrazil↔English submissions, we ran the engines as described in last year’s system description paper [5]. In the following, we focus on describing the new systems which were developed for the rest of the tasks. Our this year’s IWSLT systems were trained using monolingual and parallel data from WIT3 [16], Europarl [17], MultiUN [18], the Gigaword corpora as provided by the Linguistic Data Consortium [19], the German Political Speeches Corpus [20], and the corpora provided for the WMT shared translation task [21]. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [22] and symmetrizing the two alignments with the grow-diagfinal-and heuristic [23, 2]. Word alignments for the SLT track systems were created using fast align [24]. The SRILM toolkit [25] was employed to train 5-gram language models (LMs) with modified Kneser-Ney smoothing [26]. We trained in"
2014.iwslt-evaluation.6,eisele-chen-2010-multiun,0,0.268558,"Turing discounting • Lexical translation scores in both directions • Word and rule penalties • A rule rareness penalty • The monolingual PCFG probability of the tree fragment from which the rule was extracted • A 5-gram LM over words For our Spanish↔English and PortugueseBrazil↔English submissions, we ran the engines as described in last year’s system description paper [5]. In the following, we focus on describing the new systems which were developed for the rest of the tasks. Our this year’s IWSLT systems were trained using monolingual and parallel data from WIT3 [16], Europarl [17], MultiUN [18], the Gigaword corpora as provided by the Linguistic Data Consortium [19], the German Political Speeches Corpus [20], and the corpora provided for the WMT shared translation task [21]. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [22] and symmetrizing the two alignments with the grow-diagfinal-and heuristic [23, 2]. Word alignments for the SLT track systems were created using fast align [24]. The SRILM toolkit [25] was employed to train 5-gram language models (LMs) with modified Kneser-Ney smoothing [26]. We trained individual LMs o"
2014.iwslt-evaluation.6,W08-0509,0,0.134751,"ns, we ran the engines as described in last year’s system description paper [5]. In the following, we focus on describing the new systems which were developed for the rest of the tasks. Our this year’s IWSLT systems were trained using monolingual and parallel data from WIT3 [16], Europarl [17], MultiUN [18], the Gigaword corpora as provided by the Linguistic Data Consortium [19], the German Political Speeches Corpus [20], and the corpora provided for the WMT shared translation task [21]. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [22] and symmetrizing the two alignments with the grow-diagfinal-and heuristic [23, 2]. Word alignments for the SLT track systems were created using fast align [24]. The SRILM toolkit [25] was employed to train 5-gram language models (LMs) with modified Kneser-Ney smoothing [26]. We trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. KenLM [27] was utilized for LM scoring during decoding. Model weights for the log-linear 49 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December"
2014.iwslt-evaluation.6,N13-1073,0,0.135113,"or the rest of the tasks. Our this year’s IWSLT systems were trained using monolingual and parallel data from WIT3 [16], Europarl [17], MultiUN [18], the Gigaword corpora as provided by the Linguistic Data Consortium [19], the German Political Speeches Corpus [20], and the corpora provided for the WMT shared translation task [21]. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [22] and symmetrizing the two alignments with the grow-diagfinal-and heuristic [23, 2]. Word alignments for the SLT track systems were created using fast align [24]. The SRILM toolkit [25] was employed to train 5-gram language models (LMs) with modified Kneser-Ney smoothing [26]. We trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. KenLM [27] was utilized for LM scoring during decoding. Model weights for the log-linear 49 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 model combination [28] were optimized with batch k-best MIRA [29] to maximize B LEU [30]. Where not otherwise stated, the systems were tuned"
2014.iwslt-evaluation.6,W11-2123,0,0.137447,"[20], and the corpora provided for the WMT shared translation task [21]. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [22] and symmetrizing the two alignments with the grow-diagfinal-and heuristic [23, 2]. Word alignments for the SLT track systems were created using fast align [24]. The SRILM toolkit [25] was employed to train 5-gram language models (LMs) with modified Kneser-Ney smoothing [26]. We trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. KenLM [27] was utilized for LM scoring during decoding. Model weights for the log-linear 49 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 model combination [28] were optimized with batch k-best MIRA [29] to maximize B LEU [30]. Where not otherwise stated, the systems were tuned on dev2010. Besides participating in the evaluation campaign with our individual engines, we also collaborated with partners from the EU-BRIDGE project to produce additional joint submissions. The combined systems of the University of Edinburgh, RWTH Aachen Un"
2014.iwslt-evaluation.6,P02-1038,0,0.434172,"with the grow-diagfinal-and heuristic [23, 2]. Word alignments for the SLT track systems were created using fast align [24]. The SRILM toolkit [25] was employed to train 5-gram language models (LMs) with modified Kneser-Ney smoothing [26]. We trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. KenLM [27] was utilized for LM scoring during decoding. Model weights for the log-linear 49 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 model combination [28] were optimized with batch k-best MIRA [29] to maximize B LEU [30]. Where not otherwise stated, the systems were tuned on dev2010. Besides participating in the evaluation campaign with our individual engines, we also collaborated with partners from the EU-BRIDGE project to produce additional joint submissions. The combined systems of the University of Edinburgh, RWTH Aachen University, Karlsruhe Institute of Technology, and Fondazione Bruno Kessler are described in [31]. 2. Spoken Language Translation Edinburgh’s spoken language translation system experiments set out to compare two recent stra"
2014.iwslt-evaluation.6,N12-1047,0,0.264311,"2]. Word alignments for the SLT track systems were created using fast align [24]. The SRILM toolkit [25] was employed to train 5-gram language models (LMs) with modified Kneser-Ney smoothing [26]. We trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. KenLM [27] was utilized for LM scoring during decoding. Model weights for the log-linear 49 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 model combination [28] were optimized with batch k-best MIRA [29] to maximize B LEU [30]. Where not otherwise stated, the systems were tuned on dev2010. Besides participating in the evaluation campaign with our individual engines, we also collaborated with partners from the EU-BRIDGE project to produce additional joint submissions. The combined systems of the University of Edinburgh, RWTH Aachen University, Karlsruhe Institute of Technology, and Fondazione Bruno Kessler are described in [31]. 2. Spoken Language Translation Edinburgh’s spoken language translation system experiments set out to compare two recent strands of research in terms of their performan"
2014.iwslt-evaluation.6,P02-1040,0,0.0952689,"the SLT track systems were created using fast align [24]. The SRILM toolkit [25] was employed to train 5-gram language models (LMs) with modified Kneser-Ney smoothing [26]. We trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. KenLM [27] was utilized for LM scoring during decoding. Model weights for the log-linear 49 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 model combination [28] were optimized with batch k-best MIRA [29] to maximize B LEU [30]. Where not otherwise stated, the systems were tuned on dev2010. Besides participating in the evaluation campaign with our individual engines, we also collaborated with partners from the EU-BRIDGE project to produce additional joint submissions. The combined systems of the University of Edinburgh, RWTH Aachen University, Karlsruhe Institute of Technology, and Fondazione Bruno Kessler are described in [31]. 2. Spoken Language Translation Edinburgh’s spoken language translation system experiments set out to compare two recent strands of research in terms of their performance and their properties"
2014.iwslt-evaluation.6,2014.iwslt-evaluation.7,1,0.877033,"Missing"
2014.iwslt-evaluation.6,P14-1129,0,0.129793,"rlsruhe Institute of Technology, and Fondazione Bruno Kessler are described in [31]. 2. Spoken Language Translation Edinburgh’s spoken language translation system experiments set out to compare two recent strands of research in terms of their performance and their properties in order to understand the contributions of each. The first strand of research is bilingual neural network langauge models. There has recently been a great deal of interest bilingual neural network language models as they have shown strong gains in performance for Arabic→English, and to a lesser extent for Chinese→English [32]. It is still not clear what the exact contribution of the bilinugal language model is, and there is reason to believe that its contribution may be that it allows the SMT model to overcome strong phrase pair independence assumptions. The second strand of research is operation sequence modelling [33, 34]. The integration of the OSM model into phrase-based decoding directly addresses the problem of the phrasal independence assumption by modelling the context of phrase pair translations. We aim to compare these two different approaches and combining them. As we see, combining OSM and the bilingua"
2014.iwslt-evaluation.6,P13-2071,1,0.864533,"he contributions of each. The first strand of research is bilingual neural network langauge models. There has recently been a great deal of interest bilingual neural network language models as they have shown strong gains in performance for Arabic→English, and to a lesser extent for Chinese→English [32]. It is still not clear what the exact contribution of the bilinugal language model is, and there is reason to believe that its contribution may be that it allows the SMT model to overcome strong phrase pair independence assumptions. The second strand of research is operation sequence modelling [33, 34]. The integration of the OSM model into phrase-based decoding directly addresses the problem of the phrasal independence assumption by modelling the context of phrase pair translations. We aim to compare these two different approaches and combining them. As we see, combining OSM and the bilingual NN language model slightly outperforms all other models, including the state-of-the-art OSM model, but only for English→French and only very slightly. 2.1. Baseline For the SLT track, we trained phrase-based models using Moses with mostly default settings. We further included basic sparse features [35"
2014.iwslt-evaluation.6,N13-1001,1,0.914464,"he contributions of each. The first strand of research is bilingual neural network langauge models. There has recently been a great deal of interest bilingual neural network language models as they have shown strong gains in performance for Arabic→English, and to a lesser extent for Chinese→English [32]. It is still not clear what the exact contribution of the bilinugal language model is, and there is reason to believe that its contribution may be that it allows the SMT model to overcome strong phrase pair independence assumptions. The second strand of research is operation sequence modelling [33, 34]. The integration of the OSM model into phrase-based decoding directly addresses the problem of the phrasal independence assumption by modelling the context of phrase pair translations. We aim to compare these two different approaches and combining them. As we see, combining OSM and the bilingual NN language model slightly outperforms all other models, including the state-of-the-art OSM model, but only for English→French and only very slightly. 2.1. Baseline For the SLT track, we trained phrase-based models using Moses with mostly default settings. We further included basic sparse features [35"
2014.iwslt-evaluation.6,N09-1025,0,0.0844804,"34]. The integration of the OSM model into phrase-based decoding directly addresses the problem of the phrasal independence assumption by modelling the context of phrase pair translations. We aim to compare these two different approaches and combining them. As we see, combining OSM and the bilingual NN language model slightly outperforms all other models, including the state-of-the-art OSM model, but only for English→French and only very slightly. 2.1. Baseline For the SLT track, we trained phrase-based models using Moses with mostly default settings. We further included basic sparse features [35] and we used factors. For German→English we used POS tags, morphological tags and lemmas as factors in decoding [11], and for English→German we used POS tags and morphological tags on the target side. Table 1 lists the factors used for the translation model, and the factors over which we trained OSM models. The SLT and the MT systems were trained in a similar fashion, with the main difference being that for SLT no prereordering was performed for German→English as this relies on grammatically correct test sentences, and automatic speech recognition (ASR) output, especially for German, is diffic"
2014.iwslt-evaluation.6,2006.iwslt-papers.1,0,0.165301,"ma l, pos p, morphology m) and the size of the parallel and monolingual training data in millions of words. for WMT, and the LDC Gigaword for French and English. The number of words of training data can be seen in Table 1. 2.2. Monolingual Punctuation Models One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalisation and this is one of the main stylistic differences. Previous research [36, 5] suggests that it is preferrable to punctuate the text before translation, which is what we did by training a monolingual translation system for our two source languages: German and English. The “source language” of the punctuation model has punctuation and capitalisation stripped, and the “target language” is the full original written text. Our handling of punctuation uses a phrase-based translation model with no distortion or reordering, and we tuned the model to the ASR input text (dev2010 for English, and dev2012 for German) using batch MIRA and the B LEU score. After running ASR output th"
2014.iwslt-evaluation.6,D13-1106,0,0.0838934,"help even more when sequence models are applied over more general factors such as POS tags and GIZA++’s mkcls clusters [5]. For this experiment we applied the best OSM settings from last year’s IWSLT experiments which included models over words, lemmas, POS tags, and clusters depending on the language pair. See Table 1 for details. 50 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 2.4. Bilingual Neural Network Language Model There has recently been a great deal of interest in including neural networks in machine translation [37, 38]. There is hope that neural networks provide a way to relax some of the more egregious independence assuptions made in translation models. The challenge with neural networks however, is that they are computationally very expensive, and getting them to operate at scale requires sophisticated efficiency techniques. A recent paper which was able to fully integrate a neural network which includes both source side and target side context in decoding [32], and they managed to show big improvements for a small Arabic→English task, and smaller improvements for a Chinese→English task. We implemented a"
2014.iwslt-evaluation.6,P14-1066,0,0.0858383,"help even more when sequence models are applied over more general factors such as POS tags and GIZA++’s mkcls clusters [5]. For this experiment we applied the best OSM settings from last year’s IWSLT experiments which included models over words, lemmas, POS tags, and clusters depending on the language pair. See Table 1 for details. 50 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 2.4. Bilingual Neural Network Language Model There has recently been a great deal of interest in including neural networks in machine translation [37, 38]. There is hope that neural networks provide a way to relax some of the more egregious independence assuptions made in translation models. The challenge with neural networks however, is that they are computationally very expensive, and getting them to operate at scale requires sophisticated efficiency techniques. A recent paper which was able to fully integrate a neural network which includes both source side and target side context in decoding [32], and they managed to show big improvements for a small Arabic→English task, and smaller improvements for a Chinese→English task. We implemented a"
2014.iwslt-evaluation.6,D13-1140,0,0.162286,"-the-art translation models. We implemented a BiNNLM as a feature function inside Moses, following closely the implementation outlined in [32]. The main focus of our design is to make the Moses specific code flexible and independent of the neural network language model that would be used for scoring. As a result any NNLM could implement the interface and be used by Moses during decoding. Some features such as backoff to POS tag in case of unknown word or use of special < null > token to pad an incomplete parse in the chart decoder are made optional. Currently the implemented backends are NPLM [39] and OxLM [40]. Implementation is available for both phrase based and hierarchical Moses. For our experiments we chose NPLM to be our NNLM backend. We chose it, because it features noise contrastive estimation (NCE) which allows us to avoid having to apply softmax to normalize the outputs, as it is infeasible to do so with large vocabularies. Another benefit of NPLM is that when using NCE and a neural network with one hidden layer we can precompute the values for the first hidden layer of all vocabulary terms, similarly to what [32] do. We also modified the NPLM code a bit and used Magma enabl"
2014.iwslt-evaluation.6,E14-4029,1,0.892491,"Missing"
2014.iwslt-evaluation.6,P12-1049,0,0.0479377,"Missing"
2014.iwslt-evaluation.6,W14-3362,1,0.88321,"cal tags (the latter trained on WIT3 only). Model weights of the phrase-based in-domain system were optimized on dev2010. Syntax-based System. The contrastive 1 system is a string-to-tree translation system with similar features as the ones described in [15]. The target-side data was parsed with BitPar [48], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [49]. Augmenting the system with non-syntactic phrases [50] and adding soft source syntactic constraints [51] yielded further improvements. Model weights of the syntaxbased system were optimized on a concatenation of dev2010 and dev2012. System Combination. We combined the outputs of the phrase-based primary system, the auxiliary phrase-based indomain system, and the string-to-tree syntax-based system with the MT system combination approach implemented in the Jane toolkit [52]. The parameters of the system combination were optimized on tst2012. The consensus translation produced by the system combination (syscom) was submitted as contrastive 2. 53 Pro"
2014.iwslt-evaluation.6,W14-4018,1,0.890764,"weights of the phrase-based in-domain system were optimized on dev2010. Syntax-based System. The contrastive 1 system is a string-to-tree translation system with similar features as the ones described in [15]. The target-side data was parsed with BitPar [48], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [49]. Augmenting the system with non-syntactic phrases [50] and adding soft source syntactic constraints [51] yielded further improvements. Model weights of the syntaxbased system were optimized on a concatenation of dev2010 and dev2012. System Combination. We combined the outputs of the phrase-based primary system, the auxiliary phrase-based indomain system, and the string-to-tree syntax-based system with the MT system combination approach implemented in the Jane toolkit [52]. The parameters of the system combination were optimized on tst2012. The consensus translation produced by the system combination (syscom) was submitted as contrastive 2. 53 Proceedings of the 11th International Workshop on Spo"
2014.iwslt-evaluation.6,E14-2008,1,0.863115,"parate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [49]. Augmenting the system with non-syntactic phrases [50] and adding soft source syntactic constraints [51] yielded further improvements. Model weights of the syntaxbased system were optimized on a concatenation of dev2010 and dev2012. System Combination. We combined the outputs of the phrase-based primary system, the auxiliary phrase-based indomain system, and the string-to-tree syntax-based system with the MT system combination approach implemented in the Jane toolkit [52]. The parameters of the system combination were optimized on tst2012. The consensus translation produced by the system combination (syscom) was submitted as contrastive 2. 53 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 4. Summary The Edinburgh submissions for IWSLT cover many language pairs and research techniques. We have implemented a bilingual neural network language model feature in Moses and have demonstrated that it can lead to state-of-the-art results for English→French. BiNNLM seems less beneficial for German→Engl"
2014.iwslt-evaluation.6,N06-2013,0,0.101422,"luster-ids. This result contradicts our findings in last year IWSLT paper [5] where we reported significant gains using class-based models on many European language pairs with English as source language. Monolingual Arabic Data. Unlike parallel data, adding Gigaword and UN monolingual data in English→Arabic translation task gave significant improvements. The gains are shown in Table 5. 3.3. German→English MT 3.2. Arabic-English MT We carried out a number of experiments for the ArabicEnglish language pair which we now discuss briefly. Tokenization. We used MADA tokenizer for source-side Arabic [43] and tried different segmentation schemes including D*, S2 and ATB. The ATB segmentation consistently outperformed other schemes. For the German→English MT task system, prereordering [46] and compound splitting [47] were applied to the German source language side in a preprocessing step. A factored translation model was employed. Source side factors are word, lemma, POS tag, and morphological tag. Target side factors are word, lemma, and POS tag. Supplementary to the features listed in Section 6, we 52 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, De"
2014.iwslt-evaluation.6,D11-1033,0,0.239047,"Missing"
2014.iwslt-evaluation.6,P05-1066,1,0.888458,"source language. Monolingual Arabic Data. Unlike parallel data, adding Gigaword and UN monolingual data in English→Arabic translation task gave significant improvements. The gains are shown in Table 5. 3.3. German→English MT 3.2. Arabic-English MT We carried out a number of experiments for the ArabicEnglish language pair which we now discuss briefly. Tokenization. We used MADA tokenizer for source-side Arabic [43] and tried different segmentation schemes including D*, S2 and ATB. The ATB segmentation consistently outperformed other schemes. For the German→English MT task system, prereordering [46] and compound splitting [47] were applied to the German source language side in a preprocessing step. A factored translation model was employed. Source side factors are word, lemma, POS tag, and morphological tag. Target side factors are word, lemma, and POS tag. Supplementary to the features listed in Section 6, we 52 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 incorporated two additional LMs into the German→English MT system: a 7-gram LM over POS tags and a 7-gram LM over lemmas (both trained on WIT3 only). Model weight"
2014.iwslt-evaluation.6,E03-1076,1,0.855843,"Arabic Data. Unlike parallel data, adding Gigaword and UN monolingual data in English→Arabic translation task gave significant improvements. The gains are shown in Table 5. 3.3. German→English MT 3.2. Arabic-English MT We carried out a number of experiments for the ArabicEnglish language pair which we now discuss briefly. Tokenization. We used MADA tokenizer for source-side Arabic [43] and tried different segmentation schemes including D*, S2 and ATB. The ATB segmentation consistently outperformed other schemes. For the German→English MT task system, prereordering [46] and compound splitting [47] were applied to the German source language side in a preprocessing step. A factored translation model was employed. Source side factors are word, lemma, POS tag, and morphological tag. Target side factors are word, lemma, and POS tag. Supplementary to the features listed in Section 6, we 52 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 incorporated two additional LMs into the German→English MT system: a 7-gram LM over POS tags and a 7-gram LM over lemmas (both trained on WIT3 only). Model weights were optimized on a concat"
2014.iwslt-evaluation.6,C04-1024,0,0.157242,"t2012 24.9 24.1 24.8 26.0 27.8 26.7 26.5 27.8 23.4 22.2 23.1 24.5 Table 8: Results for the English→German MT task (casesensitive B LEU scores). The contrastive 2 submission is a system combination of three systems which was tuned on tst2012. LM over Brown clusters and a 7-gram LM over morphological tags (the latter trained on WIT3 only). Model weights of the phrase-based in-domain system were optimized on dev2010. Syntax-based System. The contrastive 1 system is a string-to-tree translation system with similar features as the ones described in [15]. The target-side data was parsed with BitPar [48], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [49]. Augmenting the system with non-syntactic phrases [50] and adding soft source syntactic constraints [51] yielded further improvements. Model weights of the syntaxbased system were optimized on a concatenation of dev2010 and dev2012. System Combination. We combined the outputs of the phrase-based primary system, the auxiliary phrase-based indomain system, a"
2014.iwslt-evaluation.6,J03-1002,0,\N,Missing
2014.iwslt-evaluation.6,2011.iwslt-evaluation.18,0,\N,Missing
2014.iwslt-evaluation.7,D14-1003,1,0.921764,"slation of spoken language. The IWSLT TED talks task constitutes an interesting framework for empirical testing of some of the systems for spoken language translation which are developed as part of the project. In this work, we describe the EU-BRIDGE submissions to the 2014 IWSLT translation task. This year, we combined several single systems of RWTH, UEDIN, KIT, and FBK for the German→English SLT, German→English MT, English→German MT, and English→French MT tasks. Additionally to the standard system combination pipeline presented in [1, 2], we applied a recurrent neural network rescoring step [3] for the English→French MT task. Similar cooperative approaches based on system combination have proven to be valuable for machine translation in previous joint submissions, e.g. [4, 5]. 2. RWTH Aachen University RWTH applied the identical training pipeline and models on both language pairs: The state-of-the-art phrase-based baseline systems were augmented with a hierarchical reordering model, several additional language models (LMs) and maximum expected B LEU training for phrasal, lexical and reordering models. Further, RWTH employed rescoring with novel recurrent neural language and translat"
2014.iwslt-evaluation.7,W10-1738,1,0.885248,"and maximum expected B LEU training for phrasal, lexical and reordering models. Further, RWTH employed rescoring with novel recurrent neural language and translation models. The same systems were used for the SLT track, where RWTH ad57 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 ditionally performed punctuation prediction on the automatic transcriptions employing hierarchical phrase-based translation. Both the phrase-based and the hierarchical decoder are implemented in RWTH’s publicly available translation toolkit Jane [6, 7]. The model weights of all systems were tuned with standard Minimum Error Rate Training [8] on the provided dev2012 set. RWTH used B LEU as optimization objective. For the German→English translation direction, in a preprocessing step the German source was decompounded [9] and part-of-speech-based long-range verb reordering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-"
2014.iwslt-evaluation.7,P03-1021,0,0.488353,"employed rescoring with novel recurrent neural language and translation models. The same systems were used for the SLT track, where RWTH ad57 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 ditionally performed punctuation prediction on the automatic transcriptions employing hierarchical phrase-based translation. Both the phrase-based and the hierarchical decoder are implemented in RWTH’s publicly available translation toolkit Jane [6, 7]. The model weights of all systems were tuned with standard Minimum Error Rate Training [8] on the provided dev2012 set. RWTH used B LEU as optimization objective. For the German→English translation direction, in a preprocessing step the German source was decompounded [9] and part-of-speech-based long-range verb reordering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-gram word class language model (wcLM). All of them used interpolated Kneser-Ney smoothing."
2014.iwslt-evaluation.7,popovic-ney-2006-pos,1,0.798687,"th and 5th, 2014 ditionally performed punctuation prediction on the automatic transcriptions employing hierarchical phrase-based translation. Both the phrase-based and the hierarchical decoder are implemented in RWTH’s publicly available translation toolkit Jane [6, 7]. The model weights of all systems were tuned with standard Minimum Error Rate Training [8] on the provided dev2012 set. RWTH used B LEU as optimization objective. For the German→English translation direction, in a preprocessing step the German source was decompounded [9] and part-of-speech-based long-range verb reordering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-gram word class language model (wcLM). All of them used interpolated Kneser-Ney smoothing. For the general domain LM, RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selectio"
2014.iwslt-evaluation.7,P13-2121,1,0.819366,"mplemented in RWTH’s publicly available translation toolkit Jane [6, 7]. The model weights of all systems were tuned with standard Minimum Error Rate Training [8] on the provided dev2012 set. RWTH used B LEU as optimization objective. For the German→English translation direction, in a preprocessing step the German source was decompounded [9] and part-of-speech-based long-range verb reordering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-gram word class language model (wcLM). All of them used interpolated Kneser-Ney smoothing. For the general domain LM, RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWT"
2014.iwslt-evaluation.7,P10-2041,0,0.0916594,"rdering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-gram word class language model (wcLM). All of them used interpolated Kneser-Ney smoothing. For the general domain LM, RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWTH trained 200 classes on the target side of the bilingual training data using an in-house tool similar to mkcls [14]. With these class definitions, RWTH applied the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU ob"
2014.iwslt-evaluation.7,E99-1010,0,0.0737032,"them used interpolated Kneser-Ney smoothing. For the general domain LM, RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWTH trained 200 classes on the target side of the bilingual training data using an in-house tool similar to mkcls [14]. With these class definitions, RWTH applied the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for"
2014.iwslt-evaluation.7,D13-1138,1,0.85854,"RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWTH trained 200 classes on the target side of the bilingual training data using an in-house tool similar to mkcls [14]. With these class definitions, RWTH applied the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for a gradient based update method. Leave-oneout [17] was applied to circumv"
2014.iwslt-evaluation.7,P12-1031,0,0.0125863,"lection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWTH trained 200 classes on the target side of the bilingual training data using an in-house tool similar to mkcls [14]. With these class definitions, RWTH applied the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for a gradient based update method. Leave-oneout [17] was applied to circumvent over-fitting. Here, RWTH followed an approach similar to [18], where each feature type was condensed into a single feature for the log-linear model combination. In the first pass, RWTH trained phrase pair and"
2014.iwslt-evaluation.7,P10-1049,1,0.833909,"the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for a gradient based update method. Leave-oneout [17] was applied to circumvent over-fitting. Here, RWTH followed an approach similar to [18], where each feature type was condensed into a single feature for the log-linear model combination. In the first pass, RWTH trained phrase pair and phrase-internal word pair features, and in the second pass a hierarchical reordering model, resulting altogether in an additional eight models for log-linear combination. Recurrent Neural Network Models All systems applied rescoring on 1000-best lists using recurrent language and translation models. The recurrency was handled with the long short-term memory (LST"
2014.iwslt-evaluation.7,D14-1132,0,0.157332,"M. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for a gradient based update method. Leave-oneout [17] was applied to circumvent over-fitting. Here, RWTH followed an approach similar to [18], where each feature type was condensed into a single feature for the log-linear model combination. In the first pass, RWTH trained phrase pair and phrase-internal word pair features, and in the second pass a hierarchical reordering model, resulting altogether in an additional eight models for log-linear combination. Recurrent Neural Network Models All systems applied rescoring on 1000-best lists using recurrent language and translation models. The recurrency was handled with the long short-term memory (LSTM) architecture [19] and RWTH used a class-factored output layer for increased efficienc"
2014.iwslt-evaluation.7,2011.iwslt-papers.7,1,0.944851,"ort-term memory (LSTM) architecture [19] and RWTH used a class-factored output layer for increased efficiency as described in [20]. All neural networks were trained on the TED portion of the data with 2000 word classes. In addition to the recurrent language model (RNN-LM), RWTH applied the deep bidirectional word-based translation model (RNN-BTM) described in [3], which is capable of taking the full source context into account for each translation decision. Spoken Language Translation For the SLT task, RWTH reintroduced punctuation and case information before the actual translation similar to [21]. However, RWTH employed a hierarchical phrase-based system with a maximum of one nonterminal symbol per rule in place of a phrase-based system. A punctuation prediction system based on hierarchical translation is able to capture long-range dependencies between words and punctuation marks and is more robust for unseen word sequences. The model weights are tuned with standard MERT on 100best lists. As optimization criterion RWTH used F2 -Score rather than B LEU or W ER. More details can be found in [22]. Since punctuation predicting and recasing were applied before the actual translation, the f"
2014.iwslt-evaluation.7,2014.iwslt-papers.17,1,0.734908,"RWTH reintroduced punctuation and case information before the actual translation similar to [21]. However, RWTH employed a hierarchical phrase-based system with a maximum of one nonterminal symbol per rule in place of a phrase-based system. A punctuation prediction system based on hierarchical translation is able to capture long-range dependencies between words and punctuation marks and is more robust for unseen word sequences. The model weights are tuned with standard MERT on 100best lists. As optimization criterion RWTH used F2 -Score rather than B LEU or W ER. More details can be found in [22]. Since punctuation predicting and recasing were applied before the actual translation, the final translation systems from the MT track could be kept completely unchanged. 3. University of Edinburgh The UEDIN translation engines [23] are based on the open source Moses toolkit [24]. UEDIN set up phrase-based systems for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided"
2014.iwslt-evaluation.7,P07-2045,1,0.0190208,"n hierarchical translation is able to capture long-range dependencies between words and punctuation marks and is more robust for unseen word sequences. The model weights are tuned with standard MERT on 100best lists. As optimization criterion RWTH used F2 -Score rather than B LEU or W ER. More details can be found in [22]. Since punctuation predicting and recasing were applied before the actual translation, the final translation systems from the MT track could be kept completely unchanged. 3. University of Edinburgh The UEDIN translation engines [23] are based on the open source Moses toolkit [24]. UEDIN set up phrase-based systems for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided by the Linguistic Data Consortium, the German Political Speeches Corpus, and the Common Crawl, 109 , and News Commentary corpora from the WMT shared task training data. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [26]"
2014.iwslt-evaluation.7,N04-1035,0,0.0565459,"equences. The model weights are tuned with standard MERT on 100best lists. As optimization criterion RWTH used F2 -Score rather than B LEU or W ER. More details can be found in [22]. Since punctuation predicting and recasing were applied before the actual translation, the final translation systems from the MT track could be kept completely unchanged. 3. University of Edinburgh The UEDIN translation engines [23] are based on the open source Moses toolkit [24]. UEDIN set up phrase-based systems for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided by the Linguistic Data Consortium, the German Political Speeches Corpus, and the Common Crawl, 109 , and News Commentary corpora from the WMT shared task training data. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [26] and symmetrizing the two trained alignments. Word alignments for the SLT track system were created using fast align [27]. The SRILM toolkit [2"
2014.iwslt-evaluation.7,W08-0509,0,0.192359,"[24]. UEDIN set up phrase-based systems for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided by the Linguistic Data Consortium, the German Political Speeches Corpus, and the Common Crawl, 109 , and News Commentary corpora from the WMT shared task training data. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [26] and symmetrizing the two trained alignments. Word alignments for the SLT track system were created using fast align [27]. The SRILM toolkit [28] was employed to train 5-gram LMs with modified Kneser-Ney smoothing [29]. UEDIN trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six"
2014.iwslt-evaluation.7,N13-1073,0,0.0453396,"e syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided by the Linguistic Data Consortium, the German Political Speeches Corpus, and the Common Crawl, 109 , and News Commentary corpora from the WMT shared task training data. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [26] and symmetrizing the two trained alignments. Word alignments for the SLT track system were created using fast align [27]. The SRILM toolkit [28] was employed to train 5-gram LMs with modified Kneser-Ney smoothing [29]. UEDIN trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sp"
2014.iwslt-evaluation.7,C14-1041,1,0.839592,"ndividual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable"
2014.iwslt-evaluation.7,N12-1047,0,0.0681194,"them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable to punctuate the text before translation, which is"
2014.iwslt-evaluation.7,P02-1040,0,0.0918061,"d to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable to punctuate the text before translation, which is what UEDIN did by trai"
2014.iwslt-evaluation.7,2006.iwslt-papers.1,1,0.862433,"els over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable to punctuate the text before translation, which is what UEDIN did by training a translation system on the German side of the parallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT tas"
2014.iwslt-evaluation.7,2012.iwslt-papers.15,1,0.927241,"els over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable to punctuate the text before translation, which is what UEDIN did by training a translation system on the German side of the parallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT tas"
2014.iwslt-evaluation.7,P05-1066,1,0.733044,"ferrable to punctuate the text before translation, which is what UEDIN did by training a translation system on the German side of the parallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT task system, prereordering [37] and compound splitting [38] were applied to the German source language side in a preprocessing step. A factored translation model [39] was employed. Source side factors are word, lemma, part-of-speech (POS) tag, and morphological tag. Target side factors are word, lemma, and POS tag. UEDIN incorporated two additional LMs into the German→English MT system: a 7-gram LM over POS tags (trained on WIT3 only) and a 7-gram LM over lemmas (trained on WIT3 only). Model weights were optimized on a concatenation of dev2010 and dev2012. English→French MT UEDIN contributed two phrase-based systems for the"
2014.iwslt-evaluation.7,E03-1076,1,0.858704,"xt before translation, which is what UEDIN did by training a translation system on the German side of the parallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT task system, prereordering [37] and compound splitting [38] were applied to the German source language side in a preprocessing step. A factored translation model [39] was employed. Source side factors are word, lemma, part-of-speech (POS) tag, and morphological tag. Target side factors are word, lemma, and POS tag. UEDIN incorporated two additional LMs into the German→English MT system: a 7-gram LM over POS tags (trained on WIT3 only) and a 7-gram LM over lemmas (trained on WIT3 only). Model weights were optimized on a concatenation of dev2010 and dev2012. English→French MT UEDIN contributed two phrase-based systems for the English→French EU-BRIDGE sy"
2014.iwslt-evaluation.7,2012.amta-papers.9,1,0.84942,"arallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT task system, prereordering [37] and compound splitting [38] were applied to the German source language side in a preprocessing step. A factored translation model [39] was employed. Source side factors are word, lemma, part-of-speech (POS) tag, and morphological tag. Target side factors are word, lemma, and POS tag. UEDIN incorporated two additional LMs into the German→English MT system: a 7-gram LM over POS tags (trained on WIT3 only) and a 7-gram LM over lemmas (trained on WIT3 only). Model weights were optimized on a concatenation of dev2010 and dev2012. English→French MT UEDIN contributed two phrase-based systems for the English→French EU-BRIDGE system combination. Both comprise Brown clusters with 200 classes as additional factors on source and target"
2014.iwslt-evaluation.7,D08-1089,0,0.176922,"ign [27]. The SRILM toolkit [28] was employed to train 5-gram LMs with modified Kneser-Ney smoothing [29]. UEDIN trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation a"
2014.iwslt-evaluation.7,W14-3324,1,0.784121,"ical tag. UEDIN-A was trained with all corpora, whereas for UEDIN-B the parallel training data was restricted to the indomain WIT3 corpus. Additional features of the systems are: a 5-gram LM over Brown clusters, a 7-gram LM over morphological tags (UEDIN-A: trained on all data, UEDIN-B: trained on WIT3 only), and a 7-gram LM over POS tags (UEDIN-A, not UEDIN-B). Model weights of UEDIN-B were optimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house ph"
2014.iwslt-evaluation.7,2012.iwslt-papers.17,1,0.881764,"ain 5-gram LMs with modified Kneser-Ney smoothing [29]. UEDIN trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic diff"
2014.iwslt-evaluation.7,C04-1024,0,0.0400394,"ereas for UEDIN-B the parallel training data was restricted to the indomain WIT3 corpus. Additional features of the systems are: a 5-gram LM over Brown clusters, a 7-gram LM over morphological tags (UEDIN-A: trained on all data, UEDIN-B: trained on WIT3 only), and a 7-gram LM over POS tags (UEDIN-A, not UEDIN-B). Model weights of UEDIN-B were optimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models wer"
2014.iwslt-evaluation.7,2011.iwslt-evaluation.18,1,0.873679,"ined on WIT3 only), and a 7-gram LM over POS tags (UEDIN-A, not UEDIN-B). Model weights of UEDIN-B were optimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for all directions, as well as on the additional monolingual training data. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standa"
2014.iwslt-evaluation.7,W14-3362,1,0.610881,"N-A, not UEDIN-B). Model weights of UEDIN-B were optimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for all directions, as well as on the additional monolingual training data. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for"
2014.iwslt-evaluation.7,W14-4018,1,0.774295,"ptimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for all directions, as well as on the additional monolingual training data. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In t"
2014.iwslt-evaluation.7,2011.iwslt-evaluation.9,1,0.861968,"m with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for all directions, as well as on the additional monolingual training data. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In the SLT task, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were"
2014.iwslt-evaluation.7,2007.tmi-papers.21,0,0.0614729,"ta. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In the SLT task, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabiliti"
2014.iwslt-evaluation.7,W09-0413,1,0.842557,"pora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In the SLT task, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the tra"
2014.iwslt-evaluation.7,W13-0805,1,0.85195,"ifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In the SLT task, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual langu"
2014.iwslt-evaluation.7,W08-1006,0,0.0150981,"k, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [54] and a discriminative word lexicon using source context features [55]. During decoding, KIT used several LMs to adapt the system to the ta"
2014.iwslt-evaluation.7,2012.amta-papers.19,1,0.839901,"e rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [54] and a discriminative word lexicon using source context features [55]. During decoding, KIT used several LMs to adapt the system to the task and to better model the sentence structure using a class-based LM. For the German→English task, KIT used one LM trained on all data, an in-domain LM trained only on the WIT3 corpus, and one LM trained on 5M sentences selected using cross-entropy difference [13]. As classes KIT used the clusters obtained using the mkcls algorithm on the WI"
2014.iwslt-evaluation.7,W11-2124,1,0.902739,"for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [54] and a discriminative word lexicon using source context features [55]. During decoding, KIT used several LMs to adapt the system to the task and to better model the sentence structure using a class-based LM. For the German→English task, KIT used one LM trained on all data, an in-domain LM trained only on the WIT3 corpus, and one LM trained on 5M sentences selected using cross-entropy difference [13]. As classes KIT used the clusters obtained using the mkcls algorithm on the WIT3 corpus. For German↔ English, KIT used a 9-gram LM with 100 or 1000 clusters and for the English→French MT task, a cl"
2014.iwslt-evaluation.7,W13-2264,1,0.835602,"ed by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [54] and a discriminative word lexicon using source context features [55]. During decoding, KIT used several LMs to adapt the system to the task and to better model the sentence structure using a class-based LM. For the German→English task, KIT used one LM trained on all data, an in-domain LM trained only on the WIT3 corpus, and one LM trained on 5M sentences selected using cross-entropy difference [13]. As classes KIT used the clusters obtained using the mkcls algorithm on the WIT3 corpus. For German↔ English, KIT used a 9-gram LM with 100 or 1000 clusters and for the English→French MT task, a cluster-based 4-gram LM was trained on 500 clusters. For English→German"
2014.iwslt-evaluation.7,2012.eamt-1.60,1,0.892622,"Missing"
2014.iwslt-evaluation.7,D11-1033,0,0.167316,"Missing"
2014.iwslt-evaluation.7,W05-0909,0,0.085167,"m multiple hypotheses which are outputs of different translation engines. The consensus translations can be better in terms of translation quality than any of the individual hypotheses. To combine the engines of the project partners for the EU-BRIDGE joint setups, we applied a system combination implementation that has been developed at RWTH Aachen University [1]. In Fig. 1 an overview is illustrated. We first address the generation of a confusion network (CN) from I input translations. For that we need a pairwise alignment between all input hypotheses. This alignment is calculated via METEOR [60]. The hypotheses are then reordered to match the word order of a selected skeleton hypothesis. Instead of using only one of the input hypothesis as skeleton, we generate I different CNs, each having one of the input systems as skeleton. The final lattice is the union of all I previous generated CNs. In Fig. 2 an example confusion network of I = 4 input translations with one skeleton translation is illustrated. Between two adjacent nodes, we always have a choice between the I different system output words. The confusion network decoding step involves determining the shortest path through the ne"
2014.iwslt-evaluation.7,2006.amta-papers.25,0,0.0356913,"andard set of models is a word penalty, a 3-gram language model trained on the input hypotheses, and for each system one binary voting feature. During decoding the binary voting feature for system i (1 ≤ i ≤ I) is 1 iff the word is from system i, otherwise 0. The M different model weights λm are trained with MERT [8]. the red cab the a a red blue green train car car Figure 2: System A: the red cab ; System B: the red train ; System C: a blue car ; System D: a green car ; Reference: the blue car . 7. Results In this section, we present our experimental results. All reported B LEU [34] and T ER [61] scores are case-sensitive with one reference. All system combination results have been generated with RWTH’s open source system combination implementation Jane [1]. German→English SLT For the German→English SLT task, we combined three different individual systems generated by UEDIN, KIT, and RWTH. Experimental results are given in Table 1. The final system combination yields improvements of 1.5 points in B LEU and 1.2 points in T ER compared to the best single system (KIT). All single systems as well as the system combination parameters were tuned on dev2012. For this year’s IWSLT SLT track,"
2014.iwslt-evaluation.7,E06-1005,1,\N,Missing
2014.iwslt-evaluation.7,P11-1105,1,\N,Missing
2014.iwslt-evaluation.7,W10-1711,1,\N,Missing
2014.iwslt-evaluation.7,2010.iwslt-evaluation.22,1,\N,Missing
2014.iwslt-evaluation.7,E14-2008,1,\N,Missing
2014.iwslt-evaluation.7,2014.iwslt-evaluation.6,1,\N,Missing
2014.iwslt-evaluation.7,J03-1002,1,\N,Missing
2014.iwslt-evaluation.7,C12-3061,1,\N,Missing
2014.iwslt-evaluation.7,2013.iwslt-evaluation.16,1,\N,Missing
2014.iwslt-evaluation.7,W14-3310,1,\N,Missing
2015.iwslt-evaluation.4,eisele-chen-2010-multiun,0,\N,Missing
2015.iwslt-evaluation.4,N04-1035,0,\N,Missing
2015.iwslt-evaluation.4,2014.iwslt-evaluation.7,1,\N,Missing
2015.iwslt-evaluation.4,W96-0213,0,\N,Missing
2015.iwslt-evaluation.4,C04-1024,0,\N,Missing
2015.iwslt-evaluation.4,J93-2003,0,\N,Missing
2015.iwslt-evaluation.4,E99-1010,0,\N,Missing
2015.iwslt-evaluation.4,D08-1089,0,\N,Missing
2015.iwslt-evaluation.4,W13-2221,0,\N,Missing
2015.iwslt-evaluation.4,P02-1040,0,\N,Missing
2015.iwslt-evaluation.4,W14-3362,1,\N,Missing
2015.iwslt-evaluation.4,D07-1091,0,\N,Missing
2015.iwslt-evaluation.4,W06-1607,0,\N,Missing
2015.iwslt-evaluation.4,P07-2045,1,\N,Missing
2015.iwslt-evaluation.4,W08-0336,0,\N,Missing
2015.iwslt-evaluation.4,N04-1022,0,\N,Missing
2015.iwslt-evaluation.4,E14-2008,1,\N,Missing
2015.iwslt-evaluation.4,N03-1017,0,\N,Missing
2015.iwslt-evaluation.4,N13-1001,0,\N,Missing
2015.iwslt-evaluation.4,P02-1038,0,\N,Missing
2015.iwslt-evaluation.4,2014.iwslt-evaluation.6,1,\N,Missing
2015.iwslt-evaluation.4,J03-1002,0,\N,Missing
2015.iwslt-evaluation.4,W13-0804,1,\N,Missing
2015.iwslt-evaluation.4,D14-1132,0,\N,Missing
2015.iwslt-evaluation.4,P06-1121,0,\N,Missing
2015.iwslt-evaluation.4,D07-1079,0,\N,Missing
2015.iwslt-evaluation.4,J10-2004,0,\N,Missing
2015.iwslt-evaluation.4,2013.iwslt-evaluation.16,1,\N,Missing
2015.iwslt-evaluation.4,2005.mtsummit-papers.11,0,\N,Missing
2015.iwslt-evaluation.4,W14-3310,1,\N,Missing
2015.iwslt-evaluation.4,W15-3001,1,\N,Missing
2015.iwslt-evaluation.4,2011.iwslt-evaluation.18,0,\N,Missing
2015.iwslt-evaluation.4,J07-2003,0,\N,Missing
2015.iwslt-evaluation.4,W15-3013,1,\N,Missing
2015.iwslt-evaluation.4,W14-4018,1,\N,Missing
2015.iwslt-evaluation.4,tiedemann-2012-parallel,0,\N,Missing
2015.iwslt-evaluation.4,W12-3150,0,\N,Missing
2015.iwslt-evaluation.4,D07-1078,0,\N,Missing
2015.iwslt-evaluation.4,W08-0509,0,\N,Missing
2015.iwslt-evaluation.4,D08-1076,0,\N,Missing
2015.iwslt-evaluation.4,W11-2107,0,\N,Missing
2015.iwslt-evaluation.4,W11-2123,0,\N,Missing
2015.iwslt-evaluation.4,N12-1047,0,\N,Missing
2015.iwslt-evaluation.4,W14-3324,1,\N,Missing
2015.iwslt-evaluation.4,2012.eamt-1.60,0,\N,Missing
2015.iwslt-evaluation.4,N13-1003,0,\N,Missing
2015.iwslt-evaluation.4,N15-1175,0,\N,Missing
2015.mtsummit-papers.19,D11-1033,0,0.07366,"Missing"
2015.mtsummit-papers.19,2010.amta-papers.16,0,0.133002,"Missing"
2015.mtsummit-papers.19,J96-1002,0,0.0774689,"f reserving the in-domain devel2 http://www.statmt.org/wmt15/translation-task.html Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 243 opment sets for tuning source LM interpolation weights. Furthermore, we argue that source-side scores of the interpolated LMs employed in our classifier may to some extent resemble targetside scores of the interpolated LMs which are applied in the respective domain-adapted SMT systems. 5.2 Maximum Entropy Classifiers Maximum entropy text classifiers can utilize a larger number of features in order to predict the label (Berger et al., 1996). We incorporate features from single words, pairs of adjacent words, the first word of the sentence, and the last word of the sentence. The model is trained with L-BFGS (Byrd et al., 1995) and regularized using a Gaussian prior. We build maximum entropy (ME) classifiers under two different training conditions: using the MT development sets (which are rather small) as training data, and using selected other corpora as training data (which might not always exactly match what is defined as in-domain to the MT systems, as the development sets essentially constitute the domains). In a further flav"
2015.mtsummit-papers.19,W09-0432,0,0.0237446,"ch and Ney, 2002) on an in-domain development set (Pecina et al., 2012). • Model combination (of language models, translation models, or reordering models) via interpolation or other schemes, e.g. phrase table fill-up (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Nakov, 2008; Bisazza et al., 2011; Niehues and Waibel, 2012; Chen et al., 2013). • Data selection (Moore and Lewis, 2010; Axelrod et al., 2011). • Instance weighting (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012). • Further exploitation of in-domain monolingual data (Ueffing et al., 2007; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009; Lambert et al., 2011). • Domain-specific features, e.g. binary features indicating the provenance of phrase pairs as implemented in the open-source Moses toolkit (Durrani et al., 2013b) or “domain augmentation” (Clark et al., 2012). However, few authors have tackled the question of how to benefit from domain adaptation in scenarios where a domain label of the input is not present. An important aspect of our approach to multi-domain MT is the need for domain classification. Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 2"
2015.mtsummit-papers.19,2011.iwslt-evaluation.18,0,0.0660573,"d English→Greek language pairs using training corpora of diverse origin, totalling tens of millions of parallel sentences. 2 Related Work A significant amount of research on domain adaptation for SMT has been conducted in recent years. Some methods which are commonly used are: • Tuning of the decoder model weights (Och and Ney, 2002) on an in-domain development set (Pecina et al., 2012). • Model combination (of language models, translation models, or reordering models) via interpolation or other schemes, e.g. phrase table fill-up (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Nakov, 2008; Bisazza et al., 2011; Niehues and Waibel, 2012; Chen et al., 2013). • Data selection (Moore and Lewis, 2010; Axelrod et al., 2011). • Instance weighting (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012). • Further exploitation of in-domain monolingual data (Ueffing et al., 2007; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009; Lambert et al., 2011). • Domain-specific features, e.g. binary features indicating the provenance of phrase pairs as implemented in the open-source Moses toolkit (Durrani et al., 2013b) or “domain augmentation” (Clark et al., 2012). However, fe"
2015.mtsummit-papers.19,2012.eamt-1.60,0,0.144845,"Missing"
2015.mtsummit-papers.19,2013.iwslt-evaluation.1,0,0.0173438,"e than any of the domain-adapted systems on two out of four language pairs (En→De: +0.3; En→It: -0.2; En→Pt: +0.9; En→El: -0.2). Apart from English→Portuguese, the differences are small. Multi-domain SMT clearly outperforms mixed-domain SMT for English→Portuguese (up to +1.0 on all) and English→Greek (up to +0.6 on all). The choice of the domain classifier barely matters wrt. translation quality. Due to its compact model, the MEdev classifier would for instance be a reasonable choice despite not providing the highest classification accuracy. 8 MT English→Italian: +4.3 points B LEU on tst2013 (Cettolo et al., 2013). MT English→Portuguese: +2.8 points B LEU on tst2014 (Cettolo et al., 2014). 9 http://matrix.statmt.org Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 249 20 15 15 10 10 5 5 German 26 .3 26 BLEU 21 .9 21 .7 21 .9 22 .7 22 .5 22 .5 25 20 0 .2 26 .8 26 .8 8 30 28 . .1 .0 27 .2 27 .2 27 .0 27 .7 27 25 BLEU 25 .6 30 25 30 6 35 32 35 .6 32 . 40 .0 40 .8 Mixed-domain-tuned Multi-domain Oracle-domain 31 TED-tuned Europarl-tuned News-tuned Italian 0 Portuguese Greek Figure 1: B LEU scores on a concatenation of all test sets. Compared to oracle-domain SMT, wh"
2015.mtsummit-papers.19,N13-1114,0,0.0120964,"rpora of diverse origin, totalling tens of millions of parallel sentences. 2 Related Work A significant amount of research on domain adaptation for SMT has been conducted in recent years. Some methods which are commonly used are: • Tuning of the decoder model weights (Och and Ney, 2002) on an in-domain development set (Pecina et al., 2012). • Model combination (of language models, translation models, or reordering models) via interpolation or other schemes, e.g. phrase table fill-up (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Nakov, 2008; Bisazza et al., 2011; Niehues and Waibel, 2012; Chen et al., 2013). • Data selection (Moore and Lewis, 2010; Axelrod et al., 2011). • Instance weighting (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012). • Further exploitation of in-domain monolingual data (Ueffing et al., 2007; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009; Lambert et al., 2011). • Domain-specific features, e.g. binary features indicating the provenance of phrase pairs as implemented in the open-source Moses toolkit (Durrani et al., 2013b) or “domain augmentation” (Clark et al., 2012). However, few authors have tackled the question of how to"
2015.mtsummit-papers.19,N12-1047,0,0.0462838,"ty and word penalty. • Distance-based distortion cost. • A hierarchical lexicalized reordering model (Galley and Manning, 2008). • A 5-gram operation sequence model (Durrani et al., 2013a). • Seven binary features indicating absolute occurrence count classes of phrase pairs. • Sparse phrase length features. • Sparse lexical features for the top 200 words. • A 5-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). We discard singleton n-grams of order three and higher. Feature weights are optimized to maximize B LEU (Papineni et al., 2002) with batch MIRA (Cherry and Foster, 2012) on 1000-best lists. We prune the phrase table to a maximum of 100 5 http://dumps.wikimedia.org 6 https://github.com/bwbaugh/wikipedia-extractor Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 245 best translation options per distinct source side and apply a minimum score threshold of 0.0001 on the source-to-target phrase translation probability. We use cube pruning in decoding. Pop limit and stack limit are set to 1000 for tuning and to 5000 for testing. We disallow reordering over punctuation. Furthermore, Minimum Bayes Risk decoding is employed for"
2015.mtsummit-papers.19,2012.amta-papers.4,0,0.040486,"akov, 2008; Bisazza et al., 2011; Niehues and Waibel, 2012; Chen et al., 2013). • Data selection (Moore and Lewis, 2010; Axelrod et al., 2011). • Instance weighting (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012). • Further exploitation of in-domain monolingual data (Ueffing et al., 2007; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009; Lambert et al., 2011). • Domain-specific features, e.g. binary features indicating the provenance of phrase pairs as implemented in the open-source Moses toolkit (Durrani et al., 2013b) or “domain augmentation” (Clark et al., 2012). However, few authors have tackled the question of how to benefit from domain adaptation in scenarios where a domain label of the input is not present. An important aspect of our approach to multi-domain MT is the need for domain classification. Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 241 Xu et al. (2007) perform domain classification for a Chinese→English task. The domains are newswire and newsgroup. The classifiers operate on whole documents rather than on individual sentences. The authors propose two techniques for domain classification. Th"
2015.mtsummit-papers.19,N13-1001,0,0.1814,"ster and Kuhn, 2007; Koehn and Schroeder, 2007; Nakov, 2008; Bisazza et al., 2011; Niehues and Waibel, 2012; Chen et al., 2013). • Data selection (Moore and Lewis, 2010; Axelrod et al., 2011). • Instance weighting (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012). • Further exploitation of in-domain monolingual data (Ueffing et al., 2007; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009; Lambert et al., 2011). • Domain-specific features, e.g. binary features indicating the provenance of phrase pairs as implemented in the open-source Moses toolkit (Durrani et al., 2013b) or “domain augmentation” (Clark et al., 2012). However, few authors have tackled the question of how to benefit from domain adaptation in scenarios where a domain label of the input is not present. An important aspect of our approach to multi-domain MT is the need for domain classification. Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 241 Xu et al. (2007) perform domain classification for a Chinese→English task. The domains are newswire and newsgroup. The classifiers operate on whole documents rather than on individual sentences. The authors prop"
2015.mtsummit-papers.19,W13-2212,1,0.871039,"Missing"
2015.mtsummit-papers.19,2011.iwslt-evaluation.1,0,0.0159256,"on political matters from parliamentary proceedings. News texts are written news articles. TED talks, Europarl, and News could be described as “genres”. We denote them as domains throughout this paper because the term “domain” is well established in related machine translation research literature and often used in a broad sense. TED talks, Europarl, and News have been highly relevant domains in recent machine translation research. The International Workshop on Spoken Language Translation1 (IWSLT) hosts a yearly open evaluation campaign which focuses on the translation of TED talks since 2011 (Federico et al., 2011). The European Parliament Proceedings Parallel Corpus (Koehn, 2005) has been an influential resource for machine translation research ever since its first release over 1 http://www.iwslt.org Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 242 a decade ago. It is freely available and includes parallel text for 21 European languages. Test sets and training data that enables research on machine translation of texts from the News domain have regularly been released for the shared translation task of the Workshop on Statistical Machine Translation2 (WMT). T"
2015.mtsummit-papers.19,D10-1044,0,0.0372183,"in adaptation for SMT has been conducted in recent years. Some methods which are commonly used are: • Tuning of the decoder model weights (Och and Ney, 2002) on an in-domain development set (Pecina et al., 2012). • Model combination (of language models, translation models, or reordering models) via interpolation or other schemes, e.g. phrase table fill-up (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Nakov, 2008; Bisazza et al., 2011; Niehues and Waibel, 2012; Chen et al., 2013). • Data selection (Moore and Lewis, 2010; Axelrod et al., 2011). • Instance weighting (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012). • Further exploitation of in-domain monolingual data (Ueffing et al., 2007; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009; Lambert et al., 2011). • Domain-specific features, e.g. binary features indicating the provenance of phrase pairs as implemented in the open-source Moses toolkit (Durrani et al., 2013b) or “domain augmentation” (Clark et al., 2012). However, few authors have tackled the question of how to benefit from domain adaptation in scenarios where a domain label of the input is not present. An important aspect of our approach t"
2015.mtsummit-papers.19,W07-0717,0,0.047497,"on the English→German, English→Italian, English→Portuguese, and English→Greek language pairs using training corpora of diverse origin, totalling tens of millions of parallel sentences. 2 Related Work A significant amount of research on domain adaptation for SMT has been conducted in recent years. Some methods which are commonly used are: • Tuning of the decoder model weights (Och and Ney, 2002) on an in-domain development set (Pecina et al., 2012). • Model combination (of language models, translation models, or reordering models) via interpolation or other schemes, e.g. phrase table fill-up (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Nakov, 2008; Bisazza et al., 2011; Niehues and Waibel, 2012; Chen et al., 2013). • Data selection (Moore and Lewis, 2010; Axelrod et al., 2011). • Instance weighting (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012). • Further exploitation of in-domain monolingual data (Ueffing et al., 2007; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009; Lambert et al., 2011). • Domain-specific features, e.g. binary features indicating the provenance of phrase pairs as implemented in the open-source Moses toolkit (Durrani et al., 201"
2015.mtsummit-papers.19,W06-1607,0,0.0720378,"the News Crawl corpora provided for the WMT 2015 shared translation task. Plain text was obtained from the Wikipedia XML dumps with the Wikipedia Extractor6 tool. Statistics of the additional monolingual training corpora are presented in Table 2. 6.2 Machine Translation Systems Word alignments are created by aligning the data in both directions and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). We extract phrases up to a maximum length of five. The MT systems comprise these features: • Phrase translation log-probabilities, smoothed with Good-Turing smoothing (Foster et al., 2006), and lexical translation log-probabilities in both directions. • Phrase penalty and word penalty. • Distance-based distortion cost. • A hierarchical lexicalized reordering model (Galley and Manning, 2008). • A 5-gram operation sequence model (Durrani et al., 2013a). • Seven binary features indicating absolute occurrence count classes of phrase pairs. • Sparse phrase length features. • Sparse lexical features for the top 200 words. • A 5-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). We discard singleton n-grams of order three and higher. Feature wei"
2015.mtsummit-papers.19,D08-1089,0,0.0234049,"ual training corpora are presented in Table 2. 6.2 Machine Translation Systems Word alignments are created by aligning the data in both directions and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). We extract phrases up to a maximum length of five. The MT systems comprise these features: • Phrase translation log-probabilities, smoothed with Good-Turing smoothing (Foster et al., 2006), and lexical translation log-probabilities in both directions. • Phrase penalty and word penalty. • Distance-based distortion cost. • A hierarchical lexicalized reordering model (Galley and Manning, 2008). • A 5-gram operation sequence model (Durrani et al., 2013a). • Seven binary features indicating absolute occurrence count classes of phrase pairs. • Sparse phrase length features. • Sparse lexical features for the top 200 words. • A 5-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). We discard singleton n-grams of order three and higher. Feature weights are optimized to maximize B LEU (Papineni et al., 2002) with batch MIRA (Cherry and Foster, 2012) on 1000-best lists. We prune the phrase table to a maximum of 100 5 http://dumps.wikimedia.org 6 https"
2015.mtsummit-papers.19,W08-0509,0,0.0165542,"respective LM yields the maximum LM score. Overall, we end up with four variations: MEtrain Classifier trained on medium-sized training corpora with the basic set of features. MEtrain+lm Classifier trained on medium-sized training corpora with the basic set of features plus source LM indicator features. MEdev Classifier trained on the MT development sets with the basic set of features. MEdev+lm Classifier trained on the MT development sets with the basic set of features plus source LM indicator features. 6 Experimental Setup We use Moses (Koehn et al., 2007) for machine translation, MGIZA++ (Gao and Vogel, 2008) to train word alignments, KenLM (Heafield, 2011) for LM training and scoring, SRILM (Stolcke, 2002) for LM interpolation, and the Stanford Classifier3 for maximum entropy text classification. We present experimental results on English→German, English→Italian, English→Portuguese, and English→Greek translation tasks. 6.1 Training Data Our SMT systems are trained with the following bilingual corpora: • • • • • • • • TED from WIT3 (Cettolo et al., 2012) Europarl (Koehn, 2005) JRC-Acquis 3.0 (Steinberger et al., 2006) DGT’s Translation Memory (Steinberger et al., 2012) as distributed in OPUS (Tied"
2015.mtsummit-papers.19,W12-3154,1,0.854639,"the same domain which can be employed for training and tuning. The adaptation task is then defined as utilizing a small amount of in-domain training resources effectively in order to learn system parameters that are more appropriate for translating in-domain input. The in-domain training resources constitute a minor fraction of the overall training data only, the majority of which has a domain mismatch with the designated application. The downside of systems that have been highly tweaked towards the characteristics of a single domain is a diminished translation quality on out-of-domain data (Haddow and Koehn, 2012). Online translation systems, on the other hand, are usually designed for open-domain scenarios where the domain of the input text is not predefined. Being able to take advantage of the benefits of domain adaptation while not having to compromise quality on out-of-domain data would be desirable for online systems. Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 240 A viable utilization of domain adaptation approaches in open-domain online translation systems comes in two components: • A number of different parameter sets, each tuned to optimize transla"
2015.mtsummit-papers.19,W11-2123,0,0.0283971,"e end up with four variations: MEtrain Classifier trained on medium-sized training corpora with the basic set of features. MEtrain+lm Classifier trained on medium-sized training corpora with the basic set of features plus source LM indicator features. MEdev Classifier trained on the MT development sets with the basic set of features. MEdev+lm Classifier trained on the MT development sets with the basic set of features plus source LM indicator features. 6 Experimental Setup We use Moses (Koehn et al., 2007) for machine translation, MGIZA++ (Gao and Vogel, 2008) to train word alignments, KenLM (Heafield, 2011) for LM training and scoring, SRILM (Stolcke, 2002) for LM interpolation, and the Stanford Classifier3 for maximum entropy text classification. We present experimental results on English→German, English→Italian, English→Portuguese, and English→Greek translation tasks. 6.1 Training Data Our SMT systems are trained with the following bilingual corpora: • • • • • • • • TED from WIT3 (Cettolo et al., 2012) Europarl (Koehn, 2005) JRC-Acquis 3.0 (Steinberger et al., 2006) DGT’s Translation Memory (Steinberger et al., 2012) as distributed in OPUS (Tiedemann, 2012) OPUS European Central Bank (ECB) OPU"
2015.mtsummit-papers.19,2005.mtsummit-papers.11,0,0.144636,"ews articles. TED talks, Europarl, and News could be described as “genres”. We denote them as domains throughout this paper because the term “domain” is well established in related machine translation research literature and often used in a broad sense. TED talks, Europarl, and News have been highly relevant domains in recent machine translation research. The International Workshop on Spoken Language Translation1 (IWSLT) hosts a yearly open evaluation campaign which focuses on the translation of TED talks since 2011 (Federico et al., 2011). The European Parliament Proceedings Parallel Corpus (Koehn, 2005) has been an influential resource for machine translation research ever since its first release over 1 http://www.iwslt.org Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 242 a decade ago. It is freely available and includes parallel text for 21 European languages. Test sets and training data that enables research on machine translation of texts from the News domain have regularly been released for the shared translation task of the Workshop on Statistical Machine Translation2 (WMT). The WMT “newstest” corpora have become important test sets to measur"
2015.mtsummit-papers.19,P07-2045,1,0.0125752,"an associated source LM indicator feature fires if the respective LM yields the maximum LM score. Overall, we end up with four variations: MEtrain Classifier trained on medium-sized training corpora with the basic set of features. MEtrain+lm Classifier trained on medium-sized training corpora with the basic set of features plus source LM indicator features. MEdev Classifier trained on the MT development sets with the basic set of features. MEdev+lm Classifier trained on the MT development sets with the basic set of features plus source LM indicator features. 6 Experimental Setup We use Moses (Koehn et al., 2007) for machine translation, MGIZA++ (Gao and Vogel, 2008) to train word alignments, KenLM (Heafield, 2011) for LM training and scoring, SRILM (Stolcke, 2002) for LM interpolation, and the Stanford Classifier3 for maximum entropy text classification. We present experimental results on English→German, English→Italian, English→Portuguese, and English→Greek translation tasks. 6.1 Training Data Our SMT systems are trained with the following bilingual corpora: • • • • • • • • TED from WIT3 (Cettolo et al., 2012) Europarl (Koehn, 2005) JRC-Acquis 3.0 (Steinberger et al., 2006) DGT’s Translation Memory"
2015.mtsummit-papers.19,N03-1017,0,0.00929731,"raining corpora are presented in Table 1. For language modeling on the target side, we furthermore add monolingual corpora from recent (April 2015) Wikipedia database dumps5 and—for German—the News Crawl corpora provided for the WMT 2015 shared translation task. Plain text was obtained from the Wikipedia XML dumps with the Wikipedia Extractor6 tool. Statistics of the additional monolingual training corpora are presented in Table 2. 6.2 Machine Translation Systems Word alignments are created by aligning the data in both directions and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). We extract phrases up to a maximum length of five. The MT systems comprise these features: • Phrase translation log-probabilities, smoothed with Good-Turing smoothing (Foster et al., 2006), and lexical translation log-probabilities in both directions. • Phrase penalty and word penalty. • Distance-based distortion cost. • A hierarchical lexicalized reordering model (Galley and Manning, 2008). • A 5-gram operation sequence model (Durrani et al., 2013a). • Seven binary features indicating absolute occurrence count classes of phrase pairs. • Sparse phrase length features. • Sparse lexical featur"
2015.mtsummit-papers.19,W07-0733,0,0.0947904,"Missing"
2015.mtsummit-papers.19,W11-2132,0,0.289447,"et al., 2012). • Model combination (of language models, translation models, or reordering models) via interpolation or other schemes, e.g. phrase table fill-up (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Nakov, 2008; Bisazza et al., 2011; Niehues and Waibel, 2012; Chen et al., 2013). • Data selection (Moore and Lewis, 2010; Axelrod et al., 2011). • Instance weighting (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012). • Further exploitation of in-domain monolingual data (Ueffing et al., 2007; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009; Lambert et al., 2011). • Domain-specific features, e.g. binary features indicating the provenance of phrase pairs as implemented in the open-source Moses toolkit (Durrani et al., 2013b) or “domain augmentation” (Clark et al., 2012). However, few authors have tackled the question of how to benefit from domain adaptation in scenarios where a domain label of the input is not present. An important aspect of our approach to multi-domain MT is the need for domain classification. Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 241 Xu et al. (2007) perform domain classification fo"
2015.mtsummit-papers.19,D08-1076,0,0.0236215,"are evaluated. The authors show that a pipeline with the SVM classifier is effective in multi-domain translation. Wang et al. (2012) distinguish generic and patent domain data in experiments on 20 language pairs. For domain classification, the authors rely on averaged perceptron classifiers with various phrase-based features. The machine translation development sets serve as training data for the classifiers. An interesting aspect of their translation experiments is that they utilize a multi-domain optimization in order to jointly tune weights for all domains in a single run of lattice MERT (Macherey et al., 2008). In a related strand of research, source-side text classifiers have recently been employed in order to detect Arabic dialects and select SMT systems accordingly (Salloum et al., 2014; Mansour et al., 2014). 3 Text Domains Our application scenario is an online translation service with the requirement to provide highquality translation not only of texts from a single domain, but of a wider range of text types. We therefore study a use case where the translation system is supposed to perform well on the following domains: TED talks, Europarl, and News. These three domains are fairly coarsegraine"
2015.mtsummit-papers.19,2014.amta-researchers.26,0,0.035468,"pairs. For domain classification, the authors rely on averaged perceptron classifiers with various phrase-based features. The machine translation development sets serve as training data for the classifiers. An interesting aspect of their translation experiments is that they utilize a multi-domain optimization in order to jointly tune weights for all domains in a single run of lattice MERT (Macherey et al., 2008). In a related strand of research, source-side text classifiers have recently been employed in order to detect Arabic dialects and select SMT systems accordingly (Salloum et al., 2014; Mansour et al., 2014). 3 Text Domains Our application scenario is an online translation service with the requirement to provide highquality translation not only of texts from a single domain, but of a wider range of text types. We therefore study a use case where the translation system is supposed to perform well on the following domains: TED talks, Europarl, and News. These three domains are fairly coarsegrained. Different documents from one of the domains are mostly not consistent regarding the covered topics. While all three domains comprise heterogeneous topics, the domains are set apart from each other by mea"
2015.mtsummit-papers.19,2012.iwslt-papers.7,0,0.0639241,"in recent years. Some methods which are commonly used are: • Tuning of the decoder model weights (Och and Ney, 2002) on an in-domain development set (Pecina et al., 2012). • Model combination (of language models, translation models, or reordering models) via interpolation or other schemes, e.g. phrase table fill-up (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Nakov, 2008; Bisazza et al., 2011; Niehues and Waibel, 2012; Chen et al., 2013). • Data selection (Moore and Lewis, 2010; Axelrod et al., 2011). • Instance weighting (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012). • Further exploitation of in-domain monolingual data (Ueffing et al., 2007; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009; Lambert et al., 2011). • Domain-specific features, e.g. binary features indicating the provenance of phrase pairs as implemented in the open-source Moses toolkit (Durrani et al., 2013b) or “domain augmentation” (Clark et al., 2012). However, few authors have tackled the question of how to benefit from domain adaptation in scenarios where a domain label of the input is not present. An important aspect of our approach to multi-domain MT is the need for domain cl"
2015.mtsummit-papers.19,2011.eamt-1.19,0,0.0209504,"-2012 sets (En→De) and on newstest2009 (En→It). We use newssyscomb2009 as an English→Italian News domain test set for lack of other English-Italian News test sets. Note that newssyscomb2009 is a small set of only 502 sentences. No News test data was available to us for the English→Portuguese and English→Greek language pairs, so we experiment with only two domains (TED and Europarl) on these tasks. The Portuguese TED development and test sets are Brazilian Portuguese whereas the Europarl sets are European Portuguese. The two Portuguese dialects have a number of differences in written language. Marujo et al. (2011) give a brief overview. 6.2.2 Domain-Adapted SMT For our domain adaptation experiments, we first tune the systems with the features described above on the respective in-domain development set (TED-tuned, Europarl-tuned, News-tuned). We next replace the large baseline LM with a domain-specific interpolated LM (+ LM interp.). We then add binary features indicating the provenance of phrase pairs (+ LM interp. + indicator feat.). 6.2.3 Mixed-Domain SMT We build mixed-domain SMT systems by tuning on a development corpus containing samples of texts from all domains. We include a balanced amount of d"
2015.mtsummit-papers.19,D09-1074,0,0.0508134,"ount of research on domain adaptation for SMT has been conducted in recent years. Some methods which are commonly used are: • Tuning of the decoder model weights (Och and Ney, 2002) on an in-domain development set (Pecina et al., 2012). • Model combination (of language models, translation models, or reordering models) via interpolation or other schemes, e.g. phrase table fill-up (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Nakov, 2008; Bisazza et al., 2011; Niehues and Waibel, 2012; Chen et al., 2013). • Data selection (Moore and Lewis, 2010; Axelrod et al., 2011). • Instance weighting (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012). • Further exploitation of in-domain monolingual data (Ueffing et al., 2007; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009; Lambert et al., 2011). • Domain-specific features, e.g. binary features indicating the provenance of phrase pairs as implemented in the open-source Moses toolkit (Durrani et al., 2013b) or “domain augmentation” (Clark et al., 2012). However, few authors have tackled the question of how to benefit from domain adaptation in scenarios where a domain label of the input is not present. An important asp"
2015.mtsummit-papers.19,P10-2041,0,0.0521304,"s of millions of parallel sentences. 2 Related Work A significant amount of research on domain adaptation for SMT has been conducted in recent years. Some methods which are commonly used are: • Tuning of the decoder model weights (Och and Ney, 2002) on an in-domain development set (Pecina et al., 2012). • Model combination (of language models, translation models, or reordering models) via interpolation or other schemes, e.g. phrase table fill-up (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Nakov, 2008; Bisazza et al., 2011; Niehues and Waibel, 2012; Chen et al., 2013). • Data selection (Moore and Lewis, 2010; Axelrod et al., 2011). • Instance weighting (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012). • Further exploitation of in-domain monolingual data (Ueffing et al., 2007; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009; Lambert et al., 2011). • Domain-specific features, e.g. binary features indicating the provenance of phrase pairs as implemented in the open-source Moses toolkit (Durrani et al., 2013b) or “domain augmentation” (Clark et al., 2012). However, few authors have tackled the question of how to benefit from domain adaptation in scenari"
2015.mtsummit-papers.19,W08-0320,0,0.0269359,"ortuguese, and English→Greek language pairs using training corpora of diverse origin, totalling tens of millions of parallel sentences. 2 Related Work A significant amount of research on domain adaptation for SMT has been conducted in recent years. Some methods which are commonly used are: • Tuning of the decoder model weights (Och and Ney, 2002) on an in-domain development set (Pecina et al., 2012). • Model combination (of language models, translation models, or reordering models) via interpolation or other schemes, e.g. phrase table fill-up (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Nakov, 2008; Bisazza et al., 2011; Niehues and Waibel, 2012; Chen et al., 2013). • Data selection (Moore and Lewis, 2010; Axelrod et al., 2011). • Instance weighting (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012). • Further exploitation of in-domain monolingual data (Ueffing et al., 2007; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009; Lambert et al., 2011). • Domain-specific features, e.g. binary features indicating the provenance of phrase pairs as implemented in the open-source Moses toolkit (Durrani et al., 2013b) or “domain augmentation” (Clark et a"
2015.mtsummit-papers.19,2012.amta-papers.19,0,0.106597,"ge pairs using training corpora of diverse origin, totalling tens of millions of parallel sentences. 2 Related Work A significant amount of research on domain adaptation for SMT has been conducted in recent years. Some methods which are commonly used are: • Tuning of the decoder model weights (Och and Ney, 2002) on an in-domain development set (Pecina et al., 2012). • Model combination (of language models, translation models, or reordering models) via interpolation or other schemes, e.g. phrase table fill-up (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Nakov, 2008; Bisazza et al., 2011; Niehues and Waibel, 2012; Chen et al., 2013). • Data selection (Moore and Lewis, 2010; Axelrod et al., 2011). • Instance weighting (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012). • Further exploitation of in-domain monolingual data (Ueffing et al., 2007; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009; Lambert et al., 2011). • Domain-specific features, e.g. binary features indicating the provenance of phrase pairs as implemented in the open-source Moses toolkit (Durrani et al., 2013b) or “domain augmentation” (Clark et al., 2012). However, few authors have tackled the"
2015.mtsummit-papers.19,P02-1038,0,0.261406,"cross all domains. However, a high-quality generic system with a single parameter set that does not depend on a domain label is appealing. In the empirical part of this paper, we compare multi-domain and mixed-domain SMT on the English→German, English→Italian, English→Portuguese, and English→Greek language pairs using training corpora of diverse origin, totalling tens of millions of parallel sentences. 2 Related Work A significant amount of research on domain adaptation for SMT has been conducted in recent years. Some methods which are commonly used are: • Tuning of the decoder model weights (Och and Ney, 2002) on an in-domain development set (Pecina et al., 2012). • Model combination (of language models, translation models, or reordering models) via interpolation or other schemes, e.g. phrase table fill-up (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Nakov, 2008; Bisazza et al., 2011; Niehues and Waibel, 2012; Chen et al., 2013). • Data selection (Moore and Lewis, 2010; Axelrod et al., 2011). • Instance weighting (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012). • Further exploitation of in-domain monolingual data (Ueffing et al., 2007; Bertoldi and Fede"
2015.mtsummit-papers.19,J03-1002,0,0.00454929,"of all bilingual training corpora are presented in Table 1. For language modeling on the target side, we furthermore add monolingual corpora from recent (April 2015) Wikipedia database dumps5 and—for German—the News Crawl corpora provided for the WMT 2015 shared translation task. Plain text was obtained from the Wikipedia XML dumps with the Wikipedia Extractor6 tool. Statistics of the additional monolingual training corpora are presented in Table 2. 6.2 Machine Translation Systems Word alignments are created by aligning the data in both directions and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). We extract phrases up to a maximum length of five. The MT systems comprise these features: • Phrase translation log-probabilities, smoothed with Good-Turing smoothing (Foster et al., 2006), and lexical translation log-probabilities in both directions. • Phrase penalty and word penalty. • Distance-based distortion cost. • A hierarchical lexicalized reordering model (Galley and Manning, 2008). • A 5-gram operation sequence model (Durrani et al., 2013a). • Seven binary features indicating absolute occurrence count classes of phrase pairs. • Sparse phrase length features. •"
2015.mtsummit-papers.19,P02-1040,0,0.0960693,"ities in both directions. • Phrase penalty and word penalty. • Distance-based distortion cost. • A hierarchical lexicalized reordering model (Galley and Manning, 2008). • A 5-gram operation sequence model (Durrani et al., 2013a). • Seven binary features indicating absolute occurrence count classes of phrase pairs. • Sparse phrase length features. • Sparse lexical features for the top 200 words. • A 5-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). We discard singleton n-grams of order three and higher. Feature weights are optimized to maximize B LEU (Papineni et al., 2002) with batch MIRA (Cherry and Foster, 2012) on 1000-best lists. We prune the phrase table to a maximum of 100 5 http://dumps.wikimedia.org 6 https://github.com/bwbaugh/wikipedia-extractor Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 245 best translation options per distinct source side and apply a minimum score threshold of 0.0001 on the source-to-target phrase translation probability. We use cube pruning in decoding. Pop limit and stack limit are set to 1000 for tuning and to 5000 for testing. We disallow reordering over punctuation. Furthermore, Mi"
2015.mtsummit-papers.19,C12-1135,0,0.0349806,"Missing"
2015.mtsummit-papers.19,2014.eamt-1.39,0,0.0710152,"Missing"
2015.mtsummit-papers.19,P14-2125,0,0.0205524,"iments on 20 language pairs. For domain classification, the authors rely on averaged perceptron classifiers with various phrase-based features. The machine translation development sets serve as training data for the classifiers. An interesting aspect of their translation experiments is that they utilize a multi-domain optimization in order to jointly tune weights for all domains in a single run of lattice MERT (Macherey et al., 2008). In a related strand of research, source-side text classifiers have recently been employed in order to detect Arabic dialects and select SMT systems accordingly (Salloum et al., 2014; Mansour et al., 2014). 3 Text Domains Our application scenario is an online translation service with the requirement to provide highquality translation not only of texts from a single domain, but of a wider range of text types. We therefore study a use case where the translation system is supposed to perform well on the following domains: TED talks, Europarl, and News. These three domains are fairly coarsegrained. Different documents from one of the domains are mostly not consistent regarding the covered topics. While all three domains comprise heterogeneous topics, the domains are set apart"
2015.mtsummit-papers.19,2009.mtsummit-posters.17,0,0.0569352,"main development set (Pecina et al., 2012). • Model combination (of language models, translation models, or reordering models) via interpolation or other schemes, e.g. phrase table fill-up (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Nakov, 2008; Bisazza et al., 2011; Niehues and Waibel, 2012; Chen et al., 2013). • Data selection (Moore and Lewis, 2010; Axelrod et al., 2011). • Instance weighting (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012). • Further exploitation of in-domain monolingual data (Ueffing et al., 2007; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009; Lambert et al., 2011). • Domain-specific features, e.g. binary features indicating the provenance of phrase pairs as implemented in the open-source Moses toolkit (Durrani et al., 2013b) or “domain augmentation” (Clark et al., 2012). However, few authors have tackled the question of how to benefit from domain adaptation in scenarios where a domain label of the input is not present. An important aspect of our approach to multi-domain MT is the need for domain classification. Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 241 Xu et al. (2007) perform d"
2015.mtsummit-papers.19,2012.amta-papers.21,0,0.246487,"has been conducted in recent years. Some methods which are commonly used are: • Tuning of the decoder model weights (Och and Ney, 2002) on an in-domain development set (Pecina et al., 2012). • Model combination (of language models, translation models, or reordering models) via interpolation or other schemes, e.g. phrase table fill-up (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Nakov, 2008; Bisazza et al., 2011; Niehues and Waibel, 2012; Chen et al., 2013). • Data selection (Moore and Lewis, 2010; Axelrod et al., 2011). • Instance weighting (Matsoukas et al., 2009; Foster et al., 2010; Shah et al., 2012; Mansour and Ney, 2012). • Further exploitation of in-domain monolingual data (Ueffing et al., 2007; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009; Lambert et al., 2011). • Domain-specific features, e.g. binary features indicating the provenance of phrase pairs as implemented in the open-source Moses toolkit (Durrani et al., 2013b) or “domain augmentation” (Clark et al., 2012). However, few authors have tackled the question of how to benefit from domain adaptation in scenarios where a domain label of the input is not present. An important aspect of our approach to multi-domain MT i"
2015.mtsummit-papers.19,steinberger-etal-2012-dgt,0,0.0441212,"Missing"
2015.mtsummit-papers.19,steinberger-etal-2006-jrc,0,0.0787518,"Missing"
2015.mtsummit-papers.19,tiedemann-2012-parallel,0,0.0570125,"Missing"
2015.mtsummit-papers.19,2012.amta-papers.18,0,0.0451164,"security in the area of computing. Empirical results on Chinese→English and English→Chinese tasks are presented. The authors build a Support Vector Machine (SVM) classifier using Term Frequency Inverse Sentence Frequency features over bigrams of stemmed content words. Classification is carried out on the level of individual sentences. The SVM is trained on the SMT training corpora (∼226k sentences in total). Several setups with different domain-adapted and domain-agnostic systems are evaluated. The authors show that a pipeline with the SVM classifier is effective in multi-domain translation. Wang et al. (2012) distinguish generic and patent domain data in experiments on 20 language pairs. For domain classification, the authors rely on averaged perceptron classifiers with various phrase-based features. The machine translation development sets serve as training data for the classifiers. An interesting aspect of their translation experiments is that they utilize a multi-domain optimization in order to jointly tune weights for all domains in a single run of lattice MERT (Macherey et al., 2008). In a related strand of research, source-side text classifiers have recently been employed in order to detect"
2015.mtsummit-papers.19,2007.mtsummit-papers.68,0,0.705796,"chwenk and Senellart, 2009; Lambert et al., 2011). • Domain-specific features, e.g. binary features indicating the provenance of phrase pairs as implemented in the open-source Moses toolkit (Durrani et al., 2013b) or “domain augmentation” (Clark et al., 2012). However, few authors have tackled the question of how to benefit from domain adaptation in scenarios where a domain label of the input is not present. An important aspect of our approach to multi-domain MT is the need for domain classification. Proceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 241 Xu et al. (2007) perform domain classification for a Chinese→English task. The domains are newswire and newsgroup. The classifiers operate on whole documents rather than on individual sentences. The authors propose two techniques for domain classification. Their first technique is based on interpolated LMs: a general-domain LM is interpolated with LMs which were trained on in-domain development sets, resulting in a number of domain-specific interpolated LMs. The interpolation weight is heuristically chosen. The classifier computes LM perplexities over input documents and assigns the domain with the lowest per"
2020.emnlp-main.187,E14-1049,0,0.0292768,"d the language coverage. However, our method primarily differs as it is mainly based in linear algebra, encodes information from both sources since the beginning, and can deal with a small number of shared entries (e.g. 23 from LW ) to compute robust representations. There has been very little work on adopting typology knowledge for NMT. There is not a deep integration of the topics (Ponti et al., 2019), but one shallow and prominent case is the ranking method (Lin et al., 2019) that we analysed in §6. Finally, CCA and its variants have been previously used to derive embeddings at word-level (Faruqui and Dyer, 2014; Dhillon et al., 2015; Osborne et al., 2016). Kudugunta et al. (2019) also used SVCCA but to inspect sentence-level representations, where they uncover relevant insights about language similarity that are aligned with our results in §5. However, as far as we know, this is the first time a CCA-based method has been used to compute language-level representations. 9 Takeaways and practical tool We summarise our key findings as follows: • SVCCA can fuse linguistic typology KB entries with NMT-learned embeddings without diminishing the originally encoded typological and genetic similarity of langu"
2020.emnlp-main.187,2020.acl-main.560,0,0.044494,"VCCA can fuse linguistic typology KB entries with NMT-learned embeddings without diminishing the originally encoded typological and genetic similarity of languages. • Our method is a robust alternative for identifying clusters and choosing related languages for multilingual transfer in NMT. The advantage is notable when it is not feasible to pretrain a ranking model or learn embeddings from a massive multilingual system. Assessing new languages is an important ability, given that most of them do not have even enough monolingual corpora to learn embeddings from multilingual language modelling (Joshi et al., 2020). • Factored language embeddings encode more information to agglomerate related languages than the initial pseudo-token setting. Furthermore, we make our code available as an open-source tool9 , together with our LT factoredembeddings, to compute multi-view language representations using SVCCA. We enable the option to use other language vectors from lang2vec (Phonology or Phonetic Inventory) as the KB-source, and to upload new task-learned embeddings from different settings, such as one-to-many or many-to-many NMT, and also multilingual language modelling. Besides, given a list of languages to"
2020.emnlp-main.187,W18-2716,0,0.0132958,"and training. Similar to Tan et al. (2019), we train small transformer models (Vaswani et al., 2017). We jointly learn 90k shared sub-words with the byte pair encoding (Sennrich et al., 2016) algorithm built in SentencePiece (Kudo and Richardson, 2018). We also oversample all the training data of the less-resourced languages in each cluster, and shuffle them proportionally in all batches. We use Nematus (Sennrich et al., 2017) only to extract the factored language embeddings from the TED-53 corpus (LT ). Besides, given the large number of experiments, we also choose the efficient Marian NMT (Junczys-Dowmunt et al., 2018) toolkit for training the rest of systems. With Marian NMT, we only use the basic pseudo-token setting for identifying the source language, as we did not need to retrieve new language embeddings after training. Besides, we allow the Marian NMT framework to automatically determine the minibatch size given the sentence-length and available memory (mini-batch-fit parameter). We train our models with up to four NVIDIA P100 GPUs using Adam optimiser (Kingma and Ba, 2014) with default parameters (β1 = 0.9, β2 = 0.98, ε = 10−9 ) and early stopping at 5 validation steps for the cross-entropy metric. F"
2020.emnlp-main.187,E17-2002,0,0.436869,"2004). The two-step transformation of SVD followed by CCA is called singular vector canonical correlation analysis (SVCCA; Raghu et al., 2017) in the context of understanding the representation learning throughout neural network layers. That being said, we use SVCCA to get language representations and not to inspect a neural architecture.2 3 Methodology and research questions To embed linguistic typology knowledge in dense representations for a broad set of languages, we employ SVCCA (§2) with the following sources: KB view. We employ the language vectors from the URIEL and lang2vec database (Littell et al., 2017). Precisely, we work with the k-NN vectors of the Syntax feature class (US ; 103 feats.), that are composed of binary features encoded from WALS (Dryer and Haspelmath, 2013). (NMT) Learned view. Firstly, we exploit the NMT-learned embeddings from the Bible (LB ; 512 dim.) (Malaviya et al., 2017). Up to 731 entries are available in lang2vec that intersects with US . They were trained in a many-to-English NMT model with a pseudo-token identifying the source language at the beginning of every input sentence. Secondly, we take the many-to-English language embeddings learned for the language cluste"
2020.emnlp-main.187,D17-1268,0,0.524204,"ons and not to inspect a neural architecture.2 3 Methodology and research questions To embed linguistic typology knowledge in dense representations for a broad set of languages, we employ SVCCA (§2) with the following sources: KB view. We employ the language vectors from the URIEL and lang2vec database (Littell et al., 2017). Precisely, we work with the k-NN vectors of the Syntax feature class (US ; 103 feats.), that are composed of binary features encoded from WALS (Dryer and Haspelmath, 2013). (NMT) Learned view. Firstly, we exploit the NMT-learned embeddings from the Bible (LB ; 512 dim.) (Malaviya et al., 2017). Up to 731 entries are available in lang2vec that intersects with US . They were trained in a many-to-English NMT model with a pseudo-token identifying the source language at the beginning of every input sentence. Secondly, we take the many-to-English language embeddings learned for the language clustering task on multilingual NMT (LW ; 256 dim.) (Tan et al., 2019), where they use 23 languages of the WIT3 corpus (Cettolo et al., 2012). One main difference for the latter is the use of factors in the architecture, meaning that the embedding of every input token was concatenated with the embedde"
2020.emnlp-main.187,N15-1036,0,0.0314384,"ANK shows the accumulated training size (in thousands) for the top-3 candidates, whereas with SVCCA we approximate the amount of data and include the number of languages between brackets. 8 However, we do not answer what multilingual NMT really transfers to the low-resource languages. We left that question for further research, together with optimising the k number of languages or the amount of data per each language. Related work For language-level representations, URIEL and lang2vec (Littell et al., 2017) allow a straightforward extraction of typological binary features from different KBs. Murawaki (2015, 2017, 2018) exploits them to build latent language representations with independent binary variables. Language features are encoded from data-driven tasks as well, such as NMT (Malaviya et al., 2017) or language ¨ modelling (Tsvetkov et al., 2016; Ostling and Tiedemann, 2017; Bjerva and Augenstein, 2018b) 2398 with complementary linguistic-related target tasks (Bjerva and Augenstein, 2018a). Our approach is most similar to Bjerva et al. (2019a), as they build a generative model from typological features and use language embeddings, extracted from factored language modelling at character-leve"
2020.emnlp-main.187,I17-1046,0,0.327368,"Missing"
2020.emnlp-main.187,D18-1468,0,0.0459798,"Missing"
2020.emnlp-main.187,W18-6319,0,0.0174961,"ems. With Marian NMT, we only use the basic pseudo-token setting for identifying the source language, as we did not need to retrieve new language embeddings after training. Besides, we allow the Marian NMT framework to automatically determine the minibatch size given the sentence-length and available memory (mini-batch-fit parameter). We train our models with up to four NVIDIA P100 GPUs using Adam optimiser (Kingma and Ba, 2014) with default parameters (β1 = 0.9, β2 = 0.98, ε = 10−9 ) and early stopping at 5 validation steps for the cross-entropy metric. Finally, the sacreBLEU version string (Post, 2018) is as follows: BLEU+case.mixed+numrefs.1+smooth.exp +tok.13a+version.1.3.7. Clustering settings. We first list the baselines and our approaches, with the number of clusters/models between brackets: 1. Individual [53]: Pairwise model per language. 2. Massive [1]: A single model for all languages. 3. Language families [20]: Based on historical linguistics. We divide the 33 Indo-European languages into 7 branches. Moreover, 11 groups only have one language. 4. KB [3]: US (Syntax) tends to agglomerate large clusters (with 4-13-33 languages), behaving similar to a massive model (Fig. 2c). 5. Learn"
2020.emnlp-main.187,N18-2084,0,0.211501,"se 23 languages of the WIT3 corpus (Cettolo et al., 2012). One main difference for the latter is the use of factors in the architecture, meaning that the embedding of every input token was concatenated with the embedded pseudo-token that identifies the source language. The second difference is the neural architecture used to extract the embeddings: the former use a recurrent neural network, whereas the latter a small transformer model (Vaswani et al., 2017). Finally, we train a new set of embeddings (LT ) that we extracted from the 53 languages of the TED corpus (many-to-English) processed by Qi et al. (2018), using the approach of Tan et al. (2019).3 What knowledge do we represent? Each source embeds specialised knowledge to assess language relatedness. The KB vectors can measure typological similarity, whereas task-learned embeddings correlates with other kinds of language relationships (e.g. genetic) (Bjerva et al., 2019b). To analyse whether each kind of knowledge is induced with SVCCA, we assess the tasks of typological feature prediction (§4) and reconstruction of a language phylogeny (§5). What is the benefit for multilingual NMT (and NLP)? Language-level representations can evaluate the di"
2020.emnlp-main.187,P17-1049,0,0.0476411,"), there is a positive correlation between the language distances in a phylogenetic tree and a pairwise distance-matrix of task-learned representations. Our goal therefore 4 In other words, for SVCCA, it is difficult to deal with the noise provided in the learned embeddings. In Figures 6a and 6b of the Appendix, we observe noisy agglomerations in the dendrograms (obtained by clustering different language representations), which is preserved after the fusing with the KB vectors through SVCCA as we can see in Fig. 6c) Inference of a phylogenetic tree Experimental design. Based on previous work (Rabinovich et al., 2017), we take a tree of 17 IndoEuropean languages (Serva and Petroni, 2008) as a Gold Standard (GS), which is shown in Figure 1a.5 We also use agglomerative clustering with variance minimisation (Ward Jr, 1963) as linkage, but we employ cosine similarity as Bjerva et al. (2019b). We also consider a concatenation (⊕) of the KB and NMT-learned views as a baseline. It is essential to highlight that none of the NMTlearned and ⊕ vectors have all the 17 language entries of the GS. Therefore, we can already see one of the significant advantages of the SVCCA vectors, as we are able to represent “unknown”"
2020.emnlp-main.187,W18-6327,0,0.0488129,"we showed that the knowledge and language relationship encoded in both sources is preserved in the combined representation. Moreover, our approach offers important advantages because we can evaluate projected languages with entries in only one of the views and can easily extend the language coverage. The benefits are noticeable in multilingual NMT tasks, like language clustering and ranking related languages for multilingual transfer. We plan to study how to deeply incorporate our typologically-enriched embeddings in multilingual NMT, where there are promising avenues in parameter selection (Sachan and Neubig, 2018) and generation (Platanios et al., 2018). Acknowledgments This work was supported by funding from the European Union’s Horizon 2020 research and innovation programme under grant agreements No 825299 (GoURMET) and the EPSRC fellowship grant EP/S001271/1 (MTStretch). Also, it was performed using resources provided by the Cambridge Service for Data Driven Discovery (CSD3) operated by the University of Cambridge Research Computing Service (http://www. csd3.cam.ac.uk/), provided by Dell EMC and Intel using Tier-2 funding from the Engineering and Physical Sciences Research Council (capital 2399 9 ht"
2020.emnlp-main.187,E17-3017,1,0.824589,"Missing"
2020.emnlp-main.187,P16-1162,1,0.180217,"o better evaluate the extensibility of clusters and because it is also used to train the L ANG R ANK model. The list of languages, set sizes and other details are included in Appendix A. Before preprocessing the text, we drop any sentences from the training sets which overlap with any of the test sets. Since we are building many-toEnglish multilingual systems, this is important, as any such overlap will bias the results. Model and training. Similar to Tan et al. (2019), we train small transformer models (Vaswani et al., 2017). We jointly learn 90k shared sub-words with the byte pair encoding (Sennrich et al., 2016) algorithm built in SentencePiece (Kudo and Richardson, 2018). We also oversample all the training data of the less-resourced languages in each cluster, and shuffle them proportionally in all batches. We use Nematus (Sennrich et al., 2017) only to extract the factored language embeddings from the TED-53 corpus (LT ). Besides, given the large number of experiments, we also choose the efficient Marian NMT (Junczys-Dowmunt et al., 2018) toolkit for training the rest of systems. With Marian NMT, we only use the basic pseudo-token setting for identifying the source language, as we did not need to r"
2020.emnlp-main.187,D19-1089,0,0.150491,"Missing"
2020.emnlp-main.187,P19-1583,0,0.0172706,"06 0.4 0.125 former, we observe that many of the smaller mul0.4 0.04 0.100 0.2 tilingual models outperform the translation accu0.2 2 6 10 14 18 2 6 10 14 18 2 6 10 14 18 2 6 10 14 18 racy of the massive system. The result suggests # clusters # #clusters clusters # clusters that the amount of data is not the most important confound for supporting multilingual transfer in a Figure 4: Silhouette analysis for the LT ∗ embeddings trained using an initial pseudo-token (left) and the LB low-resource language, which is aligned with the Bible vectors (right). Both cases present a downtrend literature (Wang and Neubig, 2019). curve with scores below 0.2. The hierarchies of LT ∗ Comparing the two ranking approaches, we oband LB are shown in Figures 6b and 6a (in the Apserve that SVCCA approximates the performance pendix), respectively. of L ANG R ANK in most of the cases. We note that L ANG R ANK prefers related languages with large datasets, as it only requires three candidates 7 Factors over initial pseudo-tokens to group around half a million training samples, whereas SVCCA suggests to include from three to We additionally argue that the configuration used ten languages to reach a similar amount of paral- to co"
2020.emnlp-main.187,E17-2102,0,\N,Missing
2020.emnlp-main.187,J19-3005,0,\N,Missing
2020.emnlp-main.187,D18-1039,0,\N,Missing
2020.emnlp-main.187,N19-1156,0,\N,Missing
2020.emnlp-main.187,Q16-1030,0,\N,Missing
2020.emnlp-main.187,D19-1167,0,\N,Missing
2020.emnlp-main.187,2012.eamt-1.60,0,\N,Missing
2020.emnlp-main.187,W18-0207,0,\N,Missing
2020.emnlp-main.615,N19-1071,1,0.849973,"incorporate prior knowledge into NMT. Zhang et al. (2017) exploit linguistic real-valued features, such as dictionaries or length ratios, to construct the distribution for regularizing the TM’s posteriors. Recently, Ren et al. (2019) used posterior regularization for unsupervised NMT, by employing an SMT model, which is robust to noisy data, as a prior over a neural TM to guide it in the iterative back-translation process. Finally, LMs have been used in a similar fashion as priors over latent text sequences in discrete latent variable models (Miao and Blunsom, 2016; Havrylov and Titov, 2017; Baziotis et al., 2019). 7 Conclusions In this work, we present a simple approach for incorporating knowledge from monolingual data to NMT . Specifically, we use a LM trained on targetside monolingual data, to regularize the output distributions of a TM. This method is more efficient than alternative approaches that used pretrained LM s, because it is not required during inference. Also, we avoid the translation errors introduced by LM -fusion, because the TM is able to deviate from the prior when needed. We empirically show that while this method works by simply changing the training objective, it achieves better r"
2020.emnlp-main.615,J93-2003,0,0.117719,"Missing"
2020.emnlp-main.615,D16-1139,0,0.0250432,"et al. (2019), GPT-2; Radford et al. (2019)), without compromising speed or efficiency. 3.1 Relation to Knowledge Distillation The regularization term in Eq. (1) resembles knowledge distillation (KD) (Ba and Caruana, 2014; Bucila et al., 2006; Hinton et al., 2015), where the soft output probabilities of a big teacher model are used to train a small compact student model, by minhard target label smoothing language model Figure 1: Targets with LS and LM-prior. imizing their DKL . However, in standard KD the teacher is trained on the same task as the student, like in KD for machine translation (Kim and Rush, 2016). However, the proposed LM-prior is trained on a different task that requires only monolingual data, unlike TM teachers that require parallel data. We exploit this connection to KD and following Hinton et al. (2015) we use a softmaxtemperature parameter τ ≥ 1 to control the smoothi /τ ) ness of the output distributions pi = Pexp(s , j exp(sj /τ )) where si is the un-normalized score of each word i (i.e., logit). Higher values of τ produce smoother distributions. Intuitively, this controls how much information encoded in the tail of the LM’s distributions, we expose to the TM. Specifically, a w"
2020.emnlp-main.615,W18-6319,0,0.0133939,"r the “Base + Prior (3M)” model. 4.1 Experiments We compare the proposed LM-prior with other approaches that incorporate a pretrained LM or regularize the outputs of the TM. First, we consider a vanilla NMT baseline without LS. Next, we compare with fusion techniques, namely shallow-fusion (Gulcehre et al., 2015) and POSTNORM (Stahlberg et al., 2018), which in the original paper outperformed other fusion methods. We also separately compare with label smoothing (LS), because it is another regularization method that uses soft targets. We report detokenized case-sensitive BLEU using sacre- BLEU (Post, 2018)4 , and decode with beam search of size 5. The LMs are fixed during training for both POSTNORM and the prior. We tune the hyper-parameters of each method on the DE→EN dev-set. We set the interpolation weight for shallow-fusion to β=0.1, the smoothing parameter for LS to α = 0.1. For the LM-prior we set the regularization weight to λ=0.5 and the temperature for LKL to τ =2. 4.2 Results First, we use in all methods LMs trained on the same amount of monolingual data, which is 3M sentences. We used the total amount of available Turkish monolingual data (3M) as the lowest common denominator. This i"
2020.emnlp-main.615,E17-2025,0,0.0342767,"scores for LMs trained on each language’s monolingual data, computed on a small heldout validation set per language. TM s. Table 2 lists all their hyperparameters. For the TMs we found that constraining their capacity and applying strong regularization was crucial, otherwise they suffered from over-fitting. We also found that initializing all weights with glorotuniform (Glorot and Bengio, 2010) initialization and using pre-norm residual connections (Xiong et al., 2020; Nguyen and Salazar, 2019), improved stability. We also tied the embedding and the output (projection) layers of the decoders (Press and Wolf, 2017; Inan et al., 2017). We optimized our models with Adam (Kingma and Ba, 2015) with a learning rate of 0.0002 and a linear warmup for the first 8K steps, followed by inverted squared decay and with mini-batches with 5000 tokens per batch. We evaluated each model on the dev set every 5000 batches, by decoding using greedy sampling, and stopped training if the BLEU score did not increase after 10 iterations. For the LM training we followed the same optimization process as for the TMs. However, we use Transformer-large configuration, in order to obtain a powerful LM-prior. Crucially, we did not ap"
2020.emnlp-main.615,W17-3204,0,0.0284764,"e LM can be viewed as teaching the TM about the target language. The proposed approach does not compromise decoding speed, because the LM is used only at training time, unlike previous work that requires it during inference. We present an analysis on the effects that different methods have on the distributions of the TM. Results on two low-resource machine translation datasets show clear improvements even with limited monolingual data. 1 Introduction Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) relies heavily on large parallel corpora (Koehn and Knowles, 2017) and needs careful hyperparameter tuning, in order to work in low-resource settings (Sennrich and Zhang, 2019). A popular approach for addressing data scarcity is to exploit abundant monolingual corpora via data augmentation techniques, such as back-translation (Sennrich et al., 2016). Although back-translation usually leads to significant performance gains (Hoang et al., 2018), it requires training separate models and expensive translation of large amounts of monolingual data. However, when faced with lack of training data, a more principled approach is to consider exploiting prior informatio"
2020.emnlp-main.615,D18-2012,0,0.0225969,". As monolingual data for English and German we use the News Crawls 2016 articles (Bojar et al., 2016) and for Turkish we concatenate all the available News Crawls data from 2010-2018, which contain 3M sentences. For English and German we subsample 3M sentences to match the Turkish data, as well as 30M to measure the effect of stronger LM s. We remove sentences longer than 50 words. Pre-processing We perform punctuation normalization and truecasing and remove pairs, in which either of the sentences has more than 60 words or length ratio over 1.5. The text is tokenized with sentencepiece (SPM; Kudo and Richardson (2018)) with the “unigram” model. For each language we learn a separate SPM model with 16K symbols, trained on its respective side of the parallel data. For English, we train SPM on the concatenation of the English-side of the training data from each dataset, in order to have a single English vocabulary and be able to re-use the same LM. Model Configuration In all experiments, we use the Transformer architecture for both the LMs and 1 2 http://www.statmt.org/wmt18/translation-task.html http://opus.nlpl.eu/SETIMES2.php value TM LM 512 1024 6 8 0.3 1024 4096 6 16 0.3 Table 2: Hyperparameters of the TM"
2020.emnlp-main.615,D15-1166,0,0.100369,"Missing"
2020.emnlp-main.615,D16-1031,0,0.0294066,"roaches that have used posterior regularization to incorporate prior knowledge into NMT. Zhang et al. (2017) exploit linguistic real-valued features, such as dictionaries or length ratios, to construct the distribution for regularizing the TM’s posteriors. Recently, Ren et al. (2019) used posterior regularization for unsupervised NMT, by employing an SMT model, which is robust to noisy data, as a prior over a neural TM to guide it in the iterative back-translation process. Finally, LMs have been used in a similar fashion as priors over latent text sequences in discrete latent variable models (Miao and Blunsom, 2016; Havrylov and Titov, 2017; Baziotis et al., 2019). 7 Conclusions In this work, we present a simple approach for incorporating knowledge from monolingual data to NMT . Specifically, we use a LM trained on targetside monolingual data, to regularize the output distributions of a TM. This method is more efficient than alternative approaches that used pretrained LM s, because it is not required during inference. Also, we avoid the translation errors introduced by LM -fusion, because the TM is able to deviate from the prior when needed. We empirically show that while this method works by simply cha"
2020.emnlp-main.615,D17-1039,0,0.0238213,"-fusion, but with the LM used also during training, instead of used just in inference, and interpolating with λ=1. Fusion methods face the same computational limitation as noisy channel, since the LM needs to be used during inference. Also, probability interpolation methods, such as shallow fusion or POSTNORM , use a fixed weight for all time-steps, which can lead to translation errors. Gated fusion (Gulcehre et al., 2015; Sriram et al., 2018) is more flexible, but requires changing the network architecture. Other Approaches Transfer-learning is another approach for exploiting pretrained LMs. Ramachandran et al. (2017), first proposed to use LMs trained on monolingual corpora to initialize the encoder and decoder of a TM. Skorokhodov et al. (2018) extended this idea to Transformer architectures (Vaswani et al., 2017). This approach requires the TM to have identical architecture to the LM, which can be a limitation if the LM is huge. Domhan and Hieber (2017) used language modeling as extra signal, by training the decoder of a TM also as a LM on target-side monolingual data. Sennrich et al. (2016) replaced the source with a NULL token, while training on monolingual data. Both, reported mixed results, with mar"
2020.emnlp-main.615,P16-1009,1,0.881381,"ave on the distributions of the TM. Results on two low-resource machine translation datasets show clear improvements even with limited monolingual data. 1 Introduction Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) relies heavily on large parallel corpora (Koehn and Knowles, 2017) and needs careful hyperparameter tuning, in order to work in low-resource settings (Sennrich and Zhang, 2019). A popular approach for addressing data scarcity is to exploit abundant monolingual corpora via data augmentation techniques, such as back-translation (Sennrich et al., 2016). Although back-translation usually leads to significant performance gains (Hoang et al., 2018), it requires training separate models and expensive translation of large amounts of monolingual data. However, when faced with lack of training data, a more principled approach is to consider exploiting prior information. Language models (LM) trained on target-side monolingual data have been used for years as priors in statistical machine translation (SMT) (Brown et al., 1993) via the noisy channel model. This approach has been adopted to NMT, with the neural noisy channel (Yu et al., 2017; Yee et a"
2020.emnlp-main.615,P19-1021,0,0.102559,"coding speed, because the LM is used only at training time, unlike previous work that requires it during inference. We present an analysis on the effects that different methods have on the distributions of the TM. Results on two low-resource machine translation datasets show clear improvements even with limited monolingual data. 1 Introduction Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) relies heavily on large parallel corpora (Koehn and Knowles, 2017) and needs careful hyperparameter tuning, in order to work in low-resource settings (Sennrich and Zhang, 2019). A popular approach for addressing data scarcity is to exploit abundant monolingual corpora via data augmentation techniques, such as back-translation (Sennrich et al., 2016). Although back-translation usually leads to significant performance gains (Hoang et al., 2018), it requires training separate models and expensive translation of large amounts of monolingual data. However, when faced with lack of training data, a more principled approach is to consider exploiting prior information. Language models (LM) trained on target-side monolingual data have been used for years as priors in statisti"
2020.emnlp-main.615,W18-2205,0,0.0156839,"ce the same computational limitation as noisy channel, since the LM needs to be used during inference. Also, probability interpolation methods, such as shallow fusion or POSTNORM , use a fixed weight for all time-steps, which can lead to translation errors. Gated fusion (Gulcehre et al., 2015; Sriram et al., 2018) is more flexible, but requires changing the network architecture. Other Approaches Transfer-learning is another approach for exploiting pretrained LMs. Ramachandran et al. (2017), first proposed to use LMs trained on monolingual corpora to initialize the encoder and decoder of a TM. Skorokhodov et al. (2018) extended this idea to Transformer architectures (Vaswani et al., 2017). This approach requires the TM to have identical architecture to the LM, which can be a limitation if the LM is huge. Domhan and Hieber (2017) used language modeling as extra signal, by training the decoder of a TM also as a LM on target-side monolingual data. Sennrich et al. (2016) replaced the source with a NULL token, while training on monolingual data. Both, reported mixed results, with marginal gains. 3 Language Model Prior We propose to move the LM out of the TM and use it as a prior over its decoder, by employing po"
2020.emnlp-main.615,W18-6321,0,0.282985,"oisy channel model. This approach has been adopted to NMT, with the neural noisy channel (Yu et al., 2017; Yee et al., 2019). However, neural noisy channel models face a computational challenge, because they model the “reverse translation probability” p(x|y). Specifically, they require multiple passes over the source sentence x as they generate the target sentence y, or sophisticated architectures to reduce the passes. LM s have also been used in NMT for reweighting the predictions of translation models (TM), or as additional context, via LM-fusion (Gulcehre et al., 2015; Sriram et al., 2018; Stahlberg et al., 2018). But, as the LM is required during decoding, it adds a significant computation overhead. Another challenge is balancing the TM and the LM, whose ratio is either fixed (Stahlberg et al., 2018) or requires changing the model architecture (Gulcehre et al., 2015; Sriram et al., 2018). In this work, we propose to use a LM trained on target-side monolingual corpora as a weakly informative prior. We add a regularization term, which drives the output distributions of the TM to be probable under the distributions of the LM. This gives flexibility to the TM, by enabling it to deviate from the LM when n"
2020.emnlp-main.615,D19-1571,0,0.0790326,"., 2016). Although back-translation usually leads to significant performance gains (Hoang et al., 2018), it requires training separate models and expensive translation of large amounts of monolingual data. However, when faced with lack of training data, a more principled approach is to consider exploiting prior information. Language models (LM) trained on target-side monolingual data have been used for years as priors in statistical machine translation (SMT) (Brown et al., 1993) via the noisy channel model. This approach has been adopted to NMT, with the neural noisy channel (Yu et al., 2017; Yee et al., 2019). However, neural noisy channel models face a computational challenge, because they model the “reverse translation probability” p(x|y). Specifically, they require multiple passes over the source sentence x as they generate the target sentence y, or sophisticated architectures to reduce the passes. LM s have also been used in NMT for reweighting the predictions of translation models (TM), or as additional context, via LM-fusion (Gulcehre et al., 2015; Sriram et al., 2018; Stahlberg et al., 2018). But, as the LM is required during decoding, it adds a significant computation overhead. Another cha"
2020.emnlp-main.615,P17-1139,0,0.0174501,"nto the future”. However, in our work we address a different problem (low-resource NMT) and have different motivation. Also, we consider auto-regressive LMs as priors, which have clear interpretation, unlike BERT that is not strictly a LM and requires bidirectional context. Note that, large pretrained LMs, such as BERT or GPT-2, have not yet achieved the transformative results in NMT that we observe in natural language understanding tasks (e.g., GLUE benchmark (Wang et al., 2019)). There are also other approaches that have used posterior regularization to incorporate prior knowledge into NMT. Zhang et al. (2017) exploit linguistic real-valued features, such as dictionaries or length ratios, to construct the distribution for regularizing the TM’s posteriors. Recently, Ren et al. (2019) used posterior regularization for unsupervised NMT, by employing an SMT model, which is robust to noisy data, as a prior over a neural TM to guide it in the iterative back-translation process. Finally, LMs have been used in a similar fashion as priors over latent text sequences in discrete latent variable models (Miao and Blunsom, 2016; Havrylov and Titov, 2017; Baziotis et al., 2019). 7 Conclusions In this work, we pre"
2020.emnlp-main.615,W16-2301,1,\N,Missing
2020.emnlp-main.615,N19-1423,0,\N,Missing
2020.emnlp-main.615,D19-5611,0,\N,Missing
2020.emnlp-main.615,D19-5603,0,\N,Missing
2020.iwltp-1.3,W19-6723,1,0.787839,"Missing"
2020.iwltp-1.3,2020.iwltp-1.7,1,0.806176,"Missing"
2020.lrec-1.471,E06-1032,0,0.505949,"erms of standard measurements. Previously dominant phrase-based and syntax-based Statistical Machine Translation (SMT) techniques (Koehn et al., 2007; Junczys-Dowmunt et al., 2016) naturally take into account phrasal components, and there has been significant research on MWEs in these frameworks; however, for NMT, due to a lack of phrasal segmentation, it is less obvious how to address specific language phenomena such as MWEs. Moreover, while standard metrics are effective in terms of system comparison, their ability to account for more fine-grained improvements in MT is less straightforward (Callison-Burch et al., 2006), and their effectiveness has been questioned. Therefore, evaluating the performance of NMT architectures in translating MWEs remains an open challenge. The aim of this study is to empirically verify whether integrating information on MWEs either through targeted training examples or through explicit annotation in the target language can help disambiguating between simple phrasal units and non-compositional expressions, and thus be beneficial to NMT. In our first approach, we try augmenting our training data with entries from a bilingual and a monolingual MWE dictionary, adding a relatively sm"
2020.lrec-1.471,calzolari-etal-2002-towards,0,0.149513,"Missing"
2020.lrec-1.471,2014.iwslt-evaluation.1,0,0.0413729,"Missing"
2020.lrec-1.471,W14-3346,0,0.0133235,"ural networks (RNNs), one encoding the input forward from left to right, the other backward from right to left, so that all the context from the input is available at each time step (not only the preceding words), and the hidden state of a word is represented by the concatenation of these hidden states. In the implementation we employ in all our experiments, training is performed by cross-entropy minimization on the parallel training corpus with Adam (Kingma and Ba, 2014), a variant of the stochastic gradient descent algorithm. Periodic validation is performed on smoothed sentence-level BLEU (Chen and Cherry, 2014) and early stopping on this metric is applied for training stabilization. 3.2. Integrating input features: factored NMT In order to allow to specify for arbitrary linguistic input features for each word, in the methods we describe in Sections 6.3.2. and 6.4. we represent the encoder input as a concatenation of input features, as originally proposed by Sennrich and Haddow (2016). The idea is that, for each feature, a separate embedding vector is created, and all feature vectors are then concatenated to form a factored representation of the input word, whose length is equal to the total embeddin"
2020.lrec-1.471,J17-4005,0,0.0604607,"Missing"
2020.lrec-1.471,W14-3348,0,0.0387646,"in the target text, (iii) evaluating the translation quality based on criteria such as adequacy (full, partial, etc.) and fluency (fluent, non–native, disfluent, etc.) (Ramisch et al., 2013). The first two tasks are usually completed by MWE extraction tools and automatic alignment, the third is usually carried out via instance-based manual inspection of the output translation. When not evaluated in terms of accuracy over a manually compiled gold standard (Monti et al., 2015), translation quality is evaluated through standard measures such as word–based BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) or character–based chrF (Popović, 2015), but none of them specifically addresses the translation of MWEs (although character–based metrics have similarities to our proposed Score_mwe measure, as we will discuss in Section 4.2.). Background Neural machine translation The neural machine translation toolkit used in all our experiments, Nematus (Sennrich et al., 2017), implements a bi-directional encoder-decoder architecture with attention, similar to the model described by Bahdanau et al. (2015). The Encoder consists of two single recurrent neural networks (RNNs), one encoding the input forward"
2020.lrec-1.471,D18-1045,0,0.0218367,"Missing"
2020.lrec-1.471,I17-1004,0,0.0301938,"Missing"
2020.lrec-1.471,S17-1006,0,0.0239901,"tating MWEs on the source side, which consistently improved general and MWE translation quality and markedly outperformed its baseline according to all metrics. A natural extension of this method will be using a more sophisticated MWE identification method to also include discontinuous MWEs or MWEs appearing in marked configurations (such as topicalized or passive forms). In conclusion, we believe that efforts should be made in the direction of improving the existing MWE identification systems. Recent approaches have experimented on developing MWE identification methods using neural networks (Gharbieh et al., 2017) (Klyueva et al., 2017), which however are supervised and thus heavily rely on the availability of annotated resources. In the future, it may be interesting to verify whether it is possible to automatically induce MWE identification with neural networks in an unsupervised fashion, for example modelling the internal compositionality of an expression by embedding it as a whole, and compare its vector representation with that of its single components. If such techniques prove successful, it may be possible to integrate them into a neural machine translation architecture, for instance by making MW"
2020.lrec-1.471,W17-1707,0,0.166739,"e side, which consistently improved general and MWE translation quality and markedly outperformed its baseline according to all metrics. A natural extension of this method will be using a more sophisticated MWE identification method to also include discontinuous MWEs or MWEs appearing in marked configurations (such as topicalized or passive forms). In conclusion, we believe that efforts should be made in the direction of improving the existing MWE identification systems. Recent approaches have experimented on developing MWE identification methods using neural networks (Gharbieh et al., 2017) (Klyueva et al., 2017), which however are supervised and thus heavily rely on the availability of annotated resources. In the future, it may be interesting to verify whether it is possible to automatically induce MWE identification with neural networks in an unsupervised fashion, for example modelling the internal compositionality of an expression by embedding it as a whole, and compare its vector representation with that of its single components. If such techniques prove successful, it may be possible to integrate them into a neural machine translation architecture, for instance by making MWE identification part o"
2020.lrec-1.471,P07-2045,1,0.0427962,"Kim, 2010; Constant et al., 2017). In the last few years, Neural Machine Translation (NMT) has proved the best performing framework compared to previous methodologies, with neural architectures producing ever more natural-sounding target language. Even so, NMT output is sometimes a poor translation of the source sentence (Nguyen and Chiang, 2018) and it is therefore important to investigate specific linguistic phenomena and improve translation quality not only in terms of standard measurements. Previously dominant phrase-based and syntax-based Statistical Machine Translation (SMT) techniques (Koehn et al., 2007; Junczys-Dowmunt et al., 2016) naturally take into account phrasal components, and there has been significant research on MWEs in these frameworks; however, for NMT, due to a lack of phrasal segmentation, it is less obvious how to address specific language phenomena such as MWEs. Moreover, while standard metrics are effective in terms of system comparison, their ability to account for more fine-grained improvements in MT is less straightforward (Callison-Burch et al., 2006), and their effectiveness has been questioned. Therefore, evaluating the performance of NMT architectures in translating"
2020.lrec-1.471,2005.mtsummit-papers.11,0,0.10806,"Missing"
2020.lrec-1.471,W02-0109,0,0.0510288,"Missing"
2020.lrec-1.471,N18-1031,0,0.0489712,"Missing"
2020.lrec-1.471,P02-1040,0,0.109735,"(ii) identifying their translation in the target text, (iii) evaluating the translation quality based on criteria such as adequacy (full, partial, etc.) and fluency (fluent, non–native, disfluent, etc.) (Ramisch et al., 2013). The first two tasks are usually completed by MWE extraction tools and automatic alignment, the third is usually carried out via instance-based manual inspection of the output translation. When not evaluated in terms of accuracy over a manually compiled gold standard (Monti et al., 2015), translation quality is evaluated through standard measures such as word–based BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) or character–based chrF (Popović, 2015), but none of them specifically addresses the translation of MWEs (although character–based metrics have similarities to our proposed Score_mwe measure, as we will discuss in Section 4.2.). Background Neural machine translation The neural machine translation toolkit used in all our experiments, Nematus (Sennrich et al., 2017), implements a bi-directional encoder-decoder architecture with attention, similar to the model described by Bahdanau et al. (2015). The Encoder consists of two single recurrent neural networks"
2020.lrec-1.471,W15-3049,0,0.0886417,"quality based on criteria such as adequacy (full, partial, etc.) and fluency (fluent, non–native, disfluent, etc.) (Ramisch et al., 2013). The first two tasks are usually completed by MWE extraction tools and automatic alignment, the third is usually carried out via instance-based manual inspection of the output translation. When not evaluated in terms of accuracy over a manually compiled gold standard (Monti et al., 2015), translation quality is evaluated through standard measures such as word–based BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) or character–based chrF (Popović, 2015), but none of them specifically addresses the translation of MWEs (although character–based metrics have similarities to our proposed Score_mwe measure, as we will discuss in Section 4.2.). Background Neural machine translation The neural machine translation toolkit used in all our experiments, Nematus (Sennrich et al., 2017), implements a bi-directional encoder-decoder architecture with attention, similar to the model described by Bahdanau et al. (2015). The Encoder consists of two single recurrent neural networks (RNNs), one encoding the input forward from left to right, the other backward f"
2020.lrec-1.471,C10-3015,0,0.0618311,"Missing"
2020.lrec-1.471,2013.mtsummit-wmwumttt.8,0,0.0115849,"a), outperforming state-of-the-art results for English-Turkish. Our second method (6.2.2.) is inspired by these approaches, which we extend by applying them specifically to MWEs. Most studies focussing on MWE translation evaluation aim to evaluate how well the MWEs identified in the source text are translated into the target language. This involves (i) identifying the MWEs in the source text, (ii) identifying their translation in the target text, (iii) evaluating the translation quality based on criteria such as adequacy (full, partial, etc.) and fluency (fluent, non–native, disfluent, etc.) (Ramisch et al., 2013). The first two tasks are usually completed by MWE extraction tools and automatic alignment, the third is usually carried out via instance-based manual inspection of the output translation. When not evaluated in terms of accuracy over a manually compiled gold standard (Monti et al., 2015), translation quality is evaluated through standard measures such as word–based BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) or character–based chrF (Popović, 2015), but none of them specifically addresses the translation of MWEs (although character–based metrics have similarities to our"
2020.lrec-1.471,W16-2209,0,0.332584,"formed by cross-entropy minimization on the parallel training corpus with Adam (Kingma and Ba, 2014), a variant of the stochastic gradient descent algorithm. Periodic validation is performed on smoothed sentence-level BLEU (Chen and Cherry, 2014) and early stopping on this metric is applied for training stabilization. 3.2. Integrating input features: factored NMT In order to allow to specify for arbitrary linguistic input features for each word, in the methods we describe in Sections 6.3.2. and 6.4. we represent the encoder input as a concatenation of input features, as originally proposed by Sennrich and Haddow (2016). The idea is that, for each feature, a separate embedding vector is created, and all feature vectors are then concatenated to form a factored representation of the input word, whose length is equal to the total embedding size. This is done for an arbitrary number of input features |F |according to the equation 1 ~ ~hj−1 ) ~ (k|F |E X k ) + U h~j = tanh(W k=1 k j (1) where k is vector concatenation, Ek is a feature embedding ~ and matrix, Kk is the vocabulary size of input feature k, W ~ U are weight matrices. 4. 4.1. Evaluation methods BLEU score For general translation evaluation we use deto"
2020.lrec-1.471,P16-1009,1,0.921132,"is affected by the accuracy of the selected tool and by noise of the data, we aim to minimize false positives by using a manually compiled bilingual dictionary to select the MWE candidates on the source side. Other studies exploit monolingual data to learn a better language model and integrate it into the decoder (Gulcehre et al., 2015). Monolingual data in the target language are paired with synthetic back-translations on the source side (Edunov et al., 2018), and this new bi-text is used as additional training text. This method is effective in improving translation quality, as described in Sennrich et al. (2016a), outperforming state-of-the-art results for English-Turkish. Our second method (6.2.2.) is inspired by these approaches, which we extend by applying them specifically to MWEs. Most studies focussing on MWE translation evaluation aim to evaluate how well the MWEs identified in the source text are translated into the target language. This involves (i) identifying the MWEs in the source text, (ii) identifying their translation in the target text, (iii) evaluating the translation quality based on criteria such as adequacy (full, partial, etc.) and fluency (fluent, non–native, disfluent, etc.) ("
2020.lrec-1.471,P16-1162,1,0.549315,"is affected by the accuracy of the selected tool and by noise of the data, we aim to minimize false positives by using a manually compiled bilingual dictionary to select the MWE candidates on the source side. Other studies exploit monolingual data to learn a better language model and integrate it into the decoder (Gulcehre et al., 2015). Monolingual data in the target language are paired with synthetic back-translations on the source side (Edunov et al., 2018), and this new bi-text is used as additional training text. This method is effective in improving translation quality, as described in Sennrich et al. (2016a), outperforming state-of-the-art results for English-Turkish. Our second method (6.2.2.) is inspired by these approaches, which we extend by applying them specifically to MWEs. Most studies focussing on MWE translation evaluation aim to evaluate how well the MWEs identified in the source text are translated into the target language. This involves (i) identifying the MWEs in the source text, (ii) identifying their translation in the target text, (iii) evaluating the translation quality based on criteria such as adequacy (full, partial, etc.) and fluency (fluent, non–native, disfluent, etc.) ("
2020.lrec-1.471,E17-3017,1,0.824665,"slation. When not evaluated in terms of accuracy over a manually compiled gold standard (Monti et al., 2015), translation quality is evaluated through standard measures such as word–based BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) or character–based chrF (Popović, 2015), but none of them specifically addresses the translation of MWEs (although character–based metrics have similarities to our proposed Score_mwe measure, as we will discuss in Section 4.2.). Background Neural machine translation The neural machine translation toolkit used in all our experiments, Nematus (Sennrich et al., 2017), implements a bi-directional encoder-decoder architecture with attention, similar to the model described by Bahdanau et al. (2015). The Encoder consists of two single recurrent neural networks (RNNs), one encoding the input forward from left to right, the other backward from right to left, so that all the context from the input is available at each time step (not only the preceding words), and the hidden state of a word is represented by the concatenation of these hidden states. In the implementation we employ in all our experiments, training is performed by cross-entropy minimization on the"
2020.ngt-1.1,W19-5301,0,0.0795298,"Missing"
2020.ngt-1.1,N19-1423,0,0.00776584,"o 15 system submission papers. We elicted two double-blind reviews for each submission, avoiding conflicts of interest. With regards to thematology there were 8 papers with a focus on Natural Language Generation and 8 with the application of Machine Translation 1 Proceedings of the 4th Workshop on Neural Generation and Translation (WNGT 2020), pages 1–9 c Online, July 10, 2020. 2020 Association for Computational Linguistics www.aclweb.org/anthology/D19-56%2d in mind. The underlying emphasis across submissions was placed this year on capitalizing on the use of pre-training models (e.g., BERT; (Devlin et al., 2019) especially for low-resource datasets. The quality of the accepted publications was very high; there was a significant drop in numbers though in comparison to last year (36 accepted papers from 68 submissions) which is most likely due to the extra overhead on conducting research under lockdown policies sanctioned globally due to COVID19 pandemic. 3 GPU is relatively small compared to the NVIDIA V100 GPU, but the newer Turing architecture introduces support for 4-bit and 8-bit integer operations in Tensor Cores. In practice, however, participants used floating-point operations on the GPU even t"
2020.ngt-1.1,D13-1176,0,0.0312763,"Second, we describe the results of the three shared tasks 1) efficient neural machine translation (NMT) where participants were tasked with creating NMT systems that are both accurate and efficient, and 2) document-level generation and translation (DGT) where participants were tasked with developing systems that generate summaries from structured data, potentially with assistance from text in another language and 3) STAPLE task: creation of as many possible translations of a given input text. This last shared task was organised by Duolingo. 1 Introduction 2 Neural sequence to sequence models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) are the workhorse behind a wide variety of different natural language processing tasks such as machine translation, generation, summarization and simplification. The 4th Workshop on Neural Machine Translation and Generation (WNGT 2020) provided a forum for research in applications of neural models to machine translation and other language generation tasks (including summarization, NLG from structured data, dialog response generation, among others). Overall, the workshop was held with two goals. First, it aimed to synthesize the current state of"
2020.ngt-1.1,W04-1013,0,0.0806851,"of parameters and 8-bit quantization. OpenNMT’s small lower-quality models have low CPU RAM and Docker image size; UEdin is Pareto-optimal for higher-quality models. OpenNMT was the only team to optimize for these metrics in their system description. In their multicore CPU submission, OpenNMT shared memory amongst processes while other participants simply used multiple processes with copies of the model. 4 4.1 Evaluation Measures We employ standard evaluation metrics for the tasks above along two axes following (Hayashi et al., 2019): Textual Accuracy: BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) as measures for surface-level texutal accuracy compared to reference summaries. Document Generation and Translation Task Following the previous workshop, we continued with the shared task of document-level generation and translation. This task is motivated as the central evaluation testbed for document-level generation systems with different types of inputs by providing parallel dataset consisting of structured tables and text in two languages. We host various tracks within the testbed based on input and output constraints and investigate and contrast the system differences. In particular, we"
2020.ngt-1.1,2020.ngt-1.28,0,0.0265263,"tput, but in certain cases, it is desirable to have many possible translations of a given input text. At Duolingo, the world’s largest online language-learning platform,7 we grade translationbased challenges with sets of human-curated acceptable translation options. Given the many ways of expressing a piece of text, these sets are slow to create, and may be incomplete. This process is ripe for improvement with the aid of rich multi-output translation and paraphrase systems. To this end, we introduce a shared task called STAPLE: Simultaneous Translation and Paraphrasing for Language Education (Mayhew et al., 2020). 4.4 5.1 4.3 Baselines We prepared two baselines for different tracks: FairSeq-19 We use FairSeq (Ng et al., 2019) (WMT’19 single model6 ) for MT and MT+NLG tracks. Submitted Systems One team participated in the task, who focused on the German-English MT track of the task. In this shared task, participants are given a training set consisting of 2500 to 4000 English sentences (or prompts), each of which is paired with a list of comprehensive translations in the target language, weighted and ordered by normalized learner response frequency. At test time, participants are given 500 English promp"
2020.ngt-1.1,D18-1325,0,0.021768,"guage, weighted and ordered by normalized learner response frequency. At test time, participants are given 500 English prompts, and are required to produce the set of comprehensive translations for each prompt. We also provide a high-quality automatic reference translation for each prompt, in the event that a participant wants to work on paraphrase-only approaches. The target languages were Hungarian, Japanese, Korean, Portuguese, and Vietnamese. Team FJWU developed a system around Transformer-based sequence-to-sequence model. Additionally, the model employed hierarchical attention following (Miculicich et al., 2018) for both encoder and decoder to account for the documentlevel context. The system was trained in a twostage process, where a base (sentence-level) NMT model was trained followed by the training of hierarchcal attention networks component. To handle the scarcity of in-domain translation data, they experimented with upsizing the in-domain data up to three times to construct training data. Their ablation experiments showed that this upsizing of in-domain data is effective at increasing the BLEU score. 4.5 Task Description 5.2 Submitted Systems There were 20 participants who submitted to the deve"
2020.ngt-1.1,W19-5333,0,0.0306894,"Missing"
2020.ngt-1.1,P02-1040,0,0.107839,"guage. mostly driven by the number of parameters and 8-bit quantization. OpenNMT’s small lower-quality models have low CPU RAM and Docker image size; UEdin is Pareto-optimal for higher-quality models. OpenNMT was the only team to optimize for these metrics in their system description. In their multicore CPU submission, OpenNMT shared memory amongst processes while other participants simply used multiple processes with copies of the model. 4 4.1 Evaluation Measures We employ standard evaluation metrics for the tasks above along two axes following (Hayashi et al., 2019): Textual Accuracy: BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) as measures for surface-level texutal accuracy compared to reference summaries. Document Generation and Translation Task Following the previous workshop, we continued with the shared task of document-level generation and translation. This task is motivated as the central evaluation testbed for document-level generation systems with different types of inputs by providing parallel dataset consisting of structured tables and text in two languages. We host various tracks within the testbed based on input and output constraints and investigate and contrast the system differen"
2020.ngt-1.1,W18-6319,0,0.0283845,"Missing"
2020.wmt-1.5,2020.acl-main.688,0,0.063009,"Missing"
2020.wmt-1.5,W19-5304,1,0.842298,"th language model objectives and translation using an unrelated high-resource language pair (German-English), and iterative backtranslation. For English-Inuktitut, we explore the use of multilingual systems, which, despite not being part of the primary submission, would have achieved the best results on the test set. 1 Language direction Dev Test EN→TA TA→EN 12.30 21.00 8.40 16.60 EN→IN IN→EN 27.0 48.8 8.2 23.0 Table 1: Summary of results for all UEDIN submissions according to the automatic evaluation (BLEU). 2 Introduction English↔Tamil As for our English↔Gujarati systems last year at WMT19 (Bawden et al., 2019), we use pretraining and data augmentation to tackle the low-resource language pair English–Tamil. Our experiments show that pre-training, training on backtranslated data and then fine-tuning is useful in both directions, although we introduce slight variations in the training and fine-tuning approaches used for each language direction. The University of Edinburgh participated in the WMT20 news translation shared task for EnglishTamil and English-Inuktitut in both translation directions.1,2 Neither language pair benefits from large quantities of parallel data, so we approach training using dif"
2020.wmt-1.5,W09-0432,0,0.0387841,"mixture of De-En and mBART pretraining for intermediate MT models used for data augmentation (see the next paragraph). 3. We use this new backtranslated data together with the genuine parallel data oversampled 7 times to train a second mBART-pretrained Ta→En model. After early stopping, we continue training using genuine parallel data only. We then use this model to backtranslate all the monolingual Tamil data. Iterative backtranslation Data augmentation by backtranslating monolingual data has long been used in MT to provide greater amounts of in-domain parallel data in low resource settings (Bertoldi and Federico, 2009; Bojar and Tamchyna, 2011). We use backtranslation to translate the monolingual in-domain English and Tamil texts into the other language using an intermediate MT model and use the resulting synthetic parallel data to train new MT models. We apply this iteratively (Hoang et al., 2018), as shown in Figure 1, to produce successively better MT models, initialising the models at each stage using either mBART or De-En pretraining. The intermediate MT models used to produce backtranslations are in white and the final models, which are then fine-tuned (as specified in the section entitled Final mode"
2020.wmt-1.5,W11-2138,0,0.0245002,"etraining for intermediate MT models used for data augmentation (see the next paragraph). 3. We use this new backtranslated data together with the genuine parallel data oversampled 7 times to train a second mBART-pretrained Ta→En model. After early stopping, we continue training using genuine parallel data only. We then use this model to backtranslate all the monolingual Tamil data. Iterative backtranslation Data augmentation by backtranslating monolingual data has long been used in MT to provide greater amounts of in-domain parallel data in low resource settings (Bertoldi and Federico, 2009; Bojar and Tamchyna, 2011). We use backtranslation to translate the monolingual in-domain English and Tamil texts into the other language using an intermediate MT model and use the resulting synthetic parallel data to train new MT models. We apply this iteratively (Hoang et al., 2018), as shown in Figure 1, to produce successively better MT models, initialising the models at each stage using either mBART or De-En pretraining. The intermediate MT models used to produce backtranslations are in white and the final models, which are then fine-tuned (as specified in the section entitled Final model creation) are in grey. EN"
2020.wmt-1.5,W19-5206,0,0.0409474,"wn to be an effective and simple way of boosting performance (Kocmi and Bojar, 2018; Aji et al., 2020). For the De-En models, we had to choose between (i) initialising only model parameters and (ii) preserving all model and training parameters from the parent model (similar to Grundkiewicz et al. (2019)). We chose the first option as it produced better results in our experiments. For mBART pretraining, we use all Tamil and English monolingual data without shuffling or deduplication. We tag the input segments with a language tag and a domain tag: either in-domain (news) or out-of-domain as in (Caswell et al., 2019). For XLM pretraining we use the deduplicated and shuffled corpus (since cross-sentence context is not needed) and we subsample the English because of computing cost. We also use domain tags, with language information provided in the form of language embeddings as per the standard implementation. For De-En pre-training, we use all De-En parallel data described in Table 2, with a joint EnglishTamil-German vocabulary. We experiment with pretraining models in the two directions (De→En and En→De) and find that the De→En model produces better results when fine-tuned on TA-EN data. Approach used We"
2020.wmt-1.5,W19-4427,0,0.0557209,"Missing"
2020.wmt-1.5,W18-6325,0,0.0679772,"Missing"
2020.wmt-1.5,W18-2703,0,0.0167668,"tinue training using genuine parallel data only. We then use this model to backtranslate all the monolingual Tamil data. Iterative backtranslation Data augmentation by backtranslating monolingual data has long been used in MT to provide greater amounts of in-domain parallel data in low resource settings (Bertoldi and Federico, 2009; Bojar and Tamchyna, 2011). We use backtranslation to translate the monolingual in-domain English and Tamil texts into the other language using an intermediate MT model and use the resulting synthetic parallel data to train new MT models. We apply this iteratively (Hoang et al., 2018), as shown in Figure 1, to produce successively better MT models, initialising the models at each stage using either mBART or De-En pretraining. The intermediate MT models used to produce backtranslations are in white and the final models, which are then fine-tuned (as specified in the section entitled Final model creation) are in grey. EN-TA: Pretrained mBART Pretrained mBART 2 4 1 TA-EN: Pretrained mBART 3 A 5. We use 5M of these final backtranslations along with the Ta-En genuine parallel data oversampled 15 times to fine-tune a De-En pretrained model and use this to generate the final back"
2020.wmt-1.5,P07-2045,1,0.0298104,"best. For Ta→En, we finetune on genuine parallel data and 500k of the top scored backtranslations.7 For En→Ta we fine-tune on a mixture of genuine parallel data, synthetic data produced using multi-agent dual learning (MADL; Wang et al., 2019; Kim et al., 2019) and the top 1M backtranslations. This MADL data comprises a mixture of forward translations and backtranslations of the parallel data created using intermediate models in both directions. We also carried out preliminary experiments with multilingual training using other Indian languages and experiments with phrase-based MT using Moses (Koehn et al., 2007) but they did not achieve good results. De-En pretraining For models with De-En pretraining, we trained a SentencePiece model with a vocabulary size of 32k on roughly equal amounts of Tamil, English and German data (subsampling Ger8 We implement an online “training harness” that reads monolingual sentences in English and Tamil, converts them to mBART training examples by applying noise and sends them to the Marian training process. Code and training scripts: https://github.com/Avmb/marian-mBART 7 Scoring is done using dual conditional cross-entropy filtering as specified in Footnote 6. 95 man"
2020.wmt-1.5,W18-6478,0,0.0273804,"successive models used for backtranslations (BT) (as shown in Figure 1). Each row uses backtranslations produced by the system from the previous row. 5 Iterative backtranslation Legend 4. We repeat step 2 with this latest backtranslated data, generating the final backtranslations to be used for the Ta→En direction. In addition to the described strategy, we also experimented with training different pretrained models using backtranslations produced by different ing steps are filtered using the same processing as described in Section 2.1, filtered using dual conditional cross-entropy filtering (Junczys-Dowmunt, 2018) and the top sentences are selected to train the next step. The En→Ta backtranslations at this step and the follow94 2.3 models (e.g. training an mBart pretrained model on XLM-produced backtranslations). We report a small selection of these experiments here for one of the backtranslation steps, comparing the use of alternatives to system 2 (from Figure 1). These results are shown in Table 5: (i) a pretrained mBART model trained on backtranslations from each of the pretrained models, and (ii) a pretrained De-En model trained on XLM backtranslations. For this particular step of the iterative pro"
2020.wmt-1.5,D18-2012,0,0.0425258,"News 2019) Synthetic (from iu CommonCrawl) News Commentary Europarl Paracrawl Yandex UN Table 7: Data used for the multilingual EnglishInuktitut models. Size is given in sentence pairs. For the bilingual systems, our preprocessing pipeline consisted of corpus cleaning and segmentation. For corpus cleaning, we used the clean-corpus-n.perl script from the Moses toolkit (Koehn et al., 2007). This applies a maximum length threshold of 80 as well as removing empty sentences and sentence pairs with length ratios greater than 9:1. For segmentation, we trained language-specific SentencePiece models (Kudo and Richardson, 2018) with a vocabulary size of 32,000 BPE subwords and a vocabulary threshold of 50. English↔Inuktitut Compared to English-Tamil, the English-Inuktitut language pair is relatively well-resourced at approximately 1.3M sentence pairs. As such we were able to train conventional bilingual Transformer systems, which formed the basis of our submission. We also trained multilingual systems, but opted not to use these in our submission as results on the dev set did not appear to be promising (although 96 Preprocessing was identical for the multilingual systems except that for the English→ {de,iu,ru} we ad"
2020.wmt-1.5,2020.tacl-1.47,0,0.0242686,"step approach to training our models, consisting of: (i) pre-training model parameters using either an mBART language model or a translation model for the highly resourced De-En language pair, (ii) iterative backtranslation to produce synthetic parallel data of increasing quality, and (iii) final model creation consisting of finetuning pretrained models using both genuine parallel and backtranslated data. We provide the details of these three steps below. System Pre-training We experimented with several pretraining objectives: language modelling using XLM (Lample and Conneau, 2019a) or mBART (Liu et al., 2020), and MT pre-training using a higher-resourced language pair (namely EnglishGerman). Using a higher-resourced language pair for pretraining, even if this pair is unrelated to the language pair on which the model is fine-tuned, has EN→TA dev test TA→EN dev test Parallel-only baseline 5.10 3.10 10.10 10.60 XLM mBART De-En 7.44 7.40 7.30 5.00 4.65 5.00 13.44 14.00 13.60 10.90 13.40 14.20 Table 3: Comparison of pre-training methods for EN↔TA (BLEU) after fine-tuning on parallel data. 4 An alphabetic character is one belonging to the language in question: the Latin alphabet for English and the Tami"
2020.wmt-1.5,P02-1040,0,0.106491,"Missing"
2020.wmt-1.5,W18-6319,0,0.0395405,"Missing"
2020.wmt-1.5,E17-3017,1,0.871261,"Missing"
2020.wmt-1.5,P16-1162,1,0.389901,"Missing"
2021.eacl-main.90,P19-1581,0,0.0194285,"9) also use BERT for contextual data augmentation, but with a goal of improving language model tasks such as sentiment analysis. The constraints for this task are very different; rather than having to produce a translation for augmented data, these approaches have to maintain the sentiment label of the sentences. Gao et al. (2019) work in a similar context to us, but focus on overall translation performance rather than learning new words, and apply contextual data augmentation during the training step, thus removing the challenge of adapting to new data as it becomes available. Similar to us, Huck et al. (2019) focus on improving the MT of words which are unseen in the training set. They use bilingual lexicons to hypothesize translations for their unseen terms. They find these translations in monolingual target side data and backtranslate them inserting the unseen term. They show that this improves translation performance in the medical domain. However they do not analyze the accuracy of translation of the novel terms, or explore how fast you can learn from very few examples. 3 Lifelong learning from post-edits MT models are inevitably adapted towards the topics and vocabulary from the time period a"
2021.eacl-main.90,W17-4774,0,0.016344,"ics emerge, bringing new vocabulary that has been rarely or never seen in the data used for training. The willingness of a journalist to use MT technology is dependent on the general quality of the models, but also on whether they can learn from the journalist’s corrections, to avoid them having to correct the same errors time and time again. ∗ Work done at University of Edinburgh Various strategies have been explored to learn from a journalist’s post-edits. One option is to use an automatic post-editing (APE) model trained on the journalist’s post-edits. However, state-of-theart APE systems (Junczys-Dowmunt and Grundkiewicz, 2017) typically require large numbers of post-edits for training, which are rarely available or hard to generate (particularly for low-resource languages). An alternative, commonly used strategy is to fine-tune models to in-domain data, but this is prone to overfitting (Miceli Barone et al., 2017). More advanced ways of continually learning through fine-tuning have been explored, selecting similar training instances based on their similarity with test sentences (Li et al., 2018; Turchi et al., 2017). These methods achieve good results according to automatic MT metrics but can also overfit when trai"
2021.eacl-main.90,W18-2716,0,0.0375957,"Missing"
2021.eacl-main.90,N18-2072,0,0.0284468,"ion in (Kothur et al., 2018). A challenge these works report with finetuning is overfitting, which we encounter systematically when evaluating our various techniques. Works on fine-tuning also explore several regularization techniques (Simianer et al., 2019) when adapting to new data, which we choose to leave out of our comparison due to added hyper-parameter choices and complexity of implementation in our experiments – we do however believe that future 1 https://gitlab.com/farid-fari/ fewshot-learning work implementing these techniques could potentially outperform ours. Fadaee et al. (2017); Kobayashi (2018); Wu et al. (2019) and Gao et al. (2019) explore a similar contextual data augmentation technique, albeit in different scenarios and with different goals. This technique synthesizes new sentences by using sentences from the training set and substituting different words into them. In (Fadaee et al., 2017) the goal is to enhance overall translation performance by focusing on words that appear rarely in the training data, but in our case we are training our system to learn new words which were not in the training set at all, based simply on a ground-truth translation of this new word by a human."
2021.eacl-main.90,D17-1156,0,0.0516566,"Missing"
2021.eacl-main.90,P02-1040,0,0.110438,"ber of corrections that need to be seen. We show in our experiments on the GujaratiEnglish language pair that it is not only possible 1049 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1049–1062 April 19 - 23, 2021. ©2021 Association for Computational Linguistics to surpass the accuracy of our baseline fine-tuning approach, but also of a reference model which has already seen the new words dozens to hundreds of times during training. However we find that in most cases adaptation to new words comes at a variable cost in BLEU (Papineni et al., 2002), due to overfitting to the new examples. We show that this cost can be kept at a minimum by padding our data-augmentation approach with randomly selected sentences from our training set. The appropriate choice of hyper-parameters is also important for final performance. Our code is freely available online.1 2 Related work The topic of few-shot learning from post-edits is relatively novel, and we were therefore left with few comparison points. A somewhat similar task that requires quick adaptation is low-resource MT, for which transfer learning (Zoph et al., 2016) and meta-learning (Gu et al.,"
2021.eacl-main.90,P16-1162,1,0.221118,"data. However, only the genuine parallel data is used to select augmentation candidates to ensure their high quality. For testing, we use the concatenation of newstest2019 and newsdev2019. Preprocessing All data augmentation was run after tokenization but before subword segmentation to keep a consistent notion of a ‘word’. We 3 We experimented with SGD but observed no significant differences with Adam. 4 http://data.statmt.org/wmt19_systems/ en-gu/train.sh 1053 first apply tokenization using the Moses tokenizer (Koehn et al., 2007) and then apply sub-word segmentation using the BPE strategy (Sennrich et al., 2016) and the fastbpe implementation.5,6 Word alignment is carried out using fast align (Dyer et al., 2013).7 number of fine-tuning sentences and the original number of reference sentences provided by the journalist. For a word w, as the number of occurrences offered to the model grows from 1 to 20, the number of fine-tuning sentences grows from r to 20r. This ratio r is calculated as follows: Evaluation data The rare words to filter out are chosen to be the 100 rarest words in the training set that appear at least 5 times in the test set and 20 times in the training set, and are manually filtered"
2021.eacl-main.90,N19-1206,0,0.151766,"ually adapt a model to post-edits. This approach does not apply to our scenario, where new words appear and therefore a similarity search cannot yield sentences containing these new words. Our baseline approach (which we call finetune) is present in these works, known as adaptation a posteriori in (Turchi et al., 2017) and also appears as single-sentence adaptation in (Kothur et al., 2018). A challenge these works report with finetuning is overfitting, which we encounter systematically when evaluating our various techniques. Works on fine-tuning also explore several regularization techniques (Simianer et al., 2019) when adapting to new data, which we choose to leave out of our comparison due to added hyper-parameter choices and complexity of implementation in our experiments – we do however believe that future 1 https://gitlab.com/farid-fari/ fewshot-learning work implementing these techniques could potentially outperform ours. Fadaee et al. (2017); Kobayashi (2018); Wu et al. (2019) and Gao et al. (2019) explore a similar contextual data augmentation technique, albeit in different scenarios and with different goals. This technique synthesizes new sentences by using sentences from the training set and s"
2021.eacl-main.90,D16-1163,0,0.0308042,"a variable cost in BLEU (Papineni et al., 2002), due to overfitting to the new examples. We show that this cost can be kept at a minimum by padding our data-augmentation approach with randomly selected sentences from our training set. The appropriate choice of hyper-parameters is also important for final performance. Our code is freely available online.1 2 Related work The topic of few-shot learning from post-edits is relatively novel, and we were therefore left with few comparison points. A somewhat similar task that requires quick adaptation is low-resource MT, for which transfer learning (Zoph et al., 2016) and meta-learning (Gu et al., 2018) approaches exist. These techniques generally apply for adaptation from hundreds of thousands of sentences, rather than a dozen available in our scenario; this is because we aim to learn individual new words rather than a whole language or domain. A widespread technique in the MT literature to adapt a model to new data is fine-tuning, often used for domain adaptation. Turchi et al. (2017) and Li et al. (2018) explore the use of a similarity search in the training corpus in order to fine-tune an MT model before translating a novel sentence or to gradually ada"
2021.emnlp-main.69,maynard-etal-2008-evaluating,0,0.0444713,"maries. These htm 2 are weakly labelled with leaf nodes of the ICD-9 https://github.com/modr00cka/CoPHE 907 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 907–912 c November 7–11, 2021. 2021 Association for Computational Linguistics precision/recall/F1 score (from hereon referred to as standard metrics) of individual prediction level (leaf) codes treat all mispredictions equally – e.g., having 425.0 be mispredicted as 425.3 is penalised the same way as mispredicting it as 305.1. This phenomenon has been addressed in information extraction (IE) by Maynard et al. (2008) through the use of distance metrics. The IE setting assumes both the gold standard and predictions to be associated with specified spans within the input text. This means an individual prediction can be associated with a true label, allowing direct comparison between them. The LMTC setting uses weak labels – predictions and true labels appear on the document level, without exact association to spans within the text. Due to the absence of information regarding associated spans, direct links between individual predictions and true labels do not exist. Label comparison is performed on full vecto"
2021.emnlp-main.69,N18-1100,0,0.0292004,"epth of the ontology. We propose a set of metrics for hierarchical evaluation using the depthbased representation. We compare the evaluation scores from the proposed metrics with previously used metrics on prior art LMTC models for ICD-9 coding in MIMIC-III. We also propose further avenues of research involving the proposed ontological representation. 1 Introduction ontology1 . ICD-9 is a tree-structured ontology of medical conditions and procedures. Since the release of the MIMIC-III dataset there have been several attempts at training neural models for automated coding of medical documents (Mullenbach et al., 2018; Rios and Kavuluru, 2018; Falis et al., 2019; Chalkidis et al., 2020; Dong et al., 2021). While some prior art has made use of ontological structure (Rios and Kavuluru, 2018; Falis et al., 2019; Manginas et al., 2020), the task has mostly been treated as a flat prediction of the assigned leaves. This is reflected in the evaluation metrics that are used across previous work – precision, recall and F1 score on flat predictions. These metrics applied to flat vectors disregard the rich ontological structure. A notable exception is Manginas et al. (2020) – they fine-tune a BERT (Devlin et al., 201"
2021.emnlp-main.69,D18-1352,0,0.277553,"propose a set of metrics for hierarchical evaluation using the depthbased representation. We compare the evaluation scores from the proposed metrics with previously used metrics on prior art LMTC models for ICD-9 coding in MIMIC-III. We also propose further avenues of research involving the proposed ontological representation. 1 Introduction ontology1 . ICD-9 is a tree-structured ontology of medical conditions and procedures. Since the release of the MIMIC-III dataset there have been several attempts at training neural models for automated coding of medical documents (Mullenbach et al., 2018; Rios and Kavuluru, 2018; Falis et al., 2019; Chalkidis et al., 2020; Dong et al., 2021). While some prior art has made use of ontological structure (Rios and Kavuluru, 2018; Falis et al., 2019; Manginas et al., 2020), the task has mostly been treated as a flat prediction of the assigned leaves. This is reflected in the evaluation metrics that are used across previous work – precision, recall and F1 score on flat predictions. These metrics applied to flat vectors disregard the rich ontological structure. A notable exception is Manginas et al. (2020) – they fine-tune a BERT (Devlin et al., 2018) model, such that diffe"
2021.emnlp-main.87,2021.emnlp-main.468,0,0.0383686,"uage Modelling, XDM - Cross-lingual Dialogue Modelling, RM - Response Masking. Italics is the response in the given chat. Both XDM and RM are new designs for intermediate tasks, tailored for cross-lingual dialogue tasks. We also experimented with combining monolingual and cross-lingual objectives but our pilot experiments did not show any considerable improvement over the individual objectives. For tasks where combining multiple objectives has worked, those tasks required higher reasoning and inference capabilities like coreference resolution or question answering (Pruksachatkun et al., 2020; Aghajanyan et al., 2021). Such highly specific task data is not available for all languages and even further limited for conversational tasks. We will explore this direction in future. Similarly, our initial experiments suggested that simply combining data from multiple languages for a multilingual intermediate task has lower performance than individual crosslingual intermediate tasks. Thus, designing multilingual intermediate tasks is far from trivial and we will also explore this in future. Multilingual WoZ dataset (Mrkši´c et al., 2017b). As the datasets vary in difficulty and languages, we choose a different amou"
2021.emnlp-main.87,Q19-1038,0,0.0224936,"h limited labelled data in the target language for the MultiWoZ dataset over the baseline. 3. We propose two new intermediate tasks: Crosslingual dialogue modelling (XDM) and Response masking (RM) that can be extended to other crosslingual dialogue tasks. 2 Related Work Intermediate fine-tuning of large language models: Training deep neural networks on large unlabelled text data to learn meaningful representations has shown remarkable success on several downstream tasks. These representations can be monolingual (Qiu et al., 2020) or multilingual (Devlin et al., 2019; Conneau and Lample, 2019; Artetxe and Schwenk, 2019) depending on the underlying training data. These representations are 1 further refined to suit the downstream task by fineOur code is available at https://github.com/ nikitacs16/xlift_dst tuning the pretrained model on related data and/or 1138 tasks. This “intermediate” fine-tuning is done before fine-tuning the task-specific architecture on the downstream task. In adaptive intermediate fine-tuning, a pretrained model is fine-tuned with the same objectives used during pretraining on data that is closer to the distribution of the target task. This is referred to as task adaptive pretraining (T"
2021.emnlp-main.87,2020.lrec-1.53,0,0.108542,"all the submissions in the shared task used the translated version of the dataset and treated the problem as a monolingual dialogue state tracking setup. We use the Multilingual WoZ dataset and the parallel MultiWoZ dataset to demonstrate the effectiveA popular benchmark for cross-lingual dialogue ness of our methods. As there are no existing bench1139 marks for cross-lingual dialogue state tracking for the parallel MultiWoZ dataset, we use the slotutterance matching belief tracker (SUMBT) (Lee et al., 2019) as our baseline, which was the state-ofthe-art for the English MultiWoZ 2.1 dataset (Eric et al., 2020). The SUMBT model uses BERT encoder to obtain contextual semantic vectors for the utterances, slot-names, and slot values. It then uses a multi-head attention network to learn the relationship between slot-names and slot-values appearing in the text to predict the dialogue states. Monolingual dialogue modelling (MonoDM): Dialogue history is an important component of any dialogue task. We select K continuous subtitles from the monolingual subtitles data where K is chosen randomly between 2 to 15 for every example. By choosing a random K, we ensure that the examples contain varied length dialogu"
2021.emnlp-main.87,2021.eacl-main.270,0,0.0671976,"Missing"
2021.emnlp-main.87,2020.acl-main.740,0,0.032183,"Missing"
2021.emnlp-main.87,D19-1433,0,0.0493058,"Missing"
2021.emnlp-main.87,P18-1031,0,0.0645101,"Missing"
2021.emnlp-main.87,P19-1546,0,0.0278763,"Missing"
2021.emnlp-main.87,L16-1147,0,0.478897,"termediate fine-tuning also becomes an important addition in the intermediate fine-tuning literature which has largely focused on related monolingual tasks. Our best method leads to an impressive performance on the standard benchmark of the Multilingual WoZ 2.0 dataset (Mrkši´c et al., 2017b) and the recently released parallel MultiWoZ 2.1 dataset (Gunasekara et al., 2020). It uses dialogue history and parallel conversational context confirming that our design principles based on conversation history and cross-lingual conversations are important. Our methods use 200K parallel movie subtitles (Lison and Tiedemann, 2016) for intermediate training and this data is already available for 1782 language pairs allowing extension to new language pairs. 1 Our contributions can be summarized as follows: 1. To the best of our knowledge, this is the first work to use parallel data for intermediate finetuning of multilingual models for multilingual dialogue tasks. We provide strong empirical evidence on four language directions in two datasets for lowresource and zero-shot data scenarios. 2. Our proposed intermediate fine-tuning techniques produce data-efficient target language dialogue state trackers. We achieve state-o"
2021.emnlp-main.87,P19-1373,0,0.0517639,"Missing"
2021.emnlp-main.87,P17-1163,0,0.0487528,"Missing"
2021.emnlp-main.87,Q17-1022,0,0.0606135,"Missing"
2021.emnlp-main.87,2020.aacl-main.56,0,0.0327191,"-lingual intermediate fine-tuning of pretrained multilingual language models for the task of cross-lingual dialogue state tracking. We experimented with existing intermediate tasks and introduced two new cross-lingual intermediate tasks based on the parallel and dialogue-level nature of the movie subtitles corpus. Our best method had significant improvement in performance for the parallel MultiWoZ dataset and Multilingual WoZ dataset. We also demonstrated the data efficiency of our methods. Our intermediate tasks were trained on a generic dataset unlike the related high resource tasks used in Phang et al. (2020). As OpenSubtitles is available for 1782 language pairs, we speculate that using these cross-lingual intermediate tasks will be effective for languages where a collection of large training datasets for dialogue tasks is not feasible. We speculate that this setup can be useful for crosslingual domain transfer too - when such benchmark becomes available for dialogue tasks. We hope that our method can serve as a strong baseline for future work in multilingual dialogue. We emphasized using dialogue history nformation while designing intermediate tasks. For ablation Acknowledgements studies, we fin"
2021.emnlp-main.87,2020.acl-main.467,0,0.061459,"Missing"
2021.emnlp-main.87,D18-1299,0,0.0376903,"Missing"
2021.emnlp-main.87,N19-1380,0,0.0467241,"Missing"
2021.emnlp-main.87,E17-1042,0,0.0741207,"Missing"
2021.emnlp-main.87,2020.tacl-1.19,0,0.042181,"Missing"
2021.emnlp-main.87,2020.acl-demos.19,0,0.0354218,"Missing"
2021.findings-acl.261,K16-1002,0,0.100823,"Missing"
2021.findings-acl.261,W18-6315,0,0.0204696,", unlike masking. Introduction Neural machine translation (NMT) is notoriously data-hungry (Koehn and Knowles, 2017). To learn a strong model it requires large, high-quality and in-domain parallel data, which exist only for a few language-pairs. The most successful approach for improving low-resource NMT is backtranslation (Sennrich et al., 2016), that exploits abundant monolingual corpora to augment the parallel with synthetic data. However, in low-resource settings, it may fail to improve or even degrade translation quality if the initial model is not strong enough (Imankulova et al., 2017; Burlot and Yvon, 2018). Unsupervised pretraining is a complementary technique, that has revolutionized many natural language understanding (NLU) tasks (Wang et al., 2019). The dominant approach is to train a (large) model on a lot of unlabeled data using the masked language modeling (MLM; Devlin et al. (2019)) objective and then finetune it on a downstream task. Besides improving generalization, good initialization drastically reduces the need for labelled data. This paradigm has been applied recently to NMT yielding impressive results in low-resource settings, with models such as XLM (Conneau and Lample, 2019), MA"
2021.findings-acl.261,N19-1423,0,0.153631,"ow-resource NMT is backtranslation (Sennrich et al., 2016), that exploits abundant monolingual corpora to augment the parallel with synthetic data. However, in low-resource settings, it may fail to improve or even degrade translation quality if the initial model is not strong enough (Imankulova et al., 2017; Burlot and Yvon, 2018). Unsupervised pretraining is a complementary technique, that has revolutionized many natural language understanding (NLU) tasks (Wang et al., 2019). The dominant approach is to train a (large) model on a lot of unlabeled data using the masked language modeling (MLM; Devlin et al. (2019)) objective and then finetune it on a downstream task. Besides improving generalization, good initialization drastically reduces the need for labelled data. This paradigm has been applied recently to NMT yielding impressive results in low-resource settings, with models such as XLM (Conneau and Lample, 2019), MASS (Song et al., 2019) and BART/mBART (Lewis et al., 2020b; Liu et al., 2020), that adapt MLM to sequence-to-sequence architectures. Although pretraining alone is not enough to outperform backtranslation, it helps the initial model to produce synthetic data of sufficient quality, and com"
2021.findings-acl.261,D18-1045,0,0.0708412,"Missing"
2021.findings-acl.261,P17-2090,0,0.0168307,"on all permutations of other tokens in a sentence and Song et al. (2020) extend this to sequence-level pretraining for NLU. MARGE (Lewis et al., 2020a) explores multi-lingual pretraining for document-level NMT, by reconstructing texts from a set of retrieved relevant documents. Clark et al. (2020) propose the replaced token detection (RTD) objective for pretraining text encoders. They replace tokens with samples from a MLM and train the encoder as a discriminator to predict whether each word is real or fake. Similar ideas have been previously explored in NMT with contextual data augmentation (Fadaee et al., 2017; Kobayashi, 2018; Gao et al., 2019). 2957 1 Code at github.com/cbaziotis/nmt-pretraining-objectives 3 Pretraining Our pretraining model is a multilingual denoising sequence autoencoder, based on the Transformer (Vaswani et al., 2017). We assume access to a corpus of unpaired data, containing text in two languages A, B. Given a text sequence of N tokens x = hx1 , x2 , ..., xN i we first add noise to it and obtain its corrupted version x0 . An encoder transforms x0 into a sequence of contextualized representations h(x0 ) = hh1 , h2 , ..., hN i, which are given as input to the decoder, that prod"
2021.findings-acl.261,P19-1555,0,0.0152663,"a sentence and Song et al. (2020) extend this to sequence-level pretraining for NLU. MARGE (Lewis et al., 2020a) explores multi-lingual pretraining for document-level NMT, by reconstructing texts from a set of retrieved relevant documents. Clark et al. (2020) propose the replaced token detection (RTD) objective for pretraining text encoders. They replace tokens with samples from a MLM and train the encoder as a discriminator to predict whether each word is real or fake. Similar ideas have been previously explored in NMT with contextual data augmentation (Fadaee et al., 2017; Kobayashi, 2018; Gao et al., 2019). 2957 1 Code at github.com/cbaziotis/nmt-pretraining-objectives 3 Pretraining Our pretraining model is a multilingual denoising sequence autoencoder, based on the Transformer (Vaswani et al., 2017). We assume access to a corpus of unpaired data, containing text in two languages A, B. Given a text sequence of N tokens x = hx1 , x2 , ..., xN i we first add noise to it and obtain its corrupted version x0 . An encoder transforms x0 into a sequence of contextualized representations h(x0 ) = hh1 , h2 , ..., hN i, which are given as input to the decoder, that produces a reconstruction of x. The reco"
2021.findings-acl.261,D19-1632,0,0.0466467,"Missing"
2021.findings-acl.261,W17-5704,0,0.0232334,"h resemble real sentences, unlike masking. Introduction Neural machine translation (NMT) is notoriously data-hungry (Koehn and Knowles, 2017). To learn a strong model it requires large, high-quality and in-domain parallel data, which exist only for a few language-pairs. The most successful approach for improving low-resource NMT is backtranslation (Sennrich et al., 2016), that exploits abundant monolingual corpora to augment the parallel with synthetic data. However, in low-resource settings, it may fail to improve or even degrade translation quality if the initial model is not strong enough (Imankulova et al., 2017; Burlot and Yvon, 2018). Unsupervised pretraining is a complementary technique, that has revolutionized many natural language understanding (NLU) tasks (Wang et al., 2019). The dominant approach is to train a (large) model on a lot of unlabeled data using the masked language modeling (MLM; Devlin et al. (2019)) objective and then finetune it on a downstream task. Besides improving generalization, good initialization drastically reduces the need for labelled data. This paradigm has been applied recently to NMT yielding impressive results in low-resource settings, with models such as XLM (Conne"
2021.findings-acl.261,2020.emnlp-main.75,0,0.0801411,"Missing"
2021.findings-acl.261,2021.naacl-main.7,0,0.0768002,"Missing"
2021.mtsummit-research.8,2020.wmt-1.5,1,0.795255,"en prepended to the source sentence to specify the output language. We use WMT data (see Section 4.1) for training and early stopping. Training of the from-scratch system. Training consists of fine-tuning a pretrained model with Pashto–English parallel data, using it to generate initial backtranslations which are combined with the parallel data and used to train another round of the model, starting again from a pretrained model. At this point, we include the first 220,000 sentence pairs of “Bytedance” filtered parallel data, sorted by filtering rank. Following similar work with English–Tamil (Bawden et al., 2020), we start with our mBART-like model and we fine-tune it in the Pashto→English direction with our parallel data. Then we use this model to backtranslate the Pashto monolingual data, generating a pseudo-parallel corpus which we combine with our true parallel corpus and use to train a English→Pashto model again starting from mBART. We use this model to backtranslate the first 5,000,000 monolingual English sentences (we also experimented with the full corpus, but found minimal difference), and we train another round of Pashto→English followed by another round of English→Pashto, both initialized f"
2021.mtsummit-research.8,W18-2716,0,0.065695,"Missing"
2021.mtsummit-research.8,D18-2012,0,0.0212368,"f these could be Pashto speakers. Pashto (also spelled Pukhto and Pakhto is an Iranian language of the Indo-European family and is grouped with other Iranian languages such as Persian, Dari, Tajiki, in spite of major linguistic diferences among them. Pashto is written with a unique enriched Perso-Arabic script with 45 letters and four diacritics. Translating between English and Pashto poses interesting challenges. Pashto has a richer morphology than that of English; the induced data sparseness may partly be remedied with segmentation in subword units tokenization models such as SentencePiece (Kudo and Richardson, 2018), as used in mBART50. There are Pashto categories in Pashto that do not overtly exist in English (such as verb aspect or the oblique case in general nouns) and categories in English that do not overtly exist in Pashto (such as definite and indefinite articles), which may pose a certain challenge when having to generate correct text in machine translation output. Due to the chronic political and social instability and conflict that Afghanistan has experienced in its recent history, the country features prominently in global news coverage. Closely following the developments there remains a key p"
2021.mtsummit-research.8,2020.tacl-1.47,0,0.318683,"he BBC and DW for a short period of time. Given the impact of the COVID-19 pandemic, a twomonth period was considered realistic. On 1 February 2021, BBC and DW revealed the chosen language to be Pashto. By completing and documenting how this challenge was addressed, we prove we are able to bootstrap a new high quality neural machine translation task within a very limited window of time. There has also been a considerable amount of recent interest in using pretrained language models for improving performance on downstream natural language processing tasks, especially in a low resource setting (Liu et al., 2020; Brown et al., 2020; Qiu et al., 2020), but how best to do this is still an open question. A key question in this work is how best to use training data which is not English (en) to Pashto (ps) translations. We experimented, on the one hand, with pretraining models on a high-resource language pair (German–English, one of the most studied high-resource language pairs) and, on the other hand, with fine-tuning an existing large pretrained translation model (mBART50) trained on parallel data involving English and 49 languages including Pashto (Tang et al., 2020). We show that both approaches perfo"
2021.mtsummit-research.8,W17-4770,0,0.0257416,"Missing"
2021.mtsummit-research.8,W18-6319,0,0.0127791,"tively small decrease in memory consumption: for example, the GPU memory requirements of mBART n–to–n at inference time (setting the maximum number of tokens per mini-batch to 100) moved from around 4 GB to around 3 GB. 5 Results and Discussion Tables 2 and 3 show BLEU and chrF2 scores, respectively, for the English to Pashto systems with different test sets. The evaluation metrics for the Google MT system are also included for reference purposes. Similarly, tables 4 and 5 show BLEU and chrF2 scores, respectively, for the Pashto to English systems. All the scores were computed with sacrebleu (Post, 2018). The test sets considered are the two in-house parallel sets created by BBC and DW (see Section 3) and the devtest set provided in the FLORES19 benchmark (2,698 sentences). 19 https://github.com/facebookresearch/flores 8 Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 99 Google from-scratch mBART50 + small + small, large + small, large, synthetic BBC test DW test FLORES devtest 35.03 20.00 19.42 22.55 25.27 25.38 24.65 15.06 15.30 17.50 19.13 17.88 21.54 14.90 14.59 14.77 17.71 17.08 Table 4: BLEU scores of the Pa"
D08-1078,J93-2003,0,0.00596267,"ge pairs, translation quality is still low. Certain systematic differences between languages can be used to predict this. Many researchers have speculated on the reasons why machine translation is hard. However, there has never been, to our knowledge, an analysis of what the actual contribution of different aspects of language pairs is to translation performance. This understanding of where the difficulties lie will allow researchers to know where to most gainfully direct their efforts to improving the current models of machine translation. Many of the challenges of SMT were first outlined by Brown et al. (1993). The original IBM Models were broken down into separate translation and distortion models, recognizing the importance of word order differences in modeling translation. Brown et al. also highlighted the importance of modeling morphology, both for reducing sparse counts and improving parameter estimation and for the correct production of translated forms. We see these two factors, reordering and morphology, as fundamental to the quality of machine translation output, and we would like to quantify their impact on system performance. It is not sufficient, however, to analyze the morphological co"
D08-1078,W08-0309,1,0.276713,"ance levels of coefficients and R2 are also 751 = 0.16 pt da es = 0.24 fr nl fi = 0.32 de = 0.4 Target Languages Figure 7. System performance - the width of the squares indicates the system performance in terms of the B LEU score. Explanatory Variable Target Vocab. Size Language Similarity Reordering Amount Target Vocab. Size2 Language Similarity2 Interaction: Reord/Sim We used the phrase-based model Moses (Koehn et al., 2007) for the experiments with all the standard settings, including a lexicalized reordering model, and a 5-gram language model. Tests were run on the ACL WSMT 2008 test set (Callison-Burch et al., 2008). 6.1 sv en el Coefficient -3.885 3.274 -1.883 1.017 -1.858 -1.4536 *** *** *** *** ** *** Table 3. The impact of the various explanatory features on the B LEU score via their coefficients in the minimal adequate model. given where * means p &lt; 0.05, ** means p &lt; 0.01, and *** means p &lt; 0.001. 7 7.1 Results Combined Model The first question we are interested in answering is which factors contribute most and how they interact. We fit a multiple regression model to the data. The source vocabulary size has no significant effect on the outcome. All explanatory variable vectors were normalized to be"
D08-1078,W07-0729,0,0.0116329,"rms of the same lemma as completely independent of one another. This can result in sparse statistics and poorly estimated models. Furthermore, different variations of the lemma may result in crucial differences in meaning that affect the quality of the translation. Work on improving MT systems’ treatment of morphology has focussed on either reducing word forms to lemmas to reduce sparsity (Goldwater and McClosky, 2005; Talbot and Osborne, 2006) or including morphological information in decod746 en fr it es pt el nl sv da de fi Language Figure 1. Average vocabulary size for each language. ing (Dyer, 2007). Although there is a significant amount of research into improving the treatment of morphology, in this paper we aim to discover the effect that different levels of morphology have on translation. We measure the amount of morphological complexity that exists in both languages and then relate this to translation performance. Some languages seem to be intuitively more complex than others, for instance Finnish appears more complex than English. There is, however, no obvious way of measuring this complexity. One method of measuring complexity is by choosing a number of hand-picked, intuitive prop"
D08-1078,H05-1085,0,0.0139242,"lexity of the language pairs involved in translation is widely recognized as one of the factors influencing translation performance. However, most statistical translation systems treat different inflected forms of the same lemma as completely independent of one another. This can result in sparse statistics and poorly estimated models. Furthermore, different variations of the lemma may result in crucial differences in meaning that affect the quality of the translation. Work on improving MT systems’ treatment of morphology has focussed on either reducing word forms to lemmas to reduce sparsity (Goldwater and McClosky, 2005; Talbot and Osborne, 2006) or including morphological information in decod746 en fr it es pt el nl sv da de fi Language Figure 1. Average vocabulary size for each language. ing (Dyer, 2007). Although there is a significant amount of research into improving the treatment of morphology, in this paper we aim to discover the effect that different levels of morphology have on translation. We measure the amount of morphological complexity that exists in both languages and then relate this to translation performance. Some languages seem to be intuitively more complex than others, for instance Finnis"
D08-1078,N03-1017,1,0.0249735,"nce. 749 Experimental Design We select the German-English language pair because it has a reasonably high level of reordering. A manually aligned German-English corpus was provided by Chris Callison-Burch and consists of the first 220 sentences of test data from the 2006 ACL Workshop on Machine Translation (WMT06) test set. This test set is from a held out portion of the Europarl corpus. The automatic alignments were extracted by appending the manually aligned sentences on to the respective Europarl v3 corpora and aligning them using GIZA++ (Och and Ney, 2003) and the growfinal-diag algorithm (Koehn et al., 2003). 5.3.2 P Automatic Alignments Results In order to use automatic alignments to extract reordering statistics, we need to show that reorderings from automatic alignments are comparable to those from manual alignments. We first look at global reordering statistics and then we look in more detail at the reordering distribution of the corpora. Table 2 shows the amount of reordering in the WMT06 test corpora, with both manual and automatic alignments, and in the automatically aligned Europarl DE-EN parallel corpus. fi 0.8 nl Source Languages 0.4 0.6 ACL Test Manual ACL Test Automatic Euromatrix 0.2"
D08-1078,P07-2045,1,0.0191775,"etermine whether the coefficients for the independent variables are reliably different from zero. We also test how well the model explains the data using an R2 test. The two-tailed significance levels of coefficients and R2 are also 751 = 0.16 pt da es = 0.24 fr nl fi = 0.32 de = 0.4 Target Languages Figure 7. System performance - the width of the squares indicates the system performance in terms of the B LEU score. Explanatory Variable Target Vocab. Size Language Similarity Reordering Amount Target Vocab. Size2 Language Similarity2 Interaction: Reord/Sim We used the phrase-based model Moses (Koehn et al., 2007) for the experiments with all the standard settings, including a lexicalized reordering model, and a 5-gram language model. Tests were run on the ACL WSMT 2008 test set (Callison-Burch et al., 2008). 6.1 sv en el Coefficient -3.885 3.274 -1.883 1.017 -1.858 -1.4536 *** *** *** *** ** *** Table 3. The impact of the various explanatory features on the B LEU score via their coefficients in the minimal adequate model. given where * means p &lt; 0.05, ** means p &lt; 0.01, and *** means p &lt; 0.001. 7 7.1 Results Combined Model The first question we are interested in answering is which factors contribute m"
D08-1078,2005.mtsummit-papers.11,1,0.302052,"ance. It is not sufficient, however, to analyze the morphological complexity of the source and target languages. It is also very important to know how similar the morphology is between the two languages, as two languages which are morphologically complex in very similar ways, could be relatively easy to translate. Therefore, we also include a measure of the family relatedness of languages in our analysis. The impact of these factors on translation is measured by using linear regression models. We perform the analysis with data from 110 different language pairs drawn from the Europarl project (Koehn, 2005). This contains parallel data for the 11 official language pairs of the European Union, providing a rich variety of different language characteristics for our experiments. Many research papers report results on only one or two languages pairs. By analyzing so many language pairs, we are able to provide a much wider perspective on the challenges facing machine translation. This analysis is important as it provides very strong motivation for further research. The findings of this paper are as follows: (1) each of the main effects, reordering, target language complexity and language relatedness,"
D08-1078,J03-1002,0,0.00182469,"ce side, normalized by the length of the source sentence. 749 Experimental Design We select the German-English language pair because it has a reasonably high level of reordering. A manually aligned German-English corpus was provided by Chris Callison-Burch and consists of the first 220 sentences of test data from the 2006 ACL Workshop on Machine Translation (WMT06) test set. This test set is from a held out portion of the Europarl corpus. The automatic alignments were extracted by appending the manually aligned sentences on to the respective Europarl v3 corpora and aligning them using GIZA++ (Och and Ney, 2003) and the growfinal-diag algorithm (Koehn et al., 2003). 5.3.2 P Automatic Alignments Results In order to use automatic alignments to extract reordering statistics, we need to show that reorderings from automatic alignments are comparable to those from manual alignments. We first look at global reordering statistics and then we look in more detail at the reordering distribution of the corpora. Table 2 shows the amount of reordering in the WMT06 test corpora, with both manual and automatic alignments, and in the automatically aligned Europarl DE-EN parallel corpus. fi 0.8 nl Source Languages 0.4"
D08-1078,P02-1040,0,0.10138,"Missing"
D08-1078,2006.amta-papers.25,0,0.0889646,"Missing"
D08-1078,P06-1122,1,0.768304,"nvolved in translation is widely recognized as one of the factors influencing translation performance. However, most statistical translation systems treat different inflected forms of the same lemma as completely independent of one another. This can result in sparse statistics and poorly estimated models. Furthermore, different variations of the lemma may result in crucial differences in meaning that affect the quality of the translation. Work on improving MT systems’ treatment of morphology has focussed on either reducing word forms to lemmas to reduce sparsity (Goldwater and McClosky, 2005; Talbot and Osborne, 2006) or including morphological information in decod746 en fr it es pt el nl sv da de fi Language Figure 1. Average vocabulary size for each language. ing (Dyer, 2007). Although there is a significant amount of research into improving the treatment of morphology, in this paper we aim to discover the effect that different levels of morphology have on translation. We measure the amount of morphological complexity that exists in both languages and then relate this to translation performance. Some languages seem to be intuitively more complex than others, for instance Finnish appears more complex than"
D08-1078,J97-3002,0,0.271232,"largely driven by syntactic differences between languages and can involve complex rearrangements between nodes in synchronous trees. Modeling reordering exactly would require a synchronous tree-substitution grammar. This representation would be sparse and heterogeneous, limiting its usefulness as a basis for analysis. We make an important simplifying assumption in order for the detection and extraction of reordering data to be tractable and useful. We assume that reordering is a binary process occurring between two blocks that are adjacent in the source. This is similar to the ITG constraint (Wu, 1997), however our reorderings are not dependent on a synchronous grammar or a derivation which covers the sentences. There are also similarities with the Human-Targeted Transla748 A consistent block means that between Atmin and Atmax there are no target word positions aligned to source words outside of the block’s source span As . A reordering is consistent if the block projected from rAsmin to rBsmax is consistent. The following algorithm detects reorderings and determines the dimensions of the blocks involved. We step through all the source words, and if a word is reordered in the target with re"
D11-1079,2009.mtsummit-papers.1,0,0.616093,"s are quite similar and not shown here. Translation outputs are in lower case. tures the boxed area as a whole and uses Rule 10 to perform the right global reordering. X → (dui1 X2 gandao3 manyi4 .5 , expressed3 satisfaction4 with1 X2 .5 ) 5 (10) Related Work In recent years, there has been a growing body of research on using dependency for statistical machine translation. Some directly encodes dependency in the translation model (Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Mi and Liu, 2010), while others use dependency as a soft constraint (Cherry, 2008; Bach et al., 2009a,b; Chang et al., 2009). Among them, Shen et al. (2008) report that just filtering the phrase table by the socalled well-formed target dependency structure does not help, yet adding a target dependency language model improves performance significantly. Our intuitive interpretation is that the target dependency language model capitalizes on two characteristics of the dependency structure: it is based on words and it directly connects head and child. Therefore, the target dependency language model makes good use of the dependency representation as well as the target side training data. We follo"
D11-1079,N09-2001,0,0.254564,"s are quite similar and not shown here. Translation outputs are in lower case. tures the boxed area as a whole and uses Rule 10 to perform the right global reordering. X → (dui1 X2 gandao3 manyi4 .5 , expressed3 satisfaction4 with1 X2 .5 ) 5 (10) Related Work In recent years, there has been a growing body of research on using dependency for statistical machine translation. Some directly encodes dependency in the translation model (Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Mi and Liu, 2010), while others use dependency as a soft constraint (Cherry, 2008; Bach et al., 2009a,b; Chang et al., 2009). Among them, Shen et al. (2008) report that just filtering the phrase table by the socalled well-formed target dependency structure does not help, yet adding a target dependency language model improves performance significantly. Our intuitive interpretation is that the target dependency language model capitalizes on two characteristics of the dependency structure: it is based on words and it directly connects head and child. Therefore, the target dependency language model makes good use of the dependency representation as well as the target side training data. We follo"
D11-1079,W09-0434,1,0.743819,"distance reodering. 1 Introduction Reordering, especially movement over longer distances, continues to be a hard problem in statistical machine translation. It motivates much of the recent work on tree-based translation models, such as the hierarchical phrase-based model (Chiang, 2007) which extends the phrase-based model (Koehn et al., 2003) by allowing the so-called hierarchical phrases containing subphrases. The hierarchical phrase-based model captures the recursiveness of language without relying on syntactic annotation, and promises better reordering than the phrase-based model. However, Birch et al. (2009) find that although the hierarchical phrasebased model outperforms the phrase-based model in 857 terms of medium-range reordering, it does equally poorly in long-distance reordering due to constraints to guarantee efficiency. Syntax-based models that use phrase structure constituent labels as non-terminals in their transfer rules, exemplified by that of Galley et al. (2004), produce smarter and syntactically motivated reordering. However, when working with off-the-shelf tools for parsing and alignment, this approach may impose harsh limits on rule extraction and requires serious efforts of opt"
D11-1079,W10-1749,1,0.625285,"nts, with the “bin-2” setting problem of overfitting sets in when we use 3 bins (with slightly higher tuning BLEU, not shown here). We also studied the effect of adding features incrementally onto the baseline with the “bin-2” setting, as shown in Table 3. On average, all three features seem to have similar contributions. 3.3 Using LRscore as the Tuning Metric Since our features are proposed to address the reordering problem and BLEU is not sensitive enough to reordering (especially in long-distance cases), we have also tried tuning with a metric that highlights reordering, i.e., the LRscore (Birch and Osborne, 2010). LRscore is a linear interpolation of a lexical metric and a reordering metric. We interpolated BLEU (as the lexical metric) with the Kendall’s tau permutation distance (as the reordering metric). The Kendall’s tau permutation distance measures the relative word order difference between the transla862 tion output and the reference(s) and is particularly sensitive to long-distance reordering. Testing results in terms of BLEU, LRscore and TER (Snover et al., 2006) are shown in Table 4. Tuned with the LRscore, our feature-augmented model achieves further average improvements (compare “bin-2” and"
D11-1079,P11-1103,1,0.527866,"gs in the translation, and recall as the number of reproduced reorderings divided by the number of reorder3 One of our reviewers points out that according to the inductive learning theory, it is counter-intuitive to improve on BLEU and TER if we optimize by the LRscore. Yet we do observe some other papers reporting increased TER or other metric scores when BLEU is used for tuning (Carpuat and Wu, 2007; Shen et al., 2008), suggesting that MT evaluation might be too complicated to be characterized just with inductive learning. Similar results based on extensive experiments can also be found in (Birch and Osborne, 2011). Setting baseline bin-2 baseline-lr bin-2-lr MT02 37.5 36.8 37.0 37.7 MT05 36.2 35.9 35.6 36.7 MT08 33.2 31.8 32.2 33.2 0.5 0.4 Average 35.6 34.8 (-0.8) 34.9 (-0.7) 35.9 (+0.3) 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Reordering Widths 0.6 Table 6: Overall recall for the test sets. 0.1 0.2 Recall 0.4 0.5 baseline bin−2 bin−2−lr 0.0 ings in the reference. Then we average the precision and recall over all four reference translations. Details of measuring reproduced reordering can be found in Birch et al. (2008). An important difference in this work is in handling many-to-one and one-to-many alignments"
D11-1079,D08-1078,1,0.886194,"ductive learning. Similar results based on extensive experiments can also be found in (Birch and Osborne, 2011). Setting baseline bin-2 baseline-lr bin-2-lr MT02 37.5 36.8 37.0 37.7 MT05 36.2 35.9 35.6 36.7 MT08 33.2 31.8 32.2 33.2 0.5 0.4 Average 35.6 34.8 (-0.8) 34.9 (-0.7) 35.9 (+0.3) 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Reordering Widths 0.6 Table 6: Overall recall for the test sets. 0.1 0.2 Recall 0.4 0.5 baseline bin−2 bin−2−lr 0.0 ings in the reference. Then we average the precision and recall over all four reference translations. Details of measuring reproduced reordering can be found in Birch et al. (2008). An important difference in this work is in handling many-to-one and one-to-many alignments, as we only retain the first word alignment for any source or target word which has multiple alignments. This is consistent with our treatment in dependency orientation classification, and results in more reorderings being extracted. From Table 5 we can see that our features improve precision by an average of 4.7 absolute points when BLEU is used for tuning (“bin-2”). Switching from BLEU to the LRscore (“bin-2-lr”), we gain 2.2 points more and have a total improvement of 6.9 absolute points on average."
D11-1079,D07-1007,0,0.0204947,"ecall for reordering is to investigate whether reorderings in the references are reproduced in the translations. We calculate precision as the number of reproduced reorderings divided by the total number of reorderings in the translation, and recall as the number of reproduced reorderings divided by the number of reorder3 One of our reviewers points out that according to the inductive learning theory, it is counter-intuitive to improve on BLEU and TER if we optimize by the LRscore. Yet we do observe some other papers reporting increased TER or other metric scores when BLEU is used for tuning (Carpuat and Wu, 2007; Shen et al., 2008), suggesting that MT evaluation might be too complicated to be characterized just with inductive learning. Similar results based on extensive experiments can also be found in (Birch and Osborne, 2011). Setting baseline bin-2 baseline-lr bin-2-lr MT02 37.5 36.8 37.0 37.7 MT05 36.2 35.9 35.6 36.7 MT08 33.2 31.8 32.2 33.2 0.5 0.4 Average 35.6 34.8 (-0.8) 34.9 (-0.7) 35.9 (+0.3) 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Reordering Widths 0.6 Table 6: Overall recall for the test sets. 0.1 0.2 Recall 0.4 0.5 baseline bin−2 bin−2−lr 0.0 ings in the reference. Then we average the precision"
D11-1079,W09-2307,0,0.0661964,"d not shown here. Translation outputs are in lower case. tures the boxed area as a whole and uses Rule 10 to perform the right global reordering. X → (dui1 X2 gandao3 manyi4 .5 , expressed3 satisfaction4 with1 X2 .5 ) 5 (10) Related Work In recent years, there has been a growing body of research on using dependency for statistical machine translation. Some directly encodes dependency in the translation model (Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Mi and Liu, 2010), while others use dependency as a soft constraint (Cherry, 2008; Bach et al., 2009a,b; Chang et al., 2009). Among them, Shen et al. (2008) report that just filtering the phrase table by the socalled well-formed target dependency structure does not help, yet adding a target dependency language model improves performance significantly. Our intuitive interpretation is that the target dependency language model capitalizes on two characteristics of the dependency structure: it is based on words and it directly connects head and child. Therefore, the target dependency language model makes good use of the dependency representation as well as the target side training data. We follow the second line of res"
D11-1079,P08-1009,0,0.228185,"version outputs are quite similar and not shown here. Translation outputs are in lower case. tures the boxed area as a whole and uses Rule 10 to perform the right global reordering. X → (dui1 X2 gandao3 manyi4 .5 , expressed3 satisfaction4 with1 X2 .5 ) 5 (10) Related Work In recent years, there has been a growing body of research on using dependency for statistical machine translation. Some directly encodes dependency in the translation model (Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Mi and Liu, 2010), while others use dependency as a soft constraint (Cherry, 2008; Bach et al., 2009a,b; Chang et al., 2009). Among them, Shen et al. (2008) report that just filtering the phrase table by the socalled well-formed target dependency structure does not help, yet adding a target dependency language model improves performance significantly. Our intuitive interpretation is that the target dependency language model capitalizes on two characteristics of the dependency structure: it is based on words and it directly connects head and child. Therefore, the target dependency language model makes good use of the dependency representation as well as the target side trai"
D11-1079,P05-1033,0,0.824256,"will also be favored by our cohesion penalty feature. However, ignorant of the syntactic structure, the 860 glue rule penalty may penalize a reasonably cohesive derivation such as Derivation 5 and at the same time promote a less cohesive hierarchical translation, such as Derivation 6. Compared with constituency constraints based on the phrase structure, our cohesion penalty derived from the binary dependency parsing has two different characteristics. First, our cohesion penalty is by nature more tolerant to some meaningful noncontituent translations. For example, constituency constraints in (Chiang, 2005; Marton and Resnik, 2008; Chiang et al., 2009) would penalize Rule 7 below which is useful for German–English translation (Koehn et al., 2003), and Rule 8 which can be applied to the Figure 1 sentence. Fuzzy constituency constraints can solve this problem with a combination of product categories and slash categories (Chiang, 2010). Yet our cohesion penalty by nature admits these translations as cohesive (with no extra cost from es and Aozhou since both are locally resolved). Admittedly, our current implementation of the cohesion penalty is blind to some other meaningful nonconstituent colloca"
D11-1079,J07-2003,0,0.897572,"witch the tuning function from BLEU to the LRscore which promotes reordering, we observe total improvements of 1.21 BLEU, 1.30 LRscore and 3.36 TER over the baseline. On average our approach improves reordering precision and recall by 6.9 and 0.3 absolute points, respectively, and is found to be especially effective for long-distance reodering. 1 Introduction Reordering, especially movement over longer distances, continues to be a hard problem in statistical machine translation. It motivates much of the recent work on tree-based translation models, such as the hierarchical phrase-based model (Chiang, 2007) which extends the phrase-based model (Koehn et al., 2003) by allowing the so-called hierarchical phrases containing subphrases. The hierarchical phrase-based model captures the recursiveness of language without relying on syntactic annotation, and promises better reordering than the phrase-based model. However, Birch et al. (2009) find that although the hierarchical phrasebased model outperforms the phrase-based model in 857 terms of medium-range reordering, it does equally poorly in long-distance reordering due to constraints to guarantee efficiency. Syntax-based models that use phrase struc"
D11-1079,P10-1146,0,0.0779101,"on the phrase structure, our cohesion penalty derived from the binary dependency parsing has two different characteristics. First, our cohesion penalty is by nature more tolerant to some meaningful noncontituent translations. For example, constituency constraints in (Chiang, 2005; Marton and Resnik, 2008; Chiang et al., 2009) would penalize Rule 7 below which is useful for German–English translation (Koehn et al., 2003), and Rule 8 which can be applied to the Figure 1 sentence. Fuzzy constituency constraints can solve this problem with a combination of product categories and slash categories (Chiang, 2010). Yet our cohesion penalty by nature admits these translations as cohesive (with no extra cost from es and Aozhou since both are locally resolved). Admittedly, our current implementation of the cohesion penalty is blind to some other meaningful nonconstituent collocations, such as neighbouring siblings of a common uncovered head (regulated as the “floating structure” in (Shen et al., 2008)). A concrete example is Rule 9 which is useful for the Figure 1 sentence. To address this problem, another feature can be defined in the same manner to capture how each head word is translated with its child"
D11-1079,N09-1025,0,0.368393,"enalty feature. However, ignorant of the syntactic structure, the 860 glue rule penalty may penalize a reasonably cohesive derivation such as Derivation 5 and at the same time promote a less cohesive hierarchical translation, such as Derivation 6. Compared with constituency constraints based on the phrase structure, our cohesion penalty derived from the binary dependency parsing has two different characteristics. First, our cohesion penalty is by nature more tolerant to some meaningful noncontituent translations. For example, constituency constraints in (Chiang, 2005; Marton and Resnik, 2008; Chiang et al., 2009) would penalize Rule 7 below which is useful for German–English translation (Koehn et al., 2003), and Rule 8 which can be applied to the Figure 1 sentence. Fuzzy constituency constraints can solve this problem with a combination of product categories and slash categories (Chiang, 2010). Yet our cohesion penalty by nature admits these translations as cohesive (with no extra cost from es and Aozhou since both are locally resolved). Admittedly, our current implementation of the cohesion penalty is blind to some other meaningful nonconstituent collocations, such as neighbouring siblings of a commo"
D11-1079,D08-1024,0,0.0396653,"countries2 ) (9) Second, our cohesion penalty can be by nature more discriminative. Compared with the constituency constraints, the cohesion penalty is integer-valued, and can be made sensitive to the depth of each word in the dependency hierarchy (see Section 2.4). Inspired by (Marton and Resnik, 2008; Chiang et al., 2009), the cohesion penalty could also be made sensitive to the dependency relation of each word. However, this drastically increases the number of features and requires a tuning algorithm which scales better to high-dimensional model spaces, such as MIRA (Watanabe et al., 2007; Chiang et al., 2008). pobj 是 Depth 1 top Depth 2 澳洲 . 之一 有 prep 与 Depth 4 pobj Depth 5 nummod 少数 国家 cpm dobj Bin 1 3 Bin 2 3.1 nn rcmod Depth 3 the tree levels spread out. punct attr 邦交 的 北韩 Figure 3: Using 2 bins for the dependency parse tree of the Figure 1 sentence. 2.3 Unaligned Penalty The dependency orientation and cohesion penalty cannot be applied to unaligned source words. This may lead to search error, such as dropping (i.e., unaligning) key content words that are important for lexical translation and reordering. The problem is mitigated by an unaligned penalty applicable to all words in the dependency"
D11-1079,P05-1066,1,0.750058,"slations as well as some nonconstituent translations, if not all of them (as discussed in Section 2.2). Our dependency orientation feature is similar to the order model within dependency treelet translation (Quirk et al., 2005). Yet instead of a head-relative position number for each modifier word, we simply predict the head-dependent orientation which is either monotone or reversed. Our coarser-grained approach is more robust from a machine learning perspective, yet still captures prominent and long-distance reordering patterns observed in Chinese–English (Wang et al., 2007), German–English (Collins et al., 2005), Japanese– English (Katz-Brown and Collins, 2008) and translation from English to a group of SOV languages (Xu et al., 2009). Not committed to specific language pairs, we learn orientation classification from the word-aligned parallel data through maximum entropy training as Zens and Ney (2006) and Chang et al. (2009) for phrase-based translation and Xiong et al. (2006) for the BTG model (Wu, 1996). While Chang et al. (2009) also make use of source dependency, their orientation classification concerns two subsequent phrase pairs in the leftto-right phrase-based decoding (as apposed to each de"
D11-1079,de-marneffe-etal-2006-generating,0,0.00377784,"Missing"
D11-1079,P05-1067,0,0.0590493,"[and the european union ( eu )]]]] .] Figure 5: Example translations from the NIST MT08 set, output by the baseline model and “bin-2” model. The “-lr” version outputs are quite similar and not shown here. Translation outputs are in lower case. tures the boxed area as a whole and uses Rule 10 to perform the right global reordering. X → (dui1 X2 gandao3 manyi4 .5 , expressed3 satisfaction4 with1 X2 .5 ) 5 (10) Related Work In recent years, there has been a growing body of research on using dependency for statistical machine translation. Some directly encodes dependency in the translation model (Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Mi and Liu, 2010), while others use dependency as a soft constraint (Cherry, 2008; Bach et al., 2009a,b; Chang et al., 2009). Among them, Shen et al. (2008) report that just filtering the phrase table by the socalled well-formed target dependency structure does not help, yet adding a target dependency language model improves performance significantly. Our intuitive interpretation is that the target dependency language model capitalizes on two characteristics of the dependency structure: it is based on words and it directly connects h"
D11-1079,W02-1039,0,0.169076,"rch, and derive three word-based soft constraints from the source dependency parsing. Note that although we reuse the word “cohesion” to name one of the constraints, our work is different from (Cherry, 2008; Bach et al., 2009a,b) which have successfully defined another cohesion constraint from the source depen864 dency structure, with the aim of improving reordering in phrase-based MT. To take a glance, Cherry (2008) and Bach et al. (2009b) define cohesion as translating a source dependency subtree contiguously into the target side without interruption (span or subtree overlapping), following Fox (2002). This span-based cohesion constraint has a different criterion from our wordbased cohesion penalty and often leads to opposite conclusions. Bach et al. (2009a) also use cohesion to correlate with the lexicalized reordering model (Tillman, 2004; Koehn et al., 2005), whereas we define an orthogonal dependency orientation feature to explicitly model head-dependent reordering. The fundamental difference, however, is rooted in the translation model. Their span-based cohesion constraint is implemented as an “interruption check” to encourage finishing a subtree before translating something else. Thi"
D11-1079,N04-1035,0,0.105467,"archical phrases containing subphrases. The hierarchical phrase-based model captures the recursiveness of language without relying on syntactic annotation, and promises better reordering than the phrase-based model. However, Birch et al. (2009) find that although the hierarchical phrasebased model outperforms the phrase-based model in 857 terms of medium-range reordering, it does equally poorly in long-distance reordering due to constraints to guarantee efficiency. Syntax-based models that use phrase structure constituent labels as non-terminals in their transfer rules, exemplified by that of Galley et al. (2004), produce smarter and syntactically motivated reordering. However, when working with off-the-shelf tools for parsing and alignment, this approach may impose harsh limits on rule extraction and requires serious efforts of optimization (Wang et al., 2010). An alternative approach is to augment the general hierarchical phrase-based model with soft syntactic constraints. Here, we derive three word-based, complementary constraints from the source dependency parsing, including: • A dependency orientation feature, trained with maximum entropy on the word-aligned parallel data, which directly models t"
D11-1079,C10-1050,0,0.0351318,"you (English have), showing that yu is a prepositional modifier of you. We use the Stanford Parser1 to generate dependency parsing, which automatically extracts dependency relations from phrase structure parsing (de Marneffe et al., 2006). 2.1 Dependency Orientation Based on the assumption that constituents generally move as a whole (Quirk et al., 2005), we decompose the sentence reordering probability into the reordering probability for each aligned source word with respect to its head, excluding the root word at the top of the dependency hierarchy which does not have a head word. Similarly, Hayashi et al. (2010) also take a word-based reordering approach for HPBMT, but they model all possible pairwise orientation from the source side as a general linear ordering problem (Tromble and Eisner, 2009). To be more specific, we have a maximum entropy orientation classifier that predicts the probability of a source word being translated in a monotone or reversed manner with respect to its head. For example, 1 (b) S1 http://nlp.stanford.edu/software/lex-parser.shtml 858 S2 S3 S1 T1 ihead T1 idep T2 idep T2 ihead T3 T3 T4 T4 S2 S3 Figure 2: Word alignments to illustrate orientation classification. In (a), mono"
D11-1079,2009.iwslt-papers.4,1,0.828278,"re2 ) for dependency orientation classification. We trained three 5-gram language models with modified Kneser-Ney smoothing (Kneser and Ney, 1995): one on the English half of the parallel corpus, one on the Xinhua part of the Gigaword corpus, one on the AFP part, and interpolated them for best fit to the tuning set (Schwenk and Koehn, 2008). We used NIST MT06 evaluation data (1664 lines) as our tuning set, and tested on NIST MT02 (878 lines), MT05 (1082 lines) and MT08 (1357 lines). Our baseline system was the Moses implementation of the hierarchical phrase-based model with standard settings (Hoang et al., 2009). When only 1 bin was used, 3 additional features were added to the baseline, one each from the soft dependency constraints. When we used 2 or 3 bins, the additional feature counts doubled or tripled. We preserved terminal alignment alongside nonterminal alignment during the rule extraction and output word alignments together with translated strings. Since the features we currently define are based entirely on the source side, we used preprocessing to speed up decoding of our feature-augmented model. All experiments were tuned with MERT (Och, 2003). 3.2 Using BLEU as the Tuning Metric As a sta"
D11-1079,D10-1014,0,0.0673881,"e span matches the source constituent as defined by phrase structure parsing. Finer-grained constituency constraints significantly improve hierarchical phrase-based MT when applied on the source side (Marton and Resnik, 2008; Chiang et al., 2009), or on the target side in a more tolerant fashion (Zollmann and Venugopal, 2006). Using both source and target syntax, but relaxing on rule extraction and substitution enables HPBMT to produce more well-formed and syntactically richer derivations (Chiang, 2010). Softening constituency matching with latent syntactic distributions proves to be helpful (Huang et al., 2010). Compared to constituency-based approaches, our cohesion penalty based on the dependency structure naturally supports constituent translations as well as some nonconstituent translations, if not all of them (as discussed in Section 2.2). Our dependency orientation feature is similar to the order model within dependency treelet translation (Quirk et al., 2005). Yet instead of a head-relative position number for each modifier word, we simply predict the head-dependent orientation which is either monotone or reversed. Our coarser-grained approach is more robust from a machine learning perspectiv"
D11-1079,W04-3250,1,0.716511,".82 BLEU / LRscore / TER MT05 MT08 32.23 / 40.50 / 68.15 28.09 / 37.17 / 66.82 33.18 / 41.62 / 65.59 28.63 / 38.12 / 65.36 32.28 / 40.61 / 67.61 27.99 / 37.27 / 66.98 33.44 / 41.80 / 64.88 29.10 / 38.38 / 64.14 Average 31.44 / 39.84 / 67.97 32.28 / 40.94 / 65.51 31.50 / 39.98 / 67.56 32.65 / 41.14 / 64.61 Table 4: Results for the baseline model and the complete feature-augmented model with 2 bins (“bin-2”), using BLEU and LRscore (“-lr”) as the tuning function. The BLEU scores of “bin-2” and “bin-2-lr” are significantly better than baseline (p &lt; 0.05), computed by paired bootstrap resampling (Koehn, 2004). Setting baseline bin-1 bin-2 bin-3 MT02 34.01 34.20 35.04 34.35 MT05 32.23 32.13 33.18 32.79 BLEU MT08 28.09 28.41 28.63 28.37 Average 31.44 31.58(+.14) 32.28(+.84) 31.84(+.40) Table 2: Results of the baseline model as well as our complete feature-augmented model with 1, 2 and 3 bins. BLEU is the tuning function. Setting MT02 baseline 34.01 dep 34.26 dep+coP 34.47 dep+coP+unP 35.04 MT05 32.23 32.58 32.81 33.18 BLEU MT08 28.09 28.07 28.61 28.63 Average 31.44 31.64(+.20) 31.96(+.52) 32.28(+.84) Table 3: Contributions of the three soft dependency constraints, with the “bin-2” setting problem of"
D11-1079,2005.iwslt-1.8,1,0.161694,"fined another cohesion constraint from the source depen864 dency structure, with the aim of improving reordering in phrase-based MT. To take a glance, Cherry (2008) and Bach et al. (2009b) define cohesion as translating a source dependency subtree contiguously into the target side without interruption (span or subtree overlapping), following Fox (2002). This span-based cohesion constraint has a different criterion from our wordbased cohesion penalty and often leads to opposite conclusions. Bach et al. (2009a) also use cohesion to correlate with the lexicalized reordering model (Tillman, 2004; Koehn et al., 2005), whereas we define an orthogonal dependency orientation feature to explicitly model head-dependent reordering. The fundamental difference, however, is rooted in the translation model. Their span-based cohesion constraint is implemented as an “interruption check” to encourage finishing a subtree before translating something else. This check is very effective for phrase-based decoding which searches over an entire space within the distortion limit in order to advance a hypothesis. In fact, it constrains reordering for the phrase-based model, as Cherry finds that the cohesion constraint is used"
D11-1079,N03-1017,1,0.0284058,"hich promotes reordering, we observe total improvements of 1.21 BLEU, 1.30 LRscore and 3.36 TER over the baseline. On average our approach improves reordering precision and recall by 6.9 and 0.3 absolute points, respectively, and is found to be especially effective for long-distance reodering. 1 Introduction Reordering, especially movement over longer distances, continues to be a hard problem in statistical machine translation. It motivates much of the recent work on tree-based translation models, such as the hierarchical phrase-based model (Chiang, 2007) which extends the phrase-based model (Koehn et al., 2003) by allowing the so-called hierarchical phrases containing subphrases. The hierarchical phrase-based model captures the recursiveness of language without relying on syntactic annotation, and promises better reordering than the phrase-based model. However, Birch et al. (2009) find that although the hierarchical phrasebased model outperforms the phrase-based model in 857 terms of medium-range reordering, it does equally poorly in long-distance reordering due to constraints to guarantee efficiency. Syntax-based models that use phrase structure constituent labels as non-terminals in their transfer"
D11-1079,P08-1114,0,0.588206,"favored by our cohesion penalty feature. However, ignorant of the syntactic structure, the 860 glue rule penalty may penalize a reasonably cohesive derivation such as Derivation 5 and at the same time promote a less cohesive hierarchical translation, such as Derivation 6. Compared with constituency constraints based on the phrase structure, our cohesion penalty derived from the binary dependency parsing has two different characteristics. First, our cohesion penalty is by nature more tolerant to some meaningful noncontituent translations. For example, constituency constraints in (Chiang, 2005; Marton and Resnik, 2008; Chiang et al., 2009) would penalize Rule 7 below which is useful for German–English translation (Koehn et al., 2003), and Rule 8 which can be applied to the Figure 1 sentence. Fuzzy constituency constraints can solve this problem with a combination of product categories and slash categories (Chiang, 2010). Yet our cohesion penalty by nature admits these translations as cohesive (with no extra cost from es and Aozhou since both are locally resolved). Admittedly, our current implementation of the cohesion penalty is blind to some other meaningful nonconstituent collocations, such as neighbouri"
D11-1079,P10-1145,0,0.0127649,"MT08 set, output by the baseline model and “bin-2” model. The “-lr” version outputs are quite similar and not shown here. Translation outputs are in lower case. tures the boxed area as a whole and uses Rule 10 to perform the right global reordering. X → (dui1 X2 gandao3 manyi4 .5 , expressed3 satisfaction4 with1 X2 .5 ) 5 (10) Related Work In recent years, there has been a growing body of research on using dependency for statistical machine translation. Some directly encodes dependency in the translation model (Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Mi and Liu, 2010), while others use dependency as a soft constraint (Cherry, 2008; Bach et al., 2009a,b; Chang et al., 2009). Among them, Shen et al. (2008) report that just filtering the phrase table by the socalled well-formed target dependency structure does not help, yet adding a target dependency language model improves performance significantly. Our intuitive interpretation is that the target dependency language model capitalizes on two characteristics of the dependency structure: it is based on words and it directly connects head and child. Therefore, the target dependency language model makes good use"
D11-1079,P03-1021,0,0.0373922,"-based model with standard settings (Hoang et al., 2009). When only 1 bin was used, 3 additional features were added to the baseline, one each from the soft dependency constraints. When we used 2 or 3 bins, the additional feature counts doubled or tripled. We preserved terminal alignment alongside nonterminal alignment during the rule extraction and output word alignments together with translated strings. Since the features we currently define are based entirely on the source side, we used preprocessing to speed up decoding of our feature-augmented model. All experiments were tuned with MERT (Och, 2003). 3.2 Using BLEU as the Tuning Metric As a standard practice, we first used BLEU (Papineni et al., 2002) as the objective function for tuning. Table 2 shows the results of the baseline model as well as our complete feature-augmented model with different bin numbers. With the “bin-2” setting, we get substantial improvement of up to 1.03 BLEU points (on MT02 data), and 0.84 BLEU points on average. Using more than one bin (i.e., differentiating tree depths) is generally beneficial, although the 2 http://www.umiacs.umd.edu/∼hal/megam/index.html Setting baseline bin-2 baseline-lr bin-2-lr MT02 34.0"
D11-1079,P02-1040,0,0.118839,"onal features were added to the baseline, one each from the soft dependency constraints. When we used 2 or 3 bins, the additional feature counts doubled or tripled. We preserved terminal alignment alongside nonterminal alignment during the rule extraction and output word alignments together with translated strings. Since the features we currently define are based entirely on the source side, we used preprocessing to speed up decoding of our feature-augmented model. All experiments were tuned with MERT (Och, 2003). 3.2 Using BLEU as the Tuning Metric As a standard practice, we first used BLEU (Papineni et al., 2002) as the objective function for tuning. Table 2 shows the results of the baseline model as well as our complete feature-augmented model with different bin numbers. With the “bin-2” setting, we get substantial improvement of up to 1.03 BLEU points (on MT02 data), and 0.84 BLEU points on average. Using more than one bin (i.e., differentiating tree depths) is generally beneficial, although the 2 http://www.umiacs.umd.edu/∼hal/megam/index.html Setting baseline bin-2 baseline-lr bin-2-lr MT02 34.01 / 41.85 / 68.93 35.04 / 43.07 / 65.58 34.23 / 42.06 / 68.08 35.42 / 43.25 / 64.82 BLEU / LRscore / TER"
D11-1079,P05-1034,0,0.535922,"s shown in Figure 1. The basic unit of dependency parsing is a triple consisting of the dependent word, the head word and the dependency relation that connects them. For example, in Figure 1, an arrow labelled prep goes from the word yu (English with) to the word you (English have), showing that yu is a prepositional modifier of you. We use the Stanford Parser1 to generate dependency parsing, which automatically extracts dependency relations from phrase structure parsing (de Marneffe et al., 2006). 2.1 Dependency Orientation Based on the assumption that constituents generally move as a whole (Quirk et al., 2005), we decompose the sentence reordering probability into the reordering probability for each aligned source word with respect to its head, excluding the root word at the top of the dependency hierarchy which does not have a head word. Similarly, Hayashi et al. (2010) also take a word-based reordering approach for HPBMT, but they model all possible pairwise orientation from the source side as a general linear ordering problem (Tromble and Eisner, 2009). To be more specific, we have a maximum entropy orientation classifier that predicts the probability of a source word being translated in a monot"
D11-1079,I08-2089,1,0.587986,"rallel training corpus with 2.1 million Chinese–English sentence pairs, aligned by GIZA++. The Chinese side was parsed by the Stanford Parser. Then we extracted 33.8 million examples from the parsed Chinese side to discriminatively train 1.1 million features (using the MegaM software2 ) for dependency orientation classification. We trained three 5-gram language models with modified Kneser-Ney smoothing (Kneser and Ney, 1995): one on the English half of the parallel corpus, one on the Xinhua part of the Gigaword corpus, one on the AFP part, and interpolated them for best fit to the tuning set (Schwenk and Koehn, 2008). We used NIST MT06 evaluation data (1664 lines) as our tuning set, and tested on NIST MT02 (878 lines), MT05 (1082 lines) and MT08 (1357 lines). Our baseline system was the Moses implementation of the hierarchical phrase-based model with standard settings (Hoang et al., 2009). When only 1 bin was used, 3 additional features were added to the baseline, one each from the soft dependency constraints. When we used 2 or 3 bins, the additional feature counts doubled or tripled. We preserved terminal alignment alongside nonterminal alignment during the rule extraction and output word alignments toge"
D11-1079,P09-1037,0,0.0590482,"tive to GIZA++, we would like to experiment with syntactically informed aligners that better handle function words which often exhibit high alignment ambiguity due to low cross-lingual correspondence. Finally, since our soft dependency constraints promote reordering without increasing model complexity, further gains can be achieved when combining our approach with orthogonal studies to improve the quantity and quality of hierarchical (reordering) rules, such as relaxing hierarchical rule extraction constraints (Setiawan and Resnik, 2010) and selectively lexicalizing rules with function words (Setiawan et al., 2009). Acknowledgments We would like to thank Miles Osborne, Adam Lopez, Barry Haddow, Hieu Hoang, Philip Williams and Michael Auli in the Edinburgh SMT group as well as Kevin Knight, David Chiang and Andrew Dai for inspiring discussions. We appreciate Pichuan Chang, Huihsin Tseng, Richard Zens, Matthew Snover and Nguyen Bach for helping us understand their brilliant work. Many thanks to the anonymous reviewers for their insightful comments and suggestions. This work was supported in part by the EuroMatrixPlus project funded by the European Commission (7th Framework Programme) and in part under the"
D11-1079,N10-1052,0,0.0192205,"Rscore which uses word alignment to compute the permutation distance. As an alternative to GIZA++, we would like to experiment with syntactically informed aligners that better handle function words which often exhibit high alignment ambiguity due to low cross-lingual correspondence. Finally, since our soft dependency constraints promote reordering without increasing model complexity, further gains can be achieved when combining our approach with orthogonal studies to improve the quantity and quality of hierarchical (reordering) rules, such as relaxing hierarchical rule extraction constraints (Setiawan and Resnik, 2010) and selectively lexicalizing rules with function words (Setiawan et al., 2009). Acknowledgments We would like to thank Miles Osborne, Adam Lopez, Barry Haddow, Hieu Hoang, Philip Williams and Michael Auli in the Edinburgh SMT group as well as Kevin Knight, David Chiang and Andrew Dai for inspiring discussions. We appreciate Pichuan Chang, Huihsin Tseng, Richard Zens, Matthew Snover and Nguyen Bach for helping us understand their brilliant work. Many thanks to the anonymous reviewers for their insightful comments and suggestions. This work was supported in part by the EuroMatrixPlus project fu"
D11-1079,P08-1066,0,0.558226,"ish translation (Koehn et al., 2003), and Rule 8 which can be applied to the Figure 1 sentence. Fuzzy constituency constraints can solve this problem with a combination of product categories and slash categories (Chiang, 2010). Yet our cohesion penalty by nature admits these translations as cohesive (with no extra cost from es and Aozhou since both are locally resolved). Admittedly, our current implementation of the cohesion penalty is blind to some other meaningful nonconstituent collocations, such as neighbouring siblings of a common uncovered head (regulated as the “floating structure” in (Shen et al., 2008)). A concrete example is Rule 9 which is useful for the Figure 1 sentence. To address this problem, another feature can be defined in the same manner to capture how each head word is translated with its children. X → (es1 gibt2 , there1 is2 ) (7) X → (Aozhou1 shi2 , Australia1 is2 ) (8) X → (shaoshu1 guojia2 , few1 countries2 ) (9) Second, our cohesion penalty can be by nature more discriminative. Compared with the constituency constraints, the cohesion penalty is integer-valued, and can be made sensitive to the depth of each word in the dependency hierarchy (see Section 2.4). Inspired by (Mar"
D11-1079,2006.amta-papers.25,0,0.0604956,"ring (especially in long-distance cases), we have also tried tuning with a metric that highlights reordering, i.e., the LRscore (Birch and Osborne, 2010). LRscore is a linear interpolation of a lexical metric and a reordering metric. We interpolated BLEU (as the lexical metric) with the Kendall’s tau permutation distance (as the reordering metric). The Kendall’s tau permutation distance measures the relative word order difference between the transla862 tion output and the reference(s) and is particularly sensitive to long-distance reordering. Testing results in terms of BLEU, LRscore and TER (Snover et al., 2006) are shown in Table 4. Tuned with the LRscore, our feature-augmented model achieves further average improvements (compare “bin-2” and “bin-2-lr”) of 0.20 LRscore as well as 0.37 BLEU and 0.90 TER. Note that while the BLEU increase can largely be seen as a projection of the LRscore increase back into its lexical component, the consistent TER drop confirms that our improvement is not metric-specific3 . Altogether the final improvement is 1.21 BLEU, 1.30 LRscore and 3.36 TER on average over the baseline. However, an important question is how our features affect short, medium and long-distance reo"
D11-1079,N04-4026,0,0.103896,"successfully defined another cohesion constraint from the source depen864 dency structure, with the aim of improving reordering in phrase-based MT. To take a glance, Cherry (2008) and Bach et al. (2009b) define cohesion as translating a source dependency subtree contiguously into the target side without interruption (span or subtree overlapping), following Fox (2002). This span-based cohesion constraint has a different criterion from our wordbased cohesion penalty and often leads to opposite conclusions. Bach et al. (2009a) also use cohesion to correlate with the lexicalized reordering model (Tillman, 2004; Koehn et al., 2005), whereas we define an orthogonal dependency orientation feature to explicitly model head-dependent reordering. The fundamental difference, however, is rooted in the translation model. Their span-based cohesion constraint is implemented as an “interruption check” to encourage finishing a subtree before translating something else. This check is very effective for phrase-based decoding which searches over an entire space within the distortion limit in order to advance a hypothesis. In fact, it constrains reordering for the phrase-based model, as Cherry finds that the cohesio"
D11-1079,D09-1105,0,0.0521927,"om phrase structure parsing (de Marneffe et al., 2006). 2.1 Dependency Orientation Based on the assumption that constituents generally move as a whole (Quirk et al., 2005), we decompose the sentence reordering probability into the reordering probability for each aligned source word with respect to its head, excluding the root word at the top of the dependency hierarchy which does not have a head word. Similarly, Hayashi et al. (2010) also take a word-based reordering approach for HPBMT, but they model all possible pairwise orientation from the source side as a general linear ordering problem (Tromble and Eisner, 2009). To be more specific, we have a maximum entropy orientation classifier that predicts the probability of a source word being translated in a monotone or reversed manner with respect to its head. For example, 1 (b) S1 http://nlp.stanford.edu/software/lex-parser.shtml 858 S2 S3 S1 T1 ihead T1 idep T2 idep T2 ihead T3 T3 T4 T4 S2 S3 Figure 2: Word alignments to illustrate orientation classification. In (a), monotone (M); in (b), reversed (R). given the alignment in Figure 2(a), with the alignment points (idep , jdep ) for the source dependent word and (ihead , jhead ) for the source head word, we"
D11-1079,D07-1077,1,0.872369,"naturally supports constituent translations as well as some nonconstituent translations, if not all of them (as discussed in Section 2.2). Our dependency orientation feature is similar to the order model within dependency treelet translation (Quirk et al., 2005). Yet instead of a head-relative position number for each modifier word, we simply predict the head-dependent orientation which is either monotone or reversed. Our coarser-grained approach is more robust from a machine learning perspective, yet still captures prominent and long-distance reordering patterns observed in Chinese–English (Wang et al., 2007), German–English (Collins et al., 2005), Japanese– English (Katz-Brown and Collins, 2008) and translation from English to a group of SOV languages (Xu et al., 2009). Not committed to specific language pairs, we learn orientation classification from the word-aligned parallel data through maximum entropy training as Zens and Ney (2006) and Chang et al. (2009) for phrase-based translation and Xiong et al. (2006) for the BTG model (Wu, 1996). While Chang et al. (2009) also make use of source dependency, their orientation classification concerns two subsequent phrase pairs in the leftto-right phras"
D11-1079,J10-2004,0,0.039313,"although the hierarchical phrasebased model outperforms the phrase-based model in 857 terms of medium-range reordering, it does equally poorly in long-distance reordering due to constraints to guarantee efficiency. Syntax-based models that use phrase structure constituent labels as non-terminals in their transfer rules, exemplified by that of Galley et al. (2004), produce smarter and syntactically motivated reordering. However, when working with off-the-shelf tools for parsing and alignment, this approach may impose harsh limits on rule extraction and requires serious efforts of optimization (Wang et al., 2010). An alternative approach is to augment the general hierarchical phrase-based model with soft syntactic constraints. Here, we derive three word-based, complementary constraints from the source dependency parsing, including: • A dependency orientation feature, trained with maximum entropy on the word-aligned parallel data, which directly models the headdependent orientation for source words; • An integer-valued cohesion penalty that complements the dependency orientation feature, and fires when a word is not translated with its head. It measures derivation well-formedness and is used to indirec"
D11-1079,D07-1080,0,0.0219106,"haoshu1 guojia2 , few1 countries2 ) (9) Second, our cohesion penalty can be by nature more discriminative. Compared with the constituency constraints, the cohesion penalty is integer-valued, and can be made sensitive to the depth of each word in the dependency hierarchy (see Section 2.4). Inspired by (Marton and Resnik, 2008; Chiang et al., 2009), the cohesion penalty could also be made sensitive to the dependency relation of each word. However, this drastically increases the number of features and requires a tuning algorithm which scales better to high-dimensional model spaces, such as MIRA (Watanabe et al., 2007; Chiang et al., 2008). pobj 是 Depth 1 top Depth 2 澳洲 . 之一 有 prep 与 Depth 4 pobj Depth 5 nummod 少数 国家 cpm dobj Bin 1 3 Bin 2 3.1 nn rcmod Depth 3 the tree levels spread out. punct attr 邦交 的 北韩 Figure 3: Using 2 bins for the dependency parse tree of the Figure 1 sentence. 2.3 Unaligned Penalty The dependency orientation and cohesion penalty cannot be applied to unaligned source words. This may lead to search error, such as dropping (i.e., unaligning) key content words that are important for lexical translation and reordering. The problem is mitigated by an unaligned penalty applicable to all wo"
D11-1079,P96-1021,0,0.0524512,"h is more robust from a machine learning perspective, yet still captures prominent and long-distance reordering patterns observed in Chinese–English (Wang et al., 2007), German–English (Collins et al., 2005), Japanese– English (Katz-Brown and Collins, 2008) and translation from English to a group of SOV languages (Xu et al., 2009). Not committed to specific language pairs, we learn orientation classification from the word-aligned parallel data through maximum entropy training as Zens and Ney (2006) and Chang et al. (2009) for phrase-based translation and Xiong et al. (2006) for the BTG model (Wu, 1996). While Chang et al. (2009) also make use of source dependency, their orientation classification concerns two subsequent phrase pairs in the leftto-right phrase-based decoding (as apposed to each dependent word and its head) and is therefore less linguistically-motivated. 865 6 Conclusion We have derived three novel features from the source dependency structure for hierarchical phrase-based MT. They work as a whole to capitalize on two characteristics of the dependency representation: it is directly based on words and it directly connects head and child. The effectiveness of our approach has b"
D11-1079,P06-1066,0,0.0927272,"r reversed. Our coarser-grained approach is more robust from a machine learning perspective, yet still captures prominent and long-distance reordering patterns observed in Chinese–English (Wang et al., 2007), German–English (Collins et al., 2005), Japanese– English (Katz-Brown and Collins, 2008) and translation from English to a group of SOV languages (Xu et al., 2009). Not committed to specific language pairs, we learn orientation classification from the word-aligned parallel data through maximum entropy training as Zens and Ney (2006) and Chang et al. (2009) for phrase-based translation and Xiong et al. (2006) for the BTG model (Wu, 1996). While Chang et al. (2009) also make use of source dependency, their orientation classification concerns two subsequent phrase pairs in the leftto-right phrase-based decoding (as apposed to each dependent word and its head) and is therefore less linguistically-motivated. 865 6 Conclusion We have derived three novel features from the source dependency structure for hierarchical phrase-based MT. They work as a whole to capitalize on two characteristics of the dependency representation: it is directly based on words and it directly connects head and child. The effect"
D11-1079,W07-0706,0,0.0439119,"e 5: Example translations from the NIST MT08 set, output by the baseline model and “bin-2” model. The “-lr” version outputs are quite similar and not shown here. Translation outputs are in lower case. tures the boxed area as a whole and uses Rule 10 to perform the right global reordering. X → (dui1 X2 gandao3 manyi4 .5 , expressed3 satisfaction4 with1 X2 .5 ) 5 (10) Related Work In recent years, there has been a growing body of research on using dependency for statistical machine translation. Some directly encodes dependency in the translation model (Ding and Palmer, 2005; Quirk et al., 2005; Xiong et al., 2007; Shen et al., 2008; Mi and Liu, 2010), while others use dependency as a soft constraint (Cherry, 2008; Bach et al., 2009a,b; Chang et al., 2009). Among them, Shen et al. (2008) report that just filtering the phrase table by the socalled well-formed target dependency structure does not help, yet adding a target dependency language model improves performance significantly. Our intuitive interpretation is that the target dependency language model capitalizes on two characteristics of the dependency structure: it is based on words and it directly connects head and child. Therefore, the target dep"
D11-1079,N09-1028,0,0.0828397,"feature is similar to the order model within dependency treelet translation (Quirk et al., 2005). Yet instead of a head-relative position number for each modifier word, we simply predict the head-dependent orientation which is either monotone or reversed. Our coarser-grained approach is more robust from a machine learning perspective, yet still captures prominent and long-distance reordering patterns observed in Chinese–English (Wang et al., 2007), German–English (Collins et al., 2005), Japanese– English (Katz-Brown and Collins, 2008) and translation from English to a group of SOV languages (Xu et al., 2009). Not committed to specific language pairs, we learn orientation classification from the word-aligned parallel data through maximum entropy training as Zens and Ney (2006) and Chang et al. (2009) for phrase-based translation and Xiong et al. (2006) for the BTG model (Wu, 1996). While Chang et al. (2009) also make use of source dependency, their orientation classification concerns two subsequent phrase pairs in the leftto-right phrase-based decoding (as apposed to each dependent word and its head) and is therefore less linguistically-motivated. 865 6 Conclusion We have derived three novel featu"
D11-1079,W06-3108,0,0.0648542,", we simply predict the head-dependent orientation which is either monotone or reversed. Our coarser-grained approach is more robust from a machine learning perspective, yet still captures prominent and long-distance reordering patterns observed in Chinese–English (Wang et al., 2007), German–English (Collins et al., 2005), Japanese– English (Katz-Brown and Collins, 2008) and translation from English to a group of SOV languages (Xu et al., 2009). Not committed to specific language pairs, we learn orientation classification from the word-aligned parallel data through maximum entropy training as Zens and Ney (2006) and Chang et al. (2009) for phrase-based translation and Xiong et al. (2006) for the BTG model (Wu, 1996). While Chang et al. (2009) also make use of source dependency, their orientation classification concerns two subsequent phrase pairs in the leftto-right phrase-based decoding (as apposed to each dependent word and its head) and is therefore less linguistically-motivated. 865 6 Conclusion We have derived three novel features from the source dependency structure for hierarchical phrase-based MT. They work as a whole to capitalize on two characteristics of the dependency representation: it i"
D11-1079,W06-3119,0,0.174907,"t al., 2009a,b) (They do have a non-empty intersection, but neither subsumes the other). Therefore, our cohesion penalty is better suited for the hierarchical phrase-based model. To discourage nonconstituent translation, Chiang (2005) has proposed a constituency feature to examine whether a source rule span matches the source constituent as defined by phrase structure parsing. Finer-grained constituency constraints significantly improve hierarchical phrase-based MT when applied on the source side (Marton and Resnik, 2008; Chiang et al., 2009), or on the target side in a more tolerant fashion (Zollmann and Venugopal, 2006). Using both source and target syntax, but relaxing on rule extraction and substitution enables HPBMT to produce more well-formed and syntactically richer derivations (Chiang, 2010). Softening constituency matching with latent syntactic distributions proves to be helpful (Huang et al., 2010). Compared to constituency-based approaches, our cohesion penalty based on the dependency structure naturally supports constituent translations as well as some nonconstituent translations, if not all of them (as discussed in Section 2.2). Our dependency orientation feature is similar to the order model with"
D11-1079,D08-1076,0,\N,Missing
D16-1134,W13-2322,0,0.335895,"Missing"
D16-1134,W05-0909,0,0.252262,"Missing"
D16-1134,W13-2203,1,0.896683,"Missing"
D16-1134,W12-4204,1,0.908186,"Missing"
D16-1134,W11-2101,1,0.914645,"Missing"
D16-1134,W14-4005,0,0.0250442,"Missing"
D16-1134,P16-2013,0,0.0371161,"Missing"
D16-1134,W07-0738,0,0.0835328,"Missing"
D16-1134,N15-1124,0,0.125815,"Missing"
D16-1134,P15-1174,0,0.0311734,"Missing"
D16-1134,P07-2045,1,0.0114599,"Missing"
D16-1134,W05-0904,0,0.093592,"Missing"
D16-1134,W11-1002,0,0.0373567,"Missing"
D16-1134,lo-wu-2014-reliability,0,0.0405899,"Missing"
D16-1134,oepen-lonning-2006-discriminant,0,0.0919776,"Missing"
D16-1134,2006.amta-papers.25,0,0.300368,"Missing"
D16-1134,W15-3502,1,0.478779,"Missing"
D16-1134,P02-1040,0,\N,Missing
D19-5601,W18-2716,0,0.0593982,"Missing"
D19-5601,W04-1013,0,0.0851055,"-text NLG and MT along two axes: • MT+NLG: RotoWire, WMT19, Monolingual RotoWire refers to the RotoWire dataset (Wiseman et al., 2017) (train/valid), WMT19 refers to the set of parallel corpora allowable by the WMT 2019 English-German task, and Monolingual refers to monolingual data allowable by the same WMT 2019 task, pre-trained embeddings (e.g., GloVe (Pennington et al., 2014)), pre-trained contextualized embeddings (e.g., BERT (Devlin et al., 2019)), pre-trained language models (e.g., GPT-2 (Radford et al., 2019)). Textual Accuracy Measures: We used BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) as measures for texutal accuracy compared to reference summaries. Content Accuracy Measures: We evaluate the fidelity of the generated content to the input data using relation generation (RG), content selection (CS), and content ordering (CO) metrics (Wiseman et al., 2017). 2 model for both languages together, using a shared BPE vocabulary obtained from target game summaries and by prefixing the target text with the target language indicator. For MT and MT+NLG tracks, they mined the in-domain data by extracting basketball-related texts from Newscrawl when one of the following conditions are m"
D19-5601,D14-1162,0,0.0824718,"Missing"
D19-5601,D15-1044,0,0.0588225,"types of inputs. The results of the shared task are summarized in Sections 3 and 4. Introduction 2 Neural sequence to sequence models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) are now a workhorse behind a wide variety of different natural language processing tasks such as machine translation, generation, summarization and simplification. The 3rd Workshop on Neural Machine Translation and Generation (WNGT 2019) provided a forum for research in applications of neural models to machine translation and other language generation tasks (including summarization (Rush et al., 2015), NLG from structured data (Wen et al., 2015), dialog response generation (Vinyals and Le, 2015), among others). Overall, the workshop was held with two goals. First, it aimed to synthesize the current state of knowledge in neural machine translation and generation: this year we continued to encourage submissions that not only advance the state of the art through algorithmic advances, but also analyze and understand the current state of the art, pointing to future research directions. Towards this Summary of Research Contributions We published a call for long papers, extended abstracts for pre"
D19-5601,W18-6502,0,0.0159592,"ocument-level Generation and Translation # documents Avg. # tokens (En) Avg. # tokens (De) Vocabulary size (En) Vocabulary size (De) The first shared task at the workshop focused on document-level generation and translation. Many recent attempts at NLG have focused on sentencelevel generation (Lebret et al., 2016; Gardent et al., 2017). However, real world language generation applications tend to involve generation of much larger amount of text such as dialogues or multisentence summaries. The inputs to NLG systems also vary from structured data such as tables (Lebret et al., 2016) or graphs (Wang et al., 2018), to textual data (Nallapati et al., 2016). Because of such difference in data and domain, comparison between different methods has been nontrivial. This task aims to (1) push forward such document-level generation technology by providing a testbed, and (2) examine the differences between generation based on different types of inputs including both structured data and translations in another language. In particular, we provided the following 6 tracks which focus on different input/output requirements: Valid Test 242 323 320 4163 5425 240 328 324 - 241 329 325 - Table 1: Data statistics of Roto"
D19-5601,D13-1176,0,\N,Missing
D19-5601,D15-1199,0,\N,Missing
D19-5601,P02-1040,0,\N,Missing
D19-5601,P10-2041,0,\N,Missing
D19-5601,W14-3302,0,\N,Missing
D19-5601,D17-1239,0,\N,Missing
D19-5601,W14-7001,0,\N,Missing
D19-5601,W17-4717,0,\N,Missing
D19-5601,W17-3518,0,\N,Missing
D19-5601,N19-1423,0,\N,Missing
D19-5619,P17-4012,0,0.0255823,"aracter RNN to continue the beam search. When the beam search is complete, the most likely character sequence is generated as the best hypothesis. We evaluate decoding architectures using different levels of granularity in the vocabulary units and the attention mechanism, including the standard decoding architecture implemented either with subword (Sennrich et al., 2016) or fully character-level (Cherry et al., 2018) units, which constitute the baseline approaches, and the hierarchical decoding architecture, by implementing all in Pytorch (Paszke et al., 2017) within the OpenNMT-py framework (Klein et al., 2017). In order to evaluate how each generative method performs in languages with different morphological typology, we model the machine translation task from English into five languages from different language families and exhibiting distinct morphological typology: Arabic (templatic), Czech (mostly fusional, partially agglutinative), German (fusional), Italian (fusional) and Turkish (agglutinative). We use the TED Talks corpora (Cettolo, 2012) for training the NMT models, which range from 110K to 240K sentences, and the official development and test sets from IWSLT1 (Cettolo et al., 2017). The lo"
D19-5619,W17-4710,1,0.844901,"t al., 2018) evaluation sets. All models are implemented using gated recurrent units (GRU) (Cho et al., 2014) with the same number of parameters. The hierarchical decoding model implements a 3-layer GRU architecture, which is compared with a character-level decoder which also uses a 3-layer stacked GRU architecture. The subword-level decoder has a 2-layer stacked GRU architecture, to account also for the larger number of embedding parameters. The models using the standard architecture have the attention mechanism after the first GRU layer, and have residual connections after the second layer (Barone et al., 2017). The hierarchical decoder implements the attention mechanism after the second layer and has a residual connection between the first and second layers. The source sides of the data used for training characterlevel NMT models are segmented using BPE with 16,000 merge rules on the IWSLT data, and 32,000 on WMT. For subword-based models we learn shared merging rules for BPE for 16,000 (in IWSLT) and 32,000 (in WMT) units. The models use an embedding and hidden unit size of 512 under low-resource (IWSLT) and 1024 under high-resource (WMT) settings, and are trained using the Adam (Kinga and Ba, 201"
D19-5619,P16-1100,0,0.0429342,"Missing"
D19-5619,D15-1166,0,0.0569219,"ctional recurrent neural network (RNN) predicts the most likely output token yi in the target sequence using an approximate search algorithm based on the previous target token yi´1 , represented with the embedding of the previous token in the target sequence, the previous decoder hidden state, representing the sequence history, and the current attention context in the source sequence, represented by the context vector ct . The latter is a linear combination of the encoder hidden states, whose weights are dynamically computed by a dot product based similarity metric called the attention model (Luong et al., 2015). The probability of generating each target word yi is estimated via a softmax function T ezj ppyi “ zj |x; θq “ řK k“1 3 In this paper, we explore the benefit of integrating a notion of hierarchy into the decoding architecture which could increase the computational efficiency in character-level NMT, following the work of (Luong and Manning, 2016). In this architecture, the input embedding layer of the decoder is augmented with a character-level bi-RNN, which estimates a composition function over the embeddings of the characters in each word in order to compute the distributed representations"
D19-5619,P02-1040,0,0.103618,"egmented using BPE with 16,000 merge rules on the IWSLT data, and 32,000 on WMT. For subword-based models we learn shared merging rules for BPE for 16,000 (in IWSLT) and 32,000 (in WMT) units. The models use an embedding and hidden unit size of 512 under low-resource (IWSLT) and 1024 under high-resource (WMT) settings, and are trained using the Adam (Kinga and Ba, 2015) optimizer with a learning rate of 0.0003 and decay of 0.5, batch size of 100 and a dropout of 0.2. Decoding in all models is performed with a beam size of 5. The accuracy of each output is measured in terms of the BLEU metric (Papineni et al., 2002) and the significance of the improvements are measured using bootstrap hypothesis testing (Clark et al., 2011). 1 The International Workshop on Spoken Language Translation. 2 The Conference on Machine Translation, with shared task organized for news translation. 5 Experiments 190 Variation Paradigm contrast features Positive vs. comparative adjective Present vs. future tense Negation Singular vs. plural noun Present vs. past tense Compound generation Indicative vs. conditional mode Average Agreement features Pronoun vs. Nouns (gender) Pronoun vs. Nouns (number) Pronoun (plural) Pronoun (relati"
D19-5619,W18-6433,0,0.0568282,"Missing"
D19-5619,2012.eamt-1.60,1,0.62811,"baseline approaches, and the hierarchical decoding architecture, by implementing all in Pytorch (Paszke et al., 2017) within the OpenNMT-py framework (Klein et al., 2017). In order to evaluate how each generative method performs in languages with different morphological typology, we model the machine translation task from English into five languages from different language families and exhibiting distinct morphological typology: Arabic (templatic), Czech (mostly fusional, partially agglutinative), German (fusional), Italian (fusional) and Turkish (agglutinative). We use the TED Talks corpora (Cettolo, 2012) for training the NMT models, which range from 110K to 240K sentences, and the official development and test sets from IWSLT1 (Cettolo et al., 2017). The low-resource settings for the training data allows us to examine the quality of the internal representations learned by each decoder under high data sparseness. In order to evaluate how the performance of each method scales with increasing data size, we evaluate the models also by training with a multidomain training data using the public data sets from WMT2 (Bojar et al., 2016) in the English-to-German direction, followed by an analysis on e"
D19-5619,P16-1162,1,0.481276,"Missing"
D19-5619,D18-1461,1,0.843197,"cal distribution are often discarded during translation since they are not found in the vocabulary. The prominent approach to overcome this limitation is to segment words into subword units (Sennrich et al., 2016) and perform translation based on a vocabulary composed of these units. However, subword segmentation methods generally rely on statistical heuristics that lack any linguistic notion. Moreover, they are typically deployed as a pre-processing step before training the NMT model, hence, the predicted set of subword units are essentially not optimized for the translation task. Recently, (Cherry et al., 2018) extended the approach of NMT based on subword units to implement the translation model directly at the level of characters, which could reach comparable performance to the subword-based model, although this would require much larger networks which may be more difficult to train. The major reason to this requirement may lie behind the fact that treating the characters as individual tokens at the same level and processing the input sequences in linear time increases the difficulty of the learning task, where translation would then be modeled as a mapping between the characters in two languages."
D19-5619,W14-4012,0,0.12813,"Missing"
D19-5619,P11-2031,0,0.0325467,"n shared merging rules for BPE for 16,000 (in IWSLT) and 32,000 (in WMT) units. The models use an embedding and hidden unit size of 512 under low-resource (IWSLT) and 1024 under high-resource (WMT) settings, and are trained using the Adam (Kinga and Ba, 2015) optimizer with a learning rate of 0.0003 and decay of 0.5, batch size of 100 and a dropout of 0.2. Decoding in all models is performed with a beam size of 5. The accuracy of each output is measured in terms of the BLEU metric (Papineni et al., 2002) and the significance of the improvements are measured using bootstrap hypothesis testing (Clark et al., 2011). 1 The International Workshop on Spoken Language Translation. 2 The Conference on Machine Translation, with shared task organized for news translation. 5 Experiments 190 Variation Paradigm contrast features Positive vs. comparative adjective Present vs. future tense Negation Singular vs. plural noun Present vs. past tense Compound generation Indicative vs. conditional mode Average Agreement features Pronoun vs. Nouns (gender) Pronoun vs. Nouns (number) Pronoun (plural) Pronoun (relative-gender) Pronoun (relative-number) Positive vs. superlative adjective Simple vs. coordinated verbs (number)"
E14-1014,J07-4004,0,0.151191,"y lexicalized formalisms, the problem of grammar generalization can be cast largely as a problem of lexical extension. The present paper focuses on learning lexical categories for words that are unseen or lowfrequency in labeled data, from unlabeled data. Since lexical categories in a strongly lexicalized formalism are complex, fine-grained (and far more numerous than simple part-of-speech tags), they are relatively sparse in labeled data. Despite performing at state-of-the-art levels, a major source of error made by CCG parsers is related to unseen and low-frequency words (Hockenmaier, 2003; Clark and Curran, 2007; Thomforde and Steedman, 2011). The unseen words for which we learn categories are surprisingly commonplace words of English; examples are conquered, apprehended, subdivided, scoring, denotes, hunted, obsessed, residing, migrated (Wikipedia). Correctly learning to parse the predicate-argument structures associated with such words (expressed as lexical categories in the case of CCG), is important for opendomain parsing, not only for CCG but indeed for any parser. We show that a simple self-training method, Viterbi-EM (Neal and Hinton, 1998) when used to enhance the lexicon of a strongly-lexica"
E14-1014,W04-3215,1,0.816077,"Missing"
E14-1014,D08-1050,0,0.435833,"ts, we used sentences from the unlabeled WSJ portion of the ACL/DCI corpus (LDC93T1, 1993), and the WSJ portion of the ANC corpus (Reppen et al., 2005), limited to sentences containing 20 words or less, creating datasets of approximately 10, 20 and 40 million words each. Additionally, we have a dataset of 140 million words – 40M WSJ words plus an additional 100M from the New York Times. For domain-adaptation experiments, we use two different datasets. The first one consists of question-sentences – 1328 unlabeled questions, obtained by removing the manual annotation of the question corpus from Rimell and Clark (2008). The second out-of-domain dataset consists of Wikipedia data, approximately 40 million words in size, with sentence length &lt; 20 words. 5.2 Experimental setup We ran our semi-supervised method using our parser with a smoothed lexicon (from §4.1.1) as the initial model, on unlabeled data of different sizes/domains. For comparison, we also ran experiments using a POS-backed off parser (the original Hockenmaier and Steedman (2002) LexCat model) as the initial model. Viterbi-EM converged at 4-5 iterations. We then parsed various test sets using the semi-supervised lexicons thus obtained. In all ex"
E14-1014,P10-1152,0,0.0235071,"are now in the lexicon. Lexical entries for words in the parse are determined not by the POS-tag from a tagger, but directly by the parsing model, thus making the parse less susceptible to tagging errors. 5 Semi-supervised Learning We use Viterbi-EM (Neal and Hinton, 1998) as the self-training method. Viterbi-EM is an alternative to EM where instead of using the model parameters to find a true posterior from unlabeled data, a posterior based on the single maximumprobability (Viterbi) parse is used. Viterbi-EM has been used in various NLP tasks before and often performs better than classic EM (Cohen and Smith, 2010; Goldwater and Johnson, 2005; Spitkovsky et al., 2010). In practice, a given parsing model is used to obtain Viterbi parses of unlabeled sentences. The Viterbi parses are then treated as training data for a new model. This process is iterated until convergence. Since we are interested in learning the lexicon, we only consider lexical counts from Viterbi parses of the unlabeled sentences. Other parameters of the model are held at their supervised values. We conducted some experiments where we 3 For instance, we find that assigning all categories to unseen verbs gives a lexical category accurac"
E14-1014,W10-2902,0,0.0505347,"Missing"
E14-1014,P97-1003,0,0.328252,"an (2002) and Hockenmaier (2003)1 , except for the treatment of unseen and low-frequency words. We use a model (the LexCat model in Hockenmaier (2003)) that conditions the generation of constituents in the parse tree on the lexical category of the head word of the constituent, but not on the head word itself. While fully-lexicalized models that condition on words (and thus model wordto-word dependencies) are more accurate than unlexicalized ones like the LexCat model, we use an unlexicalized model2 for two reasons: first, 1 These generative models are similar to the Collins’ headbased models (Collins, 1997), where for every node, a head is generated first, and then a sister conditioned on the head. Details of the models are in Hockenmaier and Steedman (2002) and Hockenmaier 2003:pg 166. 2 A terminological clarification: unlexicalized here refers to the model, in the sense that head-word information is not used for rule-expansion. The formalism itself (CCG) is referred to as strongly-lexicalized, as used in the title of the paper. Formalisms like CCG and LTAG are considered strongly-lexicalized since linguistic knowledge (functions mapping words to syntactic structures/semantic interpretations) i"
E14-1014,C08-1025,1,0.765131,"about the word (for instance, its part-of-speech). Based on the part-of-speech of an unseen word in the unlabeled or test corpus, we add an entry to the lexicon of the word with the top n categories that have been seen with that part-of-speech in the labeled data. Each new entry of (w, cat), where w is a word and cat is a CCG category, is associated with a count c(w, cat), obtained as described below. Once all (w, cat) entries are added to the lexicon along with their counts, a probability model P (w|cat) is calculated over the entire lexicon. Our smoothing method is based on a method used in Deoskar (2008) for smoothing a PCFG lexicon. Eq. 1 and 2 apply it to CCG entries for unseen and rare words. In the first step, an outof-the-box POS tagger is used to tag the unlabeled or test corpus (we use the C&C tagger). Counts of words and POS-tags ccorpus (w, T ) are obtained from the tagged corpus. For the CCG lexicon, we ultimately need a count for a word w and a CCG category cat. To get this count, we split the count of a word and POS-tag amongst all categories seen with that tag in the supervised data in the same ratio as the ratio of the categories in the supervised data. In Eq. 1, this ratio is c"
E14-1014,W01-0521,0,0.030695,"een words. 1 Introduction An important open problem in natural language parsing is to generalize supervised parsers, which are trained on hand-labeled data, using unlabeled data. The problem arises because further handlabeled data in the amounts necessary to significantly improve supervised parsers are very unlikely to be made available. Generalization is also necessary in order to achieve good performance on parsing in textual domains other than the domain of the available labeled data. For example, parsers trained on Wall Street Journal (WSJ) data suffer a fall in accuracy on other domains (Gildea, 2001). In this paper, we use self-training to generalize the lexicon of a Combinatory Categorial Grammar (CCG) (Steedman, 2000) parser. CCG is a strongly lexicalized formalism, in which every word is associated with a syntactic category (similar to an elementary syntactic structure) indicat126 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 126–134, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics Charniak, 1993). We found that in order for performance to improve, unlabeled data should be used o"
E14-1014,D09-1058,0,0.0187492,"n word is derived from partial parses of sentences with just that one word unknown. The method is capable of inducing unseen categories types (the third kind of sparsity mentioned in §2.1), but due to algorithmic and efficiency issues, it did not achieve the broad-coverage needed for grammar generalisation of a high-end parser. It is more relevant for low-resource languages which do not have substantial labeled data and category type discovery is important. Some notable positive results for non-CCG parsers are McClosky et al. (2006) who use a parser-reranker combination. Koo et al. (2008) and Suzuki et al. (2009) use unsupervised wordclusters as features in a dependency parser to get lexical dependencies. This has some notional similarity to categories, since, like categories, clusters are less fine-grained than words but more finegrained than POS-tags. 4 our lexicon smoothing procedure (described in the next section) introduces new words and new categories for words into the lexicon. Lexical categories are added to the lexicon for seen and unseen words, but no new category types are introduced. Since the LexCat model conditions rule expansions on lexical categories, but not on words, it is still able"
E14-1014,P02-1043,1,0.856429,"s in fact more generally relevant than for CCG parsers alone—the dependence of parsers on POS-taggers was cited as one of the problems in domain-adaptation of parsers in the NAACL2012 shared task on parsing the web (Petrov and McDonald, 2012). Lease and Charniak (2005) obtained an improvement in the accuracy of the Charniak (2000) parser on a biomedical domain simply by training a new POS tagger model. In the following section, we describe an alternative smoothing-based approach to handling unSupervised Parser The CCG parser used in this paper is a reimplementation of the generative parser of Hockenmaier and Steedman (2002) and Hockenmaier (2003)1 , except for the treatment of unseen and low-frequency words. We use a model (the LexCat model in Hockenmaier (2003)) that conditions the generation of constituents in the parse tree on the lexical category of the head word of the constituent, but not on the head word itself. While fully-lexicalized models that condition on words (and thus model wordto-word dependencies) are more accurate than unlexicalized ones like the LexCat model, we use an unlexicalized model2 for two reasons: first, 1 These generative models are similar to the Collins’ headbased models (Collins,"
E14-1014,D11-1115,1,0.823079,", the problem of grammar generalization can be cast largely as a problem of lexical extension. The present paper focuses on learning lexical categories for words that are unseen or lowfrequency in labeled data, from unlabeled data. Since lexical categories in a strongly lexicalized formalism are complex, fine-grained (and far more numerous than simple part-of-speech tags), they are relatively sparse in labeled data. Despite performing at state-of-the-art levels, a major source of error made by CCG parsers is related to unseen and low-frequency words (Hockenmaier, 2003; Clark and Curran, 2007; Thomforde and Steedman, 2011). The unseen words for which we learn categories are surprisingly commonplace words of English; examples are conquered, apprehended, subdivided, scoring, denotes, hunted, obsessed, residing, migrated (Wikipedia). Correctly learning to parse the predicate-argument structures associated with such words (expressed as lexical categories in the case of CCG), is important for opendomain parsing, not only for CCG but indeed for any parser. We show that a simple self-training method, Viterbi-EM (Neal and Hinton, 1998) when used to enhance the lexicon of a strongly-lexicalized parser can be an effectiv"
E14-1014,W09-3306,0,0.419633,"erent sizes of unlabeled data on accuracy of unseen verbs for the two testsets TEST- HOV and TEST-4 SEC . Improvements are monotonic with increasing unlabeled data sizes, up to 40M words. The additional 100M words of NYT also improve the models but to a lesser degree, possibly due to the difference in domain. The graphs indicate that the method will lead to more improvements as more unlabeled data (especially WSJ data) is added. Wikipedia We obtain statistically significant improvements in overall scores over a testset consisting of Wikipedia sentences hand-annotated with CCG categories (from Honnibal et al. (2009)) (Table 4). We also obtained improvements in lexical category accuracy on unseen words, and on unseen verbs alone (not shown), but could not prove significance. This testset contains only 200 sentences, and counts for unseen words are too small for significance tests, although there are numeric improvements. However, the overall improvement is statistically significantly, showing that adapting the lexicon alone is effective for a new domain. 7 This could be because verbs in the Zipfian tail have more idiosyncratic subcategorization patterns than mid-frequency verbs, and thus are harder for a"
E14-1014,P08-1068,0,0.0504195,"category for an unknown word is derived from partial parses of sentences with just that one word unknown. The method is capable of inducing unseen categories types (the third kind of sparsity mentioned in §2.1), but due to algorithmic and efficiency issues, it did not achieve the broad-coverage needed for grammar generalisation of a high-end parser. It is more relevant for low-resource languages which do not have substantial labeled data and category type discovery is important. Some notable positive results for non-CCG parsers are McClosky et al. (2006) who use a parser-reranker combination. Koo et al. (2008) and Suzuki et al. (2009) use unsupervised wordclusters as features in a dependency parser to get lexical dependencies. This has some notional similarity to categories, since, like categories, clusters are less fine-grained than words but more finegrained than POS-tags. 4 our lexicon smoothing procedure (described in the next section) introduces new words and new categories for words into the lexicon. Lexical categories are added to the lexicon for seen and unseen words, but no new category types are introduced. Since the LexCat model conditions rule expansions on lexical categories, but not o"
E14-1014,I05-1006,0,0.0253009,"rategy has given good performance in general for CCG parsers, but it has the disadvantage that POS tagging errors are propagated. The parser can never recover from a tagging error, a problem that is serious for words in the Zipfian tail, where these words might also be unseen for the POS tagger and hence more likely to be tagged incorrectly. This issue is in fact more generally relevant than for CCG parsers alone—the dependence of parsers on POS-taggers was cited as one of the problems in domain-adaptation of parsers in the NAACL2012 shared task on parsing the web (Petrov and McDonald, 2012). Lease and Charniak (2005) obtained an improvement in the accuracy of the Charniak (2000) parser on a biomedical domain simply by training a new POS tagger model. In the following section, we describe an alternative smoothing-based approach to handling unSupervised Parser The CCG parser used in this paper is a reimplementation of the generative parser of Hockenmaier and Steedman (2002) and Hockenmaier (2003)1 , except for the treatment of unseen and low-frequency words. We use a model (the LexCat model in Hockenmaier (2003)) that conditions the generation of constituents in the parse tree on the lexical category of the"
E14-1014,N06-1020,0,0.343354,"data is that of Thomforde and Steedman (2011), in which a CCG category for an unknown word is derived from partial parses of sentences with just that one word unknown. The method is capable of inducing unseen categories types (the third kind of sparsity mentioned in §2.1), but due to algorithmic and efficiency issues, it did not achieve the broad-coverage needed for grammar generalisation of a high-end parser. It is more relevant for low-resource languages which do not have substantial labeled data and category type discovery is important. Some notable positive results for non-CCG parsers are McClosky et al. (2006) who use a parser-reranker combination. Koo et al. (2008) and Suzuki et al. (2009) use unsupervised wordclusters as features in a dependency parser to get lexical dependencies. This has some notional similarity to categories, since, like categories, clusters are less fine-grained than words but more finegrained than POS-tags. 4 our lexicon smoothing procedure (described in the next section) introduces new words and new categories for words into the lexicon. Lexical categories are added to the lexicon for seen and unseen words, but no new category types are introduced. Since the LexCat model co"
E14-1014,J94-2001,0,0.0411992,"parser. We show that a simple self-training method, Viterbi-EM (Neal and Hinton, 1998) when used to enhance the lexicon of a strongly-lexicalized parser can be an effective strategy for self-training and domain-adaptation. Our learnt lexicons improve on the lexical category accuracy of two supervised CCG parsers (Hockenmaier (2003) and the Clark and Curran (2007) parser, C&C) on within-domain (WSJ) and out-of-domain test sets (a question corpus and a Wikipedia corpus). In most prior work, when EM was initialized based on labeled data, its performance did not improve over the supervised model (Merialdo, 1994; Statistical parsers trained on labeled data suffer from sparsity, both grammatical and lexical. For parsers based on strongly lexicalized grammar formalisms (such as CCG , which has complex lexical categories but simple combinatory rules), the problem of sparsity can be isolated to the lexicon. In this paper, we show that semi-supervised Viterbi-EM can be used to extend the lexicon of a generative CCG parser. By learning complex lexical entries for low-frequency and unseen words from unlabeled data, we obtain improvements over our supervised model for both indomain (WSJ) and out-of-domain (q"
E14-1014,Q13-1015,1,\N,Missing
E14-1014,J07-3004,1,\N,Missing
E14-1014,W05-0615,0,\N,Missing
E17-3017,W14-3346,0,0.0173175,"2016). Nematus is available under a permissive BSD license. By default, the training objective in Nematus is cross-entropy minimization on a parallel training corpus. Training is performed via stochastic gradient descent, or one of its variants with adaptive learning rate (Adadelta (Zeiler, 2012), RmsProp (Tieleman and Hinton, 2012), Adam (Kingma and Ba, 2014)). Additionally, Nematus supports minimum risk training (MRT) (Shen et al., 2016) to optimize towards an arbitrary, sentence-level loss function. Various MT metrics are supported as loss function, including smoothed sentence-level B LEU (Chen and Cherry, 2014), METEOR (Denkowski and Lavie, 2011), BEER (Stanojevic and Sima’an, 2014), and any interpolation of implemented metrics. To stabilize training, Nematus supports early stopping based on cross entropy, or an arbitrary loss function defined by the user. 4 Conclusion Acknowledgments This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreements 645452 (QT21), 644333 (TraMOOC), 644402 (HimL) and 688139 (SUMMA). Usability Features References In addition to the main algorithms to train and decode with an NMT model, Nematus includes fe"
E17-3017,W11-2107,0,0.0197012,"r a permissive BSD license. By default, the training objective in Nematus is cross-entropy minimization on a parallel training corpus. Training is performed via stochastic gradient descent, or one of its variants with adaptive learning rate (Adadelta (Zeiler, 2012), RmsProp (Tieleman and Hinton, 2012), Adam (Kingma and Ba, 2014)). Additionally, Nematus supports minimum risk training (MRT) (Shen et al., 2016) to optimize towards an arbitrary, sentence-level loss function. Various MT metrics are supported as loss function, including smoothed sentence-level B LEU (Chen and Cherry, 2014), METEOR (Denkowski and Lavie, 2011), BEER (Stanojevic and Sima’an, 2014), and any interpolation of implemented metrics. To stabilize training, Nematus supports early stopping based on cross entropy, or an arbitrary loss function defined by the user. 4 Conclusion Acknowledgments This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreements 645452 (QT21), 644333 (TraMOOC), 644402 (HimL) and 688139 (SUMMA). Usability Features References In addition to the main algorithms to train and decode with an NMT model, Nematus includes features aimed towards facilitating ex"
E17-3017,W14-3354,0,0.0551932,"Missing"
E17-3017,E17-2025,0,0.133573,"Missing"
E17-3017,W16-2209,1,0.131218,"axout before the softmax layer. • In both encoder and decoder word embedding layers, we do not use additional biases. • Compared to Look, Generate, Update decoder phases in Bahdanau et al. (2015), we implement Look, Update, Generate which drastically simplifies the decoder implementation (see Table 1). • Optionally, we perform recurrent Bayesian dropout (Gal, 2015). • Instead of a single word embedding at each source position, our input representations allows multiple features (or “factors”) at each time step, with the final embedding being the concatenation of the embeddings of each feature (Sennrich and Haddow, 2016). • We allow tying of embedding matrices (Press and Wolf, 2017; Inan et al., 2016). We will here describe some differences in more detail: available at https://github.com/rsennrich/nematus https://github.com/nyu-dl/dl4mt-tutorial 65 Proceedings of the EACL 2017 Software Demonstrations, Valencia, Spain, April 3-7 2017, pages 65–68 c 2017 Association for Computational Linguistics z0j being the reset and update gate activations. In this formulation, W0 , U0 , Wr0 , U0r , Wz0 , U0z are trained model parameters; σ is the logistic sigmoid activation function. The attention mechanism, ATT, inputs the"
E17-3017,W16-2323,1,0.216734,"Sutskever et al., 2014) has recently established itself as a new state-of-the art in machine translation. We present Nematus1 , a new toolkit for Neural Machine Translation. Nematus has its roots in the dl4mt-tutorial.2 We found the codebase of the tutorial to be compact, simple and easy to extend, while also producing high translation quality. These characteristics make it a good starting point for research in NMT. Nematus has been extended to include new functionality based on recent research, and has been used to build top-performing systems to last year’s shared translation tasks at WMT (Sennrich et al., 2016) and IWSLT (Junczys-Dowmunt and Birch, 2016). Nematus is implemented in Python, and based on the Theano framework (Theano Development Team, 2016). It implements an attentional encoder–decoder architecture similar to Bahdanau et al. (2015). Our neural network architecture differs in some aspect from theirs, and we will discuss differences in more detail. We will also describe additional functionality, aimed to enhance usability and performance, which has been implemented in Nematus. 1 2 Neural Network Architecture • In the decoder, we use a feedforward hidden layer with tanh non-linearity rathe"
E17-3017,P16-1159,0,0.0772511,"ll documented toolkit to support their research. The toolkit is by no means limited to research, and has been used to train MT systems that are currently in production (WIPO, 2016). Nematus is available under a permissive BSD license. By default, the training objective in Nematus is cross-entropy minimization on a parallel training corpus. Training is performed via stochastic gradient descent, or one of its variants with adaptive learning rate (Adadelta (Zeiler, 2012), RmsProp (Tieleman and Hinton, 2012), Adam (Kingma and Ba, 2014)). Additionally, Nematus supports minimum risk training (MRT) (Shen et al., 2016) to optimize towards an arbitrary, sentence-level loss function. Various MT metrics are supported as loss function, including smoothed sentence-level B LEU (Chen and Cherry, 2014), METEOR (Denkowski and Lavie, 2011), BEER (Stanojevic and Sima’an, 2014), and any interpolation of implemented metrics. To stabilize training, Nematus supports early stopping based on cross entropy, or an arbitrary loss function defined by the user. 4 Conclusion Acknowledgments This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreements 645452 (QT2"
E17-3029,P09-1039,0,0.0167833,"Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 Knowledge Base Construction This component provides a knowledge base of factual relations between entities, built with a model based on Universal Schemas (Riedel et al., 2013), a low-rank matrix factorization approach.The entity relations are extracted jointly across multiple languages, with entities pairs as rows and a set of structured relations and textual patterns as columns. The relations provide information about how various entities present in n"
E17-3029,N13-1008,1,0.77865,"Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 Knowledge Base Construction This component provides a knowledge base of factual relations between entities, built with a model based on Universal Schemas (Riedel et al., 2013), a low-rank matrix factorization approach.The entity relations are extracted jointly across multiple languages, with entities pairs as rows and a set of structured relations and textual patterns as columns. The relations provide information about how various entities present in news https://github.com/andre-martins/ TurboParser 118 Marcin Junczys-Dowmunt, Tomasz Dwojak, and Hieu Hoang. 2016. Is neural machine translation ready for deployment? A case study on 30 translation directions. CoRR, abs/1610.01108. documents are connected. 2.9 Storyline Construction and Summarization Storylines are co"
E17-3029,E17-3017,1,0.751559,"m a multilingual corpus of nearly 600k documents in 8 of the 9 SUMMA languages (all except Latvian), which were manually annotated by journalists at Deutsche Welle. The document model is a hierarchical attention network with attention at each level of the hierarchy, inspired by Yang et al. (2016), followed by a sigmoid classification layer. 2.4 Machine Translation 2.7 Deep Semantic Tagging The lingua franca within SUMMA is English. Machine translation based on neural networks is used to translate content into English automatically. The back-end MT systems are trained with the Nematus Toolkit (Sennrich et al., 2017); translation is performed with AmuNMT (Junczys-Dowmunt et al., 2016). The system also has a component that performs semantic parsing into Abstract Meaning Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is"
E17-3029,P13-1020,0,0.025126,"Missing"
E17-3029,E17-1051,1,0.815701,"lassification layer. 2.4 Machine Translation 2.7 Deep Semantic Tagging The lingua franca within SUMMA is English. Machine translation based on neural networks is used to translate content into English automatically. The back-end MT systems are trained with the Nematus Toolkit (Sennrich et al., 2017); translation is performed with AmuNMT (Junczys-Dowmunt et al., 2016). The system also has a component that performs semantic parsing into Abstract Meaning Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 K"
N16-1005,2015.iwslt-papers.5,0,0.0232567,"2) have used a bilingual English–German corpus to automatically annotate the T-V distinction, and train a classifier to predict the address from monolingual English text. Applying a source-side classifier is potential future work, although we note that the baseline encoder– decoder NMT system already has some disambiguating power. Our T-V classification is more comprehensive, including more pronoun forms and imperative verbs. Previous research on neural language models has proposed including various types of extra information, such as topic, genre or document context (Mikolov and Zweig, 2012; Aransa et al., 2015; Ji et al., 2015; Wang and Cho, 2015). Our method is somewhat similar, with the main novel idea being that we can target specific phenomena, such as honorifics, via an automatic annotation of the target side of a parallel corpus. On the modelling side, our method is slightly different in that we pass the extra information to the encoder of an encoder–decoder network, rather than the (decoder) hidden layer or output layer. We found this to be very effective, but trying different architectures is potential future work. In rule-based machine translation, user options to control the level of poli"
N16-1005,D14-1179,0,0.022336,"Missing"
N16-1005,E12-1064,0,0.0189134,"LEU indicates that the T-V distinction is relevant for translation. We expect that the actual relevance for humans depends on the task. For gisting, we expect the T-V distinction to have little effect on comprehensibility. For professional translation that uses MT with postediting, producing the desired honorifics is likely to improve post-editing speed and satisfaction. In an evaluation of MT for subtitle translation, Etchegoyhen et al. (2014) highlight the production of the appropriate T-V form as “a limitation of MT technology” that was “often frustrat[ing]” to post-editors. 6 Related Work Faruqui and Pado (2012) have used a bilingual English–German corpus to automatically annotate the T-V distinction, and train a classifier to predict the address from monolingual English text. Applying a source-side classifier is potential future work, although we note that the baseline encoder– decoder NMT system already has some disambiguating power. Our T-V classification is more comprehensive, including more pronoun forms and imperative verbs. Previous research on neural language models has proposed including various types of extra information, such as topic, genre or document context (Mikolov and Zweig, 2012; Ar"
N16-1005,P15-1001,0,0.153295,"describe rules to automatically annotate the T-V distinction in German text. • we describe how to use target-side T-V annotation in NMT training to control the level of politeness at test time via side constraints. • we perform oracle experiments to demonstrate the impact of controlling politeness in NMT. 2 Background: Neural Machine Translation Attentional neural machine translation (Bahdanau et al., 2015) is the current state of the art for 35 Proceedings of NAACL-HLT 2016, pages 35–40, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics English→German (Jean et al., 2015b; Luong and Manning, 2015). We follow the neural machine translation architecture by Bahdanau et al. (2015), which we will briefly summarize here. However, our approach is not specific to this architecture. The neural machine translation system is implemented as an attentional encoder-decoder network. The encoder is a bidirectional neural network with gated recurrent units (Cho et al., 2014) that reads an input sequence x = (x1 , ..., xm ) and calculates − → −→ a forward sequence of hidden states (h1 , ..., hm ), ← − ←− and a backward sequence (h1 , ..., hm ). The hidden → − ← − states hj and"
N16-1005,2015.iwslt-evaluation.11,0,0.149954,"Missing"
N16-1005,I05-6011,0,0.0321512,"roach is simple and applicable to a wide range of NMT architectures and our experiments suggest that the incorporation of the side constraint as an extra source token is very effective. 4 Automatic Training Set Annotation Our approach relies on annotating politeness in the training set to obtain the politeness feature which we discussed previously. We choose a sentence-level annotation because a target-side honorific may have no word-level correspondence in the source. We will discuss the annotation of German as an example, but our method could be applied to other languages, such as Japanese (Nariyama et al., 2005). German has distinct pronoun forms for informal and polite address, as shown in Table 1. A further difference between informal and polite speech are imperative verbs, and the original imperative forms are considered informal. The polite alternative is to use 3rd person plural forms with subject in position 2: • Ruf mich zurück. (informal) (Call me back.) • Rufen Sie mich zurück. (polite) (Call you me back.) We automatically annotate politeness on a sentence level with rules based on a morphosyntactic annotation by ParZu (Sennrich et al., 2013). Sentences containing imperative verbs are labell"
N16-1005,R13-1079,1,0.389147,"Missing"
N16-1005,tiedemann-2012-parallel,0,0.0240309,"olite forms and (neutral) 3rd person forms by their capitalization. If a sentence matches rules for both classes, we label it as informal – we found that our lowestprecision rule is the annotation of sentence-initial Sie. All sentences without a match are considered neutral. 5 Evaluation Our empirical research questions are as follows: • can we control the production of honorifics in neural machine translation via side constraints? • how important is the T-V distinction for translation quality (as measured by B LEU)? 5.1 Data and Methods We perform English→German experiments on OpenSubtitles (Tiedemann, 2012)1 , a parallel corpus of movie subtitles. Machine translation is commonly used in the professional translation of movie subtitles in a post-editing workflow, and politeness is considered an open problem for subtitle translation (Etchegoyhen et al., 2014). We use OpenSubtitles2012 as training corpus, and random samples from OpenSubtitles2013 for testing. The training corpus consists of of 5.58 million sentence pairs, out of which we label 0.48 million sentence pairs as polite, and 1.09 million as informal. We train an attentional encoder-decoder NMT system using Groundhog2 (Bahdanau et al., 201"
N16-1005,P16-1162,1,\N,Missing
N18-1118,W16-2348,1,0.898526,"Missing"
N18-1118,W09-2404,0,0.253424,"Missing"
N18-1118,L16-1100,0,0.184876,"Missing"
N18-1118,W16-2360,0,0.064602,"Missing"
N18-1118,D17-1263,0,0.069335,"Missing"
N18-1118,E17-3017,1,0.865828,"Missing"
N18-1118,P16-1162,1,0.644258,"Missing"
N18-1118,P17-2031,0,0.068079,"Missing"
N18-1118,L16-1147,0,0.126825,"Missing"
N18-1118,W17-4811,0,0.207132,"Missing"
N18-1118,D17-1301,0,0.230729,"Missing"
N18-1118,N16-1004,0,0.0681389,"Missing"
N18-1118,P02-1040,0,0.10402,"Missing"
N18-1118,W17-4702,1,0.90194,"Missing"
P07-2045,N03-2002,0,0.152204,"nfusion networks. This input type has been used successfully for speech to text translation (Shen et al. 2006). Every factor on the target language can have its own language model. Since many factors, like lemmas and POS tags, are less sparse than surface forms, it is possible to create a higher order language models for these factors. This may encourage more syntactically correct output. In Figure 3 we apply two language models, indicated by the shaded arrows, one over the words and another over the lemmas. Moses is also able to integrate factored language models, such as those described in (Bilmes and Kirchhoff 2003) and (Axelrod 2006). 4 Confusion Network Decoding Machine translation input currently takes the form of simple sequences of words. However, there are increasing demands to integrate machine translation technology into larger information processing systems with upstream NLP/speech processing tools (such as named entity recognizers, speech recognizers, morphological analyzers, etc.). These upstream processes tend to generate multiple, erroneous hypotheses with varying confidence. Current MT systems are designed to process only one input hypothesis, making them vulnerable to errors in the input."
P07-2045,koen-2004-pharaoh,0,0.148177,"to be duplicated. This has also hindered effective comparisons of the different elements of the systems. By providing a free and complete toolkit, we hope that this will stimulate the development of the field. For this system to be adopted by the community, it must demonstrate performance that is comparable to the best available systems. Moses has shown that it achieves results comparable to the most competitive and widely used statistical machine translation systems in translation quality and run-time (Shen et al. 2006). It features all the capabilities of the closed sourced Pharaoh decoder (Koehn 2004). Apart from providing an open-source toolkit for SMT, a further motivation for Moses is to extend phrase-based translation with factors and confusion network decoding. The current phrase-based approach to statistical machine translation is limited to the mapping of small text chunks without any explicit use of linguistic information, be it morphological, syntactic, or semantic. These additional sources of information have been shown to be valuable when integrated into pre-processing or post-processing steps. Moses also integrates confusion network decoding, which allows the translation of amb"
P07-2045,D07-1091,1,0.158367,"Missing"
P07-2045,N03-1017,1,0.161374,"informatik.rwth-aachen.de. 5 redpony@umd.edu. 6 bojar@ufal.ms.mff.cuni.cz. 7 07aec_2@williams.edu. 8 evh4@cornell.edu 2 Abstract We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks. 1 Motivation Phrase-based statistical machine translation (Koehn et al. 2003) has emerged as the dominant paradigm in machine translation research. However, until now, most work in this field has been carried out on proprietary and in-house research systems. This lack of openness has created a high barrier to entry for researchers as many of the components required have had to be duplicated. This has also hindered effective comparisons of the different elements of the systems. By providing a free and complete toolkit, we hope that this will stimulate the development of the field. For this system to be adopted by the community, it must demonstrate performance that is co"
P07-2045,P03-1021,0,0.176468,"ent data structures in Moses for the memory-intensive translation model and language model allow the exploitation of much larger data resources with limited hardware. 177 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177–180, c Prague, June 2007. 2007 Association for Computational Linguistics 2 Toolkit 3 The toolkit is a complete out-of-the-box translation system for academic research. It consists of all the components needed to preprocess data, train the language models and the translation models. It also contains tools for tuning these models using minimum error rate training (Och 2003) and evaluating the resulting translations using the BLEU score (Papineni et al. 2002). Moses uses standard external tools for some of the tasks to avoid duplication, such as GIZA++ (Och and Ney 2003) for word alignments and SRILM for language modeling. Also, since these tasks are often CPU intensive, the toolkit has been designed to work with Sun Grid Engine parallel environment to increase throughput. In order to unify the experimental stages, a utility has been developed to run repeatable experiments. This uses the tools contained in Moses and requires minimal changes to set up and customiz"
P07-2045,J03-1002,0,0.164868,"L 2007 Demo and Poster Sessions, pages 177–180, c Prague, June 2007. 2007 Association for Computational Linguistics 2 Toolkit 3 The toolkit is a complete out-of-the-box translation system for academic research. It consists of all the components needed to preprocess data, train the language models and the translation models. It also contains tools for tuning these models using minimum error rate training (Och 2003) and evaluating the resulting translations using the BLEU score (Papineni et al. 2002). Moses uses standard external tools for some of the tasks to avoid duplication, such as GIZA++ (Och and Ney 2003) for word alignments and SRILM for language modeling. Also, since these tasks are often CPU intensive, the toolkit has been designed to work with Sun Grid Engine parallel environment to increase throughput. In order to unify the experimental stages, a utility has been developed to run repeatable experiments. This uses the tools contained in Moses and requires minimal changes to set up and customize. The toolkit has been hosted and developed under sourceforge.net since inception. Moses has an active research community and has reached over 1000 downloads as of 1st March 2007. The main online pre"
P07-2045,P02-1040,0,0.148118,"d language model allow the exploitation of much larger data resources with limited hardware. 177 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177–180, c Prague, June 2007. 2007 Association for Computational Linguistics 2 Toolkit 3 The toolkit is a complete out-of-the-box translation system for academic research. It consists of all the components needed to preprocess data, train the language models and the translation models. It also contains tools for tuning these models using minimum error rate training (Och 2003) and evaluating the resulting translations using the BLEU score (Papineni et al. 2002). Moses uses standard external tools for some of the tasks to avoid duplication, such as GIZA++ (Och and Ney 2003) for word alignments and SRILM for language modeling. Also, since these tasks are often CPU intensive, the toolkit has been designed to work with Sun Grid Engine parallel environment to increase throughput. In order to unify the experimental stages, a utility has been developed to run repeatable experiments. This uses the tools contained in Moses and requires minimal changes to set up and customize. The toolkit has been hosted and developed under sourceforge.net since inception. Mo"
P07-2045,N07-1062,1,0.152186,"up gigabytes of disk space, but for the translation of a single sentence only a tiny fraction of this table is needed. Moses implements an efficient representation of the phrase translation table. Its key properties are a prefix tree structure for source words and on demand loading, i.e. only the fraction of the phrase table that is needed to translate a sentence is loaded into the working memory of the decoder. For the Chinese-English NIST task, the memory requirement of the phrase table is reduced from 1.7 gigabytes to less than 20 mega bytes, with no loss in translation quality and speed (Zens and Ney 2007). The other large data resource for statistical machine translation is the language model. Almost unlimited text resources can be collected from the Internet and used as training data for language modeling. This results in language models that are too large to easily fit into memory. The Moses system implements a data structure for language models that is more efficient than the canonical SRILM (Stolcke 2002) implementation used in most systems. The language model on disk is also converted into this binary format, resulting in a minimal loading time during start-up of the decoder. An even more"
P07-2045,D08-1076,0,\N,Missing
P07-2045,2006.iwslt-evaluation.8,1,\N,Missing
P11-1103,W05-0909,0,0.0286258,"quality of word order is poor (Birch et al., 2010). There are currently two main approaches to evaluating reordering. The first is exemplified by the B LEU score (Papineni et al., 2002), which counts the number of matching n-grams between the reference and the hypothesis. Word order is captured by the proportion of longer n-grams which match. This method does not consider the position of matching words, and only captures ordering differences if there is an exact match between the words in the translation and the reference. Another approach is taken by two other commonly used metrics, METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006). They both search for an alignment between the translation and the reference, and from this they calculate a penalty based on the number of differences in order between the two sentences. When block moves are allowed the search space is very large, and matching stems and synonyms introduces errors. Importantly, none of these metrics capture the distance by which words are out of order. Also, they conflate reordering performance with the quality of the lexical items in the translation, making it difficult to tease apart the impact of changes. More sophisticated me"
P11-1103,D09-1030,0,0.0491438,"Missing"
P11-1103,N10-1080,0,0.035834,"to optimise, leading to the joint best scores at test time. This is an important result, as it shows that by training with the LRscore objective function, B LEU scores do not decrease, which is desirable as B LEU scores are usually reported in the field. The LRscore also results in better scores when evaluated with itself and the other two baseline metrics, TER and METEOR. Reordering and the lexical metrics are orthogonal information sources, and this shows that combining them results in better performing systems. B LEU has shown to be a strong baseline metric to use as an objective function (Cer et al., 2010), and so the LRscore performance in Table 5 is a good result. Examining the weights that result from the different MERT runs, the only notable difference is that the weight of the distortion cost is considerably lower with the LRscore. This shows more trust in the quality of reorderings. Although it is interesting to look at the model weights, any final conclusion on the impact of the metrics on training must depend on human evaluation of translation quality. Type Sentence Reference silicon valley is still a rich area in the united states. the average salary in the area was us $62,400 a year,"
P11-1103,N03-1017,0,0.0582288,"Missing"
P11-1103,2005.mtsummit-papers.11,0,0.00692566,"n the source and the reference sentences, and the source and the translated sentences. In an ideal scenario, the translation system outputs the alignments and the reference set can be selected to have gold standard human alignments. However, the data that we use to evaluate metrics does not have any gold standard alignments and we must train automatic alignment models to generate them. We used version two of the Berkeley alignment model (Liang et al., 2006), with the posterior threshold set at 0.5. Our Spanish-, French- and German-English alignment models are trained using Europarl version 5 (Koehn, 2005). The Czech-English alignment model is trained on sections 0-2 of the Czech-English Parallel Corpus, version 0.9 (Bojar and Zabokrtsky, 2009). The metric scores are calculated for the test set from the 2009 workshop on machine translation. It consists of 2525 sentences in English, French, German, Spanish and Czech. These sentences have been translated by different machine translation systems and the output submitted to the workshop. The system output along with human evaluations can be downloaded from the web1 . The B LEU score has five parameters, one for each n-gram, and one for the brevity"
P11-1103,W08-0312,0,0.0169787,"metric scores are calculated for the test set from the 2009 workshop on machine translation. It consists of 2525 sentences in English, French, German, Spanish and Czech. These sentences have been translated by different machine translation systems and the output submitted to the workshop. The system output along with human evaluations can be downloaded from the web1 . The B LEU score has five parameters, one for each n-gram, and one for the brevity penalty. These parameters are set to a default uniform value of one. METEOR has 3 parameters which have been trained for human judgements of rank (Lavie and Agarwal, 2008). METEOR version 0.7 was used. The other baseline metric used was TER version 0.7.25. We adapt TER by subtracting it from one, so that all 1 http://www.statmt.org/wmt09/results.html metric increases mean an improvement in the translation. The TER metric has five parameters which have not been trained. Using rank judgements, we do not have absolute scores and so we cannot compare translations across different sentences and extract correlation statistics. We therefore use the method adopted in the 2009 workshop on machine translation (Callison-Burch et al., 2009). We ascertained how consistent t"
P11-1103,N06-1014,0,0.0164325,"and out of English. In total there were 52,265 pairwise rank judgements collected. Our reordering metric relies upon word alignments that are generated between the source and the reference sentences, and the source and the translated sentences. In an ideal scenario, the translation system outputs the alignments and the reference set can be selected to have gold standard human alignments. However, the data that we use to evaluate metrics does not have any gold standard alignments and we must train automatic alignment models to generate them. We used version two of the Berkeley alignment model (Liang et al., 2006), with the posterior threshold set at 0.5. Our Spanish-, French- and German-English alignment models are trained using Europarl version 5 (Koehn, 2005). The Czech-English alignment model is trained on sections 0-2 of the Czech-English Parallel Corpus, version 0.9 (Bojar and Zabokrtsky, 2009). The metric scores are calculated for the test set from the 2009 workshop on machine translation. It consists of 2525 sentences in English, French, German, Spanish and Czech. These sentences have been translated by different machine translation systems and the output submitted to the workshop. The system o"
P11-1103,C04-1072,0,0.0283945,"measure of the precision of the word choice. The 4-gram B LEU score includes some measure of the local reordering success in the precision of the longer n-grams. B LEU is an important baseline, and improving on it by including more reordering information is an interesting result. The lexical component of the system can be any meaningful metric for a particular target language. If a researcher was interested in morphologically rich languages, for example, METEOR could be used. We use the LRscore to return sentence level scores as well system level scores, and when doing so the smoothed B LEU (Lin and Och, 2004) is used. 3 Consistency with Human Judgements Automatic metrics must be validated by comparing their scores with human judgements. We train the metric parameter to optimise consistency with human preference judgements across different language pairs and then we show that the LRscore is 1030 more consistent with humans than other commonly used metrics. 3.1 Experimental Design Human judgement of rank has been chosen as the official determinant of translation quality for the 2009 Workshop on Machine Translation (Callison-Burch et al., 2009). We used human ranking data from this workshop to evalua"
P11-1103,P03-1021,0,0.098362,"glish and English-Czech language pairs, which have the least amount of reordering. METEOR lags behind for the language pairs with the most reordering, the German-English and English-German pairs. Here LR-KB4 is the best metric, which shows that metrics which are sensitive to the distance words are out of order are more appropriate for situations with a reasonable amount of reordering. 4 Optimising Translation Models Automatic metrics are useful for evaluation, but they are essential for training model parameters. In this section we apply the LRscore as the objective function in MERT training (Och, 2003). MERT minimises translation errors according to some automatic evaluation metric while searching for the best parameter settings over the N-best output. A MERT trained model is likely to exhibit the properties that Metric METEOR TER B LEU1 B LEU LR-HB1 LR-HB4 LR-KB1 LR-KB4 de-en 58.6 53.2 56.1 58.7 59.7 60.4 60.4 61.0 es-en 58.3 50.1 57.0 55.5 60.0 57.3 59.7 57.2 fr-en 58.3 52.6 56.7 57.7 58.6 58.7 58.0 58.5 cz-en 59.4 47.5 52.5 57.2 53.2 57.2 54.0 58.6 en-de 52.6 48.6 52.1 54.1 54.6 54.8 54.1 54.8 en-es 55.7 49.6 54.2 56.7 55.6 57.3 54.7 56.8 en-fr 61.2 58.3 62.3 63.7 63.7 63.3 63.4 63.1 en-"
P11-1103,P02-1040,0,0.108428,"aining with the LRscore leads to output which is preferred by humans. Moreover, the translations incur no penalty in terms of B LEU scores. 1 Introduction Research in machine translation has focused broadly on two main goals, improving word choice and improving word order in translation output. Current machine translation metrics rely upon indirect methods for measuring the quality of the word order, and their ability to capture the quality of word order is poor (Birch et al., 2010). There are currently two main approaches to evaluating reordering. The first is exemplified by the B LEU score (Papineni et al., 2002), which counts the number of matching n-grams between the reference and the hypothesis. Word order is captured by the proportion of longer n-grams which match. This method does not consider the position of matching words, and only captures ordering differences if there is an exact match between the words in the translation and the reference. Another approach is taken by two other commonly used metrics, METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006). They both search for an alignment between the translation and the reference, and from this they calculate a penalty based on the"
P11-1103,2006.amta-papers.25,0,0.108377,"ch et al., 2010). There are currently two main approaches to evaluating reordering. The first is exemplified by the B LEU score (Papineni et al., 2002), which counts the number of matching n-grams between the reference and the hypothesis. Word order is captured by the proportion of longer n-grams which match. This method does not consider the position of matching words, and only captures ordering differences if there is an exact match between the words in the translation and the reference. Another approach is taken by two other commonly used metrics, METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006). They both search for an alignment between the translation and the reference, and from this they calculate a penalty based on the number of differences in order between the two sentences. When block moves are allowed the search space is very large, and matching stems and synonyms introduces errors. Importantly, none of these metrics capture the distance by which words are out of order. Also, they conflate reordering performance with the quality of the lexical items in the translation, making it difficult to tease apart the impact of changes. More sophisticated metrics, such as the RTE metric"
P11-1103,D08-1027,0,0.0272483,"Missing"
P11-1103,W09-0401,0,\N,Missing
P16-1009,W09-0432,0,0.169739,"obtain another improvement of 0.8–1.0 B LEU. Back-translation Quality for Synthetic Data One question that our previous experiments leave open is how the quality of the automatic backtranslation affects training with synthetic data. To investigate this question, we back-translate the same German monolingual corpus with three different German→English systems: • with our baseline system and greedy decoding 4.3 Contrast to Phrase-based SMT The back-translation of monolingual target data into the source language to produce synthetic parallel text has been previously explored for phrasebased SMT (Bertoldi and Federico, 2009; Lambert et al., 2011). While our approach is technically similar, synthetic parallel data fulfills novel roles • with our baseline system and beam search (beam size 12). This is the same system used for the experiments in Table 3. 10 We also experimented with higher ratios of monolingual data, but this led to decreased B LEU scores. 91 name training data baseline (Gülçehre et al., 2015) deep fusion (Gülçehre et al., 2015) baseline parallel parallelsynth parallel/parallelsynth Gigawordmono parallel/Gigawordmono Gigawordsynth parallel/Gigawordsynth instances tst2011 18.4 20.2 18.6 19.9 18.8 21"
P16-1009,P15-1001,0,0.225572,"quences of subword units (Sennrich et al., 2016), and can represent any additional training data with the existing network vocabulary that was learned on the parallel data. In all experiments, the network vocabulary remains fixed. 4.1.1 We evaluate NMT training on parallel text, and with additional monolingual data, on English↔German and Turkish→English, using training and test data from WMT 15 for English↔German, IWSLT 15 for English→German, and IWSLT 14 for Turkish→English. Data and Methods We use Groundhog3 as the implementation of the NMT system for all experiments (Bahdanau et al., 2015; Jean et al., 2015a). We generally follow the settings and training procedure described by Sennrich et al. (2016). For English↔German, we report case-sensitive B LEU on detokenized text with mteval-v13a.pl for comparison to official WMT and IWSLT results. For Turkish→English, we report case-sensitive B LEU on tokenized text with multi-bleu.perl for comparison to results by Gülçehre et al. (2015). Gülçehre et al. (2015) determine the network vocabulary based on the parallel training data, 3 English↔German We use all parallel training data provided by WMT 2015 (Bojar et al., 2015)4 . We use the News Crawl corpora"
P16-1009,J90-2002,0,0.474829,"Missing"
P16-1009,2012.eamt-1.60,0,0.0565125,"n et al., 2012). We also use early stopping, based on B LEU measured every three hours on tst2010, which we treat as development set. For Turkish→English, we use gradient clipping with threshold 5, following Gülçehre et al. (2015), in contrast to the threshold 1 that we use for English↔German, following Jean et al. (2015a). Table 2: Turkish→English training data. cabulary size is 90 000. We also perform experiments on the IWSLT 15 test sets to investigate a cross-domain setting.5 The test sets consist of TED talk transcripts. As indomain training data, IWSLT provides the WIT3 parallel corpus (Cettolo et al., 2012), which also consists of TED talks. 4.1.2 4.2 4.2.1 Results English→German WMT 15 Table 3 shows English→German results with WMT training and test data. We find that mixing parallel training data with monolingual data with a dummy source side in a ratio of 1-1 improves quality by 0.4–0.5 B LEU for the single system, 1 B LEU for the ensemble. We train the system for twice as long as the baseline to provide the training algorithm with a similar amount of parallel training instances. To ensure that the quality improvement is due to the monolingual training instances, and not just increased trainin"
P16-1009,P07-2045,1,0.0662497,"Table 8: Phrase-based SMT results (English→German) on WMT test sets (average of newstest201{4,5}), and IWSLT test sets (average of tst201{3,4,5}), and average B LEU gain from adding synthetic data for both PBSMT and NMT. 6 4 2 0 5 10 15 20 training time (training instances 25 30 ·106 ) Figure 1: Turkish→English training and development set (tst2010) cross-entropy as a function of training time (number of training instances) for different systems. in NMT. To explore the relative effectiveness of backtranslated data for phrase-based SMT and NMT, we train two phrase-based SMT systems with Moses (Koehn et al., 2007), using only WMTparallel , or both WMTparallel and WMTsynth_de for training the translation and reordering model. Both systems contain the same language model, a 5-gram Kneser-Ney model trained on all available WMT data. We use the baseline features described by Haddow et al. (2015). Results are shown in Table 8. In phrase-based SMT, we find that the use of back-translated training data has a moderate positive effect on the WMT test sets (+0.7 B LEU), but not on the IWSLT test sets. This is in line with the expectation that the main effect of back-translated data for phrasebased SMT is domain"
P16-1009,W11-2132,0,0.0362203,"f 0.8–1.0 B LEU. Back-translation Quality for Synthetic Data One question that our previous experiments leave open is how the quality of the automatic backtranslation affects training with synthetic data. To investigate this question, we back-translate the same German monolingual corpus with three different German→English systems: • with our baseline system and greedy decoding 4.3 Contrast to Phrase-based SMT The back-translation of monolingual target data into the source language to produce synthetic parallel text has been previously explored for phrasebased SMT (Bertoldi and Federico, 2009; Lambert et al., 2011). While our approach is technically similar, synthetic parallel data fulfills novel roles • with our baseline system and beam search (beam size 12). This is the same system used for the experiments in Table 3. 10 We also experimented with higher ratios of monolingual data, but this led to decreased B LEU scores. 91 name training data baseline (Gülçehre et al., 2015) deep fusion (Gülçehre et al., 2015) baseline parallel parallelsynth parallel/parallelsynth Gigawordmono parallel/Gigawordmono Gigawordsynth parallel/Gigawordsynth instances tst2011 18.4 20.2 18.6 19.9 18.8 21.2 7.2m 6m/6m 7.6m/7.6m"
P16-1009,D14-1179,0,0.0846514,"Missing"
P16-1009,2015.iwslt-evaluation.11,0,0.370228,"MorphTR 6 89 name training instances syntax-based (Sennrich and Haddow, 2015) Neural MT (Jean et al., 2015b) parallel 37m (parallel) +monolingual 49m (parallel) / 49m (monolingual) +synthetic 44m (parallel) / 36m (synthetic) B LEU newstest2014 newstest2015 single ens-4 single ens-4 22.6 24.4 22.4 19.9 20.4 22.8 23.6 20.4 21.4 23.2 24.6 22.7 23.8 25.7 26.5 Table 3: English→German translation performance (B LEU) on WMT training/test sets. Ens-4: ensemble of 4 models. Number of training instances varies due to differences in training time and speed. name 1 2 3 4 5 fine-tuning data instances NMT (Luong and Manning, 2015) (single model) NMT (Luong and Manning, 2015) (ensemble of 8) parallel +synthetic 2+WITmono_de WMTparallel / WITmono 200k/200k 2+WITsynth_de WITsynth 200k 2+WITparallel WIT 200k tst2013 29.4 31.4 25.2 26.5 26.6 28.2 30.4 B LEU tst2014 27.6 22.6 23.5 23.6 24.4 25.9 tst2015 30.1 24.0 25.5 25.4 26.7 28.4 Table 4: English→German translation performance (B LEU) on IWSLT test sets (TED talks). Single models. test sets, which are news texts. We investigate if monolingual training data is especially valuable if it can be used to adapt a model to a new genre or domain, specifically adapting a system tr"
P16-1009,D15-1166,0,0.659364,"lgorithm with a similar amount of parallel training instances. To ensure that the quality improvement is due to the monolingual training instances, and not just increased training time, we also continued training our baseline system for another week, but saw no improvements in B LEU. Including synthetic data during training is very effective, and yields an improvement over our baseline by 2.8–3.4 B LEU. Our best ensemble system also outperforms a syntax-based baseline (Sennrich and Haddow, 2015) by 1.2–2.1 B LEU. We also substantially outperform NMT results reported by Jean et al. (2015a) and Luong et al. (2015), who previously reported SOTA result.8 We note that the difference is particularly large for single systems, since our ensemble is not as diverse as that of Luong et al. (2015), who used 8 independently trained ensemble components, whereas we sampled 4 ensemble components from the same training run. Turkish→English We use data provided for the IWSLT 14 machine translation track (Cettolo et al., 2014), namely the WIT3 parallel corpus (Cettolo et al., 2012), which consists of TED talks, and the SETimes corpus (Tyers and Alperen, 2010).6 After removal of sentence pairs which contain empty lines"
P16-1009,W15-3013,1,0.372853,"06 ) Figure 1: Turkish→English training and development set (tst2010) cross-entropy as a function of training time (number of training instances) for different systems. in NMT. To explore the relative effectiveness of backtranslated data for phrase-based SMT and NMT, we train two phrase-based SMT systems with Moses (Koehn et al., 2007), using only WMTparallel , or both WMTparallel and WMTsynth_de for training the translation and reordering model. Both systems contain the same language model, a 5-gram Kneser-Ney model trained on all available WMT data. We use the baseline features described by Haddow et al. (2015). Results are shown in Table 8. In phrase-based SMT, we find that the use of back-translated training data has a moderate positive effect on the WMT test sets (+0.7 B LEU), but not on the IWSLT test sets. This is in line with the expectation that the main effect of back-translated data for phrasebased SMT is domain adaptation (Bertoldi and Federico, 2009). Both the WMT test sets and the News Crawl corpora which we used as monolingual data come from the same source, a web crawl of newspaper articles.11 In contrast, News Crawl is out-of-domain for the IWSLT test sets. In contrast to phrase-based"
P16-1009,N06-1020,0,0.0227411,"parameters are tuned on further parallel training data, but the language model parameters are fixed during the finetuning stage. Jean et al. (2015b) also report on experiments with reranking of NMT output with a 5-gram language model, but improvements are small (between 0.1–0.5 B LEU). The production of synthetic parallel texts bears resemblance to data augmentation techniques used in computer vision, where datasets are often augmented with rotated, scaled, or otherwise distorted variants of the (limited) training set (Rowley et al., 1996). Another similar avenue of research is selftraining (McClosky et al., 2006; Schwenk, 2008). The main difference is that self-training typically refers to scenario where the training set is enhanced with training instances with artificially produced output labels, whereas we start with human-produced output (i.e. the translation), and artificially produce an input. We expect that this is more robust towards noise in the automatic translation. Improving NMT with monolingual source data, following similar work on phrasebased SMT (Schwenk, 2008), remains possible future work. Domain adaptation of neural networks via continued training has been shown to be effective for"
P16-1009,2008.iwslt-papers.6,0,0.00923665,"n further parallel training data, but the language model parameters are fixed during the finetuning stage. Jean et al. (2015b) also report on experiments with reranking of NMT output with a 5-gram language model, but improvements are small (between 0.1–0.5 B LEU). The production of synthetic parallel texts bears resemblance to data augmentation techniques used in computer vision, where datasets are often augmented with rotated, scaled, or otherwise distorted variants of the (limited) training set (Rowley et al., 1996). Another similar avenue of research is selftraining (McClosky et al., 2006; Schwenk, 2008). The main difference is that self-training typically refers to scenario where the training set is enhanced with training instances with artificially produced output labels, whereas we start with human-produced output (i.e. the translation), and artificially produce an input. We expect that this is more robust towards noise in the automatic translation. Improving NMT with monolingual source data, following similar work on phrasebased SMT (Schwenk, 2008), remains possible future work. Domain adaptation of neural networks via continued training has been shown to be effective for neural language"
P16-1009,D15-1248,1,0.477278,"r the single system, 1 B LEU for the ensemble. We train the system for twice as long as the baseline to provide the training algorithm with a similar amount of parallel training instances. To ensure that the quality improvement is due to the monolingual training instances, and not just increased training time, we also continued training our baseline system for another week, but saw no improvements in B LEU. Including synthetic data during training is very effective, and yields an improvement over our baseline by 2.8–3.4 B LEU. Our best ensemble system also outperforms a syntax-based baseline (Sennrich and Haddow, 2015) by 1.2–2.1 B LEU. We also substantially outperform NMT results reported by Jean et al. (2015a) and Luong et al. (2015), who previously reported SOTA result.8 We note that the difference is particularly large for single systems, since our ensemble is not as diverse as that of Luong et al. (2015), who used 8 independently trained ensemble components, whereas we sampled 4 ensemble components from the same training run. Turkish→English We use data provided for the IWSLT 14 machine translation track (Cettolo et al., 2014), namely the WIT3 parallel corpus (Cettolo et al., 2012), which consists of T"
P16-1009,P16-1162,1,0.590859,"monolingual data set into English. The German→English system used for this is the baseline system (parallel). Translation took about a week on an NVIDIA Titan Black GPU. For experiments in German→English, we back-translate 4 200 000 monolingual English sentences into German, using the English→German system +synthetic. Note that we always use single models for backtranslation, not ensembles. We leave it to future work to explore how sensitive NMT training with synthetic data is to the quality of the backtranslation. We tokenize and truecase the training data, and represent rare words via BPE (Sennrich et al., 2016). Specifically, we follow Sennrich et al. (2016) in performing BPE on the joint vocabulary with 89 500 merge operations. The network voEvaluation 4.1 sentences 4 200 000 200 000 160 000 000 3 600 000 118 000 000 4 200 000 4 github.com/sebastien-j/LV_groundhog 88 http://www.statmt.org/wmt15/ dataset WIT SETimes Gigawordmono Gigawordsynth sentences 160 000 160 000 177 000 000 3 200 000 We found overfitting to be a bigger problem than with the larger English↔German data set, and follow Gülçehre et al. (2015) in using Gaussian noise (stddev 0.01) (Graves, 2011), and dropout on the output layer (p="
P16-1009,W15-4006,0,0.0304229,"Missing"
P16-1162,D15-1249,0,0.114088,": 2, &apos;n e w e s t &lt;/w&gt;&apos;:6, &apos;w i d e s t &lt;/w&gt;&apos;:3} num_merges = 10 for i in range(num_merges): pairs = get_stats(vocab) best = max(pairs, key=pairs.get) vocab = merge_vocab(best, vocab) print(best) r· lo lo w e r· → → → → r· lo low er· Figure 1: BPE merge operations learned from dictionary {‘low’, ‘lowest’, ‘newer’, ‘wider’}. gorithm 1. In practice, we increase efficiency by indexing all pairs, and updating data structures incrementally. The main difference to other compression algorithms, such as Huffman encoding, which have been proposed to produce a variable-length encoding of words for NMT (Chitnis and DeNero, 2015), is that our symbol sequences are still interpretable as subword units, and that the network can generalize to translate and produce new words (unseen at training time) on the basis of these subword units. Figure 1 shows a toy example of learned BPE operations. At test time, we first split words into sequences of characters, then apply the learned operations to merge the characters into larger, known symbols. This is applicable to any word, and allows for open-vocabulary networks with fixed symbol vocabularies.3 In our example, the OOV ‘lower’ would be segmented into ‘low er·’. 3 The only sym"
P16-1162,D14-1179,0,0.315754,"Missing"
P16-1162,W02-0603,0,0.0880881,"t of the k most frequent word types unsegmented. Only the unigram representation is truly open-vocabulary. However, the unigram representation performed poorly in preliminary experiments, and we report translation results with a bigram representation, which is empirically better, but unable to produce some tokens in the test set with the training set vocabulary. We report statistics for several word segmentation techniques that have proven useful in previous SMT research, including frequency-based compound splitting (Koehn and Knight, 2003), rulebased hyphenation (Liang, 1983), and Morfessor (Creutz and Lagus, 2002). We find that they only moderately reduce vocabulary size, and do not solve the unknown word problem, and we thus find them unsuitable for our goal of open-vocabulary translation without back-off dictionary. BPE meets our goal of being open-vocabulary, and the learned merge operations can be applied to the test set to obtain a segmentation with no unknown symbols.10 Its main difference from the character-level model is that the more compact representation of BPE allows for shorter sequences, and that the attention model operates on variable-length units.11 Table 1 shows BPE with 59 500 merge"
P16-1162,E14-4029,0,0.0798953,"ranslated independently, our NMT models show robustness towards oversplitting. 2 Subword Translation The main motivation behind this paper is that the translation of some words is transparent in 1716 pothesis in Sections 4 and 5. First, we discuss different subword representations. 3.1 Related Work For Statistical Machine Translation (SMT), the translation of unknown words has been the subject of intensive research. A large proportion of unknown words are names, which can just be copied into the target text if both languages share an alphabet. If alphabets differ, transliteration is required (Durrani et al., 2014). Character-based translation has also been investigated with phrase-based models, which proved especially successful for closely related languages (Vilar et al., 2007; Tiedemann, 2009; Neubig et al., 2012). The segmentation of morphologically complex words such as compounds is widely used for SMT, and various algorithms for morpheme segmentation have been investigated (Nießen and Ney, 2000; Koehn and Knight, 2003; Virpioja et al., 2007; Stallard et al., 2012). Segmentation algorithms commonly used for phrase-based SMT tend to be conservative in their splitting decisions, whereas we aim for an"
P16-1162,N13-1073,0,0.0814933,"s), and continue training each with a fixed embedding layer (as suggested by (Jean et al., 2015)) for 12 hours. We perform two independent training runs for each models, once with cut-off for gradient clipping (Pascanu et al., 2013) of 5.0, once with a cut-off of 1.0 – the latter produced better single models for most settings. We report results of the system that performed best on our development set (newstest2013), and of an ensemble of all 8 models. We use a beam size of 12 for beam search, with probabilities normalized by sentence length. We use a bilingual dictionary based on fast-align (Dyer et al., 2013). For our baseline, this serves as back-off dictionary for rare words. We also use the dictionary to speed up translation for all experiments, only performing the softmax over a filtered list of candidate translations (like Jean et al. (2015), we use K = 30000; K 0 = 10). 4.1 Subword statistics Apart from translation quality, which we will verify empirically, our main objective is to represent an open vocabulary through a compact fixed-size subword vocabulary, and allow for efficient training and decoding.8 Statistics for different segmentations of the Ger6 Clipped unigram precision is essenti"
P16-1162,W15-3013,1,0.108755,"ased system in terms of B LEU, but not in terms of CHR F3. Regarding other neural systems, Luong et al. (2015a) report a B LEU score of 25.9 on newstest2015, but we note that they use an ensemble of 8 independently trained models, and also report strong improvements from applying dropout, which we did not use. We are confident that our improvements to the translation of rare words are orthogonal to improvements achievable through other improvements in the network archi1720 tecture, training algorithm, or better ensembles. For English→Russian, the state of the art is the phrase-based system by Haddow et al. (2015). It outperforms our WDict baseline by 1.5 B LEU. The subword models are a step towards closing this gap, and BPE-J90k yields an improvement of 1.3 B LEU, and 2.0 CHR F3, over WDict. As a further comment on our translation results, we want to emphasize that performance variability is still an open problem with NMT. On our development set, we observe differences of up to 1 B LEU between different models. For single systems, we report the results of the model that performs best on dev (out of 8), which has a stabilizing effect, but how to control for randomness deserves further attention in futu"
P16-1162,P15-1001,0,0.284282,"z o.o. Samsung R&D Institute Poland. lem, and especially for languages with productive word formation processes such as agglutination and compounding, translation models require mechanisms that go below the word level. As an example, consider compounds such as the German Abwasser|behandlungs|anlange ‘sewage water treatment plant’, for which a segmented, variable-length representation is intuitively more appealing than encoding the word as a fixed-length vector. For word-level NMT models, the translation of out-of-vocabulary words has been addressed through a back-off to a dictionary look-up (Jean et al., 2015; Luong et al., 2015b). We note that such techniques make assumptions that often do not hold true in practice. For instance, there is not always a 1-to-1 correspondence between source and target words because of variance in the degree of morphological synthesis between languages, like in our introductory compounding example. Also, word-level models are unable to translate or generate unseen words. Copying unknown words into the target text, as done by (Jean et al., 2015; Luong et al., 2015b), is a reasonable strategy for names, but morphological changes and transliteration is often required, e"
P16-1162,D13-1176,0,0.0821851,"ansliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 B LEU, respectively. 1 Introduction Neural machine translation has recently shown impressive results (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). However, the translation of rare words is an open problem. The vocabulary of neural models is typically limited to 30 000–50 000 words, but translation is an open-vocabulary probThe research presented in this publication was conducted in cooperation with Samsung Electronics Polska sp. z o.o. Samsung R&D Institute Poland. lem, and especially for languages with productive word formation processes such as agglutination and compounding, translation models require mechanisms that go below the word level. As an example, consider compounds such as the"
P16-1162,E03-1076,0,0.106965,"ch. A large proportion of unknown words are names, which can just be copied into the target text if both languages share an alphabet. If alphabets differ, transliteration is required (Durrani et al., 2014). Character-based translation has also been investigated with phrase-based models, which proved especially successful for closely related languages (Vilar et al., 2007; Tiedemann, 2009; Neubig et al., 2012). The segmentation of morphologically complex words such as compounds is widely used for SMT, and various algorithms for morpheme segmentation have been investigated (Nießen and Ney, 2000; Koehn and Knight, 2003; Virpioja et al., 2007; Stallard et al., 2012). Segmentation algorithms commonly used for phrase-based SMT tend to be conservative in their splitting decisions, whereas we aim for an aggressive segmentation that allows for open-vocabulary translation with a compact network vocabulary, and without having to resort to back-off dictionaries. The best choice of subword units may be taskspecific. For speech recognition, phone-level language models have been used (Bazzi and Glass, 2000). Mikolov et al. (2012) investigate subword language models, and propose to use syllables. For multilingual segmen"
P16-1162,P07-2045,1,0.138461,"ranslation of rare and unseen words in neural machine translation by representing them via subword units? • Which segmentation into subword units performs best in terms of vocabulary size, text size, and translation quality? We perform experiments on data from the shared translation task of WMT 2015. For English→German, our training set consists of 4.2 million sentence pairs, or approximately 100 million tokens. For English→Russian, the training set consists of 2.6 million sentence pairs, or approximately 50 million tokens. We tokenize and truecase the data with the scripts provided in Moses (Koehn et al., 2007). We use newstest2013 as development set, and report results on newstest2014 and newstest2015. We report results with B LEU (mteval-v13a.pl), and CHR F3 (Popovi´c, 2015), a character n-gram F3 score which was found to correlate well with 4 In practice, we simply concatenate the source and target side of the training set to learn joint BPE. 5 Since the Russian training text also contains words that use the Latin alphabet, we also apply the Latin BPE operations. 1718 human judgments, especially for translations out of English (Stanojevi´c et al., 2015). Since our main claim is concerned with the"
P16-1162,D15-1176,0,0.0929974,"resort to back-off dictionaries. The best choice of subword units may be taskspecific. For speech recognition, phone-level language models have been used (Bazzi and Glass, 2000). Mikolov et al. (2012) investigate subword language models, and propose to use syllables. For multilingual segmentation tasks, multilingual algorithms have been proposed (Snyder and Barzilay, 2008). We find these intriguing, but inapplicable at test time. Various techniques have been proposed to produce fixed-length continuous word vectors based on characters or morphemes (Luong et al., 2013; Botha and Blunsom, 2014; Ling et al., 2015a; Kim et al., 2015). An effort to apply such techniques to NMT, parallel to ours, has found no significant improvement over word-based approaches (Ling et al., 2015b). One technical difference from our work is that the attention mechanism still operates on the level of words in the model by Ling et al. (2015b), and that the representation of each word is fixed-length. We expect that the attention mechanism benefits from our variable-length representation: the network can learn to place attention on different subword units at each step. Recall our introductory example Abwasserbehandlungsanlang"
P16-1162,W13-3512,0,0.0897464,"Missing"
P16-1162,D15-1166,0,0.660487,"Institute Poland. lem, and especially for languages with productive word formation processes such as agglutination and compounding, translation models require mechanisms that go below the word level. As an example, consider compounds such as the German Abwasser|behandlungs|anlange ‘sewage water treatment plant’, for which a segmented, variable-length representation is intuitively more appealing than encoding the word as a fixed-length vector. For word-level NMT models, the translation of out-of-vocabulary words has been addressed through a back-off to a dictionary look-up (Jean et al., 2015; Luong et al., 2015b). We note that such techniques make assumptions that often do not hold true in practice. For instance, there is not always a 1-to-1 correspondence between source and target words because of variance in the degree of morphological synthesis between languages, like in our introductory compounding example. Also, word-level models are unable to translate or generate unseen words. Copying unknown words into the target text, as done by (Jean et al., 2015; Luong et al., 2015b), is a reasonable strategy for names, but morphological changes and transliteration is often required, especially if alphabe"
P16-1162,P15-1002,0,0.130638,"Institute Poland. lem, and especially for languages with productive word formation processes such as agglutination and compounding, translation models require mechanisms that go below the word level. As an example, consider compounds such as the German Abwasser|behandlungs|anlange ‘sewage water treatment plant’, for which a segmented, variable-length representation is intuitively more appealing than encoding the word as a fixed-length vector. For word-level NMT models, the translation of out-of-vocabulary words has been addressed through a back-off to a dictionary look-up (Jean et al., 2015; Luong et al., 2015b). We note that such techniques make assumptions that often do not hold true in practice. For instance, there is not always a 1-to-1 correspondence between source and target words because of variance in the degree of morphological synthesis between languages, like in our introductory compounding example. Also, word-level models are unable to translate or generate unseen words. Copying unknown words into the target text, as done by (Jean et al., 2015; Luong et al., 2015b), is a reasonable strategy for names, but morphological changes and transliteration is often required, especially if alphabe"
P16-1162,P12-1018,0,0.0565115,"s in Sections 4 and 5. First, we discuss different subword representations. 3.1 Related Work For Statistical Machine Translation (SMT), the translation of unknown words has been the subject of intensive research. A large proportion of unknown words are names, which can just be copied into the target text if both languages share an alphabet. If alphabets differ, transliteration is required (Durrani et al., 2014). Character-based translation has also been investigated with phrase-based models, which proved especially successful for closely related languages (Vilar et al., 2007; Tiedemann, 2009; Neubig et al., 2012). The segmentation of morphologically complex words such as compounds is widely used for SMT, and various algorithms for morpheme segmentation have been investigated (Nießen and Ney, 2000; Koehn and Knight, 2003; Virpioja et al., 2007; Stallard et al., 2012). Segmentation algorithms commonly used for phrase-based SMT tend to be conservative in their splitting decisions, whereas we aim for an aggressive segmentation that allows for open-vocabulary translation with a compact network vocabulary, and without having to resort to back-off dictionaries. The best choice of subword units may be taskspe"
P16-1162,C00-2162,0,0.0990133,"ct of intensive research. A large proportion of unknown words are names, which can just be copied into the target text if both languages share an alphabet. If alphabets differ, transliteration is required (Durrani et al., 2014). Character-based translation has also been investigated with phrase-based models, which proved especially successful for closely related languages (Vilar et al., 2007; Tiedemann, 2009; Neubig et al., 2012). The segmentation of morphologically complex words such as compounds is widely used for SMT, and various algorithms for morpheme segmentation have been investigated (Nießen and Ney, 2000; Koehn and Knight, 2003; Virpioja et al., 2007; Stallard et al., 2012). Segmentation algorithms commonly used for phrase-based SMT tend to be conservative in their splitting decisions, whereas we aim for an aggressive segmentation that allows for open-vocabulary translation with a compact network vocabulary, and without having to resort to back-off dictionaries. The best choice of subword units may be taskspecific. For speech recognition, phone-level language models have been used (Bazzi and Glass, 2000). Mikolov et al. (2012) investigate subword language models, and propose to use syllables."
P16-1162,W15-3049,0,0.534349,"Missing"
P16-1162,E12-1015,0,0.0627825,"ignment model αij , which models the probability that yi is aligned to xj . The alignment model is a singlelayer feedforward neural network that is learned jointly with the rest of the network through backpropagation. A detailed description can be found in (Bahdanau et al., 2015). Training is performed on a parallel corpus with stochastic gradient descent. For translation, a beam search with small beam size is employed. 3 • cognates and loanwords. Cognates and loanwords with a common origin can differ in regular ways between languages, so that character-level translation rules are sufficient (Tiedemann, 2012). Example: claustrophobia (English) Klaustrophobie (German) Êëàóñòðîôîáèÿ (Klaustrofobiâ) (Russian) Neural Machine Translation • morphologically complex words. Words containing multiple morphemes, for instance formed via compounding, affixation, or inflection, may be translatable by translating the morphemes separately. Example: solar system (English) Sonnensystem (Sonne + System) (German) Naprendszer (Nap + Rendszer) (Hungarian) In an analysis of 100 rare tokens (not among the 50 000 most frequent types) in our German training data1 , the majority of tokens are potentially translatable from E"
P16-1162,W07-0705,0,0.0774644,"words is transparent in 1716 pothesis in Sections 4 and 5. First, we discuss different subword representations. 3.1 Related Work For Statistical Machine Translation (SMT), the translation of unknown words has been the subject of intensive research. A large proportion of unknown words are names, which can just be copied into the target text if both languages share an alphabet. If alphabets differ, transliteration is required (Durrani et al., 2014). Character-based translation has also been investigated with phrase-based models, which proved especially successful for closely related languages (Vilar et al., 2007; Tiedemann, 2009; Neubig et al., 2012). The segmentation of morphologically complex words such as compounds is widely used for SMT, and various algorithms for morpheme segmentation have been investigated (Nießen and Ney, 2000; Koehn and Knight, 2003; Virpioja et al., 2007; Stallard et al., 2012). Segmentation algorithms commonly used for phrase-based SMT tend to be conservative in their splitting decisions, whereas we aim for an aggressive segmentation that allows for open-vocabulary translation with a compact network vocabulary, and without having to resort to back-off dictionaries. The best"
P16-1162,2007.mtsummit-papers.65,0,0.105199,"f unknown words are names, which can just be copied into the target text if both languages share an alphabet. If alphabets differ, transliteration is required (Durrani et al., 2014). Character-based translation has also been investigated with phrase-based models, which proved especially successful for closely related languages (Vilar et al., 2007; Tiedemann, 2009; Neubig et al., 2012). The segmentation of morphologically complex words such as compounds is widely used for SMT, and various algorithms for morpheme segmentation have been investigated (Nießen and Ney, 2000; Koehn and Knight, 2003; Virpioja et al., 2007; Stallard et al., 2012). Segmentation algorithms commonly used for phrase-based SMT tend to be conservative in their splitting decisions, whereas we aim for an aggressive segmentation that allows for open-vocabulary translation with a compact network vocabulary, and without having to resort to back-off dictionaries. The best choice of subword units may be taskspecific. For speech recognition, phone-level language models have been used (Bazzi and Glass, 2000). Mikolov et al. (2012) investigate subword language models, and propose to use syllables. For multilingual segmentation tasks, multiling"
P16-1162,D15-1248,1,0.126623,"Missing"
P16-1162,P08-1084,0,0.0964265,"ion algorithms commonly used for phrase-based SMT tend to be conservative in their splitting decisions, whereas we aim for an aggressive segmentation that allows for open-vocabulary translation with a compact network vocabulary, and without having to resort to back-off dictionaries. The best choice of subword units may be taskspecific. For speech recognition, phone-level language models have been used (Bazzi and Glass, 2000). Mikolov et al. (2012) investigate subword language models, and propose to use syllables. For multilingual segmentation tasks, multilingual algorithms have been proposed (Snyder and Barzilay, 2008). We find these intriguing, but inapplicable at test time. Various techniques have been proposed to produce fixed-length continuous word vectors based on characters or morphemes (Luong et al., 2013; Botha and Blunsom, 2014; Ling et al., 2015a; Kim et al., 2015). An effort to apply such techniques to NMT, parallel to ours, has found no significant improvement over word-based approaches (Ling et al., 2015b). One technical difference from our work is that the attention mechanism still operates on the level of words in the model by Ling et al. (2015b), and that the representation of each word is f"
P16-1162,P12-2063,0,0.075538,"es, which can just be copied into the target text if both languages share an alphabet. If alphabets differ, transliteration is required (Durrani et al., 2014). Character-based translation has also been investigated with phrase-based models, which proved especially successful for closely related languages (Vilar et al., 2007; Tiedemann, 2009; Neubig et al., 2012). The segmentation of morphologically complex words such as compounds is widely used for SMT, and various algorithms for morpheme segmentation have been investigated (Nießen and Ney, 2000; Koehn and Knight, 2003; Virpioja et al., 2007; Stallard et al., 2012). Segmentation algorithms commonly used for phrase-based SMT tend to be conservative in their splitting decisions, whereas we aim for an aggressive segmentation that allows for open-vocabulary translation with a compact network vocabulary, and without having to resort to back-off dictionaries. The best choice of subword units may be taskspecific. For speech recognition, phone-level language models have been used (Bazzi and Glass, 2000). Mikolov et al. (2012) investigate subword language models, and propose to use syllables. For multilingual segmentation tasks, multilingual algorithms have been"
P16-1162,W15-3031,0,0.0681316,"Missing"
P16-1162,2009.eamt-1.3,0,0.0688114,"t in 1716 pothesis in Sections 4 and 5. First, we discuss different subword representations. 3.1 Related Work For Statistical Machine Translation (SMT), the translation of unknown words has been the subject of intensive research. A large proportion of unknown words are names, which can just be copied into the target text if both languages share an alphabet. If alphabets differ, transliteration is required (Durrani et al., 2014). Character-based translation has also been investigated with phrase-based models, which proved especially successful for closely related languages (Vilar et al., 2007; Tiedemann, 2009; Neubig et al., 2012). The segmentation of morphologically complex words such as compounds is widely used for SMT, and various algorithms for morpheme segmentation have been investigated (Nießen and Ney, 2000; Koehn and Knight, 2003; Virpioja et al., 2007; Stallard et al., 2012). Segmentation algorithms commonly used for phrase-based SMT tend to be conservative in their splitting decisions, whereas we aim for an aggressive segmentation that allows for open-vocabulary translation with a compact network vocabulary, and without having to resort to back-off dictionaries. The best choice of subwor"
P18-4020,P07-2045,1,0.0306875,"Missing"
P18-4020,E17-2025,0,0.0357645,"83.9 73.0 67.9 54.8 46.6 35.3 40 23.5 20 12.4 60 0 100 Deep RNN 80 60 40 20 7.8 42.5 33.437.1 28.2 22.8 17.2 13.1 0 100 Transformer 80 54.9 49.0 42.9 37.6 30.4 23.4 16.4 60 40 20 0 9.1 1 2 3 4 5 6 7 8 Number of GPUs Figure 1: Training speed in thousands of source tokens per second for shallow RNN, deep RNN and Transformer model. Dashed line projects linear scale-up based on single-GPU performance. ory to maximize speed and memory usage. This guarantees that a chosen memory budget will not be exceeded during training. All models use tied embeddings between source, target and output embeddings (Press and Wolf, 2017). Contrary to Sennrich et al. (2017a) or Vaswani et al. (2017), we do not average checkpoints, but maintain a continuously updated exponentially averaged model over the entire training run. Following Vaswani et al. (2017), the learning rate is set to 0.0003 and decayed as the inverse square root of the number of updates after 16,000 updates. When training the transformer model, a linearly growing learning rate is used during the first 16,000 iterations, starting with 0 until the base learning rate is reached. 118 W a¨ Si hl e e n ei n Ta en s be tatu - rfe h ss la im tz M e fe nu¨ s . tleg e E"
P18-4020,W17-4774,1,0.824066,"s. Scaling is not linear (dashed lines), but close. The tokens-per-second rate (w/s) for Nematus on the same data on a single GPU is about 2800 w/s for the shallow model. Nematus does not have multiGPU training. Marian achieves about 4 times faster training on a single GPU and about 30 times faster training on 8 GPUs for identical models. Translation speed. The back-translation of 10M sentences with a shallow model takes about four State-of-the-art in Neural Automatic Post-Editing In our submission to the Automatic Post-Editing shared task at WMT-2017 (Bojar et al., 2017b) and follow-up work (Junczys-Dowmunt and Grundkiewicz, 2017a,b), we explore multiple neural architectures adapted for the task of automatic postediting of machine translation output as implementations in Marian. We focus on neural end-to-end models that combine both inputs mt (raw MT output) and src (source language input) in a single neural architecture, modeling {mt, src} → pe directly, where pe is post-edited corrected output. These models are based on multi-source neural translation models introduced by Zoph and Knight (2016). Furthermore, we investigate the effect of hard-attention models or neural transductors (Aharoni and Goldberg, 2016) which"
P18-4020,E17-3017,1,0.852816,"Missing"
P18-4020,I17-1013,1,0.851844,"s. Scaling is not linear (dashed lines), but close. The tokens-per-second rate (w/s) for Nematus on the same data on a single GPU is about 2800 w/s for the shallow model. Nematus does not have multiGPU training. Marian achieves about 4 times faster training on a single GPU and about 30 times faster training on 8 GPUs for identical models. Translation speed. The back-translation of 10M sentences with a shallow model takes about four State-of-the-art in Neural Automatic Post-Editing In our submission to the Automatic Post-Editing shared task at WMT-2017 (Bojar et al., 2017b) and follow-up work (Junczys-Dowmunt and Grundkiewicz, 2017a,b), we explore multiple neural architectures adapted for the task of automatic postediting of machine translation output as implementations in Marian. We focus on neural end-to-end models that combine both inputs mt (raw MT output) and src (source language input) in a single neural architecture, modeling {mt, src} → pe directly, where pe is post-edited corrected output. These models are based on multi-source neural translation models introduced by Zoph and Knight (2016). Furthermore, we investigate the effect of hard-attention models or neural transductors (Aharoni and Goldberg, 2016) which"
P18-4020,P16-1162,1,0.624087,"work, we implemented many efficient meta- ple scripts at https://github.com/marian-nmt/ algorithms. These include multi-device (GPU or marian-examples. 117 test2017 UEdin WMT17 (single) +Ensemble of 4 +R2L Reranking 33.9 35.1 36.2 27.5 28.3 28.3 Deep RNN (single) +Ensemble of 4 +R2L Reranking 34.3 35.3 35.9 27.7 28.2 28.7 Transformer (single) +Ensemble of 4 +R2L Reranking 35.6 36.4 36.8 28.8 29.4 29.5 Source tokens per second ×103 test2016 Source tokens per second ×103 System • preprocessing of training data, tokenization, true-casing4 , vocabulary reduction to 36,000 joint BPE subword units (Sennrich et al., 2016) with a separate tool.5 • training of a shallow model for backtranslation on parallel WMT17 data; • translation of 10M German monolingual news sentences to English; concatenation of artificial training corpus with original data (times two) to produce new training data; • training of four left-to-right (L2R) deep models (either RNN-based or Transformer-based); • training of four additional deep models with right-to-left (R2L) orientation; 6 • ensemble-decoding with four L2R models resulting in an n-best list of 12 hypotheses per input sentence; • rescoring of n-best list with four R2L models, a"
P18-4020,N18-1055,1,0.879647,"Missing"
P18-4020,P17-4012,0,0.0756084,"lconquers-patent-translation-in-majorwipo-roll-out/ Marian has minimal dependencies (only Boost and CUDA or a BLAS library) and enables barrierfree optimization at all levels: meta-algorithms such as MPI-based multi-node training, efficient batched beam search, compact implementations of new models, custom operators, and custom GPU kernels. Intel has contributed and is optimizing a CPU backend. Marian grew out of a C++ re-implementation of Nematus (Sennrich et al., 2017b), and still maintains binary-compatibility for common models. Hence, we will compare speed mostly against Nematus. OpenNMT (Klein et al., 2017), perhaps one of the most popular toolkits, has been reported to have training speed competitive to Nematus. Marian is distributed under the MIT license and available from https://marian-nmt. github.io or the GitHub repository https: //github.com/marian-nmt/marian. 2 Design Outline We will very briefly discuss the design of Marian. Technical details of the implementations will be provided in later work. 2.1 Custom Auto-Differentiation Engine The deep-learning back-end included in Marian is based on reverse-mode auto-differentiation with dynamic computation graphs and among the established mach"
W06-3123,J93-2003,0,0.0104668,"translation lexicons for both words and phrases. The joint model has been shown to outperform standard models on restricted data sets such as the small data track for Chinese-English in the 2004 NIST MT Evaluation (Przybocki, 2004). Introduction Machine translation is a hard problem because of the highly complex, irregular and diverse nature of natural languages. It is impossible to accurately model all the linguistic rules that shape the translation process, and therefore a principled approach uses statistical methods to make optimal decisions given incomplete data. The original IBM Models (Brown et al., 1993) learn word-to-word alignment probabilities which makes it computationally feasible to estimate model parameters from large amounts of training data. Phrase-based SMT models, such as the alignment template model (Och, 2003), improve on word-based models because phrases provide local context which leads to better lexical choice and more reliable local reordering. However, most phrase-based models extract their phrase pairs from previously word-aligned corpora using adhoc heuristics. These models perform no search 154 However, considering all possible phrases and all their possible alignments va"
W06-3123,P05-1066,1,0.176316,"hill-climbing are retained. Only a very small proportion of the alignment space can be searched and this reduces the chances of finding optimum parameters. The small number of alignments visited would lead to data sparseness and over-fitting. Another factor could be efficiency trade-offs like the fast but not optimal competitive linking search for phrasal alignments. 4.3 German-English submission We also submitted a German-English system using the standard approach to phrase extraction. The purpose of this submission was to validate the syntactic reordering method that we previously proposed (Collins et al., 2005). We parse the German training and test corpus and reorder it according to a set of manually devised rules. Then, we use our phrase-based system with standard phraseextraction, lexicalized reordering, lexical scoring, 5-gram LM, and the Pharaoh decoder. On the development test set, the syntactic reordering improved performance from 26.86 to 27.70. The best submission in last year’s shared task achieved a score of 24.77 on this set. 5 Conclusion We presented the first attempt at creating a systematic framework which uses word alignment constraints to guide phrase-based EM training. This shows c"
W06-3123,N03-1017,1,0.177164,"idence word alignments for each sentence. These high confidence alignments could incorporate information from both statistical and linguistic sources. In this paper we use the points of high confidence from the intersection of the bi-directional Viterbi word alignments to constrain the model, increasing performance and decreasing complexity. Proceedings of the Workshop on Statistical Machine Translation, pages 154–157, c New York City, June 2006. 2006 Association for Computational Linguistics 2 2.1 Translation Models Standard Phrase-based Model Most phrase-based translation models (Och, 2003; Koehn et al., 2003; Vogel et al., 2003) rely on a pre-existing set of word-based alignments from which they induce their parameters. In this project we use the model described by Koehn et al. (2003) which extracts its phrase alignments from a corpus that has been word aligned. From now on we refer to this phrase-based translation model as the standard model. The standard model decomposes the foreign input sentence F into a sequence of I phrases f 1 , . . . , f I . Each foreign phrase fi is translated to an English phrase ei using the probability distribution θ(f i |ei ). English phrases may be reordered using a"
W06-3123,W02-1018,0,0.2878,"the computational complexity of estimating parameters at the phrase level. We present the first model to use word alignments for constraining the space of phrasal alignments searched during Expectation Maximization (EM) training. Constraining the joint model improves performance, showing results that are very close to stateof-the-art phrase-based models. It also allows it to scale up to larger corpora and therefore be more widely applicable. 1 for optimal phrasal alignments. Even though this is an efficient strategy, it is a departure from the rigorous statistical framework of the IBM Models. Marcu and Wong (2002) proposed the joint probability model which directly estimates the phrase translation probabilities from the corpus in a theoretically governed way. This model neither relies on potentially sub-optimal word alignments nor on heuristics for phrase extraction. Instead, it searches the phrasal alignment space, simultaneously learning translation lexicons for both words and phrases. The joint model has been shown to outperform standard models on restricted data sets such as the small data track for Chinese-English in the 2004 NIST MT Evaluation (Przybocki, 2004). Introduction Machine translation i"
W06-3123,P97-1063,0,0.0262337,"ssible alignments C, each of which is defined as the product of the probability of all individual concepts: p(F, E) = X Y p(&lt; ei , f i &gt;) (1) C∈C &lt;ei ,f i &gt;∈C The model is trained by initializing the translation table using Stirling numbers of the second kind to efficiently estimate p(&lt; ei , f i &gt;) by calculating the proportion of alignments which contain p(&lt; ei , f i &gt;) compared to the total number of alignments in the sentence (Marcu and Wong, 2002). EM is then performed by first discovering an initial phrasal alignments using a greedy algorithm similar to the competitive linking algorithm (Melamed, 1997). The highest probability phrase pairs are iteratively selected until all phrases are are linked. Then hill-climbing is performed by searching once for each iteration for all merges, splits, moves and swaps that improve the probability of the initial phrasal alignment. Fractional counts are collected for all alignments visited. Training the IBM models is computationally challenging, but the joint model is much more demanding. Considering all possible segmentations of phrases and all their possible alignments vastly increases the number of possible alignments that can be formed between two sent"
W06-3123,P02-1038,0,0.0182161,"er corpora. After the initialization phase of the training, all phrase pairs with counts less 156 No. Concepts BLEU Time(min) Unconstrained 6,178k 19.93 299 Constrained 1,457k 22.13 169 Table 1. The impact of constraining the joint model trained on 10,000 sentences of the German-English Europarl corpora and tested with the Europarl test set used in Koehn et al. (2003) than 10 million times that of the phrase pair with the highest count, are pruned from the phrase table. The model is also parallelized in order to speed up training. The translation models are included within a log-linear model (Och and Ney, 2002) which allows a weighted combination of features functions. For the comparison of the basic systems in Table 2 only three features were used for both the joint and the standard model: p(e|f ), p(f |e) and the language model, and they were given equal weights. The results in Table 2 show that the joint model is capable of training on large data sets, with a reasonable performance compared to the standard model. However, here it seems that the standard model has a slight advantage. This is almost certainly related to the fact that the joint model results in a much smaller phrase table. Pruning e"
W06-3123,2003.mtsummit-papers.53,0,0.0236842,"ts for each sentence. These high confidence alignments could incorporate information from both statistical and linguistic sources. In this paper we use the points of high confidence from the intersection of the bi-directional Viterbi word alignments to constrain the model, increasing performance and decreasing complexity. Proceedings of the Workshop on Statistical Machine Translation, pages 154–157, c New York City, June 2006. 2006 Association for Computational Linguistics 2 2.1 Translation Models Standard Phrase-based Model Most phrase-based translation models (Och, 2003; Koehn et al., 2003; Vogel et al., 2003) rely on a pre-existing set of word-based alignments from which they induce their parameters. In this project we use the model described by Koehn et al. (2003) which extracts its phrase alignments from a corpus that has been word aligned. From now on we refer to this phrase-based translation model as the standard model. The standard model decomposes the foreign input sentence F into a sequence of I phrases f 1 , . . . , f I . Each foreign phrase fi is translated to an English phrase ei using the probability distribution θ(f i |ei ). English phrases may be reordered using a relative distortion"
W06-3123,koen-2004-pharaoh,0,\N,Missing
W06-3123,P02-1040,0,\N,Missing
W06-3123,P04-1023,1,\N,Missing
W06-3123,J04-4002,0,\N,Missing
W06-3123,W06-3105,0,\N,Missing
W06-3123,W06-3114,1,\N,Missing
W06-3123,2005.mtsummit-papers.11,1,\N,Missing
W06-3123,2005.iwslt-1.8,1,\N,Missing
W06-3123,P00-1056,0,\N,Missing
W06-3123,P03-1021,0,\N,Missing
W07-0702,J99-2004,0,0.392573,"Missing"
W07-0702,N03-2002,0,0.0405063,"general form α/β or αβ where α and β are themselves categories. An example of a CCG parse is given: Peter However, in many cases multiple dependencies are desirable. For instance translating CCG supertags independently of words could introduce errors. Multiple dependencies require some form of backing off to simpler models in order to cover the cases where, for instance, the word has been seen in training, but not with that particular supertag. Different backoff paths are possible, and it would be interesting but prohibitively slow to apply a strategy similar to generalised parallel backoff (Bilmes and Kirchhoff, 2003) which is used in factored language models. Backoff in factored language models is made more difficult because there is no obvious backoff path. This is compounded for factored phrase-based translation models where one has N X eats apples NP (SNP)/NP NP > SNP &lt; S where the derivation proceeds as follows: “eats” is combined with “apples” under the operation of forward application. “eats” can be thought of as a function that takes a NP to the right and returns a SNP. Similarly the phrase “eats apples” can be thought of as a function which takes a noun phrase NP to the left and returns a sente"
W07-0702,2005.mtsummit-papers.11,1,0.0284006,"se, far from the position of the English verb. 4.1 Experimental Setup The experiments were run using Moses2 , an open source factored statistical machine translation system. The SRILM language modelling toolkit (Stolcke, 2002) was used with modified Kneser-Ney discounting and interpolation. The CCG supertagger (Clark, 2002; Clark and Curran, 2004) was provided with the C&C Language Processing Tools3 . The supertagger was trained on the CCGBank in English (Hockenmaier and Steedman, 2005) and in German (Hockenmaier, 2006). The Dutch-English parallel training data comes from the Europarl corpus (Koehn, 2005) and excludes the proceedings from the last quarter of 2000. 2 3 This consists of 855,677 sentences with a maximum of 50 words per sentence. 500 sentences of tuning data and the 2000 sentences of test data are taken from the ACL Workshop on Building and Using Parallel Texts4 . The German-English experiments use data from the NAACL 2006 Workshop on Statistical Machine Translation5 . The data consists of 751,088 sentences of training data, 500 sentences of tuning data and 3064 sentences of test data. The English and German training sets were POS tagged and supertagged before lowercasing. The lan"
W07-0702,N03-1019,0,0.0194211,"actored translation models allow the inclusion of supertags as a factor in the source or target language. We show that this results in an improvement in the quality of translation and that the value of syntactic supertags in flat structured phrase-based models is largely due to better local reorderings. 1 Introduction In large-scale machine translation evaluations, phrase-based models generally outperform syntaxbased models1 . Phrase-based models are effective because they capture the lexical dependencies between languages. However, these models, which are equivalent to finite-state machines (Kumar and Byrne, 2003), are unable to model long range word order differences. Phrase-based models also lack the ability to incorporate the generalisations implicit in syntactic knowledge and they do not respect linguistic phrase boundaries. This makes it difficult to improve reordering in phrase-based models. Syntax-based models can overcome some of the problems associated with phrase-based models because they are able to capture the long range structural mappings that occur in translation. Recently In this paper we investigate the idea of incorporating syntax into phrase-based models, thereby leveraging the stren"
W07-0702,E06-1032,1,0.156668,"Missing"
W07-0702,W06-1606,0,0.0525496,"Missing"
W07-0702,P05-1033,0,0.0609583,".c.birch-mayne@sms.ed.ac.uk Miles Osborne Philipp Koehn miles@inf.ed.ac.uk pkoehn@inf.ed.ac.uk School of Informatics University of Edinburgh 2 Buccleuch Place Edinburgh, EH8 9LW, UK Abstract there have been a few syntax-based models that show performance comparable to the phrase-based models (Chiang, 2005; Marcu et al., 2006). However, reliably learning powerful rules from parallel data is very difficult and prone to problems with sparsity and noise in the data. These models also suffer from a large search space when decoding with an integrated language model, which can lead to search errors (Chiang, 2005). Combinatorial Categorial Grammar (CCG) supertags present phrase-based machine translation with an opportunity to access rich syntactic information at a word level. The challenge is incorporating this information into the translation process. Factored translation models allow the inclusion of supertags as a factor in the source or target language. We show that this results in an improvement in the quality of translation and that the value of syntactic supertags in flat structured phrase-based models is largely due to better local reorderings. 1 Introduction In large-scale machine translation"
W07-0702,P03-1021,0,0.0379746,"s) with more general models (which only depends on words). This paper is the first to suggest this approach for combining multiple information sources in machine translation. Although the addition of supertags to phrasebased translation does show some improvement, their overall impact is limited. Sequence models over supertags clearly result in some improvements in local reordering but syntactic information contains long distance dependencies which are simply not utilised in phrase-based models. 2 Factored Models forms. The factored translation model combines features in a log-linear fashion (Och, 2003). The most likely target sentence tˆ is calculated using the decision rule in Equation 1: ( tˆ = arg max t M X tˆ ∝ M X ) λm hm (sF1 s , tF1 t ) (1) m=1 λm hm (sF1 s , tF1 t ) (2) m=1 where M is the number of features, hm (sF1 s , tF1 t ) are the feature functions over the factors, and λ are the weights which combine the features which are optimised using minimum error rate training (Venugopal and Vogel, 2005). Each function depends on a vector sF1 s of source factors and a vector tF1 t of target factors. An example of a factored model used in upcoming experiments is: tˆ ∝ M X λm hm (sw , twc"
W07-0702,P04-1014,0,0.00805384,"ct of CCG supertags on the source, translating from German into English. These language pairs present a considerable reordering challenge. For example, Dutch and German have SOV word order in subordinate clauses. This means that the verb often appears at the end of the clause, far from the position of the English verb. 4.1 Experimental Setup The experiments were run using Moses2 , an open source factored statistical machine translation system. The SRILM language modelling toolkit (Stolcke, 2002) was used with modified Kneser-Ney discounting and interpolation. The CCG supertagger (Clark, 2002; Clark and Curran, 2004) was provided with the C&C Language Processing Tools3 . The supertagger was trained on the CCGBank in English (Hockenmaier and Steedman, 2005) and in German (Hockenmaier, 2006). The Dutch-English parallel training data comes from the Europarl corpus (Koehn, 2005) and excludes the proceedings from the last quarter of 2000. 2 3 This consists of 855,677 sentences with a maximum of 50 words per sentence. 500 sentences of tuning data and the 2000 sentences of test data are taken from the ACL Workshop on Building and Using Parallel Texts4 . The German-English experiments use data from the NAACL 2006"
W07-0702,P05-1034,0,0.0456775,"ted in range. If it can replace 14 CCG supertags, it suggests that supertags’ influence is also within a local range. 4.7 CCG Supertags on Source Sequence models over supertags improve the performance of phrase-based machine translation. However, this is a limited way of leveraging the rich syntactic information available in the CCG categories. We explore the potential of letting supertags direct translation by including them as a factor on the source. This is similar to syntax-directed translation originally proposed for compiling (Aho and Ullman, 1969), and also used in machine translation (Quirk et al., 2005; Huang et al., 2006). Information about the source words’ syntactic function and subcategorisation can directly influence the hypotheses being searched in decoding. These experiments were performed on the German to English translation task, in contrast to the Dutch to English results given in previous experiments. We use a model which combines more specific dependencies on source words and source CCG supertags, with a more general model which only has dependancies on the source word, see Equation 4. We explore two different ways of balancing the statistical evidence from these multiple source"
W07-0702,W02-2203,0,0.352084,"e “eats apples” can be thought of as a function which takes a noun phrase NP to the left and returns a sentence S. This operation is called backward application. A sentence together with its CCG categories already contains most of the information present in a full parse. Because these categories are lexicalised, they can easily be included into factored phrasebased translation. CCG supertags are categories that have been provided by a supertagger. Supertags were introduced by Bangalore (1999) as a way of increasing parsing efficiency by reducing the number of structures assigned to each word. Clark (2002) developed a suppertagger for CCG which uses a conditional maximum entropy model to estimate the probability of words being assigned particular categories. Here is an example of a sentence that has been supertagged in the training corpus: We all agree on that . NP NPNP (S[dcl]NP)/PP PP/NP NP . The verb “agree” has been assigned a complex supertag (S[dcl]NP)/PP which determines the type and direction of its arguments. This information can be used to improve the quality of translation. 4 Experiments The first set of experiments explores the effect of CCG supertags on the target, translating f"
W07-0702,P07-1037,0,0.518309,"Missing"
W07-0702,P06-1064,0,0.010386,"d order in subordinate clauses. This means that the verb often appears at the end of the clause, far from the position of the English verb. 4.1 Experimental Setup The experiments were run using Moses2 , an open source factored statistical machine translation system. The SRILM language modelling toolkit (Stolcke, 2002) was used with modified Kneser-Ney discounting and interpolation. The CCG supertagger (Clark, 2002; Clark and Curran, 2004) was provided with the C&C Language Processing Tools3 . The supertagger was trained on the CCGBank in English (Hockenmaier and Steedman, 2005) and in German (Hockenmaier, 2006). The Dutch-English parallel training data comes from the Europarl corpus (Koehn, 2005) and excludes the proceedings from the last quarter of 2000. 2 3 This consists of 855,677 sentences with a maximum of 50 words per sentence. 500 sentences of tuning data and the 2000 sentences of test data are taken from the ACL Workshop on Building and Using Parallel Texts4 . The German-English experiments use data from the NAACL 2006 Workshop on Statistical Machine Translation5 . The data consists of 751,088 sentences of training data, 500 sentences of tuning data and 3064 sentences of test data. The Engli"
W07-0702,W06-3601,0,0.00472148,"can replace 14 CCG supertags, it suggests that supertags’ influence is also within a local range. 4.7 CCG Supertags on Source Sequence models over supertags improve the performance of phrase-based machine translation. However, this is a limited way of leveraging the rich syntactic information available in the CCG categories. We explore the potential of letting supertags direct translation by including them as a factor on the source. This is similar to syntax-directed translation originally proposed for compiling (Aho and Ullman, 1969), and also used in machine translation (Quirk et al., 2005; Huang et al., 2006). Information about the source words’ syntactic function and subcategorisation can directly influence the hypotheses being searched in decoding. These experiments were performed on the German to English translation task, in contrast to the Dutch to English results given in previous experiments. We use a model which combines more specific dependencies on source words and source CCG supertags, with a more general model which only has dependancies on the source word, see Equation 4. We explore two different ways of balancing the statistical evidence from these multiple sources. The first way to c"
W07-0702,N03-1017,1,0.00863018,"actors. Phrase-based models are limited to sequences of words as their units with no access to additional linguistic knowledge. Factors allow for richer translation models, for example, the gender or tense of a word can be expressed. Factors also allow the model to generalise, for example, the lemma of a word could be used to generalise to unseen inflected 10 Figure 1. Factored translation with source words determining target words and CCG supertags For our experiments we used the following features: the translation probabilities P r(sF1 s |tF1 t ) and P r(tF1 t |sF1 s ), the lexical weights (Koehn et al., 2003) lex(sF1 s |tF1 t ) and lex(tF1 t |sF1 s ), and a phrase penalty e, which allows the model to learn a preference for longer or shorter phrases. Added to these features is the word penalty e−1 which allows the model to learn a preference for longer or shorter sentences, the distortion model d that prefers monotone word order, and the language model probability P r(t). All these features are logged when combined in the log-linear model in order to retain the impact of very unlikely translations or sequences. One of the strengths of the factored model is it allows for n-gram distributions over fa"
W07-0702,W06-2918,1,0.902719,"overcome some of the problems associated with phrase-based models because they are able to capture the long range structural mappings that occur in translation. Recently In this paper we investigate the idea of incorporating syntax into phrase-based models, thereby leveraging the strengths of both the phrase-based models and syntactic structures. This is done using CCG supertags, which provide a rich source of syntactic information. CCG contains most of the structure of the grammar in the lexicon, which makes it possible to introduce CCG supertags as a factor in a factored translation model (Koehn et al., 2006). Factored models allow words to be vectors of features: one factor could be the surface form and other factors could contain linguistic information. Factored models allow for the easy inclusion of supertags in different ways. The first approach is to generate CCG supertags as a factor in the target and then apply an n-gram model over them, increasing the probability of more frequently seen sequences of supertags. This is a simple way of including syntactic information in a phrase-based model, and has also been suggested by Hassan et al. (2007). For both Arabic-English (Hassan et al., 2007) an"
W07-0702,P05-1003,1,0.828589,"Missing"
W07-0702,N04-4026,0,0.0161138,"solation contributes more to translation performance than any other sequence model. Even with a high order language model, applying the CCG supertag sequence model still seems to improve performance. This means that even if we use a more powerful language model, the structural information contained in the supertags continues to be beneficial. 4.6 Lexicalised Reordering vs. Supertags In this experiment we investigate using a stronger reordering model to see how it compares to the contribution that CCG supertag sequence models make. Moses implements the lexicalised reordering model described by Tillman (2004), which learns whether phrases prefer monotone, inverse or disjoint orientations with regard to adjacent phrases. We apply this reordering models to the following experiments. Model sw , tw sw , twc None 23.97 24.42 Lex. Reord. 24.72 24.78 Table 5. Dutch-English models with and without a lexicalised reordering model. In Table 5 we can see that lexicalised reordering improves translation performance for both models. However, the improvement that was seen using CCG supertags without lexicalised reordering, almost disappears when using a stronger reordering model. This suggests that CCG supertags"
W07-0702,P07-2045,1,\N,Missing
W07-0702,2006.iwslt-evaluation.8,0,\N,Missing
W09-0434,D08-1078,1,0.823619,"e of these systems adequately deal with longer range reordering. Our analysis provides a deeper understanding of why hierarchical models demonstrate better performance for Chinese-English translation, and also why phrase-based approaches do well at Arabic-English. We begin by reviewing related work in Section 2. Section 3 describes our method for extracting and measuring reorderings in aligned and parsed parallel corpora. We apply our techniques to human aligned parallel treebank sentences in Section 4, and to machine translation outputs in Section 5. We summarise our findings in Section 6. 2 Birch et al. (2008) proposed a method for extracting reorderings from aligned parallel sentences. We extend this method in order to constrain the reorderings to a derivation over the source sentence where possible. 3 Measuring Reordering Reordering is largely driven by syntactic differences between languages and can involve complex rearrangements between nodes in synchronous trees. Modeling reordering exactly would be sparse and heterogeneous and thus we make an important simplifying assumption in order for the detection and extraction of reordering data to be tractable and useful. We assume that reordering is a"
W09-0434,E06-1032,1,0.705755,"it very difficult to model long distance movement of words between languages. Synchronous grammar models can encode structural mappings between languages which allow complex, long distance reordering. Some grammar-based models such as the hierarchical model (Chiang, 2005) and the syntactified target language phrases model (Marcu et al., 2006) have shown better performance than phrase-based models on certain language pairs. To date our understanding of the variation in reordering performance between phrase-based and synchronous grammar models has been limited to relative B LEU scores. However, Callison-Burch et al. (2006) showed that B LEU score alone is insufficient for comparing reordering as it only measures a partial ordering on n-grams. There has been little direct research on empirically evaluating reordering. We evaluate the reordering characteristics of these two paradigms on Chinese-English and Arabic-English translation. Our main findings are as follows: (1) Chinese-English parallel sentences exhibit many medium and long-range reorderings, but less short range ones than Arabic-English, (2) phrase-based models account for short-range reorderings better than hierarchical models do, (3) Reordering is a"
W09-0434,H05-1098,0,0.0367168,"ere are few empirical studies of reordering behaviour in the statistical machine translation literature. Fox (2002) showed that many common reorderings fall outside the scope of synchronous grammars that only allow the reordering of child nodes. This study was performed manually and did not compare different language pairs or translation paradigms. There are some comparative studies of the reordering restrictions that can be imposed on the phrase-based or grammar-based models (Zens and Ney, 2003; Wellington et al., 2006), however these do not look at the reordering performance of the systems. Chiang et al. (2005) proposed a more fine-grained method of comparing the output of two translation systems by using the frequency of POS sequences in the output. This method is a first step towards a better understanding of comparative reordering performance, but neglects the question of what kind of reordering is occurring in corpora and in translation output. Zollmann et al. (2008) performed an empirical comparison of the B LEU score performance of hierarchical models with phrase-based models. They tried to ascertain which is the stronger model under different reordering scenarios by varying distortion limits"
W09-0434,P05-1033,0,0.0716465,"last few years, showing state-of-the-art performance for many language pairs. They search all possible reorderings within a restricted window, and their output is guided by the language model and a lexicalised reordering model (Och et al., 2004), both of which are local in scope. However, the lack of structure in phrase-based models makes it very difficult to model long distance movement of words between languages. Synchronous grammar models can encode structural mappings between languages which allow complex, long distance reordering. Some grammar-based models such as the hierarchical model (Chiang, 2005) and the syntactified target language phrases model (Marcu et al., 2006) have shown better performance than phrase-based models on certain language pairs. To date our understanding of the variation in reordering performance between phrase-based and synchronous grammar models has been limited to relative B LEU scores. However, Callison-Burch et al. (2006) showed that B LEU score alone is insufficient for comparing reordering as it only measures a partial ordering on n-grams. There has been little direct research on empirically evaluating reordering. We evaluate the reordering characteristics of"
W09-0434,J07-2003,0,0.0242398,"e the reorderings in the parallel corpora with the reorderings that exist in the translated sentences. We com200 367 379 367 379 250 1.51 0.57 200 0.82 0.25 Table 1. The RQuantity and the number of sentences for each reordering test set. 0 2 3 4 5 6 7−8 9−10 16−20 Widths of Reorderings 22 Figure 6. Number of reorderings in the CH-EN test set plotted against the total width of the reorderings. 16 18 20 MOSES HIERO Reordering Test Corpus 14 5.1 50 pare two state-of-the-art models: the phrase-based system Moses (Koehn et al., 2007) (with lexicalised reordering), and the hierarchical model Hiero (Chiang, 2007). We use default settings for both models: a distortion limit of seven for Moses, and a maximum source span limit of 10 words for Hiero. We trained both models on subsets of the NIST 2008 data sets, consisting mainly of news data, totalling 547,420 CH-EN and 1,069,658 AREN sentence pairs. We used a trigram language model on the entire English side (211M words) of the NIST 2008 Chinese-English training corpus. Minimum error rate training was performed on the 2002 NIST test for CH-EN, and the 2004 NIST test set for AR-EN. Low Medium High 150 High 100 Medium Number of Reorderings None Low Average"
W09-0434,W02-1039,0,0.161844,"we can see a sentence pair with an alignment and a parse tree over the source. We perform a depth first recursion through the tree, extracting the reorderings that occur between whole sibling nodes. Initially a reordering is detected between the leaf nodes P and NN. The block growing algorithm described in Birch et al. (2008) is then used to grow block A to include NT and NN, and block B to include P and NR. The source and target spans of these nodes do not overlap the spans Related Work There are few empirical studies of reordering behaviour in the statistical machine translation literature. Fox (2002) showed that many common reorderings fall outside the scope of synchronous grammars that only allow the reordering of child nodes. This study was performed manually and did not compare different language pairs or translation paradigms. There are some comparative studies of the reordering restrictions that can be imposed on the phrase-based or grammar-based models (Zens and Ney, 2003; Wellington et al., 2006), however these do not look at the reordering performance of the systems. Chiang et al. (2005) proposed a more fine-grained method of comparing the output of two translation systems by usin"
W09-0434,N03-1017,0,0.040066,"Missing"
W09-0434,P07-2045,1,0.00797446,"on models perform specifically with regard to reordering. To evaluate this, we compare the reorderings in the parallel corpora with the reorderings that exist in the translated sentences. We com200 367 379 367 379 250 1.51 0.57 200 0.82 0.25 Table 1. The RQuantity and the number of sentences for each reordering test set. 0 2 3 4 5 6 7−8 9−10 16−20 Widths of Reorderings 22 Figure 6. Number of reorderings in the CH-EN test set plotted against the total width of the reorderings. 16 18 20 MOSES HIERO Reordering Test Corpus 14 5.1 50 pare two state-of-the-art models: the phrase-based system Moses (Koehn et al., 2007) (with lexicalised reordering), and the hierarchical model Hiero (Chiang, 2007). We use default settings for both models: a distortion limit of seven for Moses, and a maximum source span limit of 10 words for Hiero. We trained both models on subsets of the NIST 2008 data sets, consisting mainly of news data, totalling 547,420 CH-EN and 1,069,658 AREN sentence pairs. We used a trigram language model on the entire English side (211M words) of the NIST 2008 Chinese-English training corpus. Minimum error rate training was performed on the 2002 NIST test for CH-EN, and the 2004 NIST test set for AR"
W09-0434,W04-3250,0,0.218516,"Missing"
W09-0434,W06-1606,0,0.0281669,"uage pairs. They search all possible reorderings within a restricted window, and their output is guided by the language model and a lexicalised reordering model (Och et al., 2004), both of which are local in scope. However, the lack of structure in phrase-based models makes it very difficult to model long distance movement of words between languages. Synchronous grammar models can encode structural mappings between languages which allow complex, long distance reordering. Some grammar-based models such as the hierarchical model (Chiang, 2005) and the syntactified target language phrases model (Marcu et al., 2006) have shown better performance than phrase-based models on certain language pairs. To date our understanding of the variation in reordering performance between phrase-based and synchronous grammar models has been limited to relative B LEU scores. However, Callison-Burch et al. (2006) showed that B LEU score alone is insufficient for comparing reordering as it only measures a partial ordering on n-grams. There has been little direct research on empirically evaluating reordering. We evaluate the reordering characteristics of these two paradigms on Chinese-English and Arabic-English translation."
W09-0434,J04-4002,0,0.0234622,"Missing"
W09-0434,P06-1123,0,0.0178902,"lude P and NR. The source and target spans of these nodes do not overlap the spans Related Work There are few empirical studies of reordering behaviour in the statistical machine translation literature. Fox (2002) showed that many common reorderings fall outside the scope of synchronous grammars that only allow the reordering of child nodes. This study was performed manually and did not compare different language pairs or translation paradigms. There are some comparative studies of the reordering restrictions that can be imposed on the phrase-based or grammar-based models (Zens and Ney, 2003; Wellington et al., 2006), however these do not look at the reordering performance of the systems. Chiang et al. (2005) proposed a more fine-grained method of comparing the output of two translation systems by using the frequency of POS sequences in the output. This method is a first step towards a better understanding of comparative reordering performance, but neglects the question of what kind of reordering is occurring in corpora and in translation output. Zollmann et al. (2008) performed an empirical comparison of the B LEU score performance of hierarchical models with phrase-based models. They tried to ascertain"
W09-0434,J97-3002,0,0.241369,"lex rearrangements between nodes in synchronous trees. Modeling reordering exactly would be sparse and heterogeneous and thus we make an important simplifying assumption in order for the detection and extraction of reordering data to be tractable and useful. We assume that reordering is a binary process occurring between two blocks that are adjacent in the source. We extend the methods proposed by Birch et al. (2008) to identify and measure reordering. Modeling reordering as the inversion in order of two adjacent blocks is similar to the approach taken by the Inverse Transduction Model (ITG) (Wu, 1997), except that here we are not limited to a binary tree. We also detect and include non-syntactic reorderings as they constitute a significant proportion of the reorderings. Birch et al. (2008) defined the extraction process for a sentence pair that has been word aligned. This method is simple, efficient and applicable to all aligned sentence pairs. However, if we have access to the syntax tree, we can more accurately determine the groupings of embedded reorderings, and we can also access interesting information about the reordering such as the type of constituents that get reordered. Figure 1"
W09-0434,P03-1019,0,0.0432249,", and block B to include P and NR. The source and target spans of these nodes do not overlap the spans Related Work There are few empirical studies of reordering behaviour in the statistical machine translation literature. Fox (2002) showed that many common reorderings fall outside the scope of synchronous grammars that only allow the reordering of child nodes. This study was performed manually and did not compare different language pairs or translation paradigms. There are some comparative studies of the reordering restrictions that can be imposed on the phrase-based or grammar-based models (Zens and Ney, 2003; Wellington et al., 2006), however these do not look at the reordering performance of the systems. Chiang et al. (2005) proposed a more fine-grained method of comparing the output of two translation systems by using the frequency of POS sequences in the output. This method is a first step towards a better understanding of comparative reordering performance, but neglects the question of what kind of reordering is occurring in corpora and in translation output. Zollmann et al. (2008) performed an empirical comparison of the B LEU score performance of hierarchical models with phrase-based models"
W09-0434,C08-1144,0,0.0520931,"comparative studies of the reordering restrictions that can be imposed on the phrase-based or grammar-based models (Zens and Ney, 2003; Wellington et al., 2006), however these do not look at the reordering performance of the systems. Chiang et al. (2005) proposed a more fine-grained method of comparing the output of two translation systems by using the frequency of POS sequences in the output. This method is a first step towards a better understanding of comparative reordering performance, but neglects the question of what kind of reordering is occurring in corpora and in translation output. Zollmann et al. (2008) performed an empirical comparison of the B LEU score performance of hierarchical models with phrase-based models. They tried to ascertain which is the stronger model under different reordering scenarios by varying distortion limits the strength of language models. They show that the hierarchical models do slightly better for Chinese-English systems, but worse for Arabic-English. However, there was no analysis of the reorderings existing in their parallel corpora, or on what kinds of reorderings were produced in their output. We perform a focused evaluation of these issues. 198 A A B B Figure"
W09-0434,N04-1021,0,\N,Missing
W10-1749,P03-1069,0,0.0217815,"ic R is calculated as follows: The Kendall’s tau metric is possibly the most interesting for measuring reordering as it is sensitive to all relative orderings. It consequently measures not only how many reordering there are but also the distance that words are reordered. In statistics, Spearman’s rho and Kendall’s tau are widely used non-parametric measures of association for two rankings. In natural language processing research, Kendall’s tau has been used as a means of estimating the distance between a system-generated and a human-generated goldstandard order for the sentence ordering task (Lapata, 2003). Kendall’s tau has also been used in machine translation as a cost function in a reordering model (Eisner and Tromble, 2006) and an MT metric called ROUGE-S (Lin and Och, R = d ∗ BP Where we either take the Hamming distance dH or the Kendall’s tau distance dτ as the reordering distance d and then we apply the brevity penalty BP . The brevity penalty is calculated as:  1 if t > r BP = e1−r/t if t ≤ r where t is the length of the translation, and r is the closest reference length. R is calculated at the sentence level, and the scores are averaged over a test set. This average is then combined"
W10-1749,P04-1077,0,0.0139617,"Missing"
W10-1749,P02-1040,0,0.108188,"Missing"
W10-1749,W05-0909,0,0.0515296,"Missing"
W10-1749,2006.amta-papers.25,0,0.0476832,"aluating Lexical and Reordering Quality in MT Alexandra Birch University of Edinburgh United Kingdom a.c.birch-mayne@s0454866.ed.ac.uk Abstract to measure the quality of word order is to count the number of matching n-grams between the reference and the hypothesis. This is the approach taken by the BLEU score (Papineni et al., 2002). This method discounts any n-gram which is not identical to a reference n-gram, and also does not consider the relative position of the strings. They can be anywhere in the sentence. Another common approach is typified by METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006). They calculate an ordering penalty for a hypothesis based on the minimum number of chunks the translation needs to be broken into in order to align it to the reference. The disadvantage of the second approach is that aligning sentences with very different words can be inaccurate. Also there is no notion of how far these blocks are out of order. More sophisticated metrics, such as the RTE metric (Pad´o et al., 2009), use higher level syntactic or even semantic analysis to determine the quality of the translation. These approaches are useful, but can be very slow, require annotation, they are"
W10-1749,W09-0401,0,\N,Missing
W11-2916,J07-4002,0,0.0303435,"Missing"
W11-2916,D07-1101,0,0.0197279,"on some data sets, but not on others. A look at the dev. sets show that second-order features were active in 9 of the first 60 decisions in the WSJ, but in 0 of the first 60 in Brown. We speculate that use of word senses as features, taking after Stetina and Nagao (1997), might result in better generality across domains, but leave this to future work. [NP salad [PP with [NP dressing]]] The active features are hsalad, withi and hsalad, with, dressingi. This latter type of feature represents straightforward specialization of the concept used by higher-order dependencies for parsing, especially Carreras (2007), to the problem of prepositional phrases. Our estimates of θD1 and θD2 are arrived at using structured perceptron training (Collins, 2002). The models trained on all examples in our training section (i.e. those of type NP and of type VP). We pick the number of perceptron training iterations for each model by maximizing performance on a held-out set, using our parameter tuning split described in §3.3. We only use feature instances that have occurred at least twice in training. If Φ(x, d) is a feature vector characterizing (x, d), the perceptron algorithm will output a parameter vector θDi , an"
W11-2916,W95-0103,0,0.10579,"h is better than either is alone, and which outperforms the modern parser of Petrov and Klein (2007) by a significant margin. We show that, when learning from unlabelled data, it can be beneficial to model the generation of modifiers of a head collectively, rather than individually. Finally, we suggest that our pair of models will be interesting to combine using new techniques for discriminatively constraining EM. 1 He ate a salad [PP with a fork] [PP of plastic] Prepositional phrase attachment is an important sub-problem of parsing in and of itself. Structural heuristics perform poorly (cf., Collins and Brooks, 1995), and so lexical knowledge is crucial. Moreover, the highly lexicalized nature of prepositional phrase attachment makes it a kind of microcosm of the general problem of learning dependency structure, and so acts as a computationally less-demanding testing ground on which to try out learning techniques. We have endeavoured to approach the problem with a strategy that might be likely to generalize: a mix of generative and discriminative lexical models, trained using techniques that have worked for parsers. The main contributions of this paper are: • We compare the performance on the prepositiona"
W11-2916,W02-1001,0,0.0694339,"the WSJ, but in 0 of the first 60 in Brown. We speculate that use of word senses as features, taking after Stetina and Nagao (1997), might result in better generality across domains, but leave this to future work. [NP salad [PP with [NP dressing]]] The active features are hsalad, withi and hsalad, with, dressingi. This latter type of feature represents straightforward specialization of the concept used by higher-order dependencies for parsing, especially Carreras (2007), to the problem of prepositional phrases. Our estimates of θD1 and θD2 are arrived at using structured perceptron training (Collins, 2002). The models trained on all examples in our training section (i.e. those of type NP and of type VP). We pick the number of perceptron training iterations for each model by maximizing performance on a held-out set, using our parameter tuning split described in §3.3. We only use feature instances that have occurred at least twice in training. If Φ(x, d) is a feature vector characterizing (x, d), the perceptron algorithm will output a parameter vector θDi , and the “score” assigned to a pair (x, d) under this interpretation will be θDi · Φ(x, d), with the predicted derivation being the d with the"
W11-2916,H94-1048,0,0.198696,"e derivations. In terms of size, our WSJ2-21 set has 29, 750 examples. The GigaWord set has 8, 038, 001 examples. 2 However, we do appeal to the labels of a portion of the out-of-domain Brown data once, in order to fix a single experimental parameter, which is the number of iterations of EM to run on the unlabelled data, cf. note 6. 1 Our data sets extracted from the Penn Treebank will be available on request to those with the relevant license(s) to use Penn Treebank data. 131 3.4 Baseline Parser Berkeley 5 SM Berkeley 6 SM Much past work has tested on the 4-tuple, binary decision data set of Ratnaparkhi et al. (1994). This data does not have all of the information required by our approach, and is based on a preliminary version of the Penn Treebank (version 0.75), which is incomplete and difficult to work with. Thus, we could not compare our work directly with past work. WSJ Dev. Test 85.3 83.0 84.6 83.0 Brown Dev. Test 82.4 81.1 83.3 82.7 Table 1: Performance of the Berkeley parser on the prepositional phrase attachment task. The best scores on each data set will be our baseline. on each data set as the baseline on that data set.3 In order to evaluate our performance, then, we will compare our model again"
W11-2916,P05-1045,0,0.00463106,"formance on prepositional phrase attachment of the Berkeley parser (Petrov and Klein, 2007), which is popular, readily available, and essentially state-of-art among supervised parsing methods. And, as we said, this is precisely the technology that we use to process unlabelled data, so it makes sense that our model should improve upon this in order to be of any use. 3.5 Reduction of Open-Class Words In all experiments, all nouns and verbs were replaced by more general forms. If applicable, nouns were replaced by their NER label, either person, place or organization, using the NER classifier of Finkel et al. (2005). All numeral strings of two or four digits were replaced with a symbol representing year, and all other numeral strings were replaced with a symbol representing numeric value. A word not reduced in either of these ways was replaced by its stem using the stemmer designed by Minnen et al. (2001).4 Finally, this reduced form is paired with the category of the word c ∈ {NOUN, VERB} to distinguish uses of words that can either be nouns or verbs. We find that these reductions improve performance slightly and also reduce the size of the generative probability table. So, we need to evaluate the prepo"
W11-2916,P98-2177,0,0.103701,"Missing"
W11-2916,P05-1003,0,0.0286159,"the performance of the model that combines θDi 0 with θGCol 0 on our WSJ sections 2-4, with a simple search over values [0, .01, · · · , .99, 1]. The optimal values for the k i were k 1 = .70 and k 2 = .71. A Combination of Models The Combination We now have two models of prepositional phrase attachment. One is estimated from labelled data, and one from unlabelled data. Each performs well in isolation. But, we find that the combination of the two in a logarithmic opinion pool framework works better than either does alone. With roots in Bordley (1982), a logarithmic opinion pool, as defined in Smith et al. (2005), has the form plop (d|x) = 1 Y Z lop 6.2 pα (d|x)w a α In related work, Hinton (1999) describes a product of experts, i.e. multiple models trained to work together, so that an unweighted product, will be sensible. Petrov (2010) has success with the unweighted product of the scores of several similar parsing models. Smith et al. (2005) adjust weights by maximizing the likelihood of a labelled dataset. Here, we weight the component models so as to maximize performance on a held out set. Where i is either 1 or 2, let plop,i (d |x; θDi , θGCol ) = 1 · pDi (d |x; θDi )k i · pGCol (d |x; θGCol )1 −"
W11-2916,W01-0521,0,0.0145759,"ter tuning. Scoring Performance When evaluating a prediction procedure, we will give it a series of attachment problems and ask for the derivations. In most cases, the score we will focus on is what we can call the binary decision score, i.e., the percentage of the time in which the first prepositional phrase following a verb–directobject pair is attached correctly. In this case, we are reporting the same score as is typically reported on this task, so as to avoid introducing a new metric. To be clear, then, when scoring, in this way, We split up the Brown portion of the treebank similarly to Gildea (2001)—i.e. we split it into 10 sections such that the s’th sentence is assigned to section s mod 10. We then use sections 0-2 development test set, 4-6 for tuning, and sections 7-9 are left for final evaluation. Our divisions of the Penn Treebank are chosen to resemble the canonical training-test split for parsing, but we use more sections for testing, to obtain more reliable test scores, as there are far fewer decisions to test on in each section in our task, when compared to parsing. [VP set [NP rate [PP on [NP refund]] ] [PP at [NP 5 percent]]]] we only ask where [PP on [NP refund]] attaches, an"
W11-2916,W97-0109,0,0.0526708,"hould be of interest to those interested the use of lexical features for parsing. The models that use lexical features outperform the semi-lexical model of Petrov and Klein (2007). Prepositional phrase attachment may be one area where lexicalized models are especially important. We also see that the use of second-order features buys extra performance on some data sets, but not on others. A look at the dev. sets show that second-order features were active in 9 of the first 60 decisions in the WSJ, but in 0 of the first 60 in Brown. We speculate that use of word senses as features, taking after Stetina and Nagao (1997), might result in better generality across domains, but leave this to future work. [NP salad [PP with [NP dressing]]] The active features are hsalad, withi and hsalad, with, dressingi. This latter type of feature represents straightforward specialization of the concept used by higher-order dependencies for parsing, especially Carreras (2007), to the problem of prepositional phrases. Our estimates of θD1 and θD2 are arrived at using structured perceptron training (Collins, 2002). The models trained on all examples in our training section (i.e. those of type NP and of type VP). We pick the numbe"
W11-2916,C02-1004,0,0.0663202,"Missing"
W11-2916,P91-1030,0,0.356516,"Missing"
W11-2916,P00-1014,0,0.0716085,"Missing"
W11-2916,N10-1003,0,0.0208612,"Combination We now have two models of prepositional phrase attachment. One is estimated from labelled data, and one from unlabelled data. Each performs well in isolation. But, we find that the combination of the two in a logarithmic opinion pool framework works better than either does alone. With roots in Bordley (1982), a logarithmic opinion pool, as defined in Smith et al. (2005), has the form plop (d|x) = 1 Y Z lop 6.2 pα (d|x)w a α In related work, Hinton (1999) describes a product of experts, i.e. multiple models trained to work together, so that an unweighted product, will be sensible. Petrov (2010) has success with the unweighted product of the scores of several similar parsing models. Smith et al. (2005) adjust weights by maximizing the likelihood of a labelled dataset. Here, we weight the component models so as to maximize performance on a held out set. Where i is either 1 or 2, let plop,i (d |x; θDi , θGCol ) = 1 · pDi (d |x; θDi )k i · pGCol (d |x; θGCol )1 −k i Zi Results Table 4 compares our two combined models against our individuals models, and our Berkeley parser baseline. We see that the combined models outperform their component models on all tasks. That is, the LOP1st-order"
W11-2916,N07-1051,0,\N,Missing
W11-2916,J93-2004,0,\N,Missing
W11-2916,J93-1005,0,\N,Missing
W11-2916,C94-2195,0,\N,Missing
W11-2916,J03-4003,0,\N,Missing
W11-2916,C98-2172,0,\N,Missing
W11-2916,N09-1069,0,\N,Missing
W13-2203,W12-4204,0,0.54368,"valuate machine translation is not new. Gim´enez and M`arquez (2007) proposed using automatically assigned semantic role labels as a feature in a combined MT metric. The main difference between this application of semantic roles and MEANT is that arguments for specific verbs are taken into account, instead of just applying the subset agent, patient and benefactor. This idea would probably help human annotators to handle sentences with passives, copulas and other constructions which do not easily match the most basic arguments. On the other hand, verb specific arguments are language dependent. Bojar and Wu (2012), applying HMEANT to English-to-Czech MT output, identified a number of problems with HMEANT, and suggested a variety of improvements. In some respects, this work is very similar, except that our goal is to evaluate HMEANT along a range of intrinsic properties, to determine how useful the metric really is to evaluation campaigns such as the workshop on machine translation. 3 Evaluation with HMEANT 3.1 Annotation Procedure The goal of the HMEANT metric is to capture essential semantic content, but still be simple and fast. There are two stages to the annotation, the first of which is semantic r"
W13-2203,W07-0718,1,0.869138,"ion, there is still no consensus on how to evaluate machine translation based on human judgements. (Hutchins and Somers, 1992; Przybocki et al., 2009). One obvious approach is to ask annotators to rate translation candidates on a numerical scale. Under the DARPA TIDES program, the Linguistic Data Consortium (2002) developed an evaluation scheme that relies on two five-point scales representing fluency and adequacy. This was also the human evaluation scheme used in the annual MT competitions sponsored by NIST (2005). In an analysis of human evaluation results for the WMT ’07 workshop, however, Callison-Burch et al. (2007) found high correlation between fluency and adequacy scores assigned by individual annotators, suggesting that human annotators are not able to separate these two evaluation dimensions easily. Furthermore these absolute scores show low inter-annotator agreement. Instead of giving absolute quality assessments, annotators appeared to be using their ratings to rank translation candidates according to their overall preference for one over the other. In line with these findings, Callison-Burch et al. (2007) proposed to let annotators rank translation candidates directly, without asking them to assi"
W13-2203,P11-1023,0,0.125845,"on Street Edinburgh, EH8 9AB, UK Abstract tion might be useful for different purposes. If the MT is going to be the basis of a human translator’s work-flow, then post-editing effort seems like a natural fit. However, for people using MT for gisting, what we really want is some measure of how much meaning has been retained. We clearly need a metric which tries to answer the question, how much of the meaning does the translation capture. In this paper, we explore the use of human evaluation metrics which attempt to capture the extent of this meaning retention. In particular, we consider HMEANT (Lo and Wu, 2011a), a metric that uses semantic role labels to measure how much of the “who, why, when, where” has been preserved. For HMEANT evaluation, annotators are instructed to identify verbs as heads of semantic frames. Then they attach role fillers to the heads and finally they align heads and role fillers in the candidate translation with those in a reference translation. In a series of papers, Lo and Wu (2010, 2011b,a, 2012) explored a number of questions, evaluating HMEANT by using correlation statistics to compare it to judgements of human adequacy and contrastive evaluations. Given the drawbacks"
W13-2203,lo-wu-2010-evaluating,0,0.0158008,"ing does the translation capture. In this paper, we explore the use of human evaluation metrics which attempt to capture the extent of this meaning retention. In particular, we consider HMEANT (Lo and Wu, 2011a), a metric that uses semantic role labels to measure how much of the “who, why, when, where” has been preserved. For HMEANT evaluation, annotators are instructed to identify verbs as heads of semantic frames. Then they attach role fillers to the heads and finally they align heads and role fillers in the candidate translation with those in a reference translation. In a series of papers, Lo and Wu (2010, 2011b,a, 2012) explored a number of questions, evaluating HMEANT by using correlation statistics to compare it to judgements of human adequacy and contrastive evaluations. Given the drawbacks of those evaluation measures, which we discuss in Sec. 2, they could just as well have been evaluating the human adequacy and contrastive judgements using HMEANT. Human evaluation metrics need to be judged on other intrinsic qualities, which we describe below. The aim of this paper is to evaluate the effectiveness of HMEANT, with the goal of using it to judge the relative merits of different MT systems,"
W13-2203,W11-1002,0,0.330392,"on Street Edinburgh, EH8 9AB, UK Abstract tion might be useful for different purposes. If the MT is going to be the basis of a human translator’s work-flow, then post-editing effort seems like a natural fit. However, for people using MT for gisting, what we really want is some measure of how much meaning has been retained. We clearly need a metric which tries to answer the question, how much of the meaning does the translation capture. In this paper, we explore the use of human evaluation metrics which attempt to capture the extent of this meaning retention. In particular, we consider HMEANT (Lo and Wu, 2011a), a metric that uses semantic role labels to measure how much of the “who, why, when, where” has been preserved. For HMEANT evaluation, annotators are instructed to identify verbs as heads of semantic frames. Then they attach role fillers to the heads and finally they align heads and role fillers in the candidate translation with those in a reference translation. In a series of papers, Lo and Wu (2010, 2011b,a, 2012) explored a number of questions, evaluating HMEANT by using correlation statistics to compare it to judgements of human adequacy and contrastive evaluations. Given the drawbacks"
W13-2203,N12-1017,0,0.0331991,"eded by a human to convert the machine translation so as to convey the same meaning as the reference. This type of evaluation is of some use when one is using MT to aid human translation (although the relationship between number of edits and actual effort is not straightforward (Koponen, 2012)), but it is not so helpful when one’s task is gisting. The number of edits need not correlate with the severity of the semantic differences between the two sentences. The loss of a negative, for instance, is only one edit away from the original, but the semantics change completely. Alternatively, HyTER (Dreyer and Marcu, 2012) is an annotation tool which allows a user to create an exponential number of correct translations for a given sentence. These references are then efficiently exploited to compare with machine translation output. The authors argue that the current metrics fail simply because they have access to sets of reference translations which are simply too small. However, the fact is that even if one does have access to large numbers of translations, it is very difficult to determine whether the reference correctly captures the essential semantic content of the references. The idea of using semantic role"
W13-2203,W12-4206,0,0.0163075,"evaluation, as it is essential for building better automatic metrics, and therefore a more fundamental problem. The overall HMEANT score for MT evaluation is computed as the f-score from the counts of matches of frames and their role fillers between the reference and the MT output. Unmatched frames are excluded from the calculation together with all their corresponding roles. In recognition that preservation of some types of semantic relations may be more important than others for a human to understand a sentence, one may want to weight them differently in the computation of the HMEANT score. Lo and Wu (2012) train weights for each role filler type to optimise correlation with human adequacy judgements. As an unsupervised alternative, they suggest weighting roles according to their frequency as approximation to their importance. Since the main focus of the current paper is the annotation of the actions, roles and alignments that HMEANT depends on, we do not explore such different weight-setting schemes, but set the weights uniformly, with the exception of a partial alignment, which is given a weight of 0.5. HMEANT is thus defined as follows: 4 4.1 Systems and Data Sets We performed HMEANT evaluati"
W13-2203,P08-4006,1,0.829538,"espective sentence. They were then presented with the output of several machine translation systems for the same source sentence, one system at a time, with the reference translation and its annotations visible in the left half of the screen (cf. Fig. 1). For each system, the annotators were asked to annotate semantic frames and slot fillers in the translation first, and then align them with frame heads and slot fillers in the human reference translation. Annotations and alignment were performed with Edi-HMEANT2 , a web-based annotation tool for HMEANT that we developed on the basis of Yawat (Germann, 2008). The tool allows the alignment of slots from different semantic frames, and the alignment of slots of different types; however, such alignments are not considered in the computation of the final HMEANT score. The annotation guidelines were essentially those used in Bojar and Wu (2012), with some additional English examples, and a complete set of German examples. For ease of comparison with prior work, we used the same set of semantic role labels as Bojar and Wu (2012), shown in Table 1. Given the restriction that the head of a frame can consist of only one word, a convention was made that all"
W13-2203,W12-3101,0,0.0132441,"02), METEOR (Lavie and Denkowski, 2009) and TER (Snover et al., 2009b), which have greatly accelerated progress in MT research, rely on shallow surface properties of the translations, and only indirectly capture whether or not the translation preserves the meaning. This has meant that 53 sent a subset of the competing systems, and these rankings must be combined with other annotators judgements on five other system outputs to compute an overall ranking. The methodology for interpreting the contrastive evaluations has been the subject of much recent debate in the community (Bojar et al., 2011; Lopez, 2012). There has been some effort to overcome these problems. HTER (Snover et al., 2009a) is a metric which counts the number of edits needed by a human to convert the machine translation so as to convey the same meaning as the reference. This type of evaluation is of some use when one is using MT to aid human translation (although the relationship between number of edits and actual effort is not straightforward (Koponen, 2012)), but it is not so helpful when one’s task is gisting. The number of edits need not correlate with the severity of the semantic differences between the two sentences. The lo"
W13-2203,J05-1004,0,0.0443523,"s used for the WMT shared task, it is still reasonably efficient considering the fine-grained nature of the evaluation. On average, annotators evaluated about 10 sentences per hour. 2 Related Work Even though the idea that machine translation requires a semantic representation of the translated content is as old as the idea of computer-based translation itself (Weaver, 1955), it has not been until recently that people have begun to combine statistical models with semantic representations. Jones et al. (2012), for example, represent meaning as directed acyclic graphs and map these to PropBank (Palmer et al., 2005) style dependencies. To evaluate such approaches properly, we need evaluation metrics that capture the accuracy of the translation. Current automatic metrics of machine translation, such as BLEU (Papineni et al., 2002), METEOR (Lavie and Denkowski, 2009) and TER (Snover et al., 2009b), which have greatly accelerated progress in MT research, rely on shallow surface properties of the translations, and only indirectly capture whether or not the translation preserves the meaning. This has meant that 53 sent a subset of the competing systems, and these rankings must be combined with other annotator"
W13-2203,C12-1083,0,0.0311324,"uage. Efficiency Whilst HMEANT evaluation will never be as fast as, for example, the contrastive judgements used for the WMT shared task, it is still reasonably efficient considering the fine-grained nature of the evaluation. On average, annotators evaluated about 10 sentences per hour. 2 Related Work Even though the idea that machine translation requires a semantic representation of the translated content is as old as the idea of computer-based translation itself (Weaver, 1955), it has not been until recently that people have begun to combine statistical models with semantic representations. Jones et al. (2012), for example, represent meaning as directed acyclic graphs and map these to PropBank (Palmer et al., 2005) style dependencies. To evaluate such approaches properly, we need evaluation metrics that capture the accuracy of the translation. Current automatic metrics of machine translation, such as BLEU (Papineni et al., 2002), METEOR (Lavie and Denkowski, 2009) and TER (Snover et al., 2009b), which have greatly accelerated progress in MT research, rely on shallow surface properties of the translations, and only indirectly capture whether or not the translation preserves the meaning. This has mea"
W13-2203,P02-1040,0,0.101424,"a that machine translation requires a semantic representation of the translated content is as old as the idea of computer-based translation itself (Weaver, 1955), it has not been until recently that people have begun to combine statistical models with semantic representations. Jones et al. (2012), for example, represent meaning as directed acyclic graphs and map these to PropBank (Palmer et al., 2005) style dependencies. To evaluate such approaches properly, we need evaluation metrics that capture the accuracy of the translation. Current automatic metrics of machine translation, such as BLEU (Papineni et al., 2002), METEOR (Lavie and Denkowski, 2009) and TER (Snover et al., 2009b), which have greatly accelerated progress in MT research, rely on shallow surface properties of the translations, and only indirectly capture whether or not the translation preserves the meaning. This has meant that 53 sent a subset of the competing systems, and these rankings must be combined with other annotators judgements on five other system outputs to compute an overall ranking. The methodology for interpreting the contrastive evaluations has been the subject of much recent debate in the community (Bojar et al., 2011; Lop"
W13-2203,W12-3123,0,0.0290208,"ts to compute an overall ranking. The methodology for interpreting the contrastive evaluations has been the subject of much recent debate in the community (Bojar et al., 2011; Lopez, 2012). There has been some effort to overcome these problems. HTER (Snover et al., 2009a) is a metric which counts the number of edits needed by a human to convert the machine translation so as to convey the same meaning as the reference. This type of evaluation is of some use when one is using MT to aid human translation (although the relationship between number of edits and actual effort is not straightforward (Koponen, 2012)), but it is not so helpful when one’s task is gisting. The number of edits need not correlate with the severity of the semantic differences between the two sentences. The loss of a negative, for instance, is only one edit away from the original, but the semantics change completely. Alternatively, HyTER (Dreyer and Marcu, 2012) is an annotation tool which allows a user to create an exponential number of correct translations for a given sentence. These references are then efficiently exploited to compare with machine translation output. The authors argue that the current metrics fail simply bec"
W13-2203,W09-0441,0,0.049194,"translated content is as old as the idea of computer-based translation itself (Weaver, 1955), it has not been until recently that people have begun to combine statistical models with semantic representations. Jones et al. (2012), for example, represent meaning as directed acyclic graphs and map these to PropBank (Palmer et al., 2005) style dependencies. To evaluate such approaches properly, we need evaluation metrics that capture the accuracy of the translation. Current automatic metrics of machine translation, such as BLEU (Papineni et al., 2002), METEOR (Lavie and Denkowski, 2009) and TER (Snover et al., 2009b), which have greatly accelerated progress in MT research, rely on shallow surface properties of the translations, and only indirectly capture whether or not the translation preserves the meaning. This has meant that 53 sent a subset of the competing systems, and these rankings must be combined with other annotators judgements on five other system outputs to compute an overall ranking. The methodology for interpreting the contrastive evaluations has been the subject of much recent debate in the community (Bojar et al., 2011; Lopez, 2012). There has been some effort to overcome these problems."
W13-2203,Y12-1062,0,0.0303318,"Missing"
W13-2203,W07-0738,0,\N,Missing
W13-2203,W12-3129,0,\N,Missing
W13-2203,P13-1023,0,\N,Missing
W13-2203,W11-2101,0,\N,Missing
W15-3013,D14-1132,0,0.0417768,"igated sparse lexicalized reordering features (Section 2.4) on the German-English language pair in both translation directions. Two methods for learning the weights of the sparse lexicalized reordering feature set have been compared: (1.) direct tuning in MIRA along with all other features in the model combination (sparse LR (MIRA)), and (2.) separate optimization with stochastic gradient descent (SGD) with a maximum expected B LEU objective (sparse LR (SGD)). For the latter variant, we used the MT tuning set for training (13 573 sentence pairs) and otherwise followed the approach outlined by Auli et al. (2014). We tuned the baseline feature weights with MIRA before SGD training and ran two final MIRA iterations after it. SGD training was stopped after 80 epochs. Empirical results for the German-English language pair are presented in Table 5. We observe minor gains of up to +0.2 points B LEU. The results are not consistent in the two translation directions: The MIRA-trained variant seems to perform better when translating from German, the SGD-trained variant when translating to German. However, in both cases the baseline score is almost identical to the best results with sparse lexicalized reorderin"
W15-3013,D11-1033,0,0.0915467,"Missing"
W15-3013,N12-1047,0,0.185989,"Missing"
W15-3013,N13-1003,0,0.0324335,"a to train 5gram language models with modified Kneser-Ney smoothing (Chen and Goodman, 1998). Typically, language models for each monolingual corpus were first trained using either KenLM (Heafield et al., 2013) or the SRILM toolkit (Stolcke, 2002) and then linearly interpolated using weights tuned to minimize perplexity on the development set. 3.4 Sparse Lexicalized Reordering Baseline Features We follow the standard approach to SMT of scoring translation hypotheses using a weighted linear combination of features. The core features of our We implemented sparse lexicalized reordering features (Cherry, 2013) in Moses and evaluated 127 Baseline (no clusters) Comprehensive setup w/o sparse features w/o language model w/o reordering model w/o operation sequence model de-en 28.0 28.5 (+.5) 28.2 (–.3) 28.3 (–.2) 28.5 (±.0) 28.3 (–.2) en-de 20.5 20.5 (±.0) 20.4 (–.1) 20.5 (±.0) 20.5 (±.0) 20.3 (–.1) cs-en 29.1 29.7 (+.6) 29.6 (–.1) 29.5 (–.2) 29.7 (±.0) en-cs 21.2 21.8 (+.6) 21.7 (–.1) 21.4 (–.4) 21.8 (±.0) 21.7 (–.1) ru-en 31.8 32.3 (+.5) 32.2 (–.1) 31.5 (–.8) 32.3 (±.0) 32.0 (–.3) en-ru 29.1 29.7 (+.6) 30.0 (+.3) 29.2 (–.6) 29.8 (+.1) 29.5 (–.2) avg ∆ +.5 –.2 –.4 ±.0 –.2 Table 1: Use of additional fe"
W15-3013,P05-1066,1,0.679903,"the OPUS (Tiedemann, 2012) par129 4.3 System Baseline Submitted BiLM source & combined & NPLM Czech↔English The development of the Czech↔English systems followed the ideas in Section 2.3, i.e., with a focus on word classes (50, 200, 600 classes) for all component models. We combined the test sets from 2008 to 2012 for tuning. No neural language model or bilingual language model was used. 4.4 Table 4: Experimental results (cased B LEU) for English→Russian averaged over newstest2013 and newstest2014. Russian↔English From German. For translation from German, we applied syntactic pre-reordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) in a preprocessing step on the source side. A rich set of translation factors was exploited in addition to word surface forms: Och clusters (50 classes), morphological tags, partof-speech tags, and word stems on the German side (Schmid, 2000), as well as Och clusters (50 classes), part-of-speech tags (Ratnaparkhi, 1996), and word stems (Porter, 1980) on the English side. The factors were utilized in the translation model and in OSMs. The lexicalized reordering model was trained on stems. Individual 7gram Och cluster LMs were trained with KenLM’s"
W15-3013,2014.iwslt-evaluation.7,1,0.858337,"mission we tested bilingual LMs on the French↔English tasks and on English→Russian task. For French↔English, we had resource issues4 in training such large Introduction The Edinburgh/JHU phrase-based translation systems for our participation in the WMT 2015 shared translation task1 are based on the open source Moses toolkit (Koehn et al., 2007). We built upon Edinburgh’s strong baselines from WMT submissions in previous years (Durrani et al., 2014a) as well as our recent research within the framework of other evaluation campaigns and projects such as IWSLT2 and EU-BRIDGE3 (Birch et al., 2014; Freitag et al., 2014a; Freitag et al., 2014b). We first discuss novel features that we integrated into our systems for the 2015 Edinburgh/JHU submission. Next we give a general system overview with details on our training pipeline and decoder configuration. We finally present empirical results for the individual language pairs and translation directions. 1 http://www.statmt.org/wmt15/ http://workshop2014.iwslt.org 3 http://www.eu-bridge.eu 4 These can now be addressed using the -mmap option to create a binarized version of the corpus which is then memory-mapped. 2 126 Proceedings of the Tenth Workshop on Statisti"
W15-3013,P14-1129,0,0.0621031,"Missing"
W15-3013,P13-2071,1,0.880126,"-based feature functions are used in all cases. B LEU scores on newstest2014 are reported. 4 model are a 5-gram LM score, phrase translation and lexical translation scores, word and phrase penalties, and a linear distortion score. The phrase translation probabilities are smoothed with GoodTuring smoothing (Foster et al., 2006). We used the hierarchical lexicalized reordering model (Galley and Manning, 2008) with 4 possible orientations (monotone, swap, discontinuous left and discontinuous right) in both left-to-right and rightto-left direction. We also used the operation sequence model (OSM) (Durrani et al., 2013) with 4 count based supportive features. We further employed domain indicator features (marking which training corpus each phrase pair was found in), binary phrase count indicator features, sparse phrase length features, and sparse source word deletion, target word insertion, and word translation features (limited to the top K words in each language, typically with K = 50). 3.5 In this section we describe peculiarities of individual systems and present experimental results. 4.1 French↔English Our submitted systems for the French-English language pair are quite similar for the two translation d"
W15-3013,D08-1089,0,0.29008,"ature functions based on Och clusters (see Section 2.3). The last four lines refer to ablation studies where one of the sets of clustered feature functions is removed from the comprehensive setup. Note that the word-based feature functions are used in all cases. B LEU scores on newstest2014 are reported. 4 model are a 5-gram LM score, phrase translation and lexical translation scores, word and phrase penalties, and a linear distortion score. The phrase translation probabilities are smoothed with GoodTuring smoothing (Foster et al., 2006). We used the hierarchical lexicalized reordering model (Galley and Manning, 2008) with 4 possible orientations (monotone, swap, discontinuous left and discontinuous right) in both left-to-right and rightto-left direction. We also used the operation sequence model (OSM) (Durrani et al., 2013) with 4 count based supportive features. We further employed domain indicator features (marking which training corpus each phrase pair was found in), binary phrase count indicator features, sparse phrase length features, and sparse source word deletion, target word insertion, and word translation features (limited to the top K words in each language, typically with K = 50). 3.5 In this"
W15-3013,W08-0509,0,0.0395025,"morphological tags, and basically no additional gains were observed due to the class based feature functions. 2.4 3 3.1 System Overview Preprocessing The training data was preprocessed using scripts from the Moses toolkit. We first normalized the data using the normalize-punctuation.perl script, then performed tokenization (using the -a option), and then truecasing. We did not perform any corpus filtering other than the standard Moses method, which removes sentence pairs with extreme length ratios. 3.2 Word Alignment For word alignment we used either fast_align (Dyer et al., 2013) or MGIZA++ (Gao and Vogel, 2008), followed by the standard grow-diag-final-and symmetrization heuristic. An empirical comparison of fast_align and MGIZA++ on the FinnishEnglish and English-Russian language pairs using the constrained data sets did not reveal any significant difference. 3.3 Language Model We used all available monolingual data to train 5gram language models with modified Kneser-Ney smoothing (Chen and Goodman, 1998). Typically, language models for each monolingual corpus were first trained using either KenLM (Heafield et al., 2013) or the SRILM toolkit (Stolcke, 2002) and then linearly interpolated using weig"
W15-3013,W14-3309,1,0.921919,"at test time the alignment is supplied by the decoder. The bilingual LM is trained using a feedforward neural network and we use the NPLM toolkit for this. Prior to submission we tested bilingual LMs on the French↔English tasks and on English→Russian task. For French↔English, we had resource issues4 in training such large Introduction The Edinburgh/JHU phrase-based translation systems for our participation in the WMT 2015 shared translation task1 are based on the open source Moses toolkit (Koehn et al., 2007). We built upon Edinburgh’s strong baselines from WMT submissions in previous years (Durrani et al., 2014a) as well as our recent research within the framework of other evaluation campaigns and projects such as IWSLT2 and EU-BRIDGE3 (Birch et al., 2014; Freitag et al., 2014a; Freitag et al., 2014b). We first discuss novel features that we integrated into our systems for the 2015 Edinburgh/JHU submission. Next we give a general system overview with details on our training pipeline and decoder configuration. We finally present empirical results for the individual language pairs and translation directions. 1 http://www.statmt.org/wmt15/ http://workshop2014.iwslt.org 3 http://www.eu-bridge.eu 4 These"
W15-3013,P13-2121,1,0.902744,"Missing"
W15-3013,P07-1019,0,0.0483912,"ed language models, we tested with 50 Och clusters, 200 Och clusters, and with both class-based LMs. For the bilingual LM, we created both “combined” (a 5-gram on the target and a 9-gram on the source) and “source” (1-gram on the target and 15-gram on Tuning Since our feature set (generally around 500 to 1000 features) was too large for MERT, we used k-best batch MIRA for tuning (Cherry and Foster, 2012). To speed up tuning we applied threshold pruning to the phrase table, based on the direct translation model probability. 3.6 Experimental Results Decoding In decoding we applied cube pruning (Huang and Chiang, 2007) with a stack size of 5000 (reduced to 1000 for tuning), Minimum Bayes Risk decoding (Kumar and Byrne, 2004), a maximum phrase length of 5, a distortion limit of 6, 100best translation options and the no-reorderingover-punctuation heuristic (Koehn and Haddow, 2009). 128 System Baseline Submitted 50 classes 200 classes 50+200 classes BiLM combined BiLM source & combined NPLM fr-en 33.0 32.7 32.8 32.9 32.9 32.9 33.2 33.0 System Baseline Submitted Without OPUS 50 classes 200 classes 50+200 classes BiLM combined BiLM source & combined NPLM en-fr 33.5 33.6 33.8 33.9 33.7 33.6 33.5 34.2 Table 2: Com"
W15-3013,E14-4029,1,0.939408,"at test time the alignment is supplied by the decoder. The bilingual LM is trained using a feedforward neural network and we use the NPLM toolkit for this. Prior to submission we tested bilingual LMs on the French↔English tasks and on English→Russian task. For French↔English, we had resource issues4 in training such large Introduction The Edinburgh/JHU phrase-based translation systems for our participation in the WMT 2015 shared translation task1 are based on the open source Moses toolkit (Koehn et al., 2007). We built upon Edinburgh’s strong baselines from WMT submissions in previous years (Durrani et al., 2014a) as well as our recent research within the framework of other evaluation campaigns and projects such as IWSLT2 and EU-BRIDGE3 (Birch et al., 2014; Freitag et al., 2014a; Freitag et al., 2014b). We first discuss novel features that we integrated into our systems for the 2015 Edinburgh/JHU submission. Next we give a general system overview with details on our training pipeline and decoder configuration. We finally present empirical results for the individual language pairs and translation directions. 1 http://www.statmt.org/wmt15/ http://workshop2014.iwslt.org 3 http://www.eu-bridge.eu 4 These"
W15-3013,N13-1073,0,0.0525479,"ure functions based on POS and morphological tags, and basically no additional gains were observed due to the class based feature functions. 2.4 3 3.1 System Overview Preprocessing The training data was preprocessed using scripts from the Moses toolkit. We first normalized the data using the normalize-punctuation.perl script, then performed tokenization (using the -a option), and then truecasing. We did not perform any corpus filtering other than the standard Moses method, which removes sentence pairs with extreme length ratios. 3.2 Word Alignment For word alignment we used either fast_align (Dyer et al., 2013) or MGIZA++ (Gao and Vogel, 2008), followed by the standard grow-diag-final-and symmetrization heuristic. An empirical comparison of fast_align and MGIZA++ on the FinnishEnglish and English-Russian language pairs using the constrained data sets did not reveal any significant difference. 3.3 Language Model We used all available monolingual data to train 5gram language models with modified Kneser-Ney smoothing (Chen and Goodman, 1998). Typically, language models for each monolingual corpus were first trained using either KenLM (Heafield et al., 2013) or the SRILM toolkit (Stolcke, 2002) and then"
W15-3013,E03-1076,1,0.79605,"Baseline Submitted BiLM source & combined & NPLM Czech↔English The development of the Czech↔English systems followed the ideas in Section 2.3, i.e., with a focus on word classes (50, 200, 600 classes) for all component models. We combined the test sets from 2008 to 2012 for tuning. No neural language model or bilingual language model was used. 4.4 Table 4: Experimental results (cased B LEU) for English→Russian averaged over newstest2013 and newstest2014. Russian↔English From German. For translation from German, we applied syntactic pre-reordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) in a preprocessing step on the source side. A rich set of translation factors was exploited in addition to word surface forms: Och clusters (50 classes), morphological tags, partof-speech tags, and word stems on the German side (Schmid, 2000), as well as Och clusters (50 classes), part-of-speech tags (Ratnaparkhi, 1996), and word stems (Porter, 1980) on the English side. The factors were utilized in the translation model and in OSMs. The lexicalized reordering model was trained on stems. Individual 7gram Och cluster LMs were trained with KenLM’s --discount_fallback --prune '0 0 1' parameters,"
W15-3013,P07-2045,1,0.00919205,"gned source token. At training time, the aligned source token is found from the automatic alignment, and at test time the alignment is supplied by the decoder. The bilingual LM is trained using a feedforward neural network and we use the NPLM toolkit for this. Prior to submission we tested bilingual LMs on the French↔English tasks and on English→Russian task. For French↔English, we had resource issues4 in training such large Introduction The Edinburgh/JHU phrase-based translation systems for our participation in the WMT 2015 shared translation task1 are based on the open source Moses toolkit (Koehn et al., 2007). We built upon Edinburgh’s strong baselines from WMT submissions in previous years (Durrani et al., 2014a) as well as our recent research within the framework of other evaluation campaigns and projects such as IWSLT2 and EU-BRIDGE3 (Birch et al., 2014; Freitag et al., 2014a; Freitag et al., 2014b). We first discuss novel features that we integrated into our systems for the 2015 Edinburgh/JHU submission. Next we give a general system overview with details on our training pipeline and decoder configuration. We finally present empirical results for the individual language pairs and translation d"
W15-3013,W06-1607,0,0.204091,"29.8 (+.1) 29.5 (–.2) avg ∆ +.5 –.2 –.4 ±.0 –.2 Table 1: Use of additional feature functions based on Och clusters (see Section 2.3). The last four lines refer to ablation studies where one of the sets of clustered feature functions is removed from the comprehensive setup. Note that the word-based feature functions are used in all cases. B LEU scores on newstest2014 are reported. 4 model are a 5-gram LM score, phrase translation and lexical translation scores, word and phrase penalties, and a linear distortion score. The phrase translation probabilities are smoothed with GoodTuring smoothing (Foster et al., 2006). We used the hierarchical lexicalized reordering model (Galley and Manning, 2008) with 4 possible orientations (monotone, swap, discontinuous left and discontinuous right) in both left-to-right and rightto-left direction. We also used the operation sequence model (OSM) (Durrani et al., 2013) with 4 count based supportive features. We further employed domain indicator features (marking which training corpus each phrase pair was found in), binary phrase count indicator features, sparse phrase length features, and sparse source word deletion, target word insertion, and word translation features"
W15-3013,W14-3310,1,0.838548,"mission we tested bilingual LMs on the French↔English tasks and on English→Russian task. For French↔English, we had resource issues4 in training such large Introduction The Edinburgh/JHU phrase-based translation systems for our participation in the WMT 2015 shared translation task1 are based on the open source Moses toolkit (Koehn et al., 2007). We built upon Edinburgh’s strong baselines from WMT submissions in previous years (Durrani et al., 2014a) as well as our recent research within the framework of other evaluation campaigns and projects such as IWSLT2 and EU-BRIDGE3 (Birch et al., 2014; Freitag et al., 2014a; Freitag et al., 2014b). We first discuss novel features that we integrated into our systems for the 2015 Edinburgh/JHU submission. Next we give a general system overview with details on our training pipeline and decoder configuration. We finally present empirical results for the individual language pairs and translation directions. 1 http://www.statmt.org/wmt15/ http://workshop2014.iwslt.org 3 http://www.eu-bridge.eu 4 These can now be addressed using the -mmap option to create a binarized version of the corpus which is then memory-mapped. 2 126 Proceedings of the Tenth Workshop on Statisti"
W15-3013,P10-2041,0,0.0404632,"Missing"
W15-3013,E99-1010,0,0.349917,"Missing"
W15-3013,2014.amta-researchers.3,0,0.061645,"rved a small improvement in translation performance. 2.3 Comprehensive Use of Word Classes In Edinburgh’s submission from the previous year, we used automatically generated word classes in additional language models and in additional operation sequence models (Durrani et al., 2014b). This year, we pushed the use of word classes into the remaining feature functions: the reordering model and the sparse word features. We generated Och clusters (Och, 1999) — a variant of Brown clusters — using mkcls. We have to choose a hyper parameter: the number of clusters. Our experiments and also prior work (Stewart et al., 2014) suggest that instead of committing to a single value, it is beneficial to use multiple numbers and use them in multiple feature functions concurrently. We used 50, 200, 600, and 2000 clusters, hence having 4 additional interpolated language models, 4 additional operation sequence models, 4 additional lexicalized reordering models, and 4 additional sets of sparse features. The feature functions for word classes were trained exactly the same way as the corresponding feature functions for words. For instance, this means that the word class language model required training of individual models on"
W15-3013,tiedemann-2012-parallel,0,0.0549009,"ploying dropout to prevent overfitting (Srivastava et al., 2014), enabling us to train the models for at least 2 epochs. We note that, as with French↔English, our application of bilingual LM did not result in significant improvement. Finnish and English are quite distantly related, but we can speculate that using words as a representation for Finnish is not appropriate. The NPLM, however, offers modest (+0.4) improvements over the baseline in both directions. Finnish↔English For the Finnish-English language pair we built systems using only the constrained data, and systems using all the OPUS (Tiedemann, 2012) par129 4.3 System Baseline Submitted BiLM source & combined & NPLM Czech↔English The development of the Czech↔English systems followed the ideas in Section 2.3, i.e., with a focus on word classes (50, 200, 600 classes) for all component models. We combined the test sets from 2008 to 2012 for tuning. No neural language model or bilingual language model was used. 4.4 Table 4: Experimental results (cased B LEU) for English→Russian averaged over newstest2013 and newstest2014. Russian↔English From German. For translation from German, we applied syntactic pre-reordering (Collins et al., 2005) and c"
W15-3013,D13-1140,0,0.0464299,"n of the University of Edinburgh and the Johns Hopkins University for the shared translation task of the EMNLP 2015 Tenth Workshop on Statistical Machine Translation (WMT 2015). We set up phrase-based statistical machine translation systems for all ten language pairs of this year’s evaluation campaign, which are English paired with Czech, Finnish, French, German, and Russian in both translation directions. 2.1 Neural Network LM with NPLM For some language pairs (notably French↔English and Finnish↔English) we experimented with feed-forward neural network language models using the NPLM toolkit (Vaswani et al., 2013). This toolkit enables such language models to be trained efficiently on large datasets, and provides a querying API which is fast enough to be used during decoding. NPLM is fully integrated into Moses, including appropriate wrapper scripts for training the language models within the Moses experiment management system. Novel research directions we investigated include: neural network language models and bilingual neural network language models, a comprehensive use of word classes, and sparse lexicalized reordering features. 1 Novel Methods 2.2 Bilingual Neural Network LM We also experimented w"
W15-3013,W15-3024,1,0.849652,"not the English. In fact French→English was the only language pair where NPLM did not improve B LEU after building the LM on all data. It is possible that the limited morphology of English means that the improved generalisation of the NPLM is not as helpful, and also that the conventional n-gram LM is already strong for this language pair. 4.2 fi-en 19.6 19.7 17.0 19.4 19.8 19.7 19.1 19.1 20.0 allel data. Our baselines include this extra data, but we also show results just using the constrained parallel data. We did not employ the morphological splitting as in Edinburgh’s syntax-based system (Williams et al., 2015) and consequently the English→Finnish systems performed poorly in development and we did not submit a phrase-based system for this pair. Our development setup was similar to French↔English; we used the newsdev2015 for tuning and test during system development (in 2-fold cross-validation) then for the submission and subsequent experiments we used the whole of newsdev2015 for tuning. Also in common with our work on French↔English, we performed several post-submission experiments to examine the effect of class-based language models, bilingual LM and NPLM. We show the results in Table 3. For train"
W15-3013,W09-0429,1,\N,Missing
W15-3013,N04-1022,0,\N,Missing
W15-3013,2014.iwslt-evaluation.6,1,\N,Missing
W15-3013,C14-1041,1,\N,Missing
W15-3013,W14-3324,1,\N,Missing
W16-2204,P13-2074,0,0.0218328,"a lexical semantic taxonomy and clustering words based on cooccurrences within a window or syntactic features extracted from dependency-parsed data. Modeling reordering and deletion of semantic roles (Wu and Fung, 2009; Liu and Gildea, 2010; Li et al., 2013) has been another line of research on improving translation of predicate-argument structures. Liu and Gildea (2010) propose modeling reordering of a complete semantic frame while Li et al. (2013) propose finer grained features that distinguish between predicate-argument reordering and argument-argument reordering. Gao and Vogel (2011) and Bazrafshan and Gildea (2013) annotate target non-terminals with the semantic roles they cover in order to extract synchronous grammar rules that cover the entire predicate argument structure. These models however do not encode information about the lexical semantic affinities between target predicates and their arguments. In this work we focus on using selectional preference over predicate and arguments in the target as this is a simple way of leveraging external knowledge in the translation framework. 3 3.1 SelP ref (p, r) = KL(P (c|p, r) k P (c|r)) X P (c|p, r) = P (c|p, r)log P (c|r) c (1) where KL is the Kullback - L"
W16-2204,D15-1158,0,0.0306369,"Missing"
W16-2204,N12-1047,0,0.0390144,"ated sentence, its dependency tree produced by the string-to-tree system and the triples extracted at decoding time. We consider the following main arguments: nsubj, nsubjpass, dobj, iobj and prep arguments attached to both verbs and nouns. Table 4 shows the number of extracted triples. Type of relation main prep nsubj nsubjpass dobj iobj the harmonic mean of precision and recall over head-word chains of length 1 to 4. The head-word chains are extracted directly from the dependency tree produced by the string-to-tree decoder and from the parsed reference. Tuning is performed using batch MIRA (Cherry and Foster, 2012) on 1000-best lists. We report evaluation scores averaged over the newstest2013, newstest2014 and newstest2015 data sets provided by WMT15. 5 5.1 Error analysis We wanted to get an idea about how often the verb and its arguments are mistranslated. For this purpose we manually annotated errors in sentences with more than 5 words and at most 15 words. With this criterion we avoided translations with scrambled predicate-argument structures. Each sentence had roughly one main verb. To have a more reliable error annotation we first post-edited 100 translations from the baseline system. We then comp"
W16-2204,N10-1000,0,0.0597296,"Missing"
W16-2204,D12-1096,0,0.0528117,"Missing"
W16-2204,W12-3308,0,0.0157896,"istributional semantics SelAssoc(p, r, c) = P (c|p, r)log PP(c|p,r) (c|r) SelStr(p, r) (2) We give examples of the selectional preference strength and selectional association scores for different verbs and their arguments in Table 2. The verb see takes on many arguments as direct objects and therefore has a lower selectional preference strength for this syntactic relation. In contrast the predicate hereditary takes on fewer arguments for which it has a stronger selectional preference. Several selectional preference models have been used as features in discriminative syntactic parsing systems. Cohen et al. (2012) observe 34 Verb see Relation dobj SelPref 0.56 is–hereditary nsubj 1.69 drink dobj 3.90 Argument PRN movie episode disease monarchy title water wine glass SelAssoc 0.123 0.022 0.001 0.267 0.148 0.082 0.144 0.061 0.027 Table 2: Example of selectional preference (SelPref) and selectional association (SelAssoc) scores for different verbs. PRN is the class of pronouns. propriate for a machine translation task where the vocabulary has millions of words and English is not the only targeted language. Therefore we adapt Resnik’s selectional association measure in two ways. that when parsing out-of-do"
W16-2204,N13-1060,0,0.0277343,"Missing"
W16-2204,D14-1004,0,0.0228193,"Missing"
W16-2204,J10-4007,0,0.0190829,"Missing"
W16-2204,W05-0904,0,0.0221325,"26 percent of the verbs are mistranslated and about 10 percent of the arguments. Mistranslated verbs are problematic since the feature produces the selectional association scores for the wrong verb. Although the semantic affinity is mutual, the formulation of the score conditions on the verb. In the cases when both the verb and the argument are mistranslated the association score might be high although the translation is not faithful to the source. For both tuning and evaluation of all machine translation systems we use a combination of the cased BLEU score and head-word chain metric (HWCM ) (Liu and Gildea, 2005). The HWCM metric implemented in the Moses toolkit computes 4 Evaluation Coordination is not resolved at decoding time. 37 5.2 Evaluation of the Selectional Preference Feature not improve the evaluation scores as shown in the fifth row of Table 6. The lack of variance in automatic evaluation scores can be explained by: a) the feature touches only a few words in the translation and b) the relation between a predicate and its argument is identified at later stages of the bottom-up chart-based decoding when many lexical choices have already been pruned out. The SelAssoc scores, similar to mutual"
W16-2204,P06-1121,0,0.125949,"Missing"
W16-2204,C10-1081,0,0.138729,"lack of affinity. presents the results of automatic evaluation as well as a qualitative analysis of the machine translated output. their core and prepositional arguments as well as selectional preferences of nouns for their prepositional arguments. Previous work has addressed the selectional preferences of prepositions for noun classes (Weller et al., 2014) but not the semantic affinities between a predicate and its argument class. Another line of research on improving translation of predicate-argument structures includes modeling reordering and deletion of semantic roles (Wu and Fung, 2009; Liu and Gildea, 2010; Li et al., 2013). These models however do not encode information about the lexical semantic affinities between target predicates and their arguments. Sennrich (2015) proposes a relational dependency language model (RDLM) for string-to-tree machine translation. One component of RDLM predicts the head word of a dependent conditioned on a wide syntactic context. Our feature is different as it quantifies the amount of information that the predicate carries about the argument class filling a particular syntactic function. 2 Related work From a syntactic perspective, a correct predicate-argument s"
W16-2204,D14-1162,0,0.08159,"lization capabilities as a measure of distributional similarity between word embeddings. van Noord (2007) has shown that bilexical association scores computed using PMI for all types of dependency relations are a useful feature for improving dependency parsing in Dutch. 3.2 In the first model SelAssoc L we compute the co-occurrence statistics defined in Eq 2 over lemmas of the predicate and argument head words. In the second model SelAssoc C we replace the WordNet classes in Eq 2 with word clusters1 . We obtain the word clusters by applying the k-means algorithm to the glovec word embeddings (Pennington et al., 2014). Prepositional phrase attachment remains a frequent and challenging error for syntactic parsers (Kummerfeld et al., 2012) and translation of prepositions is a challenge for SMT (Weller et al., 2014). Therefore we decide to use two separate features: one for main arguments (nsubj, nsubjpass, dobj, iobj) and one for prepositional arguments. Adaptation of Selectional Preference Models for Syntax-Based Machine Translation. We are interested in modeling selectional preferences of verbs for their core and prepositional arguments as well as selectional preferences of nouns for their prepositional ar"
W16-2204,P13-1058,0,0.0139115,"lexical semantic affinities between target predicates and their arguments. Sennrich (2015) proposes a relational dependency language model (RDLM) for string-to-tree machine translation. One component of RDLM predicts the head word of a dependent conditioned on a wide syntactic context. Our feature is different as it quantifies the amount of information that the predicate carries about the argument class filling a particular syntactic function. 2 Related work From a syntactic perspective, a correct predicate-argument structure will have the sub-categorization frame of the predicate filled in. Weller et al. (2013) use sub-categorization information to improve case-prediction for noun phrases when translating into German. Case prediction for noun phrases is important in the German language as it indicates the grammatical function. Their approach however did not produce strong improvements over the baseline. From a large corpus annotated with dependency relations, they extract verb-noun tuples and their associated syntactic functions: direct object, indirect object, subject. They also extract triples of verb-preposition-noun in order to predict the case of noun-phrases within prepositionalphrases. The pr"
W16-2204,P10-1044,0,0.017205,"l as an increase in precision for verb translation. However the features generally did not improve automatic evaluation metrics. We conclude that mistranslated verbs, errors in the target syntactic trees produced by the decoder and underspecified syntactic relations are negatively impacting these features. The paper is structured as follows. Section 2 describes related work on improving translation of predicate-argument structures. Section 3 introduces the selectional preference feature. Section 4 describes the experimental setup and Section 5 33 similarity (Erk et al., 2010; S´eaghdha, 2010; Ritter et al., 2010), clustering (Sun and Korhonen, 2009), multi-modal datasets (Shutova et al., 2015), and neural networks (Cruys, 2014). Our feature is based on the measure proposed by Resnik (1996). It uses unsupervised clusters to generalize over seen arguments. Resnik (1996) uses selectional preferences of predicates for word sense disambiguation. The information theoretic measure for selectional preference proposed by Resnik quantifies the difference between the posterior distribution of an argument class given the verb and the prior distribution of the class. For instance, ”person” has a higher prior proba"
W16-2204,P10-1045,0,0.056561,"Missing"
W16-2204,2014.amta-researchers.21,0,0.43219,"1: Examples of errors in the predicate-argument structure produced by a syntax-based MT system. a) mistranslated verb b) mistranslated noun. Semantic affinity scores are shown on the right. Higher scores indicate a stronger affinity. Negative scores indicate a lack of affinity. presents the results of automatic evaluation as well as a qualitative analysis of the machine translated output. their core and prepositional arguments as well as selectional preferences of nouns for their prepositional arguments. Previous work has addressed the selectional preferences of prepositions for noun classes (Weller et al., 2014) but not the semantic affinities between a predicate and its argument class. Another line of research on improving translation of predicate-argument structures includes modeling reordering and deletion of semantic roles (Wu and Fung, 2009; Liu and Gildea, 2010; Li et al., 2013). These models however do not encode information about the lexical semantic affinities between target predicates and their arguments. Sennrich (2015) proposes a relational dependency language model (RDLM) for string-to-tree machine translation. One component of RDLM predicts the head word of a dependent conditioned on a"
W16-2204,W12-3150,1,0.857394,"ng arc at position i, RDLM is defined as: 1 35 We have not done experiments with WordNet classes. root det nn NNP DT NNP Minister the Prime met prep IN of pobj NNP cc conj:and India CC NNP and Japan punct prep VBD nsubj IN pobj . in NNP . Tokyo relation nsubj prep in prep of predicate met met Minister argument Minister Tokyo India Figure 1: Example of a translation and its dependency tree in constituency representation produced by the string-to-tree SMT system. Triples extracted during decoding are shown on the right. P (S, D) ≈ n Y i=1 menting GHKM rule extraction (Galley et al., 2004, 2006; Williams and Koehn, 2012). The string-to-tree translation model is based on a synchronous context-free grammar (SCFG) that is extracted from word-aligned parallel data with target-side syntactic annotation. The system was trained on all available data provided at WMT15 2 (Bojar et al., 2015). The number of sentences in the training, tuning and test sets are shown in Table 3. We use the following rule extraction parameters: Rule Depth = 5, Node Count = 20, Rule Size = 5. At decoding time we give a high penalty to glue rules and allow non-terminals to span a maximum of 50 words. We train a 5-gram language model on all a"
W16-2204,Q15-1013,0,0.0878161,"per explores whether knowledge about semantic affinities between the target predicates and their argument fillers is useful for translating ambiguous predicates and arguments. We propose a selectional preference feature based on the selectional association measure of Resnik (1996) and integrate it in a string-to-tree decoder. The feature models selectional preferences of verbs for their core and prepositional arguments as well as selectional preferences of nouns for their prepositional arguments. We compare our features with a variant of the neural relational dependency language model (RDLM) (Sennrich, 2015) and find that neither of the features improves automatic evaluation metrics. We conclude that mistranslated verbs, errors in the target syntactic trees produced by the decoder and underspecified syntactic relations are negatively impacting these features. 1 root → hRB ∼0 V BZ ∼1 sich nsubj ∼2 prep∼3 , RB ∼0 nsubj ∼2 V BZ ∼1 prep∼3 i This rule is useful for reordering the verb and its arguments according to the target side word order. However the rule does not contain the lexical head for the verb, the subject and the prepositional modifier. Therefore the entire predicate argument structure is"
W16-2204,N09-2004,0,0.0606923,"e scores indicate a lack of affinity. presents the results of automatic evaluation as well as a qualitative analysis of the machine translated output. their core and prepositional arguments as well as selectional preferences of nouns for their prepositional arguments. Previous work has addressed the selectional preferences of prepositions for noun classes (Weller et al., 2014) but not the semantic affinities between a predicate and its argument class. Another line of research on improving translation of predicate-argument structures includes modeling reordering and deletion of semantic roles (Wu and Fung, 2009; Liu and Gildea, 2010; Li et al., 2013). These models however do not encode information about the lexical semantic affinities between target predicates and their arguments. Sennrich (2015) proposes a relational dependency language model (RDLM) for string-to-tree machine translation. One component of RDLM predicts the head word of a dependent conditioned on a wide syntactic context. Our feature is different as it quantifies the amount of information that the predicate carries about the argument class filling a particular syntactic function. 2 Related work From a syntactic perspective, a correc"
W16-2204,P15-1092,0,0.0231686,"Missing"
W16-2204,P11-4010,0,0.0122939,"wstest2015 data sets provided by WMT15. 5 5.1 Error analysis We wanted to get an idea about how often the verb and its arguments are mistranslated. For this purpose we manually annotated errors in sentences with more than 5 words and at most 15 words. With this criterion we avoided translations with scrambled predicate-argument structures. Each sentence had roughly one main verb. To have a more reliable error annotation we first post-edited 100 translations from the baseline system. We then compared the translations with their post-editions and annotated error categories using the BLAST tool (Stymne, 2011). We considered a sense error category when there was a wrong lexical choice for the head of a main argument, a prepositional modifier or the main verb. We also annotated mistranslated prepositions. Number of triples 540,109,283 810,118,653 315,852,775 32,111,962 188,412,178 3,732,368 Table 4: Number of relation triples extracted from parsed data. The data consists of the English side of the parallel data and Gigaword. main arguments include: nsubj, nsubjpass, dobj, iobj. Error Category Preposition Sense Main argument Prep modifier Main verb We integrate the feature in a bottom-up chart decode"
W16-2204,D09-1067,0,0.0114607,"verb translation. However the features generally did not improve automatic evaluation metrics. We conclude that mistranslated verbs, errors in the target syntactic trees produced by the decoder and underspecified syntactic relations are negatively impacting these features. The paper is structured as follows. Section 2 describes related work on improving translation of predicate-argument structures. Section 3 introduces the selectional preference feature. Section 4 describes the experimental setup and Section 5 33 similarity (Erk et al., 2010; S´eaghdha, 2010; Ritter et al., 2010), clustering (Sun and Korhonen, 2009), multi-modal datasets (Shutova et al., 2015), and neural networks (Cruys, 2014). Our feature is based on the measure proposed by Resnik (1996). It uses unsupervised clusters to generalize over seen arguments. Resnik (1996) uses selectional preferences of predicates for word sense disambiguation. The information theoretic measure for selectional preference proposed by Resnik quantifies the difference between the posterior distribution of an argument class given the verb and the prior distribution of the class. For instance, ”person” has a higher prior probability than ”insect” to appear in the"
W16-2204,N04-1035,0,\N,Missing
W16-2204,W07-2201,0,\N,Missing
W16-2204,W11-1012,0,\N,Missing
W16-2204,D14-1082,0,\N,Missing
W16-2204,W15-3001,1,\N,Missing
W16-2204,W12-3018,0,\N,Missing
W16-2204,W11-2123,0,\N,Missing
W16-2323,P16-1009,1,0.795503,"d better ensembles. Decoding is performed with beam search with a beam size of 12. For some language pairs, we used the AmuNMT C++ decoder4 as a more efficient alternative to the theano implementation of the dl4mt tutorial. Introduction We participated in the WMT 2016 shared news translation task by building neural translation systems for four language pairs: English↔Czech, English↔German, English↔Romanian and English↔Russian. Our systems are based on an attentional encoder-decoder (Bahdanau et al., 2015), using BPE subword segmentation for open-vocabulary translation with a fixed vocabulary (Sennrich et al., 2016b). We experimented with using automatic back-translations of the 2.1 Byte-pair encoding (BPE) To enable open-vocabulary translation, we segment words via byte-pair encoding (BPE)5 (Sen1 We have released the implementation that we used for the experiments as an open source toolkit: https://github. com/rsennrich/nematus 2 We have released scripts, sample configs, synthetic training data and trained models: https://github.com/ rsennrich/wmt16-scripts 3 https://github.com/nyu-dl/ dl4mt-tutorial 4 https://github.com/emjotde/amunmt 5 https://github.com/rsennrich/ subword-nmt 371 Proceedings of the"
W16-2323,P16-1162,1,0.641761,"d better ensembles. Decoding is performed with beam search with a beam size of 12. For some language pairs, we used the AmuNMT C++ decoder4 as a more efficient alternative to the theano implementation of the dl4mt tutorial. Introduction We participated in the WMT 2016 shared news translation task by building neural translation systems for four language pairs: English↔Czech, English↔German, English↔Romanian and English↔Russian. Our systems are based on an attentional encoder-decoder (Bahdanau et al., 2015), using BPE subword segmentation for open-vocabulary translation with a fixed vocabulary (Sennrich et al., 2016b). We experimented with using automatic back-translations of the 2.1 Byte-pair encoding (BPE) To enable open-vocabulary translation, we segment words via byte-pair encoding (BPE)5 (Sen1 We have released the implementation that we used for the experiments as an open source toolkit: https://github. com/rsennrich/nematus 2 We have released scripts, sample configs, synthetic training data and trained models: https://github.com/ rsennrich/wmt16-scripts 3 https://github.com/nyu-dl/ dl4mt-tutorial 4 https://github.com/emjotde/amunmt 5 https://github.com/rsennrich/ subword-nmt 371 Proceedings of the"
W16-2323,W16-2316,1,\N,Missing
W17-4707,W07-0702,1,0.671821,"oposed serializing the target constituency trees and Eriguchi et al. (2017) model target dependency relations by augmenting the NMT decoder with a RNN grammar (Dyer et al., 2016). In our work, we use CCG supertags which are a more compact representation of global syntax. Furthermore, we do not focus on model architectures, and instead we explore the more general problem of including target syntax in NMT: comparing tightly and loosely coupled syntactic information and showing source and target syntax are complementary. Previous work on integrating CCG supertags in factored phrase-based models (Birch et al., 2007) made strong independence assumptions between the target word sequence and the CCG categories. In this work we take advantage of the expressive power of recurrent neural networks to learn representations that generate both words and CCG supertags, conditioned on the entire lexical and syntactic target history. ments, and also tense and morphological aspects of the word in a given context. Consider the sentence in Figure 1. This sentence contains two PP attachments and could lead to several disambiguation possibilities (“in” can attach to “Netanyahu” or “receives”, and “of” can attach to “capit"
W17-4707,J07-2003,0,0.0156397,"ining. • We show that incorporating source-side linguistic information is complimentary to our method, further improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories, i.e., supertags, that indicate context-sensitive morpho-syntactic properties of a word in a sentence. Th"
W17-4707,W14-4012,0,0.028354,"Missing"
W17-4707,P17-2021,0,0.0761778,"ework to show source and target syntax provide complementary information. Applying more tightly coupled linguistic factors on the target for NMT has been previously investigated. Niehues et al. (2016) proposed a factored RNN-based language model for re-scoring an n-best list produced by a phrase-based MT system. In recent work, Mart´ınez et al. (2016) implemented a factored NMT decoder which generated both lemmas and morphological tags. The two factors were then post-processed to generate the word form. Unfortunately no real gain was reported for these experiments. Concurrently with our work, Aharoni and Goldberg (2017) proposed serializing the target constituency trees and Eriguchi et al. (2017) model target dependency relations by augmenting the NMT decoder with a RNN grammar (Dyer et al., 2016). In our work, we use CCG supertags which are a more compact representation of global syntax. Furthermore, we do not focus on model architectures, and instead we explore the more general problem of including target syntax in NMT: comparing tightly and loosely coupled syntactic information and showing source and target syntax are complementary. Previous work on integrating CCG supertags in factored phrase-based model"
W17-4707,D14-1179,0,0.00956885,"Missing"
W17-4707,N16-1024,0,0.0146616,"al. (2016) proposed a factored RNN-based language model for re-scoring an n-best list produced by a phrase-based MT system. In recent work, Mart´ınez et al. (2016) implemented a factored NMT decoder which generated both lemmas and morphological tags. The two factors were then post-processed to generate the word form. Unfortunately no real gain was reported for these experiments. Concurrently with our work, Aharoni and Goldberg (2017) proposed serializing the target constituency trees and Eriguchi et al. (2017) model target dependency relations by augmenting the NMT decoder with a RNN grammar (Dyer et al., 2016). In our work, we use CCG supertags which are a more compact representation of global syntax. Furthermore, we do not focus on model architectures, and instead we explore the more general problem of including target syntax in NMT: comparing tightly and loosely coupled syntactic information and showing source and target syntax are complementary. Previous work on integrating CCG supertags in factored phrase-based models (Birch et al., 2007) made strong independence assumptions between the target word sequence and the CCG categories. In this work we take advantage of the expressive power of recurr"
W17-4707,P16-1231,0,0.0172413,"butions over separate target vocabularies for the words and the tags: (7) (8) pword y At training time we pre-process the target sequence to add the syntactic annotation and then split only the words into byte-pair-encoding (BPE) (Sennrich et al., 2016b) sub-units. At = T1 Y word p(yjword |x, y1:j−1 ) T2 Y tag p(yktag |x, y1:k−1 ) j ptag y = k 71 (9) (10) The final loss is the sum of the losses for the two decoders: l = −(log(pword ) + log(ptag y y )) DE-EN RO-EN (11) test 2,994 1,984 sets in Table 1. Dependency labels are annotated with ParZU (Sennrich et al., 2013) for German and SyntaxNet (Andor et al., 2016) for Romanian. All the neural MT systems are attentional encoder-decoder networks (Bahdanau et al., 2015) as implemented in the Nematus toolkit (Sennrich et al., 2017).4 We use similar hyper-parameters to those reported by (Sennrich et al., 2016a; Sennrich and Haddow, 2016) with minor modifications: we used mini-batches of size 60 and Adam optimizer (Kingma and Ba, 2014). We select the best single models according to BLEU on the development set and use the four best single models for the ensembles. To show that we report results over strong baselines, table 2 compares the scores obtained by ou"
W17-4707,P16-1078,0,0.0552534,"ntence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories, i.e., supertags, that indicate context-sensitive morpho-syntactic properties of a word in a sentence. The combinators of CCG allow the supertags to capture global syntactic constraints locally. Though NMT 69 Source-side BPE: Obama IOB: O CCG: NP Target-side receives O ((S[dcl]NP)/PP)/NP Net+ B NP an+ I NP yahu E NP in O PP/NP the O NP/N capital O N of O (NPNP)/NP USA"
W17-4707,D16-1025,0,0.0250595,"Missing"
W17-4707,N04-1035,0,0.0228682,"ht coupling of target words and syntax (by interleaving) improves translation quality more than the decoupled signal from multitask training. • We show that incorporating source-side linguistic information is complimentary to our method, further improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned wi"
W17-4707,W16-2208,0,0.0269734,"Missing"
W17-4707,P02-1040,0,0.120603,"of the training data is annotated with CCG lexical tags2 using EasySRL (Lewis et al., 2015) and the available pre-trained model3 . Some longer sentences cannot be processed by the parser and therefore we eliminate them from our training and test data. We report the sentence counts for the filtered data This work 31.0 27.8 28.0 25.6 Sennrich et. al 28.5 26.8 27.8 23.9 Table 2: Comparison of baseline systems in this work and in Sennrich et al. (2016a). Casesensitive BLEU scores reported over newstest2016 with mteval-13a.perl. 1 Normalized diacritics. During training we validate our models with (Papineni et al., 2002) on development sets: newstest2013 for German↔English and newsdev2016 for Romanian↔English. We evaluate the systems on newstest2016 test sets for both lanBLEU 1 We use the same data and annotations for the interleaving approach. 2 The CCG tags include features such as the verb tense (e.g. [ng] for continuous form) or the sentence type (e.g. [pss] for passive). 3 https://github.com/uwnlp/EasySRL 4 https://github.com/rsennrich/nematus There are different encodings for letters with cedilla (s¸,t¸) used interchangeably throughout the corpus. https://en.wikipedia.org/wiki/Romanian_ alphabet#ISO_885"
W17-4707,D13-1176,0,0.032968,"urther improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories, i.e., supertags, that indicate context-sensitive morpho-syntactic properties of a word in a sentence. The combinators of CCG allow the supertags to capture global syntactic constraints locally. Though NMT 69 Source-side BPE: O"
W17-4707,W05-0908,0,0.0147936,"lish and newsdev2016 for Romanian↔English. We evaluate the systems on newstest2016 test sets for both lanBLEU 1 We use the same data and annotations for the interleaving approach. 2 The CCG tags include features such as the verb tense (e.g. [ng] for continuous form) or the sentence type (e.g. [pss] for passive). 3 https://github.com/uwnlp/EasySRL 4 https://github.com/rsennrich/nematus There are different encodings for letters with cedilla (s¸,t¸) used interchangeably throughout the corpus. https://en.wikipedia.org/wiki/Romanian_ alphabet#ISO_8859 5 72 guage pairs and use bootstrap resampling (Riezler and Maxwell, 2005) to test statistical significance. We compute BLEU with multi-bleu.perl over tokenized sentences both on the development sets, for early stopping, and on the test sets for evaluating our systems. Words are segmented into sub-units that are learned jointly for source and target using BPE (Sennrich et al., 2016b), resulting in a vocabulary size of 85,000. The vocabulary size for CCG supertags was 500. For the experiments with source-side features we use the BPE sub-units and the IOB tags as baseline features. We keep the total word embedding size fixed to 500 dimensions. We allocate 10 dimension"
W17-4707,Q15-1013,1,0.864408,"n the decoupled signal from multitask training. • We show that incorporating source-side linguistic information is complimentary to our method, further improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories, i.e., supertags, that indicate context-sensitive morpho-syntacti"
W17-4707,D15-1169,0,0.0163034,"DE→EN EN→DE RO→EN EN→RO1 Experimental Setup and Evaluation 4.1 dev 2,986 1,984 Table 1: Number of sentences in the training, development and test sets. We use EasySRL to label the English side of the parallel corpus with CCG supertags1 instead of using a corpus with gold annotations as in Luong et al. (2016). 4 train 4,468,314 605,885 Data and methods We train the neural MT systems on all the parallel data available at WMT16 (Bojar et al., 2016) for the German↔English and Romanian↔English language pairs. The English side of the training data is annotated with CCG lexical tags2 using EasySRL (Lewis et al., 2015) and the available pre-trained model3 . Some longer sentences cannot be processed by the parser and therefore we eliminate them from our training and test data. We report the sentence counts for the filtered data This work 31.0 27.8 28.0 25.6 Sennrich et. al 28.5 26.8 27.8 23.9 Table 2: Comparison of baseline systems in this work and in Sennrich et al. (2016a). Casesensitive BLEU scores reported over newstest2016 with mteval-13a.perl. 1 Normalized diacritics. During training we validate our models with (Papineni et al., 2002) on development sets: newstest2013 for German↔English and newsdev2016"
W17-4707,E17-3017,1,0.894752,"Missing"
W17-4707,W16-2209,1,0.896448,") show that NMT significantly improves over phrase-based SMT, in particular with respect to morphology and word order, but that results can still be improved for longer sentences and complex syntactic phenomena such as prepositional phrase (PP) attachment. Another study by Shi et al. (2016) shows that the encoder layer of NMT partially learns syntactic information about the source language, however complex syntactic phenomena such as coordination or PP attachment are poorly modeled. Recent work which incorporates additional source-side linguistic information in NMT models (Luong et al., 2016; Sennrich and Haddow, 2016) show that even though neural models have strong learning capabilities, explicit features can still improve translation quality. In this work, we examine the benefit of incorporating global syntactic information on the target-side. We also address the question of how best to incorporate this information. For language pairs where syntactic resources are available on both the source and target-side, we show that approaches to incorporate source syntax and target syntax are complementary. We propose a method for tightly coupling words and syntax by interleaving the target syntactic representation"
W17-4707,W07-0701,0,0.202057,"words and syntax (by interleaving) improves translation quality more than the decoupled signal from multitask training. • We show that incorporating source-side linguistic information is complimentary to our method, further improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories,"
W17-4707,W16-2323,1,0.729608,"ish, a lowresource pair and also several syntactic phenomena including prepositional phrase attachment. Furthermore, a tight coupling of words and syntax improves translation quality more than multitask training. By combining target-syntax with adding source-side dependency labels in the embedding layer, we obtain a total improvement of 0.9 BLEU for German→English and 1.2 BLEU for Romanian→English. 1 Introduction Sequence-to-sequence neural machine translation (NMT) models (Sutskever et al., 2014; Cho et al., 2014b; Bahdanau et al., 2015) are state-of-the-art on a multitude of language-pairs (Sennrich et al., 2016a; Junczys-Dowmunt et al., 2016). Part of the appeal of neural models is that they can learn to implicitly model phenomena which underlie high quality output, and some syntax is indeed cap68 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 68–79 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics et al. (2016) co-train a translation model and a source-side syntactic parser which share the encoder. Our multitask models extend their work to attention-based NMT models and to predicting target-side syntax as the sec"
W17-4707,W16-2204,1,0.813749,"signal from multitask training. • We show that incorporating source-side linguistic information is complimentary to our method, further improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories, i.e., supertags, that indicate context-sensitive morpho-syntactic properties of a word"
W17-4707,P16-1162,1,0.46286,"ish, a lowresource pair and also several syntactic phenomena including prepositional phrase attachment. Furthermore, a tight coupling of words and syntax improves translation quality more than multitask training. By combining target-syntax with adding source-side dependency labels in the embedding layer, we obtain a total improvement of 0.9 BLEU for German→English and 1.2 BLEU for Romanian→English. 1 Introduction Sequence-to-sequence neural machine translation (NMT) models (Sutskever et al., 2014; Cho et al., 2014b; Bahdanau et al., 2015) are state-of-the-art on a multitude of language-pairs (Sennrich et al., 2016a; Junczys-Dowmunt et al., 2016). Part of the appeal of neural models is that they can learn to implicitly model phenomena which underlie high quality output, and some syntax is indeed cap68 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 68–79 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics et al. (2016) co-train a translation model and a source-side syntactic parser which share the encoder. Our multitask models extend their work to attention-based NMT models and to predicting target-side syntax as the sec"
W17-4707,R13-1079,1,0.793726,"1 and T2 . This results in two probability distributions over separate target vocabularies for the words and the tags: (7) (8) pword y At training time we pre-process the target sequence to add the syntactic annotation and then split only the words into byte-pair-encoding (BPE) (Sennrich et al., 2016b) sub-units. At = T1 Y word p(yjword |x, y1:j−1 ) T2 Y tag p(yktag |x, y1:k−1 ) j ptag y = k 71 (9) (10) The final loss is the sum of the losses for the two decoders: l = −(log(pword ) + log(ptag y y )) DE-EN RO-EN (11) test 2,994 1,984 sets in Table 1. Dependency labels are annotated with ParZU (Sennrich et al., 2013) for German and SyntaxNet (Andor et al., 2016) for Romanian. All the neural MT systems are attentional encoder-decoder networks (Bahdanau et al., 2015) as implemented in the Nematus toolkit (Sennrich et al., 2017).4 We use similar hyper-parameters to those reported by (Sennrich et al., 2016a; Sennrich and Haddow, 2016) with minor modifications: we used mini-batches of size 60 and Adam optimizer (Kingma and Ba, 2014). We select the best single models according to BLEU on the development set and use the four best single models for the ensembles. To show that we report results over strong baselin"
W17-4707,D16-1159,0,0.0475338,"Alexandra Birch1 1 School of Informatics, University of Edinburgh 2 Adam Mickiewicz University 3 Dep. of Computer Science, Johns Hopkins University {m.nadejde,siva.reddy, rico.sennrich, a.birch}@ed.ac.uk {t.dwojak,junczys}@amu.edu.pl, phi@jhu.edu Abstract tured by these models. In a detailed analysis, Bentivogli et al. (2016) show that NMT significantly improves over phrase-based SMT, in particular with respect to morphology and word order, but that results can still be improved for longer sentences and complex syntactic phenomena such as prepositional phrase (PP) attachment. Another study by Shi et al. (2016) shows that the encoder layer of NMT partially learns syntactic information about the source language, however complex syntactic phenomena such as coordination or PP attachment are poorly modeled. Recent work which incorporates additional source-side linguistic information in NMT models (Luong et al., 2016; Sennrich and Haddow, 2016) show that even though neural models have strong learning capabilities, explicit features can still improve translation quality. In this work, we examine the benefit of incorporating global syntactic information on the target-side. We also address the question of h"
W17-4707,W12-3150,1,0.846453,"erleaving) improves translation quality more than the decoupled signal from multitask training. • We show that incorporating source-side linguistic information is complimentary to our method, further improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories, i.e., supertags, that indi"
W17-4710,W14-4012,0,0.19765,"Missing"
W17-4710,D14-1179,0,0.0569526,"Missing"
W17-4710,Q16-1027,0,0.426398,"schemes (Pascanu et al., 2014) that give rise to different, orthogonal, definitions of depth (Zhang et al., 2016) which can affect the model performance depending on a given task. This is further complicated in sequence-to-sequence models as they contain multiple sub-networks, recurrent or feed-forward, each of which can be deep in different ways, giving rise to a large number of possible configurations. In this work we focus on stacked and deep transition recurrent architectures as defined by Pascanu et al. (2014). Different types of stacked architectures have been successfully used for NMT (Zhou et al., 2016; Wu et al., 2016). However, there is a lack of empirical comparisons of different deep architectures. Deep transition architectures have been successfully used for language modeling (Zilly et al., 2016), but not for NMT so far. We evaluate these architectures, both alone and in combination, varying the connection scheme between the different components and their depth over the different dimensions, measuring the performance of the different configurations on the WMT news translation task.1 Related work includes that of Britz et al. (2017), who have performed an exploration of NMT architecture"
W17-4710,E17-2060,1,0.824839,"o models are large and their results are highlighted separately. tances, since each layer may lose information during forward computation or backpropagation. This may not be a significant issue in the encoder, as the attention mechanism provides short paths from any source word state to the decoder, but the decoder contains no such shortcuts between its states, therefore it might be possible that this negatively affects its ability to model long-distance relationships in the target text, such as subject–verb agreement. Here, we seek to answer this question by testing our models on Lingeval97 (Sennrich, 2017), a test set which provides contrastive translation pairs for different types of errors. For the example of subject-verb agreement, contrastive translations are created from a reference translation by changing the grammatical number of the verb, and we can measure how often the NMT model prefers the correct reference over the contrastive variant. In Figure 5, we show accuracy as a function of the distance between subject and verb. We find that information is successfully passed over long distances by the deep recurrent transition network. Even for decisions that require information to be carri"
W17-4710,E17-3017,1,0.827403,"der in this work are GRU (Cho et al., 2014a) sequence-to-sequence transducers (Sutskever et al., 2014; Cho et al., 2014b) with attention (Bahdanau et al., 2015). In this section we describe the baseline system and the variants that we evaluated. 2.1 Figure 1: Deep transition decoder GRU transition for the current time step is carried over as the ""state"" input of the first GRU transition for the next time step. Applying this architecture to NMT is a novel contribution. Baseline Architecture As our baseline, we use the NMT architecture implemented in Nematus, which is described in more depth by Sennrich et al. (2017b). We augment it with layer normalization (Ba et al., 2016), which we have found to both improve translation quality and make training considerably faster. For our discussion, it is relevant that the baseline architecture already exhibits two types of depth: 2.2.1 Deep Transition Encoder As in a baseline shallow Nematus system, the encoder is a bidirectional recurrent neural network. Let Ls be the encoder recurrence depth, then for the i-th source word in the forward direction the → − → − forward source word state h i ≡ h i,Ls is computed as:  →  → − − h i,1 = GRU1 xi , h i−1,Ls  →  → − −"
W17-4710,W17-4739,1,\N,Missing
W17-4739,P10-2041,0,0.0320685,"Missing"
W17-4739,buck-etal-2014-n,1,0.80914,"Missing"
W17-4739,W17-4705,0,0.0149621,"Missing"
W17-4739,D14-1179,0,0.00518473,"Missing"
W17-4739,P16-1009,1,0.524474,"ablative experiments, reporting on the effectivenes of layer normalization, deep architectures, and different ensembling techniques. 1 Introduction We participated in the WMT17 shared news translation task for 12 translation directions, translating between English and Czech, German, Latvian, Russian, Turkish and Chinese, and in the WMT17 shared biomedical translation task for English to Czech, German, Polish and Romanian.1 We submitted neural machine translation systems trained with Nematus (Sennrich et al., 2017). Our setup is based on techniques described in last year’s system description (Sennrich et al., 2016a), including the use of subword models (Sennrich et al., 1 2 Novelties Here we describe the main differences to last year’s systems. We provide trained models and training commands at http://data.statmt.org/wmt17_systems/ 389 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 389–399 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics 2.1 Subword Segmentation stacked architecture. Implementations of both of these architectures are available in Nematus. For completeness, we here reproduce the description of the"
W17-4739,P16-1162,1,\N,Missing
W17-4739,E17-2025,0,\N,Missing
W18-2701,W18-2713,0,0.0259017,"LSTMbased encoder-decoders with attention (Bahdanau et al., 2015). 3.4 Submitted Systems Four teams, Team Amun, Team Marian, Team OpenNMT, and Team NICT submitted to the shared task, and we will summarize each below. Before stepping in to the details of each system, we first note general trends that all or many systems attempted. The first general trend was a fast C++ decoder, with Teams Amun, Marian, and NICT using the Amun or Marian decoders included in the Marian toolkit,4 and team OpenNMT 4 5 https://marian-nmt.github.io 3 http://opennmt.net 3.4.1 3.4.4 Team Amun Team NICT’s contribution (Imamura and Sumita, 2018) to the shared task was centered around using self-training as a way to improve NMT accuracy without changing the architecture. Specifically, they used a method of randomly sampling pseudo-source sentences from a back-translation model (Imamura et al., 2018) and used this to augment the data set to increase coverage. They tested two basic architectures for the actual translation model, a recurrent neural network-based model trained using OpenNMT, and a self-attentional model trained using Marian, finally submitting the self-attentional model using Marian as their sole contribution to the share"
W18-2701,W18-2716,0,0.0254636,"xamination for future shared tasks. Next, considering memory usage, we can see again that the submissions from the Marian team tend to be the most efficient. One exception is the extremely small memory system OpenNMT-Tiny, which achieves significantly lower translation accuracies, but fits in a mere 220MB of memory on the CPU. In this first iteration of the task, we attempted to establish best practices and strong baselines upon which to build efficient test-time methods for NMT. One characteristic of the first iteration of the task was that the basic model architectures Team Marian’s system (Junczys-Dowmunt et al., 2018) used the Marian C++ decoder, and concentrated on new optimizations for the CPU. The team distilled a large self-attentional model into two types of “student” models: a smaller self-attentional model using average attention networks (Zhang et al., 2018), a new higherspeed version of the original Transformer model (Vaswani et al., 2017), and a standard RNN-based decoder. They also introduced an auto-tuning approach that chooses which of multiple matrix multiplication implementations is most efficient in the current context, then uses this implementation going forward. This resulted in the Maria"
W18-2701,W18-2708,0,0.0238604,"ization methods for adaptation (Khayrallah et al., 2018) and “extreme adaptation” to individual speakers (Michel and Neubig, 2018) Accuracy Measures: As a measure of translation accuracy, we used BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) scores. Data augmentation: A number of the contributed papers examined ways to augment data for more efficient training. These include methods for considering multiple back translations (Imamura et al., 2018), iterative back translation (Hoang et al., 2018b), bidirectional multilingual training (Niu et al., 2018), and document level adaptation (Kothur et al., 2018) Computational Efficiency Measures: We measured the amount of time it takes to translate the entirety of the test set on CPU or GPU. Time for loading models was measured by having the model translate an empty file, then subtracting this from the total time to translate the test set file. Inadequate resources: Several contributions involved settings in which resources were insufficient, such as investigating the impact of noise (Khayrallah and Koehn, 2018), missing data in multi-source settings (Nishimura et al., 2018) and one-shot learning (Pham et al., 2018). Memory Efficiency Measures: We me"
W18-2701,N18-2078,0,0.0233178,"r Computational Linguistics Translation (Nakazawa et al., 2017)) was focused on creating systems for NMT that are not only accurate, but also efficient. Efficiency can include a number of concepts, including memory efficiency and computational efficiency. This task concerns itself with both, and we cover the detail of the evaluation below. model research, with the contributions being concentrated on the following topics: Linguistic structure: How can we incorporate linguistic structure in neural MT or generation models? Contributions examined the effect of considering semantic role structure (Marcheggiani et al., 2018), latent structure (Bastings et al., 2018), and structured self-attention (Bisk and Tran, 2018). 3.1 The first step to the evaluation was deciding what we want to measure. In the case of the shared task, we used metrics to measure several different aspects connected to how good the system is. These were measured for systems that were run on CPU, and also systems that were run on GPU. Domain adaptation: Some contributions examined regularization methods for adaptation (Khayrallah et al., 2018) and “extreme adaptation” to individual speakers (Michel and Neubig, 2018) Accuracy Measures: As a meas"
W18-2701,D15-1199,0,0.0577417,"Missing"
W18-2701,P18-2050,1,0.824817,"g semantic role structure (Marcheggiani et al., 2018), latent structure (Bastings et al., 2018), and structured self-attention (Bisk and Tran, 2018). 3.1 The first step to the evaluation was deciding what we want to measure. In the case of the shared task, we used metrics to measure several different aspects connected to how good the system is. These were measured for systems that were run on CPU, and also systems that were run on GPU. Domain adaptation: Some contributions examined regularization methods for adaptation (Khayrallah et al., 2018) and “extreme adaptation” to individual speakers (Michel and Neubig, 2018) Accuracy Measures: As a measure of translation accuracy, we used BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) scores. Data augmentation: A number of the contributed papers examined ways to augment data for more efficient training. These include methods for considering multiple back translations (Imamura et al., 2018), iterative back translation (Hoang et al., 2018b), bidirectional multilingual training (Niu et al., 2018), and document level adaptation (Kothur et al., 2018) Computational Efficiency Measures: We measured the amount of time it takes to translate the entirety of the t"
W18-2701,P18-1166,0,0.0179704,"lation accuracies, but fits in a mere 220MB of memory on the CPU. In this first iteration of the task, we attempted to establish best practices and strong baselines upon which to build efficient test-time methods for NMT. One characteristic of the first iteration of the task was that the basic model architectures Team Marian’s system (Junczys-Dowmunt et al., 2018) used the Marian C++ decoder, and concentrated on new optimizations for the CPU. The team distilled a large self-attentional model into two types of “student” models: a smaller self-attentional model using average attention networks (Zhang et al., 2018), a new higherspeed version of the original Transformer model (Vaswani et al., 2017), and a standard RNN-based decoder. They also introduced an auto-tuning approach that chooses which of multiple matrix multiplication implementations is most efficient in the current context, then uses this implementation going forward. This resulted in the MarianTinyRNN system using an RNN-based model, and the Marian-Trans-Small-AAN, MarianTrans-Base-AAN, Marian-Trans-Big, MarianTrans-Big-int8 systems, which use different varieties and sizes of self-attentional models. 3.4.3 Team NICT Team OpenNMT Team OpenNMT"
W18-2701,W18-2712,0,0.0187161,", and document level adaptation (Kothur et al., 2018) Computational Efficiency Measures: We measured the amount of time it takes to translate the entirety of the test set on CPU or GPU. Time for loading models was measured by having the model translate an empty file, then subtracting this from the total time to translate the test set file. Inadequate resources: Several contributions involved settings in which resources were insufficient, such as investigating the impact of noise (Khayrallah and Koehn, 2018), missing data in multi-source settings (Nishimura et al., 2018) and one-shot learning (Pham et al., 2018). Memory Efficiency Measures: We measured: (1) the size on disk of the model, (2) the number of parameters in the model, and (3) the peak consumption of the host memory and GPU memory. These metrics were measured by having participants submit a container for the virtualization environment Docker1 , then measuring from outside the container the usage of computation time and memory. All evaluations were performed on dedicated instances on Amazon Web Services2 , specifically of type m5.large for CPU evaluation, and p3.2xlarge (with a NVIDIA Tesla V100 GPU). Model analysis: There were also many me"
W18-2701,D15-1044,0,0.0437311,"cipants were tasked with creating NMT systems that are both accurate and efficient. 1 Introduction Neural sequence to sequence models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) are now a workhorse behind a wide variety of different natural language processing tasks such as machine translation, generation, summarization and simplification. The 2nd Workshop on Neural Machine Translation and Generation (WNMT 2018) provided a forum for research in applications of neural models to machine translation and other language generation tasks (including summarization (Rush et al., 2015), NLG from structured data (Wen et al., 2015), dialog response generation (Vinyals and Le, 2015), among others). Overall, the workshop was held with two goals: First, it aimed to synthesize the current state of knowledge in neural machine translation and generation: This year we will continue to encourage submissions that not only advance the state of 2 Summary of Research Contributions We published a call for long papers, extended abstracts for preliminary work, and crosssubmissions of papers submitted to other venues. The goal was to encourage discussion and interaction with researchers from"
W18-2701,W18-2715,0,0.0477416,"Missing"
W18-2701,P17-2091,0,0.117559,"most efficient in the current context, then uses this implementation going forward. This resulted in the MarianTinyRNN system using an RNN-based model, and the Marian-Trans-Small-AAN, MarianTrans-Base-AAN, Marian-Trans-Big, MarianTrans-Big-int8 systems, which use different varieties and sizes of self-attentional models. 3.4.3 Team NICT Team OpenNMT Team OpenNMT (Senellart et al., 2018) built a system based on the OpenNMT toolkit. The model was based on a large self-attentional teacher model distilled into a smaller, fast RNN-based model. The system also used a version of vocabulary selection (Shi and Knight, 2017), and a method to increase the size of the encoder but decrease the size of the decoder to improve the efficiency of beam search. They submitted two systems, OpenNMT-Small and OpenNMT-Tiny, which were two variously-sized implementations of this model. 4 used relatively standard, with the valuable contributions lying in solid engineering work and best practices in neural network optimization such as low-precision calculation and model distillation. With these contributions, we now believe we have very strong baselines upon which future iterations of the task can build, examining novel architect"
W18-2701,W18-2702,0,0.0225666,"ntainer for the virtualization environment Docker1 , then measuring from outside the container the usage of computation time and memory. All evaluations were performed on dedicated instances on Amazon Web Services2 , specifically of type m5.large for CPU evaluation, and p3.2xlarge (with a NVIDIA Tesla V100 GPU). Model analysis: There were also many methods that analyzed modeling and design decisions, including investigations of individual neuron contributions (Bau et al., 2018), parameter sharing (Jean et al., 2018), controlling output characteristics (Fan et al., 2018), and shared attention (Unanue et al., 2018) 3 Evaluation Measures Shared Task 3.2 Many shared tasks, such as the ones run by the Conference on Machine Translation (Bojar et al., 2017), aim to improve the state of the art for MT with respect to accuracy: finding the most accurate MT system regardless of computational cost. However, in production settings, the efficiency of the implementation is also extremely important. The shared task for WNMT (inspired by the “small NMT” task at the Workshop on Asian Data The data used was from the WMT 2014 EnglishGerman task (Bojar et al., 2014), using the preprocessed corpus provided by the Stanford"
W18-2701,D13-1176,0,\N,Missing
W18-2701,P02-1040,0,\N,Missing
W18-2701,W14-3302,0,\N,Missing
W18-2701,W17-4717,0,\N,Missing
W18-2701,W18-2704,0,\N,Missing
W18-2701,W18-2711,1,\N,Missing
W18-2701,W18-2707,0,\N,Missing
W18-2701,W18-2706,0,\N,Missing
W18-2701,W18-2709,0,\N,Missing
W18-2701,W18-2705,0,\N,Missing
W18-6320,2009.mtsummit-posters.5,0,0.123004,"Missing"
W18-6320,W15-4918,1,0.909017,"Missing"
W18-6320,W16-2301,1,0.829153,"Missing"
W18-6320,2001.mtsummit-papers.20,0,0.488483,"Missing"
W18-6320,1999.mtsummit-1.42,0,0.32113,"Missing"
W18-6320,W11-2123,0,0.0202271,"Missing"
W18-6320,E06-1032,0,0.205885,"Missing"
W18-6320,N07-2020,0,0.0997368,"Missing"
W18-6320,L16-1048,0,0.0241495,"Missing"
W18-6320,2014.eamt-1.40,0,0.0438492,"Missing"
W18-6320,2000.tc-1.5,0,0.320841,"Missing"
W18-6320,W15-2402,0,0.0382638,"Missing"
W18-6320,stymne-etal-2012-eye,0,0.038532,"Missing"
W18-6320,P07-2045,1,0.0112762,"Missing"
W18-6320,1993.tmi-1.22,0,0.82749,"Missing"
W18-6320,W11-2401,0,0.033505,"Missing"
W18-6320,2012.freeopmt-1.3,0,0.127684,"Missing"
W18-6320,weiss-ahrenberg-2012-error,0,0.0760185,"Missing"
W18-6320,N16-1125,0,0.0253847,"Missing"
W19-5304,D18-1332,1,0.852639,"ameters that work effectively for the ZH-EN direction. Transfomer-base with larger feed-forward network We test Wang et al.’s (2018) recommendation to use the base transformer architecture and increase the feed-forward network (FFNN) size to 4096 instead of using a transformer-big model. Ultra-large mini-batches We follow Smith et al.’s (2018) recommendation to dramatically increase the mini-batch size towards the end of training in order to improve convergence.12 Once our model stopped improving on the development set, we increased the mini-batch size 50-fold by delaying the gradient update (Bogoychev et al., 2018) to avoid running into memory issues. This increases the average mini-batch size to 13,500 words. 3.4 German → English Following the success of Edunov et al. (2018) in WMT18, we decided to focus on the use of large amounts of monolingual data in the target language. 12 BLEU Word-level segmentation for ZH Transformer-base + Larger FFNN + Ultra-large mini-batches + Ultra-large mini-batches Transformer-big 24.1 23.7 24.4 24.2 11.3 Character-level segmentation for ZH Transformer-base 20.4 Table 7: ZH→EN results on the development set. In addition, we performed fine tuning on data selected specific"
W19-5304,P18-4020,1,0.871629,"Missing"
W19-5304,N19-1423,0,0.010992,"671 10,650 9,979 7,807 5,083 7,993 1.4M 7.0 21.1 2.1 17.0 1.5 26.4 19.1 19.1 13.4 HI News Common crawl Emille Wiki-dump News Bombay IIT News 200M 3.7M 0.9M 0.4M 0.2M 45.1M 23.6M Semi-supervised MT with cross-lingual language model pre-training We followed the unsupervised training approach in (Lample and Conneau, 2019) to train two MT systems, one for EN↔GU and a second for HI→GU.6 This involves training unsupervised NMT models with an additional supervised MT training step. Initialisation of the models is done by pre-training parameters using a masked language modelling objective as in Bert (Devlin et al., 2019), individually for each language (MLM, which stands for masked language modelling) and/or cross-lingually (TLM, which stands for translation language modelling). The TLM objective is the MLM objective Monolingual data EN GU Creation of synthetic parallel data 23.6 21.9 16.6 17.7 15.4 18.7 17.0 4 Table 2: EN-GU Parallel training data used. Average length is calculated in number of tokens per sentence. For the parallel corpora, this is calculated for the first language indicated (i.e. EN, GU, then EN) We pre-processed all data using standard scripts 104 anoopkunchukuttan.github.io/indic_ nlp_lib"
W19-5304,D18-1045,0,0.193835,"lish, and English→Czech. For all translation directions, we created or used back-translations of monolingual data in the target language as additional synthetic training data. For English↔Gujarati, we also explored semisupervised MT with cross-lingual language model pre-training, and translation pivoting through Hindi. For translation to and from Chinese, we investigated character-based tokenisation vs. sub-word segmentation of Chinese text. For German→English, we studied the impact of vast amounts of back-translated training data on translation quality, gaining a few additional insights over Edunov et al. (2018). For English→Czech, we compared different pre-processing and tokenisation regimes. 1 Introduction The University of Edinburgh participated in the WMT19 Shared Task on News Translation in six language directions: English-Gujarati (EN↔GU), English-Chinese (EN↔ZH), GermanEnglish (DE→EN) and English-Czech (EN→CS). All our systems are neural machine translation (NMT) systems trained in constrained data conditions with the Marian1 toolkit (Junczys-Dowmunt et al., 2018). The different language pairs pose very different challenges, due to the characteristics of the languages involved and arguably mor"
W19-5304,I05-3017,0,0.0875027,"times the number of tokens on the other side, following Haddow et al. (2018). After pre-processing, the corpus size was 23.6M sentences. We applied BPE with 32,000 merge operations to the English side of the corpora and then removed any tokens appearing fewer than 10 times (which were mostly noise), 11 https://github.com/fxsjy/jieba Character-level Chinese A Chinese characterlevel model is not the same as an English character level model, as it is relatively common for Chinese characters to represent whole words by themselves (in the PKU corpus used for the 2005 Chinese segmentation bakeoff (Emerson, 2005), a Chinese word contains on average 1.6 characters). As such, a Chinese character-level model is much more similar to using a BPE model with very few merge operations on English. We hypothesised that using raw Chinese characters in tokenised text makes sense as they form natural subword units. We segmented all Chinese sentences into characters, but kept non-Chinese characters unsegmented in order to allow for English words and numbers to be kept together as individual units. We then applied BPE with 1,000 merges, which splits the English words in the corpora into mostly trigrams and numbers a"
W19-5304,W17-4713,0,0.0339139,"e to 13,500 words. 3.4 German → English Following the success of Edunov et al. (2018) in WMT18, we decided to focus on the use of large amounts of monolingual data in the target language. 12 BLEU Word-level segmentation for ZH Transformer-base + Larger FFNN + Ultra-large mini-batches + Ultra-large mini-batches Transformer-big 24.1 23.7 24.4 24.2 11.3 Character-level segmentation for ZH Transformer-base 20.4 Table 7: ZH→EN results on the development set. In addition, we performed fine tuning on data selected specifically for the test set prior to translation, similar to the method suggested by Farajian et al. (2017), but with data selection for the entire test set instead of individual sentences. 4.1 Approach Our approach this year is summarised as follows. 1. Back-translate all available mono-lingual English NewsCrawl data (after filtering out very long sentences). As can be seen in Table 8, the amount of monolingual data vastly outweighs the amount of parallel data available. Results We identified the best single system for each language direction (Tables 6 and 7) and ensembled four models trained separately using different random seeds. We also trained right-to-left models, but they got lower scores o"
W19-5304,W18-6412,1,0.838955,"he original parallel data is inconsistently segmented across different corpora so in order to get a consistent segmentation, we desegmented all the Chinese data and resegmented it using the Jieba tokeniser with the default dictionary.11 We then removed any sentences that did not contain Chinese characters on the Chinese side or contained only Chinese characters on the English side. We also cleaned up all sentences containing links, sentences longer than 50 words, as well as sentences in which the number of tokens on either side was > 1.3 times the number of tokens on the other side, following Haddow et al. (2018). After pre-processing, the corpus size was 23.6M sentences. We applied BPE with 32,000 merge operations to the English side of the corpora and then removed any tokens appearing fewer than 10 times (which were mostly noise), 11 https://github.com/fxsjy/jieba Character-level Chinese A Chinese characterlevel model is not the same as an English character level model, as it is relatively common for Chinese characters to represent whole words by themselves (in the PKU corpus used for the 2005 Chinese segmentation bakeoff (Emerson, 2005), a Chinese word contains on average 1.6 characters). As such,"
W19-5304,W18-2703,0,0.0200237,"the IndicNLP tokeniser4 before Moses tokenisation was applied. We also applied subword segmentation using BPE (Sennrich et al., 2016b), with joint subword vocabularies. We experimented with different numbers of BPE operations during training. 2.2 Data augmentation techniques such as backtranslation (Sennrich et al., 2016a; Edunov et al., 2018), which can be used to produce additional synthetic parallel data from monolingual data, are standard in MT. However they require a sufficiently good intermediate MT model to produce translations that are of reasonable quality to be useful for training (Hoang et al., 2018). This is extremely hard to achieve for this language pair. Our preliminary attempt at parallel-only training yielded a very low BLEU score of 7.8 on the GU→EN development set using a Nematus-trained shallow RNN with heavy regularisation,5 and similar scores were found for a Moses phrase-based translation system. Our solution was to train models for the creation of synthetic data that exploit both monolingual and parallel data during training. 2.2.1 Parallel data EN-GU GU-HI EN-HI Software data Wikipedia Wiki titles v1 Govin Bilingual dictionary Bible Emille Emille Bombay IIT 107,637 18,033 11"
W19-5304,P18-1007,0,0.0213961,"air encoding (BPE) (Sennrich et al., 2016b) can be simplified with no loss to performance. We replace BPE with the segmentation algorithm based on a Unigram Language Model (ULM) from SentencePiece, which is built into Marian. In both cases we learn 32k subword units jointly on 10M sampled English and Czech sentences. We gradually remove the elements of the pipeline and find no significant difference between the two segmentation algorithms (Table 10). We do observe a performance drop when subword resampling is used, but this has been shown to be more effective particularly for Asian languages (Kudo, 2018). For the following English-Czech experiments, we use ULM segmentation on raw text. 5.2 Experiment settings We use the transformer-base and transformer-big architectures described in Section 3.3. Models are regularised with dropout between transformer layers of 0.2 and in attention of 0.1 and feed-forward layers of 0.1, label smoothing and exponential smoothing: 0.1 and 0.0001 respectively. We optimise with Adam with a learning rate of 0.0003 and linear warm-up for first 16k updates, followed by inverted squared decay. For Transformer Big models we decrease the learning rate to 0.0002. We use"
W19-5304,D18-2012,0,0.108128,"Missing"
W19-5304,J82-2005,0,0.72863,"Missing"
W19-5304,W17-4710,1,0.899961,"Missing"
W19-5304,W18-6424,0,0.035354,". We use mini-batches dynamically fitted into 48GB of GPU memory on 4 GPUs and delay gradient updates to every second iteration, which results in mini-batches of 1-1.2k sentences. We use early stopping with a patience of 5 based on the wordlevel cross-entropy on the newsdev2016 data set. Each model is validated every 5k updates, and we use the best model checkpoint according to uncased BLEU score. Decoding is performed with beam search with a beam size of 6 with length normalisation. Additionally, we reconstruct Czech quotation marks using regular expressions as the only post-processing step (Popel, 2018). 112 5.3 Experiments and Results Results of our models are shown in Table 11. Lang. System Dev 2017 2018 EN-CS Transformer-base + Data filtering 26.7 27.1 22.9 23.4 22.9 22.6 CS-EN Transformer-base + Back-translation 32.6 37.3 28.8 31.9 30.3 32.4 EN-CS Base + Back-transl. → Transformer-big + Ensemble x2 28.4 29.6 29.6 25.1 26.3 26.5 25.1 26.2 26.3 of varying the ratio between between genuine and synthetic parallel training data. For EN→ZH, we showed that character-based decoding into Chinese produces better results than the standard subword segmentation approach. In EN→CS, we also studied the"
W19-5304,W18-6319,0,0.0360199,"↔ZH, we investigate character-level pre-processing for Chinese compared with subword segmentation. For EN→CS, we show that it is possible in high resource settings to simplify pre-processing by removing steps. 1 https://marian-nmt.github.io NMT Training settings In all experiments, we test state-of-the-art training techniques, including using ultra-large mini-batches for DE→EN and EN↔ZH, implemented as optimiser delay. Results summary Automatic evaluation results for all final systems on the WMT19 test set are summarised in Table 1. Throughout the paper, BLEU is calculated using S ACRE BLEU2 (Post, 2018) unless otherwise indicated. A selection of our final models are available to download.3 Gujarati ↔ English 2 One of the main challenges for translation between English↔Gujarati is that it is a low-resource language pair; there is little openly available parallel data and much of this data is domain-specific and/or noisy (cf. Section 2.1). Our aim was therefore to experiment how additional available data 2 https://github.com/mjpost/sacreBLEU See data.statmt.org/wmt19_systems/ for our released models and running scripts. 3 103 Proceedings of the Fourth Conference on Machine Translation (WMT), V"
W19-5304,P16-1009,1,0.927459,"ote that we did not have access to the corpora provided by the Technology Development for Indian Languages Programme, as they were only available to Indian citizens. Lang(s) Corpus #sents Ave. len. from the Moses toolkit (Koehn et al., 2007): normalisation, tokenisation, cleaning (of training data only, with a maximum sentence length of 80 tokens) and true-casing for English data, using a model trained on all available news data. The Gujarati data was additionally pre-tokenised using the IndicNLP tokeniser4 before Moses tokenisation was applied. We also applied subword segmentation using BPE (Sennrich et al., 2016b), with joint subword vocabularies. We experimented with different numbers of BPE operations during training. 2.2 Data augmentation techniques such as backtranslation (Sennrich et al., 2016a; Edunov et al., 2018), which can be used to produce additional synthetic parallel data from monolingual data, are standard in MT. However they require a sufficiently good intermediate MT model to produce translations that are of reasonable quality to be useful for training (Hoang et al., 2018). This is extremely hard to achieve for this language pair. Our preliminary attempt at parallel-only training yiel"
W19-5304,P16-1162,1,0.705506,"ote that we did not have access to the corpora provided by the Technology Development for Indian Languages Programme, as they were only available to Indian citizens. Lang(s) Corpus #sents Ave. len. from the Moses toolkit (Koehn et al., 2007): normalisation, tokenisation, cleaning (of training data only, with a maximum sentence length of 80 tokens) and true-casing for English data, using a model trained on all available news data. The Gujarati data was additionally pre-tokenised using the IndicNLP tokeniser4 before Moses tokenisation was applied. We also applied subword segmentation using BPE (Sennrich et al., 2016b), with joint subword vocabularies. We experimented with different numbers of BPE operations during training. 2.2 Data augmentation techniques such as backtranslation (Sennrich et al., 2016a; Edunov et al., 2018), which can be used to produce additional synthetic parallel data from monolingual data, are standard in MT. However they require a sufficiently good intermediate MT model to produce translations that are of reasonable quality to be useful for training (Hoang et al., 2018). This is extremely hard to achieve for this language pair. Our preliminary attempt at parallel-only training yiel"
W19-5304,W18-6430,0,0.0890608,"Missing"
W19-5304,P07-2045,1,\N,Missing
W19-5304,W18-6401,0,\N,Missing
W19-5304,W18-6425,0,\N,Missing
W19-5304,W17-4739,1,\N,Missing
