2020.acl-main.215,E17-2026,0,0.0144879,"a limited timespan of the input. The loss per timestep is defined as the categorical crossˆ (t) entropy loss between the softmax prediction y (t) and the one-hot encoded ground truth target y , L (n,t)  a) s) Θ; X(n,t , X(n,t a s  = K X (t) (t) yk log(ˆ yk ). k=1 The full set of learnable parameters Θ is jointly optimized by mini-batch stochastic gradient descent. 2.2 Multitask objective In addition to the loss functions defined above, we also consider multitask training. This has been reported to improve performance in many different domains by including a suitably related auxiliary task (Bingel and Søgaard, 2017; Mart´ınez Alonso and Plank, 2017). For the task of labelling segments in the input sequences as pertaining to annotations from among a set of K − 1 positive classes and one negative class, we propose the auxiliary task of binary labelling of segments as pertaining to either the negative class or any of the K − 1 positive classes. For question tracking, this amounts to doing binary labelling of segments that are questions of any kind. The hope is that this will make the training signal stronger since the sparsity of each of the classes, e.g. questions, is reduced by collapsing them into one s"
2020.acl-main.215,H91-1098,0,0.443344,"Missing"
2020.acl-main.215,P18-1128,0,0.0152037,"audio (A) and text (T) modalities with variations of MultiQT using modality concatenation (MultiQT) or tensor fusion (MultiQT-TF) and the auxiliary task (MultiQT-MT). The evaluation metrics are precision (P), recall (R), and (F1) at the macro level per TIMESTEP or INSTANCE. We report means and standard deviations for five-fold cross-validation runs. All F1 differences are statistically significant at p &lt; 0.001, save for between MulitQT [T] & MulitQT-MT [T], and MulitQT [A+T] & MulitQT-TF-MT [A+T] (p ≈ 0.64). We employ the approximate randomization test with R = 1000 and Bonferonni correction (Dror et al., 2018). Bold face indicates the highest F1 score within each metric and MultiQT model group. to the increased difficulty of the task: While speech intonation may be a significant feature for detecting questions in general, discerning between specific questions is easier with access to transcribed keywords. Including the auxiliary binary classification task (MultiQT-MT) shows no significant improvement over MultiQT. We hypothesize that this may be due to training on a subset of all questions such that there are unlabelled questions in the training data which add noise to the binary task. Applying ten"
2020.acl-main.215,D10-1080,0,0.0254191,"k otherwise non-sequential tasks such as syntactic parsing (G´omez-Rodr´ıguez and Vilares, 2018; Strzyz et al., 2019). By contrast, assigning labels to audio sequences 2370 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2370–2380 c July 5 - 10, 2020. 2020 Association for Computational Linguistics of human speech is comparatively less charted out. When addressed, speech labeling typically adopts a solution by proxy, which is to automatically transcribe speech into text, and then apply a text-only model (Surdeanu et al., 2005; Moll´a et al., 2007; Eidelman et al., 2010). The challenge then becomes not to natively label speech, but to adapt the model to adverse conditions of speech recognition error rates. Such models typically feature in end-to-end applications such as dialogue state tracking (Henderson et al., 2014; Ram et al., 2018). Recent advances in end-to-end neural network learning offer promise to directly label linguistic categories from speech alone (Ghannay et al., 2018). From another viewpoint, multimodal learning is successfully applied to multimedia processing where the modalities such as text, speech, and video are closely aligned. However, co"
2020.acl-main.215,D18-1162,0,0.0217932,"Missing"
2020.acl-main.215,W14-4337,0,0.0368604,"nguistics, pages 2370–2380 c July 5 - 10, 2020. 2020 Association for Computational Linguistics of human speech is comparatively less charted out. When addressed, speech labeling typically adopts a solution by proxy, which is to automatically transcribe speech into text, and then apply a text-only model (Surdeanu et al., 2005; Moll´a et al., 2007; Eidelman et al., 2010). The challenge then becomes not to natively label speech, but to adapt the model to adverse conditions of speech recognition error rates. Such models typically feature in end-to-end applications such as dialogue state tracking (Henderson et al., 2014; Ram et al., 2018). Recent advances in end-to-end neural network learning offer promise to directly label linguistic categories from speech alone (Ghannay et al., 2018). From another viewpoint, multimodal learning is successfully applied to multimedia processing where the modalities such as text, speech, and video are closely aligned. However, contributions there typically feature classification tasks such as sentiment analysis and not finer-grained multimedia sequence labeling (Zadeh et al., 2017). Our contributions. We propose a novel neural architecture to incrementally label questions in"
2020.acl-main.215,W17-4602,0,0.0316484,"Missing"
2020.acl-main.215,P16-1101,0,0.0186094,"computationally prohibitive to discover alignments between speech and its automatic transcription. For these reasons, we cannot utilize standard approaches to multimodal learning which typically rely on near-perfect crossmodal alignments between short and well-defined segments (Baltruˇsaitis et al., 2018). Context and relevance. Learning to label sequences of text is one of the more thoroughly explored topics in natural language processing. In recent times, neural networks are applied not only to sequential labeling like part-of-speech tagging (Plank et al., 2016) or named entity recognition (Ma and Hovy, 2016), but also to cast into a labeling framework otherwise non-sequential tasks such as syntactic parsing (G´omez-Rodr´ıguez and Vilares, 2018; Strzyz et al., 2019). By contrast, assigning labels to audio sequences 2370 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2370–2380 c July 5 - 10, 2020. 2020 Association for Computational Linguistics of human speech is comparatively less charted out. When addressed, speech labeling typically adopts a solution by proxy, which is to automatically transcribe speech into text, and then apply a text-only model (S"
2020.acl-main.215,E17-1005,0,0.0197899,"e loss per timestep is defined as the categorical crossˆ (t) entropy loss between the softmax prediction y (t) and the one-hot encoded ground truth target y , L (n,t)  a) s) Θ; X(n,t , X(n,t a s  = K X (t) (t) yk log(ˆ yk ). k=1 The full set of learnable parameters Θ is jointly optimized by mini-batch stochastic gradient descent. 2.2 Multitask objective In addition to the loss functions defined above, we also consider multitask training. This has been reported to improve performance in many different domains by including a suitably related auxiliary task (Bingel and Søgaard, 2017; Mart´ınez Alonso and Plank, 2017). For the task of labelling segments in the input sequences as pertaining to annotations from among a set of K − 1 positive classes and one negative class, we propose the auxiliary task of binary labelling of segments as pertaining to either the negative class or any of the K − 1 positive classes. For question tracking, this amounts to doing binary labelling of segments that are questions of any kind. The hope is that this will make the training signal stronger since the sparsity of each of the classes, e.g. questions, is reduced by collapsing them into one shared class. We use the same loss f"
2020.acl-main.215,U07-1010,0,0.123835,"Missing"
2020.acl-main.215,P17-1163,0,0.0429222,"Missing"
2020.acl-main.215,P16-2067,0,0.0311874,"ith the real-time processing constraint makes it computationally prohibitive to discover alignments between speech and its automatic transcription. For these reasons, we cannot utilize standard approaches to multimodal learning which typically rely on near-perfect crossmodal alignments between short and well-defined segments (Baltruˇsaitis et al., 2018). Context and relevance. Learning to label sequences of text is one of the more thoroughly explored topics in natural language processing. In recent times, neural networks are applied not only to sequential labeling like part-of-speech tagging (Plank et al., 2016) or named entity recognition (Ma and Hovy, 2016), but also to cast into a labeling framework otherwise non-sequential tasks such as syntactic parsing (G´omez-Rodr´ıguez and Vilares, 2018; Strzyz et al., 2019). By contrast, assigning labels to audio sequences 2370 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2370–2380 c July 5 - 10, 2020. 2020 Association for Computational Linguistics of human speech is comparatively less charted out. When addressed, speech labeling typically adopts a solution by proxy, which is to automatically transcribe speec"
2020.acl-main.215,N19-1077,0,0.0353108,"Missing"
2020.coling-main.345,P14-1126,0,0.0570066,"Missing"
2020.coling-main.345,D11-1006,0,0.0370746,"merges parses by all 42 parsers, but uses oracle performance as parser weights; E NS -A LL – ensembles all 42 parsers, with equal weights. An exception is the MSP model which is not an ensemble model, but rather trains a single parser on the concatenation of all training treebanks. Ma & Mi: average performance across 20 languages, macro- and micro-averaged scores, respectively. The best result in each column, not considering oracle scores, is in bold. 6 Related Work Parsing languages with no training data has been a very active topic of research for nearly a decade since the pivotal works by McDonald et al. (2011) and Petrov et al. (2012). Many diverse approaches are explored along the lines of model transfer, annotation projection, machine translation (T¨ackstr¨om et al., 2013; Guo et al., 2015; Zhang and Barzilay, 2015; Tiedemann and Agi´c, 2016; Rasooli and Collins, 2017), and selective sharing based on language typology (Naseem et al., 2012) and structural similarity (Ponti et al., 2018; Meng et al., 2019). However, vast majority of prior work involves bulk evaluation, whereby transfer parsers are validated by mean accuracy on test data. Such evaluation protocols stand in contrast with the fact tha"
2020.coling-main.345,D19-1103,0,0.0113354,"lumn, not considering oracle scores, is in bold. 6 Related Work Parsing languages with no training data has been a very active topic of research for nearly a decade since the pivotal works by McDonald et al. (2011) and Petrov et al. (2012). Many diverse approaches are explored along the lines of model transfer, annotation projection, machine translation (T¨ackstr¨om et al., 2013; Guo et al., 2015; Zhang and Barzilay, 2015; Tiedemann and Agi´c, 2016; Rasooli and Collins, 2017), and selective sharing based on language typology (Naseem et al., 2012) and structural similarity (Ponti et al., 2018; Meng et al., 2019). However, vast majority of prior work involves bulk evaluation, whereby transfer parsers are validated by mean accuracy on test data. Such evaluation protocols stand in contrast with the fact that languages exhibit high variance in syntactic structure, which calls for a sensitive treatment of every sentence. While an oracle single-source parser may be appropriate for the majority of sentences in a given dataset, instance-based treatment closes the gap to the best achievable result given an array of pretrained parsers, as we also show in §2. Early efforts in this line of research include data"
2020.coling-main.345,K19-1029,0,0.0286747,"Missing"
2020.coling-main.345,P12-1066,0,0.0306445,"and micro-averaged scores, respectively. The best result in each column, not considering oracle scores, is in bold. 6 Related Work Parsing languages with no training data has been a very active topic of research for nearly a decade since the pivotal works by McDonald et al. (2011) and Petrov et al. (2012). Many diverse approaches are explored along the lines of model transfer, annotation projection, machine translation (T¨ackstr¨om et al., 2013; Guo et al., 2015; Zhang and Barzilay, 2015; Tiedemann and Agi´c, 2016; Rasooli and Collins, 2017), and selective sharing based on language typology (Naseem et al., 2012) and structural similarity (Ponti et al., 2018; Meng et al., 2019). However, vast majority of prior work involves bulk evaluation, whereby transfer parsers are validated by mean accuracy on test data. Such evaluation protocols stand in contrast with the fact that languages exhibit high variance in syntactic structure, which calls for a sensitive treatment of every sentence. While an oracle single-source parser may be appropriate for the majority of sentences in a given dataset, instance-based treatment closes the gap to the best achievable result given an array of pretrained parsers, as we als"
2020.coling-main.345,petrov-etal-2012-universal,0,0.639447,"2020; Lauscher et al., 2020). Therefore, cross-lingual transfer of dependency parsers has profiled as the most viable strategy to use parsing technology in resource-low languages (McDonald et al., 2011; Søgaard, 2011; Kondratyuk ¨ un et al., 2020). Delexicalized transfer is conceptually the least demanding option and Straka, 2019; Ust¨ in terms of language-specific resource requirements. The only provision, in order to transfer the parser trained on a delexicalized treebank of a resource-rich language, is a POS tagger in a low-resource target language based on the Universal POS (UPOS) tagset (Petrov et al., 2012). Delexicalized transfer is nowadays used primarily as a simple yet competitive baseline for more sophisticated transfer models when porting parsing technology in a new language. However, in realistic truly low-resource setups, one cannot guarantee additional resources such as parallel sentences (Ma and Xia, 2014; Rasooli and Collins, 2015; Rasooli and Collins, 2017; Wang et al., 2019; Zhang et al., 2019), word alignments (Lacroix et al., 2016), sufficiently large monolingual corpus in the target language (Mulcaire et al., 2019), and language coverage This work is licensed under a Creative Com"
2020.coling-main.345,P11-1157,0,0.0929328,"Missing"
2020.coling-main.345,P18-1142,1,0.928731,"Missing"
2020.coling-main.345,J19-3005,1,0.844979,"Missing"
2020.coling-main.345,D15-1039,0,0.0451007,"Missing"
2020.coling-main.345,Q17-1020,0,0.0720177,"ific resource requirements. The only provision, in order to transfer the parser trained on a delexicalized treebank of a resource-rich language, is a POS tagger in a low-resource target language based on the Universal POS (UPOS) tagset (Petrov et al., 2012). Delexicalized transfer is nowadays used primarily as a simple yet competitive baseline for more sophisticated transfer models when porting parsing technology in a new language. However, in realistic truly low-resource setups, one cannot guarantee additional resources such as parallel sentences (Ma and Xia, 2014; Rasooli and Collins, 2015; Rasooli and Collins, 2017; Wang et al., 2019; Zhang et al., 2019), word alignments (Lacroix et al., 2016), sufficiently large monolingual corpus in the target language (Mulcaire et al., 2019), and language coverage This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 3886 Proceedings of the 28th International Conference on Computational Linguistics, pages 3886–3898 Barcelona, Spain (Online), December 8-13, 2020 in massively multilingual language models which are used as the basis for modern parsers (Kondratyuk ¨ un et al.,"
2020.coling-main.345,P15-2040,0,0.277129,"Missing"
2020.coling-main.345,N06-2033,0,0.09635,"determine a threshold τ ∈ [0, 1] that defines the set of “good enough” parsers, in relative terms w.r.t. the performance of the best parser. The sets of parsers whose trees are to be merged are obtained as follows: {iILPS }τ (j) = {i|∀i : yˆi,j ≥ max(ˆ yi,j ) · τ }, (8) {iSBPSILPS }τ = {i |∀i : y¯i ≥ max(¯ yi ) · τ }. (9) where Eq (8) refers to the pure ILPS setting, and Eq. (9) refers to the SBPSILPS setting. 3.4 Reparsing After selecting multiple parsers in the ensemble settings, we need to merge their produced parse trees into a final tree. Such a step is commonly referred to as reparsing (Sagae and Lavie, 2006). Here we resort to a standard reparsing procedure in which we: (1) merge the trees produced by individual parsers into a weighted graph G – the parser i contributes to an edge with the weight wi = yˆi,j (for pure ILPS; for SBPSILPS , wi = y¯i ) if the parser i predicted that edge, and with wi = 0 otherwise; (2) induce the Maximum Spanning Tree (MST) of G (Edmonds, 1967) as the final parse of the input UPOS-sentence (see again Figure 2). 4 Experimental Setup Data. We perform all experiments on the UD v2.3 dataset,7 as it contains a wide array of both resourcerich languages with large treebanks"
2020.coling-main.345,D18-1545,0,0.0710174,"Missing"
2020.coling-main.345,C12-2115,0,0.20997,"ance-based treatment closes the gap to the best achievable result given an array of pretrained parsers, as we also show in §2. Early efforts in this line of research include data point selection where language models are used to capture the prevalent syntactic structure of a language and score potential training instances such that a multi-source parser is trained on the mixture of training instances that are most similar to the test language instances (Søgaard, 2011). Instead of instance selection one can also apply instance reweighting in accordance to their similarity to the test language (Søgaard and Wulff, 2012). Regardless of whether we attempt to align languages on an instance-level or on a treebank-level there is a need for a similarity measure between languages. Prior work relied on existing manually curated resources such as the URIEL ˇ database (Littell et al., 2017), using the KL-Divergence on POS-trigrams (Rosa and Zabokrtsk´ y, 2015), or handcrafted features derived from the datasets at hand. Our work is most similar to the recent work of Lin et al. (2019): they learn to score and rank languages in order to predict the top transfer languages. However, contrary to their work, our approach doe"
2020.coling-main.345,P11-2120,0,0.0388067,"ve treatment of every sentence. While an oracle single-source parser may be appropriate for the majority of sentences in a given dataset, instance-based treatment closes the gap to the best achievable result given an array of pretrained parsers, as we also show in §2. Early efforts in this line of research include data point selection where language models are used to capture the prevalent syntactic structure of a language and score potential training instances such that a multi-source parser is trained on the mixture of training instances that are most similar to the test language instances (Søgaard, 2011). Instead of instance selection one can also apply instance reweighting in accordance to their similarity to the test language (Søgaard and Wulff, 2012). Regardless of whether we attempt to align languages on an instance-level or on a treebank-level there is a need for a similarity measure between languages. Prior work relied on existing manually curated resources such as the URIEL ˇ database (Littell et al., 2017), using the KL-Divergence on POS-trigrams (Rosa and Zabokrtsk´ y, 2015), or handcrafted features derived from the datasets at hand. Our work is most similar to the recent work of Lin"
2020.coling-main.345,N13-1126,0,0.0427885,"Missing"
2020.coling-main.345,2020.emnlp-main.180,0,0.0293142,"Missing"
2020.coling-main.345,D19-1102,0,0.0129897,"¨ un et al., 2020). Thus, delexicalized transfer still remains a widely useful baseline and Straka, 2019; Ust¨ and plausible option (Johannsen et al., 2016; Agi´c, 2017). Cross-lingual transfer comes in two main flavors. We either (1) choose the best parser from a set of available parsers, trained on treebanks of various resource-rich languages (single-best parser selection, SBPS) or (2) use the parser trained on a mixture of treebanks of (ideally related) resource-rich languages (multi-source parser transfer, MSP). Other transfer paradigms, like data augmentation (S¸ahin and Steedman, 2018; Vania et al., 2019), assume the existence of at least a small treebank for a target language, violating the assumption of a (treebank-wise) fully low-resource target language. Both SBPS and MSP rely on some measure of structural alignment between languages in order to select either the single best source language parser (SBPS) or a set of (syntactically related) source languages (MSP). Existing solutions rely on measures like the Kullback–Leibler (KL) divergence between sourceˇ and target-language distributions of POS trigrams (Rosa and Zabokrtsk´ y, 2015), which can be unreliable for small target language corpo"
2020.coling-main.345,Q16-1035,0,0.0200005,"er languages. However, contrary to their work, our approach does not employ a model to learn the ranking, but transforms the labels to directly reflect the ranking when we train the scoring model. In addition, we stress the importance of instance-based learning for cross-lingual parser transfer in particular. Another core difference is that our approach is an end-to-end system without external resources or handcrafted static features. Instead, our framework relies on trainable parser embeddings that encode the necessary features in a single representation. From another viewpoint, the work of (Wang and Eisner, 2016; Wang and Eisner, 2018a; Wang and Eisner, 2018b) explores the potential of synthesizing and reordering delexicalized POS sequences to come up with better parser transfer without unrealistic assumptions on target-language resources. Their work in synthetic delexicalization is compatible with ours as it lends itself entirely to instance-based parsing. Finally, the line of work by Ammar et al. (2016) in learning monolithic models over multiple languages, 3894 and its continuation for zero-shot learning by Kondratyuk and Straka (2019) also promises to abstract away from language boundaries, but s"
2020.coling-main.345,Q18-1046,0,0.0679321,"w-resource languages with small test treebanks. For our experiments, we select 42 languages with the largest treebanks as our resource-rich source languages for training, and a set of 20 typologically diverse low-resource 7 https://universaldependencies.org/ 3891 Figure 3: Performance (UAS) for single-parser se- Figure 4: Performance (UAS) for ensemble (i.e., lection models, micro- and macro- averaged, respec- few-parser selection) models, micro- and macrotively, across 20 test languages. averaged, respectively, across 20 test languages. languages for testing.8 Following established practice (Wang and Eisner, 2018b), at inference we use gold UPOS-tags of test treebanks for all models in comparison.9 ILPS Hyperparameters are optimized via fixed-split cross-validation on our training set (see §3.1). We set the embedding size for both parser embeddings and UPOS-tag embeddings, as well as the hidden size of the feed-forward Transformer layers to 256. The transformer encoder has NT = 3 layers with 8 attention heads in each layer. We update the model in mini-batches of 16 examples, using Adam (Kingma and Ba, 2015) with the default parameters: β1 = 0.9, β2 = 0.999, and  = 10−8 , with an initial learning rate"
2020.coling-main.345,D18-1163,0,0.0518162,"w-resource languages with small test treebanks. For our experiments, we select 42 languages with the largest treebanks as our resource-rich source languages for training, and a set of 20 typologically diverse low-resource 7 https://universaldependencies.org/ 3891 Figure 3: Performance (UAS) for single-parser se- Figure 4: Performance (UAS) for ensemble (i.e., lection models, micro- and macro- averaged, respec- few-parser selection) models, micro- and macrotively, across 20 test languages. averaged, respectively, across 20 test languages. languages for testing.8 Following established practice (Wang and Eisner, 2018b), at inference we use gold UPOS-tags of test treebanks for all models in comparison.9 ILPS Hyperparameters are optimized via fixed-split cross-validation on our training set (see §3.1). We set the embedding size for both parser embeddings and UPOS-tag embeddings, as well as the hidden size of the feed-forward Transformer layers to 256. The transformer encoder has NT = 3 layers with 8 attention heads in each layer. We update the model in mini-batches of 16 examples, using Adam (Kingma and Ba, 2015) with the default parameters: β1 = 0.9, β2 = 0.999, and  = 10−8 , with an initial learning rate"
2020.coling-main.345,D19-1575,0,0.0388791,"Missing"
2020.coling-main.345,D15-1213,0,0.017293,"her trains a single parser on the concatenation of all training treebanks. Ma & Mi: average performance across 20 languages, macro- and micro-averaged scores, respectively. The best result in each column, not considering oracle scores, is in bold. 6 Related Work Parsing languages with no training data has been a very active topic of research for nearly a decade since the pivotal works by McDonald et al. (2011) and Petrov et al. (2012). Many diverse approaches are explored along the lines of model transfer, annotation projection, machine translation (T¨ackstr¨om et al., 2013; Guo et al., 2015; Zhang and Barzilay, 2015; Tiedemann and Agi´c, 2016; Rasooli and Collins, 2017), and selective sharing based on language typology (Naseem et al., 2012) and structural similarity (Ponti et al., 2018; Meng et al., 2019). However, vast majority of prior work involves bulk evaluation, whereby transfer parsers are validated by mean accuracy on test data. Such evaluation protocols stand in contrast with the fact that languages exhibit high variance in syntactic structure, which calls for a sensitive treatment of every sentence. While an oracle single-source parser may be appropriate for the majority of sentences in a given"
2020.coling-main.345,D19-1092,0,0.0217857,"Missing"
2020.coling-main.345,P15-1119,0,\N,Missing
2020.coling-main.345,N16-1121,0,\N,Missing
2020.coling-main.345,P16-2091,1,\N,Missing
2020.coling-main.345,W17-0401,1,\N,Missing
2020.coling-main.345,K17-3002,0,\N,Missing
2020.coling-main.345,E17-2002,0,\N,Missing
2020.coling-main.345,N19-1423,0,\N,Missing
agic-etal-2010-towards,W07-1702,1,\N,Missing
agic-etal-2010-towards,P07-1124,0,\N,Missing
agic-etal-2010-towards,P02-1053,0,\N,Missing
agic-etal-2010-towards,devitt-ahmad-2008-sentiment,0,\N,Missing
agic-etal-2010-towards,ahmad-etal-2006-sentiments,0,\N,Missing
agic-etal-2010-towards,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
agic-etal-2014-croatian,C10-1011,0,\N,Missing
agic-etal-2014-croatian,W06-2920,0,\N,Missing
agic-etal-2014-croatian,H05-1066,0,\N,Missing
agic-etal-2014-croatian,Q13-1034,0,\N,Missing
agic-etal-2014-croatian,W13-4903,1,\N,Missing
agic-etal-2014-croatian,berovic-etal-2012-croatian,1,\N,Missing
agic-etal-2014-croatian,N13-1013,0,\N,Missing
agic-etal-2014-croatian,C12-2082,0,\N,Missing
agic-etal-2014-croatian,D07-1096,0,\N,Missing
agic-etal-2014-croatian,P13-2017,0,\N,Missing
agic-ljubesic-2014-setimes,C10-1011,0,\N,Missing
agic-ljubesic-2014-setimes,E03-1009,0,\N,Missing
agic-ljubesic-2014-setimes,W06-2920,0,\N,Missing
agic-ljubesic-2014-setimes,P05-1045,0,\N,Missing
agic-ljubesic-2014-setimes,H05-1066,0,\N,Missing
agic-ljubesic-2014-setimes,W03-0419,0,\N,Missing
agic-ljubesic-2014-setimes,W13-4903,1,\N,Missing
agic-ljubesic-2014-setimes,W13-2408,1,\N,Missing
agic-ljubesic-2014-setimes,berovic-etal-2012-croatian,1,\N,Missing
agic-ljubesic-2014-setimes,D07-1096,0,\N,Missing
agic-ljubesic-2014-setimes,erjavec-etal-2010-jos,0,\N,Missing
agic-tadic-2006-evaluating,dzeroski-etal-2000-morphosyntactic,0,\N,Missing
agic-tadic-2006-evaluating,A00-2013,0,\N,Missing
agic-tadic-2006-evaluating,W03-2906,1,\N,Missing
agic-tadic-2006-evaluating,J95-4004,0,\N,Missing
agic-tadic-2006-evaluating,A00-1031,0,\N,Missing
agic-tadic-2006-evaluating,J01-2002,0,\N,Missing
agic-tadic-2006-evaluating,tadic-2000-building,1,\N,Missing
berovic-etal-2012-croatian,D07-1099,0,\N,Missing
berovic-etal-2012-croatian,passarotti-dellorletta-2010-improvements,0,\N,Missing
berovic-etal-2012-croatian,nivre-etal-2006-maltparser,0,\N,Missing
berovic-etal-2012-croatian,nilsson-nivre-2008-malteval,0,\N,Missing
berovic-etal-2012-croatian,tadic-2002-building,1,\N,Missing
berovic-etal-2012-croatian,W06-2920,0,\N,Missing
berovic-etal-2012-croatian,dzeroski-etal-2006-towards,0,\N,Missing
berovic-etal-2012-croatian,D07-1096,0,\N,Missing
berovic-etal-2012-croatian,D07-1119,0,\N,Missing
D18-1061,Q16-1022,1,0.912057,"Missing"
D18-1061,P13-1057,0,0.0191641,"pensate for the absence of direct supervision, work in crosslingual learning and distant supervision has discovered creative use for a number of alternative data sources to learn feasible models: – aligned parallel corpora to project POS annotations to target languages (Yarowsky et al., 2001; Agi´c et al., 2015; Fang and Cohn, 2016), – noisy tag dictionaries for type-level approximation of full supervision (Li et al., 2012), – combination of projection and type constraints (Das and Petrov, 2011; T¨ackstr¨om et al., 2013), – rapid annotation of seed training data (Garrette and Baldridge, 2013; Garrette et al., 2013). However, only one or two compatible sources of distant supervision are typically employed. In reality severely under-resourced languages may require a more pragmatic “take what you can get” viewpoint. Our results suggest that combining supervision sources is the way to go about creating viable low-resource taggers. We propose a method to strike a balance between model simplicity and the capacity to easily integrate heterogeneous learning signals. Our system is a uniform neural model for POS tagging that learns from disparate sources of distant supervision (D S D S). We use it to combine: i)"
D18-1061,E17-2040,1,0.905539,"Missing"
D18-1061,W18-3401,1,0.886224,"Missing"
D18-1061,A00-1031,0,0.742426,"Missing"
D18-1061,Q16-1023,0,0.0929291,"Missing"
D18-1061,W06-2920,0,0.0763302,"Missing"
D18-1061,L16-1498,0,0.0355781,"n l-dimensional space. We represent ~esrc as concatenation of all embedded m properties of length l, and a zero vector otherwise. Tuning on the dev set, we found the second embedding approach to perform best, and simple concatenation outperformed mean vector representations. We evaluate two dictionary sources, motivated by ease of accessibility to many languages: W IK TIONARY , a word type dictionary that maps tokens to one of the 12 Universal POS tags (Li et al., 2012; Petrov et al., 2012); and U NI M ORPH, a morphological dictionary that provides inflectional paradigms across 350 languages (Kirov et al., 2016). For Wiktionary, we use the freely available dictionaries from Li et al. (2012) and Agi´c et al. (2017). The size of the dictionaries ranges from a few thousands (e.g., Hindi and Bulgarian) to 2M (Finnish UniMorph). Sizes are provided in Table 1, first columns. UniMorph covers between 8-38 morphological properties (for English and Finnish, respectively). Word embeddings. Embeddings are available for many languages. Pre-initialization of w ~ offers consistent and considerable performance improvements in our distant supervision setup (Section 4). We use off-the-shelf Polyglot embeddings (AlRfou"
D18-1061,P17-1177,0,0.0384161,"the languages from Li et al. (2012) and all the remaining languages in Table 1. Cohn, 2016; Kann et al., 2018). Our paper contributes to this literature by leveraging a range of prior directions in a unified, neural test bed. Most prior work on neural sequence prediction follows the commonly perceived wisdom that hand-crafted features are unnecessary for deep learning methods. They rely on end-to-end training without resorting to additional linguistic resources. Our study shows that this is not the case. Only few prior studies investigate such sources, e.g., for MT (Sennrich and Haddow, 2016; Chen et al., 2017; Li et al., 2017; Passban et al., 2018) and Sagot and Mart´ınez Alonso (2017) for POS tagging use lexicons, but only as n-hot features and without examining the cross-lingual aspect. 7 Conclusions We show that our approach of distant supervision from disparate sources (D S D S) is simple yet surprisingly effective for low-resource POS tagging. Only 5k instances of projected data paired with off-the-shelf embeddings and lexical information integrated into a neural tagger are sufficient to reach a new state of the art, and both data selection and embeddings are essential components to boost neu"
D18-1061,P17-1064,0,0.0349681,"Li et al. (2012) and all the remaining languages in Table 1. Cohn, 2016; Kann et al., 2018). Our paper contributes to this literature by leveraging a range of prior directions in a unified, neural test bed. Most prior work on neural sequence prediction follows the commonly perceived wisdom that hand-crafted features are unnecessary for deep learning methods. They rely on end-to-end training without resorting to additional linguistic resources. Our study shows that this is not the case. Only few prior studies investigate such sources, e.g., for MT (Sennrich and Haddow, 2016; Chen et al., 2017; Li et al., 2017; Passban et al., 2018) and Sagot and Mart´ınez Alonso (2017) for POS tagging use lexicons, but only as n-hot features and without examining the cross-lingual aspect. 7 Conclusions We show that our approach of distant supervision from disparate sources (D S D S) is simple yet surprisingly effective for low-resource POS tagging. Only 5k instances of projected data paired with off-the-shelf embeddings and lexical information integrated into a neural tagger are sufficient to reach a new state of the art, and both data selection and embeddings are essential components to boost neural tagging perfo"
D18-1061,P11-1061,0,0.336529,"e languages lack manually annotated data to learn even the most basic models such as part-of-speech (POS) taggers. To compensate for the absence of direct supervision, work in crosslingual learning and distant supervision has discovered creative use for a number of alternative data sources to learn feasible models: – aligned parallel corpora to project POS annotations to target languages (Yarowsky et al., 2001; Agi´c et al., 2015; Fang and Cohn, 2016), – noisy tag dictionaries for type-level approximation of full supervision (Li et al., 2012), – combination of projection and type constraints (Das and Petrov, 2011; T¨ackstr¨om et al., 2013), – rapid annotation of seed training data (Garrette and Baldridge, 2013; Garrette et al., 2013). However, only one or two compatible sources of distant supervision are typically employed. In reality severely under-resourced languages may require a more pragmatic “take what you can get” viewpoint. Our results suggest that combining supervision sources is the way to go about creating viable low-resource taggers. We propose a method to strike a balance between model simplicity and the capacity to easily integrate heterogeneous learning signals. Our system is a uniform"
D18-1061,D12-1127,0,0.229408,"Missing"
D18-1061,H01-1035,0,0.93712,"urprisingly effective, resulting in a new state of the art without access to any gold annotated data. Figure 1: Illustration of D S D S (Distant Supervision from Disparate Sources). 1 Introduction Low-resource languages lack manually annotated data to learn even the most basic models such as part-of-speech (POS) taggers. To compensate for the absence of direct supervision, work in crosslingual learning and distant supervision has discovered creative use for a number of alternative data sources to learn feasible models: – aligned parallel corpora to project POS annotations to target languages (Yarowsky et al., 2001; Agi´c et al., 2015; Fang and Cohn, 2016), – noisy tag dictionaries for type-level approximation of full supervision (Li et al., 2012), – combination of projection and type constraints (Das and Petrov, 2011; T¨ackstr¨om et al., 2013), – rapid annotation of seed training data (Garrette and Baldridge, 2013; Garrette et al., 2013). However, only one or two compatible sources of distant supervision are typically employed. In reality severely under-resourced languages may require a more pragmatic “take what you can get” viewpoint. Our results suggest that combining supervision sources is the way t"
D18-1061,N18-1006,0,0.0796088,"Missing"
D18-1061,petrov-etal-2012-universal,0,0.086565,", with m the number of lexicon properties; b) by embedding the lexical features, i.e., ~esrc is a lexicon src embedded into an l-dimensional space. We represent ~esrc as concatenation of all embedded m properties of length l, and a zero vector otherwise. Tuning on the dev set, we found the second embedding approach to perform best, and simple concatenation outperformed mean vector representations. We evaluate two dictionary sources, motivated by ease of accessibility to many languages: W IK TIONARY , a word type dictionary that maps tokens to one of the 12 Universal POS tags (Li et al., 2012; Petrov et al., 2012); and U NI M ORPH, a morphological dictionary that provides inflectional paradigms across 350 languages (Kirov et al., 2016). For Wiktionary, we use the freely available dictionaries from Li et al. (2012) and Agi´c et al. (2017). The size of the dictionaries ranges from a few thousands (e.g., Hindi and Bulgarian) to 2M (Finnish UniMorph). Sizes are provided in Table 1, first columns. UniMorph covers between 8-38 morphological properties (for English and Finnish, respectively). Word embeddings. Embeddings are available for many languages. Pre-initialization of w ~ offers consistent and consider"
D18-1061,P16-2067,1,0.92707,"Missing"
D18-1061,W17-6304,0,0.0822315,"Missing"
D18-1061,W16-2209,0,0.0294493,"M iterations, separate for the languages from Li et al. (2012) and all the remaining languages in Table 1. Cohn, 2016; Kann et al., 2018). Our paper contributes to this literature by leveraging a range of prior directions in a unified, neural test bed. Most prior work on neural sequence prediction follows the commonly perceived wisdom that hand-crafted features are unnecessary for deep learning methods. They rely on end-to-end training without resorting to additional linguistic resources. Our study shows that this is not the case. Only few prior studies investigate such sources, e.g., for MT (Sennrich and Haddow, 2016; Chen et al., 2017; Li et al., 2017; Passban et al., 2018) and Sagot and Mart´ınez Alonso (2017) for POS tagging use lexicons, but only as n-hot features and without examining the cross-lingual aspect. 7 Conclusions We show that our approach of distant supervision from disparate sources (D S D S) is simple yet surprisingly effective for low-resource POS tagging. Only 5k instances of projected data paired with off-the-shelf embeddings and lexical information integrated into a neural tagger are sufficient to reach a new state of the art, and both data selection and embeddings are essential comp"
D18-1061,Q13-1001,0,0.536998,"Missing"
D18-1061,C18-1327,0,0.0725938,"Missing"
D18-1061,W13-3520,0,\N,Missing
D18-1061,N13-1014,0,\N,Missing
D18-1061,P15-2044,1,\N,Missing
D18-1061,K16-1018,0,\N,Missing
E14-2003,W07-1702,1,0.860388,"Missing"
E14-2003,P05-1045,0,0.00899297,"ambiguation is based on FreeLing. Frame extraction is rule-based since no SRL corpus is available for Croatian. • Chinese: Chinese shallow and deep processing is based on a word segmentation component ICTCLAS8 and a semantic dependency parser trained on CSDN corpus. Then, rulebased frame extraction is performed (no SRL corpus nor WordNet are available for Chinese). • Spanish, English, and Catalan: all modules are based on FreeLing (Padr´o and Stanilovsky, 2012) and Treeler. • German: German shallow processing is based on OpenNLP6 , Stanford POS tagger and NE extractor (Toutanova et al., 2003; Finkel et al., 2005). Dependency parsing, semantic role labeling, word sense disambiguation, and SRL-based frame extraction are based on FreeLing and Treeler. • Slovene: Slovene shallow processing is proˇ vided by JSI Enrycher7 (Stajner et al., 2010), which consists of the Obeliks morphosyntactic analysis library (Grˇcar et al., 2012), the LemmaGen lemmatizer (Jurˇsiˇc et al., 2010) ˇ and a CRF-based entity extractor (Stajner et al., 2012). Dependency parsing, word sense Each language analysis service is able to process thousands of words per second when performing shallow analysis (up to NE recognition), and hun"
E14-2003,Q13-1018,1,0.896749,"Missing"
E14-2003,W14-0150,0,0.021713,"ervices following a lightweigth SOA architecture approach, and they are publically accessible and shared through META-SHARE.1 1 2 Linguistic Analyzers Apart from basic state-of-the-art tokenizers, lemmatizers, PoS/MSD taggers, and NE recognizers, each pipeline requires deeper processors able to build the target language-independent semantic representantion. For that, we rely on three steps: dependency parsing, semantic role labeling and word sense disambiguation. These three processes, combined with multilingual ontological resouces such as different WordNets and PredicateMatrix (L´opez de la Calle et al., 2014), a lexical semantics resource combining WordNet, FrameNet, and VerbNet, are the key to the construction of our semantic representation. Introduction Project XLike2 goal is to develop technology able to gather documents in a variety of languages and genres (news, blogs, tweets, etc.) and to extract language-independent knowledge from them, in order to provide new and better services to publishers, media monitoring, and business intelligence. Thus, project use cases are provided by STA (Slovenian Press Agency) and Bloomberg, as well as New York Times as an associated partner. Research partners"
E14-2003,P05-1012,0,0.0466071,"d by STA (Slovenian Press Agency) and Bloomberg, as well as New York Times as an associated partner. Research partners in the project are Joˇzef Stefan Institute (JSI), Karlsruhe Institute of Technology (KIT), Universitat Polit`ecnica de Catalunya (UPC), University of Zagreb (UZG), and Tsinghua University (THU). The Spanish company iSOCO is in charge of integration of all components developed in the project. This paper deals with the language technology developed within the project XLike to convert in2.1 Dependency Parsing We use graph-based methods for dependency parsing, namely, MSTParser3 (McDonald et al., 2005) is used for Chinese and Croatian, and Treeler4 is used for the other languages. Treeler is a library developed by the UPC team that implements several statistical methods for tagging and parsing. We use these tools in order to train dependency parsers for all XLike languages using standard available treebanks. 1 accessible and shared here means that the services are publicly callable, not that the code is open-source. 3 http://www.meta-share.eu 2 http://www.xlike.org 4 http://sourceforge.net/projects/mstparser http://treeler.lsi.upc.edu 9 Proceedings of the Demonstrations at the 14th Conferen"
E14-2003,N03-1033,0,0.0120884,"c, 2012). Word sense disambiguation is based on FreeLing. Frame extraction is rule-based since no SRL corpus is available for Croatian. • Chinese: Chinese shallow and deep processing is based on a word segmentation component ICTCLAS8 and a semantic dependency parser trained on CSDN corpus. Then, rulebased frame extraction is performed (no SRL corpus nor WordNet are available for Chinese). • Spanish, English, and Catalan: all modules are based on FreeLing (Padr´o and Stanilovsky, 2012) and Treeler. • German: German shallow processing is based on OpenNLP6 , Stanford POS tagger and NE extractor (Toutanova et al., 2003; Finkel et al., 2005). Dependency parsing, semantic role labeling, word sense disambiguation, and SRL-based frame extraction are based on FreeLing and Treeler. • Slovene: Slovene shallow processing is proˇ vided by JSI Enrycher7 (Stajner et al., 2010), which consists of the Obeliks morphosyntactic analysis library (Grˇcar et al., 2012), the LemmaGen lemmatizer (Jurˇsiˇc et al., 2010) ˇ and a CRF-based entity extractor (Stajner et al., 2012). Dependency parsing, word sense Each language analysis service is able to process thousands of words per second when performing shallow analysis (up to NE"
E14-2003,C12-2001,0,0.0402396,"Missing"
E14-2003,E09-1005,0,0.0627699,"s performed for all languages with a publicly available WordNet. This includes all languages in the project except Chinese. The goal of WSD is to map specific languages to a common semantic space, in this case, WN synsets. Thanks to existing connections between WN and other resources, SUMO and OpenCYC sense codes are also output when available. Thanks to PredicateMatrix, the obtained concepts can be projected to FrameNet, achieving a normalization of the semantic roles produced by the SRL (which are treebank-dependent, and thus, not the same for all languages). The used WSD engine is the UKB (Agirre and Soroa, 2009) implementation provided by FreeLing (Padr´o and Stanilovsky, 2012). 2.4 3 Cross-lingual Semantic Annotation This step adds further semantic annotations on top of the results obtained by linguistic processing. All XLike languages are covered. The goal is to map word phrases in different languages into the same semantic interlingua, which consists of resources specified in knowledge bases such as Wikipedia and Linked Open Data (LOD) sources. Cross-lingual semantic annotation is performed in two stages: (1) first, candidate concepts in the knowledge base are linked to the linguistic resources ba"
E14-2003,padro-stanilovsky-2012-freeling,1,\N,Missing
E17-1022,W15-5301,1,0.894616,"Missing"
E17-1022,P15-2044,1,0.884906,"Missing"
E17-1022,W09-0106,0,0.0296839,"t such a formalism lends itself more naturally to a simple and linguistically sound rulebased approach to cross-lingual parsing. In this paper we present such an approach. Our system is a dependency parser that requires no training, and relies solely on explicit part-ofspeech (POS) constraints that UD imposes. In particular, UD prescribes that trees are single-rooted, and that function words like adpositions, auxiliaries, and determiners are always dependents of content words, while other formalisms might treat them as heads (De Marneffe et al., 2014). We ascribe our work to the viewpoints of Bender (2009) about the incorporation of linguistic knowledge in language-independent systems. We propose UDP, the first training-free parser for Universal Dependencies (UD). Our algorithm is based on PageRank and a small set of head attachment rules. It features two-step decoding to guarantee that function words are attached as leaf nodes. The parser requires no training, and it is competitive with a delexicalized transfer system. UDP offers a linguistically sound unsupervised alternative to cross-lingual parsing for UD, which can be used as a baseline for such systems. The parser has very few parameters"
E17-1022,A00-1031,0,0.0296335,"ord forms. If there is more than one treebank per language, we use the treebank that has the 5.2 The resulting trees always pass the validation script in github.com/UniversalDependencies/tools. They also had a special connection to some extremists They - also had - • • • • a special connection - some extremists • • - to • • • • • - - Evaluation setup Our system relies solely on POS tags. To estimate the quality degradation of our system under non-gold POS scenarios, we evaluate UDP on two alternative scenarios. The first is predicted POS (UDPP ), where we tag the respective test set with TnT (Brants, 2000) trained on each language’s training set. The second is a naive typeconstrained two-POS tag scenario (UDPN ), and approximates a lower bound. We give each word either CONTENT or FUNCTION tag, depending on the word’s frequency. The 100 most frequent words of the input test section receive the FUNC TION tag. 4 −→ Baseline • • • • • - Table 4: Matrix representation of the directed graph for the words in the sentence. 234 Finally, we compare our parser UDP to a supervised cross-lingual system (MSD). It is a multisource delexicalized transfer parser, referred to as multi-dir in the original paper b"
E17-1022,P11-1061,0,0.0305356,"ing, 2004; Spitkovsky et al., 2010a; Spitkovsky et al., 2010b). Still, the performance of these parsers falls far behind the approaches involving any sort of supervision. Our work builds on the line of research on ruleaided unsupervised dependency parsing by Gillenwater et al. (2010) and Naseem et al. (2010), and also relates to Søgaard’s (2012a; 2012b) work. Our parser, however, features two key differences: Related work Cross-lingual learning Recent years have seen exciting developments in cross-lingual linguistic structure prediction based on transfer or projection of POS and dependencies (Das and Petrov, 2011; McDonald et al., 2011). These works mainly use supervised learning and domain adaptation techniques for the target language. The first group of approaches deals with annotation projection (Yarowsky et al., 2001), whereby parallel corpora are used to transfer annotations between resource-rich source languages and lowresource target languages. Projection relies on the availability and quality of parallel corpora, sourceside taggers and parsers, but also tokenizers, sentence aligners, and word aligners for sources and targets. Hwa et al. (2005) were the first to project syntactic dependencies,"
E17-1022,de-marneffe-etal-2014-universal,0,0.0875731,"Missing"
E17-1022,W12-1909,0,0.0167797,"head attachment rules. It features two-step decoding to guarantee that function words are attached as leaf nodes. The parser requires no training, and it is competitive with a delexicalized transfer system. UDP offers a linguistically sound unsupervised alternative to cross-lingual parsing for UD, which can be used as a baseline for such systems. The parser has very few parameters and is distinctly robust to domain change across languages. 1 Introduction Grammar induction and unsupervised dependency parsing are active fields of research in natural language processing (Klein and Manning, 2004; Gelling et al., 2012). However, many data-driven approaches struggle with learning relations that match the conventions of the test data, e.g., Klein and Manning reported the tendency of their DMV parser to make determiners the heads of German nouns, which would not be an error if the test data used a DP analysis (Abney, 1987). Even supervised transfer approaches (McDonald et al., 2011) suffer from target adaptation problems when facing word order differences. The Universal Dependencies (UD) project (Nivre et al., 2015; Nivre et al., 2016) offers a dependency formalism that aims at providing a consistent represent"
E17-1022,P10-2036,0,0.0744508,"roach that takes a fresh angle on both aspects. Specifically, we propose a parser that i) requires no training data, and in contrast ii) critically relies on exploiting the UD constraints. These two characteristics make our parser unsupervised. Data-driven unsupervised dependency parsing is now a well-established discipline (Klein and Manning, 2004; Spitkovsky et al., 2010a; Spitkovsky et al., 2010b). Still, the performance of these parsers falls far behind the approaches involving any sort of supervision. Our work builds on the line of research on ruleaided unsupervised dependency parsing by Gillenwater et al. (2010) and Naseem et al. (2010), and also relates to Søgaard’s (2012a; 2012b) work. Our parser, however, features two key differences: Related work Cross-lingual learning Recent years have seen exciting developments in cross-lingual linguistic structure prediction based on transfer or projection of POS and dependencies (Das and Petrov, 2011; McDonald et al., 2011). These works mainly use supervised learning and domain adaptation techniques for the target language. The first group of approaches deals with annotation projection (Yarowsky et al., 2001), whereby parallel corpora are used to transfer ann"
E17-1022,P16-2091,1,0.781898,"Missing"
E17-1022,P06-1063,0,0.117258,"Missing"
E17-1022,P04-1061,0,0.494345,"eRank and a small set of head attachment rules. It features two-step decoding to guarantee that function words are attached as leaf nodes. The parser requires no training, and it is competitive with a delexicalized transfer system. UDP offers a linguistically sound unsupervised alternative to cross-lingual parsing for UD, which can be used as a baseline for such systems. The parser has very few parameters and is distinctly robust to domain change across languages. 1 Introduction Grammar induction and unsupervised dependency parsing are active fields of research in natural language processing (Klein and Manning, 2004; Gelling et al., 2012). However, many data-driven approaches struggle with learning relations that match the conventions of the test data, e.g., Klein and Manning reported the tendency of their DMV parser to make determiners the heads of German nouns, which would not be an error if the test data used a DP analysis (Abney, 1987). Even supervised transfer approaches (McDonald et al., 2011) suffer from target adaptation problems when facing word order differences. The Universal Dependencies (UD) project (Nivre et al., 2015; Nivre et al., 2016) offers a dependency formalism that aims at providing"
E17-1022,P14-1126,0,0.0366931,"et al., 2001), whereby parallel corpora are used to transfer annotations between resource-rich source languages and lowresource target languages. Projection relies on the availability and quality of parallel corpora, sourceside taggers and parsers, but also tokenizers, sentence aligners, and word aligners for sources and targets. Hwa et al. (2005) were the first to project syntactic dependencies, and Tiedemann et al. (2014; 2016) improved on their projection algorithm. Current state of the art in cross-lingual dependency parsing involves leveraging parallel corpora for annotation projection (Ma and Xia, 2014; Rasooli and Collins, 2015). The second group of approaches deals with transferring source parsing models to target languages. Zeman and Resnik (2008) were the first to introduce the idea of delexicalization: removing lexical features by training and cross-lingually applying parsers solely on POS sequences. Søgaard (2011) and McDonald et al. (2011) independently extended the approach by using multiple sources, requiring uniform POS and dependency representations (McDonald et al., 2013). Both model transfer and annotation projection rely on a large number of presumptions to derive their compet"
E17-1022,P13-2109,0,0.0770734,"Missing"
E17-1022,D11-1006,0,0.500127,"y few parameters and is distinctly robust to domain change across languages. 1 Introduction Grammar induction and unsupervised dependency parsing are active fields of research in natural language processing (Klein and Manning, 2004; Gelling et al., 2012). However, many data-driven approaches struggle with learning relations that match the conventions of the test data, e.g., Klein and Manning reported the tendency of their DMV parser to make determiners the heads of German nouns, which would not be an error if the test data used a DP analysis (Abney, 1987). Even supervised transfer approaches (McDonald et al., 2011) suffer from target adaptation problems when facing word order differences. The Universal Dependencies (UD) project (Nivre et al., 2015; Nivre et al., 2016) offers a dependency formalism that aims at providing a consistent representation across languages, while enforcing a few hard constraints. The arrival of such treebanks, expanded and improved on a regular basis, provides a new milestone for crosslingual dependency parsing research (McDonald et al., 2013). Contributions We introduce, to the best of our knowledge, the first unsupervised rule-based dependency parser for Universal Dependencies"
E17-1022,W10-2105,1,0.869888,"Missing"
E17-1022,D15-1039,0,0.0523361,"ereby parallel corpora are used to transfer annotations between resource-rich source languages and lowresource target languages. Projection relies on the availability and quality of parallel corpora, sourceside taggers and parsers, but also tokenizers, sentence aligners, and word aligners for sources and targets. Hwa et al. (2005) were the first to project syntactic dependencies, and Tiedemann et al. (2014; 2016) improved on their projection algorithm. Current state of the art in cross-lingual dependency parsing involves leveraging parallel corpora for annotation projection (Ma and Xia, 2014; Rasooli and Collins, 2015). The second group of approaches deals with transferring source parsing models to target languages. Zeman and Resnik (2008) were the first to introduce the idea of delexicalization: removing lexical features by training and cross-lingually applying parsers solely on POS sequences. Søgaard (2011) and McDonald et al. (2011) independently extended the approach by using multiple sources, requiring uniform POS and dependency representations (McDonald et al., 2013). Both model transfer and annotation projection rely on a large number of presumptions to derive their competitive parsing models. By and"
E17-1022,N10-1116,0,0.0314682,"hey typically do not exploit constraints placed on linguistic structures through a formalism, and they do so by design. With the emergence of UD as the practical standard for multilingual POS and syntactic dependency annotation, we argue for an approach that takes a fresh angle on both aspects. Specifically, we propose a parser that i) requires no training data, and in contrast ii) critically relies on exploiting the UD constraints. These two characteristics make our parser unsupervised. Data-driven unsupervised dependency parsing is now a well-established discipline (Klein and Manning, 2004; Spitkovsky et al., 2010a; Spitkovsky et al., 2010b). Still, the performance of these parsers falls far behind the approaches involving any sort of supervision. Our work builds on the line of research on ruleaided unsupervised dependency parsing by Gillenwater et al. (2010) and Naseem et al. (2010), and also relates to Søgaard’s (2012a; 2012b) work. Our parser, however, features two key differences: Related work Cross-lingual learning Recent years have seen exciting developments in cross-lingual linguistic structure prediction based on transfer or projection of POS and dependencies (Das and Petrov, 2011; McDonald et"
E17-1022,D10-1120,0,0.0928531,"on both aspects. Specifically, we propose a parser that i) requires no training data, and in contrast ii) critically relies on exploiting the UD constraints. These two characteristics make our parser unsupervised. Data-driven unsupervised dependency parsing is now a well-established discipline (Klein and Manning, 2004; Spitkovsky et al., 2010a; Spitkovsky et al., 2010b). Still, the performance of these parsers falls far behind the approaches involving any sort of supervision. Our work builds on the line of research on ruleaided unsupervised dependency parsing by Gillenwater et al. (2010) and Naseem et al. (2010), and also relates to Søgaard’s (2012a; 2012b) work. Our parser, however, features two key differences: Related work Cross-lingual learning Recent years have seen exciting developments in cross-lingual linguistic structure prediction based on transfer or projection of POS and dependencies (Das and Petrov, 2011; McDonald et al., 2011). These works mainly use supervised learning and domain adaptation techniques for the target language. The first group of approaches deals with annotation projection (Yarowsky et al., 2001), whereby parallel corpora are used to transfer annotations between resource"
E17-1022,W10-2902,0,0.029288,"hey typically do not exploit constraints placed on linguistic structures through a formalism, and they do so by design. With the emergence of UD as the practical standard for multilingual POS and syntactic dependency annotation, we argue for an approach that takes a fresh angle on both aspects. Specifically, we propose a parser that i) requires no training data, and in contrast ii) critically relies on exploiting the UD constraints. These two characteristics make our parser unsupervised. Data-driven unsupervised dependency parsing is now a well-established discipline (Klein and Manning, 2004; Spitkovsky et al., 2010a; Spitkovsky et al., 2010b). Still, the performance of these parsers falls far behind the approaches involving any sort of supervision. Our work builds on the line of research on ruleaided unsupervised dependency parsing by Gillenwater et al. (2010) and Naseem et al. (2010), and also relates to Søgaard’s (2012a; 2012b) work. Our parser, however, features two key differences: Related work Cross-lingual learning Recent years have seen exciting developments in cross-lingual linguistic structure prediction based on transfer or projection of POS and dependencies (Das and Petrov, 2011; McDonald et"
E17-1022,W12-1910,1,0.850332,"ed on the fly at runtime. We refer henceforth to our UD parser as UDP. 231 3.1 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: PageRank setup Our system uses the PageRank (PR) algorithm (Page et al., 1999) to estimate the relevance of the content words of a sentence. PR uses a random walk to estimate which nodes in the graph are more likely to be visited often, and thus, it gives higher rank to nodes with more incoming edges, as well as to nodes connected to those. Using PR to score word relevance requires an effective graphbuilding strategy. We have experimented with the strategies by Søgaard (2012b), such as words being connected to adjacent words, but our system fares best strictly using the dependency rules in Table 1 to build the graph. UD trees are often very flat, and a highly connected graph yields a PR distribution that is closer to uniform, thereby removing some of the difference of word relevance. We build a multigraph of all words in the sentence covered by the head-dependent rules in Table 1, giving each word an incoming edge for each eligible dependent, i.e., ADV depends on ADJ and VERB . This strategy does not always yield connected graphs, and we use a teleport probabilit"
E17-1022,C14-1175,0,0.0824867,"Missing"
E17-1022,H01-1035,0,0.0694204,"on ruleaided unsupervised dependency parsing by Gillenwater et al. (2010) and Naseem et al. (2010), and also relates to Søgaard’s (2012a; 2012b) work. Our parser, however, features two key differences: Related work Cross-lingual learning Recent years have seen exciting developments in cross-lingual linguistic structure prediction based on transfer or projection of POS and dependencies (Das and Petrov, 2011; McDonald et al., 2011). These works mainly use supervised learning and domain adaptation techniques for the target language. The first group of approaches deals with annotation projection (Yarowsky et al., 2001), whereby parallel corpora are used to transfer annotations between resource-rich source languages and lowresource target languages. Projection relies on the availability and quality of parallel corpora, sourceside taggers and parsers, but also tokenizers, sentence aligners, and word aligners for sources and targets. Hwa et al. (2005) were the first to project syntactic dependencies, and Tiedemann et al. (2014; 2016) improved on their projection algorithm. Current state of the art in cross-lingual dependency parsing involves leveraging parallel corpora for annotation projection (Ma and Xia, 20"
E17-1022,I08-3008,0,0.275845,"s. Projection relies on the availability and quality of parallel corpora, sourceside taggers and parsers, but also tokenizers, sentence aligners, and word aligners for sources and targets. Hwa et al. (2005) were the first to project syntactic dependencies, and Tiedemann et al. (2014; 2016) improved on their projection algorithm. Current state of the art in cross-lingual dependency parsing involves leveraging parallel corpora for annotation projection (Ma and Xia, 2014; Rasooli and Collins, 2015). The second group of approaches deals with transferring source parsing models to target languages. Zeman and Resnik (2008) were the first to introduce the idea of delexicalization: removing lexical features by training and cross-lingually applying parsers solely on POS sequences. Søgaard (2011) and McDonald et al. (2011) independently extended the approach by using multiple sources, requiring uniform POS and dependency representations (McDonald et al., 2013). Both model transfer and annotation projection rely on a large number of presumptions to derive their competitive parsing models. By and large, these presumptions are unrealistic and exclusive to a group of very closely related, resource-rich IndoEuropean lan"
E17-1022,Q16-1022,1,\N,Missing
E17-2040,P15-2044,1,0.928871,"Missing"
E17-2040,Q16-1022,1,0.909051,"Missing"
E17-2040,petrov-etal-2012-universal,0,0.0285098,"We express the quality of predicted rankings using precision (P@1) and Kendall’s τb statistic (Knight, 1966). Data. We train and test our taggers on data from UD version 1.2 (Nivre et al., 2015). We intersect this collection with the dictionaries we make available for this experiment: 9 of the Wiktionaries come from Li et al. (2012), and we collect 16 new on top of that. Thus, we experiment with a total of 25 languages from the UD. We refer to the 9 languages of Li et al. (2012) as development languages. To make the Wiktionaries and the UD data compatible, we map all POS tags to the tagset by Petrov et al. (2012). We estimate the frequencies for the +freq variants of the soft metrics by using the multilingual Bible corpus by Christodouloupoulos and Steedman (2014) and the Watchtower corpus (Agi´c et al., 2016) combined. We translate the English Wiktionary from Li et al. (2012) by using bilingual dictionaries from Wiktionary to obtain Dtrans for 20 languages.3 precision(D, G) = ∑i=1 ∣D∣ ∣{Di ∩Gi }∣ ∣{t∈Di }∣ recall(D, G) = ∑i=1 ∣D∣ ∣{Di ∩Gi }∣ ∣{t∈Gi }∣ Namely, for each word wi covered by both D and G, we check how many tags Di and Gi intersect, and then use the intersection to estimate dictionary prec"
E17-2040,P13-2109,0,0.0384735,"Missing"
E17-2040,K15-1033,1,0.885938,"Missing"
E17-2040,P13-2017,0,0.0419349,"Missing"
E17-2040,P16-2067,1,0.880248,"Missing"
E17-2040,A00-1031,0,0.765935,"Missing"
E17-2040,N13-1014,0,0.131467,"Missing"
E17-2040,D13-1032,0,0.0602914,"Missing"
E17-2040,P15-2111,0,0.0278218,"Missing"
E17-2040,D12-1127,0,\N,Missing
K15-1033,E06-1040,0,0.0477065,"ng et al., 2013), we see human judgments as an important supplement to extrinsic evaluation. To the best of our knowledge, no prior study has analyzed the correlation between dependency parsing metrics and human judgments. For a range of other NLP tasks, metrics have been evaluated by how well they correlate with human judgments. For instance, the standard automatic metrics for certain tasks—such as BLEU in machine translation, or ROUGE-N and NIST in summarization or natural language generation—were evaluated, reaching correlation coefficients well above .80 (Papineni et al., 2002; Lin, 2004; Belz and Reiter, 2006; Callison-Burch et al., 2007). We find that correlations between evaluation metrics and human judgments are weaker for dependency parsing than other NLP tasks—our correlation coefficients are typically between .35 and .55—and that inter-annotator agreement is sometimes higher than human-metric agreement. Moreover, our analysis (§5) reveals that humans have a preference for attachment over labeling decisions, and that attachments closer to the root are more important. Our findings suggest that the currently employed metrics are not fully adequate. Using automatic measures such as labeled and u"
K15-1033,P02-1040,0,0.100561,"appropriate downstream tasks (Elming et al., 2013), we see human judgments as an important supplement to extrinsic evaluation. To the best of our knowledge, no prior study has analyzed the correlation between dependency parsing metrics and human judgments. For a range of other NLP tasks, metrics have been evaluated by how well they correlate with human judgments. For instance, the standard automatic metrics for certain tasks—such as BLEU in machine translation, or ROUGE-N and NIST in summarization or natural language generation—were evaluated, reaching correlation coefficients well above .80 (Papineni et al., 2002; Lin, 2004; Belz and Reiter, 2006; Callison-Burch et al., 2007). We find that correlations between evaluation metrics and human judgments are weaker for dependency parsing than other NLP tasks—our correlation coefficients are typically between .35 and .55—and that inter-annotator agreement is sometimes higher than human-metric agreement. Moreover, our analysis (§5) reveals that humans have a preference for attachment over labeling decisions, and that attachments closer to the root are more important. Our findings suggest that the currently employed metrics are not fully adequate. Using automa"
K15-1033,W06-2920,0,0.0844944,"0 sentences for each of the 5 languages) annotated with human judgments for the preferred automatically parsed dependency tree, enabling further research in this direction. 2 UCP = LCP = We evaluate seven dependency parsing metrics, described in this section. Given a labeled gold tree G = hV, EG , lG (·)i and a labeled predicted tree P = hV, EP , lP (·)i, let E ⊂ V × V be the set of directed edges from dependents to heads, and let l : V × V → L be the edge labeling function, with L the set of dependency labels. The three most commonly used metrics are those from the CoNLL 2006–7 shared tasks (Buchholz and Marsi, 2006): unlabeled attachment score (UAS), label accuracy (LA), both introduced by Eisner (1996), and labeled attachment score (LAS), the pivotal dependency parsing metric introduced by Nivre et al. (2004). LAS = 3 Experiment In our analysis, we compare the metrics with human judgments. We examine how well the automatic metrics correlate with each other, as well as with human judgments, and whether interannotator agreement exceeds annotator-metric agreement. |{e |e ∈ EG ∩ EP }| |V | |{e |lG (e) = lP (e), e ∈ EG ∩ EP }| |V | LA = |{v |Vverb , cG (v) = cP (v) ∧ lG (v, ·) = lP (v, ·)}| |Vverb | For the"
K15-1033,W07-0718,0,0.188994,"e human judgments as an important supplement to extrinsic evaluation. To the best of our knowledge, no prior study has analyzed the correlation between dependency parsing metrics and human judgments. For a range of other NLP tasks, metrics have been evaluated by how well they correlate with human judgments. For instance, the standard automatic metrics for certain tasks—such as BLEU in machine translation, or ROUGE-N and NIST in summarization or natural language generation—were evaluated, reaching correlation coefficients well above .80 (Papineni et al., 2002; Lin, 2004; Belz and Reiter, 2006; Callison-Burch et al., 2007). We find that correlations between evaluation metrics and human judgments are weaker for dependency parsing than other NLP tasks—our correlation coefficients are typically between .35 and .55—and that inter-annotator agreement is sometimes higher than human-metric agreement. Moreover, our analysis (§5) reveals that humans have a preference for attachment over labeling decisions, and that attachments closer to the root are more important. Our findings suggest that the currently employed metrics are not fully adequate. Using automatic measures such as labeled and unlabeled attachment scores is"
K15-1033,P11-1067,0,0.0697005,"Missing"
K15-1033,C96-1058,0,0.12707,"ly parsed dependency tree, enabling further research in this direction. 2 UCP = LCP = We evaluate seven dependency parsing metrics, described in this section. Given a labeled gold tree G = hV, EG , lG (·)i and a labeled predicted tree P = hV, EP , lP (·)i, let E ⊂ V × V be the set of directed edges from dependents to heads, and let l : V × V → L be the edge labeling function, with L the set of dependency labels. The three most commonly used metrics are those from the CoNLL 2006–7 shared tasks (Buchholz and Marsi, 2006): unlabeled attachment score (UAS), label accuracy (LA), both introduced by Eisner (1996), and labeled attachment score (LAS), the pivotal dependency parsing metric introduced by Nivre et al. (2004). LAS = 3 Experiment In our analysis, we compare the metrics with human judgments. We examine how well the automatic metrics correlate with each other, as well as with human judgments, and whether interannotator agreement exceeds annotator-metric agreement. |{e |e ∈ EG ∩ EP }| |V | |{e |lG (e) = lP (e), e ∈ EG ∩ EP }| |V | LA = |{v |Vverb , cG (v) = cP (v) ∧ lG (v, ·) = lP (v, ·)}| |Vverb | For the final figure of seven different parsing metrics, on top of the previous five, in our expe"
K15-1033,C12-1147,0,0.117595,"r NLP tasks. Also, inter-annotator agreement is sometimes higher than the agreement between judgments and metrics, indicating that the standard metrics fail to capture certain aspects of parse quality, such as the relevance of root attachment or the relative importance of the different parts of speech. 1 Introduction In dependency parser evaluation, the standard accuracy metrics—labeled and unlabeled attachment scores—are defined simply as averages over correct attachment decisions. Several authors have pointed out problems with these metrics; they are both sensitive to annotation guidelines (Schwartz et al., 2012; Tsarfaty et al., 2011), and they fail to say anything about how parsers fare on rare, but important linguistic constructions (Nivre et al., 2010). Both criticisms rely on the intuition that some parsing errors are more important than others, and that our metrics should somehow reflect that. There are sentences that are hard to annotate because they are ambiguous, or because they contain phenomena peripheral to linguistic theory, such as punctuation, clitics, or fragments. Manning (2011) discusses similar issues for part-ofspeech tagging. 315 Proceedings of the 19th Conference on Computationa"
K15-1033,N13-1070,1,0.846825,"ges: Croatian, Danish, English, German, and Spanish. For the human judgments, we asked professional linguists with dependency annotation experience to judge which of two parsers produced the better parse. Our stance here is that, insofar experts are able to annotate dependency trees, they are also able to determine the quality of a predicted syntactic structure, which we can in turn use to evaluate parser evaluation metrics. Even though downstream evaluation is critical in assessing the usefulness of parses, it also presents non-trivial challenges in choosing the appropriate downstream tasks (Elming et al., 2013), we see human judgments as an important supplement to extrinsic evaluation. To the best of our knowledge, no prior study has analyzed the correlation between dependency parsing metrics and human judgments. For a range of other NLP tasks, metrics have been evaluated by how well they correlate with human judgments. For instance, the standard automatic metrics for certain tasks—such as BLEU in machine translation, or ROUGE-N and NIST in summarization or natural language generation—were evaluated, reaching correlation coefficients well above .80 (Papineni et al., 2002; Lin, 2004; Belz and Reiter,"
K15-1033,D11-1036,0,0.220614,"r-annotator agreement is sometimes higher than the agreement between judgments and metrics, indicating that the standard metrics fail to capture certain aspects of parse quality, such as the relevance of root attachment or the relative importance of the different parts of speech. 1 Introduction In dependency parser evaluation, the standard accuracy metrics—labeled and unlabeled attachment scores—are defined simply as averages over correct attachment decisions. Several authors have pointed out problems with these metrics; they are both sensitive to annotation guidelines (Schwartz et al., 2012; Tsarfaty et al., 2011), and they fail to say anything about how parsers fare on rare, but important linguistic constructions (Nivre et al., 2010). Both criticisms rely on the intuition that some parsing errors are more important than others, and that our metrics should somehow reflect that. There are sentences that are hard to annotate because they are ambiguous, or because they contain phenomena peripheral to linguistic theory, such as punctuation, clitics, or fragments. Manning (2011) discusses similar issues for part-ofspeech tagging. 315 Proceedings of the 19th Conference on Computational Language Learning, pag"
K15-1033,N13-1132,0,0.022444,"78 .437 .250* .469 .404 .230* .195* .297 .331 .232 .318 .323 .171 .223 .466 .453 .310 .501 .331 .120* .190* .540 .397 .467 .446 .405* .120* .143* .457 .425 .324* .448 .361* .126* .195* Table 4: Correlations between human judgments and metrics (micro avg). * means significantly different from LAS ρ using Fisher’s z-transform. Bold: highest correlation per language. correlated, e.g., LAS and LA, and UAS and NED, but some exhibit very low correlation coefficients. Next we study correlations with human judgments (Table 4). In order to aggregate over the annotations, we use an item-response model (Hovy et al., 2013). The correlations are relatively weak compared to similar findings for other NLP tasks. For instance, ROUGE-1 (Lin, 2004) correlates strongly with perceived summary quality, with a coefficient of 0.99. The same holds for BLEU and human judgments of machine translation quality (Papineni et al., 2002). We find that, overall, LAS is the metric that correlates best with human judgments. It is closely followed by UAS, which does not differ significantly from LAS, albeit the correlations for UAS are slightly lower on average. NED is in turn highly correlated with UAS. The correlations for the predi"
K15-1033,E12-1006,0,0.0277816,"Missing"
K15-1033,W04-1013,0,0.191111,"tasks (Elming et al., 2013), we see human judgments as an important supplement to extrinsic evaluation. To the best of our knowledge, no prior study has analyzed the correlation between dependency parsing metrics and human judgments. For a range of other NLP tasks, metrics have been evaluated by how well they correlate with human judgments. For instance, the standard automatic metrics for certain tasks—such as BLEU in machine translation, or ROUGE-N and NIST in summarization or natural language generation—were evaluated, reaching correlation coefficients well above .80 (Papineni et al., 2002; Lin, 2004; Belz and Reiter, 2006; Callison-Burch et al., 2007). We find that correlations between evaluation metrics and human judgments are weaker for dependency parsing than other NLP tasks—our correlation coefficients are typically between .35 and .55—and that inter-annotator agreement is sometimes higher than human-metric agreement. Moreover, our analysis (§5) reveals that humans have a preference for attachment over labeling decisions, and that attachments closer to the root are more important. Our findings suggest that the currently employed metrics are not fully adequate. Using automatic measure"
K15-1033,P05-1012,0,0.0845214,". |{v |v ∈ V, lG (v, ·) = lP (v, ·)}| |V | Data In our experiments we use data from five languages: The English (en), German (de) and Spanish (es) treebanks from the Universal Dependencies (UD v1.0) project (Nivre et al., 2015), the Copenhagen Dependency Treebank (da) (BuchKromann, 2003), and the Croatian Dependency Treebank (hr) (Agi´c and Merkler, 2013). We keep the original POS tags for all datasets (17 tags in case of UD, 13 tags for Croatian, and 23 for Danish). Data characteristics are in Table 1. For the parsing systems, we follow McDonald and Nivre (2007) and use the second order MST (McDonald et al., 2005), as well as Malt parser with pseudo-projectivization (Nivre and Nilsson, 2005) and default parameters. For each language, we train the parsers on the canonical training section. We randomly select 200 sentences from the test sections, where our two deWe include two further metrics—namely, labeled (LCP) and unlabeled (UCP) complete predications—to give account for the relevance of correct predicate prediction for parsing quality. LCP is inspired by the complete predicates metric from the SemEval 2015 shared task on semantic parsing (Oepen et al., 2015).2 LCP is triggered by a verb (i.e., set o"
K15-1033,P05-1013,0,0.0676584,"a from five languages: The English (en), German (de) and Spanish (es) treebanks from the Universal Dependencies (UD v1.0) project (Nivre et al., 2015), the Copenhagen Dependency Treebank (da) (BuchKromann, 2003), and the Croatian Dependency Treebank (hr) (Agi´c and Merkler, 2013). We keep the original POS tags for all datasets (17 tags in case of UD, 13 tags for Croatian, and 23 for Danish). Data characteristics are in Table 1. For the parsing systems, we follow McDonald and Nivre (2007) and use the second order MST (McDonald et al., 2005), as well as Malt parser with pseudo-projectivization (Nivre and Nilsson, 2005) and default parameters. For each language, we train the parsers on the canonical training section. We randomly select 200 sentences from the test sections, where our two deWe include two further metrics—namely, labeled (LCP) and unlabeled (UCP) complete predications—to give account for the relevance of correct predicate prediction for parsing quality. LCP is inspired by the complete predicates metric from the SemEval 2015 shared task on semantic parsing (Oepen et al., 2015).2 LCP is triggered by a verb (i.e., set of nodes Vverb ) and checks whether all its core arguments match, i.e., all outg"
K15-1033,W04-2407,0,0.0333814,"dependency parsing metrics, described in this section. Given a labeled gold tree G = hV, EG , lG (·)i and a labeled predicted tree P = hV, EP , lP (·)i, let E ⊂ V × V be the set of directed edges from dependents to heads, and let l : V × V → L be the edge labeling function, with L the set of dependency labels. The three most commonly used metrics are those from the CoNLL 2006–7 shared tasks (Buchholz and Marsi, 2006): unlabeled attachment score (UAS), label accuracy (LA), both introduced by Eisner (1996), and labeled attachment score (LAS), the pivotal dependency parsing metric introduced by Nivre et al. (2004). LAS = 3 Experiment In our analysis, we compare the metrics with human judgments. We examine how well the automatic metrics correlate with each other, as well as with human judgments, and whether interannotator agreement exceeds annotator-metric agreement. |{e |e ∈ EG ∩ EP }| |V | |{e |lG (e) = lP (e), e ∈ EG ∩ EP }| |V | LA = |{v |Vverb , cG (v) = cP (v) ∧ lG (v, ·) = lP (v, ·)}| |Vverb | For the final figure of seven different parsing metrics, on top of the previous five, in our experiments we also include the neutral edge direction metric (NED) (Schwartz et al., 2011), and tree edit distan"
K15-1033,C10-1094,0,0.118629,"cs fail to capture certain aspects of parse quality, such as the relevance of root attachment or the relative importance of the different parts of speech. 1 Introduction In dependency parser evaluation, the standard accuracy metrics—labeled and unlabeled attachment scores—are defined simply as averages over correct attachment decisions. Several authors have pointed out problems with these metrics; they are both sensitive to annotation guidelines (Schwartz et al., 2012; Tsarfaty et al., 2011), and they fail to say anything about how parsers fare on rare, but important linguistic constructions (Nivre et al., 2010). Both criticisms rely on the intuition that some parsing errors are more important than others, and that our metrics should somehow reflect that. There are sentences that are hard to annotate because they are ambiguous, or because they contain phenomena peripheral to linguistic theory, such as punctuation, clitics, or fragments. Manning (2011) discusses similar issues for part-ofspeech tagging. 315 Proceedings of the 19th Conference on Computational Language Learning, pages 315–320, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics Contributions We present i)"
K15-1033,S15-2153,0,0.0389892,"e (2007) and use the second order MST (McDonald et al., 2005), as well as Malt parser with pseudo-projectivization (Nivre and Nilsson, 2005) and default parameters. For each language, we train the parsers on the canonical training section. We randomly select 200 sentences from the test sections, where our two deWe include two further metrics—namely, labeled (LCP) and unlabeled (UCP) complete predications—to give account for the relevance of correct predicate prediction for parsing quality. LCP is inspired by the complete predicates metric from the SemEval 2015 shared task on semantic parsing (Oepen et al., 2015).2 LCP is triggered by a verb (i.e., set of nodes Vverb ) and checks whether all its core arguments match, i.e., all outgoing dependency edges except for punctuation. Since LCP is a very strict metric, we also evaluate UCP, its unlabeled variant. Given a function cX (v) that retrieves the set of child nodes of a node v from a tree X, we first define UCP as follows, and then incorporate the label matching for LCP: 1 The dataset is publicly available at https:// bitbucket.org/lowlands/release 2 http://alt.qcri.org/semeval2015/ 3 316 http://www.tsarfaty.com/unipar/ L ANG PARSER LAS UAS LA NED TED"
K15-1033,S14-2008,0,\N,Missing
L16-1676,W15-5301,1,0.750211,"Missing"
L16-1676,W13-2408,1,0.927236,"Missing"
L16-1676,W13-4903,1,0.895257,"Missing"
L16-1676,P07-2053,0,0.357836,"Missing"
L16-1676,W14-0405,1,0.88338,"Missing"
L16-1676,R15-1050,1,0.895159,"Missing"
L16-1676,L16-1242,1,0.830604,"Missing"
L16-1676,P14-5003,0,0.0722968,"Missing"
L16-1676,W03-2906,0,0.144031,"Missing"
L18-1614,Q16-1031,0,0.0119471,"al. (2016), or Wang et al. (2017), which mark a string of very notable results. Yet, the SNLI corpus is in English only. As of recently, it includes more test data from multiple genres,2 but it remains exclusive to English. Following Bender (2009) in seeking true language independence, we propose to extend the current NLI research beyond English, and further into the majority realm of low-resource languages. Since training data is generally unavailable for most languages, work on transfer learning is abundant for the basic NLP tasks such as tagging and syntactic parsing (Das and Petrov, 2011; Ammar et al., 2016). By contrast, the research in cross-lingual entailment is not as plentiful (Negri et al., 2013). To the best of our knowledge, at this point there are no contributions to SNLI-style cross-lingual inference, or for that matter, work on languages other than English at all. Contributions. In the absence of training data for languages other than English, we propose a set of baselines for cross-lingual neural inference. We adapt to the target languages either by i) employing multilingual word embeddings or alternatively by ii) translating the input sentebces into English. We create multilingual te"
L18-1614,W09-0106,0,0.0147075,"lment. Now revamped as natural language inference (NLI) by Bowman et al. (2015) with their SNLI dataset, the task of differentiating contradictory, entailing, and unrelated pairs of sentences (Fig. 1) has entertained a large number of proposals.1 The timely challenge lends itself to various deep learning approaches such as by Rockt¨aschel et al. (2015), Parikh et al. (2016), or Wang et al. (2017), which mark a string of very notable results. Yet, the SNLI corpus is in English only. As of recently, it includes more test data from multiple genres,2 but it remains exclusive to English. Following Bender (2009) in seeking true language independence, we propose to extend the current NLI research beyond English, and further into the majority realm of low-resource languages. Since training data is generally unavailable for most languages, work on transfer learning is abundant for the basic NLP tasks such as tagging and syntactic parsing (Das and Petrov, 2011; Ammar et al., 2016). By contrast, the research in cross-lingual entailment is not as plentiful (Negri et al., 2013). To the best of our knowledge, at this point there are no contributions to SNLI-style cross-lingual inference, or for that matter,"
L18-1614,D15-1075,0,0.129205,"Missing"
L18-1614,S17-2001,0,0.0526247,"exercise caution: SNLI sentences are image captions, mostly ≤15 words long and thus relatively easy to translate (cf. Bowman et al. (2015), Fig. 2) in comparison to, e.g., newspaper text. 3892 5. Related Work Prior to SNLI, there has been work in cross-lingual textual entailment using parallel corpora (Mehdad et al., 2011) and lexical resources (Castillo, 2011), or crowdsourcing for multilingual training data by Negri et al. (2011). We also note two shared tasks, on cross-lingual entailment with five languages (Negri et al., 2013) and English relatedness and inference (Marelli et al., 2014). Cer et al. (2017) provide multilingual evaluation data within a shared task in semantic textual similarity. There, paired snippets of text are evaluated for their degree of equivalence, and could thus be treated as a fine-grained proxy for SNLI-style evaluations. SNLI is the first large-scale dataset for NLI in English (Bowman et al., 2015), two orders of magnitude larger than any predecessor. It was recently expanded with test data for multiple genres of English to allow for cross-domain evaluation.12 Prior to our work, there have been no SNLI-style cross-lingual methods or evaluations. 6. Conclusions We have"
L18-1614,P11-1061,0,0.053294,"Missing"
L18-1614,P14-1006,0,0.0203332,"al least-squared error. This transformation matrix can then be used on words not seen in the bilingual dictionary. Multilingual embeddings. If parallel sentences or even just parallel documents are available for two or more languages, we can use this data to embed their vocabularies in a shared representation. For example, through an EnglishRussian parallel corpus we would represent the words of the two languages in a shared space. There are several competing approaches to training word embeddings over parallel sentences. In this paper, we experiment with four. BICVM : The seminal approach by Hermann and Blunsom (2014) for inducing bilingual compositional representations from sentence-aligned parallel corpora only.4 INVERT: Inverted indexing over parallel corpus sentence IDs as indexing features, with SVD dimensionality reduction on top, following Søgaard et al. (2015) in the recent implementation by Levy et al. (2017).5 Instead of embedding just language pairs, this method embeds multiple languages into the same space. It is thus distinctly multilingual, rather than just bilingual. RANDOM : Our implementation of the approach by Vuli´c and Moens (2016) whereby bilingual SGNS embeddings of Mikolov et al. (20"
L18-1614,P07-2045,0,0.0130276,"Missing"
L18-1614,E17-1072,0,0.0135761,"r example, through an EnglishRussian parallel corpus we would represent the words of the two languages in a shared space. There are several competing approaches to training word embeddings over parallel sentences. In this paper, we experiment with four. BICVM : The seminal approach by Hermann and Blunsom (2014) for inducing bilingual compositional representations from sentence-aligned parallel corpora only.4 INVERT: Inverted indexing over parallel corpus sentence IDs as indexing features, with SVD dimensionality reduction on top, following Søgaard et al. (2015) in the recent implementation by Levy et al. (2017).5 Instead of embedding just language pairs, this method embeds multiple languages into the same space. It is thus distinctly multilingual, rather than just bilingual. RANDOM : Our implementation of the approach by Vuli´c and Moens (2016) whereby bilingual SGNS embeddings of Mikolov et al. (2013b) are trained on top of merged pairs 4 5 eng to ... ... to eng ara 25.58 37.48 fra 55.80 46.90 spa 39.65 44.04 rus 30.31 31.17 Table 1: Machine translation quality (BLEU) for translating the test data from and into English. of parallel sentences with randomly shuffled tokens. RATIO : Similar to RANDOM"
L18-1614,S14-2001,0,0.0291049,"approach in general, we exercise caution: SNLI sentences are image captions, mostly ≤15 words long and thus relatively easy to translate (cf. Bowman et al. (2015), Fig. 2) in comparison to, e.g., newspaper text. 3892 5. Related Work Prior to SNLI, there has been work in cross-lingual textual entailment using parallel corpora (Mehdad et al., 2011) and lexical resources (Castillo, 2011), or crowdsourcing for multilingual training data by Negri et al. (2011). We also note two shared tasks, on cross-lingual entailment with five languages (Negri et al., 2013) and English relatedness and inference (Marelli et al., 2014). Cer et al. (2017) provide multilingual evaluation data within a shared task in semantic textual similarity. There, paired snippets of text are evaluated for their degree of equivalence, and could thus be treated as a fine-grained proxy for SNLI-style evaluations. SNLI is the first large-scale dataset for NLI in English (Bowman et al., 2015), two orders of magnitude larger than any predecessor. It was recently expanded with test data for multiple genres of English to allow for cross-domain evaluation.12 Prior to our work, there have been no SNLI-style cross-lingual methods or evaluations. 6."
L18-1614,P11-1134,0,0.279581,"Missing"
L18-1614,D11-1062,0,0.0702231,"Missing"
L18-1614,S13-2005,0,0.120392,"rpus is in English only. As of recently, it includes more test data from multiple genres,2 but it remains exclusive to English. Following Bender (2009) in seeking true language independence, we propose to extend the current NLI research beyond English, and further into the majority realm of low-resource languages. Since training data is generally unavailable for most languages, work on transfer learning is abundant for the basic NLP tasks such as tagging and syntactic parsing (Das and Petrov, 2011; Ammar et al., 2016). By contrast, the research in cross-lingual entailment is not as plentiful (Negri et al., 2013). To the best of our knowledge, at this point there are no contributions to SNLI-style cross-lingual inference, or for that matter, work on languages other than English at all. Contributions. In the absence of training data for languages other than English, we propose a set of baselines for cross-lingual neural inference. We adapt to the target languages either by i) employing multilingual word embeddings or alternatively by ii) translating the input sentebces into English. We create multilingual test data to facilitate evaluation by manually translating 4 × 1,332 premise-hypothesis sentence p"
L18-1614,D16-1244,0,0.0944515,"Missing"
L18-1614,D14-1162,0,0.0795498,"Missing"
L18-1614,P15-1165,1,0.795463,"Missing"
L18-1614,W14-1614,1,0.870907,"Missing"
L18-1614,L16-1561,0,0.040501,"Missing"
P13-2137,C12-2001,0,0.0354826,"Missing"
P13-2137,J10-4006,0,0.567318,"re (K¨ubler et al., 2009). Their obvious problem, of course, is that they require a large parsed corpus. In this paper, we describe the construction of a Distributional Memory for Croatian (D M .H R), a free word order language. To do so, we parse hrWaC (Ljubeˇsi´c and Erjavec, 2011), a 1.2B-token Croatian web corpus. We evaluate D M .H R on a synonym choice task, where it outperforms the standard bag-of-word model for nouns and verbs. We report on the first structured distributional semantic model for Croatian, D M .H R. It is constructed after the model of the English Distributional Memory (Baroni and Lenci, 2010), from a dependencyparsed Croatian web corpus, and covers about 2M lemmas. We give details on the linguistic processing and the design principles. An evaluation shows state-of-theart performance on a semantic similarity task with particularly good performance on nouns. The resource is freely available. 1 Introduction Most current work in lexical semantics is based on the Distributional Hypothesis (Harris, 1954), which posits a correlation between the degree of words’ semantic similarity and the similarity of the contexts in which they occur. Using this hypothesis, word meaning representations"
P13-2137,broda-etal-2008-corpus,0,0.0212408,"syntax-based (cooccurrence defined syntactically, syntactic objects as dimensions). Syntax-based models have several desirable properties. First, they are model to fine-grained types of semantic similarity such as predicate-argument plausibility (Erk et al., 2010). Second, they are more versatile – Baroni and Lenci (2010) have presented a generic framework, the Distributional Memory (DM), which is applicable 2 Related Work Vector space semantic models have been applied to a number of Slavic languages, including Bulgarian (Nakov, 2001a), Czech (Smrˇz and Rychl´y, 2001), Polish (Piasecki, 2009; Broda et al., 2008; Broda and Piasecki, 2008), and Russian (Nakov, 2001b; Mitrofanova et al., 2007). Previous work on distributional semantic models for Croatian dealt with similarity prediction (Ljubeˇsi´c et al., 2008; Jankovi´c et al., 2011) and synonym detection (Karan et al., 2012), however using only wordbased and not syntactic-based models. So far the only DM for a language other than English is the German D M .D E by Pad´o and Utt (2012), who describe the process of building D M .D E and the evaluation on a synonym choice task. Our work is similar, though each language has its own challenges. Croatian,"
P13-2137,1993.eamt-1.1,0,0.518212,"Missing"
P13-2137,P07-2053,0,0.033919,"Missing"
P13-2137,W06-2932,0,0.0123606,". SET IMES .H R consists of 90K tokens and 4K sentences, manually lemmatized and MSD-tagged according to Multext East v4 tagset (Erjavec, 2012), with the help of the Croatian Lemmatization Server (Tadi´c, 2005). It is used also as a basis for a novel formalism for syntactic annotation and dependency parsing of Croatian (Agi´c and Merkler, 2013). On the basis of previous evaluation for Croatian (Agi´c et al., 2008; Agi´c et al., 2009; Agi´c, 2012) and availability and licensing considerations, we chose HunPos tagger (Hal´acsy et al., 2007), CST lemmatizer (Ingason et al., 2008), and MSTParser (McDonald et al., 2006) to process hrWaC. We evaluated the tools on 100-sentence test sets from SET IMES .H R and Wikipedia; performance on Wikipedia should be indicative of the performance on a cross-domain dataset, such as hrWaC. In Table 1 we show lemmatization and tagging accuracy, as well as dependency parsing accuracy in terms of labeled attachment score (LAS). The results show that lemmatization, tagging and parsing accuracy improves on the state of the art for Croatian. The SET IMES .H R dependency parsing models are publicly available.3 Syntactic patterns. We collect the co-occurrence counts of tuples using"
P13-2137,D07-1060,0,0.239074,"Missing"
P13-2137,J10-4007,1,\N,Missing
P15-1165,P14-2131,0,0.0530581,"a mean vector to out-of-vocabulary words. System For our system, we simply augment the delexicalized POS tagger with the I NVERTED distributional representation of the current word. The best parameter setting on Spanish development data was σ = 0.01, δ = 160. 3.3 http://code.google.com/p/uni-dep-tb/ https://code.google.com/p/ wikily-supervised-pos-tagger/ 8 For our embeddings baselines, we augment the feature space by adding embedding vectors for head h and dependent d. We experimented with different versions of combining embedding vectors, from firing separate h and d per-dimension features (Bansal et al., 2014) to combining their information. We found that combining the embeddings of h and d is effective and consistently use the absolute difference between the embedding vectors, since that worked better than addition and multiplication on development data. Delexicalized transfer (D ELEX) uses three (3) iterations over the data in both the single-source and the multi-source set-up, a parameter set on the Spanish development data. The remaining parameters were obtained by averaging over performance with different embeddings on the Spanish development data, obtaining: σ = 0.005, δ = 20, i = 3, and abso"
P15-1165,P14-1023,0,0.0109756,"proaches focus on different kinds of similarity, some more syntactic, some more semantic. The representations are typically either clusters of distributionally similar words, e.g., Brown et al. (1992), or vector representations. In this paper, we focus on vector representations. In vector-based approaches, similar representations are vectors close in some multi-dimensional space. 2.1 Count-based and prediction-based representations There are, briefly put, two approaches to inducing vector-based distributional word representations from large corpora: count-based and predictionbased approaches (Baroni et al., 2014). Countbased approaches represent words by their cooccurrences. Dimensionality reduction is typically performed on a raw or weighted co-occurrence matrix using methods such as singular value decomposition (SVD), a method for maximizing the variance in a dataset in few dimensions. In our inverted indexing, we use raw co-occurrence data. Prediction-based methods use discriminative learning techniques to learn how to predict words from their context, or vice versa. They rely on a neural network architecture, and once the network converges, they use word representations from a middle layer as thei"
P15-1165,C10-1011,1,0.695318,"ing. For compatibility with Xiao and Guo (2014), we also present results on CoNLL 2006 and 2007 treebanks for languages for which we had baseline and system word representations (de, es, sv). Our parameter settings for these experiments were the same as those tuned on the Spanish development data from the Google Universal Treebanks v. 1.0. Baselines The most obvious baseline in our experiments is delexicalized transfer (D ELEX) (McDonald et al., 2011; Søgaard, 2011). This baseline system simply learns models without lexical features. We use a modified version of the first-order Mate 7 parser (Bohnet, 2010) that also takes continuousvalued embeddings as input an disregards features that include lexical items. 3.4 Word alignment Data We use the manually word-aligned EnglishSpanish Europarl data from Graca et al. (2008). The dataset contains 100 sentences. The annotators annotated whether word alignments were certain or possible, and we present results with all word alignments and with only the certain ones. See Graca et al. (2008) for details. Baselines For word alignment, we simply align every aligned word in the gold data, for which we have a word embedding, to its (Euclidean) nearest neighbor"
P15-1165,J92-4003,0,0.127536,"al and sparse models. Also, simple bagof-words models fail to capture the relatedness of words. In many tasks, synonymous words should be treated alike, but their bag-of-words representations are as different as those of dog and therefore. Distributional word representations are supposed to capture distributional similarities between words. Intuitively, we want similar words to have similar representations. Known approaches focus on different kinds of similarity, some more syntactic, some more semantic. The representations are typically either clusters of distributionally similar words, e.g., Brown et al. (1992), or vector representations. In this paper, we focus on vector representations. In vector-based approaches, similar representations are vectors close in some multi-dimensional space. 2.1 Count-based and prediction-based representations There are, briefly put, two approaches to inducing vector-based distributional word representations from large corpora: count-based and predictionbased approaches (Baroni et al., 2014). Countbased approaches represent words by their cooccurrences. Dimensionality reduction is typically performed on a raw or weighted co-occurrence matrix using methods such as sing"
P15-1165,W02-1001,0,0.0575931,"s well as tag dictionaries (Li et al., 2012) needed for the POS tagging experiments. Baselines One baseline method is a typeconstrained structured perceptron with only ortographic features, which are expected to transfer across languages. The type constraints come from Wiktionary, a crowd-sourced tag dictionary.8 Type constraints from Wiktionary were first used by Li et al. (2012), but note that their set-up is unsupervised learning. T¨ackstr¨om et al. (2013) also used type constraints in a supervised set-up. Our learning algorithm is the structured perceptron algorithm originally proposed by Collins (2002). In our POS tagging experiments, we always do 10 passes over the data. We also present two other baselines, where we augment the feature representation with different embeddings for the target word, K LEMENTIEV and C HANDAR. With all the embeddings in POS tagging, we assign a mean vector to out-of-vocabulary words. System For our system, we simply augment the delexicalized POS tagger with the I NVERTED distributional representation of the current word. The best parameter setting on Spanish development data was σ = 0.01, δ = 160. 3.3 http://code.google.com/p/uni-dep-tb/ https://code.google.com"
P15-1165,N15-1157,1,0.757356,"peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackstr¨om et al., 2013; Xiao and Guo, 2014). In cross-lingual POS tagging, mostly annotation projection has been explored (Fossum and Abney, 2005; Das and Petrov, 2011), since all features in POS tagging models are typically lexical. However, using bilingual word representations was recently explored as an alternative to projectionbased approaches (Gouws and Søgaard, 2015). The major drawback of using bi-lexical representations is that it limits us to using a single source language. T¨ackstr¨om et al. (2013) obtained significant improvements using bilingual word clusters over a single source delexicalized transfer model, for example, but even better results were obtained with delexicalized transfer in McDonald et al. (2011) by simply using several source languages. This paper introduces a simple method for obtaining truly inter-lingual word representations in order to train models with lexical features on several source languages at the same time. Briefly put,"
P15-1165,graca-etal-2008-building,0,0.0195628,"tings for these experiments were the same as those tuned on the Spanish development data from the Google Universal Treebanks v. 1.0. Baselines The most obvious baseline in our experiments is delexicalized transfer (D ELEX) (McDonald et al., 2011; Søgaard, 2011). This baseline system simply learns models without lexical features. We use a modified version of the first-order Mate 7 parser (Bohnet, 2010) that also takes continuousvalued embeddings as input an disregards features that include lexical items. 3.4 Word alignment Data We use the manually word-aligned EnglishSpanish Europarl data from Graca et al. (2008). The dataset contains 100 sentences. The annotators annotated whether word alignments were certain or possible, and we present results with all word alignments and with only the certain ones. See Graca et al. (2008) for details. Baselines For word alignment, we simply align every aligned word in the gold data, for which we have a word embedding, to its (Euclidean) nearest neighbor in the target sentence. We evaluate this strategy by its precision (P@1). System We compare I NVERTED with K LEMEN TIEV and C HANDAR . To ensure a fair comparison, we use the subset of words covered by all three emb"
P15-1165,C12-1089,0,0.772355,"th rely on three level architectures with input, output and a middle layer for intermediate target word representations. The major difference is that skip-gram uses the target word as input and the context as output, whereas the CBOW model does it the other way around. Learning goes by back-propagation, and random target words are used as negative examples. Levy and Goldberg (2014) show that prediction-based representations obtained with the skip-gram model can be related to count-based ones obtained with PMI. They argue that which is best, varies across tasks. 2.1.2 Bilingual representations Klementiev et al. (2012) learn distinct embedding models for the source and target languages, but while learning to minimize the sum of the two models’ losses, they jointly learn a regularizing interaction matrix, enforcing word pairs aligned in parallel text to have similar representations. Note that Klementiev et al. (2012) rely on word-aligned parallel text, and thereby on a large-coverage soft mapping of source words to target words. Other approaches rely on small coverage dictionaries with hard 1:1 mappings between words. Klementiev et al. (2012) do not use skip-gram or CBOW, but the language model presented in"
P15-1165,P14-2050,0,0.00763126,"ed Table 1: Three nearest neighbors in the English training data of six words occurring in the Spanish test data, in the embeddings used in our experiments. Only 2/6 words were in the German data. skip-gram model and CBOW. The two models both rely on three level architectures with input, output and a middle layer for intermediate target word representations. The major difference is that skip-gram uses the target word as input and the context as output, whereas the CBOW model does it the other way around. Learning goes by back-propagation, and random target words are used as negative examples. Levy and Goldberg (2014) show that prediction-based representations obtained with the skip-gram model can be related to count-based ones obtained with PMI. They argue that which is best, varies across tasks. 2.1.2 Bilingual representations Klementiev et al. (2012) learn distinct embedding models for the source and target languages, but while learning to minimize the sum of the two models’ losses, they jointly learn a regularizing interaction matrix, enforcing word pairs aligned in parallel text to have similar representations. Note that Klementiev et al. (2012) rely on word-aligned parallel text, and thereby on a lar"
P15-1165,D12-1127,0,0.0369956,"Missing"
P15-1165,D11-1006,0,0.435697,"the availability of English resources and the availability of parallel data for (and translations between) English and most other languages. In cross-lingual syntactic parsing, for example, two approaches to cross-lingual learning have been explored, namely annotation projection and delexicalized transfer. Annotation projection (Hwa et al., 2005) uses word-alignments in human translations to project predicted sourceside analyses to the target language, producing a noisy syntactically annotated resource for the target language. On the other hand, delexicalized transfer (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011) simply removes lexical features from mono-lingual parsing models, but assumes reliable POS tagging for the target language. Delexicalized transfer works particularly well when resources from several source languages are used for training; learning from multiple other languages prevents over-fitting to the peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackstr¨om et al., 2013; Xiao"
P15-1165,D09-1139,0,0.0521645,"Missing"
P15-1165,P10-1114,0,0.0276803,"on and word alignment, we fix the number of dimensions to 40. For both our baselines and systems, we also tune a scaling factor σ ∈ {1.0, 0.1, 0.01, 0.001} for POS tagging and dependency parsing, using the scaling method from Turian et al. (2010), also used in Gouws and Søgaard (2015). We do not scale our embeddings for document classification or word alignment. 3 Experiments The data set characteristics are found in Table 2.3. 3.1 Document classification Data Our first document classification task is topic classification on the cross-lingual multi-domain sentiment analysis dataset A MAZON in Prettenhofer and Stein (2010).4 We represent each document by the average of the representations of those words that we find both in the documents and in our embeddings. Rather than classifying reviews by sentiment, we classify by topic, trying to discriminate between book reviews, music reviews and DVD reviews, as a three-way classification problem, training on English and testing on German. Unlike in the other tasks below, we always 4 use unscaled word representations, since these are our only features. All word representations have 40 dimensions. The other document classification task is a fourway classification proble"
P15-1165,P11-1061,0,0.039505,"the target language. Delexicalized transfer works particularly well when resources from several source languages are used for training; learning from multiple other languages prevents over-fitting to the peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackstr¨om et al., 2013; Xiao and Guo, 2014). In cross-lingual POS tagging, mostly annotation projection has been explored (Fossum and Abney, 2005; Das and Petrov, 2011), since all features in POS tagging models are typically lexical. However, using bilingual word representations was recently explored as an alternative to projectionbased approaches (Gouws and Søgaard, 2015). The major drawback of using bi-lexical representations is that it limits us to using a single source language. T¨ackstr¨om et al. (2013) obtained significant improvements using bilingual word clusters over a single source delexicalized transfer model, for example, but even better results were obtained with delexicalized transfer in McDonald et al. (2011) by simply using several source lan"
P15-1165,P11-2120,1,0.534532,"lish resources and the availability of parallel data for (and translations between) English and most other languages. In cross-lingual syntactic parsing, for example, two approaches to cross-lingual learning have been explored, namely annotation projection and delexicalized transfer. Annotation projection (Hwa et al., 2005) uses word-alignments in human translations to project predicted sourceside analyses to the target language, producing a noisy syntactically annotated resource for the target language. On the other hand, delexicalized transfer (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011) simply removes lexical features from mono-lingual parsing models, but assumes reliable POS tagging for the target language. Delexicalized transfer works particularly well when resources from several source languages are used for training; learning from multiple other languages prevents over-fitting to the peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackstr¨om et al., 2013; Xiao and Guo, 2014)."
P15-1165,I05-1075,0,0.0207395,"reliable POS tagging for the target language. Delexicalized transfer works particularly well when resources from several source languages are used for training; learning from multiple other languages prevents over-fitting to the peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackstr¨om et al., 2013; Xiao and Guo, 2014). In cross-lingual POS tagging, mostly annotation projection has been explored (Fossum and Abney, 2005; Das and Petrov, 2011), since all features in POS tagging models are typically lexical. However, using bilingual word representations was recently explored as an alternative to projectionbased approaches (Gouws and Søgaard, 2015). The major drawback of using bi-lexical representations is that it limits us to using a single source language. T¨ackstr¨om et al. (2013) obtained significant improvements using bilingual word clusters over a single source delexicalized transfer model, for example, but even better results were obtained with delexicalized transfer in McDonald et al. (2011) by simply u"
P15-1165,Q13-1001,0,0.0450202,"Missing"
P15-1165,P10-1040,0,0.0800173,"NLL 07 – D EPENDENCY PARSING en es de sv 18.6 – – – 447k – – – en es – – – – – 206 357 389 – 5.7k 5.7k 5.7k – 0.841 0.616 n/a E UROPARL – W ORD A LIGNMENT 100 100 – – 0.370 0.533 Table 2: Characteristics of the data sets. Embeddings coverage (token-level) for K LEMENTIEV, C HAN DAR and I NVERTED on the test sets. We use the common vocabulary on W ORD A LIGNMENT . sification and word alignment, we fix the number of dimensions to 40. For both our baselines and systems, we also tune a scaling factor σ ∈ {1.0, 0.1, 0.01, 0.001} for POS tagging and dependency parsing, using the scaling method from Turian et al. (2010), also used in Gouws and Søgaard (2015). We do not scale our embeddings for document classification or word alignment. 3 Experiments The data set characteristics are found in Table 2.3. 3.1 Document classification Data Our first document classification task is topic classification on the cross-lingual multi-domain sentiment analysis dataset A MAZON in Prettenhofer and Stein (2010).4 We represent each document by the average of the representations of those words that we find both in the documents and in our embeddings. Rather than classifying reviews by sentiment, we classify by topic, trying t"
P15-1165,W14-1613,0,0.53136,"2011; Søgaard, 2011) simply removes lexical features from mono-lingual parsing models, but assumes reliable POS tagging for the target language. Delexicalized transfer works particularly well when resources from several source languages are used for training; learning from multiple other languages prevents over-fitting to the peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackstr¨om et al., 2013; Xiao and Guo, 2014). In cross-lingual POS tagging, mostly annotation projection has been explored (Fossum and Abney, 2005; Das and Petrov, 2011), since all features in POS tagging models are typically lexical. However, using bilingual word representations was recently explored as an alternative to projectionbased approaches (Gouws and Søgaard, 2015). The major drawback of using bi-lexical representations is that it limits us to using a single source language. T¨ackstr¨om et al. (2013) obtained significant improvements using bilingual word clusters over a single source delexicalized transfer model, for example, b"
P15-1165,I08-3008,0,0.017072,"asons for this; namely, the availability of English resources and the availability of parallel data for (and translations between) English and most other languages. In cross-lingual syntactic parsing, for example, two approaches to cross-lingual learning have been explored, namely annotation projection and delexicalized transfer. Annotation projection (Hwa et al., 2005) uses word-alignments in human translations to project predicted sourceside analyses to the target language, producing a noisy syntactically annotated resource for the target language. On the other hand, delexicalized transfer (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011) simply removes lexical features from mono-lingual parsing models, but assumes reliable POS tagging for the target language. Delexicalized transfer works particularly well when resources from several source languages are used for training; learning from multiple other languages prevents over-fitting to the peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackst"
P15-2044,I05-1075,0,0.892023,"the mappings in Petrov et al. (2011) for six new languages (Hindi, Croatian, Icelandic, Norwegian, Persian, and Serbian). The models, mappings, as well as a complete list of all the resources used in these experiments, are available at https://bitbucket.org/lowlands/. Introduction Most previous work in cross-lingual NLP has been limited to training and evaluating on no more than a dozen languages, typically all from the major Indo-European languages. While it has been observed repeatedly that using multiple source languages improves performance (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Fossum and Abney, 2005; McDonald et al., 2011), most available techniques work best for closely related languages. In contrast, this paper presents an effort to learn POS taggers for truly low-resource languages, with minimum assumptions about the available language resources. Most low-resource languages 2 Experiments Our approach is a combination of simple techniques. Part of the process is depicted in Figure 1, and the algorithm is presented in Algorithm 1. Assume we have n languages for which we assume the availability of m verses of the Bible. We run IBM-21 on all n(n − 1) pairs of languages. Assume also manual"
P15-2044,N13-1014,0,0.0620146,"Missing"
P15-2044,D12-1127,0,0.146746,"Missing"
P15-2044,D11-1006,0,0.108197,"t al. (2011) for six new languages (Hindi, Croatian, Icelandic, Norwegian, Persian, and Serbian). The models, mappings, as well as a complete list of all the resources used in these experiments, are available at https://bitbucket.org/lowlands/. Introduction Most previous work in cross-lingual NLP has been limited to training and evaluating on no more than a dozen languages, typically all from the major Indo-European languages. While it has been observed repeatedly that using multiple source languages improves performance (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Fossum and Abney, 2005; McDonald et al., 2011), most available techniques work best for closely related languages. In contrast, this paper presents an effort to learn POS taggers for truly low-resource languages, with minimum assumptions about the available language resources. Most low-resource languages 2 Experiments Our approach is a combination of simple techniques. Part of the process is depicted in Figure 1, and the algorithm is presented in Algorithm 1. Assume we have n languages for which we assume the availability of m verses of the Bible. We run IBM-21 on all n(n − 1) pairs of languages. Assume also manually POS-annotated trainin"
P15-2044,W03-0414,0,0.043386,"tical to, overlaps, is a subset, or is a superset of the Wiktionary tags. German, and Spanish to Czech and French. The resulting annotated target language corpora enable them to train POS taggers for these languages. Yarowsky and Ngai (2001) showed similar results using just the Hansards corpus on English to French and Chinese. Our work is inspired by these approaches, yet broader in scope on both the source and target side. Das and Petrov (2011) use word-aligned text to automatically create type-level tag dictionaries. Earlier work on building tag dictionaries from word-aligned text includes Probst (2003). Their tag dictionaries contain target language trigrams to be able to disambiguate ambiguous target language words. To handle the noise in the automatically obtained dictionaries, they use label propagation on a similarity graph to smooth and expand the label distributions. Our approach is similar to theirs in using projections to obtain type-level tag dictionaries, but we keep the token supervision and type supervision apart and end up with a model more similar to that of T¨ackstr¨om et al. (2013), who combine word-aligned text with crowdsourced type-level tag dictionaries. T¨ackstr¨om et a"
P15-2044,W14-5302,0,0.0715114,"ollow T¨ackstr¨om et al. (2013) in using our automatically created, not crowdsourced, tag dictionaries to prune tags during search, but we use word alignments to obtain token-level annotations that we use as annotated training data, similar to Fossum Related work The Bible has been used as a resource for machine translation and multi-lingual information retrieval before, e.g., (Chew et al., 2006). It has also been used in cross-lingual POS tagging (Yarowsky et al., 2001; Fossum and Abney, 2005), NP-chunking (Yarowsky et al., 2001; Yarowsky and Ngai, 2001) and cross-lingual dependency parsing (Sukhareva and Chiarcos, 2014) before. Yarowsky et al. (2001) and Fossum and Abney (2005) use word-aligned parallel translations of the Bible to project the predictions of POS taggers for several language pairs, including English, 271 and Abney (2005), Yarowsky et al. (2001), and Yarowsky and Ngai (2001). Duong et al. (2013) use word-alignment probabilities to select training data for their cross-lingual POS models. They consider a simple single-source training set-up. We also tried ranking projected training data by confidence, using an ensemble of projections from 17–99 source languages and majority voting to obtain prob"
P15-2044,Q13-1001,0,0.0845969,"Missing"
P15-2044,N01-1026,0,0.568625,"cly available and extend the mappings in Petrov et al. (2011) for six new languages (Hindi, Croatian, Icelandic, Norwegian, Persian, and Serbian). The models, mappings, as well as a complete list of all the resources used in these experiments, are available at https://bitbucket.org/lowlands/. Introduction Most previous work in cross-lingual NLP has been limited to training and evaluating on no more than a dozen languages, typically all from the major Indo-European languages. While it has been observed repeatedly that using multiple source languages improves performance (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Fossum and Abney, 2005; McDonald et al., 2011), most available techniques work best for closely related languages. In contrast, this paper presents an effort to learn POS taggers for truly low-resource languages, with minimum assumptions about the available language resources. Most low-resource languages 2 Experiments Our approach is a combination of simple techniques. Part of the process is depicted in Figure 1, and the algorithm is presented in Algorithm 1. Assume we have n languages for which we assume the availability of m verses of the Bible. We run IBM-21 on all n(n − 1) pairs of langu"
P15-2044,N10-1083,0,0.0532965,"Missing"
P15-2044,H01-1035,0,0.956542,"for 100 languages publicly available and extend the mappings in Petrov et al. (2011) for six new languages (Hindi, Croatian, Icelandic, Norwegian, Persian, and Serbian). The models, mappings, as well as a complete list of all the resources used in these experiments, are available at https://bitbucket.org/lowlands/. Introduction Most previous work in cross-lingual NLP has been limited to training and evaluating on no more than a dozen languages, typically all from the major Indo-European languages. While it has been observed repeatedly that using multiple source languages improves performance (Yarowsky et al., 2001; Yarowsky and Ngai, 2001; Fossum and Abney, 2005; McDonald et al., 2011), most available techniques work best for closely related languages. In contrast, this paper presents an effort to learn POS taggers for truly low-resource languages, with minimum assumptions about the available language resources. Most low-resource languages 2 Experiments Our approach is a combination of simple techniques. Part of the process is depicted in Figure 1, and the algorithm is presented in Algorithm 1. Assume we have n languages for which we assume the availability of m verses of the Bible. We run IBM-21 on al"
P15-2044,A00-1031,0,0.37703,"than we assume, as well as a representative sample of unlabeled data. Such data is simply not available for many of the languages considered here. The weakly supervised system in Li et al. (2012) (L I) also relies on crowd-sourced type-level tag dictionaries, not available for most of the languages of concern to us. We present their reported results. Finally, we train the two base POS taggers (G AR and T N T) on the manually annotated data available for 17 of our languages, to be able to compare against state-of-the-art performance of supervised POS taggers. ModelsWe train T N T POS taggers (Brants, 2000) using only token-level projections. We also train semi-supervised POS taggers using the approach in Garrette and Baldridge (2013) (G AR), using both projections and dictionaries, as well as the unlabelled Bible translations.3 We use the English data as development data. We train T N T and G AR 4 3 github.com/dhgarrette/ low-resource-pos-tagging-2014/ 5 270 github.com/percyliang/brown-cluster code.google.com/p/wikily-supervised-pos-tagger/ Results Our results on the 25 test languages are consistently better than the unsupervised baselines, with the exceptions of Marathi and Persian, and by a v"
P15-2044,W06-1009,0,0.0336178,"2013) constrain Viterbi search via type-level tag dictionaries, pruning all tags not licensed by the dictionary. For the remaining tags, they use high-confidence word alignments to further prune the Viterbi search. We follow T¨ackstr¨om et al. (2013) in using our automatically created, not crowdsourced, tag dictionaries to prune tags during search, but we use word alignments to obtain token-level annotations that we use as annotated training data, similar to Fossum Related work The Bible has been used as a resource for machine translation and multi-lingual information retrieval before, e.g., (Chew et al., 2006). It has also been used in cross-lingual POS tagging (Yarowsky et al., 2001; Fossum and Abney, 2005), NP-chunking (Yarowsky et al., 2001; Yarowsky and Ngai, 2001) and cross-lingual dependency parsing (Sukhareva and Chiarcos, 2014) before. Yarowsky et al. (2001) and Fossum and Abney (2005) use word-aligned parallel translations of the Bible to project the predictions of POS taggers for several language pairs, including English, 271 and Abney (2005), Yarowsky et al. (2001), and Yarowsky and Ngai (2001). Duong et al. (2013) use word-alignment probabilities to select training data for their cross-"
P15-2044,W02-1001,0,0.0695296,"un IBM-21 on all n(n − 1) pairs of languages. Assume also manually POS-annotated training data 1 268 github.com/clab/fast_align Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 268–272, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics l1 EN … HR DE word type in the target language as a type-level tag dictionary. We combine the tag dictionary and the token-level projections to train discriminative, type-constrained POS taggers (Collins, 2002; T¨ackstr¨om et al., 2013). Below we refer to these POS taggers as using k sources (k-S RC). These n many POS taggers can now also be used to obtain predictions for all word tokens in our tensor object. This corresponds to doing the second loop over lines 8–17 in Algorithm 1. For each of our n languages, we thus complete the tensor by projecting tags into word tokens from the n − 1 remaining source languages. For the k supervised languages, we project the tags produced by the supervised POS taggers rather than the tags obtained by projection. We can then train our final POS taggers for all n"
P15-2044,P11-1061,0,0.356124,"the NLTK corpora, the HamleDT resources, and the Universal Dependencies project. We provide a complete overview of the resources at https://bitbucket.org/lowlands/ using k or n − 1 source languages, leading to four taggers in total. Baselines Our baselines are two standard unsupervised POS induction algorithms: Brown clustering using the implementation by Percy Liang4 and second-order unsupervised HMMs using logistic regression for emission probabilities (BergKirkpatrick et al., 2010; Li et al., 2012), with and without our Bible tag dictionaries.5 Upper bounds The weakly supervised system in Das and Petrov (2011) (DAS) relies on larger volumes of more representative and perfectly tokenized parallel data than we assume, as well as a representative sample of unlabeled data. Such data is simply not available for many of the languages considered here. The weakly supervised system in Li et al. (2012) (L I) also relies on crowd-sourced type-level tag dictionaries, not available for most of the languages of concern to us. We present their reported results. Finally, we train the two base POS taggers (G AR and T N T) on the manually annotated data available for 17 of our languages, to be able to compare agains"
P15-2044,P13-2112,0,0.144625,"Missing"
P16-2091,Q16-1022,1,0.867526,"Missing"
P16-2091,A00-1031,0,0.265162,"st alignments across typologically dis¨ tant language pairs (Ostling, 2015). We modify Parallel texts We exploit two sources of parallel text: the Edinburgh Multilingual Bible corpus (EBC) (Christodouloupoulos and Steedman, 2014), and our own collection of online texts published by the Wathctower Society (WTC).4 While 4 ∈ {0, 1} Flow φi,k,j,l Data sources 3 ∈ {0, 1} 5 https://github.com/bplank/ multilingualtokenizer http://hdl.handle.net/11234/1-1548 https://www.jw.org/ 563 the aligner to output alignment probabilities. All the source-side texts are POS-tagged and dependency parsed using TnT (Brants, 2000) and TurboParser (Martins et al., 2013). We use our own fork of the arc-factored TurboParser to output the edge weight matrices.6 4 Approach Experiments 4.1 ILP 51.62 (18) 53.58 (20) DCA 48.39 (8) 48.40 (0) D ELEX 42.44 (1) 47.35 (3) Gold POS EBC WTC 65.43 (25) 66.51 (23) 59.94 (2) 55.73 (0) 64.13 (–) 66.68 (–) Table 1: Macro-averaged UAS scores summarizing our evaluation. EBC: Edinburgh Bible corpus, WTC: Watchtower corpus. Numbers of languages with top performance per system are reported in brackets. All parsers use their respective EBC or WTC taggers.8 Setup In our experiments, as in the pr"
P16-2091,C14-1175,0,0.438713,"aggers, yielding the top score for 16/23 languages from the overlap. Notably, on several non-IndoEuropean languages, we observe significant improvements. For example, on Indonesian, DCA improves over D ELEX by 12 points UAS, while ILP adds 6 more points on top. We observe a similar pattern for Arabic and Estonian. We note that D ELEX tops ILP and DCA on only 1 EBC and 3 WTC languages, and by a narrow margin. ILP The ILP-based joint projection algorithm we presented in Section 2. DCA Our implementation of the de facto standard annotation projection algorithm of Hwa et al. (2005), as refined by Tiedemann (2014). In contrast to our ILP approach, it uses heuristics to ensure dependency tree constraints on a sourcetarget sentence pair basis. We gather all the pairwise projections into a target sentence graph and then perform maximum spanning tree decoding following Sagae and Lavie (2006). D ELEX The multi-source direct delexicalized transfer baseline of McDonald et al. (2011). Each source is represented by an approximately equal number of sentences. 4.2 Predicted POS EBC WTC Analysis A projected parse is allowed to be a composite of edges from many source languages. To find out to what degree this actu"
P16-2091,mayer-cysouw-2014-creating,0,0.0280542,"gual taggers and parsers, we use the Universal Dependencies (UD) version 1.2 treebanks with the corresponding test sets.3 Preprocessing We use simple sentence splitting and tokenization models to segment the parallel corpora.5 To sentence- and word-align the individual language pairs, we use a Gibbs samplingbased IBM1 alignment model called efmaral ¨ (Ostling, 2015). IBM1 has been shown to lead to more robust alignments across typologically dis¨ tant language pairs (Ostling, 2015). We modify Parallel texts We exploit two sources of parallel text: the Edinburgh Multilingual Bible corpus (EBC) (Christodouloupoulos and Steedman, 2014), and our own collection of online texts published by the Wathctower Society (WTC).4 While 4 ∈ {0, 1} Flow φi,k,j,l Data sources 3 ∈ {0, 1} 5 https://github.com/bplank/ multilingualtokenizer http://hdl.handle.net/11234/1-1548 https://www.jw.org/ 563 the aligner to output alignment probabilities. All the source-side texts are POS-tagged and dependency parsed using TnT (Brants, 2000) and TurboParser (Martins et al., 2013). We use our own fork of the arc-factored TurboParser to output the edge weight matrices.6 4 Approach Experiments 4.1 ILP 51.62 (18) 53.58 (20) DCA 48.39 (8) 48.40 (0) D ELEX 42"
P16-2091,H01-1035,0,0.863149,"n Integer Linear Programming (ILP) algorithm that simultaneously projects annotation for multiple tasks from multiple source languages, relying on parallel corpora available for hundreds of languages. When training POS taggers and dependency parsers on jointly projected POS tags and syntactic dependencies using our algorithm, we obtain better performance than a standard approach on 20/23 languages using one parallel corpus; and 18/27 languages using another. 1 Introduction Cross-language annotation projection for unsupervised POS tagging and syntactic parsing was introduced fifteen years ago (Yarowsky et al., 2001; Hwa et al., 2005), and the best unsupervised dependency parsers today rely on annotation projection (Rasooli and Collins, 2015). Despite the maturity of the field, there is an inherent language bias in previous work on crosslanguage annotation projection. Cross-language annotation projection experiments require training data in m source languages, a parallel corpus of translations from the m source languages into the target language of interest, as well as evaluation data for the target language.1 Since the canonical resource for parallel text is the Europarl Corpus (Koehn, 2005), which cove"
P16-2091,P11-1061,0,0.129341,"ss-lingual taggers and parsers for 18/27 and 20/23 languages, depending on the parallel corpora used. We made no unrealistic assumptions as to the availability of parallel texts and preprocessing tools for the target languages. Our code and data is freely available.9 Related work In recent years, we note an increased interest for work in cross-lingual processing, and particularly in POS tagging and dependency parsing of lowresource languages. Yarowsky et al. (2001) proposed the idea of inducing NLP tools via parallel corpora. Their contribution started a line of work in annotation projection. Das and Petrov (2011) used graph-based label propagation to yield competitive POS taggers, while Hwa et al. (2005) introduced the projection of dependency trees. Tiedemann (2014) further improved this approach to single-source projection in the context of synthesizing dependency treebanks (Tiedemann and Agi´c, 2016). The current state of the art in cross-lingual dependency parsing also involves exploiting large parallel corpora (Ma and Xia, 2014; Rasooli and Acknowledgements This research is partially funded by the ERC Starting Grant LOWLANDS (#313695). 9 https://bitbucket.org/lowlands/ release 565 References Kenj"
P16-2091,I08-3008,0,0.295176,". Note that D ELEX is trained on gold POS and therefore has an advantage in this 6 https://github.com/andersjo/ TurboParser 8 We do not include D ELEX in the comparison for the gold POS scenario only. In this particular scenario, D ELEX is also trained on gold POS, and thus biased: the cross-lingual taggers do not have gold POS available for training, and the same holds for D ELEX and projected POS. Manually annotated data We annotate a small number of sentences in English from EBC and 564 Collins, 2015). Transferring models by training parsers without lexical features was first introduced by Zeman and Resnik (2008). McDonald et al. (2011) and Søgaard (2011) coupled delexicalization with contributions from multiple sources, while McDonald et al. (2013) were the first to leverage uniform representations of POS and syntactic dependencies in cross-lingual parsing. Even more recently, Agi´c et al. (2015) exposed a bias towards closely related Indo-European languages shared by most previous work on annotation projection, while introducing a bias-free projection algorithm for learning 100 POS taggers from multiple sources. Their line of work is non-trivially extended to multilingual dependency parsing by Agi´c"
P16-2091,2005.mtsummit-papers.11,0,0.0204285,"o (Yarowsky et al., 2001; Hwa et al., 2005), and the best unsupervised dependency parsers today rely on annotation projection (Rasooli and Collins, 2015). Despite the maturity of the field, there is an inherent language bias in previous work on crosslanguage annotation projection. Cross-language annotation projection experiments require training data in m source languages, a parallel corpus of translations from the m source languages into the target language of interest, as well as evaluation data for the target language.1 Since the canonical resource for parallel text is the Europarl Corpus (Koehn, 2005), which covers languages spoken in the European parliament, annotation projection is Contributions We present a novel ILP-based algorithm for jointly projecting POS labels and dependency annotations across word-aligned parallel corpora. The performance of our algorithm compares favorably to that of a state-of-the-art projection algorithm, as well as to multi-source delexicalized transfer. Our experiments include between 23 and 27 languages using two parallel corpora that are available for hundreds of languages, namely a collection of Bibles and Watchtower periodicals. Finally, we make both the"
P16-2091,P14-1126,0,0.244444,"Missing"
P16-2091,P13-2109,0,0.101891,"Missing"
P16-2091,D11-1006,0,0.623582,"rmance of our algorithm compares favorably to that of a state-of-the-art projection algorithm, as well as to multi-source delexicalized transfer. Our experiments include between 23 and 27 languages using two parallel corpora that are available for hundreds of languages, namely a collection of Bibles and Watchtower periodicals. Finally, we make both the parallel corpora and the code publicly available.2 2 Projection algorithm The projection algorithm is divided into two distinct steps. First, we project potential syntactic 1 All previous work that we are aware of—with the possible exception of McDonald et al. (2011); but see Sections 2 and 5—uses only a single source (m = 1), but in our experiments, we use multiple source languages. 2 https://bitbucket.org/lowlands/ release 561 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 561–566, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics the POS labels are unknown, which is to say that every target token is ambiguous between |Σ |POS tags. We represent this ambiguity in the graph by creating a vertex for each possible combination of target word and POS. Concretely, if a source se"
P16-2091,P15-2034,0,0.0686959,"languages, we focus on the subsets that overlap with the UD languages to facilitate evaluation. For EBC, that amounts to 27 languages, and 23 for WTC. Treebanks To train the source-side taggers and dependency parsers, and to evaluate the crosslingual taggers and parsers, we use the Universal Dependencies (UD) version 1.2 treebanks with the corresponding test sets.3 Preprocessing We use simple sentence splitting and tokenization models to segment the parallel corpora.5 To sentence- and word-align the individual language pairs, we use a Gibbs samplingbased IBM1 alignment model called efmaral ¨ (Ostling, 2015). IBM1 has been shown to lead to more robust alignments across typologically dis¨ tant language pairs (Ostling, 2015). We modify Parallel texts We exploit two sources of parallel text: the Edinburgh Multilingual Bible corpus (EBC) (Christodouloupoulos and Steedman, 2014), and our own collection of online texts published by the Wathctower Society (WTC).4 While 4 ∈ {0, 1} Flow φi,k,j,l Data sources 3 ∈ {0, 1} 5 https://github.com/bplank/ multilingualtokenizer http://hdl.handle.net/11234/1-1548 https://www.jw.org/ 563 the aligner to output alignment probabilities. All the source-side texts are PO"
P16-2091,D15-1039,0,0.139868,"languages, relying on parallel corpora available for hundreds of languages. When training POS taggers and dependency parsers on jointly projected POS tags and syntactic dependencies using our algorithm, we obtain better performance than a standard approach on 20/23 languages using one parallel corpus; and 18/27 languages using another. 1 Introduction Cross-language annotation projection for unsupervised POS tagging and syntactic parsing was introduced fifteen years ago (Yarowsky et al., 2001; Hwa et al., 2005), and the best unsupervised dependency parsers today rely on annotation projection (Rasooli and Collins, 2015). Despite the maturity of the field, there is an inherent language bias in previous work on crosslanguage annotation projection. Cross-language annotation projection experiments require training data in m source languages, a parallel corpus of translations from the m source languages into the target language of interest, as well as evaluation data for the target language.1 Since the canonical resource for parallel text is the Europarl Corpus (Koehn, 2005), which covers languages spoken in the European parliament, annotation projection is Contributions We present a novel ILP-based algorithm for"
P16-2091,N06-2033,0,\N,Missing
P16-2091,P15-2044,1,\N,Missing
P17-2107,D13-1033,0,0.0705626,"Missing"
P17-2107,Q16-1022,1,0.789796,"Missing"
P17-2107,C10-1011,0,0.0830878,"Missing"
P17-2107,P13-1043,0,0.0586178,"Missing"
P17-2107,A00-1031,0,0.628016,"Missing"
P17-2107,P12-2003,0,0.0651115,"Missing"
P17-2107,D15-1162,0,0.0774527,"Missing"
P17-2107,N15-1080,0,0.0584184,"Missing"
P17-2107,L16-1262,0,0.0527392,"Missing"
P19-1310,D18-1214,0,0.0587474,"Missing"
P19-1310,P18-1073,0,0.124612,"he copyright holder; see https:// www.jw.org/en/terms-of-use/. For all practical purposes their custom terms of use are very closely aligned with the more well-known CC2 We acknowledge the anonymous area chair who contributed this valuable argument as part of their meta-review. 3205 EN ET HR MR MT EN ET HR MR MT – 0.314 0.269 0.094 0.131 0.280 – 0.334 0.144 0.206 0.254 0.302 – 0.112 0.164 0.0 0.001 0.002 – 0.141 0.001 0.0 0.0 0.001 – Table 2: BLI results (MRR scores) on a small subset of JW300 language pairs. The scores with the best-performing unsupervised cross-lingual word embedding model (Artetxe et al., 2018) are in gray cells over the main diagonal; the scores with a simple supervised method (Smith et al., 2017) are below the main diagonal. Better performance for each pair in bold. BY-NC-SA license.3 3 3.1 Experiments Cross-lingual word embedding induction A recent trend in cross-lingual word embedding induction are fully unsupervised projection-based methods that learn on the basis of monolingual data only (Conneau et al., 2018; Alvarez-Melis and Jaakkola, 2018; Chen and Cardie, 2018, inter alia). The main idea is to construct a seed bilingual dictionary in an unsupervised fashion relying on adv"
P19-1310,Q17-1010,0,0.0299646,"lingual lexicon induction task (BLI).4 For the demonstration purposes, we work with all pairs from the following language set: English (EN), Estonian (ET), Croatian (HR), Marathi (MR), and Maltese (MT). Our seed bilingual dictionaries are extracted from the JW300 corpora by taking the most probable target translation for each source word from IBM1-based word translation tables. Following prior work, we use the 5K most frequent translation pairs from training, while the next 2K pairs are used for testing. We use 300-dim monolingual fastText embeddings pretrained on Wikipedia for all languages (Bojanowski et al., 2017),5 but the same trends are observed with other monolingual embeddings. The results in terms of Mean Reciprocal Rank (MRR) are summarized in Table 2. The BLI results are straightforward to interpret: for all experimental runs a simple supervised model with its supervision extracted from the JW300 corpus outperforms its unsupervised competition, further confirming the findings of Glavaˇs et al. (2019). The unsupervised model is even unable to converge for most language pairs, yielding extremely low MRR scores. The scores on another test set (Conneau et al., 2018) for EN - ET and EN - HR also fav"
P19-1310,D18-1024,0,0.0226289,"set of JW300 language pairs. The scores with the best-performing unsupervised cross-lingual word embedding model (Artetxe et al., 2018) are in gray cells over the main diagonal; the scores with a simple supervised method (Smith et al., 2017) are below the main diagonal. Better performance for each pair in bold. BY-NC-SA license.3 3 3.1 Experiments Cross-lingual word embedding induction A recent trend in cross-lingual word embedding induction are fully unsupervised projection-based methods that learn on the basis of monolingual data only (Conneau et al., 2018; Alvarez-Melis and Jaakkola, 2018; Chen and Cardie, 2018, inter alia). The main idea is to construct a seed bilingual dictionary in an unsupervised fashion relying on adversarial training (Conneau et al., 2018), monolingual similarity distributions (Artetxe et al., 2018) or PCA projection similarities (Hoshen and Wolf, 2018), and then learn (gradually refined) projections of two monolingual embedding spaces into a shared cross-lingual space (by also iteratively refining the seed dictionary). Such models hold promise to support crosslingual representation learning for resource-poor language pairs. However, besides their problems with training diverg"
P19-1310,P11-1061,0,0.196274,"-of-speech projection. 1 Introduction In natural language processing (NLP) the rule of thumb is that if we possess some parallel data for a low-resource target language, then we can yield feasible basic tools such as part-of-speech taggers for that language. Without such distant supervision, this task and many others remain unattainable, leaving the majority of languages in the world without basic language technology. Parallel data features a prominent role in building multilingual word representations (Ruder et al., 2017), annotation projection for parts-of-speech and syntactic dependencies (Das and Petrov, 2011; Tiedemann, 2014) and naturally machine translation. The shortage of parallel data in turn creates a bottleneck in cross-lingual processing: without parallel sentences, we cannot yield usable models, nor can we robustly evaluate them, if even just approximately (cf. Agi´c et al. 2017). This absence has over the recent years materialized the proxy fallacy, whereby intended low-resource methods are tested by proxy, exclusively on resource-rich languages, because of the absence of test data or the lack of effort to produce it for approximate evaluation. We seek to alleviate these issues by a sig"
P19-1310,P19-1070,1,0.871609,"Missing"
P19-1310,W19-3621,0,0.0272487,"s, words, and alignments, as well as an illustration of their distributions. Counts are reported for languages with at least one non-empty alignment to another language. Some languages have multiple datasets, e.g. different scripts, sign language. Moreover, the ideological bias of JW300 is fairly well-defined. In that sense, while bias may invalidate the use of our corpus in some application areas, we argue that a wide-coverage collection of parallel data with known bias may in fact be valuable for research on bias in NLP (Bolukbasi et al., 2016; Caliskan et al., 2017; Dev and Phillips, 2019; Gonen and Goldberg, 2019), especially in multilingual settings (Lauscher and Glavaˇs, 2019).2 JW300 excels in low-resource language coverage. For example, OPUS offers over 100 million English-German parallel sentences, and JW300 only 2.1 million. However, in another example, for Afrikaans-Croatian the counts are 300 thousand in OPUS and 990 thousand in JW300, and moreover, the OPUS data for this language pair contains only Linux localizations. Availability. Our dataset is freely available for all non-commercial use. The exact terms of use are provided by the copyright holder; see https:// www.jw.org/en/terms-of-use/."
P19-1310,D18-1043,0,0.0204819,". Better performance for each pair in bold. BY-NC-SA license.3 3 3.1 Experiments Cross-lingual word embedding induction A recent trend in cross-lingual word embedding induction are fully unsupervised projection-based methods that learn on the basis of monolingual data only (Conneau et al., 2018; Alvarez-Melis and Jaakkola, 2018; Chen and Cardie, 2018, inter alia). The main idea is to construct a seed bilingual dictionary in an unsupervised fashion relying on adversarial training (Conneau et al., 2018), monolingual similarity distributions (Artetxe et al., 2018) or PCA projection similarities (Hoshen and Wolf, 2018), and then learn (gradually refined) projections of two monolingual embedding spaces into a shared cross-lingual space (by also iteratively refining the seed dictionary). Such models hold promise to support crosslingual representation learning for resource-poor language pairs. However, besides their problems with training divergence (Søgaard et al., 2018), a recent empirical study (Glavaˇs et al., 2019) has demonstrated that even most robust projectionbased unsupervised models cannot match the performance of projection-based methods which require only 1K-5K seed translation pairs. The largesca"
P19-1310,D18-1330,0,0.0604572,"Missing"
P19-1310,L18-1293,0,0.0562168,"Missing"
P19-1310,2005.mtsummit-papers.11,0,0.387508,"Since these systems are complementary, future work could further explore the benefits of injecting the improved JW300 projections to more complex learners such as D S D S. In particular, D S D S would likely benefit from better projections, since the ones that its current instance uses are inferior to JW300. 4 Related work Our work is a contribution to the pool of massively multilingual resources. In that pool we already singled out OPUS (Tiedemann, 2012) as the largest collection of freely available parallel sentences to date. OPUS is a collection that covers large datasets such as Europarl (Koehn, 2005), OpenSubtitles (Lison and Tiedemann, 2016), along with many others. OPUS also contains a smaller snapshot of Tatoeba, whose original collection hosts 337 languages and 22,427 (±106,815) sentences on average.6 Moving from OPUS and Tatoeba towards greater linguistic breadth, there are several publicly available Bible datasets, most notably those by Mayer and Cysouw (2014) and Christodouloupoulos and Steedman (2015). The Bible datasets are typically aligned by verse and not by sentence, because verse identifiers are assigned by humans, with absolute accuracy. However, a verse sometimes comprises"
P19-1310,2013.mtsummit-papers.10,0,0.101788,"Missing"
P19-1310,S19-1010,0,0.0620545,"Missing"
P19-1310,L16-1147,0,0.0602004,"ementary, future work could further explore the benefits of injecting the improved JW300 projections to more complex learners such as D S D S. In particular, D S D S would likely benefit from better projections, since the ones that its current instance uses are inferior to JW300. 4 Related work Our work is a contribution to the pool of massively multilingual resources. In that pool we already singled out OPUS (Tiedemann, 2012) as the largest collection of freely available parallel sentences to date. OPUS is a collection that covers large datasets such as Europarl (Koehn, 2005), OpenSubtitles (Lison and Tiedemann, 2016), along with many others. OPUS also contains a smaller snapshot of Tatoeba, whose original collection hosts 337 languages and 22,427 (±106,815) sentences on average.6 Moving from OPUS and Tatoeba towards greater linguistic breadth, there are several publicly available Bible datasets, most notably those by Mayer and Cysouw (2014) and Christodouloupoulos and Steedman (2015). The Bible datasets are typically aligned by verse and not by sentence, because verse identifiers are assigned by humans, with absolute accuracy. However, a verse sometimes comprises several sentences, or alternatively just p"
P19-1310,mayer-cysouw-2014-creating,0,0.0581759,"l of massively multilingual resources. In that pool we already singled out OPUS (Tiedemann, 2012) as the largest collection of freely available parallel sentences to date. OPUS is a collection that covers large datasets such as Europarl (Koehn, 2005), OpenSubtitles (Lison and Tiedemann, 2016), along with many others. OPUS also contains a smaller snapshot of Tatoeba, whose original collection hosts 337 languages and 22,427 (±106,815) sentences on average.6 Moving from OPUS and Tatoeba towards greater linguistic breadth, there are several publicly available Bible datasets, most notably those by Mayer and Cysouw (2014) and Christodouloupoulos and Steedman (2015). The Bible datasets are typically aligned by verse and not by sentence, because verse identifiers are assigned by humans, with absolute accuracy. However, a verse sometimes comprises several sentences, or alternatively just parts of one sentence, thus in effect replacing one type of alignment noise with another. Our results strongly favor JW300 for part-of-speech projection. 6 https://tatoeba.org/eng/stats/ sentences_by_language Prior to our work, Agi´c et al. (2016) have also collected a smaller dataset from jw.org to produce cross-lingual dependen"
P19-1310,petrov-etal-2012-universal,0,0.126649,"Missing"
P19-1310,D18-1061,1,0.907582,"Missing"
P19-1310,P16-2067,0,0.185619,"ce word vs packs a label distribution p(l|vs ) of tagger confidences across parts of speech l ∈ L. On top of this parallel dataset, we implement the best practices in annotation projection of sequential labels from multiple sources with low-resource target languages in mind: – Word alignments are obtained from an IBM1 ¨ model Efmaral (Ostling and Tiedemann, 2016) as Agi´c et al. (2016) show that simpler alignment models favor low-resource languages. Thus we acquire all a(vs , vt ) ∈ A. – Source sentences are tagged for parts of speech by a state-of-the-art neural tagger with default settings (Plank et al., 2016). That way all source words attain a tag distribution p(l|vs ). – Source tags are projected through the word alignments and accumulated at the target ends: BALLOT (l|vt ) = X The part-of-speech tag for each target word vt is finally decoded through simple weighted majority voting: = arg max BALLOT(l|vt ). l – The sentences are further filtered so as to remove noisy instances. The model by Plank et al. (2018) is used, whereby for training we select only the top 10 thousand target sentences ranked by mean word alignment coverage ct : ct = n 1X ci,t . n i=1 Mean coverage ct is defined through ind"
P19-1310,P18-1072,1,0.903108,"Missing"
P19-1310,Q13-1001,0,0.0672287,"Missing"
P19-1310,tiedemann-2012-parallel,0,0.857484,"sentences like English, French, and Italian which are all rich in resources. However, the long tail of lowresource languages typically still offers between 50-100 thousand sentences. articles sentences tokens alignments Comparison. With its balance between multilingual breadth and monolingual depth, JW300 fills an important gap in cross-lingual resources: it comprises a multitude of low-resource languages while still offering ample sentences for each individual language, and parallel sentences for language pairs. To illustrate, for JW300 the breadth × depth ratio is 1.2x larger than for OPUS (Tiedemann, 2012), 2x larger than for the full Bible, and even 3x that of New Testament (see Figure 1). JW300 still does come with its own caveats. The crucial one is surely bias: For example, could we indiscriminately use JW300 to train complex machine learning systems that further propagate the attitude of jw.org towards gender differences? From another viewpoint, however, should we rather train part-of-speech taggers through multi-source annotation projection from Watchtower articles on one side, or OPUS Ubuntu menu localizations or Bible psalms on the other side? 343 417 54,376 µ σ 3,202.34 261,573.37 3,54"
P19-1310,C14-1175,0,0.0228576,"1 Introduction In natural language processing (NLP) the rule of thumb is that if we possess some parallel data for a low-resource target language, then we can yield feasible basic tools such as part-of-speech taggers for that language. Without such distant supervision, this task and many others remain unattainable, leaving the majority of languages in the world without basic language technology. Parallel data features a prominent role in building multilingual word representations (Ruder et al., 2017), annotation projection for parts-of-speech and syntactic dependencies (Das and Petrov, 2011; Tiedemann, 2014) and naturally machine translation. The shortage of parallel data in turn creates a bottleneck in cross-lingual processing: without parallel sentences, we cannot yield usable models, nor can we robustly evaluate them, if even just approximately (cf. Agi´c et al. 2017). This absence has over the recent years materialized the proxy fallacy, whereby intended low-resource methods are tested by proxy, exclusively on resource-rich languages, because of the absence of test data or the lack of effort to produce it for approximate evaluation. We seek to alleviate these issues by a significant new addit"
P19-1310,H01-1035,0,0.0789081,"he findings of Glavaˇs et al. (2019). The unsupervised model is even unable to converge for most language pairs, yielding extremely low MRR scores. The scores on another test set (Conneau et al., 2018) for EN - ET and EN - HR also favour the supervised model: 0.342 vs. 0.313 on EN - ET, and 0.289 vs. 0.261 on EN - HR. In sum, these preliminary experiments indicate the potential of JW300 in guiding cross-lingual representation learning. 3.2 Part-of-speech projection Massively parallel data has proven most useful in inducing basic NLP models such as part-of-speech taggers. The formative work by Yarowsky et al. (2001) has inspired many influential works in projecting sequential labels from multiple source languages (Das and Petrov, 2011; T¨ackstr¨om et al., 2013), as well as projecting more complex annotations such as syntactic and semantic dependencies (Hwa et al., 2005; Pad´o and Lapata, 2009; Agi´c et al., 2016). Here we implement an experiment with projecting parts of speech from multiple sources to multiple targets following the line of work by Agi´c et al. (2015) and subsequently Plank et al. (2018), to showcase our corpus. 4 We expect even better performance with recently developed more sophisticate"
padro-etal-2014-language,J93-2004,0,\N,Missing
padro-etal-2014-language,E06-1011,0,\N,Missing
padro-etal-2014-language,W07-1702,1,\N,Missing
padro-etal-2014-language,N03-1033,0,\N,Missing
padro-etal-2014-language,W03-1712,0,\N,Missing
padro-etal-2014-language,W08-2102,1,\N,Missing
padro-etal-2014-language,E09-1005,0,\N,Missing
padro-etal-2014-language,P05-1012,0,\N,Missing
padro-etal-2014-language,P08-1068,1,\N,Missing
padro-etal-2014-language,W09-1201,0,\N,Missing
padro-etal-2014-language,W14-0150,0,\N,Missing
padro-etal-2014-language,Q13-1018,1,\N,Missing
padro-etal-2014-language,taule-etal-2008-ancora,0,\N,Missing
padro-etal-2014-language,D07-1101,1,\N,Missing
padro-etal-2014-language,padro-stanilovsky-2012-freeling,1,\N,Missing
padro-etal-2014-language,P05-1045,0,\N,Missing
Q16-1022,A00-1031,0,0.555773,"observe a major advantage in using reverse-mode alignment for POS projection (4-5 accuracy points absolute).10 In addition, we use the IBM1 aligner efmaral11 by ¨ Ostling (2015). The intuition behind using IBM1 is that IBM2 introduces a bias toward more closely related languages, and we confirm this intuition through our experiments. We modify both aligners so that they output the alignment probability for each aligned token pair. Tagging and parsing The source-sides of the two multi-parallel corpora, EBC and WTC, are POStagged by taggers trained on the respective source languages, using TnT (Brants, 2000). We parse the corpora using TurboParser (Martins et al., 2013). The parser is used in simple arc-factored mode with pruning.12 We alter it to output per-sentence arc 7 http://universaldependencies.org/ format.html 8 https://github.com/coastalcph/ ud-conversion-tools. 9 Parameters used: utf, bisent, cautious, realign. 10 Parameters used: d, o, v, r. 11 Also reverse mode, with default settings, see https:// github.com/robertostling/efmaral. 12 Parameters used: basic. weight matrices.13 4 Experiments Outline For each sentence in a target language corpus, we retrieve the aligned sentences in the"
Q16-1022,P11-1061,0,0.159967,"ning data. In our dependency graph projection, we normalize the weights per sentence. For future development, we note that corpus-level normalization might achieve the same balancing effect while still preserving possibly important language-specific signals regarding structural disambiguations. EBC and WTC constitute a (hopefully small) subset of the publicly available multilingual parallel corpora. The outdated EBC texts can be replaced by newer ones, and the EBC itself replaced or aug310 Related work POS tagging While projection annotation of POS labels goes back to Yarowsky’s seminal work, Das and Petrov (2011) recently renewed interest in this problem. Das and Petrov (2011) go beyond our approach to POS annotation by combining annotation projection and unsupervised learning techniques, but they restrict themselves to Indo-European languages and a coarser tagset. Li et al. (2012) introduce an approach that leverages potentially noisy, but sizeable POS tag dictionaries in the form of Wiktionaries for 9 resource-rich languages. Garrette et al. (2013) also consider the problem of learning POS taggers for truly low-resource languages, but suggest crowdsourcing such POS tag dictionaries. Finally, Agi´c e"
Q16-1022,P13-1057,0,0.0330031,"newer ones, and the EBC itself replaced or aug310 Related work POS tagging While projection annotation of POS labels goes back to Yarowsky’s seminal work, Das and Petrov (2011) recently renewed interest in this problem. Das and Petrov (2011) go beyond our approach to POS annotation by combining annotation projection and unsupervised learning techniques, but they restrict themselves to Indo-European languages and a coarser tagset. Li et al. (2012) introduce an approach that leverages potentially noisy, but sizeable POS tag dictionaries in the form of Wiktionaries for 9 resource-rich languages. Garrette et al. (2013) also consider the problem of learning POS taggers for truly low-resource languages, but suggest crowdsourcing such POS tag dictionaries. Finally, Agi´c et al. (2015) were the first to introduce the idea of learning models for more than a dozen truly low-resource languages in one go, and our contribution can be seen as a non-trivial extension of theirs. Parsing With the exception of Zeman and Resnik (2008), initial work on cross-lingual dependency parsing focused on annotation projection (Hwa et al., 2005; Spreyer et al., 2010). McDonald et al. (2011) and Søgaard (2011) simultaneously took up"
Q16-1022,P16-2091,1,0.868444,"Missing"
Q16-1022,D12-1127,0,0.0917574,"Missing"
Q16-1022,C14-1075,0,0.0462189,"dea of delexicalized transfer after Zeman and Resnik (2008), but more importantly, they also introduced the idea of multi-source cross-lingual transfer in the context of dependency parsing. McDonald et al. (2011) were the first to combine annotation projection and multi-source transfer, the approach taken in this paper. Annotation projection has been explored in the context of cross-lingual dependency parsing since Hwa et al. (2005). Notable approaches include the 18 http://www.ohchr.org/EN/UDHR/Pages/ SearchByLang.aspx 19 http://opus.lingfil.uu.se/ soft projection of reliable dependencies by Li et al. (2014), and the work of Ma and Xia (2014), who make use of the source-side distributions through a training objective function. Tiedemann and Agi´c (2016) provide a more detailed overview of model transfer and annotation projection, while introducing a competitive machine translation-based approach to synthesizing dependency treebanks. In their work, we note the IBM4 word alignments favor more closely related languages, and that building machine translation systems requires parallel data in quantities that far surpass EBC and WTC combined. The best results reported to date were presented by Rasooli"
Q16-1022,P14-1126,0,0.169202,"r Zeman and Resnik (2008), but more importantly, they also introduced the idea of multi-source cross-lingual transfer in the context of dependency parsing. McDonald et al. (2011) were the first to combine annotation projection and multi-source transfer, the approach taken in this paper. Annotation projection has been explored in the context of cross-lingual dependency parsing since Hwa et al. (2005). Notable approaches include the 18 http://www.ohchr.org/EN/UDHR/Pages/ SearchByLang.aspx 19 http://opus.lingfil.uu.se/ soft projection of reliable dependencies by Li et al. (2014), and the work of Ma and Xia (2014), who make use of the source-side distributions through a training objective function. Tiedemann and Agi´c (2016) provide a more detailed overview of model transfer and annotation projection, while introducing a competitive machine translation-based approach to synthesizing dependency treebanks. In their work, we note the IBM4 word alignments favor more closely related languages, and that building machine translation systems requires parallel data in quantities that far surpass EBC and WTC combined. The best results reported to date were presented by Rasooli and Collins (2015). They use the in"
Q16-1022,P13-2109,0,0.0643836,"Missing"
Q16-1022,P05-1012,0,0.0672856,"Missing"
Q16-1022,D11-1006,0,0.675369,"es in the arc projection, but we use unit votes in POS voting. The opposite yields the best IBM2 scores: binarizing the alignment scores in dependency projection, while weight-voting the POS tags. We also evaluated a number of different normalization techniques in projection, only to arrive at standardization and softmax as by far the best choices. Baselines and upper bounds We compare our systems to three competitive baselines, as well as three informed upper bounds or oracles. First, we list our baselines. D ELEX -MS: This is the multi-source direct delexicalized parser transfer baseline of McDonald et al. (2011).15 DCA-P ROJ: This is the direct correspondence assumption (DCA)-based approach to projection, i.e., the de facto standard for projecting dependencies. First introduced by Hwa et al. (2005), it was recently elucidated by Tiedemann (2014), whose implementation we follow here. In contrast to our approach, 15 Referred to as multi-dir in the original paper. DCA projects trees on a source-target sentence pair basis, relying on heuristics and spurious nodes or edges to maintain the tree structure. In the setup, we basically plug DCA into our projection-voting pipeline instead of our own method. R E"
Q16-1022,P13-2017,0,0.0731737,"Missing"
Q16-1022,P15-2034,0,0.0166339,"there are more candidates, we select one through POS ranking.8 Alignment We sentence- and word-align all language pairs in both our multi-parallel corpora. We use hunalign (Varga et al., 2005) to perform conservative sentence alignment.9 The selected sentence pairs then enter word alignment. Here, we use two different aligners. The first one is IBM2 fastalign by Dyer et al. (2013), where we adopt the setup of Agi´c et al. (2015) who observe a major advantage in using reverse-mode alignment for POS projection (4-5 accuracy points absolute).10 In addition, we use the IBM1 aligner efmaral11 by ¨ Ostling (2015). The intuition behind using IBM1 is that IBM2 introduces a bias toward more closely related languages, and we confirm this intuition through our experiments. We modify both aligners so that they output the alignment probability for each aligned token pair. Tagging and parsing The source-sides of the two multi-parallel corpora, EBC and WTC, are POStagged by taggers trained on the respective source languages, using TnT (Brants, 2000). We parse the corpora using TurboParser (Martins et al., 2013). The parser is used in simple arc-factored mode with pruning.12 We alter it to output per-sentence a"
Q16-1022,petrov-etal-2012-universal,0,0.0990105,"S tagging Below, we present results with POS taggers based on annotation projection with both IBM1 and IBM2; cf. Table 3. We train TnT with default settings on the projected annotations. Note that we use the resulting POS taggers in our dependency parsing experiments in order not to have our parsers assume the existence of POS-annotated corpora. For a more extensive assessment, we refer to the work by Agi´c et al. (2015) who report baseline and upper bounds. In contrast to their work, we consider two different alignment models and use the UD POS tagset (17 tags), in contrast to the 12 tags of Petrov et al. (2012). This makes our POS tagging problem slightly more challenging, but our parsing models potentially benefit from the extended tagset.14 Dependency parsing We use arc-factored TurboParser for all parsing models, applying the same setup as in preprocessing. There are three sets of models: our systems, baselines, and upper bounds. 13 Our fork of TurboParser is available from https:// github.com/andersjo/TurboParser. 14 For example, the AUX vs. VERB distinction from UD POS does not exist the tagset of Petrov et al. (2012), and neither does NOUN vs. PROPN (proper noun). 307 Our systems are trained o"
Q16-1022,D15-1039,0,0.358424,"(73m), Hausa (50m), and Kurdish (30m). Cross-lingual transfer learning—or simply cross-lingual learning—refers to work on using annotated resources in other (source) languages to induce models for such low-resource (target) languages. Even simple cross-lingual learning techniques outperform unsupervised grammar induction by a large margin. Most work in cross-lingual learning, however, makes assumptions about the availability of linguistic resources that do not hold for the majority of low-resource languages. The best cross-lingual dependency parsing results reported to date were presented by Rasooli and Collins (2015). They use the intersection of languages covered in the Google dependency treebanks project and those contained in the Europarl corpus. Consequently, they only consider closely related Indo-European languages for which high-quality tokenization can be obtained with simple heuristics. In other words, we argue that recent approaches to cross-lingual POS tagging and dependency parsing are biased toward Indo-European languages, in particular the Germanic and Romance families. The bias is not hard to explain: treebanks, as well as large volumes of parallel data, are readily available for many Germa"
Q16-1022,P15-2040,0,0.143668,"Missing"
Q16-1022,N06-2033,0,0.0734653,"entation we follow here. In contrast to our approach, 15 Referred to as multi-dir in the original paper. DCA projects trees on a source-target sentence pair basis, relying on heuristics and spurious nodes or edges to maintain the tree structure. In the setup, we basically plug DCA into our projection-voting pipeline instead of our own method. R EPARSE: For this baseline, we parse a target sentence using multiple single-source delexicalized parsers. Then, we collect the output trees in a graph, unit-voting the individual edge weights, and finally using DMST to compute the best dependency tree (Sagae and Lavie, 2006). Now, we explain the three upper bounds: D ELEX -SB: This result is using the best singlesource delexicalized system for a given target language following McDonald et al. (2013). We parse a target with multiple single-source delexicalized parsers, and select the best-performing one. S ELF -T RAIN: For this result we parse the targetlanguage EBC and WTC data, train parsers on the output predictions, and evaluate the resulting parsers on the evaluation data. Note this result is available only for the source languages. Also, note that while we refer to this as self-training, we do not concatenat"
Q16-1022,spreyer-etal-2010-training,0,0.0207293,"naries in the form of Wiktionaries for 9 resource-rich languages. Garrette et al. (2013) also consider the problem of learning POS taggers for truly low-resource languages, but suggest crowdsourcing such POS tag dictionaries. Finally, Agi´c et al. (2015) were the first to introduce the idea of learning models for more than a dozen truly low-resource languages in one go, and our contribution can be seen as a non-trivial extension of theirs. Parsing With the exception of Zeman and Resnik (2008), initial work on cross-lingual dependency parsing focused on annotation projection (Hwa et al., 2005; Spreyer et al., 2010). McDonald et al. (2011) and Søgaard (2011) simultaneously took up the idea of delexicalized transfer after Zeman and Resnik (2008), but more importantly, they also introduced the idea of multi-source cross-lingual transfer in the context of dependency parsing. McDonald et al. (2011) were the first to combine annotation projection and multi-source transfer, the approach taken in this paper. Annotation projection has been explored in the context of cross-lingual dependency parsing since Hwa et al. (2005). Notable approaches include the 18 http://www.ohchr.org/EN/UDHR/Pages/ SearchByLang.aspx 19"
Q16-1022,P11-2120,1,0.889189,"rich languages. Garrette et al. (2013) also consider the problem of learning POS taggers for truly low-resource languages, but suggest crowdsourcing such POS tag dictionaries. Finally, Agi´c et al. (2015) were the first to introduce the idea of learning models for more than a dozen truly low-resource languages in one go, and our contribution can be seen as a non-trivial extension of theirs. Parsing With the exception of Zeman and Resnik (2008), initial work on cross-lingual dependency parsing focused on annotation projection (Hwa et al., 2005; Spreyer et al., 2010). McDonald et al. (2011) and Søgaard (2011) simultaneously took up the idea of delexicalized transfer after Zeman and Resnik (2008), but more importantly, they also introduced the idea of multi-source cross-lingual transfer in the context of dependency parsing. McDonald et al. (2011) were the first to combine annotation projection and multi-source transfer, the approach taken in this paper. Annotation projection has been explored in the context of cross-lingual dependency parsing since Hwa et al. (2005). Notable approaches include the 18 http://www.ohchr.org/EN/UDHR/Pages/ SearchByLang.aspx 19 http://opus.lingfil.uu.se/ soft projection"
Q16-1022,W14-1614,1,0.84878,"Missing"
Q16-1022,tiedemann-2012-parallel,0,0.0365742,"Missing"
Q16-1022,C14-1175,0,0.211283,"ncy such that (ut , vt ) becomes a dependency edge in the target sentence, making the a dependent of word. Obviously, dependency annotation projection is more challenging than projecting POS, as there is a structural constraint: the projected edges must form a dependency tree on the target side. Hwa et al. (2005) were the first to consider this problem, applying heuristics to ensure well-formed trees on the target side. The heuristics were not perfect, as they have been shown to result in excessive non-projectivity and the introduction of spurious relations and tokens (Tiedemann et al., 2014; Tiedemann, 2014). These design choices all lead to di1 https://bitbucket.org/lowlands/release Figure 1: An outline of dependency annotation projection, voting, and decoding in our method, using two sources i (German) and j (Croatian) and a target t (English). Part 1 represents the multi-parallel corpus preprocessing, while parts 2 and 3 relate to our projection method. The graphs are represented as adjacency matrices with column indices encoding dependency heads. We highlight how the weight of target edge (ut = was, vt = beginning) is computed from the two contributing sources. minished parsing quality. We in"
Q16-1022,H01-1035,0,0.430375,"ts weight matrices from multiple sources, rather than dependency trees or individual dependencies from a single source. (iii) We show that our approach performs significantly better than commonly used heuristics for annotation projection, as well as than delexicalized transfer baselines. Moreover, in comparison to these systems, our approach performs particularly well on truly low-resource non-Indo-European languages. 302 All code and data are made freely available for general use.1 2 Weighted annotation projection Motivation Our approach is based on the general idea of annotation projection (Yarowsky et al., 2001) using parallel sentences. The goal is to augment an unannotated target sentence with syntactic annotations projected from one or more source sentences through word alignments. The principle is illustrated in Figure 1, where the source languages are German and Croatian, and the target is English. The simplest case is projecting POS labels, which are observed in the source sentences but unknown in the target language. In order to induce the grammatical category of the target word beginning, we project POS from the aligned words Anfang and poˇcetku, both of which are correctly annotated as N OUN"
Q16-1022,I08-3008,0,0.081872,"ges and a coarser tagset. Li et al. (2012) introduce an approach that leverages potentially noisy, but sizeable POS tag dictionaries in the form of Wiktionaries for 9 resource-rich languages. Garrette et al. (2013) also consider the problem of learning POS taggers for truly low-resource languages, but suggest crowdsourcing such POS tag dictionaries. Finally, Agi´c et al. (2015) were the first to introduce the idea of learning models for more than a dozen truly low-resource languages in one go, and our contribution can be seen as a non-trivial extension of theirs. Parsing With the exception of Zeman and Resnik (2008), initial work on cross-lingual dependency parsing focused on annotation projection (Hwa et al., 2005; Spreyer et al., 2010). McDonald et al. (2011) and Søgaard (2011) simultaneously took up the idea of delexicalized transfer after Zeman and Resnik (2008), but more importantly, they also introduced the idea of multi-source cross-lingual transfer in the context of dependency parsing. McDonald et al. (2011) were the first to combine annotation projection and multi-source transfer, the approach taken in this paper. Annotation projection has been explored in the context of cross-lingual dependency"
Q16-1022,P15-2044,1,\N,Missing
Q16-1022,N13-1073,0,\N,Missing
S14-2081,J93-2004,0,0.0456754,"tered in both the closed track and the open track of the challenge, recording a peak average labeled F1 score of 78.60. 1 Introduction In the semantic dependency parsing (SDP) task of SemEval 2014, the meaning of a sentence is represented in terms of binary head-argument relations between the lexical units – bi-lexical dependencies (Oepen et al., 2014). Since words can be semantic dependents of multiple other words, this framework results in graph representations of sentence meaning. For the SDP task, three such annotation layers are provided on top of the WSJ text of the Penn Treebank (PTB) (Marcus et al., 1993): – DM: the reduction of DeepBank HPSG annotation (Flickinger et al., 2012) into bi-lexical dependencies following (Oepen and Lønning, 2006; Ivanova et al., 2012), – PAS: the predicate-argument structures derived from the training set of the Enju HPSG parser (Miyao et al., 2004) and – PCEDT: a subset of the tectogrammatical annotation layer from the English side of the Prague Czech-English Dependency Treebank (Cinkov´a et al., 2009). 2 Data and Systems We present the basic statistics for the SDP training sets in Table 1. The graphs contain no cycles, i.e., all SDP meaning representations are d"
S14-2081,D12-1133,0,0.0291587,"on the training data, use the converted training sets to train syntactic dependency parsers (Bohnet, 2010) and utilize the parsing models on the development and test data. The parsing outputs are converted back to graphs by simply re-flipping all the edges denoted as flipped. 2.3 PAS 2.4 Parsing and Top Node Detection The same syntactic parser and top node detector are used in both LOCAL and DFS. Both systems ran in the closed SDP track, with no additional features for learning, and in the open track, where they used the SDP companion data, i.e., the outputs of a syntactic dependency parser (Bohnet and Nivre, 2012) and phrase-based parser (Petrov et al., 2006) as additional features. Our choice of parser was based on the high non-projectivity of the resulting trees, while parsers of (Bohnet and Nivre, 2012; Bohnet et al., 2013) could also be used, among others. We use the parser out of the box, i.e., without any parameter tuning or additional features other than what was previously listed for the open track. Top node detection is implemented separately, by training a sequence labeling model (Lafferty et al., 2001; Kudo, 2005) on tokens and part-ofspeech tags from the training sets. Its accuracy is given"
S14-2081,Q13-1034,0,0.047291,"Missing"
S14-2081,C10-1011,0,0.0664041,"roduce the coordinated nodes. We conclude that edges in reentrancies, for which the source nodes have zero indegree, could be flipped by changing places of their source and target nodes and encoding the switch in the edge labels by appending the suffix flipped to the existing labels. This is the basis for our first system: LOCAL. In it, we locally flip all edges in reentrancies for which the source node has zero indegree and run the BASELINE conversion on the resulting graphs. We apply this conversion on the training data, use the converted training sets to train syntactic dependency parsers (Bohnet, 2010) and utilize the parsing models on the development and test data. The parsing outputs are converted back to graphs by simply re-flipping all the edges denoted as flipped. 2.3 PAS 2.4 Parsing and Top Node Detection The same syntactic parser and top node detector are used in both LOCAL and DFS. Both systems ran in the closed SDP track, with no additional features for learning, and in the open track, where they used the SDP companion data, i.e., the outputs of a syntactic dependency parser (Bohnet and Nivre, 2012) and phrase-based parser (Petrov et al., 2006) as additional features. Our choice of"
S14-2081,P13-1091,0,0.030687,"nce, with word forms as nodes and labeled dependency relations as edges pointing from functors to arguments. The SDP-annotated PTB text is split into training (sections 00–19), development (sec. 20) and testing sets (sec. 21). This in turn makes the SDP parsing task a problem of datadriven graph parsing, in which systems are to be trained for producing dependency graph representations of sentences respecting the three underlying schemes. While a number of theoretical and preliminary contributions to data-driven graph parsing exist (Sagae and Tsujii, 2008; Das et al., 2010; Jones et al., 2013; Chiang et al., 2013; Henderson et al., 2013), our goal here is to investigate the simplest approach that can achieve competitive performance. Our starting point is the observation that the SDP graphs are relatively tree-like. On it, we build a system for data-driven graph parsing by (1) transforming dependency graphs into dependency trees in preprocessing, (2) training and using syntactic dependency parsers over these trees and (3) transforming their output back into graphs in postprocessing. This way, we inherit the accuracy and speed of syntactic dependency parsers. The secondary benefit is insight into the st"
S14-2081,S14-2008,0,0.19623,"Missing"
S14-2081,P06-1055,0,0.0168421,"sets to train syntactic dependency parsers (Bohnet, 2010) and utilize the parsing models on the development and test data. The parsing outputs are converted back to graphs by simply re-flipping all the edges denoted as flipped. 2.3 PAS 2.4 Parsing and Top Node Detection The same syntactic parser and top node detector are used in both LOCAL and DFS. Both systems ran in the closed SDP track, with no additional features for learning, and in the open track, where they used the SDP companion data, i.e., the outputs of a syntactic dependency parser (Bohnet and Nivre, 2012) and phrase-based parser (Petrov et al., 2006) as additional features. Our choice of parser was based on the high non-projectivity of the resulting trees, while parsers of (Bohnet and Nivre, 2012; Bohnet et al., 2013) could also be used, among others. We use the parser out of the box, i.e., without any parameter tuning or additional features other than what was previously listed for the open track. Top node detection is implemented separately, by training a sequence labeling model (Lafferty et al., 2001; Kudo, 2005) on tokens and part-ofspeech tags from the training sets. Its accuracy is given in Table 3. We use only the tokens and parts"
S14-2081,N10-1138,0,0.0151698,"koller@ling.uni-potsdam.de Abstract tence, with word forms as nodes and labeled dependency relations as edges pointing from functors to arguments. The SDP-annotated PTB text is split into training (sections 00–19), development (sec. 20) and testing sets (sec. 21). This in turn makes the SDP parsing task a problem of datadriven graph parsing, in which systems are to be trained for producing dependency graph representations of sentences respecting the three underlying schemes. While a number of theoretical and preliminary contributions to data-driven graph parsing exist (Sagae and Tsujii, 2008; Das et al., 2010; Jones et al., 2013; Chiang et al., 2013; Henderson et al., 2013), our goal here is to investigate the simplest approach that can achieve competitive performance. Our starting point is the observation that the SDP graphs are relatively tree-like. On it, we build a system for data-driven graph parsing by (1) transforming dependency graphs into dependency trees in preprocessing, (2) training and using syntactic dependency parsers over these trees and (3) transforming their output back into graphs in postprocessing. This way, we inherit the accuracy and speed of syntactic dependency parsers. The"
S14-2081,C08-1095,0,0.203579,"am zagic@uni-potsdam.de koller@ling.uni-potsdam.de Abstract tence, with word forms as nodes and labeled dependency relations as edges pointing from functors to arguments. The SDP-annotated PTB text is split into training (sections 00–19), development (sec. 20) and testing sets (sec. 21). This in turn makes the SDP parsing task a problem of datadriven graph parsing, in which systems are to be trained for producing dependency graph representations of sentences respecting the three underlying schemes. While a number of theoretical and preliminary contributions to data-driven graph parsing exist (Sagae and Tsujii, 2008; Das et al., 2010; Jones et al., 2013; Chiang et al., 2013; Henderson et al., 2013), our goal here is to investigate the simplest approach that can achieve competitive performance. Our starting point is the observation that the SDP graphs are relatively tree-like. On it, we build a system for data-driven graph parsing by (1) transforming dependency graphs into dependency trees in preprocessing, (2) training and using syntactic dependency parsers over these trees and (3) transforming their output back into graphs in postprocessing. This way, we inherit the accuracy and speed of syntactic depen"
S14-2081,J13-4006,0,0.046933,"as nodes and labeled dependency relations as edges pointing from functors to arguments. The SDP-annotated PTB text is split into training (sections 00–19), development (sec. 20) and testing sets (sec. 21). This in turn makes the SDP parsing task a problem of datadriven graph parsing, in which systems are to be trained for producing dependency graph representations of sentences respecting the three underlying schemes. While a number of theoretical and preliminary contributions to data-driven graph parsing exist (Sagae and Tsujii, 2008; Das et al., 2010; Jones et al., 2013; Chiang et al., 2013; Henderson et al., 2013), our goal here is to investigate the simplest approach that can achieve competitive performance. Our starting point is the observation that the SDP graphs are relatively tree-like. On it, we build a system for data-driven graph parsing by (1) transforming dependency graphs into dependency trees in preprocessing, (2) training and using syntactic dependency parsers over these trees and (3) transforming their output back into graphs in postprocessing. This way, we inherit the accuracy and speed of syntactic dependency parsers. The secondary benefit is insight into the structure of the semantic r"
S14-2081,W12-3602,0,0.0838369,"parsing (SDP) task of SemEval 2014, the meaning of a sentence is represented in terms of binary head-argument relations between the lexical units – bi-lexical dependencies (Oepen et al., 2014). Since words can be semantic dependents of multiple other words, this framework results in graph representations of sentence meaning. For the SDP task, three such annotation layers are provided on top of the WSJ text of the Penn Treebank (PTB) (Marcus et al., 1993): – DM: the reduction of DeepBank HPSG annotation (Flickinger et al., 2012) into bi-lexical dependencies following (Oepen and Lønning, 2006; Ivanova et al., 2012), – PAS: the predicate-argument structures derived from the training set of the Enju HPSG parser (Miyao et al., 2004) and – PCEDT: a subset of the tectogrammatical annotation layer from the English side of the Prague Czech-English Dependency Treebank (Cinkov´a et al., 2009). 2 Data and Systems We present the basic statistics for the SDP training sets in Table 1. The graphs contain no cycles, i.e., all SDP meaning representations are directed acyclic graphs (DAGs). DM and PAS are automatically derived from HPSG annotations, while PCEDT is based on manual tectogrammatical annotation. This is ref"
S14-2081,W13-1810,0,0.017541,"tsdam.de Abstract tence, with word forms as nodes and labeled dependency relations as edges pointing from functors to arguments. The SDP-annotated PTB text is split into training (sections 00–19), development (sec. 20) and testing sets (sec. 21). This in turn makes the SDP parsing task a problem of datadriven graph parsing, in which systems are to be trained for producing dependency graph representations of sentences respecting the three underlying schemes. While a number of theoretical and preliminary contributions to data-driven graph parsing exist (Sagae and Tsujii, 2008; Das et al., 2010; Jones et al., 2013; Chiang et al., 2013; Henderson et al., 2013), our goal here is to investigate the simplest approach that can achieve competitive performance. Our starting point is the observation that the SDP graphs are relatively tree-like. On it, we build a system for data-driven graph parsing by (1) transforming dependency graphs into dependency trees in preprocessing, (2) training and using syntactic dependency parsers over these trees and (3) transforming their output back into graphs in postprocessing. This way, we inherit the accuracy and speed of syntactic dependency parsers. The secondary benefit i"
S14-2081,oepen-lonning-2006-discriminant,0,\N,Missing
seljan-etal-2010-corpus,J93-1004,0,\N,Missing
seljan-etal-2010-corpus,steinberger-etal-2006-jrc,0,\N,Missing
seljan-etal-2010-corpus,P98-1117,0,\N,Missing
seljan-etal-2010-corpus,C98-1113,0,\N,Missing
seljan-etal-2010-corpus,tadic-2000-building,1,\N,Missing
seljan-etal-2010-corpus,ceausu-etal-2006-acquis,0,\N,Missing
vuckovic-etal-2010-improving,tadic-2002-building,1,\N,Missing
vuckovic-etal-2010-improving,vuckovic-etal-2008-rule,1,\N,Missing
vuckovic-etal-2010-improving,W00-0732,0,\N,Missing
vuckovic-etal-2010-improving,W06-2920,0,\N,Missing
vuckovic-etal-2010-improving,W03-2906,1,\N,Missing
vuckovic-etal-2010-improving,A00-1031,0,\N,Missing
vuckovic-etal-2010-improving,tadic-2000-building,1,\N,Missing
vuckovic-etal-2010-improving,D07-1096,0,\N,Missing
vuckovic-etal-2010-improving,agic-tadic-2006-evaluating,1,\N,Missing
vuckovic-etal-2010-improving,erjavec-2004-multext,0,\N,Missing
vuckovic-etal-2010-improving,L00-1000,0,\N,Missing
W13-2408,A00-1031,0,0.456422,"Missing"
W13-2408,erjavec-2004-multext,0,0.0523663,"Missing"
W13-2408,2012.freeopmt-1.6,0,0.0181909,"ion set.test lemmatization-capable decision-tree-based TreeTagger12 (Schmid, 1995), support vector machine tagger SVMTool13 (Gim´enez and M`arquez, 2004) and CST’s14 data-driven rule-based lemmatizer (Ingason et al., 2008). Keeping in mind the previously mentioned state-of-the-art scores on Serbian 1984 corpus and statistical lemmatization capability, we also tested BTagger (Gesmundo and Samardˇzi´c, 2012a; Gesmundo and Samardˇzi´c, 2012b). Since some lemmatizers and taggers are capable of using an external morphological lexicon, we used a MTE v5r1 version of Apertium’s lexicon of Croatian15 (Peradin and Tyers, 2012) where applicable.16 All tools are well-documented and successfully applied across languages, as indicated in related work. wiki.test Model hr sr hr sr CST + lex 97.78 97.04 95.95 95.52 96.59 96.38 96.30 96.61 Table 5: Overall lemmatization accuracy with and without the inflectional lexicon zation and tagging accuracy as well as processing speed in both training and testing, the top performing tools are CST lemmatizer and HunPos tagger. Thus, we chose these two for further investigation in the following batches of experiments. It should be noted that, even though its performance is comparable"
W13-2408,E12-1050,0,0.107609,"Missing"
W13-2408,P11-2009,0,0.0681194,"Missing"
W13-2408,P12-2072,0,0.0888677,"Missing"
W13-2408,W07-1709,0,0.0136785,"Missing"
W13-2408,gesmundo-samardzic-2012-lemmatising,0,0.149047,"Missing"
W13-2408,W03-2906,0,0.447022,"Missing"
W13-2408,gimenez-marquez-2004-svmtool,0,0.0700347,"Missing"
W13-2408,P07-2053,0,0.192258,"Missing"
W13-4903,C12-2001,0,0.0246753,"Missing"
W13-4903,W13-2408,1,0.697207,"Missing"
W13-4903,D12-1133,0,0.190783,"e- and domain-aware test samples. By these we also target at establishing the need for domain adaptation for parsing. 5. No research was done in investigating the effects of preprocessing and linguistic feature selection to dependency parsing for these languages. As these are highly inflectional, having very large morphosyntactic tagsets, we seek to inspect the impact of preprocessing choices on their dependency parsing. There is ample research on the effect preprocessing has on dependency parsing (Goldberg and Elhadad, 2009; Mohamed, 2011) and on joint morphological and syntactic processing (Bohnet and Nivre, 2012), but none of it included any of the South Slavic languages. 3.2 Workflow We define three batches of experiments to meet the research objectives: 1. to select the best Croatian dependency formalism with respect to its overall parsing accuracy on Croatian and Serbian – with an emphasis on the most important syntactic categories that match across formalisms – and incidentally to establish the need for annotation projection, 2. to inspect the impact of state-of-the-art automatic preprocessing on dependency parsing of both languages and 3. to establish the importance of specific Croatian and Serbi"
W13-4903,W06-2920,0,0.922022,"orphosyntax, see the respective contemporary grammars (Sili´c and Pranjkovi´c, 2005; Stanojˇci´c and Popovi´c, 2008). layer does exist (Tadi´c et al., 2012; Vitas et al., 2012), approaches to full syntactic analysis of the two languages were up to this point very sparse and very recent (Agi´c and Merkler, 2013). As linguistic tradition supports dependency-based syntactic formalisms for the two languages (B¨ohmov´a et al., 2003; Tadi´c, 2007), it should be noted that they have not participated in the previous collaborative research efforts in dependency parsing, such as the CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007). Furthermore, regardless of the specific research topic, the communities dealing with natural language processing of Croatian, Serbian and other closely related languages from their group are still to reach the common level of awareness with respect to public availability of their research. Contributions to availability of Croatian and Serbian resources have once again been very few and recent (Tadi´c and Varadi, 2012), especially for free culture licensing. Through the line of research we propose here,2 we seek to provide state-of-the-art in dependency parsing for both C"
W13-4903,W09-3819,0,0.0198041,"e to randomization. We seek to partially account for these effects by designing a set of language- and domain-aware test samples. By these we also target at establishing the need for domain adaptation for parsing. 5. No research was done in investigating the effects of preprocessing and linguistic feature selection to dependency parsing for these languages. As these are highly inflectional, having very large morphosyntactic tagsets, we seek to inspect the impact of preprocessing choices on their dependency parsing. There is ample research on the effect preprocessing has on dependency parsing (Goldberg and Elhadad, 2009; Mohamed, 2011) and on joint morphological and syntactic processing (Bohnet and Nivre, 2012), but none of it included any of the South Slavic languages. 3.2 Workflow We define three batches of experiments to meet the research objectives: 1. to select the best Croatian dependency formalism with respect to its overall parsing accuracy on Croatian and Serbian – with an emphasis on the most important syntactic categories that match across formalisms – and incidentally to establish the need for annotation projection, 2. to inspect the impact of state-of-the-art automatic preprocessing on dependenc"
W13-4903,P07-1050,0,0.019343,"oatian and Serbian. Croatian Dependency Treebank (HOBS) project was initiated by (Tadi´c, 2007). However, its sufficiency in size increase, followed by the first experiments with dependency parsing of Croatian, did not appear soon enough to be included in the CoNLL 3 4 http://creativecommons.org/licenses/by-sa/3.0/ http://nlp.ffzg.hr/resources/models/tagging/ shared tasks and the overview of (Tadi´c et al., 2012). Preliminary experiments in transition-based (Berovi´c et al., 2012) and graph-based parsing have been augmented by a hybrid approach which included integrating a graph-based parser (Hall, 2007) and a valency lexicon (Agi´c, 2012). Due to uncovered partial inadequacies of the HOBS formalism at describing certain syntactic properties of Croatian, a new line of research was initiated, aiming at creating a more simplistic dependency-based formalism for data-driven parsing of Croatian (Agi´c and Merkler, 2013). It provided a new freely available dependency treebank, the S ETIMES .H R Treebank, and derived state-of-the-art dependency parsing models.5 On the downside, S ETIMES .H R is a prototype with currently less than 2 500 sentences and a documented need for addressing certain annotati"
W13-4903,P10-1001,0,0.080817,"Missing"
W13-4903,P13-2109,0,0.031069,"Missing"
W13-4903,W06-2932,0,0.0152579,"h the need for annotation projection, 2. to inspect the impact of state-of-the-art automatic preprocessing on dependency parsing of both languages and 3. to establish the importance of specific Croatian and Serbian morphosyntactic features of the most frequent parts of speech in modeling syntactic fenomena for dependency parsing. In the first batch, we use HOBS in two instances and S ETIMES .H R to create parsing models and test them on Croatian and Serbian test samples. Drawing from previous research, we use a standard nonprojective graph-based MSTParser generator with second-order features (McDonald et al., 2006), as this setting favors Croatian (Agi´c, 2012) and related languages such as Czech and Slovene (Buchholz and Marsi, 2006). We are aware of the existence of novel dependency parsers that implement approaches to handling non-local dependencies and outperform MSTParser on a set of languages, such as (Bohnet and Nivre, 2012). They are not included here due to temporal constraints and the fact that we were provided with prebuilt MSTParser models for the HOBS instances and needed to ensure their comparability with S ETIMES .H R. As we mainly deal with the concept of resource sharing between closely"
W13-4903,W11-0302,0,0.0184793,"to partially account for these effects by designing a set of language- and domain-aware test samples. By these we also target at establishing the need for domain adaptation for parsing. 5. No research was done in investigating the effects of preprocessing and linguistic feature selection to dependency parsing for these languages. As these are highly inflectional, having very large morphosyntactic tagsets, we seek to inspect the impact of preprocessing choices on their dependency parsing. There is ample research on the effect preprocessing has on dependency parsing (Goldberg and Elhadad, 2009; Mohamed, 2011) and on joint morphological and syntactic processing (Bohnet and Nivre, 2012), but none of it included any of the South Slavic languages. 3.2 Workflow We define three batches of experiments to meet the research objectives: 1. to select the best Croatian dependency formalism with respect to its overall parsing accuracy on Croatian and Serbian – with an emphasis on the most important syntactic categories that match across formalisms – and incidentally to establish the need for annotation projection, 2. to inspect the impact of state-of-the-art automatic preprocessing on dependency parsing of bot"
W13-4903,W03-2911,0,0.0254311,"Missing"
W13-4903,C12-3054,0,0.084651,"Missing"
W13-4903,vuckovic-etal-2008-rule,0,0.0569244,"Missing"
W13-4903,H01-1035,0,0.0172314,"ith highly inflectional languages, we also investigate the influence of morphological preprocessing and morphosyntactic feature selection on parsing perfor2 This work was partly financed by the EU FP7 STREP project XLike (FP7-288342). 22 Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 22–33, c Seattle, Washington, USA, 18 October 2013. 2013 Association for Computational Linguistics mance. We aim to use this first inquiry as a decision point regarding further advancements in resource interchangeability in terms of, e.g., annotation projection (Yarowsky et al., 2001) and domain adaptation (Søgaard, 2013). Availability is highly emphasized, as we provide our resources and models to the public under the CC-BY-SA-3.0 license.3 We stress the essential role of free culture licensing in enabling and maturing NLP for under-resourced languages. In the following section, we give an overview of related work in computational processing of Croatian and Serbian morphology and syntax. Further, we define the experiment objectives and describe the resources and experiment workflow. We elaborate on the obtained results and conclude by sketching possible future research pl"
W13-4903,D12-1030,0,0.045773,"Missing"
W13-4903,berovic-etal-2012-croatian,1,\N,Missing
W14-1614,de-marneffe-etal-2006-generating,0,0.0246402,"Missing"
W14-1614,D12-1001,0,0.257649,"can assume that SMT will produce output that is much closer to the input than manual translations in parallel texts usually are. Even if this may seem like a short-coming in general, in the case of annotation projection it should rather be an advantage, because it makes it more straightforward and less error-prone to transfer annotation from source to target. Furthermore, the alignment between words and phrases is inherently provided as an output of all common SMT models. Hence, no additional procedures have to be performed on top of the translated corpus. Recent research (Zhao et al., 2009; Durrett et al., 2012) has attempted to address synthetic data creation for syntactic parsing via bilingual lexica. We seek to build on this work by utilizing more advanced translation techniques. Further in the paper, we first describe the tools and resources used in our experiments (§2). We elaborate on our approach to translating treebanks (§3) and projecting syntactic annotations (§4) for a new language. Finally, we provide empirical evaluation of the suggested approach (§5) and observe a substantial increase in parsing accuracy over the delexicalized parsing baselines. 2 Resources and Tools In our experiments,"
W14-1614,D13-1205,0,0.0415106,"Missing"
W14-1614,D07-1097,1,0.448738,"Missing"
W14-1614,P13-2121,0,0.0129603,"Missing"
W14-1614,2005.mtsummit-papers.11,0,0.0215141,"anslation, we select the popular Moses toolbox (Koehn et al., 2007) and the phrasebased translation paradigm as our basic framework. Phrase-based SMT has the advantage of being straightforward and efficient in training and decoding, while maintaining robustness and reliability for many language pairs. More details about the setup and the translation procedures are given in Section 3 below. The most essential ingredient for translation performance is the parallel corpus used for training the translation models. For our experiments we use the freely available and widely used Europarl corpus v7 (Koehn, 2005).4 It is commonly used for training SMT models and includes parallel data for all languages represented in the Universal Treebank except Korean, which we will, therefore, leave out in our experiments. For tuning we apply the newstest 2012 data provided by the annual workshop on statistical machine translation.5 For language modeling, we use a combination of 2 http://www.maltparser.org/ http://nil.fdi.ucm.es/maltoptimizer/ 4 http://www.statmt.org/europarl/ 5 http://www.statmt.org/wmt14 3 DE DE EN ES FR SV 94 M 94 M 103 M 96 M 105 M 104 M 81 M 89 M 89 M 91 M SV 2.0 M 1.9 M 1.9 M 1.8 M 2.0 M 2.0"
W14-1614,D11-1006,0,0.540929,"Missing"
W14-1614,P80-1024,0,0.824454,"Missing"
W14-1614,P12-1066,0,0.342974,"ired. In addition, training can be performed on gold standard annotation. However, model transfer assumes a common fea130 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 130–140, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics ture representation across languages (McDonald et al., 2013), which can be a strong bottleneck. Several extensions have been proposed to make the approach more robust. First of all, multiple source languages can be involved to increase the statistical basis for learning (McDonald et al., 2011; Naseem et al., 2012), a strategy that can also be used in the case of annotation projection. Cross-lingual word clusters can be created to obtain additional universal features (T¨ackstr¨om et al., 2012). Techniques for target language adaptation can be used to improve model transfer with multiple sources (T¨ackstr¨om et al., 2013b). 1.2 The Translation Approach In this paper, we propose a third strategy, based on automatically translating training data to a new language in order to create annotated resources directly from the original source. Recent advances in statistical machine translation (SMT) combined with"
W14-1614,nivre-etal-2006-maltparser,1,0.855992,"Missing"
W14-1614,W06-2933,1,0.560504,"Missing"
W14-1614,J03-1002,0,0.00756484,"tom row = number of sentences in monolingual corpora. Europarl and News data provided from the same source. The statistics of the corpora are given in Table 1. 3 Translating Treebanks The main contribution of this paper is the empirical study of automatic treebank translation for parser transfer. We compare three different translation approaches in order to investigate the influence of several parameters. All of them are based on automatic word alignment and subsequent extraction of translation equivalents as common in phrase-based SMT. In particular, word alignment is performed using GIZA++ (Och and Ney, 2003) and IBM model 4 as the final model for creating the Viterbi word alignments for all parallel corpora used in our experiments. For the extraction of translation tables, we use the Moses toolkit with its standard settings to extract phrase tables with a maximum of seven tokens per phrase from a symmetrized word alignment. Symmetrization is done using the grow-diagfinal-and heuristics (Koehn et al., 2003). We tune phrase-based SMT models using minimum error rate training (Och, 2003) and the development data for each language pair. The language model is a standard 5-gram model estimated from the"
W14-1614,N03-1017,0,0.016743,"rs. All of them are based on automatic word alignment and subsequent extraction of translation equivalents as common in phrase-based SMT. In particular, word alignment is performed using GIZA++ (Och and Ney, 2003) and IBM model 4 as the final model for creating the Viterbi word alignments for all parallel corpora used in our experiments. For the extraction of translation tables, we use the Moses toolkit with its standard settings to extract phrase tables with a maximum of seven tokens per phrase from a symmetrized word alignment. Symmetrization is done using the grow-diagfinal-and heuristics (Koehn et al., 2003). We tune phrase-based SMT models using minimum error rate training (Och, 2003) and the development data for each language pair. The language model is a standard 5-gram model estimated from the monolingual data using modified Kneser-Ney smoothing without pruning (applying KenLM tools (Heafield et al., 2013)). Our first translation approach is based on a very simple word-by-word translation model. For this, we select the most reliable translations of single words from the phrase translation tables extracted from the parallel corpora as described above. We restrict the model to tokens with alpha"
W14-1614,P03-1021,0,0.0115556,"lation equivalents as common in phrase-based SMT. In particular, word alignment is performed using GIZA++ (Och and Ney, 2003) and IBM model 4 as the final model for creating the Viterbi word alignments for all parallel corpora used in our experiments. For the extraction of translation tables, we use the Moses toolkit with its standard settings to extract phrase tables with a maximum of seven tokens per phrase from a symmetrized word alignment. Symmetrization is done using the grow-diagfinal-and heuristics (Koehn et al., 2003). We tune phrase-based SMT models using minimum error rate training (Och, 2003) and the development data for each language pair. The language model is a standard 5-gram model estimated from the monolingual data using modified Kneser-Ney smoothing without pruning (applying KenLM tools (Heafield et al., 2013)). Our first translation approach is based on a very simple word-by-word translation model. For this, we select the most reliable translations of single words from the phrase translation tables extracted from the parallel corpora as described above. We restrict the model to tokens with alphabetic characters only using pre-defined Unicode character 132 sets. The selecti"
W14-1614,P07-2045,0,0.00757185,"Missing"
W14-1614,petrov-etal-2012-universal,0,0.209401,"Missing"
W14-1614,J96-1001,0,0.0271066,"this, we select the most reliable translations of single words from the phrase translation tables extracted from the parallel corpora as described above. We restrict the model to tokens with alphabetic characters only using pre-defined Unicode character 132 sets. The selection of translation alternatives is based on the Dice coefficient, which combines the two essential conditional translation probabilities given in the phrase table. The Dice coefficient is in fact the harmonic mean of these two probabilities and has successfully been used for the extraction of translation equivalents before (Smadja et al., 1996): 2 p(s, t) Dice(s, t) = =2 p(s) + p(t)  1 1 + p(s|t) p(t|s) O RIGINAL DE EN ES FR SV 14.0 0.00 7.90 13.3 4.20 WORD - BASED DE EN ES FR −1 SV MT DE EN ES FR SV – 43.3 54.9 68.2 34.1 49.1 – 25.1 39.6 5.20 62.6 27.6 – 32.8 21.6 52.8 34.8 12.3 – 33.7 60.4 0.00 18.3 57.8 – P HRASE - BASED MT DE Other association measures would be possible as well but Smadja et al. (1996) argue that the Dice coefficient is more robust with respect to low frequency events than other common metrics such as pointwise mutual information, which can be a serious issue with the unsmoothed probability estimations in stan"
W14-1614,N12-1052,0,0.404703,"Missing"
W14-1614,N13-1126,1,0.679337,"Missing"
W14-1614,H01-1035,0,0.857418,"nguages (Bender, 2013). Many applications require robust tools and the development of language-specific resources is expensive and time consuming. Furthermore, many tasks such as data-driven syntactic parsing require strong supervision to achieve reasonable results for real-world applications, since the performance of fully unsupervised methods lags behind by a large margin in comparison with the state of the Previous Cross-Lingual Approaches Annotation projection relies on the mapping of linguistic annotation across languages using parallel corpora and automatic alignment as basic resources (Yarowsky et al., 2001; Hwa et al., 2005; T¨ackstr¨om et al., 2013a). Tools that exist for the source language are used to annotate the source side of the corpus and projection heuristics are then applied to map the annotation through word alignment onto the corresponding target language text. Target language tools can then be trained on the projected annotation assuming that the mapping is sufficiently correct. Less frequent, but also possible, is the scenario where the source side of the corpus contains manual annotation (Agi´c et al., 2012). This addresses the problem created by projecting noisy annotations, but"
W14-1614,I08-3008,0,0.60358,"rsers on. More elaborated phrase-based models together with advanced annotation projection strategies do not necessarily lead to any improvements. As future work, we want to improve our model by (i) studying the impact of other SMT properties and improve the quality of treebank translation, (ii) implementing more sophisticated methods for annotation projection and (iii) using n-best lists provided by SMT models to introduce additional synthetic data using a single resource. We also aim at (iv) applying our approach to transfer parsing for closely related languages (see Agi´c et al. (2012) and Zeman and Resnik (2008) for related work), (v) testing it in a multi-source transfer scenario (McDonald et al., 2011) and, finally, (vi) comparing different dependency parsing paradigms within our experimental framework. Multi-source approaches are especially appealing using the translation approach. However, initial experiments (which we omit in this presentation) revealed that simple concatenation is not sufficient to obtain results that improve upon the single-best translated treebanks. A careful selection of appropriate training examples and their weights given to the training procedure seems to be essential to"
W14-1614,P11-2033,1,0.630094,"to slightly higher scores as we have noted in our experiments but we do not report those numbers here. Note also that the columns represent the target languages (used for testing), while the rows denote the source languages (used in training), as in McDonald et al. (2013). From the table, we can see that the baseline scores are compatible with the ones in the original experiments presented by (McDonald et al., 2013), included in Table 3 for reference. The differences are due to parser selection, as they use a transition-based parser with beam search and perceptron learning along the lines of Zhang and Nivre (2011) whereas we rely on greedy transition-based parsing with linear support vector machines. In the following, we will compare results to our baseline as we have a comparable setup in those experiments. However, most improvements shown below also apply in comparison with (McDonald et al., 2013). 135 5.2 Translated Treebanks Now we turn to the experiments on translated treebanks. We consider two setups. First, we look at the effect of translation when training delexicalized parsers. In this way, we can perform a direct comparison to the baseline performance presented WORD - BASED M ONOLINGUAL DE EN"
W14-1614,P09-1007,0,0.0505267,"omain. Secondly, we can assume that SMT will produce output that is much closer to the input than manual translations in parallel texts usually are. Even if this may seem like a short-coming in general, in the case of annotation projection it should rather be an advantage, because it makes it more straightforward and less error-prone to transfer annotation from source to target. Furthermore, the alignment between words and phrases is inherently provided as an output of all common SMT models. Hence, no additional procedures have to be performed on top of the translated corpus. Recent research (Zhao et al., 2009; Durrett et al., 2012) has attempted to address synthetic data creation for syntactic parsing via bilingual lexica. We seek to build on this work by utilizing more advanced translation techniques. Further in the paper, we first describe the tools and resources used in our experiments (§2). We elaborate on our approach to translating treebanks (§3) and projecting syntactic annotations (§4) for a new language. Finally, we provide empirical evaluation of the suggested approach (§5) and observe a substantial increase in parsing accuracy over the delexicalized parsing baselines. 2 Resources and To"
W14-1614,E12-2012,1,\N,Missing
W14-1614,W06-2920,0,\N,Missing
W14-1614,Q13-1001,1,\N,Missing
W14-1614,D07-1096,1,\N,Missing
W14-1614,P13-2017,1,\N,Missing
W14-4203,agic-ljubesic-2014-setimes,1,0.849797,"Missing"
W14-4203,D12-1001,0,0.038206,"tively small POS tagsets. In our contribution, the goal is to observe the properties of cross-lingual parsing in an environment of relatively free-word-order languages, which are related and characterized by rich morphology and very large morphosyntactic tagsets. We experiment with four different small- and medium-size dependency treebanks of Croatian and Slovene, and cross-lingually parse into Croatian, Serbian and Slovene. Along with monolingual and direct transfer parsing, we make use of the SMT framework of Tiedemann et al. (2014). We are motivated by: ∎ ∎ Other approaches: More recently, Durrett et al. (2012) suggested a hybrid approach that involves bilingual lexica in cross-lingual phrasebased parsing. In their approach, a source-side treebank is adapted to a target language by ”translating” the source words to target words through a bilingual lexicon. This approach is advanced by Tiedemann et al. (2014), who utilize fullscale statistical machine translation (SMT) systems for generating synthetic target language treebanks. This approach relates to annotation projection, while bypassing the issue of dependency parsing noise as gold standard annotations are projected. The SMT noise is in turn miti"
W14-4203,dzeroski-etal-2006-towards,0,0.0614452,"Missing"
W14-4203,erjavec-etal-2010-jos,1,0.904291,"Missing"
W14-4203,W13-4903,1,0.893948,"Missing"
W14-4203,agic-etal-2014-croatian,1,0.86035,"Missing"
W14-4203,W11-2123,0,0.0105658,"eebank translation experiments. Namely, our translations are based on automatic word alignment and subsequent extraction of translation equivalents as common in phrase-based SMT. We perform word alignment by using GIZA++ (Och and Ney, 2003), while utilizing IBM model 4 for creating the Viterbi word alignments for parallel corpora. For the extraction of translation tables, we use the de facto standard SMT toolbox Moses (Koehn et al., 2007) with default settings. Phrasebased SMT models are tuned using minimum error rate training (Och, 2003). Our monolingual language modeling using KenLM tools9 (Heafield, 2011) produces standard 5-gram language models using modified Kneser-Ney smoothing without pruning. For building the translation models, we use the OpenSubtitles parallel resources from OPUS10 (Tiedemann, 2009) for the Croatian-Slovene pair. Even if we expect this to be a rather noisy parallel resource, we justify the choice by (1) the fact that no other parallel corpora11 of Croatian and Slovene exist, other than Orwell’s 1984 from the Multext East project, which is too small for SMT training and falls into a very narrow domain, and (2) evidence from (Tiedemann et al., 2014) that the SMT-supported"
W14-4203,P07-2045,0,0.0036197,"cies are simply copied. Treebank Translation and Annotation Projection For machine translation, we closely adhere to the setup implemented by Tiedemann et al. (2014) in their treebank translation experiments. Namely, our translations are based on automatic word alignment and subsequent extraction of translation equivalents as common in phrase-based SMT. We perform word alignment by using GIZA++ (Och and Ney, 2003), while utilizing IBM model 4 for creating the Viterbi word alignments for parallel corpora. For the extraction of translation tables, we use the de facto standard SMT toolbox Moses (Koehn et al., 2007) with default settings. Phrasebased SMT models are tuned using minimum error rate training (Och, 2003). Our monolingual language modeling using KenLM tools9 (Heafield, 2011) produces standard 5-gram language models using modified Kneser-Ney smoothing without pruning. For building the translation models, we use the OpenSubtitles parallel resources from OPUS10 (Tiedemann, 2009) for the Croatian-Slovene pair. Even if we expect this to be a rather noisy parallel resource, we justify the choice by (1) the fact that no other parallel corpora11 of Croatian and Slovene exist, other than Orwell’s 1984"
W14-4203,berovic-etal-2012-croatian,1,0.897051,"Missing"
W14-4203,H05-1066,0,0.170863,"Missing"
W14-4203,E12-1009,0,0.0200767,"In contrast, the MTE 4 tagsets are not adjusted, i.e., each test set only has a single language-specific MTE 4 annotation. We rely on their underlying similarities in feature representations to suffice for improved cross-lingual parsing performance. 3 3.2 In all experiments, we use the graph-based dependency parser by Bohnet (2010) with default settings. We base our parser choice on its stateof-the-art performance across various morphologically rich languages in the SPMLR 2013 shared task (Seddah et al., 2013). While newer contributions targeted at joint morphological and syntactic analysis (Bohnet and Kuhn, 2012; Bohnet et al., 2013) report slightly higher scores, we chose the former one for speed and robustness, and because we use gold standard POS/MSD annotations. The choice of gold standard preprocessing is motivated by previous research in parsing Croatian and Serbian (Agi´c et al., 2013), and by insight of Seddah et al. (2013), who report a predictable linear decrease in accuracy for automatic preprocessing. This decrease amounts to approximately 3 points LAS for Croatian and Serbian across various test cases in (Agi´c et al., 2013). We observe effects of (de)lexicalization and of using full MSD"
W14-4203,D11-1006,0,0.0813785,"from the source treebank for training the parser (Zeman and Resnik, 2008; McDonald et al., 2013). This in turn relies on the same underlying feature model, typically drawing from a shared part-of-speech (POS) representation such as the Universal POS Tagset of Petrov et al. (2012). Negative effects of using such an impoverished shared representation are typically addressed by adapting the model to better fit the target language. This includes selecting source language data points appropriate for the target language (Søgaard, 2011; T¨ackstr¨om et al., 2013), transferring from multiple sources (McDonald et al., 2011) and using cross-lingual word clusters (T¨ackstr¨om et al., 2012). These approaches need no projection and enable the usage of source-side gold standard annotations, but they all rely on a shared feature representation across languages, which can be seen as a strong bottleneck. Also, while most of the earlier research made use of heterogenous treebanks and thus yielded linguistically implausible observations, research stemming from an uniform dependency scheme across languages (De Marneffe and Manning, 2008; McDonald et al., 2013) made it possible to perform more consistent experiments and to"
W14-4203,Q13-1034,0,0.0226137,"tagsets are not adjusted, i.e., each test set only has a single language-specific MTE 4 annotation. We rely on their underlying similarities in feature representations to suffice for improved cross-lingual parsing performance. 3 3.2 In all experiments, we use the graph-based dependency parser by Bohnet (2010) with default settings. We base our parser choice on its stateof-the-art performance across various morphologically rich languages in the SPMLR 2013 shared task (Seddah et al., 2013). While newer contributions targeted at joint morphological and syntactic analysis (Bohnet and Kuhn, 2012; Bohnet et al., 2013) report slightly higher scores, we chose the former one for speed and robustness, and because we use gold standard POS/MSD annotations. The choice of gold standard preprocessing is motivated by previous research in parsing Croatian and Serbian (Agi´c et al., 2013), and by insight of Seddah et al. (2013), who report a predictable linear decrease in accuracy for automatic preprocessing. This decrease amounts to approximately 3 points LAS for Croatian and Serbian across various test cases in (Agi´c et al., 2013). We observe effects of (de)lexicalization and of using full MSD tagset as opposed to"
W14-4203,C10-1011,0,0.0126143,"ne. The attributes are language-dependent, as well as their positions in the tag, which are also dependent on the part of speech, denoted by position zero in the tag. and we leave SMT and annotation projection into Serbian for future work. tation layer matching its training set. In contrast, the MTE 4 tagsets are not adjusted, i.e., each test set only has a single language-specific MTE 4 annotation. We rely on their underlying similarities in feature representations to suffice for improved cross-lingual parsing performance. 3 3.2 In all experiments, we use the graph-based dependency parser by Bohnet (2010) with default settings. We base our parser choice on its stateof-the-art performance across various morphologically rich languages in the SPMLR 2013 shared task (Seddah et al., 2013). While newer contributions targeted at joint morphological and syntactic analysis (Bohnet and Kuhn, 2012; Bohnet et al., 2013) report slightly higher scores, we chose the former one for speed and robustness, and because we use gold standard POS/MSD annotations. The choice of gold standard preprocessing is motivated by previous research in parsing Croatian and Serbian (Agi´c et al., 2013), and by insight of Seddah"
W14-4203,W06-2920,0,0.559283,"ing from the Multext East project (Erjavec, 2012). A detailed assessment of the current state of development for morphosyntactic and syntactic processing of these languages is given by Agi´c et al. (2013) and Uszkoreit and Rehm (2012). Here, we provide only a short description. 2.1 sl PDT: The PDT-based Slovene Dependency Treebank (Dˇzeroski et al., 2006) is built on top of a rather small portion of Orwell’s novel 1984 from the Multext East project (Erjavec, 2012). Even if the project was discontinued, it is still heavily used as part of the venerable CoNLL 2006 and 2007 shared task datasets (Buchholz and Marsi, 2006; Nivre et al., 2007).4 Treebanks sl SSJ: The Slovene take on simplifying syntactic annotations resulted in the 10-tag strong JOS Corpus of Slovene (Erjavec et al., 2010). Similar to hr SET, this new annotation scheme is loosely We use two Croatian and two Slovene dependency treebanks.1 One for each language is based on the Prague Dependency Treebank (PDT) (B¨ohmov´a et al., 2003) annotation scheme, while the other two introduced novel and more simplified syntactic tagsets. All four treebanks use adaptations of 2 HOBS is available through META-SHARE (Tadi´c and V´aradi, 2012). 3 http://nlp.ffz"
W14-4203,W08-1301,0,0.120382,"Missing"
W14-4203,J03-1002,0,0.00392914,"om full-blown SMT phrase tables on a much larger scale. The trees projection from source to target is trivial since the number and the ordering of words between them does not change. Thus, the dependencies are simply copied. Treebank Translation and Annotation Projection For machine translation, we closely adhere to the setup implemented by Tiedemann et al. (2014) in their treebank translation experiments. Namely, our translations are based on automatic word alignment and subsequent extraction of translation equivalents as common in phrase-based SMT. We perform word alignment by using GIZA++ (Och and Ney, 2003), while utilizing IBM model 4 for creating the Viterbi word alignments for parallel corpora. For the extraction of translation tables, we use the de facto standard SMT toolbox Moses (Koehn et al., 2007) with default settings. Phrasebased SMT models are tuned using minimum error rate training (Och, 2003). Our monolingual language modeling using KenLM tools9 (Heafield, 2011) produces standard 5-gram language models using modified Kneser-Ney smoothing without pruning. For building the translation models, we use the OpenSubtitles parallel resources from OPUS10 (Tiedemann, 2009) for the Croatian-Sl"
W14-4203,P03-1021,0,0.0162054,"re to the setup implemented by Tiedemann et al. (2014) in their treebank translation experiments. Namely, our translations are based on automatic word alignment and subsequent extraction of translation equivalents as common in phrase-based SMT. We perform word alignment by using GIZA++ (Och and Ney, 2003), while utilizing IBM model 4 for creating the Viterbi word alignments for parallel corpora. For the extraction of translation tables, we use the de facto standard SMT toolbox Moses (Koehn et al., 2007) with default settings. Phrasebased SMT models are tuned using minimum error rate training (Och, 2003). Our monolingual language modeling using KenLM tools9 (Heafield, 2011) produces standard 5-gram language models using modified Kneser-Ney smoothing without pruning. For building the translation models, we use the OpenSubtitles parallel resources from OPUS10 (Tiedemann, 2009) for the Croatian-Slovene pair. Even if we expect this to be a rather noisy parallel resource, we justify the choice by (1) the fact that no other parallel corpora11 of Croatian and Slovene exist, other than Orwell’s 1984 from the Multext East project, which is too small for SMT training and falls into a very narrow domain"
W14-4203,E12-1015,1,0.822716,"al., 2014) that the SMT-supported cross-lingual parsing approach is very robust to translation noise. For translating Croatian treebanks into Slovene and vice versa, we implement and test four different methods of translation. They are coupled with approaches to annotation projection from the source side gold dependency trees to the target translations via the word alignment information available from SMT. CHAR: By this acronym, we refer to an approach known as character-based statistical machine translation. It is shown to perform very well for closely related languages (Vilar et al., 2007; Tiedemann, 2012; Tiedemann and Nakov, 2013). The motivation for character-level translation is the ability of such models to better generalize the mapping between similar languages especially in cases of rich productive morphology and limited amounts of training data. With this, character-level models largely reduce the number of out-of-vocabulary words. In a nutshell, our character-based model performs word-to-word translation using character-level modeling. Similar to LOOKUP, this is also a word-to-word translation model, which also requires no adaptation of the source dependency trees – they are once agai"
W14-4203,petrov-etal-2012-universal,0,0.133887,"Missing"
W14-4203,C14-1175,1,0.913328,"h efforts directed towards their processing despite 13 Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 13–24, c October 29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics dency treebank, i.e., it is already manually annotated for syntactic dependencies (Agi´c et al., 2012). This removes the automatic parsing noise, while the issues with word alignment and annotation heuristics still remain. better word alignment quality for synthetic data. The influence of various projection algorithms in this approach is further investigated by Tiedemann (2014). This line of cross-lingual parsing research substantially improves over previous work. Model transfer: In its simplest form, transferring a model amounts to training a source language parser and running it directly on the target language. It is usually coupled with delexicalization, i.e., removing all lexical features from the source treebank for training the parser (Zeman and Resnik, 2008; McDonald et al., 2013). This in turn relies on the same underlying feature model, typically drawing from a shared part-of-speech (POS) representation such as the Universal POS Tagset of Petrov et al. (201"
W14-4203,W00-0726,0,0.19959,"Missing"
W14-4203,rosa-etal-2014-hamledt,0,0.0286999,"Missing"
W14-4203,W07-0705,0,0.0138065,"e from (Tiedemann et al., 2014) that the SMT-supported cross-lingual parsing approach is very robust to translation noise. For translating Croatian treebanks into Slovene and vice versa, we implement and test four different methods of translation. They are coupled with approaches to annotation projection from the source side gold dependency trees to the target translations via the word alignment information available from SMT. CHAR: By this acronym, we refer to an approach known as character-based statistical machine translation. It is shown to perform very well for closely related languages (Vilar et al., 2007; Tiedemann, 2012; Tiedemann and Nakov, 2013). The motivation for character-level translation is the ability of such models to better generalize the mapping between similar languages especially in cases of rich productive morphology and limited amounts of training data. With this, character-level models largely reduce the number of out-of-vocabulary words. In a nutshell, our character-based model performs word-to-word translation using character-level modeling. Similar to LOOKUP, this is also a word-to-word translation model, which also requires no adaptation of the source dependency trees – t"
W14-4203,P11-2120,0,0.0583405,"is usually coupled with delexicalization, i.e., removing all lexical features from the source treebank for training the parser (Zeman and Resnik, 2008; McDonald et al., 2013). This in turn relies on the same underlying feature model, typically drawing from a shared part-of-speech (POS) representation such as the Universal POS Tagset of Petrov et al. (2012). Negative effects of using such an impoverished shared representation are typically addressed by adapting the model to better fit the target language. This includes selecting source language data points appropriate for the target language (Søgaard, 2011; T¨ackstr¨om et al., 2013), transferring from multiple sources (McDonald et al., 2011) and using cross-lingual word clusters (T¨ackstr¨om et al., 2012). These approaches need no projection and enable the usage of source-side gold standard annotations, but they all rely on a shared feature representation across languages, which can be seen as a strong bottleneck. Also, while most of the earlier research made use of heterogenous treebanks and thus yielded linguistically implausible observations, research stemming from an uniform dependency scheme across languages (De Marneffe and Manning, 2008;"
W14-4203,zeman-etal-2012-hamledt,0,0.0349623,"Missing"
W14-4203,H01-1035,0,0.288855,"ing the under-resourced (target) language. Annotation projection: In this approach, dependency trees are projected from a source language to a target language using word alignments in parallel corpora. It is based on a presumption that source-target parallel corpora are more readily available than dependency treebanks. The approach comes in two varieties. In the first one, parallel corpora are exploited by applying the available state-of-the-art parsers on the source side and subsequent projection to the target side using word alignments and heuristics for resolving possible link ambiguities (Yarowsky et al., 2001; Hwa et al., 2005). Since dependency parsers typically make heavy use of various morphological and other features, the apparent benefit of this approach is the possibility of straightforward projection of these features, resulting in a featurerich representation for the target language. On the downside, the annotation projection noise adds up to dependency parsing noise and errors in word alignment, influencing the quality of the resulting target language parser. The other variety is rare, since it relies on parallel corpora in which the source side is a depenIntroduction A large majority of"
W14-4203,I08-3008,0,0.473743,"the issues with word alignment and annotation heuristics still remain. better word alignment quality for synthetic data. The influence of various projection algorithms in this approach is further investigated by Tiedemann (2014). This line of cross-lingual parsing research substantially improves over previous work. Model transfer: In its simplest form, transferring a model amounts to training a source language parser and running it directly on the target language. It is usually coupled with delexicalization, i.e., removing all lexical features from the source treebank for training the parser (Zeman and Resnik, 2008; McDonald et al., 2013). This in turn relies on the same underlying feature model, typically drawing from a shared part-of-speech (POS) representation such as the Universal POS Tagset of Petrov et al. (2012). Negative effects of using such an impoverished shared representation are typically addressed by adapting the model to better fit the target language. This includes selecting source language data points appropriate for the target language (Søgaard, 2011; T¨ackstr¨om et al., 2013), transferring from multiple sources (McDonald et al., 2011) and using cross-lingual word clusters (T¨ackstr¨om"
W14-4203,N12-1052,0,0.0746976,"Missing"
W14-4203,N13-1126,0,0.244645,"Missing"
W14-4203,W03-2906,0,0.0299337,"Missing"
W14-4203,C12-3054,0,0.062378,"Missing"
W14-4203,R13-1088,1,0.834557,"the SMT-supported cross-lingual parsing approach is very robust to translation noise. For translating Croatian treebanks into Slovene and vice versa, we implement and test four different methods of translation. They are coupled with approaches to annotation projection from the source side gold dependency trees to the target translations via the word alignment information available from SMT. CHAR: By this acronym, we refer to an approach known as character-based statistical machine translation. It is shown to perform very well for closely related languages (Vilar et al., 2007; Tiedemann, 2012; Tiedemann and Nakov, 2013). The motivation for character-level translation is the ability of such models to better generalize the mapping between similar languages especially in cases of rich productive morphology and limited amounts of training data. With this, character-level models largely reduce the number of out-of-vocabulary words. In a nutshell, our character-based model performs word-to-word translation using character-level modeling. Similar to LOOKUP, this is also a word-to-word translation model, which also requires no adaptation of the source dependency trees – they are once again simply copied to target se"
W14-4203,W14-1614,1,0.849521,"Missing"
W14-4203,W13-4917,0,\N,Missing
W14-4203,D07-1096,0,\N,Missing
W14-4203,P13-2017,0,\N,Missing
W15-0126,S14-2081,1,0.886398,"ns and Almeida (2014) adapt TurboParser (Martins et al., 2013) for graph processing, and Ribeyre et al. (2014) utilize the parser of Sagae and Tsujii (2008). Graph parsing by tree approximations and post-processing was most notably performed by the topperforming system of the competition, the one by Du et al. (2014). Their tree approximations are obtained by depth-first and breadth-first graph traversals, possibly changing the edge directions in the direction of the traversal. However, this was not sufficient to win the competition, since two other teams – Schluter et al. (2014) and Agi´c and Koller (2014) – also implemented a similar approach and scored in the mid range. For overall premium accuracy, Du et al. (2014) applied voting over the outputs of 17 different tree approximation–based systems, which arguably makes for a computationally inefficient resulting system. The single top-performing tree approximation system was the one by Schluter et al. (2014), which is closely followed by Agi´c and Koller (2014). The latter one are the only to provide some linguistic insight into the SDP graphs. 2 Semantic Dependency Graphs In this section, we take a closer look at the semantic dependency graphs"
W15-0126,C10-1011,0,0.057073,"g and untrimming, since conservative trimming does not increase the label sets. 5 Graph Parsing We proceed to evaluate our tree approximations in graph parsing. Here, our previously outlined parsing pipeline is applied: training graphs are converted to trees using different pre-processing approximations, parsers are trained and applied on test data, outputs are converted to graphs and evaluated against the gold standard graphs. We observe the labeled F1 scores (LF) and exact matches (LM). Experiment Setup For dependency tree parsing, we use the mate-tools state-of-the-art graphbased parser of Bohnet (2010). As in the shared task, we experiment in two tracks: the open track, and the closed track. In the closed track, for training the parser, we use only the features available in the SDP training data, i.e., word forms, parts of speech and lemmas. In the open track, we also pack additional features from the SDP companion dataset – automatic dependency and phrase-based parses of the SDP training and testing sets – as well as the Brown clustering features (Brown et al., 1992). For top node detection, we use a sequence tagger based on conditional random fields (CRFs). To guess the top nodes in the c"
W15-0126,J92-4003,0,0.089862,"and exact matches (LM). Experiment Setup For dependency tree parsing, we use the mate-tools state-of-the-art graphbased parser of Bohnet (2010). As in the shared task, we experiment in two tracks: the open track, and the closed track. In the closed track, for training the parser, we use only the features available in the SDP training data, i.e., word forms, parts of speech and lemmas. In the open track, we also pack additional features from the SDP companion dataset – automatic dependency and phrase-based parses of the SDP training and testing sets – as well as the Brown clustering features (Brown et al., 1992). For top node detection, we use a sequence tagger based on conditional random fields (CRFs). To guess the top nodes in the closed track, we use words and POS tags as features, while we add the companion syntactic features in the open track. 5.1 Results The evaluation results are listed in Table 3. The overall performance of our basic DFS tree approximation parser is identical to the one of Agi´c and Koller (2014) in the closed track. In the open track, however, we improve by 2-3 points in LF due to better top node detection, and improved tree parser accuracy due to the introduction of additio"
W15-0126,S14-2080,0,0.0199245,"ork, which we evaluate as the top-performing nonvoting parser based on tree approximations on the SDP data (§5). Related work In the SDP 2014 campaign, Kuhlmann (2014) adapted the tree parsing algorithm of Eisner (1996), while Thomson et al. (2014) implement a novel approach to graph parsing. Martins and Almeida (2014) adapt TurboParser (Martins et al., 2013) for graph processing, and Ribeyre et al. (2014) utilize the parser of Sagae and Tsujii (2008). Graph parsing by tree approximations and post-processing was most notably performed by the topperforming system of the competition, the one by Du et al. (2014). Their tree approximations are obtained by depth-first and breadth-first graph traversals, possibly changing the edge directions in the direction of the traversal. However, this was not sufficient to win the competition, since two other teams – Schluter et al. (2014) and Agi´c and Koller (2014) – also implemented a similar approach and scored in the mid range. For overall premium accuracy, Du et al. (2014) applied voting over the outputs of 17 different tree approximation–based systems, which arguably makes for a computationally inefficient resulting system. The single top-performing tree app"
W15-0126,C96-1058,0,0.12084,"a tree approximation that strikes a good and linguistically plausible empirical balance between loss minimization and parsing accuracy. Outline We provide a detailed account of the properties of SDP graphs (§2), introduce approaches to tree approximations (§3) and evaluate them (§4). We use this linguistic insight to produce a linguistically motivated tree approximation–based parsing framework, which we evaluate as the top-performing nonvoting parser based on tree approximations on the SDP data (§5). Related work In the SDP 2014 campaign, Kuhlmann (2014) adapted the tree parsing algorithm of Eisner (1996), while Thomson et al. (2014) implement a novel approach to graph parsing. Martins and Almeida (2014) adapt TurboParser (Martins et al., 2013) for graph processing, and Ribeyre et al. (2014) utilize the parser of Sagae and Tsujii (2008). Graph parsing by tree approximations and post-processing was most notably performed by the topperforming system of the competition, the one by Du et al. (2014). Their tree approximations are obtained by depth-first and breadth-first graph traversals, possibly changing the edge directions in the direction of the traversal. However, this was not sufficient to wi"
W15-0126,W12-3602,1,0.861041,"aphs from SDP 2014. The three SDP annotation layers over WSJ text stem from different semantic representations, but all result in directed acyclic graphs (DAGs) for describing sentence semantics. The three representations can be characterized as follows (Oepen et al., 2014; Miyao et al., 2014). 1. DM semantic dependencies stem from the gold-standard HPSG annotations of the WSJ text, as provided by the LinGO English Resource Grammar (ERG; Flickinger, 2000; Flickinger et al., 2012). The resource was converted to bi-lexical dependencies in preparation for the task by Oepen and Lønning (2006) and Ivanova et al. (2012) by a two-step lossy conversion. 2. PAS bi-lexical dependencies are also derived from HPSG annotations of PTB, which were originally aimed at providing a training set for the wide-coverage HPSG parser Enju (Miyao and Tsujii, 2008). As noted in the task description, while DM HPSG annotations were manual, the annotations for training Enju were automatically constructed from the Penn Treebank bracketings by Miyao et al. (2004). 3. PCEDT originates from the English part of the Prague Czech–English Dependency Treebank. In this project, the WSJ part of PTB was translated into Czech, and both sides w"
W15-0126,S14-2068,0,0.0196154,"n the SDP evaluation framework. This system implements a tree approximation that strikes a good and linguistically plausible empirical balance between loss minimization and parsing accuracy. Outline We provide a detailed account of the properties of SDP graphs (§2), introduce approaches to tree approximations (§3) and evaluate them (§4). We use this linguistic insight to produce a linguistically motivated tree approximation–based parsing framework, which we evaluate as the top-performing nonvoting parser based on tree approximations on the SDP data (§5). Related work In the SDP 2014 campaign, Kuhlmann (2014) adapted the tree parsing algorithm of Eisner (1996), while Thomson et al. (2014) implement a novel approach to graph parsing. Martins and Almeida (2014) adapt TurboParser (Martins et al., 2013) for graph processing, and Ribeyre et al. (2014) utilize the parser of Sagae and Tsujii (2008). Graph parsing by tree approximations and post-processing was most notably performed by the topperforming system of the competition, the one by Du et al. (2014). Their tree approximations are obtained by depth-first and breadth-first graph traversals, possibly changing the edge directions in the direction of t"
W15-0126,J93-2004,0,0.0542364,"Missing"
W15-0126,P13-2109,0,0.0284804,"y. Outline We provide a detailed account of the properties of SDP graphs (§2), introduce approaches to tree approximations (§3) and evaluate them (§4). We use this linguistic insight to produce a linguistically motivated tree approximation–based parsing framework, which we evaluate as the top-performing nonvoting parser based on tree approximations on the SDP data (§5). Related work In the SDP 2014 campaign, Kuhlmann (2014) adapted the tree parsing algorithm of Eisner (1996), while Thomson et al. (2014) implement a novel approach to graph parsing. Martins and Almeida (2014) adapt TurboParser (Martins et al., 2013) for graph processing, and Ribeyre et al. (2014) utilize the parser of Sagae and Tsujii (2008). Graph parsing by tree approximations and post-processing was most notably performed by the topperforming system of the competition, the one by Du et al. (2014). Their tree approximations are obtained by depth-first and breadth-first graph traversals, possibly changing the edge directions in the direction of the traversal. However, this was not sufficient to win the competition, since two other teams – Schluter et al. (2014) and Agi´c and Koller (2014) – also implemented a similar approach and scored"
W15-0126,S14-2082,0,0.0349885,"between loss minimization and parsing accuracy. Outline We provide a detailed account of the properties of SDP graphs (§2), introduce approaches to tree approximations (§3) and evaluate them (§4). We use this linguistic insight to produce a linguistically motivated tree approximation–based parsing framework, which we evaluate as the top-performing nonvoting parser based on tree approximations on the SDP data (§5). Related work In the SDP 2014 campaign, Kuhlmann (2014) adapted the tree parsing algorithm of Eisner (1996), while Thomson et al. (2014) implement a novel approach to graph parsing. Martins and Almeida (2014) adapt TurboParser (Martins et al., 2013) for graph processing, and Ribeyre et al. (2014) utilize the parser of Sagae and Tsujii (2008). Graph parsing by tree approximations and post-processing was most notably performed by the topperforming system of the competition, the one by Du et al. (2014). Their tree approximations are obtained by depth-first and breadth-first graph traversals, possibly changing the edge directions in the direction of the traversal. However, this was not sufficient to win the competition, since two other teams – Schluter et al. (2014) and Agi´c and Koller (2014) – also"
W15-0126,S14-2056,1,0.866199,"he single top-performing tree approximation system was the one by Schluter et al. (2014), which is closely followed by Agi´c and Koller (2014). The latter one are the only to provide some linguistic insight into the SDP graphs. 2 Semantic Dependency Graphs In this section, we take a closer look at the semantic dependency graphs from SDP 2014. The three SDP annotation layers over WSJ text stem from different semantic representations, but all result in directed acyclic graphs (DAGs) for describing sentence semantics. The three representations can be characterized as follows (Oepen et al., 2014; Miyao et al., 2014). 1. DM semantic dependencies stem from the gold-standard HPSG annotations of the WSJ text, as provided by the LinGO English Resource Grammar (ERG; Flickinger, 2000; Flickinger et al., 2012). The resource was converted to bi-lexical dependencies in preparation for the task by Oepen and Lønning (2006) and Ivanova et al. (2012) by a two-step lossy conversion. 2. PAS bi-lexical dependencies are also derived from HPSG annotations of PTB, which were originally aimed at providing a training set for the wide-coverage HPSG parser Enju (Miyao and Tsujii, 2008). As noted in the task description, while D"
W15-0126,J08-1002,0,0.0147916,"characterized as follows (Oepen et al., 2014; Miyao et al., 2014). 1. DM semantic dependencies stem from the gold-standard HPSG annotations of the WSJ text, as provided by the LinGO English Resource Grammar (ERG; Flickinger, 2000; Flickinger et al., 2012). The resource was converted to bi-lexical dependencies in preparation for the task by Oepen and Lønning (2006) and Ivanova et al. (2012) by a two-step lossy conversion. 2. PAS bi-lexical dependencies are also derived from HPSG annotations of PTB, which were originally aimed at providing a training set for the wide-coverage HPSG parser Enju (Miyao and Tsujii, 2008). As noted in the task description, while DM HPSG annotations were manual, the annotations for training Enju were automatically constructed from the Penn Treebank bracketings by Miyao et al. (2004). 3. PCEDT originates from the English part of the Prague Czech–English Dependency Treebank. In this project, the WSJ part of PTB was translated into Czech, and both sides were manually in accordance with the Prague-style rules for tectogrammatical analysis (Cinkov´a et al., 2009). The dataset is postprocessed by the task organizers to match the requirements for bi-lexical dependencies. Nodes in SDP"
W15-0126,P05-1013,0,0.0536753,"wo types of edge removal, which we name deletion and trimming. In deletion, it is not possible to reconstruct the removed edge in post-processing, i.e. the removed edge is permanently lost. In trimming, by contrast, the removed edge can be reconstructed – or untrimmed – in post-processing, either deterministically or with a certain success rate. In the shared task, a number of systems approached trimming through label overloading. In label overloading, a deletion of one edge is recorded in another kept edge, similar to encoding non-projective dependency arcs in pseudo-projective tree parsing (Nivre and Nilsson, 2005). In post-processing, the information stored in overloaded labels is used to attempt edge untrimming. We proceed to explore several ways of performing tree approximations, which include a mixture of edge removals via deletion and trimming. 3.1 Baselines Three baselines are used in this research. We re-implement the official SDP shared task baseline, and the local edge flipping and depth-first flipping systems of Agi´c and Koller (2014). OFFICIAL : The official baseline tree approximation only performs deletions and artificial edge insertions to satisfy the dependency tree constraints. No trimm"
W15-0126,S14-2008,1,0.883555,"evaluation framework. This system implements a tree approximation that strikes a good and linguistically plausible empirical balance between loss minimization and parsing accuracy. Outline We provide a detailed account of the properties of SDP graphs (§2), introduce approaches to tree approximations (§3) and evaluate them (§4). We use this linguistic insight to produce a linguistically motivated tree approximation–based parsing framework, which we evaluate as the top-performing nonvoting parser based on tree approximations on the SDP data (§5). Related work In the SDP 2014 campaign, Kuhlmann (2014) adapted the tree parsing algorithm of Eisner (1996), while Thomson et al. (2014) implement a novel approach to graph parsing. Martins and Almeida (2014) adapt TurboParser (Martins et al., 2013) for graph processing, and Ribeyre et al. (2014) utilize the parser of Sagae and Tsujii (2008). Graph parsing by tree approximations and post-processing was most notably performed by the topperforming system of the competition, the one by Du et al. (2014). Their tree approximations are obtained by depth-first and breadth-first graph traversals, possibly changing the edge directions in the direction of t"
W15-0126,S14-2012,0,0.0192021,"properties of SDP graphs (§2), introduce approaches to tree approximations (§3) and evaluate them (§4). We use this linguistic insight to produce a linguistically motivated tree approximation–based parsing framework, which we evaluate as the top-performing nonvoting parser based on tree approximations on the SDP data (§5). Related work In the SDP 2014 campaign, Kuhlmann (2014) adapted the tree parsing algorithm of Eisner (1996), while Thomson et al. (2014) implement a novel approach to graph parsing. Martins and Almeida (2014) adapt TurboParser (Martins et al., 2013) for graph processing, and Ribeyre et al. (2014) utilize the parser of Sagae and Tsujii (2008). Graph parsing by tree approximations and post-processing was most notably performed by the topperforming system of the competition, the one by Du et al. (2014). Their tree approximations are obtained by depth-first and breadth-first graph traversals, possibly changing the edge directions in the direction of the traversal. However, this was not sufficient to win the competition, since two other teams – Schluter et al. (2014) and Agi´c and Koller (2014) – also implemented a similar approach and scored in the mid range. For overall premium accuracy,"
W15-0126,C08-1095,0,0.0793274,"roaches to tree approximations (§3) and evaluate them (§4). We use this linguistic insight to produce a linguistically motivated tree approximation–based parsing framework, which we evaluate as the top-performing nonvoting parser based on tree approximations on the SDP data (§5). Related work In the SDP 2014 campaign, Kuhlmann (2014) adapted the tree parsing algorithm of Eisner (1996), while Thomson et al. (2014) implement a novel approach to graph parsing. Martins and Almeida (2014) adapt TurboParser (Martins et al., 2013) for graph processing, and Ribeyre et al. (2014) utilize the parser of Sagae and Tsujii (2008). Graph parsing by tree approximations and post-processing was most notably performed by the topperforming system of the competition, the one by Du et al. (2014). Their tree approximations are obtained by depth-first and breadth-first graph traversals, possibly changing the edge directions in the direction of the traversal. However, this was not sufficient to win the competition, since two other teams – Schluter et al. (2014) and Agi´c and Koller (2014) – also implemented a similar approach and scored in the mid range. For overall premium accuracy, Du et al. (2014) applied voting over the outp"
W15-0126,S14-2034,0,0.235629,"ovel approach to graph parsing. Martins and Almeida (2014) adapt TurboParser (Martins et al., 2013) for graph processing, and Ribeyre et al. (2014) utilize the parser of Sagae and Tsujii (2008). Graph parsing by tree approximations and post-processing was most notably performed by the topperforming system of the competition, the one by Du et al. (2014). Their tree approximations are obtained by depth-first and breadth-first graph traversals, possibly changing the edge directions in the direction of the traversal. However, this was not sufficient to win the competition, since two other teams – Schluter et al. (2014) and Agi´c and Koller (2014) – also implemented a similar approach and scored in the mid range. For overall premium accuracy, Du et al. (2014) applied voting over the outputs of 17 different tree approximation–based systems, which arguably makes for a computationally inefficient resulting system. The single top-performing tree approximation system was the one by Schluter et al. (2014), which is closely followed by Agi´c and Koller (2014). The latter one are the only to provide some linguistic insight into the SDP graphs. 2 Semantic Dependency Graphs In this section, we take a closer look at th"
W15-0126,S14-2027,0,0.217765,"Missing"
W15-0126,oepen-lonning-2006-discriminant,1,\N,Missing
W15-5301,W13-2408,1,0.900938,"Missing"
W15-5301,agic-etal-2014-croatian,1,0.892533,"Missing"
W15-5301,W14-4203,1,0.874786,"Missing"
W15-5301,C12-2001,0,0.0377519,"Missing"
W15-5301,petrov-etal-2012-universal,0,0.0873193,"Missing"
W15-5301,W06-2920,0,0.0915796,"two sets of experiments. The first one features monolingual parsing of Croatian and the transfer, albeit trivial, of Croatian parsers to Serbian as a target language, while in the second one, we transfer delexicalized parsers from a number of well-resourced languages to Croatian and Serbian as targets in a cross-lingual parsing scenario. 3.1 Data. In the first batch of experiments, we train the parsers on the 3,557 sentences from S E TIMES .H R and Croatian UD, i.e., we omit the development set from all runs. In the second batch, we use the source treebanks from the CoNLL 2006-2007 datasets (Buchholz and Marsi, 2006; Nivre et al., 2007), and the UD version 1.0 release.5 The test sets always remain the same, albeit they do appear in their lexicalized or delexicalized forms: they are the 4 x 100 Croatian and Serbian newswire (NEWS) and Wikipedia (WIKI) samples. Next, we provide a more detailed insight into the experiments as we discuss the results of the two batches. Setup Parser. In all our test runs, we use the graphbased parser of Bohnet (2010).4 It trains and parses very fast, and it records top-level performance across a number of morphologically rich languages (Seddah et al., 2013). Other than that,"
W15-5301,berovic-etal-2012-croatian,1,0.884436,"Missing"
W15-5301,P11-1061,0,0.0311314,"or a similar language—say, Croatian—exist within an uniform representations framework such as UD. This work opened up a cross-lingual parsing research avenue that addresses issues such as multisource transfer, in which multiple source treebanks are combined to improve target language parsing (McDonald et al., 2011), or annotation projection, in which the trees are transferred via parallel corpora and parsers trained on the projections (Tiedemann, 2014). Apart from dependency parsing, this line of work also includes the developments in cross-lingual POS tagging, mainly drawing from the work of Das and Petrov (2011), even if seeded much earlier through the seminal work of Yarowsky et al. (2001). Most of this work, however, does not include the under-resourced SEE languages, and thus we stress that topic in particular in our paper. We introduce a new dependency treebank for Croatian within the Universal Dependencies framework. We construct it on top of the S ETIMES .H R corpus, augmenting the resource by additional part-of-speech and dependency-syntactic annotation layers adherent to the framework guidelines. In this contribution, we outline the treebank design choices, and we use the resource to benchmar"
W15-5301,de-marneffe-etal-2014-universal,0,0.0629764,"Missing"
W15-5301,C14-1175,0,0.0265215,"accuracies. In short, their research indicates that enabling POS tagging and dependency parsing for, e.g., Macedonian would largely benefit should a treebank for a similar language—say, Croatian—exist within an uniform representations framework such as UD. This work opened up a cross-lingual parsing research avenue that addresses issues such as multisource transfer, in which multiple source treebanks are combined to improve target language parsing (McDonald et al., 2011), or annotation projection, in which the trees are transferred via parallel corpora and parsers trained on the projections (Tiedemann, 2014). Apart from dependency parsing, this line of work also includes the developments in cross-lingual POS tagging, mainly drawing from the work of Das and Petrov (2011), even if seeded much earlier through the seminal work of Yarowsky et al. (2001). Most of this work, however, does not include the under-resourced SEE languages, and thus we stress that topic in particular in our paper. We introduce a new dependency treebank for Croatian within the Universal Dependencies framework. We construct it on top of the S ETIMES .H R corpus, augmenting the resource by additional part-of-speech and dependenc"
W15-5301,N13-1070,0,0.0174332,".H R scheme is inherently harder to parse, since it plateaus for both POS feature sets, while UD benefits from the change (back) to UPOS. The first observation is unsurprising given that UPOS differentiates, e.g., between main and auxiliary verbs, or common and proper nouns, while MTE4 POS does not. The second observation is much more interesting, especially given the syntactic tagset differences, as there are only 15 tags in S ETIMES .H R, and 39 in Croatian UD. The result seems to indicate that UD outperforms S ETIMES .H R without sacrificing the expressivity. However, we do note—following Elming et al. (2013)—that our evaluation is intrinsic, and that the two treebanks should be compared on downstream tasks that require parses as input. 5 3.3 ity, as we know from a large body of related work from McDonald et al. (2011) on. In contrast to the CoNLL scores, the UD parsers perform much better, and in much more accordance with our typological intuitions. The best two parsers are trained on Bulgarian and Czech data, the latter one scoring a notable 69.9 and 71.9 points UAS on Croatian and Serbian. The LAS scores are expectedly much lower, and the accuracies are consistent with related work (McDonald et"
W15-5301,H01-1035,0,0.0088822,"mework such as UD. This work opened up a cross-lingual parsing research avenue that addresses issues such as multisource transfer, in which multiple source treebanks are combined to improve target language parsing (McDonald et al., 2011), or annotation projection, in which the trees are transferred via parallel corpora and parsers trained on the projections (Tiedemann, 2014). Apart from dependency parsing, this line of work also includes the developments in cross-lingual POS tagging, mainly drawing from the work of Das and Petrov (2011), even if seeded much earlier through the seminal work of Yarowsky et al. (2001). Most of this work, however, does not include the under-resourced SEE languages, and thus we stress that topic in particular in our paper. We introduce a new dependency treebank for Croatian within the Universal Dependencies framework. We construct it on top of the S ETIMES .H R corpus, augmenting the resource by additional part-of-speech and dependency-syntactic annotation layers adherent to the framework guidelines. In this contribution, we outline the treebank design choices, and we use the resource to benchmark dependency parsing of Croatian and Serbian. We also experiment with cross-ling"
W15-5301,N13-1013,0,0.0236449,"companied by cross-domain test sets for Croatian and Serbian, ii) a set of experiments with parsing the two languages within the UD framework, and iii) cross-lingual parsing experiments targeting Croatian and Serbian by source models from two sets of 10 treebanks. We make our datasets available under free-culture licensing.3 2 text. While the usefulness of this particular approach in contrast to opting for an entirely different text sample could be argued, our decision was motivated by i) facilitating empirical comparability across different annotation schemes, and by ii) the line of work by Johansson (2013) with combining diverse treebanks for improved dependency parsing, which we wish to explore in future work focusing on sharing parsers between closely related languages. Treebank UD requires adherence to POS tagset, dependency attachment, and edge labeling guidelines, as well as to the universal morphological feature specifications, the inclusion of which is at this point not mandatory. We provide an UD treebank for Croatian, implementing all the annotation layers. 2.1 2.2 S ETIMES .H R implements the Multext East version 4 morphosyntactic tagset (MTE4) (Erjavec, 2012). We manually convert it"
W15-5301,D11-1006,0,0.526441,"uation of dependency parsers, and ii) facilitate typologically motivated transfer of dependency parsers to under-resourced languages with improved accuracies. In short, their research indicates that enabling POS tagging and dependency parsing for, e.g., Macedonian would largely benefit should a treebank for a similar language—say, Croatian—exist within an uniform representations framework such as UD. This work opened up a cross-lingual parsing research avenue that addresses issues such as multisource transfer, in which multiple source treebanks are combined to improve target language parsing (McDonald et al., 2011), or annotation projection, in which the trees are transferred via parallel corpora and parsers trained on the projections (Tiedemann, 2014). Apart from dependency parsing, this line of work also includes the developments in cross-lingual POS tagging, mainly drawing from the work of Das and Petrov (2011), even if seeded much earlier through the seminal work of Yarowsky et al. (2001). Most of this work, however, does not include the under-resourced SEE languages, and thus we stress that topic in particular in our paper. We introduce a new dependency treebank for Croatian within the Universal De"
W17-0401,Q16-1022,1,0.866877,"Missing"
W17-0401,P12-1066,0,0.191979,"over cross-lingual POS tags and varying quantities of input text, our method remarkably outperformed even the informed upper bound delexicalized system. We emphasize the importance of acknowledging specifics of actual low-resource languages through realistic experiment design when proposing solutions aimed at addressing these languages. WALS data has been heavily exploited in NLP research. In that line of work, and partly related to our paper, Søgaard and Wulff (2012) proposed adapting delexicalized parsers through distancebased instance weighting over WALS data. Their work in turn relates to Naseem et al. (2012), who also use WALS features in a multilingual parser adaptation model. The research by Naseem et al. (2012) and T¨ackstr¨om et al. (2013) addresses the issues with multi-source delexicalized transfer by selectively sharing model parameters, also with typological motivation through WALS features. This line of work has seen subsequent improvements by Zhang and Barzilay (2015), who introduce a hierarchical tensor-based model for constraining the learned representations based on desired feature interactions. Georgi et al. (2010) and Rama and Kolachina (2012) used WALS to evaluate the concept of l"
W17-0401,A00-1031,0,0.0954475,"rsing. The classifier commits early on to one answer, assigning it a high confidence, and for languages with fewer related source languages in the model, source selection might be significantly off. For example, take this Hungarian sentence: count(ti ) f (ti ) = P count(tj ) ∀tj We inherit the properties of the original KL - POS proposal, but we introduce one minor change: while i) special tag values are used to encode sentence beginnings and endings, and ii) the source counts for unseen trigrams are smoothed for the distance to be well-defined, we use linear interpolation smoothing following Brants (2000) rather than set these counts to 1 in the Rosa and ˇ Zabokrtsk´ y (2015a) implementation. In plain words, this measure compares the relative frequencies of target POS trigrams ti ∈ T to the frequencies of these trigrams in all the sources S ∈ S, and then we select the one associated with the lowest KL divergence. For our example sentence, KL - POS predicts Finnish to be the best source parser. The sentence is, however, in Croatian, for which the Finnish parser ranks as 19/26 in our experiment. In contrast, if we feed KL - POS five sentences at a time, it selects Slovene (1/26). We expect KL -"
W17-0401,P11-1061,0,0.0198803,"it suggests that we have yet to exhaust the search space of predictive features for sentence-level source ranking. For example, we could use the UD development data to learn models that predict the rank7 Related work Research in cross-lingual POS tagging and dependency parsing is nowadays plentiful, but only a fraction of it focuses on truly low-resource languages and realistic proposals. McDonald et al. (2011) were among the first notable exceptions to use real cross-lingual POS taggers in their multi-source parser transfer ex8 periments. They employed the label propagationbased taggers from Das and Petrov (2011). Agi´c et al. (2015; 2016) used a simpler approach to projection, but they were the first to propose multilingual projection for building taggers and parsers for 100+ low-resource languages in one pass. Zeman and Resnik (2008) used perplexity per word as a metric to select the source training instances that relate to the target data. Søgaard (2011) extended their approach to sequences of POS tags, and to multiple sources. Their metrics in turn relate to LANG - ID and KL - POS , but their approach is based on test-set granularity and the selection of appropriate data for training the parsers,"
W17-0401,P15-2034,0,0.112875,"ptation model. The research by Naseem et al. (2012) and T¨ackstr¨om et al. (2013) addresses the issues with multi-source delexicalized transfer by selectively sharing model parameters, also with typological motivation through WALS features. This line of work has seen subsequent improvements by Zhang and Barzilay (2015), who introduce a hierarchical tensor-based model for constraining the learned representations based on desired feature interactions. Georgi et al. (2010) and Rama and Kolachina (2012) used WALS to evaluate the concept of language similarity for fa¨ cilitating cross-lingual NLP. Ostling (2015) used WALS to evaluate word order typologies induced through word alignments. O’Horan et al. (2016) provide a comprehensive survey on the usage of typological information in NLP. Acknowledgements We are thankful to H´ector Mart´ınez Alonso, Barbara Plank, and Natalie Schluter for their valuable comments on an earlier version of the paper. We also thank the anonymous reviewers for their feedback. Finally, we acknowledge the NVIDIA Corporation for supporting our research. References ˇ Zeljko Agi´c, Dirk Hovy, and Anders Søgaard. 2015. If all you have is a bit of the Bible: Learning POS taggers f"
W17-0401,petrov-etal-2012-universal,0,0.0868511,"Missing"
W17-0401,P11-1157,0,0.198584,"Missing"
W17-0401,C10-1044,0,0.10544,"sed instance weighting over WALS data. Their work in turn relates to Naseem et al. (2012), who also use WALS features in a multilingual parser adaptation model. The research by Naseem et al. (2012) and T¨ackstr¨om et al. (2013) addresses the issues with multi-source delexicalized transfer by selectively sharing model parameters, also with typological motivation through WALS features. This line of work has seen subsequent improvements by Zhang and Barzilay (2015), who introduce a hierarchical tensor-based model for constraining the learned representations based on desired feature interactions. Georgi et al. (2010) and Rama and Kolachina (2012) used WALS to evaluate the concept of language similarity for fa¨ cilitating cross-lingual NLP. Ostling (2015) used WALS to evaluate word order typologies induced through word alignments. O’Horan et al. (2016) provide a comprehensive survey on the usage of typological information in NLP. Acknowledgements We are thankful to H´ector Mart´ınez Alonso, Barbara Plank, and Natalie Schluter for their valuable comments on an earlier version of the paper. We also thank the anonymous reviewers for their feedback. Finally, we acknowledge the NVIDIA Corporation for supporting"
W17-0401,C12-2095,0,0.0124689,"er WALS data. Their work in turn relates to Naseem et al. (2012), who also use WALS features in a multilingual parser adaptation model. The research by Naseem et al. (2012) and T¨ackstr¨om et al. (2013) addresses the issues with multi-source delexicalized transfer by selectively sharing model parameters, also with typological motivation through WALS features. This line of work has seen subsequent improvements by Zhang and Barzilay (2015), who introduce a hierarchical tensor-based model for constraining the learned representations based on desired feature interactions. Georgi et al. (2010) and Rama and Kolachina (2012) used WALS to evaluate the concept of language similarity for fa¨ cilitating cross-lingual NLP. Ostling (2015) used WALS to evaluate word order typologies induced through word alignments. O’Horan et al. (2016) provide a comprehensive survey on the usage of typological information in NLP. Acknowledgements We are thankful to H´ector Mart´ınez Alonso, Barbara Plank, and Natalie Schluter for their valuable comments on an earlier version of the paper. We also thank the anonymous reviewers for their feedback. Finally, we acknowledge the NVIDIA Corporation for supporting our research. References ˇ Ze"
W17-0401,D15-1039,0,0.079671,"Missing"
W17-0401,P15-2040,0,0.379798,"Missing"
W17-0401,P16-2091,1,0.863128,"Missing"
W17-0401,W15-2209,0,0.0374534,"Missing"
W17-0401,P12-3005,0,0.0286625,"Missing"
W17-0401,N06-2033,0,0.0585977,"entences. The experiment workflow is condensed in Algorithm 1. It shows how we arrive at best source predictions and reparsed trees for a target sample T . In the algorithm sketch, we assume g = |T |, i.e., the granularity is implied by the sample size, but further we provide results for varying g. Any edge weighting in reparsing is made internal to DMST. Reparsing. We collect all single-source parses of target sentences t ∈ T into a dependency graph. The graph Gt = (V, E) has target tokens as vertices V . The edges (uS , v) ∈ E originate in the delexicalized source parsers hS , ∀S. Following Sagae and Lavie (2006), we can apply directed maximum spanning tree decoding DMST(Gt ), resulting in a voted dependency Summary. We discern that our COMBINED approach yields the best overall scores in the realistic scenario, both in source selection and in reparsing. The latter score remarkably even surpasses the informed upper bound SINGLE - BEST system by 0.36 points UAS. It reaches the highest UAS over cross-lingual POS in both selection and reparsing, while KL - POS closely beats it in reparsing over fully supervised POS. We form a general ordering of the four approaches following these summary results: COMBINE"
W17-0401,P14-1126,0,0.300733,"Missing"
W17-0401,C12-2115,0,0.591602,"f delexicalized parsers. It is a robust and scalable source parser selection and reparsing system for low-resource languages. In a realistic experiment over cross-lingual POS tags and varying quantities of input text, our method remarkably outperformed even the informed upper bound delexicalized system. We emphasize the importance of acknowledging specifics of actual low-resource languages through realistic experiment design when proposing solutions aimed at addressing these languages. WALS data has been heavily exploited in NLP research. In that line of work, and partly related to our paper, Søgaard and Wulff (2012) proposed adapting delexicalized parsers through distancebased instance weighting over WALS data. Their work in turn relates to Naseem et al. (2012), who also use WALS features in a multilingual parser adaptation model. The research by Naseem et al. (2012) and T¨ackstr¨om et al. (2013) addresses the issues with multi-source delexicalized transfer by selectively sharing model parameters, also with typological motivation through WALS features. This line of work has seen subsequent improvements by Zhang and Barzilay (2015), who introduce a hierarchical tensor-based model for constraining the lear"
W17-0401,D11-1006,0,0.64318,"We use the full UD test sets for all 26 languages. However, we vary the sample size or granularity g in best source prediction. It is implemented as a moving window over the test sets, with sizes of 1 to 100. Baselines and upper bounds. We set the oracle SINGLE - BEST source parsing results as the main reference point for our evaluation. We compare all systems to these scores, as our benchmarking goals are to i) reach SINGLE - BEST performance through best source prediction and to ii) surpass it by weighted reparsing. We compare our approach to the standard multisource delexicalized parser of McDonald et al. (2011) (multi-dir in their paper, MULTI here). In training, we uniformly sample from the contributing sources up to 10k sentences. The experiment workflow is condensed in Algorithm 1. It shows how we arrive at best source predictions and reparsed trees for a target sample T . In the algorithm sketch, we assume g = |T |, i.e., the granularity is implied by the sample size, but further we provide results for varying g. Any edge weighting in reparsing is made internal to DMST. Reparsing. We collect all single-source parses of target sentences t ∈ T into a dependency graph. The graph Gt = (V, E) has tar"
W17-0401,P11-2120,0,0.110568,"anguages and realistic proposals. McDonald et al. (2011) were among the first notable exceptions to use real cross-lingual POS taggers in their multi-source parser transfer ex8 periments. They employed the label propagationbased taggers from Das and Petrov (2011). Agi´c et al. (2015; 2016) used a simpler approach to projection, but they were the first to propose multilingual projection for building taggers and parsers for 100+ low-resource languages in one pass. Zeman and Resnik (2008) used perplexity per word as a metric to select the source training instances that relate to the target data. Søgaard (2011) extended their approach to sequences of POS tags, and to multiple sources. Their metrics in turn relate to LANG - ID and KL - POS , but their approach is based on test-set granularity and the selection of appropriate data for training the parsers, while ours deals with varying input sizes and source parser selection at runtime. Figure 2: Distribution of per-sentence top-scoring source parsers over their test-set ranks. Blue: Percentage of sentences for which the best parser was ranked #1 in test set-based evaluation. Red: Sentences where the best parser was ranked #2-25, i.e., not ranked #1."
W17-0401,N13-1126,0,0.350468,"Missing"
W17-0401,I08-3008,0,0.493895,"in cross-lingual POS tagging and dependency parsing is nowadays plentiful, but only a fraction of it focuses on truly low-resource languages and realistic proposals. McDonald et al. (2011) were among the first notable exceptions to use real cross-lingual POS taggers in their multi-source parser transfer ex8 periments. They employed the label propagationbased taggers from Das and Petrov (2011). Agi´c et al. (2015; 2016) used a simpler approach to projection, but they were the first to propose multilingual projection for building taggers and parsers for 100+ low-resource languages in one pass. Zeman and Resnik (2008) used perplexity per word as a metric to select the source training instances that relate to the target data. Søgaard (2011) extended their approach to sequences of POS tags, and to multiple sources. Their metrics in turn relate to LANG - ID and KL - POS , but their approach is based on test-set granularity and the selection of appropriate data for training the parsers, while ours deals with varying input sizes and source parser selection at runtime. Figure 2: Distribution of per-sentence top-scoring source parsers over their test-set ranks. Blue: Percentage of sentences for which the best par"
W17-0401,D15-1213,0,0.493881,"ted in NLP research. In that line of work, and partly related to our paper, Søgaard and Wulff (2012) proposed adapting delexicalized parsers through distancebased instance weighting over WALS data. Their work in turn relates to Naseem et al. (2012), who also use WALS features in a multilingual parser adaptation model. The research by Naseem et al. (2012) and T¨ackstr¨om et al. (2013) addresses the issues with multi-source delexicalized transfer by selectively sharing model parameters, also with typological motivation through WALS features. This line of work has seen subsequent improvements by Zhang and Barzilay (2015), who introduce a hierarchical tensor-based model for constraining the learned representations based on desired feature interactions. Georgi et al. (2010) and Rama and Kolachina (2012) used WALS to evaluate the concept of language similarity for fa¨ cilitating cross-lingual NLP. Ostling (2015) used WALS to evaluate word order typologies induced through word alignments. O’Horan et al. (2016) provide a comprehensive survey on the usage of typological information in NLP. Acknowledgements We are thankful to H´ector Mart´ınez Alonso, Barbara Plank, and Natalie Schluter for their valuable comments o"
W17-0401,D13-1032,0,0.0872054,"Missing"
W17-0401,P15-2044,1,\N,Missing
W17-0401,L16-1262,0,\N,Missing
W17-0415,P13-2017,0,0.0601873,"Missing"
W17-0415,K15-1033,1,0.889279,"Missing"
W17-0415,L16-1262,0,0.0579019,"Missing"
W17-0415,D15-1039,0,0.0184183,"s: Czech, Chinese, Finnish, English, Ancient Greek-PROIEL, Kazakh, Tamil, and Hebrew (cf. Table 2). Our method differs in that it is entirely empirical, based on delexicalised parsing model similarity. Note that we also control for treebank size and exclude all morphological information. Coptic Hebrew Indonesian, and Dutch Table 1: Representative languages for UD parsing resource development. 3 method we propose for the development of parsing resources is the following: Delexicalised and projection-based parser approaches form the state-of-the-art for cross-lingual dependency parsing systems (Rasooli and Collins, 2015). Moreover, as shown by Agi´c et al. (2016) in upper-bound experiments, languages that are well-known to hold similar syntactic behaviours to one another, given that they come from the same language family, often generate better crosslingual parsers for one another. In our approach, we use delexicalised crosslingual parsing scores to the indicate parser generalisation capacity from one language to another. As such, these parsing scores can be seen as a sort of similarity score between languages. The more similar the POS sequences and associated syntactic structures are between languages, the m"
W17-0415,C10-1011,0,\N,Missing
W17-0415,Q16-1022,1,\N,Missing
W17-1407,W15-5301,1,0.830449,"Missing"
W17-1407,C10-1011,0,0.0267272,"or N if the lemma ends in -nje Table 1: Automatic conversion from UD v1 to UD v2. add morphosyntactic definitions (MSD) following the modified Multext-East version 4 format (Erjavec, 2012) documented in the draft of version 5.2 MSD annotation is first added automatically using the state-of-the-art Croatian tagger described by Ljubeˇsi´c et al. (2016), and then corrected manually by two experts native in Serbian, resulting in gold MSD labels. Once morphologically annotated, the Serbian side of SETimes.HR, coined SETimes.SR, was then parsed using the mate-tools, a graphbased dependency parser (Bohnet, 2010) trained on the Croatian UD v1.2 treebank data. The parser was trained with default parameters. 3 Category Comparison and Adaptation In this step, we perform manual inspection of a sample of parsed sentences in order to decide what categories and relations to use for Serbian. We extract and evaluate a handful of examples of all annotated relations, comparing the annotation to the general guidelines and to the language-specific en2 40 http://nl.ijs.si/ME/V5/msd/html/ In expl reparandum det nummod Out NA NA det:numgov nummod:gov compound amod nmod flat ALL ALL compound det ALL xcomp Context ALL"
W17-1407,W06-2920,0,0.0608056,"annotation guidelines. We describe the automatic and manual annotation procedures, discuss the annotation of Slavicspecific categories (case governing quantifiers, reflexive pronouns, question particles) and propose an approach to handling deverbal nouns in Slavic languages. 1 Introduction The notion Universal Dependencies (UD) refers to an international movement started with the goal to reduce to a minimum cross-linguistic variation in the formalisms used to label syntactic structure (McDonald et al., 2013; Nivre et al., 2016). This goal was defined following multilingual parsing campaigns (Buchholz and Marsi, 2006; Hajiˇc et al., 2009) that revealed substantial cross-linguistic differences in the sets of labels and relations used in different treebanks, making it hard to compare parsers’ performances across languages (McDonald and Nivre, 2007). In this paper, we document the process of building a UD treebank for Serbian underlining the advantages of using the existing general framework, but also data and tools already available for other languages. The availability of shared resources is especially important for languages such as Serbian, which, more than 20 years after the publication of Penn Treebank"
W17-1407,L16-1676,1,0.880613,"Missing"
W17-1407,D07-1013,0,0.040589,"andling deverbal nouns in Slavic languages. 1 Introduction The notion Universal Dependencies (UD) refers to an international movement started with the goal to reduce to a minimum cross-linguistic variation in the formalisms used to label syntactic structure (McDonald et al., 2013; Nivre et al., 2016). This goal was defined following multilingual parsing campaigns (Buchholz and Marsi, 2006; Hajiˇc et al., 2009) that revealed substantial cross-linguistic differences in the sets of labels and relations used in different treebanks, making it hard to compare parsers’ performances across languages (McDonald and Nivre, 2007). In this paper, we document the process of building a UD treebank for Serbian underlining the advantages of using the existing general framework, but also data and tools already available for other languages. The availability of shared resources is especially important for languages such as Serbian, which, more than 20 years after the publication of Penn Treebank (Marcus et al., 1994), still 39 has no resource with annotated syntactic structure, lagging behind its close relatives for which UD annotation is available. Labeled as automatic conversion with manual corrections in the UD documentat"
W17-1407,P13-2017,0,0.063635,"Missing"
W17-1407,C12-1160,1,0.886366,"Missing"
W17-1407,J93-2004,0,\N,Missing
W17-1407,L16-1262,0,\N,Missing
W18-6018,W17-0401,1,0.868213,"Missing"
W18-6018,Q16-1022,1,0.898089,"Missing"
W18-6018,C10-1011,0,0.0400524,"Missing"
W18-6018,W18-5815,0,0.0643468,"Missing"
W18-6018,C18-1006,0,0.0263738,"ependency syntax parsing experiments in both monolingual and cross-lingual approaches. 1 Introduction and Background Shipibo-Konibo is a language of the Panoan family spoken by around 35,000 native speakers in the Amazon region of Peru. It is a language with agglutinative processes, with a majority presence of suffixes and some clitics (neither a word nor an affix). Additionally, it presents word orders different from the dominant Spanish language. To the best of our knowledge, there are no other Universal Dependencies (UD) treebanks for an indigenous language of South America, as surveyed by Mager et al. (2018). The closest resource is a treebank developed for a Quechuan variant; however, it was not designed under the UD guidelines (Rios et al., 2008). Another related case is the application of UD for the annotation of the native North American language Arapaho (Algonquian) (Wagner et al., 2016). Thus, ShipiboKonibo would be the first South American indigenous language with this kind of computational resource1 . Natural Language Processing (NLP) efforts for Shipibo-Konibo have developed a POS-tagger, a 1 2 Treebank Annotation The annotation workflow of the Universal Dependencies (UD) treebank for Sh"
W18-6018,de-marneffe-etal-2014-universal,0,0.0908303,"Missing"
W18-6018,L18-1655,1,0.845116,"Missing"
W18-6018,D13-1032,0,0.0267182,"ing noun head). So adjective precedes noun head order dominates versus the earlier finding by Faust (1973) reported in WALS of no dominant order. 5 Monolingual Parsing 5.2 Cross-Lingual Parsing We conducted an experiment with single-source cross-lingual delexicalized parser transfer from the UD v2.0 source languages into ShipiboKonibo as the target language, in the vein of Zeman and Resnik (2008). In the experiment, we used the mate-tools graph-based parser by Bohnet (2010) with default settings. The entire Shipibo-Konibo treebank was our test set. We tagged the treebank for POS using MarMoT (Mueller et al., 2013) via 10-fold crossvalidation with a mean accuracy of 93.94±1.38 (s.d.). As we performed delexicalized transfer, all training and test data used only the following CoNLL-U features: ID, POS, HEAD, and DEParsing for Shipibo-Konibo Dependency syntax parsing is a complex task that usually requires a lot of annotated data, thus we decided to perform experiments in two different scenarios. The first one treats the treebank as an isolated corpus using monolingual methods, whereas the second one presents a cross-lingual experiment to identify which other languages from the UD v2.0 collection can suppo"
W18-6018,W15-1821,0,0.0165722,"ace with BRAT allows the graphical The treebank will be available for the next UD release 151 Proceedings of the Second Workshop on Universal Dependencies (UDW 2018), pages 151–161 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics syntactic relations. For instance, in the ShipiboKonibo sentence ea=ra joke (I came), ea is the pronoun (I) in a dependency of nsubj from the verb joke (came), whereas =ra is an evidential clitic in the dependency of aux:valid. Languages with similar morphological profiles have treebanks in Universal Dependencies, such as Finnish (Pyysalo et al., 2015), Turkish (Sulubacak et al., 2016) or Kazakh (Tyers and Washington, 2015). Nevertheless, those treebanks do not tend to systematically label bound morphemes as independent words, as we aim to do in the development of our treebank because of the reasons mentioned above. annotation of syntactic information over the segmentation. We used part of speech and relation names determined prior to the decision to conform to UD v2.0. 2. To compile segmented corpus into UD v2.0 format: Gather all annotations from ChAnot and BRAT into single file in UD v2.0 format. Compress detail segmentation of prefixes"
W18-6018,zeman-2008-reusable,0,0.0351215,"im Perf, Hab, Iter, Imp, And, Ven Loc, Ela, Abl, Abs, Dat, Dis, Gen, Ill, Abe, Equa, Erg, Com, All, Tem, Ine, Voc, Chez Fh, Nfh Jus, Frus, Des, Imp, Prev, Ind, Int Sing, Plur, Dual 1, 2, 3 Neg, Pos Past1, Past2, Past3, Past4, Past5, Past6, Fut1, Fut2 Part, Inf Mid, Rcp, Act, Cau, App Nomcl, Spcl, Lfcl Universal Morphological Features Tense=Past1, Past2, Past3, Past4, Past5, Past6 Shipibo-Konibo presents six productive past categories. These tense categories are expressed by verbal bound morphemes. These features are presented in Table 4. The universal morphological features of UD are based on Zeman (2008)’s “Reusable tagset conversion using tagset drivers” with the concept of an expandable feature structure that could support any tagset. Tagset labels aim to “distinguish additional lexical and grammatical properties of words, not covered by the POS tags” (Nivre et al., 2017). A list of the morphological features and values used in the Shipibo-Konibo treebank annotation are given in Table 3; most are already defined in Universal Dependencies. The few morphological features of Shipibo-Konibo that require labels not currently in Universal Dependencies are underlined in Table 3. The new morphologi"
W18-6018,I08-3008,0,0.0800688,"Missing"
W18-6018,E12-2021,0,0.0550305,"Missing"
W18-6018,K17-3009,0,0.066992,"Missing"
W18-6018,W16-1719,0,0.0227814,"a majority presence of suffixes and some clitics (neither a word nor an affix). Additionally, it presents word orders different from the dominant Spanish language. To the best of our knowledge, there are no other Universal Dependencies (UD) treebanks for an indigenous language of South America, as surveyed by Mager et al. (2018). The closest resource is a treebank developed for a Quechuan variant; however, it was not designed under the UD guidelines (Rios et al., 2008). Another related case is the application of UD for the annotation of the native North American language Arapaho (Algonquian) (Wagner et al., 2016). Thus, ShipiboKonibo would be the first South American indigenous language with this kind of computational resource1 . Natural Language Processing (NLP) efforts for Shipibo-Konibo have developed a POS-tagger, a 1 2 Treebank Annotation The annotation workflow of the Universal Dependencies (UD) treebank for Shipibo-Konibo is described in §2.1. In particular, specific consideration has been given for word segmentation with respect to clitics, which is detailed in §2.2. 2.1 Annotation Workflow Annotation followed a sequential flow: 1. To annotate Shipibo-Konibo corpus in ChAnot (Mercado et al., 2"
W18-6125,P15-2044,1,0.922313,"Missing"
W18-6125,P12-1073,0,0.107954,"Missing"
W18-6125,2005.mtsummit-papers.11,0,0.136598,"Missing"
W18-6125,Q16-1022,1,0.922249,"Missing"
W18-6125,N16-1030,0,0.0353023,"to train a tagger. We optimize this parameter for maximum NER scores on development data. Language similarity. Some source languages arguably help some targets more than others. We model this relation through language similarity between source and target WALS feature vectors (Dryer and Haspelmath, 2013): vs and vt . We implement language similarity as inverse normalized Hamming distance between the two vectors: 1 − dh (vs , vt ). Only the non-null fields are taken into account. Similarity is contrasted to random selection in our experiment. Tagger. We implement a bi-LSTM NE tagger inspired by Lample et al. (2016) and Plank et al. (2016). We tune it on English development data at two bi-LSTM layers (d = 300), a final dense layer (d = 4), 10 training epochs with SGD, and regular and recurrent dropout at p = 0.5. We use pretrained fastText embeddings (Bojanowski et al., 2017). Currently fastText supports 294 languages and is superior to random initialization in our tagger. Other than through fastText, we don’t make explicit use of sub-word embeddings. Our monolingual F1 score on English is 86.35 under the more standard IOB2 encoding. We do not aim to produce a state-of-the-art model, but to contrast the"
W18-6125,P14-1126,0,0.0505417,"ith heterogeneous datasets, and 2 massive parallel corpora. In terms of crosslingual breadth, ours is one of the largest NER experiments to date,1 and the only one that focuses on standalone annotation projection. We uncover that the specific conditions that do make NER projection work are not trivially met at a feasibly large scale by true low-resource languages. Motivation Annotation projection plays a crucial role in crosslingual NLP. For instance, the state of the art approaches to low-resource part-of-speech tagging (Das and Petrov, 2011; T¨ackstr¨om et al., 2013) and dependency parsing (Ma and Xia, 2014; Rasooli and Collins, 2015) all make use of parallel corpora under the source-target language dichotomy in some way or another. Beyond syntactic tasks, aligned corpora facilitate cross-lingual transfer through multilingual embeddings (Ruder et al., 2017) across diverse tasks. What about named entity recognition (NER)? This sequence labeling task with ample source languages appears like an easy target for projection. However, as recently argued by Mayhew et al. (2017), the issue is more complex: 2 Multilingual projection We project NE labels from multiple sources into multiple targets through"
W18-6125,Q17-1010,0,0.0184823,"eature vectors (Dryer and Haspelmath, 2013): vs and vt . We implement language similarity as inverse normalized Hamming distance between the two vectors: 1 − dh (vs , vt ). Only the non-null fields are taken into account. Similarity is contrasted to random selection in our experiment. Tagger. We implement a bi-LSTM NE tagger inspired by Lample et al. (2016) and Plank et al. (2016). We tune it on English development data at two bi-LSTM layers (d = 300), a final dense layer (d = 4), 10 training epochs with SGD, and regular and recurrent dropout at p = 0.5. We use pretrained fastText embeddings (Bojanowski et al., 2017). Currently fastText supports 294 languages and is superior to random initialization in our tagger. Other than through fastText, we don’t make explicit use of sub-word embeddings. Our monolingual F1 score on English is 86.35 under the more standard IOB2 encoding. We do not aim to produce a state-of-the-art model, but to contrast the scores for various annotation projection parameters. We use our tagger both to annotate the source sides of parallel corpora, and to train projected target language NER models. All reported NE tagging results are means over 4 runs. Tagger performance. Some source N"
W18-6125,magnini-etal-2006-cab,0,0.0478144,"Missing"
W18-6125,D17-1269,0,0.11691,"Missing"
W18-6125,I17-2016,0,0.0916885,"f source languages in projection. Means for all experiment languages. (a) Absolute (b) Relative Figure 4: Absolute and relative counts for NE labels in Europarl and Watchtower for overlapping source languages. Al-Rfou et al. (2015) work with 40 languages where NE annotations are derived from Wikipedia and Freebase, while they use a mix of humanannotated and machine-translated data for evaluation. Similarly, Pan et al. (2017) build and evaluate Wikipedia-based models for 282 languages; out of those, 20 are evaluated for NE linking and 9 for NER on human annotations that are not from Wikipedia. Cotterell and Duh (2017) jointly predict NE for high- and low-resource languages with a character-level neural CRF model. Their evaluation involves 15 diverse languages across 5 language families. The DARPA LORELEI program (Christianson et al., 2018) features challenges in low-resource NER development for “surprise” languages under time constraints. Alternatives. In search for feasible alternatives, we conducted a proof-of-concept replication of the work by Mayhew et al. (2017), who rely on “cheap translation” of training data from multiple sources using bilingual lexicons. The replication involved only one language,"
W18-6125,L16-1689,0,0.041671,"Missing"
W18-6125,P11-1061,0,0.0505594,"ultiple sources for low-resource NER. It includes 17 diverse languages with heterogeneous datasets, and 2 massive parallel corpora. In terms of crosslingual breadth, ours is one of the largest NER experiments to date,1 and the only one that focuses on standalone annotation projection. We uncover that the specific conditions that do make NER projection work are not trivially met at a feasibly large scale by true low-resource languages. Motivation Annotation projection plays a crucial role in crosslingual NLP. For instance, the state of the art approaches to low-resource part-of-speech tagging (Das and Petrov, 2011; T¨ackstr¨om et al., 2013) and dependency parsing (Ma and Xia, 2014; Rasooli and Collins, 2015) all make use of parallel corpora under the source-target language dichotomy in some way or another. Beyond syntactic tasks, aligned corpora facilitate cross-lingual transfer through multilingual embeddings (Ruder et al., 2017) across diverse tasks. What about named entity recognition (NER)? This sequence labeling task with ample source languages appears like an easy target for projection. However, as recently argued by Mayhew et al. (2017), the issue is more complex: 2 Multilingual projection We pr"
W18-6125,P17-1135,0,0.0851595,"Missing"
W18-6125,N13-1073,0,0.0338438,"cate the languages2 and domains they cover. We take into account a set of additional design choices in multi-source NER projection beyond what the algorithm itself encodes. Parallel text. We contrast two sources of parallel data: Europarl (Koehn, 2005) and Watchtower (Agi´c et al., 2016). The former covers only 21 resource-rich languages but with 400k-2M parallel sentences for each language pair, while the latter currently spans over 300 languages, but with only 10-100k sentences per pair. Europarl comes with near-perfect sentence alignment and tokenization, and we align its words using IBM2 (Dyer et al., 2013). For Watchtower we inherit the original noisy preprocessing: simple whitespace tokenization, automatic sentence alignment, and IBM1 word alignments by Agi´c et al. (2016) as they show that IBM1 in particular helps debias for low-resource languages. Sentence selection. We compare two ways to sample the target sentences for training: at random vs. through word-alignment coverage ranking. A target word covered if it has an incoming alignment edge from at least one source word. We mark the target sentences by percentage of covered words from each source, and rank them by mean coverage across sour"
W18-6125,Q13-1001,0,0.0670758,"Missing"
W18-6125,N12-1052,0,0.208691,"Missing"
W18-6125,P17-1178,0,0.0244473,"ield workable POS taggers or dependency parsers (cf. Agi´c et al. 2016). (a) Europarl Figure 3: Cross-lingual NER learning curves for precision, recall, and F1 in relation to the number n of source languages in projection. Means for all experiment languages. (a) Absolute (b) Relative Figure 4: Absolute and relative counts for NE labels in Europarl and Watchtower for overlapping source languages. Al-Rfou et al. (2015) work with 40 languages where NE annotations are derived from Wikipedia and Freebase, while they use a mix of humanannotated and machine-translated data for evaluation. Similarly, Pan et al. (2017) build and evaluate Wikipedia-based models for 282 languages; out of those, 20 are evaluated for NE linking and 9 for NER on human annotations that are not from Wikipedia. Cotterell and Duh (2017) jointly predict NE for high- and low-resource languages with a character-level neural CRF model. Their evaluation involves 15 diverse languages across 5 language families. The DARPA LORELEI program (Christianson et al., 2018) features challenges in low-resource NER development for “surprise” languages under time constraints. Alternatives. In search for feasible alternatives, we conducted a proof-of-c"
W18-6125,W17-1412,0,0.0250552,"Missing"
W18-6125,W02-2024,0,0.67094,"Missing"
W18-6125,P16-2067,0,0.0360319,"imize this parameter for maximum NER scores on development data. Language similarity. Some source languages arguably help some targets more than others. We model this relation through language similarity between source and target WALS feature vectors (Dryer and Haspelmath, 2013): vs and vt . We implement language similarity as inverse normalized Hamming distance between the two vectors: 1 − dh (vs , vt ). Only the non-null fields are taken into account. Similarity is contrasted to random selection in our experiment. Tagger. We implement a bi-LSTM NE tagger inspired by Lample et al. (2016) and Plank et al. (2016). We tune it on English development data at two bi-LSTM layers (d = 300), a final dense layer (d = 4), 10 training epochs with SGD, and regular and recurrent dropout at p = 0.5. We use pretrained fastText embeddings (Bojanowski et al., 2017). Currently fastText supports 294 languages and is superior to random initialization in our tagger. Other than through fastText, we don’t make explicit use of sub-word embeddings. Our monolingual F1 score on English is 86.35 under the more standard IOB2 encoding. We do not aim to produce a state-of-the-art model, but to contrast the scores for various annot"
W18-6125,W13-2412,0,0.0277376,"Missing"
W18-6125,K16-1022,0,0.143428,"Missing"
W18-6125,D15-1039,0,0.0769616,"datasets, and 2 massive parallel corpora. In terms of crosslingual breadth, ours is one of the largest NER experiments to date,1 and the only one that focuses on standalone annotation projection. We uncover that the specific conditions that do make NER projection work are not trivially met at a feasibly large scale by true low-resource languages. Motivation Annotation projection plays a crucial role in crosslingual NLP. For instance, the state of the art approaches to low-resource part-of-speech tagging (Das and Petrov, 2011; T¨ackstr¨om et al., 2013) and dependency parsing (Ma and Xia, 2014; Rasooli and Collins, 2015) all make use of parallel corpora under the source-target language dichotomy in some way or another. Beyond syntactic tasks, aligned corpora facilitate cross-lingual transfer through multilingual embeddings (Ruder et al., 2017) across diverse tasks. What about named entity recognition (NER)? This sequence labeling task with ample source languages appears like an easy target for projection. However, as recently argued by Mayhew et al. (2017), the issue is more complex: 2 Multilingual projection We project NE labels from multiple sources into multiple targets through sentence and word align“For"
W18-6125,P13-1106,0,0.181391,"Missing"
W18-6125,santos-etal-2006-harem,0,0.0412535,"Missing"
W18-6125,P15-1165,1,0.906353,"Missing"
